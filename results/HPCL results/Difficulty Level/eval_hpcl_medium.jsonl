{"predict": "(20, CUDA, CUDA_CHECK(cudaMalloc((void **)&dev_max, sizeof(FP) * SEQLEN * DIM));); (40, CUDA, cudaFree(dev_max);); (22, CUDA, CUDA_CHECK(cudaMemcpy(dev_denom, host_denom, sizeof(FP) * SEQLEN * DIM, cudaMemcpyHostToDevice));); (18, CUDA, CUDA_CHECK(cudaMalloc((void **)&dev_denom, sizeof(FP) * SEQLEN * DIM));); (18, CUDA, CUDA_CHECK(cudaMalloc((void **)&QK, sizeof(FP) * SEQLEN * SEQLEN));); (18, CUDA, CUDA_CHECK(cudaMemcpy(dev_max, host_max, sizeof(FP) * SEQLEN * DIM, cudaMemcpyHostToDevice));); (33, CUDA, cudaFree(QK);); (34, CUDA, cudaFree(dev_denom);)", "label": "(20, CUDA, CUDA_CHECK(cudaMalloc((void **)&dev_max, sizeof(FP) * SEQLEN * DIM));); (39, CUDA, cudaFree(dev_max);); (22, CUDA, CUDA_CHECK(cudaMemcpy(dev_denom, host_denom, sizeof(FP) * SEQLEN * DIM, cudaMemcpyHostToDevice));); (18, CUDA, CUDA_CHECK(cudaMalloc((void **)&dev_denom, sizeof(FP) * SEQLEN * DIM));); (18, CUDA, CUDA_CHECK(cudaMalloc((void **)&QK, sizeof(FP) * SEQLEN * SEQLEN));); (18, CUDA, CUDA_CHECK(cudaMemcpy(dev_max, host_max, sizeof(FP) * SEQLEN * DIM, cudaMemcpyHostToDevice));); (33, CUDA, cudaFree(QK);); (34, CUDA, cudaFree(dev_denom);)"}
{"predict": "(16, OpenMP, #pragma omp critical(out))", "label": "(32, OpenMP, #pragma omp critical(cerr)); (15, OpenMP, #pragma omp critical(out))"}
{"predict": "(31, MPI, MPI_T_finalize();); (4, MPI, MPI_T_init_thread(required, &provided);); (11, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)", "label": "(31, MPI, MPI_T_finalize();); (4, MPI, MPI_T_init_thread(required, &provided);); (6, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(18, OpenMP, #pragma omp parallel for reduction(+ : totalSum))", "label": "(19, OpenMP, #pragma omp parallel for simd reduction(+:totalSum) schedule(guided) aligned(data: 64) num_threads(N_THREADS)); (10, OpenMP, #pragma omp parallel for num_threads(N_THREADS))"}
{"predict": "(16, CUDA, cudaMemcpy(printfbuf_local, bufptr, CUPRINTF_MAX_LEN, cudaMemcpyDeviceToHost);)", "label": "(15, CUDA, cudaMemcpy(printfbuf_local, bufptr, CUPRINTF_MAX_LEN, cudaMemcpyDeviceToHost);)"}
{"predict": "(16, OpenMP, #pragma omp parallel sections firstprivate(, )); (36, OpenMP, #pragma omp parallel sections firstprivate(x)); (9, OpenMP, #pragma omp parallel sections firstprivate(,); (1, OpenMP, #pragma omp parallel sections firstprivate(); (37, OpenMP, #pragma omp parallel sections firstprivate(x, y)); (40, OpenMP, #pragma omp parallel sections firstprivate(x, y, z)); (20, OpenMP, #pragma omp parallel sections firstprivate(int)); (14, OpenMP, #pragma omp parallel sections firstprivate()); (23, OpenMP, #pragma omp parallel sections firstprivate(0))", "label": "(26, OpenMP, #pragma omp parallel sections firstprivate(int)); (10, OpenMP, #pragma omp parallel sections firstprivate(,); (14, OpenMP, #pragma omp parallel sections firstprivate(, )); (34, OpenMP, #pragma omp parallel sections lastprivate(x) firstprivate(x)); (17, OpenMP, #pragma omp parallel sections firstprivate()); (36, OpenMP, #pragma omp parallel sections lastprivate(x, y) firstprivate(x, y)); (25, OpenMP, #pragma omp parallel sections firstprivate(0)); (-3, OpenMP, #pragma omp parallel sections firstprivate(); (37, OpenMP, #pragma omp parallel sections lastprivate(x, y, z) firstprivate(x, y, z))"}
{"predict": "(27, MPI, MPI_Irecv(b, 10, MPI_INT, rank, 0, MPI_COMM_WORLD, &request);); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (15, MPI, MPI_Wait(&request, &status);); (13, MPI, MPI_Isend(a, 10, MPI_INT, rank, 0, MPI_COMM_WORLD, &request);); (5, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (20, MPI, MPI_Get_count(&status, MPI_INT, &count);)", "label": "(29, MPI, MPI_Recv(NULL, 0, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &status);); (16, MPI, MPI_Isend(a, 0, MPI_INT, rank, 0, MPI_COMM_WORLD, &request);); (24, MPI, MPI_Wait(&request, &status);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (24, MPI, MPI_Isend(NULL, 0, MPI_INT, rank, 0, MPI_COMM_WORLD, &request);); (14, MPI, MPI_Get_count(&status, MPI_INT, &count);); (12, MPI, MPI_Recv(b, 1, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &status);)"}
{"predict": "(54, OpenMP, #pragma omp parallel for collapse(2))", "label": "(60, OpenMP, #pragma omp parallel for)"}
{"predict": "(18, OpenCL, auto mappedPtr1 = clEnqueueMapBuffer(&cmdQ, b1, CL_TRUE, CL_MAP_READ, 0, 8, 0, nullptr, nullptr, nullptr);); (31, OpenCL, clReleaseEvent(gatingEvent);); (14, OpenCL, auto b1 = clCreateBuffer(context, CL_MEM_READ_WRITE, 20, nullptr, nullptr);); (19, OpenCL, auto gatingEvent = clCreateUserEvent(context, nullptr);); (19, OpenCL, clEnqueueWaitForEvents(&cmdQ, 1, &gatingEvent);); (27, OpenCL, clReleaseMemObject(b1);)", "label": "(16, OpenCL, auto b1 = clCreateBuffer(context, CL_MEM_READ_WRITE, 20, nullptr, nullptr);); (24, OpenCL, void *ptr2 = clEnqueueMapBuffer(&cmdQ, b2, CL_TRUE, CL_MAP_READ, 0, 8, 0, nullptr, nullptr, nullptr);); (32, OpenCL, clReleaseMemObject(b2);); (17, OpenCL, void *ptr1 = clEnqueueMapBuffer(&cmdQ, b1, CL_FALSE, CL_MAP_READ, 0, 8, 1, &gatingEvent, nullptr, nullptr);); (29, OpenCL, clReleaseMemObject(b1);); (31, OpenCL, clReleaseEvent(gatingEvent);); (25, OpenCL, clEnqueueUnmapMemObject(pCmdQ, b2, ptr2, 0, nullptr, nullptr);); (12, OpenCL, auto gatingEvent = clCreateUserEvent(context, nullptr);); (13, OpenCL, clEnqueueUnmapMemObject(&cmdQ, b1, ptr1, 0, nullptr, nullptr);); (8, OpenCL, auto b2 = clCreateBuffer(context, CL_MEM_READ_WRITE, 20, nullptr, nullptr);)"}
{"predict": "(58, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (4, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (70, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamDestroy(stream));)", "label": "(56, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (4, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (35, CUDA, ASSERT_EQ(cudaSuccess, cudaMemsetAsync(dstData->basePtr(), 0, dstBufSize * sizeof(T), stream));); (54, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamDestroy(stream));)"}
{"predict": "(38, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (44, MPI, MPI_Allgather(&item_offset, 1, MPI_INT, rank_disps, 1, MPI_INT, MPI_COMM_WORLD);)", "label": "(38, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (44, MPI, MPI_Allgather(&item_offset, 1, MPI_INT, rank_disps, 1, MPI_INT, MPI_COMM_WORLD);); (80, MPI, reduce_start = MPI_Wtime();)"}
{"predict": "(26, CUDA, cudaMemcpy(d_A, A, m * k * sizeof(float), cudaMemcpyHostToDevice);); (56, CUDA, cudaFree(d_A);); (31, CUDA, cudaMemcpy(C, d_C, m * n * sizeof(float), cudaMemcpyDeviceToHost);); (55, CUDA, cudaFree(d_B);); (55, CUDA, cudaFree(d_C);); (17, CUDA, cudaMalloc((void **)&d_B, k * n * sizeof(float));); (15, CUDA, cudaMalloc((void **)&d_A, m * k * sizeof(float));); (16, CUDA, cudaMalloc((void **)&d_C, m * n * sizeof(float));); (20, CUDA, cudaMemcpy(d_C, C, m * n * sizeof(float), cudaMemcpyHostToDevice);); (18, CUDA, cudaMemcpy(d_B, B, k * n * sizeof(float), cudaMemcpyHostToDevice);)", "label": "(26, CUDA, cudaMemcpy(d_A, A, m * k * sizeof(float), cudaMemcpyHostToDevice);); (56, CUDA, cudaFree(d_A);); (31, CUDA, cudaMemcpy(C, d_C, m * n * sizeof(float), cudaMemcpyDeviceToHost);); (55, CUDA, cudaFree(d_B);); (55, CUDA, cudaFree(d_C);); (17, CUDA, cudaMalloc((void **)&d_B, k * n * sizeof(float));); (15, CUDA, cudaMalloc((void **)&d_A, m * k * sizeof(float));); (16, CUDA, cudaMalloc((void **)&d_C, m * n * sizeof(float));); (20, CUDA, cudaMemcpy(d_C, C, m * n * sizeof(float), cudaMemcpyHostToDevice);); (18, CUDA, cudaMemcpy(d_B, B, k * n * sizeof(float), cudaMemcpyHostToDevice);)"}
{"predict": "(15, CUDA, CUDASimulation cudaSimulation(model);)", "label": "(15, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(6, TBB, c.set_core_type(tbb::info::core_types().back());); (3, TBB, tbb::task_arena::constraints c;); (46, TBB, double t2d_auto = pforTranspose2d<tbb::auto_partitioner>(N, a, b, gs);); (2, TBB, c.set_numa_id(tbb::info::numa_nodes()[0]);); (37, TBB, pforTranspose2d<tbb::auto_partitioner>(N, a, b, gs);); (40, TBB, double t2d_simple = pforTranspose2d<tbb::simple_partitioner>(N, a, b, gs);); (3, TBB, c.set_max_concurrency(std::min(8, tbb::info::default_concurrency(c)));); (1, TBB, tbb::task_arena a(c);); (28, TBB, double t1d_simple = pforTranspose<tbb::simple_partitioner>(N, a, b, gs);); (30, TBB, pforTranspose<tbb::auto_partitioner>(N, a, b, gs);); (30, TBB, double t1d_auto = pforTranspose<tbb::auto_partitioner>(N, a, b, gs);); (33, TBB, pforTranspose<tbb::simple_partitioner>(N, a, b, gs);)", "label": "(47, TBB, double t2d_auto = pforTranspose2d<tbb::auto_partitioner>(N, a, b, gs);); (45, TBB, pforTranspose2d<tbb::auto_partitioner>(N, a, b, gs);); (4, TBB, c.set_core_type(tbb::info::core_types().back());); (1, TBB, tbb::task_arena::constraints c;); (48, TBB, double t2d_simple = pforTranspose2d<tbb::simple_partitioner>(N, a, b, gs);); (37, TBB, double t1d_simple = pforTranspose<tbb::simple_partitioner>(N, a, b, gs);); (-1, TBB, c.set_numa_id(tbb::info::numa_nodes()[0]);); (29, TBB, pforTranspose<tbb::auto_partitioner>(N, a, b, gs);); (29, TBB, double t1d_auto = pforTranspose<tbb::auto_partitioner>(N, a, b, gs);); (32, TBB, pforTranspose<tbb::simple_partitioner>(N, a, b, gs);); (-2, TBB, c.set_max_concurrency(std::min(8, tbb::info::default_concurrency(c)));); (-2, TBB, tbb::task_arena a(c);); (39, TBB, pforTranspose2d<tbb::simple_partitioner>(N, a, b, gs);)"}
{"predict": "(5, CUDA, cudaStream_t stream = at::cuda::getCurrentCUDAStream();)", "label": "(5, CUDA, cudaStream_t stream = at::cuda::getCurrentCUDAStream();)"}
{"predict": "(21, MPI, MPI_Wait(&request, MPI_STATUS_IGNORE);); (5, MPI, MPI_Send_init(sbuf, size, MPI_BYTE, to, tagno, MPI_COMM_WORLD, &request);); (12, MPI, MPI_Send(NULL, 0, MPI_BYTE, 1, 100, MPI_COMM_WORLD);); (14, MPI, MPI_Recv(NULL, 0, MPI_BYTE, rank_world - 1, 100, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)", "label": "(13, MPI, MPI_Recv(NULL, 0, MPI_BYTE, to, 100, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (11, MPI, MPI_Send(NULL, 0, MPI_BYTE, from, 100, MPI_COMM_WORLD);); (4, MPI, MPI_Irecv(rbuf, size, MPI_BYTE, from, tagno, MPI_COMM_WORLD, &request);); (18, MPI, MPI_Send(sbuf, size, MPI_BYTE, to, tagno, MPI_COMM_WORLD);); (20, MPI, MPI_Wait(&request, MPI_STATUS_IGNORE);)"}
{"predict": "(13, MPI, MPI_Abort(MPI_COMM_WORLD, MPIT_result);); (14, MPI, MPIT_result = MPI_T_pvar_session_create(session);); (8, MPI, MPIT_result = MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);)", "label": "(13, MPI, MPI_Abort(MPI_COMM_WORLD, MPIT_result);); (14, MPI, MPIT_result = MPI_T_pvar_session_create(session);); (8, MPI, MPIT_result = MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);)"}
{"predict": "(8, OpenMP, #pragma omp for schedule(MY_SCHEDULE)); (3, OpenMP, #pragma omp parallel); (6, OpenMP, #pragma omp atomic)", "label": "(9, OpenMP, #pragma omp atomic); (3, OpenMP, #pragma omp parallel); (5, OpenMP, #pragma omp for schedule(MY_SCHEDULE))"}
{"predict": "(14, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (11, CUDA, CUDA_CHECK(cudaGetDevice(&current_device_id));); (11, CUDA, CUDA_CHECK(cudaGetDeviceProperties(&props, current_device_id));)", "label": "(14, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (11, CUDA, CUDA_CHECK(cudaGetDevice(&current_device_id));); (11, CUDA, CUDA_CHECK(cudaGetDeviceProperties(&props, current_device_id));)"}
{"predict": "(3, OpenMP, #pragma omp parallel); (3, OpenMP, #pragma omp master taskloop)", "label": "(3, OpenMP, #pragma omp parallel); (3, OpenMP, #pragma omp master taskloop)"}
{"predict": "(25, MPI, MPI_Comm_rank(comm_row, &mycol);); (25, MPI, MPI_Comm_rank(comm_col, &myrow);); (25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)", "label": "(25, MPI, MPI_Comm_rank(comm_row, &mycol);); (25, MPI, MPI_Comm_rank(comm_col, &myrow);); (25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(56, OpenMP, #pragma omp simd)", "label": "(56, OpenMP, #pragma omp simd)"}
{"predict": "(7, CUDA, __shared__ float block_acc[32];)", "label": "(7, CUDA, __shared__ float block_acc[32];)"}
{"predict": "(25, OpenMP, #pragma omp parallel for)", "label": "(25, OpenMP, #pragma omp parallel for)"}
{"predict": "(35, OpenMP, iis3dhhc_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)", "label": "(35, OpenMP, iis3dhhc_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)"}
{"predict": "(47, OpenMP, #pragma omp target exit data nowait map(from: a) depend(in : argc, argv[i:argc], x[:]) if (target exit data: b)); (38, OpenMP, #pragma omp target exit data map(from: a) if (b > g) nowait depend(in : argc, argv[i:argc], x[:])); (53, OpenMP, #pragma omp target exit data map(from: c) depend(in : argc, argv[i:argc], x[:]) nowait); (7, OpenMP, #pragma omp target exit data map(from: a) if (target exit data: b)); (60, OpenMP, #pragma omp target exit data nowait map(from: x[0:10], c) depend(in : argc, argv[i:argc], x[:])); (10, OpenMP, #pragma omp target exit data map(from: a) if (b > g)); (47, OpenMP, #pragma omp target exit data nowait depend(in : argc, argv[i:argc], x[:]) map(from: a)); (10, OpenMP, #pragma omp target exit data map(release: c) if(b>g)); (13, OpenMP, #pragma omp target exit data map(from: x[0:10], c)); (26, OpenMP, #pragma omp target exit data nowait map(from: a) if (target exit data: b)); (59, OpenMP, #pragma omp target exit data nowait map(from: c) depend(in : argc, argv[i:argc], x[:]) map(release: d)); (28, OpenMP, #pragma omp target exit data map(from: a) if (b > g) nowait); (10, OpenMP, #pragma omp target exit data map(always, delete: x[0:10])); (11, OpenMP, #pragma omp target exit data map(from: c) map(release: d)); (37, OpenMP, #pragma omp target exit data depend(in : argc, argv[i:argc], x[:]) map(release: c) nowait if(b>g)); (19, OpenMP, #pragma omp target exit data nowait map(always, release: e)); (2, OpenMP, #pragma omp target exit data map(from: a)); (-6, OpenMP,", "label": "(72, OpenMP, #pragma omp target exit data depend(in : argc,argv[i:argc],x[:]) map(release: c) nowait if(b>g)); (17, OpenMP, #pragma omp target exit data map(release: c) if(b>g)); (52, OpenMP, #pragma omp target exit data nowait map(from: c) map(release: d)); (57, OpenMP, #pragma omp target exit data nowait depend(in : argc,argv[i:argc],x[:]) map(from: a)); (1, OpenMP, #pragma omp target exit data map(from: a)); (16, OpenMP, #pragma omp target exit data map(from: x[0:10], c)); (24, OpenMP, #pragma omp target exit data map(from: c) map(release: d)); (41, OpenMP, #pragma omp target exit data map(release: c) nowait if(b>g)); (58, OpenMP, #pragma omp target exit data map(from: a) if (b > g) nowait depend(in : argc,argv[i:argc],x[:])); (72, OpenMP, #pragma omp target exit data nowait depend(in : argc,argv[i:argc],x[:]) map(always,release: e)); (5, OpenMP, #pragma omp target exit data map(from: c)); (58, OpenMP, #pragma omp target exit data map(from: c) depend(in : argc,argv[i:argc],x[:]) nowait); (30, OpenMP, #pragma omp target exit data map(from: a) if (b > g) nowait); (38, OpenMP, #pragma omp target exit data nowait map(from: x[0:10], c)); (19, OpenMP, #pragma omp target exit data map(always,release: e)); (30, OpenMP, #pragma omp target exit data map(from: c) nowait); (62, OpenMP, #pragma omp target exit data nowait map(from: c) depend(in : argc,argv[i:argc],x[:]) map(release: d)); (58, OpenMP, #pragma omp target exit data nowait map(from: x[0:10], c) depend(in : argc,argv[i:argc],x[:])); (-6, OpenMP, #pragma omp target exit data map(from: a) if (b > g)); (5, OpenMP, #pragma omp target exit data map(delete: x[0:10])); (43, OpenMP, #pragma omp target exit data nowait map(from: a) depend(in : argc,argv[i:argc],x[:]) if (target exit data: b)); (36, OpenMP, #pragma omp target exit data nowait map(always,release: e)); (17, OpenMP, #pragma omp target exit data nowait map(from: a) if (target exit data: b)); (-14, OpenMP, #pragma omp target exit data map(from: a) if (target exit data: b)); (12, OpenMP, #pragma omp target exit data nowait map(from: a)); (2, OpenMP, #pragma omp target exit data map(always, delete: x[0:10]))"}
{"predict": "(9, CUDA, extent = make_cudaExtent(4, 5, 6);); (1, CUDA, cudaExtent extent = make_cudaExtent(1, 2, 3);); (10, CUDA, extent = make_cudaExtent(0, 0, 0);); (6, CUDA, extent = make_cudaExtent(2, 3, 4);)", "label": "(10, CUDA, extent = make_cudaExtent(4, 5, 0);); (3, CUDA, cudaExtent extent = make_cudaExtent(4, 5, 6);)"}
{"predict": "(16, CUDA, cudaMalloc((void**)&device_array, num_bytes);); (37, CUDA, cudaDeviceSynchronize();); (43, CUDA, cudaError_t error = cudaGetLastError();); (47, CUDA, printf(\"CUDA error: %s\\n\", cudaGetErrorString(error));); (65, CUDA, cudaFree(device_array);)", "label": "(16, CUDA, cudaMalloc((void**)&device_array, num_bytes);); (49, CUDA, printf(\"CUDA Error: %s\\n\", cudaGetErrorString(error));); (35, CUDA, cudaThreadSynchronize();); (42, CUDA, cudaError_t error = cudaGetLastError();); (66, CUDA, cudaFree(device_array);); (54, CUDA, cudaMemcpy(host_array, device_array, num_bytes, cudaMemcpyDeviceToHost);)"}
{"predict": "(21, OpenMP, int numThreads = omp_get_max_threads();); (27, OpenMP, int threadId = omp_get_thread_num();)", "label": "(19, OpenMP, int numThreads = omp_get_max_threads();); (25, OpenMP, #pragma omp parallel for); (35, OpenMP, int threadId = omp_get_thread_num();)"}
{"predict": "(36, MPI, MPI_Finalize();); (2, MPI, MPI_Init_thread(&argc, &argv, 2, &mpi_threads_provided);); (2, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (0, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(35, MPI, MPI_Finalize();); (2, MPI, MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &mpi_threads_provided);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);)"}
{"predict": "(17, OpenMP, #pragma omp target parallel defaultmap(none:pointer)); (21, OpenMP, #pragma omp target parallel defaultmap(none:pointer) defaultmap(none:scalar) map(argc)); (4, OpenMP, #pragma omp target parallel defaultmap(tofrom); (10, OpenMP, #pragma omp target parallel defaultmap (tofrom:); (2, OpenMP, #pragma omp target parallel defaultmap(tofrom:); (20, OpenMP, #pragma omp target parallel defaultmap(none:pointer) defaultmap(none:scalar) map(g) map(arr[1:2]) map(bar.a)); (45, OpenMP, #pragma omp target defaultmap(none:pointer)); (10, OpenMP, #pragma omp target parallel defaultmap(tofrom pointer)); (24, OpenMP, #pragma omp parallel); (2, OpenMP, #pragma omp target parallel defaultmap (tofrom: scalar)); (38, OpenMP, #pragma omp target parallel defaultmap(none:pointer) map(B::x)); (12, OpenMP, #pragma omp target parallel defaultmap(none:pointer) defaultmap(none:scalar) map(g)); (-1, OpenMP, #pragma omp target parallel defaultmap (scalar:); (4, OpenMP, #pragma omp target parallel defaultmap(tofrom) allocate(omp_thread_mem_alloc: argc)); (23, OpenMP, #pragma omp target defaultmap(none:pointer) map(g)); (12, OpenMP, #pragma omp target parallel defaultmap(none:aggregate) defaultmap(none:scalar) map(argc)); (17, OpenMP, #pragma omp target parallel defaultmap(none:pointer) defaultmap(none:scalar) map(vla[1:2])); (14, OpenMP, #pragma omp target parallel defaultmap(none:aggregate) defaultmap(none:scalar) map(bar.a)); (36, OpenMP, #pragma omp target parallel defaultmap(none:pointer) map(d)); (30, OpenMP, #pragma omp target parallel defaultmap(none:pointer) map(baz.p)); (27, OpenMP, #pragma omp target parallel defaultmap(none:pointer) map(*baz.p)); (3, OpenMP, #pragma omp target parallel defaultmap(tofrom scalar)); (17, OpenMP, #pragma omp target parallel defaultmap(none:aggregate) map", "label": "(12, OpenMP, #pragma omp target parallel defaultmap(tofrom,); (1, OpenMP, #pragma omp target parallel defaultmap(); (14, OpenMP, #pragma omp target parallel defaultmap(tofrom, scalar); (24, OpenMP, #pragma omp target parallel defaultmap(none:pointer) defaultmap(none:scalar) map(g)); (35, OpenMP, #pragma omp target defaultmap(none:aggregate)); (26, OpenMP, #pragma omp target parallel defaultmap(none:aggregate) defaultmap(none:scalar) map(argc)); (8, OpenMP, #pragma omp target parallel defaultmap (scalar:); (47, OpenMP, #pragma omp target defaultmap(none:pointer)); (32, OpenMP, #pragma omp parallel); (14, OpenMP, #pragma omp target parallel defaultmap(none:pointer)); (0, OpenMP, #pragma omp target parallel defaultmap(tofrom)); (-7, OpenMP, #pragma omp target parallel defaultmap()); (7, OpenMP, #pragma omp target parallel defaultmap(tofrom:scalar) defaultmap(to:scalar)); (8, OpenMP, #pragma omp target parallel defaultmap(alloc:pointer) defaultmap(to:scalar) defaultmap(firstprivate:pointer)); (15, OpenMP, #pragma omp target parallel defaultmap(none:pointer) defaultmap(none:scalar) defaultmap(none:aggregate) map(g) firstprivate(argc)); (-7, OpenMP, #pragma omp target parallel defaultmap (tofrom:); (9, OpenMP, #pragma omp target parallel defaultmap(none:pointer) defaultmap(none:scalar) map(argc)); (-11, OpenMP, #pragma omp target parallel defaultmap(tofrom)"}
{"predict": "(57, MPI, MPI_Allreduce(&my_bytes, &max_bytes, 1, MPI_UNSIGNED_LONG, MPI_MAX, comm);)", "label": "(57, MPI, MPI_Allreduce(&my_bytes, &max_bytes, 1, MPI_UNSIGNED_LONG, MPI_MAX, comm);)"}
{"predict": "(11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (16, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (5, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)", "label": "(11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (19, MPI, MPI_Barrier(MPI_COMM_WORLD);); (13, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (4, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(14, CUDA, __shared__ thrust::cuda_cub::core::uninitialized_array<T, 1024> tmp;)", "label": "(15, CUDA, __shared__ thrust::cuda_cub::core::uninitialized_array<T, 1024> tmp;)"}
{"predict": "(11, OpenMP, #pragma omp target enter data map(alloc : a [0:array_size], b [0:array_size])); (18, OpenMP, #pragma omp target teams distribute parallel for); (3, OpenMP, #pragma omp target map(tofrom : warp_size)); (98, OpenMP, #pragma omp target exit data map(release : a [0:array_size], b [0:array_size]))", "label": "(11, OpenMP, #pragma omp target enter data map(alloc : a [0:array_size], b [0:array_size])); (18, OpenMP, #pragma omp target teams distribute parallel for); (3, OpenMP, #pragma omp target map(tofrom : warp_size)); (98, OpenMP, #pragma omp target exit data map(release : a [0:array_size], b [0:array_size]))"}
{"predict": "(21, MPI, MPI_Error_class(err, &errclass);); (37, MPI, MPI_Free_mem(ap);); (16, MPI, err = MPI_Alloc_mem(count, MPI_INFO_NULL, &ap);); (9, MPI, MPI_Info_create(&hints);); (22, MPI, err = MPI_Alloc_mem(count, hints, &ap);); (36, MPI, MPI_Info_free(&hints);); (13, MPI, MPI_Info_set(hints, \"type\", hintstrs[i]);)", "label": "(23, MPI, MPI_Error_class(err, &errclass);); (36, MPI, MPI_Free_mem(ap);); (16, MPI, MPI_Info_set(hints, \"mpich_buf_type\", hintstrs[i]);); (10, MPI, MPI_Info_set(hints, \"mpi_minimum_memory_alignment\", \"4096\");); (8, MPI, MPI_Info_create(&hints);); (20, MPI, err = MPI_Alloc_mem(count, MPI_INFO_NULL, &ap);); (3, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (35, MPI, MPI_Info_free(&hints);); (12, MPI, err = MPI_Alloc_mem(count, hints, &ap);)"}
{"predict": "(2, CUDA, __shared__ float s[Ny][Nx];)", "label": "(2, CUDA, __shared__ float s[Ny][Nx];)"}
{"predict": "(13, OpenCL, clReleaseMemObject(bufParam);); (8, OpenCL, clReleaseMemObject(bufY);); (3, OpenCL, clReleaseMemObject(bufX);)", "label": "(5, OpenCL, clReleaseMemObject(bufX);); (8, OpenCL, clReleaseMemObject(bufY);); (11, OpenCL, clReleaseMemObject(bufParam);)"}
{"predict": "(33, MPI, MPI_Cart_coords(MPI_COMM_SELF, 0, 0, NULL);); (15, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (13, MPI, MPI_Cart_create(MPI_COMM_WORLD, 1, dims, periods, 0, &size);); (27, MPI, MPI_Cart_coords(MPI_COMM_SELF, newrank, 0, NULL);); (45, MPI, MPI_Comm_free(&size);); (30, MPI, MPI_Cart_rank(MPI_COMM_SELF, NULL, &newrank);); (43, MPI, MPI_Barrier(MPI_COMM_WORLD);); (12, MPI, MPI_Cart_rank(MPI_COMM_SELF, NULL, &newrank);)", "label": "(9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (15, MPI, MPI_Cart_map(MPI_COMM_WORLD, 1, dims, periods, &newrank);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (29, MPI, MPI_Cart_map(MPI_COMM_WORLD, 0, dims, periods, &newrank);)"}
{"predict": "(25, MPI, MPI_Wait(&req, &status);); (12, MPI, MPI_Bcast_init(&a, 1, MPI_INT, 0, MPI_COMM_WORLD, MPI_INFO_NULL, &req);); (15, MPI, MPI_Test(&req, &flag, &status);); (25, MPI, MPI_Request_free(&req);)", "label": "(25, MPI, MPI_Wait(&req, &status);); (12, MPI, MPI_Bcast_init(&a, 1, MPI_INT, 0, MPI_COMM_WORLD, MPI_INFO_NULL, &req);); (15, MPI, MPI_Test(&req, &flag, &status);); (25, MPI, MPI_Request_free(&req);)"}
{"predict": "(10, OpenMP, #pragma omp parallel for); (32, OpenMP, #pragma omp atomic)", "label": "(10, OpenMP, #pragma omp parallel for); (32, OpenMP, #pragma omp atomic)"}
{"predict": "(12, OpenMP, #pragma omp parallel for proc_bind(close), proc_bind(spread)); (14, OpenMP, #pragma omp parallel for proc_bind(x)); (6, OpenMP, #pragma omp parallel for proc_bind()); (16, OpenMP, #pragma omp parallel for proc_bind(master)); (21, OpenMP, #pragma omp parallel proc_bind(close)); (0, OpenMP, #pragma omp parallel for proc_bind(); (8, OpenMP, #pragma omp parallel for proc_bind(master); (-5, OpenMP, #pragma omp parallel for proc_bind)", "label": "(14, OpenMP, #pragma omp parallel for proc_bind(close), proc_bind(spread)); (10, OpenMP, #pragma omp parallel for proc_bind(master); (19, OpenMP, #pragma omp parallel for proc_bind(master)); (5, OpenMP, #pragma omp parallel for proc_bind()); (-2, OpenMP, #pragma omp parallel for proc_bind); (12, OpenMP, #pragma omp parallel for proc_bind(x)); (19, OpenMP, #pragma omp parallel proc_bind(close)); (19, OpenMP, #pragma omp parallel for proc_bind(spread)); (-3, OpenMP, #pragma omp parallel for proc_bind()"}
{"predict": "(6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &comm_size);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (7, MPI, vtkh::SetMPICommHandle(MPI_Comm_c2f(MPI_COMM_WORLD));)", "label": "(6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &comm_size);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (7, MPI, vtkh::SetMPICommHandle(MPI_Comm_c2f(MPI_COMM_WORLD));)"}
{"predict": "(16, CUDA, CUDASimulation cudaSimulation(model);)", "label": "(16, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(5, OpenMP, #pragma omp target enter data map(to: data)); (13, OpenMP, #pragma omp target teams distribute parallel for reduction(+:data[:N][:N])); (21, OpenMP, #pragma omp target exit data map(from: data))", "label": "(14, OpenMP, #pragma omp parallel for collapse(2)); (12, OpenMP, #pragma omp target map(tofrom: data))"}
{"predict": "(78, CUDA, CUDA_CHECK(cudaFree(d_C));); (63, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (46, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_B), sizeof(data_type) * B.size()));); (74, CUDA, CUDA_CHECK(cudaFree(d_B));); (45, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_C), sizeof(data_type) * C.size()));); (79, CUDA, CUDA_CHECK(cudaDeviceReset());); (76, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (40, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (68, CUDA, CUDA_CHECK(cudaFree(d_A));); (34, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)", "label": "(78, CUDA, CUDA_CHECK(cudaFree(d_C));); (63, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (46, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_B), sizeof(data_type) * B.size()));); (74, CUDA, CUDA_CHECK(cudaFree(d_B));); (45, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_C), sizeof(data_type) * C.size()));); (79, CUDA, CUDA_CHECK(cudaDeviceReset());); (76, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (40, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (68, CUDA, CUDA_CHECK(cudaFree(d_A));); (34, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(7, OpenCL, err = clGetDeviceInfo(device, CL_DEVICE_IL_VERSION, 0, NULL, &str_size);)", "label": "(7, OpenCL, err = clGetDeviceInfo(device, CL_DEVICE_IL_VERSION, 0, NULL, &str_size);)"}
{"predict": "(81, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (81, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)", "label": "(81, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (81, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)"}
{"predict": "(14, OpenMP, refcnt = --knode->refcnt;)", "label": "(26, OpenCL, clReleaseContext(knode->key.context);); (24, OpenCL, clReleaseProgram(kern->program);)"}
{"predict": "(36, OpenMP, #pragma omp teams distribute parallel for simd safelen (foobool(argc)), safelen (true), safelen (-5)); (1, OpenMP, #pragma omp teams distribute parallel for simd safelen); (26, OpenMP, #pragma omp teams distribute parallel for simd safelen (foobool(1) > 0 ? 1 : 2)); (37, OpenMP, #pragma omp teams distribute parallel for simd safelen (S1)); (8, OpenMP, #pragma omp teams distribute parallel for simd safelen ()); (12, OpenMP, #pragma omp teams distribute parallel for simd safelen (4); (51, OpenMP, #pragma omp teams distribute parallel for simd safelen(safelen(tmain<int, char, -1, -2>(argc, argv)); (15, OpenMP, #pragma omp teams distribute parallel for simd safelen (2+2))); (-7, OpenMP, #pragma omp target); (-2, OpenMP, #pragma omp teams distribute parallel for simd safelen (); (41, OpenMP, #pragma omp teams distribute parallel for simd safelen (argv[1]=2))", "label": "(7, OpenMP, #pragma omp teams distribute parallel for simd safelen (); (21, OpenMP, #pragma omp teams distribute parallel for simd safelen (2+2))); (10, OpenMP, #pragma omp teams distribute parallel for simd safelen ()); (54, OpenMP, #pragma omp teams distribute parallel for simd safelen(safelen(tmain<int, char, -1, -2>(argc, argv)); (37, OpenMP, #pragma omp teams distribute parallel for simd safelen (S1)); (12, OpenMP, #pragma omp teams distribute parallel for simd safelen (4); (45, OpenMP, #pragma omp teams distribute parallel for simd safelen (argv[1]=2)); (21, OpenMP, #pragma omp teams distribute parallel for simd safelen (foobool(1) > 0 ? 1 : 2)); (28, OpenMP, #pragma omp teams distribute parallel for simd safelen (foobool(argc)), safelen (true), safelen (-5)); (-8, OpenMP, #pragma omp target); (-8, OpenMP, #pragma omp teams distribute parallel for simd safelen)"}
{"predict": "(24, MPI, MPI_Finalize();); (12, MPI, MPI_Init(&argc,&argv);)", "label": "(24, MPI, MPI_Finalize();); (12, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(62, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (6, CUDA, CUDA_CHECK(cudaStreamCreate(&stream));); (46, CUDA, CUDA_CHECK(cudaGetLastError());); (53, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));)", "label": "(62, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (6, CUDA, CUDA_CHECK(cudaStreamCreate(&stream));); (46, CUDA, CUDA_CHECK(cudaGetLastError());); (52, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));)"}
{"predict": "(7, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)", "label": "(7, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)"}
{"predict": "(12, CUDA, cudaSetDevice(devs[devi]);)", "label": "(11, OpenMP, #pragma omp parallel for)"}
{"predict": "(5, OpenCL, clReleaseMemObject(objA);); (8, OpenCL, clReleaseMemObject(objB);); (11, OpenCL, clReleaseMemObject(objC);)", "label": "(5, OpenCL, clReleaseMemObject(objA);); (8, OpenCL, clReleaseMemObject(objB);); (11, OpenCL, clReleaseMemObject(objC);)"}
{"predict": "(6, OpenMP, #pragma omp parallel shared(nthreads)); (7, OpenMP, #pragma omp single); (2, OpenMP, my_dev = omp_get_default_device();); (19, OpenMP, #pragma omp parallel shared(nthreads))", "label": "(7, OpenMP, #pragma omp parallel); (7, OpenMP, #pragma omp single); (3, OpenMP, my_dev = omp_get_initial_device();); (6, OpenMP, nthreads = omp_get_num_threads();); (10, OpenMP, #pragma omp parallel shared(nthreads))"}
{"predict": "(6, CUDA, __shared__ float sdata2[THREAD_PER_BLOCK];); (5, CUDA, __shared__ float scale;)", "label": "(7, CUDA, __shared__ float scale;); (5, CUDA, __shared__ float sdata2[THREAD_PER_BLOCK];)"}
{"predict": "(25, OpenMP, #pragma omp teams shared (argc > 0 ? argv[1] : argv[2])); (63, OpenMP, #pragma omp teams shared(i)); (51, OpenMP, #pragma omp teams shared(e, g)); (4, OpenMP, #pragma omp target); (35, OpenMP, #pragma omp teams shared(ba)); (37, OpenMP, #pragma omp teams shared(ca)); (46, OpenMP, #pragma omp teams private(i)); (12, OpenMP, #pragma omp teams shared (argc); (16, OpenMP, #pragma omp teams shared (argc,); (2, OpenMP, #pragma omp teams shared (); (9, OpenMP, #pragma omp teams shared ()); (47, OpenMP, #pragma omp teams firstprivate(i)); (12, OpenMP, #pragma omp teams shared (argc z)); (20, OpenMP, #pragma omp teams shared (a, b, c, d, f)); (26, OpenMP, #pragma omp teams shared(da)); (1, OpenMP, #pragma omp teams shared (argc); (12, OpenMP, #pragma omp teams shared (S1)); (49, OpenMP, #pragma omp teams shared(j)); (19, OpenMP, #pragma omp teams shared(S2::S2s)); (39, OpenMP, #pragma omp teams firstprivate(i)); (19, OpenMP, #pragma omp teams shared(S2::S2sc)); (53, OpenMP, #pragma omp teams reduction(+:i)); (-10, OpenMP, #pragma omp teams shared); (26, OpenMP, #pragma omp teams shared(h, B::x)); (-7, OpenMP, #pragma omp teams shared ())", "label": "(8, OpenMP, #pragma omp teams shared); (61, OpenMP, #pragma omp teams shared(i)); (45, OpenMP, #pragma omp teams shared(e, g, z)); (4, OpenMP, #pragma omp target); (13, OpenMP, #pragma omp teams shared (argc); (51, OpenMP, #pragma omp teams firstprivate(i), shared(i)); (17, OpenMP, #pragma omp teams shared (argc > 0 ? argv[1] : argv[2])); (22, OpenMP, #pragma omp teams shared (S1)); (18, OpenMP, #pragma omp teams shared (argc)); (35, OpenMP, #pragma omp teams shared(da)); (31, OpenMP, #pragma omp teams shared(ca)); (54, OpenMP, #pragma omp teams shared(j)); (23, OpenMP, #pragma omp teams shared (argv[1])); (37, OpenMP, #pragma omp teams shared(h, B::x)); (39, OpenMP, #pragma omp teams private(i)); (-1, OpenMP, #pragma omp teams shared ()); (22, OpenMP, #pragma omp teams shared(ba)); (3, OpenMP, #pragma omp teams shared (argc,); (14, OpenMP, #pragma omp teams shared (a, b, c, d, f)); (-8, OpenMP, #pragma omp teams shared (); (48, OpenMP, #pragma omp teams firstprivate(i))"}
{"predict": "(14, OpenMP, #pragma omp target map(l_val)); (4, OpenMP, #pragma omp parallel); (5, OpenMP, #pragma omp single)", "label": "(15, OpenMP, #pragma omp target map(l_val)); (9, OpenMP, #pragma omp task); (3, OpenMP, #pragma omp parallel); (4, OpenMP, #pragma omp single)"}
{"predict": "(8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (40, MPI, mpi_errno = MPI_Comm_free(&stream_comm);); (13, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (41, MPI, mpi_errno = MPI_Comm_free(&stream_comm);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (13, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(58, MPI, MPI_Type_commit(file_realm_types);); (56, MPI, MPI_Type_contiguous_c((max_end_offset - min_st_offset + 1), MPI_BYTE, file_realm_types);)", "label": "(58, MPI, MPI_Type_commit(file_realm_types);); (56, MPI, MPI_Type_contiguous_c((max_end_offset - min_st_offset + 1), MPI_BYTE, file_realm_types);)"}
{"predict": "(25, OpenCL, status = clBuildProgram(program, 1, &device, NULL, NULL, NULL);); (64, OpenCL, clReleaseProgram(program);)", "label": "(25, OpenCL, OCL_ASSERT(clBuildProgram(program, 1, &device, NULL, NULL, NULL) == CL_SUCCESS);); (26, OpenCL, kernel = clCreateKernel(program, \"compiler_ceil\", &status);)"}
{"predict": "(4, CUDA, CudaCheckError();)", "label": "(4, CUDA, CudaCheckError();); (49, CUDA, dynamics.freeCudaMem();)"}
{"predict": "(2, OpenMP, #pragma omp parallel for); (19, OpenMP, __kmpc_omp_task(nullptr, gtid, task2);); (10, OpenMP, __kmpc_omp_task(nullptr, gtid, task1);)", "label": "(13, OpenMP, __kmpc_omp_task(nullptr, gtid, task1);); (1, OpenMP, #pragma omp parallel for); (18, OpenMP, __kmpc_omp_task(nullptr, gtid, task2);)"}
{"predict": "(27, OpenMP, auto start_clock = omp_get_wtime();); (44, OpenMP, auto stop_clock = omp_get_wtime();)", "label": "(27, OpenMP, auto start_clock = omp_get_wtime();); (44, OpenMP, auto stop_clock = omp_get_wtime();)"}
{"predict": "(19, CUDA, ASSERT_EQ(cudaGetLastError(), cudaSuccess) << \"Kernel launch error.\";)", "label": "(19, CUDA, ASSERT_EQ(cudaGetLastError(), cudaSuccess) << \"Kernel launch error.\";)"}
{"predict": "(22, OpenCL, kernel = clCreateKernel(program, \"cmrt_utest_genx\", &status);); (65, OpenCL, clReleaseProgram(program);)", "label": "(23, OpenCL, kernel = clCreateKernel(program, \"simplemov\", &status);); (20, OpenCL, OCL_ASSERT(clBuildProgram(program, 1, &device, NULL, NULL, NULL) == CL_SUCCESS);)"}
{"predict": "(4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (14, MPI, double end = MPI_Wtime();); (3, MPI, double start = MPI_Wtime();); (8, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(5, MPI, MPI_Barrier(MPI_COMM_WORLD);); (5, MPI, double start = MPI_Wtime();); (12, MPI, double end = MPI_Wtime();); (0, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(9, MPI, MPI_Init(&argc, &argv);); (18, MPI, MPI_Finalize();); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (31, MPI, MPI_Wait(&send_request, &status);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (28, MPI, MPI_Isend(&send_value, 1, MPI_INT, (rank + 1) % size, 4711, MPI_COMM_WORLD, &send_request);)", "label": "(9, MPI, MPI_Init(&argc, &argv);); (18, MPI, MPI_Finalize();); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (31, MPI, MPI_Wait(&recv_request, &status);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (17, MPI, MPI_Isend(&send_value, 1, MPI_INT, (rank + 1) % size, 4711, MPI_COMM_WORLD, &send_request);); (27, MPI, MPI_Wait(&send_request, &status);)"}
{"predict": "(2, MPI, MPI_Comm cComm = MPI_Comm_f2c(*comm);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wSize);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wRank);); (5, MPI, MPI_Comm_rank(cComm, &cRank);); (3, MPI, MPI_Comm_size(cComm, &cSize);)", "label": "(2, MPI, MPI_Comm cComm = MPI_Comm_f2c(*comm);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wSize);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wRank);); (5, MPI, MPI_Comm_rank(cComm, &cRank);); (3, MPI, MPI_Comm_size(cComm, &cSize);)"}
{"predict": "(52, CUDA, CHECK_CUDA(cudaFreeHost(hIndices));); (15, CUDA, CHECK_CUDA(cudaMallocHost(&hIndicesRef, ySize * sizeof(uint32_t)));); (19, CUDA, CHECK_CUDA(cudaMalloc(&dx, xSize * sizeof(float)));); (50, CUDA, CHECK_CUDA(cudaFreeHost(hIndicesRef));); (8, CUDA, CHECK_CUDA(cudaMallocHost(&hx, xSize * sizeof(float)));); (-3, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (44, CUDA, CHECK_CUDA(cudaFreeHost(hx));); (42, CUDA, CHECK_CUDA(cudaFree(dIndices));); (40, CUDA, CHECK_CUDA(cudaFree(dx));); (4, CUDA, CHECK_CUDA(cudaMallocHost(&hyRef, ySize * sizeof(float)));); (5, CUDA, CHECK_CUDA(cudaMallocHost(&hIndices, ySize * sizeof(uint32_t)));); (40, CUDA, CHECK_CUDA(cudaFreeHost(hyRef));); (11, CUDA, CHECK_CUDA(cudaMalloc(&dIndices, ySize * sizeof(uint32_t)));)", "label": "(52, CUDA, CHECK_CUDA(cudaFreeHost(hIndices));); (15, CUDA, CHECK_CUDA(cudaMallocHost(&hIndicesRef, ySize * sizeof(uint32_t)));); (19, CUDA, CHECK_CUDA(cudaMalloc(&dx, xSize * sizeof(float)));); (50, CUDA, CHECK_CUDA(cudaFreeHost(hIndicesRef));); (8, CUDA, CHECK_CUDA(cudaMallocHost(&hx, xSize * sizeof(float)));); (-3, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (44, CUDA, CHECK_CUDA(cudaFreeHost(hx));); (42, CUDA, CHECK_CUDA(cudaFree(dIndices));); (40, CUDA, CHECK_CUDA(cudaFree(dx));); (4, CUDA, CHECK_CUDA(cudaMallocHost(&hyRef, ySize * sizeof(float)));); (5, CUDA, CHECK_CUDA(cudaMallocHost(&hIndices, ySize * sizeof(uint32_t)));); (40, CUDA, CHECK_CUDA(cudaFreeHost(hyRef));); (11, CUDA, CHECK_CUDA(cudaMalloc(&dIndices, ySize * sizeof(uint32_t)));)"}
{"predict": "(3, CUDA, cudaExtent extent = make_cudaExtent(4, 5, 6);)", "label": "(4, CUDA, cudaExtent extent = make_cudaExtent(4, 5, 6);)"}
{"predict": "(9, SYCL, sycl::queue handle = getSYCLDefaultDeviceDefaultQueue();)", "label": "(9, SYCL, sycl::queue handle = getSYCLDefaultDeviceDefaultQueue();)"}
{"predict": "(9, MPI, MPI_Init(&argc, &argv);); (27, MPI, MPI_Finalize();); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (37, MPI, MPI_Wait(&send_request, &status);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (37, MPI, MPI_Wait(&recv_request, &status);); (28, MPI, MPI_Send(&send_value, 9, MPI_INT, (rank + 1) % size, 4711, MPI_COMM_WORLD);); (18, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (28, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(9, MPI, MPI_Init(&argc, &argv);); (25, MPI, MPI_Finalize();); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (30, MPI, MPI_Isend(&send_value, 9, MPI_INT, (rank + 1) % size, 4711, MPI_COMM_WORLD, &send_request);); (34, MPI, MPI_Wait(&recv_request, &status);); (32, MPI, MPI_Wait(&send_request, &status);)"}
{"predict": "(26, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice));); (22, CUDA, CHECK(cudaMalloc((void**)&d_y, M));); (35, CUDA, CHECK(cudaMemcpy(h_z, d_z, M, cudaMemcpyDeviceToHost));); (44, CUDA, CHECK(cudaFree(d_z));); (20, CUDA, CHECK(cudaMalloc((void**)&d_z, M));); (39, CUDA, CHECK(cudaFree(d_y));); (15, CUDA, CHECK(cudaMalloc((void**)&d_x, M));); (39, CUDA, CHECK(cudaFree(d_x));); (26, CUDA, CHECK(cudaGetLastError());); (26, CUDA, CHECK(cudaDeviceSynchronize());)", "label": "(48, CUDA, CHECK(cudaFree(d_y));); (26, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice));); (47, CUDA, CHECK(cudaFree(d_z));); (35, CUDA, CHECK(cudaMemcpy(h_z, d_z, M, cudaMemcpyDeviceToHost));); (18, CUDA, CHECK(cudaMalloc((void**)&d_x, M));); (18, CUDA, CHECK(cudaMalloc((void**)&d_y, M));); (41, CUDA, CHECK(cudaFree(d_x));); (17, CUDA, CHECK(cudaMalloc((void**)&d_z, M));); (26, CUDA, CHECK(cudaGetLastError());); (26, CUDA, CHECK(cudaDeviceSynchronize());); (18, CUDA, CHECK(cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice));)"}
{"predict": "(31, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&device_buffer, size_bytes));); (32, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(device_buffer, input.data(), size_bytes, cudaMemcpyHostToDevice));); (41, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(device_buffer));); (35, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (33, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (37, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(output.data(), device_buffer, size_bytes, cudaMemcpyDeviceToHost));)", "label": "(31, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&device_buffer, size_bytes));); (32, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(device_buffer, input.data(), size_bytes, cudaMemcpyHostToDevice));); (41, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(device_buffer));); (35, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (33, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (37, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(output.data(), device_buffer, size_bytes, cudaMemcpyDeviceToHost));)"}
{"predict": "(13, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(13, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(9, MPI, MPI_Accumulate(one, nlevels, MPI_INT, 0, 0, 1, get_type, MPI_SUM, win);); (10, MPI, MPI_Win_unlock(0, win);); (8, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, 0, 0, win);)", "label": "(12, MPI, MPI_Win_unlock(0, win);); (9, MPI, MPI_Accumulate(one, nlevels, MPI_INT, 0, 0, 1, acc_type, MPI_SUM, win);); (9, MPI, MPI_Get(get_array, nlevels, MPI_INT, 0, 0, 1, get_type, win);); (6, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, 0, 0, win);)"}
{"predict": "(58, OpenMP, #pragma omp target data map(tofrom: A[0:length]) map(to: B[0:length], C[0:length]))", "label": "(31, OpenMP, std::cout << \"OpenMP default device = \" << omp_get_default_device() << std::endl;)"}
{"predict": "(33, MPI, MPI_Finalize();); (7, MPI, rc = MPI_Init(&argc, &argv);); (22, MPI, rc = MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (8, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(23, MPI, rc = MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (32, MPI, MPI_Finalize();); (6, MPI, rc = MPI_Init(NULL, NULL);)"}
{"predict": "(15, OpenMP, #pragma omp target teams distribute parallel for map(a, b))", "label": "(15, OpenMP, #pragma omp target parallel for)"}
{"predict": "(4, CUDA, extern __shared__ float shared[];)", "label": "(4, CUDA, extern __shared__ float shared[];)"}
{"predict": "(31, TBB, generate_test<parallel_test, tbb::flow::tuple<float, double, int, double, double, long, int, float, long> >::do_test();); (27, TBB, generate_test<parallel_test, tbb::flow::tuple<float, int, double, float, long, float, long> >::do_test();); (23, TBB, generate_test<parallel_test, tbb::flow::tuple<double, double, int, int, short> >::do_test();); (17, TBB, generate_test<parallel_test, tbb::flow::tuple<float, double, int, double, double, float, long, int, float, long> >::do_test();); (11, TBB, generate_test<parallel_test, tbb::flow::tuple<double, double, int, long> >::do_test();); (4, TBB, generate_test<parallel_test, tbb::flow::tuple<float, double, int, double> >::do_test();); (5, TBB, generate_test<parallel_test, tbb::flow::tuple<float, double, int, double, double, long, int, float, long> >::do_test();); (7, TBB, generate_test<parallel_test, tbb::flow::tuple<float, int, long> >::do_test();); (2, TBB, generate_test<parallel_test, tbb::flow::tuple<float, double, double, double, float, int, long, float, long> >::do_test();)", "label": "(31, TBB, generate_test<parallel_test, tbb::flow::tuple<float, double, int, double, double, long, int, float, long> >::do_test();); (27, TBB, generate_test<parallel_test, tbb::flow::tuple<float, int, double, float, long, float, long> >::do_test();); (23, TBB, generate_test<parallel_test, tbb::flow::tuple<double, double, int, int, short> >::do_test();); (15, TBB, generate_test<serial_test, tbb::flow::tuple<float, double, int, double, double, float, long, int, float, long> >::do_test();); (3, TBB, generate_test<serial_test, tbb::flow::tuple<float, double> >::do_test();); (4, TBB, generate_test<serial_test, tbb::flow::tuple<float, double, int, long> >::do_test();); (16, TBB, generate_test<parallel_test, tbb::flow::tuple<float, int, long> >::do_test();); (5, TBB, generate_test<serial_test, tbb::flow::tuple<double, double, int, long, int, short> >::do_test();); (7, TBB, generate_test<serial_test, tbb::flow::tuple<float, double, double, double, float, int, float, long> >::do_test();)"}
{"predict": "(25, MPI, MPI_Iscatter(in_buf, elems, MPI_FLOAT, out_buf, elems, MPI_FLOAT, rank, comm, &reqs[nproc + i]);); (12, MPI, MPI_Comm_size(comm, &nproc);); (20, MPI, MPI_Iscatter(NULL, -1, MPI_DATATYPE_NULL, out_buf, elems, MPI_FLOAT, i, comm, &reqs[i]);); (25, MPI, MPI_Waitall(nproc, reqs, MPI_STATUSES_IGNORE);); (8, MPI, MPI_Comm_rank(comm, &rank);)", "label": "(25, MPI, MPI_Isend(&out_buf[elems * i], elems, MPI_FLOAT, i, 0, comm, &reqs[i + nproc]);); (20, MPI, MPI_Irecv(&in_buf[elems * i], elems, MPI_FLOAT, i, 0, comm, &reqs[i]);); (11, MPI, MPI_Comm_size(comm, &nproc);); (9, MPI, MPI_Comm_rank(comm, &rank);); (24, MPI, MPI_Waitall(nproc * 2, reqs, MPI_STATUSES_IGNORE);)"}
{"predict": "(28, CUDA, PyErr_SetString(PyExc_Exception, LOG_PY_UTILS \"cudaOverlay() input and output image formats are different\");); (32, CUDA, PYCUDA_ASSERT_NOGIL(cudaOverlay(input->base.ptr, input->width, input->height, x, y, output->base.ptr, output->width, output->height, output->format, stream));); (20, CUDA, PyErr_SetString(PyExc_Exception, LOG_PY_UTILS \"cudaOverlay() failed to get input/output image pointers (should be cudaImage)\");)", "label": "(22, CUDA, PyErr_SetString(PyExc_Exception, LOG_PY_UTILS \"cudaOverlay() failed to get input/output image pointers (should be cudaImage)\");); (32, CUDA, PYCUDA_ASSERT_NOGIL(cudaOverlay(input->base.ptr, input->width, input->height, output->base.ptr, output->width, output->height, output->format, x, y, stream));); (26, CUDA, PyErr_SetString(PyExc_Exception, LOG_PY_UTILS \"cudaOverlay() input and output image formats are different\");)"}
{"predict": "(16, OpenMP, #pragma omp target teams distribute); (33, OpenMP, #pragma omp atomic); (18, OpenMP, #pragma omp parallel for private(dx, dy, dz, r, ig2))", "label": "(33, OpenMP, #pragma omp atomic); (15, OpenMP, #pragma omp target teams distribute parallel for collapse(2) private(dx, dy, dz, r, ig2))"}
{"predict": "(19, OpenMP, #pragma omp for private(k)); (36, OpenMP, #pragma omp for private(j,k)); (54, OpenMP, #pragma omp for private(j)); (76, OpenMP, #pragma omp for private(k)); (52, OpenMP, #pragma omp for); (88, OpenMP, #pragma omp for private(j,k) schedule(static,1)); (18, OpenMP, #pragma omp for private(j)); (85, OpenMP, #pragma omp for private(j) schedule(static,1)); (15, OpenMP, #pragma omp for private(k))", "label": "(19, OpenMP, #pragma omp for private(k)); (37, OpenMP, #pragma omp for private ( k)); (66, OpenMP, #pragma omp for private (k))"}
{"predict": "(22, CUDA, __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];); (20, CUDA, __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];)", "label": "(22, CUDA, __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];); (20, CUDA, __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];)"}
{"predict": "(6, OpenMP, #pragma omp atomic  hint(AMD_safe_fp_atomics)); (34, OpenMP, #pragma omp atomic  hint(AMD_unsafe_fp_atomics)); (2, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:sum)); (17, OpenMP, #pragma omp atomic  hint(AMD_fast_fp_atomics))", "label": "(6, OpenMP, #pragma omp atomic  hint(AMD_safe_fp_atomics)); (34, OpenMP, #pragma omp atomic  hint(AMD_unsafe_fp_atomics)); (2, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:sum)); (17, OpenMP, #pragma omp atomic  hint(AMD_fast_fp_atomics))"}
{"predict": "(13, CUDA, CHECK(cudaMalloc(&d_y, M));); (37, CUDA, CHECK(cudaFree(d_y));); (12, CUDA, CHECK(cudaMalloc(&d_z, M));); (35, CUDA, CHECK(cudaFree(d_z));); (9, CUDA, CHECK(cudaMalloc(&d_x, M));); (31, CUDA, CHECK(cudaFree(d_x));)", "label": "(37, CUDA, CHECK(cudaFree(d_y));); (14, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice));); (36, CUDA, CHECK(cudaFree(d_z));); (10, CUDA, CHECK(cudaMalloc(&d_y, M));); (8, CUDA, CHECK(cudaMalloc(&d_x, M));); (31, CUDA, CHECK(cudaFree(d_x));); (8, CUDA, CHECK(cudaMalloc(&d_z, M));); (9, CUDA, CHECK(cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice));)"}
{"predict": "(7, CUDA, cudaMalloc((void **)&sm_o, sizeof(float) * m * m);); (10, CUDA, cudaDeviceSynchronize();); (35, CUDA, cudaFree(sm_o);)", "label": "(10, CUDA, cudaDeviceSynchronize();); (5, CUDA, cudaMalloc((void **)&sm_o, sizeof(float) * m * m);); (35, CUDA, cudaFree(sm_o);)"}
{"predict": "(2, CUDA, __shared__ int hist[256];)", "label": "(2, CUDA, __shared__ int hist[256];)"}
{"predict": "(31, MPI, MPI_Comm_create(MPI_COMM_WORLD, comm_group, &comm_hdls[i]);); (37, MPI, MPI_Group_free(&comm_group);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (8, MPI, MPI_Comm_group(MPI_COMM_WORLD, &world_group);); (8, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (26, MPI, mpi_errno = MPI_Comm_create(MPI_COMM_WORLD, comm_group, &comm_hdls[i]);)", "label": "(37, MPI, MPI_Group_free(&comm_group);); (29, MPI, mpi_errno = MPI_Comm_create(MPI_COMM_WORLD, comm_group, &comm_hdls[i]);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (8, MPI, MPI_Comm_group(MPI_COMM_WORLD, &world_group);); (45, MPI, MPI_Group_free(&world_group);); (8, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(6, MPI, MPI_Comm_size(comm, &size);); (4, MPI, MPI_Comm_rank(comm, &rank);)", "label": "(6, MPI, MPI_Comm_size(comm, &size);); (4, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(15, CUDA, __shared__ float rr[TILE_T][N], kk[TILE_T][N];)", "label": "(15, CUDA, __shared__ float rr[TILE_T][N], kk[TILE_T][N];)"}
{"predict": "(4, TBB, tbb::task_scheduler_handle schBlock1;); (12, TBB, bool ok = tbb::finalize(schBlock1, std::nothrow);); (5, TBB, tbb::task_scheduler_handle schBlock = tbb::task_scheduler_handle{tbb::attach{}};); (14, TBB, tbb::finalize(schBlock, std::nothrow);)", "label": "(15, TBB, bool ok = tbb::finalize(schBlock, std::nothrow);); (11, TBB, bool ok = tbb::finalize(schBlock1, std::nothrow);); (4, TBB, tbb::task_scheduler_handle schBlock1;)"}
{"predict": "(18, CUDA, cudaMemcpy(d_in, in.data(), in_bytes, cudaMemcpyHostToDevice);); (32, CUDA, cudaFree(d_in);); (13, CUDA, cudaMalloc((void**)(&d_in), in_bytes);); (13, CUDA, cudaMalloc((void**)(&d_out_min), out_bytes);); (25, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (23, CUDA, assert(cudaMemcpyAsync(out_min.data(), d_out_min, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);); (28, CUDA, cudaFree(d_out_min);)", "label": "(18, CUDA, cudaMemcpy(d_in, in.data(), in_bytes, cudaMemcpyHostToDevice);); (32, CUDA, cudaFree(d_in);); (13, CUDA, cudaMalloc((void**)(&d_in), in_bytes);); (13, CUDA, cudaMalloc((void**)(&d_out_min), out_bytes);); (25, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (23, CUDA, assert(cudaMemcpyAsync(out_min.data(), d_out_min, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);); (28, CUDA, cudaFree(d_out_min);)"}
{"predict": "(3, CUDA, cudaExtent extent = make_cudaExtent(10, 20, 0);)", "label": "(3, CUDA, cudaExtent extent = make_cudaExtent(10, 20, 0);)"}
{"predict": "(4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (9, MPI, MPI_Recv(&t, 1, MPI_DOUBLE, i, 11, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (1, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (11, MPI, MPI_Send(&t, 1, MPI_DOUBLE, 0, 11, MPI_COMM_WORLD);)", "label": "(4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (9, MPI, MPI_Recv(&t, 1, MPI_DOUBLE, i, 11, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (1, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (11, MPI, MPI_Send(&t, 1, MPI_DOUBLE, 0, 11, MPI_COMM_WORLD);)"}
{"predict": "(43, SYCL, sycl::platform Plt{sycl::default_selector_v};)", "label": "(41, SYCL, sycl::platform Plt = Dev.get_platform();)"}
{"predict": "(100, OpenMP, omp_flush();)", "label": "(18, OpenMP, #pragma omp threadprivate (ident))"}
{"predict": "(66, CUDA, CUDA_CHECK(cudaFree(d_C));); (52, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (37, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_C), sizeof(data_type) * C.size()));); (69, CUDA, CUDA_CHECK(cudaDeviceReset());); (66, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (33, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (59, CUDA, CUDA_CHECK(cudaFree(d_A));); (27, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)", "label": "(66, CUDA, CUDA_CHECK(cudaFree(d_C));); (52, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (37, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_C), sizeof(data_type) * C.size()));); (69, CUDA, CUDA_CHECK(cudaDeviceReset());); (66, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (33, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (59, CUDA, CUDA_CHECK(cudaFree(d_A));); (27, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(7, MPI, MPI_Comm_rank(team.comm, &rank);)", "label": "(7, MPI, MPI_Comm_rank(team.comm, &rank);)"}
{"predict": "(5, CUDA, thrust::complex<Q> Bi;)", "label": "(5, CUDA, thrust::complex<Q> Bi;)"}
{"predict": "(51, CUDA, cudaError_t result = cudaGetLastError();)", "label": "(51, CUDA, cudaError_t result = cudaGetLastError();)"}
{"predict": "(55, OpenMP, elapsed_computation_time = omp_get_wtime() - start_computation_time;); (29, CUDA, cudaMalloc(&f_d,   NX*NY*NZ*sizeof(FLOAT));); (29, CUDA, cudaMalloc(&fn_d,  NX*NY*NZ*sizeof(FLOAT));); (53, OpenMP, elapsed_time_total = omp_get_wtime() - start_time_total;); (29, CUDA, cudaMemcpy(f_d,f,  NX*NY*NZ*sizeof(FLOAT), cudaMemcpyHostToDevice);); (19, OpenMP, start_time_total = omp_get_wtime();); (19, OpenMP, start_computation_time = omp_get_wtime();); (45, CUDA, cudaMemcpy(f, f_d,  NX*NY*NZ*sizeof(FLOAT), cudaMemcpyDeviceToHost);); (26, CUDA, cudaMemcpy(fn_d, fn, NX*NY*NZ*sizeof(FLOAT), cudaMemcpyHostToDevice);)", "label": "(33, CUDA, cudaMalloc( (void**) &fn_d,  XYZ_SIZE*sizeof(FLOAT));); (30, OpenMP, start_time_total = omp_get_wtime();); (55, OpenMP, elapsed_time_total = omp_get_wtime() - start_time_total;); (33, OpenMP, start_computation_time = omp_get_wtime();); (50, OpenMP, elapsed_computation_time = omp_get_wtime() - start_computation_time;); (29, CUDA, cudaMemcpy(f_d,f,XYZ_SIZE*sizeof(FLOAT), cudaMemcpyHostToDevice);); (50, CUDA, cudaMemcpy(f, f_d,XYZ_SIZE*sizeof(FLOAT), cudaMemcpyDeviceToHost);); (25, CUDA, cudaMalloc( (void**) &f_d,  XYZ_SIZE*sizeof(FLOAT));); (51, CUDA, cudaFree(fn_d);); (49, CUDA, cudaFree(f_d);)"}
{"predict": "(12, OpenMP, #pragma omp master); (9, OpenMP, #pragma omp parallel default(shared))", "label": "(12, OpenMP, #pragma omp master); (9, OpenMP, #pragma omp parallel default(shared))"}
{"predict": "(25, MPI, MPI_Allreduce(bufin, bufout, count, dtype, MPI_SUM, comm);); (14, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (9, MPI, MPI_Comm_size(comm, &size);)", "label": "(10, MPI, MPI_Comm_size(comm, &size);); (15, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (22, MPI, MPI_Allreduce(bufin, bufout, count, dtype, MPI_SUM, comm);)"}
{"predict": "(17, CUDA, CHECK(cudaMalloc(&(atom->g_NN), MN * sizeof(int)));); (24, CUDA, CHECK(cudaMalloc(&(atom->g_pe), N * sizeof(real)));); (18, CUDA, CHECK(cudaMalloc(&(atom->g_fx), N * sizeof(real)));); (20, CUDA, CHECK(cudaMalloc(&(atom->g_z), N * sizeof(real)));); (16, CUDA, CHECK(cudaMalloc(&(atom->g_y), N * sizeof(real)));); (18, CUDA, CHECK(cudaMalloc(&(atom->g_fy), N * sizeof(real)));); (14, CUDA, CHECK(cudaMalloc(&(atom->g_x), N * sizeof(real)));); (16, CUDA, CHECK(cudaMalloc(&(atom->g_fz), N * sizeof(real)));); (21, CUDA, CHECK(cudaMalloc(&(atom->g_ke), N * sizeof(real)));); (11, CUDA, CHECK(cudaMalloc(&(atom->g_m), N * sizeof(real)));); (13, CUDA, CHECK(cudaMalloc(&(atom->g_vx), N * sizeof(real)));); (14, CUDA, CHECK(cudaMalloc(&(atom->g_vy), N * sizeof(real)));); (15, CUDA, CHECK(cudaMalloc(&(atom->g_vz), N * sizeof(real)));); (18, CUDA, CHECK(cudaMalloc(&(atom->g_box), 6 * sizeof(real)));)", "label": "(23, CUDA, CHECK(cudaMalloc((void**)&atom->g_fz, sizeof(real) * N));); (19, CUDA, CHECK(cudaMalloc((void**)&atom->g_z, sizeof(real) * N));); (17, CUDA, CHECK(cudaMalloc((void**)&atom->g_y, sizeof(real) * N));); (21, CUDA, CHECK(cudaMalloc((void**)&atom->g_pe, sizeof(real) * N));); (14, CUDA, CHECK(cudaMalloc((void**)&atom->g_x, sizeof(real) * N));); (12, CUDA, CHECK(cudaMalloc((void**)&atom->g_NL, sizeof(int) * N * MN));); (16, CUDA, CHECK(cudaMalloc((void**)&atom->g_fy, sizeof(real) * N));); (9, CUDA, CHECK(cudaMalloc((void**)&atom->g_NN, sizeof(int) * N));); (13, CUDA, CHECK(cudaMalloc((void**)&atom->g_fx, sizeof(real) * N));)"}
{"predict": "(25, OpenCL, err = clEnqueueWriteBuffer( queue, buf[1], CL_TRUE, 0, sizeof(int), &dim_arg_global, 0, NULL, NULL);)", "label": "(25, OpenCL, err = clEnqueueWriteBuffer( queue, buf[1], CL_TRUE, 0, sizeof(int), &dim_arg_global, 0, NULL, NULL);)"}
{"predict": "(8, OpenMP, #pragma omp parallel); (14, OpenMP, omp_set_lock(&l);); (21, OpenMP, omp_unset_lock(&l);); (3, OpenMP, int numThreads = omp_get_max_threads();); (24, OpenMP, omp_destroy_lock(&l);); (2, OpenMP, omp_init_lock(&l);)", "label": "(7, OpenMP, omp_init_lock(&l);); (27, OpenMP, omp_destroy_lock(&l);); (7, OpenMP, #pragma omp parallel); (14, OpenMP, omp_set_lock(&l);); (21, OpenMP, omp_unset_lock(&l);); (-1, OpenMP, auto numThreads = omp_get_max_threads();)"}
{"predict": "(35, OpenMP, #pragma omp task shared(j)); (42, OpenMP, #pragma omp task shared(i)); (17, OpenMP, #pragma omp task shared (argc > 0 ? argv[1] : argv[2])); (20, OpenMP, #pragma omp task shared (argv[1])); (7, OpenMP, #pragma omp task shared (); (32, OpenMP, #pragma omp task shared(i)); (17, OpenMP, #pragma omp task shared (a, b, c, d, f)); (22, OpenMP, #pragma omp task shared(ba)); (26, OpenMP, #pragma omp task shared(e, g)); (17, OpenMP, #pragma omp task shared (argv[1], z)); (4, OpenMP, #pragma omp task shared (argc); (26, OpenMP, #pragma omp task shared(h, B::x)); (5, OpenMP, #pragma omp task shared (argc,); (27, OpenMP, #pragma omp task private(i), shared(i)); (24, OpenMP, #pragma omp task firstprivate(i), shared(i)); (14, OpenMP, #pragma omp task shared(ca)); (-5, OpenMP, #pragma omp task shared ()); (14, OpenMP, #pragma omp task shared(da)); (-4, OpenMP, #pragma omp task shared (argc)); (25, OpenMP, #pragma omp task shared(i)); (14, OpenMP, #pragma omp task shared(S2::S2sc)); (16, OpenMP, #pragma omp task shared(h)); (-2, OpenMP, #pragma omp task shared (argc,)", "label": "(45, OpenMP, #pragma omp parallel firstprivate(i)); (40, OpenMP, #pragma omp parallel private(i)); (17, OpenMP, #pragma omp task shared(argc, z)); (39, OpenMP, #pragma omp task shared(i)); (7, OpenMP, #pragma omp task shared()); (20, OpenMP, #pragma omp task shared(argv[1])); (29, OpenMP, #pragma omp task shared(h, B::x)); (26, OpenMP, #pragma omp task shared(e, g)); (31, OpenMP, #pragma omp task firstprivate(i), shared(i)); (34, OpenMP, #pragma omp task shared(j)); (17, OpenMP, #pragma omp task shared(ba)); (6, OpenMP, #pragma omp task shared(argc > 0 ? argv[1] : argv[2])); (-5, OpenMP, #pragma omp task shared); (-4, OpenMP, #pragma omp task shared(); (23, OpenMP, #pragma omp task private(i), shared(i)); (0, OpenMP, #pragma omp task shared(argc,); (7, OpenMP, #pragma omp task shared(a, b, c, d, f)); (4, OpenMP, #pragma omp task shared(S1)); (13, OpenMP, #pragma omp task shared(da)); (-6, OpenMP, #pragma omp task shared(argc); (9, OpenMP, #pragma omp task shared(ca))"}
{"predict": "(45, OpenMP, #pragma omp target is_device_ptr(ps) map(ps->a)); (18, OpenMP, #pragma omp target is_device_ptr(alloc)); (31, OpenMP, #pragma omp target is_device_ptr(p) map(p)); (27, OpenMP, #pragma omp target is_device_ptr(p) map(p+1)); (39, OpenMP, #pragma omp target map(s) is_device_ptr(s)); (7, OpenMP, #pragma omp target is_device_ptr()); (48, OpenMP, #pragma omp target map(ps) is_device_ptr(ps)); (18, OpenMP, #pragma omp target is_device_ptr(p) map(p->a)); (8, OpenMP, #pragma omp target is_device_ptr(alloc); (10, OpenMP, #pragma omp target is_device_ptr(d)); (29, OpenMP, #pragma omp target is_device_ptr(p) map(p[1])); (19, OpenMP, #pragma omp target is_device_ptr(ps) map(ps)); (31, OpenMP, #pragma omp target is_device_ptr(ps) map(ps->a)); (14, OpenMP, #pragma omp target is_device_ptr(ps) map(ps->b)); (30, OpenMP, #pragma omp target map(s) is_device_ptr(s->a)); (32, OpenMP, #pragma omp target map(ps->a) is_device_ptr(ps)); (16, OpenMP, #pragma omp target map(ps->a) is_device_ptr(ps->a)); (3, OpenMP, #pragma omp target is_device_ptr(i)); (0, OpenMP, #pragma omp target is_device_ptr(i[0])); (33, OpenMP, #pragma omp target map(ps->a) is_device_ptr(ps->a)); (8, OpenMP, #pragma omp target is_device_ptr(ps) map(ps->ib)); (33, OpenMP, #pragma omp target map(ps) is_device_ptr(ps->a)); (2, OpenMP, #pragma omp target is_device_ptr(s)); (14, OpenMP, #pragma omp target is_device_ptr(p) map((p+1)->a)); (33, OpenMP, #pragma omp target map(ps->a) map(ps)); (22, OpenMP, #pragma omp target map(s) map(s->a) is_device_ptr(s)); (25,", "label": "(14, OpenMP, #pragma omp target map(p)); (51, OpenMP, #pragma omp target map(from: ps->ps->ps)); (38, OpenMP, #pragma omp target map(ps->i)); (31, OpenMP, #pragma omp target map(s.ps->ps->ps)); (32, OpenMP, #pragma omp target map(to: s.ps->ps->s.f[:22])); (27, OpenMP, #pragma omp target map(to: s.ps->ps)); (20, OpenMP, #pragma omp target map(to: s.p[:22])); (17, OpenMP, #pragma omp target map(s.p)); (2, OpenMP, #pragma omp target map(i)); (37, OpenMP, #pragma omp target map(to: ps->p[:22])); (18, OpenMP, #pragma omp target map(s.ps)); (27, OpenMP, #pragma omp target map(ps)); (30, OpenMP, #pragma omp target map(ps->s.f)); (41, OpenMP, #pragma omp target map(ps->ps->ps->ps)); (8, OpenMP, #pragma omp target map(s.s.f)); (-3, OpenMP, #pragma omp target map(i[1:23])); (-8, OpenMP, #pragma omp target map(d)); (3, OpenMP, #pragma omp target map(s.i)); (26, OpenMP, #pragma omp target map(from: ps->p)); (41, OpenMP, #pragma omp target map(from: s.f[:22]) map(to: ps->p[:33])); (38, OpenMP, #pragma omp target map(to: s.f[:22]) map(from: s.p[:33])); (41, OpenMP, #pragma omp target map(from: s.f[:22], s.s) map(to: ps->p[:33])); (-4, OpenMP, #pragma omp target map(s)); (25, OpenMP, #pragma omp target map(ps->ps)); (6, OpenMP, #pragma omp target map(from: s.ps->s.i)); (31, OpenMP, #pragma omp target map(to: ps->ps->ps->s.f[:22])); (-10, OpenMP, #pragma omp target map(p[1:24])); (23, OpenMP, #pragma omp target map(from: ps->ps->s.i))"}
{"predict": "(13, MPI, MPI_Comm_size(comm, &ranks);); (11, MPI, MPI_Comm_rank(comm, &rank);)", "label": "(13, MPI, MPI_Comm_size(comm, &ranks);); (11, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(26, OpenCL, rc = clSetKernelArg(read_kernel, 4, sizeof(cl_mem), &mc->err_second_read); CLERR;); (18, OpenCL, rc = clSetKernelArg(read_kernel, 0, sizeof(cl_mem), &mc->device_mem); CLERR;); (22, OpenCL, rc = clEnqueueNDRangeKernel(queue, write_kernel, 1, NULL, global_work_size, local_work_size, 0, NULL, NULL); CLERR;); (25, OpenCL, rc = clEnqueueNDRangeKernel(queue, read_kernel, 1, NULL, global_work_size, local_work_size, 0, NULL, NULL);CLERR;); (11, OpenCL, rc = clSetKernelArg(write_kernel, 1, sizeof(TYPE), &p2); CLERR;); (9, OpenCL, rc = clSetKernelArg(write_kernel, 0, sizeof(cl_mem), &mc->device_mem); CLERR;); (13, OpenCL, rc = clSetKernelArg(read_kernel, 2, sizeof(cl_mem), &mc->err_current); CLERR;); (14, OpenCL, rc = clSetKernelArg(read_kernel, 3, sizeof(cl_mem), &mc->err_expect); CLERR;); (10, OpenCL, rc = clSetKernelArg(read_kernel, 1, sizeof(TYPE), &p2); CLERR;); (20, OpenCL, rc = clSetKernelArg(read_kernel, 5, sizeof(cl_mem), &mc->err_count); CLERR;); (18, OpenCL, rc = clSetKernelArg(read_kernel, 6, sizeof(cl_mem), &mc->err_second_read); CLERR;); (8, OpenCL, rc = clSetKernelArg(write_kernel, 2, sizeof(cl_mem), &mc->err_current); CLERR;); (2, OpenCL, rc = clSetKernelArg(write_kernel, 3, sizeof(cl_mem), &mc->err_expect); CLERR;); (17, OpenCL, rc = clSetKernelArg(read_kernel, 7, sizeof(cl_mem), &mc->err_addr); CLERR;); (-1, OpenCL, rc = clSetKernelArg(write_kernel, 4, sizeof(cl_mem), &mc->err_count); CLERR;); (3,", "label": "(18, OpenCL, rc = clEnqueueNDRangeKernel(queue, write_kernel, 1, NULL, global_work_size, local_work_size, 0, NULL, NULL); CLERR;); (23, OpenCL, rc = clSetKernelArg(read_kernel, 4, sizeof(TYPE), &p2); CLERR;); (35, OpenCL, clReleaseKernel(read_kernel);); (28, OpenCL, rc = clEnqueueNDRangeKernel(queue, read_kernel, 1, NULL, global_work_size, local_work_size, 0, NULL, NULL);CLERR;); (5, OpenCL, cl_kernel read_kernel = clCreateKernel(program, \"kernel_modtest_read\", &rc); CLERR;); (20, OpenCL, rc = clSetKernelArg(read_kernel, 5, sizeof(cl_mem), &mc->err_count); CLERR;); (14, OpenCL, rc = clSetKernelArg(read_kernel, 0, sizeof(cl_mem), &mc->device_mem); CLERR;); (22, OpenCL, rc = clSetKernelArg(read_kernel, 9, sizeof(cl_mem), &mc->err_second_read); CLERR;); (0, OpenCL, cl_kernel write_kernel = clCreateKernel(program, \"kernel_modtest_write\", &rc); CLERR;); (8, OpenCL, rc = clSetKernelArg(write_kernel, 4, sizeof(TYPE), &p2); CLERR;); (11, OpenCL, rc = clSetKernelArg(read_kernel, 1, sizeof(cl_ulong), &mc->mem_size); CLERR;); (3, OpenCL, rc = clSetKernelArg(write_kernel, 1, sizeof(cl_ulong), &mc->mem_size); CLERR;); (14, OpenCL, rc = clSetKernelArg(read_kernel, 6, sizeof(cl_mem), &mc->err_addr); CLERR;); (0, OpenCL, rc = clSetKernelArg(write_kernel, 0, sizeof(cl_mem), &mc->device_mem); CLERR;); (22, OpenCL, clReleaseKernel(write_kernel);); (7, OpenCL, rc = clSetKernelArg(read_kernel, 2, sizeof(unsigned int), &offset); CLERR;); (0, OpenCL, rc = clSetKernelArg(write_kernel, 3, sizeof(TYPE), &p1); CLERR;); (6, OpenCL, rc = clSetKernelArg(read_kernel, 3, sizeof(TYPE), &p1); CLERR;); (-3, OpenCL, rc = clSetKernelArg(write_kernel, 2, sizeof(unsigned int), &offset); CLERR;); (9, OpenCL, rc = clSetKernelArg(read_kernel, 8, sizeof(cl_mem), &mc->err_current); CLERR;); (7, OpenCL, rc = clSetKernelArg(read_kernel, 7, sizeof(cl_mem), &mc->err_expect); CLERR;)"}
{"predict": "(9, CUDA, __shared__ float v[_N_], gy[_N_];)", "label": "(10, CUDA, __shared__ float v[_N_], gy[_N_];)"}
{"predict": "(13, CUDA, cudaDeviceSynchronize();); (13, CUDA, cudaMemcpy(&count,listp+fluidnum,sizeof(unsigned),cudaMemcpyDeviceToHost);); (6, CUDA, cudaMemset(listp+fluidnum,0,sizeof(unsigned));)", "label": "(14, CUDA, cudaDeviceSynchronize();); (12, CUDA, cudaMemcpy(&count,listp+fluidnum,sizeof(unsigned),cudaMemcpyDeviceToHost);); (6, CUDA, cudaMemset(listp+fluidnum,0,sizeof(unsigned));)"}
{"predict": "(8, MPI, MPI_Finalize();); (0, MPI, MPI_Init(&argc,&argv);)", "label": "(8, MPI, MPI_Finalize();); (0, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(5, MPI, MPI_Init(&argc, &argv);); (60, MPI, MPI_Finalize();); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (36, MPI, MPI_Recv(&message, 1, MPI_INT, prev, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (18, MPI, MPI_Send(&message, 1, MPI_INT, next, tag, MPI_COMM_WORLD);)", "label": "(6, MPI, MPI_Init(&argc, &argv);); (25, MPI, MPI_Send(&message, 1, MPI_INT, next, tag, MPI_COMM_WORLD);); (60, MPI, MPI_Finalize();); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (34, MPI, MPI_Recv(&message, 1, MPI_INT, prev, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)"}
{"predict": "(182, CUDA, CHECK_CUDA(cudaStreamDestroy(preprocess_stream));); (125, CUDA, CHECK_CUDA(cudaStreamCreate(&preprocess_stream));); (68, CUDA, CHECK_CUDA(cudaStreamSynchronize(mono->get_stream()));)", "label": "(151, CUDA, checkCudaRuntime(cudaMemcpyAsync(image_device, image_host, size_image, cudaMemcpyHostToDevice, preprocess_stream));); (125, CUDA, checkCudaRuntime(cudaStreamCreate(&preprocess_stream));); (67, CUDA, checkCudaRuntime(cudaStreamSynchronize(mono->get_stream()));)"}
{"predict": "(121, CUDA, EXPECT_EQ(cudaDeviceSynchronize(), cudaSuccess);); (119, CUDA, EXPECT_TRUE(blockManager.verifyQueueIntegrity(maxAttentionWindow));)", "label": "(114, CUDA, EXPECT_EQ(cudaMemset(block2Ptr->data(), 0, blockSize * sizeof(T)), cudaSuccess);); (74, CUDA, EXPECT_EQ(cudaDeviceSynchronize(), cudaSuccess);)"}
{"predict": "(25, CUDA, cudaMemcpy(hostFloats2, gpuFloats2, 4 * sizeof(float), cudaMemcpyDeviceToHost);); (37, CUDA, cudaFree(gpuFloats3);); (5, CUDA, cudaMalloc((void**)(&gpuFloats2), N * sizeof(float));); (20, CUDA, cudaMemcpy(hostFloats1, gpuFloats1, 4 * sizeof(float), cudaMemcpyDeviceToHost);); (35, CUDA, cudaFree(gpuFloats1);); (18, CUDA, cudaMemcpy(hostFloats3, gpuFloats3, 4 * sizeof(float), cudaMemcpyDeviceToHost);); (34, CUDA, cudaFree(gpuFloats2);); (-2, CUDA, cudaMalloc((void**)(&gpuFloats1), N * sizeof(float));); (5, CUDA, cudaMalloc((void**)(&gpuFloats3), N * sizeof(float));)", "label": "(25, CUDA, cudaMemcpy(hostFloats2, gpuFloats2, 4 * sizeof(float), cudaMemcpyDeviceToHost);); (37, CUDA, cudaFree(gpuFloats1);); (8, CUDA, cudaMalloc((void**)(&gpuFloats2), N * sizeof(float));); (21, CUDA, cudaMemcpy(hostFloats1, gpuFloats1, 4 * sizeof(float), cudaMemcpyDeviceToHost);); (3, CUDA, cudaMalloc((void**)(&gpuFloats1), N * sizeof(float));); (35, CUDA, cudaFree(gpuFloats3);); (20, CUDA, cudaMemcpy(hostFloats3, gpuFloats3, 4 * sizeof(float), cudaMemcpyDeviceToHost);); (32, CUDA, cudaFree(gpuFloats2);); (5, CUDA, cudaMalloc((void**)(&gpuFloats3), N * sizeof(float));)"}
{"predict": "(6, CUDA, __shared__ cuda::atomic<void *> barrier;)", "label": "(9, CUDA, __shared__ cuda::barrier<cuda::thread_scope_block> barrier;)"}
{"predict": "(6, CUDA, auto points_geometry_it = thrust::make_counting_iterator(0);); (8, CUDA, auto linestring_geometry_it  = thrust::make_counting_iterator(0);)", "label": "(6, CUDA, auto points_geometry_it = thrust::make_counting_iterator(0);); (8, CUDA, auto linestring_geometry_it  = thrust::make_counting_iterator(0);)"}
{"predict": "(11, CUDA, extern __shared__ char shared_mem[];)", "label": "(11, CUDA, extern __shared__ char shared_mem[];)"}
{"predict": "(59, CUDA, CHECK_CUDA(cudaMalloc((void**)&ibuf[i].pixel_data[c], sz));); (56, CUDA, CHECK_CUDA(cudaFree(ibuf[i].pixel_data[c]));)", "label": "(59, CUDA, CHECK_CUDA(cudaMalloc((void**)&ibuf[i].pixel_data[c], sz));); (56, CUDA, CHECK_CUDA(cudaFree(ibuf[i].pixel_data[c]));)"}
{"predict": "(93, MPI, ascent_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (101, MPI, MPI_Barrier(comm);); (16, MPI, MPI_Comm_rank(comm, &par_rank);); (16, MPI, MPI_Comm_size(comm, &par_size);)", "label": "(93, MPI, ascent_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (99, MPI, MPI_Barrier(comm);); (16, MPI, MPI_Comm_rank(comm, &par_rank);); (16, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(181, SYCL, sycl::free(mklC, sycl_context);); (177, SYCL, sycl::free(mklB, sycl_context);); (171, SYCL, sycl::free(mklA, sycl_context);); (163, SYCL, float *mklC = (float *)sycl::malloc_device<float>(M * N, sycl_context);); (167, SYCL, float *mklB = (float *)sycl::malloc_device<float>(N * K, sycl_context);); (161, SYCL, float *mklA = (float *)sycl::malloc_device<float>(M * K, sycl_context);); (216, SYCL, sycl::free(mklC_trans, sycl_context);); (214, SYCL, sycl::free(mklB_trans, sycl_context);); (176, SYCL, sycl::free(mklTemp, sycl_context);); (162, SYCL, float *mklTemp = (float *)sycl::malloc_device<float>(M * N, sycl_context);); (198, SYCL, sycl::free(mklA_trans, sycl_context);); (221, SYCL, sycl_queue = nullptr;); (153, SYCL, mklA = (float *)sycl::malloc_device<float>(M * K, sycl_context);); (158, SYCL, mklB = (float *)sycl::malloc_device<float>(N * K, sycl_context);); (35, SYCL, sycl::queue sycl_queue;); (154, SYCL, mklC = (float *)sycl::malloc_device<float>(M * N, sycl_context);); (207, SYCL, sycl::free(mklA, sycl_context);); (156, SYCL, mklA_trans = (float *)sycl::malloc_device<float>(M * K, sycl_context);); (163, SYCL, mklB_trans = (float *)sycl::malloc_device<float>(N * K, sycl_context);); (213, SYCL, sycl::free(mklB, sycl_context););", "label": "(54, SYCL, std::vector<sycl::device> sycl_devices(1);); (3, SYCL, sycl::queue sycl_queue;)"}
{"predict": "(3, OpenMP, #pragma omp parallel); (3, OpenMP, #pragma omp master taskloop simd)", "label": "(4, OpenMP, #pragma omp master taskloop simd); (2, OpenMP, #pragma omp parallel)"}
{"predict": "(36, CUDA, __shared__ _VOLATILE_ float bSmem[16][128+4];); (34, CUDA, __shared__ _VOLATILE_ float aSmem[16][128+4];)", "label": "(36, CUDA, __shared__ _VOLATILE_ float bSmem[16][128+4];); (34, CUDA, __shared__ _VOLATILE_ float aSmem[16][128+4];)"}
{"predict": "(3, CUDA, __shared__ float homo[TESTHOMO_LOOPS*8];); (3, CUDA, __shared__ int cnts[TESTHOMO_TESTS*TESTHOMO_LOOPS];)", "label": "(3, CUDA, __shared__ float homo[8*TESTHOMO_LOOPS];); (3, CUDA, __shared__ int cnts[TESTHOMO_TESTS*TESTHOMO_LOOPS];)"}
{"predict": "(47, MPI, MPI_Win_free(&win[i]);); (20, MPI, int sam = MPI_Alloc_shared_memory((rank==0?99:0)*sizeof(char), &base_ptr[i]);); (0, MPI, MPI_Init(&argc, &argv);); (49, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(node, &rank);); (43, MPI, MPI_Free_shared_memory(base_ptr[i]);); (-3, MPI, int ss = MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, 0, MPI_INFO_NULL, &node);); (30, MPI, MPI_Barrier(node);); (18, MPI, s[i] = MPI_Win_create(base_ptr[i], (rank==0?99:0)*sizeof(char), 1, MPI_INFO_NULL, node, &win[i]);)", "label": "(25, MPI, s[i] = MPI_Win_allocate_shared((rank==0?99:0)*sizeof(char), 1, MPI_INFO_NULL, node, &base_ptr[i], &win[i]);); (36, MPI, MPI_Barrier(node);); (-1, MPI, MPI_Init(&argc, &argv);); (44, MPI, if(win[i] != MPI_WIN_NULL) MPI_Win_free(&win[i]);); (46, MPI, MPI_Finalize();); (2, MPI, MPI_Comm_rank(node, &rank);); (16, MPI, int sam = MPI_Alloc_mem((rank==0?99:0)*sizeof(char), MPI_INFO_NULL, &base_ptr[i]);); (25, MPI, MPI_Win_shared_query(win[i], 0, &size[i], &a[i], &ptr[i]);); (-5, MPI, int ss = MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, 0, MPI_INFO_NULL, &node);)"}
{"predict": "(47, SYCL, sycl::free(cd, queue);)", "label": "(9, SYCL, const bool doHalfs = dev.has(sycl::aspect::fp16);); (47, SYCL, sycl::free(cd, queue);); (12, SYCL, const bool doDoubles = dev.has(sycl::aspect::fp64);)"}
{"predict": "(42, CUDA, __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];); (37, CUDA, __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];)", "label": "(42, CUDA, __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];); (37, CUDA, __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];)"}
{"predict": "(2, OpenMP, #pragma omp target device); (27, OpenMP, #pragma omp target device (ancestor)); (10, OpenMP, #pragma omp target device (z-argc))); (15, OpenMP, #pragma omp target device (argc), device (argc+1)); (18, OpenMP, #pragma omp target device (-2)); (15, OpenMP, #pragma omp target device (S1)); (8, OpenMP, #pragma omp target device (device_num : argc > 0 ? argv[1] : argv[2])); (9, OpenMP, #pragma omp target device (argc: argc + argc)); (16, OpenMP, #pragma omp target device (-10u)); (17, OpenMP, #pragma omp target device (3.14)); (-2, OpenMP, #pragma omp target device (argc); (-7, OpenMP, #pragma omp target device (); (-2, OpenMP, #pragma omp target device (argc:); (-8, OpenMP, #pragma omp target device ())", "label": "(8, OpenMP, #pragma omp target teams device (argc); (5, OpenMP, #pragma omp target teams device ()); (10, OpenMP, #pragma omp target teams device (argc > 0 ? argv[1] : argv[2])); (15, OpenMP, #pragma omp target teams device (S1)); (12, OpenMP, #pragma omp target teams device (argc), device (argc+1)); (15, OpenMP, #pragma omp target teams device (-2)); (8, OpenMP, #pragma omp target teams device (argc + argc + z)); (15, OpenMP, #pragma omp target teams device (-10u)); (2, OpenMP, #pragma omp target teams device (argc))); (-7, OpenMP, #pragma omp target teams device); (14, OpenMP, #pragma omp target teams device (3.14)); (-7, OpenMP, #pragma omp target teams device ()"}
{"predict": "(37, OpenMP, #pragma omp parallel private(i)); (32, OpenMP, #pragma omp single private(i)); (22, OpenMP, #pragma omp single private(argv[1])); (25, OpenMP, #pragma omp single private(h)); (34, OpenMP, #pragma omp single private(j)); (25, OpenMP, #pragma omp single shared(i)); (12, OpenMP, #pragma omp single private(argc)); (13, OpenMP, #pragma omp single private(argc > 0 ? argv[1] : argv[2])); (16, OpenMP, #pragma omp single private(e, g)); (19, OpenMP, #pragma omp single private(B::x)); (23, OpenMP, #pragma omp parallel); (6, OpenMP, #pragma omp single private(argc); (32, OpenMP, #pragma omp single private(m)); (-5, OpenMP, #pragma omp single private); (6, OpenMP, #pragma omp single private(argc,); (-4, OpenMP, #pragma omp single private(); (-2, OpenMP, #pragma omp single private()); (23, OpenMP, #pragma omp single private(i)); (13, OpenMP, #pragma omp single private(a, b)); (-3, OpenMP, #pragma omp single private(argc)", "label": "(11, OpenMP, #pragma omp single private()); (39, OpenMP, #pragma omp parallel private(i)); (21, OpenMP, #pragma omp single private(a, b)); (16, OpenMP, #pragma omp single private(argc)); (23, OpenMP, #pragma omp single private(e, g)); (2, OpenMP, #pragma omp single private); (19, OpenMP, #pragma omp single private(argv[1])); (26, OpenMP, #pragma omp parallel); (9, OpenMP, #pragma omp single private(argc > 0 ? argv[1] : argv[2])); (20, OpenMP, #pragma omp single private(h, B::x)); (11, OpenMP, #pragma omp single private(S1)); (2, OpenMP, #pragma omp single private(argc); (34, OpenMP, #pragma omp single private(m)); (2, OpenMP, #pragma omp single private(argc,); (27, OpenMP, #pragma omp single private(j)); (16, OpenMP, #pragma omp single shared(i)); (-7, OpenMP, #pragma omp single private(); (22, OpenMP, #pragma omp parallel shared(i)); (18, OpenMP, #pragma omp single private(i))"}
{"predict": "(49, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(rh)); (36, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(raa)); (53, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(da)); (22, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(z)); (15, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(k)); (38, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(h)); (25, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(aa))", "label": "(37, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(raa)); (48, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(rh)); (41, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(h)); (28, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(aa)); (51, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(da)); (20, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(z)); (13, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(k))"}
{"predict": "(34, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(34, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(23, MPI, MPI_Comm_split(m?m->getPCU()->GetMPIComm():MPI_COMM_WORLD, group, groupRank, &groupComm);)", "label": "(23, MPI, MPI_Comm_split(expandedPCU->GetMPIComm(), group, groupRank, &groupComm);); (31, MPI, MPI_Comm_free(&groupComm);)"}
{"predict": "(112, OpenMP, #pragma omp master taskloop simd nogroup grainsize(argc) simdlen(4)); (46, OpenMP, #pragma omp master taskloop simd priority(argc)); (23, OpenMP, #pragma omp master taskloop simd); (122, OpenMP, #pragma omp master taskloop simd if(argc) shared(argc, argv) collapse(2) num_tasks(4) safelen(32)); (84, OpenMP, #pragma omp master taskloop simd priority(argc) nogroup grainsize(argc) simdlen(4))", "label": "(4, OpenMP, #pragma omp task); (25, OpenMP, #pragma omp master taskloop priority(argc)); (46, OpenMP, #pragma omp master taskloop); (71, OpenMP, #pragma omp master taskloop if(argc) shared(argc, argv) collapse(2) num_tasks(4)); (87, OpenMP, #pragma omp cancel taskgroup); (87, OpenMP, #pragma omp cancellation point taskgroup)"}
{"predict": "(67, CUDA, cudaFree(B);); (55, CUDA, cudaEventRecord(stop);); (55, CUDA, cudaEventSynchronize(stop);); (66, CUDA, cudaFree(C);); (43, CUDA, cudaEventRecord(start);); (30, CUDA, cudaGetDeviceCount(&num_of_total_devices);); (19, CUDA, cudaEventCreate(&stop);); (54, CUDA, cudaEventElapsedTime(&elapsedTime, start, stop);); (16, CUDA, cudaEventCreate(&start);); (58, CUDA, cudaFree(A);)", "label": "(67, CUDA, cudaFree(B);); (54, CUDA, cudaEventRecord(stop);); (54, CUDA, cudaEventSynchronize(stop);); (65, CUDA, cudaFree(C);); (44, CUDA, cudaEventRecord(start);); (29, CUDA, cudaGetDeviceCount(&num_of_total_devices);); (18, CUDA, cudaEventCreate(&stop);); (53, CUDA, cudaEventElapsedTime(&elapsedTime, start, stop);); (15, CUDA, cudaEventCreate(&start);); (57, CUDA, cudaFree(A);)"}
{"predict": "(11, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (9, CUDA, cudaMalloc((void**)(&d_in1), in1_bytes);); (10, CUDA, cudaMemcpy(d_in1, in1.data(), in1_bytes, cudaMemcpyHostToDevice);); (31, CUDA, cudaFree(d_out);); (23, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (29, CUDA, cudaFree(d_in1);)", "label": "(11, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (9, CUDA, cudaMalloc((void**)(&d_in1), in1_bytes);); (11, CUDA, cudaMemcpy(d_in1, in1.data(), in1_bytes, cudaMemcpyHostToDevice);); (31, CUDA, cudaFree(d_out);); (23, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (28, CUDA, cudaFree(d_in1);)"}
{"predict": "(8, OpenMP, #pragma omp parallel for schedule (static) if(n>OMP_LIMIT_COMPUTESTEP))", "label": "(8, OpenMP, #pragma omp parallel for schedule (guided))"}
{"predict": "(9, SYCL, sycl::queue q = get_queue();); (36, SYCL, sycl::free(data, q);); (8, SYCL, int* data = sycl::malloc_device<int>(1,q);)", "label": "(9, SYCL, sycl::queue q = get_queue();); (36, SYCL, sycl::free(data, q);); (8, SYCL, int* data = sycl::malloc_device<int>(1,q);)"}
{"predict": "(4, CUDA, cudaGetDeviceProperties( &prop, i );); (3, CUDA, cudaGetDeviceCount( &count );)", "label": "(6, CUDA, HANDLE_ERROR( cudaGetDeviceProperties( &prop, i ) );); (3, CUDA, HANDLE_ERROR( cudaGetDeviceCount( &count ) );)"}
{"predict": "(10, TBB, tbb::flow::make_edge( lim, r );); (22, TBB, tbb::flow::limiter_node< T > lim_copy( lim );); (3, TBB, tbb::flow::graph g;); (3, TBB, tbb::flow::limiter_node< T > lim(g, i);); (4, TBB, tbb::atomic<int> accept_count;)", "label": "(10, TBB, tbb::flow::make_edge( lim, r );); (22, TBB, tbb::flow::limiter_node< T > lim_copy( lim );); (3, TBB, tbb::flow::graph g;); (3, TBB, tbb::flow::limiter_node< T > lim(g, i);); (4, TBB, tbb::atomic<int> accept_count;)"}
{"predict": "(15, OpenMP, #pragma omp loop); (12, OpenMP, #pragma omp parallel); (28, OpenMP, #pragma omp parallel default(none), shared(EMTX)); (19, OpenMP, #pragma omp loop default(none), private(i) shared(MTX))", "label": "(15, OpenMP, #pragma omp loop); (19, OpenMP, #pragma omp target teams distribute parallel for map(MTX) reduction(*:EMTX)); (11, OpenMP, #pragma omp target teams loop map(MTX) reduction(*:MTX))"}
{"predict": "(67, TBB, tbb::task_group tg;)", "label": "(67, TBB, tbb::task_group tg;)"}
{"predict": "(23, OpenMP, #pragma omp for reduction(+:counts_team)); (8, OpenMP, #pragma omp for); (18, OpenMP, #pragma omp target map(tofrom:counts_team)); (4, OpenMP, #pragma omp parallel); (18, OpenMP, #pragma omp teams reduction(+:counts_team)); (0, OpenMP, #pragma omp target map(tofrom:counts1)); (0, OpenMP, #pragma omp teams reduction(+:counts2))", "label": "(26, OpenMP, #pragma omp for reduction(+:counts_team)); (8, OpenMP, #pragma omp for); (15, OpenMP, #pragma omp target teams map(from:counts2)); (4, OpenMP, #pragma omp parallel); (0, OpenMP, #pragma omp target teams map(from:counts1)); (17, OpenMP, #pragma omp allocate(counts_team) allocator(omp_pteam_mem_alloc))"}
{"predict": "(16, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(16, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(48, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (3, MPI, MPI_Init(&argc, &argv);); (40, MPI, MPI_Recv(&localpi, 1, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, &status);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (44, MPI, MPI_Send(&localpi, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);)", "label": "(42, MPI, MPI_Recv(&localpi, 1, MPI_DOUBLE, MPI_ANY_SOURCE, 0, MPI_COMM_WORLD, &status);); (47, MPI, MPI_Send(&localpi, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);); (4, MPI, MPI_Init(&argc, &argv);); (48, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(49, CUDA, cudaFreeHost( foutHost );); (47, CUDA, cudaFreeArray( array );)", "label": "(50, CUDA, cudaFreeHost( foutHost );); (27, CUDA, CUDART_CHECK(surf1Dmemset( array, 3.141592654f, 0, NUM_VALUES ));)"}
{"predict": "(72, OpenMP, #pragma omp target map(h_x [0:nconf * numatm], h_y [0:nconf * numatm], h_z [0:nconf * numatm], h_g2 [0:nbin]))", "label": "(72, OpenMP, #pragma omp target data map(h_x [0:nconf * numatm], h_y [0:nconf * numatm], h_z [0:nconf * numatm], h_g2 [0:nbin]))"}
{"predict": "(6, OpenMP, #pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp parallel for)"}
{"predict": "(3, SYCL, int* data = sycl::malloc_shared<int>(7, q);); (19, SYCL, hipsycl::rt::runtime_keep_alive_token rt;); (0, SYCL, sycl::queue q = get_queue();); (67, SYCL, sycl::free(data, q);)", "label": "(4, SYCL, int* data = sycl::malloc_shared<int>(7, q);); (19, SYCL, hipsycl::rt::runtime_keep_alive_token rt;); (1, SYCL, sycl::queue q = get_queue();); (67, SYCL, sycl::free(data, q);)"}
{"predict": "(13, MPI, MPI_Init(&argc, &argv);); (39, MPI, MPI_Finalize();); (12, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (12, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (12, MPI, MPI_Dims_create(size, NR_DIMS, dims);); (16, MPI, MPI_Cart_coords(cart_comm, rank, NR_DIMS, cart_coords);); (25, MPI, #pragma omp parallel reduction(+:sum)); (14, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (29, MPI, avg = sum/(local_rows*local_cols);)", "label": "(34, OpenMP, #pragma omp for reduction(+:sum)); (32, OpenMP, printf(\"rank %d:%d has %d threads\\n\", rank, thread_num, omp_get_num_threads());); (11, MPI, MPI_Init(&argc, &argv);); (42, MPI, MPI_Finalize();); (26, OpenMP, #pragma omp parallel); (36, MPI, MPI_Reduce(&sum, &avg, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (25, OpenMP, int thread_num = omp_get_thread_num();); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (7, MPI, MPI_Dims_create(size, NR_DIMS, dims);); (13, MPI, MPI_Cart_coords(cart_comm, rank, NR_DIMS, cart_coords);); (10, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(40, CUDA, CUDA_SAFE_CALL(cudaFree(x.data()));); (38, CUDA, CUDA_SAFE_CALL(cudaFree(A.data()));); (36, CUDA, CUDA_SAFE_CALL(cudaDeviceSynchronize());); (38, CUDA, CUDA_SAFE_CALL(cudaFree(y.data()));)", "label": "(40, CUDA, CUDA_SAFE_CALL(cudaFree(x.data()));); (38, CUDA, CUDA_SAFE_CALL(cudaFree(A.data()));); (36, CUDA, CUDA_SAFE_CALL(cudaDeviceSynchronize());); (38, CUDA, CUDA_SAFE_CALL(cudaFree(y.data()));)"}
{"predict": "(7, TBB, tbb::task_scheduler_init init( p );)", "label": "(7, TBB, tbb::task_scheduler_init init( p );)"}
{"predict": "(13, CUDA, CUDASimulation cudaSimulation(model);)", "label": "(13, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(52, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(c, host_c.data(), global_c_size * sizeof(TC), cudaMemcpyDefault));); (70, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(b));); (38, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&b, global_b_size * sizeof(TB)));); (50, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (65, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(a));); (34, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&a, global_a_size * sizeof(TA)));); (56, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(host_output.data(), c, global_c_size * sizeof(TC), cudaMemcpyDefault));); (33, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&c, global_c_size * sizeof(TC)));); (44, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(a, host_a.data(), global_a_size * sizeof(TA), cudaMemcpyDefault));); (63, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(c));); (43, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(b, host_b.data(), global_b_size * sizeof(TB), cudaMemcpyDefault));)", "label": "(41, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&b, global_b_size * sizeof(TB)));); (71, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(c));); (68, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(a));); (37, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&a, global_a_size * sizeof(TA)));); (47, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(a, host_a.data(), global_a_size * sizeof(TA), cudaMemcpyHostToDevice));); (49, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (47, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(c, host_c.data(), global_a_size * sizeof(TC), cudaMemcpyHostToDevice));); (45, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(b, host_b.data(), global_a_size * sizeof(TB), cudaMemcpyHostToDevice));); (58, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(host_output.data(), c, global_c_size * sizeof(TC), cudaMemcpyDeviceToHost));); (33, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&c, global_c_size * sizeof(TC)));); (61, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(b));)"}
{"predict": "(65, CUDA, cudaFree( ddata );)", "label": "(62, CUDA, cudaFree( ddata );)"}
{"predict": "(14, CUDA, cudaMalloc((void**)&dA_buff, sizeof(float*) * batch_num);); (16, CUDA, cudaMalloc((void**)&dC_buff, sizeof(float*) * batch_num);); (28, CUDA, cudaFree(dA_buff);); (13, CUDA, cudaMalloc((void**)&dB_buff, sizeof(float*) * batch_num);); (27, CUDA, cudaFree(dB_buff);); (25, CUDA, cudaFree(dC_buff);); (10, CUDA, cudaMemcpy(dA_buff, A_buff, sizeof(float*) * batch_num, cudaMemcpyHostToDevice);); (11, CUDA, cudaMemcpy(dC_buff, C_buff, sizeof(float*) * batch_num, cudaMemcpyHostToDevice);); (8, CUDA, cudaMemcpy(dB_buff, B_buff, sizeof(float*) * batch_num, cudaMemcpyHostToDevice);)", "label": "(18, CUDA, cudaMemcpy(dB_buff, B_buff, sizeof(float*) * batch_num, cudaMemcpyHostToDevice);); (12, CUDA, cudaMalloc(&dA_buff, sizeof(float*) * batch_num);); (31, CUDA, cudaFree(dC_buff);); (29, CUDA, cudaFree(dB_buff);); (27, CUDA, cudaFree(dA_buff);); (12, CUDA, cudaMemcpy(dA_buff, A_buff, sizeof(float*) * batch_num, cudaMemcpyHostToDevice);); (8, CUDA, cudaMalloc(&dB_buff, sizeof(float*) * batch_num);); (12, CUDA, cudaMemcpy(dC_buff, C_buff, sizeof(float*) * batch_num, cudaMemcpyHostToDevice);); (7, CUDA, cudaMalloc(&dC_buff, sizeof(float*) * batch_num);)"}
{"predict": "(51, CUDA, cudaFreeArray( texArray );); (28, CUDA, CUDART_CHECK(tex2D_time( &ms, texArray, 0, 0, Width, Height, threadWidth, threadHeight ));)", "label": "(29, CUDA, CUDART_CHECK(surf2DmemsetArray_time( &ms, texArray, 3.141592654f, threadWidth, threadHeight ));); (50, CUDA, cudaFreeArray( texArray );)"}
{"predict": "(17, CUDA, double gpu_sum = thrust::reduce(thrust::device,dptr,dptr+steps);); (9, CUDA, float *dptr = thrust::raw_pointer_cast(&dsums[0]);); (9, CUDA, thrust::device_vector<float> dsums(steps);)", "label": "(11, CUDA, float *dptr = thrust::raw_pointer_cast(&dsums[0]);); (14, CUDA, double gpu_sum = thrust::reduce(dsums.begin(),dsums.end());); (8, CUDA, thrust::device_vector<float> dsums(steps);)"}
{"predict": "(27, OpenMP, #pragma omp parallel for)", "label": "(12, OpenMP, #pragma omp parallel for)"}
{"predict": "(40, CUDA, cudaFree(d_inPtr);); (24, CUDA, cudaMalloc((void**)&d_outPtr, size * sizeof(float));); (30, CUDA, cudaMemcpy(&result_gpu, &d_outPtr[0], sizeof(float), cudaMemcpyDeviceToHost);); (24, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);); (35, CUDA, cudaFree(d_outPtr);); (19, CUDA, cudaMalloc((void**)& d_inPtr, size * sizeof(float));)", "label": "(40, CUDA, cudaFree(d_inPtr);); (24, CUDA, cudaMalloc((void**)&d_outPtr, size * sizeof(float));); (30, CUDA, cudaMemcpy(&result_gpu, &d_outPtr[0], sizeof(float), cudaMemcpyDeviceToHost);); (24, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);); (35, CUDA, cudaFree(d_outPtr);); (19, CUDA, cudaMalloc((void**)& d_inPtr, size * sizeof(float));)"}
{"predict": "(29, CUDA, CUDA_CALL(cudaDeviceSynchronize());); (6, CUDA, CUDA_CALL(cudaMalloc((void**)&data, size * sizeof(T)));); (50, CUDA, CUDA_CALL(cudaFree(data));)", "label": "(52, CUDA, CUDA_CALL(cudaFree(data));); (28, CUDA, CUDA_CALL(cudaDeviceSynchronize());); (6, CUDA, CUDA_CALL(cudaMalloc((void**)&data, size * sizeof(T)));)"}
{"predict": "(45, OpenMP, #pragma omp parallel sections shared(j)); (34, OpenMP, #pragma omp parallel sections shared(i)); (12, OpenMP, #pragma omp parallel sections shared (argc,); (21, OpenMP, #pragma omp parallel sections shared (argv[1])); (23, OpenMP, #pragma omp parallel sections shared(ba)); (30, OpenMP, #pragma omp parallel sections firstprivate(i)); (35, OpenMP, #pragma omp parallel sections private(i)); (9, OpenMP, #pragma omp parallel sections shared (argc)); (0, OpenMP, #pragma omp parallel sections shared); (11, OpenMP, #pragma omp parallel sections shared (argc > 0 ? argv[1] : argv[2])); (22, OpenMP, #pragma omp parallel sections shared(da)); (35, OpenMP, #pragma omp parallel sections shared(i)); (18, OpenMP, #pragma omp parallel sections shared(ca)); (13, OpenMP, #pragma omp parallel sections shared (a, b, c, d, f, k)); (-1, OpenMP, #pragma omp parallel sections shared (argc); (17, OpenMP, #pragma omp parallel sections shared(e, g)); (26, OpenMP, #pragma omp parallel sections shared(j)); (3, OpenMP, #pragma omp parallel sections shared (S1)); (27, OpenMP, #pragma omp parallel sections firstprivate(i)); (9, OpenMP, #pragma omp parallel sections shared (argv[1])); (-6, OpenMP, #pragma omp parallel sections shared (); (16, OpenMP, #pragma omp parallel sections shared(h, B::x))", "label": "(25, OpenMP, #pragma omp parallel sections shared(argv[1])); (34, OpenMP, #pragma omp parallel sections shared(h, B::x)); (41, OpenMP, #pragma omp parallel sections shared(i)); (20, OpenMP, #pragma omp parallel sections shared(a, b, c, d, f, k)); (9, OpenMP, #pragma omp parallel sections shared(argc); (10, OpenMP, #pragma omp parallel sections shared(argc,); (33, OpenMP, #pragma omp parallel sections firstprivate(i)); (22, OpenMP, #pragma omp parallel sections shared(ca)); (19, OpenMP, #pragma omp parallel sections shared(ba)); (28, OpenMP, #pragma omp parallel sections private(i)); (-3, OpenMP, #pragma omp parallel sections shared); (20, OpenMP, #pragma omp parallel sections shared(da)); (5, OpenMP, #pragma omp parallel sections shared(argc > 0 ? argv[1] : argv[2])); (8, OpenMP, #pragma omp parallel sections shared(S1)); (5, OpenMP, #pragma omp parallel sections shared(argc)); (30, OpenMP, #pragma omp parallel sections shared(j)); (-5, OpenMP, #pragma omp parallel sections shared()); (-8, OpenMP, #pragma omp parallel sections shared(); (15, OpenMP, #pragma omp parallel sections shared(e, g))"}
{"predict": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(25, OpenMP, #pragma omp parallel for)", "label": "(25, OpenMP, #pragma omp parallel for)"}
{"predict": "(45, CUDA, extern __shared__ __align__(sizeof(float)) unsigned char buf[];)", "label": "(13, CUDA, extern __shared__ char buf[];)"}
{"predict": "(66, MPI, MPI_Comm_size(fd->comm, &nprocs);); (66, MPI, MPI_Comm_rank(fd->comm, &myrank);)", "label": "(66, MPI, MPI_Comm_size(fd->comm, &nprocs);); (66, MPI, MPI_Comm_rank(fd->comm, &myrank);)"}
{"predict": "(18, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(18, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(9, OpenMP, #pragma omp target); (10, OpenMP, #pragma omp distribute firstprivate(g, g1, svar, sfvar)); (46, OpenMP, #pragma omp distribute firstprivate(t_var, vec, s_arr, s_arr, var, var, svar)); (7, OpenMP, #pragma omp teams)", "label": "(9, OpenMP, #pragma omp target); (10, OpenMP, #pragma omp distribute simd firstprivate(g, g1, svar, sfvar)); (46, OpenMP, #pragma omp distribute simd firstprivate(t_var, vec, s_arr, s_arr, var, var, svar)); (7, OpenMP, #pragma omp teams)"}
{"predict": "(12, CUDA, CUDASimulation cudaSimulation(model);)", "label": "(12, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(33, CUDA, cudaFree(d_inPtr);); (21, CUDA, cudaMalloc((void**)&d_outPtr, size * sizeof(float));); (26, CUDA, cudaMemcpy(&result_gpu, &d_outPtr[0], sizeof(float), cudaMemcpyDeviceToHost);); (20, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);); (28, CUDA, cudaFree(d_outPtr);); (15, CUDA, cudaMalloc((void**)& d_inPtr, size * sizeof(float));)", "label": "(33, CUDA, cudaFree(d_inPtr);); (21, CUDA, cudaMalloc((void**)&d_outPtr, size * sizeof(float));); (26, CUDA, cudaMemcpy(&result_gpu, &d_outPtr[0], sizeof(float), cudaMemcpyDeviceToHost);); (21, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);); (28, CUDA, cudaFree(d_outPtr);); (16, CUDA, cudaMalloc((void**)& d_inPtr, size * sizeof(float));)"}
{"predict": "(25, MPI, MPI_Allreduce( &temperr, &errcomp, 1, MPI_FLOAT, MPI_MAX, slucomm);); (22, MPI, MPI_Allreduce( &tempxnorm, &xnorm, 1, MPI_DOUBLE, MPI_MAX, slucomm);); (20, MPI, MPI_Allreduce( &temperr, &err, 1, MPI_DOUBLE, MPI_MAX, slucomm);)", "label": "(26, MPI, MPI_Allreduce( &temperr, &errcomp, 1, MPI_FLOAT, MPI_MAX, slucomm);); (23, MPI, MPI_Allreduce( &tempxnorm, &xnorm, 1, MPI_DOUBLE, MPI_MAX, slucomm);); (20, MPI, MPI_Allreduce( &temperr, &err, 1, MPI_DOUBLE, MPI_MAX, slucomm);)"}
{"predict": "(17, MPI, err = MPI_Intercomm_merge(inter, 0, &abc_intra);); (25, MPI, MPI_Comm_disconnect(&ab_c_inter);); (22, MPI, MPI_Comm_disconnect(&ac_intra);); (14, MPI, MPI_Barrier(inter);); (20, MPI, MPI_Comm_disconnect(&ab_inter);); (10, MPI, err = MPI_Intercomm_create(ab_intra, 0, ac_intra, count, tag, &inter);); (20, MPI, MPI_Comm_disconnect(&abc_intra);); (13, MPI, err = MPI_Barrier(abc_intra);); (17, MPI, MPI_Comm_disconnect(&ac_inter);); (1, MPI, MPI_Comm_rank(abc_intra, &rank);); (12, MPI, err = MPI_Intercomm_merge(inter, 1, &ab_c_inter);); (-3, MPI, MPI_Comm_disconnect(&ab_intra);); (-5, MPI, MPI_Comm_disconnect(&ac_inter);)", "label": "(14, MPI, err = MPI_Barrier(ab_c_inter);); (16, MPI, err = MPI_Intercomm_merge(ab_c_inter, 0, &abc_intra);); (25, MPI, MPI_Comm_disconnect(&ab_inter);); (21, MPI, MPI_Comm_free(&ab_intra);); (18, MPI, MPI_Comm_free(&abc_intra);); (5, MPI, err = MPI_Intercomm_create(ab_intra, 0, ac_intra, count, tag, &ab_c_inter);); (17, MPI, MPI_Comm_free(&ab_c_inter);); (18, MPI, MPI_Comm_free(&ac_intra);); (20, MPI, MPI_Comm_disconnect(&ac_inter);); (10, MPI, err = MPI_Barrier(abc_intra);)"}
{"predict": "(56, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)", "label": "(2, MPI, MPI_Init(&argc, &argv);); (55, MPI, MPI_Finalize();)"}
{"predict": "(23, OpenCL, clReleaseEvent(events[i]);)", "label": "(23, OpenCL, clReleaseEvent(events[i]);)"}
{"predict": "(25, MPI, CHECK(MPI_File_set_view(fileh, 0, MPI_INT, MPI_INT, datarep[i], MPI_INFO_NULL));); (44, MPI, MPI_Barrier(MPI_COMM_WORLD);); (30, MPI, CHECK(MPI_File_close(&fileh));); (12, MPI, MPI_Init(&argc, &argv);); (45, MPI, MPI_Finalize();); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (37, MPI, CHECK(MPI_File_delete(TEST_FILENAME, MPI_INFO_NULL));)", "label": "(14, MPI, MPI_Init(&argc, &argv);); (47, MPI, MPI_Finalize();); (33, MPI, MPI_Barrier(MPI_COMM_WORLD);); (12, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (40, MPI, CHECK(MPI_File_delete(TEST_FILENAME, MPI_INFO_NULL));); (27, MPI, CHECK(MPI_File_close(&fileh));); (19, MPI, CHECK(MPI_File_set_view(fileh, 0, MPI_INT, MPI_INT, datarep[i], MPI_INFO_NULL));)"}
{"predict": "(18, MPI, MPI_Send(NULL, 0, MPI_BYTE, p->nbor, 1, MPI_COMM_WORLD);); (22, MPI, MPI_Recv(NULL, 0, MPI_BYTE, p->nbor, 1, MPI_COMM_WORLD, &status);); (32, MPI, latency = (MPI_Wtime() - t0) / (2 * g_latency012_reps);)", "label": "(28, MPI, MPI_Recv(NULL, 0, MPI_BYTE, p->nbor2, 1, MPI_COMM_WORLD, &status);); (22, MPI, MPI_Recv(NULL, 0, MPI_BYTE, p->nbor, 1, MPI_COMM_WORLD, &status);); (25, MPI, MPI_Send(NULL, 0, MPI_BYTE, p->nbor2, 1, MPI_COMM_WORLD);); (34, MPI, latency = (MPI_Wtime() - t0) / (2 * g_latency012_reps);); (18, MPI, MPI_Send(NULL, 0, MPI_BYTE, p->nbor, 1, MPI_COMM_WORLD);); (10, MPI, t0 = MPI_Wtime();)"}
{"predict": "(24, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (19, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (50, MPI, MPI_Finalize();); (47, MPI, MPI_T_finalize();); (18, MPI, MPI_T_init_thread(required, &provided);); (31, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(24, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (19, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (50, MPI, MPI_Finalize();); (47, MPI, MPI_T_finalize();); (18, MPI, MPI_T_init_thread(required, &provided);); (31, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(22, MPI, MPI_Gatherv_c(sendbuf, count, datatype, recvbuf, counts, displs, datatype, root, comm);); (26, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);)", "label": "(22, MPI, MPI_Gatherv_c(sendbuf, count, datatype, recvbuf, counts, displs, datatype, root, comm);); (26, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);)"}
{"predict": "(12, MPI, MPI_Init(&argc, &argv);); (34, MPI, MPI_Finalize();); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (11, MPI, MPI_Dims_create(size, NR_DIMS, dims);); (17, MPI, MPI_Cart_coords(cart_comm, rank, NR_DIMS, cart_coords);); (13, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(12, MPI, MPI_Init(&argc, &argv);); (34, MPI, MPI_Finalize();); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (11, MPI, MPI_Dims_create(size, NR_DIMS, dims);); (17, MPI, MPI_Cart_coords(cart_comm, rank, NR_DIMS, cart_coords);); (27, MPI, MPI_Barrier(MPI_COMM_WORLD);); (13, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(40, CUDA, cudaMalloc((void **)&df_C, M * N * sizeof(float));); (37, CUDA, cudaMalloc((void **)&df_B, K * N * sizeof(float));); (40, CUDA, cudaStat = cudaStreamCreate(&stream);); (58, CUDA, cudaFree(df_A);); (58, CUDA, cudaFree(df_B);); (34, CUDA, cudaMalloc((void **)&df_A, M * K * sizeof(float));); (57, CUDA, cudaFree(df_C);); (53, CUDA, cudaStreamDestroy(stream);)", "label": "(39, CUDA, cudaMalloc((void **)&df_C, M * N * sizeof(float));); (62, CUDA, cudaFree(df_A);); (36, CUDA, cudaMalloc((void **)&df_B, K * N * sizeof(float));); (38, CUDA, cudaStat = cudaStreamCreate(&stream);); (61, CUDA, cudaFree(df_C);); (32, CUDA, cudaMalloc((void **)&df_A, M * K * sizeof(float));); (55, CUDA, cudaStreamDestroy(stream);); (48, CUDA, cudaStreamSynchronize(stream);); (56, CUDA, cudaFree(df_B);)"}
{"predict": "(28, MPI, MPI_Comm_disconnect(&result.global);); (26, MPI, MPI_Comm_free(&result.local);); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &result.size);); (1, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &result.rank);); (9, MPI, MPI_Comm_split(MPI_COMM_WORLD, result.group, result.rank, &result.local);); (13, MPI, MPI_Comm_rank(result.global, &result.ranks[0]);); (11, MPI, MPI_Comm_split(MPI_COMM_WORLD, 1 - result.group, result.rank, &result.global);); (13, MPI, MPI_Comm_rank(result.local, &result.ranks[1]);)", "label": "(4, MPI, MPI_Comm_size(result.global, &result.size);); (4, MPI, MPI_Comm_rank(result.global, &result.rank);); (1, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &result.global);); (15, MPI, MPI_Comm_rank(result.local, &result.local_rank);); (12, MPI, MPI_Comm_split(result.global, result.group, result.rank, &result.local);); (-3, MPI, MPI_Init(nullptr, nullptr);); (11, MPI, MPI_Comm_size(result.local, &result.local_size);)"}
{"predict": "(54, MPI, MPI_Comm_free(&comm2);); (47, MPI, MPI_Ssend(buffer, 0, MPI_INT, rank, 1, MPI_COMM_WORLD);); (31, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &comm2);); (43, MPI, MPI_Barrier(comm4);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (38, MPI, MPI_Comm_dup(comm1, &comm4);); (44, MPI, MPI_Comm_free(&comm4);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (37, MPI, MPI_Comm_free(&comm1);); (3, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (43, MPI, MPI_Comm_free(&comm2);); (36, MPI, MPI_Recv(buffer, 0, MPI_INT, rank, 2, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (26, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &comm1);)", "label": "(54, MPI, MPI_Comm_free(&comm2);); (46, MPI, MPI_Ssend(buffer, 0, MPI_INT, rank, 1, MPI_COMM_WORLD);); (31, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &comm2);); (43, MPI, MPI_Barrier(comm4);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (38, MPI, MPI_Comm_dup(comm1, &comm4);); (46, MPI, MPI_Comm_free(&comm4);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (45, MPI, MPI_Comm_free(&comm1);); (39, MPI, MPI_Recv(buffer, 0, MPI_INT, rank, 2, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (30, MPI, MPI_Ssend(buffer, 0, MPI_INT, rank, 0, MPI_COMM_WORLD);); (21, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &comm1);)"}
{"predict": "(35, OpenMP, #pragma omp target teams distribute parallel for map(a, b))", "label": "(35, OpenMP, #pragma omp target teams distribute parallel for map(a, b))"}
{"predict": "(20, MPI, MPI_Comm_free(&dup_comm);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (13, MPI, MPI_Comm_group(dup_comm, &group);); (16, MPI, mpi_errno = MPI_Comm_create_group(dup_comm, NULL, 0, &group);); (5, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (10, MPI, MPI_Error_class(mpi_errno, &errclass);); (5, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &dup_comm);); (14, MPI, MPI_Group_free(&group);)", "label": "(16, MPI, mpi_errno = MPI_Comm_create_group(dup_comm, group, 10, NULL);); (20, MPI, MPI_Comm_free(&dup_comm);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (18, MPI, MPI_Group_free(&group);); (12, MPI, MPI_Error_class(mpi_errno, &errclass);); (7, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (5, MPI, MPI_Comm_group(dup_comm, &group);); (3, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &dup_comm);)"}
{"predict": "(55, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(26, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(14, OpenMP, #pragma omp target map(tofrom : fd) map(fd.b.k : N1::vec::len)); (8, OpenMP, #pragma omp target map(tofrom : fd.a) map(fd.b.k : N1::vec::len))", "label": "(20, OpenMP, #pragma omp target update to(mapper(id): fd)); (10, OpenMP, #pragma omp declare mapper(id: N1::vec v) map(v.len)); (14, OpenMP, #pragma omp target map(mapper(id) alloc: fd)); (15, OpenMP, #pragma omp target map(mapper(idd) alloc: fd.b)); (19, OpenMP, #pragma omp target update from(mapper(idd): fd.b)); (4, OpenMP, #pragma omp declare mapper(id: struct foodat v) map(v.a)); (15, OpenMP, #pragma omp target update to(mapper(idd): fd.b)); (3, OpenMP, #pragma omp declare mapper(idd: struct foodatchild v) map(v.k)); (14, OpenMP, #pragma omp target update from(mapper(id): fd))"}
{"predict": "(12, CUDA, __shared__ T tails[SMALL_KERNEL_BLOCK * SMALL_KERNEL_BLOCK];); (19, CUDA, extern __shared__ char _arrays[];)", "label": "(12, CUDA, __shared__ T tails[SMALL_KERNEL_BLOCK * SMALL_KERNEL_BLOCK];); (18, CUDA, extern __shared__ char _arrays[];)"}
{"predict": "(47, MPI, MPI_Finalize();); (15, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myid);)", "label": "(45, MPI, ierr = MPI_Finalize();); (15, MPI, ierr = MPI_Comm_rank(MPI_COMM_WORLD, &myid);)"}
{"predict": "(9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (14, MPI, MPI_Dims_create(tests[k].size, tests[k].dim, dims);)", "label": "(9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (14, MPI, MPI_Dims_create(tests[k].size, tests[k].dim, dims);)"}
{"predict": "(8, MPI, MPI_Type_get_extent(sddt, &lb, &extent);)", "label": "(6, MPI, MPI_Type_get_extent(sddt, &lb, &extent);)"}
{"predict": "(11, CUDA, CUDA_CHECK(cudaGetDeviceProperties(&props, 0));)", "label": "(11, CUDA, CUDA_CHECK(cudaGetDeviceProperties(&props, 0));)"}
{"predict": "(15, OpenCL, result = clGetPlatformInfo(platform, CL_PLATFORM_EXTENSIONS, set_size, &extensions[0], NULL);); (11, OpenCL, result = clGetPlatformInfo(platform, CL_PLATFORM_EXTENSIONS, 0, NULL, &set_size);)", "label": "(12, OpenCL, result = clGetPlatformInfo(platform, CL_PLATFORM_EXTENSIONS, 0, NULL, &set_size);); (14, OpenCL, result = clGetPlatformInfo(platform, CL_PLATFORM_EXTENSIONS, extensions.size(), extensions.data(), NULL);)"}
{"predict": "(32, OpenMP, #pragma omp teams distribute parallel for simd num_threads (argv[1]=2)); (46, OpenMP, #pragma omp teams distribute parallel for simd num_threads (num_threads(tmain<int, -1>(argc)); (48, OpenMP, #pragma omp teams distribute parallel for simd num_threads (z+argc)); (35, OpenMP, #pragma omp teams distribute parallel for simd num_threads (S1)); (20, OpenMP, #pragma omp teams distribute parallel for simd num_threads (argc > 0 ? argv[1] : argv[2])); (16, OpenMP, #pragma omp teams distribute parallel for simd num_threads (argc + z)); (2, OpenMP, #pragma omp teams distribute parallel for simd num_threads (); (-3, OpenMP, #pragma omp target); (9, OpenMP, #pragma omp teams distribute parallel for simd num_threads (argc); (24, OpenMP, #pragma omp teams distribute parallel for simd num_threads (argc), num_threads (argc+1)); (-1, OpenMP, #pragma omp teams distribute parallel for simd num_threads ()); (34, OpenMP, #pragma omp teams distribute parallel for simd num_threads (argv[1]=argc))", "label": "(47, OpenMP, #pragma omp teams num_teams (3.14)); (42, OpenMP, #pragma omp teams num_teams (-10u)); (17, OpenMP, #pragma omp teams num_teams (argc))); (4, OpenMP, #pragma omp teams num_teams (); (7, OpenMP, #pragma omp teams num_teams ()); (10, OpenMP, #pragma omp teams num_teams (argc); (25, OpenMP, #pragma omp teams num_teams (argc), num_teams (argc+1)); (32, OpenMP, #pragma omp teams num_teams (-2)); (15, OpenMP, #pragma omp teams num_teams (argc > 0 ? argv[1] : argv[2])); (-7, OpenMP, #pragma omp target); (25, OpenMP, #pragma omp teams num_teams (S1)); (16, OpenMP, #pragma omp teams num_teams (argc + argc-z)); (-9, OpenMP, #pragma omp teams num_teams)"}
{"predict": "(5, CUDA, __shared__ typename BlockMergeSort::TempStorage temp_storage_shuffle;)", "label": "(5, CUDA, __shared__ typename BlockMergeSort::TempStorage temp_storage_shuffle;)"}
{"predict": "(14, MPI, MPI_Comm_size(comm, &size);); (33, MPI, MPI_Op_free(&op);); (15, MPI, MPI_Reduce(buf, buf, count, MPI_INT, op, 0, comm);); (6, MPI, MPI_Op_create((MPI_User_function *) addem, 1, &op);); (11, MPI, MPI_Comm_rank(comm, &rank);)", "label": "(22, MPI, MPI_Allreduce(MPI_IN_PLACE, buf, count, MPI_INT, op, comm);); (13, MPI, MPI_Comm_size(comm, &size);); (35, MPI, MPI_Op_free(&op);); (6, MPI, MPI_Op_create(mysum, 0, &op);); (11, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(33, MPI, MPI_Comm_size(comm, &ranks);); (31, MPI, MPI_Comm_rank(comm, &rank);); (13, MPI, MPI_Type_get_true_extent(keysat, &true_lb, &true_extent);)", "label": "(34, MPI, MPI_Comm_size(comm, &ranks);); (32, MPI, MPI_Comm_rank(comm, &rank);); (12, MPI, MPI_Type_get_true_extent(keysat, &true_lb, &true_extent);)"}
{"predict": "(11, MPI, MPI_Info_get(info, key, sizeof(buff), buff, &flag);); (7, MPI, MPI_Info_create(&info);); (7, MPI, MPI_Info_set(info, key, val);)", "label": "(11, MPI, MPI_Info_get(info, key, sizeof(buff) - 1, buff, &flag);); (8, MPI, MPI_Info_create(&info);); (19, MPI, MPI_Info_free(&info);); (7, MPI, MPI_Info_set(info, key, val);)"}
{"predict": "(13, CUDA, __shared__ float Bs[maxPipelineStages][BLOCK_SIZE_X][BLOCK_SIZE_X];); (8, CUDA, __shared__ float As[maxPipelineStages][BLOCK_SIZE_X][BLOCK_SIZE_X];)", "label": "(13, CUDA, __shared__ float Bs[maxPipelineStages][BLOCK_SIZE_X][BLOCK_SIZE_X];); (8, CUDA, __shared__ float As[maxPipelineStages][BLOCK_SIZE_X][BLOCK_SIZE_X];)"}
{"predict": "(4, OpenMP, #pragma omp target); (7, OpenMP, #pragma omp parallel)", "label": "(7, OpenMP, #pragma omp target map(tofrom : device)); (9, OpenMP, printf(\"omp_is_initial_device() returned false on host\\n\");); (12, OpenMP, printf(\"omp_is_initial_device() returned true on device\\n\");)"}
{"predict": "(2, TBB, tbb::atomic<int> counter;)", "label": "(32, TBB, tbb::tbb_allocator<t1>().destroy(my_storage);); (35, TBB, tbb::tbb_allocator<t2>().destroy(my_storage);); (0, TBB, tbb::atomic<int> counter;)"}
{"predict": "(46, OpenMP, #pragma omp teams reduction(+:sum)); (43, OpenMP, #pragma omp target map(tofrom: b, sum, t) map(alloc: a)); (46, OpenMP, #pragma omp distribute parallel for reduction(+:sum)); (48, OpenMP, t[i] = omp_get_team_num();)", "label": "(46, OpenMP, #pragma omp teams reduction(+:sum)); (43, OpenMP, #pragma omp target map(tofrom: b, sum, t) map(alloc: a)); (46, OpenMP, #pragma omp distribute parallel for reduction(+:sum)); (48, OpenMP, t[i] = omp_get_team_num();)"}
{"predict": "(8, OpenMP, omp_test_nest_lock(&nest_lock);); (3, OpenMP, omp_init_nest_lock(&nest_lock);); (6, OpenMP, omp_set_nest_lock(&nest_lock);); (10, OpenMP, omp_unset_nest_lock(&nest_lock);); (10, OpenMP, omp_destroy_nest_lock(&nest_lock);)", "label": "(6, OpenMP, omp_unset_nest_lock(&nest_lock);); (2, OpenMP, omp_init_nest_lock(&nest_lock);); (3, OpenMP, omp_test_nest_lock(&nest_lock);); (5, OpenMP, omp_set_nest_lock(&nest_lock);); (9, OpenMP, omp_destroy_nest_lock(&nest_lock);)"}
{"predict": "(24, CUDA, CHECK(cudaMalloc(&d_B, M));); (24, CUDA, CHECK(cudaMemcpy(d_A, h_A, M, cudaMemcpyHostToDevice));); (46, CUDA, CHECK(cudaFree(d_B));); (20, CUDA, CHECK(cudaMalloc(&d_A, M));); (32, CUDA, CHECK(cudaMemcpy(h_B, d_B, M, cudaMemcpyDeviceToHost));); (42, CUDA, CHECK(cudaFree(d_A));)", "label": "(24, CUDA, CHECK(cudaMalloc(&d_B, M));); (24, CUDA, CHECK(cudaMemcpy(d_A, h_A, M, cudaMemcpyHostToDevice));); (46, CUDA, CHECK(cudaFree(d_B));); (20, CUDA, CHECK(cudaMalloc(&d_A, M));); (32, CUDA, CHECK(cudaMemcpy(h_B, d_B, M, cudaMemcpyDeviceToHost));); (42, CUDA, CHECK(cudaFree(d_A));)"}
{"predict": "(5, CUDA, extern __shared__ char shared_memory[];)", "label": "(5, CUDA, extern __shared__ char shared_memory[];)"}
{"predict": "(52, CUDA, CubDebugExit(cudaMemcpy(d_flags, h_flags, sizeof(unsigned char) * num_items, cudaMemcpyHostToDevice));); (50, CUDA, CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));)", "label": "(52, CUDA, CubDebugExit(cudaMemcpy(d_flags, h_flags, sizeof(unsigned char) * num_items, cudaMemcpyHostToDevice));); (50, CUDA, CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));)"}
{"predict": "(37, MPI, ascent::mpi_comm(MPI_Comm_c2f(comm));); (16, MPI, MPI_Comm_rank(comm, &par_rank);); (16, MPI, MPI_Comm_size(comm, &par_size);)", "label": "(33, MPI, flow::Workspace::set_default_mpi_comm(MPI_Comm_c2f(comm));); (16, MPI, MPI_Comm_rank(comm, &par_rank);); (16, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(8, OpenMP, int tid = omp_get_thread_num();); (4, OpenMP, #pragma omp parallel num_threads(8)); (2, OpenMP, omp_set_affinity_format(formats[i]);)", "label": "(8, OpenMP, int tid = omp_get_thread_num();); (4, OpenMP, #pragma omp parallel num_threads(8)); (2, OpenMP, omp_set_affinity_format(formats[i]);)"}
{"predict": "(52, CUDA, HANDLE_ERROR( cudaFree( dev_a ) );); (52, CUDA, HANDLE_ERROR( cudaFree( dev_b ) );); (49, CUDA, HANDLE_ERROR( cudaFree( dev_partial_c ) );)", "label": "(48, CUDA, HANDLE_ERROR( cudaFree( dev_a ) );); (49, CUDA, HANDLE_ERROR( cudaFree( dev_partial_c ) );); (47, CUDA, HANDLE_ERROR( cudaFree( dev_b ) );)"}
{"predict": "(22, OpenMP, omp_destroy_nest_lock(&lck);); (9, OpenMP, #pragma omp for); (10, OpenMP, omp_set_nest_lock(&lck);); (10, OpenMP, #pragma omp flush); (15, OpenMP, omp_unset_nest_lock(&lck);); (3, OpenMP, #pragma omp parallel shared(lck)); (1, OpenMP, omp_init_nest_lock(&lck);)", "label": "(12, OpenMP, omp_set_nest_lock(&lck);); (18, OpenMP, omp_unset_nest_lock(&lck);); (8, OpenMP, #pragma omp for); (10, OpenMP, #pragma omp flush); (4, OpenMP, #pragma omp parallel shared(lck)); (2, OpenMP, omp_init_nest_lock(&lck);); (16, OpenMP, omp_destroy_nest_lock(&lck);)"}
{"predict": "(5, OpenMP, #pragma omp parallel for)", "label": "(5, OpenMP, #pragma omp parallel for)"}
{"predict": "(58, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (58, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)", "label": "(58, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (58, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)"}
{"predict": "(32, OpenMP, #pragma omp target); (34, OpenMP, int device_id = omp_get_default_device();); (54, OpenMP, #pragma omp target data use_device_ptr(a))", "label": "(32, OpenMP, #pragma omp target); (35, OpenMP, int device_id = omp_get_default_device();); (54, OpenMP, #pragma omp target data use_device_ptr(a))"}
{"predict": "(14, CUDA, cudaMemcpy(d_A, h_A, DSIZE*sizeof(float), cudaMemcpyHostToDevice);); (22, CUDA, cudaMemcpy(h_C, d_C, DSIZE*sizeof(float), cudaMemcpyDeviceToHost);); (11, CUDA, cudaMemcpy(d_B, h_B, DSIZE*sizeof(float), cudaMemcpyHostToDevice);); (7, CUDA, cudaMalloc(&d_A, DSIZE*sizeof(float));); (20, CUDA, cudaDeviceSynchronize();); (8, CUDA, cudaMalloc(&d_C, DSIZE*sizeof(float));); (11, CUDA, cudaMalloc(&d_B, DSIZE*sizeof(float));)", "label": "(28, CUDA, cudaCheckErrors(\"kernel execution failure or cudaMemcpy H2D failure\");); (14, CUDA, cudaMemcpy(d_A, h_A, DSIZE*sizeof(float), cudaMemcpyHostToDevice);); (16, CUDA, cudaCheckErrors(\"cudaMemcpy H2D failure\");); (23, CUDA, cudaMemcpy(h_C, d_C, DSIZE*sizeof(float), cudaMemcpyDeviceToHost);); (13, CUDA, cudaMemcpy(d_B, h_B, DSIZE*sizeof(float), cudaMemcpyHostToDevice);); (7, CUDA, cudaMalloc(&d_C, DSIZE*sizeof(float));); (5, CUDA, cudaMalloc(&d_B, DSIZE*sizeof(float));); (6, CUDA, cudaCheckErrors(\"cudaMalloc failure\");); (15, CUDA, cudaCheckErrors(\"kernel launch failure\");); (1, CUDA, cudaMalloc(&d_A, DSIZE*sizeof(float));)"}
{"predict": "(16, OpenMP, #pragma omp target teams distribute parallel for collapse(2) map(to: A[0:N1], B[0:N1]) map(from: C[0:N2]))", "label": "(17, OpenMP, #pragma omp target teams distribute parallel for collapse(2)); (23, OpenMP, #pragma omp target exit data map(delete:A[0:N1],B[0:N1],C[0:N2])); (14, OpenMP, #pragma omp target enter data map(to: A[0:N1],B[0:N1],C[0:N2])); (20, OpenMP, #pragma omp target update from(C[0:N2]))"}
{"predict": "(57, OpenCL, clReleaseEvent(event);)", "label": "(54, OpenCL, clReleaseEvent(event);)"}
{"predict": "(50, CUDA, CHECK_CUDA(cudaMalloc(&dy, dataSize * sizeof(int32_t)));); (75, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (32, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (76, CUDA, CHECK_CUDA(cudaFree(dy));); (45, CUDA, CHECK_CUDA(cudaMalloc(&dx, dataSize * sizeof(int32_t)));); (32, CUDA, CHECK_HIEDNN(hiednnSetCudaStream(handle, 0));); (72, CUDA, CHECK_CUDA(cudaFree(dx));)", "label": "(50, CUDA, CHECK_CUDA(cudaMalloc(&dy, dataSize * sizeof(int32_t)));); (75, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (32, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (76, CUDA, CHECK_CUDA(cudaFree(dy));); (45, CUDA, CHECK_CUDA(cudaMalloc(&dx, dataSize * sizeof(int32_t)));); (32, CUDA, CHECK_HIEDNN(hiednnSetCudaStream(handle, 0));); (72, CUDA, CHECK_CUDA(cudaFree(dx));)"}
{"predict": "(28, OpenMP, #pragma omp parallel for reduction(&&: Red)); (8, OpenMP, #pragma omp parallel for reduction(-: Red)); (11, OpenMP, #pragma omp parallel for reduction(&: Red)); (14, OpenMP, #pragma omp parallel for reduction(|: Red)); (16, OpenMP, #pragma omp parallel for reduction(^: Red)); (19, OpenMP, #pragma omp parallel for reduction(&&: Red)); (2, OpenMP, #pragma omp parallel for reduction(+: Red)); (24, OpenMP, #pragma omp parallel for reduction(||: Red))", "label": "(28, OpenMP, #pragma omp parallel for reduction(&&: Red)); (7, OpenMP, #pragma omp parallel for reduction(-: Red)); (14, OpenMP, #pragma omp parallel for reduction(&: Red)); (17, OpenMP, #pragma omp parallel for reduction(|: Red)); (0, OpenMP, #pragma omp parallel for reduction(+: Red)); (19, OpenMP, #pragma omp parallel for reduction(^: Red)); (6, OpenMP, #pragma omp parallel for reduction(*: Red)); (25, OpenMP, #pragma omp parallel for reduction(||: Red))"}
{"predict": "(13, MPI, MPI_Type_size(type, &size);); (19, MPI, MPI_Type_free(&type);); (13, MPI, mpi_errno = MPI_Type_ub(type, NULL);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (11, MPI, MPI_Error_class(mpi_errno, &errclass);); (4, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(13, MPI, MPI_Type_size(type, &size);); (14, MPI, mpi_errno = MPI_Type_ub(type, NULL);); (18, MPI, MPI_Type_free(&type);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (11, MPI, MPI_Error_class(mpi_errno, &errclass);); (4, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(25, MPI, MPI_Start(&req);); (11, MPI, MPI_Info_create(&info);); (33, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (9, MPI, MPI_Comm_size(comm, &size);); (23, MPI, MPI_Reduce_scatter_block_init(sendbuf, &recvbuf, 1, MPI_INT, MPI_SUM, comm, info, &req);); (34, MPI, MPI_Info_free(&info);); (6, MPI, MPI_Comm_rank(comm, &rank);)", "label": "(26, MPI, MPI_Start(&req);); (11, MPI, MPI_Comm_size(comm, &size);); (13, MPI, MPI_Info_create(&info);); (24, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (36, MPI, MPI_Info_free(&info);); (34, MPI, MPI_Request_free(&req);); (18, MPI, MPI_Reduce_scatter_init(sendbuf, &recvbuf, recvcounts, MPI_INT, MPI_SUM, comm, info, &req);); (6, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(4, MPI, MPI_Init(&argc, &argv);); (30, MPI, MPI_Finalize();); (3, MPI, int ierr = MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(4, MPI, MPI_Init(&argc, &argv);); (30, MPI, MPI_Finalize();); (3, MPI, int ierr = MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(29, MPI, MPI_Isend(&in[window_size * stride], 1, type, left, tag, comm, &requests[window_size]);); (22, MPI, MPI_Isend(&in[w * stride], 1, type, right, tag, comm, &requests[w]);); (24, MPI, MPI_Irecv(&out[(left + 1) * window_size], 1, type, left, tag, comm, &requests[window_size + w]);); (15, MPI, MPI_Waitall(2 * window_size, requests, MPI_STATUSES_IGNORE);); (26, MPI, MPI_Irecv(&out[(right + 1) * window_size], 1, type, right, tag, comm, &requests[2 * window_size - 1]);); (15, MPI, MPI_Isend(&in[(rank + 1) * stride], 1, type, rank, tag, comm, &requests[0]);); (27, MPI, MPI_Isend(&ack, 1, MPI_CHAR, group % 2 == 0 ? left : right, tag, comm, &requests[2 * window_size]);); (11, MPI, MPI_Irecv(&out[window_size * stride], 1, type, rank, tag, comm, &requests[window_size - 1]);)", "label": "(24, MPI, MPI_Recv(&ack, 1, MPI_CHAR, right, tag, comm, MPI_STATUS_IGNORE);); (26, MPI, MPI_Irecv(out, count, type, set_src ? left : MPI_ANY_SOURCE, tag, comm, &requests[w]);); (28, MPI, MPI_Isend(in, count, type, left, (tag == MPI_ANY_TAG ? 0 : tag), comm, &requests[w + window_size]);); (20, MPI, MPI_Waitall(2 * window_size, requests, MPI_STATUSES_IGNORE);); (17, MPI, MPI_Isend(in, count, type, right, (tag == MPI_ANY_TAG ? 0 : tag), comm, &requests[w + window_size]);); (13, MPI, MPI_Irecv(out, count, type, set_src ? right : MPI_ANY_SOURCE, tag, comm, &requests[w]);); (27, MPI, MPI_Send(&ack, 1, MPI_CHAR, left, (tag == MPI_ANY_TAG ? 0 : tag), comm);)"}
{"predict": "(4, TBB, tbb::task_group_context tgc;)", "label": "(5, TBB, tbb::task_group_context tgc;); (15, TBB, tgc.set_priority(tbb::priority_high);); (4, TBB, tgc.set_priority(tbb::priority_normal);)"}
{"predict": "(24, OpenMP, #pragma omp distribute parallel for dist_schedule(static, ch)); (33, OpenMP, #pragma omp distribute parallel for schedule(static)); (3, OpenMP, #pragma omp target); (46, OpenMP, #pragma omp distribute parallel for schedule(dynamic)); (56, OpenMP, #pragma omp distribute parallel for schedule(dynamic, ch)); (10, OpenMP, #pragma omp distribute parallel for dist_schedule(static)); (2, OpenMP, #pragma omp distribute parallel for dist_schedule(); (14, OpenMP, #pragma omp distribute parallel for dist_schedule(static,)); (47, OpenMP, #pragma omp distribute parallel for schedule(static, ch)); (23, OpenMP, #pragma omp distribute parallel for schedule)); (33, OpenMP, #pragma omp distribute parallel for dynamic); (49, OpenMP, #pragma omp distribute parallel for schedule(static, ch)); (5, OpenMP, #pragma omp distribute parallel for dist_schedule()); (35, OpenMP, #pragma omp distribute parallel for schedule(dynamic, ch))", "label": "(42, OpenMP, #pragma omp distribute parallel for schedule(static, ch)); (49, OpenMP, #pragma omp distribute parallel for schedule(dynamic)); (7, OpenMP, #pragma omp distribute parallel for); (15, OpenMP, #pragma omp distribute parallel for dist_schedule(static)); (30, OpenMP, #pragma omp distribute parallel for schedule(static)); (53, OpenMP, #pragma omp distribute parallel for schedule(dynamic, ch)); (5, OpenMP, #pragma omp cancel for); (0, OpenMP, #pragma omp target); (18, OpenMP, #pragma omp distribute parallel for dist_schedule(static, ch)); (-1, OpenMP, #pragma omp teams)"}
{"predict": "(22, CUDA, __shared__  volatile ll_t smem[_TPB_];)", "label": "(22, CUDA, __shared__ volatile ll_t smem[_TPB_];)"}
{"predict": "(28, CUDA, cudaMemcpy(dst, src, bytes, kind);); (30, CUDA, cudaDeviceSynchronize();); (21, CUDA, cudaProfilerStart();); (29, CUDA, cudaProfilerStop();)", "label": "(27, CUDA, cudaMemcpy(dst, src, bytes, kind);)"}
{"predict": "(8, TBB, tbb::atomic<int> b;); (7, TBB, std::list< std::pair< const tbb::atomic<int>, int > > arrIntTbb;)", "label": "(9, TBB, tbb::atomic<int> b;); (6, TBB, std::list< std::pair< const int, tbb::atomic<int> > > arrIntTbb;)"}
{"predict": "(44, MPI, MPI_Pack(a, 1, xpose, buffer, bufsize, &position, MPI_COMM_WORLD);); (59, MPI, MPI_Type_free(&row);); (31, MPI, MPI_Type_commit(&xpose);); (56, MPI, MPI_Type_free(&xpose);); (28, MPI, MPI_Type_hvector(100, 1, sizeofint, row, &xpose);); (23, MPI, MPI_Type_extent(MPI_INT, &sizeofint);); (31, MPI, MPI_Pack_size(1, xpose, MPI_COMM_WORLD, &bufsize);); (34, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (39, MPI, MPI_Unpack(buffer, bufsize, &position, b, 100 * 100, MPI_INT, MPI_COMM_WORLD);); (22, MPI, MPI_Type_vector(100, 1, 100, MPI_INT, &row);)", "label": "(43, MPI, MPI_Pack(a, 1, xpose, buffer, bufsize, &position, MPI_COMM_WORLD);); (59, MPI, MPI_Type_free(&row);); (31, MPI, MPI_Type_commit(&xpose);); (56, MPI, MPI_Type_free(&xpose);); (28, MPI, MPI_Type_hvector(100, 1, sizeofint, row, &xpose);); (23, MPI, MPI_Type_extent(MPI_INT, &sizeofint);); (30, MPI, MPI_Pack_size(1, xpose, MPI_COMM_WORLD, &bufsize);); (34, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (39, MPI, MPI_Unpack(buffer, bufsize, &position, b, 100 * 100, MPI_INT, MPI_COMM_WORLD);); (22, MPI, MPI_Type_vector(100, 1, 100, MPI_INT, &row);)"}
{"predict": "(6, CUDA, __shared__ typename cub::block_reduce<T, BlockSize>::TempStorage scratch;)", "label": "(4, CUDA, __shared__ cg::block_tile_memory<BlockSize> scratch;)"}
{"predict": "(61, CUDA, cudaFree( deviceInt );); (61, CUDA, cudaFreeHost( hostInt );)", "label": "(59, CUDA, cudaFree( deviceInt );); (59, CUDA, cudaFreeHost( hostInt );)"}
{"predict": "(9, OpenCL, clReleaseMemObject(objiAmax);); (4, OpenCL, clReleaseMemObject(objX);); (11, OpenCL, clReleaseMemObject(objScratch);)", "label": "(5, OpenCL, clReleaseMemObject(objX);); (12, OpenCL, clReleaseMemObject(objScratch);); (7, OpenCL, clReleaseMemObject(objiAmax);)"}
{"predict": "(19, TBB, tbb::concurrent_bounded_queue<T> q;); (3, TBB, tbb::concurrent_queue<T> q;)", "label": "(19, TBB, REMARK(\"Wraparound on tbb::concurrent_bounded_queue...\");); (2, TBB, tbb::concurrent_queue<T>* q = new tbb::concurrent_queue<T>;); (16, TBB, tbb::concurrent_bounded_queue<T>* q = new tbb::concurrent_bounded_queue<T>;)"}
{"predict": "(17, OpenMP, #pragma omp parallel); (27, OpenMP, #pragma omp for); (8, OpenMP, #pragma omp master)", "label": "(25, OpenMP, printf(\"Thread %d sleeps now for %d seconds\\n\", omp_get_thread_num(), SLEEPTIME);); (8, OpenMP, #pragma omp parallel); (31, OpenMP, printf(\"Thread %d wakes up again\\n\", omp_get_thread_num());)"}
{"predict": "(45, SYCL, GEP->setMetadata(hipsycl::compiler::MDKind::Arrayified, MDAlloca);); (36, SYCL, Alloca->setMetadata(hipsycl::compiler::MDKind::Arrayified, MDAlloca);)", "label": "(45, SYCL, GEP->setMetadata(hipsycl::compiler::MDKind::Arrayified, MDAlloca);); (36, SYCL, Alloca->setMetadata(hipsycl::compiler::MDKind::Arrayified, MDAlloca);)"}
{"predict": "(22, MPI, MPI_Isend(in + (left + 1) * i, 1, type, left, tag, comm, &requests[w]);); (20, MPI, MPI_Waitall(window_size, requests, MPI_STATUSES_IGNORE);); (16, MPI, MPI_Irecv(out + (left + 1) * i, 1, type, left, tag, comm, &requests[w]);); (12, MPI, MPI_Irecv(&ack, 1, MPI_CHAR, right, tag, comm, &requests[0]);); (15, MPI, MPI_Isend(&ack, 1, MPI_CHAR, right, tag, comm, &requests[window_size - 1]);)", "label": "(21, MPI, MPI_Recv(&ack, 1, MPI_CHAR, right, tag, comm, MPI_STATUS_IGNORE);); (23, MPI, MPI_Irecv(out, count, type, set_src ? left : MPI_ANY_SOURCE, tag, comm, &requests[w]);); (18, MPI, MPI_Waitall(window_size, requests, MPI_STATUSES_IGNORE);); (15, MPI, MPI_Isend(in, count, type, right, (tag == MPI_ANY_TAG ? 0 : tag), comm, &requests[w]);); (23, MPI, MPI_Send(&ack, 1, MPI_CHAR, left, (tag == MPI_ANY_TAG ? 0 : tag), comm);)"}
{"predict": "(6, OpenMP, #pragma omp task shared(var, a)); (17, OpenMP, #pragma omp taskwait); (1, OpenMP, #pragma omp parallel num_threads(2) shared(var, a)); (4, OpenMP, #pragma omp master)", "label": "(4, OpenMP, #pragma omp master); (5, OpenMP, #pragma omp taskgroup); (6, OpenMP, #pragma omp task shared(var, a)); (0, OpenMP, #pragma omp parallel num_threads(2) shared(var, a))"}
{"predict": "(15, MPI, MPI_Alltoallv(data_out, counts, displs, MPI_INT, data_in, counts, displs, MPI_INT, comm);); (2, MPI, MPI_Comm_size(comm, &size);); (2, MPI, MPI_Comm_rank(comm, &rank);)", "label": "(3, MPI, MPI_Comm_size(comm, &size);); (3, MPI, MPI_Comm_rank(comm, &rank);); (13, MPI, MPI_Alltoallv(data_out, counts, displs, MPI_INT, data_in, counts, displs, MPI_INT, comm);)"}
{"predict": "(13, CUDA, cudaMalloc(&dev_B, size_b);); (30, CUDA, cudaFree(dev_B);); (10, CUDA, cudaMalloc(&dev_A, size_a);); (27, CUDA, cudaFree(dev_A);); (14, CUDA, cudaDeviceSynchronize();); (7, CUDA, cudaMemcpy(dev_A, argv[1], size_a, cudaMemcpyHostToDevice);); (20, CUDA, cudaMemcpy(dev_A, argv[1], size_a, cudaMemcpyHostToDevice);)", "label": "(13, CUDA, cudaMalloc(&dev_B, size_b);); (13, CUDA, cudaDeviceSynchronize();); (10, CUDA, cudaMalloc(&dev_A, size_a);); (27, CUDA, cudaFree(dev_B);); (25, CUDA, cudaFree(dev_A);)"}
{"predict": "(120, OpenMP, #pragma omp target teams map(t.D)); (111, OpenMP, #pragma omp target data map(to: t.C)); (116, OpenMP, #pragma omp target teams map(r.C)); (109, OpenMP, #pragma omp target teams map(r.D)); (151, OpenMP, #pragma omp target teams map(u.C)); (151, OpenMP, #pragma omp target data map(to: u.C)); (110, OpenMP, #pragma omp target teams map(t.C)); (103, OpenMP, #pragma omp target teams map(r.B)); (109, OpenMP, #pragma omp target teams map(t.D)); (139, OpenMP, #pragma omp target teams map(u.D)); (140, OpenMP, #pragma omp target data map(to: u.D)); (100, OpenMP, #pragma omp target teams map(r.A)); (84, OpenMP, #pragma omp target teams map(marr[:2] [1:][:])); (76, OpenMP, #pragma omp target teams map(marr[:2][:][:1])); (95, OpenMP, #pragma omp target teams map(SA<int,123>::B)); (111, OpenMP, #pragma omp target teams map(r.E)); (112, OpenMP, #pragma omp target data map(to: r.E)); (95, OpenMP, #pragma omp target teams map(SA<int,123>::C)); (106, OpenMP, #pragma omp target teams map(r.C)); (73, OpenMP, #pragma omp target teams map(marr[:2][:][1:])); (99, OpenMP, #pragma omp target data map(to: t)); (100, OpenMP, #pragma omp target teams map(t.B)); (133, OpenMP, #pragma omp target teams map(u.B)); (75, OpenMP, #pragma omp target teams map(marr2[:][0][:])); (96, OpenMP, #pragma omp target teams map(t.A)); (100, OpenMP, #pragma omp target data map(to: r.C)); (105, OpenMP, #pragma omp target teams map(r)); (133, OpenMP, #pragma omp target data map", "label": "(35, OpenMP, #pragma omp target teams map(marr[:arg][n:])); (166, OpenMP, #pragma omp target teams map(r.S.Arr[4:5])); (53, OpenMP, #pragma omp target teams map(marr[:arg][:2][:])); (120, OpenMP, #pragma omp target teams map(r.PtrS[0], r.PtrS->B)); (117, OpenMP, #pragma omp target teams map(r.ArrS[0].A, t.ArrS[1].A)); (136, OpenMP, #pragma omp target teams map(r.C, r.D)); (9, OpenMP, #pragma omp target teams map(r)); (80, OpenMP, #pragma omp target teams map(mvla2[:1][:2][:10])); (123, OpenMP, #pragma omp target teams map(r.RPtrS, r.RPtrS->B)); (136, OpenMP, #pragma omp target teams map(r.C) map(r.C)); (21, OpenMP, #pragma omp target teams map(marr[arg:][:arg][:])); (189, OpenMP, #pragma omp target data map(to: t)); (157, OpenMP, #pragma omp target teams map(r.S.Ptr[4:5])); (54, OpenMP, #pragma omp target teams map(marr[:2][2+2-4:][0:5+5])); (35, OpenMP, #pragma omp target teams map(marr[:2][:][1:])); (12, OpenMP, #pragma omp target teams map(marr[arg:][:][:])); (93, OpenMP, #pragma omp target teams map(r.ArrS[:arg].B)); (0, OpenMP, #pragma omp target teams map(marr[2][0:2][0:2])); (80, OpenMP, #pragma omp target teams map(mptr[:1][:2][0:2])); (53, OpenMP, #pragma omp target teams map(marr2[:1][:2][0])); (65, OpenMP, #pragma omp target teams map(mvla2[:1][:2][:11])); (86, OpenMP, #pragma omp target teams map(r.ArrS[:1].B)); (35, OpenMP, #pragma omp target teams map(marr[:1][3:1][:2])); (6, OpenMP, #pragma omp target teams map(marr[arg:])); (29, OpenMP, #pragma omp target teams map(marr[:1][0][:])); (134, OpenMP, #pragma omp target teams map(r.Ptr[3:5])); (74, OpenMP, #pragma omp target teams map(mptr[:1][:][0:2])); (108, OpenMP, #pragma omp target teams map(r.S.Arr[:12])); (125, OpenMP, #pragma omp target teams map(r.A)); (90, OpenMP, #pragma omp target teams map(r.ArrS[0].A, r.ArrS[1].A)); (95, OpenMP, #pragma omp target teams map(r.PtrS, r.PtrS->B)); (164, OpenMP, #pragma omp target teams map(t.D)); (149, OpenMP, #pragma omp target teams map(r.D)); (96, OpenMP, #pragma omp target teams map(r.RPtrS[0], r.RPtrS->B)); (77, OpenMP, #pragma omp target teams map(r.ArrS[0].Arr[1:23])); (98, OpenMP, #pragma omp target teams map(r.RPtrS->A, r.RPtrS->B)); (45, OpenMP, #pragma omp target teams map(mvla[1][2:arg][:])); (142, OpenMP, #pragma omp target data map(to: r.C)); (21, OpenMP, #pragma omp target teams map(marr[:1][3:arg][:2])); (8, OpenMP, #pragma omp target teams map(marr[:2][:][:1])); (37, OpenMP, #pragma omp target teams map(mvla[:2][:arg][:])); (61, OpenMP, #pragma omp target teams map(mptr[:2][:1][0:2])); (71, OpenMP, #pragma omp target teams map(r.ArrS[0].Arr[1:arg])); (142, OpenMP, #pragma omp target data map(to: t.Ptr)); (31, OpenMP, #pragma omp target teams map(mvla[:1][:][0])); (112, OpenMP, #pragma omp target teams map(r.Arr[3:5])); (46, OpenMP, #pragma omp target teams map(mptr[:1][:2-1][2:4-3])); (108, OpenMP"}
{"predict": "(4, CUDA, checkCudaErrors(cudaFree(cusolverData.cusolverInfo));); (2, CUDA, checkCudaErrors(cudaFree(cusolverData.cusolverH));)", "label": "(7, CUDA, cudaFree(bufferOffset.W);); (3, CUDA, cudaFree(bufferOffset.srcShiftSum);); (1, CUDA, cudaFree(bufferOffset.dstMean);); (10, CUDA, cudaFree(cusolverData.cusolverBuffer);); (5, CUDA, cudaFree(bufferOffset.J);); (3, CUDA, cudaFree(bufferOffset.r);); (4, CUDA, cudaFree(bufferOffset.calc_buffer);); (-1, CUDA, cudaFree(bufferOffset.LtL);); (-6, CUDA, cudaFree(bufferOffset.srcMean);); (-4, CUDA, cudaFree(bufferOffset.dstShiftSum);); (4, CUDA, cudaFree(cusolverData.cusolverInfo);)"}
{"predict": "(4, CUDA, cudaMallocHost((void **)&h_data, sizeof(SiftPoint)*data.maxPts);); (10, CUDA, cudaMemcpy(h_data, data.m_data, sizeof(SiftPoint)*data.maxPts, cudaMemcpyDeviceToHost);)", "label": "(8, CUDA, safeCall(cudaMemcpy(h_data, data.d_data, sizeof(SiftPoint)*data.numPts, cudaMemcpyDeviceToHost));)"}
{"predict": "(14, OpenMP, #pragma omp parallel for allocate(argc) private(argc, b), firstprivate(c, d), lastprivate(d, f) collapse(N) schedule(static, N) ordered(N) if (parallel :argc) num_threads(N) default(shared) shared(e) reduction(+ : h)); (5, OpenMP, #pragma omp parallel for schedule(dynamic) default(none) copyin(g) linear(a) allocate(a) lastprivate(conditional:d,e,f) order(concurrent))", "label": "(7, OpenMP, #pragma omp parallel for schedule(dynamic) default(none) copyin(g) linear(a) allocate(a) lastprivate(conditional: d, e,f) order(concurrent)); (5, OpenMP, #pragma omp threadprivate(g)); (11, OpenMP, #pragma omp parallel for allocate(argc) private(argc, b), firstprivate(c, d), lastprivate(d, f) collapse(N) schedule(static, N) ordered(N) if (parallel :argc) num_threads(N) default(shared) shared(e) reduction(+ : h))"}
{"predict": "(33, TBB, ASSERT(default_ss == tbb::global_control::active_value(tbb::global_control::thread_stack_size), NULL);); (5, TBB, tbb::task_scheduler_init t(h_c+1);); (1, TBB, const unsigned h_c = tbb::tbb_thread::hardware_concurrency();); (20, TBB, size_t default_ss = tbb::global_control::active_value(tbb::global_control::thread_stack_size);)", "label": "(31, TBB, ASSERT(default_ss == tbb::global_control::active_value(tbb::global_control::thread_stack_size), NULL);); (3, TBB, const unsigned h_c = tbb::tbb_thread::hardware_concurrency();); (5, TBB, tbb::task_scheduler_init t(h_c+1);); (20, TBB, size_t default_ss = tbb::global_control::active_value(tbb::global_control::thread_stack_size);)"}
{"predict": "(35, CUDA, HANDLE_ERROR(cudaMalloc((void**)&state_deriv_GPU, sizeof(float) * DYN::STATE_DIM));); (38, CUDA, HANDLE_ERROR(cudaMemcpyAsync(s_d, s_gpu.data(), sizeof(float) * DYN::STATE_DIM, cudaMemcpyHostToDevice, s1));); (30, CUDA, HANDLE_ERROR(cudaMalloc((void**)&s_d, sizeof(float) * DYN::STATE_DIM));); (31, CUDA, HANDLE_ERROR(cudaMalloc((void**)&u_d, sizeof(float) * DYN::CONTROL_DIM));); (42, CUDA, CudaCheckError();); (36, CUDA, HANDLE_ERROR(cudaMemcpyAsync(u_d, u.data(), sizeof(float) * DYN::CONTROL_DIM, cudaMemcpyHostToDevice, s1));); (25, CUDA, cudaStreamCreate(&s1);); (36, CUDA, HANDLE_ERROR(cudaStreamSynchronize(s1));); (43, CUDA, HANDLE_ERROR(cudaMemcpyAsync(s_gpu.data(), s_d, sizeof(float) * DYN::STATE_DIM, cudaMemcpyDeviceToHost, s1));); (26, CUDA, HANDLE_ERROR(cudaMemcpyAsync(s_d, s_gpu.data(), sizeof(float) * DYN::STATE_DIM, cudaMemcpyHostToDevice, s1));); (23, CUDA, HANDLE_ERROR(cudaMemcpyAsync(u_d, u.data(), sizeof(float) * DYN::CONTROL_DIM, cudaMemcpyHostToDevice, s1));)", "label": "(34, CUDA, HANDLE_ERROR(cudaMalloc((void**)&state_deriv_GPU, sizeof(float) * DYN::STATE_DIM));); (37, CUDA, HANDLE_ERROR(cudaMemcpyAsync(s_d, s_gpu.data(), sizeof(float) * DYN::STATE_DIM, cudaMemcpyHostToDevice, s1));); (46, CUDA, CudaCheckError();); (34, CUDA, HANDLE_ERROR(cudaMemcpyAsync(u_d, u.data(), sizeof(float) * DYN::CONTROL_DIM, cudaMemcpyHostToDevice, s1));); (29, CUDA, HANDLE_ERROR(cudaMalloc((void**)&s_d, sizeof(float) * DYN::STATE_DIM));); (27, CUDA, HANDLE_ERROR(cudaMalloc((void**)&u_d, sizeof(float) * DYN::CONTROL_DIM));); (45, CUDA, HANDLE_ERROR(cudaMemcpyAsync(s_gpu.data(), s_d, sizeof(float) * DYN::STATE_DIM, cudaMemcpyDeviceToHost, s1));); (32, CUDA, HANDLE_ERROR(cudaStreamSynchronize(s1));); (19, CUDA, cudaStreamCreate(&s1);)"}
{"predict": "(69, CUDA, cudaStream_t stream = at::cuda::getCurrentCUDAStream();)", "label": "(69, CUDA, cudaStream_t stream = at::cuda::getCurrentCUDAStream();)"}
{"predict": "(7, MPI, MPI_Proc_null_init(buf, 10, MPI_INT, tag, MPI_COMM_WORLD, MPI_INFO_NULL, &r);)", "label": "(8, MPI, MPI_Start(&r);); (11, MPI, MPI_Send_init(buf, 10, MPI_INT, MPI_PROC_NULL, tag, MPI_COMM_WORLD, &r);); (7, MPI, MPI_Wait(&r, &s);); (4, MPI, MPI_Recv_init(buf, 10, MPI_INT, MPI_PROC_NULL, tag, MPI_COMM_WORLD, &r);); (6, MPI, MPI_Request_free(&r);)"}
{"predict": "(10, OpenMP, #pragma omp target if (p)); (11, OpenMP, #pragma omp target if (a)); (12, OpenMP, #pragma omp target if (1 0)); (2, OpenMP, #pragma omp target if ()); (4, OpenMP, #pragma omp target if (foobool(p)); (5, OpenMP, #pragma omp target if (S1)); (0, OpenMP, #pragma omp target if (); (6, OpenMP, #pragma omp target if (a > 0)); (7, OpenMP, #pragma omp target if (p) map(p->a)); (-3, OpenMP, #pragma omp target if (p)", "label": "(14, OpenMP, #pragma omp parallel reduction(+ : incomplete, ([10])p)); (1, OpenMP, #pragma omp parallel reduction(); (2, OpenMP, #pragma omp parallel reduction(unknown); (9, OpenMP, #pragma omp parallel reduction(inscan, + : a)); (6, OpenMP, #pragma omp parallel reduction(default, + : a)); (1, OpenMP, #pragma omp parallel reduction(default,); (2, OpenMP, #pragma omp parallel reduction(unknown, +: a))"}
{"predict": "(52, OpenCL, clReleaseEvent(event);)", "label": "(51, OpenCL, clReleaseEvent(event);)"}
{"predict": "(16, MPI, MPI_Init(&argc, &argv);); (21, MPI, MPI_Finalize();)", "label": "(16, MPI, MPI_Init(&argc, &argv);); (20, MPI, MPI_Finalize();)"}
{"predict": "(11, MPI, MPI_Abort(MPI_COMM_WORLD, MPIT_result);); (12, MPI, MPIT_result = MPI_T_pvar_session_create(session);); (6, MPI, MPIT_result = MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);)", "label": "(11, MPI, MPI_Abort(MPI_COMM_WORLD, MPIT_result);); (12, MPI, MPIT_result = MPI_T_pvar_session_create(session);); (6, MPI, MPIT_result = MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);)"}
{"predict": "(6, CUDA, __shared__ float block_acc[32];)", "label": "(6, CUDA, __shared__ float block_acc[32];)"}
{"predict": "(3, OpenMP, int maxThreads = omp_get_max_threads();)", "label": "(3, OpenMP, auto maxThreads = omp_get_max_threads();)"}
{"predict": "(6, MPI, MPI_Init(&argc, &argv);); (19, MPI, MPI_Finalize();); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (7, MPI, MPI_Comm_get_parent(&intercomm);); (11, MPI, err = MPI_Recv(str, 4, MPI_CHAR, 3, 0, intercomm, MPI_STATUS_IGNORE);); (8, MPI, err = MPI_Send(\"hi\", 3, MPI_CHAR, 3, 0, intercomm);)", "label": "(6, MPI, MPI_Init(&argc, &argv);); (19, MPI, MPI_Finalize();); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (7, MPI, MPI_Comm_get_parent(&intercomm);); (11, MPI, err = MPI_Recv(str, 4, MPI_CHAR, 3, 0, intercomm, MPI_STATUS_IGNORE);); (8, MPI, err = MPI_Send(\"hi\", 3, MPI_CHAR, 3, 0, intercomm);)"}
{"predict": "(31, CUDA, cute::cuda_safe_call(cudaDeviceSynchronize());); (29, CUDA, cute::cuda_safe_call(cudaGetLastError());)", "label": "(31, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (29, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());)"}
{"predict": "(6, OpenMP, #pragma omp parallel if (argc))); (19, OpenMP, #pragma omp parallel if(parallel : argc) if (argc)); (13, OpenMP, #pragma omp parallel if(parallel :); (1, OpenMP, #pragma omp parallel if ()); (9, OpenMP, #pragma omp parallel if(if(tmain(argc, argv)); (0, OpenMP, #pragma omp parallel if (argc); (8, OpenMP, #pragma omp parallel if(parallel); (-4, OpenMP, #pragma omp parallel if (); (2, OpenMP, #pragma omp parallel if (argv[1]=2)); (-7, OpenMP, #pragma omp parallel if); (6, OpenMP, #pragma omp parallel if(parallel : argc); (8, OpenMP, #pragma omp parallel if(parallel : argc) if (parallel:argc)); (-4, OpenMP, #pragma omp parallel if (foobool(argc)), if (true)); (-1, OpenMP, #pragma omp parallel if (1 0)); (4, OpenMP, #pragma omp parallel if(parallel : argc) if (for:argc)); (-8, OpenMP, #pragma omp parallel if (argc > 0 ? argv[1] : argv[2])); (-7, OpenMP, #pragma omp parallel if (S1)); (0, OpenMP, #pragma omp parallel if(parallel : argc>> z)); (-7, OpenMP, #pragma omp parallel if (argc argc))", "label": "(5, OpenMP, #pragma omp task final (argc); (3, OpenMP, #pragma omp task final ()); (8, OpenMP, #pragma omp task final (argv[1]=2)); (4, OpenMP, #pragma omp task final (argc > 0 ? argv[1] : argv[2])); (8, OpenMP, #pragma omp task final (1 0)); (1, OpenMP, #pragma omp task final (z+argc))); (2, OpenMP, #pragma omp task final (foobool(argc)), final (true)); (6, OpenMP, #pragma omp task final(if(tmain(argc, argv)); (-5, OpenMP, #pragma omp task final (); (2, OpenMP, #pragma omp task final (argc argc)); (-8, OpenMP, #pragma omp task final); (-2, OpenMP, #pragma omp task final (S1))"}
{"predict": "(10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (17, MPI, MPI_Type_commit(&eq_type);); (25, MPI, MPI_Op_free(&op);); (19, MPI, MPI_Reduce(&data, &result, 1, eq_type, op, 0, MPI_COMM_WORLD);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (11, MPI, MPI_Type_create_struct(2, blklens, displs, types, &eq_type);); (12, MPI, MPI_Op_create((MPI_User_function *) string_int_equal, 1, &op);); (20, MPI, MPI_Type_free(&eq_type);)", "label": "(17, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (21, MPI, MPI_Reduce(&data, &result, 1, eq_type, op, 0, MPI_COMM_WORLD);); (25, MPI, MPI_Op_free(&op);); (10, MPI, MPI_Type_create_struct(2, blklens, displs, types, &eq_type);); (10, MPI, MPI_Type_commit(&eq_type);); (3, MPI, MPI_Op_create(myop, 0, &op);); (10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (21, MPI, MPI_Type_free(&eq_type);)"}
{"predict": "(4, OpenMP, #pragma omp parallel reduction(+:var1) reduction(-:var2))", "label": "(7, OpenMP, #pragma omp critical(add_first)); (3, OpenMP, #pragma omp parallel shared(var1, var2)); (7, OpenMP, #pragma omp atomic); (8, OpenMP, #pragma omp single)"}
{"predict": "(13, TBB, test_extract<tbb::flow::rejecting>();); (7, TBB, lightweight_testing::test<tbb::flow::function_node>(10);); (12, TBB, test_extract<tbb::flow::queueing>();)", "label": "(9, TBB, test_ports_return_references<tbb::flow::rejecting>();); (9, TBB, lightweight_testing::test<tbb::flow::multifunction_node>(10);); (13, TBB, test_extract<tbb::flow::rejecting>();); (5, TBB, test_ports_return_references<tbb::flow::queueing>();); (12, TBB, test_extract<tbb::flow::queueing>();)"}
{"predict": "(1, CUDA, __shared__ float s_mem[32];)", "label": "(1, CUDA, __shared__ float s_mem[32];)"}
{"predict": "(12, MPI, rc = MPI_Type_get_extent(type, &tmp_lb, &attr->child_extent);); (14, MPI, rc = MPI_Type_dup(type, newtype);); (19, MPI, rc = MPI_Type_free(&type);)", "label": "(15, MPI, rc = MPI_Type_dup(type, newtype);); (11, MPI, rc = MPI_Type_get_extent(type, &tmp_lb, &attr->child_type_extent);); (19, MPI, rc = MPI_Type_free(&type);)"}
{"predict": "(60, OpenMP, #pragma omp parallel for reduction(+ : ret_sum))", "label": "(60, OpenMP, #pragma omp parallel for reduction(+ : ret_sum))"}
{"predict": "(1, TBB, tbb::task::spawn_root_and_wait( *new tbb::empty_task );); (22, TBB, tbb::task::destroy( *dummy_root );); (3, TBB, tbb::task* dummy_root = new( tbb::task::allocate_root() ) tbb::empty_task;)", "label": "(3, TBB, tbb::task* dummy_root = new( tbb::task::allocate_root() ) tbb::empty_task;); (22, TBB, tbb::task::destroy( *dummy_root );); (13, TBB, tbb::task::spawn_root_and_wait(t);); (16, TBB, tbb::task::enqueue(t);)"}
{"predict": "(32, OpenMP, #pragma omp target update from(z[:10])); (17, OpenMP, #pragma omp target update to(z[:10])); (18, OpenMP, #pragma omp target map(to:k[:5]) map(always, tofrom:x) map(tofrom:y))", "label": "(28, OpenMP, #pragma omp target update from(z[:10])); (17, OpenMP, #pragma omp target update to(z[:10])); (18, OpenMP, #pragma omp target map(to:k[:5]) map(always, tofrom:x) map(tofrom:y))"}
{"predict": "(29, OpenMP, #pragma omp target update from(z[:10])); (17, OpenMP, #pragma omp target update to(z[:10])); (18, OpenMP, #pragma omp target map(to:k[:5]) map(always, tofrom:x) map(tofrom:y))", "label": "(28, OpenMP, #pragma omp target update from(z[:10])); (17, OpenMP, #pragma omp target update to(z[:10])); (18, OpenMP, #pragma omp target map(to:k[:5]) map(always, tofrom:x) map(tofrom:y))"}
{"predict": "(17, OpenMP, fprintf(stderr, \"Error: invalid nesting order specified: %d\\n\", nest);); (17, OpenMP, abort();)", "label": "(18, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(3, OpenMP, #pragma omp target); (3, OpenMP, #pragma omp teams distribute)", "label": "(4, OpenMP, #pragma omp teams distribute); (2, OpenMP, #pragma omp target)"}
{"predict": "(41, CUDA, return cudaGetLastError();)", "label": "(41, CUDA, return cudaGetLastError();)"}
{"predict": "(14, CUDA, extern __shared__ float alpha_buffer_sh[];)", "label": "(14, CUDA, extern __shared__ float alpha_buffer_sh[];)"}
{"predict": "(65, MPI, rc = MPI_Type_free(&type);)", "label": "(65, MPI, rc = MPI_Type_free(&type);)"}
{"predict": "(11, OpenMP, #pragma omp atomic hint(AMD_fast_fp_atomics)); (4, OpenMP, #pragma omp atomic hint(AMD_safe_fp_atomics)); (1, OpenMP, #pragma omp atomic hint(AMD_fp_atomics)); (6, OpenMP, #pragma omp atomic hint(AMD_pteam_fp_atomics)); (3, OpenMP, #pragma omp atomic hint(AMD_concurrent_fp_atomics)); (6, OpenMP, #pragma omp atomic hint(AMD_global_fp_atomics))", "label": "(12, OpenMP, #pragma omp atomic hint(1) hint(1)); (7, OpenMP, #pragma omp atomic hint(a); (4, OpenMP, #pragma omp atomic hint(+); (1, OpenMP, #pragma omp atomic hint(); (6, OpenMP, #pragma omp atomic hint(a)); (-3, OpenMP, #pragma omp atomic hint)"}
{"predict": "(7, MPI, MPI_Init(&argc, &argv);); (26, MPI, MPI_Finalize();); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(25, MPI, MPI_Finalize();); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Init(NULL, NULL);)"}
{"predict": "(2, OpenMP, #pragma omp target parallel for if (false)); (5, OpenMP, #pragma omp target parallel for if (Arg)); (8, OpenMP, #pragma omp target parallel for if (target: Arg))", "label": "(6, OpenMP, #pragma omp target teams distribute parallel for if (false)); (9, OpenMP, #pragma omp target teams distribute parallel for if (parallel: Arg)); (0, OpenMP, #pragma omp target teams distribute parallel for if (true))"}
{"predict": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(6, CUDA, thrust::device_ptr<thrust::complex<double>> d_ptr = thrust::device_malloc<thrust::complex<double>>(1);); (14, CUDA, thrust::device_free(d_ptr);); (8, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize());); (8, CUDA, qmc_cuda::cuda_check(cudaGetLastError());)", "label": "(6, CUDA, thrust::device_ptr<thrust::complex<double>> d_ptr = thrust::device_malloc<thrust::complex<double>>(1);); (14, CUDA, thrust::device_free(d_ptr);); (9, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize());); (7, CUDA, qmc_cuda::cuda_check(cudaGetLastError());)"}
{"predict": "(13, OpenCL, clReleaseMemObject(objScratch);); (8, OpenCL, clReleaseMemObject(objNrm2);); (3, OpenCL, clReleaseMemObject(objX);)", "label": "(5, OpenCL, clReleaseMemObject(objX);); (12, OpenCL, clReleaseMemObject(objScratch);); (7, OpenCL, clReleaseMemObject(objNrm2);)"}
{"predict": "(67, MPI, MPI_Waitall(2, request, status);)", "label": "(67, MPI, MPI_Waitall(2, request, status);)"}
{"predict": "(22, MPI, MPI_Group_compare(group, wingroup, &result);); (21, MPI, MPI_Win_get_group(win, &group);); (18, MPI, MPI_Win_create(buf, 10 * sizeof(int), sizeof(int), MPI_INFO_NULL, comm, &win);); (27, MPI, MPI_Win_free(&win);); (19, MPI, MPI_Comm_group(comm, &wingroup);); (25, MPI, MPI_Group_free(&wingroup);); (22, MPI, MPI_Group_free(&group);)", "label": "(28, MPI, MPI_Group_free(&wingroup);); (19, MPI, MPI_Win_create(buf, sizeof(int) * 10, sizeof(int), MPI_INFO_NULL, comm, &win);); (28, MPI, MPI_Win_free(&win);); (20, MPI, MPI_Group_compare(group, wingroup, &result);); (25, MPI, MPI_Group_free(&group);); (17, MPI, MPI_Comm_group(comm, &group);); (15, MPI, MPI_Win_get_group(win, &wingroup);)"}
{"predict": "(14, CUDA, ColorSpinorParam cudaParam(cpuParam, *inv_param, QUDA_CUDA_FIELD_LOCATION);)", "label": "(14, CUDA, ColorSpinorParam cudaParam(cpuParam, *inv_param, QUDA_CUDA_FIELD_LOCATION);)"}
{"predict": "(15, OpenMP, #pragma omp target parallel for)", "label": "(15, OpenMP, #pragma omp target parallel for)"}
{"predict": "(11, CUDA, HANDLE_ERROR(cudaMalloc((void**)&initial_x_d, sizeof(float) * 2 * DYN_T::STATE_DIM));); (11, CUDA, HANDLE_ERROR(cudaMalloc((void**)&cost_trajectories_d, sizeof(float) * 2 * num_rollouts));)", "label": "(25, CUDA, HANDLE_ERROR(cudaStreamSynchronize(stream));); (9, CUDA, HANDLE_ERROR(cudaMalloc((void**)&initial_x_d, sizeof(float) * 2 * DYN_T::STATE_DIM));); (9, CUDA, HANDLE_ERROR(cudaMalloc((void**)&cost_trajectories_d, sizeof(float) * 2 * num_rollouts));)"}
{"predict": "(6, CUDA, extern __shared__ float s_buffer[];)", "label": "(6, CUDA, extern __shared__ float s_buffer[];)"}
{"predict": "(37, CUDA, cudaMalloc((void **)&result_data, tensor1->size * sizeof(float));)", "label": "(37, CUDA, cudaMalloc((void **)&result_data, tensor1->size * sizeof(float));)"}
{"predict": "(29, CUDA, CUDA_CHECK(cudaMemcpy(P_host, P_device, size, cudaMemcpyDeviceToHost));); (35, CUDA, cudaFree(P_device);); (10, CUDA, CUDA_CHECK(cudaMemcpy(M_device, M_host, size, cudaMemcpyHostToDevice));); (34, CUDA, cudaFree(N_device);); (34, CUDA, cudaFree(M_device);); (25, CUDA, CUDA_CHECK(cudaDeviceSynchronize());); (7, CUDA, CUDA_CHECK(cudaMemcpy(N_device, N_host, size, cudaMemcpyHostToDevice));); (2, CUDA, CUDA_CHECK(cudaMalloc((void**)&N_device, size));); (9, CUDA, CUDA_CHECK(cudaMalloc((void**)&P_device, size));;); (-1, CUDA, CUDA_CHECK(cudaMalloc((void**)&M_device, size));)", "label": "(29, CUDA, CUDA_CHECK(cudaMemcpy(P_host, P_device, size, cudaMemcpyDeviceToHost));); (35, CUDA, cudaFree(P_device);); (10, CUDA, CUDA_CHECK(cudaMemcpy(M_device, M_host, size, cudaMemcpyHostToDevice));); (34, CUDA, cudaFree(N_device);); (34, CUDA, cudaFree(M_device);); (25, CUDA, CUDA_CHECK(cudaDeviceSynchronize());); (7, CUDA, CUDA_CHECK(cudaMemcpy(N_device, N_host, size, cudaMemcpyHostToDevice));); (2, CUDA, CUDA_CHECK(cudaMalloc((void**)&N_device, size));); (9, CUDA, CUDA_CHECK(cudaMalloc((void**)&P_device, size));;); (-1, CUDA, CUDA_CHECK(cudaMalloc((void**)&M_device, size));)"}
{"predict": "(11, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)", "label": "(11, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)"}
{"predict": "(6, OpenMP, #pragma omp task default()); (14, OpenMP, #pragma omp parallel default(shared)); (0, OpenMP, #pragma omp task default); (2, OpenMP, #pragma omp task default (none); (10, OpenMP, #pragma omp parallel); (-2, OpenMP, #pragma omp task default()", "label": "(17, OpenMP, #pragma omp task default(firstprivate)); (4, OpenMP, #pragma omp task default(shared), default(shared)); (0, OpenMP, #pragma omp task default(); (0, OpenMP, #pragma omp task default()); (5, OpenMP, #pragma omp task default(none)); (1, OpenMP, #pragma omp task default(x)); (7, OpenMP, #pragma omp task default(shared)); (-6, OpenMP, #pragma omp task default); (-4, OpenMP, #pragma omp task default(none)"}
{"predict": "(4, TBB, const oneapi::tbb::blocked_range3d<int> r( 0, i, 5, 0, j, 3, 0, k, 1 );); (4, TBB, oneapi::tbb::parallel_for( r, Striker() );)", "label": "(4, TBB, const tbb::blocked_range3d<int> r( 0, i, 5, 0, j, 3, 0, k, 1 );); (4, TBB, tbb::parallel_for( r, Striker() );)"}
{"predict": "(34, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (50, CUDA, CUDA_CHECK(cudaDeviceReset());); (47, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (23, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (41, CUDA, CUDA_CHECK(cudaFree(d_A));); (17, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)", "label": "(34, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (50, CUDA, CUDA_CHECK(cudaDeviceReset());); (47, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (23, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (41, CUDA, CUDA_CHECK(cudaFree(d_A));); (17, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(4, TBB, const oneapi::tbb::blocked_range3d<int> r( 0, i, 5, 0, j, 3, 0, k, 1 );); (4, TBB, oneapi::tbb::parallel_for( r, Striker() );)", "label": "(4, TBB, const tbb::blocked_range3d<int> r( 0, i, 5, 0, j, 3, 0, k, 1 );); (4, TBB, tbb::parallel_for( r, Striker() );)"}
{"predict": "(36, CUDA, CUDASimulation cudaSimulation(model);)", "label": "(36, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(9, OpenMP, #pragma omp target); (11, OpenMP, #pragma omp distribute parallel for lastprivate(g, g1, svar, sfvar)); (8, OpenMP, #pragma omp teams)", "label": "(9, OpenMP, #pragma omp target); (65, OpenMP, #pragma omp distribute parallel for lastprivate(t_var, vec, s_arr, s_arr, var, var, svar)); (9, OpenMP, #pragma omp distribute parallel for lastprivate(g, g1, svar, sfvar)); (7, OpenMP, #pragma omp teams)"}
{"predict": "(66, OpenMP, iis2mdc_offset_temp_comp_set(&mag_ctx, PROPERTY_ENABLE);)", "label": "(66, OpenMP, iis2mdc_offset_temp_comp_set(&mag_ctx, PROPERTY_ENABLE);)"}
{"predict": "(71, CUDA, checkCudaErrors( ::cudaDeviceSynchronize() );)", "label": "(101, CUDA, checkHIPErrors( ::cudaDeviceSynchronize() );)"}
{"predict": "(22, OpenMP, #pragma omp target defaultmap(none:pointer)); (28, OpenMP, #pragma omp target defaultmap(none:pointer) defaultmap(none:scalar) map(argc)); (2, OpenMP, #pragma omp target defaultmap(tofrom,); (9, OpenMP, #pragma omp target defaultmap (tofrom:); (4, OpenMP, #pragma omp target defaultmap(tofrom); (10, OpenMP, #pragma omp target defaultmap(tofrom:)); (28, OpenMP, #pragma omp target defaultmap(none:pointer) defaultmap(none:scalar) defaultmap(none:aggregate) map(g) firstprivate(argc)); (30, OpenMP, #pragma omp target defaultmap(none:pointer) defaultmap(none:scalar) map(argc) is_device_ptr(g)); (12, OpenMP, #pragma omp target defaultmap (scalar:); (13, OpenMP, #pragma omp target defaultmap(alloc:); (-4, OpenMP, #pragma omp target defaultmap()); (14, OpenMP, #pragma omp target defaultmap(tofrom, alloc:); (0, OpenMP, #pragma omp target defaultmap (tofrom ()", "label": "(9, OpenMP, #pragma omp target defaultmap(tofrom:); (4, OpenMP, #pragma omp target defaultmap()); (25, OpenMP, #pragma omp target defaultmap(none:pointer)); (10, OpenMP, #pragma omp target defaultmap(tofrom,); (40, OpenMP, #pragma omp parallel); (-4, OpenMP, #pragma omp target defaultmap); (27, OpenMP, #pragma omp target defaultmap(none:pointer) defaultmap(none:scalar) defaultmap(none:aggregate) map(g) firstprivate(argc)); (8, OpenMP, #pragma omp target defaultmap(scalar:); (11, OpenMP, #pragma omp target defaultmap(tofrom: scalar) defaultmap(tofrom: scalar)); (20, OpenMP, #pragma omp target defaultmap(none:pointer) defaultmap(none:scalar) map(argc)); (1, OpenMP, #pragma omp target defaultmap(tofrom)); (6, OpenMP, #pragma omp target defaultmap(tofrom, scalar); (23, OpenMP, #pragma omp target defaultmap(none:aggregate)); (-10, OpenMP, #pragma omp target defaultmap(); (9, OpenMP, #pragma omp target defaultmap(tofrom : scalar) defaultmap(to: scalar)); (16, OpenMP, #pragma omp target defaultmap(none:pointer) defaultmap(none:scalar) map(g)); (9, OpenMP, #pragma omp target defaultmap(alloc: pointer) defaultmap(to: scalar) defaultmap(firstprivate: pointer)); (20, OpenMP, #pragma omp target defaultmap(none:aggregate) defaultmap(none:scalar) map(argc)); (-11, OpenMP, #pragma omp target defaultmap(tofrom)"}
{"predict": "(13, CUDA, cudaEventRecord(stop);); (13, CUDA, cudaEventSynchronize(stop);); (21, CUDA, cudaFree(d_data);); (10, CUDA, cudaMemcpy(d_data, h_data, SIZE * sizeof(float), cudaMemcpyHostToDevice);); (15, CUDA, cudaMemcpy(h_data, d_data, SIZE * sizeof(float), cudaMemcpyDeviceToHost);); (-1, CUDA, cudaEventCreate(&stop);); (4, CUDA, cudaEventRecord(start);); (-5, CUDA, cudaMalloc(&d_data, SIZE * sizeof(float));); (-3, CUDA, cudaEventCreate(&start);); (7, CUDA, cudaEventElapsedTime(&milliseconds, start, stop);)", "label": "(12, CUDA, cudaEventRecord(stop);); (12, CUDA, cudaEventSynchronize(stop);); (9, CUDA, cudaMemcpy(d_data, h_data, SIZE * sizeof(float), cudaMemcpyHostToDevice);); (7, CUDA, cudaEventRecord(start);); (-1, CUDA, cudaMalloc((void **)&d_data, SIZE * sizeof(float));); (10, CUDA, cudaEventElapsedTime(&milliseconds, start, stop);); (1, CUDA, cudaEventCreate(&stop);); (13, CUDA, cudaMemcpy(h_data, d_data, SIZE * sizeof(float), cudaMemcpyDeviceToHost);); (19, CUDA, cudaFree(d_data);); (-3, CUDA, cudaEventCreate(&start);)"}
{"predict": "(10, OpenMP, #pragma omp parallel for); (45, OpenMP, #pragma omp atomic)", "label": "(10, OpenMP, #pragma omp parallel for); (45, OpenMP, #pragma omp atomic)"}
{"predict": "(16, CUDA, thrust::complex<T> const* m_(m + batch * mstride);); (14, CUDA, __shared__ thrust::cuda_cub::core::uninitialized_array<thrust::complex<T>, 256> tmp;)", "label": "(20, CUDA, thrust::complex<T> const* m_ = m + batch * mstride;); (16, CUDA, tmp[t] = thrust::complex<T>(0.0);); (9, CUDA, __shared__ thrust::cuda_cub::core::uninitialized_array<thrust::complex<T>, 64> tmp;)"}
{"predict": "(19, CUDA, __shared__ T B_thread_block_tile[BLOCK_TILE_SIZE_K][BLOCK_TILE_SIZE_X];); (17, CUDA, __shared__ T A_thread_block_tile[BLOCK_TILE_SIZE_Y][BLOCK_TILE_SIZE_K];)", "label": "(19, CUDA, __shared__ T B_thread_block_tile[BLOCK_TILE_SIZE_K][BLOCK_TILE_SIZE_X];); (17, CUDA, __shared__ T A_thread_block_tile[BLOCK_TILE_SIZE_Y][BLOCK_TILE_SIZE_K];)"}
{"predict": "(29, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (72, CUDA, CHECK_CUDA(cudaFree(dX));); (43, CUDA, CHECK_CUDA(cudaMalloc(&dX, size * sizeof(int32_t)));); (29, CUDA, CHECK_HIEDNN(hiednnSetCudaStream(handle, 0));); (67, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (69, CUDA, CHECK_CUDA(cudaFree(dY));); (40, CUDA, CHECK_CUDA(cudaMalloc(&dY, size * sizeof(int32_t)));)", "label": "(29, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (72, CUDA, CHECK_CUDA(cudaFree(dX));); (43, CUDA, CHECK_CUDA(cudaMalloc(&dX, size * sizeof(int32_t)));); (29, CUDA, CHECK_HIEDNN(hiednnSetCudaStream(handle, 0));); (67, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (69, CUDA, CHECK_CUDA(cudaFree(dY));); (40, CUDA, CHECK_CUDA(cudaMalloc(&dY, size * sizeof(int32_t)));)"}
{"predict": "(16, SYCL, using namespace cl::sycl::access;); (4, SYCL, cl::sycl::queue queue;)", "label": "(16, SYCL, using namespace cl::sycl::access;); (4, SYCL, cl::sycl::queue queue;)"}
{"predict": "(15, MPI, MPI_Irecv(buff, count, type, 0, tag, comm, &r);); (8, MPI, MPI_Comm_rank(comm, &rank);)", "label": "(22, MPI, MPI_Request_free(&r);); (14, MPI, MPI_Recv_init(buff, count, type, 0, tag, comm, &r);); (2, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(26, OpenCL, auto clCompressedImage = clCreateImageWithPropertiesINTEL(pContext, nullptr, CL_MEM_READ_WRITE | CL_MEM_COPY_HOST_PTR | CL_MEM_IMAGE_USE_LINEAR_STORAGE_INTEL, &imageDescriptor, &imageFormat, compressedBuffer.get(), &retVal);); (64, OpenCL, clReleaseMemObject(clCompressedImage);)", "label": "(64, OpenCL, clReleaseMemObject(clCompressedImage);); (25, OpenCL, auto clCompressedImage = clCreateImage(context, CL_MEM_READ_WRITE, &imageFormat, &imageDescriptor, nullptr, &retVal);)"}
{"predict": "(24, CUDA, cudaSetDevice(deviceId);); (20, CUDA, cudaGetDeviceCount(&nDevices);); (6, MPI, MPIManager mpi_manager(MPI_COMM_WORLD);)", "label": "(24, CUDA, cudaSetDevice(deviceId);); (20, CUDA, cudaGetDeviceCount(&nDevices);); (6, MPI, MPIManager mpi_manager(MPI_COMM_WORLD);)"}
{"predict": "(113, CUDA, cudaFree(d_diffData);); (16, CUDA, cudaMalloc((void**) &d_fcLayer, m * c * sizeof(double));); (74, CUDA, cudaMalloc((void**) &d_diffData, m * c * sizeof(double));); (105, CUDA, cudaFree(d_softmaxData);); (107, CUDA, cudaFree(d_gradData);); (13, CUDA, cudaMalloc((void**) &d_softmaxData, m * c * sizeof(double));); (103, CUDA, cudaFree(d_fcLayer);); (43, CUDA, cudaDeviceSynchronize();); (64, CUDA, cudaMalloc((void**) &d_gradData, m * c * sizeof(double));); (103, CUDA, cudaFree(d_diffData);); (12, CUDA, cudaMemcpy(d_fcLayer, fcLayer, m * c * sizeof(double), cudaMemcpyHostToDevice);)", "label": "(73, CUDA, cudaMalloc((void**) &d_gradData, m * c * sizeof(double));); (116, CUDA, cudaFree(d_fcLayer);); (118, CUDA, cudaFree(d_diffData);); (116, CUDA, cudaFree(d_gradData);); (12, CUDA, cudaMalloc((void**) &d_fcLayer, m * c * sizeof(double));); (17, CUDA, cudaMalloc((void**) &d_softmaxData, m * c * sizeof(double));); (43, CUDA, cudaDeviceSynchronize();); (11, CUDA, cudaMemcpy(d_fcLayer, fcLayer, m * c * sizeof(double), cudaMemcpyHostToDevice);); (72, CUDA, cudaMalloc((void**) &d_diffData, m * c * sizeof(double));); (109, CUDA, cudaFree(d_softmaxData);); (83, CUDA, cudaMemcpy(result_backward, d_gradData, m * c * sizeof(double), cudaMemcpyDeviceToHost);); (44, CUDA, cudaMemcpy(result, d_softmaxData, m * c * sizeof(double), cudaMemcpyDeviceToHost);); (69, CUDA, cudaMemcpy(d_diffData, diffData, m * c * sizeof(double), cudaMemcpyHostToDevice);)"}
{"predict": "(4, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (5, TBB, tbb::tick_count t1 = tbb::tick_count::now();)", "label": "(4, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (5, TBB, tbb::tick_count t1 = tbb::tick_count::now();)"}
{"predict": "(11, MPI, MPI_Init(&argc, &argv);); (44, MPI, MPI_Finalize();); (10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (35, MPI, MPI_Recv(recvbuf.data(), block_size, MPI_INT, 0, rank, MPI_COMM_WORLD, &status);); (26, MPI, MPI_Send(&sendbuf[i*block_size], block_size, MPI_INT, i, i, MPI_COMM_WORLD);); (20, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)", "label": "(11, MPI, MPI_Init(&argc, &argv);); (44, MPI, MPI_Finalize();); (10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (35, MPI, MPI_Recv(recvbuf.data(), block_size, MPI_INT, 0, rank, MPI_COMM_WORLD, &status);); (26, MPI, MPI_Send(&sendbuf[i*block_size], block_size, MPI_INT, i, i, MPI_COMM_WORLD);); (20, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(23, MPI, MPI_Reduce(gFrstCost, gFrstCostAcc, numForests, MPI_DOUBLE, MPI_SUM, 0, grid3d->zscp.comm);)", "label": "(23, MPI, MPI_Reduce(gFrstCost, gFrstCostAcc, numForests, MPI_DOUBLE, MPI_SUM, 0, grid3d->zscp.comm);)"}
{"predict": "(11, CUDA, __shared__ float u_[_N_];); (11, CUDA, __shared__ float r[_N_], k[_N_], v[_N_], w_[_N_], gy[_N_];)", "label": "(11, CUDA, __shared__ float u_[_N_];); (11, CUDA, __shared__ float r[_N_], k[_N_], v[_N_], w_[_N_], gy[_N_];)"}
{"predict": "(12, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(12, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(8, TBB, tbb::flow::make_edge(inner_node1, tbb::flow::input_port<1>(inner_node2));); (11, TBB, tbb::flow::make_edge(inner_node2, tbb::flow::input_port<1>(inner_node3));); (9, TBB, tbb::flow::make_edge(inner_node1, tbb::flow::input_port<0>(inner_node2));); (1, TBB, tbb::flow::function_node<int, int> outer_node(g, tbb::flow::unlimited, outer_body());); (12, TBB, tbb::flow::make_edge(outer_node/*inner_node3*/, inner_node3);); (-3, TBB, tbb::flow::graph g;); (-2, TBB, tbb::flow::broadcast_node<int> input(g);); (28, TBB, tbb::flow::remove_edge(input, outer_node);); (-3, TBB, tbb::flow::broadcast_node<int> output(g);)", "label": "(38, TBB, CHECK_MESSAGE( (tbb::flow::output_port<0>(outer_node).try_get(out) == output.try_get(out)), \"output from outer_node does not match output from graph\");); (13, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<1>(outer_node) == &tbb::flow::input_port<1>(inner_node1)), \"input port 1 of inner_node1 not bound to input port 1 in outer_node\");); (0, TBB, tbb::flow::composite_node<std::tuple<int, int>, std::tuple<int> > outer_node(g);); (8, TBB, outer_node.set_external_ports(base_type::input_ports_type(tbb::flow::input_port<0>(inner_node1), tbb::flow::input_port<1>(inner_node1)), base_type::output_ports_type(tbb::flow::output_port<0>(inner_node3)));); (-1, TBB, typedef tbb::flow::composite_node<std::tuple<int, int>, std::tuple<int> > base_type;); (18, TBB, tbb::flow::make_edge(inner_node2, tbb::flow::input_port<0>(inner_node3));); (18, TBB, tbb::flow::make_edge(inner_node2, tbb::flow::input_port<1>(inner_node3));); (13, TBB, tbb::flow::make_edge(inner_node1, tbb::flow::input_port<0>(inner_node2));); (-7, TBB, tbb::flow::graph g;); (6, TBB, CHECK_MESSAGE( (&tbb::flow::output_port<0>(outer_node) == &tbb::flow::output_port<0>(inner_node3)), \"output port 0 of inner_node3 not bound to output port 0 in outer_node\");); (11, TBB, tbb::flow::make_edge(inner_node1, tbb::flow::input_port<1>(inner_node2));); (2, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<0>(outer_node) == &tbb::flow::input_port<0>(inner_node1)), \"input port 0 of inner_node1 not bound to input port 0 in outer_node\");); (-8, TBB, tbb::flow::broadcast_node<int> input(g);); (-8, TBB, tbb::flow::queue_node<int> output(g);)"}
{"predict": "(29, OpenMP, #pragma omp target teams distribute simd safelen (foobool(argc)), safelen (true), safelen (-5)); (46, OpenMP, #pragma omp target teams distribute simd safelen(safelen(tmain<int, char, -1, -2>(argc, argv)); (-1, OpenMP, #pragma omp target teams distribute simd safelen); (19, OpenMP, #pragma omp target teams distribute simd safelen (foobool(1) > 0 ? 1 : 2)); (9, OpenMP, #pragma omp target teams distribute simd safelen (4); (0, OpenMP, #pragma omp target teams distribute simd safelen (); (27, OpenMP, #pragma omp target teams distribute simd safelen (S1)); (10, OpenMP, #pragma omp target teams distribute simd safelen (2+2))); (1, OpenMP, #pragma omp target teams distribute simd safelen ())", "label": "(29, OpenMP, #pragma omp target teams distribute simd safelen (foobool(argc)), safelen (true), safelen (-5)); (46, OpenMP, #pragma omp target teams distribute simd safelen(safelen(tmain<int, char, -1, -2>(argc, argv)); (-1, OpenMP, #pragma omp target teams distribute simd safelen); (19, OpenMP, #pragma omp target teams distribute simd safelen (foobool(1) > 0 ? 1 : 2)); (9, OpenMP, #pragma omp target teams distribute simd safelen (4); (0, OpenMP, #pragma omp target teams distribute simd safelen (); (27, OpenMP, #pragma omp target teams distribute simd safelen (S1)); (10, OpenMP, #pragma omp target teams distribute simd safelen (2+2))); (1, OpenMP, #pragma omp target teams distribute simd safelen ())"}
{"predict": "(30, MPI, MPI_Bcast( work, diag_len[p], MPI_FLOAT, pkk, grid->comm );); (27, MPI, MPI_Bcast( work, lwork, MPI_FLOAT, pkk, grid->comm );)", "label": "(28, MPI, MPI_Bcast( work, lwork, MPI_FLOAT, pkk, grid->comm );); (29, MPI, MPI_Bcast( work, diag_len[p], MPI_FLOAT, pkk, grid->comm );)"}
{"predict": "(5, CUDA, cudaMalloc(&curandStates, sizeof(curandState_t) * batchSize);); (47, CUDA, cudaFree(curandStates);)", "label": "(48, CUDA, cudaFree(curandStates);); (4, CUDA, cudaMalloc(&curandStates, sizeof(curandState_t) * 2 * batchSize);)"}
{"predict": "(24, MPI, MPI_Error_string(ierr, str, &slen);); (15, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(29, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_ARE_FATAL);); (23, MPI, MPI_Error_string(ierr, str, &slen);); (14, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(40, TBB, tbb::flow::make_edge(src, gamma);); (40, TBB, tbb::flow::make_edge(gamma, tint);); (40, TBB, tbb::flow::make_edge(tint, write);); (0, TBB, tbb::flow::graph g;)", "label": "(40, TBB, tbb::flow::make_edge(src, gamma);); (40, TBB, tbb::flow::make_edge(gamma, tint);); (40, TBB, tbb::flow::make_edge(tint, write);); (0, TBB, tbb::flow::graph g;)"}
{"predict": "(3, CUDA, __shared__ unsigned sidxs[nTPB];)", "label": "(3, CUDA, __shared__ unsigned sidxs[nTPB];)"}
{"predict": "(5, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&me);); (1, MPI, MPI_Init(&argc, &argv);); (37, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD,&np);); (14, MPI, MPI_Allreduce(&mysizeint, &totalsize, 1, MPI_INT64_T, MPI_SUM, MPI_COMM_WORLD);); (22, MPI, MPI_Allgather(&mysizeint, 1, MPI_INT, counts.data(), 1, MPI_INT, MPI_COMM_WORLD);); (29, MPI, MPI_Gatherv(in.c_str(), (int)in.size(), MPI_CHAR, out.data(), counts.data(), displs.data(), MPI_CHAR, 0, MPI_COMM_WORLD);)", "label": "(19, MPI, MPI_Reduce(&mysize, &totalsize, 1, MPI_INT64_T, MPI_SUM, 0, MPI_COMM_WORLD);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&me);); (0, MPI, MPI_Init(&argc, &argv);); (23, MPI, MPI_Gather(&mysizeint, 1, MPI_INT, &(counts[0]), 1, MPI_INT, 0, MPI_COMM_WORLD);); (37, MPI, MPI_Finalize();); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD,&np);); (29, MPI, MPI_Gatherv(in.c_str(), mysizeint, MPI_CHAR, &(out[0]), &(counts[0]), &(displs[0]), MPI_CHAR, 0, MPI_COMM_WORLD);)"}
{"predict": "(22, OpenMP, #pragma omp teams distribute parallel for order(concurrent)); (0, OpenMP, #pragma omp target); (8, OpenMP, #pragma omp teams distribute parallel for order(); (11, OpenMP, #pragma omp teams distribute parallel for order(none); (18, OpenMP, #pragma omp teams distribute parallel for order(concurrent); (-2, OpenMP, #pragma omp teams distribute parallel for order); (6, OpenMP, #pragma omp teams distribute parallel for order(concurrent))", "label": "(18, OpenMP, #pragma omp teams distribute parallel for order(concurrent); (1, OpenMP, #pragma omp teams distribute parallel for order); (20, OpenMP, #pragma omp teams distribute parallel for order(concurrent)); (-2, OpenMP, #pragma omp target); (10, OpenMP, #pragma omp teams distribute parallel for order(none); (5, OpenMP, #pragma omp teams distribute parallel for order()"}
{"predict": "(56, CUDA, flamegpu::CUDASimulation cudaSimulation(model, argc, argv);)", "label": "(56, CUDA, flamegpu::CUDASimulation  cudaSimulation(model, argc, argv);)"}
{"predict": "(24, TBB, typedef tbb::concurrent_hash_map<MyKey,MyData2> YourTable2;); (36, TBB, typedef tbb::concurrent_hash_map<YourKey,YourData2> YourTable1;); (1, TBB, typedef tbb::concurrent_hash_map<MyKey,MyData> MyTable;); (23, TBB, typedef tbb::concurrent_hash_map<YourKey,YourData> YourTable;); (34, TBB, typedef tbb::concurrent_hash_map<MyKey,MyData2> MyTable2;)", "label": "(36, TBB, typedef tbb::concurrent_hash_map<MyKey,MyData2,YourHashCompare> YourTable2;); (34, TBB, typedef tbb::concurrent_hash_map<MyKey,MyData2,YourHashCompare,MyAllocator> YourTable1;)"}
{"predict": "(12, CUDA, CHECK(cudaFree(m_output_src_device));); (12, CUDA, CHECK(cudaFree(m_output_objects_device));); (5, CUDA, CHECK(cudaFree(m_input_resize_device));); (11, CUDA, CHECK(cudaFree(m_output_boxes_device));); (11, CUDA, CHECK(cudaFree(m_output_scores_device));); (6, CUDA, CHECK(cudaFree(m_input_src_device));); (10, CUDA, CHECK(cudaFree(m_output_classes_device));); (5, CUDA, CHECK(cudaFree(m_input_rgb_device));); (5, CUDA, CHECK(cudaFree(m_input_hwc_device));); (-2, CUDA, CHECK(cudaFree(m_input_norm_device));)", "label": "(8, CUDA, CHECK(cudaFree(m_output_boxes_device));); (9, CUDA, CHECK(cudaFree(m_output_classes_device));); (2, CUDA, CHECK(cudaFree(m_input_resize_device));); (0, CUDA, CHECK(cudaFree(m_input_src_device));); (3, CUDA, CHECK(cudaFree(m_output_num_device));); (0, CUDA, CHECK(cudaFree(m_input_rgb_device));); (3, CUDA, CHECK(cudaFree(m_output_scores_device));)"}
{"predict": "(9, MPI, MPI_Comm_rank(comm_world, &rank_world);)", "label": "(9, MPI, MPI_Comm_rank(comm_world, &rank_world);)"}
{"predict": "(40, MPI, MPI_Request_free(&reqs[i]);)", "label": "(39, MPI, MPI_Request_free(&reqs[i]);)"}
{"predict": "(15, CUDA, cudaMemcpy(d_A, h_A, DSIZE*sizeof(float), cudaMemcpyHostToDevice);); (22, CUDA, cudaMemcpy(h_C, d_C, DSIZE*sizeof(float), cudaMemcpyDeviceToHost);); (8, CUDA, cudaMalloc(&d_A, DSIZE*sizeof(float));); (20, CUDA, cudaCheckErrors(\"kernel execution failure or cudaMemcpy H2D failure\");); (14, CUDA, cudaMemcpy(d_B, h_B, DSIZE*sizeof(float), cudaMemcpyHostToDevice);); (5, CUDA, cudaMalloc(&d_B, DSIZE*sizeof(float));); (2, CUDA, cudaMalloc(&d_C, DSIZE*sizeof(float));)", "label": "(15, CUDA, cudaMemcpy(d_A, h_A, DSIZE*sizeof(float), cudaMemcpyHostToDevice);); (20, CUDA, cudaCheckErrors(\"kernel launch failure\");); (8, CUDA, cudaMalloc(&d_A, DSIZE*sizeof(float));)"}
{"predict": "(35, MPI, MPI_Finalize();); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &comm_size);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (8, MPI, vtkh::SetMPICommHandle(MPI_Comm_c2f(MPI_COMM_WORLD));)", "label": "(35, MPI, MPI_Finalize();); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &comm_size);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (8, MPI, vtkh::SetMPICommHandle(MPI_Comm_c2f(MPI_COMM_WORLD));)"}
{"predict": "(19, CUDA, result = cudaMalloc((void**)&devPtrA, M*N*sizeof(*a));); (30, CUDA, cudaFree(devPtrA);)", "label": "(19, CUDA, result = cudaMalloc((void**)&devPtrA, M*N*sizeof(*a));); (30, CUDA, cudaFree(devPtrA);)"}
{"predict": "(6, CUDA, __shared__ uint s_permutations[160];)", "label": "(6, CUDA, __shared__ uint s_permutations[160];)"}
{"predict": "(4, CUDA, __shared__ uchar4 s_img[256];)", "label": "(4, CUDA, __shared__ uint4 s_img[TPB];)"}
{"predict": "(14, MPI, MPI_Group_incl(worldGroup, groupSize, ranks.data(), &group);); (8, MPI, MPI_Comm_group(comm, &group);); (5, MPI, MPI_Comm_group(MPI_COMM_WORLD, &worldGroup);); (12, MPI, MPI_Group_free(&worldGroup);); (10, MPI, MPI_Group_size(group, &groupSize);); (12, MPI, MPI_Group_free(&group);)", "label": "(10, MPI, MPICHECK(MPI_Group_size(group, &groupSize));); (6, MPI, MPICHECK(MPI_Comm_group(comm, &group));); (13, MPI, MPICHECK(MPI_Group_translate_ranks(group, groupSize, ranks.data(), worldGroup, worldRanks.data()));); (13, MPI, MPICHECK(MPI_Group_free(&group));); (2, MPI, MPICHECK(MPI_Comm_group(MPI_COMM_WORLD, &worldGroup));); (12, MPI, MPICHECK(MPI_Group_free(&worldGroup));)"}
{"predict": "(28, MPI, MPI_Recv(NULL, 0, MPI_CHAR, 0, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)", "label": "(43, MPI, printf(\" MPI library version      : %s\\n\", mpi_version());)"}
{"predict": "(17, CUDA, checkCudaErrors(cudaFree(workload->d_data));); (17, CUDA, checkCudaErrors(cudaFreeHost(workload->h_data));); (3, CUDA, checkCudaErrors(cudaSetDevice(workload->cudaDeviceID));); (14, CUDA, checkCudaErrors(cudaStreamDestroy(workload->stream));)", "label": "(15, CUDA, checkCudaErrors(cudaFree(workload->d_data));); (15, CUDA, checkCudaErrors(cudaFreeHost(workload->h_data));); (3, CUDA, checkCudaErrors(cudaSetDevice(workload->cudaDeviceID));); (14, CUDA, checkCudaErrors(cudaStreamDestroy(workload->stream));)"}
{"predict": "(9, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)", "label": "(9, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(11, CUDA, int *d_num_out = thrust::raw_pointer_cast(num_out.data());); (4, CUDA, thrust::device_vector<int> data(num_items);); (39, CUDA, h_num_out = *d_num_out;); (29, CUDA, thrust::fill(flags.begin(), flags.end(), RandomValue(2) == 0);); (4, CUDA, thrust::device_vector<int> flags(num_items);); (17, CUDA, d_tmp_storage = thrust::raw_pointer_cast(tmp_storage.data());); (23, CUDA, thrust::device_vector<char> tmp_storage(tmp_storage_size);); (-2, CUDA, thrust::device_vector<int> num_out(1);); (25, CUDA, data = thrust::shuffle(data.begin(), data.end(), g);)", "label": "(10, CUDA, int *d_num_out = thrust::raw_pointer_cast(num_out.data());); (10, CUDA, int *d_data = thrust::raw_pointer_cast(data.data());); (24, CUDA, d_tmp_storage = thrust::raw_pointer_cast(tmp_storage.data());); (32, CUDA, thrust::fill(flags.begin(), flags.begin() + num_selected, true);); (3, CUDA, thrust::device_vector<bool> flags(num_items);); (20, CUDA, thrust::device_vector<char> tmp_storage(tmp_storage_size);); (0, CUDA, thrust::device_vector<int> data(num_items);); (29, CUDA, thrust::fill(flags.begin() + num_selected, flags.end(), false);); (26, CUDA, thrust::sequence(data.begin(), data.end());); (28, CUDA, thrust::shuffle(flags.begin(), flags.end(), g);); (-5, CUDA, thrust::device_vector<int> num_out(1);); (36, CUDA, cudaMemcpy(&h_num_out, d_num_out, sizeof(int), cudaMemcpyDeviceToHost);); (0, CUDA, bool *d_flags = thrust::raw_pointer_cast(flags.data());)"}
{"predict": "(104, CUDA, checkCudaErrors(cudaFreeHost(h_result));); (101, CUDA, checkCudaErrors(cudaFree(d_partial_sums));); (101, CUDA, checkCudaErrors(cudaFreeHost(h_partial_sums));); (69, CUDA, checkCudaErrors(cudaEventRecord(stop, 0));); (60, CUDA, checkCudaErrors(cudaEventCreate(&start));); (6, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, cuda_device));); (31, CUDA, checkCudaErrors(cudaMallocHost(&h_result, n_elements*sizeof(int)));); (59, CUDA, checkCudaErrors(cudaEventCreate(&stop));); (61, CUDA, checkCudaErrors(cudaEventRecord(start, 0));); (64, CUDA, checkCudaErrors(cudaEventElapsedTime(&inc, start, stop));); (26, CUDA, checkCudaErrors(cudaMallocHost(&h_partial_sums, partial_sz));); (89, CUDA, checkCudaErrors(cudaFreeHost(h_data));); (86, CUDA, checkCudaErrors(cudaFree(d_data));); (28, CUDA, checkCudaErrors(cudaMalloc(&d_partial_sums, partial_sz));); (53, CUDA, checkCudaErrors(cudaMemcpy(d_data, h_data, sz, cudaMemcpyHostToDevice));); (23, CUDA, checkCudaErrors(cudaMallocHost(&h_data, sz));); (84, CUDA, checkCudaErrors(cudaFreeHost(h_partial_sums));); (-8, CUDA, cuda_device = findCudaDevice(argc, (const char **)argv);); (64, CUDA, checkCudaErrors(cudaMemcpy(h_data, d_data, sz, cudaMemcpyDeviceToHost));); (13, CUDA, checkCudaErrors(cudaMalloc(&d_data, sz));); (65, CUDA, checkCudaErrors(cudaEventSynchronize(stop));); (77, CUDA, checkCudaErrors(cudaFreeHost(h_concurrent_scans));); (20, CUDA, checkCudaErrors(cudaMemset(d_partial_sums, 0, partial_sz));); (72, CUDA, checkCudaErrors(cudaFreeHost(h_shmem_test));); (20, CUDA, checkCudaErrors(cudaMemset(d_data, 0, sz));)", "label": "(76, CUDA, checkCudaErrors(cudaEventRecord(stop, 0));); (15, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, cuda_device));); (94, CUDA, checkCudaErrors(cudaFree(d_partial_sums));); (57, CUDA, checkCudaErrors(cudaEventCreate(&start));); (57, CUDA, checkCudaErrors(cudaEventCreate(&stop));); (73, CUDA, checkCudaErrors(cudaEventElapsedTime(&inc, start, stop));); (66, CUDA, checkCudaErrors(cudaEventRecord(start, 0));); (70, CUDA, checkCudaErrors(cudaEventSynchronize(stop));); (86, CUDA, checkCudaErrors(cudaFreeHost(h_partial_sums));); (84, CUDA, checkCudaErrors(cudaFreeHost(h_result));); (55, CUDA, checkCudaErrors(cudaMalloc((void **)&d_data, sz));); (84, CUDA, checkCudaErrors(cudaFree(d_data));); (18, CUDA, checkCudaErrors(cudaMallocHost((void **)&h_result, sizeof(int)*n_elements));); (79, CUDA, checkCudaErrors(cudaFreeHost(h_data));); (56, CUDA, checkCudaErrors(cudaMemcpy(d_data, h_data, sz, cudaMemcpyHostToDevice));); (-4, CUDA, cuda_device = findCudaDevice(argc, (const char **)argv);); (65, CUDA, checkCudaErrors(cudaMemcpy(h_result, d_data, sz, cudaMemcpyDeviceToHost));); (50, CUDA, checkCudaErrors(cudaMemset(d_partial_sums, 0, partial_sz));); (11, CUDA, checkCudaErrors(cudaMallocHost((void **)&h_data, sizeof(int)*n_elements));); (-5, CUDA, checkCudaErrors(cudaGetDevice(&cuda_device));); (46, CUDA, checkCudaErrors(cudaMalloc((void **)&d_partial_sums, partial_sz));); (48, CUDA, checkCudaErrors(cudaMallocHost((void **)&h_partial_sums, partial_sz));)"}
{"predict": "(18, OpenMP, #pragma omp target update from(t) device (argc); (2, OpenMP, #pragma omp target update from()); (19, OpenMP, #pragma omp target update from(t) device (argc), device (argc+1)); (14, OpenMP, #pragma omp target update from(t) device (argc + argc + t)); (4, OpenMP, #pragma omp target update from(t) device (argc); (5, OpenMP, #pragma omp target update from(t) device (argc > 0 ? argv[1] : argv[2])); (6, OpenMP, #pragma omp target update from(t) device (device_num: argc)); (-3, OpenMP, #pragma omp target update to(t) device (); (9, OpenMP, #pragma omp target update from(t) device (-2)); (-6, OpenMP, #pragma omp target update from(t) device); (-2, OpenMP, #pragma omp target update from(t) device ()); (0, OpenMP, #pragma omp target update from(t) device (argc))); (-7, OpenMP, #pragma omp target update from(t) device ()", "label": "(1, OpenMP, #pragma omp target teams defaultmap); (2, OpenMP, #pragma omp target teams defaultmap(); (9, OpenMP, #pragma omp target teams defaultmap(tofrom)); (12, OpenMP, #pragma omp target teams defaultmap(tofrom,); (3, OpenMP, #pragma omp target teams defaultmap(tofrom); (4, OpenMP, #pragma omp target teams defaultmap (tofrom:); (13, OpenMP, #pragma omp target teams defaultmap(tofrom, scalar); (-2, OpenMP, #pragma omp target teams defaultmap()); (5, OpenMP, #pragma omp target teams defaultmap(tofrom scalar)); (8, OpenMP, #pragma omp target teams defaultmap (scalar:)"}
{"predict": "(51, CUDA, CHECK(cudaFree(d_y));); (29, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice));); (50, CUDA, CHECK(cudaFree(d_z));); (38, CUDA, CHECK(cudaMemcpy(h_z, d_z, M, cudaMemcpyDeviceToHost));); (21, CUDA, CHECK(cudaMalloc((void**)&d_x, M));); (21, CUDA, CHECK(cudaMalloc((void**)&d_y, M));); (45, CUDA, CHECK(cudaFree(d_x));); (20, CUDA, CHECK(cudaMalloc((void**)&d_z, M));); (22, CUDA, CHECK(cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice));)", "label": "(52, CUDA, CHECK(cudaFree(d_y));); (30, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice));); (51, CUDA, CHECK(cudaFree(d_z));); (39, CUDA, CHECK(cudaMemcpy(h_z, d_z, M, cudaMemcpyDeviceToHost));); (22, CUDA, CHECK(cudaMalloc((void**)&d_x, M));); (22, CUDA, CHECK(cudaMalloc((void**)&d_y, M));); (45, CUDA, CHECK(cudaFree(d_x));); (21, CUDA, CHECK(cudaMalloc((void**)&d_z, M));); (30, CUDA, CHECK(cudaGetLastError());); (30, CUDA, CHECK(cudaDeviceSynchronize());); (22, CUDA, CHECK(cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice));)"}
{"predict": "(32, MPI, MPI_Send(lnzval, len2, MPI_DOUBLE, 0, jb, grid3d->zscp.comm);); (36, MPI, MPI_Recv(lnzval, len2, MPI_DOUBLE, layer, jb, grid3d->zscp.comm, &status);)", "label": "(32, MPI, MPI_Send(lnzval, len2, MPI_DOUBLE, 0, jb, grid3d->zscp.comm);); (36, MPI, MPI_Recv(lnzval, len2, MPI_DOUBLE, layer, jb, grid3d->zscp.comm, &status);)"}
{"predict": "(2, OpenMP, #pragma omp target device); (26, OpenMP, #pragma omp target device (ancestor)); (10, OpenMP, #pragma omp target device (k)); (13, OpenMP, #pragma omp target device (argc), device (argc+1)); (15, OpenMP, #pragma omp target device (S1)); (16, OpenMP, #pragma omp target device (-2)); (19, OpenMP, #pragma omp target device (argv[1]=2)); (4, OpenMP, #pragma omp target device (device_num : argc > 0 ? argv[1] : argv[2])); (5, OpenMP, #pragma omp target device (argc: argc + argc)); (10, OpenMP, #pragma omp target device (argc), device (k)); (14, OpenMP, #pragma omp target device (L1)); (-6, OpenMP, #pragma omp target device (argc); (-3, OpenMP, #pragma omp target device (argc:); (-7, OpenMP, #pragma omp target device (); (-2, OpenMP, #pragma omp target device (argc: argc)); (5, OpenMP, #pragma omp target device (3.14)); (-9, OpenMP, #pragma omp target device ())", "label": "(24, OpenMP, #pragma omp target parallel device (3.14)); (9, OpenMP, #pragma omp target parallel device (argc))); (20, OpenMP, #pragma omp target parallel device (-10u)); (5, OpenMP, #pragma omp target parallel device (argc); (12, OpenMP, #pragma omp target parallel device (k), device (argc+1)); (-3, OpenMP, #pragma omp target parallel device); (6, OpenMP, #pragma omp target parallel device (argc > 0 ? argv[1] : argv[2])); (11, OpenMP, #pragma omp target parallel device (S1)); (12, OpenMP, #pragma omp target parallel device (-2)); (-3, OpenMP, #pragma omp target parallel device ()); (4, OpenMP, #pragma omp target parallel device (argc + argc)); (-7, OpenMP, #pragma omp target parallel device ()"}
{"predict": "(63, OpenMP, double stop = omp_get_wtime();); (70, CUDA, CUDA_RT_CALL(cudaFree(a));); (18, CUDA, CUDA_RT_CALL(cudaDeviceSynchronize());); (10, CUDA, CUDA_RT_CALL(cudaMemset(a, 0, nx * ny * sizeof(float)));); (6, CUDA, CUDA_RT_CALL(cudaMalloc(&a, nx * ny * sizeof(float)));); (63, CUDA, CUDA_RT_CALL(cudaFree(l2_norm_d));); (13, CUDA, CUDA_RT_CALL(cudaGetLastError());); (32, CUDA, CUDA_RT_CALL(cudaMemset(l2_norm_d, 0, sizeof(float)));); (36, CUDA, CUDA_RT_CALL(cudaMemcpy(l2_norm_h, l2_norm_d, sizeof(float), cudaMemcpyDeviceToHost));); (5, CUDA, CUDA_RT_CALL(cudaMemset(a_new, 0, nx * ny * sizeof(float)));); (14, CUDA, CUDA_RT_CALL(cudaMallocHost(&l2_norm_h, sizeof(float)));); (54, CUDA, CUDA_RT_CALL(cudaMemcpy(a_ref_h, a, nx * ny * sizeof(float), cudaMemcpyDeviceToHost));); (58, CUDA, CUDA_RT_CALL(cudaFree(a_new));); (-2, CUDA, CUDA_RT_CALL(cudaMalloc(&a_new, nx * ny * sizeof(float)));); (22, OpenMP, double start = omp_get_wtime();); (8, CUDA, CUDA_RT_CALL(cudaMalloc(&l2_norm_d, sizeof(float)));); (51, CUDA, CUDA_RT_CALL(cudaFreeHost(l2_norm_h));)", "label": "(63, OpenMP, double stop = omp_get_wtime();); (70, CUDA, CUDA_RT_CALL(cudaFree(a));); (18, CUDA, CUDA_RT_CALL(cudaDeviceSynchronize());); (10, CUDA, CUDA_RT_CALL(cudaMemset(a, 0, nx * ny * sizeof(float)));); (6, CUDA, CUDA_RT_CALL(cudaMalloc(&a, nx * ny * sizeof(float)));); (63, CUDA, CUDA_RT_CALL(cudaFree(l2_norm_d));); (13, CUDA, CUDA_RT_CALL(cudaGetLastError());); (32, CUDA, CUDA_RT_CALL(cudaMemset(l2_norm_d, 0, sizeof(float)));); (36, CUDA, CUDA_RT_CALL(cudaMemcpy(l2_norm_h, l2_norm_d, sizeof(float), cudaMemcpyDeviceToHost));); (5, CUDA, CUDA_RT_CALL(cudaMemset(a_new, 0, nx * ny * sizeof(float)));); (14, CUDA, CUDA_RT_CALL(cudaMallocHost(&l2_norm_h, sizeof(float)));); (54, CUDA, CUDA_RT_CALL(cudaMemcpy(a_ref_h, a, nx * ny * sizeof(float), cudaMemcpyDeviceToHost));); (58, CUDA, CUDA_RT_CALL(cudaFree(a_new));); (-2, CUDA, CUDA_RT_CALL(cudaMalloc(&a_new, nx * ny * sizeof(float)));); (22, OpenMP, double start = omp_get_wtime();); (8, CUDA, CUDA_RT_CALL(cudaMalloc(&l2_norm_d, sizeof(float)));); (51, CUDA, CUDA_RT_CALL(cudaFreeHost(l2_norm_h));)"}
{"predict": "(20, OpenMP, #pragma omp distribute parallel for proc_bind(master); (37, OpenMP, #pragma omp distribute parallel for proc_bind(spread)); (25, OpenMP, #pragma omp distribute parallel for proc_bind(close), proc_bind(spread)); (-1, OpenMP, #pragma omp target); (30, OpenMP, #pragma omp parallel proc_bind(close)); (14, OpenMP, #pragma omp distribute parallel for proc_bind()); (1, OpenMP, #pragma omp distribute parallel for proc_bind); (36, OpenMP, #pragma omp distribute parallel for proc_bind(master)); (-4, OpenMP, #pragma omp teams)", "label": "(20, OpenMP, #pragma omp distribute parallel for proc_bind(master); (4, OpenMP, #pragma omp distribute parallel for proc_bind); (13, OpenMP, #pragma omp distribute parallel for proc_bind()); (7, OpenMP, #pragma omp distribute parallel for proc_bind(); (26, OpenMP, #pragma omp distribute parallel for proc_bind(x)); (38, OpenMP, #pragma omp distribute parallel for proc_bind(spread)); (19, OpenMP, #pragma omp distribute parallel for proc_bind(close), proc_bind(spread)); (-4, OpenMP, #pragma omp target); (32, OpenMP, #pragma omp parallel proc_bind(close)); (27, OpenMP, #pragma omp distribute parallel for proc_bind(master)); (-6, OpenMP, #pragma omp teams)"}
{"predict": "(15, OpenMP, #pragma omp target parallel unknown()); (10, OpenMP, #pragma omp target parallel }); (2, OpenMP, #pragma omp target parallel (); (54, OpenMP, #pragma omp target parallel copyin(pvt)); (1, OpenMP, #pragma omp target parallel [); (55, OpenMP, #pragma omp target parallel default(none)); (-3, OpenMP, #pragma omp target parallel); (4, OpenMP, #pragma omp target parallel ))", "label": "(69, OpenMP, #pragma omp cancellation point parallel); (66, OpenMP, #pragma omp cancellation point taskgroup); (60, OpenMP, #pragma omp cancel for); (57, OpenMP, #pragma omp cancel); (64, OpenMP, #pragma omp cancel parallel); (50, OpenMP, #pragma omp target parallel copyin(pvt)); (-5, OpenMP, #pragma omp target parallel); (-4, OpenMP, #pragma omp target parallel (); (53, OpenMP, #pragma omp cancellation point); (57, OpenMP, #pragma omp cancel taskgroup); (-1, OpenMP, #pragma omp target parallel )); (0, OpenMP, #pragma omp target parallel }); (4, OpenMP, #pragma omp target parallel unknown()); (50, OpenMP, #pragma omp cancellation point for); (-9, OpenMP, #pragma omp target parallel [); (49, OpenMP, #pragma omp cancel sections); (49, OpenMP, #pragma omp cancellation point sections); (-10, OpenMP, #pragma omp target parallel ])"}
{"predict": "(9, CUDA, cudaGetDeviceProperties( &prop, 0 );); (5, CUDA, cudaSetDeviceFlags( cudaDeviceMapHost );); (3, CUDA, cudaSetDevice( 0 );)", "label": "(9, CUDA, cudaGetDeviceProperties( &prop, 0 );); (5, CUDA, cudaSetDeviceFlags( cudaDeviceMapHost );); (3, CUDA, cudaSetDevice( 0 );)"}
{"predict": "(1, MPI, MPI_Init(&argc, &argv);); (61, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &P);)", "label": "(1, MPI, MPI_Init(&argc, &argv);); (61, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &P);)"}
{"predict": "(23, TBB, tbb::flow::make_edge( a1, a3 );); (20, TBB, tbb::flow::make_edge( s, a0 );); (21, TBB, tbb::flow::make_edge( a2, a1 );); (17, TBB, tbb::flow::make_edge( c, a2 );); (-2, TBB, tbb::flow::graph g;); (36, TBB, tbb::flow::remove_edge( a2, a1 );); (16, TBB, tbb::flow::make_edge( a0, c );); (19, TBB, tbb::flow::queue_node<int> q(g);); (-4, TBB, tbb::flow::broadcast_node<int> s(g);); (-2, TBB, tbb::flow::control_node<int> c(g);)", "label": "(61, TBB, tbb::flow::make_edge( a1, tbb::flow::input_port<1>(a3) );); (58, TBB, tbb::flow::remove_edge( a2, s2 );); (60, TBB, tbb::flow::make_edge( a3, s2 );); (0, TBB, tbb::flow::function_node<int,int> c(g, tbb::flow::unlimited, cube_body());); (19, TBB, CHECK_MESSAGE( (&std::get<1>(a3.input_ports()) == &tbb::flow::input_port<1>(a3.j)), \"input_port1 of j not bound to input port 1 in composite_node a3\");); (25, TBB, tbb::flow::make_edge( c, tbb::flow::input_port<1>(a0) );); (21, TBB, tbb::flow::queue_node<int> q(g);); (25, TBB, tbb::flow::make_edge( c, tbb::flow::input_port<0>(a1) );); (27, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(a0), tbb::flow::input_port<0>(a2) );); (20, TBB, tbb::flow::make_edge( s, tbb::flow::input_port<0>(a0) );); (16, TBB, tbb::flow::function_node<int,int> s2(g, tbb::flow::unlimited, square_body());); (22, TBB, tbb::flow::make_edge( c, tbb::flow::input_port<1>(a1) );); (24, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(a1), tbb::flow::input_port<1>(a2) );); (9, TBB, CHECK_MESSAGE( (&std::get<0>(a3.input_ports()) == &tbb::flow::input_port<0>(a3.j)), \"input_port 0 of j not bound to input port 0 in composite_node a3\");); (3, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<0>(a2) == &tbb::flow::input_port<0>(a2.j)), \"input_port 0 of j not bound to input port 0 in composite_node a2\");); (24, TBB, tbb::flow::make_edge( s2, q );); (-15, TBB, tbb::flow::graph g;); (-15, TBB, tbb::flow::function_node<int,int> s(g, tbb::flow::unlimited, square_body());); (0, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<1>(a2) == &tbb::flow::input_port<1>(a2.j)), \"input_port 1 of j not bound to input port 1 in composite_node a2\");); (-12, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<0>(a0) == &tbb::flow::input_port<0>(a0.j)), \"input_port 0 of j not bound to input port 0 in composite_node a0\");); (18, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(a2), s2 );); (-12, TBB, CHECK_MESSAGE( (&tbb::flow::output_port<0>(a0) == &a0.f), \"f not bound to output port 0 in composite_node a0\");); (-18, TBB, tbb::flow::function_node<int,int> p(g, tbb::flow::unlimited, passthru_body());); (35, TBB, tbb::flow::remove_edge(s2, q);); (-11, TBB, CHECK_MESSAGE( (&std::get<1>(a0.input_ports()) == &tbb::flow::input_port<1>(a0.j)), \"input_port1 of j not bound to input port 1 in composite_node a1\");); (-17, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<1>(a0) == &tbb::flow::input_port<1>(a0.j)), \"input_port 1 of j not bound to input port 1 in composite_node a0\");); (-14, TBB, CHECK_MESSAGE( (&std::get<0>(a0.input_ports()) == &tbb::flow::input_port<0>(a0.j)), \"input_port 0 of j not bound to input port 0 in composite_node a1\");); (-8, TBB, CHECK_MESSAGE( (&tbb::flow::output_port<0>(a2) == &a2.f), \"f not bound to output port 0 in composite_node a2\");); (32, TBB, tbb::flow::make_edge( a0, a3 );)"}
{"predict": "(30, CUDA, CUDASimulation cudaSimulation(model);)", "label": "(29, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(39, CUDA, checkCudaErrors(cudaEventRecord(start));); (42, CUDA, checkCudaErrors(cudaEventRecord(end));); (42, CUDA, checkCudaErrors(cudaEventSynchronize(end));); (52, CUDA, checkCudaErrors(cudaEventElapsedTime(&elapsedTime, start, end));); (9, CUDA, checkCudaErrors(cudaEventCreate(&end));); (38, CUDA, checkCudaErrors(cudaEventSynchronize(start));); (5, CUDA, checkCudaErrors(cudaEventCreate(&start));)", "label": "(13, CUDA, checkCudaErrors(cudaEventCreate(&start));); (13, CUDA, checkCudaErrors(cudaEventCreate(&end));); (41, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (38, CUDA, checkCudaErrors(cudaEventRecord(end));); (35, CUDA, checkCudaErrors(cudaEventRecord(start));); (50, CUDA, checkCudaErrors(cudaEventElapsedTime(&elapsedTime, start, end));)"}
{"predict": "(26, OpenMP, #pragma omp target); (40, OpenMP, #pragma omp teams distribute thread_limit(1024) num_teams(64,16)); (27, OpenMP, #pragma omp distribute); (46, OpenMP, #pragma omp parallel for)", "label": "(65, OpenMP, #pragma omp target update to(u1_real, u1_imag))"}
{"predict": "(24, MPI, MPI_Bcast(&tmp_val, 1, MPI_INT, 0, fd->comm);)", "label": "(25, MPI, MPI_Bcast(&tmp_val, 1, MPI_INT, 0, fd->comm);)"}
{"predict": "(56, OpenMP, #pragma omp simd)", "label": "(56, OpenMP, #pragma omp simd)"}
{"predict": "(20, TBB, tbb::cache_aligned_allocator<ThreadData> alloc;); (21, TBB, thread_sinks[i] = alloc(data);)", "label": "(11, OpenMP, #pragma omp parallel for num_threads(N_THREADS))"}
{"predict": "(31, OpenCL, retVal = clEnqueueNDRangeKernel(pCmdQ, multiDeviceKernel.get(), 1, nullptr, globalWorkSize, nullptr, 0, nullptr, nullptr);); (20, OpenCL, retVal = clEnqueueMemcpyINTEL(pCmdQ, true, compressedDeviceMemAlloc1, bufferData, bufferSize, 0, nullptr, nullptr);); (14, OpenCL, retVal = clEnqueueMemcpyINTEL(pCmdQ, true, compressedDeviceMemAlloc2, bufferData, bufferSize, 0, nullptr, nullptr);); (37, OpenCL, retVal = clEnqueueMemcpyINTEL(pCmdQ, true, uncompressibleHostMemAlloc, compressedDeviceMemAlloc2, bufferSize, 0, nullptr, nullptr);)", "label": "(31, OpenCL, retVal = clEnqueueNDRangeKernel(pCmdQ, multiDeviceKernel.get(), 1, nullptr, globalWorkSize, nullptr, 0, nullptr, nullptr);); (20, OpenCL, retVal = clEnqueueMemcpyINTEL(pCmdQ, true, compressedDeviceMemAlloc1, bufferData, bufferSize, 0, nullptr, nullptr);); (14, OpenCL, retVal = clEnqueueMemcpyINTEL(pCmdQ, true, compressedDeviceMemAlloc2, bufferData, bufferSize, 0, nullptr, nullptr);); (37, OpenCL, retVal = clEnqueueMemcpyINTEL(pCmdQ, true, uncompressibleHostMemAlloc, compressedDeviceMemAlloc2, bufferSize, 0, nullptr, nullptr);)"}
{"predict": "(28, TBB, tbb::task_arena* result_arena = allocate_and_construct_with_dummy_parameters<tbb::task_arena>(dummy_max_concurrency, dummy_reserved_for_masters);); (13, TBB, tbb::task_arena* result_arena = allocate_and_construct_with_dummy_parameters<tbb::task_arena>(dummy_max_concurrency);)", "label": "(30, TBB, result_arena = new tbb::task_arena(tmp);); (18, TBB, result_arena = new tbb::task_arena( arena_args... );); (12, TBB, tbb::task_arena* result_arena = nullptr;); (26, TBB, tbb::task_arena tmp(dummy_max_concurrency, dummy_reserved_for_masters);); (19, TBB, result_arena = new tbb::task_arena;)"}
{"predict": "(33, MPI, rc = MPI_Type_get_extent(obj->DTP_datatype, &lb, &extent);); (31, MPI, rc = MPI_Type_get_true_extent(obj->DTP_datatype, &true_lb, &true_extent);)", "label": "(32, MPI, rc = MPI_Type_get_true_extent(obj->DTP_datatype, &true_lb, &true_extent);); (22, MPI, rc = MPI_Type_commit(&obj->DTP_datatype);); (32, MPI, rc = MPI_Type_get_extent(obj->DTP_datatype, &lb, &extent);)"}
{"predict": "(67, OpenCL, clReleaseMemObject(mem[i]);)", "label": "(67, OpenCL, clReleaseMemObject(mem[i]);)"}
{"predict": "(21, CUDA, cudaMallocHost((void**)&src_host, sizeof(float) * size);); (27, CUDA, cudaDeviceSynchronize();); (31, CUDA, cudaFreeHost(src_host);); (19, CUDA, cudaMallocHost((void**)&tar_host, sizeof(float) * size);); (40, CUDA, cudaFreeHost(tar_host);)", "label": "(19, CUDA, cudaMallocHost(&tar_host, size * sizeof(float));); (17, CUDA, cudaMallocHost(&src_host, size * sizeof(float));)"}
{"predict": "(3, OpenMP, #pragma omp parallel for collapse(2))", "label": "(3, OpenMP, #pragma omp parallel num_threads(2)); (12, OpenMP, #pragma omp ordered depend(source)); (7, OpenMP, #pragma omp ordered depend(sink : i - 1, j) depend(sink : i, j - 1)); (1, OpenMP, #pragma omp for ordered(2))"}
{"predict": "(3, OpenMP, #pragma omp parallel); (3, OpenMP, #pragma omp taskloop)", "label": "(3, OpenMP, #pragma omp parallel); (3, OpenMP, #pragma omp taskloop)"}
{"predict": "(22, OpenMP, omp_target_free(d_a, omp_get_default_device());); (13, OpenMP, d_a = (int *)omp_target_alloc(DataSize, omp_get_default_device());); (14, OpenMP, omp_target_associate_ptr(p_a, d_a, DataSize, 0, omp_get_default_device());); (17, OpenMP, omp_target_disassociate_ptr(p_a, omp_get_default_device());)", "label": "(16, OpenMP, omp_target_associate_ptr(p_a, d_a, DataSize, 0, omp_get_default_device());); (21, OpenMP, omp_target_free(d_a, omp_get_default_device());); (12, OpenMP, d_a = (int *)omp_target_alloc(DataSize, omp_get_default_device());); (17, OpenMP, omp_target_disassociate_ptr(p_a, omp_get_default_device());)"}
{"predict": "(50, OpenMP, EXPECT_TRUE(check_test_image(output_file, t_apcomp_baseline_dir()));); (42, OpenMP, EXPECT_EQ(40, in_partials[0].size());); (8, MPI, MPI_Comm_rank(comm, &par_rank);); (12, MPI, MPI_Barrier(comm);); (7, MPI, MPI_Comm_size(comm, &par_size);)", "label": "(50, OpenMP, EXPECT_TRUE(check_test_image(output_file, t_apcomp_baseline_dir()));); (9, MPI, MPI_Comm_rank(comm, &par_rank);); (10, MPI, apcomp::mpi_comm(MPI_Comm_c2f(comm));); (8, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(40, MPI, MPI_Waitall(1, &request, &status);); (25, MPI, MPI_Waitall(1, &request, &status);); (20, MPI, MPI_Grequest_start(query_fn, free_fn, cancel_fn, NULL, &request);); (15, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (33, MPI, MPI_Testall(1, &request, &outcount, indices, &status);)", "label": "(27, MPI, MPI_Wait(&request, &status);); (40, MPI, MPI_Waitall(1, &request, &status);); (16, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (31, MPI, MPI_Waitsome(1, &request, &outcount, indices, &status);); (19, MPI, MPI_Grequest_start(query_fn, free_fn, cancel_fn, NULL, &request);)"}
{"predict": "(3, CUDA, cudaGetDevice(&device_id);); (3, CUDA, cudaGetDeviceProperties(&device_prop, device_id);)", "label": "(3, CUDA, cudaGetDevice(&device_id);); (3, CUDA, cudaGetDeviceProperties(&device_prop, device_id);)"}
{"predict": "(18, MPI, timers[t] = MPI_Wtime();); (4, MPI, MPI_Type_size(sdt, &outsize);); (21, MPI, timers[t] = (MPI_Wtime() - timers[t]) / cycles;); (10, MPI, MPI_Pack(sbuf, scount, sdt, packed_buf, outsize, &position, MPI_COMM_WORLD);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myself);)", "label": "(18, MPI, timers[t] = MPI_Wtime();); (22, MPI, timers[t] = (MPI_Wtime() - timers[t]) / cycles;); (11, MPI, MPI_Pack(sbuf, scount, sdt, packed_buf, outsize, &position, MPI_COMM_WORLD);); (2, MPI, MPI_Type_size(sdt, &outsize);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myself);)"}
{"predict": "(26, OpenMP, #pragma omp target parallel device(global + a) depend(in); (102, OpenMP, #pragma omp target parallel if (0) firstprivate(global) depend(out); (84, OpenMP, #pragma omp target parallel device(global + a) nowait depend(inout)", "label": "(86, OpenMP, #pragma omp target teams device(global + a) nowait depend(inout); (25, OpenMP, #pragma omp target teams device(global + a) depend(in); (100, OpenMP, #pragma omp target teams if (0) firstprivate(global) depend(out)"}
{"predict": "(48, CUDA, __shared__ TB smemB[cosize_v<BSmemLayout>];); (46, CUDA, __shared__ TA smemA[cosize_v<ASmemLayout>];)", "label": "(48, CUDA, __shared__ TB smemB[cosize_v<BSmemLayout>];); (46, CUDA, __shared__ TA smemA[cosize_v<ASmemLayout>];)"}
{"predict": "(82, CUDA, checkCudaErrors(cudaFree(src));); (12, CUDA, checkCudaErrors(cudaMallocPitch((void **)(&dst), &DeviceStride, Size.width * sizeof(float), Size.height));); (36, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (78, CUDA, checkCudaErrors(cudaFree(dst));); (8, CUDA, checkCudaErrors(cudaMallocPitch((void **)(&src), &DeviceStride, Size.width * sizeof(float), Size.height));); (40, CUDA, getLastCudaError(\"Kernel execution failed\");)", "label": "(13, CUDA, checkCudaErrors(cudaMallocPitch((void **)&src, &DeviceStride, Size.width * sizeof(float), Size.height));); (79, CUDA, checkCudaErrors(cudaFree(src));); (12, CUDA, checkCudaErrors(cudaMallocPitch((void **)&dst, &DeviceStride, Size.width * sizeof(float), Size.height));); (35, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (40, CUDA, getLastCudaError(\"Kernel execution failed\");); (74, CUDA, checkCudaErrors(cudaFree(dst));)"}
{"predict": "(35, CUDA, __shared__ _VOLATILE_ float bSmem[16][128+4];); (33, CUDA, __shared__ _VOLATILE_ float aSmem[16][128+4];)", "label": "(35, CUDA, __shared__ _VOLATILE_ float bSmem[16][128+4];); (33, CUDA, __shared__ _VOLATILE_ float aSmem[16][128+4];)"}
{"predict": "(27, CUDA, cudaMalloc((void**)&deviceC, size);); (53, CUDA, cudaFree(deviceB);); (38, CUDA, cudaMemcpy(hostC, deviceC, size, cudaMemcpyDeviceToHost);); (52, CUDA, cudaFree(deviceC);); (24, CUDA, cudaMalloc((void**)&deviceB, size);); (26, CUDA, cudaMemcpy(deviceA, hostA, size, cudaMemcpyHostToDevice);); (26, CUDA, cudaMemcpy(deviceB, hostB, size, cudaMemcpyHostToDevice);); (47, CUDA, cudaFree(deviceA);); (19, CUDA, cudaMalloc((void**)&deviceA, size);)", "label": "(28, CUDA, cudaMalloc((void**)&deviceC, size);); (55, CUDA, cudaFree(deviceB);); (40, CUDA, cudaMemcpy(hostC, deviceC, size, cudaMemcpyDeviceToHost);); (54, CUDA, cudaFree(deviceC);); (51, CUDA, cudaFree(deviceA);); (22, CUDA, cudaMalloc((void**)&deviceB, size);); (25, CUDA, cudaMemcpy(deviceA, hostA, size, cudaMemcpyHostToDevice);); (25, CUDA, cudaMemcpy(deviceB, hostB, size, cudaMemcpyHostToDevice);); (18, CUDA, cudaMalloc((void**)&deviceA, size);)"}
{"predict": "(77, MPI, MPI_Comm_size(fd->comm, &nprocs);); (77, MPI, MPI_Comm_rank(fd->comm, &myrank);)", "label": "(77, MPI, MPI_Comm_size(fd->comm, &nprocs);); (77, MPI, MPI_Comm_rank(fd->comm, &myrank);)"}
{"predict": "(13, OpenMP, omp_set_schedule(sched = omp_sched_from_str(argv[2]), ref_chunk);); (15, OpenMP, ref_sched = omp_sched_from_str(argv[2]);); (25, OpenMP, omp_get_schedule(&sched, &chunk);)", "label": "(11, OpenMP, ref_sched = str2omp_sched(argv[2]);); (13, OpenMP, omp_get_schedule(&sched, &chunk);)"}
{"predict": "(10, CUDA, RX_CHECK(ctx, cudaMalloc(&ctx->d_scratchpads, ctx->d_scratchpads_size));); (12, CUDA, RX_CHECK(ctx, cudaMemset(ctx->d_scratchpads, 0xFF, ctx->d_scratchpads_size));); (9, CUDA, RX_CHECK(ctx, cudaMemcpy(ctx->d_dataset, dataset, dataset_size, cudaMemcpyHostToDevice));); (4, CUDA, RX_CHECK(ctx, cudaMallocHost(&ctx->rx_dataset_host, dataset_size));); (10, CUDA, RX_CHECK(ctx, cudaMalloc(&ctx->d_result_count, sizeof(uint32_t)));); (11, CUDA, RX_CHECK(ctx, cudaMemset(ctx->d_result_count, 0, sizeof(uint32_t)));); (5, CUDA, RX_CHECK(ctx, cudaMalloc(&ctx->d_dataset, dataset_size));)", "label": "(9, CUDA, CUDA_CHECK(ctx->device_id, cudaMalloc(&ctx->d_rx_dataset, dataset_size));); (15, CUDA, CUDA_CHECK(ctx->device_id, cudaMalloc(&ctx->d_rx_vm_states, batch_size * 2560));); (4, CUDA, CUDA_CHECK(ctx->device_id, cudaHostGetDevicePointer(&ctx->d_rx_dataset, const_cast<void *>(dataset), 0));); (7, CUDA, CUDA_CHECK(ctx->device_id, cudaMemcpy(ctx->d_rx_dataset, dataset, dataset_size, cudaMemcpyHostToDevice));); (11, CUDA, CUDA_CHECK(ctx->device_id, cudaMalloc(&ctx->d_rx_entropy, batch_size * (128 + 2560)));); (12, CUDA, CUDA_CHECK(ctx->device_id, cudaMalloc(&ctx->d_rx_rounding, batch_size * sizeof(uint32_t)));); (8, CUDA, CUDA_CHECK(ctx->device_id, cudaMalloc(&ctx->d_rx_hashes, batch_size * 64));); (6, CUDA, CUDA_CHECK(ctx->device_id, cudaMalloc(&ctx->d_long_state, ctx->d_scratchpads_size));)"}
{"predict": "(6, OpenMP, #pragma omp target map(tofrom : counter_N0)); (7, OpenMP, #pragma omp parallel for reduction(+ : counter_N0))", "label": "(8, OpenMP, #pragma omp taskloop shared(counter_N0)); (5, OpenMP, #pragma omp target data map(tofrom : counter_N0)); (8, OpenMP, #pragma omp target teams distribute parallel for map(tofrom : counter_N0) nowait); (9, OpenMP, #pragma omp atomic update)"}
{"predict": "(43, MPI, MPI_Send( recvbuf, knsupc * nrhs, MPI_FLOAT, receiver, k,  grid3d->zscp.comm);); (51, MPI, MPI_Recv( recvbuf, knsupc * nrhs, MPI_FLOAT, sender, k, grid3d->zscp.comm, &status );)", "label": "(52, MPI, MPI_Recv( recvbuf, knsupc * nrhs, MPI_FLOAT, sender, k, grid3d->zscp.comm, &status );); (42, MPI, MPI_Send( recvbuf, knsupc * nrhs, MPI_FLOAT, receiver, k,  grid3d->zscp.comm);)"}
{"predict": "(6, CUDA, cudaGridDependencySynchronize();); (85, CUDA, cudaTriggerProgrammaticLaunchCompletion();)", "label": "(27, CUDA, cudaGridDependencySynchronize();); (85, CUDA, cudaTriggerProgrammaticLaunchCompletion();)"}
{"predict": "(24, CUDA, __shared__ __align__(sizeof(double)) char loadSmemBuffer[kUpdateKVCacheKernelShmSize];)", "label": "(31, CUDA, __shared__ char loadSmemBuffer[kUpdateKVCacheKernelShmSize];)"}
{"predict": "(5, OpenMP, #pragma omp teams distribute); (19, OpenMP, #pragma omp target update from (u0_real, u0_imag, u1_real, u1_imag)); (3, OpenMP, #pragma omp target map (to: u0_real, u0_imag, u1_real, u1_imag, twiddle))", "label": "(6, OpenMP, #pragma omp teams distribute); (7, OpenMP, #pragma omp parallel for); (8, OpenMP, #pragma omp simd); (1, OpenMP, #pragma omp target map (alloc: u0_real,u0_imag,u1_real,u1_imag,twiddle))"}
{"predict": "(16, MPI, MPI_Abort(MPI_COMM_WORLD, MPIT_result);); (17, MPI, MPIT_result = MPI_T_pvar_session_create(session);); (11, MPI, MPIT_result = MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);)", "label": "(16, MPI, MPI_Abort(MPI_COMM_WORLD, MPIT_result);); (17, MPI, MPIT_result = MPI_T_pvar_session_create(session);); (11, MPI, MPIT_result = MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);)"}
{"predict": "(16, OpenMP, #pragma omp target teams distribute parallel for); (6, OpenMP, #pragma omp target enter data map(to:W[:M])); (14, OpenMP, #pragma omp target exit data map(from:W[:M])); (5, OpenMP, #pragma omp target map(tofrom:W[0:M]))", "label": "(7, OpenMP, #pragma omp target data map(tofrom: W[:M])); (12, OpenMP, #pragma omp target teams); (12, OpenMP, #pragma omp distribute parallel for)"}
{"predict": "(11, OpenMP, #pragma omp parallel); (12, OpenMP, #pragma omp sections private(i,sum0) lastprivate(i0)); (21, OpenMP, #pragma omp critical)", "label": "(13, OpenMP, #pragma omp section); (10, OpenMP, #pragma omp parallel); (21, OpenMP, #pragma omp critical)"}
{"predict": "(57, MPI, MPI_Finalize();); (10, MPI, MPI_Init(&argc,&argv);)", "label": "(57, MPI, MPI_Finalize();); (10, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(17, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (14, CUDA, cudaMalloc((void**)(&d_in1), in1_bytes);); (29, CUDA, assert(cudaMemcpyAsync(out.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);); (16, CUDA, cudaMemcpy(d_in1, in1.data(), in1_bytes, cudaMemcpyHostToDevice);); (40, CUDA, cudaFree(d_out);); (38, CUDA, cudaFree(d_in2);); (26, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (35, CUDA, cudaFree(d_in1);); (8, CUDA, cudaMalloc((void**)(&d_in2), in2_bytes);); (11, CUDA, cudaMemcpy(d_in2, in2.data(), in2_bytes, cudaMemcpyHostToDevice);)", "label": "(17, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (14, CUDA, cudaMalloc((void**)(&d_in1), in1_bytes);); (29, CUDA, assert(cudaMemcpyAsync(out.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);); (16, CUDA, cudaMemcpy(d_in1, in1.data(), in1_bytes, cudaMemcpyHostToDevice);); (40, CUDA, cudaFree(d_out);); (38, CUDA, cudaFree(d_in2);); (26, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (35, CUDA, cudaFree(d_in1);); (8, CUDA, cudaMalloc((void**)(&d_in2), in2_bytes);); (11, CUDA, cudaMemcpy(d_in2, in2.data(), in2_bytes, cudaMemcpyHostToDevice);)"}
{"predict": "(27, CUDA, cudaEventRecord(start);); (34, CUDA, cudaEventRecord(end);); (35, CUDA, cudaEventSynchronize(end);); (22, CUDA, cudaEventCreate(&start);); (22, CUDA, cudaEventCreate(&end);); (31, CUDA, cudaEventElapsedTime(&msec, start, end);)", "label": "(27, CUDA, cudaEventRecord(start);); (31, CUDA, cudaEventRecord(end);); (31, CUDA, cudaEventSynchronize(end);); (23, CUDA, cudaEventCreate(&end);); (21, CUDA, cudaEventCreate(&start);); (31, CUDA, cudaEventElapsedTime(&msec, start, end);)"}
{"predict": "(13, CUDA, cudaMalloc((void **)&d_arr, sizeof(int) * len);); (16, CUDA, cudaMemcpy(d_arr, arr, sizeof(int) * len, cudaMemcpyHostToDevice);); (53, CUDA, cudaFree(d_out);); (11, CUDA, cudaMalloc((void **)&d_out, sizeof(int) * len);); (23, CUDA, cudaMemcpy(out, d_out, sizeof(int) * len, cudaMemcpyDeviceToHost);); (49, CUDA, cudaFree(d_arr);)", "label": "(13, CUDA, cudaMalloc((void **)&d_arr, sizeof(int) * len);); (16, CUDA, cudaMemcpy(d_arr, arr, sizeof(int) * len, cudaMemcpyHostToDevice);); (53, CUDA, cudaFree(d_out);); (11, CUDA, cudaMalloc((void **)&d_out, sizeof(int) * len);); (23, CUDA, cudaMemcpy(out, d_out, sizeof(int) * len, cudaMemcpyDeviceToHost);); (49, CUDA, cudaFree(d_arr);)"}
{"predict": "(19, MPI, rc = MPI_Comm_size(MPI_COMM_WORLD, &size);); (9, MPI, rc = MPI_Comm_get_attr(MPI_COMM_WORLD, MPI_UNIVERSE_SIZE, &v, &flag);); (15, MPI, MPI_Comm_set_attr(MPI_COMM_WORLD, MPI_UNIVERSE_SIZE, (void *) (MPI_Aint) (size + 1));)", "label": "(19, MPI, rc = MPI_Comm_size(MPI_COMM_WORLD, &size);); (8, MPI, rc = MPI_Comm_get_attr(MPI_COMM_WORLD, MPI_UNIVERSE_SIZE, &v, &flag);)"}
{"predict": "(15, CUDA, CUSOLVER_CHECK(cudaMalloc(&d_work, workspaceInBytesOnDevice));); (33, CUDA, CUSOLVER_CHECK(cudaFree(d_work));)", "label": "(33, CUDA, CUDA_CHECK(cudaFree(d_work));); (14, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_work), workspaceInBytesOnDevice));)"}
{"predict": "(12, OpenMP, #pragma omp target enter data map(alloc); (4, OpenMP, #pragma omp target map(tofrom : warp_size)); (137, OpenMP, #pragma omp target exit data map(release)", "label": "(12, OpenMP, #pragma omp target enter data map(alloc); (4, OpenMP, #pragma omp target map(tofrom : warp_size)); (135, OpenMP, #pragma omp target exit data map(release)"}
{"predict": "(18, OpenMP, #pragma omp parallel for simd if(parallel :); (5, OpenMP, #pragma omp parallel for simd if ()); (20, OpenMP, #pragma omp parallel for simd if(parallel : argc) if (simd :argc)); (14, OpenMP, #pragma omp parallel for simd if(if(t true)); (16, OpenMP, #pragma omp parallel for simd if(parallel : argc); (7, OpenMP, #pragma omp parallel for simd if (foobool(argc)), if (true)); (1, OpenMP, #pragma omp parallel for simd if (argc); (2, OpenMP, #pragma omp parallel for simd if (argc > 0 ? argv[1] : argv[2])); (-4, OpenMP, #pragma omp parallel for simd if); (7, OpenMP, #pragma omp parallel for simd if (1 0)); (2, OpenMP, #pragma omp parallel for simd if (S1)); (-4, OpenMP, #pragma omp parallel for simd if (); (8, OpenMP, #pragma omp parallel for simd if(parallel); (-6, OpenMP, #pragma omp parallel for simd if (argc))); (-2, OpenMP, #pragma omp parallel for simd if (argc argc)); (8, OpenMP, #pragma omp parallel for simd if(parallel : argc) if (argc)); (10, OpenMP, #pragma omp parallel for simd if(parallel : argc) if (for:argc))", "label": "(6, OpenMP, #pragma omp target parallel for device ()); (23, OpenMP, #pragma omp target parallel for device (3.14)); (18, OpenMP, #pragma omp target parallel for device (-2)); (13, OpenMP, #pragma omp target parallel for device (argc), device (argc+1)); (14, OpenMP, #pragma omp target parallel for device (S1)); (5, OpenMP, #pragma omp target parallel for device (argc))); (-4, OpenMP, #pragma omp target parallel for device); (1, OpenMP, #pragma omp target parallel for device (argc); (14, OpenMP, #pragma omp target parallel for device (-10u)); (5, OpenMP, #pragma omp target parallel for device (z + argc)); (-6, OpenMP, #pragma omp target parallel for device (); (1, OpenMP, #pragma omp target parallel for device (argc > 0 ? argv[1] : argv[2]))"}
{"predict": "(16, MPI, MPI_Comm_dup(comm, &prg->comm);); (45, MPI, MPI_Ibcast(prg->values, count, MPI_UINT64_T, 0, prg->comm, &prg->bcast_req);); (46, MPI, MPI_Cancel(&prg->bcast_req);); (41, MPI, MPI_Comm_rank(prg->comm, &rank);); (38, MPI, prg->time_start = MPI_Wtime();)", "label": "(47, MPI, MPI_Ibcast(&(prg->keep_going), 1, MPI_INT, 0, prg->comm, &(prg->bcast_req));); (15, MPI, MPI_Comm_dup(comm, &prg->comm);); (37, MPI, prg->time_start = MPI_Wtime();); (42, MPI, MPI_Comm_rank(prg->comm, &rank);)"}
{"predict": "(28, MPI, MPI_DATATYPE_ALLOC(uval_new, lenv);); (25, MPI, MPI_INT_ALLOC(usub_new, lens);); (59, MPI, MPI_INT_ALLOC(lsub_new, len1);); (60, MPI, MPI_DATATYPE_ALLOC(lnzval_new, len2);)", "label": "(28, MPI, MPI_DATATYPE_ALLOC(uval_new, lenv);); (25, MPI, MPI_INT_ALLOC(usub_new, lens);); (59, MPI, MPI_INT_ALLOC(lsub_new, len1);); (60, MPI, MPI_DATATYPE_ALLOC(lnzval_new, len2);)"}
{"predict": "(13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (13, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (36, MPI, MPI_Comm_free(&cart);)", "label": "(13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (37, MPI, MPI_Comm_free(&cart);); (12, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);)"}
{"predict": "(17, MPI, MPI_Recv(buff, count, type, 0, tag, comm, MPI_STATUS_IGNORE);); (4, MPI, MPI_Comm_rank(comm, &rank);)", "label": "(20, MPI, MPI_Recv(buff, count, type, 0, tag, comm, MPI_STATUS_IGNORE);); (4, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(18, MPI, MPI_Isend(&a, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, &request);); (21, MPI, MPI_Wait(&request, MPI_STATUS_IGNORE);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (2, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (16, MPI, MPI_Recv(&a, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (19, MPI, MPI_Request_free(&request);); (5, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(24, MPI, MPI_Request_free(&request);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (15, MPI, MPI_Psend_init(&a, 1, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, MPI_INFO_NULL, &request);); (16, MPI, MPI_Precv_init(&a, 1, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_INFO_NULL, &request);); (21, MPI, MPI_Barrier(MPI_COMM_WORLD);); (6, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(8, OpenMP, double after0=secs_elapsed();); (9, OpenMP, #pragma omp target data map(to: m_gate[:nCells]) map(tofrom: Vm[:nCells])); (9, OpenMP, #pragma omp target teams distribute parallel for thread_limit(128) collapse(2) schedule(static,1))", "label": "(16, OpenMP, #pragma omp target teams distribute parallel for thread_limit(128)); (8, OpenMP, #pragma omp target enter data map(to: Vm[:nCells])); (6, OpenMP, #pragma omp target enter data map(to: m_gate[:nCells]))"}
{"predict": "(27, MPI, MPI_Type_vector(5, 1, stride, MPI_INT, &vecs[i]);); (30, MPI, MPI_Type_free(&vecs[i]);); (14, MPI, MPI_Type_commit(&mystruct);); (26, MPI, MPI_Type_create_struct(3, blockcount, displs, vecs, &mystruct);); (15, MPI, MPI_Type_vector(5, 1, stride, MPI_INT, &vecs[i]);)", "label": "(19, MPI, MPI_Type_commit(&mystruct);); (27, MPI, MPI_Type_create_hvector(0, 1, stride, MPI_INT, &vecs[i]);); (19, MPI, MPI_Type_free(&mystruct);); (8, MPI, MPI_Type_commit(&vecs[i]);); (14, MPI, MPI_Type_create_struct(3, blockcount, displs, vecs, &mystruct);); (5, MPI, MPI_Type_create_hvector(i, 1, stride, MPI_INT, &vecs[i]);); (17, MPI, MPI_Type_free(&vecs[i]);)"}
{"predict": "(14, CUDA, checkCudaErrors(cudaMalloc(&info, sizeof(int)));); (50, CUDA, checkCudaErrors(cudaFree(buffer));); (50, CUDA, checkCudaErrors(cudaFree(A));); (12, CUDA, checkCudaErrors(cudaMalloc(&buffer, sizeof(double) * bufferSize));); (12, CUDA, checkCudaErrors(cudaMalloc(&A, sizeof(double) * lda * n));); (34, CUDA, checkCudaErrors(cusolverDnXpotrs(handle, uplo, n, A, lda, b, n, info));); (41, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (44, CUDA, checkCudaErrors(cudaFree(info));)", "label": "(14, CUDA, checkCudaErrors(cudaMalloc(&info, sizeof(int)));); (14, CUDA, checkCudaErrors(cudaMalloc(&buffer, sizeof(double) * bufferSize));); (14, CUDA, checkCudaErrors(cudaMalloc(&A, sizeof(double) * lda * n));); (36, CUDA, checkCudaErrors(cusolverDnDpotrs(handle, uplo, n, 1, A, lda, x, n, info));); (37, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (43, CUDA, checkCudaErrors(cudaFree(info));); (45, CUDA, checkCudaErrors(cudaFree(buffer));); (14, CUDA, checkCudaErrors(cudaMemset(info, 0, sizeof(int)));); (46, CUDA, checkCudaErrors(cudaFree(A));)"}
{"predict": "(17, OpenMP, #pragma omp for simd collapse (foobool(argc)), collapse (true), collapse (-5)); (31, OpenMP, #pragma omp for simd collapse (2)); (3, OpenMP, #pragma omp for simd collapse ()); (0, OpenMP, #pragma omp for simd collapse (); (26, OpenMP, #pragma omp for simd collapse(collapse(tmain<int, char, -1, -2>(argc, argv)); (14, OpenMP, #pragma omp for simd collapse (S1)); (6, OpenMP, #pragma omp for simd collapse (foobool(1) > 0 ? 1 : 2)); (0, OpenMP, #pragma omp for simd collapse (4); (1, OpenMP, #pragma omp for simd collapse (2+2))); (-7, OpenMP, #pragma omp for simd collapse)", "label": "(12, OpenMP, #pragma omp for simd collapse (foobool(1) > 0 ? 1 : 2)); (8, OpenMP, #pragma omp for simd collapse (2+2))); (-1, OpenMP, #pragma omp for simd collapse); (16, OpenMP, #pragma omp for simd collapse (S1)); (-1, OpenMP, #pragma omp for simd collapse (); (0, OpenMP, #pragma omp for simd collapse ()); (26, OpenMP, #pragma omp for simd collapse (2)); (10, OpenMP, #pragma omp for simd collapse (foobool(argc)), collapse (true), collapse (-5)); (-1, OpenMP, #pragma omp for simd collapse (4); (21, OpenMP, #pragma omp for simd collapse(collapse(tmain<int, char, -1, -2>(argc, argv))"}
{"predict": "(34, MPI, MPI_Bcast(local_cache, 1, MPI_INT, 0, fd->comm);)", "label": "(35, MPI, MPI_Bcast(&tmp_val, 1, MPI_INT, 0, fd->comm);)"}
{"predict": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(11, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(11, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(60, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (78, CUDA, CUDA_CHECK(cudaDeviceReset());); (75, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (40, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (41, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_y), sizeof(data_type) * y.size()));); (39, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_x), sizeof(data_type) * x.size()));); (67, CUDA, CUDA_CHECK(cudaFree(d_y));); (65, CUDA, CUDA_CHECK(cudaFree(d_x));); (63, CUDA, CUDA_CHECK(cudaFree(d_A));); (30, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)", "label": "(60, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (78, CUDA, CUDA_CHECK(cudaDeviceReset());); (75, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (40, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (41, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_y), sizeof(data_type) * y.size()));); (39, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_x), sizeof(data_type) * x.size()));); (67, CUDA, CUDA_CHECK(cudaFree(d_y));); (65, CUDA, CUDA_CHECK(cudaFree(d_x));); (63, CUDA, CUDA_CHECK(cudaFree(d_A));); (30, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(27, TBB, tbb::flow::make_edge(source2, tbb::flow::input_port<1>(merge));); (38, TBB, tbb::flow::make_edge(merge, output);); (24, TBB, tbb::flow::make_edge(source1, tbb::flow::input_port<0>(merge));); (-1, TBB, tbb::flow::graph g;)", "label": "(37, TBB, tbb::flow::make_edge(source2, tbb::flow::input_port<1>(merge));); (35, TBB, tbb::flow::make_edge(source1, tbb::flow::input_port<0>(merge));); (36, TBB, tbb::flow::make_edge(merge, output);); (-2, TBB, tbb::flow::graph g;)"}
{"predict": "(37, CUDA, CHECK(cudaMalloc(&d_matrix_org, M));); (37, CUDA, CHECK(cudaMalloc(&d_matrix_res, M));); (37, CUDA, CHECK(cudaMemcpy(d_matrix_org, h_matrix_org, M, cudaMemcpyDefault));); (41, CUDA, CHECK(cudaEventCreate(&stop));); (49, CUDA, CHECK(cudaEventRecord(stop));); (46, CUDA, CHECK(cudaMemcpy(h_matrix_res, d_matrix_res, M, cudaMemcpyDefault));); (40, CUDA, cudaEventQuery(start);); (48, CUDA, CHECK(cudaEventSynchronize(stop));); (48, CUDA, CHECK(cudaEventElapsedTime(&curr_time, start, stop));); (86, CUDA, CHECK(cudaFree(d_matrix_org));); (86, CUDA, CHECK(cudaFree(d_matrix_res));); (34, CUDA, CHECK(cudaEventRecord(start));); (31, CUDA, CHECK(cudaEventCreate(&start));); (2, CUDA, CHECK(cudaMemcpyFromSymbol(&SIZE, c_TILE_DIM, sizeof(int)));)", "label": "(37, CUDA, CHECK(cudaMalloc(&d_matrix_org, M));); (37, CUDA, CHECK(cudaMalloc(&d_matrix_res, M));); (37, CUDA, CHECK(cudaMemcpy(d_matrix_org, h_matrix_org, M, cudaMemcpyDefault));); (42, CUDA, CHECK(cudaEventCreate(&stop));); (50, CUDA, CHECK(cudaEventRecord(stop));); (46, CUDA, CHECK(cudaMemcpy(h_matrix_res, d_matrix_res, M, cudaMemcpyDefault));); (41, CUDA, cudaEventQuery(start);); (48, CUDA, CHECK(cudaEventSynchronize(stop));); (48, CUDA, CHECK(cudaEventElapsedTime(&curr_time, start, stop));); (86, CUDA, CHECK(cudaFree(d_matrix_org));); (86, CUDA, CHECK(cudaFree(d_matrix_res));); (35, CUDA, CHECK(cudaEventRecord(start));); (32, CUDA, CHECK(cudaEventCreate(&start));); (3, CUDA, CHECK(cudaMemcpyFromSymbol(&SIZE, c_TILE_DIM, sizeof(int)));)"}
{"predict": "(15, OpenMP, #pragma omp target teams distribute parallel for); (9, OpenMP, #pragma omp target data map(from: a))", "label": "(10, OpenMP, #pragma omp target map(tofrom: a[0:100])); (12, OpenMP, #pragma omp simd)"}
{"predict": "(18, OpenMP, #pragma omp distribute parallel for simd private(x);); (11, OpenMP, #pragma omp distribute parallel for simd;); (23, OpenMP, #pragma omp distribute parallel for simd, private(x);); (2, OpenMP, #pragma omp distribute parallel for simd;); (-1, OpenMP, #pragma omp target); (0, OpenMP, #pragma omp teams)", "label": "(12, OpenMP, #pragma omp distribute parallel for simd firstprivate(x);); (5, OpenMP, #pragma omp distribute parallel for simd;); (24, OpenMP, #pragma omp distribute parallel for simd, private(x);); (0, OpenMP, #pragma omp target); (15, OpenMP, #pragma omp distribute parallel for simd private(x);); (-1, OpenMP, #pragma omp teams)"}
{"predict": "(28, OpenMP, #pragma omp teams reduction(+:sum)); (25, OpenMP, #pragma omp target map(tofrom: b, sum, t) map(alloc: a)); (28, OpenMP, #pragma omp distribute parallel for reduction(+:sum)); (30, OpenMP, t[i] = omp_get_team_num();)", "label": "(28, OpenMP, #pragma omp teams reduction(+:sum)); (25, OpenMP, #pragma omp target map(tofrom: b, sum, t) map(alloc: a)); (28, OpenMP, #pragma omp distribute parallel for reduction(+:sum)); (30, OpenMP, t[i] = omp_get_team_num();)"}
{"predict": "(21, CUDA, extern __shared__ char shared_memory[];)", "label": "(38, CUDA, extern __shared__ char shared_memory[];)"}
{"predict": "(36, MPI, MPI_Mrecv(tensor->data(), count, mpiType, &msg, &status);); (26, MPI, MPI_Mrecv(dims, 2, MPI_INT64_T, &msg, &status);); (14, MPI, MPI_Mrecv(&count, 1, MPI_INT, &msg, &status);); (28, MPI, auto const mpiType = mpiTypeFromTllmDtype(logitsDtype);)", "label": "(28, MPI, MPICHECK(MPI_Mrecv(&dims, count, MPI_INT64_T, &msg, &status));); (36, MPI, MPICHECK(MPI_Get_count(&status, MPI_UINT8_T, &count));); (40, MPI, MPICHECK(MPI_Mrecv(tensor->data(), count, MPI_UINT8_T, &msg, &status));); (16, MPI, MPICHECK(MPI_Mrecv(&mpiId, count, MPI_UINT64_T, &msg, &status));); (20, MPI, MPICHECK(MPI_Get_count(&status, MPI_INT64_T, &count));); (11, MPI, MPICHECK(MPI_Get_count(&status, MPI_UINT64_T, &count));)"}
{"predict": "(11, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(11, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(24, OpenCL, error |= clSetKernelArg(kernel, 0, sizeof(cl_mem), &dst);); (14, OpenCL, clKernelWrapper kernel = clCreateKernel(prog, \"test_usersemantic\", &error);)", "label": "(15, OpenCL, clKernelWrapper kernel = clCreateKernel(prog, \"usersemantic_test\", &error);); (22, OpenCL, error |= clSetKernelArg(kernel, 0, sizeof(dst), &dst);)"}
{"predict": "(26, MPI, MPI_Isend(buf, cnt, MPI_INT, thread_num, tag, MPI_COMM_WORLD, &r[thread_num - 1]);); (37, MPI, t = MPI_Wtime() - t;); (21, MPI, t = MPI_Wtime();); (10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (39, MPI, MPI_Waitall(wsize - 1, r, MPI_STATUSES_IGNORE);); (24, MPI, MPI_Wait(&r[thread_num - 1], MPI_STATUS_IGNORE);)", "label": "(26, MPI, MPI_Isend(buf, cnt, MPI_INT, thread_num, tag, MPI_COMM_WORLD, &r[thread_num - 1]);); (36, MPI, t = MPI_Wtime() - t;); (21, MPI, t = MPI_Wtime();); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (29, MPI, MPI_Waitall(wsize - 1, r, MPI_STATUSES_IGNORE);); (23, MPI, MPI_Wait(&r[thread_num - 1], MPI_STATUS_IGNORE);)"}
{"predict": "(12, OpenMP, #pragma omp parallel master taskloop lastprivate(s) firstprivate(s)); (6, OpenMP, #pragma omp parallel master taskloop firstprivate(s)); (13, OpenMP, #pragma omp parallel master taskloop private(s), firstprivate(s)); (7, OpenMP, #pragma omp parallel master taskloop firstprivate(s) private(s)); (-2, OpenMP, #pragma omp parallel); (0, OpenMP, #pragma omp parallel master taskloop firstprivate())", "label": "(8, OpenMP, #pragma omp target parallel for simd order(); (13, OpenMP, #pragma omp target parallel for simd order(concurrent); (15, OpenMP, #pragma omp target parallel for simd order(concurrent)); (2, OpenMP, #pragma omp target parallel for simd order); (-2, OpenMP, #pragma omp target parallel for simd lastprivate(s) firstprivate(s)); (6, OpenMP, #pragma omp target parallel for simd order(none)"}
{"predict": "(68, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (2, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (60, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(inData->basePtr(), inVec.data(), inBufSize, cudaMemcpyHostToDevice));); (72, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(testVec.data(), outData->basePtr(), outBufSize, cudaMemcpyDeviceToHost));); (66, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamDestroy(stream));)", "label": "(68, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (2, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (59, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(inData->basePtr(), inVec.data(), inBufSize, cudaMemcpyHostToDevice));); (72, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(testVec.data(), outData->basePtr(), outBufSize, cudaMemcpyDeviceToHost));); (65, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamDestroy(stream));)"}
{"predict": "(26, CUDA, CUDA_CHECK(cudaMalloc((void **)&dev_max, sizeof(FP) * SEQLEN * DIM));); (45, CUDA, cudaFree(dev_max);); (28, CUDA, CUDA_CHECK(cudaMemcpy(dev_denom, host_denom, sizeof(FP) * SEQLEN * DIM, cudaMemcpyHostToDevice));); (24, CUDA, CUDA_CHECK(cudaMalloc((void **)&dev_denom, sizeof(FP) * SEQLEN * DIM));); (24, CUDA, CUDA_CHECK(cudaMalloc((void **)&QK, sizeof(FP) * SEQLEN * SEQLEN));); (24, CUDA, CUDA_CHECK(cudaMemcpy(dev_max, host_max, sizeof(FP) * SEQLEN * DIM, cudaMemcpyHostToDevice));); (39, CUDA, cudaFree(QK);); (39, CUDA, cudaFree(dev_denom);)", "label": "(26, CUDA, CUDA_CHECK(cudaMalloc((void **)&dev_max, sizeof(FP) * SEQLEN * DIM));); (45, CUDA, cudaFree(dev_max);); (28, CUDA, CUDA_CHECK(cudaMemcpy(dev_denom, host_denom, sizeof(FP) * SEQLEN * DIM, cudaMemcpyHostToDevice));); (24, CUDA, CUDA_CHECK(cudaMalloc((void **)&dev_denom, sizeof(FP) * SEQLEN * DIM));); (24, CUDA, CUDA_CHECK(cudaMalloc((void **)&QK, sizeof(FP) * SEQLEN * SEQLEN));); (24, CUDA, CUDA_CHECK(cudaMemcpy(dev_max, host_max, sizeof(FP) * SEQLEN * DIM, cudaMemcpyHostToDevice));); (39, CUDA, cudaFree(QK);); (40, CUDA, cudaFree(dev_denom);)"}
{"predict": "(13, OpenMP, thread_num = omp_get_thread_num();); (17, OpenMP, #pragma omp for schedule(runtime) reduction(+:total) linear(idx:1)); (11, OpenMP, #pragma omp master); (26, OpenMP, #pragma omp parallel private(thread_num) shared(data, std::cerr, total, idx)); (18, OpenMP, thread_num = omp_get_thread_num();)", "label": "(19, OpenMP, #pragma omp for schedule(runtime) reduction(+:total) linear(idx:1)); (13, OpenMP, thread_num = omp_get_thread_num();); (15, OpenMP, #pragma omp master); (7, OpenMP, #pragma omp parallel shared(data, std::cerr, total, idx) default(none))"}
{"predict": "(33, MPI, MPI_Type_free(&types[i]);)", "label": "(62, MPI, MPI_Type_commit(newtype);)"}
{"predict": "(60, CUDA, cudaFree(gpu_inputs);); (58, CUDA, cudaFree(gpu_outputs);); (15, CUDA, cudaMalloc((void **)&gpu_outputs, sizeof(typename types<params>::output_t)*count);); (12, CUDA, cudaMalloc((void **)&gpu_inputs,  sizeof(typename types<params>::input_t)*count);); (16, CUDA, cudaMemcpy(gpu_inputs, cpu_inputs, sizeof(typename types<params>::input_t)*count, cudaMemcpyHostToDevice);); (55, CUDA, cudaFree(cpu_inputs);); (15, CUDA, cudaMemcpy(compare, gpu_outputs, sizeof(typename types<params>::output_t)*count, cudaMemcpyDeviceToHost);); (52, CUDA, cudaFree(cpu_outputs);)", "label": "(21, CUDA, $GPU(cudaMemcpy(compare, gpu_outputs, sizeof(typename types<params>::output_t)*count, cudaMemcpyDeviceToHost));); (15, CUDA, $GPU(cudaMalloc((void **)&gpu_outputs, sizeof(typename types<params>::output_t)*count));); (15, CUDA, $GPU(cudaMemset(gpu_outputs, 0, sizeof(typename types<params>::output_t)*count));); (58, CUDA, $GPU(cudaFree(gpu_outputs));)"}
{"predict": "(47, OpenMP, #pragma omp target teams firstprivate(j)); (38, OpenMP, #pragma omp target teams firstprivate(h, B::x)); (31, OpenMP, #pragma omp target teams firstprivate(S2::S2s)); (8, OpenMP, #pragma omp target teams firstprivate()); (46, OpenMP, #pragma omp target teams firstprivate(m)); (7, OpenMP, #pragma omp target teams firstprivate(); (28, OpenMP, #pragma omp target teams firstprivate(e, g)); (39, OpenMP, #pragma omp target teams firstprivate(i)); (29, OpenMP, #pragma omp target teams firstprivate(ca, z)); (31, OpenMP, #pragma omp target teams firstprivate(da)); (15, OpenMP, #pragma omp target teams firstprivate(a, b, c, d, f)); (3, OpenMP, #pragma omp target teams firstprivate); (17, OpenMP, #pragma omp target teams firstprivate(ba)); (28, OpenMP, #pragma omp target teams firstprivate(S2::S2sc)); (-1, OpenMP, #pragma omp target teams firstprivate); (11, OpenMP, #pragma omp target teams firstprivate(2 * 2)); (31, OpenMP, #pragma omp target teams shared(i)); (2, OpenMP, #pragma omp target teams firstprivate(argc,); (19, OpenMP, #pragma omp target teams firstprivate(S1))", "label": "(24, OpenMP, #pragma omp target parallel firstprivate (a, b, c, d, f)); (43, OpenMP, #pragma omp target parallel shared(i)); (46, OpenMP, #pragma omp target parallel firstprivate(j)); (19, OpenMP, #pragma omp target parallel firstprivate (S1)); (6, OpenMP, #pragma omp target parallel firstprivate (); (11, OpenMP, #pragma omp target parallel firstprivate (argc,); (28, OpenMP, #pragma omp target parallel firstprivate(S2::S2s)); (35, OpenMP, #pragma omp target parallel private(i), firstprivate(i)); (12, OpenMP, #pragma omp target parallel firstprivate (argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (29, OpenMP, #pragma omp target parallel firstprivate(e, g)); (30, OpenMP, #pragma omp target parallel firstprivate(h, B::x)); (19, OpenMP, #pragma omp target parallel firstprivate(ca, z)); (16, OpenMP, #pragma omp target parallel firstprivate(ba) allocate(omp_thread_mem_alloc: ba)); (-1, OpenMP, #pragma omp target parallel firstprivate ()); (36, OpenMP, #pragma omp target parallel firstprivate(m)); (-1, OpenMP, #pragma omp target parallel firstprivate (argc); (-8, OpenMP, #pragma omp target parallel firstprivate); (29, OpenMP, #pragma omp target parallel firstprivate(i)); (18, OpenMP, #pragma omp target parallel firstprivate(S2::S2sc)); (7, OpenMP, #pragma omp target parallel firstprivate (argv[1])); (-2, OpenMP, #pragma omp target parallel firstprivate (argc > 0 ? argv[1] : argv[2])); (11, OpenMP, #pragma omp target parallel firstprivate(da))"}
{"predict": "(1, TBB, tbb::tick_count start = tbb::tick_count::now();); (8, TBB, now = tbb::tick_count::now();)", "label": "(8, TBB, start = tbb::tick_count::now();); (0, TBB, tbb::tick_count now;); (8, TBB, now = tbb::tick_count::now();)"}
{"predict": "(3, OpenMP, #pragma omp parallel); (4, OpenMP, int me = omp_get_thread_num();); (6, OpenMP, #pragma omp barrier)", "label": "(17, OpenMP, #pragma omp barrier); (2, OpenMP, #pragma omp parallel); (3, OpenMP, int me = omp_get_thread_num();)"}
{"predict": "(11, CUDA, __shared__ thrust::cuda_cub::core::uninitialized_array<T, SUM_SIZE> sum;)", "label": "(11, CUDA, __shared__ uninitialized_array<T, SUM_SIZE> sum;)"}
{"predict": "(7, CUDA, __shared__ typename BlockReduce::TempStorage temp_storage;)", "label": "(7, CUDA, __shared__ typename BlockReduce::TempStorage temp_storage;)"}
{"predict": "(2, TBB, using namespace tbb::internal;)", "label": "(2, TBB, using namespace tbb::internal;)"}
{"predict": "(2, OpenMP, #pragma omp taskyield allocate(argc)); (3, OpenMP, #pragma omp taskyield untied); (3, OpenMP, #pragma omp taskyield)", "label": "(2, OpenMP, #pragma omp taskyield allocate(argc)); (3, OpenMP, #pragma omp taskyield untied); (3, OpenMP, #pragma omp taskyield)"}
{"predict": "(13, CUDA, CUDA_CHECK(cudaMemcpy(ret_host, d_ptr, sizeof(double), cudaMemcpyDeviceToHost));); (2, CUDA, double *d_ptr = thrust::raw_pointer_cast(dv_ptr);); (3, CUDA, thrust::device_ptr<double> dv_ptr = thrust::min_element(thrust::device, d1, d1 + n, max_val);); (14, CUDA, thrust::device_free(dv_ptr);)", "label": "(14, CUDA, cudaError_t cuerr = cudaMemcpy(ret_host, ret_ptr, (1)*sizeof(double), cudaMemcpyDeviceToHost);); (3, CUDA, double* d_ptr = thrust::raw_pointer_cast(dv_ptr);); (7, CUDA, thrust::device_ptr<double> ret_dev_ptr = thrust::min_element(thrust::device, dv_ptr, dv_ptr+n);); (10, CUDA, double *ret_ptr = thrust::raw_pointer_cast(ret_dev_ptr);); (-1, CUDA, thrust::device_ptr<double> dv_ptr = thrust::device_malloc(n*sizeof(double));); (14, CUDA, thrust::device_free(dv_ptr);)"}
{"predict": "(40, MPI, MPI_Pack(a, 1, xpose, buffer, bufsize, &position, MPI_COMM_WORLD);); (56, MPI, MPI_Type_free(&row);); (28, MPI, MPI_Type_commit(&xpose);); (53, MPI, MPI_Type_free(&xpose);); (25, MPI, MPI_Type_hvector(100, 1, sizeofint, row, &xpose);); (20, MPI, MPI_Type_extent(MPI_INT, &sizeofint);); (27, MPI, MPI_Pack_size(1, xpose, MPI_COMM_WORLD, &bufsize);); (31, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (36, MPI, MPI_Unpack(buffer, bufsize, &position, b, 100 * 100, MPI_INT, MPI_COMM_WORLD);); (19, MPI, MPI_Type_vector(100, 1, 100, MPI_INT, &row);)", "label": "(25, MPI, MPI_Type_get_extent(MPI_INT, &tmp_lb, &sizeofint);); (39, MPI, MPI_Pack(a, 1, xpose, buffer, bufsize, &position, MPI_COMM_WORLD);); (55, MPI, MPI_Type_free(&row);); (27, MPI, MPI_Type_commit(&xpose);); (52, MPI, MPI_Type_free(&xpose);); (28, MPI, MPI_Pack_size(1, xpose, MPI_COMM_WORLD, &bufsize);); (23, MPI, MPI_Type_create_hvector(100, 1, sizeofint, row, &xpose);); (31, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (36, MPI, MPI_Unpack(buffer, bufsize, &position, b, 100 * 100, MPI_INT, MPI_COMM_WORLD);); (19, MPI, MPI_Type_vector(100, 1, 100, MPI_INT, &row);)"}
{"predict": "(27, CUDA, CHECK(cudaMemPrefetchAsync(z, M, cudaCpuDeviceId, NULL));); (23, CUDA, CHECK(cudaMemPrefetchAsync(x, M, device_id, NULL));); (23, CUDA, CHECK(cudaMemPrefetchAsync(y, M, device_id, NULL));); (6, CUDA, CHECK(cudaMallocManaged((void **)&z, M));); (26, CUDA, CHECK(cudaFree(x));); (15, CUDA, CHECK(cudaMemPrefetchAsync(z, M, device_id, NULL));); (-1, CUDA, CHECK(cudaGetDevice(&device_id));); (1, CUDA, CHECK(cudaMallocManaged((void **)&x, M));); (24, CUDA, CHECK(cudaFree(y));); (1, CUDA, CHECK(cudaMallocManaged((void **)&y, M));); (23, CUDA, CHECK(cudaFree(z));)", "label": "(27, CUDA, CHECK(cudaMemPrefetchAsync(z, M, cudaCpuDeviceId, NULL));); (20, CUDA, CHECK(cudaMemPrefetchAsync(x, M, device_id, NULL));); (20, CUDA, CHECK(cudaMemPrefetchAsync(y, M, device_id, NULL));); (7, CUDA, CHECK(cudaMallocManaged((void **)&z, M));); (28, CUDA, CHECK(cudaFree(x));); (-2, CUDA, CHECK(cudaGetDevice(&device_id));); (2, CUDA, CHECK(cudaMallocManaged((void **)&x, M));); (26, CUDA, CHECK(cudaFree(y));); (26, CUDA, CHECK(cudaFree(z));); (14, CUDA, CHECK(cudaMemPrefetchAsync(z, M, device_id, NULL));); (19, CUDA, CHECK(cudaDeviceSynchronize());); (-2, CUDA, CHECK(cudaMallocManaged((void **)&y, M));)"}
{"predict": "(12, CUDA, HANDLE_ERROR( cudaGLSetGLDevice( 0 ) );); (52, CUDA, HANDLE_ERROR( cudaGraphicsUnmapResources( 1, &resource, 0 ) );); (38, CUDA, HANDLE_ERROR( cudaGraphicsMapResources( 1, &resource, 0 ) );)", "label": "(14, CUDA, HANDLE_ERROR( cudaGLSetGLDevice( dev ) );); (51, CUDA, HANDLE_ERROR( cudaGraphicsUnmapResources( 1, &resource, NULL ) );); (39, CUDA, HANDLE_ERROR( cudaGraphicsMapResources( 1, &resource, NULL ) );); (4, CUDA, HANDLE_ERROR( cudaChooseDevice( &dev, &prop ) );)"}
{"predict": "(36, OpenMP, #pragma omp for private(x, y)); (22, OpenMP, #pragma omp for private()); (15, OpenMP, #pragma omp for private(,); (-1, OpenMP, #pragma omp parallel); (27, OpenMP, #pragma omp for private(0)); (37, OpenMP, #pragma omp for private(x, y, z)); (30, OpenMP, #pragma omp for private(x)); (7, OpenMP, #pragma omp for private(, )); (17, OpenMP, #pragma omp for private(int)); (-6, OpenMP, #pragma omp for private()", "label": "(26, OpenMP, #pragma omp for private(int)); (10, OpenMP, #pragma omp for private(,); (35, OpenMP, #pragma omp for private(x)); (2, OpenMP, #pragma omp for private(); (-2, OpenMP, #pragma omp parallel); (26, OpenMP, #pragma omp for private(0)); (10, OpenMP, #pragma omp for private(, )); (34, OpenMP, #pragma omp for private(x, y)); (13, OpenMP, #pragma omp for private()); (36, OpenMP, #pragma omp for private(x, y, z))"}
{"predict": "(9, TBB, test_extract_on_node<tbb::flow::overwrite_node, int>();); (9, TBB, test_extract_on_node<tbb::flow::overwrite_node, float>();); (0, TBB, tbb::task_scheduler_init init(p);)", "label": "(7, TBB, test_reserving_nodes<tbb::flow::write_once_node, size_t>();); (3, TBB, tbb::task_scheduler_init init(p);); (8, TBB, test_extract_on_node<tbb::flow::write_once_node, int>();)"}
{"predict": "(13, MPI, MPI_Isend(buffer, buf_size, MPI_BYTE, 1, tag1, MPI_COMM_WORLD, &request1);); (15, MPI, MPI_Wait(&request2, &status);); (17, MPI, MPI_Irecv(buffer, buf_size, MPI_BYTE, 0, tag2, MPI_COMM_WORLD, &request2);); (15, MPI, MPI_Irecv(buffer, buf_size, MPI_BYTE, 0, tag1, MPI_COMM_WORLD, &request1);); (10, MPI, MPI_Isend(buffer, buf_size, MPI_BYTE, 1, tag2, MPI_COMM_WORLD, &request2);); (10, MPI, MPI_Wait(&request1, &status);)", "label": "(13, MPI, MPI_Isend(buffer, buf_size, MPI_BYTE, 1, tag1, MPI_COMM_WORLD, &request1);); (15, MPI, MPI_Wait(&request2, &status);); (12, MPI, MPI_Isend(buffer, buf_size, MPI_BYTE, 1, tag2, MPI_COMM_WORLD, &request2);); (16, MPI, MPI_Irecv(buffer, buf_size, MPI_BYTE, 0, tag1, MPI_COMM_WORLD, &request2);); (11, MPI, MPI_Wait(&request1, &status);); (13, MPI, MPI_Irecv(buffer, buf_size, MPI_BYTE, 0, tag2, MPI_COMM_WORLD, &request1);)"}
{"predict": "(36, OpenMP, #pragma omp target parallel for map(a, b))", "label": "(36, OpenMP, #pragma omp target parallel for map(a, b))"}
{"predict": "(22, OpenMP, #pragma omp target update to(p_a[0:N])); (17, OpenMP, #pragma omp target associate ptr(p_a) with d_a); (24, OpenMP, #pragma omp target disassociate ptr(p_a)); (10, OpenMP, d_a = (int *)omp_target_alloc(DataSize, DefaultDeviceNum);); (27, OpenMP, omp_target_free(d_a, DefaultDeviceNum);); (8, OpenMP, #pragma omp target data map(alloc : d_a [0:N])); (12, OpenMP, #pragma omp target data map(alloc : d_a [0:N]) use_device_ptr(d_a))", "label": "(32, OpenMP, omp_target_free(d_a, DefaultDeviceNum);); (23, OpenMP, #pragma omp target update to(p_a)); (12, OpenMP, d_a = (int *)omp_target_alloc(DataSize, DefaultDeviceNum);); (6, OpenMP, int DefaultDeviceNum = omp_get_default_device();); (14, OpenMP, omp_target_associate_ptr(p_a, d_a, DataSize, 0, DefaultDeviceNum);); (23, OpenMP, omp_target_disassociate_ptr(p_a, DefaultDeviceNum);)"}
{"predict": "(2, OpenMP, #pragma omp target teams distribute parallel for simd thread_limit); (4, OpenMP, #pragma omp target teams distribute parallel for simd thread_limit (); (21, OpenMP, #pragma omp target teams distribute parallel for simd thread_limit (argc), thread_limit (argc+1)); (23, OpenMP, #pragma omp target teams distribute parallel for simd thread_limit (S1)); (4, OpenMP, #pragma omp target teams distribute parallel for simd thread_limit ()); (24, OpenMP, #pragma omp target teams distribute parallel for simd thread_limit (-2)); (29, OpenMP, #pragma omp target teams distribute parallel for simd thread_limit (3.14)); (4, OpenMP, #pragma omp target teams distribute parallel for simd thread_limit (argc); (12, OpenMP, #pragma omp target teams distribute parallel for simd thread_limit (argc + argc-z)); (5, OpenMP, #pragma omp target teams distribute parallel for simd thread_limit (argc))); (22, OpenMP, #pragma omp target teams distribute parallel for simd thread_limit (-10u)); (6, OpenMP, #pragma omp target teams distribute parallel for simd thread_limit (argc > 0 ? argv[1] : argv[2]))", "label": "(29, OpenMP, #pragma omp target teams distribute num_teams (-2)); (4, OpenMP, #pragma omp target teams distribute num_teams (); (30, OpenMP, #pragma omp target teams distribute num_teams (-10u)); (5, OpenMP, #pragma omp target teams distribute num_teams ()); (7, OpenMP, #pragma omp target teams distribute num_teams (argc); (15, OpenMP, #pragma omp target teams distribute num_teams (argc + argc+z)); (17, OpenMP, #pragma omp target teams distribute num_teams (argc), num_teams (argc+1)); (28, OpenMP, #pragma omp target teams distribute num_teams (3.14)); (-6, OpenMP, #pragma omp target teams distribute num_teams); (5, OpenMP, #pragma omp target teams distribute num_teams (argc))); (7, OpenMP, #pragma omp target teams distribute num_teams (argc > 0 ? argv[1] : argv[2])); (15, OpenMP, #pragma omp target teams distribute num_teams (S1))"}
{"predict": "(2, CUDA, extern __shared__ int sums[];)", "label": "(2, CUDA, extern __shared__ int sums[];)"}
{"predict": "(23, MPI, double start_time = MPI_Wtime();); (16, MPI, MPI_Grequest_start(MPIU_Greq_query_fn, MPIU_Greq_free_fn, MPIU_Greq_cancel_fn, events[i], &aio_reqlist[i]->req);)", "label": "(24, MPI, double start_time = MPI_Wtime();); (16, MPI, errcode = MPI_Grequest_complete(aio_reqlist[i]->req);)"}
{"predict": "(8, OpenMP, #pragma omp parallel for default(none) private(index) shared(count, failed, params, max_threads, local, saved_key, cur_salt, crypt_out)); (15, OpenMP, int t = omp_get_thread_num();)", "label": "(8, OpenMP, #pragma omp parallel for default(none) private(index) shared(count, failed, params, max_threads, local, saved_key, cur_salt, crypt_out)); (15, OpenMP, int t = omp_get_thread_num();)"}
{"predict": "(11, MPI, MPI_Info_set(i1, \"key1\", \"value1\");); (9, MPI, MPI_Info_create(&i1);); (13, MPI, MPI_Info_get(i1, \"key2\", 64, value, &flag);); (10, MPI, MPI_Info_set(i1, \"key2\", \"value2\");); (5, MPI, MPI_Info_create(&i2);); (24, MPI, MPI_Info_free(&i1);); (24, MPI, MPI_Info_free(&i2);); (14, MPI, MPI_Info_get(i1, \"key1\", 64, value, &flag);); (4, MPI, MPI_Info_set(i2, \"key1\", \"value1\");)", "label": "(20, MPI, MPI_Info_get(i1, (char *) \"key1\", 64, value, &flag);); (28, MPI, MPI_Info_free(&i1);); (11, MPI, MPI_Info_set(i2, (char *) \"key2\", (char *) \"value2\");); (9, MPI, MPI_Info_set(i1, (char *) \"key1\", (char *) \"value1\");); (11, MPI, MPI_Info_get(i1, (char *) \"key2\", 64, value, &flag);); (25, MPI, MPI_Info_free(&i2);); (3, MPI, MPI_Info_create(&i1);); (3, MPI, MPI_Info_create(&i2);)"}
{"predict": "(27, OpenCL, status = clBuildProgram(program, 1, &device, buildOptions, NULL, NULL);); (9, OpenCL, cl_program program = clCreateProgramWithSource(context, 1, &source, NULL, &status);)", "label": "(27, OpenCL, status = clBuildProgram(program, 1, &device, buildOptions, NULL, NULL);); (9, OpenCL, cl_program program = clCreateProgramWithSource(context,1, &source, NULL, &status);)"}
{"predict": "(15, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)", "label": "(15, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(32, MPI, MPI_Get(&locbuf[i], 1, MPI_INT, target_rank, i, 1, MPI_INT, win);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (24, MPI, MPI_Win_unlock(rank, win);); (44, MPI, MPI_Win_free(&win);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (27, MPI, MPI_Win_flush_local(target_rank, win);); (19, MPI, MPI_Barrier(MPI_COMM_WORLD);); (23, MPI, MPI_Win_unlock_all(win);); (16, MPI, MPI_Win_lock_all(0, win);)", "label": "(25, MPI, MPI_Win_unlock(rank, win);); (48, MPI, MPI_Win_unlock_all(win);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (31, MPI, MPI_Win_flush_local(target_rank, win);); (45, MPI, MPI_Win_free(&win);); (23, MPI, MPI_Win_lock_all(0, win);); (11, MPI, MPI_Win_create(winbuf, BUFCNT * sizeof(int), sizeof(int), MPI_INFO_NULL, MPI_COMM_WORLD, &win);); (26, MPI, MPI_Get(locbuf, BUFCNT, MPI_INT, target_rank, 0, BUFCNT, MPI_INT, win);); (18, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(9, MPI, return MPI_Wait(&tmprequest, MPI_STATUS_IGNORE);)", "label": "(27, MPI, return MPI_Waitall(num_ep, block_coll_request, MPI_STATUS_IGNORE);); (8, MPI, return MPI_Wait(&tmprequest, MPI_STATUS_IGNORE);)"}
{"predict": "(14, TBB, if (Verbose && t == 0) t0 = oneapi::tbb::tick_count::now();); (4, TBB, typedef std::vector<T, oneapi::tbb::tbb_allocator<T> > container_type;); (12, TBB, typedef typename oneapi::tbb::enumerable_thread_specific< container_type, Allocator<container_type>, oneapi::tbb::ets_key_per_instance > ets_tlskey_type;); (26, TBB, oneapi::tbb::parallel_for ( oneapi::tbb::blocked_range<std::size_t> (0, N, 10000), pvrb );); (9, TBB, typedef typename oneapi::tbb::enumerable_thread_specific< container_type, Allocator<container_type>, oneapi::tbb::ets_no_key > ets_nokey_type;); (48, TBB, container_type fvs2 = oneapi::tbb::flatten2d(vs3);); (40, TBB, oneapi::tbb::parallel_reduce ( oneapi::tbb::blocked_range<std::size_t> (0, N, 10000), parallel_vector_reduce_body< typename ets_nokey_type::const_range_type, T >() );); (-4, TBB, oneapi::tbb::tick_count t0;); (31, TBB, oneapi::tbb::flattened2d<ets_nokey_type> fvs = oneapi::tbb::flatten2d(vs3);); (1, TBB, oneapi::tbb::global_control gc(oneapi::tbb::global_control::max_allowed_parallelism, p);); (38, TBB, oneapi::tbb::flattened2d<container_type> cfvs2 = oneapi::tbb::flatten2d(fvs2);)", "label": "(3, TBB, typedef std::vector<T, oneapi::tbb::tbb_allocator<T> > container_type;); (33, TBB, oneapi::tbb::flattened2d<ets_nokey_type> fvs = flatten2d(vs3);); (11, TBB, typedef typename oneapi::tbb::enumerable_thread_specific< container_type, Allocator<container_type>, oneapi::tbb::ets_key_per_instance > ets_tlskey_type;); (9, TBB, typedef typename oneapi::tbb::enumerable_thread_specific< container_type, Allocator<container_type>, oneapi::tbb::ets_no_key > ets_nokey_type;); (44, TBB, oneapi::tbb::flattened2d<ets_nokey_type> fvs2 = flatten2d(vs3, vs3.begin(), std::next(vs3.begin()));); (23, TBB, oneapi::tbb::parallel_reduce ( vs3.range(1), pvrb );); (0, TBB, oneapi::tbb::global_control gc(oneapi::tbb::global_control::max_allowed_parallelism, p);); (10, TBB, oneapi::tbb::parallel_for ( oneapi::tbb::blocked_range<int> (0, N*p, RANGE_MIN), parallel_vector_for_body<T, Allocator>( vs, allocator_name ) );); (42, TBB, const oneapi::tbb::flattened2d<ets_nokey_type>& cfvs2(fvs2);)"}
{"predict": "(18, OpenMP, gsl_decomp_cholesky_unit(v, dv);); (50, OpenMP, gsl_decomp_cholesky_unit(dm, NULL);)", "label": "(17, OpenMP, s += gsl_linalg_cholesky_decomp_unit(v, dv);)"}
{"predict": "(19, CUDA, cudaSetDevice(currentDevice);)", "label": "(19, CUDA, cudaSetDevice(currentDevice);)"}
{"predict": "(41, CUDA, cudaError_t result = cudaGetLastError();); (41, CUDA, EXPECT_EQ(result, cudaSuccess) << \" - CUDA error: \" << cudaGetErrorString(result);)", "label": "(41, CUDA, cudaError_t result = cudaGetLastError();); (41, CUDA, EXPECT_EQ(result, cudaSuccess) << \" - CUDA error: \" << cudaGetErrorString(result);)"}
{"predict": "(14, MPI, MPI_Comm_size(comm, &size);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &worldrank);); (25, MPI, MPI_Comm_group(dupcomm, &g2);); (21, MPI, MPI_Comm_group(newcomm, &g1);); (15, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (22, MPI, MPI_Group_free(&basegroup);); (8, MPI, MPI_Comm_group(comm, &basegroup);); (12, MPI, MPI_Comm_split(comm, 0, size - rank, &dupcomm);); (14, MPI, mpi_errno = MPI_Group_difference(g1, g2, NULL);); (16, MPI, MPI_Comm_free(&newcomm);); (18, MPI, MPI_Comm_free(&dupcomm);); (5, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (11, MPI, MPI_Error_class(mpi_errno, &errclass);); (3, MPI, MPI_Comm_rank(comm, &rank);); (14, MPI, MPI_Group_free(&g1);); (14, MPI, MPI_Group_free(&g2);); (6, MPI, MPI_Comm_split(comm, rank < size / 2, rank, &newcomm);)", "label": "(14, MPI, MPI_Comm_size(comm, &size);); (19, MPI, MPI_Comm_group(dupcomm, &g2);); (28, MPI, MPI_Group_free(&g2);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &worldrank);); (28, MPI, MPI_Comm_free(&dupcomm);); (23, MPI, MPI_Group_free(&basegroup);); (12, MPI, MPI_Comm_group(newcomm, &g1);); (12, MPI, MPI_Comm_dup(comm, &dupcomm);); (15, MPI, mpi_errno = MPI_Group_difference(g1, g2, NULL);); (22, MPI, MPI_Comm_free(&newcomm);); (14, MPI, MPI_Error_class(mpi_errno, &errclass);); (4, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (5, MPI, MPI_Comm_split(comm, 0, size - rank, &newcomm);); (16, MPI, MPI_Group_free(&g1);); (-1, MPI, MPI_Comm_rank(comm, &rank);); (-3, MPI, MPI_Comm_group(comm, &basegroup);)"}
{"predict": "(9, CUDA, HANDLE_ERROR(cudaMalloc((void**)&colored_noise_d, sizeof(float) * full_buffer_size));); (11, CUDA, cudaStreamCreate(&stream);); (18, CUDA, HANDLE_ERROR(cudaStreamSynchronize(stream));)", "label": "(9, CUDA, HANDLE_ERROR(cudaMalloc((void**)&colored_noise_d, sizeof(float) * full_buffer_size));); (11, CUDA, cudaStreamCreate(&stream);); (18, CUDA, HANDLE_ERROR(cudaStreamSynchronize(stream));)"}
{"predict": "(71, CUDA, CUDA_CHECK(cudaFree(cd));); (15, CUDA, CUDA_CHECK(cudaDeviceSynchronize());); (10, CUDA, CUDA_CHECK(cudaMalloc(&cd, size * sizeof(double)));); (62, CUDA, CUDA_CHECK(cudaMemcpy(c, cd, size * sizeof(double), cudaMemcpyDeviceToHost));); (10, CUDA, CUDA_CHECK(cudaMemcpy(cd, c, size * sizeof(double), cudaMemcpyHostToDevice));)", "label": "(64, CUDA, CUDA_SAFE_CALL( cudaMemcpy(c, cd, size*sizeof(double), cudaMemcpyDeviceToHost) );); (67, CUDA, CUDA_SAFE_CALL( cudaDeviceReset() );); (12, CUDA, CUDA_SAFE_CALL( cudaMemset(cd, 0, size*sizeof(double)) );); (8, CUDA, CUDA_SAFE_CALL( cudaMalloc((void**)&cd, size*sizeof(double)) );); (62, CUDA, CUDA_SAFE_CALL( cudaFree(cd) );); (12, CUDA, CUDA_SAFE_CALL( cudaThreadSynchronize() );)"}
{"predict": "(3, CUDA, using T = thrust::complex<double>;)", "label": "(3, CUDA, cudaSetDevice(WORLD.rank());); (11, CUDA, using T = thrust::complex<double>;); (40, CUDA, cudaStreamSynchronize(NULL);); (14, CUDA, thrust::host_vector<T> H = A;)"}
{"predict": "(9, OpenMP, #pragma omp target teams distribute parallel for reduction(min : min_t)); (6, OpenMP, #pragma omp target data map(to : arr_t[0 : N])); (11, OpenMP, #pragma omp target teams distribute parallel for reduction(max : max_t))", "label": "(9, OpenMP, #pragma omp target teams distribute parallel for reduction(min : min_t)); (6, OpenMP, #pragma omp target data map(to : arr_t[0 : N])); (11, OpenMP, #pragma omp target teams distribute parallel for reduction(max : max_t))"}
{"predict": "(20, MPI, err = MPI_Waitany(10, requests, &index, statuses);); (33, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_ARE_FATAL);); (33, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (15, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(20, MPI, err = MPI_Waitany(10, requests, &index, statuses);); (33, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_ARE_FATAL);); (33, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (15, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(3, OpenMP, mz_uint comp_flags = TDEFL_COMPUTE_ADLER32 | tdefl_create_comp_flags_from_zip_params(level, window_bits, strategy);)", "label": "(3, OpenMP, mz_uint comp_flags = TDEFL_COMPUTE_ADLER32 | tdefl_create_comp_flags_from_zip_params(level, window_bits, strategy);)"}
{"predict": "(20, MPI, t[k].endSync = MPI_Wtime();); (9, MPI, t[k].startOp = MPI_Wtime();); (10, MPI, MPI_Put(&one, sz, MPI_INT, destRank, j, sz, MPI_INT, win);); (12, MPI, t[k].endOp = MPI_Wtime();); (15, MPI, MPI_Win_wait(win);); (2, MPI, MPI_Win_post(exposureGroup, 0, win);); (12, MPI, MPI_Win_complete(win);); (-1, MPI, MPI_Barrier(MPI_COMM_WORLD);); (0, MPI, MPI_Win_start(accessGroup, 0, win);)", "label": "(20, MPI, t[k].endSync = MPI_Wtime();); (9, MPI, t[k].startOp = MPI_Wtime();); (10, MPI, MPI_Put(&one, sz, MPI_INT, destRank, j, sz, MPI_INT, win);); (12, MPI, t[k].endOp = MPI_Wtime();); (15, MPI, MPI_Win_wait(win);); (2, MPI, MPI_Win_post(exposureGroup, 0, win);); (12, MPI, MPI_Win_complete(win);); (-1, MPI, MPI_Barrier(MPI_COMM_WORLD);); (0, MPI, MPI_Win_start(accessGroup, 0, win);)"}
{"predict": "(10, TBB, tbb::filter_t<void, void> one_filter( filter_table[i], i_filter );); (15, TBB, tbb::atomic<int> counter;); (11, TBB, tbb::parallel_pipeline( n_tokens, one_filter );)", "label": "(10, TBB, tbb::filter_t<void, void> one_filter( filter_table[i], i_filter );); (15, TBB, tbb::atomic<int> counter;); (11, TBB, tbb::parallel_pipeline( n_tokens, one_filter );)"}
{"predict": "(47, CUDA, CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));)", "label": "(47, CUDA, CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));)"}
{"predict": "(34, CUDA, cudaFree(d_inPtr);); (18, CUDA, cudaMalloc((void**)&d_outPtr, size * sizeof(float));); (24, CUDA, cudaMemcpy(&result_gpu, &d_outPtr[0], sizeof(float), cudaMemcpyDeviceToHost);); (18, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);); (29, CUDA, cudaFree(d_outPtr);); (13, CUDA, cudaMalloc((void**)& d_inPtr, size * sizeof(float));)", "label": "(34, CUDA, cudaFree(d_inPtr);); (18, CUDA, cudaMalloc((void**)&d_outPtr, size * sizeof(float));); (24, CUDA, cudaMemcpy(&result_gpu, &d_outPtr[0], sizeof(float), cudaMemcpyDeviceToHost);); (18, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);); (29, CUDA, cudaFree(d_outPtr);); (13, CUDA, cudaMalloc((void**)& d_inPtr, size * sizeof(float));)"}
{"predict": "(12, OpenMP, iget = omp_get_teams_thread_limit();); (3, OpenMP, omp_set_num_teams(5);); (3, OpenMP, iget = omp_get_max_teams();); (8, OpenMP, omp_set_teams_thread_limit(7);)", "label": "(12, OpenMP, iget = omp_get_teams_thread_limit();); (3, OpenMP, omp_set_num_teams(iset);); (3, OpenMP, iget = omp_get_max_teams();); (8, OpenMP, omp_set_teams_thread_limit(iset);)"}
{"predict": "(13, TBB, test_resets<float,tbb::flow::queue_node<float> >();); (20, TBB, test_buffer_extract<tbb::flow::queue_node<int> >().run_tests();); (10, TBB, test_resets<int,tbb::flow::queue_node<int> >();); (0, TBB, tbb::task_scheduler_init init(p);); (5, TBB, stop = tbb::tick_count::now();); (-4, TBB, tbb::tick_count start = tbb::tick_count::now(), stop;)", "label": "(12, TBB, test_resets<int, tbb::flow::queue_node<int> >();); (2, TBB, tbb::task_scheduler_init init(p);); (7, TBB, stop = tbb::tick_count::now();); (18, TBB, test_buffer_extract<tbb::flow::queue_node<int> >().run_tests();); (-3, TBB, tbb::tick_count start = tbb::tick_count::now(), stop;); (8, TBB, test_resets<float, tbb::flow::queue_node<float> >();)"}
{"predict": "(6, CUDA, cudaMallocManaged(&y, N*sizeof(float));); (15, CUDA, cudaDeviceSynchronize();); (26, CUDA, cudaFree(y);); (4, CUDA, cudaMallocManaged(&x, N*sizeof(float));); (22, CUDA, cudaFree(x);)", "label": "(16, CUDA, cudaDeviceSynchronize();); (6, CUDA, cudaMallocManaged(&y, N*sizeof(float));); (4, CUDA, cudaMallocManaged(&x, N*sizeof(float));); (23, CUDA, cudaFree(y);); (21, CUDA, cudaFree(x);)"}
{"predict": "(17, OpenMP, #pragma omp target teams distribute parallel for if(target: true)); (4, OpenMP, #pragma omp target enter data map(alloc: vecTs[:n]) if(target: true)); (21, OpenMP, #pragma omp target exit data map(delete: vecTs[:n]) if(target: true)); (14, OpenMP, #pragma omp target map(tofrom: vecTs[i].arr[:vecTs[i].size]) if(target: true))", "label": "(17, OpenMP, #pragma omp parallel for num_threads(num_devices)); (10, OpenMP, #pragma omp target teams distribute parallel for); (3, OpenMP, size_t num_devices = omp_get_num_devices();); (17, OpenMP, #pragma omp target teams distribute parallel for device(omp_get_thread_num()))"}
{"predict": "(9, MPI, MPI_Allreduce(&time, &max_time, 1, MPI_LONG_LONG_INT, MPI_MAX, MPI_COMM_WORLD);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (10, MPI, MPI_Allreduce(&time, &sum_time, 1, MPI_LONG_LONG_INT, MPI_SUM, MPI_COMM_WORLD);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (-1, MPI, MPI_Start(&region);); (6, MPI, MPI_Allreduce(&time, &min_time, 1, MPI_LONG_LONG_INT, MPI_MIN, MPI_COMM_WORLD);); (18, MPI, MPI_End(&region);)", "label": "(9, MPI, MPI_Reduce(&time, &sum_time, 1, MPI_LONG_LONG, MPI_SUM, 0, comm_);); (6, MPI, MPI_Reduce(&time, &max_time, 1, MPI_LONG_LONG, MPI_MAX, 0, comm_);); (10, MPI, MPI_Comm_size(comm_, &size);); (8, MPI, MPI_Comm_rank(comm_, &rank);); (4, MPI, MPI_Reduce(&time, &min_time, 1, MPI_LONG_LONG, MPI_MIN, 0, comm_);)"}
{"predict": "(37, CUDA, cudaHostFree( hptr );)", "label": "(37, CUDA, cudaFreeHost( hptr );)"}
{"predict": "(20, OpenMP, #pragma omp target simd aligned(B,rp,::z: X::x)); (21, OpenMP, #pragma omp target simd aligned(::z)); (26, OpenMP, #pragma omp target simd aligned(B::ib,B:C1+C2)); (22, OpenMP, #pragma omp target simd aligned(B::bfoo())); (4, OpenMP, #pragma omp target simd aligned(B:B::bfoo())); (12, OpenMP, #pragma omp target simd aligned(X::x : ::z)); (5, OpenMP, #pragma omp target simd aligned(z:B:bfoo())); (-4, OpenMP, #pragma omp target simd aligned(B:bfoo())); (-2, OpenMP, #pragma omp target simd aligned(B::ib:B:bfoo()))", "label": "(17, OpenMP, #pragma omp target simd aligned(X::x : ::z)); (2, OpenMP, #pragma omp target simd aligned(B:bfoo())); (23, OpenMP, #pragma omp target simd aligned(B::bfoo())); (17, OpenMP, #pragma omp target simd aligned(B,rp,::z: X::x)); (24, OpenMP, #pragma omp target simd aligned(B::ib,B:C1+C2)); (6, OpenMP, #pragma omp target simd aligned(z:B:bfoo())); (0, OpenMP, #pragma omp target simd aligned(B::ib:B:bfoo())); (1, OpenMP, #pragma omp target simd aligned(B:B::bfoo())); (14, OpenMP, #pragma omp target simd aligned(::z))"}
{"predict": "(25, TBB, tbb::parallel_for(tbb::blocked_range<int>(0,n_cams), vtask);)", "label": "(25, TBB, tbb::task_scheduler_init init;); (25, TBB, tbb::parallel_for( tbb::blocked_range<int>(0,n_cams), vtask);)"}
{"predict": "(71, CUDA, retval = cudaDeviceSynchronize();); (33, CUDA, CubDebugExit(cudaGetDevice(&device_ordinal));); (42, CUDA, CubDebugExit(cudaDeviceGetAttribute(&max_block_threads, cudaDevAttrMaxActiveBlocksPerMultiprocessor, device_ordinal));); (38, CUDA, CubDebugExit(cudaDeviceGetAttribute(&sm_count, cudaDevAttrMultiProcessorCount, device_ordinal));); (39, CUDA, CubDebugExit(cudaDeviceGetAttribute(&max_sm_occupancy, cudaDevAttrMaxOccupancyPerMultiprocessor, device_ordinal));)", "label": "(34, CUDA, CubDebugExit(cudaGetDevice(&device_ordinal));); (42, CUDA, CubDebugExit(cudaDeviceGetAttribute(&max_block_threads, cudaDevAttrMaxThreadsPerBlock, device_ordinal));); (40, CUDA, CubDebugExit(cudaDeviceGetAttribute(&sm_count, cudaDevAttrMultiProcessorCount, device_ordinal));); (69, CUDA, retval = CubDebug(cudaDeviceSynchronize());)"}
{"predict": "(46, OpenMP, int i = omp_get_thread_num();); (78, CUDA, checkCudaErrors(cudaFree(d_a));); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &np);); (76, CUDA, checkCudaErrors(cudaFree(d_c));); (33, CUDA, checkCudaErrors(cudaMalloc(&d_a, bufsize));); (24, CUDA, checkCudaErrors(cudaMallocManaged(&h_a, bufsize));); (80, CUDA, checkCudaErrors(cudaFree(d_b));); (55, OpenMP, #pragma omp barrier); (42, OpenMP, #pragma omp parallel num_threads(num_operator)); (75, CUDA, checkCudaErrors(cudaFreeHost(h_b));); (75, CUDA, checkCudaErrors(cudaFreeHost(h_c));); (27, CUDA, checkCudaErrors(cudaMalloc(&d_b, bufsize));); (16, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (75, MPI, MPI_Finalize();); (25, CUDA, checkCudaErrors(cudaMalloc(&d_c, bufsize));); (10, MPI, MPI_Init(NULL, NULL);); (20, CUDA, checkCudaErrors(cudaMallocHost(&h_c, bufsize));); (47, OpenMP, #pragma omp master); (18, CUDA, checkCudaErrors(cudaMallocHost(&h_b, bufsize));); (39, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (67, CUDA, checkCudaErrors(cudaFreeHost(h_a));)", "label": "(51, OpenMP, omp_set_num_threads(num_operator);); (16, MPI, MPI_Init(&argc, &argv);); (37, CUDA, cudaMalloc((void**)&d_a, bufsize);); (80, CUDA, cudaFree(d_a);); (24, CUDA, cudaMallocHost((void**)&h_a, bufsize);); (25, CUDA, cudaMallocHost((void**)&h_c, bufsize);); (58, CUDA, cudaDeviceSynchronize();); (78, CUDA, cudaFree(d_c);); (44, OpenMP, #pragma omp parallel); (75, CUDA, cudaFree(d_b);); (30, CUDA, cudaMalloc((void**)&d_b, bufsize);); (43, OpenMP, int i = omp_get_thread_num();); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (16, CUDA, cudaMallocHost((void**)&h_b, bufsize);); (75, CUDA, cudaFreeHost(h_b);); (73, CUDA, cudaFreeHost(h_a);); (25, CUDA, cudaMalloc((void**)&d_c, bufsize);); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &np);); (74, MPI, MPI_Finalize();); (71, CUDA, cudaFreeHost(h_c);)"}
{"predict": "(38, MPI, MPI_Send(&sendbuf, 1, struct_type, rank, 0, MPI_COMM_SELF);); (31, MPI, MPI_Type_commit(&struct_type);); (54, MPI, MPI_Type_free(&contig);); (38, MPI, MPI_Recv(&recvbuf, 1, struct_type, rank, 0, MPI_COMM_SELF, &status);); (27, MPI, MPI_Type_create_struct(3, blens, displs, types, &struct_type);); (51, MPI, MPI_Get_elements(&status, struct_type, &count);); (27, MPI, MPI_Type_contiguous(3, struct_type, &contig);); (51, MPI, MPI_Type_free(&struct_type);); (23, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(29, MPI, MPI_Type_contiguous(1, struct_type, &contig);); (51, MPI, MPI_Type_free(&struct_type);); (26, MPI, MPI_Type_create_struct(3, blens, displs, types, &struct_type);); (27, MPI, MPI_Type_commit(&struct_type);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (26, MPI, MPI_Type_commit(&contig);); (45, MPI, MPI_Get_elements(&status, contig, &count);); (34, MPI, MPI_Sendrecv(&sendbuf, 1, contig, 0, 0, &recvbuf, 1, contig, 0, 0, MPI_COMM_SELF, &status);); (45, MPI, MPI_Type_free(&contig);)"}
{"predict": "(4, MPI, MPI_Init(&argc, &argv);); (13, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)", "label": "(4, MPI, MPI_Init(&argc, &argv);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (11, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(11, CUDA, err = cudaDeviceSynchronize();); (2, CUDA, err = cudaMalloc(&numErr_d, sizeof(int));); (21, CUDA, printf(\"Cuda Main: cudaFree failed with %s\\n\", cudaGetErrorString(err));); (24, CUDA, cudaFree(numErr_d);); (13, CUDA, err = cudaMemcpy(&numErr_h, numErr_d, sizeof(int), cudaMemcpyDeviceToHost);); (1, CUDA, printf(\"Cuda Main: cudaMalloc failed with %s\\n\", cudaGetErrorString(err));)", "label": "(5, CUDA, err = cudaMalloc(&numErr_d, sizeof(int));); (25, CUDA, printf(\"Cuda Main: cudaFree failed with %s\\n\", cudaGetErrorString(err));); (11, CUDA, err = cudaDeviceSynchronize();); (4, CUDA, printf(\"Cuda Main: cudaMemcpy failed with %s\\n\", cudaGetErrorString(err));); (20, CUDA, err = cudaFree(numErr_d);); (13, CUDA, err = cudaMemcpy(&numErr_h, numErr_d, sizeof(int), cudaMemcpyDeviceToHost);); (2, CUDA, cudaFree(numErr_d);)"}
{"predict": "(55, MPI, ascent_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (7, MPI, MPI_Comm_rank(comm, &par_rank);); (7, MPI, MPI_Comm_size(comm, &par_size);)", "label": "(55, MPI, ascent_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (7, MPI, MPI_Comm_rank(comm, &par_rank);); (7, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(43, MPI, ascent_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (15, MPI, MPI_Comm_rank(comm, &par_rank);); (15, MPI, MPI_Comm_size(comm, &par_size);)", "label": "(43, MPI, ascent_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (15, MPI, MPI_Comm_rank(comm, &par_rank);); (15, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(3, CUDA, extern __shared__ float shared[];)", "label": "(3, CUDA, extern __shared__ float shared[];)"}
{"predict": "(48, MPI, MPI_Startall(2, requests);); (50, MPI, MPI_Waitall(2, requests, statuses);); (57, MPI, t1 = MPI_Wtime();); (12, MPI, MPI_Init(&argc, &argv);); (13, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (32, MPI, MPI_Barrier(MPI_COMM_WORLD);); (32, MPI, t0 = MPI_Wtime();)", "label": "(48, MPI, MPI_Startall(2, requests);); (50, MPI, MPI_Waitall(2, requests, statuses);); (57, MPI, t1 = MPI_Wtime();); (12, MPI, MPI_Init(&argc, &argv);); (61, MPI, MPI_Finalize();); (11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (31, MPI, MPI_Barrier(MPI_COMM_WORLD);); (31, MPI, t0 = MPI_Wtime();)"}
{"predict": "(12, OpenCL, retVal = clGetDeviceInfo( device, CL_DEVICE_PROFILE, sizeof(profile), profile, NULL );)", "label": "(12, OpenCL, retVal = clGetDeviceInfo(device, CL_DEVICE_PROFILE, sizeof(profile), profile, NULL);)"}
{"predict": "(27, MPI, MPI_Type_get_true_extent(oldtypes[i], &true_lb, &true_extent);); (34, MPI, MPI_Type_create_struct(num, blocklens, displs, types, &aligned_type);); (34, MPI, MPI_Type_commit(&aligned_type);); (40, MPI, MPI_Type_free(&aligned_type);); (24, MPI, MPI_Type_size(oldtypes[i], &disp);)", "label": "(38, MPI, MPI_Type_free(&aligned_type);); (36, MPI, MPI_Type_commit(newtype);); (33, MPI, MPI_Type_create_struct(num, blocklens, displs, types, &aligned_type);); (33, MPI, MPI_Type_create_resized(aligned_type, 0, disp, newtype);); (23, MPI, MPI_Type_get_true_extent(oldtypes[i], &true_lb, &true_extent);)"}
{"predict": "(2, OpenMP, #pragma omp target teams distribute parallel for simd defaultmap); (7, OpenMP, #pragma omp target teams distribute parallel for simd defaultmap()); (15, OpenMP, #pragma omp target teams distribute parallel for simd defaultmap(tofrom)); (20, OpenMP, #pragma omp target teams distribute parallel for simd defaultmap(tofrom,); (10, OpenMP, #pragma omp target teams distribute parallel for simd defaultmap (tofrom:); (15, OpenMP, #pragma omp target teams distribute parallel for simd defaultmap(tofrom scalar)); (22, OpenMP, #pragma omp target teams distribute parallel for simd defaultmap(tofrom, scalar); (4, OpenMP, #pragma omp target teams distribute parallel for simd defaultmap(tofrom); (-3, OpenMP, #pragma omp target teams distribute parallel for simd defaultmap(); (17, OpenMP, #pragma omp target teams distribute parallel for simd defaultmap (scalar:)", "label": "(29, OpenMP, #pragma omp target simd defaultmap(tofrom, scalar); (1, OpenMP, #pragma omp target simd defaultmap); (9, OpenMP, #pragma omp target simd defaultmap(tofrom); (20, OpenMP, #pragma omp target simd defaultmap(tofrom,); (10, OpenMP, #pragma omp target simd defaultmap(tofrom:); (15, OpenMP, #pragma omp target simd defaultmap(tofrom scalar)); (20, OpenMP, #pragma omp target simd defaultmap(scalar:); (-2, OpenMP, #pragma omp target simd defaultmap(); (9, OpenMP, #pragma omp target simd defaultmap(tofrom)); (-1, OpenMP, #pragma omp target simd defaultmap())"}
{"predict": "(85, MPI, ascent_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (91, MPI, MPI_Barrier(comm);); (16, MPI, MPI_Comm_rank(comm, &par_rank);); (16, MPI, MPI_Comm_size(comm, &par_size);)", "label": "(85, MPI, ascent_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (91, MPI, MPI_Barrier(comm);); (16, MPI, MPI_Comm_rank(comm, &par_rank);); (16, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(10, OpenMP, #pragma omp parallel sections); (1, OpenMP, #pragma omp parallel); (5, OpenMP, #pragma omp section); (-1, OpenMP, #pragma omp master)", "label": "(2, OpenMP, #pragma omp target map(alloc:colstat[0:NZ])); (5, OpenMP, #pragma omp target map(alloc:colstat[:0])); (9, OpenMP, #pragma omp target update from(colstat))"}
{"predict": "( tbb::task_scheduler_init init( availableProcs );)", "label": "(41, TBB, ASSERT( (int)tbb::tbb_thread::hardware_concurrency() == availableProcs, NULL );); (39, TBB, ASSERT( tbb::task_scheduler_init::default_num_threads() == availableProcs, NULL );)"}
{"predict": "(6, CUDA, cudaMallocManaged(&ptr, size);); (30, CUDA, cudaFree(ptr);)", "label": "(8, CUDA, cudaSetDevice(devs[0]);); (5, CUDA, CUCH(cudaMallocManaged(&ptr, size), kmcudaMemoryAllocationFailure);); (7, CUDA, CUCH(cudaMemcpy(ptr, (*samples)[0].get(), size, cudaMemcpyDefault), kmcudaMemoryCopyError);)"}
{"predict": "(51, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (33, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_B), sizeof(data_type) * B.size()));); (65, CUDA, CUDA_CHECK(cudaDeviceReset());); (62, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (29, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (57, CUDA, CUDA_CHECK(cudaFree(d_B));); (55, CUDA, CUDA_CHECK(cudaFree(d_A));); (22, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)", "label": "(51, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (33, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_B), sizeof(data_type) * B.size()));); (61, CUDA, CUDA_CHECK(cudaFree(d_B));); (66, CUDA, CUDA_CHECK(cudaDeviceReset());); (63, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (28, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (56, CUDA, CUDA_CHECK(cudaFree(d_A));); (22, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(48, OpenMP, EXPECT_TRUE(check_test_image(output_file, t_apcomp_baseline_dir()));); (8, MPI, MPI_Comm_rank(comm, &par_rank);); (10, MPI, apcomp::mpi_comm(MPI_Comm_c2f(comm));); (46, MPI, MPI_Barrier(comm);); (7, MPI, MPI_Comm_size(comm, &par_size);)", "label": "(47, OpenMP, EXPECT_TRUE(check_test_image(output_file, t_apcomp_baseline_dir()));); (9, MPI, MPI_Comm_rank(comm, &par_rank);); (10, MPI, apcomp::mpi_comm(MPI_Comm_c2f(comm));); (8, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(13, CUDA, cudaMalloc((void **)&d_arr, sizeof(int) * len);); (16, CUDA, cudaMemcpy(d_arr, arr, sizeof(int) * len, cudaMemcpyHostToDevice);); (53, CUDA, cudaFree(d_out);); (11, CUDA, cudaMalloc((void **)&d_out, sizeof(int) * len);); (23, CUDA, cudaMemcpy(out, d_out, sizeof(int) * len, cudaMemcpyDeviceToHost);); (49, CUDA, cudaFree(d_arr);)", "label": "(13, CUDA, cudaMalloc((void **)&d_arr, sizeof(int) * len);); (16, CUDA, cudaMemcpy(d_arr, arr, sizeof(int) * len, cudaMemcpyHostToDevice);); (53, CUDA, cudaFree(d_out);); (11, CUDA, cudaMalloc((void **)&d_out, sizeof(int) * len);); (23, CUDA, cudaMemcpy(out, d_out, sizeof(int) * len, cudaMemcpyDeviceToHost);); (49, CUDA, cudaFree(d_arr);)"}
{"predict": "(3, OpenMP, int* shared_ptr = (int*)omp_target_alloc(N*sizeof(int), omp_get_default_device());); (4, OpenMP, #pragma omp target teams distribute parallel for device(0))", "label": "(4, OpenMP, int *shared_ptr = llvm_omp_target_alloc_shared(N * sizeof(int), device);); (5, OpenMP, #pragma omp target teams distribute parallel for device(device)); (0, OpenMP, const int device = omp_get_default_device();); (13, OpenMP, omp_target_free(shared_ptr, device);)"}
{"predict": "(58, MPI, t1 = MPI_Wtime();); (63, MPI, t2 = MPI_Wtime();)", "label": "(63, MPI, t2 = MPI_Wtime();); (58, MPI, t1 = MPI_Wtime();)"}
{"predict": "(138, CUDA, int accepted = thrust::count_if(flags.begin(), flags.end(), thrust::identity<bool>());)", "label": "(184, CUDA, long accepted  = thrust::count_if(flags.begin(), flags.end(), thrust::identity<bool>());)"}
{"predict": "(34, MPI, MPI_Comm_size(MPI_COMM_WORLD,&mpi_size);); (34, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&mpi_rank);)", "label": "(34, MPI, MPI_Comm_size(MPI_COMM_WORLD,&mpi_size);); (34, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&mpi_rank);)"}
{"predict": "(54, CUDA, CUDA_CHECK(cudaEventRecord(stop, 0));); (26, CUDA, CUDA_CHECK(cudaDeviceSetLimit(cudaLimitStackSize, 4 * 1024));); (55, CUDA, CUDA_CHECK(cudaEventSynchronize(stop));); (25, CUDA, CUDA_CHECK(cudaDeviceSetLimit(cudaLimitMallocHeapSize, heap_size));); (10, CUDA, CUDA_CHECK(cudaDeviceReset());); (46, CUDA, CUDA_CHECK(cudaDeviceSynchronize());); (54, CUDA, CUDA_CHECK(cudaEventElapsedTime(&milliseconds, start, stop));); (7, CUDA, CUDA_CHECK(cudaSetDevice(0));); (39, CUDA, CUDA_CHECK(cudaMallocManaged(&instances_data, num_instances * sizeof(CuEVM::evm_instance_t)));); (21, CUDA, CUDA_CHECK(cudaEventRecord(start, 0));); (69, CUDA, CUDA_CHECK(cudaFree(instances_data));)", "label": "(56, CUDA, cudaEventRecord(stop);); (22, CUDA, CUDA_CHECK(cudaDeviceSetLimit(cudaLimitStackSize, 2 * 1024));); (55, CUDA, cudaEventSynchronize(stop);); (58, CUDA, cudaEventDestroy(start);); (5, CUDA, CUDA_CHECK(cudaDeviceReset());); (21, CUDA, CUDA_CHECK(cudaDeviceSynchronize());); (46, CUDA, cudaEventRecord(start);); (55, CUDA, cudaEventDestroy(stop);); (57, CUDA, CUDA_CHECK(cudaGetLastError());); (8, CUDA, cudaDeviceGetLimit(&size_value, cudaLimitStackSize);); (12, CUDA, CUDA_CHECK(cudaDeviceSetLimit(cudaLimitMallocHeapSize, heap_size));); (40, CUDA, cudaEventCreate(&stop);); (46, CUDA, cudaEventElapsedTime(&milliseconds, start, stop);); (37, CUDA, cudaEventCreate(&start);)"}
{"predict": "(19, OpenMP, #pragma omp parallel for reduction(+:s)); (10, OpenMP, #pragma omp parallel for)", "label": "(19, OpenMP, #pragma omp parallel for reduction(+:s)); (10, OpenMP, #pragma omp parallel for)"}
{"predict": "(30, MPI, MPI_Waitall(2, requests, MPI_STATUSES_IGNORE);); (32, MPI, MPI_Recv(&a, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (20, MPI, MPI_Recv(&a, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (16, MPI, MPI_Start(&requests[0]);); (21, MPI, MPI_Waitall(2, requests, MPI_STATUSES_IGNORE);); (17, MPI, MPI_Send(&a, 1, MPI_INT, 1, 0, MPI_COMM_WORLD);); (28, MPI, MPI_Request_free(&requests[0]);); (28, MPI, MPI_Request_free(&requests[1]);); (3, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (11, MPI, MPI_Request_create(MPI_BOTTOM, 0, &requests[1]);); (9, MPI, MPI_Request_create(MPI_BOTTOM, 0, &requests[0]);)", "label": "(22, MPI, MPI_Start(&requests[0]);); (17, MPI, MPI_Barrier_init(MPI_COMM_WORLD, MPI_INFO_NULL, &requests[1]);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (14, MPI, MPI_Bcast_init(&a, 1, MPI_INT, 0, MPI_COMM_WORLD, MPI_INFO_NULL, &requests[0]);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (18, MPI, MPI_Start(&requests[1]);); (23, MPI, MPI_Waitall(2, requests, MPI_STATUSES_IGNORE);); (5, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (27, MPI, MPI_Request_free(&requests[0]);); (27, MPI, MPI_Request_free(&requests[1]);)"}
{"predict": "(11, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:sum1) reduction(+:sum1) collapse(2))", "label": "(11, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:sum1) reduction(+:sum1) collapse(2))"}
{"predict": "(16, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (22, TBB, serial_tiled_time = (tbb::tick_count::now() - t0).seconds();); (27, TBB, tbb_time = (tbb::tick_count::now() - t0).seconds();); (15, TBB, serial_time = (tbb::tick_count::now() - t0).seconds();)", "label": "(16, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (22, TBB, serial_tiled_time = (tbb::tick_count::now() - t0).seconds();); (27, TBB, tbb_time = (tbb::tick_count::now() - t0).seconds();); (15, TBB, serial_time = (tbb::tick_count::now() - t0).seconds();)"}
{"predict": "(29, CUDA, cudaMallocHost((void**)&h_aPinned, bytes);); (29, CUDA, cudaMallocHost((void**)&h_bPinned, bytes);); (27, CUDA, cudaMallocHost((void**)&h_aCached, bytes);); (116, CUDA, cudaFree(d_b);); (42, CUDA, cudaGetDeviceProperties(&prop, deviceID);); (113, CUDA, cudaFreeHost(h_bPinned);); (111, CUDA, cudaFreeHost(h_aPinned);); (22, CUDA, cudaMalloc((void**)&d_b, bytes);); (0, CUDA, cudaGetDeviceCount(&count);); (111, CUDA, cudaFreeHost(h_aCached);); (17, CUDA, cudaMalloc((void**)&d_a, bytes);); (107, CUDA, cudaFree(d_a);); (103, CUDA, cudaFreeHost(h_bCached);); (108, CUDA, cudaFreeHost(h_bPageable);); (-5, CUDA, cudaSetDevice(deviceID);); (103, CUDA, cudaFreeHost(h_aPageable);); (15, CUDA, cudaMallocHost((void**)&h_bCached, bytes);)", "label": "(50, CUDA, cudaGetDeviceProperties(&prop, 0);); (125, CUDA, cudaFree(h_aCached);); (29, CUDA, cudaMallocHost((void**)&h_aCached, bytes);); (119, CUDA, cudaFreeHost(h_aPinned);); (119, CUDA, cudaFreeHost(h_bPinned);); (29, CUDA, cudaHostAlloc((void**)&h_bPinned, bytes, cudaHostAllocWriteCombined);); (24, CUDA, cudaMalloc((void**)&d_b, bytes);); (26, CUDA, cudaHostAlloc((void**)&h_aPinned, bytes, cudaHostAllocWriteCombined);); (24, CUDA, cudaMallocHost((void**)&h_bCached, bytes);); (116, CUDA, cudaFree(d_b);); (-1, CUDA, cudaSetDevice(0);); (113, CUDA, cudaFree(d_a);); (115, CUDA, cudaFree(h_bCached);); (-10, CUDA, cudaGetDeviceCount(&count);); (-2, CUDA, cudaGetDevice(&deviceID);); (14, CUDA, cudaMalloc((void**)&d_a, bytes);)"}
{"predict": "(17, MPI, MPI_Send(buffer, 10, MPI_INT, left, 1, MPI_COMM_WORLD);); (18, MPI, MPI_Recv(buffer, 10, MPI_INT, right, 1, MPI_COMM_WORLD, &status);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myid);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &numprocs);); (14, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc, &argv);); (11, MPI, MPI_Send(buffer, 10, MPI_INT, right, 2, MPI_COMM_WORLD);); (12, MPI, MPI_Recv(buffer, 10, MPI_INT, left, 2, MPI_COMM_WORLD, &status);)", "label": "(9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myid);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &numprocs);); (16, MPI, MPI_Finalize();); (4, MPI, MPI_Init(&argc,&argv);); (12, MPI, MPI_Sendrecv_replace(buffer, 10, MPI_INT, left, 123, right, 123, MPI_COMM_WORLD, &status);)"}
{"predict": "(3, TBB, REMARK(\"is_large_object<int>::value=%d\\n\", tbb::interface6::internal::is_large_object<int>::value);); (4, TBB, REMARK(\"is_large_object<int *>::value=%d\\n\", tbb::interface6::internal::is_large_object<int *>::value);); (2, TBB, REMARK(\"is_large_object<double>::value=%d\\n\", tbb::interface6::internal::is_large_object<double>::value);); (4, TBB, REMARK(\"is_large_object<check_type<int>* >::value=%d\\n\", tbb::interface6::internal::is_large_object<check_type<int>* >::value);); (2, TBB, REMARK(\"is_large_object<check_type<int> >::value=%d\\n\", tbb::interface6::internal::is_large_object<check_type<int> >::value);); (3, TBB, REMARK(\"is_large_object<check_type<short> >::value=%d\\n\\n\", tbb::interface6::internal::is_large_object<check_type<short> >::value);); (8, TBB, tbb::task_scheduler_init init(nthread);)", "label": "(3, TBB, REMARK(\"is_large_object<int>::value=%d\\n\", tbb::interface6::internal::is_large_object<int>::value);); (4, TBB, REMARK(\"is_large_object<int *>::value=%d\\n\", tbb::interface6::internal::is_large_object<int *>::value);); (2, TBB, REMARK(\"is_large_object<double>::value=%d\\n\", tbb::interface6::internal::is_large_object<double>::value);); (4, TBB, REMARK(\"is_large_object<check_type<int>* >::value=%d\\n\", tbb::interface6::internal::is_large_object<check_type<int>* >::value);); (2, TBB, REMARK(\"is_large_object<check_type<int> >::value=%d\\n\", tbb::interface6::internal::is_large_object<check_type<int> >::value);); (3, TBB, REMARK(\"is_large_object<check_type<short> >::value=%d\\n\\n\", tbb::interface6::internal::is_large_object<check_type<short> >::value);); (8, TBB, tbb::task_scheduler_init init(nthread);)"}
{"predict": "(46, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_ARE_FATAL);); (21, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (21, MPI, MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);); (18, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (38, MPI, MPI_Error_string(ierr, str, &slen);)", "label": "(47, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_ARE_FATAL);); (21, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (21, MPI, MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);); (18, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (38, MPI, MPI_Error_string(ierr, str, &slen);)"}
{"predict": "(15, OpenMP, #pragma omp parallel); (17, OpenMP, int i = omp_get_thread_num();)", "label": "(26, OpenMP, #pragma omp barrier); (19, OpenMP, common::nvtx::range omp_scope(\"interruptible::thread-%d\", i);); (13, OpenMP, omp_set_dynamic(0);); (14, OpenMP, #pragma omp parallel reduction(+ : n_finished) reduction(+ : n_cancelled) num_threads(n_threads)); (12, OpenMP, omp_set_num_threads(n_threads);); (14, OpenMP, auto i = omp_get_thread_num();)"}
{"predict": "(35, CUDA, cudaFree(d_ret);); (7, CUDA, cudaMallocHost(&h_stride, STRIDE_MEM_SIZE);); (14, CUDA, cudaMalloc(&d_ret, sizeof(uint32_t));); (31, CUDA, cudaFree(d_stride);); (32, CUDA, cudaFree(d_clk);); (32, CUDA, cudaFreeHost(h_stride);); (26, CUDA, cudaMemcpy(h_clk, d_clk, 32 * sizeof(uint32_t), cudaMemcpyDeviceToHost);); (10, CUDA, cudaMemcpy(d_stride, h_stride, STRIDE_MEM_SIZE, cudaMemcpyHostToDevice);); (7, CUDA, cudaMalloc(&d_stride, STRIDE_MEM_SIZE);); (11, CUDA, cudaMalloc(&d_clk, 32 * sizeof(uint32_t));)", "label": "(35, CUDA, cudaFree(d_ret);); (7, CUDA, cudaMallocHost(&h_stride, STRIDE_MEM_SIZE);); (14, CUDA, cudaMalloc(&d_ret, sizeof(uint32_t));); (31, CUDA, cudaFree(d_stride);); (32, CUDA, cudaFree(d_clk);); (32, CUDA, cudaFreeHost(h_stride);); (25, CUDA, cudaMemcpy(h_clk, d_clk, 32 * sizeof(uint32_t), cudaMemcpyDeviceToHost);); (10, CUDA, cudaMemcpy(d_stride, h_stride, STRIDE_MEM_SIZE, cudaMemcpyHostToDevice);); (7, CUDA, cudaMalloc(&d_stride, STRIDE_MEM_SIZE);); (11, CUDA, cudaMalloc(&d_clk, 32 * sizeof(uint32_t));)"}
{"predict": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(17, CUDA, extern __shared__ _VOLATILE_ float sMem[];)", "label": "(17, CUDA, extern __shared__ _VOLATILE_ float sMem[];)"}
{"predict": "(36, CUDA, CHECK(cudaMalloc(&d_y, size));); (38, CUDA, CHECK(cudaEventCreate(&stop));); (41, CUDA, CHECK(cudaEventRecord(stop));); (37, CUDA, CHECK(cudaMemcpy(h_y, d_y, size, cudaMemcpyDefault));); (27, CUDA, CHECK(cudaMalloc(&d_x, M));); (27, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyDefault));); (38, CUDA, CHECK(cudaEventSynchronize(stop));); (38, CUDA, CHECK(cudaEventElapsedTime(&curr_time, start, stop));); (69, CUDA, CHECK(cudaFree(d_y));); (24, CUDA, CHECK(cudaEventCreate(&start));); (39, CUDA, CHECK(cudaMemcpy(h_y, d_y, size, cudaMemcpyDefault));); (66, CUDA, CHECK(cudaFree(d_x));); (23, CUDA, CHECK(cudaGetLastError());); (24, CUDA, CHECK(cudaEventRecord(start));)", "label": "(6, CUDA, CHECK(cudaMemcpyFromSymbol(&block_size, BLOCK_DIM, sizeof(real)));); (34, CUDA, CHECK(cudaEventCreate(&stop));); (43, CUDA, CHECK(cudaEventRecord(stop));); (78, CUDA, CHECK(cudaFree(d_y));); (33, CUDA, cudaEventQuery(start);); (41, CUDA, CHECK(cudaEventSynchronize(stop));); (41, CUDA, CHECK(cudaEventElapsedTime(&curr_time, start, stop));); (34, CUDA, CHECK(cudaMemcpy(h_y, d_y, size, cudaMemcpyDefault));); (28, CUDA, CHECK(cudaEventRecord(start));); (20, CUDA, CHECK(cudaMalloc(&d_x, M));); (70, CUDA, CHECK(cudaFree(d_x));); (20, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyDefault));); (30, CUDA, CHECK(cudaGetLastError());); (21, CUDA, CHECK(cudaEventCreate(&start));); (16, CUDA, CHECK(cudaMalloc(&d_y, size));)"}
{"predict": "(28, MPI, MPI_Type_get_extent(fd->filetype, &lb, &filetype_extent);); (28, MPI, MPI_Type_size_x(fd->filetype, &filetype_size);)", "label": "(28, MPI, MPI_Type_get_extent(fd->filetype, &lb, &filetype_extent);); (28, MPI, MPI_Type_size_x(fd->filetype, &filetype_size);)"}
{"predict": "(60, CUDA, CHECK_ERROR(cudaFree(device_params));); (12, CUDA, CHECK_ERROR(cudaMalloc((void **)&device_signals, complex_size_bytes));); (50, CUDA, CHECK_ERROR(cudaMemcpy(output_signals, device_signals, complex_size_bytes, cudaMemcpyDeviceToHost));); (56, CUDA, CHECK_ERROR(cudaFree(device_signals));); (34, CUDA, CHECK_ERROR(cudaMemcpyFromSymbol(&host_callback_ptr, device_callback_ptr, sizeof(host_callback_ptr)));); (9, CUDA, CHECK_ERROR(cudaMemcpy(device_signals, input_signals, complex_size_bytes, cudaMemcpyHostToDevice));); (17, CUDA, CHECK_ERROR(cudaMalloc((void **)&device_params, sizeof(cb_params)));); (17, CUDA, CHECK_ERROR(cudaMemcpy(device_params, &host_params, sizeof(cb_params), cudaMemcpyHostToDevice));)", "label": "(60, CUDA, CHECK_ERROR(cudaFree(device_params));); (12, CUDA, CHECK_ERROR(cudaMalloc((void **)&device_signals, complex_size_bytes));); (50, CUDA, CHECK_ERROR(cudaMemcpy(output_signals, device_signals, complex_size_bytes, cudaMemcpyDeviceToHost));); (56, CUDA, CHECK_ERROR(cudaFree(device_signals));); (34, CUDA, CHECK_ERROR(cudaMemcpyFromSymbol(&host_callback_ptr, device_callback_ptr, sizeof(host_callback_ptr)));); (9, CUDA, CHECK_ERROR(cudaMemcpy(device_signals, input_signals, complex_size_bytes, cudaMemcpyHostToDevice));); (17, CUDA, CHECK_ERROR(cudaMalloc((void **)&device_params, sizeof(cb_params)));); (17, CUDA, CHECK_ERROR(cudaMemcpy(device_params, &host_params, sizeof(cb_params), cudaMemcpyHostToDevice));)"}
{"predict": "(37, CUDA, __shared__ float sub_results[x128::size][bdx][bdy];)", "label": "(35, CUDA, __shared__ float sub_results[x128::size][32][bdy];)"}
{"predict": "(80, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (80, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)", "label": "(80, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (80, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)"}
{"predict": "(8, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (9, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;)", "label": "(8, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (9, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;)"}
{"predict": "(23, OpenMP, #pragma omp task); (11, OpenMP, #pragma omp parallel); (12, OpenMP, #pragma omp single); (23, OpenMP, printf (\"Task %i sleeping in thread %i\\n\", myi, omp_get_thread_num());); (28, OpenMP, printf (\"Task %i finished in thread %i\\n\", myi, omp_get_thread_num());)", "label": "(29, OpenMP, #pragma omp taskwait); (22, OpenMP, #pragma omp task); (9, OpenMP, #pragma omp parallel); (10, OpenMP, #pragma omp single)"}
{"predict": "(8, MPI, MPI_Comm_rank(comm, &rank);); (48, MPI, MPI_Allreduce(&index, &scr_groupdesc_count, 1, MPI_INT, MPI_SUM, comm);); (109, MPI, MPI_Allreduce(&all_valid, &scr_all_groupdescs_valid, 1, MPI_INT, MPI_LAND, comm);)", "label": "(7, MPI, MPI_Comm_rank(comm, &rank);); (49, MPI, MPI_Bcast(&num_groups, 1, MPI_INT, 0, comm);)"}
{"predict": "(6, CUDA, extern __shared__ float shared[];)", "label": "(6, CUDA, extern __shared__ float shared[];)"}
{"predict": "(35, CUDA, cudaFree(control_d);); (25, CUDA, cudaDeviceSynchronize();); (32, CUDA, cudaFree(state_der_d);); (23, CUDA, CudaCheckError();); (29, CUDA, cudaFree(state_d);)", "label": "(34, CUDA, cudaFree(control_d);); (32, CUDA, cudaFree(state_der_d);); (22, CUDA, CudaCheckError();); (29, CUDA, cudaFree(state_d);)"}
{"predict": "(37, TBB, tbb::spin_mutex::scoped_lock lock (MyMutex);)", "label": "(37, TBB, tbb::spin_mutex::scoped_lock lock (MyMutex);)"}
{"predict": "(5, OpenMP, #pragma omp target teams distribute parallel for collapse (); (12, OpenMP, #pragma omp target teams distribute parallel for collapse (4); (15, OpenMP, #pragma omp target teams distribute parallel for collapse (2+2))); (-2, OpenMP, #pragma omp target teams distribute parallel for collapse); (43, OpenMP, #pragma omp target teams distribute parallel for collapse(collapse(tmain<int, char, -1, -2>(argc, argv)); (23, OpenMP, #pragma omp target teams distribute parallel for collapse (foobool(argc)), collapse (true), collapse (-5)); (44, OpenMP, #pragma omp target teams distribute parallel for collapse (2)); (25, OpenMP, #pragma omp target teams distribute parallel for collapse (S1)); (1, OpenMP, #pragma omp target teams distribute parallel for collapse ()); (12, OpenMP, #pragma omp target teams distribute parallel for collapse (foobool(1) > 0 ? 1 : 2))", "label": "(47, OpenMP, #pragma omp target teams distribute parallel for collapse(collapse(tmain<int, char, -1, -2>(argc, argv)); (4, OpenMP, #pragma omp target teams distribute parallel for collapse (); (7, OpenMP, #pragma omp target teams distribute parallel for collapse ()); (18, OpenMP, #pragma omp target teams distribute parallel for collapse (foobool(1) > 0 ? 1 : 2)); (13, OpenMP, #pragma omp target teams distribute parallel for collapse (2+2))); (23, OpenMP, #pragma omp target teams distribute parallel for collapse (foobool(argc)), collapse (true), collapse (-5)); (7, OpenMP, #pragma omp target teams distribute parallel for collapse (4); (25, OpenMP, #pragma omp target teams distribute parallel for collapse (S1)); (-7, OpenMP, #pragma omp target teams distribute parallel for collapse); (41, OpenMP, #pragma omp target teams distribute parallel for collapse (2))"}
{"predict": "(32, CUDA, cudaFree(resourceList->outputBufferGPU);); (27, CUDA, cudaFree(resourceList->inputBufferGPU);); (44, CUDA, cudaStreamDestroy(resourceList->stream);)", "label": "(32, CUDA, cudaFree(resourceList->outputBufferGPU);); (27, CUDA, cudaFree(resourceList->inputBufferGPU);); (44, CUDA, cudaStreamDestroy(resourceList->stream);)"}
{"predict": "(66, CUDA, EXPECT_EQ(cudaSuccess, cudaMemset(input->basePtr(), 0xFF, inSampleStride * inAccess->numSamples()));); (2, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (74, CUDA, EXPECT_EQ(cudaSuccess, cudaMemcpy(test.data(), output->basePtr(), outBufSize, cudaMemcpyDeviceToHost));); (64, CUDA, EXPECT_EQ(cudaSuccess, cudaMemset(output->basePtr(), 0xFF, outSampleStride * outAccess->numSamples()));); (70, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (71, CUDA, EXPECT_EQ(cudaSuccess, cudaMemcpy(testIn.data(), input->basePtr(), inBufSize, cudaMemcpyDeviceToHost));); (78, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamDestroy(stream));)", "label": "(77, CUDA, EXPECT_EQ(cudaSuccess, cudaMemcpy(test.data(), output->basePtr(), outBufSize, cudaMemcpyDeviceToHost));); (2, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (64, CUDA, EXPECT_EQ(cudaSuccess, cudaMemset(input->basePtr(), 0xFF, inSampleStride * inAccess->numSamples()));); (72, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (72, CUDA, EXPECT_EQ(cudaSuccess, cudaMemcpy(testIn.data(), input->basePtr(), inBufSize, cudaMemcpyDeviceToHost));); (62, CUDA, EXPECT_EQ(cudaSuccess, cudaMemset(output->basePtr(), 0xFF, outSampleStride * outAccess->numSamples()));); (78, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamDestroy(stream));)"}
{"predict": "(40, OpenMP, #pragma omp teams reduction(+:sum)); (37, OpenMP, #pragma omp target map(tofrom: b, sum, t) map(alloc: a)); (40, OpenMP, #pragma omp distribute parallel for reduction(+:sum)); (42, OpenMP, t[i] = omp_get_team_num();)", "label": "(40, OpenMP, #pragma omp teams reduction(+:sum)); (37, OpenMP, #pragma omp target map(tofrom: b, sum, t) map(alloc: a)); (40, OpenMP, #pragma omp distribute parallel for reduction(+:sum)); (42, OpenMP, t[i] = omp_get_team_num();)"}
{"predict": "(4, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (5, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;)", "label": "(4, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (5, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;)"}
{"predict": "(4, CUDA, __shared__ float sdata[THREAD_PER_BLOCK];)", "label": "(4, CUDA, __shared__ float sdata[THREAD_PER_BLOCK];)"}
{"predict": "(2, CUDA, __shared__ float smem[THREADS_PER_BLOCK];)", "label": "(2, CUDA, __shared__ float smem[1024];)"}
{"predict": "(38, CUDA, CUDASimulation cudaSimulation(model);)", "label": "(38, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(29, CUDA, checkCudaErrors(cudaSetDevice(gpuID));)", "label": "(29, CUDA, checkCudaErrors(cudaSetDevice(gpuID));)"}
{"predict": "(6, TBB, c.set_core_type(tbb::info::core_types().back());); (3, TBB, tbb::task_arena::constraints c;); (4, TBB, c.set_numa_id(tbb::info::numa_nodes()[0]);); (19, TBB, tbb::tick_count t3 = tbb::tick_count::now();); (17, TBB, tbb::tick_count t2 = tbb::tick_count::now();); (20, TBB, double v4 = tbb::tick_count::now() - t5;); (2, TBB, c.set_max_concurrency(std::min(8, tbb::info::default_concurrency(c)));); (11, TBB, tbb::tick_count ts_1 = tbb::tick_count::now();); (14, TBB, double v1 = tbb::tick_count::now() - t1;); (1, TBB, c.set_max_concurrency(1);); (-3, TBB, tbb::task_arena a(c);); (5, TBB, tbb::tick_count t1 = tbb::tick_count::now();); (11, TBB, double v0 = tbb::tick_count::now() - ts_0;); (2, TBB, tbb::tick_count ts_0 = tbb::tick_count::now();); (13, TBB, double v2 = tbb::tick_count::now() - t3;); (10, TBB, double v3 = tbb::tick_count::now() - t2;); (6, TBB, tbb::tick_count t4 = tbb::tick_count::now();)", "label": "(27, TBB, tbb::tick_count t1 = tbb::tick_count::now();); (23, TBB, reducePiExample<tbb::auto_partitioner>(num_intervals, gs);); (6, TBB, c.set_max_concurrency(std::min(8, tbb::info::default_concurrency(c)));); (29, TBB, double v3 = deterministicReducePiExample<tbb::simple_partitioner>(num_intervals, gs);); (27, TBB, tbb::tick_count t3 = tbb::tick_count::now();); (-1, TBB, tbb::task_arena::constraints c;); (11, TBB, tbb::tick_count ts_1 = tbb::tick_count::now();); (18, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (27, TBB, tbb::tick_count t5 = tbb::tick_count::now();); (25, TBB, double v4 = deterministicReducePiExample<tbb::static_partitioner>(num_intervals, gs);); (5, TBB, tbb::tick_count ts_0 = tbb::tick_count::now();); (-2, TBB, tbb::task_arena a(c);); (-6, TBB, c.set_core_type(tbb::info::core_types().back());); (20, TBB, tbb::tick_count t4 = tbb::tick_count::now();); (14, TBB, double v1 = reducePiExample<tbb::simple_partitioner>(num_intervals, gs);); (15, TBB, double v2 = reducePiExample<tbb::static_partitioner>(num_intervals, gs);); (-11, TBB, c.set_numa_id(tbb::info::numa_nodes()[0]);); (12, TBB, tbb::tick_count t2 = tbb::tick_count::now();); (8, TBB, double v0 = reducePiExample<tbb::auto_partitioner>(num_intervals, gs);)"}
{"predict": "(13, CUDA, __shared__ alignas(alignof(float4)) float Bs[BLOCK_SIZE][BLOCK_SIZE];); (8, CUDA, __shared__ alignas(alignof(float4)) float As[BLOCK_SIZE][BLOCK_SIZE];)", "label": "(13, CUDA, __shared__ alignas(alignof(float4)) float Bs[BLOCK_SIZE][BLOCK_SIZE];); (8, CUDA, __shared__ alignas(alignof(float4)) float As[BLOCK_SIZE][BLOCK_SIZE];)"}
{"predict": "(78, OpenMP, lis2mdl_offset_temp_comp_set(&mag_ctx, PROPERTY_ENABLE);)", "label": "(78, OpenMP, lis2mdl_offset_temp_comp_set(&mag_ctx, PROPERTY_ENABLE);)"}
{"predict": "(8, OpenMP, num = omp_get_device_num();); (1, OpenMP, int num_devices = omp_get_num_devices();); (4, OpenMP, #pragma omp target device(i) map(from:num)); (8, OpenMP, #pragma omp target device(num) map(from:num))", "label": "(2, OpenMP, const int num_devices = omp_get_num_devices();); (6, OpenMP, #pragma omp target map(from : num) device(i)); (6, OpenMP, num = omp_get_device_num();)"}
{"predict": "(55, OpenCL, clReleaseEvent(clEvent);)", "label": "(55, OpenCL, clReleaseEvent(clEvent);)"}
{"predict": "(13, TBB, tbb::spin_mutex::scoped_lock lock(mutex);)", "label": "(9, TBB, oneapi::tbb::spin_mutex::scoped_lock l(locks[u]);); (20, TBB, oneapi::tbb::spin_mutex::scoped_lock l(locks[v]);)"}
{"predict": "(23, CUDA, __shared__ float v[_N_], r[_N_], k[_N_], gy[_N_];)", "label": "(22, CUDA, __shared__ float v[_N_], r[_N_], k[_N_], gy[_N_];)"}
{"predict": "(93, OpenMP, #pragma omp target teams distribute parallel for); (86, OpenMP, #pragma omp target parallel for)", "label": "(93, OpenMP, #pragma omp target teams distribute parallel for); (86, OpenMP, #pragma omp target parallel for)"}
{"predict": "(48, CUDA, CubDebugExit(cudaMemcpy(d_flags, h_flags, sizeof(unsigned char) * num_items, cudaMemcpyHostToDevice));); (46, CUDA, CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));)", "label": "(48, CUDA, CubDebugExit(cudaMemcpy(d_flags, h_flags, sizeof(unsigned char) * num_items, cudaMemcpyHostToDevice));); (46, CUDA, CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));)"}
{"predict": "(6, CUDA, __shared__ uint s_key[SHARED_SIZE_LIMIT];); (6, CUDA, __shared__ uint s_val[SHARED_SIZE_LIMIT];)", "label": "(6, CUDA, __shared__ uint s_key[SHARED_SIZE_LIMIT];); (6, CUDA, __shared__ uint s_val[SHARED_SIZE_LIMIT];)"}
{"predict": "(14, MPI, MPI_Type_vector(COUNT, BLOCKLENGTH, STRIDE, MPI_INT, &dt_vector);); (11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (35, MPI, MPI_Type_free(&dt_vector);); (28, MPI, MPI_Type_commit(&dt_noncontig);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (33, MPI, MPI_Op_free(&op);); (8, MPI, MPI_Type_create_hvector(COUNT, BLOCKLENGTH, STRIDE * sizeof(int), dt_vector, &dt_noncontig);)", "label": "(38, MPI, MPI_Op_free(&op);); (16, MPI, MPI_Type_free(&dt_vector);); (35, MPI, MPI_Type_free(&dt_noncontig);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (13, MPI, MPI_Type_commit(&dt_noncontig);); (9, MPI, MPI_Type_vector(1, 1, noncontig_stride, MPI_INT, &dt_vector);); (9, MPI, MPI_Type_create_resized(dt_vector, 0, sizeof(int) * noncontig_stride, &dt_noncontig);)"}
{"predict": "(13, CUDA, CUDA_CHECK(cudaGetDeviceProperties(&props, 0));)", "label": "(13, CUDA, CUDA_CHECK(cudaGetDeviceProperties(&props, 0));)"}
{"predict": "(19, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (25, TBB, serial_tiled_time = (tbb::tick_count::now() - t0).seconds();); (30, TBB, tbb_time = (tbb::tick_count::now() - t0).seconds();); (18, TBB, serial_time = (tbb::tick_count::now() - t0).seconds();)", "label": "(19, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (25, TBB, serial_tiled_time = (tbb::tick_count::now() - t0).seconds();); (30, TBB, tbb_time = (tbb::tick_count::now() - t0).seconds();); (18, TBB, serial_time = (tbb::tick_count::now() - t0).seconds();)"}
{"predict": "(52, CUDA, flamegpu::CUDASimulation  cudaSimulation(model);)", "label": "(52, CUDA, flamegpu::CUDASimulation cudaSimulation(model);)"}
{"predict": "(24, CUDA, checkCudaErrors(cudaMalloc((void **)&bLines_d, N_LINES * sizeof(BezierLine)));); (38, CUDA, checkCudaErrors(cudaFree(bLines_d));); (29, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (27, CUDA, getLastCudaError(\"computeBezierLinesCDP() execution failed\\n\");)", "label": "(24, CUDA, checkCudaErrors(cudaMalloc((void **)&bLines_d, N_LINES * sizeof(BezierLine)));); (35, CUDA, checkCudaErrors(cudaFree(bLines_d));)"}
{"predict": "(7, CUDA, __shared__ float shared_sum2[32];); (5, CUDA, __shared__ float shared_sum[32];)", "label": "(7, CUDA, __shared__ float shared_sum2[32];); (5, CUDA, __shared__ float shared_sum[32];)"}
{"predict": "(10, TBB, double serial_time = (tbb::tick_count::now() - t0).seconds();); (7, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (16, TBB, double tbb_time = (tbb::tick_count::now() - t1).seconds();); (13, TBB, tbb::tick_count t1 = tbb::tick_count::now();)", "label": "(10, TBB, double serial_time = (tbb::tick_count::now() - t0).seconds();); (7, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (16, TBB, double tbb_time = (tbb::tick_count::now() - t1).seconds();); (13, TBB, tbb::tick_count t1 = tbb::tick_count::now();)"}
{"predict": "(24, CUDA, CHECK(cudaMalloc(&d_B, M));); (24, CUDA, CHECK(cudaMemcpy(d_A, h_A, M, cudaMemcpyHostToDevice));); (42, CUDA, CHECK(cudaFree(d_B));); (19, CUDA, CHECK(cudaMalloc(&d_A, M));); (28, CUDA, CHECK(cudaMemcpy(h_B, d_B, M, cudaMemcpyDeviceToHost));); (38, CUDA, CHECK(cudaFree(d_A));)", "label": "(24, CUDA, CHECK(cudaMalloc(&d_B, M));); (24, CUDA, CHECK(cudaMemcpy(d_A, h_A, M, cudaMemcpyHostToDevice));); (42, CUDA, CHECK(cudaFree(d_B));); (20, CUDA, CHECK(cudaMalloc(&d_A, M));); (28, CUDA, CHECK(cudaMemcpy(h_B, d_B, M, cudaMemcpyDeviceToHost));); (38, CUDA, CHECK(cudaFree(d_A));)"}
{"predict": "(38, OpenCL, retVal = clReleaseMemObject(clBuffer);)", "label": "(38, OpenCL, retVal = clReleaseMemObject(clBuffer);)"}
{"predict": "(26, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&data, size_bytes));); (20, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamCreate(&stream));); (48, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (46, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (53, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(data));); (50, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamDestroy(stream));)", "label": "(26, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&data, size_bytes));); (19, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamCreate(&stream));); (48, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (46, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (53, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(data));); (53, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamDestroy(stream));)"}
{"predict": "(25, OpenMP, x = omp_target_pointer_translate(x, count * sizeof(float), 0);); (30, OpenMP, fprintf(stderr, \"line %d: [host]   x: %p\\n\", __LINE__, omp_target_pointer_address(x));); (6, OpenMP, #pragma omp target enter data map(alloc:x[0:count])); (13, OpenMP, #pragma omp target map(from:px)); (20, OpenMP, #pragma omp target map(from:x))", "label": "(15, OpenMP, #pragma omp target map(from:px)); (7, OpenMP, #pragma omp target data map(to:x[0:count])); (21, OpenMP, #pragma omp target data use_device_ptr(x)); (26, OpenMP, #pragma omp target data use_device_addr(x[0:0]))"}
{"predict": "(6, OpenMP, double st = omp_get_wtime();); (3, OpenACC, #pragma acc enter data create(out[:w*h])); (12, OpenACC, #pragma acc loop vector); (18, OpenACC, #pragma acc exit data delete(in1,in2) copyout(out[:w*h])); (16, OpenMP, printf(\"Time taken for OpenACC merge(kernel only) with pipelining: %.4f seconds\\n\", omp_get_wtime()-st);); (7, OpenACC, #pragma acc parallel loop gang present(in1, in2,out)); (-1, OpenACC, #pragma acc enter data copyin(in1[:w*h],in2[:h*w]))", "label": "(4, OpenACC, #pragma acc enter data create(in1[:w*h], in2[:h*w], out[:w*h])); (12, OpenACC, #pragma acc loop); (8, OpenACC, #pragma acc update device(in1[lower*w:(upper-lower)*w],in2[lower*w:(upper-lower)*w]) async(block%2)); (17, OpenACC, #pragma acc wait); (7, OpenACC, #pragma acc parallel loop present(in1,in2, out) async(block%2)); (13, OpenACC, #pragma acc update self(out[lower*w:(upper-lower)*w]) async(block%2)); (15, OpenACC, #pragma acc exit data delete(in1, in2, out))"}
{"predict": "(27, MPI, MPI_Win_set_attr(win, key[0], &attrval[0]);); (12, MPI, MPI_Win_create(buf, sizeof(int), sizeof(int), MPI_INFO_NULL, comm, &win);); (23, MPI, MPI_Win_set_attr(win, key[2], &attrval[2]);); (23, MPI, MPI_Win_set_attr(win, key[1], &attrval[1]);); (57, MPI, MPI_Win_free_keyval(&key[i]);); (24, MPI, MPI_Win_delete_attr(win, key[0]);); (24, MPI, MPI_Win_delete_attr(win, key[1]);); (57, MPI, MPI_Win_free(&win);); (24, MPI, MPI_Win_delete_attr(win, key[2]);)", "label": "(27, MPI, MPI_Win_set_attr(win, key[0], &attrval[0]);); (12, MPI, MPI_Win_create(buf, sizeof(int), sizeof(int), MPI_INFO_NULL, comm, &win);); (29, MPI, MPI_Win_delete_attr(win, key[0]);); (30, MPI, MPI_Win_delete_attr(win, key[2]);); (58, MPI, MPI_Win_free_keyval(&key[i]);); (20, MPI, MPI_Win_set_attr(win, key[2], &attrval[2]);); (58, MPI, MPI_Win_free(&win);); (19, MPI, MPI_Win_set_attr(win, key[1], &attrval[1]);); (24, MPI, MPI_Win_delete_attr(win, key[1]);)"}
{"predict": "(3, OpenMP, xomp_get_device_capability(devID, &major, &minor);)", "label": "(14, OpenMP, printf(\"Error: xomp_get_maxThreadBlocksPerMultiprocessor(): unhandled Compute Capability numbers%d.%d \\n\", major, minor);); (3, CUDA, major = xomp_getCudaDeviceProp(devID)-> major;); (3, CUDA, minor = xomp_getCudaDeviceProp(devID)-> minor;)"}
{"predict": "(17, MPI, MPI_Comm_dup(MPI_COMM_SELF, &self_dup);); (24, MPI, MPI_Comm_dup(comm, &self_dup);); (28, MPI, MPI_Comm_free(&comm);); (18, MPI, MPI_Barrier(self_dup);); (22, MPI, MPI_Comm_free(&self_dup);); (2, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (12, MPI, MPI_Comm_dup(&self_dup, &comm);)", "label": "(26, MPI, MPI_Comm_dup(comms[*(int *) arg], &comm);); (4, MPI, MPI_Comm_rank(comms[*(int *) arg], &rank);); (20, MPI, MPI_Comm_free(&self_dup);); (24, MPI, MPI_Comm_free(&comm);); (13, MPI, MPI_Comm_dup(MPI_COMM_SELF, &self_dup);)"}
{"predict": "(7, CUDA, __shared__ float sdata2[THREAD_PER_BLOCK];); (5, CUDA, __shared__ float sdata[THREAD_PER_BLOCK];); (31, CUDA, __shared__ float var;); (29, CUDA, __shared__ float mean;)", "label": "(6, CUDA, __shared__ float sdata[THREAD_PER_BLOCK];); (8, CUDA, __shared__ float var;); (5, CUDA, __shared__ float sdata2[THREAD_PER_BLOCK];); (5, CUDA, __shared__ float mean;)"}
{"predict": "(12, MPI, MPI_Comm_size(team.comm, &nprocs);); (10, MPI, MPI_Comm_rank(team.comm, &rank);)", "label": "(12, MPI, MPI_Comm_rank(team.comm, &rank);); (12, MPI, MPI_Comm_size(team.comm, &nprocs);)"}
{"predict": "(13, OpenCL, error = clEnqueueNDRangeKernel(cmdq, kernel_verify_lists, 1, NULL, &numLists, NULL, 0, NULL, NULL);); (5, OpenCL, cl_int *pNumCorrect = (cl_int *) clEnqueueMapBuffer(cmdq, num_correct, CL_TRUE, CL_MAP_WRITE, 0, sizeof(cl_int), 0, NULL,NULL, &error);); (15, OpenCL, pNumCorrect = (cl_int *) clEnqueueMapBuffer(cmdq, num_correct, CL_TRUE, CL_MAP_READ, 0, sizeof(cl_int), 0, NULL,NULL, &error);); (8, OpenCL, error = clEnqueueUnmapMemObject(cmdq, num_correct, pNumCorrect, 0,NULL,NULL);); (19, OpenCL, error = clReleaseKernel(kernel_verify_lists);)", "label": "(6, OpenCL, cl_int *pNumCorrect = (cl_int*) clEnqueueMapBuffer(cmdq, num_correct, CL_TRUE, CL_MAP_READ | CL_MAP_WRITE, 0, sizeof(cl_int), 0, NULL,NULL, &error);); (10, OpenCL, error = clEnqueueUnmapMemObject(cmdq, num_correct, pNumCorrect, 0,NULL,NULL);); (15, OpenCL, pNumCorrect = (cl_int*) clEnqueueMapBuffer(cmdq, num_correct, CL_TRUE, CL_MAP_READ | CL_MAP_WRITE, 0, sizeof(cl_int), 0, NULL,NULL, &error);); (11, OpenCL, error = clEnqueueNDRangeKernel(cmdq, kernel_verify_lists, 1, NULL, &numLists, NULL, 0, NULL, NULL);)"}
{"predict": "(20, CUDA, cudaMemcpy(array, dArray, size, cudaMemcpyDeviceToHost);); (11, CUDA, cudaMemcpy(dArray, array, size, cudaMemcpyHostToDevice);); (30, CUDA, cudaFree(dArray);); (10, CUDA, cudaMalloc(&dArray, size);)", "label": "(12, CUDA, checkCudaErrors(cudaMemcpy(dArray, array, size, cudaMemcpyHostToDevice));); (10, CUDA, checkCudaErrors(cudaMalloc(&dArray, size));); (19, CUDA, checkCudaErrors(cudaFree(dArray));); (17, CUDA, checkCudaErrors(cudaMemcpy(array, dArray, size, cudaMemcpyDeviceToHost));)"}
{"predict": "(9, TBB, c.set_core_type(tbb::info::core_types().back());); (6, TBB, tbb::task_arena::constraints c;); (37, TBB, tbb::affinity_partitioner aff_p;); (26, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (46, TBB, double static_time = (tbb::tick_count::now() - t0).seconds();); (6, TBB, c.set_numa_id(tbb::info::numa_nodes()[0]);); (36, TBB, double affinity_time = (tbb::tick_count::now() - t0).seconds();); (2, TBB, c.set_max_concurrency(std::min(8, tbb::info::default_concurrency(c)));); (31, TBB, t0 = tbb::tick_count::now();); (25, TBB, double auto_time = (tbb::tick_count::now() - t0).seconds();); (3, TBB, tbb::task_arena arena(c);)", "label": "(9, TBB, c.set_core_type(tbb::info::core_types().back());); (6, TBB, tbb::task_arena::constraints c;); (36, TBB, tbb::affinity_partitioner aff_p;); (5, TBB, c.set_numa_id(tbb::info::numa_nodes()[0]);); (26, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (46, TBB, double static_time = (tbb::tick_count::now() - t0).seconds();); (5, TBB, c.set_max_concurrency(std::min(8, tbb::info::default_concurrency(c)));); (36, TBB, double affinity_time = (tbb::tick_count::now() - t0).seconds();); (31, TBB, t0 = tbb::tick_count::now();); (25, TBB, double auto_time = (tbb::tick_count::now() - t0).seconds();); (2, TBB, tbb::task_arena arena(c);)"}
{"predict": "(14, MPI, MPI_Send(b, 1, newtype, 0, 0, MPI_COMM_WORLD);); (14, MPI, MPI_Recv(d, 1, newtype, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (8, MPI, MPI_Type_contiguous(1, MPI_CHAR, &newtype);)", "label": "(17, MPI, MPI_Sendrecv(b, 1, newtype, 0, 0, d, 0, newtype, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (10, MPI, MPI_Type_commit(&newtype);); (8, MPI, MPI_Type_create_hvector(0, 1, 10, MPI_DOUBLE, &newtype);); (9, MPI, MPI_Type_free(&newtype);); (10, MPI, MPI_Type_indexed(0, b, d, MPI_DOUBLE, &newtype);)"}
{"predict": "(18, CUDA, thrust::device_vector<ValueT> out_values(elements);); (10, CUDA, thrust::device_vector<KeyT> in_keys = data_source;); (14, CUDA, thrust::shuffle(data_source.begin(), data_source.end(), re);); (36, CUDA, cudaDeviceSynchronize();); (17, CUDA, thrust::host_vector<key_value_pair<KeyT, ValueT>> host_values(out_values);); (5, CUDA, thrust::device_vector<ValueT> in_values(elements);); (3, CUDA, thrust::device_vector<KeyT> out_keys(elements);); (10, CUDA, thrust::host_vector<key_value_pair<KeyT, ValueT>> host_keys(out_keys);); (5, CUDA, thrust::default_random_engine re;); (0, CUDA, thrust::sequence(in_values.begin(), in_values.end());); (-3, CUDA, thrust::sequence(in_keys.begin(), in_keys.end());)", "label": "(9, CUDA, thrust::device_vector<std::uint8_t> data_source(elements);); (12, CUDA, thrust::sequence(data_source.begin(), data_source.end());); (12, CUDA, thrust::shuffle(data_source.begin(), data_source.end(), re);); (5, CUDA, thrust::default_random_engine re;); (15, CUDA, thrust::device_vector<ValueT> in_values(data_source);); (17, CUDA, thrust::host_vector<KeyT> host_keys(in_keys);); (17, CUDA, thrust::host_vector<ValueT> host_values(in_values);); (13, CUDA, thrust::device_vector<ValueT> out_values(elements);); (8, CUDA, thrust::device_vector<KeyT> out_keys(elements);); (6, CUDA, thrust::device_vector<KeyT> in_keys(data_source);)"}
{"predict": "(84, OpenCL, sub_buf = clCreateSubBuffer(buf[1], 0, CL_BUFFER_CREATE_TYPE_REGION, &region, &error );); (87, OpenCL, void * map_ptr = clEnqueueMapBuffer(queue, buf[0], 1, CL_MAP_READ, 0, 64, 0, NULL, NULL, NULL);)", "label": "(84, OpenCL, sub_buf = clCreateSubBuffer(buf[1], 0, CL_BUFFER_CREATE_TYPE_REGION, &region, &error );); (87, OpenCL, void * map_ptr = clEnqueueMapBuffer(queue, buf[0], 1, CL_MAP_READ, 0, 64, 0, NULL, NULL, NULL);)"}
{"predict": "(62, CUDA, CHECK(cudaPeekAtLastError());)", "label": "(62, CUDA, CHECK(cudaPeekAtLastError());)"}
{"predict": "(42, CUDA, cudaFree(d_inPtr);); (27, CUDA, cudaMalloc((void**)&d_outPtr, size * sizeof(float));); (31, CUDA, cudaMemcpy(&result_gpu, &d_outPtr[0], sizeof(float), cudaMemcpyDeviceToHost);); (27, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);); (37, CUDA, cudaFree(d_outPtr);); (22, CUDA, cudaMalloc((void**)& d_inPtr, size * sizeof(float));)", "label": "(42, CUDA, cudaFree(d_inPtr);); (27, CUDA, cudaMalloc((void**)&d_outPtr, size * sizeof(float));); (32, CUDA, cudaMemcpy(&result_gpu, &d_outPtr[0], sizeof(float), cudaMemcpyDeviceToHost);); (27, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);); (37, CUDA, cudaFree(d_outPtr);); (22, CUDA, cudaMalloc((void**)& d_inPtr, size * sizeof(float));)"}
{"predict": "(65, CUDA, CHECK_CUDART(cudaFree(indata));); (19, CUDA, CHECK_CUDART(cudaMemcpy(indata, &inval, sizeof(T), cudaMemcpyHostToDevice));); (62, CUDA, CHECK_CUDART(cudaMemcpy(&outval, outdata, sizeof(T), cudaMemcpyDeviceToHost));); (14, CUDA, CHECK_CUDART(cudaMalloc((void**)&indata, sizeof(T)));); (14, CUDA, CHECK_CUDART(cudaMalloc((void**)&outdata, sizeof(T)));); (59, CUDA, CHECK_CUDART(cudaFree(outdata));)", "label": "(65, CUDA, CHECK_CUDART(cudaFree(indata));); (19, CUDA, CHECK_CUDART(cudaMemcpy(indata, &inval, sizeof(T), cudaMemcpyHostToDevice));); (61, CUDA, CHECK_CUDART(cudaMemcpy(&outval, outdata, sizeof(T), cudaMemcpyDeviceToHost));); (14, CUDA, CHECK_CUDART(cudaMalloc((void**)&indata, sizeof(T)));); (14, CUDA, CHECK_CUDART(cudaMalloc((void**)&outdata, sizeof(T)));); (59, CUDA, CHECK_CUDART(cudaFree(outdata));)"}
{"predict": "(4, OpenMP, #pragma omp parallel for)", "label": "(4, OpenMP, #pragma omp parallel for)"}
{"predict": "(29, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (24, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (55, MPI, MPI_Finalize();); (52, MPI, MPI_T_finalize();); (23, MPI, MPI_T_init_thread(required, &provided);); (36, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(29, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (24, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (55, MPI, MPI_Finalize();); (52, MPI, MPI_T_finalize();); (23, MPI, MPI_T_init_thread(required, &provided);); (36, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(1, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)", "label": "(1, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)"}
{"predict": "(14, OpenMP, #pragma omp parallel for)", "label": "(14, OpenMP, #pragma omp parallel for)"}
{"predict": "(32, TBB, tbb::flow::remove_edge( exe_node, *receivers[r] );); (8, TBB, tbb::flow::continue_node< OutputType > exe_node( g, cf );); (9, TBB, exe_node.register_predecessor( *reinterpret_cast< tbb::flow::sender< tbb::flow::continue_msg > * >(&exe_node) );); (21, TBB, tbb::flow::make_edge( exe_node, *receivers[r] );); (36, TBB, body_copy = tbb::flow::copy_body< inc_functor<OutputType> >( exe_node );); (44, TBB, g.reset(tbb::flow::rf_reset_bodies);); (0, TBB, tbb::flow::graph g;); (16, TBB, tbb::flow::continue_msg m;); (16, TBB, exe_node.try_put( m );)", "label": "(46, TBB, g.reset(tbb::flow::rf_reset_bodies);); (21, TBB, tbb::flow::make_edge( exe_node, *receivers[r] );); (7, TBB, tbb::flow::continue_node< OutputType > exe_node( g, cf );); (22, TBB, utils::NativeParallelFor( p, parallel_puts<tbb::flow::continue_msg>(exe_node) );); (36, TBB, body_copy = tbb::flow::copy_body< inc_functor<OutputType> >( exe_node );); (-1, TBB, tbb::flow::graph g;); (29, TBB, tbb::flow::remove_edge( exe_node, *receivers[r] );); (5, TBB, tbb::detail::d2::register_predecessor(exe_node, fake_sender);)"}
{"predict": "(28, MPI, auto t0 = MPI_Wtime();); (44, MPI, auto t1 = MPI_Wtime();); (47, MPI, MPI_Finalize();); (5, MPI, MPI_Init(&argc, &argv);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (36, MPI, MPI_Barrier(MPI_COMM_WORLD);); (22, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(6, MPI, MPI_Init(&argc, &argv);); (47, MPI, MPI_Finalize();); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (38, MPI, double t1 = MPI_Wtime();); (20, MPI, MPI_Barrier(MPI_COMM_WORLD);); (20, MPI, double t0 = MPI_Wtime();)"}
{"predict": "(1, TBB, tbb::task_arena a(2);)", "label": "(1, TBB, tbb::task_arena a(1);); (13, TBB, CHECK(tbb::this_task_arena::max_concurrency() == 2);)"}
{"predict": "(26, OpenMP, #pragma omp teams distribute parallel for proc_bind(master)); (4, OpenMP, #pragma omp target); (15, OpenMP, #pragma omp teams distribute parallel for proc_bind()); (9, OpenMP, #pragma omp teams distribute parallel for proc_bind(); (32, OpenMP, #pragma omp teams distribute parallel for proc_bind(spread)); (16, OpenMP, #pragma omp teams distribute parallel for proc_bind(close), proc_bind(spread)); (-1, OpenMP, #pragma omp teams distribute parallel for proc_bind); (25, OpenMP, #pragma omp parallel proc_bind(close)); (30, OpenMP, #pragma omp teams distribute parallel for proc_bind(master)); (-6, OpenMP, #pragma omp teams distribute parallel for proc_bind()", "label": "(31, OpenMP, #pragma omp teams distribute parallel for proc_bind(master)); (25, OpenMP, #pragma omp teams distribute parallel for proc_bind(x)); (4, OpenMP, #pragma omp teams distribute parallel for proc_bind); (34, OpenMP, #pragma omp teams distribute parallel for proc_bind(spread)); (6, OpenMP, #pragma omp teams distribute parallel for proc_bind(); (9, OpenMP, #pragma omp teams distribute parallel for proc_bind()); (16, OpenMP, #pragma omp teams distribute parallel for proc_bind(close), proc_bind(spread)); (-2, OpenMP, #pragma omp target); (27, OpenMP, #pragma omp parallel proc_bind(close)); (9, OpenMP, #pragma omp teams distribute parallel for proc_bind(master)"}
{"predict": "(86, MPI, Mpi.Instance.mpi_state = MPI_STATE_POST_FINALIZED;)", "label": "(40, MPI, mpi_errno = Mpi.Finalize();); (86, MPI, Mpi.CompleteFinalize();)"}
{"predict": "(41, OpenMP, lis2mdl_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)", "label": "(41, OpenMP, lis2mdl_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)"}
{"predict": "(9, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(9, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(6, CUDA, extern __shared__ float s_data[];)", "label": "(6, CUDA, extern __shared__ float s_data[];)"}
{"predict": "(30, OpenMP, #pragma omp target firstprivate(j)); (21, OpenMP, #pragma omp target firstprivate(h, B::x)); (24, OpenMP, #pragma omp target firstprivate(e, g)); (1, OpenMP, #pragma omp target firstprivate); (24, OpenMP, #pragma omp target firstprivate(m)); (7, OpenMP, #pragma omp target firstprivate(argc > 0 ? argv[1] : argv[2])); (10, OpenMP, #pragma omp target firstprivate(argv[1])); (11, OpenMP, #pragma omp target firstprivate(a, b, c, d, f)); (2, OpenMP, #pragma omp target firstprivate(argc,); (12, OpenMP, #pragma omp target firstprivate(i)); (11, OpenMP, #pragma omp target firstprivate(z, d)); (11, OpenMP, #pragma omp target firstprivate(argv[1], a, b, c, d, f)); (12, OpenMP, #pragma omp target shared(i)); (7, OpenMP, #pragma omp target firstprivate(ba)); (12, OpenMP, #pragma omp target firstprivate(j)); (-2, OpenMP, #pragma omp target firstprivate(argc); (5, OpenMP, #pragma omp target firstprivate(da)); (-6, OpenMP, #pragma omp target firstprivate()); (-5, OpenMP, #pragma omp target firstprivate(); (6, OpenMP, #pragma omp target firstprivate(S2::S2s)); (-6, OpenMP, #pragma omp target firstprivate); (-4, OpenMP, #pragma omp target firstprivate()); (6, OpenMP, #pragma omp target firstprivate(S2::S2sc)); (1, OpenMP, #pragma omp target firstprivate(ca)); (-7, OpenMP, #pragma omp target firstprivate(argc,)", "label": "(8, OpenMP, #pragma omp task private(); (17, OpenMP, #pragma omp task private(ca)); (20, OpenMP, #pragma omp task private(threadvar, B::x)); (10, OpenMP, #pragma omp task private(argc argv)); (17, OpenMP, #pragma omp task private(e, g)); (31, OpenMP, #pragma omp task private(m)); (1, OpenMP, #pragma omp task private); (9, OpenMP, #pragma omp task private(argv[1])); (1, OpenMP, #pragma omp task private()); (19, OpenMP, #pragma omp task private(j)); (4, OpenMP, #pragma omp task private(S1)); (0, OpenMP, #pragma omp task private(argc,); (5, OpenMP, #pragma omp task private(ba) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (7, OpenMP, #pragma omp task private(S2::S2s)); (5, OpenMP, #pragma omp task private(da)); (-3, OpenMP, #pragma omp task private(argc > 0 ? argv[1] : argv[2])); (11, OpenMP, #pragma omp task private(i)); (6, OpenMP, #pragma omp task shared(i), private(i)); (14, OpenMP, #pragma omp task private(i) allocate(omp_thread_mem_alloc: i)); (6, OpenMP, #pragma omp task firstprivate(i) private(i)); (10, OpenMP, #pragma omp task firstprivate(i)); (-11, OpenMP, #pragma omp task private(argc); (-7, OpenMP, #pragma omp task private(z, a, b, c, d, f))"}
{"predict": "(22, OpenMP, omp_target_free(d_a, omp_get_default_device());); (12, OpenMP, d_a = (int *)omp_target_alloc(DataSize, omp_get_default_device());); (18, OpenMP, #pragma omp target update to(p_a[0:N])); (12, OpenMP, omp_target_associate_ptr(p_a, d_a, DataSize, 0, omp_get_default_device());); (20, OpenMP, omp_target_disassociate_ptr(p_a, omp_get_default_device());)", "label": "(25, OpenMP, omp_target_free(d_a, omp_get_default_device());); (12, OpenMP, d_a = (int *)omp_target_alloc(DataSize, omp_get_default_device());); (18, OpenMP, #pragma omp target update to(p_a)); (20, OpenMP, omp_target_disassociate_ptr(p_a, omp_get_default_device());); (11, OpenMP, omp_target_associate_ptr(p_a, d_a, DataSize, 0, omp_get_default_device());)"}
{"predict": "(22, CUDA, ASSERT_EQ(cudaEventRecord(wm.ready, 0), cudaSuccess);); (28, CUDA, ASSERT_EQ(cudaDeviceSynchronize(), cudaSuccess);); (14, CUDA, EXPECT_EQ(cudaMemsetAsync(base, i, 64, 0), cudaSuccess);); (0, CUDA, EXPECT_EQ(cudaMalloc(&junk.get(), junk_size), cudaSuccess);)", "label": "(24, CUDA, ASSERT_EQ(cudaEventRecord(wm.ready, 0), cudaSuccess);); (25, CUDA, cvcuda::WorkspaceMemAllocator wa(wm, cudaStream_t(0));); (51, CUDA, EXPECT_EQ(cudaEventQuery(wm.ready), cudaSuccess);); (1, CUDA, ASSERT_EQ(cudaMalloc(&_junk, junk_size), cudaSuccess);); (14, CUDA, ASSERT_EQ(cudaMemset(junk.get(), i, junk_size), cudaSuccess);); (6, CUDA, ASSERT_EQ(cudaEventCreateWithFlags(&wm.ready, cudaEventDisableTiming), cudaSuccess);); (26, CUDA, ASSERT_EQ(cudaDeviceSynchronize(), cudaSuccess);); (50, CUDA, ASSERT_EQ(cudaEventDestroy(wm.ready), cudaSuccess);); (19, CUDA, EXPECT_EQ(cudaEventQuery(wm.ready), cudaErrorNotReady);)"}
{"predict": "(65, CUDA, CHECK_CUDA(cudaMalloc(&dy, dataSize * sizeof(int32_t)));); (98, CUDA, CHECK_CUDA(cudaFree(dUpdates));); (59, CUDA, CHECK_CUDA(cudaMalloc(&dUpdates, updatesSize * sizeof(int32_t)));); (37, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (94, CUDA, CHECK_CUDA(cudaFree(dy));); (55, CUDA, CHECK_CUDA(cudaMalloc(&dIndices, indicesSize * sizeof(int32_t)));); (36, CUDA, CHECK_HIEDNN(hiednnSetCudaStream(handle, 0));); (90, CUDA, CHECK_CUDA(cudaFree(dIndices));); (88, CUDA, CHECK_CUDA(cudaFree(dx));); (50, CUDA, CHECK_CUDA(cudaMalloc(&dx, dataSize * sizeof(int32_t)));)", "label": "(65, CUDA, CHECK_CUDA(cudaMalloc(&dy, dataSize * sizeof(int32_t)));); (100, CUDA, CHECK_CUDA(cudaFree(dUpdates));); (62, CUDA, CHECK_CUDA(cudaMalloc(&dUpdates, updatesSize * sizeof(int32_t)));); (94, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (37, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (97, CUDA, CHECK_CUDA(cudaFree(dy));); (56, CUDA, CHECK_CUDA(cudaMalloc(&dx, dataSize * sizeof(int32_t)));); (93, CUDA, CHECK_CUDA(cudaFree(dIndices));); (36, CUDA, CHECK_HIEDNN(hiednnSetCudaStream(handle, 0));); (90, CUDA, CHECK_CUDA(cudaFree(dx));); (53, CUDA, CHECK_CUDA(cudaMalloc(&dIndices, indicesSize * sizeof(int32_t)));)"}
{"predict": "(33, TBB, tbb::flow::input_node<T> src2(g, source_body<T>() );); (15, TBB, tbb::flow::function_node<T,bool> dest3(g, tbb::flow::unlimited, b3 );); (33, TBB, tbb::flow::function_node<T,bool,tbb::flow::rejecting> dest2(g, tbb::flow::serial, b2 );); (3, TBB, tbb::flow::make_edge( src, dest );); (26, TBB, tbb::flow::make_edge( src2, dest2 );); (12, TBB, tbb::flow::make_edge( src3, dest3 );); (-3, TBB, tbb::flow::graph g;); (34, TBB, tbb::flow::input_node<T> src_copy(src);); (-3, TBB, tbb::flow::input_node<T> src3(g, source_body<T>() );); (-6, TBB, tbb::flow::input_node<T> src(g, source_body<T>() );)", "label": "(28, TBB, tbb::flow::input_node<T> src2(g, my_input_body<T>() );); (18, TBB, tbb::flow::function_node<T,bool> dest3(g, tbb::flow::unlimited, b3 );); (31, TBB, tbb::flow::function_node<T,bool,tbb::flow::rejecting> dest2(g, tbb::flow::serial, b2 );); (3, TBB, tbb::flow::make_edge( src, dest );); (0, TBB, tbb::flow::input_node<T> src(g, my_input_body<T>() );); (29, TBB, tbb::flow::make_edge( src2, dest2 );); (36, TBB, tbb::flow::input_node<T> src_copy(src);); (13, TBB, tbb::flow::make_edge( src3, dest3 );); (-5, TBB, tbb::flow::graph g;); (6, TBB, tbb::flow::input_node<T> src3(g, my_input_body<T>() );)"}
{"predict": "(26, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&output_data, output_size_bytes));); (18, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&input_data, input_size_bytes));); (49, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(output_data));); (39, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (37, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (45, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(input_data));)", "label": "(26, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&output_data, output_size_bytes));); (18, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&input_data, input_size_bytes));); (48, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(output_data));); (39, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (37, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (44, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(input_data));)"}
{"predict": "(28, OpenCL, error |= clSetKernelArg(kernel, 1, sizeof(value), &value);); (26, OpenCL, error = clSetKernelArg(kernel, 0, sizeof(dst), &dst);)", "label": "(29, OpenCL, error |= clSetKernelArg(kernel, 1, sizeof(value), &value);); (27, OpenCL, error |= clSetKernelArg(kernel, 0, sizeof(dst), &dst);)"}
{"predict": "(1, MPI, MPI_Init(&argc, &argv);); (59, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &P);)", "label": "(1, MPI, MPI_Init(&argc, &argv);); (58, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &P);)"}
{"predict": "(7, CUDA, extern __shared__ __align__(sizeof(float)) uchar s[];)", "label": "(7, CUDA, extern __shared__ __align__(sizeof(float)) uchar s[];)"}
{"predict": "(22, OpenMP, #pragma omp parallel sections); (27, OpenMP, #pragma omp section); (16, OpenMP, #pragma omp cancel for); (35, OpenMP, #pragma omp parallel for reduction(+:r)); (20, OpenMP, #pragma omp cancel sections); (1, OpenMP, #pragma omp section); (3, OpenMP, #pragma omp cancel sections); (10, OpenMP, #pragma omp for); (-4, OpenMP, #pragma omp parallel); (15, OpenMP, #pragma omp parallel sections reduction(+:r)); (-2, OpenMP, #pragma omp cancel for); (33, OpenMP, #pragma omp cancel parallel); (-7, OpenMP, #pragma omp sections); (-2, OpenMP, #pragma omp parallel for)", "label": "(5, OpenMP, #pragma omp barrier); (23, OpenMP, #pragma omp task); (18, OpenMP, #pragma omp for); (0, OpenMP, #pragma omp cancel parallel if(flag)); (4, OpenMP, #pragma omp sections); (7, OpenMP, #pragma omp section); (22, OpenMP, #pragma omp parallel sections); (-6, OpenMP, #pragma omp parallel); (18, OpenMP, #pragma omp cancel taskgroup); (32, OpenMP, #pragma omp parallel for reduction(+: r)); (0, OpenMP, #pragma omp cancel sections); (11, OpenMP, #pragma omp cancel for)"}
{"predict": "(15, OpenMP, #pragma omp target map(tofrom: Inner, Outer, wayout, A)); (16, OpenMP, #pragma omp teams num_teams(1  ) thread_limit(tl) schedule(static))", "label": "(18, OpenMP, #pragma omp parallel for); (14, OpenMP, #pragma omp target map(tofrom: Inner, Outer, wayout)); (14, OpenMP, #pragma omp teams num_teams(1  ) thread_limit(tl))"}
{"predict": "(138, MPI, MPI_Barrier(comm);); (22, MPI, MPI_Comm_rank(comm, &par_rank);); (22, MPI, MPI_Comm_size(comm, &par_size);); (161, MPI, actions[0][\"scenes/s2/image_prefix\"] = u_output_file;); (113, MPI, actions[0][\"mpi_comm\"] = MPI_Comm_c2f(comm);)", "label": "(166, MPI, opts[\"mpi_comm\"] = MPI_Comm_c2f(MPI_COMM_WORLD);); (171, MPI, MPI_Barrier(comm);); (21, MPI, MPI_Comm_rank(comm, &par_rank);); (21, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (20, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (51, MPI, MPI_Finalize();); (48, MPI, MPI_T_finalize();); (19, MPI, MPI_T_init_thread(required, &provided);); (32, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (20, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (51, MPI, MPI_Finalize();); (48, MPI, MPI_T_finalize();); (19, MPI, MPI_T_init_thread(required, &provided);); (32, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(71, CUDA, checkCudaErrors( ::cudaDeviceSynchronize() );)", "label": "(101, CUDA, checkHIPErrors( ::cudaDeviceSynchronize() );)"}
{"predict": "(20, OpenMP, omp_target_free(da, /*device=*/omp_get_default_device());)", "label": "(21, OpenMP, #pragma omp target update from(a[:n])); (12, OpenMP, #pragma omp target teams distribute parallel for map(to:b[:n])); (15, OpenMP, #pragma omp target teams distribute parallel for)"}
{"predict": "(6, CUDA, gpuErrchk(cudaPointerGetAttributes(&attributes, d_int));); (28, CUDA, gpuErrchk(cudaFree(d_int));); (1, CUDA, gpuErrchk(cudaMalloc(&d_int, sizeof(int)));)", "label": "(6, CUDA, gpuErrchk(cudaPointerGetAttributes(&attributes, d_int));); (28, CUDA, gpuErrchk(cudaFree(d_int));); (1, CUDA, gpuErrchk(cudaMalloc(&d_int, sizeof(int)));)"}
{"predict": "(2, CUDA, __shared__ float2 cellImagePos;); (2, CUDA, __shared__ bool isContained;); (2, CUDA, __shared__ Color color;)", "label": "(4, CUDA, __shared__ float3 color;); (2, CUDA, __shared__ bool isContained;); (0, CUDA, __shared__ float2 cellImagePos;)"}
{"predict": "(11, OpenMP, omp_set_num_threads(nr_threads);); (12, OpenMP, #pragma omp parallel default(none) shared(a)); (13, OpenMP, #pragma omp sections); (5, OpenMP, omp_set_dynamic(0);); (12, OpenMP, #pragma omp section); (15, OpenMP, int thread_nr = omp_get_thread_num();)", "label": "(11, OpenMP, omp_set_num_threads(nr_threads);); (12, OpenMP, #pragma omp parallel default(none) shared(a)); (13, OpenMP, #pragma omp sections); (7, OpenMP, omp_set_dynamic(0);); (13, OpenMP, #pragma omp section); (15, OpenMP, int thread_nr = omp_get_thread_num();)"}
{"predict": "(2, CUDA, extern __shared__ float shared[];)", "label": "(22, CUDA, __shared__ float shared[forward_threads];)"}
{"predict": "(12, CUDA, cudaDeviceSynchronize();); (7, CUDA, cudaMalloc((void **)&sm_o, sizeof(float) * m * m);); (34, CUDA, cudaFree(sm_o);)", "label": "(12, CUDA, cudaDeviceSynchronize();); (7, CUDA, cudaMalloc((void **)&sm_o, sizeof(float) * m * m);); (34, CUDA, cudaFree(sm_o);)"}
{"predict": "(46, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (33, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_AP), sizeof(data_type) * AP.size()));); (63, CUDA, CUDA_CHECK(cudaDeviceReset());); (60, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (29, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (53, CUDA, CUDA_CHECK(cudaFree(d_AP));); (53, CUDA, CUDA_CHECK(cudaFree(d_A));); (22, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)", "label": "(46, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (32, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_AP), sizeof(data_type) * AP.size()));); (63, CUDA, CUDA_CHECK(cudaDeviceReset());); (60, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (30, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (53, CUDA, CUDA_CHECK(cudaFree(d_AP));); (53, CUDA, CUDA_CHECK(cudaFree(d_A));); (22, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(12, OpenCL, profiling_queue = clCreateCommandQueue(ctx, dev, 0, &status);); (50, OpenCL, clReleaseCommandQueue(profiling_queue);)", "label": "(40, OpenCL, clReleaseEvent(exec_event);); (11, OpenCL, profiling_queue = clCreateCommandQueue(ctx, device, CL_QUEUE_PROFILING_ENABLE, &status);); (47, OpenCL, clReleaseCommandQueue(profiling_queue);)"}
{"predict": "(19, MPI, MPIT_result = MPI_T_pvar_session_create(&session);); (12, MPI, MPIT_result = MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);)", "label": "(19, MPI, MPIT_result = MPI_T_pvar_session_create(&session);); (12, MPI, MPIT_result = MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);)"}
{"predict": "(9, MPI, MPI_Group_compare(group, wgroup, &result);); (5, MPI, MPI_File_get_group(cFile, &group);); (0, MPI, MPI_File cFile = MPI_File_f2c(*file);); (4, MPI, MPI_Comm_group(MPI_COMM_WORLD, &wgroup);); (12, MPI, MPI_Group_free(&wgroup);); (10, MPI, MPI_Group_free(&group);)", "label": "(9, MPI, MPI_Group_compare(group, wgroup, &result);); (5, MPI, MPI_File_get_group(cFile, &group);); (0, MPI, MPI_File cFile = MPI_File_f2c(*file);); (4, MPI, MPI_Comm_group(MPI_COMM_WORLD, &wgroup);); (12, MPI, MPI_Group_free(&wgroup);); (10, MPI, MPI_Group_free(&group);)"}
{"predict": "(35, CUDA, __shared__ _VOLATILE_ float aSmem2[8][128+4];); (33, CUDA, __shared__ _VOLATILE_ float bSmem1[8][128+4];); (34, CUDA, __shared__ _VOLATILE_ float bSmem2[8][128+4];); (30, CUDA, __shared__ _VOLATILE_ float aSmem1[8][128+4];)", "label": "(35, CUDA, __shared__ _VOLATILE_ float aSmem2[8][128+4];); (33, CUDA, __shared__ _VOLATILE_ float bSmem1[8][128+4];); (34, CUDA, __shared__ _VOLATILE_ float bSmem2[8][128+4];); (30, CUDA, __shared__ _VOLATILE_ float aSmem1[8][128+4];)"}
{"predict": "(3, CUDA, extern __shared__ unsigned sfdatx[];)", "label": "(3, CUDA, extern __shared__ unsigned sfdatx[];)"}
{"predict": "(1, OpenMP, #pragma omp parallel for); (12, OpenMP, #pragma omp parallel for ordered order(concurrent)); (4, OpenMP, #pragma omp parallel for ordered order(none); (6, OpenMP, #pragma omp parallel for ordered order(concurrent); (-1, OpenMP, #pragma omp task); (13, OpenMP, #pragma omp parallel for ordered order(none))", "label": "(7, OpenMP, #pragma omp target teams distribute parallel for simd order(); (15, OpenMP, #pragma omp target teams distribute parallel for simd order(concurrent)); (-1, OpenMP, #pragma omp target teams distribute parallel for simd nowait nowait); (7, OpenMP, #pragma omp target teams distribute parallel for simd order(none); (9, OpenMP, #pragma omp target teams distribute parallel for simd order(concurrent); (-1, OpenMP, #pragma omp target teams distribute parallel for simd order)"}
{"predict": "(7, TBB, tbb::tick_count t1 = tbb::tick_count::now();); (14, TBB, ASSERT( num_threads > 0, \"tbb::thread::hardware_concurrency() has returned an incorrect value\" );); (3, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (11, TBB, int num_threads = tbb::tbb_thread::hardware_concurrency();); (5, TBB, tbb::tick_count t2 = tbb::tick_count::now();)", "label": "(7, TBB, tbb::tick_count t1 = tbb::tick_count::now();); (14, TBB, ASSERT( num_threads > 0, \"tbb::thread::hardware_concurrency() has returned an incorrect value\" );); (3, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (11, TBB, int num_threads = tbb::tbb_thread::hardware_concurrency();); (5, TBB, tbb::tick_count t2 = tbb::tick_count::now();)"}
{"predict": "(16, CUDA, cudaFree(x);); (25, CUDA, cudaFree(droptol);); (23, CUDA, cudaFree(cusolver_rf_work);); (14, CUDA, cudaFree(r);); (26, CUDA, cudaFree(klu_work);)", "label": "(17, CUDA, cudaFree(devx_);); (27, CUDA, cudaFree(d_T_);); (24, CUDA, cudaFree(d_P_);); (24, CUDA, cudaFree(d_Q_);); (12, CUDA, cudaFree(devr_);); (1, CUDA, cudaFree(d_work_);)"}
{"predict": "(14, OpenMP, #pragma omp target teams distribute parallel for); (6, OpenMP, #pragma omp target teams distribute parallel for map(from: num_teams[0:NN], num_threads[0:NN]))", "label": "(7, OpenMP, printf(\"#pragma omp target teams distribute parallel for thread_limit(4)\\n\");); (10, OpenMP, num_teams[j] = omp_get_num_teams();); (13, OpenMP, printf(\"#pragma omp target teams distribute parallel for\\n\");); (7, OpenMP, num_threads[j] = omp_get_num_threads();); (4, OpenMP, #pragma omp target teams distribute parallel for)"}
{"predict": "(61, MPI, *time -= MPI_Wtime();); (42, MPI, MPI_Type_size(c_info->r_data_type, &r_size);); (74, MPI, *time += MPI_Wtime();); (39, MPI, MPI_Type_size(c_info->s_data_type, &s_size);)", "label": "(63, MPI, *time -= MPI_Wtime();); (43, MPI, MPI_Type_size(c_info->r_data_type, &r_size);); (41, MPI, MPI_Type_size(c_info->s_data_type, &s_size);); (73, MPI, *time += MPI_Wtime();)"}
{"predict": "(2, TBB, typedef typename tbb::flow::tuple_element<0,OutputTuple>::type OutputType;); (28, TBB, tbb::flow::remove_edge( tbb::flow::output_port<0>(exe_node), receivers[r] );); (4, TBB, tbb::flow::multifunction_node< InputType, OutputTuple, tbb::flow::rejecting > exe_node( g, tbb::flow::unlimited, body );); (11, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(exe_node), receivers[r] );); (-1, TBB, tbb::flow::graph g;)", "label": "(14, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(exe_node), receivers[r] );); (1, TBB, typedef typename tbb::flow::tuple_element<0,OutputTuple>::type OutputType;); (27, TBB, tbb::flow::remove_edge( tbb::flow::output_port<0>(exe_node), receivers[r] );); (3, TBB, tbb::flow::multifunction_node< InputType, OutputTuple, tbb::flow::rejecting > exe_node( g, tbb::flow::unlimited, body );); (1, TBB, tbb::flow::graph g;)"}
{"predict": "(84, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (84, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)", "label": "(84, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (84, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)"}
{"predict": "(8, MPI, dray::dray::mpi_comm(MPI_Comm_c2f(comm));)", "label": "(20, MPI, int rank = dray::dray::mpi_rank();); (9, MPI, dray::dray::mpi_comm(MPI_Comm_c2f(comm));); (19, MPI, int size = dray::dray::mpi_size();)"}
{"predict": "(9, MPI, MPI_Errhandler_free( &newerr );); (7, MPI, MPI_Comm_call_errhandler( comm, MPI_ERR_OTHER );); (4, MPI, MPI_Comm_set_errhandler( comm, newerr );); (0, MPI, MPI_Init( &argc, &argv );); (12, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_create_errhandler( ehandler, &newerr );)", "label": "(10, MPI, MPI_Comm_call_errhandler( comm, MPI_ERR_OTHER );); (8, MPI, MPI_Comm_set_errhandler( comm, newerr );); (14, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_create_errhandler( eh, &newerr );); (1, MPI, MPI_Init( &argc, &argv );); (6, MPI, MPI_Errhandler_free( &newerr );)"}
{"predict": "(11, MPI, MPI_Comm_rank(intra, &rank);); (28, MPI, MPI_Comm_free(&intra);); (14, MPI, err = MPI_Intercomm_create(intra, 0, intra, MPI_COMM_NULL, tag, &inter);); (21, MPI, MPI_Barrier(inter);); (32, MPI, MPI_Comm_free(&merge1);); (5, MPI, MPI_Comm_split(parent, 0, 1, &intra);); (27, MPI, MPI_Comm_free(&inter);); (21, MPI, err = MPI_Intercomm_merge(inter, first, &merge1);); (12, MPI, err = MPI_Intercomm_create(MPI_COMM_WORLD, 0, intra, 0, tag, &inter);); (22, MPI, err = MPI_Barrier(merge1);); (25, MPI, MPI_Comm_free(&merge1);); (-1, MPI, MPI_Comm_set_errhandler(parent, MPI_ERRORS_RETURN);)", "label": "(38, MPI, MPI_Comm_disconnect(&parent);); (27, MPI, err = MPI_Intercomm_merge(inter, 0, &merge1);); (34, MPI, MPI_Comm_free(&intra);); (31, MPI, MPI_Comm_free(&merge1);); (6, MPI, err = MPI_Intercomm_merge(parent, 1, &intra);); (30, MPI, MPI_Comm_free(&inter);); (25, MPI, err = MPI_Barrier(merge1);); (18, MPI, err = MPI_Barrier(inter);); (21, MPI, MPI_Comm_rank(merge1, &rank);); (7, MPI, err = MPI_Intercomm_create(intra, 0, MPI_COMM_NULL, 0, tag, &inter);); (1, MPI, MPI_Comm_rank(intra, &rank);); (10, MPI, err = MPI_Intercomm_create(MPI_COMM_WORLD, 0, intra, 0, tag, &inter);)"}
{"predict": "(1, OpenMP, #pragma omp target); (1, OpenMP, #pragma omp teams)", "label": "(3, OpenMP, #pragma omp distribute parallel for if (true)); (11, OpenMP, #pragma omp distribute parallel for if (false)); (19, OpenMP, #pragma omp distribute parallel for if (Arg)); (-2, OpenMP, #pragma omp target); (-2, OpenMP, #pragma omp teams)"}
{"predict": "(19, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &comms[x]);)", "label": "(25, MPI, MPI_Comm_free(&comms[x]);); (18, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &comms[x]);)"}
{"predict": "(13, OpenMP, #pragma omp sections firstprivate(,); (24, OpenMP, #pragma omp sections firstprivate()); (42, OpenMP, #pragma omp sections firstprivate(x)); (46, OpenMP, #pragma omp sections firstprivate(x, y)); (1, OpenMP, #pragma omp sections firstprivate(); (26, OpenMP, #pragma omp sections firstprivate(int)); (13, OpenMP, #pragma omp sections firstprivate(, )); (-5, OpenMP, #pragma omp parallel); (46, OpenMP, #pragma omp sections firstprivate(x, y, z)); (28, OpenMP, #pragma omp sections firstprivate(0))", "label": "(5, OpenMP, #pragma omp sections firstprivate(); (24, OpenMP, #pragma omp sections firstprivate()); (35, OpenMP, #pragma omp sections firstprivate(0)); (51, OpenMP, #pragma omp sections lastprivate(x, y, z) firstprivate(x, y, z)); (45, OpenMP, #pragma omp sections lastprivate(x, y) firstprivate(x, y)); (39, OpenMP, #pragma omp sections lastprivate(x) firstprivate(x)); (-4, OpenMP, #pragma omp parallel); (6, OpenMP, #pragma omp sections firstprivate(,); (11, OpenMP, #pragma omp sections firstprivate(, )); (22, OpenMP, #pragma omp sections firstprivate(int))"}
{"predict": "(3, OpenMP, #pragma omp target data map(to: i)); (4, OpenMP, #pragma omp target data map(tofrom: i) allocate(omp_thread_mem_alloc: i))", "label": "(10, OpenMP, #pragma omp target update to(i) nowait (3.14) device (-10u)); (3, OpenMP, #pragma omp target nowait update to(i)); (7, OpenMP, #pragma omp target update to(i) nowait device (-10u)); (5, OpenMP, #pragma omp target update to(i) nowait (argc))); (7, OpenMP, #pragma omp target update to(i) nowait nowait); (7, OpenMP, #pragma omp target update nowait to(i) nowait); (-3, OpenMP, #pragma omp nowait target update to(i)); (-1, OpenMP, #pragma omp target update nowait() to(i)); (-1, OpenMP, #pragma omp target update to(i) nowait()"}
{"predict": "(6, TBB, tbb::flow::queue_node< q3_input_type >  q3(g);); (11, TBB, ASSERT(tbb::flow::input_port<0>(my_indexer).predecessor_count() == 0, NULL);); (7, TBB, tbb::flow::make_edge(my_indexer, q3);)", "label": "(6, TBB, tbb::flow::queue_node< q3_input_type >  q3(g);); (11, TBB, ASSERT(tbb::flow::input_port<0>(my_indexer).predecessor_count() == 0, NULL);); (7, TBB, tbb::flow::make_edge(my_indexer, q3);)"}
{"predict": "(27, MPI, open_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (7, MPI, MPI_Comm_rank(comm, &par_rank);); (7, MPI, MPI_Comm_size(comm, &par_size);)", "label": "(8, MPI, MPI_Comm_rank(comm, &par_rank);); (24, MPI, open_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (7, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(20, OpenMP, #pragma omp parallel sections default(none)); (8, OpenMP, #pragma omp parallel sections default(shared), default(shared)); (4, OpenMP, #pragma omp parallel sections default()); (6, OpenMP, #pragma omp parallel sections default(none); (0, OpenMP, #pragma omp parallel sections default(); (-3, OpenMP, #pragma omp parallel sections default); (14, OpenMP, #pragma omp parallel sections default(x)); (20, OpenMP, #pragma omp parallel sections default(shared))", "label": "(5, OpenMP, #pragma omp parallel sections default()); (2, OpenMP, #pragma omp parallel sections default(); (7, OpenMP, #pragma omp parallel sections default(shared), default(shared)); (4, OpenMP, #pragma omp parallel sections default(none); (7, OpenMP, #pragma omp parallel sections default(x)); (16, OpenMP, #pragma omp parallel sections default(none)); (-5, OpenMP, #pragma omp parallel sections default); (21, OpenMP, #pragma omp parallel sections default(shared))"}
{"predict": "(17, CUDA, __shared__ float s_b[2][BK][BN];); (15, CUDA, __shared__ float s_a[2][BK][BM];)", "label": "(16, CUDA, __shared__ float s_a[2][BK][BM];); (16, CUDA, __shared__ float s_b[2][BK][BN];)"}
{"predict": "(9, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (10, TBB, tbb::tick_count t1 = tbb::tick_count::now();); (8, TBB, tbb::parallel_for(tbb::blocked_range<size_t>(0,n,n/100),TwiddleInvariant<Invariant<M,8> >(invariant));)", "label": "(8, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (9, TBB, tbb::tick_count t1 = tbb::tick_count::now();); (7, TBB, tbb::parallel_for(tbb::blocked_range<size_t>(0,n,n/100),TwiddleInvariant<Invariant<M,8> >(invariant));)"}
{"predict": "(9, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &iam );)", "label": "(9, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &iam );)"}
{"predict": "(8, OpenMP, #pragma omp parallel)", "label": "(8, OpenMP, #pragma omp parallel); (10, OpenMP, #pragma omp for); (14, OpenMP, #pragma omp critical)"}
{"predict": "(20, OpenCL, clReleaseMemObject(mobjA_);); (16, OpenCL, clReleaseMemObject(mobjX_);)", "label": "(20, OpenCL, clReleaseMemObject(mobjA_);); (16, OpenCL, clReleaseMemObject(mobjX_);)"}
{"predict": "(14, OpenCL, cl_command_queue cmdQ = clCreateCommandQueueWithProperties(context.get(), clDevice, properties, &retVal);); (38, OpenCL, retVal = clReleaseCommandQueue(cmdQ);); (1, OpenCL, clGetDeviceInfo(clDevice, CL_DEVICE_SLICE_COUNT_INTEL, sizeof(size_t), &maxSliceCount, nullptr);)", "label": "(14, OpenCL, cl_command_queue cmdQ = clCreateCommandQueueWithProperties(context.get(), clDevice, properties, &retVal);); (38, OpenCL, retVal = clReleaseCommandQueue(cmdQ);); (1, OpenCL, clGetDeviceInfo(clDevice, CL_DEVICE_SLICE_COUNT_INTEL, sizeof(size_t), &maxSliceCount, nullptr);)"}
{"predict": "(11, OpenMP, #pragma omp target teams distribute parallel for map(tofrom: v[0:N]))", "label": "(12, OpenMP, #pragma omp target teams); (9, OpenMP, #pragma omp target update to(vec.v[0:N])); (16, OpenMP, #pragma omp target update from(vec.v[0:N])); (10, OpenMP, #pragma omp distribute parallel for)"}
{"predict": "(26, OpenCL, retVal = clReleaseMemObject(buffer);); (13, OpenCL, cl_mem subBuffer = clCreateSubBuffer(buffer, flags, CL_BUFFER_CREATE_TYPE_REGION, &region, &retVal);); (22, OpenCL, retVal = clReleaseMemObject(subBuffer);); (-1, OpenCL, cl_mem notUsedBuffer = clCreateBuffer(clContext, flags, size, hostPtr, &retVal);); (25, OpenCL, EXPECT_EQ(clReleaseContext(context), CL_SUCCESS);); (1, OpenCL, cl_mem buffer = clCreateBuffer(clContext, flags, size, hostPtr, &retVal);)", "label": "(21, OpenCL, retVal = clReleaseMemObject(subBuffer);); (26, OpenCL, retVal = clReleaseMemObject(notUsedBuffer);); (28, OpenCL, EXPECT_EQ(clReleaseContext(context), CL_SUCCESS);); (-2, OpenCL, cl_mem notUsedBuffer = clCreateBuffer(clContext, flags, size, hostPtr, &retVal);); (20, OpenCL, retVal = clReleaseMemObject(buffer);); (0, OpenCL, cl_mem buffer = clCreateBuffer(clContext, flags, size, hostPtr, &retVal);); (8, OpenCL, cl_mem subBuffer = clCreateSubBuffer(buffer, flags, CL_BUFFER_CREATE_TYPE_REGION, &region, &retVal);)"}
{"predict": "(87, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (87, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)", "label": "(87, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (87, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)"}
{"predict": "(22, CUDA, nvinfer1::ICudaEngine* engine = builder->buildCudaEngine(*network);)", "label": "(22, CUDA, nvinfer1::ICudaEngine* engine = builder->buildCudaEngine(*network);)"}
{"predict": "(36, OpenCL, streams[1] = clCreateBuffer(context, CL_MEM_READ_WRITE, length, NULL, &err);); (58, OpenCL, err |= clSetKernelArg(kernel, 1, sizeof streams[1], &streams[1]);); (56, OpenCL, err |= clSetKernelArg(kernel, 0, sizeof streams[0], &streams[0]);); (57, OpenCL, err |= clSetKernelArg(kernel, 2, sizeof sampler, &sampler);)", "label": "(58, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(streams[0]), &streams[0]);); (58, OpenCL, err |= clSetKernelArg(kernel, 1, sizeof(streams[1]), &streams[1]);); (58, OpenCL, err |= clSetKernelArg(kernel, 2, sizeof(sampler), &sampler);); (33, OpenCL, streams[1] = clCreateBuffer(context, CL_MEM_READ_WRITE, length, NULL, &err);)"}
{"predict": "(17, OpenMP, #pragma omp atomic); (21, OpenMP, #pragma omp atomic)", "label": "(7, OpenMP, #pragma omp atomic)"}
{"predict": "(24, CUDA, cudaSetDevice(deviceId);); (20, CUDA, cudaGetDeviceCount(&nDevices);); (9, MPI, MPIManager mpi_manager(MPI_COMM_WORLD);)", "label": "(24, CUDA, cudaSetDevice(deviceId);); (20, CUDA, cudaGetDeviceCount(&nDevices);); (9, MPI, MPIManager mpi_manager(MPI_COMM_WORLD);)"}
{"predict": "(8, OpenMP, #pragma omp target); (3, OpenMP, #pragma omp distribute parallel for schedule(guided, argc) default(none) copyin(g) dist_schedule(static, a) private(a) shared(argc)); (19, OpenMP, #pragma omp distribute parallel for private(argc, b), firstprivate(argv, c), lastprivate(d, f) collapse(2) schedule(auto) if (argc) num_threads(a) default(shared) shared(e) reduction(+ : h) dist_schedule(static, b)); (3, OpenMP, #pragma omp teams)", "label": "(5, OpenMP, #pragma omp threadprivate(g)); (21, OpenMP, #pragma omp distribute parallel for private(argc, b), firstprivate(argv, c), lastprivate(d, f) collapse(2) schedule(auto) if (argc) num_threads(a) default(shared) shared(e) reduction(+ : h) dist_schedule(static, b)); (7, OpenMP, #pragma omp distribute parallel for schedule(guided, argc) default(none) copyin(g) dist_schedule(static, a) private(a) shared(argc)); (4, OpenMP, #pragma omp target); (4, OpenMP, #pragma omp teams)"}
{"predict": "(13, TBB, double serial_time = (tbb::tick_count::now() - t0).seconds();); (10, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (19, TBB, double tbb_time = (tbb::tick_count::now() - t1).seconds();); (16, TBB, tbb::tick_count t1 = tbb::tick_count::now();)", "label": "(13, TBB, double serial_time = (tbb::tick_count::now() - t0).seconds();); (10, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (19, TBB, double tbb_time = (tbb::tick_count::now() - t1).seconds();); (16, TBB, tbb::tick_count t1 = tbb::tick_count::now();)"}
{"predict": "(3, CUDA, extern __shared__ double sdat[];)", "label": "(3, CUDA, extern __shared__ double sdat[];)"}
{"predict": "(18, TBB, my_thread_array = tbb::cache_aligned_allocator<padded_ipc_worker>().allocate( my_n_thread );); (26, TBB, my_waker = tbb::cache_aligned_allocator<ipc_waker>().allocate(1);); (28, TBB, my_stopper = tbb::cache_aligned_allocator<ipc_stopper>().allocate(1);)", "label": "(20, TBB, my_thread_array = tbb::cache_aligned_allocator<padded_ipc_worker>().allocate( my_n_thread );); (14, TBB, my_n_thread = tbb::detail::r1::AvailableHwConcurrency();); (25, TBB, my_waker = tbb::cache_aligned_allocator<ipc_waker>().allocate(1);); (27, TBB, my_stopper = tbb::cache_aligned_allocator<ipc_stopper>().allocate(1);)"}
{"predict": "(10, MPI, MPI_Type_size_x(type, &typesize);)", "label": "(11, MPI, MPI_Type_size_x(type, &typesize);)"}
{"predict": "(80, CUDA, flamegpu::CUDASimulation cudaSimulation(model);)", "label": "(80, CUDA, flamegpu::CUDASimulation cudaSimulation(model);)"}
{"predict": "(16, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);)", "label": "(16, MPI, MPI_Comm_size(MPI_COMM_WORLD,&mpi_size);); (16, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&mpi_rank);)"}
{"predict": "(13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (28, MPI, MPI_Recv(buf, 1, newtype, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (18, MPI, MPI_Get_address(&buf[1], &displs[0]);); (38, MPI, MPI_Type_free(&newtype);); (19, MPI, MPI_Type_commit(&newtype);); (24, MPI, MPI_Send(buf, 1, newtype, 1, 0, MPI_COMM_WORLD);); (14, MPI, MPI_Type_create_struct(2, lens, displs, types, &newtype);)", "label": "(30, MPI, MPI_Send(MPI_BOTTOM, 1, newtype, 1, 0, MPI_COMM_WORLD);); (23, MPI, MPI_Type_commit(&newtype);); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (20, MPI, MPI_Type_create_struct(2, lens, displs, types, &newtype);); (15, MPI, MPI_Get_address(&count, &displs[0]);); (27, MPI, MPI_Recv(buf, 6, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (35, MPI, MPI_Type_free(&newtype);); (15, MPI, types[1] = MPI_Type_f2c(*ftype);)"}
{"predict": "(1, OpenMP, #pragma omp taskyield); (2, OpenMP, #pragma omp taskyield untied); (2, OpenMP, #pragma omp taskyield unknown)", "label": "(3, OpenMP, #pragma omp taskyield untied); (0, OpenMP, #pragma omp taskyield); (2, OpenMP, #pragma omp taskyield unknown)"}
{"predict": "(19, OpenMP, #pragma omp parallel default(shared)); (11, OpenMP, #pragma omp parallel); (25, OpenMP, #pragma omp parallel default(none)); (37, OpenMP, #pragma omp parallel default(firstprivate)); (28, OpenMP, #pragma omp parallel default(x))", "label": "(19, OpenMP, #pragma omp parallel default(shared)); (11, OpenMP, #pragma omp parallel); (38, OpenMP, #pragma omp parallel num_threads(x)); (23, OpenMP, #pragma omp parallel default(none)); (29, OpenMP, #pragma omp parallel default(firstprivate))"}
{"predict": "(15, OpenMP, #pragma omp flush); (7, OpenMP, #pragma omp parallel); (8, OpenMP, #pragma omp single); (21, OpenMP, #pragma omp critical)", "label": "(13, OpenMP, #pragma omp for private(sum0) schedule(static,1)); (7, OpenMP, #pragma omp parallel); (22, OpenMP, #pragma omp critical); (13, OpenMP, #pragma omp flush)"}
{"predict": "(33, TBB, tbb::parallel_for( tbb::blocked_range<int> (0, N, 10000), ParallelScalarBody<T>( sums, t0 ) );); (31, TBB, if (Verbose && t == 0) t0 = tbb::tick_count::now();); (12, TBB, tbb::combinable<T> sums;); (25, TBB, tbb::tick_count t0;); (11, TBB, tbb::combinable<T> assigned;); (27, TBB, tbb::combinable<T> finit_combinable(my_finit_decl);)", "label": "(30, TBB, tbb::combinable<T> finit_combinable(my_finit_decl);); (32, TBB, tbb::parallel_for( tbb::blocked_range<int>( 0, N, 10000 ), ParallelScalarBodyNoInit<T>( finit_combinable ) );); (24, TBB, if (Verbose && t == 0) t0 = tbb::tick_count::now();); (31, TBB, tbb::parallel_for( tbb::blocked_range<int>( 0, N, 10000 ), ParallelScalarBody<T>( sums ) );); (40, TBB, tbb::combinable<T> assigned;); (-2, TBB, tbb::task_scheduler_init init(tbb::task_scheduler_init::deferred);); (7, TBB, tbb::tick_count t0;); (21, TBB, tbb::combinable<T> sums;)"}
{"predict": "(12, OpenMP, omp_set_num_threads(nr_threads);); (13, OpenMP, #pragma omp parallel default(none) shared(a)); (14, OpenMP, #pragma omp sections); (5, OpenMP, omp_set_dynamic(0);); (13, OpenMP, #pragma omp section); (16, OpenMP, int thread_nr = omp_get_thread_num();)", "label": "(12, OpenMP, omp_set_num_threads(nr_threads);); (13, OpenMP, #pragma omp parallel default(none) shared(a)); (14, OpenMP, #pragma omp sections); (8, OpenMP, omp_set_dynamic(0);); (14, OpenMP, #pragma omp section); (16, OpenMP, int thread_nr = omp_get_thread_num();)"}
{"predict": "(12, CUDA, extern __shared__ float _shared_centroids[];)", "label": "(12, CUDA, extern __shared__ float _shared_centroids[];)"}
{"predict": "(21, CUDA, extern __shared__ float entire_buffer[];)", "label": "(21, CUDA, extern __shared__ float entire_buffer[];)"}
{"predict": "(3, OpenMP, #pragma omp flush); (4, OpenMP, #pragma omp flush acq_rel); (5, OpenMP, #pragma omp flush acquire); (6, OpenMP, #pragma omp flush release); (7, OpenMP, #pragma omp flush (a))", "label": "(5, OpenMP, #pragma omp flush acquire); (5, OpenMP, #pragma omp flush release); (1, OpenMP, #pragma omp flush); (4, OpenMP, #pragma omp flush(a)); (0, OpenMP, #pragma omp flush acq_rel)"}
{"predict": "(83, OpenACC, #pragma acc update host(state[(nz + 2 * hs) * (nx + 2 * hs) * NUM_VARS])); (45, OpenACC, #pragma acc data copyin(state_tmp[(nz + 2 * hs) * (nx + 2 * hs) * NUM_VARS], hy_dens_cell[nz + 2 * hs], hy_dens_theta_cell[nz + 2 * hs], hy_dens_int[nz + 1], hy_dens_theta_int[nz + 1], hy_pressure_int[nz + 1]))", "label": "(83, OpenACC, #pragma acc update host(state[(nz + 2 * hs) * (nx + 2 * hs) * NUM_VARS])); (45, OpenACC, #pragma acc data copyin(state_tmp[(nz + 2 * hs) * (nx + 2 * hs) * NUM_VARS], hy_dens_cell[nz + 2 * hs], hy_dens_theta_cell[nz + 2 * hs], hy_dens_int[nz + 1], hy_dens_theta_int[nz + 1], hy_pressure_int[nz + 1]))"}
{"predict": "(40, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(32, MPI, CALL(MPI_Waitall(static_cast<int>(reqs.size()), reqs.data(), MPI_STATUSES_IGNORE));); (20, MPI, CALL(MPI_Waitall(reqs.size(), reqs.data(), MPI_STATUSES_IGNORE));)"}
{"predict": "(28, MPI, MPI_Bcast(data, num_elements, MPI_INT, 0, MPI_COMM_WORLD);); (31, MPI, total_mpi_bcast_time += MPI_Wtime() - start;); (25, MPI, total_my_bcast_time += MPI_Wtime() - start;); (6, MPI, MPI_Init(NULL, NULL);); (42, MPI, MPI_Finalize();); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (17, MPI, double start = MPI_Wtime();); (20, MPI, MPI_Barrier(MPI_COMM_WORLD);); (23, MPI, start = MPI_Wtime();)", "label": "(28, MPI, total_my_bcast_time += MPI_Wtime();); (31, MPI, total_mpi_bcast_time -= MPI_Wtime();); (45, MPI, MPI_Finalize();); (32, MPI, total_mpi_bcast_time += MPI_Wtime();); (5, MPI, MPI_Init(NULL, NULL);); (28, MPI, MPI_Bcast(data, num_elements, MPI_INT, 0, MPI_COMM_WORLD);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (17, MPI, total_my_bcast_time -= MPI_Wtime();); (15, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(20, CUDA, cudaStream_t profileStream = sample::getCudaProfilerStream();)", "label": "(46, CUDA, env.engine.reset(runtime->deserializeCudaEngine(env.serializedEngine->data(), env.serializedEngine->size()));); (19, CUDA, auto profileStream = samplesCommon::makeCudaStream();)"}
{"predict": "(28, CUDA, cudaFree( p );)", "label": "(28, CUDA, cudaFree( p );)"}
{"predict": "(19, OpenMP, #pragma omp target teams is_device_ptr(k)); (28, OpenMP, #pragma omp target teams is_device_ptr(aa)); (37, OpenMP, #pragma omp target teams is_device_ptr(h)); (41, OpenMP, #pragma omp target teams is_device_ptr(rh)); (30, OpenMP, #pragma omp target teams is_device_ptr(raa)); (44, OpenMP, #pragma omp target teams is_device_ptr(da)); (18, OpenMP, #pragma omp target teams is_device_ptr(z))", "label": "(49, OpenMP, #pragma omp target teams is_device_ptr(da)); (23, OpenMP, #pragma omp target teams is_device_ptr(z)); (27, OpenMP, #pragma omp target teams is_device_ptr(aa)); (16, OpenMP, #pragma omp target teams is_device_ptr(k)); (30, OpenMP, #pragma omp target teams is_device_ptr(raa)); (39, OpenMP, #pragma omp target teams is_device_ptr(rh)); (33, OpenMP, #pragma omp target teams is_device_ptr(h))"}
{"predict": "(3, TBB, tbb::parallel_for( tbb::blocked_range<int>(0,m,100), GrowBy<MyVector>(v), tbb::simple_partitioner() );)", "label": "(3, TBB, tbb::parallel_for( tbb::blocked_range<int>(0,m,1000), GrowBy(v) );)"}
{"predict": "(50, OpenMP, #pragma omp parallel reduction(+ : o)); (63, OpenMP, #pragma omp parallel reduction(+ : r)); (66, OpenMP, #pragma omp parallel reduction(+ : fl)); (36, OpenMP, #pragma omp parallel reduction(^ : S1)); (29, OpenMP, #pragma omp parallel reduction(| : argc,); (71, OpenMP, #pragma omp parallel reduction(+ : m)); (46, OpenMP, #pragma omp parallel reduction(& : e, g)); (56, OpenMP, #pragma omp parallel private(i), reduction(+ : j), reduction(+ : q)); (62, OpenMP, #pragma omp parallel reduction(max : j)); (19, OpenMP, #pragma omp parallel reduction(+ : a, b, c, d, f)); (10, OpenMP, #pragma omp parallel reduction(); (2, OpenMP, #pragma omp parallel); (25, OpenMP, #pragma omp parallel reduction(min : a, b, c, d, f)); (44, OpenMP, #pragma omp parallel reduction(+ : p), reduction(+ : p)); (55, OpenMP, #pragma omp parallel reduction(+ : o)); (36, OpenMP, #pragma omp parallel reduction(&& : S2::S2s)); (12, OpenMP, #pragma omp parallel reduction(* : ca)); (37, OpenMP, #pragma omp parallel reduction(&& : S2::S2sc)); (45, OpenMP, #pragma omp parallel reduction(+ : h, k, B::x)); (19, OpenMP, #pragma omp parallel reduction(max : h.b)); (12, OpenMP, #pragma omp parallel reduction(+ : ba)); (20, OpenMP, #pragma omp parallel reduction(+ : z, o)); (54, OpenMP, #pragma omp parallel reduction(+ : fl)); (23, OpenMP, #pragma omp parallel reduction(^ : fl)); (-12, OpenMP, #pragma omp parallel reduction); (25, OpenMP, #pragma omp parallel reduction(&& : S2sc)); (15, OpenMP, #pragma omp parallel reduction(- : da)); (52, OpenMP, #pragma omp parallel private(k)); (43, OpenMP, #pragma omp parallel reduction(+ : p)); (48,", "label": "(54, OpenMP, #pragma omp parallel reduction(&& : S2::S2sc)); (25, OpenMP, #pragma omp parallel reduction(foo : argc); (44, OpenMP, #pragma omp parallel reduction(* : ca)); (78, OpenMP, #pragma omp for reduction(- : fl)); (28, OpenMP, #pragma omp parallel reduction(~ : argc)); (53, OpenMP, #pragma omp parallel reduction(+ : h, k, B::x)); (82, OpenMP, #pragma omp parallel reduction(task, + : m)); (79, OpenMP, #pragma omp parallel reduction(+ : m)); (36, OpenMP, #pragma omp parallel reduction(+ : ba)); (39, OpenMP, #pragma omp parallel reduction(- : da)); (59, OpenMP, #pragma omp parallel reduction(+ : r)); (5, OpenMP, #pragma omp parallel reduction(); (50, OpenMP, #pragma omp parallel private(i), reduction(+ : j), reduction(+ : q)); (47, OpenMP, #pragma omp parallel reduction(+ : o)); (-2, OpenMP, #pragma omp parallel); (37, OpenMP, #pragma omp parallel reduction(&& : S2::S2s)); (2, OpenMP, #pragma omp parallel reduction(-); (17, OpenMP, #pragma omp parallel reduction(&& : argc, z)); (-4, OpenMP, #pragma omp parallel reduction); (52, OpenMP, #pragma omp parallel shared(i)); (44, OpenMP, #pragma omp parallel private(k)); (21, OpenMP, #pragma omp parallel reduction(max : h.b)); (6, OpenMP, #pragma omp parallel reduction(| : argc,); (15, OpenMP, #pragma omp parallel reduction(+ : a, b, c, d, f)); (49, OpenMP, #pragma omp parallel reduction(max : j)); (5, OpenMP, #pragma omp parallel reduction(|| : argc > 0 ? argv[1] : argv[2])); (14, OpenMP, #pragma omp parallel reduction(min : a, b, c, d, f)); (45, OpenMP, #pragma omp parallel reduction(min : i)); (-4, OpenMP, #pragma omp parallel reduction(\\)); (-9, OpenMP, #pragma omp parallel reduction()); (60, OpenMP, #pragma omp parallel reduction(+:gptr[:argc])); (45, OpenMP, #pragma omp for private(fl)); (-10, OpenMP, #pragma omp parallel reduction(*)); (17, OpenMP, #pragma omp parallel reduction(^ : fl)); (2, OpenMP, #pragma omp parallel reduction(^ : S1)); (30, OpenMP, #pragma omp parallel reduction(+ : p), reduction(+ : p)); (20, OpenMP, #pragma omp parallel reduction(& : e, g))"}
{"predict": "(7, CUDA, cuda_safe_call(cudaMalloc((void**) &dX, N * sizeof(double)));); (8, CUDA, cuda_safe_call(cudaMalloc((void**) &dZ, N * sizeof(double)));); (6, CUDA, cuda_safe_call(cudaMalloc((void**) &dY, N * sizeof(double)));)", "label": "(8, CUDA, cuda_safe_call(cudaMalloc((void**) &dX, N * sizeof(double)));); (8, CUDA, cuda_safe_call(cudaMalloc((void**) &dY, N * sizeof(double)));); (8, CUDA, cuda_safe_call(cudaMalloc((void**) &dZ, N * sizeof(double)));); (11, CUDA, cuda_safe_call(cudaDeviceSynchronize());)"}
{"predict": "(8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);)", "label": "(8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);)"}
{"predict": "(17, MPI, MPI_Bcast(state, NUM_VARS * (nz + 2 * hs) * (nx + 2 * hs), MPI_DOUBLE, 0, MPI_COMM_WORLD);)", "label": "(5, OpenACC, #pragma acc parallel loop)"}
{"predict": "(3, CUDA, cudaSetDevice(0);); (16, CUDA, cudaGetDeviceCount(&deviceCount);); (8, CUDA, cudaDeviceReset();)", "label": "(18, CUDA, gpuErrchk(cudaGetDeviceCount(&deviceCount));); (1, CUDA, gpuErrchk(cudaSetDevice(0));); (2, CUDA, gpuErrchk(cudaFree(0));); (7, CUDA, gpuErrchk(cudaDeviceReset());)"}
{"predict": "(36, MPI, MPI_Error_string(ERR_CODE, aux_string, &L);)", "label": "(36, MPI, MPI_Error_string(ERR_CODE, aux_string, &L);)"}
{"predict": "(30, MPI, MPI_Bcast( work, diag_len[p], MPI_DOUBLE, pkk, grid->comm );); (27, MPI, MPI_Bcast( work, lwork, MPI_DOUBLE, pkk, grid->comm );)", "label": "(30, MPI, MPI_Bcast( work, diag_len[p], MPI_DOUBLE, pkk, grid->comm );); (27, MPI, MPI_Bcast( work, lwork, MPI_DOUBLE, pkk, grid->comm );)"}
{"predict": "(13, MPI, MPI_Allreduce(bins_ptr, global_ptr, bins_size, MPI_DOUBLE, MPI_SUM, mpi_comm);); (19, MPI, MPI_Allreduce(bins_ptr, global_ptr, bins_size, MPI_DOUBLE, MPI_MAX, mpi_comm);); (14, MPI, MPI_Allreduce(bins_ptr, global_ptr, bins_size, MPI_DOUBLE, MPI_AVERAGE, mpi_comm);); (15, MPI, MPI_Allreduce(bins_ptr, global_ptr, bins_size, MPI_DOUBLE, MPI_MIN, mpi_comm);); (21, MPI, MPI_Allreduce(bins_ptr, global_ptr, bins_size, MPI_DOUBLE, MPI_RMS, mpi_comm);); (18, MPI, MPI_Allreduce(bins_ptr, global_ptr, bins_size, MPI_DOUBLE, MPI_VAR, mpi_comm);); (14, MPI, MPI_Allreduce(bins_ptr, global_ptr, bins_size, MPI_DOUBLE, MPI_STD, mpi_comm);); (3, MPI, MPI_Comm mpi_comm = MPI_Comm_f2c(flow::Workspace::default_mpi_comm());)", "label": "(17, MPI, MPI_Allreduce(bins_ptr, global_ptr, bins_size, MPI_DOUBLE, MPI_MIN, mpi_comm);); (12, MPI, MPI_Allreduce(bins_ptr, global_ptr, bins_size, MPI_DOUBLE, MPI_SUM, mpi_comm);); (19, MPI, MPI_Allreduce(bins_ptr, global_ptr, bins_size, MPI_DOUBLE, MPI_MAX, mpi_comm);); (2, MPI, MPI_Comm mpi_comm = MPI_Comm_f2c(flow::Workspace::default_mpi_comm());)"}
{"predict": "(10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (15, MPI, MPI_Dims_create(tests[k].size, tests[k].dim, dims);)", "label": "(10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (15, MPI, MPI_Dims_create(tests[k].size, tests[k].dim, dims);)"}
{"predict": "(18, CUDA, __shared__ float b_smem[BM * BN];); (16, CUDA, __shared__ float a_smem[BM * BK];)", "label": "(17, CUDA, __shared__ float a_smem[BM * BK];); (17, CUDA, __shared__ float b_smem[BK * BN];)"}
{"predict": "(83, OpenCL, err = clReleaseMemObject(image_buffer);); (85, OpenCL, err = clReleaseMemObject(buffer);)", "label": "(86, OpenCL, err = clReleaseMemObject(buffer);); (82, OpenCL, err = clReleaseMemObject(image_buffer);)"}
{"predict": "(42, CUDA, cudaDeviceSynchronize();); (22, CUDA, cudaMalloc((void**)&d_a, bufsize);); (23, CUDA, cudaMalloc((void**)&d_c, bufsize);); (10, CUDA, cudaMallocHost((void**)&h_a, bufsize);); (65, CUDA, cudaFree(d_c);); (63, CUDA, cudaFree(d_b);); (9, CUDA, cudaMallocHost((void**)&h_c, bufsize);); (26, CUDA, cudaProfilerStart();); (40, CUDA, cudaDeviceSynchronize();); (57, CUDA, cudaFree(d_a);); (64, CUDA, cudaFreeHost(h_b);); (64, CUDA, cudaFreeHost(h_c);); (56, CUDA, cudaFreeHost(h_a);); (14, CUDA, cudaMalloc((void**)&d_b, bufsize);); (44, CUDA, cudaProfilerStop();)", "label": "(12, CUDA, cudaMallocHost((void**)&h_a, bufsize);); (13, CUDA, cudaMallocHost((void**)&h_c, bufsize);); (43, CUDA, cudaDeviceSynchronize();); (20, CUDA, cudaMalloc((void**)&d_a, bufsize);); (63, CUDA, cudaFree(d_c);); (61, CUDA, cudaFree(d_b);); (7, CUDA, cudaMallocHost((void**)&h_b, bufsize);); (64, CUDA, cudaFreeHost(h_b);); (64, CUDA, cudaFreeHost(h_c);); (56, CUDA, cudaFree(d_a);); (60, CUDA, cudaFreeHost(h_a);); (14, CUDA, cudaMalloc((void**)&d_c, bufsize);); (12, CUDA, cudaMalloc((void**)&d_b, bufsize);)"}
{"predict": "(43, OpenMP, lis2mdl_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)", "label": "(43, OpenMP, lis2mdl_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)"}
{"predict": "(11, CUDA, __shared__ float r[_N_], k[_N_], u[_N_], w[_N_];)", "label": "(11, CUDA, __shared__ float r[_N_], k[_N_], u[_N_], w[_N_];)"}
{"predict": "(50, CUDA, flamegpu::CUDASimulation cudaSimulation(model);)", "label": "(50, CUDA, flamegpu::CUDASimulation cudaSimulation(model, argc, argv);)"}
{"predict": "(81, CUDA, cudaError_t result = cudaDeviceSynchronize();); (12, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (13, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;)", "label": "(12, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (13, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;); (80, CUDA, cudaError_t result = cudaDeviceSynchronize();)"}
{"predict": "(11, MPI, MPI_Type_get_extent(int_with_key, &lb, &extent);)", "label": "(11, MPI, MPI_Type_get_extent(int_with_key, &lb, &extent);)"}
{"predict": "(14, OpenCL, clCommandQ = clCreateCommandQueue(context, devices[0], properties, &retVal);); (32, OpenCL, clWaitForEvents(1, &eventOut);); (38, OpenCL, clReleaseEvent(eventOut);); (30, OpenCL, clReleaseEvent(callbackEvent);); (11, OpenCL, retVal = clEnqueueNDRangeKernel(clCommandQ, clKernel, 1, offset, gws, nullptr, 1, &eventBlocking, &eventOut);); (36, OpenCL, clReleaseCommandQueue(clCommandQ);)", "label": "(15, OpenCL, retVal = clEnqueueNDRangeKernel(clCommandQ, clKernel, 1, offset, gws, nullptr, 1, &eventBlocking, &eventOut);); (13, OpenCL, clCommandQ = clCreateCommandQueue(context, devices[0], properties, &retVal);); (38, OpenCL, clReleaseEvent(eventOut);); (31, OpenCL, clWaitForEvents(1, &callbackEvent);); (37, OpenCL, clReleaseCommandQueue(clCommandQ);); (28, OpenCL, clWaitForEvents(1, &eventOut);)"}
{"predict": "(33, MPI, MPI_Type_free(&type);); (16, MPI, MPI_Type_get_extent(type, &lb, &extent);); (7, MPI, err = MPI_Type_indexed(3, len, disp, MPI_INT, &type);)", "label": "(33, MPI, MPI_Type_free(&type);); (16, MPI, MPI_Type_get_extent(type, &lb, &extent);); (7, MPI, err = MPI_Type_indexed(3, len, disp, MPI_INT, &type);)"}
{"predict": "(23, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (17, MPI, MPI_Alltoallw_c(sendbuf, counts, displs, types, recvbuf, counts, displs, types, comm);)", "label": "(23, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (17, MPI, MPI_Alltoallw_c(sendbuf, counts, displs, types, recvbuf, counts, displs, types, comm);)"}
{"predict": "(9, MPI, errcode = MPI_Grequest_complete(aio_req->req);)", "label": "(10, MPI, errcode = MPI_Grequest_complete(aio_req->req);)"}
{"predict": "(18, OpenMP, #pragma omp for); (8, OpenMP, #pragma omp parallel); (9, OpenMP, #pragma omp single); (11, OpenMP, #pragma omp task shared(sum)); (23, OpenMP, #pragma omp critical)", "label": "(20, OpenMP, #pragma omp flush); (13, OpenMP, #pragma omp task private(sum) shared(result, known_sum)); (7, OpenMP, #pragma omp parallel); (8, OpenMP, #pragma omp single); (21, OpenMP, #pragma omp critical)"}
{"predict": "(20, OpenCL, clReleaseMemObject(mobjAP_);); (16, OpenCL, clReleaseMemObject(mobjX_);)", "label": "(20, OpenCL, clReleaseMemObject(mobjAP_);); (16, OpenCL, clReleaseMemObject(mobjX_);)"}
{"predict": "(9, MPI, MPI_Comm_rank(comm, &comm_rank);); (10, MPI, MPI_Comm_size(comm, &comm_size);)", "label": "(9, MPI, MPI_Comm_rank(comm, &comm_rank);); (10, MPI, MPI_Comm_size(comm, &comm_size);)"}
{"predict": "(23, CUDA, cudaMalloc(reinterpret_cast<void**>(&d_messages),    message_size * batch);); (35, CUDA, cudaMemcpy(signatures.data(), d_signatures, length_signature * batch, cudaMemcpyDeviceToHost);); (21, CUDA, cudaMalloc(reinterpret_cast<void**>(&d_secret_keys), length_secret_key * batch);); (38, CUDA, cudaFree(d_signatures);); (23, CUDA, cudaMemcpy(d_secret_keys, secret_keys.data(), length_secret_key * batch, cudaMemcpyHostToDevice);); (37, CUDA, cudaFree(d_messages);); (17, CUDA, cudaMalloc(reinterpret_cast<void**>(&d_signatures),  length_signature_aligned * batch);); (36, CUDA, cudaFree(d_secret_keys);); (20, CUDA, cudaMemcpy(d_messages, messages.data(), message_size * batch, cudaMemcpyHostToDevice);)", "label": "(36, CUDA, cudaMemcpy(signatures.data(), d_signatures, length_signature_aligned * batch, cudaMemcpyDeviceToHost);); (40, CUDA, cudaFree(d_secret_keys);); (26, CUDA, cudaMemcpy(d_secret_keys, secret_keys.data(), length_secret_key * batch, cudaMemcpyHostToDevice);); (26, CUDA, cudaMemcpy(d_messages, messages.data(), message_size * batch, cudaMemcpyHostToDevice);); (39, CUDA, cudaFree(d_messages);); (17, CUDA, cudaMalloc(reinterpret_cast<void**>(&d_secret_keys), length_secret_key * batch);); (15, CUDA, cudaMalloc(reinterpret_cast<void**>(&d_signatures), length_signature_aligned * batch);); (35, CUDA, cudaFree(d_signatures);); (15, CUDA, cudaMalloc(reinterpret_cast<void**>(&d_messages), message_size * batch);)"}
{"predict": "(19, OpenMP, #pragma omp parallel for)", "label": "(19, OpenMP, #pragma omp parallel for)"}
{"predict": "(27, MPI, MPI_Finalize();); (14, MPI, MPI_Recv(&buffer, 1, MPI_INT, left, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (11, MPI, MPI_Isend(&value, 1, MPI_INT, right, tag, MPI_COMM_WORLD, &request);); (0, MPI, MPI_Init(&argc, &argv);); (11, MPI, MPI_Wait(&request, MPI_STATUS_IGNORE);); (2, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (4, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(27, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Init(NULL, NULL);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (20, MPI, MPI_Wait(&request, MPI_STATUS_IGNORE);); (6, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (9, MPI, MPI_Bcast(&factor, 1, MPI_INT, root, MPI_COMM_WORLD);)"}
{"predict": "(3, OpenMP, #pragma omp target); (4, OpenMP, #pragma omp distribute parallel for simd); (2, OpenMP, #pragma omp teams)", "label": "(3, OpenMP, #pragma omp target); (4, OpenMP, #pragma omp distribute parallel for simd); (2, OpenMP, #pragma omp teams)"}
{"predict": "(79, CUDA, CHECK(cudaFree(d_y));); (75, CUDA, CHECK(cudaFree(d_NN));); (56, CUDA, CHECK(cudaMemcpy(NL, d_NL, N*M*sizeof(int), cudaMemcpyDefault));); (34, CUDA, CHECK(cudaMalloc(&d_NL, N*M*sizeof(int)));); (32, CUDA, CHECK(cudaMalloc(&d_NN, N*sizeof(int)));); (46, CUDA, CHECK(cudaMemcpy(d_y, y.data(), N*sizeof(real), cudaMemcpyDefault));); (50, CUDA, CHECK(cudaMemcpy(NN, d_NN, N*sizeof(int), cudaMemcpyDefault));); (33, CUDA, CHECK(cudaMalloc(&d_y, N*sizeof(real)));); (68, CUDA, CHECK(cudaFree(d_x));); (30, CUDA, CHECK(cudaMalloc(&d_x, N*sizeof(real)));); (65, CUDA, CHECK(cudaFree(d_NL));)", "label": "(78, CUDA, CHECK(cudaFree(d_y));); (74, CUDA, CHECK(cudaFree(d_NN));); (56, CUDA, CHECK(cudaMemcpy(NL, d_NL, N*M*sizeof(int), cudaMemcpyDefault));); (34, CUDA, CHECK(cudaMalloc(&d_NL, N*M*sizeof(int)));); (32, CUDA, CHECK(cudaMalloc(&d_NN, N*sizeof(int)));); (46, CUDA, CHECK(cudaMemcpy(d_y, y.data(), N*sizeof(real), cudaMemcpyDefault));); (51, CUDA, CHECK(cudaMemcpy(NN, d_NN, N*sizeof(int), cudaMemcpyDefault));); (33, CUDA, CHECK(cudaMalloc(&d_y, N*sizeof(real)));); (69, CUDA, CHECK(cudaFree(d_x));); (30, CUDA, CHECK(cudaMalloc(&d_x, N*sizeof(real)));); (66, CUDA, CHECK(cudaFree(d_NL));)"}
{"predict": "(61, OpenMP, #pragma omp interop device(dev) depend(inout : ap) destroy(I)); (49, OpenMP, #pragma omp interop device(dev) depend(inout : ap) init(targetsync : I)); (88, OpenMP, #pragma omp interop init(prefer_type(\"cuda\", \"cuda_driver\", \"opencl\", \"sycl\", \"hip\", \"level_zero\"), targetsync : I)); (2, OpenMP, #pragma omp interop use(I)); (79, OpenMP, #pragma omp interop init(prefer_type(2, 4, 6, 1), targetsync : I)); (13, OpenMP, #pragma omp interop destroy(IRef)); (8, OpenMP, #pragma omp interop destroy(I)); (4, OpenMP, #pragma omp interop init(target : IRef)); (95, OpenMP, #pragma omp interop init(prefer_type(1, 2, 3, 4, 5, 6), targetsync : I)); (33, OpenMP, #pragma omp interop init(targetsync : CI)); (53, OpenMP, #pragma omp interop device(dev) depend(inout : ap) use(I)); (13, OpenMP, #pragma omp interop use(CI)); (-13, OpenMP, #pragma omp interop init(target : I))", "label": "(140, OpenMP, #pragma omp interop use(I) use(J)); (129, OpenMP, #pragma omp interop init(target:I) init(targetsync:J)); (24, OpenMP, #pragma omp interop init(target:IRef)); (31, OpenMP, #pragma omp interop destroy(IRef)); (71, OpenMP, #pragma omp interop device(dev) depend(inout:ap) destroy(I)); (159, OpenMP, #pragma omp interop destroy(I) use(J)); (4, OpenMP, #pragma omp interop use(I)); (141, OpenMP, #pragma omp interop destroy(I) destroy(J)); (47, OpenMP, #pragma omp interop device(dev) depend(inout:ap) init(targetsync:I)); (7, OpenMP, #pragma omp interop destroy(I)); (-2, OpenMP, #pragma omp interop init(target:I)); (127, OpenMP, #pragma omp interop init(target:I) use(J)); (29, OpenMP, #pragma omp interop use(CI)); (84, OpenMP, #pragma omp interop init(prefer_type(2,4,6,1),targetsync:I)); (14, OpenMP, #pragma omp interop use(IRef)); (50, OpenMP, #pragma omp interop device(dev) depend(inout:ap) use(I)); (71, OpenMP, #pragma omp interop init(prefer_type(1,2,3,4,5,6),targetsync:I)); (145, OpenMP, #pragma omp interop init(target:I) destroy(J)); (-14, OpenMP, #pragma omp interop init()"}
{"predict": "(86, CUDA, cudaFreeHost( outHost );); (84, CUDA, cudaFree( texDevice );)", "label": "(86, CUDA, cudaFreeHost( outHost );); (84, CUDA, cudaFree( texDevice );)"}
{"predict": "(31, OpenMP, #pragma omp teams reduction(+:sum)); (28, OpenMP, #pragma omp target map(tofrom: b, sum, t) map(alloc: a)); (31, OpenMP, #pragma omp distribute parallel for reduction(+:sum)); (33, OpenMP, t[i] = omp_get_team_num();)", "label": "(31, OpenMP, #pragma omp teams reduction(+:sum)); (28, OpenMP, #pragma omp target map(tofrom: b, sum, t) map(alloc: a)); (31, OpenMP, #pragma omp distribute parallel for reduction(+:sum)); (33, OpenMP, t[i] = omp_get_team_num();)"}
{"predict": "(91, OpenMP, const int NumThreads = omp_get_max_threads();)", "label": "(91, OpenMP, const int NumThreads = omp_get_max_threads();)"}
{"predict": "(6, MPI, MPI_Init(&argc, &argv);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (8, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (33, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (20, MPI, MPI_Allreduce(&errs, &toterrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);)", "label": "(6, MPI, MPI_Init(&argc, &argv);); (32, MPI, MPI_Finalize();); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (20, MPI, MPI_Reduce(&errs, &toterrs, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);)"}
{"predict": "(16, MPI, MPI_Win_free(&windows[i]);); (8, MPI, MPI_Win_create(base_ptr[i], SIZE, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &windows[i]);); (13, MPI, MPI_Win_free_keyval(&keyval_copy);); (9, MPI, MPI_Win_set_attr(windows[i], keyval, NULL);); (12, MPI, MPI_Win_free(&windows[0]);); (4, MPI, MPI_Win_create_keyval(MPI_NULL_COPY_FN, delete_fn, &keyval, NULL);); (11, MPI, MPI_Win_free_keyval(&keyval);); (3, MPI, MPI_Alloc_mem(SIZE, MPI_INFO_NULL, &base_ptr[i]);); (8, MPI, MPI_Free_mem(base_ptr[i]);)", "label": "(8, MPI, MPI_Alloc_mem(DATA_SZ, MPI_INFO_NULL, &base_ptr[0]);); (20, MPI, MPI_Win_free_keyval(&keyval);); (21, MPI, MPI_Win_free(&windows[1]);); (6, MPI, MPI_Win_create(base_ptr[0], DATA_SZ, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &windows[0]);); (12, MPI, MPI_Win_set_attr(windows[0], keyval, NULL);); (5, MPI, MPI_Alloc_mem(DATA_SZ, MPI_INFO_NULL, &base_ptr[1]);); (14, MPI, MPI_Free_mem(base_ptr[0]);); (15, MPI, MPI_Win_free_keyval(&keyval_copy);); (16, MPI, MPI_Free_mem(base_ptr[1]);); (4, MPI, MPI_Win_create_keyval(MPI_WIN_NULL_COPY_FN, delete_fn, &keyval, NULL);); (9, MPI, MPI_Win_free(&windows[0]);); (0, MPI, MPI_Win_create(base_ptr[1], DATA_SZ, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &windows[1]);); (5, MPI, MPI_Win_set_attr(windows[1], keyval, NULL);)"}
{"predict": "(2, CUDA, extern __shared__ char shared_memory[];)", "label": "(2, CUDA, extern __shared__ char shared_memory[];)"}
{"predict": "(8, CUDA, __shared__ float s_data[THREADS_PER_BLOCK * sizeof(float)];)", "label": "(5, CUDA, extern __shared__ float s_data[];)"}
{"predict": "(133, OpenMP, #pragma omp sections reduction(+ : r)); (152, OpenMP, #pragma omp parallel private(p)); (125, OpenMP, #pragma omp sections reduction(+ : p), reduction(+ : p)); (115, OpenMP, #pragma omp sections reduction(+ : h, k, B::x)); (135, OpenMP, #pragma omp parallel reduction(min : i)); (117, OpenMP, #pragma omp sections reduction(+ : o)); (78, OpenMP, #pragma omp sections reduction(* : ca)); (153, OpenMP, #pragma omp sections reduction(+ : fl)); (128, OpenMP, #pragma omp sections reduction(+ : q)); (49, OpenMP, #pragma omp sections reduction(~ : argc)); (143, OpenMP, #pragma omp parallel shared(i)); (15, OpenMP, #pragma omp sections reduction(); (111, OpenMP, #pragma omp sections reduction(+ : p)); (149, OpenMP, #pragma omp sections reduction(max : j)); (38, OpenMP, #pragma omp sections reduction(&& : argc, z)); (59, OpenMP, #pragma omp sections reduction(max : h.b)); (95, OpenMP, #pragma omp sections reduction(^ : fl)); (51, OpenMP, #pragma omp sections reduction(min : a, b, c, d, f)); (115, OpenMP, #pragma omp parallel reduction(max : h.b)); (118, OpenMP, #pragma omp parallel private(fl)); (31, OpenMP, #pragma omp sections reduction(^ : S1)); (78, OpenMP, #pragma omp sections reduction(^ : v, w)); (98, OpenMP, #pragma omp sections reduction(& : e, g)); (36, OpenMP, #pragma omp sections reduction(&& : argc)); (-3, OpenMP, #pragma omp parallel); (118, OpenMP, #pragma omp sections reduction(+ : m)); (-2, OpenMP, #pragma omp sections reduction +); (12, OpenMP, #pragma omp sections reduction()); (-9, OpenMP, #pragma omp sections reduction); (68, OpenMP, #pragma omp sections reduction(- : da)); (80, OpenMP, #pragma omp sections private", "label": "(58, OpenMP, #pragma omp sections reduction(|| : argc > 0 ? argv[1] : argv[2])); (102, OpenMP, #pragma omp sections reduction(- : da)); (131, OpenMP, #pragma omp sections reduction(+ : o)); (75, OpenMP, #pragma omp sections reduction(+ : a, b, c, d, f)); (160, OpenMP, #pragma omp sections reduction(+ : fl)); (83, OpenMP, #pragma omp sections reduction(max : h.b)); (117, OpenMP, #pragma omp sections reduction(& : e, g)); (146, OpenMP, #pragma omp sections reduction(+ : r)); (20, OpenMP, #pragma omp sections reduction(-); (129, OpenMP, #pragma omp sections private(i), reduction(+ : j), reduction(+ : q)); (164, OpenMP, #pragma omp sections reduction(+ : m)); (102, OpenMP, #pragma omp sections reduction(&& : S2::S2s)); (61, OpenMP, #pragma omp sections reduction(^ : S1)); (80, OpenMP, #pragma omp sections reduction(+ : ba)); (154, OpenMP, #pragma omp parallel reduction(* : fl)); (-3, OpenMP, #pragma omp parallel); (67, OpenMP, #pragma omp sections reduction(min : a, b, c, d, f)); (111, OpenMP, #pragma omp sections reduction(+ : h, k, B::x)); (0, OpenMP, #pragma omp sections reduction +); (-6, OpenMP, #pragma omp sections reduction); (122, OpenMP, #pragma omp parallel private(k)); (87, OpenMP, #pragma omp sections reduction(^ : fl)); (31, OpenMP, #pragma omp sections reduction(| : argc,); (120, OpenMP, #pragma omp sections reduction(+ : p), reduction(+ : p)); (154, OpenMP, #pragma omp sections reduction(task, + : m)); (13, OpenMP, #pragma omp sections reduction(*)); (132, OpenMP, #pragma omp parallel reduction(min : i)); (136, OpenMP, #pragma omp parallel private(fl)); (40, OpenMP, #pragma omp sections reduction(&& : argc, z)); (69, OpenMP, #pragma omp sections reduction(* : ca)); (3, OpenMP, #pragma omp sections reduction()); (87, OpenMP, #pragma omp sections reduction(&& : S2::S2sc)); (127, OpenMP, #pragma omp sections reduction(max : j)); (-10, OpenMP, #pragma omp sections reduction(); (9, OpenMP, #pragma omp sections reduction(\\)); (13, OpenMP, #pragma omp sections reduction(foo : argc); (121, OpenMP, #pragma omp parallel shared(i)); (26, OpenMP, #pragma omp sections reduction(~ : argc))"}
{"predict": "(2, CUDA, __shared__ uchar as[18][66];)", "label": "(2, CUDA, __shared__ uchar as[18][66];)"}
{"predict": "(52, OpenMP, #pragma omp target parallel for is_device_ptr(da)); (36, OpenMP, #pragma omp target parallel for is_device_ptr(raa)); (18, OpenMP, #pragma omp target parallel for is_device_ptr(k)); (26, OpenMP, #pragma omp target parallel for is_device_ptr(z)); (45, OpenMP, #pragma omp target parallel for is_device_ptr(h)); (49, OpenMP, #pragma omp target parallel for is_device_ptr(rh)); (30, OpenMP, #pragma omp target parallel for is_device_ptr(aa))", "label": "(25, OpenMP, #pragma omp target parallel for is_device_ptr(z)); (36, OpenMP, #pragma omp target parallel for is_device_ptr(raa)); (29, OpenMP, #pragma omp target parallel for is_device_ptr(aa)); (46, OpenMP, #pragma omp target parallel for is_device_ptr(rh)); (39, OpenMP, #pragma omp target parallel for is_device_ptr(h)); (14, OpenMP, #pragma omp target parallel for is_device_ptr(k)); (49, OpenMP, #pragma omp target parallel for is_device_ptr(da))"}
{"predict": "(4, OpenMP, #pragma omp parallel); (7, OpenMP, #pragma omp for collapse(2) private(i,j) reduction(&&:my_islarger))", "label": "(11, OpenMP, #pragma omp ordered); (3, OpenMP, #pragma omp parallel); (13, OpenMP, #pragma omp critical); (5, OpenMP, #pragma omp for private(i,j) schedule(static,1) collapse(2) ordered)"}
{"predict": "(5, OpenMP, #pragma omp section); (29, OpenMP, #pragma omp sections private(g, sivar)); (3, OpenMP, #pragma omp parallel); (57, OpenMP, #pragma omp sections private(t_var, vec, s_arr, s_arr, var, var, sivar)); (69, OpenMP, #pragma omp sections private(A::x)); (31, OpenMP, #pragma omp parallel); (32, OpenMP, #pragma omp section)", "label": "(60, OpenMP, #pragma omp sections lastprivate(t_var, vec, s_arr, var, sivar)); (4, OpenMP, #pragma omp section); (2, OpenMP, #pragma omp parallel); (28, OpenMP, #pragma omp sections lastprivate(g, sivar)); (65, OpenMP, #pragma omp sections lastprivate(CONDITIONAL A::x, B::x))"}
{"predict": "(6, CUDA, cudaMalloc(&d_clk, THREAD * sizeof(uint32_t));); (30, CUDA, cudaFree(d_ret);); (7, CUDA, cudaMalloc(&d_ret, THREAD * sizeof(void *));); (23, CUDA, cudaMemcpy(h_clk, d_clk, THREAD * sizeof(uint32_t), cudaMemcpyDeviceToHost);); (27, CUDA, cudaFree(d_ptr);); (26, CUDA, cudaFree(d_clk);); (3, CUDA, cudaMalloc(&d_ptr, THREAD * sizeof(void *));); (9, CUDA, cudaMemcpy(d_ptr, h_ptr, THREAD * sizeof(void *), cudaMemcpyHostToDevice);)", "label": "(10, CUDA, cudaMallocHost(&h_ptr, THREAD * sizeof(void *));); (29, CUDA, cudaFree(d_ptr);); (25, CUDA, cudaMemcpy(h_clk, d_clk, THREAD * sizeof(uint32_t), cudaMemcpyDeviceToHost);); (28, CUDA, cudaFree(d_ret);); (12, CUDA, cudaMemcpy(d_ptr, h_ptr, THREAD * sizeof(void *), cudaMemcpyHostToDevice);); (27, CUDA, cudaFree(d_clk);); (-1, CUDA, cudaMalloc(&d_ptr, THREAD * sizeof(void *));); (26, CUDA, cudaFreeHost(h_ptr);); (-1, CUDA, cudaMalloc(&d_clk, THREAD * sizeof(uint32_t));); (-3, CUDA, cudaMalloc(&d_ret, sizeof(void *));)"}
{"predict": "(18, TBB, typedef typename oneapi::tbb::combinable< std::vector<T> > CombinableType;); (6, TBB, oneapi::tbb::task_scheduler_init init(p);); (17, TBB, if (Verbose && t == 0) oneapi::tbb::tick_count t0 = oneapi::tbb::tick_count::now();)", "label": "(8, TBB, oneapi::tbb::global_control gc(oneapi::tbb::global_control::max_allowed_parallelism, p);); (23, TBB, oneapi::tbb::parallel_for( oneapi::tbb::blocked_range<int> (0, N, 10000), ParallelVectorForBody<T>( vs ) );); (0, TBB, typedef std::vector<T, oneapi::tbb::tbb_allocator<T> > ContainerType;); (17, TBB, typedef typename oneapi::tbb::combinable< ContainerType > CombinableType;)"}
{"predict": "(29, CUDA, cudaMalloc((void **)&d_C_ref, m * n * sizeof(float));); (68, CUDA, cudaEventElapsedTime(&elapsed_time, start, stop);); (30, CUDA, cudaMemcpy(d_A, A, m * k * sizeof(float), cudaMemcpyHostToDevice);); (32, CUDA, cudaMemcpy(d_C_ref, C_ref, m * n * sizeof(float), cudaMemcpyHostToDevice);); (36, CUDA, cudaMemcpy(C, d_C, m * n * sizeof(float), cudaMemcpyDeviceToHost);); (61, CUDA, cudaEventRecord(stop);); (61, CUDA, cudaEventSynchronize(stop);); (15, CUDA, cudaMalloc((void **)&d_B, k * n * sizeof(float));); (13, CUDA, cudaMalloc((void **)&d_A, m * k * sizeof(float));); (52, CUDA, cudaEventRecord(start);); (13, CUDA, cudaMalloc((void **)&d_C, m * n * sizeof(float));); (23, CUDA, cudaMemcpy(d_C, C, m * n * sizeof(float), cudaMemcpyHostToDevice);); (21, CUDA, cudaMemcpy(d_B, B, k * n * sizeof(float), cudaMemcpyHostToDevice);); (46, CUDA, cudaEventCreate(&stop);); (44, CUDA, cudaEventCreate(&start);)", "label": "(29, CUDA, cudaMalloc((void **)&d_C_ref, m * n * sizeof(float));); (68, CUDA, cudaEventElapsedTime(&elapsed_time, start, stop);); (30, CUDA, cudaMemcpy(d_A, A, m * k * sizeof(float), cudaMemcpyHostToDevice);); (32, CUDA, cudaMemcpy(d_C_ref, C_ref, m * n * sizeof(float), cudaMemcpyHostToDevice);); (36, CUDA, cudaMemcpy(C, d_C, m * n * sizeof(float), cudaMemcpyDeviceToHost);); (61, CUDA, cudaEventRecord(stop);); (61, CUDA, cudaEventSynchronize(stop);); (15, CUDA, cudaMalloc((void **)&d_B, k * n * sizeof(float));); (13, CUDA, cudaMalloc((void **)&d_A, m * k * sizeof(float));); (52, CUDA, cudaEventRecord(start);); (13, CUDA, cudaMalloc((void **)&d_C, m * n * sizeof(float));); (23, CUDA, cudaMemcpy(d_C, C, m * n * sizeof(float), cudaMemcpyHostToDevice);); (21, CUDA, cudaMemcpy(d_B, B, k * n * sizeof(float), cudaMemcpyHostToDevice);); (46, CUDA, cudaEventCreate(&stop);); (44, CUDA, cudaEventCreate(&start);)"}
{"predict": "(6, OpenMP, #pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp parallel for)"}
{"predict": "(25, CUDA, cudaMalloc((void**)(&d_t_right), t_right_bytes);); (28, CUDA, cudaMemcpy(d_t_right, t_right.data(), t_right_bytes, cudaMemcpyHostToDevice);); (23, CUDA, cudaMalloc((void**)(&d_t_left), t_left_bytes);); (26, CUDA, cudaMemcpy(d_t_left, t_left.data(), t_left_bytes, cudaMemcpyHostToDevice);); (49, CUDA, cudaFree(d_t_result);); (22, CUDA, cudaMalloc((void**)(&d_t_result), t_result_bytes);); (46, CUDA, cudaFree(d_t_right);); (44, CUDA, cudaFree(d_t_left);); (36, CUDA, cudaMemcpy(t_result_gpu.data(), d_t_result, t_result_bytes, cudaMemcpyDeviceToHost);)", "label": "(25, CUDA, cudaMalloc((void**)(&d_t_right), t_right_bytes);); (25, CUDA, cudaMalloc((void**)(&d_t_result), t_result_bytes);); (50, CUDA, cudaFree((void*)d_t_left);); (26, CUDA, cudaMemcpy(d_t_right, t_right.data(), t_right_bytes, cudaMemcpyHostToDevice);); (20, CUDA, cudaMalloc((void**)(&d_t_left), t_left_bytes);); (23, CUDA, cudaMemcpy(d_t_left, t_left.data(), t_left_bytes, cudaMemcpyHostToDevice);); (48, CUDA, cudaFree((void*)d_t_result);); (46, CUDA, cudaFree((void*)d_t_right);); (36, CUDA, cudaMemcpy(t_result_gpu.data(), d_t_result, t_result_bytes, cudaMemcpyDeviceToHost);)"}
{"predict": "(2, OpenMP, #pragma omp parallel num_threads(2) shared(data, out, flag)); (9, OpenMP, #pragma omp barrier); (16, OpenMP, #pragma omp atomic read); (19, OpenMP, #pragma omp atomic write); (12, OpenMP, #pragma omp atomic compare_and_swap)", "label": "(22, OpenMP, #pragma omp flush(out)); (17, OpenMP, #pragma omp atomic read); (10, OpenMP, #pragma omp atomic write); (-1, OpenMP, #pragma omp target parallel num_threads(64) map(tofrom); (6, OpenMP, #pragma omp flush(data))"}
{"predict": "(2, MPI, MPI_Init(&argc, &argv);); (34, MPI, MPI_Finalize();); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (12, MPI, MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, rank, MPI_INFO_NULL, &comm_node);); (8, MPI, MPI_Comm_size(comm_node, &size_node);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (27, MPI, MPI_Barrier(MPI_COMM_WORLD);); (10, MPI, MPI_Get_processor_name(procname, &namelen);); (26, MPI, MPI_Comm_free(&comm_node);); (5, MPI, MPI_Comm_rank(comm_node, &rank_node);)", "label": "(12, MPI, MPI_Comm_size(comm_node, &size_node);); (1, MPI, MPI_Init(&argc, &argv);); (32, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (13, MPI, MPI_Get_processor_name(procname, &namelen);); (5, MPI, MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, 0, MPI_INFO_NULL, &comm_node);); (-1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (6, MPI, MPI_Comm_rank(comm_node, &rank_node);); (19, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(12, OpenMP, #pragma omp parallel for)", "label": "(12, OpenMP, #pragma omp parallel for)"}
{"predict": "(37, CUDA, HANDLE_ERROR( cudaEventRecord( stop, 0 ) );); (52, CUDA, HANDLE_ERROR( cudaEventDestroy( start ) );); (6, CUDA, HANDLE_ERROR( cudaEventCreate( &stop ) );); (22, CUDA, HANDLE_ERROR( cudaHostGetDevicePointer( &dev_b, b, 0 ) );); (51, CUDA, HANDLE_ERROR( cudaEventDestroy( stop ) );); (27, CUDA, HANDLE_ERROR( cudaEventRecord( start, 0 ) );); (18, CUDA, HANDLE_ERROR( cudaHostGetDevicePointer( &dev_a, a, 0 ) );); (-2, CUDA, HANDLE_ERROR( cudaEventCreate( &start ) );)", "label": "(56, CUDA, HANDLE_ERROR( cudaEventDestroy( start ) );); (6, CUDA, HANDLE_ERROR( cudaEventCreate( &stop ) );); (55, CUDA, HANDLE_ERROR( cudaEventDestroy( stop ) );); (36, CUDA, HANDLE_ERROR( cudaThreadSynchronize() );); (20, CUDA, HANDLE_ERROR( cudaHostGetDevicePointer( &dev_b, b, 0 ) );); (18, CUDA, HANDLE_ERROR( cudaHostGetDevicePointer( &dev_a, a, 0 ) );); (28, CUDA, HANDLE_ERROR( cudaEventRecord( start, 0 ) );); (33, CUDA, HANDLE_ERROR( cudaEventRecord( stop, 0 ) );); (33, CUDA, HANDLE_ERROR( cudaEventSynchronize( stop ) );); (43, CUDA, HANDLE_ERROR( cudaFreeHost( b ) );); (41, CUDA, HANDLE_ERROR( cudaFreeHost( a ) );); (42, CUDA, HANDLE_ERROR( cudaFreeHost( partial_c ) );); (-6, CUDA, HANDLE_ERROR( cudaEventCreate( &start ) );)"}
{"predict": "(13, MPI, mpi_err = MPI_Type_commit(&type);); (9, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(13, MPI, mpi_err = MPI_Type_commit(&type);); (9, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(9, CUDA, __shared__ float s_variance;); (7, CUDA, __shared__ float s_mean;)", "label": "(9, CUDA, __shared__ float s_variance;); (7, CUDA, __shared__ float s_mean;)"}
{"predict": "(16, CUDA, __shared__ float Bsh[_M_ * _K_];)", "label": "(16, CUDA, extern __shared__ volatile float Bsh[];)"}
{"predict": "(19, CUDA, cudaMemcpy(pointers, cpuPointers, sizeof(uint8_t*) * total * 3, cudaMemcpyHostToDevice);); (6, CUDA, uint8_t ** pointers = (uint8_t**)FastllmCudaMalloc(sizeof(uint8_t*) * total * 3);); (21, CUDA, FastllmCudaFree(pointers);)", "label": "(19, CUDA, cudaMemcpy(pointers, cpuPointers, sizeof(uint8_t*) * total * 3, cudaMemcpyHostToDevice);); (6, CUDA, uint8_t ** pointers = (uint8_t**)FastllmCudaMalloc(sizeof(uint8_t*) * total * 3);); (20, CUDA, FastllmCudaFree(pointers);)"}
{"predict": "(25, OpenMP, #pragma omp parallel default(shared)); (10, OpenMP, #pragma omp parallel); (37, OpenMP, #pragma omp parallel num_threads(x)); (15, OpenMP, #pragma omp parallel default(none)); (28, OpenMP, #pragma omp parallel default(firstprivate))", "label": "(25, OpenMP, #pragma omp parallel default(shared)); (10, OpenMP, #pragma omp parallel); (37, OpenMP, #pragma omp parallel num_threads(x)); (15, OpenMP, #pragma omp parallel default(none)); (28, OpenMP, #pragma omp parallel default(firstprivate))"}
{"predict": "(41, CUDA, __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];); (36, CUDA, __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];)", "label": "(41, CUDA, __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];); (36, CUDA, __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];)"}
{"predict": "(12, CUDA, __shared__ unsigned char sharedHistogram[NUM_THREADS][cBuckets];)", "label": "(12, CUDA, __shared__ unsigned char sharedHistogram[NUM_THREADS][cBuckets];)"}
{"predict": "(25, TBB, tbb::flow::make_edge(bnode, tbb::flow::input_port<0>(jnode));); (7, TBB, tbb::flow::make_edge(bnode, fnode);); (22, TBB, tbb::flow::join_node<std::tuple<int,int>,tbb::flow::reserving> jnode(g);); (-1, TBB, tbb::flow::sequencer_node<int> bnode(g, seq_body());); (-3, TBB, tbb::flow::graph g;); (32, TBB, g.reset(tbb::flow::rf_clear_edges);); (-2, TBB, tbb::flow::function_node<int> fnode(g, tbb::flow::serial, serial_fn_body<int>(serial_fn_state0));); (15, TBB, tbb::flow::remove_edge(bnode, fnode);)", "label": "(25, TBB, tbb::flow::make_edge(bnode, tbb::flow::input_port<0>(jnode));); (7, TBB, tbb::flow::make_edge(bnode, fnode);); (22, TBB, tbb::flow::join_node<std::tuple<int,int>,tbb::flow::reserving> jnode(g);); (-1, TBB, tbb::flow::sequencer_node<int> bnode(g, seq_body());); (-3, TBB, tbb::flow::graph g;); (32, TBB, g.reset(tbb::flow::rf_clear_edges);); (-2, TBB, tbb::flow::function_node<int> fnode(g, tbb::flow::serial, serial_fn_body<int>(serial_fn_state0));); (15, TBB, tbb::flow::remove_edge(bnode, fnode);)"}
{"predict": "(13, CUDA, extern __shared__ char shared_mem[];)", "label": "(13, CUDA, extern __shared__ char shared_mem[];)"}
{"predict": "(51, CUDA, cuda_safe_call(cudaStreamSynchronize(stream));)", "label": "(51, CUDA, cuda_safe_call(cudaStreamSynchronize(stream));)"}
{"predict": "(7, OpenMP, #pragma omp taskloop simd allocate(d) if(taskloop: argc > N) default(shared) untied priority(N) safelen(N) linear(c) aligned(ptr) grainsize(N) reduction(+:g) in_reduction(+: d)); (13, OpenMP, #pragma omp parallel); (13, OpenMP, #pragma omp taskloop simd private(argc, b), firstprivate(c, d), lastprivate(d, f) collapse(N) shared(g) if (c) final(d) mergeable priority(f) simdlen(N) nogroup num_tasks(N)); (3, OpenMP, #pragma omp taskgroup task_reduction(+: d) allocate(d))", "label": "(15, OpenMP, #pragma omp taskloop simd private(argc, b), firstprivate(c, d), lastprivate(d, f) collapse(N) shared(g) if (c) final(d) mergeable priority(f) simdlen(N) nogroup num_tasks(N)); (13, OpenMP, #pragma omp parallel); (5, OpenMP, #pragma omp taskloop simd allocate(d) if(taskloop: argc > N) default(shared) untied priority(N) safelen(N) linear(c) aligned(ptr) grainsize(N) reduction(+:g) in_reduction(+: d)); (3, OpenMP, #pragma omp taskgroup task_reduction(+: d) allocate(d))"}
{"predict": "(10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (43, MPI, mpi_errno = MPI_Comm_free(&stream_comm);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (15, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (43, MPI, mpi_errno = MPI_Comm_free(&stream_comm);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (15, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(16, CUDA, extern __shared__ float buffer[];)", "label": "(16, CUDA, extern __shared__ float buffer[];)"}
{"predict": "(8, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc((void**)&d_out, size * sizeof(DataType)));); (32, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(h_out.data(), d_out, size * sizeof(DataType), cudaMemcpyDeviceToHost));); (28, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (35, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc((void**)&d_ref, size * sizeof(DataType)));); (48, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(d_out));); (16, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc((void**)&states, block_dim * grid_size * sizeof(RNG)));); (19, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (44, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(states));); (45, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(d_ref));); (39, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(h_ref.data(), d_ref, size * sizeof(DataType), cudaMemcpyDeviceToHost));)", "label": "(8, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc((void**)&d_out, size * sizeof(DataType)));); (32, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(h_out.data(), d_out, size * sizeof(DataType), cudaMemcpyDeviceToHost));); (28, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (35, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc((void**)&d_ref, size * sizeof(DataType)));); (48, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(d_out));); (16, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc((void**)&states, block_dim * grid_size * sizeof(RNG)));); (19, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (44, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(states));); (45, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(d_ref));); (39, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(h_ref.data(), d_ref, size * sizeof(DataType), cudaMemcpyDeviceToHost));)"}
{"predict": "(36, OpenCL, err = clSetKernelArg(scan_kernel, 1, dyn_mem_size, NULL);); (31, OpenCL, err = clSetKernelArg(scan_kernel, 0, sizeof(buffer), &buffer);)", "label": "(36, OpenCL, err = clSetKernelArg(scan_kernel, 1, dyn_mem_size, NULL);); (31, OpenCL, err = clSetKernelArg(scan_kernel, 0, sizeof(buffer), &buffer);)"}
{"predict": "(22, MPI, err = MPI_Type_create_resized(tmptype, 0, aext * sizeof(int), datatype);); (28, MPI, MPI_Type_free(&tmptype);); (12, MPI, err = MPI_Type_create_struct(1, &blen, &displ, &type, &tmptype);)", "label": "(22, MPI, err = MPI_Type_create_resized(tmptype, 0, aext * sizeof(int), datatype);); (28, MPI, MPI_Type_free(&tmptype);); (12, MPI, err = MPI_Type_create_struct(1, &blen, &displ, &type, &tmptype);)"}
{"predict": "(19, OpenMP, #pragma omp distribute parallel for proc_bind(master); (3, OpenMP, #pragma omp distribute parallel for proc_bind); (12, OpenMP, #pragma omp distribute parallel for proc_bind()); (6, OpenMP, #pragma omp distribute parallel for proc_bind(); (25, OpenMP, #pragma omp distribute parallel for proc_bind(x)); (37, OpenMP, #pragma omp distribute parallel for proc_bind(spread)); (18, OpenMP, #pragma omp distribute parallel for proc_bind(close), proc_bind(spread)); (-5, OpenMP, #pragma omp target); (31, OpenMP, #pragma omp parallel proc_bind(close)); (26, OpenMP, #pragma omp distribute parallel for proc_bind(master)); (-7, OpenMP, #pragma omp teams)", "label": "(19, OpenMP, #pragma omp distribute parallel for proc_bind(master); (3, OpenMP, #pragma omp distribute parallel for proc_bind); (12, OpenMP, #pragma omp distribute parallel for proc_bind()); (6, OpenMP, #pragma omp distribute parallel for proc_bind(); (25, OpenMP, #pragma omp distribute parallel for proc_bind(x)); (37, OpenMP, #pragma omp distribute parallel for proc_bind(spread)); (18, OpenMP, #pragma omp distribute parallel for proc_bind(close), proc_bind(spread)); (-5, OpenMP, #pragma omp target); (31, OpenMP, #pragma omp parallel proc_bind(close)); (26, OpenMP, #pragma omp distribute parallel for proc_bind(master)); (-7, OpenMP, #pragma omp teams)"}
{"predict": "(19, OpenCL, retVal = clEnqueueReleaseGLObjects(commandQueue, 1, &glBuffer, 0, nullptr, &retEvent);); (30, OpenCL, retVal = clReleaseMemObject(glBuffer);); (3, OpenCL, auto glBuffer = clCreateFromGLBuffer(&context, 0, bufferId, &retVal);); (1, OpenCL, auto commandQueue = clCreateCommandQueue(&context, context.getDevice(0), 0, &retVal);); (12, OpenCL, retVal = clReleaseEvent(retEvent);); (1, OpenCL, retVal = clEnqueueAcquireGLObjects(commandQueue, 1, &glBuffer, 0, nullptr, &retEvent);); (23, OpenCL, retVal = clReleaseCommandQueue(commandQueue);)", "label": "(19, OpenCL, retVal = clEnqueueReleaseGLObjects(commandQueue, 1, &glBuffer, 0, nullptr, &retEvent);); (30, OpenCL, retVal = clReleaseMemObject(glBuffer);); (3, OpenCL, auto glBuffer = clCreateFromGLBuffer(&context, 0, bufferId, &retVal);); (1, OpenCL, auto commandQueue = clCreateCommandQueue(&context, context.getDevice(0), 0, &retVal);); (12, OpenCL, retVal = clReleaseEvent(retEvent);); (1, OpenCL, retVal = clEnqueueAcquireGLObjects(commandQueue, 1, &glBuffer, 0, nullptr, &retEvent);); (23, OpenCL, retVal = clReleaseCommandQueue(commandQueue);)"}
{"predict": "(21, MPI, MPI_Type_set_attr(type, key[2], &attrval[2]);); (22, MPI, MPI_Type_set_attr(type, key[0], &attrval[0]);); (56, MPI, MPI_Type_free_keyval(&key[i]);); (19, MPI, MPI_Type_set_attr(type, key[1], &attrval[1]);); (23, MPI, MPI_Type_delete_attr(type, key[0]);); (23, MPI, MPI_Type_delete_attr(type, key[1]);); (23, MPI, MPI_Type_delete_attr(type, key[2]);)", "label": "(21, MPI, MPI_Type_set_attr(type, key[2], &attrval[2]);); (22, MPI, MPI_Type_set_attr(type, key[0], &attrval[0]);); (56, MPI, MPI_Type_free_keyval(&key[i]);); (19, MPI, MPI_Type_set_attr(type, key[1], &attrval[1]);); (23, MPI, MPI_Type_delete_attr(type, key[0]);); (23, MPI, MPI_Type_delete_attr(type, key[1]);); (23, MPI, MPI_Type_delete_attr(type, key[2]);)"}
{"predict": "(8, TBB, tbb::enumerable_thread_specific<HyperedgeWeight> deltas(0);)", "label": "(8, TBB, tbb::enumerable_thread_specific<HyperedgeWeight> deltas(0);)"}
{"predict": "(42, MPI, MPI_Recv(receiveBuffer.data(), msgsize, MPI_INT, 1, MPI_ANY_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (7, MPI, MPI_Init(&argc, &argv);); (48, MPI, MPI_Send(message.data(), msgsize, MPI_INT, 0, 1, MPI_COMM_WORLD);); (54, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (33, MPI, MPI_Send(message.data(), msgsize, MPI_INT, 1, tag, MPI_COMM_WORLD);); (8, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (41, MPI, MPI_Recv(receiveBuffer.data(), msgsize, MPI_INT, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status);)", "label": "(42, MPI, MPI_Recv(receiveBuffer.data(), msgsize, MPI_INT, 1, MPI_ANY_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (7, MPI, MPI_Init(&argc, &argv);); (48, MPI, MPI_Send(message.data(), msgsize, MPI_INT, 0, 1, MPI_COMM_WORLD);); (54, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (33, MPI, MPI_Send(message.data(), msgsize, MPI_INT, 1, tag, MPI_COMM_WORLD);); (8, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (41, MPI, MPI_Recv(receiveBuffer.data(), msgsize, MPI_INT, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status);)"}
{"predict": "(9, CUDA, CUDA_CHECK(cudaGetDeviceCount(&ndevices));); (9, CUDA, CUDA_CHECK(cudaSetDevice(rank % ndevices));); (0, MPI, MPI_Init(&argc, &argv);); (37, MPI, MPI_Finalize();); (1, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)", "label": "(9, CUDA, CUDA_CHECK(cudaGetDeviceCount(&ndevices));); (9, CUDA, CUDA_CHECK(cudaSetDevice(rank % ndevices));); (0, MPI, MPI_Init(&argc, &argv);); (37, MPI, MPI_Finalize();); (1, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)"}
{"predict": "(7, OpenMP, #pragma omp parallel for)", "label": "(7, OpenMP, #pragma omp parallel for)"}
{"predict": "(12, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (13, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;)", "label": "(12, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (13, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;)"}
{"predict": "(16, OpenCL, clCreateBuffer_StubWithCallback(clCreateBuffer_testBufferConstructorContextIterator);); (35, OpenCL, clReleaseCommandQueue_ExpectAndReturn(make_command_queue(0), CL_SUCCESS);); (43, OpenCL, clReleaseMemObject_ExpectAndReturn(expected, CL_SUCCESS);); (24, OpenCL, clCreateCommandQueueWithProperties_StubWithCallback(clCreateCommandQueueWithProperties_testCommandQueueFromSpecifiedContext);); (31, OpenCL, clReleaseEvent_StubWithCallback(clReleaseEvent_testCopyHostToBuffer);); (41, OpenCL, clReleaseContext_ExpectAndReturn(make_context(0), CL_SUCCESS);); (26, OpenCL, clEnqueueMapBuffer_StubWithCallback(clEnqueueMapBuffer_testCopyHostToBuffer);); (26, OpenCL, clEnqueueUnmapMemObject_StubWithCallback(clEnqueueUnmapMemObject_testCopyHostToBuffer);)", "label": "(16, OpenCL, clCreateBuffer_StubWithCallback(clCreateBuffer_testBufferConstructorContextIterator);); (35, OpenCL, clReleaseCommandQueue_ExpectAndReturn(make_command_queue(0), CL_SUCCESS);); (43, OpenCL, clReleaseMemObject_ExpectAndReturn(expected, CL_SUCCESS);); (24, OpenCL, clCreateCommandQueueWithProperties_StubWithCallback(clCreateCommandQueueWithProperties_testCommandQueueFromSpecifiedContext);); (31, OpenCL, clReleaseEvent_StubWithCallback(clReleaseEvent_testCopyHostToBuffer);); (41, OpenCL, clReleaseContext_ExpectAndReturn(make_context(0), CL_SUCCESS);); (26, OpenCL, clEnqueueMapBuffer_StubWithCallback(clEnqueueMapBuffer_testCopyHostToBuffer);); (26, OpenCL, clEnqueueUnmapMemObject_StubWithCallback(clEnqueueUnmapMemObject_testCopyHostToBuffer);)"}
{"predict": "(33, MPI, MPI_Cart_shift(cart_comm, 0, 1, &source, &destination);); (52, MPI, t1 = MPI_Wtime();); (12, MPI, MPI_Init(&argc, &argv);); (57, MPI, MPI_Finalize();); (11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (27, MPI, MPI_Cart_create(MPI_COMM_WORLD, ndims, dims, periods, 1, &cart_comm);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (52, MPI, MPI_Comm_free(&cart_comm);); (27, MPI, MPI_Comm_rank(cart_comm, &cart_id);); (29, MPI, MPI_Barrier(MPI_COMM_WORLD);); (29, MPI, t0 = MPI_Wtime();)", "label": "(33, MPI, MPI_Cart_shift(cart_comm, 0, 1, &source, &destination);); (52, MPI, t1 = MPI_Wtime();); (12, MPI, MPI_Init(&argc, &argv);); (57, MPI, MPI_Finalize();); (11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (27, MPI, MPI_Cart_create(MPI_COMM_WORLD, ndims, dims, periods, 1, &cart_comm);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (52, MPI, MPI_Comm_free(&cart_comm);); (27, MPI, MPI_Comm_rank(cart_comm, &cart_id);); (29, MPI, MPI_Barrier(MPI_COMM_WORLD);); (29, MPI, t0 = MPI_Wtime();)"}
{"predict": "(8, MPI, MPI_Comm_rank(mpi_comm, &rank);); (6, MPI, MPI_Comm mpi_comm = MPI_Comm_f2c(vtkh::GetMPICommHandle());); (56, MPI, MPI_Bcast(&entropy, 1, MPI_DOUBLE, 0, mpi_comm);)", "label": "(8, MPI, MPI_Comm_rank(mpi_comm, &rank);); (6, MPI, MPI_Comm mpi_comm = MPI_Comm_f2c(vtkh::GetMPICommHandle());); (56, MPI, MPI_Bcast(&entropy, 1, MPI_DOUBLE, 0, mpi_comm);)"}
{"predict": "(9, CUDA, CHECK(cudaMallocManaged((void **)&z, M));); (23, CUDA, CHECK(cudaFree(x));); (5, CUDA, CHECK(cudaMallocManaged((void **)&x, M));); (22, CUDA, CHECK(cudaFree(y));); (22, CUDA, CHECK(cudaFree(z));); (16, CUDA, CHECK(cudaDeviceSynchronize());); (2, CUDA, CHECK(cudaMallocManaged((void **)&y, M));)", "label": "(9, CUDA, CHECK(cudaMallocManaged((void **)&z, M));); (23, CUDA, CHECK(cudaFree(x));); (5, CUDA, CHECK(cudaMallocManaged((void **)&x, M));); (22, CUDA, CHECK(cudaFree(y));); (22, CUDA, CHECK(cudaFree(z));); (16, CUDA, CHECK(cudaDeviceSynchronize());); (2, CUDA, CHECK(cudaMallocManaged((void **)&y, M));)"}
{"predict": "(17, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (26, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(17, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (26, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(25, OpenCL, clReleaseMemObject(scratchBuff);); (20, OpenCL, clReleaseMemObject(mobjDP_);); (15, OpenCL, clReleaseMemObject(mobjY_);); (10, OpenCL, clReleaseMemObject(mobjX_);)", "label": "(25, OpenCL, clReleaseMemObject(scratchBuff);); (20, OpenCL, clReleaseMemObject(mobjDP_);); (15, OpenCL, clReleaseMemObject(mobjY_);); (10, OpenCL, clReleaseMemObject(mobjX_);)"}
{"predict": "(5, TBB, oneapi::tbb::global_control c(oneapi::tbb::global_control::max_allowed_parallelism, nthreads);); (21, TBB, static oneapi::tbb::affinity_partitioner g_ap;)", "label": "(5, TBB, oneapi::tbb::global_control c(oneapi::tbb::global_control::max_allowed_parallelism, nthreads);); (21, TBB, static oneapi::tbb::affinity_partitioner g_ap;)"}
{"predict": "(16, SYCL, using namespace cl::sycl::access;); (4, SYCL, cl::sycl::queue queue;)", "label": "(16, SYCL, using namespace cl::sycl::access;); (4, SYCL, cl::sycl::queue queue;)"}
{"predict": "(33, OpenMP, omp_target_free(d_a, DefaultDeviceNum);); (15, OpenMP, d_a = (int *)omp_target_alloc(DataSize, DefaultDeviceNum);); (23, OpenMP, #pragma omp target update to(d_a)); (5, OpenMP, int DefaultDeviceNum = omp_get_default_device();); (16, OpenMP, omp_target_associate_ptr(p_a, d_a, DataSize, 0, DefaultDeviceNum);); (24, OpenMP, omp_target_disassociate_ptr(p_a, DefaultDeviceNum);)", "label": "(33, OpenMP, omp_target_free(d_a, DefaultDeviceNum);); (15, OpenMP, d_a = (int *)omp_target_alloc(DataSize, DefaultDeviceNum);); (23, OpenMP, #pragma omp target update to(d_a)); (5, OpenMP, int DefaultDeviceNum = omp_get_default_device();); (16, OpenMP, omp_target_associate_ptr(p_a, d_a, DataSize, 0, DefaultDeviceNum);); (24, OpenMP, omp_target_disassociate_ptr(p_a, DefaultDeviceNum);)"}
{"predict": "(51, OpenCL, clReleaseEvent(ev[i]);); (24, OpenCL, clEnqueueNDRangeKernel(queue, kernel, 1, NULL, globals, locals, 2, &ev[0], &ev[2]);); (30, OpenCL, buf_data[0] = clEnqueueMapBuffer(queue, buf[0], CL_FALSE, 0, 0, BUFFERSIZE*sizeof(int), 1, &ev[2], NULL, NULL);); (14, OpenCL, clEnqueueWriteBuffer(queue, buf[0], CL_FALSE, 0, BUFFERSIZE*sizeof(int), (void *)cpu_src, 1, &ev[0], &ev[1]);)", "label": "(51, OpenCL, clReleaseEvent(ev[i]);); (24, OpenCL, clEnqueueNDRangeKernel(queue, kernel, 1, NULL, globals, locals, 2, &ev[0], &ev[2]);); (30, OpenCL, buf_data[0] = clEnqueueMapBuffer(queue, buf[0], CL_FALSE, 0, 0, BUFFERSIZE*sizeof(int), 1, &ev[2], NULL, NULL);); (14, OpenCL, clEnqueueWriteBuffer(queue, buf[0], CL_FALSE, 0, BUFFERSIZE*sizeof(int), (void *)cpu_src, 1, &ev[0], &ev[1]);)"}
{"predict": "(15, CUDA, checkCudaErrors(cudaMalloc((void **)&ringbuf, sizeof(qsortRingbuf)));); (28, CUDA, checkCudaErrors(cudaEventRecord(ev1));); (40, CUDA, checkCudaErrors(cudaEventRecord(ev2));); (38, CUDA, checkCudaErrors(cudaGetLastError());); (62, CUDA, checkCudaErrors(cudaFree(ringbuf));); (38, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (21, CUDA, checkCudaErrors(cudaEventCreate(&ev1));); (21, CUDA, checkCudaErrors(cudaEventCreate(&ev2));); (58, CUDA, checkCudaErrors(cudaFree(gpustack));)", "label": "(15, CUDA, checkCudaErrors(cudaMalloc((void **)&ringbuf, sizeof(qsortRingbuf)));); (28, CUDA, checkCudaErrors(cudaEventRecord(ev1));); (40, CUDA, checkCudaErrors(cudaEventRecord(ev2));); (38, CUDA, checkCudaErrors(cudaGetLastError());); (61, CUDA, checkCudaErrors(cudaFree(ringbuf));); (38, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (21, CUDA, checkCudaErrors(cudaEventCreate(&ev1));); (21, CUDA, checkCudaErrors(cudaEventCreate(&ev2));); (58, CUDA, checkCudaErrors(cudaFree(gpustack));)"}
{"predict": "(6, OpenMP, #pragma omp parallel num_threads(8) shared(var)); (7, OpenMP, omp_set_nest_lock(&lock);); (9, OpenMP, omp_unset_nest_lock(&lock);); (1, OpenMP, omp_init_nest_lock(&lock);); (13, OpenMP, omp_destroy_nest_lock(&lock);)", "label": "(6, OpenMP, #pragma omp parallel num_threads(8) shared(var)); (7, OpenMP, omp_set_nest_lock(&lock);); (9, OpenMP, omp_unset_nest_lock(&lock);); (1, OpenMP, omp_init_nest_lock(&lock);); (13, OpenMP, omp_destroy_nest_lock(&lock);)"}
{"predict": "(19, OpenMP, #pragma omp parallel for)", "label": "(19, OpenMP, #pragma omp parallel for)"}
{"predict": "(7, OpenMP, #pragma omp parallel sections reduction(task,*:f)); (14, OpenMP, #pragma omp parallel sections default(none), private(argc, b) firstprivate(argv) if (argc > 0) num_threads(ee) copyin(a) proc_bind(spread) reduction(| : c, d) reduction(* : e) lastprivate(argv)); (2, OpenMP, #pragma omp threadprivate(a)); (16, OpenMP, #pragma omp section)", "label": "(7, OpenMP, #pragma omp parallel sections reduction(task,*:f)); (14, OpenMP, #pragma omp parallel sections default(none), private(argc, b) firstprivate(argv) if (argc > 0) num_threads(ee) copyin(a) proc_bind(spread) reduction(| : c, d) reduction(* : e) lastprivate(argv)); (2, OpenMP, #pragma omp threadprivate(a)); (16, OpenMP, #pragma omp section)"}
{"predict": "(15, OpenMP, #pragma omp target teams distribute parallel for collapse(2) depend(out:image[y_start]) nowait); (6, OpenMP, double st = omp_get_wtime();); (28, OpenMP, double et = omp_get_wtime();); (25, OpenMP, #pragma omp taskwait); (5, OpenMP, #pragma omp target data map(alloc:image[0:width*height])); (18, OpenMP, #pragma omp target update from(image[block*block_size:block_size]))", "label": "(15, OpenMP, #pragma omp target teams distribute parallel for collapse(2) depend(out:image[y_start]) nowait); (6, OpenMP, double st = omp_get_wtime();); (28, OpenMP, double et = omp_get_wtime();); (25, OpenMP, #pragma omp taskwait); (5, OpenMP, #pragma omp target data map(alloc:image[0:width*height])); (18, OpenMP, #pragma omp target update from(image[block*block_size:block_size]))"}
{"predict": "(36, MPI, MPI_Type_size(c_info->s_data_type, &s_size);); (36, MPI, MPI_Type_size(c_info->r_data_type, &r_size);)", "label": "(36, MPI, MPI_Type_size(c_info->s_data_type, &s_size);); (36, MPI, MPI_Type_size(c_info->r_data_type, &r_size);)"}
{"predict": "(99, MPI, MPI_Alltoall(sendcounts, 1, MPI_INT, recvcounts, 1, MPI_INT, MPI_COMM_WORLD);); (46, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (40, MPI, MPI_Type_free(&keytype);)", "label": "(99, MPI, MPI_Alltoall(sendcounts, 1, MPI_INT, recvcounts, 1, MPI_INT, MPI_COMM_WORLD);); (46, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (40, MPI, MPI_Type_free(&keytype);)"}
{"predict": "(13, OpenMP, #pragma omp parallel default(none) shared(data,x,y,z,repeats))", "label": "(13, OpenMP, #pragma omp parallel default(none) shared(data,x,y,z,repeats))"}
{"predict": "(57, MPI, MPI_Send(sendBuffer, n_send, MPI_DOUBLE, neighbors[i], MPI_MY_TAG, MPI_COMM_WORLD);); (15, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (36, MPI, MPI_Irecv(x_external, n_recv, MPI_DOUBLE, neighbors[i], MPI_MY_TAG, MPI_COMM_WORLD, request+i);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(57, MPI, MPI_Send(sendBuffer, n_send, MPI_DOUBLE, neighbors[i], MPI_MY_TAG, MPI_COMM_WORLD);); (15, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (36, MPI, MPI_Irecv(x_external, n_recv, MPI_DOUBLE, neighbors[i], MPI_MY_TAG, MPI_COMM_WORLD, request+i);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(5, OpenMP, #pragma omp parallel num_threads(4)); (11, OpenMP, #pragma omp for schedule(static) nowait); (8, OpenMP, rank = omp_get_thread_num();); (16, OpenMP, #pragma omp atomic); (20, OpenMP, #pragma omp for schedule(static))", "label": "(5, OpenMP, #pragma omp parallel num_threads(4)); (11, OpenMP, #pragma omp for schedule(static) nowait); (8, OpenMP, rank = omp_get_thread_num();); (16, OpenMP, #pragma omp atomic); (20, OpenMP, #pragma omp for schedule(static))"}
{"predict": "(1, OpenMP, #pragma omp parallel for ordered ordered); (15, OpenMP, #pragma omp parallel for order(concurrent)); (8, OpenMP, #pragma omp parallel for order(none); (4, OpenMP, #pragma omp parallel for order(); (0, OpenMP, #pragma omp parallel for order); (8, OpenMP, #pragma omp parallel for order(concurrent)", "label": "(1, OpenMP, #pragma omp parallel for ordered ordered); (15, OpenMP, #pragma omp parallel for order(concurrent)); (8, OpenMP, #pragma omp parallel for order(none); (4, OpenMP, #pragma omp parallel for order(); (0, OpenMP, #pragma omp parallel for order); (8, OpenMP, #pragma omp parallel for order(concurrent)"}
{"predict": "(14, OpenMP, printf(\"Thread %d of %d\\n\", omp_get_thread_num(), omp_get_num_threads());)", "label": "(14, OpenMP, printf(\"Thread %d of %d\\n\", omp_get_thread_num(), omp_get_num_threads());)"}
{"predict": "(43, CUDA, CUDA_CHECK_GOTO(cudaGetDevice(&self->device), free_mpool, status);); (15, CUDA, cuda_st = cudaGetDeviceCount(&num_devices);)", "label": "(43, CUDA, CUDA_CHECK_GOTO(cudaGetDevice(&self->device), free_mpool, status);); (15, CUDA, cuda_st = cudaGetDeviceCount(&num_devices);)"}
{"predict": "(25, CUDA, assert(cudaMemcpyAsync(out.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);); (11, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (9, CUDA, cudaMalloc((void**)(&d_in1), in1_bytes);); (11, CUDA, cudaMemcpy(d_in1, in1.data(), in1_bytes, cudaMemcpyHostToDevice);); (39, CUDA, cudaFree(d_out);); (21, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (36, CUDA, cudaFree(d_in1);)", "label": "(25, CUDA, assert(cudaMemcpyAsync(out.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);); (11, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (9, CUDA, cudaMalloc((void**)(&d_in1), in1_bytes);); (11, CUDA, cudaMemcpy(d_in1, in1.data(), in1_bytes, cudaMemcpyHostToDevice);); (39, CUDA, cudaFree(d_out);); (21, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (36, CUDA, cudaFree(d_in1);)"}
{"predict": "(43, OpenCL, clReleaseMemObject(imageDesc.mem_object);); (14, OpenCL, imageDesc.mem_object = clCreateBuffer(&context, CL_MEM_READ_WRITE, imageDesc.image_height * imageDesc.image_width, nullptr, &retVal);)", "label": "(43, OpenCL, clReleaseMemObject(imageDesc.mem_object);); (14, OpenCL, imageDesc.mem_object = clCreateBuffer(&context, CL_MEM_READ_WRITE, imageDesc.image_height * imageDesc.image_width, nullptr, &retVal);)"}
{"predict": "(9, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)", "label": "(9, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)"}
