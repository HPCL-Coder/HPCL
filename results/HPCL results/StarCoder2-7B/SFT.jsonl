{"predict": "(42, CUDA, cudaFree(dev_max);); (42, CUDA, cudaFree(dev_denom);); (22, CUDA, CUDA_CHECK(cudaMemcpy(dev_denom, host_denom, sizeof(FP) * SEQLEN * DIM, cudaMemcpyHostToDevice));); (18, CUDA, CUDA_CHECK(cudaMalloc((void **)&dev_max, sizeof(FP) * SEQLEN * DIM));); (30, CUDA, CUDA_CHECK(cudaGetLastError());); (35, CUDA, cudaFree(QK);); (17, CUDA, CUDA_CHECK(cudaMemcpy(dev_max, host_max, sizeof(FP) * SEQLEN * DIM, cudaMemcpyHostToDevice));); (13, CUDA, CUDA_CHECK(cudaMalloc((void **)&dev_denom, sizeof(FP) * SEQLEN * DIM));); (12, CUDA, CUDA_CHECK(cudaMalloc((void **)&QK, sizeof(FP) * SEQLEN * SEQLEN));)", "label": "(20, CUDA, CUDA_CHECK(cudaMalloc((void **)&dev_max, sizeof(FP) * SEQLEN * DIM));); (39, CUDA, cudaFree(dev_max);); (22, CUDA, CUDA_CHECK(cudaMemcpy(dev_denom, host_denom, sizeof(FP) * SEQLEN * DIM, cudaMemcpyHostToDevice));); (18, CUDA, CUDA_CHECK(cudaMalloc((void **)&dev_denom, sizeof(FP) * SEQLEN * DIM));); (18, CUDA, CUDA_CHECK(cudaMalloc((void **)&QK, sizeof(FP) * SEQLEN * SEQLEN));); (18, CUDA, CUDA_CHECK(cudaMemcpy(dev_max, host_max, sizeof(FP) * SEQLEN * DIM, cudaMemcpyHostToDevice));); (33, CUDA, cudaFree(QK);); (34, CUDA, cudaFree(dev_denom);)"}
{"predict": "(15, OpenMP, #pragma omp critical(out)); (35, OpenMP, #pragma omp critical(cerr))", "label": "(32, OpenMP, #pragma omp critical(cerr)); (15, OpenMP, #pragma omp critical(out))"}
{"predict": "(5, MPI, MPI_T_init_thread(required, &provided);); (7, MPI, MPI_Abort(MPI_COMM_WORLD, -1);); (29, MPI, MPI_T_finalize();); (2, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)", "label": "(31, MPI, MPI_T_finalize();); (4, MPI, MPI_T_init_thread(required, &provided);); (6, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(29, CUDA, cudaFree(data);)", "label": "(19, OpenMP, #pragma omp parallel for simd reduction(+:totalSum) schedule(guided) aligned(data: 64) num_threads(N_THREADS)); (10, OpenMP, #pragma omp parallel for num_threads(N_THREADS))"}
{"predict": "(15, CUDA, cudaMemcpy(printfbuf_local, bufptr, sizeof(cuPrintfHeader), cudaMemcpyDeviceToHost);)", "label": "(15, CUDA, cudaMemcpy(printfbuf_local, bufptr, CUPRINTF_MAX_LEN, cudaMemcpyDeviceToHost);)"}
{"predict": "(34, OpenMP, #pragma omp sections firstprivate(x)); (10, OpenMP, #pragma omp sections firstprivate(,); (15, OpenMP, #pragma omp sections firstprivate(, )); (4, OpenMP, #pragma omp sections firstprivate()); (41, OpenMP, #pragma omp sections firstprivate(x, y, z)); (35, OpenMP, #pragma omp sections firstprivate(x, y)); (23, OpenMP, #pragma omp sections firstprivate(0)); (38, OpenMP, #pragma omp sections firstprivate(x, y, z, x)); (17, OpenMP, #pragma omp sections firstprivate(int)); (26, OpenMP, #pragma omp sections firstprivate(x))", "label": "(26, OpenMP, #pragma omp parallel sections firstprivate(int)); (10, OpenMP, #pragma omp parallel sections firstprivate(,); (14, OpenMP, #pragma omp parallel sections firstprivate(, )); (34, OpenMP, #pragma omp parallel sections lastprivate(x) firstprivate(x)); (17, OpenMP, #pragma omp parallel sections firstprivate()); (36, OpenMP, #pragma omp parallel sections lastprivate(x, y) firstprivate(x, y)); (25, OpenMP, #pragma omp parallel sections firstprivate(0)); (-3, OpenMP, #pragma omp parallel sections firstprivate(); (37, OpenMP, #pragma omp parallel sections lastprivate(x, y, z) firstprivate(x, y, z))"}
{"predict": "(17, MPI, MPI_Recv(b, 10, MPI_INT, rank, 0, MPI_COMM_WORLD, &status);); (28, MPI, MPI_Recv(b, 10, MPI_INT, rank, 0, MPI_COMM_WORLD, &status);); (15, MPI, MPI_Send(a, 10, MPI_INT, rank, 0, MPI_COMM_WORLD);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (26, MPI, MPI_Send(a, 10, MPI_INT, rank, 0, MPI_COMM_WORLD);); (13, MPI, MPI_Wait(&request, &status);); (13, MPI, MPI_Get_count(&status, MPI_INT, &count);); (5, MPI, MPI_Barrier(MPI_COMM_WORLD);); (13, MPI, MPI_Recv(b, 10, MPI_INT, rank, 0, MPI_COMM_WORLD, &status);); (2, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (3, MPI, MPI_Send(a, 10, MPI_INT, rank, 0, MPI_COMM_WORLD);)", "label": "(29, MPI, MPI_Recv(NULL, 0, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &status);); (16, MPI, MPI_Isend(a, 0, MPI_INT, rank, 0, MPI_COMM_WORLD, &request);); (24, MPI, MPI_Wait(&request, &status);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (24, MPI, MPI_Isend(NULL, 0, MPI_INT, rank, 0, MPI_COMM_WORLD, &request);); (14, MPI, MPI_Get_count(&status, MPI_INT, &count);); (12, MPI, MPI_Recv(b, 1, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &status);)"}
{"predict": "(14, OpenCL, retVal = clReleaseMemObject(image);)", "label": "(14, OpenCL, retVal = clReleaseMemObject(image);)"}
{"predict": "(53, OpenMP, #pragma omp parallel)", "label": "(60, OpenMP, #pragma omp parallel for)"}
{"predict": "(20, CUDA, CHECK(cudaMalloc((void**) &anchorsDev, 4 * params.anchorsRatioCount * params.anchorsScaleCount * sizeof(float)));)", "label": "(17, CUDA, CHECK(cudaMalloc((void**) &anchorsDev, 4 * params.anchorsRatioCount * params.anchorsScaleCount * sizeof(float)));)"}
{"predict": "(3, CUDA, extern __shared__ float entire_buffer[];)", "label": "(3, CUDA, extern __shared__ float entire_buffer[];)"}
{"predict": "(24, CUDA, cudaFree(res_dev);); (17, CUDA, cudaMalloc(&res_dev, sizeof(float));); (19, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)", "label": "(24, CUDA, cudaFree(res_dev);); (17, CUDA, cudaMalloc(&res_dev, sizeof(float));); (19, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(20, OpenCL, void *mappedPtr1 = clEnqueueMapBuffer(&cmdQ, b1, CL_TRUE, CL_MAP_WRITE, 0, 8, 0, nullptr, nullptr, nullptr);); (32, OpenCL, clReleaseEvent(gatingEvent);); (14, OpenCL, auto b1 = clCreateBuffer(context, CL_MEM_READ_WRITE, 20, nullptr, nullptr);); (24, OpenCL, clEnqueueUnmapMemObject(&cmdQ, b1, mappedPtr1, 0, nullptr, nullptr);); (15, OpenCL, auto mappedPtr2 = clEnqueueMapBuffer(&cmdQ, b2, CL_TRUE, CL_MAP_READ, 0, 8, 1, &gatingEvent, nullptr, nullptr);); (28, OpenCL, clReleaseMemObject(b1);); (10, OpenCL, auto b2 = clCreateBuffer(context, CL_MEM_READ_WRITE, 20, nullptr, nullptr);); (27, OpenCL, clReleaseMemObject(b2);); (2, OpenCL, auto gatingEvent = clCreateUserEvent(context, nullptr);)", "label": "(16, OpenCL, auto b1 = clCreateBuffer(context, CL_MEM_READ_WRITE, 20, nullptr, nullptr);); (24, OpenCL, void *ptr2 = clEnqueueMapBuffer(&cmdQ, b2, CL_TRUE, CL_MAP_READ, 0, 8, 0, nullptr, nullptr, nullptr);); (32, OpenCL, clReleaseMemObject(b2);); (17, OpenCL, void *ptr1 = clEnqueueMapBuffer(&cmdQ, b1, CL_FALSE, CL_MAP_READ, 0, 8, 1, &gatingEvent, nullptr, nullptr);); (29, OpenCL, clReleaseMemObject(b1);); (31, OpenCL, clReleaseEvent(gatingEvent);); (25, OpenCL, clEnqueueUnmapMemObject(pCmdQ, b2, ptr2, 0, nullptr, nullptr);); (12, OpenCL, auto gatingEvent = clCreateUserEvent(context, nullptr);); (13, OpenCL, clEnqueueUnmapMemObject(&cmdQ, b1, ptr1, 0, nullptr, nullptr);); (8, OpenCL, auto b2 = clCreateBuffer(context, CL_MEM_READ_WRITE, 20, nullptr, nullptr);)"}
{"predict": "(64, CUDA, cudaEventSynchronize(stop);); (37, CUDA, cudaMalloc((void**)&gpuMatrix, NUM * sizeof(float));); (37, CUDA, cudaMalloc((void**)&gpuTransposeMatrix, NUM * sizeof(float));); (112, CUDA, cudaFree(gpuMatrix);); (47, CUDA, cudaEventRecord(stop, NULL);); (46, CUDA, cudaMemcpy(gpuMatrix, Matrix, NUM * sizeof(float), cudaMemcpyHostToDevice);); (59, CUDA, cudaMemcpy(TransposeMatrix, gpuTransposeMatrix, NUM * sizeof(float), cudaMemcpyDeviceToHost);); (33, CUDA, cudaEventRecord(start, NULL);); (110, CUDA, cudaFree(gpuTransposeMatrix);); (6, CUDA, cudaGetDeviceProperties(&devProp, 0);); (13, CUDA, cudaEventCreate(&stop);); (11, CUDA, cudaEventCreate(&start);); (42, CUDA, cudaEventElapsedTime(&eventMs, start, stop);)", "label": "(53, CUDA, cudaEventSynchronize(stop);); (86, CUDA, cudaMemcpy(TransposeMatrix, gpuTransposeMatrix, NUM * sizeof(float), cudaMemcpyDeviceToHost);); (9, CUDA, cudaGetDeviceProperties(&devProp, 0);); (44, CUDA, cudaMemcpy(gpuMatrix, Matrix, NUM * sizeof(float), cudaMemcpyHostToDevice);); (114, CUDA, cudaFree(gpuMatrix);); (32, CUDA, cudaMalloc((void**)&gpuMatrix, NUM * sizeof(float));); (49, CUDA, cudaEventElapsedTime(&eventMs, start, stop);); (36, CUDA, cudaEventRecord(start, NULL);); (112, CUDA, cudaFree(gpuTransposeMatrix);); (42, CUDA, cudaEventRecord(stop, NULL);); (10, CUDA, cudaEventCreate(&stop);); (28, CUDA, cudaMalloc((void**)&gpuTransposeMatrix, NUM * sizeof(float));); (6, CUDA, cudaEventCreate(&start);)"}
{"predict": "(54, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (4, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (52, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamDestroy(stream));)", "label": "(56, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (4, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (35, CUDA, ASSERT_EQ(cudaSuccess, cudaMemsetAsync(dstData->basePtr(), 0, dstBufSize * sizeof(T), stream));); (54, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamDestroy(stream));)"}
{"predict": "(38, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (43, MPI, MPI_Allgather(&item_offset, 1, MPI_INT, rank_disps, 1, MPI_INT, MPI_COMM_WORLD);)", "label": "(38, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (44, MPI, MPI_Allgather(&item_offset, 1, MPI_INT, rank_disps, 1, MPI_INT, MPI_COMM_WORLD);); (80, MPI, reduce_start = MPI_Wtime();)"}
{"predict": "(62, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 1, NULL, threads, localThreads, 0, NULL, NULL );); (47, OpenCL, error = clSetKernelArg( kernel, 0, sizeof( streams[0] ), &streams[0] );); (48, OpenCL, error = clSetKernelArg( kernel, 1, sizeof( streams[1] ), &streams[1] );); (59, OpenCL, error = clEnqueueReadBuffer( queue, streams[1], true, 0, sizeof( cl_double ) * TEST_SIZE, outData, 0, NULL, NULL );)", "label": "(58, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 1, NULL, threads, localThreads, 0, NULL, NULL );); (46, OpenCL, error = clSetKernelArg( kernel, 0, sizeof( streams[ 0 ] ), &streams[0] );); (60, OpenCL, error = clEnqueueReadBuffer( queue, streams[1], true, 0, sizeof( cl_double ) * TEST_SIZE, outData, 0, NULL, NULL );); (46, OpenCL, error = clSetKernelArg( kernel, 1, sizeof( streams[ 1 ] ), &streams[1] );)"}
{"predict": "(57, CUDA, cudaFree(d_A);); (57, CUDA, cudaFree(d_B);); (57, CUDA, cudaFree(d_C);); (24, CUDA, cudaMemcpy(d_A, A, m * k * sizeof(float), cudaMemcpyHostToDevice);); (24, CUDA, cudaMemcpy(d_B, B, k * n * sizeof(float), cudaMemcpyHostToDevice);); (17, CUDA, cudaMalloc(&d_A, m * k * sizeof(float));); (26, CUDA, cudaMemcpy(C, d_C, m * n * sizeof(float), cudaMemcpyDeviceToHost);); (17, CUDA, cudaMalloc(&d_C, m * n * sizeof(float));); (15, CUDA, cudaMalloc(&d_B, k * n * sizeof(float));); (46, CUDA, cudaFree(d_C);)", "label": "(26, CUDA, cudaMemcpy(d_A, A, m * k * sizeof(float), cudaMemcpyHostToDevice);); (56, CUDA, cudaFree(d_A);); (31, CUDA, cudaMemcpy(C, d_C, m * n * sizeof(float), cudaMemcpyDeviceToHost);); (55, CUDA, cudaFree(d_B);); (55, CUDA, cudaFree(d_C);); (17, CUDA, cudaMalloc((void **)&d_B, k * n * sizeof(float));); (15, CUDA, cudaMalloc((void **)&d_A, m * k * sizeof(float));); (16, CUDA, cudaMalloc((void **)&d_C, m * n * sizeof(float));); (20, CUDA, cudaMemcpy(d_C, C, m * n * sizeof(float), cudaMemcpyHostToDevice);); (18, CUDA, cudaMemcpy(d_B, B, k * n * sizeof(float), cudaMemcpyHostToDevice);)"}
{"predict": "(104, CUDA, cudaCheck(cudaFree(d_inp));); (40, CUDA, cudaCheck(cudaMemset(d_dinp, 0, B * T * 3 * C * sizeof(float)));); (94, CUDA, cudaCheck(cudaFree(d_qkvr));); (104, CUDA, cudaCheck(cudaFree(d_dinp));); (34, CUDA, cudaCheck(cudaMalloc(&d_dpreatt, B * T * 3 * C * sizeof(float)));); (92, CUDA, cudaCheck(cudaFree(d_dpreatt));); (100, CUDA, cudaCheck(cudaFree(d_datt));); (30, CUDA, cudaCheck(cudaMalloc(&d_dout, B * T * C * sizeof(float)));); (26, CUDA, cudaCheck(cudaMalloc(&d_qkvr, B * T * 3 * C * sizeof(float)));); (32, CUDA, cudaCheck(cudaMemcpy(d_dout, dout, B * T * C * sizeof(float), cudaMemcpyHostToDevice));); (34, CUDA, cudaCheck(cudaMemcpy(d_qkvr, qkv, B * T * 3 * C * sizeof(float), cudaMemcpyHostToDevice));); (28, CUDA, cudaCheck(cudaMalloc(&d_dqkvr, B * T * 3 * C * sizeof(float)));); (24, CUDA, cudaCheck(cudaMalloc(&d_att, B * T * C * sizeof(float)));); (25, CUDA, cudaCheck(cudaMalloc(&d_datt, B * T * C * sizeof(float)));); (20, CUDA, cudaCheck(cudaMalloc(&d_out, B * T * C * sizeof(float)));); (21, CUDA, cudaCheck(cudaMalloc(&d_preatt, B * T * 3 * C * sizeof(float)));); (19, CUDA, cudaCheck(cudaMalloc(&d_inp, B * T * 3 * C * sizeof(float)));); (18, CUDA, cudaCheck(cudaMalloc(&d_scratch, B * T * 3 * C * sizeof(float)));); (19, CUDA, cudaCheck(cudaMemcpy", "label": "(45, CUDA, cudaCheck(cudaMemset(d_dinp, 0, B * T * 3 * C * sizeof(float)));); (47, CUDA, cudaCheck(cudaMalloc(&d_dpreatt, B * NH * T * T * sizeof(float)));); (99, CUDA, cudaCheck(cudaFree(d_inp));); (38, CUDA, cudaCheck(cudaMemset(d_att, 0, B * NH * T * T * sizeof(float)));); (105, CUDA, cudaCheck(cudaFree(d_dpreatt));); (102, CUDA, cudaCheck(cudaFree(d_dinp));); (102, CUDA, cudaCheck(cudaFree(d_dqkvr));); (28, CUDA, cudaCheck(cudaMemset(d_qkvr, 0, B * T * 3 * C * sizeof(float)));); (31, CUDA, cudaCheck(cudaMemset(d_preatt, 0, B * NH * T * T * sizeof(float)));); (93, CUDA, cudaCheck(cudaFree(d_qkvr));); (30, CUDA, cudaCheck(cudaMalloc(&d_att, B * NH * T * T * sizeof(float)));); (99, CUDA, cudaCheck(cudaFree(d_datt));); (35, CUDA, cudaCheck(cudaMemset(d_dqkvr, 0, B * T * 3 * C * sizeof(float)));); (24, CUDA, cudaCheck(cudaMemset(d_out, 0, B * T * C * sizeof(float)));); (90, CUDA, cudaCheck(cudaFree(d_preatt));); (27, CUDA, cudaCheck(cudaMalloc(&d_dout, B * T * C * sizeof(float)));); (18, CUDA, cudaCheck(cudaMalloc(&d_qkvr, B * T * 3 * C * sizeof(float)));); (16, CUDA, cudaCheck(cudaMemcpy(d_inp, qkv, B * T * 3 * C * sizeof(float), cudaMemcpyHostToDevice));); (35, CUDA, cudaCheck(cudaMemset(d_scratch, 0, B * T * C * sizeof(float)));); (19, CUDA, cudaCheck(cudaMalloc(&d_preatt, B * NH * T * T * sizeof(float)));); (91, CUDA, cudaCheck(cudaFree(d_scratch));); (22, CUDA, cudaCheck(cudaMemcpy(d_dout, dout, B * T * C * sizeof(float), cudaMemcpyHostToDevice));); (81, CUDA, cudaCheck(cudaFree(d_out));); (23, CUDA, cudaCheck(cudaMalloc(&d_dqkvr, B * T * 3 * C * sizeof(float)));); (20, CUDA, cudaCheck(cudaMalloc(&d_dinp, B * T * 3 * C * sizeof(float)));); (26, CUDA, cudaCheck(cudaMemset(d_datt, 0, B * NH * T * T * sizeof(float)));); (26, CUDA, cudaCheck(cudaMalloc(&d_scratch, B * T * C * sizeof(float)));); (9, CUDA, cudaCheck(cudaMalloc(&d_out, B * T * C * sizeof(float)));); (22, CUDA, cudaCheck(cudaMalloc(&d_datt, B * NH * T * T * sizeof(float)));); (3, CUDA, cudaCheck(cudaMalloc(&d_inp, B * T * 3 * C * sizeof(float)));); (75, CUDA, cudaCheck(cudaFree(d_att));); (18, CUDA, cudaCheck(cudaMemset(d_dpreatt, 0, B * NH * T * T * sizeof(float)));); (74, CUDA, cudaCheck(cudaFree(d_dout));)"}
{"predict": "(25, MPI, MPI_Recv(b, 10, MPI_INT, i, 27 + j, MPI_COMM_WORLD, &status);); (17, MPI, MPI_Bsend(a, 10, MPI_INT, 0, 27 + j, MPI_COMM_WORLD);); (7, MPI, MPI_Buffer_attach(buf, BUFSIZE);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)", "label": "(48, MPI, MPI_Buffer_detach(&bptr, &bl);); (17, MPI, MPI_Start(&request);); (27, MPI, MPI_Recv(b, 10, MPI_INT, i, 27 + j, MPI_COMM_WORLD, &status);); (8, MPI, MPI_Buffer_attach(buf, BUFSIZE);); (15, MPI, MPI_Wait(&request, &status);); (15, MPI, MPI_Request_free(&request);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (6, MPI, MPI_Bsend_init(a, 10, MPI_INT, 0, 27 + j, MPI_COMM_WORLD, &request);)"}
{"predict": "run_time(31, MPI, auto err = MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided);); (115, MPI, MPI_Reduce(&run_time, &max_run_time, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);); (131, MPI, MPI_Finalize();); (37, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (37, MPI, MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);); (107, MPI, MPI_Barrier(MPI_COMM_WORLD);); (28, MPI, ascent_opt[\"mpi_comm\"] = MPI_Comm_c2f(MPI_COMM_WORLD);)", "label": "(31, MPI, auto err = MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided);); (119, MPI, MPI_Finalize();); (37, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (40, MPI, ascent_opt[\"mpi_comm\"] = MPI_Comm_c2f(MPI_COMM_WORLD);); (111, MPI, MPI_Reduce(&run_time, &max_run_time, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);); (33, MPI, MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);)"}
{"predict": "(15, CUDA, CUDASimulation cudaSimulation(model);)", "label": "(15, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(54, TBB, double t2d_auto = pforTranspose2d<tbb::auto_partitioner>(N, a, b, gs);); (4, TBB, c.set_core_type(tbb::info::core_types().back());); (55, TBB, double t2d_simple = pforTranspose2d<tbb::simple_partitioner>(N, a, b, gs);); (37, TBB, double t1d_simple = pforTranspose<tbb::simple_partitioner>(N, a, b, gs);); (3, TBB, c.set_numa_id(tbb::info::numa_nodes()[0]);); (44, TBB, pforTranspose2d<tbb::auto_partitioner>(N, a, b, gs);); (37, TBB, pforTranspose<tbb::auto_partitioner>(N, a, b, gs);); (44, TBB, pforTranspose2d<tbb::simple_partitioner>(N, a, b, gs);); (37, TBB, pforTranspose<tbb::simple_partitioner>(N, a, b, gs);); (27, TBB, pforTranspose<tbb::auto_partitioner>(N, a, b, gs);); (13, TBB, double t1d_auto = pforTranspose<tbb::auto_partitioner>(N, a, b, gs);); (-5, TBB, tbb::task_arena::constraints c;); (27, TBB, pforTranspose<tbb::simple_partitioner>(N, a, b, gs);); (-8, TBB, c.set_max_concurrency(std::min(8, tbb::info::default_concurrency(c)));); (-8, TBB, tbb::task_arena a(c);)", "label": "(47, TBB, double t2d_auto = pforTranspose2d<tbb::auto_partitioner>(N, a, b, gs);); (45, TBB, pforTranspose2d<tbb::auto_partitioner>(N, a, b, gs);); (4, TBB, c.set_core_type(tbb::info::core_types().back());); (1, TBB, tbb::task_arena::constraints c;); (48, TBB, double t2d_simple = pforTranspose2d<tbb::simple_partitioner>(N, a, b, gs);); (37, TBB, double t1d_simple = pforTranspose<tbb::simple_partitioner>(N, a, b, gs);); (-1, TBB, c.set_numa_id(tbb::info::numa_nodes()[0]);); (29, TBB, pforTranspose<tbb::auto_partitioner>(N, a, b, gs);); (29, TBB, double t1d_auto = pforTranspose<tbb::auto_partitioner>(N, a, b, gs);); (32, TBB, pforTranspose<tbb::simple_partitioner>(N, a, b, gs);); (-2, TBB, c.set_max_concurrency(std::min(8, tbb::info::default_concurrency(c)));); (-2, TBB, tbb::task_arena a(c);); (39, TBB, pforTranspose2d<tbb::simple_partitioner>(N, a, b, gs);)"}
{"predict": "(16, CUDA, __shared__ FP sV[Bc][dim];); (14, CUDA, __shared__ FP sQK[Br][Bc];); (16, CUDA, __shared__ FP sSafeE[Br][Bc];); (10, CUDA, __shared__ FP sO[Br][dim];); (11, CUDA, __shared__ FP sMax[Br];); (11, CUDA, __shared__ FP sDenom[Br];); (7, CUDA, __shared__ FP sK[Bc][dim];); (5, CUDA, __shared__ FP sQ[Br][dim];)", "label": "(12, CUDA, __shared__ FP sV[Bc][dim];); (19, CUDA, __shared__ FP sMax[Br];); (9, CUDA, __shared__ FP sK[Bc][dim];); (12, CUDA, __shared__ FP sQK[Br][Bc];); (13, CUDA, __shared__ FP sSafeE[Br][Bc];); (9, CUDA, __shared__ FP sO[Br][dim];); (4, CUDA, __shared__ FP sQ[Br][dim];); (12, CUDA, __shared__ FP sDenom[Br];)"}
{"predict": "(22, CUDA, cudaSetDevice(device_id_);)", "label": "(22, CUDA, CudaCheck(cudaSetDevice(device_id_));)"}
{"predict": "(7, OpenCL, void *mappedPtr = clEnqueueMapBuffer(cmdQ.get(), buffer.get(), CL_FALSE, CL_MAP_WRITE, offset, size, 0, nullptr, nullptr, &retVal);); (13, OpenCL, retVal = clEnqueueUnmapMemObject(cmdQ.get(), buffer.get(), mappedPtr, 0, nullptr, nullptr);)", "label": "(14, OpenCL, retVal = clEnqueueUnmapMemObject(cmdQ.get(), buffer.get(), mappedPtr, 0, nullptr, nullptr);); (6, OpenCL, void *mappedPtr = clEnqueueMapBuffer(cmdQ.get(), buffer.get(), CL_FALSE, CL_MAP_WRITE, offset, size, 0, nullptr, nullptr, nullptr);)"}
{"predict": "(37, OpenMP, #pragma omp target teams distribute parallel for simd if(target : argc) if (simd :argc)); (6, OpenMP, #pragma omp target teams distribute parallel for simd if); (45, OpenMP, #pragma omp target teams distribute parallel for simd if(target : argc); (60, OpenMP, #pragma omp target teams distribute parallel for simd if(parallel : i)); (65, OpenMP, #pragma omp target teams distribute parallel for simd if(target : j)); (23, OpenMP, #pragma omp target teams distribute parallel for simd if (argc > 0? argv[1] : argv[2])); (48, OpenMP, #pragma omp parallel); (55, OpenMP, #pragma omp target teams distribute parallel for simd if(parallel : i)); (12, OpenMP, #pragma omp target teams distribute parallel for simd if (argc))); (58, OpenMP, #pragma omp target teams distribute parallel for simd if(parallel : i)); (34, OpenMP, #pragma omp target teams distribute parallel for simd if(parallel : argc)); (2, OpenMP, #pragma omp target teams distribute parallel for simd if ()); (27, OpenMP, #pragma omp target teams distribute parallel for simd if(parallel :); (35, OpenMP, #pragma omp target teams distribute parallel for simd if(parallel : argc) if (parallel:argc)); (3, OpenMP, #pragma omp target teams distribute parallel for simd if (argc); (48, OpenMP, #pragma omp target teams distribute parallel for simd if(parallel : i) if (i)); (57, OpenMP, #pragma omp target teams distribute parallel for simd if(parallel : i) if (parallel:i)); (4, OpenMP, #pragma omp target teams distribute parallel for simd if (argc))); (17, OpenMP, #pragma omp target teams distribute parallel for simd if(target :); (4, OpenMP, #pragma omp target teams distribute parallel for simd if (S1)); (58, OpenMP, #pragma omp target teams distribute parallel for simd if(parallel : i) if (argc)); (51, OpenMP, #pragma omp target teams distribute parallel for simd if(target : j) if (argc", "label": "(69, OpenMP, #pragma omp parallel private(i)); (42, OpenMP, #pragma omp master taskloop lastprivate(a, b)); (49, OpenMP, #pragma omp master taskloop lastprivate(z, e, g)); (16, OpenMP, #pragma omp master taskloop lastprivate(argc); (2, OpenMP, #pragma omp parallel); (18, OpenMP, #pragma omp master taskloop lastprivate(argc,); (25, OpenMP, #pragma omp master taskloop lastprivate(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (0, OpenMP, #pragma omp master taskloop lastprivate); (47, OpenMP, #pragma omp master taskloop lastprivate(h)); (18, OpenMP, #pragma omp master taskloop lastprivate(argc > 0? argv[1] : argv[2])); (64, OpenMP, #pragma omp master taskloop lastprivate(i)); (51, OpenMP, #pragma omp master taskloop allocate(omp_thread_mem_alloc: i) lastprivate(i)); (27, OpenMP, #pragma omp master taskloop lastprivate(S1)); (57, OpenMP, #pragma omp master taskloop lastprivate(j)); (-3, OpenMP, #pragma omp master taskloop lastprivate(); (32, OpenMP, #pragma omp master taskloop lastprivate(argv[1])); (-1, OpenMP, #pragma omp master taskloop lastprivate()); (18, OpenMP, #pragma omp master taskloop lastprivate(conditional: argc) lastprivate(conditional:); (50, OpenMP, #pragma omp parallel shared(i))"}
{"predict": "(5, CUDA, cudaStream_t stream = at::cuda::getCurrentCUDAStream();)", "label": "(5, CUDA, cudaStream_t stream = at::cuda::getCurrentCUDAStream();)"}
{"predict": "(23, MPI, MPI_Wait(&request, MPI_STATUS_IGNORE);); (13, MPI, MPI_Recv(NULL, 0, MPI_BYTE, from, 100, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (3, MPI, MPI_Send(NULL, 0, MPI_BYTE, to, 100, MPI_COMM_WORLD);); (12, MPI, MPI_Send(NULL, 0, MPI_BYTE, to, 100, MPI_COMM_WORLD);); (2, MPI, MPI_Send(sbuf, size, MPI_BYTE, to, tagno, MPI_COMM_WORLD);); (15, MPI, MPI_Send(NULL, 0, MPI_BYTE, from, 100, MPI_COMM_WORLD);); (1, MPI, MPI_Recv(NULL, 0, MPI_BYTE, to, 100, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (15, MPI, MPI_Recv(rbuf, size, MPI_BYTE, from, tagno, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (-5, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(13, MPI, MPI_Recv(NULL, 0, MPI_BYTE, to, 100, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (11, MPI, MPI_Send(NULL, 0, MPI_BYTE, from, 100, MPI_COMM_WORLD);); (4, MPI, MPI_Irecv(rbuf, size, MPI_BYTE, from, tagno, MPI_COMM_WORLD, &request);); (18, MPI, MPI_Send(sbuf, size, MPI_BYTE, to, tagno, MPI_COMM_WORLD);); (20, MPI, MPI_Wait(&request, MPI_STATUS_IGNORE);)"}
{"predict": "(14, MPI, MPI_Abort(MPI_COMM_WORLD, MPIT_result);); (10, MPI, MPIT_result = MPI_T_pvar_session_create(session);); (7, MPI, MPIT_result = MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);)", "label": "(13, MPI, MPI_Abort(MPI_COMM_WORLD, MPIT_result);); (14, MPI, MPIT_result = MPI_T_pvar_session_create(session);); (8, MPI, MPIT_result = MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);)"}
{"predict": "(6, OpenMP, #pragma omp for schedule(MY_SCHEDULE)); (3, OpenMP, #pragma omp parallel); (7, OpenMP, #pragma omp atomic)", "label": "(9, OpenMP, #pragma omp atomic); (3, OpenMP, #pragma omp parallel); (5, OpenMP, #pragma omp for schedule(MY_SCHEDULE))"}
{"predict": "(13, CUDA, CUDA_CHECK(cudaGetDeviceProperties(&props, 0));); (11, CUDA, CUDA_CHECK(cudaGetDevice(&current_device_id));)", "label": "(14, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (11, CUDA, CUDA_CHECK(cudaGetDevice(&current_device_id));); (11, CUDA, CUDA_CHECK(cudaGetDeviceProperties(&props, current_device_id));)"}
{"predict": "(9, CUDA, err = cudaFree(handle->ws_device_mem);); (3, CUDA, cudaError_t err = cudaFree(handle->ws_main_mem);)", "label": "(4, CUDA, auto err = cudaFree(handle->deviceWs);); (8, CUDA, err = cudaEventDestroy(handle->wsMutex);)"}
{"predict": "(18, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(18, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(10, OpenMP, comp_add_script_line(\"Func=DynamicFunc__append_salt%s\\n\", side==2?\"2\":\"\");); (6, OpenMP, comp_add_script_line(\"Func=DynamicFunc__%s_base64_to_output1\\n\", (bOffsetHashIn1?\"append\":\"\") );); (64, OpenMP, comp_add_script_line(\"Func=DynamicFunc__%s_base16_to_output1\\n\", (bOffsetHashIn1?\"append\":\"\") );); (10, OpenMP, comp_add_script_line(\"MaxInputLen=%d\\n\",(151, OpenMP, comp_add_script_line(\"Func=DynamicFunc__%s_base64_to_output3\\n\", (bOffsetHashIn1?\"append\":\"\") );); (164, OpenMP, comp_add_script_line(\"Func=DynamicFunc__%s_base16_to_output3\\n\", (bOffsetHashIn1?\"append\":\"\") );); (15, OpenMP, comp_add_script_line(\"MaxInputLen=%d\\n\",(3, OpenMP, comp_add_script_line(\"Flag=MGF_INPUT_BYTE\\n\");); (56, OpenMP, comp_add_script_line(\"MaxInputLenX86=%d\\n\",(314, OpenMP, comp_add_script_line(\"Func=DynamicFunc__%s_base64_to_output2\\n\", (bOffsetHashIn1?\"append\":\"\") );); (151, OpenMP, comp_add_script_line(\"Func=DynamicFunc__%s_base16_to_output2\\n\", (bOffsetHashIn1?\"append\":\"\") );); (311, OpenMP, comp_add_script_line(\"Func=DynamicFunc__%s_base64_to_output4\\n\", (bOffsetHashIn1?\"append\":\"\") );); (232, OpenMP, comp_add_script_line(\"Func=DynamicFunc__%s_base64_to_output5\\n\", (bOffsetHashIn1?\"append\":\"\") );); (144,", "label": "(39, OpenMP, comp_add_script_line(\"Func=DynamicFunc__\" #C \"_crypt_input%d_to_output1_FINAL\\n\",side);); (18, OpenMP, else                 comp_add_script_line(\"Func=DynamicFunc__append_salt%s\\n\", side==2?\"2\":\"\");); (9, OpenMP, comp_add_script_line(\"Func=DynamicFunc__clean_input2_kwik\\n\");); (61, OpenMP, comp_add_script_line(\"MaxInputLen=110\\n\");); (5, OpenMP, comp_add_script_line(\"Func=DynamicFunc__set_input_len_%d\\n\", keys_hex_len);)"}
{"predict": "(50, OpenCL, clReleaseEvent(event);)", "label": "(50, OpenCL, clReleaseEvent(event);)"}
{"predict": "(17, CUDA, ColorSpinorParam cudaParam(cpuParam, inv_param, QUDA_CUDA_FIELD_LOCATION);)", "label": "(17, CUDA, ColorSpinorParam cudaParam(cpuParam, inv_param, QUDA_CUDA_FIELD_LOCATION);)"}
{"predict": "(54, MPI, MPI_Irecv(buf_recv, size_recv, MPI_BYTE, rank_recv, 0, comm, &request[num_req]);); (61, MPI, MPI_Send(buf_send, size_send, MPI_BYTE, rank_send, 0, comm);); (40, MPI, MPI_Send(&size_send, 1, MPI_INT, rank_send, 0, comm);); (53, MPI, MPI_Waitall(num_req, request, status);); (40, MPI, MPI_Irecv(&size_recv, 1, MPI_INT, rank_recv, 0, comm, &request[num_req]);)", "label": "(55, MPI, MPI_Irecv(buf_recv, size_recv, MPI_BYTE, rank_recv, 0, comm, &request[num_req]);); (25, MPI, MPI_Irecv(&size_recv, 1, MPI_INT, rank_recv, 0, comm, &request[num_req]);); (60, MPI, MPI_Isend(buf_send, size_send, MPI_BYTE, rank_send, 0, comm, &request[num_req]);); (38, MPI, MPI_Isend(&size_send, 1, MPI_INT, rank_send, 0, comm, &request[num_req]);); (41, MPI, MPI_Waitall(num_req, request, status);)"}
{"predict": "(3, CUDA, extern __shared__ float shared[];)", "label": "(3, CUDA, extern __shared__ float shared[];)"}
{"predict": "(4, OpenMP, #pragma omp parallel for)", "label": "(4, OpenMP, #pragma omp parallel for)"}
{"predict": "(31, OpenMP, #pragma omp distribute simd safelen (1))); (4, OpenMP, #pragma omp distribute simd safelen); (37, OpenMP, #pragma omp distribute simd safelen ((ST > 0)? 1 + ST : 2)); (64, OpenMP, #pragma omp distribute simd safelen (argv[1]=2)); (73, OpenMP, #pragma omp distribute simd safelen (N)); (7, OpenMP, #pragma omp distribute simd safelen ()); (67, OpenMP, #pragma omp distribute simd safelen (4)); (47, OpenMP, #pragma omp distribute simd safelen (S)); (15, OpenMP, #pragma omp distribute simd safelen (argc); (4, OpenMP, #pragma omp distribute simd safelen ()); (25, OpenMP, #pragma omp distribute simd safelen (ST); (54, OpenMP, #pragma omp distribute simd safelen (foobool(argc)), safelen (true), safelen (-5)); (10, OpenMP, #pragma omp distribute simd safelen (argc))); (4, OpenMP, #pragma omp distribute simd safelen (argc,)", "label": "(11, OpenMP, #pragma omp distribute simd safelen (); (71, OpenMP, #pragma omp distribute simd safelen (4)); (3, OpenMP, #pragma omp distribute simd safelen); (19, OpenMP, #pragma omp distribute simd safelen (argc); (51, OpenMP, #pragma omp distribute simd safelen (S)); (44, OpenMP, #pragma omp distribute simd safelen (foobool(argc)), safelen (true), safelen (-5)); (72, OpenMP, #pragma omp distribute simd safelen (N)); (27, OpenMP, #pragma omp distribute simd safelen (1))); (58, OpenMP, #pragma omp distribute simd safelen (argv[1]=2)); (-6, OpenMP, #pragma omp target); (18, OpenMP, #pragma omp distribute simd safelen (ST); (29, OpenMP, #pragma omp distribute simd safelen ((ST > 0)? 1 + ST : 2)); (5, OpenMP, #pragma omp distribute simd safelen ()); (-9, OpenMP, #pragma omp teams)"}
{"predict": "(41, OpenMP, #pragma omp parallel for simd private(i)); (34, OpenMP, #pragma omp parallel for simd private(e, g, z)); (18, OpenMP, #pragma omp parallel for simd private(argc > 0? argv[1] : argv[2])); (3, OpenMP, #pragma omp parallel for simd private); (40, OpenMP, #pragma omp parallel shared(i)); (24, OpenMP, #pragma omp parallel for simd private(a, b)); (29, OpenMP, #pragma omp parallel for simd private(argv[1])); (49, OpenMP, #pragma omp parallel for simd private(j)); (1, OpenMP, #pragma omp parallel); (2, OpenMP, #pragma omp parallel for simd private(); (38, OpenMP, #pragma omp parallel private(i)); (3, OpenMP, #pragma omp parallel for simd private(argc); (26, OpenMP, #pragma omp parallel for simd private(h)); (15, OpenMP, #pragma omp parallel for simd private(S1)); (38, OpenMP, #pragma omp parallel for simd private(i)); (17, OpenMP, #pragma omp parallel for simd private(argv[2])); (2, OpenMP, #pragma omp parallel for simd private(argc,)", "label": "(33, OpenMP, #pragma omp parallel for simd private(argv[1])); (29, OpenMP, #pragma omp parallel for simd private(z, a, b)); (47, OpenMP, #pragma omp parallel for simd private(i)); (15, OpenMP, #pragma omp parallel for simd private(argc,); (23, OpenMP, #pragma omp parallel for simd private(S1)); (50, OpenMP, #pragma omp parallel shared(i)); (33, OpenMP, #pragma omp parallel for simd private(h)); (49, OpenMP, #pragma omp parallel private(i)); (-2, OpenMP, #pragma omp parallel); (12, OpenMP, #pragma omp parallel for simd private(argc > 0? argv[1] : argv[2])); (26, OpenMP, #pragma omp parallel for simd private(e, g)); (-2, OpenMP, #pragma omp parallel for simd private); (30, OpenMP, #pragma omp parallel for simd nowait); (44, OpenMP, #pragma omp parallel for simd private(j)); (10, OpenMP, #pragma omp parallel for simd private(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (0, OpenMP, #pragma omp parallel for simd private(argc); (-4, OpenMP, #pragma omp parallel for simd private()"}
{"predict": "(3, OpenMP, #pragma omp parallel); (3, OpenMP, #pragma omp master taskloop)", "label": "(3, OpenMP, #pragma omp parallel); (3, OpenMP, #pragma omp master taskloop)"}
{"predict": "(25, MPI, MPI_Comm_rank(comm_row, &mycol);); (25, MPI, MPI_Comm_rank(comm_col, &myrow);); (23, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)", "label": "(25, MPI, MPI_Comm_rank(comm_row, &mycol);); (25, MPI, MPI_Comm_rank(comm_col, &myrow);); (25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(56, OpenMP, #pragma omp simd)", "label": "(56, OpenMP, #pragma omp simd)"}
{"predict": "(64, MPI, MPI_Barrier(comm);); (52, MPI, MPI_Comm_size(comm, &size);); (78, MPI, MPI_Win_free(&win);); (48, MPI, MPI_Comm_rank(comm, &rank);); (50, MPI, MPI_Win_create(target.buf, target.maxbufsize, extent, MPI_INFO_NULL, comm, &win);); (26, MPI, MPI_Type_get_extent(dtp.DTP_base_type, &lb, &extent);)", "label": "(52, MPI, MPI_Comm_size(comm, &size);); (62, MPI, MPI_Barrier(comm);); (52, MPI, MPI_Win_create(target.buf, target.maxbufsize, base_type_size, MPI_INFO_NULL, comm, &win);); (69, MPI, MPI_Win_free(&win);); (25, MPI, MPI_Type_get_extent(dtp.DTP_base_type, &lb, &extent);); (46, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(1, CUDA, __shared__ float cache[threadsPerBlock];)", "label": "(1, CUDA, __shared__ float cache[threadsPerBlock];)"}
{"predict": "(258, OpenMP, #pragma omp parallel); (259, OpenMP, #pragma omp master); (148, OpenMP, int nt = omp_get_num_threads();)", "label": "(117, OpenMP, plasma_time_t stop = omp_get_wtime();); (113, OpenMP, plasma_time_t start = omp_get_wtime();)"}
{"predict": "(8, CUDA, __shared__ float block_acc[32];)", "label": "(7, CUDA, __shared__ float block_acc[32];)"}
{"predict": "(2, OpenMP, #pragma omp simd)", "label": "(3, OpenMP, #pragma omp for collapse(2) schedule(runtime))"}
{"predict": "(36, OpenMP, #pragma omp target parallel for simd linear(argv[1])); (47, OpenMP, #pragma omp target parallel for simd linear(v : i)); (26, OpenMP, #pragma omp target parallel for simd linear(argc : 5) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (2, OpenMP, #pragma omp target parallel for simd linear); (33, OpenMP, #pragma omp target parallel for simd linear(e, g, z)); (37, OpenMP, #pragma omp parallel); (39, OpenMP, #pragma omp target parallel for simd linear(i)); (4, OpenMP, #pragma omp target parallel for simd linear()); (45, OpenMP, #pragma omp target parallel for simd linear(v : j)); (4, OpenMP, #pragma omp target parallel for simd linear(argc); (14, OpenMP, #pragma omp target parallel for simd linear(argc > 0? argv[1] : argv[2])); (1, OpenMP, #pragma omp target parallel for simd linear()); (3, OpenMP, #pragma omp target parallel for simd linear(argc,)", "label": "(32, OpenMP, #pragma omp target parallel for simd linear(a, b : B::ib)); (23, OpenMP, #pragma omp target parallel for simd linear(argc : 5) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (4, OpenMP, #pragma omp target parallel for simd linear); (12, OpenMP, #pragma omp target parallel for simd linear(argc); (40, OpenMP, #pragma omp target parallel for simd linear(i)); (33, OpenMP, #pragma omp target parallel for simd linear(e, g, z)); (45, OpenMP, #pragma omp target parallel for simd linear(v : i)); (34, OpenMP, #pragma omp target parallel for simd linear(h)); (39, OpenMP, #pragma omp parallel); (12, OpenMP, #pragma omp target parallel for simd linear(argc > 0? argv[1] : argv[2])); (-1, OpenMP, #pragma omp target parallel for simd linear(); (7, OpenMP, #pragma omp target parallel for simd linear(argc,); (49, OpenMP, #pragma omp target parallel for simd linear(v : j)); (22, OpenMP, #pragma omp target parallel for simd linear(argv[1])); (13, OpenMP, #pragma omp target parallel for simd linear(S1)); (42, OpenMP, #pragma omp target parallel for simd linear(j)); (-4, OpenMP, #pragma omp target parallel for simd linear())"}
{"predict": "(25, OpenMP, #pragma omp parallel for)", "label": "(25, OpenMP, #pragma omp parallel for)"}
{"predict": "(50, OpenMP, #pragma omp target simd collapse(4,, 4)); (4, OpenMP, #pragma omp target simd collapse); (75, OpenMP, #pragma omp target simd collapse(0)); (32, OpenMP, #pragma omp target simd collapse(4,); (67, OpenMP, #pragma omp target simd collapse(2.5)); (7, OpenMP, #pragma omp target simd collapse()); (15, OpenMP, #pragma omp target simd collapse 4)); (73, OpenMP, #pragma omp target simd collapse(5 - 5)); (42, OpenMP, #pragma omp target simd collapse(4 4)); (68, OpenMP, #pragma omp target simd collapse(-5)); (21, OpenMP, #pragma omp target simd collapse(4); (3, OpenMP, #pragma omp target simd collapse(,); (23, OpenMP, #pragma omp target simd collapse(4, )); (43, OpenMP, #pragma omp target simd collapse(4, 8)); (53, OpenMP, #pragma omp target simd collapse(4, 8, 4)); (57, OpenMP, #pragma omp target simd collapse(foo())); (5, OpenMP, #pragma omp target simd collapse(, )); (61, OpenMP, #pragma omp target simd collapse(3.14)); (20, OpenMP, #pragma omp target simd collapse(4, )); (3, OpenMP, #pragma omp target simd collapse(, )); (4, OpenMP, #pragma omp target simd collapse 4)); (31, OpenMP, #pragma omp target simd collapse(4,, 4))", "label": "(41, OpenMP, #pragma omp target simd collapse(4, )); (12, OpenMP, #pragma omp target simd collapse()); (48, OpenMP, #pragma omp target simd collapse(4 4)); (63, OpenMP, #pragma omp target simd collapse(4, 8)); (82, OpenMP, #pragma omp target simd collapse(5 - 5)); (4, OpenMP, #pragma omp target simd collapse(); (15, OpenMP, #pragma omp target simd collapse(, )); (10, OpenMP, #pragma omp target simd collapse(,); (74, OpenMP, #pragma omp target simd collapse(0)); (36, OpenMP, #pragma omp target simd collapse(4)); (45, OpenMP, #pragma omp target simd collapse(4,, 4)); (63, OpenMP, #pragma omp target simd collapse(foo())); (66, OpenMP, #pragma omp target simd collapse(-5)); (23, OpenMP, #pragma omp target simd collapse(4,); (17, OpenMP, #pragma omp target simd collapse(4); (11, OpenMP, #pragma omp target simd collapse 4)); (-11, OpenMP, #pragma omp target simd collapse); (53, OpenMP, #pragma omp target simd collapse(2.5))"}
{"predict": "(97, OpenMP, nthreads = omp_get_num_threads();); (83, OpenMP, #pragma omp parallel firstprivate(niter) private(step)); (63, OpenMP, #pragma omp parallel); (85, OpenMP, #pragma omp master)", "label": "(97, OpenMP, nthreads = omp_get_num_threads();); (82, OpenMP, #pragma omp parallel firstprivate(niter) private(step)); (62, OpenMP, #pragma omp parallel); (85, OpenMP, #pragma omp master)"}
{"predict": "(35, OpenMP, iis3dhhc_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)", "label": "(35, OpenMP, iis3dhhc_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)"}
{"predict": "(5, OpenMP, #pragma omp target exit data map(from: a)); (13, OpenMP, #pragma omp target exit data map(from: c)); (47, OpenMP, #pragma omp target exit data nowait map(from: a) depend(in : argc, argv[i:argc], x[:]) if (target exit data: b)); (41, OpenMP, #pragma omp target exit data map(from: c) map(release: d)); (57, OpenMP, #pragma omp target exit data nowait depend(in : argc, argv[i:argc], x[:]) map(from: a)); (4, OpenMP, #pragma omp target exit data map(from: a) if (b > g)); (62, OpenMP, #pragma omp target exit data depend(in : argc, argv[i:argc], x[:]) map(release: c) nowait if (b > g)); (46, OpenMP, #pragma omp target exit data map(from: a) if (b > g) nowait depend(in : argc, argv[i:argc], x[:])); (18, OpenMP, #pragma omp target exit data map(delete: x[0:10])); (41, OpenMP, #pragma omp target exit data nowait map(from: c)); (47, OpenMP, #pragma omp target exit data depend(in : argc, argv[i:argc], x[:]) map(from: a)); (23, OpenMP, #pragma omp target exit data map(always,delete: x[0:10])); (33, OpenMP, #pragma omp target exit data map(release: c) nowait if (b > g)); (27, OpenMP, #pragma omp target exit data nowait map(from: a) if (target exit data: b)); (34, OpenMP, #pragma omp target exit data nowait map(from: x[0:10], c)); (3, OpenMP, #pragma omp target exit data map(release: c) if (b > g)); (17, OpenMP, #pragma omp target exit data map(always,release: e)); (10, OpenMP, #pragma omp target exit data map(x[0:10], c)); (51,", "label": "(72, OpenMP, #pragma omp target exit data depend(in : argc,argv[i:argc],x[:]) map(release: c) nowait if(b>g)); (17, OpenMP, #pragma omp target exit data map(release: c) if(b>g)); (52, OpenMP, #pragma omp target exit data nowait map(from: c) map(release: d)); (57, OpenMP, #pragma omp target exit data nowait depend(in : argc,argv[i:argc],x[:]) map(from: a)); (1, OpenMP, #pragma omp target exit data map(from: a)); (16, OpenMP, #pragma omp target exit data map(from: x[0:10], c)); (24, OpenMP, #pragma omp target exit data map(from: c) map(release: d)); (41, OpenMP, #pragma omp target exit data map(release: c) nowait if(b>g)); (58, OpenMP, #pragma omp target exit data map(from: a) if (b > g) nowait depend(in : argc,argv[i:argc],x[:])); (72, OpenMP, #pragma omp target exit data nowait depend(in : argc,argv[i:argc],x[:]) map(always,release: e)); (5, OpenMP, #pragma omp target exit data map(from: c)); (58, OpenMP, #pragma omp target exit data map(from: c) depend(in : argc,argv[i:argc],x[:]) nowait); (30, OpenMP, #pragma omp target exit data map(from: a) if (b > g) nowait); (38, OpenMP, #pragma omp target exit data nowait map(from: x[0:10], c)); (19, OpenMP, #pragma omp target exit data map(always,release: e)); (30, OpenMP, #pragma omp target exit data map(from: c) nowait); (62, OpenMP, #pragma omp target exit data nowait map(from: c) depend(in : argc,argv[i:argc],x[:]) map(release: d)); (58, OpenMP, #pragma omp target exit data nowait map(from: x[0:10], c) depend(in : argc,argv[i:argc],x[:])); (-6, OpenMP, #pragma omp target exit data map(from: a) if (b > g)); (5, OpenMP, #pragma omp target exit data map(delete: x[0:10])); (43, OpenMP, #pragma omp target exit data nowait map(from: a) depend(in : argc,argv[i:argc],x[:]) if (target exit data: b)); (36, OpenMP, #pragma omp target exit data nowait map(always,release: e)); (17, OpenMP, #pragma omp target exit data nowait map(from: a) if (target exit data: b)); (-14, OpenMP, #pragma omp target exit data map(from: a) if (target exit data: b)); (12, OpenMP, #pragma omp target exit data nowait map(from: a)); (2, OpenMP, #pragma omp target exit data map(always, delete: x[0:10]))"}
{"predict": "(62, OpenCL, TEST_OCL_CHECK(clReleaseEvent(eltwise_event));); (60, OpenCL, TEST_OCL_CHECK(clReleaseEvent(write_buffer_event));); (58, OpenCL, TEST_OCL_CHECK(clReleaseEvent(read_buffer_event));); (58, OpenCL, TEST_OCL_CHECK(clReleaseCommandQueue(ocl_queue));)", "label": "(62, OpenCL, TEST_OCL_CHECK(clReleaseEvent(read_buffer_event));); (64, OpenCL, TEST_OCL_CHECK(clReleaseCommandQueue(ocl_queue));); (52, OpenCL, TEST_OCL_CHECK(clWaitForEvents(1, &read_buffer_event));); (61, OpenCL, TEST_OCL_CHECK(clReleaseEvent(eltwise_event));); (59, OpenCL, TEST_OCL_CHECK(clReleaseEvent(write_buffer_event));)"}
{"predict": "(1, CUDA, cudaExtent extent = make_cudaExtent(4, 5, 6);); (12, CUDA, extent = make_cudaExtent(4, 5, 0);); (8, CUDA, helper.setExtent(2, extent);); (13, CUDA, extent = make_cudaExtent(4, 5, 6);)", "label": "(10, CUDA, extent = make_cudaExtent(4, 5, 0);); (3, CUDA, cudaExtent extent = make_cudaExtent(4, 5, 6);)"}
{"predict": "(37, CUDA, cudaDeviceSynchronize();); (67, CUDA, cudaFree(device_array);); (15, CUDA, cudaMalloc((void**)&device_array, num_bytes);); (43, CUDA, cudaError_t error = cudaGetLastError();); (54, CUDA, cudaMemcpy(host_array, device_array, num_bytes, cudaMemcpyDeviceToHost);); (46, CUDA, printf(\"CUDA error: %s\\n\", cudaGetErrorString(error));)", "label": "(16, CUDA, cudaMalloc((void**)&device_array, num_bytes);); (49, CUDA, printf(\"CUDA Error: %s\\n\", cudaGetErrorString(error));); (35, CUDA, cudaThreadSynchronize();); (42, CUDA, cudaError_t error = cudaGetLastError();); (66, CUDA, cudaFree(device_array);); (54, CUDA, cudaMemcpy(host_array, device_array, num_bytes, cudaMemcpyDeviceToHost);)"}
{"predict": "(59, OpenMP, omp_eval_stencils(result, dstDesc.length, sizes, offsets, indices, weights);); (59, OpenMP, omp_eval_stencils(resultDu, dstDesc.length, sizes, offsets, indices, duWeights);); (59, OpenMP, omp_eval_stencils(resultDv, dstDesc.length, sizes, offsets, indices, dvWeights);); (35, OpenMP, int threadId = omp_get_thread_num();); (18, OpenMP, int numThreads = omp_get_num_threads();)", "label": "(19, OpenMP, int numThreads = omp_get_max_threads();); (25, OpenMP, #pragma omp parallel for); (35, OpenMP, int threadId = omp_get_thread_num();)"}
{"predict": "(57, MPI, MPI_Finalize();); (23, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (30, MPI, MPI_Bcast(&params, 1, *params_type, root, MPI_COMM_WORLD);); (14, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (10, MPI, MPI_Init_thread(NULL, NULL, MPI_THREAD_FUNNELED, &thread_level);)", "label": "(56, MPI, MPI_Finalize();); (22, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (29, MPI, MPI_Bcast(&params, 1, *params_type, root, MPI_COMM_WORLD);); (14, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (10, MPI, MPI_Init_thread(NULL, NULL, MPI_THREAD_FUNNELED, &thread_level);)"}
{"predict": "(24, MPI, MPI_Bcast(fparams, 1, MPI_FLOAT, 0, MPI_COMM_WORLD);); (20, MPI, MPI_Bcast(iparams, num_int_params, MPI_INT, 0, MPI_COMM_WORLD);)", "label": "(23, MPI, MPI_Bcast(&fparams[0], 1, MPI_FLOAT, 0, MPI_COMM_WORLD);); (6, MPI, MPI_Bcast(&iparams[0], num_int_params, MPI_INT, 0, MPI_COMM_WORLD);)"}
{"predict": "(37, MPI, MPI_Finalize();); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (2, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (0, MPI, MPI_Init_thread(&argc, &argv, 1, &mpi_threads_provided);)", "label": "(35, MPI, MPI_Finalize();); (2, MPI, MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &mpi_threads_provided);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);)"}
{"predict": "(100, MPI, MPI_Send(NULL, 0, MPI_BYTE, rank + 1, 0, MPI_COMM_WORLD);); (15, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (15, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (161, MPI, MPI_Allreduce(&max_offset, out_starting_pos, 1, MPI_UINT64_T, MPI_MAX, MPI_COMM_WORLD);); (161, MPI, MPI_Allreduce(&found_one, out_count, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (158, MPI, MPI_Allreduce(&data_offset, out_offsets, 1, MPI_UINT64_T, MPI_MIN, MPI_COMM_WORLD);); (160, MPI, MPI_Allreduce(&data_size,  out_counts, 1, MPI_UINT64_T, MPI_SUM, MPI_COMM_WORLD);); (93, MPI, MPI_Recv(NULL, 0, MPI_BYTE, rank - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)", "label": "(141, MPI, MPI_Exscan(&max_offset, &starting_pos, 1, MPI_UINT64_T, MPI_MAX, MPI_COMM_WORLD);); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (258, MPI, MPI_Allreduce(&pos, &max_pos, 1, MPI_UINT64_T, MPI_MAX, MPI_COMM_WORLD);); (13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(35, CUDA, cudaEventRecord(end, s);); (35, CUDA, cudaEventSynchronize(end);); (24, CUDA, cudaEventRecord(begin, s);); (3, CUDA, cudaStreamCreate(&s);); (42, CUDA, cudaEventDestroy(end);); (30, CUDA, cudaEventElapsedTime(&time, begin, end);); (36, CUDA, cudaEventDestroy(begin);); (-3, CUDA, cudaEventCreate(&end);); (38, CUDA, cudaStreamDestroy(s);); (-5, CUDA, cudaEventCreate(&begin);)", "label": "(72, CUDA, cudaStreamDestroy(s);); (5, CUDA, cudaStreamCreate(&s);); (31, CUDA, cudaEventRecord(end, s);); (31, CUDA, cudaEventSynchronize(end);); (23, CUDA, cudaEventRecord(begin, s);); (66, CUDA, cudaEventDestroy(end);); (3, CUDA, cudaEventCreate(&end);); (63, CUDA, cudaEventDestroy(begin);); (0, CUDA, cudaEventCreate(&begin);); (27, CUDA, cudaEventElapsedTime(&time, begin, end);)"}
{"predict": "(9, MPI, dray::dray::mpi_comm(MPI_Comm_c2f(comm));)", "label": "(11, MPI, ::dray::dray::mpi_comm(MPI_Comm_c2f(comm));)"}
{"predict": "(10, OpenMP, #pragma omp target parallel defaultmap(tofrom:scalar)); (3, OpenMP, #pragma omp target parallel defaultmap); (14, OpenMP, #pragma omp target parallel defaultmap(tofrom, scalar); (46, OpenMP, #pragma omp target defaultmap(none:aggregate)); (17, OpenMP, #pragma omp target parallel defaultmap(none:scalar) map(argc)); (10, OpenMP, #pragma omp target parallel defaultmap(none:scalar)); (2, OpenMP, #pragma omp target parallel defaultmap(tofrom); (5, OpenMP, #pragma omp target parallel defaultmap(none:tofrom)); (18, OpenMP, #pragma omp target defaultmap(none:aggregate)); (18, OpenMP, #pragma omp target parallel defaultmap(none:aggregate) defaultmap(none:scalar) map(argc)); (36, OpenMP, #pragma omp target defaultmap(none:aggregate) map(i)); (22, OpenMP, #pragma omp parallel); (36, OpenMP, #pragma omp target defaultmap(none:aggregate) map(i)); (38, OpenMP, #pragma omp target defaultmap(none:aggregate) map(i+N)); (32, OpenMP, #pragma omp target defaultmap(none:aggregate) map(i)); (15, OpenMP, #pragma omp target defaultmap(none:aggregate) map(S1)); (37, OpenMP, #pragma omp target defaultmap(none:aggregate) map(i-N)); (24, OpenMP, #pragma omp target defaultmap(none:aggregate) map(argc)); (2, OpenMP, #pragma omp target parallel defaultmap(none:tofrom) map(argc))", "label": "(12, OpenMP, #pragma omp target parallel defaultmap(tofrom,); (1, OpenMP, #pragma omp target parallel defaultmap(); (14, OpenMP, #pragma omp target parallel defaultmap(tofrom, scalar); (24, OpenMP, #pragma omp target parallel defaultmap(none:pointer) defaultmap(none:scalar) map(g)); (35, OpenMP, #pragma omp target defaultmap(none:aggregate)); (26, OpenMP, #pragma omp target parallel defaultmap(none:aggregate) defaultmap(none:scalar) map(argc)); (8, OpenMP, #pragma omp target parallel defaultmap (scalar:); (47, OpenMP, #pragma omp target defaultmap(none:pointer)); (32, OpenMP, #pragma omp parallel); (14, OpenMP, #pragma omp target parallel defaultmap(none:pointer)); (0, OpenMP, #pragma omp target parallel defaultmap(tofrom)); (-7, OpenMP, #pragma omp target parallel defaultmap()); (7, OpenMP, #pragma omp target parallel defaultmap(tofrom:scalar) defaultmap(to:scalar)); (8, OpenMP, #pragma omp target parallel defaultmap(alloc:pointer) defaultmap(to:scalar) defaultmap(firstprivate:pointer)); (15, OpenMP, #pragma omp target parallel defaultmap(none:pointer) defaultmap(none:scalar) defaultmap(none:aggregate) map(g) firstprivate(argc)); (-7, OpenMP, #pragma omp target parallel defaultmap (tofrom:); (9, OpenMP, #pragma omp target parallel defaultmap(none:pointer) defaultmap(none:scalar) map(argc)); (-11, OpenMP, #pragma omp target parallel defaultmap(tofrom)"}
{"predict": "(57, MPI, MPI_Allreduce(&my_bytes, &max_bytes, 1, MPI_UNSIGNED_LONG, MPI_MAX, comm);)", "label": "(57, MPI, MPI_Allreduce(&my_bytes, &max_bytes, 1, MPI_UNSIGNED_LONG, MPI_MAX, comm);)"}
{"predict": "(15, CUDA, typename BlockReduceT::TempStorage temp_storage;)", "label": "(15, CUDA, __shared__ typename BlockReduceT::TempStorage temp_storage;)"}
{"predict": "_dispatch(3, OpenMP, int tid = omp_get_thread_num();)", "label": "(7, OpenMP, int tid = omp_get_thread_num();)"}
{"predict": "(14, OpenMP, return comp_variants(lv, rv, ascendingly, by_number, casesensitively);)", "label": "(14, OpenMP, return comp_raw(lv, rv, by_number, ascendingly, casesensitively);)"}
{"predict": "(16, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (97, CUDA, cudaFree(d_in1);); (14, CUDA, cudaMalloc((void**)(&d_in2), in2_bytes);); (12, CUDA, cudaMalloc((void**)(&d_in1), in1_bytes);); (96, CUDA, cudaFree(d_out);); (94, CUDA, cudaFree(d_in2);); (12, CUDA, cudaMemcpy(d_in1, in1.data(), in1_bytes, cudaMemcpyHostToDevice);); (13, CUDA, cudaMemcpy(d_in2, in2.data(), in2_bytes, cudaMemcpyHostToDevice);); (21, CUDA, assert(cudaMemcpyAsync(out.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, context.device().stream()) == cudaSuccess);); (21, CUDA, assert(cudaStreamSynchronize(context.device().stream()) == cudaSuccess);)", "label": "(27, CUDA, assert(cudaMemcpy(out.data(), d_out, out_bytes, cudaMemcpyDeviceToHost) == cudaSuccess);); (15, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (12, CUDA, cudaMalloc((void**)(&d_in1), in1_bytes);); (69, CUDA, assert(cudaMemcpyAsync(out.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, context.device().stream()) == cudaSuccess);); (14, CUDA, cudaMemcpy(d_in1, in1.data(), in1_bytes, cudaMemcpyHostToDevice);); (10, CUDA, cudaMalloc((void**)(&d_in2), in2_bytes);); (67, CUDA, assert(cudaStreamSynchronize(context.device().stream()) == cudaSuccess);); (12, CUDA, cudaMemcpy(d_in2, in2.data(), in2_bytes, cudaMemcpyHostToDevice);)"}
{"predict": "(11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (37, MPI, MPI_Finalize();); (8, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (3, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)", "label": "(11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (19, MPI, MPI_Barrier(MPI_COMM_WORLD);); (13, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (4, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(6, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)", "label": "(6, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(17, CUDA, __shared__ uninitialized_array<T, 1024> tmp;)", "label": "(15, CUDA, __shared__ thrust::cuda_cub::core::uninitialized_array<T, 1024> tmp;)"}
{"predict": "(67, CUDA, cudaFree( outGPU );); (65, CUDA, cudaFree( inGPU );); (61, CUDA, cudaEventDestroy( evStart );); (61, CUDA, cudaEventDestroy( evStop );)", "label": "(65, CUDA, cudaFree( outGPU );); (63, CUDA, cudaEventDestroy( evStop );); (64, CUDA, cudaFree( inGPU );); (60, CUDA, cudaEventDestroy( evStart );)"}
{"predict": "(2, CUDA, __shared__ float siftPoint[FMC2W*FMC2H];); (3, CUDA, __shared__ float maxScore[FMC2W];); (4, CUDA, __shared__ float maxScor2[FMC2W];); (3, CUDA, __shared__ int maxIndex[FMC2W];)", "label": "(3, CUDA, __shared__ float maxScore[FMC2H];); (1, CUDA, __shared__ float siftPoint[128];); (2, CUDA, __shared__ float maxScor2[FMC2H];); (2, CUDA, __shared__ int maxIndex[FMC2H];)"}
{"predict": "(170, OpenMP, #pragma omp simd); (185, OpenMP, #pragma omp atomic capture); (130, OpenMP, #pragma omp master); (176, OpenMP, #pragma\tomp\ttaskloop firstprivate (lptr1,luptr1,nlb1,thread_id1,lsub1,lusup1,nsupr1,Linv,nn,lbstart,lbend,luptr_tmp1,nbrow,lb,lptr1_tmp,rtemp_loc,nbrow_ref,lptr,nbrow1,ik,rel,lk,iknsupc,il,i,irow,fmod_tmp,ikcol,p,ii,jj,t1,t2,j) untied nogroup)", "label": "(174, OpenMP, #pragma omp simd); (112, OpenMP, #pragma\tomp\ttaskloop private (lptr1,luptr1,nlb1,thread_id1,lsub1,lusup1,nsupr1,Linv,nn,lbstart,lbend,luptr_tmp1,nbrow,lb,lptr1_tmp,rtemp_loc,nbrow_ref,lptr,nbrow1,ik,rel,lk,iknsupc,il,i,irow,fmod_tmp,ikcol,p,ii,jj,t1,t2,j) untied); (229, OpenMP, #pragma omp simd lastprivate(irow))"}
{"predict": "(10, MPI, rc = MPI_Open_port(MPI_INFO_NULL, port_name);); (13, MPI, MPI_Close_port(port_name);); (10, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (10, MPI, errclass = MPI_Lookup_name(serv_name, port_name);); (8, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(12, MPI, rc = MPI_Unpublish_name(serv_name, MPI_INFO_NULL, port_name);); (12, MPI, MPI_Error_class(rc, &errclass);); (8, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(99, OpenMP, omp_dot_complex<TC, T> omp_dot_complex(a, b, array_size);); (6, OpenMP, #pragma omp target map(tofrom : warp_size)); (99, OpenMP, #pragma omp target exit data map(release : a[0:array_size], b[0:array_size])); (54, OpenMP, #pragma omp target teams); (54, OpenMP, #pragma omp distribute parallel for); (94, OpenMP, sim_dot_complex<TC, T> sim_dot_complex(a, b, warp_size);); (49, OpenMP, #pragma omp target enter data map(to : a[0:array_size], b[0:array_size])); (97, OpenMP, #pragma omp target update from(simd_dot_complex) nowait); (50, OpenMP, #pragma omp target map(to : warp_size)); (94, OpenMP, #pragma omp target update from(omp_dot_complex) nowait); (96, OpenMP, #pragma omp target update from(sim_dot_complex))", "label": "(11, OpenMP, #pragma omp target enter data map(alloc : a [0:array_size], b [0:array_size])); (18, OpenMP, #pragma omp target teams distribute parallel for); (3, OpenMP, #pragma omp target map(tofrom : warp_size)); (98, OpenMP, #pragma omp target exit data map(release : a [0:array_size], b [0:array_size]))"}
{"predict": "(37, CUDA, cudaDeviceSynchronize();); (12, CUDA, thrust::host_vector<float>       A(Arow*Acol);); (13, CUDA, thrust::device_vector<float>  dev_C(Crow*Ccol);); (10, CUDA, thrust::host_vector<float>       B(Brow*Bcol);); (10, CUDA, thrust::device_vector<float>  dev_A(Arow*Acol);); (10, CUDA, thrust::device_vector<float>  dev_B(Brow*Bcol);); (10, CUDA, thrust::host_vector<float>       C(Crow*Ccol);)", "label": "(18, CUDA, thrust::device_vector<float> dev_B(Brow*Bcol);); (12, CUDA, thrust::host_vector<float>       A(Arow*Acol);); (14, CUDA, thrust::device_vector<float> dev_C(Crow*Ccol);); (32, CUDA, cudaDeviceSynchronize();); (10, CUDA, thrust::host_vector<float>       B(Brow*Bcol);); (10, CUDA, thrust::host_vector<float>       C(Crow*Ccol);); (11, CUDA, thrust::device_vector<float> dev_A(Arow*Acol);)"}
{"predict": "(8, MPI, MPI_Comm_rank(comm, &comm_rank);); (9, MPI, MPI_Comm_size(comm, &comm_size);)", "label": "(8, MPI, MPI_Comm_rank(comm, &comm_rank);); (9, MPI, MPI_Comm_size(comm, &comm_size);)"}
{"predict": "(70, MPI, err = MPI_Type_commit(&filetype);); (25, MPI, err = MPI_Dims_create(nprocs, 2, psize);); (86, MPI, err = MPI_Type_free(&subType);); (70, MPI, err = MPI_Type_free(&filetype);); (87, MPI, err = MPI_Type_create_struct(NVARS, array_of_blocklengths, array_of_displacements, array_of_types, &buftype);); (55, MPI, err = MPI_Type_commit(&subType);); (7, MPI, MPI_Init(&argc, &argv);); (86, MPI, err = MPI_Type_commit(&buftype);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (105, MPI, err = MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_RDONLY, MPI_INFO_NULL, &fh);); (131, MPI, err = MPI_File_read_all(fh, buf, buf_len, MPI_BYTE, &status);); (95, MPI, err = MPI_File_set_view(fh, 0, MPI_BYTE, buftype, \"native\", MPI_INFO_NULL);); (100, MPI, err = MPI_File_close(&fh);); (104, MPI, err = MPI_File_get_size(fh, &fsize);); (12, MPI, err = MPI_Type_create_subarray(2, gsize, count, start, MPI_ORDER_C, MPI_BYTE, &subType);); (144, MPI, MPI_Finalize();); (134, MPI, err = MPI_File_read_all(fh, buf, buf_len, MPI_BYTE, &status);); (138, MPI, err = MPI_File_read_all(fh, buf, buf_len, MPI_BYTE, MPI_STATUS_IGNORE);); (139, MPI, err = MPI_File_read_all(fh, buf", "label": "(14, MPI, MPI_Init(&argc, &argv);); (130, MPI, err = MPI_File_iwrite_all(fh, buf, 1, buftype, &req);); (100, MPI, err = MPI_Type_commit(&buftype);); (56, MPI, err = MPI_Type_create_subarray(2, gsize, count, start, MPI_ORDER_C, MPI_BYTE, &subType);); (57, MPI, err = MPI_Type_commit(&subType);); (68, MPI, err = MPI_Type_commit(&filetype);); (69, MPI, err = MPI_Type_free(&subType);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (115, MPI, err = MPI_File_write_all(fh, buf, 1, buftype, &status);); (125, MPI, err = MPI_File_read_all(fh, buf, 1, buftype, &status);); (129, MPI, err = MPI_File_iread_all(fh, buf, 1, buftype, &req);); (121, MPI, MPI_Wait(&req, &status);); (8, MPI, MPI_Finalize();); (106, MPI, err = MPI_File_set_view(fh, 0, MPI_BYTE, filetype, \"native\", MPI_INFO_NULL);); (134, MPI, err = MPI_Type_free(&buftype);); (131, MPI, err = MPI_Type_free(&filetype);); (127, MPI, MPI_File_close(&fh);); (10, MPI, err = MPI_Dims_create(nprocs, 2, psize);); (109, MPI, err = MPI_File_seek(fh, 0, MPI_SEEK_SET);)"}
{"predict": "(4, OpenMP, #pragma omp parallel sections); (5, OpenMP, #pragma omp section)", "label": "(5, OpenMP, #pragma omp sections); (3, OpenMP, #pragma omp parallel); (5, OpenMP, #pragma omp section)"}
{"predict": "(8, OpenMP, #pragma omp target); (38, OpenMP, #pragma omp target exit data map(delete : A[0:64]) map(delete : B[0:64]) map(delete : C[0:64])); (42, OpenMP, #pragma omp target exit data map(delete : C[0:64])); (39, OpenMP, #pragma omp target exit data map(delete : B[0:64])); (36, OpenMP, #pragma omp target exit data map(delete : A[0:64])); (43, OpenMP, #pragma omp target exit data map(from : C[0:64])); (10, OpenMP, #pragma omp target enter data map(alloc : A[0:64]) map(alloc : B[0:64]) map(alloc : C[0:64])); (40, OpenMP, #pragma omp target exit data map(from : B[0:64])); (10, OpenMP, #pragma omp target enter data map(to : B[0:64]) map(to : C[0:64]))", "label": "(42, OpenMP, #pragma omp target); (42, OpenMP, #pragma omp target firstprivate(val))"}
{"predict": "(24, MPI, err = MPI_Alloc_mem(count, MPI_INFO_NULL, &ap);); (18, MPI, MPI_Info_set(hints, hintstrs[i], NULL);); (12, MPI, MPI_Info_create(&hints);); (17, MPI, err = MPI_Alloc_mem(count, hints, &ap);); (22, MPI, MPI_Error_class(err, &errclass);); (35, MPI, MPI_Info_free(&hints);); (20, MPI, MPI_Free_mem(ap);); (4, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(23, MPI, MPI_Error_class(err, &errclass);); (36, MPI, MPI_Free_mem(ap);); (16, MPI, MPI_Info_set(hints, \"mpich_buf_type\", hintstrs[i]);); (10, MPI, MPI_Info_set(hints, \"mpi_minimum_memory_alignment\", \"4096\");); (8, MPI, MPI_Info_create(&hints);); (20, MPI, err = MPI_Alloc_mem(count, MPI_INFO_NULL, &ap);); (3, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (35, MPI, MPI_Info_free(&hints);); (12, MPI, err = MPI_Alloc_mem(count, hints, &ap);)"}
{"predict": "(11, CUDA, cuda_safe_call(cudaGetDeviceCount(&ndevs));)", "label": "(24, CUDA, cuda_safe_call(cudaStreamSynchronize(stream));); (10, CUDA, cuda_safe_call(cudaGetDeviceCount(&ndevs));)"}
{"predict": "(2, CUDA, __shared__ float s[Ny][Nx];)", "label": "(2, CUDA, __shared__ float s[Ny][Nx];)"}
{"predict": "(124, OpenMP, #pragma omp distribute simd reduction(+ : r)); (132, OpenMP, #pragma omp distribute simd reduction(+ : fl)); (45, OpenMP, #pragma omp distribute simd reduction(\\)); (128, OpenMP, #pragma omp parallel private(fl)); (78, OpenMP, #pragma omp distribute simd reduction(& : e, g)); (63, OpenMP, #pragma omp distribute simd reduction(- : da)); (50, OpenMP, #pragma omp distribute simd reduction(|| : argc > 0? argv[1] : argv[2])); (12, OpenMP, #pragma omp distribute simd reduction); (37, OpenMP, #pragma omp distribute simd reduction(foo : argc); (46, OpenMP, #pragma omp distribute simd reduction(~ : argc)); (7, OpenMP, #pragma omp distribute simd reduction(); (54, OpenMP, #pragma omp distribute simd reduction(&& : argc)); (102, OpenMP, #pragma omp parallel shared(i)); (108, OpenMP, #pragma omp parallel private(k)); (105, OpenMP, #pragma omp distribute simd reduction(+ : p), reduction(+ : p)); (3, OpenMP, #pragma omp distribute simd reduction +); (125, OpenMP, #pragma omp parallel reduction(min : i)); (77, OpenMP, #pragma omp distribute simd reduction(&& : S2::S2sc)); (80, OpenMP, #pragma omp distribute simd reduction(& : h, k, B::x)); (113, OpenMP, #pragma omp parallel private(fl)); (117, OpenMP, #pragma omp parallel shared(fl)); (13, OpenMP, #pragma omp distribute simd reduction(-); (123, OpenMP, #pragma omp parallel private(r)); (133, OpenMP, #pragma omp parallel reduction(max : j)); (5, OpenMP, #pragma omp distribute simd reduction()); (47, OpenMP, #pragma omp distribute simd reduction(&& : S2::S2s)); (130, OpenMP, #pragma omp parallel private(fl)); (57, OpenMP, #pragma omp distribute simd reduction(&", "label": "(109, OpenMP, #pragma omp distribute parallel for simd reduction(^ : fl)); (118, OpenMP, #pragma omp distribute parallel for simd reduction(&& : S2::S2sc)); (132, OpenMP, #pragma omp distribute parallel for simd reduction(+ : z, o)); (21, OpenMP, #pragma omp distribute parallel for simd reduction(); (45, OpenMP, #pragma omp distribute parallel for simd reduction(foo : argc); (140, OpenMP, #pragma omp distribute parallel for simd reduction(+ : p), reduction(+ : p)); (6, OpenMP, #pragma omp target); (87, OpenMP, #pragma omp distribute parallel for simd reduction(+ : ba)); (131, OpenMP, #pragma omp distribute parallel for simd private(i), reduction(+ : j), reduction(+ : q)); (146, OpenMP, #pragma omp distribute parallel for simd reduction(+ : r)); (161, OpenMP, #pragma omp parallel reduction(* : fl)); (151, OpenMP, #pragma omp distribute parallel for simd reduction(max : j)); (112, OpenMP, #pragma omp distribute parallel for simd reduction(& : e, g)); (172, OpenMP, #pragma omp distribute parallel for simd reduction(task, + : m)); (166, OpenMP, #pragma omp distribute parallel for simd reduction(+ : m)); (127, OpenMP, #pragma omp parallel private(k)); (63, OpenMP, #pragma omp distribute parallel for simd reduction(+ : a, b, c, d, f)); (67, OpenMP, #pragma omp distribute parallel for simd reduction(min : a, b, c, d, f)); (71, OpenMP, #pragma omp distribute parallel for simd reduction(max : h.b)); (55, OpenMP, #pragma omp distribute parallel for simd reduction(^ : S1)); (109, OpenMP, #pragma omp distribute parallel for simd reduction(+ : h, k, B::x)); (48, OpenMP, #pragma omp distribute parallel for simd reduction(&& : argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (17, OpenMP, #pragma omp distribute parallel for simd reduction(*)); (136, OpenMP, #pragma omp parallel reduction(min : i)); (20, OpenMP, #pragma omp distribute parallel for simd reduction(\\)); (140, OpenMP, #pragma omp parallel private(fl)); (8, OpenMP, #pragma omp distribute parallel for simd reduction()); (72, OpenMP, #pragma omp distribute parallel for simd reduction(* : ca)); (140, OpenMP, #pragma omp distribute parallel for simd reduction(+ : fl)); (30, OpenMP, #pragma omp distribute parallel for simd reduction(|| : argc > 0? argv[1] : argv[2])); (-1, OpenMP, #pragma omp distribute parallel for simd reduction(-); (73, OpenMP, #pragma omp distribute parallel for simd reduction(- : da)); (-13, OpenMP, #pragma omp distribute parallel for simd reduction +); (-19, OpenMP, #pragma omp distribute parallel for simd reduction); (20, OpenMP, #pragma omp distribute parallel for simd reduction(| : argc,); (123, OpenMP, #pragma omp parallel shared(i)); (78, OpenMP, #pragma omp distribute parallel for simd reduction(&& : S2::S2s)); (27, OpenMP, #pragma omp distribute parallel for simd reduction(~ : argc)); (-25, OpenMP, #pragma omp teams)"}
{"predict": "(13, OpenCL, clReleaseMemObject(bufParam);); (8, OpenCL, clReleaseMemObject(bufY);); (3, OpenCL, clReleaseMemObject(bufX);)", "label": "(5, OpenCL, clReleaseMemObject(bufX);); (8, OpenCL, clReleaseMemObject(bufY);); (11, OpenCL, clReleaseMemObject(bufParam);)"}
{"predict": "(35, MPI, MPI_Comm_create_from_topo(MPI_COMM_WORLD, 0, dims, periods, 0, &newrank);); (16, MPI, MPI_Cart_shift(MPI_COMM_WORLD, 0, 1, &newrank, &rank);); (20, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &newrank);); (46, MPI, MPI_Comm_free(&newrank);); (33, MPI, MPI_Barrier(MPI_COMM_WORLD);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (12, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &newrank);); (23, MPI, MPI_Comm_free(&newrank);); (25, MPI, MPI_Comm_create_from_topo(MPI_COMM_WORLD, 0, dims, periods, 0, &newrank);)", "label": "(9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (15, MPI, MPI_Cart_map(MPI_COMM_WORLD, 1, dims, periods, &newrank);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (29, MPI, MPI_Cart_map(MPI_COMM_WORLD, 0, dims, periods, &newrank);)"}
{"predict": "(25, OpenCL, status = clReleaseEvent(event);); (13, OpenCL, auto status = clWaitForEvents(1, &event);)", "label": "(26, OpenCL, status = clReleaseEvent(event);); (13, OpenCL, auto status = clWaitForEvents(1, &event);)"}
{"predict": "(40, TBB, tbb::parallel_for( tbb::blocked_range<int>( 0, N, 10000 ), ParallelScalarBody<T>( finit_combinable ) );); (5, TBB, tbb::task_scheduler_init init(tbb::task_scheduler_init::deferred);); (37, TBB, tbb::parallel_for( tbb::blocked_range<int>( 0, N, 10000 ), ParallelScalarBody<T>( copy_constructed ) );); (26, TBB, tbb::parallel_for( tbb::blocked_range<int>( 0, N, 10000 ), ParallelScalarBody<T>( my_finit_decl ) );); (44, TBB, tbb::combinable<T> moved2;); (46, TBB, tbb::parallel_for( tbb::blocked_range<int>( 0, N, 10000 ), ParallelScalarBody<T>( moved2 ) );); (41, TBB, tbb::combinable<T> moved1( std::move(sums ) );); (14, TBB, tbb::parallel_for( tbb::blocked_range<int>( 0, N, 10000 ), ParallelScalarBody<T>( sums ) );); (33, TBB, tbb::combinable<T> copy_constructed( sums );); (36, TBB, tbb::combinable<T> assigned;); (-6, TBB, tbb::tick_count t0;); (13, TBB, tbb::parallel_for( tbb::blocked_range<int>( 0, N, 10000 ), ParallelScalarBody<T>( my_combine<T> ) );); (17, TBB, tbb::parallel_for( tbb::blocked_range<int>( 0, N, 10000 ), ParallelScalarBody<T>( my_combine_ref<T> ) );); (14, TBB, tbb::parallel_for( tbb::blocked_range<int>( 0, N, 10000 ), ParallelScalarBody<T>( my", "label": "(41, TBB, tbb::combinable<T> copy_constructed(sums);); (31, TBB, tbb::combinable<T> finit_combinable(my_finit_decl);); (31, TBB, tbb::parallel_for( tbb::blocked_range<int>( 0, N, 10000 ), ParallelScalarBodyNoInit<T>( finit_combinable ) );); (19, TBB, if (Verbose && t == 0) t0 = tbb::tick_count::now();); (22, TBB, tbb::parallel_for( tbb::blocked_range<int>( 0, N, 10000 ), ParallelScalarBody<T>( sums ) );); (40, TBB, tbb::combinable<T> assigned;); (-3, TBB, tbb::task_scheduler_init init(tbb::task_scheduler_init::deferred);); (44, TBB, tbb::combinable<T> moved1(std::move(sums));); (2, TBB, tbb::tick_count t0;); (16, TBB, tbb::combinable<T> sums;); (45, TBB, tbb::combinable<T> moved2;)"}
{"predict": "(94, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 1, NULL, &threadSize, &groupSize, 0, NULL, NULL );); (83, OpenCL, error = clSetKernelArg( kernel, 1, sizeof( streams[1] ), &streams[1] );); (80, OpenCL, error = clSetKernelArg( kernel, 0, sizeof( streams[0] ), &streams[0] );); (84, OpenCL, error = clSetKernelArg( kernel, 2, sizeof( streams[2] ), &streams[2] );); (94, OpenCL, error = clEnqueueReadBuffer( queue, streams[2], true, 0, outStride * count, outData, 0, NULL, NULL );); (77, OpenCL, error = clSetKernelArg( kernel, 3, sizeof( outStride ), &outStride );); (47, OpenCL, clReleaseKernel( kernel );); (45, OpenCL, clReleaseProgram( program );); (46, OpenCL, clReleaseMemObject( streams[0] );); (46, OpenCL, clReleaseMemObject( streams[1] );); (46, OpenCL, clReleaseMemObject( streams[2] );)", "label": "(119, OpenCL, clReleaseMemObject( streams[2] );); (80, OpenCL, error = clSetKernelArg(kernel, 0, sizeof( streams[0] ), &streams[0] );); (119, OpenCL, clReleaseProgram( program );); (82, OpenCL, error = clSetKernelArg(kernel, 2, sizeof( streams[2] ), &streams[2] );); (114, OpenCL, clReleaseMemObject( streams[1] );); (115, OpenCL, clReleaseKernel( kernel );); (77, OpenCL, error = clSetKernelArg(kernel, 1, sizeof( streams[1] ), &streams[1] );); (92, OpenCL, error = clEnqueueReadBuffer( queue, streams[2], CL_TRUE, 0, outStride * count, outData, 0, NULL, NULL );); (86, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 1, NULL, &threadSize, &groupSize, 0, NULL, NULL );); (108, OpenCL, clReleaseMemObject( streams[0] );)"}
{"predict": "(28, MPI, MPI_Wait(&req, &status);); (15, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (10, MPI, MPI_Bcast_init(&a, 1, MPI_INT, 0, MPI_COMM_WORLD, MPI_INFO_NULL, &req);); (15, MPI, MPI_Test(&req, &flag, &status);); (25, MPI, MPI_Request_free(&req);)", "label": "(25, MPI, MPI_Wait(&req, &status);); (12, MPI, MPI_Bcast_init(&a, 1, MPI_INT, 0, MPI_COMM_WORLD, MPI_INFO_NULL, &req);); (15, MPI, MPI_Test(&req, &flag, &status);); (25, MPI, MPI_Request_free(&req);)"}
{"predict": "(10, OpenMP, #pragma omp parallel for); (32, OpenMP, #pragma omp atomic)", "label": "(10, OpenMP, #pragma omp parallel for); (32, OpenMP, #pragma omp atomic)"}
{"predict": "(25, OpenCL, clReleaseEvent(retEvent);); (25, OpenCL, clReleaseEvent(retEvent2);)", "label": "(25, OpenCL, clReleaseEvent(retEvent2);); (25, OpenCL, clReleaseEvent(retEvent);)"}
{"predict": "(18, MPI, MPI_Barrier(comm);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (16, MPI, MPI_Comm_free(&comm);)", "label": "(3, MPI, MPI_Barrier(MPI_COMM_WORLD);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (16, MPI, MPI_Comm_free(&comm);)"}
{"predict": "(104, MPI, MPI_Reduce_scatter_block(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, comm);); (76, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (29, MPI, MPI_Ireduce(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, root, comm, &req);); (55, MPI, MPI_Iallreduce(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, comm, &req);); (45, MPI, MPI_Allreduce(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, comm);); (18, MPI, MPI_Reduce(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, root, comm);); (8, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (61, MPI, MPI_Irecv(recvbuf, COUNT, MPI_INT, root, 0, comm, &req);); (91, MPI, MPI_Reduce_scatter(buf, recvbuf, recvcounts, MPI_INT, MPI_SUM, comm);); (3, MPI, MPI_Send(buf, COUNT, MPI_INT, root, 0, comm);); (23, MPI, MPI_Recv(recvbuf, COUNT, MPI_INT, root, 0, comm, MPI_STATUS_IGNORE);); (70, MPI, MPI_Sendrecv_replace(buf, COUNT, MPI_INT, root, 0, recvbuf, COUNT, MPI_INT, root, 0, comm);); (53, MPI, MPI_Recv(recvbuf, COUNT, MPI_INT, root, 0, comm, MPI_STATUS_IGNORE);); (40, MPI, MPI_Sendrecv_replace(buf, COUNT, MPI_INT, root, 0, recvbuf, COUNT, MPI_INT, root, 0, comm);); (82, MPI, MPI_Reduce_scatter_block(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, comm);)", "label": "(169, MPI, MPI_Scan(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (28, MPI, MPI_Ireduce(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD, &req);); (82, MPI, MPI_Reduce_scatter(buf, recvbuf, recvcounts, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (45, MPI, MPI_Allreduce(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (124, MPI, MPI_Reduce_scatter_block(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (59, MPI, MPI_Iallreduce(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD, &req);); (100, MPI, MPI_Ireduce_scatter(buf, recvbuf, recvcounts, MPI_INT, MPI_SUM, MPI_COMM_WORLD, &req);); (23, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (3, MPI, MPI_Reduce(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD);); (187, MPI, MPI_Exscan(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (139, MPI, MPI_Ireduce_scatter_block(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD, &req);); (171, MPI, MPI_Iscan(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD, &req);); (198, MPI, MPI_Iexscan(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD, &req);)"}
{"predict": "(10, MPI, t = MPI_Wtime();); (10, MPI, MPI_Recv(buf, cnt, MPI_INT, 0, tag, MPI_COMM_WORLD, &status);); (12, MPI, t = MPI_Wtime() - t;)", "label": "(9, MPI, t = MPI_Wtime();); (11, MPI, t = MPI_Wtime() - t;)"}
{"predict": "(15, OpenMP, #pragma omp target parallel for if (S1)); (22, OpenMP, #pragma omp target parallel for if(parallel : argc) if (parallel:argc)); (26, OpenMP, #pragma omp target parallel for if(parallel : argc) if (argc)); (0, OpenMP, #pragma omp target parallel for if ()); (1, OpenMP, #pragma omp target parallel for if (argc); (16, OpenMP, #pragma omp target parallel for if(parallel : argc); (7, OpenMP, #pragma omp target parallel for if (argc > 0? argv[1] : argv[2])); (17, OpenMP, #pragma omp target parallel for if(parallel : argc) if (for:argc)); (1, OpenMP, #pragma omp target parallel for if (foobool(argc)), if (true)); (7, OpenMP, #pragma omp target parallel for if (argc argc)); (-5, OpenMP, #pragma omp target parallel for if (argc))); (18, OpenMP, #pragma omp target parallel for if(parallel : argc) if (argc) if (for:argc))", "label": "(14, OpenMP, #pragma omp parallel for proc_bind(close), proc_bind(spread)); (10, OpenMP, #pragma omp parallel for proc_bind(master); (19, OpenMP, #pragma omp parallel for proc_bind(master)); (5, OpenMP, #pragma omp parallel for proc_bind()); (-2, OpenMP, #pragma omp parallel for proc_bind); (12, OpenMP, #pragma omp parallel for proc_bind(x)); (19, OpenMP, #pragma omp parallel proc_bind(close)); (19, OpenMP, #pragma omp parallel for proc_bind(spread)); (-3, OpenMP, #pragma omp parallel for proc_bind()"}
{"predict": "(7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &comm_size);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (44, MPI, vtkh::DestroyKokkos();)", "label": "(6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &comm_size);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (7, MPI, vtkh::SetMPICommHandle(MPI_Comm_c2f(MPI_COMM_WORLD));)"}
{"predict": "(16, CUDA, CUDASimulation cudaSimulation(model);)", "label": "(16, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(50, OpenMP, #pragma omp distribute simd aligned(x, y, z)); (73, OpenMP, #pragma omp distribute simd aligned(x :)); (65, OpenMP, #pragma omp distribute simd aligned(x)); (144, OpenMP, #pragma omp distribute simd aligned(x, y : 0)); (160, OpenMP, #pragma omp distribute simd aligned(x : 1, y, z : 1)); (17, OpenMP, #pragma omp distribute simd aligned()); (7, OpenMP, #pragma omp distribute simd aligned(,); (14, OpenMP, #pragma omp distribute simd aligned(, )); (36, OpenMP, #pragma omp distribute simd aligned(x, y)); (105, OpenMP, #pragma omp distribute simd aligned(x) aligned(z, x)); (24, OpenMP, #pragma omp distribute simd aligned(0)); (13, OpenMP, #pragma omp distribute simd aligned(int)); (109, OpenMP, #pragma omp distribute simd aligned(x, y, z) aligned(y, z)); (77, OpenMP, #pragma omp distribute simd aligned(x : 1)); (121, OpenMP, #pragma omp distribute simd aligned(x) aligned(x, y : 0)); (80, OpenMP, #pragma omp distribute simd aligned(x : 2 * 2)); (10, OpenMP, #pragma omp distribute simd aligned(x)); (16, OpenMP, #pragma omp distribute simd aligned(x,)); (73, OpenMP, #pragma omp distribute simd aligned(x :, ))", "label": "(19, OpenMP, #pragma omp distribute simd aligned(, )); (75, OpenMP, #pragma omp distribute simd aligned(x :)); (80, OpenMP, #pragma omp distribute simd aligned(x :, )); (9, OpenMP, #pragma omp distribute simd aligned(,); (-2, OpenMP, #pragma omp target); (82, OpenMP, #pragma omp distribute simd aligned(x : 1)); (52, OpenMP, #pragma omp distribute simd aligned(x, y, z)); (24, OpenMP, #pragma omp distribute simd aligned(int)); (62, OpenMP, #pragma omp distribute simd aligned(z)); (-4, OpenMP, #pragma omp distribute simd aligned(); (40, OpenMP, #pragma omp distribute simd aligned(x, y)); (32, OpenMP, #pragma omp distribute simd aligned(x)); (80, OpenMP, #pragma omp distribute simd aligned(x : 2 * 2)); (12, OpenMP, #pragma omp distribute simd aligned()); (23, OpenMP, #pragma omp distribute simd aligned(0)); (83, OpenMP, #pragma omp distribute simd aligned(x : 1, y)); (88, OpenMP, #pragma omp distribute simd aligned(x : 1, y, z : 1)); (108, OpenMP, #pragma omp distribute simd aligned(x) aligned(z, x)); (116, OpenMP, #pragma omp distribute simd aligned(x, y, z) aligned(y, z)); (-16, OpenMP, #pragma omp teams)"}
{"predict": "(12, OpenMP, #pragma omp target enter data map(to: data)); (5, OpenMP, #pragma omp target); (13, OpenMP, #pragma omp target teams); (23, OpenMP, #pragma omp target exit data map(from: data)); (5, OpenMP, #pragma omp distribute parallel for collapse(2))", "label": "(14, OpenMP, #pragma omp parallel for collapse(2)); (12, OpenMP, #pragma omp target map(tofrom: data))"}
{"predict": "(78, CUDA, CUDA_CHECK(cudaFree(d_C));); (63, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (46, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_B), sizeof(data_type) * B.size()));); (75, CUDA, CUDA_CHECK(cudaFree(d_B));); (45, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_C), sizeof(data_type) * C.size()));); (79, CUDA, CUDA_CHECK(cudaDeviceReset());); (76, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (40, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (68, CUDA, CUDA_CHECK(cudaFree(d_A));); (33, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)", "label": "(78, CUDA, CUDA_CHECK(cudaFree(d_C));); (63, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (46, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_B), sizeof(data_type) * B.size()));); (74, CUDA, CUDA_CHECK(cudaFree(d_B));); (45, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_C), sizeof(data_type) * C.size()));); (79, CUDA, CUDA_CHECK(cudaDeviceReset());); (76, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (40, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (68, CUDA, CUDA_CHECK(cudaFree(d_A));); (34, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(8, OpenCL, err = clGetDeviceInfo(device, CL_DEVICE_IL_VERSION, 0, NULL, &str_size);)", "label": "(7, OpenCL, err = clGetDeviceInfo(device, CL_DEVICE_IL_VERSION, 0, NULL, &str_size);)"}
{"predict": "(6, OpenMP, #pragma omp target map(tofrom:minval)); (9, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:minval) reduction(min:minval))", "label": "(10, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:minval) reduction(min:minval))"}
{"predict": "(48, MPI, MPI_Comm_size(intercomm, &size);); (90, MPI, MPI_Comm_rank(intracomm2, &icrank);); (65, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (91, MPI, MPI_Comm_remote_size(intracomm2, &rsize);); (152, MPI, MPI_Comm_disconnect(&intracomm3);); (144, MPI, MPI_Comm_disconnect(&intracomm2);); (30, MPI, MPI_Send(&i, 1, MPI_INT, i, 0, intercomm);); (94, MPI, MPI_Comm_size(intracomm2, &csize);); (53, MPI, MPI_Comm_rank(intracomm, &icrank);); (13, MPI, MPI_Comm_get_parent(&parentcomm);); (138, MPI, MPI_Comm_disconnect(&intracomm);); (47, MPI, MPI_Comm_remote_size(intracomm, &rsize);); (49, MPI, MPI_Comm_size(intracomm, &icsize);); (134, MPI, MPI_Comm_disconnect(&intercomm);); (48, MPI, MPI_Comm_rank(intracomm, &wrank);); (83, MPI, MPI_Intercomm_merge(intercomm, 0, &intracomm2);); (22, MPI, MPI_Recv(&i, 1, MPI_INT, i, 0, intercomm, &status);); (74, MPI, MPI_Intercomm_merge(intercomm, 1, &intracomm3);); (75, MPI, MPI_Comm_rank(intracomm3, &icrank);); (68, MPI, MPI_Intercomm_merge(intercomm, 1, &intracomm2);); (75, MPI, MPI_Comm_remote_size(intracomm3, &rsize);); (68, MPI, MPI_Comm_size(intracomm3, &size);); (74, MPI", "label": "(65, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (56, MPI, MPI_Intercomm_merge(intercomm, isChild, &intracomm);); (161, MPI, MPI_Comm_free(&intracomm3);); (95, MPI, MPI_Comm_rank(intracomm2, &icrank);); (12, MPI, MPI_Comm_get_parent(&parentcomm);); (34, MPI, MPI_Send(&i, 1, MPI_INT, i, 0, intercomm);); (141, MPI, MPI_Ssend(&errs, 1, MPI_INT, 0, 1, intercomm);); (56, MPI, MPI_Comm_size(intracomm, &icsize);); (146, MPI, MPI_Recv(&err, 1, MPI_INT, i, 1, intercomm, MPI_STATUS_IGNORE);); (117, MPI, MPI_Intercomm_merge(intercomm, 0, &intracomm3);); (65, MPI, MPI_Comm_remote_size(parentcomm, &psize);); (80, MPI, MPI_Intercomm_merge(intercomm,!isChild, &intracomm2);); (149, MPI, MPI_Comm_free(&intracomm);); (102, MPI, MPI_Comm_remote_size(intercomm, &csize);); (83, MPI, MPI_Comm_size(intracomm2, &icsize);); (49, MPI, MPI_Comm_rank(intracomm, &icrank);); (150, MPI, MPI_Comm_free(&intercomm);); (32, MPI, MPI_Recv(&i, 1, MPI_INT, 0, 0, intercomm, &status);); (114, MPI, MPI_Comm_rank(intracomm3, &icrank);); (10, MPI, MPI_Comm_rank(intercomm, &rank);); (7, MPI, MPI_Comm_remote_size(intercomm, &rsize);); (7, MPI, MPI_Comm_size(intercomm, &size);); (109, MPI, MPI_Comm_size(intracomm3, &icsize);); (139, MPI, MPI_Comm_free(&intracomm2);); (150, MPI, MPI_Finalize();)"}
{"predict": "(245, CUDA, if (d_csrRowPtrA) cudaFree(d_csrRowPtrA);); (254, CUDA, if (d_csrRowPtrC) cudaFree(d_csrRowPtrC);); (300, CUDA, cudaStat1 = cudaMemcpy(csrRowPtrC, d_csrRowPtrC, (m + 1) * sizeof(int), cudaMemcpyDeviceToHost);); (248, CUDA, if (d_csrColIndC) cudaFree(d_csrColIndC);); (302, CUDA, cudaStat1 = cudaMemcpy(csrColIndC, d_csrColIndC, nnzC * sizeof(int), cudaMemcpyDeviceToHost);); (304, CUDA, cudaStat1 = cudaMemcpy(csrValC, d_csrValC, nnzC * sizeof(float), cudaMemcpyDeviceToHost);); (250, CUDA, if (d_csrValC) cudaFree(d_csrValC);); (48, CUDA, cudaStat1 = cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking);); (240, CUDA, if (d_csrColIndA) cudaFree(d_csrColIndA);); (244, CUDA, if (d_csrValA) cudaFree(d_csrValA);); (314, CUDA, cudaDeviceReset();); (185, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrRowPtrC, (m + 1) * sizeof(int));); (185, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrColIndC, nnzC * sizeof(int));); (185, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrValC, nnzC * sizeof(float));); (233, CUDA, if (d_csrRowPtrC) cudaFree(d_csrRowPtrC);); (236, CUDA, if (stream) cudaStreamDestroy(stream);); (230, CUDA, if (d_csrRowPtrA) cudaFree(d_csrRowPtrA);); (180, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrColIndA, nn", "label": "(106, CUDA, cudaStat1 = cudaMemcpy(d_csrRowPtrA, csrRowPtrA, sizeof(int)*(m + 1), cudaMemcpyHostToDevice);); (234, CUDA, if (d_csrColIndC) cudaFree(d_csrColIndC);); (211, CUDA, cudaStat1 = cudaMemcpy(csrRowPtrC, d_csrRowPtrC, sizeof(int)*(m + 1), cudaMemcpyDeviceToHost);); (99, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrRowPtrC, sizeof(int)*(m + 1));); (94, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrValA, sizeof(float)*nnzA);); (109, CUDA, cudaStat1 = cudaMemcpy(d_csrValA, csrValA, sizeof(float)*nnzA, cudaMemcpyHostToDevice);); (221, CUDA, if (d_csrRowPtrA) cudaFree(d_csrRowPtrA);); (237, CUDA, if (stream) cudaStreamDestroy(stream);); (225, CUDA, if (d_csrRowPtrC) cudaFree(d_csrRowPtrC);); (170, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrValC, sizeof(float) * nnzC);); (227, CUDA, if (d_csrValC) cudaFree(d_csrValC);); (154, CUDA, cudaStat1 = cudaDeviceSynchronize();); (78, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrRowPtrA, sizeof(int)*(m + 1));); (204, CUDA, cudaStat1 = cudaMemcpy(csrColIndC, d_csrColIndC, sizeof(int)*nnzC, cudaMemcpyDeviceToHost);); (80, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrColIndA, sizeof(int)*nnzA);); (41, CUDA, cudaStat1 = cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking);); (94, CUDA, cudaStat1 = cudaMemcpy(d_csrColIndA, csrColIndA, sizeof(int)*nnzA, cudaMemcpyHostToDevice);); (125, CUDA, cudaStat1 = cudaMalloc((void**)&d_work, lworkInBytes);); (211, CUDA, if (d_csrColIndA) cudaFree(d_csrColIndA);); (202, CUDA, cudaStat1 = cudaMemcpy(csrValC, d_csrValC, sizeof(float)*nnzC, cudaMemcpyDeviceToHost);); (155, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrColIndC, sizeof(int) * nnzC);); (210, CUDA, if (d_csrValA) cudaFree(d_csrValA);); (228, CUDA, cudaDeviceReset();)"}
{"predict": "(81, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (81, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)", "label": "(81, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (81, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)"}
{"predict": "(25, OpenMP, omp_destroy_dynamic_cgroup(kern->cgroup);)", "label": "(26, OpenCL, clReleaseContext(knode->key.context);); (24, OpenCL, clReleaseProgram(kern->program);)"}
{"predict": "(57, MPI, t1 = MPI_Wtime();); (58, MPI, MPI_Sendrecv(sbuf, len, MPI_BYTE, partner, k, rbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (56, MPI, t1 = MPI_Wtime() - t1;); (36, MPI, MPI_Barrier(MPI_COMM_WORLD);); (50, MPI, MPI_Sendrecv(MPI_BOTTOM, 0, MPI_BYTE, partner, 0, MPI_BOTTOM, 0, MPI_BYTE, partner, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (18, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (45, MPI, MPI_Send(sbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD);); (42, MPI, MPI_Irecv(rbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD, &rreq);); (43, MPI, MPI_Wait(&rreq, MPI_STATUS_IGNORE);)", "label": "(13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (60, MPI, MPI_Irecv(rbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD, &rreq);); (57, MPI, t1 = MPI_Wtime();); (60, MPI, MPI_Wait(&rreq, MPI_STATUS_IGNORE);); (146, MPI, MPI_Recv(rbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (57, MPI, MPI_Send(sbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (34, MPI, MPI_Barrier(MPI_COMM_WORLD);); (57, MPI, t1 = MPI_Wtime() - t1;); (8, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (24, MPI, MPI_Abort(MPI_COMM_WORLD, 2);)"}
{"predict": "(1, TBB, using namespace oneapi::tbb::flow;)", "label": "(1, TBB, using namespace tbb::flow;)"}
{"predict": "(5, OpenMP, #pragma omp teams distribute parallel for simd safelen (); (20, OpenMP, #pragma omp teams distribute parallel for simd safelen (2+2))); (38, OpenMP, #pragma omp teams distribute parallel for simd safelen (foobool(argc)), safelen (true), safelen (-5)); (13, OpenMP, #pragma omp teams distribute parallel for simd safelen (4); (24, OpenMP, #pragma omp teams distribute parallel for simd safelen (foobool(1) > 0? 1 : 2)); (50, OpenMP, #pragma omp teams distribute parallel for simd safelen(safelen(tmain<int, char, -1, -2>(argc, argv)); (35, OpenMP, #pragma omp teams distribute parallel for simd safelen (S1)); (2, OpenMP, #pragma omp teams distribute parallel for simd safelen ()); (46, OpenMP, #pragma omp teams distribute parallel for simd safelen (argv[1]=2)); (3, OpenMP, #pragma omp teams distribute parallel for simd safelen (2); (13, OpenMP, #pragma omp teams distribute parallel for simd safelen (foobool(argc)), safelen (true), safelen (-5)); (17, OpenMP, #pragma omp teams distribute parallel for simd safelen (foobool(argc) > 0? foobool(argc) : 4))", "label": "(7, OpenMP, #pragma omp teams distribute parallel for simd safelen (); (21, OpenMP, #pragma omp teams distribute parallel for simd safelen (2+2))); (10, OpenMP, #pragma omp teams distribute parallel for simd safelen ()); (54, OpenMP, #pragma omp teams distribute parallel for simd safelen(safelen(tmain<int, char, -1, -2>(argc, argv)); (37, OpenMP, #pragma omp teams distribute parallel for simd safelen (S1)); (12, OpenMP, #pragma omp teams distribute parallel for simd safelen (4); (45, OpenMP, #pragma omp teams distribute parallel for simd safelen (argv[1]=2)); (21, OpenMP, #pragma omp teams distribute parallel for simd safelen (foobool(1) > 0? 1 : 2)); (28, OpenMP, #pragma omp teams distribute parallel for simd safelen (foobool(argc)), safelen (true), safelen (-5)); (-8, OpenMP, #pragma omp target); (-8, OpenMP, #pragma omp teams distribute parallel for simd safelen)"}
{"predict": "(24, MPI, MPI_Finalize();); (12, MPI, MPI_Init(&argc,&argv);)", "label": "(24, MPI, MPI_Finalize();); (12, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(55, TBB, tbb::flow::remove_edge( n, r[i] );); (1, TBB, tbb::flow::write_once_node<R> n(g);); (18, TBB, tbb::flow::make_edge( n, r[i] );); (-1, TBB, tbb::flow::graph g;)", "label": "(20, TBB, tbb::flow::make_edge( n, r[i] );); (54, TBB, tbb::flow::remove_edge( n, r[i] );); (1, TBB, tbb::flow::write_once_node<R> n(g);); (-1, TBB, tbb::flow::graph g;)"}
{"predict": "(34, OpenMP, #pragma omp distribute parallel for schedule (dynamic, foobool(1) > 0? 1 : 2 -z )); (3, OpenMP, #pragma omp distribute parallel for schedule); (5, OpenMP, #pragma omp distribute parallel for schedule (); (13, OpenMP, #pragma omp distribute parallel for schedule (auto,); (52, OpenMP, #pragma omp distribute parallel for schedule(static, argv[1]=2)); (44, OpenMP, #pragma omp distribute parallel for schedule (guided, foobool(argc)), schedule (static, true), schedule (dynamic, -5)); (15, OpenMP, #pragma omp distribute parallel for schedule (auto, )); (52, OpenMP, #pragma omp distribute parallel for schedule(guided, schedule(tmain<int, char, -1, -2>(argc, argv)); (18, OpenMP, #pragma omp distribute parallel for schedule (runtime, 3)); (23, OpenMP, #pragma omp distribute parallel for schedule (guided, 4); (35, OpenMP, #pragma omp distribute parallel for schedule (static, S1)); (46, OpenMP, #pragma omp distribute parallel for schedule(dynamic, schedule(tmain<int, char, -1, -2>(argc, argv)); (37, OpenMP, #pragma omp distribute parallel for schedule (guided, argv[1]=2)); (27, OpenMP, #pragma omp distribute parallel for schedule (static, 2+2))); (17, OpenMP, #pragma omp distribute parallel for schedule (static, 2))); (-8, OpenMP, #pragma omp target); (21, OpenMP, #pragma omp distribute parallel for schedule (dynamic, foobool(argc)), schedule (static, true), schedule (guided, -5)); (-11, OpenMP, #pragma omp distribute parallel for schedule (); (-9, OpenMP, #pragma omp teams)", "label": "(16, OpenMP, #pragma omp distribute parallel for schedule (auto); (45, OpenMP, #pragma omp distribute parallel for schedule (guided, foobool(argc)), schedule (static, true), schedule (dynamic, -5)); (48, OpenMP, #pragma omp distribute parallel for schedule (guided, S1)); (52, OpenMP, #pragma omp distribute parallel for schedule (static, argv[1]=2)); (0, OpenMP, #pragma omp distribute parallel for schedule); (31, OpenMP, #pragma omp distribute parallel for schedule (static, 2+2))); (2, OpenMP, #pragma omp distribute parallel for schedule (); (5, OpenMP, #pragma omp distribute parallel for schedule ()); (32, OpenMP, #pragma omp distribute parallel for schedule (dynamic, foobool(1) > 0? 1 : 2 - z)); (23, OpenMP, #pragma omp distribute parallel for schedule (guided, 4); (51, OpenMP, #pragma omp distribute parallel for schedule(dynamic, schedule(tmain<int, char, -1, -2>(argc, argv)); (17, OpenMP, #pragma omp distribute parallel for schedule (runtime, 3)); (-10, OpenMP, #pragma omp target); (11, OpenMP, #pragma omp distribute parallel for schedule (auto,); (6, OpenMP, #pragma omp distribute parallel for schedule (auto_dynamic); (-12, OpenMP, #pragma omp teams)"}
{"predict": "(18, CUDA, cuda_safe_call(cudaStreamSynchronize(stream));)", "label": "(18, CUDA, cuda_safe_call(cudaGetLastError());)"}
{"predict": "(40, CUDA, cudaMemcpy(deviceMatrix, hostMatrix, width * height * sizeof(float), cudaMemcpyHostToDevice);); (92, CUDA, cudaFree(deviceMatrix);); (93, CUDA, cudaFree(deviceToeplitzResult);); (94, CUDA, cudaFree(deviceDiagonalResult);); (94, CUDA, cudaFree(deviceSymmetricResult);); (94, CUDA, cudaFree(deviceSkewSymmetricResult);); (94, CUDA, cudaFree(deviceIdempotentResult);); (12, CUDA, cudaMalloc((void**)&deviceMatrix, width * height * sizeof(float));); (47, CUDA, cudaMemcpy(hostToeplitzResult, deviceToeplitzResult, sizeof(bool), cudaMemcpyDeviceToHost);); (11, CUDA, cudaMalloc((void**)&deviceToeplitzResult, sizeof(bool));); (12, CUDA, cudaMalloc((void**)&deviceDiagonalResult, sizeof(bool));); (44, CUDA, cudaMemcpy(hostDiagonalResult, deviceDiagonalResult, sizeof(bool), cudaMemcpyDeviceToHost);); (10, CUDA, cudaMalloc((void**)&deviceSymmetricResult, sizeof(bool));); (10, CUDA, cudaMalloc((void**)&deviceSkewSymmetricResult, sizeof(bool));); (10, CUDA, cudaMalloc((void**)&deviceIdempotentResult, sizeof(bool));); (41, CUDA, cudaMemcpy(hostSymmetricResult, deviceSymmetricResult, sizeof(bool), cudaMemcpyDeviceToHost);); (33, CUDA, cudaMemcpy(hostSkewSymmetricResult, deviceSkewSymmetricResult, sizeof(bool), cudaMemcpyDeviceToHost);); (41, CUDA, cudaMemcpy(hostIdempotentResult, deviceIdempotentResult, sizeof(bool), cudaMemcpyDeviceToHost);)", "label": "(21, CUDA, cudaMalloc((void**)&deviceDiagonalResult, sizeof(bool));); (55, CUDA, cudaMemcpy(hostDiagonalResult, deviceDiagonalResult, sizeof(bool), cudaMemcpyDeviceToHost);); (99, CUDA, cudaFree(deviceSymmetricResult);); (20, CUDA, cudaMalloc((void**)&deviceSkewSymmetricResult, sizeof(bool));); (54, CUDA, cudaMemcpy(hostSkewSymmetricResult, deviceSkewSymmetricResult, sizeof(bool), cudaMemcpyDeviceToHost);); (17, CUDA, cudaMalloc((void**)&deviceSymmetricResult, sizeof(bool));); (33, CUDA, cudaMemcpy(deviceMatrix, hostMatrix, width * height * sizeof(float), cudaMemcpyHostToDevice);); (37, CUDA, cudaMemcpy(deviceIdempotentResult, hostIdempotentResult, sizeof(bool), cudaMemcpyHostToDevice);); (11, CUDA, cudaMalloc((void**)&deviceMatrix, width * height * sizeof(float));); (31, CUDA, cudaMemcpy(deviceToeplitzResult, hostToeplitzResult, sizeof(bool), cudaMemcpyHostToDevice);); (49, CUDA, cudaMemcpy(hostIdempotentResult, deviceIdempotentResult, sizeof(bool), cudaMemcpyDeviceToHost);); (91, CUDA, cudaFree(deviceSkewSymmetricResult);); (31, CUDA, cudaMemcpy(deviceSkewSymmetricResult, hostSkewSymmetricResult, sizeof(bool), cudaMemcpyHostToDevice);); (44, CUDA, cudaMemcpy(hostSymmetricResult, deviceSymmetricResult, sizeof(bool), cudaMemcpyDeviceToHost);); (28, CUDA, cudaMemcpy(deviceSymmetricResult, hostSymmetricResult, sizeof(bool), cudaMemcpyHostToDevice);); (85, CUDA, cudaFree(deviceDiagonalResult);); (4, CUDA, cudaMalloc((void**)&deviceToeplitzResult, sizeof(bool));); (82, CUDA, cudaFree(deviceToeplitzResult);); (85, CUDA, cudaFree(deviceIdempotentResult);); (36, CUDA, cudaMemcpy(hostToeplitzResult, deviceToeplitzResult, sizeof(bool), cudaMemcpyDeviceToHost);); (78, CUDA, cudaFree(deviceMatrix);); (20, CUDA, cudaMemcpy(deviceDiagonalResult, hostDiagonalResult, sizeof(bool), cudaMemcpyHostToDevice);); (2, CUDA, cudaMalloc((void**)&deviceIdempotentResult, sizeof(bool));)"}
{"predict": "(65, OpenCL, clReleaseMemObject( streams[ii+1] );); (55, OpenCL, err = clEnqueueWriteBuffer( queue, streams[ii], false, 0, ptrSizes[i]*num_elements, inptr[i], 0, NULL, &writeEvent );); (65, OpenCL, clReleaseEvent(writeEvent);); (57, OpenCL, clReleaseMemObject( streams[ii] );); (62, OpenCL, err = clWaitForEvents( 1, &writeEvent );); (59, OpenCL, clReleaseMemObject( streams[ii+1] );); (75, OpenCL, clReleaseEvent( writeEvent );); (56, OpenCL, clReleaseProgram( program[i] );); (56, OpenCL, clReleaseKernel( kernel[i] );); (58, OpenCL, clReleaseContext( context );)", "label": "(57, OpenCL, err = clEnqueueWriteBuffer( queue, streams[ii], false, 0, ptrSizes[i]*num_elements, inptr[i], 0, NULL, &writeEvent );); (132, OpenCL, err = clSetKernelArg( kernel[i], 0, sizeof( cl_mem ), (void *)&streams[ii] );); (49, OpenCL, clReleaseMemObject(streams[ii]);); (135, OpenCL, clReleaseProgram( program[i] );); (123, OpenCL, clReleaseMemObject(streams[ii+1]);); (55, OpenCL, clReleaseMemObject( streams[ii+1] );); (140, OpenCL, err = clEnqueueNDRangeKernel( queue, kernel[i], 1, NULL, threads, NULL, 0, NULL, NULL );); (52, OpenCL, clReleaseMemObject( streams[ii] );); (64, OpenCL, clReleaseEvent(writeEvent);); (59, OpenCL, err = clWaitForEvents( 1, &writeEvent );); (127, OpenCL, clReleaseKernel( kernel[i] );); (123, OpenCL, err |= clSetKernelArg( kernel[i], 1, sizeof( cl_mem ), (void *)&streams[ii+1] );); (148, OpenCL, err = clEnqueueReadBuffer( queue, streams[ii+1], true, 0, outPtrSizes[i]*num_elements, outptr[i], 0, NULL, NULL );)"}
{"predict": "(48, OpenMP, #pragma omp target teams distribute parallel for simd linear (argv[1])); (12, OpenMP, #pragma omp target teams distribute parallel for simd linear (); (14, OpenMP, #pragma omp target teams distribute parallel for simd linear ()); (37, OpenMP, #pragma omp target teams distribute parallel for simd linear (S1)); (44, OpenMP, #pragma omp target teams distribute parallel for simd linear(i)); (17, OpenMP, #pragma omp target teams distribute parallel for simd linear (argc); (20, OpenMP, #pragma omp target teams distribute parallel for simd linear (argc,); (11, OpenMP, #pragma omp target teams distribute parallel for simd linear (argc); (22, OpenMP, #pragma omp target teams distribute parallel for simd linear (argc > 0? argv[1] : argv[2])); (28, OpenMP, #pragma omp target teams distribute parallel for simd linear (argc : 5)); (33, OpenMP, #pragma omp target teams distribute parallel for simd linear(e, g)); (20, OpenMP, #pragma omp target teams distribute parallel for simd linear (argc :); (36, OpenMP, #pragma omp target teams distribute parallel for simd linear(h, C::x)); (4, OpenMP, #pragma omp target teams distribute parallel for simd linear (argc,)", "label": "(15, OpenMP, #pragma omp target teams distribute parallel for simd linear (); (11, OpenMP, #pragma omp target teams distribute parallel for simd linear); (34, OpenMP, #pragma omp target teams distribute parallel for simd linear (S1)); (37, OpenMP, #pragma omp target teams distribute parallel for simd linear (a, b)); (39, OpenMP, #pragma omp target teams distribute parallel for simd linear (argv[1])); (44, OpenMP, #pragma omp target teams distribute parallel for simd linear(h, C::x)); (23, OpenMP, #pragma omp target teams distribute parallel for simd linear (argc > 0? argv[1] : argv[2])); (15, OpenMP, #pragma omp target teams distribute parallel for simd linear (argc); (10, OpenMP, #pragma omp target teams distribute parallel for simd linear ()); (24, OpenMP, #pragma omp target teams distribute parallel for simd linear (argc)); (36, OpenMP, #pragma omp target teams distribute parallel for simd linear(e, g)); (15, OpenMP, #pragma omp target teams distribute parallel for simd linear (argc,)"}
{"predict": "(84, OpenCL, clReleaseMemObject(bufX);); (46, OpenCL, clReleaseCommandQueue(queue);); (29, OpenCL, printf( \"clCreateContext() failed with %d\\n\", err );); (77, OpenCL, clReleaseEvent(event);); (74, OpenCL, clReleaseMemObject(bufY);); (33, OpenCL, printf( \"clCreateCommandQueue() failed with %d\\n\", err );); (29, OpenCL, queue = clCreateCommandQueue(ctx, device, 0, &err);); (21, OpenCL, clReleaseContext(ctx);); (48, OpenCL, err = clEnqueueWriteBuffer(queue, bufY, CL_TRUE, 0, (lenY*sizeof(cl_float)), Y, 0, NULL, NULL);); (55, OpenCL, err = clWaitForEvents(1, &event);); (17, OpenCL, ctx = clCreateContext(props, 1, &device, NULL, NULL, &err);); (42, OpenCL, bufY = clCreateBuffer(ctx, CL_MEM_READ_WRITE, (lenY*sizeof(cl_float)), NULL, &err);); (44, OpenCL, err = clEnqueueWriteBuffer(queue, bufX, CL_TRUE, 0, (lenX*sizeof(cl_float)), X, 0, NULL, NULL);); (70, OpenCL, clReleaseMemObject(bufX);); (34, OpenCL, bufX = clCreateBuffer(ctx, CL_MEM_READ_WRITE, (lenX*sizeof(cl_float)), NULL, &err);)", "label": "(56, OpenCL, err = clEnqueueWriteBuffer(queue, bufY, CL_TRUE, 0, (lenY*sizeof(cl_float)), Y, 0, NULL, NULL);); (82, OpenCL, clReleaseMemObject(bufX);); (44, OpenCL, clReleaseCommandQueue(queue);); (28, OpenCL, printf( \"clCreateContext() failed with %d\\n\", err );); (75, OpenCL, clReleaseEvent(event);); (32, OpenCL, printf( \"clCreateCommandQueue() failed with %d\\n\", err );); (29, OpenCL, queue = clCreateCommandQueue(ctx, device, 0, &err);); (46, OpenCL, bufY = clCreateBuffer(ctx, CL_MEM_READ_WRITE, (lenY*sizeof(cl_float)), NULL, &err);); (30, OpenCL, clReleaseContext(ctx);); (73, OpenCL, clReleaseMemObject(bufY);); (42, OpenCL, bufX = clCreateBuffer(ctx, CL_MEM_READ_ONLY, (lenX*sizeof(cl_float)), NULL, &err);); (44, OpenCL, err = clEnqueueWriteBuffer(queue, bufX, CL_TRUE, 0, (lenX*sizeof(cl_float)), X, 0, NULL, NULL);); (17, OpenCL, ctx = clCreateContext(props, 1, &device, NULL, NULL, &err);); (53, OpenCL, err = clWaitForEvents(1, &event);)"}
{"predict": "(57, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (63, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (5, CUDA, CUDA_CHECK(cudaStreamCreate(&stream));); (45, CUDA, CUDA_CHECK(cudaGetLastError());)", "label": "(62, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (6, CUDA, CUDA_CHECK(cudaStreamCreate(&stream));); (46, CUDA, CUDA_CHECK(cudaGetLastError());); (52, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));)"}
{"predict": "(7, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)", "label": "(7, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)"}
{"predict": "(6, CUDA, cudaSetDevice(devs[devi]);)", "label": "(11, OpenMP, #pragma omp parallel for)"}
{"predict": "(13, OpenCL, clReleaseMemObject(objC);); (8, OpenCL, clReleaseMemObject(objB);); (3, OpenCL, clReleaseMemObject(objA);)", "label": "(5, OpenCL, clReleaseMemObject(objA);); (8, OpenCL, clReleaseMemObject(objB);); (11, OpenCL, clReleaseMemObject(objC);)"}
{"predict": "(7, OpenMP, #pragma omp parallel shared(nthreads)); (7, OpenMP, #pragma omp single); (6, OpenMP, my_dev = omp_get_default_device();); (27, OpenMP, #pragma omp atomic); (18, OpenMP, #pragma omp parallel shared(nthreads)); (34, OpenMP, omp_destroy_device(my_dev);)", "label": "(7, OpenMP, #pragma omp parallel); (7, OpenMP, #pragma omp single); (3, OpenMP, my_dev = omp_get_initial_device();); (6, OpenMP, nthreads = omp_get_num_threads();); (10, OpenMP, #pragma omp parallel shared(nthreads))"}
{"predict": "(6, CUDA, __shared__ float sdata2[THREAD_PER_BLOCK];); (6, CUDA, __shared__ float scale;)", "label": "(7, CUDA, __shared__ float scale;); (5, CUDA, __shared__ float sdata2[THREAD_PER_BLOCK];)"}
{"predict": "(42, OpenMP, #pragma omp teams shared (argc)); (34, OpenMP, #pragma omp teams shared (a, b, c, d, f)); (73, OpenMP, #pragma omp teams private(i)); (3, OpenMP, #pragma omp teams shared); (50, OpenMP, #pragma omp teams shared(da)); (60, OpenMP, #pragma omp teams shared(j)); (37, OpenMP, #pragma omp teams shared(ba)); (4, OpenMP, #pragma omp teams shared (); (5, OpenMP, #pragma omp teams shared ()); (63, OpenMP, #pragma omp teams shared(i)); (20, OpenMP, #pragma omp teams shared (argc, z)); (4, OpenMP, #pragma omp teams shared (argc); (46, OpenMP, #pragma omp teams shared(h, B::x)); (15, OpenMP, #pragma omp teams shared (argc > 0? argv[1] : argv[2])); (33, OpenMP, #pragma omp teams shared(e, g)); (18, OpenMP, #pragma omp teams shared (S1)); (57, OpenMP, #pragma omp teams private(i)); (3, OpenMP, #pragma omp teams shared (argc)); (27, OpenMP, #pragma omp teams shared(ca)); (1, OpenMP, #pragma omp teams shared (argc,); (37, OpenMP, #pragma omp teams shared(i)); (45, OpenMP, #pragma omp teams private(i)); (2, OpenMP, #pragma omp teams shared (argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (16, OpenMP, #pragma omp teams shared (argv[1])); (46, OpenMP, #pragma omp teams shared(j)); (38, OpenMP, #pragma omp teams private(i)); (24, OpenMP, #pragma omp teams shared(z, d))", "label": "(8, OpenMP, #pragma omp teams shared); (61, OpenMP, #pragma omp teams shared(i)); (45, OpenMP, #pragma omp teams shared(e, g, z)); (4, OpenMP, #pragma omp target); (13, OpenMP, #pragma omp teams shared (argc); (51, OpenMP, #pragma omp teams firstprivate(i), shared(i)); (17, OpenMP, #pragma omp teams shared (argc > 0? argv[1] : argv[2])); (22, OpenMP, #pragma omp teams shared (S1)); (18, OpenMP, #pragma omp teams shared (argc)); (35, OpenMP, #pragma omp teams shared(da)); (31, OpenMP, #pragma omp teams shared(ca)); (54, OpenMP, #pragma omp teams shared(j)); (23, OpenMP, #pragma omp teams shared (argv[1])); (37, OpenMP, #pragma omp teams shared(h, B::x)); (39, OpenMP, #pragma omp teams private(i)); (-1, OpenMP, #pragma omp teams shared ()); (22, OpenMP, #pragma omp teams shared(ba)); (3, OpenMP, #pragma omp teams shared (argc,); (14, OpenMP, #pragma omp teams shared (a, b, c, d, f)); (-8, OpenMP, #pragma omp teams shared (); (48, OpenMP, #pragma omp teams firstprivate(i))"}
{"predict": "(16, OpenMP, #pragma omp target map(from : l_val)); (5, OpenMP, #pragma omp parallel); (9, OpenMP, #pragma omp task); (3, OpenMP, #pragma omp single); (12, OpenMP, #pragma omp target parallel device(argc) map(to : l_val))", "label": "(15, OpenMP, #pragma omp target map(l_val)); (9, OpenMP, #pragma omp task); (3, OpenMP, #pragma omp parallel); (4, OpenMP, #pragma omp single)"}
{"predict": "(7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (8, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (40, MPI, mpi_errno = MPI_Comm_free(&stream_comm);); (13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (41, MPI, mpi_errno = MPI_Comm_free(&stream_comm);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (13, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(58, MPI, MPI_Type_commit(file_realm_types);); (56, MPI, MPI_Type_contiguous_c((max_end_offset - min_st_offset + 1), MPI_BYTE, file_realm_types);)", "label": "(58, MPI, MPI_Type_commit(file_realm_types);); (56, MPI, MPI_Type_contiguous_c((max_end_offset - min_st_offset + 1), MPI_BYTE, file_realm_types);)"}
{"predict": "(25, OpenCL, status = clBuildProgram(program, 1, &device, NULL, NULL, NULL);)", "label": "(25, OpenCL, OCL_ASSERT(clBuildProgram(program, 1, &device, NULL, NULL, NULL) == CL_SUCCESS);); (26, OpenCL, kernel = clCreateKernel(program, \"compiler_ceil\", &status);)"}
{"predict": "(50, CUDA, dynamics.freeCudaMem();)", "label": "(4, CUDA, CudaCheckError();); (49, CUDA, dynamics.freeCudaMem();)"}
{"predict": "(18, CUDA, CUDA_CHECK(cudaGetLastError());)", "label": "(18, CUDA, CUDA_CHECK(cudaGetLastError());)"}
{"predict": "(74, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (40, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpyAsync(inData->basePtr(), inVec.data(), inBufSize, cudaMemcpyHostToDevice, stream));); (53, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (70, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamDestroy(stream));)", "label": "(39, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(inData->basePtr(), inVec.data(), inBufSize, cudaMemcpyHostToDevice));); (71, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (71, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamDestroy(stream));); (51, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));)"}
{"predict": "(8, CUDA, cudaMemGetInfo(&freeCUDAMem, &totalCUDAMem);)", "label": "(5, CUDA, cudaSetDevice(0);); (8, CUDA, cudaMemGetInfo(&freeCUDAMem, &totalCUDAMem);)"}
{"predict": "(48, OpenMP, std::cout << \"Number of threads (max)   = \" << omp_get_max_threads() << std::endl;)", "label": "(48, OpenMP, std::cout << \"Number of threads    = \" << omp_get_max_threads() << std::endl;)"}
{"predict": "(25, MPI, merr = MPI_Publish_name(serv_name, port_name, MPI_INFO_NULL, MPI_COMM_WORLD);); (25, MPI, MPI_Error_string(merr, errmsg, &msglen);); (59, MPI, merr = MPI_Unpublish_name(serv_name, MPI_COMM_WORLD);); (32, MPI, merr = MPI_Lookup_name(serv_name, MPI_INFO_NULL, port_name_out);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (64, MPI, MPI_Error_class(merr, &mclass);); (54, MPI, MPI_Close_port(port_name);); (27, MPI, MPI_Barrier(MPI_COMM_WORLD);); (12, MPI, merr = MPI_Open_port(MPI_INFO_NULL, port_name);); (38, MPI, MPI_Error_string(merr, errmsg, &msglen);); (40, MPI, merr = MPI_Lookup_name(serv_name, MPI_INFO_NULL, port_name_out);)", "label": "(35, MPI, merr = MPI_Unpublish_name(serv_name, MPI_INFO_NULL, port_name);); (24, MPI, merr = MPI_Publish_name(serv_name, MPI_INFO_NULL, port_name);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (25, MPI, MPI_Error_string(merr, errmsg, &msglen);); (41, MPI, merr = MPI_Lookup_name(serv_name, MPI_INFO_NULL, port_name_out);); (64, MPI, MPI_Error_class(merr, &mclass);); (16, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (25, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(1, OpenMP, #pragma omp parallel for)", "label": "(13, OpenMP, __kmpc_omp_task(nullptr, gtid, task1);); (1, OpenMP, #pragma omp parallel for); (18, OpenMP, __kmpc_omp_task(nullptr, gtid, task2);)"}
{"predict": "(56, TBB, tbb::concurrent_bounded_queue<int> iq4;); (37, TBB, tbb::concurrent_bounded_queue<int> iq3;); (20, TBB, tbb::concurrent_bounded_queue<int> iq2;); (1, TBB, tbb::concurrent_bounded_queue<int> iq1;)", "label": "(55, TBB, tbb::concurrent_bounded_queue<int> iq4;); (22, TBB, tbb::concurrent_bounded_queue<int> iq2;); (35, TBB, tbb::concurrent_bounded_queue<int> iq3;); (3, TBB, tbb::concurrent_bounded_queue<int> iq1;)"}
{"predict": "(22, CUDA, cudaFree(res_dev);); (15, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(bool), cudaMemcpyDeviceToHost);); (11, CUDA, cudaMalloc(&res_dev, sizeof(bool));)", "label": "(22, CUDA, cudaFree(res_dev);); (15, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(bool), cudaMemcpyDeviceToHost);); (11, CUDA, cudaMalloc(&res_dev, sizeof(bool));)"}
{"predict": "(46, OpenMP, auto stop_clock = omp_get_wtime();); (26, OpenMP, auto start_clock = omp_get_wtime();)", "label": "(27, OpenMP, auto start_clock = omp_get_wtime();); (44, OpenMP, auto stop_clock = omp_get_wtime();)"}
{"predict": "(4, OpenMP, #pragma omp target teams distribute simd firstprivate(g, g1, sivar))", "label": "(4, OpenMP, #pragma omp target teams distribute simd firstprivate(g, g1, sivar))"}
{"predict": "(19, CUDA, ASSERT_EQ(cudaGetLastError(), cudaSuccess) << \"Kernel launch error.\";); (20, CUDA, cudaDeviceSynchronize();)", "label": "(19, CUDA, ASSERT_EQ(cudaGetLastError(), cudaSuccess) << \"Kernel launch error.\";)"}
{"predict": "(21, OpenCL, status = clBuildProgram(program, 1, &device, NULL, NULL, NULL);); (24, OpenCL, kernel = clCreateKernel(program, \"cmrt_utest_genx\", &status);)", "label": "(23, OpenCL, kernel = clCreateKernel(program, \"simplemov\", &status);); (20, OpenCL, OCL_ASSERT(clBuildProgram(program, 1, &device, NULL, NULL, NULL) == CL_SUCCESS);)"}
{"predict": "(11, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (5, CUDA, getLastCudaError(\"kernel launch failed\");)", "label": "(2, CUDA, cudaGraphicsMapResources(1, &cuda_pbo_resource, 0);); (7, CUDA, cudaGraphicsUnmapResources(1, &cuda_pbo_resource, 0);)"}
{"predict": "(3, OpenMP, #pragma omp target map(tofrom: threadCount)); (4, OpenMP, #pragma omp metadirective)", "label": "(3, OpenMP, #pragma omp target map(tofrom: threadCount)); (4, OpenMP, #pragma omp metadirective)"}
{"predict": "(15, MPI, double end = MPI_Wtime();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (7, MPI, double start = MPI_Wtime();); (1, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(5, MPI, MPI_Barrier(MPI_COMM_WORLD);); (5, MPI, double start = MPI_Wtime();); (12, MPI, double end = MPI_Wtime();); (0, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(23, MPI, MPI_Wait(&send_request, &status);); (34, MPI, MPI_Send(&send_value, 1, MPI_INT, (rank + 1) % size, 4711, MPI_COMM_WORLD);); (15, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (7, MPI, MPI_Init(&argc, &argv);); (31, MPI, MPI_Send(&send_value, 1, MPI_INT, (rank + 1) % size, 4711, MPI_COMM_WORLD);); (40, MPI, MPI_Finalize();); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (19, MPI, MPI_Send(&send_value, 1, MPI_INT, (rank + 1) % size, 4711, MPI_COMM_WORLD);); (19, MPI, MPI_Wait(&recv_request, &status);); (1, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (2, MPI, MPI_Send(&send_value, 1, MPI_INT, (rank + 1) % size, 4711, MPI_COMM_WORLD);); (2, MPI, MPI_Irecv(&recv_value, 1, MPI_INT, (rank + size - 1) % size, 4711, MPI_COMM_WORLD, &recv_request);)", "label": "(9, MPI, MPI_Init(&argc, &argv);); (18, MPI, MPI_Finalize();); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (31, MPI, MPI_Wait(&recv_request, &status);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (17, MPI, MPI_Isend(&send_value, 1, MPI_INT, (rank + 1) % size, 4711, MPI_COMM_WORLD, &send_request);); (27, MPI, MPI_Wait(&send_request, &status);)"}
{"predict": "(33, OpenCL, clReleaseEvent(retEvent);)", "label": "(33, OpenCL, clReleaseEvent(retEvent);)"}
{"predict": "(7, CUDA, ensemble = new flamegpu::CUDAEnsemble(model);); (15, CUDA, ensemble = new flamegpu::CUDAEnsemble(model, 2, argv);)", "label": "(16, CUDA, EXPECT_NO_THROW(ensemble = new flamegpu::CUDAEnsemble(model, sizeof(argv) / sizeof(char*), argv));); (6, CUDA, EXPECT_NO_THROW(ensemble = new flamegpu::CUDAEnsemble(model, 0, nullptr));)"}
{"predict": "(5, MPI, MPI_Comm cComm = MPI_Comm_f2c(*comm);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wSize);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wRank);); (5, MPI, MPI_Comm_rank(cComm, &cRank);); (3, MPI, MPI_Comm_size(cComm, &cSize);)", "label": "(2, MPI, MPI_Comm cComm = MPI_Comm_f2c(*comm);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wSize);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wRank);); (5, MPI, MPI_Comm_rank(cComm, &cRank);); (3, MPI, MPI_Comm_size(cComm, &cSize);)"}
{"predict": "(149, CUDA, cudaFree(devPtrA_dev);); (150, CUDA, cudaFree(devPtrB_dev);); (151, CUDA, cudaFree(devPtrC_dev);); (134, CUDA, checkCudaErrors(cudaFreeAsync(A, streamArray[i]));); (136, CUDA, checkCudaErrors(cudaFreeAsync(C, streamArray[i]));); (132, CUDA, checkCudaErrors(cudaFreeAsync(B, streamArray[i]));); (101, CUDA, cudaError_t cudaErr = cudaStreamCreate(&streamArray[i]);); (137, CUDA, cudaFree(devPtrA[i]);); (138, CUDA, cudaFree(devPtrC[i]);); (136, CUDA, cudaFree(devPtrB[i]);); (128, CUDA, checkCudaErrors(cudaFreeAsync(devPtrA[i], streamArray[i]));); (45, CUDA, cudaFree(devPtrC[i]);); (43, CUDA, cudaFree(devPtrB[i]);); (41, CUDA, cudaFree(devPtrA[i]);); (75, CUDA, CLEANUP();); (76, CUDA, fprintf(stderr, \"!!!! cannot copy pointers to device\\n\");); (85, CUDA, fprintf(stderr, \"!!!! system memory allocation error\\n\");); (106, CUDA, checkCudaErrors(cudaStreamSynchronize(streamArray[i]));); (64, CUDA, cudaFree(devPtrB[i]);); (76, CUDA, fprintf(stderr, \"!!!! cudaStreamCreate() failed\\n\");); (90, CUDA, checkCudaErrors(cudaMallocAsync(B, matrixSizeB * sizeof(B[0]), streamArray[i]));); (92, CUDA, checkCudaErrors(cudaMallocAsync(C, matrixSizeC * sizeof(C[0]), streamArray[i]));); (90, CUDA, checkCudaErrors(cudaMallocAsync(A, matrixSizeA * sizeof(A[0]), streamArray[i]));); (114, CUDA, cudaMemcpyAsync(C, devPtrC[i], matrixSizeC * sizeof(T_ELEM), cudaMemcpyDeviceToHost, stream", "label": "(193, CUDA, cudaError_t cudaStatus = cudaDeviceSynchronize();); (103, CUDA, cudaError_t cudaErr = cudaStreamCreate(&streamArray[i]);); (163, CUDA, cudaError_t cudaStatus = cudaGetLastError();)"}
{"predict": "(19, MPI, *ierr = MPI_Info_get_valuelen( (MPI_Info)(*v1), p2, &l4 );)", "label": "(19, MPI, *ierr = MPI_Info_get_valuelen( (MPI_Info)(*v1), p2, v3, &l4 );)"}
{"predict": "(160, MPI, MPI_Type_free(recv_types + i);); (30, MPI, MPI_Alltoall(recv_size, 1, MPI_COUNT, send_size, 1, MPI_COUNT, fd->comm);)", "label": "(65, MPI, MPI_Type_commit(recv_types + j);); (30, MPI, MPI_Alltoall(recv_size, 1, MPI_COUNT, send_size, 1, MPI_COUNT, fd->comm);)"}
{"predict": "(7, CUDA, cudaDeviceSynchronize();)", "label": "(3, CUDA, cudaFree( 0 );)"}
{"predict": "(17, CUDA, CHECK_CUDA(cudaMalloc(&dx, xSize * sizeof(float)));); (50, CUDA, CHECK_CUDA(cudaFreeHost(hIndices));); (50, CUDA, CHECK_CUDA(cudaFreeHost(hIndicesRef));); (48, CUDA, CHECK_CUDA(cudaFreeHost(hx));); (16, CUDA, CHECK_CUDA(cudaMalloc(&dIndices, ySize * sizeof(uint32_t)));); (10, CUDA, CHECK_CUDA(cudaMallocHost(&hx, xSize * sizeof(float)));); (10, CUDA, CHECK_CUDA(cudaMallocHost(&hIndicesRef, ySize * sizeof(uint32_t)));); (41, CUDA, CHECK_CUDA(cudaFree(dIndices));); (43, CUDA, CHECK_CUDA(cudaFreeHost(hyRef));); (41, CUDA, CHECK_CUDA(cudaFree(dx));); (6, CUDA, CHECK_CUDA(cudaMallocHost(&hyRef, ySize * sizeof(float)));)", "label": "(52, CUDA, CHECK_CUDA(cudaFreeHost(hIndices));); (15, CUDA, CHECK_CUDA(cudaMallocHost(&hIndicesRef, ySize * sizeof(uint32_t)));); (19, CUDA, CHECK_CUDA(cudaMalloc(&dx, xSize * sizeof(float)));); (50, CUDA, CHECK_CUDA(cudaFreeHost(hIndicesRef));); (8, CUDA, CHECK_CUDA(cudaMallocHost(&hx, xSize * sizeof(float)));); (-3, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (44, CUDA, CHECK_CUDA(cudaFreeHost(hx));); (42, CUDA, CHECK_CUDA(cudaFree(dIndices));); (40, CUDA, CHECK_CUDA(cudaFree(dx));); (4, CUDA, CHECK_CUDA(cudaMallocHost(&hyRef, ySize * sizeof(float)));); (5, CUDA, CHECK_CUDA(cudaMallocHost(&hIndices, ySize * sizeof(uint32_t)));); (40, CUDA, CHECK_CUDA(cudaFreeHost(hyRef));); (11, CUDA, CHECK_CUDA(cudaMalloc(&dIndices, ySize * sizeof(uint32_t)));)"}
{"predict": "(3, CUDA, cudaExtent extent = make_cudaExtent(4, 5, 6);)", "label": "(4, CUDA, cudaExtent extent = make_cudaExtent(4, 5, 6);)"}
{"predict": "(44, SYCL, sycl::accessor A(d_A, h);); (39, SYCL, sycl::accessor B(d_B, h);); (44, SYCL, sycl::accessor C(d_C, h);); (33, SYCL, sycl::accessor A(d_A, sycl::range<1>(length), sycl::read_only);)", "label": "(39, SYCL, sycl::accessor B(d_B, h, sycl::read_only);); (37, SYCL, sycl::accessor A(d_A, h);); (38, SYCL, sycl::accessor C(d_C, h, sycl::read_only);); (81, SYCL, prk::SYCL::print_exception_details(e);)"}
{"predict": "(19, CUDA, PyCudaMemory_Init(self, args, kwds);)", "label": "(2, CUDA, LogDebug(LOG_PY_UTILS \"PyCudaMemory_New()\\n\");)"}
{"predict": "(3, MPI, dray::dray::mpi_comm(MPI_Comm_c2f(comm));); (99, MPI, MPI_Finalize();); (1, MPI, MPI_Init(NULL,NULL);)", "label": "(3, MPI, MPI_Init(nullptr, nullptr);); (100, MPI, MPI_Finalize();); (3, MPI, dray::dray::mpi_comm(MPI_Comm_c2f(comm));)"}
{"predict": "(9, SYCL, sycl::queue handle = getSYCLDefaultDeviceDefaultQueue();)", "label": "(9, SYCL, sycl::queue handle = getSYCLDefaultDeviceDefaultQueue();)"}
{"predict": "(66, OpenMP, omp_target_free(int_array, omp_get_default_device() );); (66, OpenMP, omp_target_free(dbl_array, omp_get_default_device() );); (28, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:int_array[0:N],dbl_array[0:N]) reduction(min:min[0],max[0],dbl_min[0],dbl_max[0])); (31, OpenMP, #pragma omp atomic compare hint(ompx_fast_fp_atomics)); (50, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:int_array[0:N]) reduction(min:min2,max2)); (29, OpenMP, #pragma omp atomic compare)", "label": "(24, OpenMP, double *dbl_min = (double*) omp_target_alloc(sizeof(double)*1, omp_get_default_device() );); (22, OpenMP, int *max = (int*) omp_target_alloc(sizeof(int)*1, omp_get_default_device() );); (23, OpenMP, double *dbl_max = (double*) omp_target_alloc(sizeof(double)*1, omp_get_default_device() );); (33, OpenMP, #pragma omp atomic compare); (6, OpenMP, #pragma omp target teams distribute parallel for); (38, OpenMP, #pragma omp atomic compare hint(ompx_fast_fp_atomics)); (50, OpenMP, #pragma omp target teams distribute parallel for  map(tofrom:min2,max2))"}
{"predict": "(186, CUDA, result.error = cudaEventRecord(events[0]);); (200, CUDA, std::cerr << \"cudaEventSynchronize() failed: \" << cudaGetErrorString(result.error) << std::endl;); (190, CUDA, std::cerr << \"cudaEventElapsed() failed: \" << cudaGetErrorString(result.error) << std::endl;); (203, CUDA, result.error = cudaEventElapsedTime(&runtime_ms, events[0], events[1]);); (179, CUDA, result.error = cudaEventRecord(events[1]);); (194, CUDA, result.error = cudaEventSynchronize(events[1]);); (163, CUDA, std::cerr << \"cudaEventRecord() failed: \" << cudaGetErrorString(result.error) << std::endl;); (170, CUDA, std::cerr << \"cudaEventRecord() failed: \" << cudaGetErrorString(result.error) << std::endl;); (164, CUDA, result.error = cudaEventRecord(events[2]);); (207, CUDA, cudaEventDestroy(event);); (153, CUDA, result.error = cudaEventCreate(&event);)", "label": "(202, CUDA, result.error = cudaEventRecord(events[0]);); (223, CUDA, std::cerr << \"cudaEventSynchronize() failed: \" << cudaGetErrorString(result.error) << std::endl;); (230, CUDA, std::cerr << \"cudaEventElapsed() failed: \" << cudaGetErrorString(result.error) << std::endl;); (201, CUDA, std::cerr << \"cudaEventRecord() failed: \" << cudaGetErrorString(result.error) << std::endl;); (211, CUDA, result.error = cudaEventRecord(events[1]);); (237, CUDA, (void)cudaEventDestroy(event);); (190, CUDA, std::cerr << \"cudaEventCreate() failed: \" << cudaGetErrorString(result.error) << std::endl;); (223, CUDA, result.error = cudaEventElapsedTime(&runtime_ms, events[0], events[1]);); (214, CUDA, result.error = cudaEventSynchronize(events[1]);); (185, CUDA, result.error = cudaEventCreate(&event);)"}
{"predict": "(18, MPI, MPI_Wait(&send_request, &status);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (8, MPI, MPI_Init(NULL, NULL);); (42, MPI, MPI_Sendrecv_replace(&send_value[9], 1, MPI_INT, (rank + size - 1) % size, 4711, recv_value, 10, MPI_INT, (rank + size - 1) % size, 4711, MPI_COMM_WORLD);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (37, MPI, MPI_Sendrecv_replace(&send_value[9], 1, MPI_INT, (rank + size - 1) % size, 4711, NULL, 0, MPI_INT, (rank + size - 1) % size, 4711, MPI_COMM_WORLD);); (30, MPI, MPI_Wait(&recv_request, &status);); (44, MPI, MPI_Finalize();); (23, MPI, MPI_Send(&send_value[9], 1, MPI_INT, (rank + size - 1) % size, 4711, MPI_COMM_WORLD);); (2, MPI, MPI_Isend(&send_value[i], 1, MPI_INT, (rank + 1) % size, 4711, MPI_COMM_WORLD, &send_request);); (34, MPI, MPI_Sendrecv_replace(&send_value[9], 1, MPI_INT, (rank + size - 1) % size, 4711, recv_value, 10, MPI_INT, (rank + size - 1) % size, 4711, MPI_COMM_WORLD);); (10, MPI, MPI_Irecv(&recv_value, 10, MPI_INT, (rank + size - 1) % size, 4711, MPI_COMM_WORLD, &recv_request);)", "label": "(9, MPI, MPI_Init(&argc, &argv);); (25, MPI, MPI_Finalize();); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (30, MPI, MPI_Isend(&send_value, 9, MPI_INT, (rank + 1) % size, 4711, MPI_COMM_WORLD, &send_request);); (34, MPI, MPI_Wait(&recv_request, &status);); (32, MPI, MPI_Wait(&send_request, &status);)"}
{"predict": "(53, MPI, MPI_Type_get_extent(datatype, &lb, &buftype_extent);); (54, MPI, MPI_Type_size_x(fd->filetype, &filetype_size);); (115, MPI, MPI_Get_address(combine_buf_ptr, &addr);); (49, MPI, MPI_Type_size_x(datatype, &buftype_size);); (53, MPI, MPI_Type_size_x(etype, &etype_size);)", "label": "(44, MPI, MPI_Type_size_x(fd->filetype, &filetype_size);); (52, MPI, MPI_Comm_rank(fd->comm, &rank);); (52, MPI, MPI_Comm_size(fd->comm, &nprocs);); (56, MPI, MPI_Type_get_extent(datatype, &lb, &buftype_extent);); (54, MPI, MPI_Type_size_x(datatype, &buftype_size);)"}
{"predict": "(145, MPI, MPI_Bcast(&size2,      1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (145, MPI, MPI_Bcast(&radius,     1, MPI_INT,           root, MPI_COMM_WORLD);); (322, MPI, MPI_Finalize();); (140, MPI, MPI_Bcast(&lsize2,     1, MPI_INT,           root, MPI_COMM_WORLD);); (138, MPI, MPI_Bcast(&lsize,      1, MPI_INT,           root, MPI_COMM_WORLD);); (139, MPI, MPI_Bcast(&size,       1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (296, MPI, MPI_Reduce(&vector_sum, &check_sum, 1, MPI_DOUBLE, MPI_SUM, root, MPI_COMM_WORLD);); (43, MPI, MPI_Init(&argc,&argv);); (232, MPI, MPI_Barrier(MPI_COMM_WORLD);); (43, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (137, MPI, MPI_Bcast(&iterations, 1, MPI_INT,           root, MPI_COMM_WORLD);); (40, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)", "label": "(146, MPI, MPI_Bcast(&size2,      1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (146, MPI, MPI_Bcast(&radius,     1, MPI_INT,           root, MPI_COMM_WORLD);); (303, MPI, MPI_Finalize();); (141, MPI, MPI_Bcast(&lsize2,     1, MPI_INT,           root, MPI_COMM_WORLD);); (139, MPI, MPI_Bcast(&lsize,      1, MPI_INT,           root, MPI_COMM_WORLD);); (140, MPI, MPI_Bcast(&size,       1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (277, MPI, MPI_Reduce(&vector_sum, &check_sum, 1, MPI_DOUBLE, MPI_SUM, root, MPI_COMM_WORLD);); (44, MPI, MPI_Init(&argc,&argv);); (230, MPI, MPI_Barrier(MPI_COMM_WORLD);); (44, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (138, MPI, MPI_Bcast(&iterations, 1, MPI_INT,           root, MPI_COMM_WORLD);); (41, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(67, OpenMP, #pragma omp parallel); (67, OpenMP, #pragma omp critical)", "label": "(251, OpenMP, #pragma omp parallel for shared(curdat, inc, m_count))"}
{"predict": "(55, OpenMP, #pragma omp target parallel for simd collapse(4,, 4)); (79, OpenMP, #pragma omp target parallel for simd collapse(0)); (5, OpenMP, #pragma omp target parallel for simd collapse(); (40, OpenMP, #pragma omp target parallel for simd collapse(4)); (33, OpenMP, #pragma omp target parallel for simd collapse(4,); (45, OpenMP, #pragma omp target parallel for simd collapse(4 4)); (76, OpenMP, #pragma omp target parallel for simd collapse(5 - 5)); (80, OpenMP, #pragma omp target parallel for simd firstprivate(i) collapse(2)); (15, OpenMP, #pragma omp target parallel for simd collapse(, )); (34, OpenMP, #pragma omp target parallel for simd collapse(4)); (57, OpenMP, #pragma omp target parallel for simd collapse(4, 8)); (5, OpenMP, #pragma omp target parallel for simd collapse(,); (70, OpenMP, #pragma omp target parallel for simd collapse(-5)); (14, OpenMP, #pragma omp target parallel for simd collapse 4)); (44, OpenMP, #pragma omp target parallel for simd collapse(4,, 4)); (20, OpenMP, #pragma omp target parallel for simd collapse(4, )); (4, OpenMP, #pragma omp target parallel for simd collapse(, )); (14, OpenMP, #pragma omp target parallel for simd collapse(4); (61, OpenMP, #pragma omp target parallel for simd collapse(foo())); (64, OpenMP, #pragma omp target parallel for simd collapse(2.5)); (20, OpenMP, #pragma omp target parallel for simd collapse(4,); (2, OpenMP, #pragma omp target parallel for simd collapse 4)); (66, OpenMP, #pragma omp target parallel for simd collapse(3))", "label": "(21, OpenMP, #pragma omp target parallel for simd collapse(, )); (54, OpenMP, #pragma omp target parallel for simd collapse(4,, 4)); (34, OpenMP, #pragma omp target parallel for simd collapse(4,); (79, OpenMP, #pragma omp target parallel for simd collapse(0)); (46, OpenMP, #pragma omp target parallel for simd collapse(4 4)); (85, OpenMP, #pragma omp target parallel for simd collapse(2) firstprivate(i)); (72, OpenMP, #pragma omp target parallel for simd collapse(-5)); (6, OpenMP, #pragma omp target parallel for simd collapse()); (-3, OpenMP, #pragma omp target parallel for simd collapse); (57, OpenMP, #pragma omp target parallel for simd collapse(4, 8)); (7, OpenMP, #pragma omp target parallel for simd collapse(,); (63, OpenMP, #pragma omp target parallel for simd collapse(foo())); (74, OpenMP, #pragma omp target parallel for simd collapse(5 - 5)); (57, OpenMP, #pragma omp target parallel for simd collapse(2.5)); (17, OpenMP, #pragma omp target parallel for simd collapse(4); (11, OpenMP, #pragma omp target parallel for simd collapse 4)); (-7, OpenMP, #pragma omp target parallel for simd collapse(); (24, OpenMP, #pragma omp target parallel for simd collapse(4, )); (27, OpenMP, #pragma omp target parallel for simd collapse(4))"}
{"predict": "(7, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)", "label": "(7, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)"}
{"predict": "(71, CUDA, checkCudaErrors(cufftExecC2C(plan, (cufftComplex *)d_in_signal, (cufftComplex *)d_output_signal));); (136, CUDA, checkCudaErrors(cufftDestroy(plan));); (73, CUDA, checkCudaErrors(cufftExecZ2Z(plan, (cufftDoubleComplex *)d_in_signal, (cufftDoubleComplex *)d_output_signal));); (68, CUDA, cudaDeviceSynchronize();); (13, CUDA, checkCudaErrors(cudaMalloc((void **)&d_in_signal, mem_size));); (18, CUDA, checkCudaErrors(cudaMalloc((void **)&d_output_signal, mem_size));); (12, CUDA, checkCudaErrors(cudaMallocHost((void **)&h_output_signal, mem_size));); (129, CUDA, cudaFree(d_in_signal);); (129, CUDA, cudaFree(d_output_signal);); (9, CUDA, checkCudaErrors(cudaMalloc((void **)&d_in_signal, mem_size));); (125, CUDA, cudaFreeHost(h_output_signal);); (9, CUDA, checkCudaErrors(cudaMallocHost((void **)&h_output_signal, mem_size));); (122, CUDA, cudaFreeHost(h_in_signal);); (3, CUDA, checkCudaErrors(cudaMalloc((void **)&d_output_signal, mem_size));); (6, CUDA, checkCudaErrors(cudaMallocHost((void **)&h_in_signal, mem_size));); (55, CUDA, cudaDeviceSynchronize();); (6, CUDA, checkCudaErrors(cudaMemcpy(d_in_signal, h_in_signal, mem_size, cudaMemcpyHostToDevice));); (124, CUDA, cudaFree(d_output_signal);); (6, CUDA, cudaMalloc((void **)&d_output_signal, mem_size);); (68, CUDA, cudaFreeHost(h_output_signal);)", "label": "(83, CUDA, cudaDeviceSynchronize();); (20, CUDA, checkCudaErrors(cudaMalloc((void **)&d_output_signal, mem_size));); (73, CUDA, checkCudaErrors(cufftExecZ2Z(plan, (cufftDoubleComplex *)d_in_signal, (cufftDoubleComplex *)d_output_signal, direction));); (69, CUDA, checkCudaErrors(cufftExecC2C(plan, (cufftComplex *)d_in_signal, (cufftComplex *)d_output_signal, direction));); (155, CUDA, checkCudaErrors(cufftDestroy(plan));); (9, CUDA, checkCudaErrors(cudaMalloc((void **)&d_in_signal, mem_size));); (163, CUDA, cudaDeviceReset();)"}
{"predict": "(48, CUDA, CHECK(cudaFree(d_y));); (48, CUDA, CHECK(cudaFree(d_z));); (24, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice));); (24, CUDA, CHECK(cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice));); (18, CUDA, CHECK(cudaMalloc((void**)&d_x, M));); (40, CUDA, CHECK(cudaFree(d_x));); (32, CUDA, CHECK(cudaMemcpy(h_z, d_z, M, cudaMemcpyDeviceToHost));); (17, CUDA, CHECK(cudaMalloc((void**)&d_z, M));); (15, CUDA, CHECK(cudaMalloc((void**)&d_y, M));)", "label": "(48, CUDA, CHECK(cudaFree(d_y));); (26, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice));); (47, CUDA, CHECK(cudaFree(d_z));); (35, CUDA, CHECK(cudaMemcpy(h_z, d_z, M, cudaMemcpyDeviceToHost));); (18, CUDA, CHECK(cudaMalloc((void**)&d_x, M));); (18, CUDA, CHECK(cudaMalloc((void**)&d_y, M));); (41, CUDA, CHECK(cudaFree(d_x));); (17, CUDA, CHECK(cudaMalloc((void**)&d_z, M));); (26, CUDA, CHECK(cudaGetLastError());); (26, CUDA, CHECK(cudaDeviceSynchronize());); (18, CUDA, CHECK(cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice));)"}
{"predict": "(4, MPI, MPI_Init(&argc, &argv);); (122, MPI, MPI_Finalize();); (3, MPI, MPI_Get_library_version(version, &versionlen);)", "label": "(5, MPI, MPI_Get_library_version(version, &versionlen);)"}
{"predict": "(6, OpenMP, #pragma omp target parallel for); (3, OpenMP, #pragma omp target)", "label": "(6, OpenMP, #pragma omp for reduction(+:count) lastprivate(obj)); (9, OpenMP, #pragma omp cancel for); (2, OpenMP, #pragma omp parallel private(i) num_threads(1))"}
{"predict": "(33, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(device_buffer, input.data(), size_bytes, cudaMemcpyHostToDevice));); (41, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(output.data(), device_buffer, size_bytes, cudaMemcpyDeviceToHost));); (36, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (27, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&device_buffer, size_bytes));); (46, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(device_buffer));)", "label": "(31, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&device_buffer, size_bytes));); (32, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(device_buffer, input.data(), size_bytes, cudaMemcpyHostToDevice));); (41, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(device_buffer));); (35, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (33, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (37, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(output.data(), device_buffer, size_bytes, cudaMemcpyDeviceToHost));)"}
{"predict": "(107, MPI, MPI_Allgather(st_end_offset, 2, ADIO_OFFSET, all_st_end_offsets, 2, ADIO_OFFSET, fd->comm);); (66, MPI, MPI_Comm_rank(fd->comm, &myrank);); (141, MPI, MPI_Type_get_extent(datatype, &lb, &extent);); (391, MPI, MPI_Type_free(&client_comm_dtype_arr[i]);); (385, MPI, MPI_Type_free(&agg_comm_dtype_arr[i]);); (390, MPI, MPI_Type_free(&agg_dtype);); (60, MPI, MPI_Comm_size(fd->comm, &nprocs);); (386, MPI, MPI_Type_free(&file_realm_types[i]);); (200, MPI, MPI_Type_size_x(datatype, &size);); (382, MPI, MPI_Type_free(&buftype);)", "label": "(101, MPI, MPI_Allgather(st_end_offset, 2, ADIO_OFFSET, all_st_end_offsets, 2, ADIO_OFFSET, fd->comm);); (66, MPI, MPI_Comm_rank(fd->comm, &myrank);); (522, MPI, MPI_Type_free(&client_comm_dtype_arr[i]);); (582, MPI, MPI_Type_free(&fd->file_realm_types[0]);); (62, MPI, MPI_Comm_size(fd->comm, &nprocs);); (584, MPI, MPI_Type_free(&fd->file_realm_types[i]);); (150, MPI, MPI_Type_get_extent(datatype, &lb, &extent);); (302, MPI, MPI_Type_size_x(agg_comm_dtype_arr[i], &size);); (296, MPI, MPI_Type_free(&agg_dtype);)"}
{"predict": "(13, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(13, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(18, MPI, MPI_Win_unlock(0, win);); (18, MPI, MPI_Barrier(MPI_COMM_WORLD);); (13, MPI, MPI_Accumulate(one, 1, get_type, 0, *value, 1, acc_type, MPI_SUM, win);); (15, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, 0, 0, win);); (5, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, 0, 0, win);)", "label": "(12, MPI, MPI_Win_unlock(0, win);); (9, MPI, MPI_Accumulate(one, nlevels, MPI_INT, 0, 0, 1, acc_type, MPI_SUM, win);); (9, MPI, MPI_Get(get_array, nlevels, MPI_INT, 0, 0, 1, get_type, win);); (6, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, 0, 0, win);)"}
{"predict": "(46, OpenMP, #pragma omp target enter data map(to: A[0:length], B[0:length], C[0:length])); (58, OpenMP, #pragma omp target exit data map(delete: A[0:length], B[0:length], C[0:length]))", "label": "(31, OpenMP, std::cout << \"OpenMP default device = \" << omp_get_default_device() << std::endl;)"}
{"predict": "(18, CUDA, cudaFree(res_dev);); (8, CUDA, cudaMalloc(&res_dev, sizeof(float));); (11, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)", "label": "(18, CUDA, cudaFree(res_dev);); (8, CUDA, cudaMalloc(&res_dev, sizeof(float));); (10, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(38, TBB, reduce_time = (tbb::tick_count::now() - t0).seconds();); (28, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (43, TBB, graph_time = (tbb::tick_count::now() - t0).seconds();); (27, TBB, serial_time = (tbb::tick_count::now() - t0).seconds();)", "label": "(29, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (37, TBB, reduce_time = (tbb::tick_count::now() - t0).seconds();); (43, TBB, graph_time = (tbb::tick_count::now() - t0).seconds();); (28, TBB, serial_time = (tbb::tick_count::now() - t0).seconds();)"}
{"predict": "(15, CUDA, thrust::device_ptr<int> dev_ptr(raw_ptr);); (17, CUDA, cudaFree(raw_ptr);); (5, CUDA, cudaMalloc(&raw_ptr, N * sizeof(int));)", "label": "(6, CUDA, cudaMalloc((void**) &raw_ptr, N * sizeof(int));); (11, CUDA, thrust::fill(dev_ptr, dev_ptr + N, (int) 0);); (7, CUDA, thrust::device_ptr<int> dev_ptr = thrust::device_pointer_cast(raw_ptr);); (15, CUDA, cudaFree(raw_ptr);)"}
{"predict": "(30, CUDA, Check_CudaErroorFun(\"Error en LimitsPos()...\");); (19, CUDA, cudaMemcpy(poscellh,poscell+pini,sizeof(float4)*np,cudaMemcpyDeviceToHost);); (19, CUDA, cudaMemcpy(checkh,check+pini,sizeof(byte)*np,cudaMemcpyDeviceToHost);)", "label": "(19, CUDA, cudaMemcpy(poscellh,poscell+pini,sizeof(float4)*np,cudaMemcpyDeviceToHost);); (19, CUDA, cudaMemcpy(checkh,check+pini,sizeof(byte)*np,cudaMemcpyDeviceToHost);)"}
{"predict": "(28, MPI, Mpi.SignalProgress();)", "label": "(29, MPI, Mpi.SignalProgress();)"}
{"predict": "(3, OpenMP, #pragma omp parallel for)", "label": "(3, OpenMP, #pragma omp parallel for default(none) private(idx) shared(count, sapH_cur_salt, saved_plain, crypt_key))"}
{"predict": "(34, MPI, MPI_Finalize();); (7, MPI, rc = MPI_Init(&argc, &argv);); (22, MPI, rc = MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (20, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(23, MPI, rc = MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (32, MPI, MPI_Finalize();); (6, MPI, rc = MPI_Init(NULL, NULL);)"}
{"predict": "(46, CUDA, cerr = cudaSetDevice(cur_device);); (34, CUDA, rc = yaksuri_cudai_md_alloc(type);); (36, CUDA, YAKSURI_CUDAI_CUDA_ERR_CHKANDJUMP(cerr, rc, fn_fail);); (8, CUDA, cerr = cudaSetDevice(target);); (44, CUDA, cerr = cudaFree(0);); (48, CUDA, cerr = cudaSetDevice(cur_device);); (27, CUDA, YAKSURI_CUDAI_CUDA_ERR_CHKANDJUMP(cerr, rc, fn_fail);); (26, CUDA, cerr = cudaMemcpyAsync(iov[i].iov_base, sbuf, iov[i].iov_len, cudaMemcpyDefault, stream);); (30, CUDA, cerr = cudaSetDevice(0);); (42, CUDA, cerr = cudaFree(0);); (48, CUDA, cerr = cudaSetDevice(target);); (5, CUDA, YAKSURI_CUDAI_CUDA_ERR_CHKANDJUMP(cerr, rc, fn_fail);); (29, CUDA, cerr = cudaMemcpyAsync((char *) outbuf + iov[i].iov_off, iov[i].iov_base, iov[i].iov_len, cudaMemcpyDefault, stream);); (21, CUDA, rc = yaksuri_cudai_md_alloc(type);); (3, CUDA, cerr = cudaMemcpyAsync((char *) outbuf, inbuf, count * type->size, cudaMemcpyDefault, stream);); (34, CUDA, cerr = cudaSetDevice(cur_device);); (3, CUDA, cerr = cudaSetDevice(0);); (45, CUDA, cerr = cudaFree(0);); (37, CUDA, cerr = cudaSetDevice(target);); (3, CUDA, cerr = cudaFree(0);); (3, CUDA, cerr = cudaMemcpyAsync((char *) outbuf + type->true_lb, inbuf, count * type->size, cudaMemcpyDefault, stream);); (23, CUDA, cerr = cudaSetDevice(cur_device);); (33, CUDA, cerr = cudaFree(0", "label": "(9, CUDA, uintptr_t iov_unpack_threshold = yaksuri_cudai_get_iov_unpack_threshold(info);); (29, CUDA, cudaMemcpyAsync(iov[i].iov_base, sbuf, iov[i].iov_len, cudaMemcpyDefault, stream);); (16, CUDA, YAKSURI_CUDAI_CUDA_ERR_CHKANDJUMP(cerr, rc, fn_fail);); (52, CUDA, cerr = cudaSetDevice(cur_device);); (42, CUDA, cerr = cudaGetDevice(&cur_device);); (32, CUDA, rc = yaksuri_cudai_md_alloc(type);); (43, CUDA, cerr = cudaSetDevice(target);)"}
{"predict": "(15, OpenMP, #pragma omp target teams); (16, OpenMP, #pragma omp distribute parallel for)", "label": "(15, OpenMP, #pragma omp target parallel for)"}
{"predict": "(4, CUDA, extern __shared__ float shared[];)", "label": "(4, CUDA, extern __shared__ float shared[];)"}
{"predict": "(14, OpenMP, #pragma omp parallel for)", "label": "(13, OpenMP, #pragma omp parallel for)"}
{"predict": "(14, TBB, generate_test<parallel_test, tbb::flow::tuple<float, double, int, float, double, float, double, int, float, double> >::do_test();); (26, TBB, generate_test<parallel_test, tbb::flow::tuple<float, double, int, float, double, float, double, int, float, double, int, float, double> >::do_test();); (2, TBB, generate_test<serial_test, tbb::flow::tuple<float, double> >::do_test();); (21, TBB, generate_test<serial_test, tbb::flow::tuple<float, double, int, float, double, float, double, int, float, double, int, float, double> >::do_test();); (3, TBB, generate_test<serial_test, tbb::flow::tuple<float, double, int, float, double> >::do_test();); (22, TBB, generate_test<serial_test, tbb::flow::tuple<float, double, int, float, double, float, double, int, float, double, int, float, double> >::do_test();); (6, TBB, generate_test<serial_test, tbb::flow::tuple<float, double, int, float, double, float, double> >::do_test();)", "label": "(31, TBB, generate_test<parallel_test, tbb::flow::tuple<float, double, int, double, double, long, int, float, long> >::do_test();); (27, TBB, generate_test<parallel_test, tbb::flow::tuple<float, int, double, float, long, float, long> >::do_test();); (23, TBB, generate_test<parallel_test, tbb::flow::tuple<double, double, int, int, short> >::do_test();); (15, TBB, generate_test<serial_test, tbb::flow::tuple<float, double, int, double, double, float, long, int, float, long> >::do_test();); (3, TBB, generate_test<serial_test, tbb::flow::tuple<float, double> >::do_test();); (4, TBB, generate_test<serial_test, tbb::flow::tuple<float, double, int, long> >::do_test();); (16, TBB, generate_test<parallel_test, tbb::flow::tuple<float, int, long> >::do_test();); (5, TBB, generate_test<serial_test, tbb::flow::tuple<double, double, int, long, int, short> >::do_test();); (7, TBB, generate_test<serial_test, tbb::flow::tuple<float, double, double, double, float, int, float, long> >::do_test();)"}
{"predict": "(26, MPI, MPI_Recv(out_buf + i * elems, elems, MPI_FLOAT, i, 0, comm, MPI_STATUS_IGNORE);); (12, MPI, MPI_Comm_size(comm, &nproc);); (20, MPI, MPI_Isend(in_buf + i * elems, elems, MPI_FLOAT, i, 0, comm, &reqs[nproc + i]);); (18, MPI, MPI_Send(in_buf + i * elems, elems, MPI_FLOAT, i, 0, comm);); (23, MPI, MPI_Waitall(nproc, reqs, MPI_STATUSES_IGNORE);); (8, MPI, MPI_Comm_rank(comm, &rank);); (23, MPI, MPI_Recv(in_buf + i * elems, elems, MPI_FLOAT, i, 0, comm, MPI_STATUS_IGNORE);)", "label": "(25, MPI, MPI_Isend(&out_buf[elems * i], elems, MPI_FLOAT, i, 0, comm, &reqs[i + nproc]);); (20, MPI, MPI_Irecv(&in_buf[elems * i], elems, MPI_FLOAT, i, 0, comm, &reqs[i]);); (11, MPI, MPI_Comm_size(comm, &nproc);); (9, MPI, MPI_Comm_rank(comm, &rank);); (24, MPI, MPI_Waitall(nproc * 2, reqs, MPI_STATUSES_IGNORE);)"}
{"predict": "(23, CUDA, PyErr_SetString(PyExc_Exception, LOG_PY_UTILS \"cudaOverlay() failed to get input/output image pointers (should be cudaImage)\");); (33, CUDA, PYCUDA_ASSERT_NOGIL(cudaOverlay(input->base.ptr, make_float2(x, y), output->base.ptr, output->format, stream));); (25, CUDA, PyErr_SetString(PyExc_Exception, LOG_PY_UTILS \"cudaOverlay() input and output image formats are different\");)", "label": "(22, CUDA, PyErr_SetString(PyExc_Exception, LOG_PY_UTILS \"cudaOverlay() failed to get input/output image pointers (should be cudaImage)\");); (32, CUDA, PYCUDA_ASSERT_NOGIL(cudaOverlay(input->base.ptr, input->width, input->height, output->base.ptr, output->width, output->height, output->format, x, y, stream));); (26, CUDA, PyErr_SetString(PyExc_Exception, LOG_PY_UTILS \"cudaOverlay() input and output image formats are different\");)"}
{"predict": "(3, CUDA, __shared__ float local[threads];)", "label": "(3, CUDA, __shared__ float local[threads];)"}
{"predict": "(33, OpenACC, #pragma acc atomic); (16, OpenACC, #pragma acc loop gang worker vector(8)); (12, OpenACC, #pragma acc parallel); (36, OpenACC, #pragma acc wait); (28, OpenACC, #pragma acc atomic update); (33, OpenACC, #pragma acc wait(1)); (13, OpenACC, #pragma acc loop vector(8))", "label": "(33, OpenMP, #pragma omp atomic); (15, OpenMP, #pragma omp target teams distribute parallel for collapse(2) private(dx, dy, dz, r, ig2))"}
{"predict": "(54, OpenMP, #pragma omp for private(k))", "label": "(19, OpenMP, #pragma omp for private(k)); (37, OpenMP, #pragma omp for private ( k)); (66, OpenMP, #pragma omp for private (k))"}
{"predict": "(7, MPI, t1 = MPI_Wtime();); (7, MPI, t2 = MPI_Wtime();); (32, MPI, tick = MPI_Wtick();)", "label": "(8, MPI, t1 = MPI_Wtime();); (6, MPI, MPI_Init(&argc, &argv);); (38, MPI, MPI_Finalize();); (31, MPI, tick = MPI_Wtick();); (5, MPI, t2 = MPI_Wtime();)"}
{"predict": "(45, OpenCL, err = clGetDeviceInfo(device, CL_DEVICE_EXTENSIONS, len, extensions, NULL);); (28, OpenCL, err = clGetDeviceInfo(device, CL_DEVICE_EXTENSIONS, 0, NULL, &len);); (55, OpenCL, err = clWaitForEvents(1, NULL);)", "label": "(29, OpenCL, err = clGetDeviceInfo(device, CL_DEVICE_EXTENSIONS, 0, NULL, &len);); (42, OpenCL, err = clGetDeviceInfo(device, CL_DEVICE_EXTENSIONS, len, extensions, NULL);)"}
{"predict": "(3, MPI, MPI_Finalize();); (135, MPI, MPI_Send(x.data(), chunk, MPI_DOUBLE, 0, 42, comm);); (-1, MPI, MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided);); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (1, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(2, MPI, MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided);); (136, MPI, MPI_Send(x.data(), chunk, MPI_DOUBLE, 0, 42, comm);); (2, MPI, MPI_Finalize();); (65, MPI, MPI_Allgather(&chunk, 1, MPI_INT, &domain[1], 1, MPI_INT, comm);)"}
{"predict": "(62, MPI, MPI_Comm_rank(lo_comm, &rank);); (23, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &world_comm);); (183, MPI, MPI_Comm_free(&dup_comm);); (44, MPI, MPI_Group_range_incl(world_group, 1, ranges, &lo_group);); (183, MPI, MPI_Group_free(&world_group);); (24, MPI, MPI_Comm_rank(dup_comm_world, &world_rank);); (38, MPI, MPI_Group_free(&lo_group);); (177, MPI, MPI_Comm_free(&split_comm);); (177, MPI, MPI_Comm_free(&lo_comm);); (45, MPI, MPI_Comm_create(world_comm, lo_group, &lo_comm);); (178, MPI, MPI_Group_free(&rev_group);); (27, MPI, MPI_Group_incl(world_group, 1, &world_rank, &rev_group);); (27, MPI, MPI_Comm_create(dup_comm_world, rev_group, &rev_comm);); (23, MPI, MPI_Group_free(&world_group);); (170, MPI, MPI_Barrier(MPI_COMM_WORLD);); (174, MPI, MPI_Comm_free(&world_comm);); (30, MPI, MPI_Group_range_incl(world_group, 1, ranges, &rev_group);); (10, MPI, MPI_Comm_size(dup_comm_world, &world_size);); (53, MPI, MPI_Group_free(&rev_group);); (53, MPI, MPI_Comm_rank(rev_comm, &rank);); (35, MPI, MPI_Group_incl(world_group, 1, &world_rank, &lo_group);); (44, MPI, MPI_Comm_create(dup_comm_world, lo_group, &split_comm);); (164, MPI, MPI_Finalize();); (20, MPI, MPI_Comm_rank", "label": "(172, MPI, MPI_Comm_split(dup_comm_world, color, key, &split_comm);); (120, MPI, MPI_Attr_put(lo_comm, key_1, (void *) (MPI_Aint) world_rank);); (111, MPI, MPI_Keyval_create(copy_fn, delete_fn, &key_1, &value);); (142, MPI, MPI_Abort(MPI_COMM_WORLD, 3005);); (118, MPI, MPI_Attr_put(lo_comm, key_3, (void *) 0);); (238, MPI, MPI_Comm_free(&split_comm);); (210, MPI, MPI_Comm_compare(world_comm, rev_comm, &result);); (28, MPI, MPI_Comm_create(dup_comm_world, world_group, &world_comm);); (28, MPI, MPI_Comm_rank(world_comm, &rank);); (2, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (114, MPI, MPI_Comm_dup(lo_comm, &dup_comm);); (146, MPI, MPI_Keyval_free(&key_3);); (230, MPI, MPI_Comm_free(&rev_comm);); (135, MPI, MPI_Attr_get(dup_comm, key_3, (void **) &vvalue, &flag);); (210, MPI, MPI_Comm_compare(world_comm, lo_comm, &result);); (190, MPI, MPI_Abort(MPI_COMM_WORLD, 3011);); (71, MPI, MPI_Abort(MPI_COMM_WORLD, 3003);); (228, MPI, MPI_Group_free(&world_group);); (155, MPI, MPI_Comm_size(split_comm, &size);); (-7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &world_size);); (194, MPI, MPI_Comm_create(world_comm, rev_group, &rev_comm);); (135, MPI, MPI_Keyval_free(&key_1);); (157, MPI, MPI_Abort(MPI_COMM_WORLD, 3009);); (190, MPI, MPI_Group_range_incl(world_group, 1, ranges, &rev_group);); (92, MPI, MPI_Keyval_create(MPI_NULL_COPY_FN, MPI_NULL_DELETE_FN, &key_3, &value);); (129, MPI, MPI_Abort(MPI_COMM_WORLD, 3008);); (148, MPI, MPI_Comm_rank(split_comm, &rank);); (201, MPI, MPI_Abort(MPI_COMM_WORLD, 3013);); (173, MPI, MPI_Comm_compare(lo_comm, dup_comm, &result);); (47, MPI, MPI_Abort(MPI_COMM_WORLD, 3002);); (152, MPI, MPI_Barrier(split_comm);); (40, MPI, MPI_Comm_rank(lo_comm, &rank);); (106, MPI, MPI_Abort(MPI_COMM_WORLD, 3004);); (213, MPI, MPI_Group_free(&rev_group);); (163, MPI, MPI_Abort(MPI_COMM_WORLD, 3010);); (205, MPI, MPI_Comm_free(&dup_comm_world);); (4, MPI, MPI_Abort(MPI_COMM_WORLD, 3001);); (213, MPI, MPI_Comm_free(&dup_comm);); (41, MPI, MPI_Barrier(lo_comm);); (210, MPI, MPI_Comm_free(&lo_comm);); (91, MPI, MPI_Attr_get(dup_comm, key_1, (void **) &vvalue, &flag);); (179, MPI, MPI_Abort(MPI_COMM_WORLD, 3012);)"}
{"predict": "(103, CUDA, extern __shared__ char smem_[];); (143, CUDA, __shared__ __align__(sizeof(float2)) char smem_[];)", "label": "(16, CUDA, extern __shared__ __align__(sizeof(float2)) char smem_[];)"}
{"predict": "(167, OpenMP, double stop = omp_get_wtime();); (33, CUDA, CUDA_RT_CALL(cudaMallocHost(&a_ref_h, nx * ny * sizeof(float)));); (124, CUDA, CUDA_RT_CALL(cudaDeviceSynchronize());); (26, CUDA, CUDA_RT_CALL(cudaFree(0));); (24, CUDA, CUDA_RT_CALL(cudaSetDevice(dev_id));); (180, CUDA, CUDA_RT_CALL(cudaFreeHost(a_h));); (182, CUDA, CUDA_RT_CALL(cudaFreeHost(l2_norm_h[dev_id]));); (30, CUDA, CUDA_RT_CALL(cudaMallocHost(&a_h, nx * ny * sizeof(float)));); (72, CUDA, CUDA_RT_CALL(cudaMemset(a[dev_id], 0, nx * (chunk_size[dev_id] + 2) * sizeof(float)));); (164, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_bottom_stream[dev_id]));); (71, CUDA, CUDA_RT_CALL(cudaMalloc(l2_norm_d + dev_id, sizeof(float)));); (65, CUDA, CUDA_RT_CALL(cudaMalloc(a + dev_id, nx * (chunk_size[dev_id] + 2) * sizeof(float)));); (65, CUDA, CUDA_RT_CALL(cudaMalloc(a_new + dev_id, nx * (chunk_size[dev_id] + 2) * sizeof(float)));); (158, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_top_stream[dev_id]));); (175, CUDA, CUDA_RT_CALL(cudaFreeHost(a_ref_h));); (157, CUDA, CUDA_RT_CALL(cudaStreamDestroy(compute_stream[dev_id]));); (166, CUDA, CUDA_RT_CALL(cudaFree(l2_norm_d[dev_id]));); (20, CUDA, CUDA_RT_CALL(cudaMallocHost(&l2_norm_h[dev_id], sizeof", "label": "(207, OpenMP, double stop = omp_get_wtime();); (146, CUDA, CUDA_RT_CALL(cudaStreamSynchronize(push_top_stream[bottom]));); (77, CUDA, CUDA_RT_CALL(cudaDeviceSynchronize());); (28, CUDA, CUDA_RT_CALL(cudaFree(0));); (88, CUDA, CUDA_RT_CALL(cudaDeviceCanAccessPeer(&canAccessPeer, dev_id, top));); (250, CUDA, CUDA_RT_CALL(cudaFreeHost(a_h));); (29, CUDA, CUDA_RT_CALL(cudaMallocHost(&a_ref_h, nx * ny * sizeof(float)));); (71, CUDA, CUDA_RT_CALL(cudaGetLastError());); (91, CUDA, CUDA_RT_CALL(cudaDeviceCanAccessPeer(&canAccessPeer, dev_id, bottom));); (92, CUDA, CUDA_RT_CALL(cudaDeviceEnablePeerAccess(bottom, 0));); (235, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_bottom_stream[dev_id]));); (83, CUDA, CUDA_RT_CALL(cudaDeviceEnablePeerAccess(top, 0));); (74, CUDA, CUDA_RT_CALL(cudaMalloc(l2_norm_d + dev_id, sizeof(float)));); (44, CUDA, CUDA_RT_CALL(cudaMalloc(a + dev_id, nx * (chunk_size[dev_id] + 2) * sizeof(float)));); (157, CUDA, CUDA_RT_CALL(cudaStreamSynchronize(compute_stream[dev_id]));); (45, CUDA, CUDA_RT_CALL(cudaMemset(a[dev_id], 0, nx * (chunk_size[dev_id] + 2) * sizeof(float)));); (132, CUDA, CUDA_RT_CALL(cudaStreamSynchronize(push_bottom_stream[top]));); (9, CUDA, CUDA_RT_CALL(cudaGetDeviceCount(&num_devices));); (228, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_top_stream[dev_id]));); (11, CUDA, CUDA_RT_CALL(cudaSetDevice(dev_id));); (38, CUDA, CUDA_RT_CALL(cudaMalloc(a_new + dev_id, nx * (chunk_size[dev_id] + 2) * sizeof(float)));); (228, CUDA, CUDA_RT_CALL(cudaFreeHost(l2_norm_h[dev_id]));); (230, CUDA, CUDA_RT_CALL(cudaFree(a_new[dev_id]));); (59, CUDA, CUDA_RT_CALL(cudaStreamCreate(compute_stream + dev_id));); (60, CUDA, CUDA_RT_CALL(cudaStreamCreate(push_bottom_stream + dev_id));); (36, CUDA, CUDA_RT_CALL(cudaMemset(a_new[dev_id], 0, nx * (chunk_size[dev_id] + 2) * sizeof(float)));); (230, CUDA, CUDA_RT_CALL(cudaFreeHost(a_ref_h));); (9, CUDA, CUDA_RT_CALL(cudaMallocHost(&a_h, nx * ny * sizeof(float)));); (219, CUDA, CUDA_RT_CALL(cudaStreamDestroy(compute_stream[dev_id]));); (102, OpenMP, double start = omp_get_wtime();); (57, CUDA, CUDA_RT_CALL(cudaMallocHost(l2_norm_h + dev_id, sizeof(float)));); (222, CUDA, CUDA_RT_CALL(cudaFree(a[dev_id]));); (218, CUDA, CUDA_RT_CALL(cudaFree(l2_norm_d[dev_id]));); (50, CUDA, CUDA_RT_CALL(cudaStreamCreate(push_top_stream + dev_id));)"}
{"predict": "(171, CUDA, cudaMemcpy(z_cu, z, shape.Z_len * sizeof(DT), cudaMemcpyHostToDevice);); (211, CUDA, cudaFree(x_cu);); (162, CUDA, cudaMalloc(&x_cu, shape.X_len * sizeof(ST));); (165, CUDA, cudaMemcpy(y_cu, y, shape.Y_len * sizeof(ST), cudaMemcpyHostToDevice);); (163, CUDA, cudaMemcpy(x_cu, x, shape.X_len * sizeof(ST), cudaMemcpyHostToDevice);); (160, CUDA, cudaMalloc(&z_cu, shape.Z_len * sizeof(DT));); (158, CUDA, cudaMalloc(&y_cu, shape.Y_len * sizeof(ST));); (168, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (208, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (202, CUDA, cudaFree(y_cu);); (165, CUDA, CHECK_HIEDNN(hiednnCudaBinaryElementwise(handle, shape.X_len, shape.Y_len, shape.Z_len, OP, EXT_VAL));); (199, CUDA, cudaFree(z_cu);); (160, CUDA, CHECK_HIEDNN(hiednnSetCudaStream(handle, 0));); (192, CUDA, cudaFree(x_cu);); (153, CUDA, CHECK_HIEDNN(hiednnSetCudaDevice(0));)", "label": "(129, CUDA, cudaMemcpy(z_cu, z, shape.Z_len * sizeof(DT), cudaMemcpyHostToDevice);); (221, CUDA, cudaFree(x_cu);); (122, CUDA, cudaMalloc(&x_cu, shape.X_len * sizeof(ST));); (125, CUDA, cudaMemcpy(y_cu, y, shape.Y_len * sizeof(ST), cudaMemcpyHostToDevice);); (123, CUDA, cudaMemcpy(x_cu, x, shape.X_len * sizeof(ST), cudaMemcpyHostToDevice);); (218, CUDA, cudaFree(y_cu);); (120, CUDA, cudaMalloc(&z_cu, shape.Z_len * sizeof(DT));); (118, CUDA, cudaMalloc(&y_cu, shape.Y_len * sizeof(ST));); (128, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (220, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (214, CUDA, cudaFree(z_cu);)"}
{"predict": "(23, CUDA, __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];); (23, CUDA, __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];)", "label": "(22, CUDA, __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];); (20, CUDA, __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];)"}
{"predict": "Cuda(cudaStreamSynchronize(FunCallStream));(93, CUDA, checkCuda(cudaMalloc(&A_gpu->scubufs[streamId].usub_IndirectJ3, ncols * sizeof(int_t)));); (141, CUDA, checkCuda(cudaFree(A_gpu->scubufs[streamId].bigU));)", "label": "(93, CUDA, cudaEventRecord(A_gpu->ePCIeH2D[k0], FunCallStream);); (231, CUDA, cudaEventRecord(A_gpu->GemmStart[k0], FunCallStream);); (239, CUDA, cudaStreamSynchronize(FunCallStream);)"}
{"predict": "(4, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:sum)); (35, OpenMP, #pragma omp atomic  hint(AMD_unsafe_fp_atomics)); (20, OpenMP, #pragma omp atomic  hint(AMD_fast_fp_atomics)); (15, OpenMP, #pragma omp atomic  hint(AMD_fast_fp_atomics)); (3, OpenMP, #pragma omp atomic  hint(AMD_safe_fp_atomics))", "label": "(6, OpenMP, #pragma omp atomic  hint(AMD_safe_fp_atomics)); (34, OpenMP, #pragma omp atomic  hint(AMD_unsafe_fp_atomics)); (2, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:sum)); (17, OpenMP, #pragma omp atomic  hint(AMD_fast_fp_atomics))"}
{"predict": "(4, CUDA, extern __shared__ float entire_buffer[];)", "label": "(6, CUDA, extern __shared__ float entire_buffer[];)"}
{"predict": "(17, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice));); (38, CUDA, CHECK(cudaFree(d_y));); (38, CUDA, CHECK(cudaFree(d_z));); (10, CUDA, CHECK(cudaMalloc(&d_y, M));); (10, CUDA, CHECK(cudaMalloc(&d_z, M));); (33, CUDA, CHECK(cudaFree(d_x));); (7, CUDA, CHECK(cudaMalloc(&d_x, M));); (12, CUDA, CHECK(cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice));)", "label": "(37, CUDA, CHECK(cudaFree(d_y));); (14, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice));); (36, CUDA, CHECK(cudaFree(d_z));); (10, CUDA, CHECK(cudaMalloc(&d_y, M));); (8, CUDA, CHECK(cudaMalloc(&d_x, M));); (31, CUDA, CHECK(cudaFree(d_x));); (8, CUDA, CHECK(cudaMalloc(&d_z, M));); (9, CUDA, CHECK(cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice));)"}
{"predict": "(37, CUDA, cudaFree(sm_o);); (10, CUDA, cudaDeviceSynchronize();); (4, CUDA, cudaMalloc((void **)&sm_o, sizeof(float) * m * m);); (7, CUDA, DEBUG_BLOCK(cudaGetLastError());)", "label": "(10, CUDA, cudaDeviceSynchronize();); (5, CUDA, cudaMalloc((void **)&sm_o, sizeof(float) * m * m);); (35, CUDA, cudaFree(sm_o);)"}
{"predict": "(11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (72, MPI, MPI_Free_mem(orig_buf);); (43, MPI, MPI_Put(orig_buf, 1, vector_dtp, 1, 0, 1, vector_dtp, win);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (40, MPI, MPI_Win_lock(MPI_LOCK_SHARED, 1, 0, win);); (67, MPI, MPI_Free_mem(tar_buf);); (64, MPI, MPI_Win_free(&win);); (43, MPI, MPI_Win_unlock(1, win);); (40, MPI, MPI_Barrier(MPI_COMM_WORLD);); (6, MPI, MPI_Alloc_mem(sizeof(long_double_int_t) * DATA_SIZE, MPI_INFO_NULL, &orig_buf);); (26, MPI, MPI_Win_create(NULL, 0, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &win);)", "label": "(49, MPI, MPI_Win_unlock(1, win);); (32, MPI, MPI_Win_unlock(rank, win);); (45, MPI, MPI_Win_lock(MPI_LOCK_SHARED, 1, 0, win);); (68, MPI, MPI_Type_free(&vector_dtp);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (64, MPI, MPI_Win_free(&win);); (41, MPI, MPI_Accumulate(orig_buf, 1, vector_dtp, 1, 0, 1, vector_dtp, MPI_MAXLOC, win);); (14, MPI, MPI_Type_commit(&vector_dtp);); (64, MPI, MPI_Free_mem(orig_buf);); (65, MPI, MPI_Free_mem(tar_buf);); (17, MPI, MPI_Win_lock(MPI_LOCK_SHARED, rank, 0, win);); (22, MPI, MPI_Barrier(MPI_COMM_WORLD);); (1, MPI, MPI_Alloc_mem(sizeof(long_double_int_t) * DATA_SIZE, MPI_INFO_NULL, &orig_buf);)"}
{"predict": "(2, CUDA, __shared__ int hist[256];)", "label": "(2, CUDA, __shared__ int hist[256];)"}
{"predict": "(68, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (50, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(srcDev->basePtr(), srcVec.data(), srcVec.size(), cudaMemcpyHostToDevice));); (64, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (69, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(test.data(), dstDev->basePtr(), test.size(), cudaMemcpyDeviceToHost));); (65, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamDestroy(stream));)", "label": "(74, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(test.data(), dstDev->basePtr(), test.size(), cudaMemcpyDeviceToHost));); (66, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (50, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(srcDev->basePtr(), srcVec.data(), srcVec.size(), cudaMemcpyHostToDevice));); (60, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (64, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamDestroy(stream));)"}
{"predict": "(17, OpenCL, clCreateCommandQueueWithProperties_StubWithCallback(clCreateCommandQueueWithProperties_testCommandQueueFromSpecifiedContext);); (39, OpenCL, clReleaseContext_ExpectAndReturn(expected_context, CL_SUCCESS);); (28, OpenCL, clReleasePlatform_ExpectAndReturn(make_platform(0), CL_SUCCESS);); (38, OpenCL, clReleaseDevice_ExpectAndReturn(expected_device, CL_SUCCESS);); (12, OpenCL, clReleaseCommandQueue_ExpectAndReturn(expected, CL_SUCCESS);)", "label": "(18, OpenCL, clCreateCommandQueueWithProperties_StubWithCallback(clCreateCommandQueueWithProperties_testCommandQueueFromSpecifiedContext);)"}
{"predict": "(25, OpenCL, clReleaseEvent(outEvent);)", "label": "(25, OpenCL, clReleaseEvent(outEvent);)"}
{"predict": "(142, OpenCL, error = clSetKernelArg(kernel, 0, sizeof(tinfo->inBuf), &tinfo->inBuf);)", "label": "(142, OpenCL, error = clSetKernelArg(kernel, 1, sizeof(tinfo->inBuf), &tinfo->inBuf);)"}
{"predict": "(33, CUDA, cudaGridDependencySynchronize();); (24, CUDA, __shared__ int32_t smemExpertCount[NumThreadsPerCluster];); (20, CUDA, __shared__ int32_t smemTokenCount[NumThreadsPerCluster];); (20, CUDA, __shared__ int32_t smemExpertSum[NumThreadsPerCluster];)", "label": "(10, CUDA, __shared__ typename Scan::TempStorage tempStorage;); (33, CUDA, cudaGridDependencySynchronize();); (4, CUDA, __shared__ int32_t __attribute((aligned(128))) smemExpertCount[NumThreads];); (4, CUDA, __shared__ int32_t __attribute((aligned(128))) smemExpertOffset[NumThreads];)"}
{"predict": "(142, MPI, MPI_Type_free(&stride_dt);); (138, MPI, MPI_Type_free(&trans_dt);); (1, MPI, MPI_Init_thread(&argc, &argv, requested, &provided);); (106, MPI, MPI_Type_commit(&trans_dt);); (183, MPI, t1 = MPI_Wtime();); (178, MPI, MPI_Allreduce(MPI_IN_PLACE, &t0, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (103, MPI, MPI_Type_hvector(block_order, 1, dsize, stride_dt, &trans_dt);); (100, MPI, MPI_Type_vector(block_order, 1, tile_size, MPI_DOUBLE, &stride_dt);); (13, MPI, MPI_Finalize();); (2, MPI, MPI_Comm_size(MPI_COMM_WORLD, &np);); (133, MPI, MPI_Type_free(&stride_dt);); (103, MPI, t0 = MPI_Wtime();); (87, MPI, MPI_Type_size(MPI_DOUBLE, &dsize);); (92, MPI, MPI_Barrier(MPI_COMM_WORLD);); (94, MPI, MPI_Type_commit(&stride_dt);); (3, MPI, if (provided!= requested) MPI_Abort(MPI_COMM_WORLD,provided);)", "label": "(9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &np);); (142, MPI, t1 = MPI_Wtime();); (20, MPI, MPI_Finalize();); (98, MPI, MPI_Type_commit(&trans_dt);); (144, MPI, MPI_Type_free(&stride_dt);); (159, MPI, MPI_Allreduce(MPI_IN_PLACE, &abserr, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);); (2, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (90, MPI, MPI_Type_size(MPI_DOUBLE,&dsize);); (141, MPI, MPI_Type_free(&trans_dt);); (70, MPI, MPI_Abort(MPI_COMM_WORLD,99);); (-5, MPI, if (provided < requested) MPI_Abort(MPI_COMM_WORLD,provided);); (89, MPI, MPI_Type_hvector(block_order, 1, dsize, stride_dt, &trans_dt);); (83, MPI, MPI_Type_vector(block_order, 1, block_order, MPI_DOUBLE, &stride_dt);); (54, MPI, MPI_Barrier(MPI_COMM_WORLD);); (96, MPI, t0 = MPI_Wtime();); (-11, MPI, MPI_Init_thread(&argc, &argv, requested, &provided);)"}
{"predict": "(55, CUDA, cudaMalloc((void **)&result_data, axis_size * sizeof(float));); (52, CUDA, cudaMalloc((void **)&result_data, axis_size * sizeof(float));)", "label": "(55, CUDA, cudaMalloc((void**)&result_data, axis_size * sizeof(float));); (52, CUDA, cudaMalloc((void**)&result_data, tensor->size * sizeof(float));)"}
{"predict": "(8, OpenCL, cl_status = clGetPlatformInfo(id, param_name, param_size, some_info, NULL);)", "label": "(8, OpenCL, cl_status = clGetPlatformInfo(id, param_name, param_size, some_info, NULL);)"}
{"predict": "(31, MPI, mpi_errno = MPI_Comm_create(MPI_COMM_WORLD, comm_group, &comm_hdls[i]);); (10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (27, MPI, MPI_Group_free(&comm_group);); (24, MPI, MPI_Barrier(MPI_COMM_WORLD);); (7, MPI, MPI_Comm_group(MPI_COMM_WORLD, &world_group);); (44, MPI, MPI_Group_free(&world_group);)", "label": "(37, MPI, MPI_Group_free(&comm_group);); (29, MPI, mpi_errno = MPI_Comm_create(MPI_COMM_WORLD, comm_group, &comm_hdls[i]);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (8, MPI, MPI_Comm_group(MPI_COMM_WORLD, &world_group);); (45, MPI, MPI_Group_free(&world_group);); (8, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(6, MPI, MPI_Comm_size(comm, &size);); (6, MPI, MPI_Comm_rank(comm, &rank);)", "label": "(6, MPI, MPI_Comm_size(comm, &size);); (4, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(15, CUDA, __shared__ float rr[TILE_T][N], kk[TILE_T][N];)", "label": "(15, CUDA, __shared__ float rr[TILE_T][N], kk[TILE_T][N];)"}
{"predict": "(49, MPI, MPI_T_category_get_pvars(i, numPvars, pvarIndex);); (28, MPI, MPI_T_category_get_cvars(i, numCvars, cvarIndex);); (4, MPI, MPI_T_category_get_num(&numCat);); (72, MPI, MPI_T_category_get_categories(i, numSubcat, subcatIndex);)", "label": "(51, MPI, MPI_T_category_get_pvars(i, numPvars, pvarIndex);); (28, MPI, MPI_T_category_get_cvars(i, numCvars, cvarIndex);); (3, MPI, MPI_T_category_get_num(&numCat);); (72, MPI, MPI_T_category_get_categories(i, numSubcat, subcatIndex);)"}
{"predict": "(13, CUDA, CUDASimulation cudaSimulation(model);)", "label": "(13, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(34, MPI, MPI_Bcast( nzval,   nnz, MPI_FLOAT, 0, grid->comm );); (34, MPI, MPI_Bcast( rowind,  nnz, mpi_int_t,  0, grid->comm );); (42, MPI, MPI_Bcast( colptr,  n+1, mpi_int_t,  0, grid->comm );); (33, MPI, MPI_Bcast( &nnz,   1,   mpi_int_t,  0, grid->comm );); (26, MPI, MPI_Bcast( &m,   1,   mpi_int_t,  0, grid->comm );); (34, MPI, MPI_Bcast( &m,     1,   mpi_int_t,  0, grid->comm );); (25, MPI, MPI_Bcast( &nnz, 1,   mpi_int_t,  0, grid->comm );); (23, MPI, MPI_Bcast( nzval,  nnz, MPI_FLOAT, 0, grid->comm );); (23, MPI, MPI_Bcast( rowind, nnz, mpi_int_t,  0, grid->comm );); (24, MPI, MPI_Bcast( &n,     1,   mpi_int_t,  0, grid->comm );); (32, MPI, MPI_Bcast( rowptr, m_loc+1, mpi_int_t,  0, grid->comm );); (25, MPI, MPI_Bcast( colptr, n+1, mpi_int_t,  0, grid->comm );)", "label": "(33, MPI, MPI_Bcast( nzval,  nnz, MPI_FLOAT, 0, grid->comm );); (33, MPI, MPI_Bcast( rowind, nnz, mpi_int_t,  0, grid->comm );); (45, MPI, MPI_Bcast( colptr,  n+1, mpi_int_t,  0, grid->comm );); (43, MPI, MPI_Bcast( rowind,  nnz, mpi_int_t,  0, grid->comm );); (34, MPI, MPI_Bcast( &m,   1,   mpi_int_t,  0, grid->comm );); (26, MPI, MPI_Bcast( &n,     1,   mpi_int_t,  0, grid->comm );); (24, MPI, MPI_Bcast( &m,     1,   mpi_int_t,  0, grid->comm );); (33, MPI, MPI_Bcast( &nnz, 1,   mpi_int_t,  0, grid->comm );); (27, MPI, MPI_Bcast( colptr, n+1, mpi_int_t,  0, grid->comm );); (23, MPI, MPI_Bcast( &nnz,   1,   mpi_int_t,  0, grid->comm );); (29, MPI, MPI_Bcast( &n,   1,   mpi_int_t,  0, grid->comm );); (34, MPI, MPI_Bcast( nzval,   nnz, MPI_FLOAT, 0, grid->comm );)"}
{"predict": "(11, TBB, bool ok = tbb::finalize(schBlock1, std::nothrow);); (3, TBB, tbb::task_scheduler_handle schBlock1;); (0, TBB, utils::NativeParallelFor(1, RunInNativeThread(/*blocking=*/false));)", "label": "(15, TBB, bool ok = tbb::finalize(schBlock, std::nothrow);); (11, TBB, bool ok = tbb::finalize(schBlock1, std::nothrow);); (4, TBB, tbb::task_scheduler_handle schBlock1;)"}
{"predict": "(9, OpenMP, dtcmp_op_destroy(c);)", "label": "(9, MPI, MPI_Type_free(&(c->key));)"}
{"predict": "(5, OpenMP, #pragma omp dispatch)", "label": "(2, OpenMP, #pragma omp dispatch)"}
{"predict": "(16, OpenCL, status = clSetKernelArg(global_check, 0, sizeof(cl_mem), &buffer);)", "label": "(16, OpenCL, status = clSetKernelArg(global_check, 0, sizeof(cl_mem), &buffer);)"}
{"predict": "(75, OpenMP, #pragma omp master taskloop simd reduction(&& : S2::S2s)); (107, OpenMP, #pragma omp master taskloop simd reduction(+ : r)); (70, OpenMP, #pragma omp master taskloop simd reduction(& : e, g)); (130, OpenMP, #pragma omp parallel reduction(* : fl)); (34, OpenMP, #pragma omp master taskloop simd reduction(foo : argc); (13, OpenMP, #pragma omp master taskloop simd reduction(); (42, OpenMP, #pragma omp master taskloop simd reduction(- : a, b, c, d, f)); (83, OpenMP, #pragma omp master taskloop simd reduction(+ : o)); (96, OpenMP, #pragma omp parallel shared(i)); (89, OpenMP, #pragma omp master taskloop simd private(i), reduction(+ : j), reduction(+ : q)); (79, OpenMP, #pragma omp master taskloop simd reduction(+ : p), reduction(+ : p)); (120, OpenMP, #pragma omp master taskloop simd reduction(+ : fl)); (11, OpenMP, #pragma omp master taskloop simd reduction(\\)); (69, OpenMP, #pragma omp master taskloop simd reduction(&& : S2::S2sc)); (105, OpenMP, #pragma omp parallel private(fl)); (2, OpenMP, #pragma omp master taskloop simd reduction); (4, OpenMP, #pragma omp master taskloop simd reduction()); (123, OpenMP, #pragma omp master taskloop simd reduction(+ : m)); (37, OpenMP, #pragma omp master taskloop simd reduction(* : ca)); (79, OpenMP, #pragma omp parallel reduction(min : i)); (68, OpenMP, #pragma omp master taskloop simd reduction(+ : h, k, B::x)); (3, OpenMP, #pragma omp master taskloop simd reduction(*)); (43, OpenMP, #pragma omp master taskloop simd reduction(max : h.b)); (108, OpenMP, #pragma omp master taskloop simd reduction(+ : fl)); (70, OpenMP, #pragma omp master taskloop", "label": "(45, OpenMP, #pragma omp master taskloop simd reduction(&& : argc, z)); (101, OpenMP, #pragma omp master taskloop simd reduction(max : j)); (67, OpenMP, #pragma omp master taskloop simd reduction(^ : fl)); (111, OpenMP, #pragma omp master taskloop simd reduction(+ : m)); (20, OpenMP, #pragma omp master taskloop simd reduction()); (46, OpenMP, #pragma omp master taskloop simd reduction(+ : a, b, c, d, f)); (48, OpenMP, #pragma omp master taskloop simd reduction(min : a, b, c, d, f)); (53, OpenMP, #pragma omp master taskloop simd reduction(+ : ba)); (34, OpenMP, #pragma omp master taskloop simd reduction(~ : argc)); (100, OpenMP, #pragma omp parallel reduction(* : fl)); (17, OpenMP, #pragma omp master taskloop simd reduction(*)); (1, OpenMP, #pragma omp master taskloop simd reduction); (63, OpenMP, #pragma omp master taskloop simd reduction(&& : S2::S2sc)); (68, OpenMP, #pragma omp master taskloop simd reduction(+ : h, k, B::x)); (106, OpenMP, #pragma omp master taskloop simd reduction(task, + : m)); (75, OpenMP, #pragma omp parallel private(k)); (62, OpenMP, #pragma omp master taskloop simd reduction(& : e, g)); (67, OpenMP, #pragma omp master taskloop simd reduction(+ : o)); (-3, OpenMP, #pragma omp master taskloop simd reduction +); (72, OpenMP, #pragma omp master taskloop simd reduction(+ : p), reduction(+ : p)); (43, OpenMP, #pragma omp master taskloop simd reduction(* : ca)); (9, OpenMP, #pragma omp master taskloop simd reduction(\\)); (14, OpenMP, #pragma omp master taskloop simd reduction(| : argc,); (78, OpenMP, #pragma omp parallel reduction(min : i)); (81, OpenMP, #pragma omp parallel private(fl)); (72, OpenMP, #pragma omp master taskloop simd reduction(+ : r)); (46, OpenMP, #pragma omp master taskloop simd reduction(&& : S2::S2s)); (30, OpenMP, #pragma omp master taskloop simd reduction(max : h.b)); (20, OpenMP, #pragma omp master taskloop simd reduction(^ : S1)); (10, OpenMP, #pragma omp master taskloop simd reduction(|| : argc > 0? argv[1] : argv[2])); (-12, OpenMP, #pragma omp master taskloop simd reduction(); (56, OpenMP, #pragma omp master taskloop simd private(i), reduction(+ : j), reduction(+ : q)); (85, OpenMP, #pragma omp master taskloop simd reduction(+ : m) nogroup); (-12, OpenMP, #pragma omp master taskloop simd reduction(-); (66, OpenMP, #pragma omp parallel shared(i)); (-2, OpenMP, #pragma omp master taskloop simd reduction(foo : argc); (30, OpenMP, #pragma omp master taskloop simd reduction(- : da)); (69, OpenMP, #pragma omp master taskloop simd reduction(+ : fl))"}
{"predict": "(34, CUDA, cudaFree(d_in);); (34, CUDA, cudaFree(d_out_min);); (15, CUDA, cudaMemcpy(d_in, in.data(), in_bytes, cudaMemcpyHostToDevice);); (26, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (11, CUDA, cudaMalloc((void**)(&d_in), in_bytes);); (12, CUDA, cudaMalloc((void**)(&d_out_min), out_bytes);); (23, CUDA, assert(cudaMemcpyAsync(out_min.data(), d_out_min, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);)", "label": "(18, CUDA, cudaMemcpy(d_in, in.data(), in_bytes, cudaMemcpyHostToDevice);); (32, CUDA, cudaFree(d_in);); (13, CUDA, cudaMalloc((void**)(&d_in), in_bytes);); (13, CUDA, cudaMalloc((void**)(&d_out_min), out_bytes);); (25, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (23, CUDA, assert(cudaMemcpyAsync(out_min.data(), d_out_min, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);); (28, CUDA, cudaFree(d_out_min);)"}
{"predict": "(3, CUDA, cudaExtent extent = make_cudaExtent(10, 20, 0);)", "label": "(3, CUDA, cudaExtent extent = make_cudaExtent(10, 20, 0);)"}
{"predict": "(8, CUDA, err = (type == cudaMemcpyHostToDevice)? cudaMemcpyAsync( dst, src, sz, type, 0 ) : cudaMemcpyAsync( src, dst, sz, type, 0 );)", "label": "(7, CUDA, err = cudaMemcpy( dst, src, sz, type );); (10, CUDA, ss << cudaGetErrorString(err) << endl;)"}
{"predict": "(4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (13, MPI, MPI_Recv(&t, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (8, MPI, MPI_Recv(&t, 1, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (1, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (9, MPI, MPI_Send(&t, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);)", "label": "(4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (9, MPI, MPI_Recv(&t, 1, MPI_DOUBLE, i, 11, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (1, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (11, MPI, MPI_Send(&t, 1, MPI_DOUBLE, 0, 11, MPI_COMM_WORLD);)"}
{"predict": "(41, MPI, MPI_Send(&array[0][1], 1, columntype, 1, 1, MPI_COMM_WORLD);); (6, MPI, MPI_Init(&argc, &argv);); (61, MPI, MPI_Finalize();); (47, MPI, MPI_Type_free(&columntype);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (34, MPI, MPI_Type_commit(&columntype);); (32, MPI, MPI_Type_create_hvector(8, 1, sizeof(int) * 8, MPI_INT, &columntype);); (34, MPI, MPI_Type_free(&columntype);)", "label": "(39, MPI, MPI_Type_commit(&columntype);); (7, MPI, MPI_Init(&argc, &argv);); (61, MPI, MPI_Finalize();); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (34, MPI, MPI_Type_vector(8, 1, 8, MPI_INT, &columntype);); (45, MPI, MPI_Type_free(&columntype);); (37, MPI, MPI_Send(&array[0][1], 1, columntype, 1, 1, MPI_COMM_WORLD);)"}
{"predict": "(46, SYCL, auto Plt = get_native<sycl::backend::ext_oneapi_level_zero::platform>(Ctx);)", "label": "(41, SYCL, sycl::platform Plt = Dev.get_platform();)"}
{"predict": "(67, OpenMP, #pragma omp teams distribute parallel for simd aligned(f:j)); (55, OpenMP, #pragma omp teams distribute parallel for simd aligned(v:16)); (5, OpenMP, #pragma omp teams distribute parallel for simd aligned); (35, OpenMP, #pragma omp teams distribute parallel for simd aligned (S1)); (40, OpenMP, #pragma omp teams distribute parallel for simd aligned (argv[1])); (3, OpenMP, #pragma omp target); (12, OpenMP, #pragma omp teams distribute parallel for simd aligned ()); (15, OpenMP, #pragma omp teams distribute parallel for simd aligned (argc); (6, OpenMP, #pragma omp teams distribute parallel for simd aligned (); (35, OpenMP, #pragma omp teams distribute parallel for simd aligned(e, g)); (46, OpenMP, #pragma omp teams distribute parallel for simd aligned(i)); (4, OpenMP, #pragma omp teams distribute parallel for simd aligned (argc,); (18, OpenMP, #pragma omp teams distribute parallel for simd aligned (argc : 5)); (51, OpenMP, #pragma omp parallel); (24, OpenMP, #pragma omp teams distribute parallel for simd aligned (a, b:B::ib)); (55, OpenMP, #pragma omp teams distribute parallel for simd aligned(f)); (43, OpenMP, #pragma omp teams distribute parallel for simd aligned(h)); (58, OpenMP, #pragma omp teams distribute parallel for simd aligned(f:j)); (1, OpenMP, #pragma omp teams distribute parallel for simd aligned (argc)", "label": "(45, OpenMP, #pragma omp teams distribute parallel for simd aligned(e, g)); (7, OpenMP, #pragma omp teams distribute parallel for simd aligned); (14, OpenMP, #pragma omp teams distribute parallel for simd aligned ()); (9, OpenMP, #pragma omp teams distribute parallel for simd aligned (); (45, OpenMP, #pragma omp teams distribute parallel for simd aligned(h)); (51, OpenMP, #pragma omp parallel); (18, OpenMP, #pragma omp teams distribute parallel for simd aligned (argc,); (66, OpenMP, #pragma omp teams distribute parallel for simd aligned(f:j)); (33, OpenMP, #pragma omp teams distribute parallel for simd aligned (argv[1])); (24, OpenMP, #pragma omp teams distribute parallel for simd aligned (argc : 5)); (43, OpenMP, #pragma omp teams distribute parallel for simd aligned(i)); (56, OpenMP, #pragma omp teams distribute parallel for simd aligned(f)); (17, OpenMP, #pragma omp teams distribute parallel for simd aligned (*argc > 0? argv[1] : argv[2])); (-6, OpenMP, #pragma omp target); (47, OpenMP, #pragma omp teams distribute parallel for simd aligned(v:16)); (22, OpenMP, #pragma omp teams distribute parallel for simd aligned (S1)); (4, OpenMP, #pragma omp teams distribute parallel for simd aligned (argc)"}
{"predict": "(3, TBB, TestSequential< tbb::atomic<ScopedColor1> >( \"atomic<ScopedColor1>\" );); (4, TBB, TestSequential< tbb::atomic<ScopedColor2> >( \"atomic<ScopedColor2>\" );)", "label": "(10, TBB, __TBB_STATIC_ASSERT(!HasImplicitConversionToInt< tbb::atomic<ScopedColor1> >::value, \"The implicit conversion is not expected.\" );); (12, TBB, __TBB_STATIC_ASSERT( sizeof(tbb::atomic<ScopedColor2>) == sizeof(ScopedColor2), \"tbb::atomic instantiated with scoped enum should have the same size as scoped enum.\" );); (10, TBB, __TBB_STATIC_ASSERT( sizeof(tbb::atomic<ScopedColor1>) == sizeof(ScopedColor1), \"tbb::atomic instantiated with scoped enum should have the same size as scoped enum.\" );)"}
{"predict": "(91, OpenMP, C3D_Show_El_Tree(element_el_tree);); (89, OpenMP, #pragma omp critical)", "label": "(18, OpenMP, #pragma omp threadprivate (ident))"}
{"predict": "(1, OpenMP, #pragma omp parallel num_threads(4)); (3, OpenMP, #pragma omp parallel for); (0, OpenMP, #pragma omp task)", "label": "(17, OpenMP, #pragma omp taskwait); (0, OpenMP, #pragma omp task); (2, OpenMP, #pragma omp parallel for)"}
{"predict": "(7, MPI, MPI_Init(&argc, &argv);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (48, MPI, MPI_Type_free(&vector);); (65, MPI, MPI_Finalize();); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (37, MPI, MPI_Type_vector(8, 1, 6, MPI_INT, &vector);); (37, MPI, MPI_Type_commit(&vector);)", "label": "(52, MPI, MPI_Type_free(&vector);); (6, MPI, MPI_Init(&argc, &argv);); (42, MPI, MPI_Type_create_resized(vector, 0, sizeof(int), &vector);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (64, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (37, MPI, MPI_Type_vector(8, 1, 6, MPI_INT, &vector);); (38, MPI, MPI_Type_commit(&vector);); (57, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(53, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (72, CUDA, CUDA_CHECK(cudaDeviceReset());); (70, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (35, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_C), sizeof(data_type) * C.size()));); (64, CUDA, CUDA_CHECK(cudaFree(d_C));); (33, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (60, CUDA, CUDA_CHECK(cudaFree(d_A));); (27, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)", "label": "(66, CUDA, CUDA_CHECK(cudaFree(d_C));); (52, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (37, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_C), sizeof(data_type) * C.size()));); (69, CUDA, CUDA_CHECK(cudaDeviceReset());); (66, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (33, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (59, CUDA, CUDA_CHECK(cudaFree(d_A));); (27, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(7, MPI, MPI_Comm_rank(team.comm, &rank);)", "label": "(7, MPI, MPI_Comm_rank(team.comm, &rank);)"}
{"predict": "(36, MPI, MPI_Recv(recv_buff, msg_size, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (54, MPI, MPI_Finalize();); (4, MPI, MPI_Init(&argc, &argv);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (35, MPI, double end_time = MPI_Wtime();); (49, MPI, MPI_Send(send_buff, msg_size, MPI_INT, 0, 0, MPI_COMM_WORLD);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (3, MPI, MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);); (26, MPI, MPI_Send(send_buff, msg_size, MPI_INT, 1, 0, MPI_COMM_WORLD);); (21, MPI, double start_time = MPI_Wtime();)", "label": "(59, MPI, double end_time = MPI_Wtime();); (60, MPI, MPI_Finalize();); (47, MPI, MPI_Recv(recv_buff, msg_size, MPI_INT, 1, TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (17, MPI, MPI_Bcast(&msg_size, 1, MPI_INT, 0, MPI_COMM_WORLD);); (2, MPI, MPI_Init(NULL, NULL);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (13, MPI, MPI_Bcast(&nr_msgs, 1, MPI_INT, 0, MPI_COMM_WORLD);); (48, MPI, MPI_Ssend(send_buff, msg_size, MPI_INT, 0, TAG, MPI_COMM_WORLD);); (24, MPI, double start_time = MPI_Wtime();); (31, MPI, MPI_Recv(recv_buff, msg_size, MPI_INT, 0, TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (28, MPI, MPI_Ssend(send_buff, msg_size, MPI_INT, 1, TAG, MPI_COMM_WORLD);); (0, MPI, MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);)"}
{"predict": "(6, CUDA, thrust::complex<Q> Bi;)", "label": "(5, CUDA, thrust::complex<Q> Bi;)"}
{"predict": "(51, CUDA, cudaError_t result = cudaGetLastError();)", "label": "(51, CUDA, cudaError_t result = cudaGetLastError();)"}
{"predict": "(1, TBB, tbb::concurrent_queue<Foo> queue;)", "label": "(4, TBB, tbb::concurrent_queue<Foo> queue;)"}
{"predict": "(155, CUDA, checkCudaErrors ( cudaMalloc(&A_gpu->bigV_device, buffer_size * sizeof(double)) );); (162, CUDA, checkCudaErrors(cudaMalloc(&A_gpu->lblock_gid_dat, mcb * sizeof(int_t)));); (49, CUDA, tmp_ptr = (void *) cudaMallocHost(n * sizeof(int_t));); (138, CUDA, checkCudaErrors ( cudaMalloc(&A_gpu->bigU_device, bigu_size * sizeof(double)) );); (44, CUDA, tmp_ptr = (void *) cudaMallocHost(nsupers * sizeof(int_t));); (54, CUDA, tmp_ptr = (void *) cudaMallocHost(bigu_size * sizeof(double));); (150, CUDA, checkCudaErrors(cudaMalloc(&A_gpu->Ublock_info_device, mcb * sizeof(Ublock_info_t)));); (40, CUDA, tmp_ptr = (void *) cudaMallocHost(n * sizeof(double));); (44, CUDA, tmp_ptr = (void *) cudaMallocHost(remain_l_max * sizeof(double));); (133, CUDA, checkCudaErrors ( cudaMalloc(&A_gpu->lblock_gid_ptr, n * sizeof(int_t)) );); (143, CUDA, checkCudaErrors(cudaMalloc(&A_gpu->bigV_buff, buffer_size * sizeof(double)));); (137, CUDA, checkCudaErrors(cudaMalloc(&A_gpu->lblock_gid_ptr_dat, n * sizeof(int_t)));)", "label": "(61, CUDA, checkCudaErrors(cudaMallocHost(  &tmp_ptr,  remain_l_max * sizeof(double) )) ;); (50, CUDA, checkCudaErrors(cudaMallocHost(  &tmp_ptr, (n) * sizeof(int_t) )) ;); (256, CUDA, checkCudaErrors(cudaMemcpy( (A_gpu->local_u_blk_infoVec), local_u_blk_infoVec, cum_num_u_blocks * sizeof(local_u_blk_info_t), cudaMemcpyHostToDevice)) ;); (476, CUDA, checkCudaErrors(cudaMemcpy( (A_gpu->UrowindVec), indtemp1, u_ind_len * sizeof(int_t), cudaMemcpyHostToDevice)) ;); (62, CUDA, checkCudaErrors(cudaMallocHost ( &tmp_ptr, sizeof(double) * (A_host->bufmax[1])));); (67, CUDA, checkCudaErrors(cudaMallocHost ( &tmp_ptr, sizeof(int_t) * (A_host->bufmax[2])));); (81, CUDA, checkCudaErrors(cudaMalloc(  &tmp_ptr,  buffer_size * sizeof(double))) ;); (179, CUDA, checkCudaErrors(cudaMemcpy( (A_gpu->local_l_blk_infoVec), local_l_blk_infoVec, cum_num_l_blocks * sizeof(local_l_blk_info_t), cudaMemcpyHostToDevice)) ;); (51, CUDA, checkCudaErrors(cudaMallocHost(  &tmp_ptr, mcb * sizeof(Ublock_info"}
{"predict": "(5, CUDA, extern __shared__ uint64_t s_qiHat_mod_pj[];)", "label": "(5, CUDA, extern __shared__ uint64_t s_qiHat_mod_pj[];)"}
{"predict": "(15, OpenMP, #pragma omp teams distribute shared ()); (47, OpenMP, #pragma omp teams distribute shared(ba)); (65, OpenMP, #pragma omp teams distribute shared(i)); (18, OpenMP, #pragma omp teams distribute shared (argc); (31, OpenMP, #pragma omp teams distribute shared (argv[1])); (60, OpenMP, #pragma omp teams distribute shared(j)); (5, OpenMP, #pragma omp teams distribute shared (); (36, OpenMP, #pragma omp teams distribute shared(ca)); (54, OpenMP, #pragma omp teams distribute private(i), shared(i)); (2, OpenMP, #pragma omp teams distribute shared); (47, OpenMP, #pragma omp teams distribute firstprivate(i), shared(i)); (13, OpenMP, #pragma omp teams distribute shared (argc > 0? argv[1] : argv[2])); (5, OpenMP, #pragma omp teams distribute shared (argc); (18, OpenMP, #pragma omp teams distribute shared (S1)); (43, OpenMP, #pragma omp teams distribute shared(h, B::x)); (26, OpenMP, #pragma omp teams distribute shared(da)); (12, OpenMP, #pragma omp teams distribute shared (a, b, c, d, f)); (49, OpenMP, #pragma omp teams distribute shared(i)); (37, OpenMP, #pragma omp teams distribute shared(e, g)); (3, OpenMP, #pragma omp teams distribute shared (argc,)", "label": "(26, OpenMP, #pragma omp teams distribute shared (argc)); (22, OpenMP, #pragma omp teams distribute shared (argc > 0? argv[1] : argv[2])); (39, OpenMP, #pragma omp teams distribute shared(ca)); (41, OpenMP, #pragma omp teams distribute shared(da)); (16, OpenMP, #pragma omp teams distribute shared (argc,); (3, OpenMP, #pragma omp teams distribute shared); (5, OpenMP, #pragma omp teams distribute shared (); (49, OpenMP, #pragma omp teams distribute firstprivate(i), shared(i)); (27, OpenMP, #pragma omp teams distribute shared (argv[1])); (-2, OpenMP, #pragma omp target); (55, OpenMP, #pragma omp teams distribute shared(j)); (57, OpenMP, #pragma omp teams distribute firstprivate(i)); (17, OpenMP, #pragma omp teams distribute shared (S1)); (37, OpenMP, #pragma omp teams distribute shared(h, B::x)); (18, OpenMP, #pragma omp teams distribute shared (a, b, c, d, f)); (2, OpenMP, #pragma omp teams distribute shared (argc); (46, OpenMP, #pragma omp teams distribute shared(i)); (21, OpenMP, #pragma omp teams distribute shared(ba)); (35, OpenMP, #pragma omp teams distribute private(i), shared(i)); (28, OpenMP, #pragma omp teams distribute shared(e, g)); (39, OpenMP, #pragma omp teams distribute private(i)); (-7, OpenMP, #pragma omp teams distribute shared ())"}
{"predict": "(56, OpenMP, elapsed_time_total = omp_get_wtime() - start_time_total;); (40, OpenMP, start_computation_time = omp_get_wtime();); (50, CUDA, cudaDeviceSynchronize();); (27, CUDA, cudaMalloc((void **)&fn_d, sizeof(FLOAT)*DIM_X_INNER*DIM_Y_INNER*DIM_Z_INNER);); (49, OpenMP, elapsed_computation_time = omp_get_wtime() - start_computation_time;); (24, CUDA, cudaMalloc((void **)&f_d, sizeof(FLOAT)*DIM_X_INNER*DIM_Y_INNER*DIM_Z_INNER);); (50, CUDA, cudaFree(f_d);); (50, CUDA, cudaFree(fn_d);); (23, CUDA, cudaMemcpy(f_d,f, sizeof(FLOAT)*DIM_X_INNER*DIM_Y_INNER*DIM_Z_INNER, cudaMemcpyHostToDevice);); (21, CUDA, cudaMemcpy(fn_d,fn, sizeof(FLOAT)*DIM_X_INNER*DIM_Y_INNER*DIM_Z_INNER, cudaMemcpyHostToDevice);); (23, CUDA, cudaMemcpy(time, time + dt, sizeof(FLOAT), cudaMemcpyHostToDevice);); (25, OpenMP, start_time_total = omp_get_wtime();)", "label": "(33, CUDA, cudaMalloc( (void**) &fn_d,  XYZ_SIZE*sizeof(FLOAT));); (30, OpenMP, start_time_total = omp_get_wtime();); (55, OpenMP, elapsed_time_total = omp_get_wtime() - start_time_total;); (33, OpenMP, start_computation_time = omp_get_wtime();); (50, OpenMP, elapsed_computation_time = omp_get_wtime() - start_computation_time;); (29, CUDA, cudaMemcpy(f_d,f,XYZ_SIZE*sizeof(FLOAT), cudaMemcpyHostToDevice);); (50, CUDA, cudaMemcpy(f, f_d,XYZ_SIZE*sizeof(FLOAT), cudaMemcpyDeviceToHost);); (25, CUDA, cudaMalloc( (void**) &f_d,  XYZ_SIZE*sizeof(FLOAT));); (51, CUDA, cudaFree(fn_d);); (49, CUDA, cudaFree(f_d);)"}
{"predict": "(13, OpenMP, #pragma omp master); (10, OpenMP, #pragma omp parallel default(shared))", "label": "(12, OpenMP, #pragma omp master); (9, OpenMP, #pragma omp parallel default(shared))"}
{"predict": "(22, MPI, MPI_Allreduce(buf_f16, buf, N, MPI_C_FLOAT16, MPI_SUM, MPI_COMM_WORLD);); (10, MPI, MPI_Allreduce(buf, buf, N, MPIX_BFLOAT16, MPI_SUM, MPI_COMM_WORLD);)", "label": "(10, MPI, MPI_Allreduce(MPI_IN_PLACE, buf, N, MPIX_BFLOAT16, MPI_SUM, comm);); (20, MPI, MPI_Allreduce(MPI_IN_PLACE, buf_f16, N, MPIX_C_FLOAT16, MPI_SUM, comm);)"}
{"predict": "(32, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (34, CUDA, CUDA_CHECK(cudaFree(d_data));)", "label": "(32, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (34, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(132, MPI, MPI_Waitall(k, request, status);); (88, MPI, MPI_Reduce_local(recvbuf, tempbuf, count, type, op);)", "label": "(133, MPI, MPI_Waitall(k, request, status);); (90, MPI, MPI_Reduce_local(recvbuf, tempbuf, count, type, op);)"}
{"predict": "(26, MPI, MPI_Allreduce(bufin, bufout, count, dtype, MPI_SUM, comm);); (10, MPI, MPI_Comm_size(comm, &size);); (14, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(10, MPI, MPI_Comm_size(comm, &size);); (15, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (22, MPI, MPI_Allreduce(bufin, bufout, count, dtype, MPI_SUM, comm);)"}
{"predict": "(56, CUDA, checkCudaErrors(cudaMalloc((void **)&d_Kernel, DATA_SIZE));); (114, CUDA, checkCudaErrors(cudaFree(d_Kernel));); (73, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (39, CUDA, findCudaDevice(argc, (const char **)argv);); (48, CUDA, checkCudaErrors(cudaMalloc((void **)&d_Data, DATA_SIZE));); (103, CUDA, checkCudaErrors(cudaFree(d_Data));)", "label": "(68, CUDA, checkCudaErrors(cudaMemset(d_Kernel, 0, DATA_SIZE));); (110, CUDA, checkCudaErrors(cudaFree(d_Kernel));); (40, CUDA, findCudaDevice(argc, (const char **)argv);); (72, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (49, CUDA, checkCudaErrors(cudaMalloc((void **)&d_Kernel, DATA_SIZE));); (105, CUDA, checkCudaErrors(cudaFree(d_Data));); (48, CUDA, checkCudaErrors(cudaMalloc((void **)&d_Data, DATA_SIZE));)"}
{"predict": "(46, MPI, MPI_Error_string(mpirc, mpierrstr, &mpierrlen);); (52, MPI, mpirc = MPI_File_read_at(fh, disp, (void*)&version_packed, 1, MPI_UINT64_T, &status);); (36, MPI, mpirc = MPI_File_set_view(fh, disp, MPI_UINT64_T, MPI_UINT64_T, datarep, MPI_INFO_NULL);); (3, MPI, double start_read = MPI_Wtime();); (70, MPI, mpirc = MPI_File_close(&fh);); (94, MPI, MPI_Barrier(MPI_COMM_WORLD);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Barrier(MPI_COMM_WORLD);); (2, MPI, double end_read = MPI_Wtime();); (17, MPI, MPI_Error_string(mpirc, mpierrstr, &mpierrlen);); (3, MPI, int mpirc = MPI_File_open(MPI_COMM_WORLD, (char*)name, amode, MPI_INFO_NULL, &fh);)", "label": "(39, MPI, MPI_Error_string(mpirc, mpierrstr, &mpierrlen);); (98, MPI, MPI_Barrier(MPI_COMM_WORLD);); (22, MPI, int mpirc = MPI_File_open(MPI_COMM_WORLD, (char*)name, amode, MPI_INFO_NULL, &fh);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (79, MPI, double end_read = MPI_Wtime();); (3, MPI, double start_read = MPI_Wtime();); (67, MPI, mpirc = MPI_File_close(&fh);); (30, MPI, mpirc = MPI_File_set_view(fh, disp, MPI_BYTE, MPI_BYTE, datarep, MPI_INFO_NULL);); (47, MPI, MPI_Bcast(&version, 1, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (36, MPI, mpirc = MPI_File_read_at(fh, 0, &version_packed, 8, MPI_BYTE, &status);)"}
{"predict": "(166, MPI, MPI_Allreduce(&tmp_error, error_code, 1, MPI_INT, MPI_MAX, fd->comm);); (65, MPI, MPI_Comm_size(fd->comm, &nprocs);); (141, MPI, MPI_Allreduce(&end_offset, end_offsets, 1, ADIO_OFFSET, MPI_MAX, fd->comm);); (64, MPI, MPI_Comm_rank(fd->comm, &myrank);); (137, MPI, MPI_Allreduce(&start_offset, st_offsets, 1, ADIO_OFFSET, MPI_MIN, fd->comm);); (394, MPI, MPI_Type_size_x(datatype, &size);); (138, MPI, MPI_Allreduce(&count, count_my_req_per_proc, 1, MPI_COUNT, MPI_SUM, fd->comm);); (140, MPI, MPI_Allreduce(&count, count_others_req_per_proc, 1, MPI_COUNT, MPI_SUM, fd->comm);); (384, MPI, MPI_Type_size_x(datatype, &buftype_size);); (132, MPI, MPI_Allreduce(&end_offset, &end_offset, 1, ADIO_OFFSET, MPI_MAX, fd->comm);); (130, MPI, MPI_Allreduce(&start_offset, &start_offset, 1, ADIO_OFFSET, MPI_MIN, fd->comm);); (380, MPI, MPI_Type_get_extent(datatype, &buftype_lb, &buftype_extent);); (393, MPI, MPI_Type_size_x(datatype, &extent);); (135, MPI, MPI_Allreduce(&count, &count, 1, MPI_COUNT, MPI_SUM, fd->comm);); (130, MPI, MPI_Allreduce(&end_offset, &end_offset, 1, ADIO_OFFSET, MPI_MAX, fd->comm);); (133, MPI, MPI_Allreduce(&count, &count, 1, MPI_COUNT, MPI_SUM, fd->comm);); (130", "label": "(191, MPI, MPI_Allreduce(&tmp_error, error_code, 1, MPI_INT, MPI_MAX, fd->comm);); (65, MPI, MPI_Comm_size(fd->comm, &nprocs);); (93, MPI, MPI_Allgather(&end_offset, 1, ADIO_OFFSET, end_offsets, 1, ADIO_OFFSET, fd->comm);); (91, MPI, MPI_Allgather(&start_offset, 1, ADIO_OFFSET, st_offsets, 1, ADIO_OFFSET, fd->comm);); (63, MPI, MPI_Comm_rank(fd->comm, &myrank);); (210, MPI, MPI_Type_size_x(datatype, &size);)"}
{"predict": "(16, CUDA, CHECK(cudaMalloc(&atom->g_NN, MN * sizeof(real)));); (16, CUDA, CHECK(cudaMalloc(&atom->g_pe, N * sizeof(real)));); (18, CUDA, CHECK(cudaMalloc(&atom->g_fx, N * sizeof(real)));); (16, CUDA, CHECK(cudaMalloc(&atom->g_z,  N * sizeof(real)));); (16, CUDA, CHECK(cudaMalloc(&atom->g_fy, N * sizeof(real)));); (16, CUDA, CHECK(cudaMalloc(&atom->g_fz, N * sizeof(real)));); (12, CUDA, CHECK(cudaMalloc(&atom->g_x,  N * sizeof(real)));); (13, CUDA, CHECK(cudaMalloc(&atom->g_y,  N * sizeof(real)));); (11, CUDA, CHECK(cudaMalloc(&atom->g_m,  N * sizeof(real)));)", "label": "(23, CUDA, CHECK(cudaMalloc((void**)&atom->g_fz, sizeof(real) * N));); (19, CUDA, CHECK(cudaMalloc((void**)&atom->g_z, sizeof(real) * N));); (17, CUDA, CHECK(cudaMalloc((void**)&atom->g_y, sizeof(real) * N));); (21, CUDA, CHECK(cudaMalloc((void**)&atom->g_pe, sizeof(real) * N));); (14, CUDA, CHECK(cudaMalloc((void**)&atom->g_x, sizeof(real) * N));); (12, CUDA, CHECK(cudaMalloc((void**)&atom->g_NL, sizeof(int) * N * MN));); (16, CUDA, CHECK(cudaMalloc((void**)&atom->g_fy, sizeof(real) * N));); (9, CUDA, CHECK(cudaMalloc((void**)&atom->g_NN, sizeof(int) * N));); (13, CUDA, CHECK(cudaMalloc((void**)&atom->g_fx, sizeof(real) * N));)"}
{"predict": "(25, OpenCL, err = clEnqueueWriteBuffer( queue, buf[1], CL_TRUE, 0, sizeof(int), &dim_arg_global, 0, NULL, NULL);)", "label": "(25, OpenCL, err = clEnqueueWriteBuffer( queue, buf[1], CL_TRUE, 0, sizeof(int), &dim_arg_global, 0, NULL, NULL);)"}
{"predict": "(37, OpenMP, #pragma omp parallel firstprivate(numElemReg)); (65, OpenMP, #pragma omp for nowait firstprivate(numElemReg)); (101, OpenMP, #pragma omp for firstprivate(numElemReg)); (38, OpenMP, #pragma omp for firstprivate(numElemReg)); (73, OpenMP, #pragma omp for firstprivate(numElemReg) compress(1)); (134, OpenMP, #pragma omp for firstprivate(numElemReg) nowait)", "label": "(106, OpenMP, #pragma omp parallel for firstprivate(numElemReg)); (53, OpenMP, #pragma omp for firstprivate(numElemReg)); (65, OpenMP, #pragma omp for nowait firstprivate(numElemReg, eosvmin)); (34, OpenMP, #pragma omp parallel); (74, OpenMP, #pragma omp for nowait firstprivate(numElemReg, eosvmax)); (36, OpenMP, #pragma omp for nowait firstprivate(numElemReg))"}
{"predict": "(13, CUDA, PYCUDA_CHECK_NOGIL(cudaStreamCreate(&stream, flags, priority));)", "label": "(13, CUDA, PYCUDA_ASSERT_NOGIL(cudaStreamCreateWithPriority(&stream, flags, priority));)"}
{"predict": "(38, CUDA, cudaFree(res_dev);); (31, CUDA, cudaMalloc(&res_dev, sizeof(float));); (33, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)", "label": "(38, CUDA, cudaFree(res_dev);); (31, CUDA, cudaMalloc(&res_dev, sizeof(float));); (33, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(13, MPI, MPI_Comm_size(comm, &mpi_size);); (44, MPI, rc = MPI_Open_file(comm, filename, amode, info, fd->comm_info);)", "label": "(40, MPI, MPI_Error_class(*error_code, &rc);); (12, MPI, MPI_Comm_size(comm, &mpi_size);); (42, MPI, MPI_Bcast(&rc, 1, MPI_INT, 0, comm);)"}
{"predict": "(25, CUDA, thrust::device_vector<Element> d_D = h_D;); (80, CUDA, std::cerr << \"CUDA error: \" << cudaGetErrorString(result) << std::endl;); (87, CUDA, thrust::host_vector<Element> h_D(size(tensor_shape));); (14, CUDA, thrust::host_vector<Element> h_S(size(tensor_shape));); (74, CUDA, cudaError_t result = cudaGetLastError();); (20, CUDA, thrust::device_vector<Element> d_S = h_S;); (23, CUDA, Tensor tensor_D = make_tensor(make_gmem_ptr(d_D.data()), make_layout(tensor_shape));); (19, CUDA, Tensor tensor_S = make_tensor(make_gmem_ptr(d_S.data()), make_layout(tensor_shape));)", "label": "(81, CUDA, cudaError result = cudaDeviceSynchronize();); (30, CUDA, Tensor tensor_S = make_tensor(make_gmem_ptr(thrust::raw_pointer_cast(d_S.data())), make_layout(tensor_shape));); (14, CUDA, thrust::host_vector<Element> h_S(size(tensor_shape));); (22, CUDA, thrust::device_vector<Element> d_D = h_D;); (79, CUDA, std::cerr << \"CUDA Runtime error: \" << cudaGetErrorString(result) << std::endl;); (12, CUDA, thrust::host_vector<Element> h_D(size(tensor_shape));); (26, CUDA, Tensor tensor_D = make_tensor(make_gmem_ptr(thrust::raw_pointer_cast(d_D.data())), make_layout(tensor_shape));); (17, CUDA, thrust::device_vector<Element> d_S = h_S;)"}
{"predict": "(2, CUDA, __shared__ float4 siftParts1[16*17];); (2, CUDA, __shared__ float4 siftParts2[16*17];); (0, CUDA, __shared__ float siftSum[16*16];); (0, CUDA, __shared__ float siftMean[16*16];); (0, CUDA, __shared__ float siftVar[16*16];); (0, CUDA, __shared__ float siftDev[16*16];); (1, CUDA, __shared__ float lock;)", "label": "(3, CUDA, __shared__ float siftParts2[16*64];); (1, CUDA, __shared__ float siftParts1[17*64];)"}
{"predict": "(12, TBB, my_thread_array = tbb::cache_aligned_allocator<padded_private_worker>().allocate( my_n_thread );)", "label": "(12, TBB, my_thread_array = tbb::cache_aligned_allocator<padded_private_worker>().allocate( my_n_thread );)"}
{"predict": "(54, CUDA, __shared__ bool isAccepted;); (54, CUDA, __shared__ float sSumVal;)", "label": "(52, CUDA, __shared__ bool isAccepted;); (52, CUDA, __shared__ T sSumVal;)"}
{"predict": "(54, MPI, MPI_Free_mem(dst_buf);); (10, MPI, MPI_Alloc_mem(sizeof(double) * nproc * MAXELEMS, MPI_INFO_NULL, &dst_buf);); (50, MPI, MPI_Win_free(&dst_win);); (22, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, target, 0, dst_win);); (34, MPI, MPI_Win_unlock(target, dst_win);); (13, MPI, MPI_Barrier(MPI_COMM_WORLD);); (20, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(54, MPI, MPI_Free_mem(dst_buf);); (9, MPI, MPI_Alloc_mem(sizeof(double) * nproc * MAXELEMS, MPI_INFO_NULL, &dst_buf);); (30, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, target, 0, dst_win);); (50, MPI, MPI_Win_free(&dst_win);); (13, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, me, 0, dst_win);); (17, MPI, MPI_Win_unlock(me, dst_win);); (18, MPI, MPI_Barrier(MPI_COMM_WORLD);); (28, MPI, MPI_Win_unlock(target, dst_win);)"}
{"predict": "(15, OpenMP, omp_set_lock(&l);); (24, OpenMP, omp_unset_lock(&l);); (26, OpenMP, omp_destroy_lock(&l);); (4, OpenMP, #pragma omp parallel); (16, OpenMP, #pragma omp atomic); (4, OpenMP, omp_init_lock(&l);)", "label": "(7, OpenMP, omp_init_lock(&l);); (27, OpenMP, omp_destroy_lock(&l);); (7, OpenMP, #pragma omp parallel); (14, OpenMP, omp_set_lock(&l);); (21, OpenMP, omp_unset_lock(&l);); (-1, OpenMP, auto numThreads = omp_get_max_threads();)"}
{"predict": "(43, OpenMP, #pragma omp target parallel for simd private(i)); (37, OpenMP, #pragma omp target parallel for simd private(h, B::x)); (3, OpenMP, #pragma omp target parallel for simd private); (31, OpenMP, #pragma omp target parallel for simd private(argv[1])); (4, OpenMP, #pragma omp target parallel for simd private(); (48, OpenMP, #pragma omp target parallel for simd private(m)); (33, OpenMP, #pragma omp parallel); (24, OpenMP, #pragma omp target parallel for simd private(a, b)); (2, OpenMP, #pragma omp target parallel for simd private()); (26, OpenMP, #pragma omp target parallel for simd private(e, g)); (3, OpenMP, #pragma omp target parallel for simd private(argc); (40, OpenMP, #pragma omp target parallel for simd private(j)); (45, OpenMP, #pragma omp target parallel for simd private(i)); (33, OpenMP, #pragma omp parallel shared(i)); (35, OpenMP, #pragma omp target parallel for simd shared(i)); (1, OpenMP, #pragma omp target parallel for simd private(argc,); (3, OpenMP, #pragma omp target parallel for simd private(argc > 0? argv[1] : argv[2])); (10, OpenMP, #pragma omp target parallel for simd private(S1)); (13, OpenMP, #pragma omp target parallel for simd private(z, argv[3]))", "label": "(22, OpenMP, #pragma omp target parallel for simd private(argc > 0? argv[1] : argv[2])); (36, OpenMP, #pragma omp target parallel for simd private(e, g)); (44, OpenMP, #pragma omp target parallel for simd private(i)); (49, OpenMP, #pragma omp target parallel for simd private(j)); (47, OpenMP, #pragma omp parallel private(i)); (54, OpenMP, #pragma omp target parallel for simd private(m)); (4, OpenMP, #pragma omp target parallel for simd private(); (36, OpenMP, #pragma omp parallel); (32, OpenMP, #pragma omp target parallel for simd private(h, B::x)); (22, OpenMP, #pragma omp target parallel for simd private(a, b)); (24, OpenMP, #pragma omp target parallel for simd private(argv[1])); (17, OpenMP, #pragma omp target parallel for simd private(S1)); (38, OpenMP, #pragma omp parallel shared(i)); (12, OpenMP, #pragma omp target parallel for simd private(argc, z)); (2, OpenMP, #pragma omp target parallel for simd private(argc); (-2, OpenMP, #pragma omp target parallel for simd private()); (-9, OpenMP, #pragma omp target parallel for simd private); (2, OpenMP, #pragma omp target parallel for simd private(argc,)"}
{"predict": "(343, MPI, MPI_Bcast(o_x_min, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);); (350, MPI, MPI_Bcast(o_y_max, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);); (344, MPI, MPI_Bcast(o_pixels_down, 1, MPI_INT, 0, MPI_COMM_WORLD);); (342, MPI, MPI_Bcast(o_pixels_across, 1, MPI_INT, 0, MPI_COMM_WORLD);); (351, MPI, MPI_Bcast(o_num_colors, 1, MPI_INT, 0, MPI_COMM_WORLD);); (347, MPI, MPI_Bcast(o_julia_imaginary_y, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);); (343, MPI, MPI_Bcast(o_julia_real_x, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);); (340, MPI, MPI_Bcast(o_y_min, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);); (345, MPI, MPI_Bcast(o_divergent_limit, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);); (339, MPI, MPI_Bcast(o_max_iterations, 1, MPI_INT, 0, MPI_COMM_WORLD);); (341, MPI, MPI_Bcast(o_alternate, 1, MPI_INT, 0, MPI_COMM_WORLD);); (340, MPI, MPI_Bcast(o_julia, 1, MPI_INT, 0, MPI_COMM_WORLD);); (337, MPI, MPI_Bcast(o_x_max, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);); (339, MPI, MPI_Bcast(o_save_image, 1, MPI_INT, 0, MPI_COMM_WORLD);); (337, MPI, MPI_Bcast(o_use_datatypes", "label": "(81, MPI, MPI_Finalize();)"}
{"predict": "(12, MPI, MPI_Iscan(sendbuf, recvbuf, count, datatype, MPI_SUM, comm, &req);); (8, MPI, MPI_Scan(sendbuf, recvbuf, count, datatype, MPI_SUM, comm);); (11, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);)", "label": "(9, MPI, MPI_Scan_c(sendbuf, recvbuf, count, datatype, MPI_SUM, comm);); (12, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (10, MPI, MPI_Iscan_c(sendbuf, recvbuf, count, datatype, MPI_SUM, comm, &req);)"}
{"predict": "(23, OpenMP, #pragma omp task shared (argc)); (27, OpenMP, #pragma omp task shared(ca)); (32, OpenMP, #pragma omp task shared(i)); (18, OpenMP, #pragma omp task shared (S1)); (34, OpenMP, #pragma omp task shared(j)); (2, OpenMP, #pragma omp task shared (); (37, OpenMP, #pragma omp task shared(i, z)); (3, OpenMP, #pragma omp task shared (argc); (30, OpenMP, #pragma omp task shared(e, g)); (31, OpenMP, #pragma omp task shared(h, B::x)); (15, OpenMP, #pragma omp task shared (argv[1])); (1, OpenMP, #pragma omp task shared (argc,); (3, OpenMP, #pragma omp task shared (argc > 0? argv[1] : argv[2])); (2, OpenMP, #pragma omp task shared (argc)); (22, OpenMP, #pragma omp task shared(da)); (3, OpenMP, #pragma omp task shared (argc)); (18, OpenMP, #pragma omp task shared(S2::S2s)); (3, OpenMP, #pragma omp task shared (argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (14, OpenMP, #pragma omp task shared(S2::S2sc)); (1, OpenMP, #pragma omp task shared (a, b, c, d, f)); (5, OpenMP, #pragma omp task shared(ba) allocate(omp_thread_mem_alloc: ba))", "label": "(45, OpenMP, #pragma omp parallel firstprivate(i)); (40, OpenMP, #pragma omp parallel private(i)); (17, OpenMP, #pragma omp task shared(argc, z)); (39, OpenMP, #pragma omp task shared(i)); (7, OpenMP, #pragma omp task shared()); (20, OpenMP, #pragma omp task shared(argv[1])); (29, OpenMP, #pragma omp task shared(h, B::x)); (26, OpenMP, #pragma omp task shared(e, g)); (31, OpenMP, #pragma omp task firstprivate(i), shared(i)); (34, OpenMP, #pragma omp task shared(j)); (17, OpenMP, #pragma omp task shared(ba)); (6, OpenMP, #pragma omp task shared(argc > 0? argv[1] : argv[2])); (-5, OpenMP, #pragma omp task shared); (-4, OpenMP, #pragma omp task shared(); (23, OpenMP, #pragma omp task private(i), shared(i)); (0, OpenMP, #pragma omp task shared(argc,); (7, OpenMP, #pragma omp task shared(a, b, c, d, f)); (4, OpenMP, #pragma omp task shared(S1)); (13, OpenMP, #pragma omp task shared(da)); (-6, OpenMP, #pragma omp task shared(argc); (9, OpenMP, #pragma omp task shared(ca))"}
{"predict": "(31, OpenMP, #pragma omp target enter data map(alloc : i)); (23, OpenMP, #pragma omp target enter data map(alloc : i[0:1])); (15, OpenMP, #pragma omp target enter data map(alloc : i[0])); (34, OpenMP, #pragma omp target enter data map(alloc : i[0:100])); (43, OpenMP, #pragma omp target enter data map(alloc : i[0:100], d)); (24, OpenMP, #pragma omp target enter data map(alloc : i[0:2])); (35, OpenMP, #pragma omp target enter data map(alloc : i[0:20], d)); (4, OpenMP, #pragma omp target enter data map(alloc : i[0])); (26, OpenMP, #pragma omp target enter data map(alloc : i[0:10])); (17, OpenMP, #pragma omp target enter data map(alloc : i[0:][1:])); (27, OpenMP, #pragma omp target enter data map(alloc : i[0:20])); (34, OpenMP, #pragma omp target enter data map(alloc : i[0:100][1:])); (27, OpenMP, #pragma omp target enter data map(alloc : i[0:10][1:])); (2, OpenMP, #pragma omp target enter data map(alloc : i[1][1])); (35, OpenMP, #pragma omp target enter data map(alloc : i[0:100][1:], d)); (14, OpenMP, #pragma omp target enter data map(alloc : i[0:][1:])); (36, OpenMP, #pragma omp target enter data map(alloc : i[0:100][1:], d, p[0])); (39, OpenMP, #pragma omp target enter data map(alloc : i[0:100][1:])); (1, OpenMP, #pragma omp target enter data map(alloc : i[1])); (33, OpenMP, #pragma omp target enter data map(alloc : i[0:100][1:])); (27, OpenMP, #pragma omp target enter data map(", "label": "(14, OpenMP, #pragma omp target map(p)); (51, OpenMP, #pragma omp target map(from: ps->ps->ps)); (38, OpenMP, #pragma omp target map(ps->i)); (31, OpenMP, #pragma omp target map(s.ps->ps->ps)); (32, OpenMP, #pragma omp target map(to: s.ps->ps->s.f[:22])); (27, OpenMP, #pragma omp target map(to: s.ps->ps)); (20, OpenMP, #pragma omp target map(to: s.p[:22])); (17, OpenMP, #pragma omp target map(s.p)); (2, OpenMP, #pragma omp target map(i)); (37, OpenMP, #pragma omp target map(to: ps->p[:22])); (18, OpenMP, #pragma omp target map(s.ps)); (27, OpenMP, #pragma omp target map(ps)); (30, OpenMP, #pragma omp target map(ps->s.f)); (41, OpenMP, #pragma omp target map(ps->ps->ps->ps)); (8, OpenMP, #pragma omp target map(s.s.f)); (-3, OpenMP, #pragma omp target map(i[1:23])); (-8, OpenMP, #pragma omp target map(d)); (3, OpenMP, #pragma omp target map(s.i)); (26, OpenMP, #pragma omp target map(from: ps->p)); (41, OpenMP, #pragma omp target map(from: s.f[:22]) map(to: ps->p[:33])); (38, OpenMP, #pragma omp target map(to: s.f[:22]) map(from: s.p[:33])); (41, OpenMP, #pragma omp target map(from: s.f[:22], s.s) map(to: ps->p[:33])); (-4, OpenMP, #pragma omp target map(s)); (25, OpenMP, #pragma omp target map(ps->ps)); (6, OpenMP, #pragma omp target map(from: s.ps->s.i)); (31, OpenMP, #pragma omp target map(to: ps->ps->ps->s.f[:22])); (-10, OpenMP, #pragma omp target map(p[1:24])); (23, OpenMP, #pragma omp target map(from: ps->ps->s.i))"}
{"predict": "(20, CUDA, getLastCudaError(\"generateSampleRanksKernel<0> failed\\n\");); (14, CUDA, getLastCudaError(\"generateSampleRanksKernel<1> failed\\n\");)", "label": "(15, CUDA, getLastCudaError(\"generateSampleRanksKernel<1U><<<>>> failed\\n\");); (19, CUDA, getLastCudaError(\"generateSampleRanksKernel<0U><<<>>> failed\\n\");)"}
{"predict": "(12, MPI, MPI_Comm_size(comm, &ranks);); (12, MPI, MPI_Comm_rank(comm, &rank);)", "label": "(13, MPI, MPI_Comm_size(comm, &ranks);); (11, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(18, OpenCL, rc = clEnqueueNDRangeKernel(queue, write_kernel, 1, NULL, global_work_size, local_work_size, 0, NULL, NULL); CLERR;); (12, OpenCL, rc = clSetKernelArg(readwrite_kernel, 4, sizeof(cl_mem), &mc->err_second_read); CLERR;); (14, OpenCL, rc = clEnqueueNDRangeKernel(queue, readwrite_kernel, 1, NULL, global_work_size, local_work_size, 0, NULL, NULL);CLERR;); (19, OpenCL, rc = clEnqueueNDRangeKernel(queue, read_kernel, 1, NULL, global_work_size, local_work_size, 0, NULL, NULL);CLERR;); (11, OpenCL, rc = clSetKernelArg(readwrite_kernel, 3, sizeof(cl_mem), &mc->err_count); CLERR;); (9, OpenCL, rc = clSetKernelArg(readwrite_kernel, 1, sizeof(cl_ulong), &mc->mem_size); CLERR;); (7, OpenCL, rc = clSetKernelArg(readwrite_kernel, 0, sizeof(cl_mem), &mc->device_mem); CLERR;); (13, OpenCL, rc = clEnqueueNDRangeKernel(queue, write_kernel, 1, NULL, global_work_size, local_work_size, 0, NULL, NULL);CLERR;); (17, OpenCL, rc = clEnqueueNDRangeKernel(queue, read_kernel, 1, NULL, global_work_size, local_work_size, 0, NULL, NULL);CLERR;); (5, OpenCL, rc = clSetKernelArg(readwrite_kernel, 2, sizeof(cl_mem), &mc->err_addr); CLERR;); (14, OpenCL, rc = clEnqueueNDRangeKernel(queue, read_kernel, 1, NULL, global_work_size, local_work_size, 0, NULL, NULL);CLERR;); (3, OpenCL, rc = clSetKernelArg(readwrite_kernel, 5, sizeof(cl_mem), &mc", "label": "(18, OpenCL, rc = clEnqueueNDRangeKernel(queue, write_kernel, 1, NULL, global_work_size, local_work_size, 0, NULL, NULL); CLERR;); (23, OpenCL, rc = clSetKernelArg(read_kernel, 4, sizeof(TYPE), &p2); CLERR;); (35, OpenCL, clReleaseKernel(read_kernel);); (28, OpenCL, rc = clEnqueueNDRangeKernel(queue, read_kernel, 1, NULL, global_work_size, local_work_size, 0, NULL, NULL);CLERR;); (5, OpenCL, cl_kernel read_kernel = clCreateKernel(program, \"kernel_modtest_read\", &rc); CLERR;); (20, OpenCL, rc = clSetKernelArg(read_kernel, 5, sizeof(cl_mem), &mc->err_count); CLERR;); (14, OpenCL, rc = clSetKernelArg(read_kernel, 0, sizeof(cl_mem), &mc->device_mem); CLERR;); (22, OpenCL, rc = clSetKernelArg(read_kernel, 9, sizeof(cl_mem), &mc->err_second_read); CLERR;); (0, OpenCL, cl_kernel write_kernel = clCreateKernel(program, \"kernel_modtest_write\", &rc); CLERR;); (8, OpenCL, rc = clSetKernelArg(write_kernel, 4, sizeof(TYPE), &p2); CLERR;); (11, OpenCL, rc = clSetKernelArg(read_kernel, 1, sizeof(cl_ulong), &mc->mem_size); CLERR;); (3, OpenCL, rc = clSetKernelArg(write_kernel, 1, sizeof(cl_ulong), &mc->mem_size); CLERR;); (14, OpenCL, rc = clSetKernelArg(read_kernel, 6, sizeof(cl_mem), &mc->err_addr); CLERR;); (0, OpenCL, rc = clSetKernelArg(write_kernel, 0, sizeof(cl_mem), &mc->device_mem); CLERR;); (22, OpenCL, clReleaseKernel(write_kernel);); (7, OpenCL, rc = clSetKernelArg(read_kernel, 2, sizeof(unsigned int), &offset); CLERR;); (0, OpenCL, rc = clSetKernelArg(write_kernel, 3, sizeof(TYPE), &p1); CLERR;); (6, OpenCL, rc = clSetKernelArg(read_kernel, 3, sizeof(TYPE), &p1); CLERR;); (-3, OpenCL, rc = clSetKernelArg(write_kernel, 2, sizeof(unsigned int), &offset); CLERR;); (9, OpenCL, rc = clSetKernelArg(read_kernel, 8, sizeof(cl_mem), &mc->err_current); CLERR;); (7, OpenCL, rc = clSetKernelArg(read_kernel, 7, sizeof(cl_mem), &mc->err_expect); CLERR;)"}
{"predict": "(220, MPI, MPI_Reduce(&total, &total_sum, 1, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD);); (233, MPI, MPI_Finalize();); (85, MPI, MPI_Bcast(&btype,         1, MPI_INT, root, MPI_COMM_WORLD);); (24, MPI, MPI_Init(&argc,&argv);); (81, MPI, MPI_Bcast(&iterations,    1, MPI_INT, root, MPI_COMM_WORLD);); (98, MPI, MPI_Barrier(MPI_COMM_WORLD);); (23, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (19, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);); (76, MPI, MPI_Bcast(&vector_length, 1, MPI_INT, root, MPI_COMM_WORLD);)", "label": "(211, MPI, MPI_Reduce(&total, &total_sum, 1, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD);); (234, MPI, MPI_Finalize();); (83, MPI, MPI_Bcast(&iterations,    1, MPI_INT, root, MPI_COMM_WORLD);); (25, MPI, MPI_Init(&argc,&argv);); (80, MPI, MPI_Bcast(&vector_length, 1, MPI_INT, root, MPI_COMM_WORLD);); (81, MPI, MPI_Bcast(&btype,         1, MPI_INT, root, MPI_COMM_WORLD);); (97, MPI, MPI_Barrier(MPI_COMM_WORLD);); (23, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (21, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(24, MPI, err = MPI_Alltoall(recv_size, 1, MPI_COUNT, send_size, 1, MPI_COUNT, fd->comm);); (78, MPI, MPI_Waitall(nprocs_recv, send_req, &status);); (105, MPI, MPI_Allreduce(&tmp_len[i], &rtail, 1, MPI_COUNT, MPI_MAX, fd->comm);); (90, MPI, MPI_Allreduce(&sum, &stail, 1, MPI_COUNT, MPI_SUM, fd->comm);); (95, MPI, MPI_Type_size_x(buftype, &size);); (133, MPI, MPI_Type_size_x(buftype, &buftype_size);); (84, MPI, MPI_Barrier(fd->comm);); (124, MPI, MPI_Type_size_x(MPI_BYTE, &byte_size);); (90, MPI, MPI_Type_size_x(buftype, &buftype_extent);); (96, MPI, MPI_Type_size_x(MPI_BYTE, &byte_size);)", "label": "(121, MPI, gpfsmpio_prof_cw[GPFSMPIO_CIO_T_DEXCH_SORT] += MPI_Wtime() - io_time;); (90, MPI, gpfsmpio_prof_cw[GPFSMPIO_CIO_T_DEXCH_NET] += MPI_Wtime() - io_time;); (23, MPI, int ret = MPI_Alltoall(recv_size, 1, MPI_COUNT, send_size, 1, MPI_COUNT, fd->comm);); (19, MPI, io_time = MPI_Wtime();); (24, MPI, gpfsmpio_prof_cw[GPFSMPIO_CIO_T_DEXCH_RECV_EXCH] += MPI_Wtime() - io_time;); (132, MPI, gpfsmpio_prof_cw[GPFSMPIO_CIO_T_DEXCH_SIEVE] += MPI_Wtime() - io_time;); (75, MPI, gpfsmpio_prof_cw[GPFSMPIO_CIO_T_DEXCH_SETUP] += MPI_Wtime() - io_time;)"}
{"predict": "(7, CUDA, __shared__ float v[_N_], gy[_N_];)", "label": "(10, CUDA, __shared__ float v[_N_], gy[_N_];)"}
{"predict": "(12, CUDA, cudaDeviceSynchronize();); (12, CUDA, cudaMemcpy(&count,listp+fluidnum,sizeof(unsigned),cudaMemcpyDeviceToHost);); (8, CUDA, cudaMemset(listp+fluidnum,0,sizeof(unsigned));)", "label": "(14, CUDA, cudaDeviceSynchronize();); (12, CUDA, cudaMemcpy(&count,listp+fluidnum,sizeof(unsigned),cudaMemcpyDeviceToHost);); (6, CUDA, cudaMemset(listp+fluidnum,0,sizeof(unsigned));)"}
{"predict": "(6, CUDA, st = cudaMalloc(&test_ptr, TEST_ALLOC_SIZE);); (25, CUDA, cudaFree(test_ptr);)", "label": "(6, CUDA, st = cudaMalloc(&test_ptr, TEST_ALLOC_SIZE);); (25, CUDA, cudaFree(test_ptr);)"}
{"predict": "(27, MPI, MPI_Recv(b, 10, MPI_INT, i, 27 + j, MPI_COMM_WORLD, &status);); (16, MPI, MPI_Bsend(a, 10, MPI_INT, 0, 27 + j, MPI_COMM_WORLD);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)", "label": "(45, MPI, MPI_Buffer_detach(&bptr, &bl);); (25, MPI, MPI_Recv(b, 10, MPI_INT, i, 27 + j, MPI_COMM_WORLD, &status);); (8, MPI, MPI_Buffer_attach(buf, BUFSIZE);); (14, MPI, MPI_Wait(&request, &status);); (12, MPI, MPI_Ibsend(a, 10, MPI_INT, 0, 27 + j, MPI_COMM_WORLD, &request);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)"}
{"predict": "(67, OpenMP, #pragma omp for lastprivate(j)); (49, OpenMP, #pragma omp for lastprivate(argv[1])); (42, OpenMP, #pragma omp for lastprivate(a, b)); (57, OpenMP, #pragma omp for lastprivate(i)); (32, OpenMP, #pragma omp for lastprivate(conditional: s,argc) lastprivate(conditional:); (12, OpenMP, #pragma omp for lastprivate(argc); (64, OpenMP, #pragma omp parallel private(i)); (4, OpenMP, #pragma omp for lastprivate(); (2, OpenMP, #pragma omp for lastprivate); (45, OpenMP, #pragma omp for lastprivate(e, g)); (21, OpenMP, #pragma omp for lastprivate(argc > 0? argv[1] : argv[2])); (3, OpenMP, #pragma omp for lastprivate()); (23, OpenMP, #pragma omp for lastprivate(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (1, OpenMP, #pragma omp for lastprivate(argc,); (51, OpenMP, #pragma omp parallel shared(i)); (31, OpenMP, #pragma omp for lastprivate(S1)); (5, OpenMP, #pragma omp for lastprivate(argc)", "label": "(32, OpenMP, #pragma omp for lastprivate(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (73, OpenMP, #pragma omp parallel private(i)); (22, OpenMP, #pragma omp for lastprivate(argc,); (45, OpenMP, #pragma omp for lastprivate(a, b)); (12, OpenMP, #pragma omp for lastprivate()); (2, OpenMP, #pragma omp parallel); (69, OpenMP, #pragma omp for lastprivate(j)); (60, OpenMP, #pragma omp for lastprivate(i)); (4, OpenMP, #pragma omp for lastprivate(); (11, OpenMP, #pragma omp for lastprivate(argc); (30, OpenMP, #pragma omp for lastprivate(foo:argc)); (-3, OpenMP, #pragma omp for lastprivate); (32, OpenMP, #pragma omp for lastprivate(conditional: argc,s) lastprivate(conditional:); (15, OpenMP, #pragma omp for lastprivate(argc > 0? argv[1] : argv[2])); (46, OpenMP, #pragma omp for lastprivate(h)); (37, OpenMP, #pragma omp for lastprivate(argv[1])); (40, OpenMP, #pragma omp for lastprivate(k, e, g)); (56, OpenMP, #pragma omp parallel shared(i)); (18, OpenMP, #pragma omp for lastprivate(S1))"}
{"predict": "(29, CUDA, cudaCheck(cudaGetLastError());)", "label": "(30, CUDA, cudaCheck(cudaGetLastError());)"}
{"predict": "(8, MPI, MPI_Finalize();); (0, MPI, MPI_Init(&argc,&argv);)", "label": "(8, MPI, MPI_Finalize();); (0, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(54, CUDA, if (FD &&!FD->isTemplate()) SemaRef.addCudaUsedVar(Var);)", "label": "(25, CUDA, auto UserTarget = SemaRef.IdentifyCUDATarget(FD);); (23, CUDA, auto VarTarget = SemaRef.IdentifyCUDATarget(Var);)"}
{"predict": "(54, CUDA, CHECK(cudaFree(imgs_dev));)", "label": "(35, CUDA, CHECK_CUDA(cudaMalloc((void**)&imgs_dev, img_N * 3 * img_w * img_h * sizeof(uchar)));)"}
{"predict": "(15, MPI, MPI_Alloc_mem(sizeof(*data), MPI_INFO_NULL, &data);); (100, MPI, MPI_Win_unlock(0, win);); (109, MPI, MPI_Free_mem(data);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (74, MPI, MPI_Win_fence(MPI_MODE_NOPRECEDE, win);); (114, MPI, MPI_Win_free(&win);); (43, MPI, MPI_Accumulate(&mine, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_MINLOC, win);); (33, MPI, MPI_Accumulate(&mine, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_MAXLOC, win);); (4, MPI, MPI_Win_create(data, sizeof(*data), sizeof(*data), MPI_INFO_NULL, MPI_COMM_WORLD, &win);); (4, MPI, MPI_Win_fence(0, win);); (8, MPI, MPI_Barrier(MPI_COMM_WORLD);); (57, MPI, MPI_Win_lock(MPI_LOCK_SHARED, 0, MPI_MODE_NOCHECK, win);)", "label": "(119, MPI, MPI_Free_mem(data);); (30, MPI, MPI_Win_fence(0, win);); (73, MPI, MPI_Win_fence(MPI_MODE_NOSUCCEED, win);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (12, MPI, MPI_Win_fence(MPI_MODE_NOPRECEDE, win);); (110, MPI, MPI_Win_free(&win);); (28, MPI, MPI_Accumulate(&mine, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_MAXLOC, win);); (5, MPI, MPI_Alloc_mem(sizeof(twoint_t), MPI_INFO_NULL, &data);); (71, MPI, MPI_Win_lock(MPI_LOCK_SHARED, 0, MPI_MODE_NOCHECK, win);); (72, MPI, MPI_Win_unlock(0, win);); (73, MPI, MPI_Barrier(MPI_COMM_WORLD);); (4, MPI, MPI_Win_create(data, me == 0? sizeof(twoint_t) : 0, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &win);); (41, MPI, MPI_Accumulate(&mine, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_MINLOC, win);)"}
{"predict": "(26, MPI, MPI_Send(&message, 1, MPI_INT, next, tag, MPI_COMM_WORLD);); (5, MPI, MPI_Init(&argc, &argv);); (62, MPI, MPI_Finalize();); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (36, MPI, MPI_Recv(&message, 1, MPI_INT, prev, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (44, MPI, MPI_Send(&message, 1, MPI_INT, 0, tag, MPI_COMM_WORLD);)", "label": "(6, MPI, MPI_Init(&argc, &argv);); (25, MPI, MPI_Send(&message, 1, MPI_INT, next, tag, MPI_COMM_WORLD);); (60, MPI, MPI_Finalize();); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (34, MPI, MPI_Recv(&message, 1, MPI_INT, prev, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)"}
{"predict": "(11, CUDA, cudaMalloc(&ptr, plan_size * sizeof(int32_t));); (17, CUDA, cudaDeviceSynchronize();)", "label": "(12, CUDA, CUCH(cudaMalloc(&ptr, plan_size * sizeof(int32_t)), mhcudaMemoryAllocationFailure);); (6, CUDA, CUCH(cudaSetDevice(devs[devi]), mhcudaNoSuchDevice);)"}
{"predict": "(9, MPI, MPI_Comm_rank(comm, &comm_rank);); (10, MPI, MPI_Comm_size(comm, &comm_size);)", "label": "(9, MPI, MPI_Comm_rank(comm, &comm_rank);); (10, MPI, MPI_Comm_size(comm, &comm_size);)"}
{"predict": "(40, SYCL, prk::SYCL::free_device_mem(d_A);); (52, SYCL, auto d_C = sycl::malloc_device<double>(bytes, q);); (39, SYCL, prk::SYCL::free_device_mem(d_B);); (82, SYCL, sycl::free(d_A, q);); (83, SYCL, sycl::free(d_B, q);); (83, SYCL, sycl::free(d_C, q);); (27, SYCL, prk::SYCL::print_device_platform(q);); (34, SYCL, auto d_B = sycl::malloc_device<double>(bytes, q);); (32, SYCL, auto d_A = sycl::malloc_device<double>(bytes, q);); (77, SYCL, sycl::free(d_A, q);); (78, SYCL, sycl::free(d_C, q);); (21, SYCL, prk::SYCL::queue q(sycl::default_selector_v);)", "label": "(80, SYCL, sycl::free(d_A, q);); (32, SYCL, prk::SYCL::print_device_platform(q);); (51, SYCL, double *d_C = sycl::malloc_device<double>(length, q);); (76, SYCL, sycl::free(d_B, q);); (74, SYCL, sycl::free(d_C, q);); (47, SYCL, double *d_B = sycl::malloc_device<double>(length, q);); (26, SYCL, sycl::queue q(sycl::default_selector_v);); (44, SYCL, double *d_A = sycl::malloc_device<double>(length, q);)"}
{"predict": "(68, CUDA, CUDA_CHECK(cudaStreamSynchronize(mono->get_stream()));); (123, CUDA, CUDA_CHECK(cudaStreamCreate(&preprocess_stream));); (150, CUDA, CUDA_CHECK(cudaStreamDestroy(preprocess_stream));); (142, CUDA, CUDA_CHECK(cudaMemcpyAsync(image_device, image.data, image.rows * image.cols * 3, cudaMemcpyHostToDevice, preprocess_stream));); (132, CUDA, CUDA_CHECK(cudaMalloc(&image_device, image.rows * image.cols * 3));); (139, CUDA, CUDA_CHECK(cudaMemcpyAsync(image_device, image.data, image.rows * image.cols * 3, cudaMemcpyHostToDevice, stream_));); (151, CUDA, CUDA_CHECK(cudaFree(image_device));); (120, CUDA, CUDA_CHECK(cudaMalloc(&tensor->data(), image.rows * image.cols * 3));); (124, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream_));)", "label": "(151, CUDA, checkCudaRuntime(cudaMemcpyAsync(image_device, image_host, size_image, cudaMemcpyHostToDevice, preprocess_stream));); (125, CUDA, checkCudaRuntime(cudaStreamCreate(&preprocess_stream));); (67, CUDA, checkCudaRuntime(cudaStreamSynchronize(mono->get_stream()));)"}
{"predict": "(75, CUDA, EXPECT_EQ(cudaDeviceSynchronize(), cudaSuccess);); (83, CUDA, EXPECT_EQ(cudaGetLastError(), cudaSuccess);); (90, CUDA, EXPECT_EQ(cudaGetLastError(), cudaSuccess);)", "label": "(114, CUDA, EXPECT_EQ(cudaMemset(block2Ptr->data(), 0, blockSize * sizeof(T)), cudaSuccess);); (74, CUDA, EXPECT_EQ(cudaDeviceSynchronize(), cudaSuccess);)"}
{"predict": "(42, CUDA, cudaFree(gpuFloats3);); (24, CUDA, cudaMemcpy(hostFloats2, gpuFloats2, 4 * sizeof(float), cudaMemcpyDeviceToHost);); (24, CUDA, cudaMemcpy(hostFloats3, gpuFloats3, 4 * sizeof(float), cudaMemcpyDeviceToHost);); (4, CUDA, cudaMalloc((void**)(&gpuFloats2), N * sizeof(float));); (34, CUDA, cudaFree(gpuFloats1);); (18, CUDA, cudaMemcpy(hostFloats1, gpuFloats1, 4 * sizeof(float), cudaMemcpyDeviceToHost);); (3, CUDA, cudaMalloc((void**)(&gpuFloats3), N * sizeof(float));); (36, CUDA, cudaFree(gpuFloats2);); (-4, CUDA, cudaMalloc((void**)(&gpuFloats1), N * sizeof(float));)", "label": "(25, CUDA, cudaMemcpy(hostFloats2, gpuFloats2, 4 * sizeof(float), cudaMemcpyDeviceToHost);); (37, CUDA, cudaFree(gpuFloats1);); (8, CUDA, cudaMalloc((void**)(&gpuFloats2), N * sizeof(float));); (21, CUDA, cudaMemcpy(hostFloats1, gpuFloats1, 4 * sizeof(float), cudaMemcpyDeviceToHost);); (3, CUDA, cudaMalloc((void**)(&gpuFloats1), N * sizeof(float));); (35, CUDA, cudaFree(gpuFloats3);); (20, CUDA, cudaMemcpy(hostFloats3, gpuFloats3, 4 * sizeof(float), cudaMemcpyDeviceToHost);); (32, CUDA, cudaFree(gpuFloats2);); (5, CUDA, cudaMalloc((void**)(&gpuFloats3), N * sizeof(float));)"}
{"predict": "(28, CUDA, CUDASimulation cudaSimulation(model);)", "label": "(28, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(7, CUDA, __shared__ cuda::atomic<int> barrier;); (9, CUDA, __shared__ double partialResults[32];)", "label": "(9, CUDA, __shared__ cuda::barrier<cuda::thread_scope_block> barrier;)"}
{"predict": "(6, CUDA, auto points_geometry_it = thrust::make_counting_iterator(0);); (8, CUDA, auto linestring_geometry_it  = thrust::make_counting_iterator(0);)", "label": "(6, CUDA, auto points_geometry_it = thrust::make_counting_iterator(0);); (8, CUDA, auto linestring_geometry_it  = thrust::make_counting_iterator(0);)"}
{"predict": "(130, OpenMP, #pragma omp target teams map(alloc: u21k, u31k, u41k, u51k) num_teams(num_workers2)); (25, OpenMP, #pragma omp distribute parallel for collapse(3)); (18, OpenMP, #pragma omp target teams map (alloc: rsd, rho_i, qs, u) num_teams(nz)); (105, OpenMP, #pragma omp distribute parallel for private(k,j,i,tmp,u21i,u31i,u41i,u51i,u21im1,u31im1,u41im1,u51im1,u21jm1,u31jm1,u41jm1,u51jm1) collapse(2)); (108, OpenMP, #pragma omp target teams map(alloc: u21i, u31i, u41i, u51i, u21im1, u31im1, u41im1, u51im1, u21jm1, u31jm1, u41jm1, u51jm1) num_teams(num_workers3)); (20, OpenMP, #pragma omp target teams map (alloc: rsd, rho_i, qs, u) thread_limit(128)); (103, OpenMP, #pragma omp target teams map(alloc: u21j, u31j, u41j, u51j) num_teams(num_workers3)); (135, OpenMP, #pragma omp target teams map(alloc: u21i, u31i, u41i, u51i) num_teams(num_workers3)); (104, OpenMP, #pragma omp target teams map(alloc: u21j, u31j, u41j, u51j) thread_limit(num_workers3)); (128, OpenMP, #pragma omp target teams map(alloc: u21k, u31k, u41k, u51k) thread_limit(128)); (6, OpenMP, #pragma", "label": "(569, OpenMP, #pragma omp teams distribute parallel for collapse(3) private(i,j,k) num_teams(jend - jst)); (26, OpenMP, #pragma omp distribute parallel for collapse (3) private(tmp)); (106, OpenMP, #pragma omp simd simdlen(32) private (tmp, u21i, u31i, u41i, u51i, u21im1, u31im1, u41im1, u51im1)); (21, OpenMP, #pragma omp target teams); (15, OpenMP, #pragma omp target data); (59, OpenMP, #pragma omp distribute)"}
{"predict": "(11, CUDA, extern __shared__ char shared_mem[];)", "label": "(11, CUDA, extern __shared__ char shared_mem[];)"}
{"predict": "(58, CUDA, CHECK_CUDA(cudaFree(ibuf[i].pixel_data[c]));); (52, CUDA, CHECK_CUDA(cudaMalloc(&ibuf[i].pixel_data[c], sz));)", "label": "(59, CUDA, CHECK_CUDA(cudaMalloc((void**)&ibuf[i].pixel_data[c], sz));); (56, CUDA, CHECK_CUDA(cudaFree(ibuf[i].pixel_data[c]));)"}
{"predict": "(93, MPI, ascent_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (98, MPI, MPI_Barrier(comm);); (16, MPI, MPI_Comm_rank(comm, &par_rank);); (16, MPI, MPI_Comm_size(comm, &par_size);)", "label": "(93, MPI, ascent_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (99, MPI, MPI_Barrier(comm);); (16, MPI, MPI_Comm_rank(comm, &par_rank);); (16, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(10, OpenMP, f = test_choleskyc_decomp_unit(p3);)", "label": "(10, OpenMP, f = test_choleskyc_decomp_dim(&p3.matrix, 2 * 8.0 * GSL_DBL_EPSILON);)"}
{"predict": "(206, SYCL, sycl::free(A, sycl::memory_space::global);); (213, SYCL, sycl::free(C, sycl::memory_space::global);); (205, SYCL, sycl::free(B, sycl::memory_space::global);); (209, SYCL, sycl::free(sycl_work_buffer, sycl::memory_space::device);); (214, SYCL, sycl::free(sycl_devices[0], keep_ownership);); (205, SYCL, sycl::free(sycl_work_buffer, sycl::memory_space::global);); (132, SYCL, sycl::buffer<float, 1> sycl_work_buffer(sycl::range<1>(work_size), sycl::mem_use_device_ptr);); (124, SYCL, sycl::queue sycl_queue;); (124, SYCL, std::vector<sycl::device> sycl_devices(1);); (176, SYCL, sycl::free(sycl_work_buffer, keep_ownership);); (170, SYCL, sycl::free(B, keep_ownership);); (168, SYCL, sycl::free(A, keep_ownership);); (169, SYCL, sycl::free(C, keep_ownership);); (124, SYCL, sycl::buffer<float, 1> sycl_work_buffer(sycl::range<1>(work_size));); (115, SYCL, sycl::context sycl_context;); (163, SYCL, sycl::free(sycl_devices[0], sycl::memory_space::device);); (160, SYCL, sycl::free(sycl_queue, sycl::memory_space::device);); (108, SYCL, sycl::platform sycl_platform;); (161, SYCL, sycl::free(sycl_devices[0], sycl::memory_space::global);); (115, SYCL, sycl::device sycl_device;); (16", "label": "(54, SYCL, std::vector<sycl::device> sycl_devices(1);); (3, SYCL, sycl::queue sycl_queue;)"}
{"predict": "(2, TBB, tbb::enumerable_thread_specific<size_t> num_high_degree_nodes(0);)", "label": "(15, TBB, tbb::enumerable_thread_specific<size_t> num_high_degree_nodes(0);)"}
{"predict": "(163, OpenMP, ism303dac_mg_offset_temp_comp_set(&dev_ctx_mg, PROPERTY_ENABLE);)", "label": "(162, OpenMP, ism303dac_mg_offset_temp_comp_set(&dev_ctx_mg, PROPERTY_ENABLE);)"}
{"predict": "(3, OpenMP, #pragma omp parallel); (4, OpenMP, #pragma omp master taskloop simd)", "label": "(4, OpenMP, #pragma omp master taskloop simd); (2, OpenMP, #pragma omp parallel)"}
{"predict": "(8, MPI, MPI_Info_get(info, key, MPI_MAX_INFO_VAL, buf, &flag);)", "label": "(7, MPI, MPI_Info_get(info, key, MPI_MAX_INFO_VAL, buf, &flag);)"}
{"predict": "(36, CUDA, __shared__ _VOLATILE_ float bSmem[16][128+4];); (34, CUDA, __shared__ _VOLATILE_ float aSmem[16][128+4];)", "label": "(36, CUDA, __shared__ _VOLATILE_ float bSmem[16][128+4];); (34, CUDA, __shared__ _VOLATILE_ float aSmem[16][128+4];)"}
{"predict": "(58, OpenMP, #pragma omp teams distribute parallel for simd private(i)); (32, OpenMP, #pragma omp teams distribute parallel for simd private (argc)); (37, OpenMP, #pragma omp teams distribute parallel for simd private (S1)); (50, OpenMP, #pragma omp teams distribute parallel for simd private(h, B::x)); (6, OpenMP, #pragma omp teams distribute parallel for simd private (); (27, OpenMP, #pragma omp teams distribute parallel for simd private (argc > 0? argv[1] : argv[2])); (38, OpenMP, #pragma omp teams distribute parallel for simd private(ca)); (43, OpenMP, #pragma omp teams distribute parallel for simd shared(i)); (48, OpenMP, #pragma omp teams distribute parallel for simd private(e, g)); (57, OpenMP, #pragma omp teams distribute parallel for simd private(j)); (4, OpenMP, #pragma omp teams distribute parallel for simd private ()); (12, OpenMP, #pragma omp teams distribute parallel for simd private (argc,); (33, OpenMP, #pragma omp teams distribute parallel for simd private(da)); (50, OpenMP, #pragma omp teams distribute parallel for simd private(i)); (3, OpenMP, #pragma omp teams distribute parallel for simd private (argc); (14, OpenMP, #pragma omp teams distribute parallel for simd private (argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (22, OpenMP, #pragma omp teams distribute parallel for simd private(ba, z)); (4, OpenMP, #pragma omp teams distribute parallel for simd private (argc,)", "label": "(11, OpenMP, #pragma omp teams distribute simd shared (); (43, OpenMP, #pragma omp teams distribute simd shared(da)); (30, OpenMP, #pragma omp teams distribute simd shared (a, b, c, d, f)); (5, OpenMP, #pragma omp teams distribute simd shared); (34, OpenMP, #pragma omp teams distribute simd shared(ba)); (2, OpenMP, #pragma omp target); (35, OpenMP, #pragma omp teams distribute simd shared(ca)); (55, OpenMP, #pragma omp teams distribute simd shared(i)); (57, OpenMP, #pragma omp teams distribute simd shared(j)); (17, OpenMP, #pragma omp teams distribute simd shared (argc)); (43, OpenMP, #pragma omp teams distribute simd private(i), shared(i)); (24, OpenMP, #pragma omp teams distribute simd shared (argv[1])); (17, OpenMP, #pragma omp teams distribute simd shared (S1)); (43, OpenMP, #pragma omp teams distribute simd firstprivate(i), shared(i)); (9, OpenMP, #pragma omp teams distribute simd shared (argc > 0? argv[1] : argv[2])); (35, OpenMP, #pragma omp teams distribute simd shared(h, B::x)); (43, OpenMP, #pragma omp teams distribute simd private(i)); (51, OpenMP, #pragma omp teams distribute simd firstprivate(i)); (2, OpenMP, #pragma omp teams distribute simd shared (argc,); (28, OpenMP, #pragma omp teams distribute simd shared(e, g, z)); (-6, OpenMP, #pragma omp teams distribute simd shared ()); (-4, OpenMP, #pragma omp teams distribute simd shared (argc)"}
{"predict": "(258, MPI, MPI_Finalize();); (25, MPI, MPI_Init(&argc, &argv);); (25, MPI, MPI_Comm_size(MPI_COMM_WORLD, &num_procs);); (25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myid);)", "label": "(25, MPI, MPI_Init(&argc, &argv);); (79, MPI, MPI_Finalize();); (25, MPI, MPI_Comm_size(MPI_COMM_WORLD, &num_procs);); (23, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myid);)"}
{"predict": "(1, TBB, using namespace oneapi::tbb::flow;)", "label": "(1, TBB, using namespace oneapi::tbb::flow;)"}
{"predict": "(4, CUDA, __shared__ float homo[TESTHOMO_LOOPS*8];); (2, CUDA, __shared__ int cnts[TESTHOMO_TESTS*TESTHOMO_LOOPS];)", "label": "(3, CUDA, __shared__ float homo[8*TESTHOMO_LOOPS];); (3, CUDA, __shared__ int cnts[TESTHOMO_TESTS*TESTHOMO_LOOPS];)"}
{"predict": "(57, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(ca)); (7, OpenMP, #pragma omp teams distribute parallel for simd firstprivate); (77, OpenMP, #pragma omp teams distribute parallel for simd lastprivate(argc), firstprivate(argc)); (60, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(S2::S2s)); (77, OpenMP, #pragma omp teams distribute parallel for simd private(i), firstprivate(i)); (65, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(h)); (50, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(ba)); (6, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(); (35, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(argv[1])); (3, OpenMP, #pragma omp teams distribute parallel for simd firstprivate()); (38, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(2 * 2)); (51, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(S2::S2sc)); (10, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(argc,); (6, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(argc); (73, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(j)); (24, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(a, b, c, d, f)); (17, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(S1)); (43, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(e, g)); (27, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(da)); (2, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(argc > 0? argv[1] : argv[2])); (6, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_", "label": "(34, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (85, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(j)); (7, OpenMP, #pragma omp target); (59, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(S2::S2s)); (10, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (); (21, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argc,); (36, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (a, b, c, d, f)); (47, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(ca)); (66, OpenMP, #pragma omp teams distribute parallel for simd private(i), firstprivate(i)); (13, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argc); (40, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(ba, z)); (59, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(h)); (26, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (S1)); (33, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argv[1])); (52, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(S2::S2sc)); (63, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(i)); (2, OpenMP, #pragma omp teams distribute parallel for simd firstprivate ()); (-7, OpenMP, #pragma omp teams distribute parallel for simd firstprivate); (40, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(da)); (72, OpenMP, #pragma omp teams distribute parallel for simd lastprivate(argc), firstprivate(argc)); (10, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argc > 0? argv[1] : argv[2]))"}
{"predict": "(47, MPI, MPI_Free_mem(base_ptr[i]);); (50, MPI, MPI_Comm_free(&node);); (1, MPI, MPI_Init(&argc, &argv);); (6, MPI, MPI_Comm_rank(node, &rank);); (43, MPI, MPI_Win_free(&win[i]);); (44, MPI, MPI_Finalize();); (17, MPI, MPI_Barrier(node);); (34, MPI, MPI_Win_shared_query(win[i], 0, &size[i], &a[i], &ptr[i]);); (0, MPI, MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, rank, MPI_INFO_NULL, &node);); (13, MPI, MPI_Alloc_shared((rank==0?99:0)*sizeof(char), 1, MPI_INFO_NULL, node, &base_ptr[i], &s[i]);)", "label": "(25, MPI, s[i] = MPI_Win_allocate_shared((rank==0?99:0)*sizeof(char), 1, MPI_INFO_NULL, node, &base_ptr[i], &win[i]);); (36, MPI, MPI_Barrier(node);); (-1, MPI, MPI_Init(&argc, &argv);); (44, MPI, if(win[i]!= MPI_WIN_NULL) MPI_Win_free(&win[i]);); (46, MPI, MPI_Finalize();); (2, MPI, MPI_Comm_rank(node, &rank);); (16, MPI, int sam = MPI_Alloc_mem((rank==0?99:0)*sizeof(char), MPI_INFO_NULL, &base_ptr[i]);); (25, MPI, MPI_Win_shared_query(win[i], 0, &size[i], &a[i], &ptr[i]);); (-5, MPI, int ss = MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, 0, MPI_INFO_NULL, &node);)"}
{"predict": "(20, MPI, tim::settings::mpi_init()       = false;); (20, MPI, tim::settings::mpi_finalize()   = false;)", "label": "(20, MPI, tim::settings::mpi_init()       = false;); (20, MPI, tim::settings::mpi_finalize()   = false;)"}
{"predict": "(46, SYCL, sycl::free(cd, queue);)", "label": "(9, SYCL, const bool doHalfs = dev.has(sycl::aspect::fp16);); (47, SYCL, sycl::free(cd, queue);); (12, SYCL, const bool doDoubles = dev.has(sycl::aspect::fp64);)"}
{"predict": "(43, CUDA, __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];); (37, CUDA, __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];)", "label": "(42, CUDA, __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];); (37, CUDA, __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];)"}
{"predict": "(9, CUDA, cudaStreamSynchronize(LuH->A_gpu.cuStreams[0]);)", "label": "(8, CUDA, cudaStreamSynchronize(LuH->A_gpu.cuStreams[0]);)"}
{"predict": "(10, OpenMP, #pragma omp target parallel if (argc > 0? argv[1] : argv[2])); (18, OpenMP, #pragma omp target parallel if(parallel : argc); (10, OpenMP, #pragma omp target parallel if (argc argc)); (17, OpenMP, #pragma omp target parallel if(parallel : argc) if (argc)); (3, OpenMP, #pragma omp target parallel if (argc); (0, OpenMP, #pragma omp target parallel if ()); (12, OpenMP, #pragma omp target parallel if(parallel :); (0, OpenMP, #pragma omp target parallel if (argc))); (15, OpenMP, #pragma omp target parallel if(parallel : argc) if (parallel:argc)); (5, OpenMP, #pragma omp target parallel if (S1)); (0, OpenMP, #pragma omp target parallel if (argc))); (6, OpenMP, #pragma omp target parallel if(parallel); (1, OpenMP, #pragma omp target parallel if (argc + z)); (-5, OpenMP, #pragma omp target parallel if (argc)", "label": "(8, OpenMP, #pragma omp target teams device (argc); (5, OpenMP, #pragma omp target teams device ()); (10, OpenMP, #pragma omp target teams device (argc > 0? argv[1] : argv[2])); (15, OpenMP, #pragma omp target teams device (S1)); (12, OpenMP, #pragma omp target teams device (argc), device (argc+1)); (15, OpenMP, #pragma omp target teams device (-2)); (8, OpenMP, #pragma omp target teams device (argc + argc + z)); (15, OpenMP, #pragma omp target teams device (-10u)); (2, OpenMP, #pragma omp target teams device (argc))); (-7, OpenMP, #pragma omp target teams device); (14, OpenMP, #pragma omp target teams device (3.14)); (-7, OpenMP, #pragma omp target teams device ()"}
{"predict": "(31, CUDA, checkCudaErrors(cudaMallocHost(&b, N_ROWS * sizeof(double)));); (31, CUDA, checkCudaErrors(cudaMallocHost(&A, N_ROWS * N_ROWS * sizeof(float)));); (97, CUDA, checkCudaErrors(cudaFreeHost(b));); (97, CUDA, checkCudaErrors(cudaFreeHost(A));); (95, CUDA, checkCudaErrors(cudaFree(d_A));); (95, CUDA, checkCudaErrors(cudaFree(d_b));); (95, CUDA, checkCudaErrors(cudaFree(d_x));); (23, CUDA, int dev = findCudaDevice(argc, (const char **)argv);); (62, CUDA, checkCudaErrors(cudaMalloc(&d_A, sizeof(float) * N_ROWS * N_ROWS));); (64, CUDA, checkCudaErrors(cudaMalloc(&d_x_new, sizeof(double) * N_ROWS));); (91, CUDA, checkCudaErrors(cudaFree(d_x_new));); (59, CUDA, checkCudaErrors(cudaMalloc(&d_b, sizeof(double) * N_ROWS));); (57, CUDA, checkCudaErrors(cudaMalloc(&d_x, sizeof(double) * N_ROWS));); (90, CUDA, checkCudaErrors(cudaFree(d_b));); (90, CUDA, checkCudaErrors(cudaFree(d_x));); (59, CUDA, checkCudaErrors(cudaStreamCreateWithFlags(&stream1, cudaStreamNonBlocking));); (86, CUDA, checkCudaErrors(cudaFreeHost(x));); (47, CUDA, checkCudaErrors(cudaMallocHost(&x, N_ROWS * sizeof(double)));); (80, CUDA, checkCudaErrors(cudaStreamSynchronize(stream1));); (85, CUDA, checkCudaErrors(cudaFreeHost(b));); (85, CUDA, checkCudaErrors(cudaFreeHost(A));)", "label": "(32, CUDA, checkCudaErrors(cudaMallocHost(&b, N_ROWS * sizeof(double)));); (27, CUDA, int dev = findCudaDevice(argc, (const char **)argv);); (67, CUDA, checkCudaErrors(cudaMalloc(&d_A, sizeof(float) * N_ROWS * N_ROWS));); (99, CUDA, checkCudaErrors(cudaFree(d_x_new));); (101, CUDA, checkCudaErrors(cudaFreeHost(b));); (95, CUDA, checkCudaErrors(cudaFree(d_A));); (64, CUDA, checkCudaErrors(cudaMalloc(&d_x, sizeof(double) * N_ROWS));); (92, CUDA, checkCudaErrors(cudaFree(d_b));); (93, CUDA, checkCudaErrors(cudaFree(d_x));); (59, CUDA, checkCudaErrors(cudaMalloc(&d_b, sizeof(double) * N_ROWS));); (57, CUDA, checkCudaErrors(cudaStreamCreateWithFlags(&stream1, cudaStreamNonBlocking));); (60, CUDA, checkCudaErrors(cudaMalloc(&d_x_new, sizeof(double) * N_ROWS));); (61, CUDA, checkCudaErrors(cudaMemsetAsync(d_x, 0, sizeof(double) * N_ROWS, stream1));); (21, CUDA, checkCudaErrors(cudaMallocHost(&A, N_ROWS * N_ROWS * sizeof(float)));); (90, CUDA, checkCudaErrors(cudaFreeHost(A));)"}
{"predict": "(42, OpenMP, #pragma omp single private(m)); (30, OpenMP, #pragma omp single private(i)); (36, OpenMP, #pragma omp parallel private(i)); (18, OpenMP, #pragma omp single private(S1)); (13, OpenMP, #pragma omp single private(argc)); (30, OpenMP, #pragma omp parallel); (22, OpenMP, #pragma omp single private(h, B::x)); (17, OpenMP, #pragma omp single private(a, b)); (12, OpenMP, #pragma omp single private(argc > 0? argv[1] : argv[2])); (25, OpenMP, #pragma omp single private(j)); (1, OpenMP, #pragma omp single private(); (3, OpenMP, #pragma omp single private(argc); (27, OpenMP, #pragma omp single private(i)); (21, OpenMP, #pragma omp single shared(i)); (2, OpenMP, #pragma omp single private(argc,); (26, OpenMP, #pragma omp parallel shared(i)); (4, OpenMP, #pragma omp single private(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (-1, OpenMP, #pragma omp single private(argc)", "label": "(11, OpenMP, #pragma omp single private()); (39, OpenMP, #pragma omp parallel private(i)); (21, OpenMP, #pragma omp single private(a, b)); (16, OpenMP, #pragma omp single private(argc)); (23, OpenMP, #pragma omp single private(e, g)); (2, OpenMP, #pragma omp single private); (19, OpenMP, #pragma omp single private(argv[1])); (26, OpenMP, #pragma omp parallel); (9, OpenMP, #pragma omp single private(argc > 0? argv[1] : argv[2])); (20, OpenMP, #pragma omp single private(h, B::x)); (11, OpenMP, #pragma omp single private(S1)); (2, OpenMP, #pragma omp single private(argc); (34, OpenMP, #pragma omp single private(m)); (2, OpenMP, #pragma omp single private(argc,); (27, OpenMP, #pragma omp single private(j)); (16, OpenMP, #pragma omp single shared(i)); (-7, OpenMP, #pragma omp single private(); (22, OpenMP, #pragma omp parallel shared(i)); (18, OpenMP, #pragma omp single private(i))"}
{"predict": "(89, MPI, MPI_Allreduce( &problem, &any_problem, 1, MPI_INT, MPI_LOR, smpi->world() );); (133, MPI, MPI_Barrier( smpi->world() );)", "label": "(96, MPI, MPI_Finalize();); (88, MPI, MPI_Allreduce( &problem, &any_problem, 1, MPI_INT, MPI_LOR, smpi->world() );)"}
{"predict": "(47, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(rh)); (55, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(da)); (35, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(raa)); (40, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(h)); (26, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(aa)); (17, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(k)); (28, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(z))", "label": "(37, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(raa)); (48, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(rh)); (41, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(h)); (28, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(aa)); (51, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(da)); (20, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(z)); (13, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(k))"}
{"predict": "(34, MPI, MPI_Barrier(MPI_COMM_WORLD);); (32, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(34, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(24, MPI, MPI_Send(unzval, lenv, MPI_DOUBLE, receiver, k, grid3d->zscp.comm);)", "label": "(24, MPI, MPI_Send(unzval, lenv, MPI_DOUBLE, receiver, k, grid3d->zscp.comm);)"}
{"predict": "(4, CUDA, __shared__ float4 buffer2[M7H*NUM];); (2, CUDA, __shared__ float4 buffer1[M7W*NDIM/4];)", "label": "(5, CUDA, __shared__ float4 buffer2[M7H*NUM];); (3, CUDA, __shared__ float4 buffer1[M7W*NDIM/4];)"}
{"predict": "(13, CUDA, CUDA_CHECK(cudaGetDeviceCount(&ndevices));); (11, CUDA, CUDA_CHECK(cudaSetDevice(rank % ndevices));); (93, CUDA, CUDA_CHECK(cudaMalloc(&dst_device, sizeof(int) * dst_host_expected.size()));); (91, CUDA, CUDA_CHECK(cudaMalloc(&src_device, sizeof(int) * src_host.size()));); (140, CUDA, CUDA_CHECK(cudaFree(src_device));); (138, CUDA, CUDA_CHECK(cudaFree(dst_device));); (134, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (4, MPI, MPI_Abort(comm, 1);); (130, CUDA, CUDA_CHECK(cudaFree(work));); (5, MPI, MPI_Comm_rank(comm, &rank);); (125, CUDA, CUDA_CHECK(cudaFreeHost(src_host_device));); (125, CUDA, CUDA_CHECK(cudaFreeHost(dst_host_device));); (6, MPI, MPI_Comm_size(comm, &size);); (13, CUDA, CUDA_CHECK(cudaMallocHost(&src_host_device, sizeof(int) * src_host.size()));); (11, CUDA, CUDA_CHECK(cudaMallocHost(&dst_host_device, sizeof(int) * dst_host_expected.size()));); (69, CUDA, CUDA_CHECK(cudaStreamCreate(&stream));); (85, CUDA, CUDA_CHECK(cudaMalloc(&work, workspace));); (113, MPI, MPI_Barrier(comm);); (12, CUDA, CUDA_CHECK(cudaMemcpy(src_device, src_host.data(), sizeof(int) * src_host.size(), cudaMemcpyHostToDevice));); (122, MPI, MPI_Finalize();); (10, CUDA, CUDA_CHECK(cudaMemcpy(dst_device, dst_host_expected.data(), sizeof(int) * dst_host_expected.size(), cudaMemcpyHostToDevice));); (120, CUDA, CUDA_CHECK(cudaFree(dst_device));); (117, CUDA, CUDA_CHECK(cudaFree(work));", "label": "(15, CUDA, CUDA_CHECK(cudaGetDeviceCount(&ndevices));); (77, CUDA, CUDA_CHECK(cudaStreamCreate(&stream));); (5, MPI, MPI_Comm_size(comm, &size);); (119, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (12, CUDA, CUDA_CHECK(cudaSetDevice(rank % ndevices));); (-3, MPI, MPI_Init(&argc, &argv);); (90, MPI, MPI_Allreduce(&my_io_max, &global_io_max, 1, MPI_UNSIGNED_LONG_LONG, MPI_MAX, comm);); (116, CUDA, CUDA_CHECK(cudaMemcpy(dst_host.data(), dst, dst_host_expected.size() * sizeof(int), cudaMemcpyDefault));); (139, MPI, MPI_Finalize();); (118, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (94, CUDA, CUDA_CHECK(cudaMemsetAsync(dst, 0, dst_host_expected.size(), stream));); (92, CUDA, CUDA_CHECK(cudaMemcpyAsync(src, src_host.data(), src_host.size() * sizeof(int), cudaMemcpyDefault, stream));); (-6, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(21, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(exe_node), *receivers[r] );); (63, TBB, inc_functor body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (59, TBB, tbb::flow::remove_edge( tbb::flow::output_port<0>(exe_node), *receivers[r] );); (28, TBB, tbb::flow::make_edge( *senders.back(), exe_node );); (47, TBB, tbb::flow::remove_edge( *senders.back(), exe_node );); (7, TBB, tbb::flow::multifunction_node< InputType, OutputType, tbb::flow::rejecting > exe_node( g, lc, cf );); (63, TBB, body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (-3, TBB, tbb::flow::graph g;); (1, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(exe_node), *receivers[0] );)", "label": "(66, TBB, g.reset(tbb::flow::rf_reset_bodies);); (20, TBB, tbb::flow::make_edge( exe_node, *receivers[r] );); (58, TBB, inc_functor body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (64, TBB, body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (1, TBB, tbb::flow::graph g;); (45, TBB, tbb::flow::remove_edge( exe_node, *receivers[r] );); (24, TBB, tbb::flow::make_edge( *senders.back(), exe_node );); (4, TBB, tbb::flow::function_node< InputType, OutputType > exe_node( g, lc, cf );)"}
{"predict": "(23, MPI, MPI_Comm_split(expandedPCU->GetMPIComm(), code.group, code.rank, &groupComm);); (35, MPI, MPI_Comm_free(&groupComm);)", "label": "(23, MPI, MPI_Comm_split(expandedPCU->GetMPIComm(), group, groupRank, &groupComm);); (31, MPI, MPI_Comm_free(&groupComm);)"}
{"predict": "(24, CUDA, cudaFree(du_new_dev);); (18, CUDA, CudaCheckError();); (16, CUDA, HANDLE_ERROR(cudaGetLastError())", "label": "(19, CUDA, CudaCheckError();); (23, CUDA, cudaFree(du_new_dev);)"}
{"predict": "(87, OpenCL, clReleaseMemObject(bufX);); (43, OpenCL, clReleaseCommandQueue(queue);); (27, OpenCL, printf( \"clCreateContext() failed with %d\\n\", err );); (85, OpenCL, clReleaseMemObject(bufA);); (79, OpenCL, clReleaseEvent(event);); (80, OpenCL, clReleaseMemObject(bufY);); (30, OpenCL, printf( \"clCreateCommandQueue() failed with %d\\n\", err );); (27, OpenCL, queue = clCreateCommandQueue(ctx, device, 0, &err);); (29, OpenCL, clReleaseContext(ctx);); (19, OpenCL, ctx = clCreateContext(props, 1, &device, NULL, NULL, &err);); (63, OpenCL, err = clWaitForEvents(1, &event);)", "label": "(87, OpenCL, clReleaseMemObject(bufX);); (43, OpenCL, clReleaseCommandQueue(queue);); (27, OpenCL, printf( \"clCreateContext() failed with %d\\n\", err );); (85, OpenCL, clReleaseMemObject(bufA);); (79, OpenCL, clReleaseEvent(event);); (30, OpenCL, printf( \"clCreateCommandQueue() failed with %d\\n\", err );); (27, OpenCL, queue = clCreateCommandQueue(ctx, device, 0, &err);); (29, OpenCL, clReleaseContext(ctx);); (78, OpenCL, clReleaseMemObject(bufY);); (18, OpenCL, ctx = clCreateContext(props, 1, &device, NULL, NULL, &err);); (63, OpenCL, err = clWaitForEvents(1, &event);)"}
{"predict": "(47, OpenMP, #pragma omp master taskloop simd nogroup grainsize(argc)); (25, OpenMP, #pragma omp master taskloop simd priority(argc)); (0, OpenMP, #pragma omp master taskloop simd priority); (5, OpenMP, #pragma omp master taskgroup); (46, OpenMP, #pragma omp master taskloop simd if(argc) shared(argc, argv))", "label": "(4, OpenMP, #pragma omp task); (25, OpenMP, #pragma omp master taskloop priority(argc)); (46, OpenMP, #pragma omp master taskloop); (71, OpenMP, #pragma omp master taskloop if(argc) shared(argc, argv) collapse(2) num_tasks(4)); (87, OpenMP, #pragma omp cancel taskgroup); (87, OpenMP, #pragma omp cancellation point taskgroup)"}
{"predict": "(48, CUDA, cudaEventRecord(stop);); (57, CUDA, cudaEventSynchronize(stop);); (67, CUDA, cudaFree(A);); (46, CUDA, cudaEventRecord(start);); (65, CUDA, cudaFree(C);); (46, CUDA, cudaEventElapsedTime(&elapsedTime, start, stop);); (63, CUDA, cudaFree(B);); (19, CUDA, cudaGetDeviceCount(&num_of_total_devices);); (16, CUDA, cudaSetDevice(0);); (19, CUDA, cudaEventCreate(&stop);); (59, CUDA, cudaEventDestroy(start);); (16, CUDA, cudaEventCreate(&start);)", "label": "(67, CUDA, cudaFree(B);); (54, CUDA, cudaEventRecord(stop);); (54, CUDA, cudaEventSynchronize(stop);); (65, CUDA, cudaFree(C);); (44, CUDA, cudaEventRecord(start);); (29, CUDA, cudaGetDeviceCount(&num_of_total_devices);); (18, CUDA, cudaEventCreate(&stop);); (53, CUDA, cudaEventElapsedTime(&elapsedTime, start, stop);); (15, CUDA, cudaEventCreate(&start);); (57, CUDA, cudaFree(A);)"}
{"predict": "(34, CUDA, cudaFree(d_out);); (26, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (10, CUDA, cudaMemcpy(d_in1, in1.data(), in1_bytes, cudaMemcpyHostToDevice);); (8, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (30, CUDA, cudaFree(d_in1);); (4, CUDA, cudaMalloc((void**)(&d_in1), in1_bytes);)", "label": "(11, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (9, CUDA, cudaMalloc((void**)(&d_in1), in1_bytes);); (11, CUDA, cudaMemcpy(d_in1, in1.data(), in1_bytes, cudaMemcpyHostToDevice);); (31, CUDA, cudaFree(d_out);); (23, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (28, CUDA, cudaFree(d_in1);)"}
{"predict": "(10, OpenMP, #pragma omp parallel master taskloop simd linear (argc); (22, OpenMP, #pragma omp parallel master taskloop simd linear (argv[1])); (19, OpenMP, #pragma omp parallel master taskloop simd linear (S1)); (13, OpenMP, #pragma omp parallel master taskloop simd linear (argc > 0? argv[1] : argv[2])); (20, OpenMP, #pragma omp parallel master taskloop simd linear (a, b : B::ib)); (25, OpenMP, #pragma omp parallel master taskloop simd linear(ref(i))); (3, OpenMP, #pragma omp parallel master taskloop simd linear ()); (20, OpenMP, #pragma omp parallel master taskloop simd linear(h, C::x)); (13, OpenMP, #pragma omp parallel master taskloop simd linear (argc : 5)); (33, OpenMP, #pragma omp parallel master taskloop simd linear(uval(j))); (26, OpenMP, #pragma omp parallel master taskloop simd linear(uval(i))); (21, OpenMP, #pragma omp parallel master taskloop simd linear(i)); (14, OpenMP, #pragma omp parallel master taskloop simd linear (foo:argc)); (-3, OpenMP, #pragma omp parallel); (3, OpenMP, #pragma omp parallel master taskloop simd linear (argc,); (26, OpenMP, #pragma omp parallel master taskloop simd linear(v:j)); (1, OpenMP, #pragma omp parallel master taskloop simd linear (argc :); (26, OpenMP, #pragma omp parallel master taskloop simd linear(ref(j))); (3, OpenMP, #pragma omp parallel master taskloop simd linear (argc : 5))", "label": "(17, OpenMP, #pragma omp parallel for simd linear (argc : 5) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (39, OpenMP, #pragma omp parallel for simd linear(j)); (41, OpenMP, #pragma omp parallel for simd linear(v:j)); (34, OpenMP, #pragma omp parallel for simd linear(v:i)); (27, OpenMP, #pragma omp parallel for simd linear(i, z)); (4, OpenMP, #pragma omp parallel for simd linear ()); (13, OpenMP, #pragma omp parallel for simd linear (S1)); (6, OpenMP, #pragma omp parallel for simd linear (argc,); (-3, OpenMP, #pragma omp parallel); (36, OpenMP, #pragma omp parallel for simd linear(i)); (17, OpenMP, #pragma omp parallel for simd linear(e, g)); (12, OpenMP, #pragma omp parallel for simd linear (a, b:B::ib)); (-1, OpenMP, #pragma omp parallel for simd linear (argc); (12, OpenMP, #pragma omp parallel for simd linear (argv[1])); (15, OpenMP, #pragma omp parallel for simd linear(h)); (-8, OpenMP, #pragma omp parallel for simd linear (); (-1, OpenMP, #pragma omp parallel for simd linear (argc > 0? argv[1] : argv[2]))"}
{"predict": "(8, OpenMP, #pragma omp parallel for schedule (guided))", "label": "(8, OpenMP, #pragma omp parallel for schedule (guided))"}
{"predict": "(9, SYCL, sycl::queue q = get_queue();); (36, SYCL, sycl::free(data, q);); (7, SYCL, int* data = sycl::malloc_device<int>(1,q);)", "label": "(9, SYCL, sycl::queue q = get_queue();); (36, SYCL, sycl::free(data, q);); (8, SYCL, int* data = sycl::malloc_device<int>(1,q);)"}
{"predict": "(6, CUDA, cudaGetDeviceProperties( &prop, i );); (3, CUDA, cudaGetDeviceCount( &count );)", "label": "(6, CUDA, HANDLE_ERROR( cudaGetDeviceProperties( &prop, i ) );); (3, CUDA, HANDLE_ERROR( cudaGetDeviceCount( &count ) );)"}
{"predict": "(49, MPI, MPI_Allreduce(MPI_IN_PLACE,&n,1,mpi_int_t,MPI_MAX,grid->comm);); (47, MPI, MPI_Allreduce(MPI_IN_PLACE,&nnzL,1,mpi_int_t,MPI_SUM,grid->comm);)", "label": "(49, MPI, MPI_Allreduce(MPI_IN_PLACE,&n,1,mpi_int_t,MPI_MAX,grid->comm);); (47, MPI, MPI_Allreduce(MPI_IN_PLACE,&nnzL,1,mpi_int_t,MPI_SUM,grid->comm);)"}
{"predict": "](38, MPI, MPI_Cart_create(MPI_COMM_WORLD,3,numpes,isperiodic,reorder,&comm_cart);); (38, MPI, MPI_Comm_rank(comm_cart,&mype);); (43, MPI, MPI_Cart_coords(comm_cart,mype,3,pe_coords);); (15, MPI, MPI_Comm_size(MPI_COMM_WORLD,&totpes);); (20, MPI, MPI_Finalize();); (34, MPI, MPI_Dims_create(totpes,3,numpes);); (11, MPI, MPI_Init(&argc,&argv);); (15, MPI, MPI_Bcast(filename, 256, MPI_CHAR, 0, MPI_COMM_WORLD);); (104, MPI, MPI_Allreduce(MPI_IN_PLACE, &nerrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (102, MPI, MPI_Comm_free(&comm_cart);); (96, MPI, MPI_Reduce(&malloc_size, &sum_size, 1, MPI_OFFSET, MPI_SUM, 0, MPI_COMM_WORLD);); (90, MPI, MPI_Allreduce(t, t_g, 20, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&rank);); (83, MPI, MPI_Allreduce(rates_l, rates_g, 4, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);); (29, MPI, MPI_Dims_create(totpes,2,numpes);)", "label": "(17, MPI, MPI_Comm_size(MPI_COMM_WORLD,&totpes);); (39, MPI, MPI_Cart_create(MPI_COMM_WORLD,3,numpes,isperiodic,reorder,&comm_cart);); (39, MPI, MPI_Comm_rank(comm_cart,&mype);); (19, MPI, MPI_Finalize();); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&rank);); (72, MPI, MPI_Barrier(comm_cart);); (36, MPI, MPI_Cart_coords(comm_cart,mype,3,pe_coords);); (94, MPI, MPI_Allreduce(t, t_g, 20, MPI_DOUBLE, MPI_MAX, comm_cart);); (31, MPI, MPI_Dims_create(totpes,3,numpes);); (18, MPI, MPI_Bcast(filename, 256, MPI_CHAR, 0, MPI_COMM_WORLD);); (115, MPI, MPI_Allreduce(MPI_IN_PLACE, &nerrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (5, MPI, MPI_Init(&argc,&argv);); (102, MPI, MPI_Comm_free(&comm_cart);); (87, MPI, MPI_Allreduce(rates_l, rates_g, 4, MPI_DOUBLE, MPI_MIN, comm_cart);); (105, MPI, MPI_Reduce(&malloc_size, &sum_size, 1, MPI_OFFSET, MPI_SUM, 0, MPI_COMM_WORLD);)"}
{"predict": "(6, TBB, tbb::flow::make_edge( lim, r );); (10, TBB, tbb::flow::make_edge(s, lim.decrementer());); (23, TBB, tbb::flow::limiter_node< T > lim_copy( lim );); (1, TBB, tbb::flow::graph g;); (1, TBB, tbb::flow::limiter_node< T > lim(g, i);); (3, TBB, tbb::flow::make_edge(s, lim.decrementer());); (3, TBB, empty_sender< tbb::flow::continue_msg > s;)", "label": "(10, TBB, tbb::flow::make_edge( lim, r );); (22, TBB, tbb::flow::limiter_node< T > lim_copy( lim );); (3, TBB, tbb::flow::graph g;); (3, TBB, tbb::flow::limiter_node< T > lim(g, i);); (4, TBB, tbb::atomic<int> accept_count;)"}
{"predict": "(13, OpenMP, #pragma omp loop); (15, OpenMP, #pragma omp loop reduction(* : MTX)); (17, OpenMP, #pragma omp loop reduction(* : EMTX))", "label": "(15, OpenMP, #pragma omp loop); (19, OpenMP, #pragma omp target teams distribute parallel for map(MTX) reduction(*:EMTX)); (11, OpenMP, #pragma omp target teams loop map(MTX) reduction(*:MTX))"}
{"predict": "(67, TBB, tbb::task_group tg;)", "label": "(67, TBB, tbb::task_group tg;)"}
{"predict": "(24, OpenMP, #pragma omp atomic); (19, OpenMP, #pragma omp target map(tofrom:counts_team)); (13, OpenMP, #pragma omp target teams); (3, OpenMP, #pragma omp parallel); (2, OpenMP, #pragma omp master)", "label": "(26, OpenMP, #pragma omp for reduction(+:counts_team)); (8, OpenMP, #pragma omp for); (15, OpenMP, #pragma omp target teams map(from:counts2)); (4, OpenMP, #pragma omp parallel); (0, OpenMP, #pragma omp target teams map(from:counts1)); (17, OpenMP, #pragma omp allocate(counts_team) allocator(omp_pteam_mem_alloc))"}
{"predict": "(115, MPI, MPI_Type_commit(&mpi_word);); (119, MPI, MPI_Allgather(iterstring,1,mpi_word, catstring,1,mpi_word, MPI_COMM_WORLD);); (152, MPI, MPI_Finalize();); (64, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (62, MPI, MPI_Bcast(&length,     1, MPI_LONG, root, MPI_COMM_WORLD);); (107, MPI, MPI_Type_vector(proc_length,1,Num_procs,MPI_CHAR, &mpi_word);); (18, MPI, MPI_Init(&argc,&argv);); (104, MPI, MPI_Type_commit(&mpi_word);); (17, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (15, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)", "label": "(115, MPI, MPI_Type_commit(&mpi_word);); (122, MPI, MPI_Allgather(iterstring,1,mpi_word, catstring,1,mpi_word, MPI_COMM_WORLD);); (154, MPI, MPI_Finalize();); (63, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (110, MPI, MPI_Type_contiguous(proc_length,MPI_CHAR, &mpi_word);); (17, MPI, MPI_Init(&argc,&argv);); (61, MPI, MPI_Bcast(&length,     1, MPI_LONG, root, MPI_COMM_WORLD);); (110, MPI, MPI_Barrier(MPI_COMM_WORLD);); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(10, MPI, MPI_Barrier(MPI_COMM_WORLD);); (13, MPI, MPI_Bcast(data,n,MPI_C_BOOL,ADDA_ROOT,MPI_COMM_WORLD);)", "label": "(10, MPI, MPI_Barrier(MPI_COMM_WORLD);); (12, MPI, MPI_Allreduce(data,gr_comm_buf,n,mpi_bool,MPI_LAND,MPI_COMM_WORLD);)"}
{"predict": "(16, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(16, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(41, TBB, tbb::flow::make_edge(buffer2, join_node);); (37, TBB, tbb::flow::make_edge(buffer1, join_node);); (15, TBB, tbb::flow::join_node<tuple_type, tbb::flow::reserving> join_node(g);); (12, TBB, tbb::flow::buffer_node<int> buffer2(g);); (13, TBB, tbb::flow::buffer_node<int> buffer3(g);); (10, TBB, tbb::flow::buffer_node<int> buffer1(g);); (37, TBB, tbb::flow::make_edge(join_node, function);); (-1, TBB, tbb::flow::graph g;); (43, TBB, tbb::flow::make_edge(buffer3, join_node);)", "label": "(42, TBB, tbb::flow::make_edge(buffer2, tbb::flow::input_port<1>(join));); (43, TBB, tbb::flow::make_edge(join, function);); (19, TBB, tbb::flow::join_node<tuple_type, tbb::flow::reserving> join(g);); (38, TBB, tbb::flow::make_edge(buffer1, tbb::flow::input_port<0>(join));); (-3, TBB, tbb::task_arena arena(1);); (-1, TBB, tbb::flow::graph g;); (11, TBB, tbb::flow::queue_node<int> buffer1(g);); (12, TBB, tbb::flow::queue_node<int> buffer3(g);); (35, TBB, tbb::flow::make_edge(buffer3, tbb::flow::input_port<2>(join));); (9, TBB, tbb::flow::queue_node<int> buffer2(g);)"}
{"predict": "(6, MPI, MPI_Init(&argc, &argv);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (47, MPI, MPI_Finalize();); (38, MPI, MPI_Recv(&localpi, 1, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, &status);); (2, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (37, MPI, MPI_Send(&localpi, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);); (4, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(42, MPI, MPI_Recv(&localpi, 1, MPI_DOUBLE, MPI_ANY_SOURCE, 0, MPI_COMM_WORLD, &status);); (47, MPI, MPI_Send(&localpi, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);); (4, MPI, MPI_Init(&argc, &argv);); (48, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(13, CUDA, CP_host->freeCudaMem();)", "label": "(13, CUDA, CP_host->freeCudaMem();)"}
{"predict": "(4, TBB, tbb::task_arena arena( id );)", "label": "(12, TBB, tbb::parallel_for( 0, N, 1, RoundingModeCheckBody(mode, sse_mode) );); (10, TBB, tbb::task_scheduler_init init;)"}
{"predict": "(36, OpenMP, #pragma omp parallel for num_threads(27)); (13, OpenMP, #pragma omp target teams distribute thread_limit(27)); (16, OpenMP, #pragma omp parallel for num_threads(thdLim)); (3, OpenMP, #pragma omp target data map(tofrom : varLimit[:NN], constLimit[:NN], varLimitHuge[:NN], constLimitHuge[:NN])); (24, OpenMP, #pragma omp parallel for num_threads(1024)); (16, OpenMP, varLimit[i] = omp_get_num_threads();); (34, OpenMP, constLimit[i] = omp_get_num_threads();); (41, OpenMP, #pragma omp parallel for num_threads(1024)); (43, OpenMP, constLimitHuge[i] = omp_get_num_threads();); (18, OpenMP, varLimitHuge[i] = omp_get_num_threads();)", "label": "(13, OpenMP, fprintf(stderr, \"#pragma omp target teams distribute thread_limit(thdLim) 27\\n\");); (43, OpenMP, #pragma omp target teams distribute thread_limit(1024)); (22, OpenMP, fprintf(stderr, \"#pragma omp target teams distribute thread_limit(thdLim) 1024\\n\");); (31, OpenMP, #pragma omp target teams distribute thread_limit(27)); (34, OpenMP, constLimit[i] = omp_get_num_threads();); (9, OpenMP, #pragma omp target teams distribute thread_limit(thdLim)); (42, OpenMP, constLimitHuge[i] = omp_get_num_threads();); (21, OpenMP, varLimitHuge[i] = omp_get_num_threads();); (9, OpenMP, varLimit[i] = omp_get_num_threads();)"}
{"predict": "(50, CUDA, cudaFreeHost( foutHost );); (50, CUDA, cudaFreeArray( array );)", "label": "(50, CUDA, cudaFreeHost( foutHost );); (27, CUDA, CUDART_CHECK(surf1Dmemset( array, 3.141592654f, 0, NUM_VALUES ));)"}
{"predict": "(72, OpenACC, #pragma acc data copy(h_g2 [0:nbin])); (73, OpenACC, #pragma acc wait); (75, OpenACC, #pragma acc wait(1)); (135, OpenACC, #pragma acc exit data delete(d_x, d_y, d_z, d_g2))", "label": "(72, OpenMP, #pragma omp target data map(h_x [0:nconf * numatm], h_y [0:nconf * numatm], h_z [0:nconf * numatm], h_g2 [0:nbin]))"}
{"predict": "(6, OpenMP, #pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp parallel for)"}
{"predict": "(2, SYCL, int* data = sycl::malloc_device<int>(7, q);); (18, SYCL, hipsycl::rt::runtime_keep_alive_token rt;); (3, SYCL, sycl::queue q = get_queue();); (66, SYCL, sycl::free(data, q);)", "label": "(4, SYCL, int* data = sycl::malloc_shared<int>(7, q);); (19, SYCL, hipsycl::rt::runtime_keep_alive_token rt;); (1, SYCL, sycl::queue q = get_queue();); (67, SYCL, sycl::free(data, q);)"}
{"predict": "(48, MPI, MPI_Gather( NULL, 0, MPI_BYTE, NULL, 0, MPI_BYTE, 0, MPI_COMM_WORLD );); (27, MPI, MPI_Gather( vecin, 1, vec, vecout, n, MPI_DOUBLE, root, comm );); (22, MPI, MPI_Type_vector( n, 1, stride, MPI_DOUBLE, &vec );); (22, MPI, MPI_Type_commit( &vec );); (42, MPI, MPI_Type_free( &vec );); (8, MPI, MPI_Comm_size( comm, &size );); (6, MPI, MPI_Comm_rank( comm, &rank );)", "label": "(13, MPI, MPI_Comm_size( comm, &size );); (22, MPI, MPI_Type_commit( &vec );); (26, MPI, MPI_Gather( vecin, 1, vec, vecout, n, MPI_DOUBLE, root, comm );); (46, MPI, MPI_Finalize();); (5, MPI, MPI_Init( &argc, &argv );); (35, MPI, MPI_Type_free( &vec );); (6, MPI, MPI_Comm_rank( comm, &rank );); (15, MPI, MPI_Type_vector( n, 1, stride, MPI_DOUBLE, &vec );); (39, MPI, MPI_Gather( NULL, 0, MPI_BYTE, NULL, 0, MPI_BYTE, 0, MPI_COMM_WORLD );)"}
{"predict": "(15, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (37, MPI, MPI_Finalize();); (13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (33, MPI, MPI_Reduce(&sum, &avg, 1, MPI_DOUBLE, MPI_SUM, 0, cart_comm);); (15, MPI, MPI_Dims_create(size, NR_DIMS, dims);); (15, MPI, MPI_Cart_coords(cart_comm, rank, NR_DIMS, cart_coords);); (23, MPI, MPI_Barrier(MPI_COMM_WORLD);); (3, MPI, MPI_Init(&argc, &argv);)", "label": "(34, OpenMP, #pragma omp for reduction(+:sum)); (32, OpenMP, printf(\"rank %d:%d has %d threads\\n\", rank, thread_num, omp_get_num_threads());); (11, MPI, MPI_Init(&argc, &argv);); (42, MPI, MPI_Finalize();); (26, OpenMP, #pragma omp parallel); (36, MPI, MPI_Reduce(&sum, &avg, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (25, OpenMP, int thread_num = omp_get_thread_num();); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (7, MPI, MPI_Dims_create(size, NR_DIMS, dims);); (13, MPI, MPI_Cart_coords(cart_comm, rank, NR_DIMS, cart_coords);); (10, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(7, OpenCL, clReleaseDevice_ExpectAndReturn(make_device_id(0), CL_SUCCESS);); (6, OpenCL, clReleaseKernel_ExpectAndReturn(make_kernel(0), CL_SUCCESS);)", "label": "(8, OpenCL, clReleaseKernel_ExpectAndReturn(make_kernel(0), CL_SUCCESS);); (6, OpenCL, clReleaseDevice_ExpectAndReturn(make_device_id(0), CL_SUCCESS);)"}
{"predict": "(39, CUDA, CUDA_SAFE_CALL(cudaFree(x.data()));); (37, CUDA, CUDA_SAFE_CALL(cudaFree(y.data()));); (35, CUDA, CUDA_SAFE_CALL(cudaFree(A.data()));)", "label": "(40, CUDA, CUDA_SAFE_CALL(cudaFree(x.data()));); (38, CUDA, CUDA_SAFE_CALL(cudaFree(A.data()));); (36, CUDA, CUDA_SAFE_CALL(cudaDeviceSynchronize());); (38, CUDA, CUDA_SAFE_CALL(cudaFree(y.data()));)"}
{"predict": "(22, MPI, *w = MPI_Wtime();)", "label": "(22, MPI, *w = MPI_Wtime();)"}
{"predict": "(2, CUDA, extern __shared__ float hs[][256];)", "label": "(2, CUDA, extern __shared__ half  hs[][256];)"}
{"predict": "(7, TBB, tbb::task_scheduler_init init( p );)", "label": "(7, TBB, tbb::task_scheduler_init init( p );)"}
{"predict": "(13, CUDA, CUDASimulation cudaSimulation(model);)", "label": "(13, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (44, MPI, MPI_Win_flush_all(shm_win);); (15, MPI, MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, rank, MPI_INFO_NULL, &shm_comm);); (60, MPI, MPI_Comm_free(&shm_comm);); (12, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (57, MPI, MPI_Win_free(&shm_win);); (36, MPI, MPI_Barrier(shm_comm);); (11, MPI, MPI_Comm_rank(shm_comm, &shm_rank);); (38, MPI, MPI_Win_sync(shm_win);); (10, MPI, MPI_Comm_size(shm_comm, &shm_nproc);); (23, MPI, MPI_Win_shared_query(shm_win, i, &size, &disp_unit, &base);)", "label": "(18, MPI, MPI_Comm_rank(shm_comm, &shm_rank);); (41, MPI, MPI_Win_lock_all(MPI_MODE_NOCHECK, shm_win);); (24, MPI, MPI_Win_shared_query(shm_win, MPI_PROC_NULL, &size, &disp_unit, &base);); (13, MPI, MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, rank, MPI_INFO_NULL, &shm_comm);); (10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (58, MPI, MPI_Win_unlock_all(shm_win);); (58, MPI, MPI_Win_free(&shm_win);); (58, MPI, MPI_Comm_free(&shm_comm);); (10, MPI, MPI_Comm_size(shm_comm, &shm_nproc);); (40, MPI, MPI_Barrier(shm_comm);); (38, MPI, MPI_Win_sync(shm_win);)"}
{"predict": "(55, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(c, host_c.data(), global_c_size * sizeof(TC), cudaMemcpyHostToDevice));); (50, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(a, host_a.data(), global_a_size * sizeof(TA), cudaMemcpyHostToDevice));); (51, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(b, host_b.data(), global_b_size * sizeof(TB), cudaMemcpyHostToDevice));); (40, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&b, global_b_size * sizeof(TB)));); (66, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(c));); (37, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&a, global_a_size * sizeof(TA)));); (57, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(host_output.data(), c, global_c_size * sizeof(TC), cudaMemcpyDeviceToHost));); (64, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(b));); (36, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&c, global_c_size * sizeof(TC)));); (53, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (60, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(a));)", "label": "(41, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&b, global_b_size * sizeof(TB)));); (71, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(c));); (68, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(a));); (37, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&a, global_a_size * sizeof(TA)));); (47, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(a, host_a.data(), global_a_size * sizeof(TA), cudaMemcpyHostToDevice));); (49, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (47, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(c, host_c.data(), global_a_size * sizeof(TC), cudaMemcpyHostToDevice));); (45, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(b, host_b.data(), global_a_size * sizeof(TB), cudaMemcpyHostToDevice));); (58, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(host_output.data(), c, global_c_size * sizeof(TC), cudaMemcpyDeviceToHost));); (33, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&c, global_c_size * sizeof(TC)));); (61, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(b));)"}
{"predict": "(62, CUDA, cudaFree( ddata );)", "label": "(62, CUDA, cudaFree( ddata );)"}
{"predict": "(10, CUDA, cudaStreamSynchronize(LU_v1->A_gpu.cuStreams[0]);)", "label": "(10, CUDA, cudaStreamSynchronize(LU_v1->A_gpu.cuStreams[0]);)"}
{"predict": "(2, OpenMP, omp_autotune(self, OMP_SCALE);)", "label": "(2, OpenMP, omp_autotune(self, OMP_SCALE);)"}
{"predict": "(20, CUDA, checkCudaErrors(cudaGetLastError());); (27, CUDA, checkCudaErrors(cudaFree(dA_buff[i]));); (30, CUDA, checkCudaErrors(cudaFree(dB_buff[i]));); (30, CUDA, checkCudaErrors(cudaFree(dC_buff[i]));); (17, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (12, CUDA, checkCudaErrors(cudaMalloc(&dA_buff, sizeof(float*) * batch_num));); (13, CUDA, checkCudaErrors(cudaMemcpy(dB_buff, B_buff, sizeof(float*) * batch_num, cudaMemcpyHostToDevice));); (10, CUDA, checkCudaErrors(cudaMalloc(&dB_buff, sizeof(float*) * batch_num));); (10, CUDA, checkCudaErrors(cudaMalloc(&dC_buff, sizeof(float*) * batch_num));); (12, CUDA, checkCudaErrors(cudaMemcpy(dA_buff, A_buff, sizeof(float*) * batch_num, cudaMemcpyHostToDevice));)", "label": "(18, CUDA, cudaMemcpy(dB_buff, B_buff, sizeof(float*) * batch_num, cudaMemcpyHostToDevice);); (12, CUDA, cudaMalloc(&dA_buff, sizeof(float*) * batch_num);); (31, CUDA, cudaFree(dC_buff);); (29, CUDA, cudaFree(dB_buff);); (27, CUDA, cudaFree(dA_buff);); (12, CUDA, cudaMemcpy(dA_buff, A_buff, sizeof(float*) * batch_num, cudaMemcpyHostToDevice);); (8, CUDA, cudaMalloc(&dB_buff, sizeof(float*) * batch_num);); (12, CUDA, cudaMemcpy(dC_buff, C_buff, sizeof(float*) * batch_num, cudaMemcpyHostToDevice);); (7, CUDA, cudaMalloc(&dC_buff, sizeof(float*) * batch_num);)"}
{"predict": "(13, CUDA, checkCudaErrors(cudaFree(buffer));); (13, CUDA, checkCudaErrors(cudaFreeHost(buffer_fp32));); (4, CUDA, checkCudaErrors(cudaMalloc(&buffer, length * sizeof(T)));); (4, CUDA, checkCudaErrors(cudaMallocHost(&buffer_fp32, length * sizeof(float)));)", "label": "(7, CUDA, cudaMalloc((void **)&buffer_fp32, length * sizeof(float));); (9, CUDA, cudaMalloc((void **)&buffer, length * sizeof(T));); (10, CUDA, cudaFree(buffer_fp32);)"}
{"predict": "(22, OpenMP, #pragma omp teams distribute parallel for if(cond) firstprivate(fp) reduction(+:rd))", "label": "(16, OpenMP, #pragma omp target); (22, OpenMP, #pragma omp teams distribute parallel for if(cond) firstprivate(fp) reduction(+:rd))"}
{"predict": "(19, CUDA, cudaMemcpy(resf,dat,sizeof(unsigned)*2,cudaMemcpyDeviceToHost);)", "label": "(19, CUDA, cudaMemcpy(resf,dat,sizeof(unsigned)*2,cudaMemcpyDeviceToHost);)"}
{"predict": "(50, CUDA, cudaFreeArray( texArray );); (30, CUDA, CUDART_CHECK(surf2DmemsetArray( texArray, 0 ) );); (28, CUDA, CUDART_CHECK(surf2DmemsetTime( &ms, texArray ) );)", "label": "(29, CUDA, CUDART_CHECK(surf2DmemsetArray_time( &ms, texArray, 3.141592654f, threadWidth, threadHeight ));); (50, CUDA, cudaFreeArray( texArray );)"}
{"predict": "(57, CUDA, cudaError_t error_id = cudaGetDeviceCount(&deviceCount);); (103, CUDA, printf(\"cudaGetDeviceProperties returned %d\\n-> %s\\n\", (int)error_id, cudaGetErrorString(error_id));); (91, CUDA, cudaError_t error_id = cudaGetDeviceProperties(&deviceProp, currentDevice);); (102, CUDA, checkCudaErrors(cudaSetDevice(currentDevice));); (54, CUDA, printf(\"cudaGetDeviceCount returned %d\\n-> %s\\n\", (int)error_id, cudaGetErrorString(error_id));); (100, CUDA, printf(\"cudaGetDeviceProperties returned %d\\n-> %s\\n\", (int)error_id, cudaGetErrorString(error_id));); (50, CUDA, exit(EXIT_FAILURE);)", "label": "(267, CUDA, cudaSetDevice(nDevice);); (108, CUDA, printf(\"cudaGetDeviceProperties returned %d\\n-> %s\\n\", (int)error_id, cudaGetErrorString(error_id));); (54, CUDA, cudaError_t error_id = cudaGetDeviceCount(&deviceCount);); (98, CUDA, fprintf(stderr, \"Error: device is running in <Compute Mode Prohibited>, no threads can use ::cudaSetDevice().\\n\");); (56, CUDA, printf(\"cudaGetDeviceCount returned %d\\n-> %s\\n\", (int)error_id, cudaGetErrorString(error_id));); (88, CUDA, cudaError_t error_id = cudaGetDeviceProperties(&deviceProp, currentDevice);); (96, CUDA, checkCudaErrors(cudaSetDevice(currentDevice));)"}
{"predict": "(6, CUDA, extern __shared__ float cache[];)", "label": "(5, CUDA, extern __shared__ float cache[];)"}
{"predict": "(14, OpenMP, int nestedInParallel = omp_in_parallel();); (22, OpenMP, #pragma omp atomic); (14, OpenMP, int nestedNumThreads = omp_get_num_threads();); (7, OpenMP, #pragma omp target map(inParallel, numThreads, threadNum, check1[:], check2[:])); (7, OpenMP, #pragma omp parallel); (7, OpenMP, int id = omp_get_thread_num();); (20, OpenMP, int nestedThreadNum = omp_get_thread_num();); (12, OpenMP, #pragma omp parallel num_threads(3)); (4, OpenMP, #pragma omp single); (23, OpenMP, #pragma omp target exit data map(from : inParallel, numThreads, threadNum, check1[:], check2[:]))", "label": "(77, OpenMP, #pragma omp parallel num_threads(NumThreads1)); (14, OpenMP, #pragma omp parallel); (26, OpenMP, int nestedThreadNum = omp_get_thread_num();); (7, OpenMP, inParallel = omp_in_parallel();); (22, OpenMP, int nestedNumThreads = omp_get_num_threads();); (19, OpenMP, int nestedInParallel = omp_in_parallel();); (12, OpenMP, check1[id] += omp_get_num_threads() + omp_in_parallel();); (4, OpenMP, numThreads = omp_get_num_threads();); (21, OpenMP, #pragma omp atomic); (3, OpenMP, threadNum = omp_get_thread_num();); (-2, OpenMP, #pragma omp target map(inParallel, numThreads, threadNum, check1[:], check2[:])); (6, OpenMP, int id = omp_get_thread_num();)"}
{"predict": "(14, CUDA, cudaDeviceSynchronize();); (10, CUDA, float *dptr = thrust::raw_pointer_cast(&dsums[0]);); (8, CUDA, thrust::device_vector<float> dsums(steps);); (12, CUDA, double gpu_sum = thrust::reduce(dsums.begin(),dsums.end());)", "label": "(11, CUDA, float *dptr = thrust::raw_pointer_cast(&dsums[0]);); (14, CUDA, double gpu_sum = thrust::reduce(dsums.begin(),dsums.end());); (8, CUDA, thrust::device_vector<float> dsums(steps);)"}
{"predict": "(25, OpenMP, #pragma omp parallel for)", "label": "(12, OpenMP, #pragma omp parallel for)"}
{"predict": "(12, TBB, my_thread_array = tbb::cache_aligned_allocator<padded_private_worker>().allocate( my_n_thread );)", "label": "(12, TBB, my_thread_array = tbb::cache_aligned_allocator<padded_private_worker>().allocate( my_n_thread );)"}
{"predict": "(40, CUDA, cudaFree(d_inPtr);); (23, CUDA, cudaMalloc((void**)&d_inPtr, size * sizeof(float));); (29, CUDA, cudaMemcpy(&result_gpu, &d_outPtr[0], sizeof(float), cudaMemcpyDeviceToHost);); (24, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);); (35, CUDA, cudaFree(d_outPtr);); (20, CUDA, cudaMalloc((void**)& d_outPtr, size * sizeof(float));)", "label": "(40, CUDA, cudaFree(d_inPtr);); (24, CUDA, cudaMalloc((void**)&d_outPtr, size * sizeof(float));); (30, CUDA, cudaMemcpy(&result_gpu, &d_outPtr[0], sizeof(float), cudaMemcpyDeviceToHost);); (24, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);); (35, CUDA, cudaFree(d_outPtr);); (19, CUDA, cudaMalloc((void**)& d_inPtr, size * sizeof(float));)"}
{"predict": "(75, CUDA, err = cudaFree(d_A);); (76, CUDA, err = cudaFree(d_C);); (78, CUDA, cudaEventSynchronize(stop);); (82, CUDA, cudaEventDestroy(start);); (79, CUDA, err = cudaFree(d_B);); (82, CUDA, cudaEventDestroy(stop);); (85, CUDA, cudaFreeHost(hostData);); (37, CUDA, err = cudaMemcpy(d_B, B, length, cudaMemcpyHostToDevice);); (39, CUDA, cudaMemcpyAsync(C, d_C, length, cudaMemcpyDeviceToHost, 0);); (6, CUDA, cudaEventRecord(start, 0);); (73, CUDA, err = cudaMemcpy(C, d_C, length, cudaMemcpyDeviceToHost);); (66, CUDA, err = cudaMemcpyAsync(d_C, C, length, cudaMemcpyHostToDevice, 0);); (14, CUDA, err = cudaMalloc(&d_C, length);); (10, CUDA, err = cudaMalloc(&d_B, length);); (56, CUDA, err = cudaGetLastError();); (58, CUDA, cudaDeviceSynchronize();); (2, CUDA, err = cudaMalloc(&d_A, length);); (6, CUDA, err = cudaMemcpy(d_A, A, length, cudaMemcpyHostToDevice);); (76, CUDA, cudaFree(d_C);); (6, CUDA, cudaMemcpyAsync(d_A, A, length, cudaMemcpyHostToDevice, 0);); (76, CUDA, cudaFree(d_B);); (3, CUDA, cudaMemcpyAsync(d_B, B, length, cudaMemcpyHostToDevice, 0);); (68, CUDA, err = cudaFree(d_A);); (-8, CUDA, cudaEventCreate(&stop);); (7, CUDA, cudaMemcpyAsync(d_C, C, length, cudaMemcpyHostToDevice, 0);); (-14, CUDA, cudaEventCreate(&start);); (3, CUDA, cudaMemcpy(d_C, C, length, cudaMemcpyHostToDevice);); (59, CUDA, cudaEventRecord(stop", "label": "(88, CUDA, cudaEventSynchronize(stop);); (70, CUDA, err = cudaGetLastError();); (19, CUDA, err = cudaMalloc(&d_A, length);); (12, CUDA, cudaEventRecord(start, 0);); (78, CUDA, err = cudaFree(d_C);); (87, CUDA, cudaEventDestroy(start);); (74, CUDA, err = cudaFree(d_B);); (21, CUDA, err = cudaMalloc(&d_B, length);); (78, CUDA, cudaEventRecord(stop, 0);); (32, CUDA, err = cudaMemcpy(d_A, A, length, cudaMemcpyHostToDevice);); (80, CUDA, cudaEventElapsedTime(elapsed_time, start, stop);); (1, CUDA, cudaEventCreate(&start);); (66, CUDA, err = cudaFree(d_A);); (30, CUDA, err = cudaMemcpy(d_B, B, length, cudaMemcpyHostToDevice);); (16, CUDA, err = cudaMalloc(&d_C, length);); (78, CUDA, cudaEventDestroy(stop);); (-3, CUDA, cudaEventCreate(&stop);); (57, CUDA, err = cudaMemcpy(C, d_C, length, cudaMemcpyDeviceToHost);)"}
{"predict": "(23, OpenMP, #pragma omp target parallel defaultmap (scalar:); (18, OpenMP, #pragma omp target parallel defaultmap(tofrom, scalar); (1, OpenMP, #pragma omp target parallel defaultmap); (4, OpenMP, #pragma omp target parallel defaultmap(tofrom); (8, OpenMP, #pragma omp target parallel defaultmap (tofrom:); (1, OpenMP, #pragma omp target parallel defaultmap()); (13, OpenMP, #pragma omp target parallel defaultmap (tofrom scalar)); (7, OpenMP, #pragma omp target parallel defaultmap (tofrom)); (0, OpenMP, #pragma omp target parallel defaultmap(tofrom,)", "label": "(16, OpenMP, #pragma omp target defaultmap (scalar:); (3, OpenMP, #pragma omp target defaultmap(); (10, OpenMP, #pragma omp target defaultmap(tofrom)); (11, OpenMP, #pragma omp target defaultmap(tofrom,); (16, OpenMP, #pragma omp target defaultmap(tofrom:scalar) defaultmap(tofrom:scalar)); (-3, OpenMP, #pragma omp target defaultmap); (19, OpenMP, #pragma omp target defaultmap(tofrom: something)); (11, OpenMP, #pragma omp target defaultmap(tofrom, scalar); (2, OpenMP, #pragma omp target defaultmap (tofrom:); (-3, OpenMP, #pragma omp target defaultmap()); (-2, OpenMP, #pragma omp target defaultmap(tofrom)"}
{"predict": "(29, CUDA, CUDA_CALL(cudaDeviceSynchronize());); (7, CUDA, CUDA_CALL(cudaMalloc((void**)&data, size * sizeof(T)));); (49, CUDA, CUDA_CALL(cudaFree(data));)", "label": "(52, CUDA, CUDA_CALL(cudaFree(data));); (28, CUDA, CUDA_CALL(cudaDeviceSynchronize());); (6, CUDA, CUDA_CALL(cudaMalloc((void**)&data, size * sizeof(T)));)"}
{"predict": "(16, MPI, MPI_Comm_size(comm, &size);); (27, MPI, MPI_Abort(comm, 1);); (14, MPI, MPI_Comm_rank(comm, &rank);)", "label": "(29, MPI, MPI_Abort(comm, 1);); (15, MPI, MPI_Comm_size(comm, &size);); (15, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(17, OpenMP, #pragma omp parallel sections shared (argc)); (33, OpenMP, #pragma omp parallel sections shared(e, g)); (36, OpenMP, #pragma omp parallel sections shared(j)); (24, OpenMP, #pragma omp parallel sections shared(ba)); (44, OpenMP, #pragma omp parallel sections firstprivate(i)); (38, OpenMP, #pragma omp parallel sections private(i)); (2, OpenMP, #pragma omp parallel sections shared); (25, OpenMP, #pragma omp parallel sections shared(da)); (3, OpenMP, #pragma omp parallel sections shared (); (28, OpenMP, #pragma omp parallel sections shared(h, B::x)); (13, OpenMP, #pragma omp parallel sections shared (a, b, c, d, f)); (20, OpenMP, #pragma omp parallel sections shared(ca)); (3, OpenMP, #pragma omp parallel sections shared (argc,); (1, OpenMP, #pragma omp parallel sections shared (argc); (12, OpenMP, #pragma omp parallel sections shared (argv[1])); (30, OpenMP, #pragma omp parallel sections shared(i)); (2, OpenMP, #pragma omp parallel sections shared (argc > 0? argv[1] : argv[2])); (12, OpenMP, #pragma omp parallel sections shared(k, argv[1])); (3, OpenMP, #pragma omp parallel sections shared (S1)); (16, OpenMP, #pragma omp parallel sections shared(h)); (1, OpenMP, #pragma omp parallel sections shared (argc)); (30, OpenMP, #pragma omp parallel sections shared(i))", "label": "(25, OpenMP, #pragma omp parallel sections shared(argv[1])); (34, OpenMP, #pragma omp parallel sections shared(h, B::x)); (41, OpenMP, #pragma omp parallel sections shared(i)); (20, OpenMP, #pragma omp parallel sections shared(a, b, c, d, f, k)); (9, OpenMP, #pragma omp parallel sections shared(argc); (10, OpenMP, #pragma omp parallel sections shared(argc,); (33, OpenMP, #pragma omp parallel sections firstprivate(i)); (22, OpenMP, #pragma omp parallel sections shared(ca)); (19, OpenMP, #pragma omp parallel sections shared(ba)); (28, OpenMP, #pragma omp parallel sections private(i)); (-3, OpenMP, #pragma omp parallel sections shared); (20, OpenMP, #pragma omp parallel sections shared(da)); (5, OpenMP, #pragma omp parallel sections shared(argc > 0? argv[1] : argv[2])); (8, OpenMP, #pragma omp parallel sections shared(S1)); (5, OpenMP, #pragma omp parallel sections shared(argc)); (30, OpenMP, #pragma omp parallel sections shared(j)); (-5, OpenMP, #pragma omp parallel sections shared()); (-8, OpenMP, #pragma omp parallel sections shared(); (15, OpenMP, #pragma omp parallel sections shared(e, g))"}
{"predict": "(45, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(45, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(25, OpenMP, #pragma omp parallel for)", "label": "(25, OpenMP, #pragma omp parallel for)"}
{"predict": "(46, CUDA, extern __shared__ char buf[];)", "label": "(13, CUDA, extern __shared__ char buf[];)"}
{"predict": "(66, MPI, MPI_Comm_size(fd->comm, &nprocs);); (66, MPI, MPI_Comm_rank(fd->comm, &myrank);)", "label": "(66, MPI, MPI_Comm_size(fd->comm, &nprocs);); (66, MPI, MPI_Comm_rank(fd->comm, &myrank);)"}
{"predict": "(6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &world_size);)", "label": "(6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &world_size);)"}
{"predict": "(107, OpenMP, #pragma omp distribute parallel for lastprivate(S2::S2sc)); (129, OpenMP, #pragma omp distribute parallel for lastprivate(j)); (96, OpenMP, #pragma omp distribute parallel for lastprivate(ca)); (136, OpenMP, #pragma omp distribute parallel for firstprivate(m) lastprivate(m)); (139, OpenMP, #pragma omp distribute parallel for lastprivate(i)); (55, OpenMP, #pragma omp distribute parallel for lastprivate(argv[1])); (103, OpenMP, #pragma omp distribute parallel for lastprivate(h, B::x)); (149, OpenMP, #pragma omp parallel private(xa)); (13, OpenMP, #pragma omp distribute parallel for lastprivate(argc); (121, OpenMP, #pragma omp parallel); (30, OpenMP, #pragma omp distribute parallel for lastprivate(argc > 0? argv[1] : argv[2])); (68, OpenMP, #pragma omp distribute parallel for lastprivate(xa)); (38, OpenMP, #pragma omp distribute parallel for lastprivate(S1)); (75, OpenMP, #pragma omp distribute parallel for lastprivate(S2::S2s)); (46, OpenMP, #pragma omp distribute parallel for lastprivate(a, b, c, d, f)); (7, OpenMP, #pragma omp distribute parallel for lastprivate(argc); (114, OpenMP, #pragma omp distribute parallel for lastprivate(n) firstprivate(n)); (97, OpenMP, #pragma omp distribute parallel for lastprivate(m)); (23, OpenMP, #pragma omp distribute parallel for lastprivate(argc,); (38, OpenMP, #pragma omp distribute parallel for lastprivate(2 * 2)); (121, OpenMP, #pragma omp parallel reduction(+ : xa)); (64, OpenMP, #pragma omp distribute parallel for lastprivate(e, g)); (106, OpenMP, #pragma omp distribute parallel for lastprivate(i)); (100, OpenMP, #pragma omp distribute parallel for private(xa), lastprivate(xa)); (57, OpenMP, #pragma omp distribute parallel for lastprivate(da)); (83, OpenMP", "label": "(82, OpenMP, #pragma omp distribute parallel for lastprivate(xa)); (15, OpenMP, #pragma omp distribute parallel for lastprivate(); (59, OpenMP, #pragma omp distribute parallel for lastprivate(2 * 2)); (73, OpenMP, #pragma omp distribute parallel for lastprivate(da)); (93, OpenMP, #pragma omp distribute parallel for safelen(5)); (137, OpenMP, #pragma omp distribute parallel for lastprivate(j)); (25, OpenMP, #pragma omp distribute parallel for lastprivate(argc,); (2, OpenMP, #pragma omp target); (109, OpenMP, #pragma omp distribute parallel for lastprivate(B::x)); (62, OpenMP, #pragma omp distribute parallel for lastprivate(ca)); (138, OpenMP, #pragma omp distribute parallel for firstprivate(m) lastprivate(m)); (116, OpenMP, #pragma omp distribute parallel for lastprivate(i)); (142, OpenMP, #pragma omp distribute parallel for lastprivate(n) firstprivate(n)); (94, OpenMP, #pragma omp distribute parallel for lastprivate(m)); (32, OpenMP, #pragma omp distribute parallel for lastprivate(S1)); (97, OpenMP, #pragma omp distribute parallel for lastprivate(h)); (-5, OpenMP, #pragma omp distribute parallel for lastprivate); (39, OpenMP, #pragma omp distribute parallel for lastprivate(argv[1])); (18, OpenMP, #pragma omp distribute parallel for lastprivate(argc > 0? argv[1] : argv[2])); (32, OpenMP, #pragma omp distribute parallel for lastprivate(a, b, c, d, f)); (140, OpenMP, #pragma omp distribute parallel for lastprivate(si)); (20, OpenMP, #pragma omp distribute parallel for lastprivate(argc)); (80, OpenMP, #pragma omp distribute parallel for lastprivate(e, g)); (3, OpenMP, #pragma omp distribute parallel for lastprivate(argc); (42, OpenMP, #pragma omp distribute parallel for lastprivate(ba)); (62, OpenMP, #pragma omp distribute parallel for lastprivate(S2::S2s)); (66, OpenMP, #pragma omp distribute parallel for lastprivate(S2::S2sc)); (95, OpenMP, #pragma omp distribute parallel for private(xa), lastprivate(xa)); (-7, OpenMP, #pragma omp distribute parallel for lastprivate()); (-19, OpenMP, #pragma omp teams)"}
{"predict": "(246, MPI, MPI_Finalize();); (23, MPI, MPI_Comm_size(MPI_COMM_WORLD,&Num_procs);); (78, OpenMP, omp_set_num_threads(nthread_input);); (114, OpenMP, #pragma omp parallel for); (74, OpenMP, omp_set_dynamic(0);); (76, MPI, MPI_Bcast(&iterations,1, MPI_INT, root, MPI_COMM_WORLD);); (20, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&my_ID);); (72, MPI, MPI_Bcast(&offset,1, MPI_LONG, root, MPI_COMM_WORLD);); (112, OpenMP, #pragma omp master); (21, MPI, MPI_Init(&argc,&argv);); (68, MPI, MPI_Bcast(&length,1, MPI_LONG, root, MPI_COMM_WORLD);); (113, OpenMP, nthread = omp_get_num_threads();); (120, MPI, MPI_Allreduce(&local_nstream_time,&nstream_time,1,MPI_DOUBLE,MPI_MAX,MPI_COMM_WORLD);)", "label": "(101, OpenMP, printf(\"Number of threads    = %d\\n\", omp_get_max_threads());); (24, MPI, MPI_Comm_size(MPI_COMM_WORLD,&Num_procs);); (148, MPI, MPI_Finalize();); (104, OpenMP, #pragma omp parallel for); (74, MPI, MPI_Bcast(&length,        1, MPI_LONG, root, MPI_COMM_WORLD);); (21, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&my_ID);); (75, MPI, MPI_Bcast(&nthread_input, 1, MPI_INT,  root, MPI_COMM_WORLD);); (76, OpenMP, omp_set_num_threads(nthread_input);); (16, MPI, MPI_Init(&argc,&argv);); (71, MPI, MPI_Bcast(&iterations,    1, MPI_INT,  root, MPI_COMM_WORLD);); (113, MPI, MPI_Barrier(MPI_COMM_WORLD);); (68, MPI, MPI_Bcast(&offset,        1, MPI_LONG, root, MPI_COMM_WORLD);)"}
{"predict": "(18, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(18, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(11, OpenMP, #pragma omp distribute parallel for simd private(g, g1, svar, sfvar)); (8, OpenMP, #pragma omp target); (8, OpenMP, #pragma omp teams)", "label": "(9, OpenMP, #pragma omp target); (10, OpenMP, #pragma omp distribute simd firstprivate(g, g1, svar, sfvar)); (46, OpenMP, #pragma omp distribute simd firstprivate(t_var, vec, s_arr, s_arr, var, var, svar)); (7, OpenMP, #pragma omp teams)"}
{"predict": "(7, CUDA, __shared__ block_radix_sort_t::TempStorage temp_storage;)", "label": "(7, CUDA, __shared__ block_radix_sort_t::TempStorage temp_storage;)"}
{"predict": "(12, CUDA, CUDASimulation cudaSimulation(model);)", "label": "(12, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(33, CUDA, cudaFree(d_inPtr);); (21, CUDA, cudaMalloc((void**)&d_inPtr, size * sizeof(float));); (25, CUDA, cudaMemcpy(&result_gpu, &d_outPtr[0], sizeof(float), cudaMemcpyDeviceToHost);); (21, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);); (28, CUDA, cudaFree(d_outPtr);); (17, CUDA, cudaMalloc((void**)& d_outPtr, size * sizeof(float));)", "label": "(33, CUDA, cudaFree(d_inPtr);); (21, CUDA, cudaMalloc((void**)&d_outPtr, size * sizeof(float));); (26, CUDA, cudaMemcpy(&result_gpu, &d_outPtr[0], sizeof(float), cudaMemcpyDeviceToHost);); (21, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);); (28, CUDA, cudaFree(d_outPtr);); (16, CUDA, cudaMalloc((void**)& d_inPtr, size * sizeof(float));)"}
{"predict": "(7, TBB, tbb::flow::make_edge(my_broadcast_node, my_queue);); (17, TBB, return (tbb::tick_count::now() - t0).seconds();); (10, TBB, tbb::flow::make_edge(my_queue, my_broadcast_node);); (1, TBB, tbb::flow::broadcast_node<my_type> my_broadcast_node(g);); (1, TBB, tbb::flow::queue_node<my_type> my_queue(g);); (5, TBB, tbb::tick_count t0 = tbb::tick_count::now();)", "label": "(3, TBB, tbb::flow::broadcast_node<my_type> my_broadcast_node(g);); (7, TBB, const tbb::tick_count t0 = tbb::tick_count::now();); (14, TBB, return (tbb::tick_count::now() - t0).seconds();); (-1, TBB, tbb::flow::queue_node<my_type> my_queue(g);)"}
{"predict": "(33, MPI, MPI_Bcast( &xnorm, 1, MPI_DOUBLE, 0, slucomm );); (31, MPI, MPI_Bcast( &errcomp, 1, MPI_DOUBLE, 0, slucomm );); (25, MPI, MPI_Allreduce( &temperr, &errcomp, 1, MPI_DOUBLE, MPI_MAX, slucomm );); (22, MPI, MPI_Allreduce( &tempxnorm, &xnorm, 1, MPI_DOUBLE, MPI_MAX, slucomm );); (19, MPI, MPI_Allreduce( &temperr, &err, 1, MPI_DOUBLE, MPI_MAX, slucomm );)", "label": "(26, MPI, MPI_Allreduce( &temperr, &errcomp, 1, MPI_FLOAT, MPI_MAX, slucomm);); (23, MPI, MPI_Allreduce( &tempxnorm, &xnorm, 1, MPI_DOUBLE, MPI_MAX, slucomm);); (20, MPI, MPI_Allreduce( &temperr, &err, 1, MPI_DOUBLE, MPI_MAX, slucomm);)"}
{"predict": "(8, OpenMP, #pragma omp target map(present, alloc: arr[0:0])); (6, OpenMP, #pragma omp target data map(alloc: arr[0:5]))", "label": "(8, OpenMP, #pragma omp target map(present, alloc: arr[0:0])); (6, OpenMP, #pragma omp target data map(alloc: arr[0:5])); (19, OpenMP, #pragma omp target data map(alloc: arr[0:0]))"}
{"predict": "(14, MPI, err = MPI_Barrier(inter);); (23, MPI, MPI_Comm_free(&ab_inter);); (23, MPI, MPI_Comm_free(&ab_intra);); (16, MPI, err = MPI_Intercomm_merge(inter, 0, &ab_c_inter);); (10, MPI, err = MPI_Intercomm_create(ab_intra, 0, ac_intra, tag, count, &inter);); (20, MPI, MPI_Comm_free(&ac_inter);); (20, MPI, MPI_Comm_free(&ac_intra);); (15, MPI, err = MPI_Barrier(ab_c_inter);); (17, MPI, MPI_Comm_free(&ab_c_inter);); (15, MPI, err = MPI_Intercomm_merge(inter, 1, &abc_intra);)", "label": "(14, MPI, err = MPI_Barrier(ab_c_inter);); (16, MPI, err = MPI_Intercomm_merge(ab_c_inter, 0, &abc_intra);); (25, MPI, MPI_Comm_disconnect(&ab_inter);); (21, MPI, MPI_Comm_free(&ab_intra);); (18, MPI, MPI_Comm_free(&abc_intra);); (5, MPI, err = MPI_Intercomm_create(ab_intra, 0, ac_intra, count, tag, &ab_c_inter);); (17, MPI, MPI_Comm_free(&ab_c_inter);); (18, MPI, MPI_Comm_free(&ac_intra);); (20, MPI, MPI_Comm_disconnect(&ac_inter);); (10, MPI, err = MPI_Barrier(abc_intra);)"}
{"predict": "(2, MPI, MPI_Init(&argc, &argv);); (55, MPI, MPI_Finalize();)", "label": "(2, MPI, MPI_Init(&argc, &argv);); (55, MPI, MPI_Finalize();)"}
{"predict": "(5, OpenMP, #pragma omp barrier); (3, OpenMP, #pragma omp parallel num_threads(2))", "label": "(5, OpenMP, #pragma omp parallel num_threads(2)); (3, OpenMP, omp_set_nested(1);)"}
{"predict": "(24, OpenCL, clReleaseEvent(events[i]);)", "label": "(23, OpenCL, clReleaseEvent(events[i]);)"}
{"predict": "(6, OpenMP, #pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp parallel for)"}
{"predict": "(19, TBB, oneapi::tbb::global_control control(oneapi::tbb::global_control::max_allowed_parallelism, p);); (20, TBB, utility::graph_test::graph_multifunction_test<buffer, not_gate, and_gate<2>, or_gate<3> > g;); (18, TBB, oneapi::tbb::tick_count mainStartTime = oneapi::tbb::tick_count::now();); (153, TBB, utility::graph_test::graph_multifunction_test<buffer, toggle, reserving_node<int> > g;); (3, TBB, bool verbose = false;); (18, TBB, oneapi::tbb::tick_count runStartTime = oneapi::tbb::tick_count::now();); (195, TBB, utility::graph_test::graph_multifunction_test<buffer, toggle,rejecting_node<int> > g;); (134, TBB, utility::graph_test::graph_multifunction_test<buffer, toggle, and_gate<2>, or_gate<3>, reserving_node<int> > g;); (174, TBB, utility::graph_test::graph_multifunction_test<buffer, toggle, rejecting_node<int>, reserving_node<int> > g;); (184, TBB, utility::graph_test::graph_multifunction_test<buffer, toggle, rejecting_node<int> > g;); (139, TBB, utility::graph_test::graph_multifunction_test<buffer, toggle, or_gate<3>, reserving_node<int> > g;); (10, TBB, oneapi::tbb::tick_count stopTime = oneapi::tbb::tick_count::now();); (160, TBB, utility::graph_test::graph_multifunction_test<buffer, toggle, and_gate<2>, or_gate<3>, rejecting_node<int> > g;); (153, TBB, utility::graph_test::graph_multifunction_test<buffer, toggle, and", "label": "(720, TBB, utility::report_elapsed_time((oneapi::tbb::tick_count::now() - start).seconds());); (15, TBB, oneapi::tbb::global_control c(oneapi::tbb::global_control::max_allowed_parallelism, p);); (19, TBB, oneapi::tbb::flow::graph g;); (11, TBB, oneapi::tbb::tick_count start = oneapi::tbb::tick_count::now();)"}
{"predict": "(96, CUDA, CHECK(cudaStreamDestroy(streams[i]));); (89, CUDA, CHECK(cudaFree(d_y));); (57, CUDA, CHECK(cudaStreamDestroy(streams[n]));); (19, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice));); (87, CUDA, CHECK(cudaFree(d_z));); (40, CUDA, CHECK(cudaStreamCreate(&(streams[n])));); (14, CUDA, CHECK(cudaMalloc(&d_y, M));); (81, CUDA, CHECK(cudaFreeHost(h_y2));); (68, CUDA, CHECK(cudaMallocHost(&h_x2, M));); (11, CUDA, CHECK(cudaMalloc(&d_x, M));); (77, CUDA, CHECK(cudaFreeHost(h_z2));); (69, CUDA, CHECK(cudaMallocHost(&h_z2, M));); (80, CUDA, CHECK(cudaFreeHost(h_x2));); (10, CUDA, CHECK(cudaMalloc(&d_z, M));); (13, CUDA, CHECK(cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice));); (58, CUDA, CHECK(cudaMallocHost(&h_y2, M));); (76, CUDA, CHECK(cudaFree(d_x));)", "label": "(47, CUDA, CHECK(cudaStreamCreate(&(streams[n])));); (84, CUDA, CHECK(cudaStreamDestroy(streams[i]));); (94, CUDA, CHECK(cudaFree(d_y));); (19, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice));); (93, CUDA, CHECK(cudaFree(d_z));); (53, CUDA, CHECK(cudaStreamDestroy(streams[n]));); (58, CUDA, CHECK(cudaMallocHost(&h_x2, M));); (82, CUDA, CHECK(cudaFreeHost(h_y2));); (82, CUDA, CHECK(cudaFreeHost(h_z2));); (66, CUDA, CHECK(cudaStreamCreate(&(streams[i])));); (10, CUDA, CHECK(cudaMalloc(&d_y, M));); (8, CUDA, CHECK(cudaMalloc(&d_x, M));); (83, CUDA, CHECK(cudaFree(d_x));); (53, CUDA, CHECK(cudaMallocHost(&h_z2, M));); (7, CUDA, CHECK(cudaMalloc(&d_z, M));); (73, CUDA, CHECK(cudaFreeHost(h_x2));); (7, CUDA, CHECK(cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice));); (48, CUDA, CHECK(cudaMallocHost(&h_y2, M));)"}
{"predict": "(46, MPI, CHECK(MPI_Finalize());); (13, MPI, MPI_Init(NULL, NULL);); (23, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (39, MPI, CHECK(MPI_File_delete(TEST_FILENAME, MPI_INFO_NULL));); (21, MPI, MPI_File_set_view(fileh, 0, MPI_INT, MPI_INT, datarep[i], MPI_INFO_NULL);); (28, MPI, CHECK(MPI_File_close(&fileh));)", "label": "(14, MPI, MPI_Init(&argc, &argv);); (47, MPI, MPI_Finalize();); (33, MPI, MPI_Barrier(MPI_COMM_WORLD);); (12, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (40, MPI, CHECK(MPI_File_delete(TEST_FILENAME, MPI_INFO_NULL));); (27, MPI, CHECK(MPI_File_close(&fileh));); (19, MPI, CHECK(MPI_File_set_view(fileh, 0, MPI_INT, MPI_INT, datarep[i], MPI_INFO_NULL));)"}
{"predict": "(34, MPI, latency = (MPI_Wtime() - t0) / (2 * g_latency012_reps);); (27, MPI, MPI_Send(NULL, 0, MPI_BYTE, p->nbor, 1, MPI_COMM_WORLD);); (14, MPI, t0 = MPI_Wtime();); (28, MPI, MPI_Recv(NULL, 0, MPI_BYTE, p->nbor, 1, MPI_COMM_WORLD, &status);); (16, MPI, MPI_Send(NULL, 0, MPI_BYTE, p->nbor, 1, MPI_COMM_WORLD);); (12, MPI, MPI_Recv(NULL, 0, MPI_BYTE, p->nbor, 1, MPI_COMM_WORLD, &status);)", "label": "(28, MPI, MPI_Recv(NULL, 0, MPI_BYTE, p->nbor2, 1, MPI_COMM_WORLD, &status);); (22, MPI, MPI_Recv(NULL, 0, MPI_BYTE, p->nbor, 1, MPI_COMM_WORLD, &status);); (25, MPI, MPI_Send(NULL, 0, MPI_BYTE, p->nbor2, 1, MPI_COMM_WORLD);); (34, MPI, latency = (MPI_Wtime() - t0) / (2 * g_latency012_reps);); (18, MPI, MPI_Send(NULL, 0, MPI_BYTE, p->nbor, 1, MPI_COMM_WORLD);); (10, MPI, t0 = MPI_Wtime();)"}
{"predict": "(87, CUDA, cudaFree(gpuMatrix);); (45, CUDA, cudaMalloc((void**)&gpuMatrix, NUM * sizeof(float));); (45, CUDA, cudaMalloc((void**)&gpuTransposeMatrix, NUM * sizeof(float));); (45, CUDA, cudaMemcpy(gpuMatrix, Matrix, NUM * sizeof(float), cudaMemcpyHostToDevice);); (86, CUDA, cudaFree(gpuTransposeMatrix);); (17, CUDA, cudaGetDeviceProperties(&devProp, 0);)", "label": "(20, CUDA, cudaGetDeviceProperties(&devProp, 0);); (81, CUDA, cudaFree(gpuMatrix);); (43, CUDA, cudaMalloc((void**)&gpuMatrix, NUM * sizeof(float));); (81, CUDA, cudaFree(gpuTransposeMatrix);); (43, CUDA, cudaMalloc((void**)&gpuTransposeMatrix, NUM * sizeof(float));)"}
{"predict": "(53, MPI, MPI_Finalize();); (23, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (18, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (47, MPI, MPI_T_finalize();); (18, MPI, MPI_T_init_thread(required, &provided);); (32, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(24, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (19, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (50, MPI, MPI_Finalize();); (47, MPI, MPI_T_finalize();); (18, MPI, MPI_T_init_thread(required, &provided);); (31, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(26, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (20, MPI, MPI_Gatherv_c(sendbuf, count, datatype, recvbuf, counts, displs, datatype, root, comm);); (21, MPI, MPI_Barrier(comm);)", "label": "(22, MPI, MPI_Gatherv_c(sendbuf, count, datatype, recvbuf, counts, displs, datatype, root, comm);); (26, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);)"}
{"predict": "(243, OpenMP, #pragma omp taskloop firstprivate(ii)); (251, OpenMP, #pragma omp taskloop collapse(2)); (2, OpenMP, #pragma omp parallel); (234, OpenMP, #pragma omp taskloop private(ii)); (2, OpenMP, #pragma omp taskloop); (250, OpenMP, #pragma omp taskloop lastprivate(ii)); (245, OpenMP, #pragma omp taskloop linear(ii))", "label": "(286, OpenMP, #pragma omp taskloop lastprivate(ii)); (280, OpenMP, #pragma omp taskloop private(ii)); (5, OpenMP, #pragma omp parallel); (5, OpenMP, #pragma omp taskloop); (317, OpenMP, #pragma omp taskloop collapse(2)); (265, OpenMP, #pragma omp taskloop firstprivate(ii)); (270, OpenMP, #pragma omp taskloop linear(ii))"}
{"predict": "(40, MPI, MPI_T_finalize();); (15, MPI, MPI_T_init_thread(required, &provided);)", "label": "(40, MPI, MPI_T_finalize();); (15, MPI, MPI_T_init_thread(required, &provided);)"}
{"predict": "(109, OpenCL, retVal = clReleaseKernel(kernel);); (34, OpenCL, context = clCreateContext(nullptr, 1, &device, nullptr, nullptr, &retVal);); (103, OpenCL, retVal = clReleaseCommandQueue(cmdQ);); (106, OpenCL, retVal = clReleaseProgram(pProgram);); (108, OpenCL, retVal = clReleaseContext(context);); (57, OpenCL, kernel = clCreateKernel(pProgram, \"CopyBuffer\", &retVal);); (31, OpenCL, cmdQ = clCreateCommandQueue(context, device, properties, &retVal);); (95, OpenCL, retVal = clReleaseMemObject(kernelExecQueue[0].gtpinResource);)", "label": "(104, OpenCL, retVal = clReleaseKernel(kernel);); (34, OpenCL, context = clCreateContext(nullptr, 1, &device, nullptr, nullptr, &retVal);); (108, OpenCL, retVal = clReleaseCommandQueue(cmdQ);); (110, OpenCL, retVal = clReleaseContext(context);); (57, OpenCL, kernel = clCreateKernel(pProgram, \"CopyBuffer\", &retVal);); (102, OpenCL, retVal = clReleaseProgram(pProgram);); (33, OpenCL, cmdQ = clCreateCommandQueue(context, device, properties, &retVal);)"}
{"predict": "(14, CUDA, return CUDA(cudaGetLastError());)", "label": "(14, CUDA, return CUDA(cudaGetLastError());)"}
{"predict": "(7, OpenMP, #pragma omp master); (1, OpenMP, #pragma omp target parallel map(tofrom); (9, OpenMP, #pragma omp target parallel map(tofrom, scalar); (3, OpenMP, #pragma omp target parallel nowait)", "label": "(1, OpenMP, #pragma omp target teams); (5, OpenMP, #pragma omp parallel)"}
{"predict": "(235, MPI, MPI_Info_create(&rma_winfo);); (135, MPI, MPI_Bcast(&n,          1, MPI_INT, root, MPI_COMM_WORLD);); (300, MPI, MPI_Win_free(&rma_winx);); (133, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (300, MPI, MPI_Info_free(&rma_winfo);); (300, MPI, MPI_Win_free(&rma_winy);); (40, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (233, MPI, MPI_Win_create(bottom_buf_in,  total_length_in * sizeof(DTYPE), sizeof(DTYPE), rma_winfo, MPI_COMM_WORLD, &rma_winy);); (231, MPI, MPI_Win_create(left_buf_in,   total_length_in * sizeof(DTYPE), sizeof(DTYPE), rma_winfo, MPI_COMM_WORLD, &rma_winx);); (37, MPI, MPI_Init(&argc,&argv);); (130, MPI, MPI_Bcast(&radius,     1, MPI_INT, root, MPI_COMM_WORLD);)", "label": "(210, MPI, MPI_Info_create(&rma_winfo);); (212, MPI, MPI_Info_set(rma_winfo, \"no_locks\", \"true\");); (370, MPI, MPI_Info_free(&rma_winfo);); (371, MPI, MPI_Finalize();); (128, MPI, MPI_Bcast(&n,          1, MPI_INT, root, MPI_COMM_WORLD);); (128, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (267, MPI, MPI_Win_fence(MPI_MODE_NOSTORE, rma_winx);); (39, MPI, MPI_Init(&argc,&argv);); (232, MPI, MPI_Barrier(MPI_COMM_WORLD);); (321, MPI, MPI_Reduce(&local_norm, &norm, 1, MPI_DTYPE, MPI_SUM, root, MPI_COMM_WORLD);); (38, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (234, MPI, MPI_Win_fence(MPI_MODE_NOSTORE, rma_winy);); (35, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(28, OpenCL, retVal = clReleaseMemObject(image);); (6, OpenCL, auto status = clGetDeviceInfo(&mockClDevice, CL_DEVICE_IMAGE_SUPPORT, sizeof(imageSupportInfo), &imageSupportInfo, nullptr);)", "label": "(28, OpenCL, retVal = clReleaseMemObject(image);); (6, OpenCL, auto status = clGetDeviceInfo(&mockClDevice, CL_DEVICE_IMAGE_SUPPORT, sizeof(imageSupportInfo), &imageSupportInfo, nullptr);)"}
{"predict": "(34, MPI, MPI_Finalize();); (14, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (12, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (13, MPI, MPI_Dims_create(size, NR_DIMS, dims);); (16, MPI, MPI_Cart_coords(cart_comm, rank, NR_DIMS, cart_coords);); (29, MPI, MPI_Barrier(MPI_COMM_WORLD);); (7, MPI, MPI_Init(NULL, NULL);)", "label": "(12, MPI, MPI_Init(&argc, &argv);); (34, MPI, MPI_Finalize();); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (11, MPI, MPI_Dims_create(size, NR_DIMS, dims);); (17, MPI, MPI_Cart_coords(cart_comm, rank, NR_DIMS, cart_coords);); (27, MPI, MPI_Barrier(MPI_COMM_WORLD);); (13, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(27, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(27, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(67, CUDA, cudaFree(df_A);); (42, CUDA, cudaMemsetAsync(df_C, 0, M * N * sizeof(*df_C), stream);); (67, CUDA, cudaFree(df_C);); (36, CUDA, cudaMalloc((void **)&df_B, K * N * sizeof(*df_B));); (34, CUDA, cudaMalloc((void **)&df_A, M * K * sizeof(*df_A));); (58, CUDA, cudaStreamSynchronize(stream);); (35, CUDA, cudaMalloc((void **)&df_C, M * N * sizeof(*df_C));); (59, CUDA, cudaFree(df_B);); (61, CUDA, cudaDeviceReset();); (62, CUDA, cudaStreamDestroy(stream);); (30, CUDA, cudaStreamCreate(&stream);)", "label": "(39, CUDA, cudaMalloc((void **)&df_C, M * N * sizeof(float));); (62, CUDA, cudaFree(df_A);); (36, CUDA, cudaMalloc((void **)&df_B, K * N * sizeof(float));); (38, CUDA, cudaStat = cudaStreamCreate(&stream);); (61, CUDA, cudaFree(df_C);); (32, CUDA, cudaMalloc((void **)&df_A, M * K * sizeof(float));); (55, CUDA, cudaStreamDestroy(stream);); (48, CUDA, cudaStreamSynchronize(stream);); (56, CUDA, cudaFree(df_B);)"}
{"predict": "(109, CUDA, checkCudaErrors(cudaEventCreate(&start_event));); (30, CUDA, checkCudaErrors(cudaGetDeviceProperties(&prop[i], i));); (131, CUDA, checkCudaErrors(cudaSetDevice(gpu_n - 1));); (130, CUDA, checkCudaErrors(cudaFreeHost(h0));); (130, CUDA, checkCudaErrors(cudaFree(g1));); (80, CUDA, checkCudaErrors(cudaMalloc(&g1, buf_size));); (75, CUDA, checkCudaErrors(cudaSetDevice(gpuid[1]));); (105, CUDA, checkCudaErrors(cudaEventCreate(&stop_event));); (130, CUDA, checkCudaErrors(cudaEventDestroy(stop_event));); (126, CUDA, checkCudaErrors(cudaFree(g0));); (128, CUDA, checkCudaErrors(cudaEventDestroy(start_event));); (70, CUDA, checkCudaErrors(cudaSetDevice(gpuid[0]));); (65, CUDA, checkCudaErrors(cudaMalloc(&g0, buf_size));); (124, CUDA, checkCudaErrors(cudaFreeHost(h1));); (125, CUDA, checkCudaErrors(cudaFree(l2_norm_d));); (125, CUDA, checkCudaErrors(cudaFree(l2_global_norm_d));); (67, CUDA, checkCudaErrors(cudaMallocHost(&h1, buf_size));); (90, CUDA, checkCudaErrors(cudaEventRecord(stop_event));); (86, CUDA, checkCudaErrors(cudaEventRecord(start_event));); (27, CUDA, checkCudaErrors(cudaSetDevice(i));); (112, CUDA, checkCudaErrors(cudaFreeHost(h0));); (107, CUDA, checkCudaErrors(cudaMalloc(&l2_norm_d, sizeof(float)));); (107, CUDA, checkCudaErrors(cudaMalloc(&l2_global_norm_d, sizeof(float)));); (108, CUDA, checkCudaErrors(cudaMemset(l2_norm_d, 0, sizeof(float)));); (108, CUDA", "label": "(46, CUDA, checkCudaErrors(cudaDeviceCanAccessPeer(&can_access_peer, i, j));); (99, CUDA, checkCudaErrors(cudaEventCreateWithFlags(&stop_event, eventflags));); (101, CUDA, checkCudaErrors(cudaEventRecord(start_event, 0));); (107, CUDA, checkCudaErrors(cudaMemcpy(g1, g0, buf_size, cudaMemcpyDefault));); (190, CUDA, checkCudaErrors(cudaFreeHost(h0));); (142, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (156, CUDA, checkCudaErrors(cudaMemcpy(h0, g0, buf_size, cudaMemcpyDefault));); (109, CUDA, checkCudaErrors(cudaEventRecord(stop_event, 0));); (6, CUDA, checkCudaErrors(cudaGetDeviceCount(&gpu_n));); (66, CUDA, checkCudaErrors(cudaSetDevice(gpuid[0]));); (181, CUDA, checkCudaErrors(cudaFree(g0));); (20, CUDA, checkCudaErrors(cudaGetDeviceProperties(&prop[i], i));); (74, CUDA, checkCudaErrors(cudaMalloc(&g0, buf_size));); (176, CUDA, checkCudaErrors(cudaEventDestroy(stop_event));); (62, CUDA, checkCudaErrors(cudaDeviceEnablePeerAccess(gpuid[1], 0));); (97, CUDA, checkCudaErrors(cudaMemcpy(g0, g1, buf_size, cudaMemcpyDefault));); (102, CUDA, checkCudaErrors(cudaEventElapsedTime(&time_memcpy, start_event, stop_event));); (171, CUDA, checkCudaErrors(cudaEventDestroy(start_event));); (59, CUDA, checkCudaErrors(cudaSetDevice(gpuid[1]));); (70, CUDA, checkCudaErrors(cudaMalloc(&g1, buf_size));); (58, CUDA, checkCudaErrors(cudaDeviceEnablePeerAccess(gpuid[0], 0));); (163, CUDA, checkCudaErrors(cudaDeviceDisablePeerAccess(gpuid[0]));); (160, CUDA, checkCudaErrors(cudaDeviceDisablePeerAccess(gpuid[1]));); (76, CUDA, checkCudaErrors(cudaEventCreateWithFlags(&start_event, eventflags));); (173, CUDA, checkCudaErrors(cudaSetDevice(i));); (107, CUDA, checkCudaErrors(cudaMemcpy(g0, h0, buf_size, cudaMemcpyDefault));); (91, CUDA, checkCudaErrors(cudaEventSynchronize(stop_event));); (166, CUDA, checkCudaErrors(cudaFree(g1));)"}
{"predict": "(3, CUDA, Check_CudaErroorFun(\"At the beginning.\");)", "label": "(3, CUDA, Check_CudaErroorFun(\"At the beginning.\");)"}
{"predict": "(2, MPI, MPI_Comm_size(MPI_COMM_WORLD, &result.size);); (0, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &result.rank);); (25, MPI, MPI_Comm_rank(result.inter, &result.lrank);); (25, MPI, MPI_Comm_size(result.inter, &result.lsize);); (18, MPI, MPI_Barrier(result.local);)", "label": "(4, MPI, MPI_Comm_size(result.global, &result.size);); (4, MPI, MPI_Comm_rank(result.global, &result.rank);); (1, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &result.global);); (15, MPI, MPI_Comm_rank(result.local, &result.local_rank);); (12, MPI, MPI_Comm_split(result.global, result.group, result.rank, &result.local);); (-3, MPI, MPI_Init(nullptr, nullptr);); (11, MPI, MPI_Comm_size(result.local, &result.local_size);)"}
{"predict": "(17, MPI, MPI_Barrier(MPI_COMM_WORLD);); (17, MPI, time(&time_ended);); (253, MPI, MPI_Allreduce(&time_ended, &all_time_ended, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);); (231, MPI, MPI_Allreduce(&time_started, &all_time_started, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);); (250, MPI, MPI_Allreduce(&rc, &all_rc, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);); (214, MPI, MPI_Reduce(&rc, &local_rc, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);); (246, MPI, MPI_Allreduce(&rc, &all_rc, 1, MPI_INT, MPI_MAX, MPI_COMM_WORLD);); (13, MPI, MPI_Barrier(MPI_COMM_WORLD);); (215, MPI, MPI_Reduce(&rc, &local_rc, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);); (154, MPI, double wtime_started = MPI_Wtime();); (154, MPI, double wtime_ended = MPI_Wtime();); (146, MPI, MPI_Allreduce(&wtime_ended, &all_wtime_ended, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);); (144, MPI, MPI_Allreduce(&wtime_started, &all_wtime_started, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);); (21, MPI, MPI_Barrier(MPI_COMM_WORLD);); (14, MPI, MPI_Barrier(MPI_COMM_WORLD);); (19, MPI, double time_started = MPI_Wtime();); (23, MPI, MPI_Allreduce(&time_ended, &all_time_ended, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);); (12, MPI, MPI_Allreduce(&time_started, &", "label": "(17, MPI, MPI_Barrier(MPI_COMM_WORLD);); (26, MPI, double start_compare = MPI_Wtime();); (199, MPI, double end_compare = MPI_Wtime();)"}
{"predict": "(24, CUDA, CUDASimulation cudaSimulation(model);)", "label": "(24, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (47, MPI, MPI_Comm_free(&comm4);); (40, MPI, MPI_Send(buffer, 0, MPI_INT, rank, 0, MPI_COMM_WORLD);); (47, MPI, MPI_Comm_free(&comm2);); (43, MPI, MPI_Recv(buffer, 0, MPI_INT, rank, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (45, MPI, MPI_Comm_free(&comm1);); (25, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &comm4, buffer);); (38, MPI, MPI_Barrier(MPI_COMM_WORLD);); (23, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &comm2, buffer);); (20, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &comm1, buffer);)", "label": "(54, MPI, MPI_Comm_free(&comm2);); (46, MPI, MPI_Ssend(buffer, 0, MPI_INT, rank, 1, MPI_COMM_WORLD);); (31, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &comm2);); (43, MPI, MPI_Barrier(comm4);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (38, MPI, MPI_Comm_dup(comm1, &comm4);); (46, MPI, MPI_Comm_free(&comm4);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (45, MPI, MPI_Comm_free(&comm1);); (39, MPI, MPI_Recv(buffer, 0, MPI_INT, rank, 2, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (30, MPI, MPI_Ssend(buffer, 0, MPI_INT, rank, 0, MPI_COMM_WORLD);); (21, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &comm1);)"}
{"predict": "(75, MPI, MPI_Wait(&requests[NUMBER_OF_NEIGHBORS + i], MPI_STATUS_IGNORE);); (71, MPI, MPI_Send(send_buffers[i], size, MPI_BYTE, neighbours[i], 0, MPI_COMM_WORLD);); (76, MPI, MPI_Send(send_buffers[i], size, MPI_BYTE, neighbours[i], 1, MPI_COMM_WORLD);); (84, MPI, MPI_Send(send_buffers[i], size, MPI_BYTE, neighbours[i], 0, MPI_COMM_WORLD);); (64, MPI, MPI_Irecv(recv_buffers[i], size, MPI_BYTE, neighbours[i], 1, MPI_COMM_WORLD, &requests[NUMBER_OF_NEIGHBORS + i]);); (70, MPI, MPI_Wait(&requests[i], MPI_STATUS_IGNORE);); (73, MPI, MPI_Recv(recv_buffers[i], size, MPI_BYTE, neighbours[i], 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)", "label": "(75, MPI, MPI_Waitall((NUMBER_OF_NEIGHBORS * 2), requests, MPI_STATUSES_IGNORE);); (97, MPI, time = MPI_Wtime() - time;); (71, MPI, MPI_Isend(send_buffers[i], size, MPI_BYTE, neighbours[i], 0, MPI_COMM_WORLD, &requests[NUMBER_OF_NEIGHBORS + i]);); (78, MPI, time = MPI_Wtime();); (101, MPI, MPI_Send(&report, sizeof(imb_p2p_report_t), MPI_BYTE, 0, 1, MPI_COMM_WORLD);); (65, MPI, MPI_Irecv(recv_buffers[i], size, MPI_BYTE, neighbours[i], 0, MPI_COMM_WORLD, &requests[i]);); (108, MPI, MPI_Recv(&report, sizeof(imb_p2p_report_t), MPI_BYTE, source_rank, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)"}
{"predict": "(64, CUDA, cudaEventRecord( evStop );); (76, CUDA, cudaFree( out );); (37, CUDA, cudaEventRecord( evStart );); (73, CUDA, cudaFree( in );); (60, CUDA, cudaEventSynchronize( evStop );); (70, CUDA, cudaEventDestroy( evStart );); (70, CUDA, cudaEventDestroy( evStop );)", "label": "(59, CUDA, cudaEventRecord( evStop );); (75, CUDA, cudaFree( in );); (72, CUDA, cudaEventDestroy( evStart );); (52, CUDA, cudaEventRecord( evStart );); (73, CUDA, cudaFree( out );); (70, CUDA, cudaEventDestroy( evStop );)"}
{"predict": "(35, OpenMP, #pragma omp target teams distribute parallel for map(a, b))", "label": "(35, OpenMP, #pragma omp target teams distribute parallel for map(a, b))"}
{"predict": "(38, CUDA, checkCudaErrors(cudaMemset(d_Output, 0, QRNG_DIMENSIONS * N * sizeof(float)));); (43, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (124, CUDA, checkCudaErrors(cudaFree(d_Output));)", "label": "(40, CUDA, checkCudaErrors(cudaMemset(d_Output, 0, QRNG_DIMENSIONS * N * sizeof(float)));); (44, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (124, CUDA, checkCudaErrors(cudaFree(d_Output));)"}
{"predict": "(16, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize(), \"vKKwij_to_vwKiKj\");); (14, CUDA, qmc_cuda::cuda_check(cudaGetLastError(), \"vKKwij_to_vwKiKj\");)", "label": "(15, CUDA, qmc_cuda::cuda_check(cudaGetLastError(), \"vKKwij_to_vwKiKj\");); (15, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize(), \"vKKwij_to_vwKiKj\");)"}
{"predict": "(9, OpenCL, auto userEvent = clCreateUserEvent(pContext, &retVal);); (30, OpenCL, retVal = clReleaseEvent(userEvent);)", "label": "(9, OpenCL, auto userEvent = clCreateUserEvent(pContext, &retVal);); (30, OpenCL, retVal = clReleaseEvent(userEvent);)"}
{"predict": "(10, OpenMP, #pragma omp teams distribute parallel for); (8, OpenMP, #pragma omp target); (13, OpenMP, #pragma omp teams distribute parallel for if (parallel: false))", "label": "(10, OpenMP, #pragma omp teams distribute parallel for); (8, OpenMP, #pragma omp target); (12, OpenMP, #pragma omp teams distribute parallel for if (parallel: false))"}
{"predict": "(65, MPI, MPI_Type_free(&parent_type);); (12, MPI, MPI_Pack_external_size((char *) \"external32\", 1, MPI_INT, &sizeofint);); (35, MPI, MPI_Pack(array, 1, parent_type, buf, sizeoftype, &position, MPI_COMM_WORLD);); (46, MPI, MPI_Unpack(buf, sizeoftype, &position, array, 1, parent_type, MPI_COMM_WORLD);); (21, MPI, MPI_Type_commit(&parent_type);); (18, MPI, MPI_Type_vector(10, 2, 2, MPI_INT, &parent_type);); (22, MPI, MPI_Pack_external_size((char *) \"external32\", 1, parent_type, &sizeoftype);)", "label": "(12, MPI, MPI_Pack_external_size((char *) \"external32\", 1, MPI_INT, &sizeofint);); (66, MPI, MPI_Type_free(&parent_type);); (36, MPI, MPI_Pack_external((char *) \"external32\", array, 1, parent_type, buf, sizeoftype, &position);); (46, MPI, MPI_Unpack_external((char *) \"external32\", buf, sizeoftype, &position, array, 1, parent_type);); (21, MPI, MPI_Pack_external_size((char *) \"external32\", 1, parent_type, &sizeoftype);); (18, MPI, MPI_Type_commit(&parent_type);); (15, MPI, MPI_Type_vector(10, 2, 2, MPI_INT, &parent_type);)"}
{"predict": "(16, SYCL, auto B = sycl::malloc_device<TB>( nelems, q);); (16, SYCL, auto C = sycl::malloc_device<TC>( nelems, q);); (50, SYCL, sycl::free(h_a, q);); (4, SYCL, auto h_a = sycl::malloc_host<TA>( nelems, q);); (4, SYCL, auto h_b = sycl::malloc_host<TB>( nelems, q);); (50, SYCL, sycl::free(h_b, q);); (50, SYCL, sycl::free(h_c, q);); (11, SYCL, auto A = sycl::malloc_device<TA>( nelems, q);); (44, SYCL, sycl::free(A, q);); (44, SYCL, sycl::free(B, q);); (44, SYCL, sycl::free(C, q);); (3, SYCL, auto h_c = sycl::malloc_host<TC>( nelems, q);)", "label": "(82, SYCL, auto isA_fp16 = (typeid(TA) == typeid(sycl::half));); (28, SYCL, sycl::free(h_a, q);); (6, SYCL, auto h_c = sycl::malloc_host<TC>( nelems, q);); (4, SYCL, auto h_b = sycl::malloc_host<TB>( nelems, q);); (111, SYCL, sycl::free(h_c, q);); (14, SYCL, auto  A = sycl::malloc_device<TA>( nelems, q);); (94, SYCL, auto isC_fp16 = (typeid(TC) == typeid(sycl::half));); (23, SYCL, sycl::free(h_b, q);); (49, SYCL, sycl::free(A, q);); (47, SYCL, sycl::free(B, q);); (-4, SYCL, auto h_a = sycl::malloc_host<TA>( nelems, q);); (80, SYCL, auto isB_fp16 = (typeid(TB) == typeid(sycl::half));); (9, SYCL, auto  C = sycl::malloc_device<TC>( nelems, q);); (7, SYCL, auto  B = sycl::malloc_device<TB>( nelems, q);); (41, SYCL, sycl::free(C, q);)"}
{"predict": "(13, MPI, mpi_errno = MPI_Comm_create_group(MPI_COMM_WORLD, group, 0, NULL);); (17, MPI, MPI_Group_free(&group);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (4, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (8, MPI, MPI_Comm_group(MPI_COMM_WORLD, &group);); (12, MPI, MPI_Error_class(mpi_errno, &errclass);); (5, MPI, MPI_Comm_set_errhandler(dup_comm, MPI_ERRORS_RETURN);); (5, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &dup_comm);)", "label": "(16, MPI, mpi_errno = MPI_Comm_create_group(dup_comm, group, 10, NULL);); (20, MPI, MPI_Comm_free(&dup_comm);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (18, MPI, MPI_Group_free(&group);); (12, MPI, MPI_Error_class(mpi_errno, &errclass);); (7, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (5, MPI, MPI_Comm_group(dup_comm, &group);); (3, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &dup_comm);)"}
{"predict": "(18, MPI, MPI_Type_commit(&xpose[j]);); (35, MPI, MPI_Type_free(&xpose[j]);); (32, MPI, MPI_Type_free(&column[j]);); (13, MPI, MPI_Type_hvector(ncols, 1, sizeof(double), MPI_DOUBLE, &column[j]);); (13, MPI, MPI_Type_create_hpose(nrows, 1, column, xpose);); (15, MPI, t[i] = MPI_Wtime();); (16, MPI, MPI_Allreduce(&tmp, &t[i], 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);)", "label": "(24, MPI, t[i] = MPI_Wtime() - ttmp;); (19, MPI, MPI_Type_commit(&xpose[j]);); (34, MPI, MPI_Type_free(&column[j]);); (32, MPI, MPI_Type_free(&xpose[j]);); (11, MPI, ttmp = MPI_Wtime();); (13, MPI, MPI_Type_vector(nrows, 1, ncols, MPI_INT, &column[j]);); (13, MPI, MPI_Type_create_hvector(ncols, 1, sizeof(int), column[j], &xpose[j]);)"}
{"predict": "(18, CUDA, cudaDeviceSynchronize();)", "label": "(18, CUDA, ASSERT_EQ(cudaGetLastError(), cudaSuccess) << \"Kernel launch error.\";)"}
{"predict": "(2, CUDA, __shared__ float buffer[128];); (2, CUDA, __shared__ float sums[4];); (0, CUDA, __shared__ float gauss[16];)", "label": "(3, CUDA, __shared__ float buffer[128];); (3, CUDA, __shared__ float sums[4];); (0, CUDA, __shared__ float gauss[16];)"}
{"predict": "(6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (24, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(26, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(4, MPI, MPI_Barrier(MPI_COMM_WORLD);); (12, MPI, MPI_Recv(buffer, n, MPI_INT, task, 123, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (4, MPI, MPI_Send(MPI_BOTTOM, 0, MPI_INT, task, 123, MPI_COMM_WORLD);)", "label": "(12, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(18, CUDA, checkCudaErrors(cudaGetLastError());); (13, CUDA, checkCudaErrors(cudaStreamSynchronize(stream));)", "label": "(15, CUDA, checkCudaErrors(cudaGetLastError());); (13, CUDA, checkCudaErrors(cudaStreamSynchronize(stream));)"}
{"predict": "(14, MPI, rc = MPI_Type_get_extent(type, &tmp_lb, &attr->child_type_extent);); (94, MPI, rc = MPI_Type_create_resized(type, 0, extent, newtype);); (92, MPI, rc = MPI_Type_free(&type);); (36, MPI, rc = MPI_Type_commit(newtype);)", "label": "(14, MPI, rc = MPI_Type_get_extent(type, &tmp_lb, &attr->child_type_extent);); (127, MPI, rc = MPI_Type_free(&type);)"}
{"predict": "(14, OpenMP, #pragma omp target update to(fd.b.k)); (8, OpenMP, #pragma omp target update to(fd.a)); (8, OpenMP, #pragma omp target enter data map(to:fd))", "label": "(20, OpenMP, #pragma omp target update to(mapper(id): fd)); (10, OpenMP, #pragma omp declare mapper(id: N1::vec v) map(v.len)); (14, OpenMP, #pragma omp target map(mapper(id) alloc: fd)); (15, OpenMP, #pragma omp target map(mapper(idd) alloc: fd.b)); (19, OpenMP, #pragma omp target update from(mapper(idd): fd.b)); (4, OpenMP, #pragma omp declare mapper(id: struct foodat v) map(v.a)); (15, OpenMP, #pragma omp target update to(mapper(idd): fd.b)); (3, OpenMP, #pragma omp declare mapper(idd: struct foodatchild v) map(v.k)); (14, OpenMP, #pragma omp target update from(mapper(id): fd))"}
{"predict": "(13, CUDA, cudaDeviceSynchronize();)", "label": "(4, CUDA, cudaMemcpy(state.vx, state.dvx, size_f, cudaMemcpyDeviceToHost);); (4, CUDA, cudaMemcpy(state.vy, state.dvy, size_f, cudaMemcpyDeviceToHost);); (4, CUDA, cudaMemcpy(state.rho, state.drho, size_f, cudaMemcpyDeviceToHost);)"}
{"predict": "(3, CUDA, extern __shared__ T _arrays[];)", "label": "(12, CUDA, __shared__ T tails[SMALL_KERNEL_BLOCK * SMALL_KERNEL_BLOCK];); (18, CUDA, extern __shared__ char _arrays[];)"}
{"predict": "(135, OpenCL, error = clSetKernelArg(kernel, 3, sizeof(cl_uint), &ftz);); (132, OpenCL, error = clSetKernelArg(kernel, 1, sizeof(tinfo->inBuf), &tinfo->inBuf);); (132, OpenCL, error = clSetKernelArg(kernel, 2, sizeof(tinfo->inBuf2), &tinfo->inBuf2);); (134, OpenCL, error = clSetKernelArg(kernel, 4, sizeof(ulps), &ulps);); (130, OpenCL, error = clSetKernelArg(kernel, 2, buffer_size, NULL);); (128, OpenCL, error = clSetKernelArg(kernel, 1, buffer_size, NULL);); (126, OpenCL, error = clSetKernelArg(kernel, 0, buffer_size, NULL);)", "label": "(134, OpenCL, error = clSetKernelArg(kernel, 1, sizeof(tinfo->inBuf), &tinfo->inBuf);)"}
{"predict": "(46, MPI, MPI_Finalize();); (16, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myid);)", "label": "(45, MPI, ierr = MPI_Finalize();); (15, MPI, ierr = MPI_Comm_rank(MPI_COMM_WORLD, &myid);)"}
{"predict": "(16, MPI, MPI_Dims_create(tests[k].size, tests[k].dim, dims);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (13, MPI, MPI_Dims_create(tests[k].size, tests[k].dim, dims);)", "label": "(9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (14, MPI, MPI_Dims_create(tests[k].size, tests[k].dim, dims);)"}
{"predict": "(8, MPI, MPI_Finalize();); (0, MPI, MPI_Init(&argc,&argv);)", "label": "(8, MPI, MPI_Finalize();); (0, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(41, CUDA, cudaMemsetAsync(sortClassValidCountPtr, 0, N * sizeof(int), stream);)", "label": "(21, CUDA, CUASSERT(cudaMemsetAsync(sortClassValidCountPtr, 0, N * sizeof(int), stream));); (19, CUDA, CUASSERT(cudaMemsetAsync(argMaxBBoxPtr, 0, N * samples * 4 * sizeof(float), stream));); (20, CUDA, CUASSERT(cudaMemsetAsync(sortClassPosPtr, 0, N * (NClass + 1) * sizeof(int), stream));); (20, CUDA, CUASSERT(cudaMemsetAsync(sortClassSampleIdxPtr, 0, N * (samples + 1) * sizeof(int), stream));); (15, CUDA, CUASSERT(cudaMemsetAsync(argMaxScorePtr, 0, N * samples * sizeof(float), stream));)"}
{"predict": "(14, MPI, MPI_Send(sendbuf.data(), kSendLength, MPI_INT, 0, tag, MPI_COMM_WORLD);); (16, MPI, MPI_Send(pre_info, 2, MPI_INT, 0, kTag, MPI_COMM_WORLD);)", "label": "(11, MPI, MPI_Send(pre_info, 2, MPI_INT, 0, kTag, MPI_COMM_WORLD);)"}
{"predict": "(48, OpenCL, err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &global, NULL, 0, NULL, NULL);); (42, OpenCL, clKernelWrapper kernel = clCreateKernel(prog, spvName, &err);); (26, OpenCL, clMemWrapper in = clCreateBuffer(context, CL_MEM_READ_WRITE, bytes, NULL, &err);); (50, OpenCL, err = clEnqueueReadBuffer(queue, in, CL_TRUE, 0, bytes, &h_out[0], 0, NULL, NULL);); (27, OpenCL, err = clEnqueueWriteBuffer(queue, in, CL_TRUE, 0, bytes, &h_in[0], 0, NULL, NULL);); (38, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(cl_mem), &in);)", "label": "(32, OpenCL, err = clEnqueueWriteBuffer(queue, in, CL_TRUE, 0, bytes, &h_in[0], 0, NULL, NULL);); (44, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(cl_mem), &in);); (51, OpenCL, err = clEnqueueReadBuffer(queue, in, CL_TRUE, 0, bytes, &h_out[0], 0, NULL, NULL);); (26, OpenCL, clMemWrapper in = clCreateBuffer(context, CL_MEM_READ_WRITE, bytes, NULL, &err);); (38, OpenCL, clKernelWrapper kernel = clCreateKernel(prog, spvName, &err);); (44, OpenCL, err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &global, NULL, 0, NULL, NULL);)"}
{"predict": "(6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (6, MPI, MPI_Error_string(err, errorString, &errorStringLen);)", "label": "(6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (6, MPI, MPI_Error_string(err, errorString, &errorStringLen);); (10, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(7, MPI, MPI_Get_library_version(version, &resultlen);)", "label": "(7, MPI, MPI_Get_library_version(version, &resultlen);)"}
{"predict": "(20, TBB, tbb::internal::segmented_iterator<nested_vec, std::pair<int,inner_container> > my_si(my_vec);); (61, TBB, tbb::internal::segmented_iterator<nested_vec, const std::pair<int,inner_container> > my_csi(my_vec);); (78, TBB, tbb::internal::segmented_iterator<nested_vec, const std::pair<int,inner_container> > my_csi(my_vec);); (28, TBB, tbb::internal::segmented_iterator<nested_vec, std::pair<int,inner_container> > my_si(my_vec);); (80, TBB, tbb::internal::segmented_iterator<nested_vec, std::pair<int,inner_container> > my_si(my_vec);); (36, TBB, tbb::internal::segmented_iterator<nested_vec, std::pair<int,inner_container> > my_si(my_vec);); (43, TBB, tbb::internal::segmented_iterator<nested_vec, const std::pair<int,inner_container> > my_csi(my_vec);)", "label": "(61, TBB, tbb::internal::segmented_iterator<nested_vec, const T> my_csi(my_vec);); (18, TBB, tbb::internal::segmented_iterator<nested_vec, T> my_si(my_vec);)"}
{"predict": "(57, OpenCL, err = clEnqueueUnmapMemObject(ctx->image_queue, mem, data, 0, NULL, NULL);); (24, OpenCL, cl_mem buf = clCreateBuffer(ctx, flags, sz, data, &err);); (32, OpenCL, data = clEnqueueMapBuffer(ctx->image_queue, buf, CL_TRUE, CL_MAP_WRITE, 0, sz, 0, NULL, NULL, &err);); (37, OpenCL, clReleaseMemObject(buf);); (56, OpenCL, clReleaseMemObject(mem);)", "label": "(61, OpenCL, clReleaseMemObject(mem);); (33, OpenCL, cl_mem buf = clCreateBuffer(ctx, CL_MEM_USE_HOST_PTR, sz, data, &err);); (23, OpenCL, ctx->image_queue = clCreateCommandQueueWithProperties(ctx, ctx->devices[0], 0, &err);); (55, OpenCL, err = clEnqueueCopyBufferToImage(ctx->image_queue, buf, mem, 0, origin, region, 0, NULL, NULL);); (38, OpenCL, clReleaseMemObject(buf);)"}
{"predict": "(64, OpenMP, #pragma omp distribute parallel for firstprivate(ba)); (86, OpenMP, #pragma omp distribute parallel for firstprivate(S2::S2s)); (14, OpenMP, #pragma omp distribute parallel for firstprivate()); (96, OpenMP, #pragma omp distribute parallel for firstprivate(m)); (115, OpenMP, #pragma omp distribute parallel for firstprivate(j)); (75, OpenMP, #pragma omp distribute parallel for firstprivate(xa)); (67, OpenMP, #pragma omp distribute parallel for firstprivate(da)); (30, OpenMP, #pragma omp distribute parallel for firstprivate(argc,); (36, OpenMP, #pragma omp distribute parallel for firstprivate(S1)); (44, OpenMP, #pragma omp distribute parallel for firstprivate(argv[1])); (87, OpenMP, #pragma omp distribute parallel for safelen(5)); (50, OpenMP, #pragma omp distribute parallel for firstprivate(2 * 2)); (8, OpenMP, #pragma omp distribute parallel for firstprivate(argc); (95, OpenMP, #pragma omp distribute parallel for private(xa), firstprivate(xa)); (9, OpenMP, #pragma omp distribute parallel for firstprivate(argc > 0? argv[1] : argv[2])); (12, OpenMP, #pragma omp distribute parallel for firstprivate(argc)); (55, OpenMP, #pragma omp distribute parallel for firstprivate(S2::S2sc)); (74, OpenMP, #pragma omp distribute parallel for firstprivate(e, g)); (34, OpenMP, #pragma omp distribute parallel for firstprivate(a, b, c, d, f)); (4, OpenMP, #pragma omp distribute parallel for firstprivate(argc, k)); (6, OpenMP, #pragma omp distribute parallel for firstprivate(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (86, OpenMP, #pragma omp distribute parallel for firstprivate(i)); (75, OpenMP, #pragma omp distribute parallel for firstprivate(h", "label": "(51, OpenMP, #pragma omp distribute parallel for firstprivate(a, b, c, d, f)); (86, OpenMP, #pragma omp distribute parallel for firstprivate(S2::S2s)); (158, OpenMP, #pragma omp parallel private(i)); (94, OpenMP, #pragma omp distribute parallel for safelen(5)); (98, OpenMP, #pragma omp distribute parallel for firstprivate(e, g)); (170, OpenMP, #pragma omp distribute parallel for firstprivate(si)); (70, OpenMP, #pragma omp distribute parallel for firstprivate(da)); (75, OpenMP, #pragma omp distribute parallel for firstprivate(xa)); (28, OpenMP, #pragma omp distribute parallel for firstprivate(argc > 0? argv[1] : argv[2])); (113, OpenMP, #pragma omp distribute parallel for firstprivate(i)); (-1, OpenMP, #pragma omp target); (30, OpenMP, #pragma omp distribute parallel for firstprivate(argc)); (133, OpenMP, #pragma omp distribute parallel for lastprivate(n) firstprivate(n)); (99, OpenMP, #pragma omp distribute parallel for firstprivate(k, h, B::x)); (111, OpenMP, #pragma omp parallel); (151, OpenMP, #pragma omp parallel reduction(+ : i)); (0, OpenMP, #pragma omp distribute parallel for firstprivate(); (44, OpenMP, #pragma omp distribute parallel for firstprivate(2 * 2)); (53, OpenMP, #pragma omp distribute parallel for firstprivate(ca)); (114, OpenMP, #pragma omp distribute parallel for firstprivate(j)); (36, OpenMP, #pragma omp distribute parallel for firstprivate(argv[1])); (96, OpenMP, #pragma omp distribute parallel for private(xa), firstprivate(xa)); (70, OpenMP, #pragma omp distribute parallel for firstprivate(S2::S2sc)); (-2, OpenMP, #pragma omp distribute parallel for firstprivate()); (2, OpenMP, #pragma omp distribute parallel for firstprivate(argc); (114, OpenMP, #pragma omp distribute parallel for lastprivate(g) firstprivate(g)); (20, OpenMP, #pragma omp distribute parallel for firstprivate(S1)); (4, OpenMP, #pragma omp distribute parallel for firstprivate(argc,); (79, OpenMP, #pragma omp distribute parallel for firstprivate(m)); (37, OpenMP, #pragma omp distribute parallel for firstprivate(ba)); (-19, OpenMP, #pragma omp distribute parallel for firstprivate); (-21, OpenMP, #pragma omp teams)"}
{"predict": "(11, OpenCL, auto commandQueue = clCreateCommandQueueWithProperties(context.get(), clDevice, properties.data(), &retVal);); (22, OpenCL, clReleaseCommandQueue(commandQueue);)", "label": "(11, OpenCL, auto commandQueue = clCreateCommandQueueWithProperties(pContext, testedClDevice, properties.data(), &retVal);); (21, OpenCL, clReleaseCommandQueue(commandQueue);)"}
{"predict": "(9, MPI, MPI_Type_get_extent(sddt, &lb, &extent);)", "label": "(6, MPI, MPI_Type_get_extent(sddt, &lb, &extent);)"}
{"predict": "(69, OpenMP, const int th=omp_get_thread_num();); (10, OpenMP, #pragma omp parallel for schedule (guided))", "label": "(71, OpenMP, const int th=omp_get_thread_num();); (10, OpenMP, #pragma omp parallel for schedule (guided))"}
{"predict": "(11, CUDA, CUDA_CHECK(cudaGetDeviceProperties(&props, 0));)", "label": "(11, CUDA, CUDA_CHECK(cudaGetDeviceProperties(&props, 0));)"}
{"predict": "(9, MPI, MPI_Finalize();); (1, MPI, MPI_Init(NULL, NULL);)", "label": "(3, MPI, MPI_Init(&argc, &argv);); (9, MPI, MPI_Finalize();)"}
{"predict": "(13, OpenCL, result = clGetPlatformInfo(platform, CL_PLATFORM_EXTENSIONS, 0, NULL, &set_size);); (15, OpenCL, result = clGetPlatformInfo(platform, CL_PLATFORM_EXTENSIONS, set_size, &extensions[0], NULL);)", "label": "(12, OpenCL, result = clGetPlatformInfo(platform, CL_PLATFORM_EXTENSIONS, 0, NULL, &set_size);); (14, OpenCL, result = clGetPlatformInfo(platform, CL_PLATFORM_EXTENSIONS, extensions.size(), extensions.data(), NULL);)"}
{"predict": "(18, OpenMP, #pragma omp teams num_teams (argc > 0? argv[1] : argv[2])); (37, OpenMP, #pragma omp teams num_teams (S1)); (1, OpenMP, #pragma omp teams num_teams (); (3, OpenMP, #pragma omp teams num_teams ()); (26, OpenMP, #pragma omp teams num_teams (argc), num_teams (argc+1)); (30, OpenMP, #pragma omp teams num_teams (argc), num_teams (argc+z)); (37, OpenMP, #pragma omp teams num_teams (3.14)); (2, OpenMP, #pragma omp teams num_teams (argc); (30, OpenMP, #pragma omp teams num_teams (-2)); (14, OpenMP, #pragma omp teams num_teams (argc + argc*z)); (23, OpenMP, #pragma omp teams num_teams (foobool(argc)), num_teams (true), num_teams (-5)); (5, OpenMP, #pragma omp teams num_teams (argc))); (34, OpenMP, #pragma omp teams num_teams (-10u)); (-8, OpenMP, #pragma omp target); (36, OpenMP, #pragma omp teams num_teams (num_teams(tmain<int, -1>(argc)), num_teams (num_teams(tmain<int, -2>(argc)))", "label": "(47, OpenMP, #pragma omp teams num_teams (3.14)); (42, OpenMP, #pragma omp teams num_teams (-10u)); (17, OpenMP, #pragma omp teams num_teams (argc))); (4, OpenMP, #pragma omp teams num_teams (); (7, OpenMP, #pragma omp teams num_teams ()); (10, OpenMP, #pragma omp teams num_teams (argc); (25, OpenMP, #pragma omp teams num_teams (argc), num_teams (argc+1)); (32, OpenMP, #pragma omp teams num_teams (-2)); (15, OpenMP, #pragma omp teams num_teams (argc > 0? argv[1] : argv[2])); (-7, OpenMP, #pragma omp target); (25, OpenMP, #pragma omp teams num_teams (S1)); (16, OpenMP, #pragma omp teams num_teams (argc + argc-z)); (-9, OpenMP, #pragma omp teams num_teams)"}
{"predict": "(100, MPI, MPI_Waitall(2, request, status);); (109, MPI, MPI_Recv(buf_recv, count, MPI_BYTE, rank_recv, 0, comm, &status[0]);); (103, MPI, MPI_Send(buf_send, nread, MPI_BYTE, rank_send, 0, comm);); (76, MPI, MPI_Recv(buf_recv, shuffile_mpi_buf_size, MPI_BYTE, rank_recv, 0, comm, &status[0]);); (95, MPI, MPI_Send(buf_send, count, MPI_BYTE, rank_send, 0, comm);)", "label": "(101, MPI, MPI_Wait(&request[1], &status[1]);); (110, MPI, MPI_Wait(&request[0], &status[0]);); (110, MPI, MPI_Get_count(&status[0], MPI_BYTE, &nwrite);); (74, MPI, MPI_Irecv(buf_recv, shuffile_mpi_buf_size, MPI_BYTE, rank_recv, 0, comm, &request[0]);); (96, MPI, MPI_Isend(buf_send, nread, MPI_BYTE, rank_send, 0, comm, &request[1]);)"}
{"predict": "(7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (51, MPI, MPI_Send(port, PORT_SIZE, MPI_CHAR, members[0], 0, MPI_COMM_WORLD);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (38, MPI, MPI_Open_port(port_info, port);); (40, MPI, MPI_Comm_free(&pgroup_old);); (37, MPI, MPI_Intercomm_merge(pgroup_new, 0 /* LOW */, &pgroup);); (45, MPI, MPI_Send(port, PORT_SIZE, MPI_CHAR, members[i], 0, MPI_COMM_WORLD);); (61, MPI, MPI_Recv(port, PORT_SIZE, MPI_CHAR, members[0], 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (36, MPI, MPI_Close_port(port);); (56, MPI, MPI_Comm_dup(MPI_COMM_SELF, &pgroup_new);); (42, MPI, MPI_Comm_free(&pgroup_new);); (31, MPI, MPI_Intercomm_merge(pgroup_new, 1 /* HIGH */, &pgroup);)", "label": "(48, MPI, MPI_Comm_accept(port, MPI_INFO_NULL, 0, pgroup, &pgroup_new);); (52, MPI, MPI_Comm_free(&pgroup_old);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (64, MPI, MPI_Comm_connect(port, MPI_INFO_NULL, 0, MPI_COMM_SELF, &pgroup_new);); (42, MPI, MPI_Send(port, PORT_SIZE, MPI_CHAR, members[i], 0, MPI_COMM_WORLD);); (35, MPI, MPI_Open_port(port_info, port);); (44, MPI, MPI_Comm_free(&pgroup_new);); (48, MPI, MPI_Close_port(port);); (66, MPI, MPI_Comm_connect(port, MPI_INFO_NULL, 0, pgroup, &pgroup_new);)"}
{"predict": "(5, CUDA, __shared__ typename BlockMergeSort::TempStorage temp_storage_shuffle;)", "label": "(5, CUDA, __shared__ typename BlockMergeSort::TempStorage temp_storage_shuffle;)"}
{"predict": "(15, MPI, MPI_Comm_size(comm, &size);); (21, MPI, MPI_Allreduce(MPI_IN_PLACE, buf, count, MPI_INT, op, comm);); (8, MPI, MPI_Op_create((MPI_User_function *) assoc, 0, &op);); (33, MPI, MPI_Op_free(&op);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(22, MPI, MPI_Allreduce(MPI_IN_PLACE, buf, count, MPI_INT, op, comm);); (13, MPI, MPI_Comm_size(comm, &size);); (35, MPI, MPI_Op_free(&op);); (6, MPI, MPI_Op_create(mysum, 0, &op);); (11, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(34, MPI, MPI_Comm_size(comm, &ranks);); (32, MPI, MPI_Comm_rank(comm, &rank);); (13, MPI, MPI_Type_get_true_extent(keysat, &true_lb, &true_extent);)", "label": "(34, MPI, MPI_Comm_size(comm, &ranks);); (32, MPI, MPI_Comm_rank(comm, &rank);); (12, MPI, MPI_Type_get_true_extent(keysat, &true_lb, &true_extent);)"}
{"predict": "(10, OpenMP, #pragma omp parallel for)", "label": "(10, OpenMP, #pragma omp parallel for)"}
{"predict": "(6, OpenMP, #pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp parallel for)"}
{"predict": "(15, CUDA, cudaStreamDestroy(stream);); (10, CUDA, cudaStreamCreate(&stream);)", "label": "(11, CUDA, cudaStreamCreate(&stream);); (14, CUDA, cudaStreamDestroy(stream);)"}
{"predict": "(3, CUDA, __shared__ clock_t s_clocks[ warpSize ];)", "label": "(3, CUDA, __shared__ clock_t s_clocks[32];)"}
{"predict": "(9, MPI, MPI_Info_create(&info);); (10, MPI, MPI_Info_get(info, key, sizeof(buff) - 1, buff, &flag);); (19, MPI, MPI_Info_free(&info);); (6, MPI, MPI_Info_set(info, key, val);)", "label": "(11, MPI, MPI_Info_get(info, key, sizeof(buff) - 1, buff, &flag);); (8, MPI, MPI_Info_create(&info);); (19, MPI, MPI_Info_free(&info);); (7, MPI, MPI_Info_set(info, key, val);)"}
{"predict": "(110, MPI, MPI_Type_commit(&mpi_word);); (173, MPI, MPI_Finalize();); (64, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (62, MPI, MPI_Bcast(&length,     1, MPI_LONG, root, MPI_COMM_WORLD);); (118, MPI, MPI_Allgather(iterstring,1,mpi_word, catstring,1,mpi_word, MPI_COMM_WORLD);); (105, MPI, MPI_Type_contiguous(proc_length, MPI_CHAR, &mpi_word);); (18, MPI, MPI_Init(&argc,&argv);); (101, MPI, MPI_Type_commit(&mpi_word);); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (98, MPI, MPI_Type_vector(1, proc_length, Num_procs, MPI_CHAR, &mpi_word);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);); (95, MPI, MPI_Type_free(&mpi_word);); (110, MPI, MPI_Barrier(MPI_COMM_WORLD);); (103, MPI, MPI_Bcast(basestring, proc_length+1, MPI_CHAR, root, MPI_COMM_WORLD);); (162, MPI, MPI_Reduce(&basesum, &checksum, 1, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD);); (10, MPI, MPI_Abort(MPI_COMM_WORLD, -1);); (103, MPI, MPI_Bcast(iterstring, proc_length+1, MPI_CHAR, root, MPI_COMM_WORLD);); (15, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (15, MPI, MPI_Bcast(&length,     1, MPI_LONG, root, MPI_COMM_WORLD);); (94, MPI, MPI_Bcast(catstring, length+1, MPI_CHAR, root, MPI_COMM_WORLD", "label": "(113, MPI, MPI_Type_commit(&mpi_word);); (120, MPI, MPI_Allgather(iterstring,1,mpi_word, catstring,1,mpi_word, MPI_COMM_WORLD);); (152, MPI, MPI_Finalize();); (61, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (108, MPI, MPI_Type_contiguous(proc_length,MPI_CHAR, &mpi_word);); (17, MPI, MPI_Init(&argc,&argv);); (59, MPI, MPI_Bcast(&length,     1, MPI_LONG, root, MPI_COMM_WORLD);); (108, MPI, MPI_Barrier(MPI_COMM_WORLD);); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(9, CUDA, __shared__ float As[maxPipelineStages][BLOCK_SIZE_X][BLOCK_SIZE_X];); (12, CUDA, __shared__ float Bs[maxPipelineStages][BLOCK_SIZE_X][BLOCK_SIZE_X];)", "label": "(13, CUDA, __shared__ float Bs[maxPipelineStages][BLOCK_SIZE_X][BLOCK_SIZE_X];); (8, CUDA, __shared__ float As[maxPipelineStages][BLOCK_SIZE_X][BLOCK_SIZE_X];)"}
{"predict": "(11, CUDA, extern __shared__ float sdata[];)", "label": "(11, CUDA, extern __shared__ float sdata[];)"}
{"predict": "(10, OpenMP, #pragma omp target map(from:device)); (6, OpenMP, #pragma omp target device(0) map(to:host)); (8, OpenMP, #pragma omp target device(1) map(from:device))", "label": "(7, OpenMP, #pragma omp target map(tofrom : device)); (9, OpenMP, printf(\"omp_is_initial_device() returned false on host\\n\");); (12, OpenMP, printf(\"omp_is_initial_device() returned true on device\\n\");)"}
{"predict": "(2, TBB, tbb::atomic<int> counter;); (2, TBB, tbb::atomic<int> output_counter;)", "label": "(32, TBB, tbb::tbb_allocator<t1>().destroy(my_storage);); (35, TBB, tbb::tbb_allocator<t2>().destroy(my_storage);); (0, TBB, tbb::atomic<int> counter;)"}
{"predict": "(57, CUDA, checkCudaErrors(cufftPlan2d(&planc2r, DIM, DIM, CUFFT_C2R));); (43, CUDA, checkCudaErrors(cudaMalloc((void **) &vyfield, sizeof(float) * DIM * DIM));); (47, CUDA, checkCudaErrors(cudaMalloc((void **) &vxfield, sizeof(float) * DIM * DIM));); (43, CUDA, checkCudaErrors(cudaMemset(vyfield, 0, sizeof(float) * DIM * DIM));); (37, CUDA, checkCudaErrors(cudaMalloc((void **) &dvfield, sizeof(cData) * DIM * DIM));); (40, CUDA, checkCudaErrors(cudaMalloc((void **) &dvfield_tmp, sizeof(cData) * DIM * DIM));); (42, CUDA, checkCudaErrors(cudaMemset(dvfield, 0, sizeof(cData) * DIM * DIM));); (36, CUDA, checkCudaErrors(cudaMalloc((void **) &vxfield_tmp, sizeof(float) * DIM * DIM));); (44, CUDA, checkCudaErrors(cufftPlan2d(&planr2c, DIM, DIM, CUFFT_R2C));); (38, CUDA, checkCudaErrors(cudaMemset(vxfield, 0, sizeof(float) * DIM * DIM));); (33, CUDA, checkCudaErrors(cudaMalloc((void **) &vyfield_tmp, sizeof(float) * DIM * DIM));); (51, CUDA, checkCudaErrors(cufftPlan2d(&planxy, DIM, DIM, CUFFT_C2C));)", "label": "(56, CUDA, checkCudaErrors(cufftPlan2d(&planc2r, DIM, DIM, CUFFT_C2R));); (54, CUDA, checkCudaErrors(cufftPlan2d(&planr2c, DIM, DIM, CUFFT_R2C));); (37, CUDA, checkCudaErrors(cudaMallocPitch((void **)&dvfield, &tPitch, sizeof(cData)*DIM, DIM));); (38, CUDA, checkCudaErrors(cudaMemcpy(dvfield, hvfield, sizeof(cData) * DS, cudaMemcpyHostToDevice));); (40, CUDA, checkCudaErrors(cudaMalloc((void **)&vyfield, sizeof(cData) * PDS));); (38, CUDA, checkCudaErrors(cudaMalloc((void **)&vxfield, sizeof(cData) * PDS));)"}
{"predict": "(47, OpenMP, #pragma omp teams reduction(+:sum)); (43, OpenMP, #pragma omp target map(tofrom: b, sum, t) map(alloc: a)); (45, OpenMP, #pragma omp distribute parallel for reduction(+:sum)); (48, OpenMP, t[i] = omp_get_team_num();)", "label": "(46, OpenMP, #pragma omp teams reduction(+:sum)); (43, OpenMP, #pragma omp target map(tofrom: b, sum, t) map(alloc: a)); (46, OpenMP, #pragma omp distribute parallel for reduction(+:sum)); (48, OpenMP, t[i] = omp_get_team_num();)"}
{"predict": "(180, CUDA, cudaCheck(cudaFree(back_acts.d_back_emb));); (52, CUDA, cudaCheck(cudaMalloc(&params_memory_gpu, num_parameters * sizeof(float)));); (176, CUDA, cudaCheck(cudaFree(back_acts.d_back_x));); (174, CUDA, cudaCheck(cudaFree(back_acts.d_back_h));); (170, CUDA, cudaCheck(cudaFree(back_acts.d_back_c));); (173, CUDA, cudaCheck(cudaFree(back_acts.d_back_silu));); (174, CUDA, cudaCheck(cudaFree(back_acts.d_back_plus_emb));); (51, CUDA, cudaCheck(cudaMalloc(&grads_memory_gpu, num_parameters * sizeof(float)));); (47, CUDA, cudaCheck(cudaMemcpy(params_memory_gpu, params_memory_cpu, num_parameters * sizeof(float), cudaMemcpyHostToDevice));); (165, CUDA, cudaCheck(cudaFree(acts.d_emb));); (167, CUDA, cudaCheck(cudaFree(acts.d_silu));); (165, CUDA, cudaCheck(cudaFree(acts.d_plus_emb));); (167, CUDA, cudaCheck(cudaFree(acts.d_x));); (162, CUDA, cudaCheck(cudaFree(acts.d_c));); (166, CUDA, cudaCheck(cudaFree(acts.d_h));); (164, CUDA, cudaCheck(cudaFree(acts.d_gn));); (43, CUDA, cudaCheck(cudaMemset(grads_memory_gpu, 0, num_parameters * sizeof(float)));); (160, CUDA, cudaCheck(cudaFree(acts.d_W));); (155, CUDA, cudaCheck(cudaFree(acts.d_E));); (40, CUDA, cudaCheck(cudaMemcpy(grads_memory_gpu, grads_memory_cpu, num_parameters * sizeof(float), cudaMemcpyHostToDevice));); (155, CUDA, cudaCheck(cudaFree(acts.", "label": "(125, CUDA, cudaCheck(cudaMalloc(&acts_memory_gpu, num_acts_params * sizeof(float)));); (257, CUDA, cudaCheck(cudaFree(acts_memory_gpu));); (173, CUDA, cudaCheck(cudaMalloc(&back_acts_memory_gpu, num_back_acts * sizeof(float)));); (254, CUDA, cudaCheck(cudaFree(params_memory_gpu));); (259, CUDA, cudaCheck(cudaFree(acts.emb));); (53, CUDA, cudaCheck(cudaMemset(grads_memory_gpu, 0, num_parameters * sizeof(float)));); (170, CUDA, cudaCheck(cudaMemset(back_acts_memory_gpu, 0, num_back_acts * sizeof(float)));); (182, CUDA, cudaCheck(cudaMemcpy(back_acts.dout, debug_states.dout, B * C_out * H_out * W_out * sizeof(float), cudaMemcpyHostToDevice));); (254, CUDA, cudaCheck(cudaFree(acts.input));); (117, CUDA, cudaCheck(cudaMemset(acts_memory_gpu, 0, num_acts_params * sizeof(float)));); (43, CUDA, cudaCheck(cudaMemcpy(params_memory_gpu, params_memory_cpu, num_parameters * sizeof(float), cudaMemcpyHostToDevice));); (249, CUDA, cudaCheck(cudaFree(back_acts_memory_gpu));); (122, CUDA, cudaCheck(cudaMemcpy(acts.input, debug_states.input, state_sizes[0] * sizeof(float), cudaMemcpyHostToDevice));); (122, CUDA, cudaCheck(cudaMemcpy(acts.emb, debug_states.emb, state_sizes[1] * sizeof(float), cudaMemcpyHostToDevice));); (43, CUDA, cudaCheck(cudaMalloc(&grads_memory_gpu, num_parameters * sizeof(float)));); (116, CUDA, cudaCheck(cudaMalloc(&acts.input, B * C * H * W * sizeof(float)));); (36, CUDA, cudaCheck(cudaMalloc(&params_memory_gpu, num_parameters * sizeof(float)));); (115, CUDA, cudaCheck(cudaMalloc(&acts.emb, B * C_emb * sizeof(float)));); (241, CUDA, cudaCheck(cudaFree(grads_memory_gpu));)"}
{"predict": "(5, OpenMP, omp_set_nest_lock(&nest_lock);); (10, OpenMP, omp_destroy_nest_lock(&nest_lock);); (3, OpenMP, omp_init_nest_lock(&nest_lock);); (8, OpenMP, omp_unset_nest_lock(&nest_lock);)", "label": "(6, OpenMP, omp_unset_nest_lock(&nest_lock);); (2, OpenMP, omp_init_nest_lock(&nest_lock);); (3, OpenMP, omp_test_nest_lock(&nest_lock);); (5, OpenMP, omp_set_nest_lock(&nest_lock);); (9, OpenMP, omp_destroy_nest_lock(&nest_lock);)"}
{"predict": "(88, MPI, NBC_Error(\"Error in MPI_Waitall(%i, %p) (%i)\", handle->req_count, handle->req_array, res);); (55, MPI, res = MPI_Waitall(1, tmp, MPI_STATUSES_IGNORE);)", "label": "(73, MPI, Irecv_time -= MPI_Wtime();); (55, MPI, Isend_time += MPI_Wtime();); (88, MPI, Irecv_time += MPI_Wtime();); (35, MPI, Isend_time -= MPI_Wtime();)"}
{"predict": "(24, CUDA, CHECK(cudaMalloc(&d_B, M));); (24, CUDA, CHECK(cudaMemcpy(d_A, h_A, M, cudaMemcpyHostToDevice));); (46, CUDA, CHECK(cudaFree(d_B));); (19, CUDA, CHECK(cudaMalloc(&d_A, M));); (32, CUDA, CHECK(cudaMemcpy(h_B, d_B, M, cudaMemcpyDeviceToHost));); (42, CUDA, CHECK(cudaFree(d_A));)", "label": "(24, CUDA, CHECK(cudaMalloc(&d_B, M));); (24, CUDA, CHECK(cudaMemcpy(d_A, h_A, M, cudaMemcpyHostToDevice));); (46, CUDA, CHECK(cudaFree(d_B));); (20, CUDA, CHECK(cudaMalloc(&d_A, M));); (32, CUDA, CHECK(cudaMemcpy(h_B, d_B, M, cudaMemcpyDeviceToHost));); (42, CUDA, CHECK(cudaFree(d_A));)"}
{"predict": "(35, OpenMP, printf( \"# starting test: OpenMP [auto] \\n\" );); (344, MPI, MPI_Finalize();); (4, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &rank );); (5, MPI, MPI_Init_thread( &argc, &argv, MPI_THREAD_MULTIPLE, &rc );); (64, OpenMP, #pragma omp parallel for if(n>OMP_MIN_SIZE)); (4, MPI, MPI_Finalize();); (320, MPI, MPI_Allreduce(MPI_IN_PLACE,d7,max,MPI_DOUBLE,MPI_MAX,MPI_COMM_WORLD);); (314, MPI, MPI_Allreduce(MPI_IN_PLACE,d5,max,MPI_DOUBLE,MPI_MAX,MPI_COMM_WORLD);); (310, MPI, MPI_Allreduce(MPI_IN_PLACE,d4,max,MPI_DOUBLE,MPI_MAX,MPI_COMM_WORLD);); (304, MPI, MPI_Allreduce(MPI_IN_PLACE,d2,max,MPI_DOUBLE,MPI_MAX,MPI_COMM_WORLD);); (300, MPI, MPI_Allreduce(MPI_IN_PLACE,d0,max,MPI_DOUBLE,MPI_MAX,MPI_COMM_WORLD);); (294, MPI, MPI_Allreduce(MPI_IN_PLACE,d1,max,MPI_DOUBLE,MPI_MAX,MPI_COMM_WORLD);); (307, MPI, MPI_Allreduce(MPI_IN_PLACE,d6,max,MPI_DOUBLE,MPI_MAX,MPI_COMM_WORLD);); (6, OpenMP, printf( \"num threads %d \\n\", omp_get_max_threads() );); (3, MPI, MPI_Barrier( MPI_COMM_WORLD );); (278, MPI, MPI_Comm_set_errhandler( MPI_COMM_WORLD, MPI_ERRORS_RETURN );)", "label": "(8, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &rank );); (364, MPI, MPI_Finalize();); (34, OpenMP, fprintf( stderr, \"starting test: OpenMP ON with %d threads... \\n\", omp_get_max_threads() );); (164, OpenMP, #pragma omp parallel if(n>OMP_MIN_SIZE)); (168, OpenMP, #pragma omp for private(i,j,n4)); (63, OpenMP, #pragma omp parallel for if(n>OMP_MIN_SIZE)); (1, MPI, MPI_Init_thread( &argc, &argv, MPI_THREAD_FUNNELED, &provided );); (28, OpenMP, printf( \"# starting test: OpenMP ON with %d threads... \\n\", omp_get_max_threads() );)"}
{"predict": "(4, CUDA, extern __shared__ char shared_memory[];)", "label": "(5, CUDA, extern __shared__ char shared_memory[];)"}
{"predict": "(12, MPI, err = MPI_Type_struct(3, blocks, disps, types, &eviltype);); (124, MPI, MPI_Type_free(&eviltype);); (67, MPI, err = MPI_Type_get_extent(eviltype, &lb, &extent);); (35, MPI, err = MPI_Type_extent(eviltype, &aval);); (86, MPI, err = MPI_Type_ub(eviltype, &aval);); (100, MPI, err = MPI_Type_get_true_extent(eviltype, &true_lb, &aval);); (26, MPI, err = MPI_Type_size(eviltype, &val);); (45, MPI, err = MPI_Type_lb(eviltype, &aval);)", "label": "(23, MPI, err = MPI_Type_size(eviltype, &val);); (11, MPI, err = MPI_Type_struct(3, blocks, disps, types, &eviltype);); (66, MPI, err = MPI_Type_get_extent(eviltype, &lb, &extent);); (86, MPI, err = MPI_Type_ub(eviltype, &aval);); (121, MPI, MPI_Type_free(&eviltype);); (99, MPI, err = MPI_Type_get_true_extent(eviltype, &true_lb, &aval);); (32, MPI, err = MPI_Type_extent(eviltype, &aval);); (46, MPI, err = MPI_Type_lb(eviltype, &aval);)"}
{"predict": "(62, MPI, MPI_Sendrecv(NULL, 0, MPI_INT, 0, 0, NULL, 0, MPI_INT, 0, 0, MPI_COMM_WORLD);); (49, MPI, MPI_Group_incl(comm_group, 1, &destrank, &group);); (79, MPI, MPI_Win_free(&win);); (37, MPI, MPI_Win_allocate(0, 1, MPI_INFO_NULL, CommDeuce, &base_ptr, &win);); (59, MPI, MPI_Win_unlock(rank, win);); (26, MPI, MPI_Barrier(MPI_COMM_WORLD);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (44, MPI, MPI_Win_create(B, SIZE * sizeof(int), sizeof(int), MPI_INFO_NULL, CommDeuce, &win);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (73, MPI, MPI_Group_free(&comm_group);); (7, MPI, MPI_Comm_group(MPI_COMM_WORLD, &comm_group);); (35, MPI, MPI_Put(A, SIZE, MPI_INT, 1, 0, 1, MPI_INT, win);); (30, MPI, MPI_Win_allocate(SIZE * sizeof(int), sizeof(int), MPI_INFO_NULL, CommDeuce, &A, &win);); (72, MPI, MPI_Free_mem(A);); (10, MPI, i = MPI_Alloc_mem(SIZE * sizeof(int), MPI_INFO_NULL, &A);); (56, MPI, MPI_Win_start(group, 0, win);); (65, MPI, MPI_Win_post(group, 0, win);); (20, MPI, i = MPI_Alloc_mem(SIZE * sizeof(int), MPI_INFO_NULL, &B);); (57, MPI, MPI_Win_complete(win);); (3, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (71, MPI", "label": "(67, MPI, MPI_Win_unlock(rank, win);); (28, MPI, i = MPI_Alloc_mem(SIZE * sizeof(int), MPI_INFO_NULL, &B);); (45, MPI, MPI_Win_start(group, 0, win);); (92, MPI, MPI_Win_free(&win);); (13, MPI, MPI_Comm_split(MPI_COMM_WORLD, (rank < 2), rank, &CommDeuce);); (66, MPI, MPI_Win_post(group, 0, win);); (44, MPI, MPI_Send(B, SIZE, MPI_INT, 1, 100, MPI_COMM_WORLD);); (34, MPI, MPI_Win_allocate(0, 1, MPI_INFO_NULL, CommDeuce, &base_ptr, &win);); (47, MPI, MPI_Win_allocate(SIZE * sizeof(int), sizeof(int), MPI_INFO_NULL, CommDeuce, &B, &win);); (63, MPI, MPI_Recv(A, SIZE, MPI_INT, 0, 100, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (63, MPI, MPI_Win_wait(win);); (41, MPI, MPI_Free_mem(B);); (37, MPI, MPI_Win_complete(win);); (35, MPI, MPI_Put(A, SIZE, MPI_INT, 1, 0, SIZE, MPI_INT, win);); (0, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (5, MPI, i = MPI_Alloc_mem(SIZE * sizeof(int), MPI_INFO_NULL, &A);); (82, MPI, MPI_Comm_free(&CommDeuce);); (77, MPI, MPI_Group_free(&comm_group);); (-9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (7, MPI, MPI_Comm_group(CommDeuce, &comm_group);); (-10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (75, MPI, MPI_Free_mem(A);); (71, MPI, MPI_Group_free(&group);); (23, MPI, MPI_Group_incl(comm_group, 1, &destrank, &group);); (38, MPI, MPI_Win_create(B, SIZE * sizeof(int), sizeof(int), MPI_INFO_NULL, CommDeuce, &win);)"}
{"predict": "(53, CUDA, CubDebugExit(cudaMemcpy(d_flags, h_flags, sizeof(unsigned char) * num_items, cudaMemcpyHostToDevice));); (50, CUDA, CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));)", "label": "(52, CUDA, CubDebugExit(cudaMemcpy(d_flags, h_flags, sizeof(unsigned char) * num_items, cudaMemcpyHostToDevice));); (50, CUDA, CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));)"}
{"predict": "(57, MPI, MPI_Start(&req);); (12, MPI, MPI_Comm_rank(comm, &myrank);); (77, MPI, MPI_Type_free(&stype[i][j]);); (52, MPI, MPI_Info_create(&info);); (56, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (72, MPI, MPI_Type_free(&xtype[i][j]);); (55, MPI, MPI_Request_free(&req);); (57, MPI, MPI_Info_free(&info);); (7, MPI, MPI_Comm_size(comm, &p);)", "label": "(58, MPI, MPI_Start(&req);); (13, MPI, MPI_Comm_rank(comm, &myrank);); (76, MPI, MPI_Type_free(&stype[i][j]);); (50, MPI, MPI_Info_create(&info);); (55, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (72, MPI, MPI_Type_free(&xtype[i][j]);); (56, MPI, MPI_Request_free(&req);); (6, MPI, MPI_Comm_size(comm, &p);); (55, MPI, MPI_Info_free(&info);)"}
{"predict": "(176, OpenCL, clGetDeviceInfo(vkGPU->device, CL_DEVICE_VENDOR_ID, sizeof(cl_int), &vendorID, 0);); (88, CUDA, res = cudaMalloc((void**)&buffer, bufferSize);); (94, OpenCL, buffer = clCreateBuffer(vkGPU->context, CL_MEM_READ_WRITE, bufferSize, 0, &res);)", "label": "(161, OpenCL, clGetDeviceInfo(vkGPU->device, CL_DEVICE_VENDOR_ID, sizeof(cl_int), &vendorID, 0);); (81, CUDA, res = cudaMalloc((void**)&buffer, bufferSize);); (90, OpenCL, buffer = clCreateBuffer(vkGPU->context, CL_MEM_READ_WRITE, bufferSize, 0, &res);)"}
{"predict": "(26, MPI, MPI_Comm_remote_size(intercomm, &rsize);); (89, MPI, MPI_Comm_disconnect(&parentcomm);); (73, MPI, MPI_Recv(&i, 1, MPI_INT, 0, 0, intercomm, &status);); (93, MPI, MPI_Finalize();); (48, MPI, MPI_Comm_get_name(intercomm, cname, &rlen);); (36, MPI, MPI_Send(&i, 1, MPI_INT, i, 0, intercomm);); (18, MPI, MPI_Comm_size(intercomm, &size);); (68, MPI, MPI_Send(&errs, 1, MPI_INT, 0, 0, intercomm);); (77, MPI, MPI_Comm_disconnect(&intercomm);); (8, MPI, MPI_Comm_get_parent(&parentcomm);); (10, MPI, MPI_Comm_rank(intercomm, &rank);)", "label": "(26, MPI, MPI_Comm_size(intercomm, &size);); (84, MPI, MPI_Comm_disconnect(&intercomm);); (91, MPI, MPI_Finalize();); (11, MPI, MPI_Comm_get_parent(&parentcomm);); (66, MPI, MPI_Recv(&i, 1, MPI_INT, 0, 0, intercomm, &status);); (32, MPI, MPI_Send(&i, 1, MPI_INT, i, 0, intercomm);); (52, MPI, MPI_Comm_get_name(intercomm, cname, &rlen);); (69, MPI, MPI_Ssend(&errs, 1, MPI_INT, 0, 1, intercomm);); (35, MPI, MPI_Recv(&err, 1, MPI_INT, i, 1, intercomm, MPI_STATUS_IGNORE);); (18, MPI, MPI_Comm_rank(intercomm, &rank);); (15, MPI, MPI_Comm_remote_size(intercomm, &rsize);)"}
{"predict": "(69, OpenMP, #pragma omp teams distribute parallel for collapse (N)); (37, OpenMP, #pragma omp teams distribute parallel for collapse ((ST > 0)? 1 + ST : 2)); (46, OpenMP, #pragma omp teams distribute parallel for collapse (foobool(argc)), collapse (true), collapse (-5)); (50, OpenMP, #pragma omp teams distribute parallel for collapse (argv[1]=2)); (4, OpenMP, #pragma omp teams distribute parallel for collapse (); (26, OpenMP, #pragma omp teams distribute parallel for collapse (1)); (6, OpenMP, #pragma omp teams distribute parallel for collapse ()); (29, OpenMP, #pragma omp teams distribute parallel for collapse (N)); (57, OpenMP, #pragma omp teams distribute parallel for collapse (2)); (10, OpenMP, #pragma omp teams distribute parallel for collapse (argc); (16, OpenMP, #pragma omp teams distribute parallel for collapse (ST); (-7, OpenMP, #pragma omp teams distribute parallel for collapse); (36, OpenMP, #pragma omp teams distribute parallel for collapse (S)); (57, OpenMP, #pragma omp teams distribute parallel for collapse (1)); (-10, OpenMP, #pragma omp target)", "label": "(31, OpenMP, #pragma omp teams distribute parallel for collapse (1)); (35, OpenMP, #pragma omp teams distribute parallel for collapse ((ST > 0)? 1 + ST : 2)); (1, OpenMP, #pragma omp teams distribute parallel for collapse); (10, OpenMP, #pragma omp teams distribute parallel for collapse ()); (44, OpenMP, #pragma omp distribute collapse (S)); (21, OpenMP, #pragma omp teams distribute parallel for collapse (ST); (62, OpenMP, #pragma omp teams distribute parallel for collapse (N)); (13, OpenMP, #pragma omp teams distribute parallel for collapse (argc); (65, OpenMP, #pragma omp teams distribute parallel for collapse (2)); (49, OpenMP, #pragma omp teams distribute parallel for collapse (argv[1]=2)); (-8, OpenMP, #pragma omp target); (-3, OpenMP, #pragma omp teams distribute parallel for collapse (); (32, OpenMP, #pragma omp teams distribute parallel for collapse (foobool(argc)), collapse (true), collapse (-5))"}
{"predict": "(44, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (44, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (47, MPI, double start = MPI_Wtime();); (150, MPI, MPI_Allreduce(&local_count, out_count, 1, MPI_UINT64_T, MPI_SUM, MPI_COMM_WORLD);); (325, MPI, double end = MPI_Wtime();); (46, MPI, MPI_Barrier(MPI_COMM_WORLD);); (316, MPI, MPI_Allreduce(MPI_IN_PLACE, &time_ms, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);); (118, MPI, MPI_Exscan(&overlap_size, &offset, 1, MPI_UINT64_T, MPI_SUM, MPI_COMM_WORLD);); (320, MPI, MPI_Allreduce(MPI_IN_PLACE, &mem_used, 1, MPI_UINT64_T, MPI_SUM, MPI_COMM_WORLD);); (313, MPI, MPI_Allreduce(MPI_IN_PLACE, &bw, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);); (233, MPI, MPI_Reduce(&local_count, out_count, 1, MPI_UINT64_T, MPI_SUM, 0, MPI_COMM_WORLD);); (139, MPI, MPI_Bcast(&overlap_before, 1, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (244, MPI, MPI_Send(&local_count, 1, MPI_UINT64_T, right, 0, MPI_COMM_WORLD);); (311, MPI, MPI_Allreduce(MPI_IN_PLACE, &mem_avail, 1, MPI_UINT64_T, MPI_SUM, MPI_COMM_WORLD);); (228, MPI, MPI_Send(&local_count, 1, MPI_UINT64_T, left, 0, MPI_COMM_WORLD);); (233, MPI, MPI_Send", "label": "(45, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (47, MPI, double start = MPI_Wtime();); (237, MPI, double now = MPI_Wtime();); (41, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(57, MPI, MPI_Barrier(comm);); (16, MPI, MPI_Comm_rank(comm, &par_rank);); (36, MPI, ascent_opt[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (53, MPI, MPI_Reduce(MPI_IN_PLACE, NULL, 0, MPI_DATATYPE_NULL, MPI_SUM, 0, comm);); (14, MPI, MPI_Comm_size(comm, &par_size);)", "label": "(33, MPI, flow::Workspace::set_default_mpi_comm(MPI_Comm_c2f(comm));); (16, MPI, MPI_Comm_rank(comm, &par_rank);); (16, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(3, OpenCL, cl_mem smallBuffer = clCreateBuffer(clContext, flags, size, hostPtr, &retVal);); (12, OpenCL, retVal = clReleaseMemObject(smallBuffer);)", "label": "(13, OpenCL, retVal = clReleaseMemObject(smallBuffer);); (17, OpenCL, EXPECT_EQ(clReleaseContext(context), CL_SUCCESS);); (1, OpenCL, cl_mem smallBuffer = clCreateBufferWithProperties(clContext, &memProperties, flags, size, hostPtr, &retVal);)"}
{"predict": "(14, OpenMP, omp_set_num_threads(omp_get_num_threads());); (3, OpenMP, #pragma omp parallel num_threads(8)); (8, OpenMP, omp_set_affinity(s);); (3, OpenMP, int tid = omp_get_thread_num();); (0, OpenMP, omp_set_affinity_format(formats[i]);)", "label": "(8, OpenMP, int tid = omp_get_thread_num();); (4, OpenMP, #pragma omp parallel num_threads(8)); (2, OpenMP, omp_set_affinity_format(formats[i]);)"}
{"predict": "(16, CUDA, __shared__ uninitialized_array<thrust::complex<T2>, nid> cache;); (28, CUDA, cache[id] += static_cast<thrust::complex<T2>>(A_[i * nocc] * B_[i * nocc]);); (24, CUDA, cache[id] = thrust::complex<T>(0.0);); (13, CUDA, thrust::complex<T2> const* A_(Tab + blockIdx.x * nocc * nocc * nchol + ((a * nocc + b) * nocc) * nchol);); (13, CUDA, thrust::complex<T> alp = static_cast<thrust::complex<T>>(alpha);); (14, CUDA, thrust::complex<T2> const* B_(Tab + blockIdx.x * nocc * nocc * nchol + ((b * nocc + a) * nocc) * nchol);)", "label": "(16, CUDA, __shared__ uninitialized_array<thrust::complex<T>, 1024> cache;); (29, CUDA, cache[id] += static_cast<thrust::complex<T>>(A_[(a * nchol + i) * nocc + b] * A_[(b * nchol + i) * nocc + a]);); (21, CUDA, thrust::complex<T2> const* A_(Tab + blockIdx.x * nocc * nocc * nchol);); (18, CUDA, cache[id]              = thrust::complex<T>(0.0);); (18, CUDA, thrust::complex<T> alp = static_cast<thrust::complex<T>>(alpha);)"}
{"predict": "cast<void **>(&d_C[i]),\n(177, CUDA, CUDA_CHECK(cudaFree(d_C_array));); (177, CUDA, CUDA_CHECK(cudaFree(d_B_array));); (184, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (174, CUDA, CUDA_CHECK(cudaFree(d_A_array));); (178, CUDA, CUDA_CHECK(cudaDeviceReset());); (89, CUDA, CUDA_CHECK(cudaMemcpy(d_A[i], A_array[i].data(), sizeof(data_type) * A_array[i].size(), cudaMemcpyHostToDevice));); (89, CUDA, CUDA_CHECK(cudaMemcpy(d_B[i], B_array[i].data(), sizeof(data_type) * B_array[i].size(), cudaMemcpyHostToDevice));)", "label": "(169, CUDA, CUDA_CHECK(cudaFree(d_B_array));); (169, CUDA, CUDA_CHECK(cudaFree(d_C_array));); (177, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (165, CUDA, CUDA_CHECK(cudaFree(d_A_array));); (168, CUDA, CUDA_CHECK(cudaFree(d_A[i]));); (169, CUDA, CUDA_CHECK(cudaFree(d_C[i]));); (133, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (174, CUDA, CUDA_CHECK(cudaDeviceReset());); (165, CUDA, CUDA_CHECK(cudaFree(d_B[i]));); (75, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(51, CUDA, HANDLE_ERROR( cudaFree( dev_a ) );); (51, CUDA, HANDLE_ERROR( cudaFree( dev_b ) );); (49, CUDA, HANDLE_ERROR( cudaFree( dev_partial_c ) );)", "label": "(48, CUDA, HANDLE_ERROR( cudaFree( dev_a ) );); (49, CUDA, HANDLE_ERROR( cudaFree( dev_partial_c ) );); (47, CUDA, HANDLE_ERROR( cudaFree( dev_b ) );)"}
{"predict": "(7, OpenMP, #pragma omp parallel num_threads(4)); (11, OpenMP, omp_set_lock(&lck);); (14, OpenMP, omp_unset_lock(&lck);); (11, OpenMP, #pragma omp flush); (4, OpenMP, #pragma omp single); (13, OpenMP, omp_destroy_lock(&lck);); (1, OpenMP, omp_init_lock(&lck);)", "label": "(12, OpenMP, omp_set_nest_lock(&lck);); (18, OpenMP, omp_unset_nest_lock(&lck);); (8, OpenMP, #pragma omp for); (10, OpenMP, #pragma omp flush); (4, OpenMP, #pragma omp parallel shared(lck)); (2, OpenMP, omp_init_nest_lock(&lck);); (16, OpenMP, omp_destroy_nest_lock(&lck);)"}
{"predict": "(15, MPI, MPI_Type_get_true_extent(inkey, &key_true_lb, &key_true_extent);); (18, MPI, MPI_Type_get_true_extent(inkeysat, &keysat_true_lb, &keysat_true_extent);)", "label": "(19, MPI, MPI_Type_get_true_extent(inkeysat, &keysat_true_lb, &keysat_true_extent);); (76, MPI, MPI_Type_dup(inkey,    &values->key);); (13, MPI, MPI_Type_get_true_extent(inkey, &key_true_lb, &key_true_extent);); (75, MPI, MPI_Type_dup(inkeysat, &values->keysat);)"}
{"predict": "(7, TBB, oneapi::tbb::task_group_context h1, h2;); (5, TBB, oneapi::tbb::task_group g;)", "label": "(6, TBB, tbb::task_group g;); (6, TBB, tbb::task_handle h1 = make_task(g, x, n-1);); (6, TBB, tbb::task_handle h2 = make_task(g, y, n-2);)"}
{"predict": "(17, CUDA, CUDA_FUNC(cudaStreamDestroy(resources->exec_streams[i]));)", "label": "(17, CUDA, CUDA_FUNC(cudaStreamDestroy(resources->exec_streams[i]));)"}
{"predict": "(31, MPI, error_code = MPI_Comm_test_inter(comm, &flag);); (56, MPI, MPI_Comm_dup(comm, &dupcomm);); (160, MPI, MPI_Comm_rank(dupcomm, &rank);); (174, MPI, MPI_Barrier(dupcomm);); (66, MPI, MPI_Allreduce(&amode, &tmp_amode, 1, MPI_INT, ADIO_same_amode, dupcomm);)", "label": "(31, MPI, error_code = MPI_Comm_test_inter(comm, &flag);); (56, MPI, MPI_Comm_dup(comm, &dupcomm);); (126, MPI, MPI_Comm_rank(dupcomm, &rank);); (136, MPI, MPI_Barrier(dupcomm);); (67, MPI, MPI_Allreduce(&amode, &tmp_amode, 1, MPI_INT, ADIO_same_amode, dupcomm);)"}
{"predict": "(5, OpenMP, #pragma omp parallel for)", "label": "(5, OpenMP, #pragma omp parallel for)"}
{"predict": "(14, MPI, MPI_Bcast( &nbases, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (38, MPI, MPI_Bcast( &nzones, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (26, MPI, MPI_Bcast( &iphysdim, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (24, MPI, MPI_Bcast( &icelldim, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (83, MPI, MPI_Bcast( zonename, 33, MPI_CHAR, Parallel::serverid, MPI_COMM_WORLD );); (83, MPI, MPI_Bcast( isize.data(), index_dim * 3, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (102, MPI, MPI_Bcast( &nNodes, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (104, MPI, MPI_Bcast( irmin.data(), index_dim, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (104, MPI, MPI_Bcast( irmax.data(), index_dim, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (2, MPI, MPI_Bcast( &fileId, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );)", "label": "(224, MPI, MPI_Bcast( &bocotype, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (190, MPI, MPI_Bcast( &nbocos, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (202, MPI, MPI_Bcast( &location, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (38, MPI, MPI_Bcast( &nzones, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (84, MPI, MPI_Bcast( zonename, 33, MPI_CHAR, Parallel::serverid, MPI_COMM_WORLD );); (295, MPI, MPI_Bcast( range.data(), npnts * index_dim, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (219, MPI, MPI_Bcast( &ptset_type, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (251, MPI, MPI_Bcast( pnts.data(), npnts * index_dim, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (81, MPI, MPI_Bcast( isize.data(), index_dim * 3, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (113, MPI, MPI_Bcast"}
{"predict": "(58, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (58, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)", "label": "(58, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (58, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)"}
{"predict": "(1, CUDA, using alloc = thrust::cuda::allocator<double>;); (2, CUDA, cudaDeviceSynchronize();)", "label": "(3, CUDA, using alloc = thrust::cuda::allocator<double>;); (11, CUDA, cudaDeviceSynchronize(); benchmark::DoNotOptimize(y.base()); benchmark::ClobberMemory();)"}
{"predict": "(32, OpenMP, #pragma omp target); (55, OpenMP, #pragma omp target data use_device_ptr(a))", "label": "(32, OpenMP, #pragma omp target); (35, OpenMP, int device_id = omp_get_default_device();); (54, OpenMP, #pragma omp target data use_device_ptr(a))"}
{"predict": "(34, CUDA, cudaFree(d_A);); (35, CUDA, cudaFree(d_C);); (33, CUDA, cudaFree(d_B);); (17, CUDA, cudaMemcpy(d_A, h_A, DSIZE*sizeof(float), cudaMemcpyHostToDevice);); (10, CUDA, cudaMalloc((void**)&d_B, DSIZE*sizeof(float));); (17, CUDA, cudaMemcpy(d_B, h_B, DSIZE*sizeof(float), cudaMemcpyHostToDevice);); (10, CUDA, cudaMalloc((void**)&d_C, DSIZE*sizeof(float));); (18, CUDA, cudaMemcpy(h_C, d_C, DSIZE*sizeof(float), cudaMemcpyDeviceToHost);); (6, CUDA, cudaMalloc((void**)&d_A, DSIZE*sizeof(float));)", "label": "(28, CUDA, cudaCheckErrors(\"kernel execution failure or cudaMemcpy H2D failure\");); (14, CUDA, cudaMemcpy(d_A, h_A, DSIZE*sizeof(float), cudaMemcpyHostToDevice);); (16, CUDA, cudaCheckErrors(\"cudaMemcpy H2D failure\");); (23, CUDA, cudaMemcpy(h_C, d_C, DSIZE*sizeof(float), cudaMemcpyDeviceToHost);); (13, CUDA, cudaMemcpy(d_B, h_B, DSIZE*sizeof(float), cudaMemcpyHostToDevice);); (7, CUDA, cudaMalloc(&d_C, DSIZE*sizeof(float));); (5, CUDA, cudaMalloc(&d_B, DSIZE*sizeof(float));); (6, CUDA, cudaCheckErrors(\"cudaMalloc failure\");); (15, CUDA, cudaCheckErrors(\"kernel launch failure\");); (1, CUDA, cudaMalloc(&d_A, DSIZE*sizeof(float));)"}
{"predict": "(16, OpenMP, #pragma omp target teams distribute parallel for collapse(2))", "label": "(17, OpenMP, #pragma omp target teams distribute parallel for collapse(2)); (23, OpenMP, #pragma omp target exit data map(delete:A[0:N1],B[0:N1],C[0:N2])); (14, OpenMP, #pragma omp target enter data map(to: A[0:N1],B[0:N1],C[0:N2])); (20, OpenMP, #pragma omp target update from(C[0:N2]))"}
{"predict": "(16, OpenMP, #pragma omp parallel for)", "label": "(16, OpenMP, #pragma omp parallel for)"}
{"predict": "(55, OpenCL, clReleaseEvent(event);)", "label": "(54, OpenCL, clReleaseEvent(event);)"}
{"predict": "(17, MPI, *v6 = MPI_Cart_c2f(MPI_COMM_WORLD, l4, v3, v5, v6);); (18, MPI, *ierr = MPI_Cartdim_get(MPI_COMM_WORLD, v6);)", "label": "(17, MPI, *ierr = MPI_Cart_create( (MPI_Comm)(*v1), *v2, v3, l4, l5, (MPI_Comm *)(v6) );)"}
{"predict": "(172, MPI, MPI_Wait( &send_req[p], &status);); (172, MPI, MPI_Wait( &send_req[procs+p], &status);); (164, MPI, MPI_Recv( itemp, it, mpi_int_t, p, p, grid->comm, &status );)", "label": "(191, MPI, MPI_Wait( &send_req[p], &status);); (191, MPI, MPI_Wait( &send_req[procs+p], &status);); (170, MPI, MPI_Recv( itemp, it, mpi_int_t, p, p, grid->comm, &status );)"}
{"predict": "(16, CUDA, return CUDA(cudaGetLastError());)", "label": "(16, CUDA, return CUDA(cudaGetLastError());)"}
{"predict": "(49, CUDA, CHECK_CUDA(cudaMalloc(&dy, dataSize * sizeof(int32_t)));); (32, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (77, CUDA, CHECK_CUDA(cudaFree(dy));); (46, CUDA, CHECK_CUDA(cudaMalloc(&dx, dataSize * sizeof(int32_t)));); (33, CUDA, CHECK_HIEDNN(hiednnSetCudaStream(handle, 0));); (72, CUDA, CHECK_CUDA(cudaFree(dx));); (73, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));)", "label": "(50, CUDA, CHECK_CUDA(cudaMalloc(&dy, dataSize * sizeof(int32_t)));); (75, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (32, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (76, CUDA, CHECK_CUDA(cudaFree(dy));); (45, CUDA, CHECK_CUDA(cudaMalloc(&dx, dataSize * sizeof(int32_t)));); (32, CUDA, CHECK_HIEDNN(hiednnSetCudaStream(handle, 0));); (72, CUDA, CHECK_CUDA(cudaFree(dx));)"}
{"predict": "(23, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(23, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(23, OpenMP, #pragma omp target teams distribute parallel for reduction(&&: Red)); (16, OpenMP, #pragma omp target teams distribute parallel for reduction(&: Red)); (2, OpenMP, #pragma omp target teams distribute parallel for reduction(-: Red)); (10, OpenMP, #pragma omp target teams distribute parallel for reduction(|: Red)); (31, OpenMP, #pragma omp target teams distribute parallel for reduction(+: Red)); (24, OpenMP, #pragma omp target teams distribute parallel for reduction(||: Red)); (2, OpenMP, #pragma omp target teams distribute parallel for reduction(*: Red)); (13, OpenMP, #pragma omp target teams distribute parallel for reduction(&&: Red)); (16, OpenMP, #pragma omp target teams distribute parallel for reduction(&: Red))", "label": "(28, OpenMP, #pragma omp parallel for reduction(&&: Red)); (7, OpenMP, #pragma omp parallel for reduction(-: Red)); (14, OpenMP, #pragma omp parallel for reduction(&: Red)); (17, OpenMP, #pragma omp parallel for reduction(|: Red)); (0, OpenMP, #pragma omp parallel for reduction(+: Red)); (19, OpenMP, #pragma omp parallel for reduction(^: Red)); (6, OpenMP, #pragma omp parallel for reduction(*: Red)); (25, OpenMP, #pragma omp parallel for reduction(||: Red))"}
{"predict": "(39, OpenMP, #pragma omp simd aligned(f:j)); (33, OpenMP, #pragma omp simd aligned(f)); (13, OpenMP, #pragma omp simd aligned (argc : 5)); (25, OpenMP, #pragma omp simd aligned(i)); (14, OpenMP, #pragma omp simd aligned (*argc > 0? argv[1] : argv[2])); (2, OpenMP, #pragma omp simd aligned (); (30, OpenMP, #pragma omp simd aligned(v:16)); (21, OpenMP, #pragma omp simd aligned(h)); (5, OpenMP, #pragma omp simd aligned (argc); (1, OpenMP, #pragma omp simd aligned ()); (13, OpenMP, #pragma omp simd aligned (argv[1])); (22, OpenMP, #pragma omp simd aligned(v:1)); (14, OpenMP, #pragma omp simd aligned(e, g)); (5, OpenMP, #pragma omp simd aligned (S1)); (1, OpenMP, #pragma omp simd aligned (argc,); (3, OpenMP, #pragma omp simd aligned (argc :); (34, OpenMP, #pragma omp simd aligned(f:j) aligned(f:j)); (23, OpenMP, #pragma omp parallel); (24, OpenMP, #pragma omp simd aligned(f:i))", "label": "(25, OpenMP, #pragma omp simd aligned(e, g)); (18, OpenMP, #pragma omp simd aligned (argc : 5)); (29, OpenMP, #pragma omp simd aligned(i)); (5, OpenMP, #pragma omp simd aligned (); (17, OpenMP, #pragma omp simd aligned (S1)); (28, OpenMP, #pragma omp parallel); (40, OpenMP, #pragma omp simd aligned(f:j)); (10, OpenMP, #pragma omp simd aligned (*argc > 0? argv[1] : argv[2])); (6, OpenMP, #pragma omp simd aligned (argc,); (14, OpenMP, #pragma omp simd aligned (argv[1])); (-4, OpenMP, #pragma omp simd aligned); (-1, OpenMP, #pragma omp simd aligned ()); (25, OpenMP, #pragma omp simd aligned(v:16)); (28, OpenMP, #pragma omp simd aligned(f)); (14, OpenMP, #pragma omp simd aligned(h)); (-3, OpenMP, #pragma omp simd aligned (argc)"}
{"predict": "(13, MPI, mpi_errno = MPI_Type_ub(type, NULL);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (7, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (13, MPI, MPI_Error_class(mpi_errno, &errclass);); (6, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(13, MPI, MPI_Type_size(type, &size);); (14, MPI, mpi_errno = MPI_Type_ub(type, NULL);); (18, MPI, MPI_Type_free(&type);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (11, MPI, MPI_Error_class(mpi_errno, &errclass);); (4, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(13, MPI, MPI_Comm_size(comm, &size);); (27, MPI, MPI_Reduce_scatter_init(sendbuf, &recvbuf, recvcounts, MPI_INT, MPI_SUM, comm, info, &req);); (24, MPI, MPI_Info_create(&info);); (36, MPI, MPI_Info_free(&info);); (36, MPI, MPI_Request_free(&req);); (11, MPI, MPI_Comm_rank(comm, &rank);); (30, MPI, MPI_Barrier(comm);)", "label": "(26, MPI, MPI_Start(&req);); (11, MPI, MPI_Comm_size(comm, &size);); (13, MPI, MPI_Info_create(&info);); (24, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (36, MPI, MPI_Info_free(&info);); (34, MPI, MPI_Request_free(&req);); (18, MPI, MPI_Reduce_scatter_init(sendbuf, &recvbuf, recvcounts, MPI_INT, MPI_SUM, comm, info, &req);); (6, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(4, MPI, MPI_Init(&argc, &argv);); (30, MPI, MPI_Finalize();); (3, MPI, int ierr = MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(4, MPI, MPI_Init(&argc, &argv);); (30, MPI, MPI_Finalize();); (3, MPI, int ierr = MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(7, CUDA, __shared__ block_radix_sort_t::TempStorage temp_storage;)", "label": "(7, CUDA, __shared__ block_radix_sort_t::TempStorage temp_storage;)"}
{"predict": "(24, CUDA, extern __shared__ char smem_[];); (26, CUDA, __shared__ half2 smem_red[VECS_PER_CTA][WARPS];)", "label": "(24, CUDA, extern __shared__ char smem_[];); (26, CUDA, __shared__ half2 smem_red[VECS_PER_CTA][WARPS];)"}
{"predict": "(133, OpenCL, clReleaseProgram(plan->program);); (68, OpenCL, err = clGetDeviceInfo(devices[i], CL_DEVICE_NAME, sizeof(devicename), devicename, NULL);); (98, OpenCL, clReleaseProgram(plan->program);); (94, OpenCL, err = clBuildProgram(plan->program, 0, NULL, NULL, NULL, NULL);); (133, OpenCL, clReleaseContext(context);); (64, OpenCL, err = clBuildProgram(plan->program, 0, NULL, NULL, NULL, NULL);)", "label": "(76, OpenCL, err = clBuildProgram(plan->program, 1, &devices[i], coptions, NULL, NULL);); (59, OpenCL, plan->program = clCreateProgramWithSource(context, 1, (const char**) &source_str, NULL, &err);); (68, OpenCL, err = clGetDeviceInfo(devices[i], CL_DEVICE_TYPE, sizeof(device_type), &device_type, NULL);); (88, OpenCL, err = clGetDeviceInfo(devices[i], CL_DEVICE_NAME, sizeof(devicename), devicename, NULL);)"}
{"predict": "(28, MPI, MPI_Recv(&ack, 1, MPI_CHAR, left, tag, comm, MPI_STATUS_IGNORE);); (21, MPI, MPI_Send(out, count, type, left, tag, comm);); (17, MPI, MPI_Send(in, count, type, right, tag, comm);); (17, MPI, MPI_Waitall(2 * window_size, requests, MPI_STATUSES_IGNORE);); (23, MPI, MPI_Recv(&ack, 1, MPI_CHAR, right, tag, comm, MPI_STATUS_IGNORE);)", "label": "(24, MPI, MPI_Recv(&ack, 1, MPI_CHAR, right, tag, comm, MPI_STATUS_IGNORE);); (26, MPI, MPI_Irecv(out, count, type, set_src? left : MPI_ANY_SOURCE, tag, comm, &requests[w]);); (28, MPI, MPI_Isend(in, count, type, left, (tag == MPI_ANY_TAG? 0 : tag), comm, &requests[w + window_size]);); (20, MPI, MPI_Waitall(2 * window_size, requests, MPI_STATUSES_IGNORE);); (17, MPI, MPI_Isend(in, count, type, right, (tag == MPI_ANY_TAG? 0 : tag), comm, &requests[w + window_size]);); (13, MPI, MPI_Irecv(out, count, type, set_src? right : MPI_ANY_SOURCE, tag, comm, &requests[w]);); (27, MPI, MPI_Send(&ack, 1, MPI_CHAR, left, (tag == MPI_ANY_TAG? 0 : tag), comm);)"}
{"predict": "(21, OpenMP, #pragma omp parallel master taskloop simd collapse (foobool(argc)), collapse (true), collapse (-5)); (36, OpenMP, #pragma omp parallel master taskloop simd collapse (2)); (3, OpenMP, #pragma omp parallel master taskloop simd collapse ()); (10, OpenMP, #pragma omp parallel master taskloop simd collapse (1))); (1, OpenMP, #pragma omp parallel master taskloop simd collapse ()); (12, OpenMP, #pragma omp parallel master taskloop simd collapse ((ST > 0)? 1 + ST : 2)); (15, OpenMP, #pragma omp parallel master taskloop simd collapse (S)); (25, OpenMP, #pragma omp parallel master taskloop simd collapse (1)); (5, OpenMP, #pragma omp parallel master taskloop simd collapse (ST); (27, OpenMP, #pragma omp parallel master taskloop simd collapse (N)); (28, OpenMP, #pragma omp parallel master taskloop simd collapse (4)); (-3, OpenMP, #pragma omp parallel master taskloop simd collapse (argc); (8, OpenMP, #pragma omp parallel master taskloop simd collapse (foobool(1) > 0? 1 : 2)); (1, OpenMP, #pragma omp parallel master taskloop simd collapse (foobool(argc)))", "label": "(13, OpenMP, #pragma omp parallel master taskloop simd collapse (ST); (5, OpenMP, #pragma omp parallel master taskloop simd collapse ()); (15, OpenMP, #pragma omp parallel master taskloop simd collapse ((ST > 0)? 1 + ST : 2)); (1, OpenMP, #pragma omp parallel master taskloop simd collapse (); (20, OpenMP, #pragma omp parallel master taskloop simd collapse (S)); (10, OpenMP, #pragma omp parallel master taskloop simd collapse (1))); (27, OpenMP, #pragma omp parallel master taskloop simd collapse (1)); (30, OpenMP, #pragma omp parallel master taskloop simd collapse (2)); (14, OpenMP, #pragma omp parallel master taskloop simd collapse (foobool(argc)), collapse (true), collapse (-5)); (-7, OpenMP, #pragma omp parallel master taskloop simd collapse); (0, OpenMP, #pragma omp parallel master taskloop simd collapse (argc); (24, OpenMP, #pragma omp parallel master taskloop simd collapse (N))"}
{"predict": "(1, TBB, tbb::global_control gc(tbb::global_control::max_allowed_parallelism, 2);); (3, TBB, tbb::task_group_context tgc;)", "label": "(5, TBB, tbb::task_group_context tgc;); (15, TBB, tgc.set_priority(tbb::priority_high);); (4, TBB, tgc.set_priority(tbb::priority_normal);)"}
{"predict": "(8, OpenMP, #pragma omp parallel for)", "label": "(8, OpenMP, #pragma omp parallel for)"}
{"predict": "(13, OpenMP, #pragma omp target); (14, OpenMP, #pragma omp teams)", "label": "(13, OpenMP, #pragma omp parallel); (13, OpenMP, #pragma omp task if (task: false))"}
{"predict": "(23, OpenMP, #pragma omp distribute parallel for dist_schedule(static, ch)); (30, OpenMP, #pragma omp distribute parallel for schedule(static, ch)); (45, OpenMP, #pragma omp distribute parallel for schedule(static)); (53, OpenMP, #pragma omp distribute parallel for schedule(dynamic, ch)); (12, OpenMP, #pragma omp distribute parallel for dist_schedule(static)); (56, OpenMP, #pragma omp distribute parallel for schedule(dynamic)); (1, OpenMP, #pragma omp distribute parallel for); (36, OpenMP, #pragma omp distribute parallel for schedule(static, 10)); (44, OpenMP, #pragma omp distribute parallel for schedule(dynamic)); (3, OpenMP, #pragma omp target); (36, OpenMP, #pragma omp distribute parallel for schedule(static, 100)); (20, OpenMP, #pragma omp distribute parallel for dist_schedule(static, 10)); (24, OpenMP, #pragma omp distribute parallel for schedule(static, 100)); (-4, OpenMP, #pragma omp teams)", "label": "(42, OpenMP, #pragma omp distribute parallel for schedule(static, ch)); (49, OpenMP, #pragma omp distribute parallel for schedule(dynamic)); (7, OpenMP, #pragma omp distribute parallel for); (15, OpenMP, #pragma omp distribute parallel for dist_schedule(static)); (30, OpenMP, #pragma omp distribute parallel for schedule(static)); (53, OpenMP, #pragma omp distribute parallel for schedule(dynamic, ch)); (5, OpenMP, #pragma omp cancel for); (0, OpenMP, #pragma omp target); (18, OpenMP, #pragma omp distribute parallel for dist_schedule(static, ch)); (-1, OpenMP, #pragma omp teams)"}
{"predict": "(68, OpenMP, #pragma omp teams reduction(+:i)); (28, OpenMP, #pragma omp distribute private(argv[1])); (30, OpenMP, #pragma omp distribute private(ca)); (34, OpenMP, #pragma omp distribute private(h, B::x)); (10, OpenMP, #pragma omp distribute private (argc); (40, OpenMP, #pragma omp teams private(i)); (37, OpenMP, #pragma omp teams); (39, OpenMP, #pragma omp distribute private(i)); (4, OpenMP, #pragma omp distribute private ()); (10, OpenMP, #pragma omp distribute private (argc,); (44, OpenMP, #pragma omp distribute firstprivate(i)); (3, OpenMP, #pragma omp distribute private (argc); (23, OpenMP, #pragma omp distribute private(e, g)); (18, OpenMP, #pragma omp distribute private(ba)); (25, OpenMP, #pragma omp distribute private(j)); (2, OpenMP, #pragma omp distribute private (argc > 0? argv[1] : argv[2])); (13, OpenMP, #pragma omp distribute private(da)); (18, OpenMP, #pragma omp distribute private(i)); (1, OpenMP, #pragma omp distribute private (argc)); (36, OpenMP, #pragma omp distribute private(j)); (38, OpenMP, #pragma omp teams reduction(+:i)); (57, OpenMP, #pragma omp distribute private(i) reduction(+:i)); (2, OpenMP, #pragma omp distribute private (S1)); (27, OpenMP, #pragma omp target)", "label": "(35, OpenMP, #pragma omp distribute private(e, g)); (51, OpenMP, #pragma omp parallel private(i)); (5, OpenMP, #pragma omp distribute private); (12, OpenMP, #pragma omp distribute private (argc,); (9, OpenMP, #pragma omp distribute private (argc); (36, OpenMP, #pragma omp target); (19, OpenMP, #pragma omp distribute private (argv[1])); (12, OpenMP, #pragma omp distribute private (argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (9, OpenMP, #pragma omp distribute private (argc > 0? argv[1] : argv[2])); (14, OpenMP, #pragma omp distribute private (a, b, c, d, f)); (17, OpenMP, #pragma omp distribute private(ba)); (39, OpenMP, #pragma omp distribute private(j)); (-1, OpenMP, #pragma omp distribute private ()); (24, OpenMP, #pragma omp distribute private(h)); (44, OpenMP, #pragma omp teams reduction(+:i)); (18, OpenMP, #pragma omp distribute private(S2::S2s)); (43, OpenMP, #pragma omp distribute private(i)); (14, OpenMP, #pragma omp distribute private(da)); (-9, OpenMP, #pragma omp distribute private (); (2, OpenMP, #pragma omp distribute private (S1)); (29, OpenMP, #pragma omp teams private(i)); (8, OpenMP, #pragma omp distribute private(ca)); (17, OpenMP, #pragma omp distribute); (22, OpenMP, #pragma omp distribute firstprivate(i)); (30, OpenMP, #pragma omp teams firstprivate(i)); (17, OpenMP, #pragma omp teams)"}
{"predict": "(23, CUDA, __shared__  ll_t smem[_TPB_];)", "label": "(22, CUDA, __shared__ volatile ll_t smem[_TPB_];)"}
{"predict": "(3, MPI, MPI_Init(&argc, &argv);); (24, MPI, MPI_Finalize();); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (1, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (88, MPI, MPI_Barrier(params.comm);); (105, MPI, MPI_Bcast(&ierr, 1, MPI_INT, 0, params.comm);)", "label": "(3, MPI, MPI_Init(&argc, &argv);); (303, MPI, MPI_Finalize();); (23, MPI, MPI_Barrier(MPI_COMM_WORLD);); (222, MPI, MPI_Allreduce(&local_opt_worst_time, &opt_worst_time, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);)"}
{"predict": "(7, CUDA, cudaMemcpy(data,datag+inidata,sizeof(float3)*ndata,cudaMemcpyDeviceToHost);); (7, CUDA, fcuda::Check_CudaErroorFun(\"DgReduSumFloat3\");)", "label": "(7, CUDA, fcuda::Check_CudaErroorFun(\"DgReduSumFloat3\");); (5, CUDA, cudaMemcpy(data,datag+inidata,sizeof(float3)*ndata,cudaMemcpyDeviceToHost);)"}
{"predict": "(222, OpenMP, #define GOMP_CRITICAL_SWP(FLAG))", "label": "(127, OpenMP, #define GOMP_CRITICAL_SWP(FLAG))"}
{"predict": "(31, CUDA, checkCudaErrors(cudaMemcpyAsync(dst, src, bytes, kind));); (31, CUDA, cudaDeviceSynchronize();); (29, CUDA, checkCudaErrors(cudaMemcpyAsync(dst, src, bytes, kind));); (31, CUDA, checkCudaErrors(cudaMemcpyAsync(dst, src, bytes, kind));); (29, CUDA, checkCudaErrors(cudaMemcpyAsync(dst, src, bytes, kind));)", "label": "(27, CUDA, cudaMemcpy(dst, src, bytes, kind);)"}
{"predict": "(8, TBB, tbb::atomic<int> b;); (6, TBB, std::list< std::pair<const int, tbb::atomic<int> > > arrIntTbb;)", "label": "(9, TBB, tbb::atomic<int> b;); (6, TBB, std::list< std::pair< const int, tbb::atomic<int> > > arrIntTbb;)"}
{"predict": "(345, MPI, MPI_Finalize();); (38, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (33, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (96, MPI, MPI_Query_thread(&omp_mpi_level);)", "label": "(97, MPI, MPI_Query_thread(&omp_mpi_level);); (38, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (33, MPI, MPI_Init_thread(&argc, &argv, required, &provided);)"}
{"predict": "(367, MPI, start_exch = MPI_Wtime();); (150, MPI, start_comm_time = MPI_Wtime();); (180, MPI, end_comm_time = MPI_Wtime();); (408, MPI, start_write_time = MPI_Wtime();); (414, MPI, end_write_time = MPI_Wtime();); (505, MPI, end_exch = MPI_Wtime();)", "label": "(391, MPI, start_exch = MPI_Wtime();); (143, MPI, start_comm_time = MPI_Wtime();); (156, MPI, end_comm_time = MPI_Wtime();); (435, MPI, start_write_time = MPI_Wtime();); (442, MPI, end_write_time = MPI_Wtime();); (495, MPI, end_exch = MPI_Wtime();)"}
{"predict": "(33, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (35, CUDA, CUDA_CHECK(cudaFree(d_data));)", "label": "(33, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (35, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(57, MPI, MPI_Type_free(&row);); (44, MPI, MPI_Unpack(buffer, bufsize, &position, b, 1, xpose, MPI_COMM_WORLD);); (34, MPI, MPI_Pack(a, 1, xpose, buffer, bufsize, &position, MPI_COMM_WORLD);); (28, MPI, MPI_Type_commit(&xpose);); (28, MPI, MPI_Type_free(&xpose);); (24, MPI, MPI_Type_create_hvector(100, 1, sizeofint, row, &xpose);); (36, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (24, MPI, MPI_Pack_size(1, xpose, MPI_COMM_WORLD, &bufsize);); (20, MPI, MPI_Type_size(MPI_INT, &sizeofint);); (31, MPI, MPI_Type_set_name(xpose, \"Xpose\");); (20, MPI, MPI_Type_vector(100, 1, 100, MPI_INT, &row);)", "label": "(43, MPI, MPI_Pack(a, 1, xpose, buffer, bufsize, &position, MPI_COMM_WORLD);); (59, MPI, MPI_Type_free(&row);); (31, MPI, MPI_Type_commit(&xpose);); (56, MPI, MPI_Type_free(&xpose);); (28, MPI, MPI_Type_hvector(100, 1, sizeofint, row, &xpose);); (23, MPI, MPI_Type_extent(MPI_INT, &sizeofint);); (30, MPI, MPI_Pack_size(1, xpose, MPI_COMM_WORLD, &bufsize);); (34, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (39, MPI, MPI_Unpack(buffer, bufsize, &position, b, 100 * 100, MPI_INT, MPI_COMM_WORLD);); (22, MPI, MPI_Type_vector(100, 1, 100, MPI_INT, &row);)"}
{"predict": "(25, MPI, MPI_Isend(sendbuf, BUF_COUNT, MPI_INT, i, TAG, MPI_COMM_WORLD, &sendreqs[i]);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (43, MPI, MPI_Waitall(nprocs, sendreqs, sendstats);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (19, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (42, MPI, MPI_Waitall(nprocs, sendreqs, sendstats);); (13, MPI, MPI_Barrier(MPI_COMM_WORLD);); (21, MPI, MPI_Isend(sendbuf, BUF_COUNT, MPI_INT, i, 0, MPI_COMM_WORLD, &sendreqs[i]);)"}
{"predict": "(3, MPI, MPI_Init(&argc, &argv);); (49, CUDA, cudaStat = cudaSetDevice(localDeviceId);); (194, CUDA, cudaStat = cudaMemcpyAsync(&h_work_getrs, d_work_getrs, workspaceInBytesOnHost_getrs, cudaMemcpyDeviceToHost, localStream);); (222, CUDA, cudaStat = cudaStreamSynchronize(localStream);); (104, CUDA, cudaStat = cudaMalloc((void**)&d_info_geqrf, sizeof(*d_info_geqrf));); (158, CUDA, cudaStat = cudaMemsetAsync(d_info_getrf, 0, sizeof(*d_info_getrf), localStream);); (150, CUDA, cudaStat = cudaMalloc((void**)&d_work_getrf, workspaceInBytesOnDevice_getrf);); (231, CUDA, cudaStat = cudaFreeAsync(d_A, localStream);); (235, CUDA, cudaStat = cudaFreeAsync(d_tau, localStream);); (144, CUDA, cudaStat = cudaMalloc((void**)&d_A, M*N*sizeof(*d_A));); (229, CUDA, cudaStat = cudaFreeAsync(d_info_geqrf, localStream);); (232, CUDA, cudaStat = cudaFreeAsync(d_work_geqrf, localStream);); (244, MPI, MPI_Finalize();); (238, CUDA, cudaStat = cudaFreeAsync(d_tau, localStream);); (183, CUDA, cudaStat = cudaMemcpyAsync(&h_work_getrf, d_work_getrf, workspaceInBytesOnHost_getrf, cudaMemcpyDeviceToHost, localStream);); (125, CUDA, cudaStat = cudaMalloc((void**)&d_tau, M*sizeof(*d_tau));); (235, CUDA, cudaStat = cudaFreeAsync(d_A, localStream);); (143, CUDA, cudaStat = cudaMemsetAsync(d_tau, 0, M*sizeof(*d_tau), localStream);); (133, CUDA, cudaStat = cudaMalloc((void**)&d_work_geq", "label": "(358, CUDA, cudaStat = cudaMemcpy(h_A, d_global_Q, sizeof(double) * lda * colsA, cudaMemcpyDeviceToHost);); (196, CUDA, cudaStat = cudaMalloc((void**)&d_info_geqrf, sizeof(int));); (27, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rankId);); (429, CUDA, cudaStat = cudaFree(d_tau);); (-1, MPI, MPI_Init(NULL, NULL);); (274, CUDA, cudaStat = cudaMemcpyAsync(&h_info_geqrf, d_info_geqrf, sizeof(int), cudaMemcpyDeviceToHost, localStream);); (471, CUDA, cudaStat = cudaStreamDestroy(localStream);); (474, MPI, MPI_Barrier(MPI_COMM_WORLD);); (42, CUDA, cudaStat = cudaSetDevice(localDeviceId);); (43, CUDA, cudaStat = cudaFree(0);); (194, CUDA, cudaStat = cudaMemset(d_info_geqrf, 0, sizeof(int));); (428, CUDA, cudaStat = cudaFree(d_work_geqrf);); (178, CUDA, cudaStat = cudaMalloc((void**)&d_tau, localColsA * sizeof(double));); (301, CUDA, cudaStat = cudaMalloc((void**)&d_global_Q, lda * colsA * sizeof(double));); (213, CUDA, cudaStat = cudaMalloc((void**)&d_work_geqrf, workspaceInBytesOnDevice_geqrf);); (431, CUDA, cudaStat = cudaFree(d_info_geqrf);); (12, MPI, MPI_Comm_size(MPI_COMM_WORLD, &rankSize);); (154, CUDA, cudaStat = cudaMalloc((void**)&d_A, localColsA * LLDA * sizeof(double));); (305, CUDA, cudaStat = cudaMemcpy(d_global_R, d_global_Q, sizeof(double) * lda * colsA, cudaMemcpyDeviceToDevice);); (302, CUDA, cudaStat = cudaMemcpy(d_global_Q, h_A, sizeof(double) * lda * colsA, cudaMemcpyHostToDevice);); (53, CUDA, cudaStat = cudaStreamCreate(&localStream);); (463, MPI, MPI_Finalize();); (403, CUDA, cudaStat = cudaFree(d_A);); (293, CUDA, cudaStat = cudaMalloc((void**)&d_global_R, lda * colsA * sizeof(double));); (294, CUDA, cudaStat = cudaMalloc((void**)&d_global_tau, colsA * sizeof(double));)"}
{"predict": "(7, CUDA, __shared__ typename cg::Scratch_allocator<T>::template allocator_holder<cg::zero_allocator<T>> scratch;)", "label": "(4, CUDA, __shared__ cg::block_tile_memory<BlockSize> scratch;)"}
{"predict": "(61, CUDA, cudaFreeHost( hostInt );); (59, CUDA, cudaFree( deviceInt );)", "label": "(59, CUDA, cudaFree( deviceInt );); (59, CUDA, cudaFreeHost( hostInt );)"}
{"predict": "(99, CUDA, cudaFree( dptrForce[i] );); (97, CUDA, cudaFree( dptrPosMass[i] );); (26, CUDA, cudaSetDevice( oldDevice );)", "label": "(98, CUDA, cudaFree( dptrPosMass[i] );); (100, CUDA, cudaSetDevice( oldDevice );); (97, CUDA, cudaFree( dptrForce[i] );)"}
{"predict": "(9, OpenCL, clReleaseMemObject(objiAmax);); (3, OpenCL, clReleaseMemObject(objX);); (11, OpenCL, clReleaseMemObject(objScratch);)", "label": "(5, OpenCL, clReleaseMemObject(objX);); (12, OpenCL, clReleaseMemObject(objScratch);); (7, OpenCL, clReleaseMemObject(objiAmax);)"}
{"predict": "(21, TBB, tbb::concurrent_bounded_queue<T>* q = new( tbb::cache_aligned_allocator<hacked_bounded_concurrent_queue>().allocate(1) ) hacked_bounded_concurrent_queue();); (3, TBB, tbb::concurrent_queue<T>* q = new( tbb::cache_aligned_allocator<hacked_concurrent_queue>().allocate(1) ) hacked_concurrent_queue();); (16, TBB, tbb::concurrent_bounded_queue<T>* q = new( tbb::cache_aligned_allocator<tbb::concurrent_bounded_queue>().allocate(1) ) tbb::concurrent_bounded_queue();)", "label": "(19, TBB, REMARK(\"Wraparound on tbb::concurrent_bounded_queue...\");); (2, TBB, tbb::concurrent_queue<T>* q = new tbb::concurrent_queue<T>;); (16, TBB, tbb::concurrent_bounded_queue<T>* q = new tbb::concurrent_bounded_queue<T>;)"}
{"predict": "(24, OpenMP, #pragma omp parallel); (8, OpenMP, #pragma omp parallel num_threads(4))", "label": "(25, OpenMP, printf(\"Thread %d sleeps now for %d seconds\\n\", omp_get_thread_num(), SLEEPTIME);); (8, OpenMP, #pragma omp parallel); (31, OpenMP, printf(\"Thread %d wakes up again\\n\", omp_get_thread_num());)"}
{"predict": "(37, SYCL, Alloca->setMetadata(hipsycl::compiler::MDKind::Arrayified, MDAlloca);)", "label": "(45, SYCL, GEP->setMetadata(hipsycl::compiler::MDKind::Arrayified, MDAlloca);); (36, SYCL, Alloca->setMetadata(hipsycl::compiler::MDKind::Arrayified, MDAlloca);)"}
{"predict": "lastprivate((99, OpenMP, #pragma omp parallel private(xa)); (105, OpenMP, #pragma omp parallel master taskloop simd firstprivate(j)); (10, OpenMP, #pragma omp parallel master taskloop simd firstprivate); (110, OpenMP, #pragma omp parallel master taskloop simd firstprivate(si)); (121, OpenMP, #pragma omp parallel master taskloop simd firstprivate(B::x)); (67, OpenMP, #pragma omp parallel master taskloop simd firstprivate(S2::S2s)); (5, OpenMP, #pragma omp parallel); (25, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argc,); (56, OpenMP, #pragma omp parallel master taskloop simd firstprivate(da)); (13, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argc); (78, OpenMP, #pragma omp parallel master taskloop simd safelen(5)); (65, OpenMP, #pragma omp parallel master taskloop simd firstprivate(S2::S2sc)); (92, OpenMP, #pragma omp parallel master taskloop simd lastprivate(g) firstprivate(g)); (79, OpenMP, #pragma omp parallel master taskloop simd private(xa), firstprivate(xa)); (30, OpenMP, #pragma omp parallel master taskloop simd firstprivate(a, b, c, d, f)); (34, OpenMP, #pragma omp parallel master taskloop simd firstprivate(2 * 2)); (72, OpenMP, #pragma omp parallel master taskloop simd firstprivate(h)); (96, OpenMP, #pragma omp parallel master taskloop simd firstprivate(i)); (14, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argc > 0? argv[1] : argv[2])); (4, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (", "label": "(22, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argc); (78, OpenMP, #pragma omp parallel master taskloop simd safelen(5)); (81, OpenMP, #pragma omp parallel master taskloop simd firstprivate(e, g)); (125, OpenMP, #pragma omp parallel private(i)); (87, OpenMP, #pragma omp parallel master taskloop simd firstprivate(h)); (53, OpenMP, #pragma omp parallel master taskloop simd firstprivate(ca)); (48, OpenMP, #pragma omp parallel master taskloop simd firstprivate(ba)); (27, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (38, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argv[1])); (90, OpenMP, #pragma omp parallel master taskloop simd firstprivate(i)); (-1, OpenMP, #pragma omp parallel); (64, OpenMP, #pragma omp parallel master taskloop simd firstprivate(S2::S2sc)); (128, OpenMP, #pragma omp parallel master taskloop simd firstprivate(B::x)); (119, OpenMP, #pragma omp parallel reduction(+ : i)); (88, OpenMP, #pragma omp parallel shared(xa)); (23, OpenMP, #pragma omp parallel master taskloop simd firstprivate(S1)); (14, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argc > 0? argv[1] : argv[2])); (78, OpenMP, #pragma omp parallel master taskloop simd private(xa), firstprivate(xa)); (53, OpenMP, #pragma omp parallel master taskloop simd firstprivate(S2::S2s)); (125, OpenMP, #pragma omp parallel master taskloop simd firstprivate(si)); (-6, OpenMP, #pragma omp parallel master taskloop simd firstprivate(); (-3, OpenMP, #pragma omp parallel master taskloop simd firstprivate()); (65, OpenMP, #pragma omp parallel master taskloop simd firstprivate(m)); (84, OpenMP, #pragma omp parallel master taskloop simd firstprivate(j)); (18, OpenMP, #pragma omp parallel master taskloop simd firstprivate(a, b, c, d, f)); (86, OpenMP, #pragma omp parallel master taskloop simd lastprivate(g) firstprivate(g)); (89, OpenMP, #pragma omp parallel master taskloop simd lastprivate(n) firstprivate(n)); (-1, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argc,); (39, OpenMP, #pragma omp parallel master taskloop simd firstprivate(xa)); (21, OpenMP, #pragma omp parallel master taskloop simd firstprivate(2 * 2)); (32, OpenMP, #pragma omp parallel master taskloop simd firstprivate(da)); (-21, OpenMP, #pragma omp parallel master taskloop simd firstprivate)"}
{"predict": "(10, OpenMP, #pragma omp parallel for private(i2)); (14, OpenMP, #pragma omp flush)", "label": "(14, OpenMP, #pragma omp flush); (9, OpenMP, #pragma omp parallel for reduction(+:sum) schedule(static,1) private(i) private(i2))"}
{"predict": "(24, MPI, MPI_Recv(&ack, 1, MPI_CHAR, left, tag, comm, MPI_STATUS_IGNORE);); (18, MPI, MPI_Send(in, count, type, left, tag, comm);); (18, MPI, MPI_Waitall(window_size, requests, MPI_STATUSES_IGNORE);); (21, MPI, MPI_Send(out, count, type, right, tag, comm);); (20, MPI, MPI_Recv(&ack, 1, MPI_CHAR, right, tag, comm, MPI_STATUS_IGNORE);)", "label": "(21, MPI, MPI_Recv(&ack, 1, MPI_CHAR, right, tag, comm, MPI_STATUS_IGNORE);); (23, MPI, MPI_Irecv(out, count, type, set_src? left : MPI_ANY_SOURCE, tag, comm, &requests[w]);); (18, MPI, MPI_Waitall(window_size, requests, MPI_STATUSES_IGNORE);); (15, MPI, MPI_Isend(in, count, type, right, (tag == MPI_ANY_TAG? 0 : tag), comm, &requests[w]);); (23, MPI, MPI_Send(&ack, 1, MPI_CHAR, left, (tag == MPI_ANY_TAG? 0 : tag), comm);)"}
{"predict": "(10, OpenMP, if (!strncmp(curTok, \"dynamic\", 6)) push_pcode(\"dynamic_call\", dynamic_call, 0);); (11, OpenMP, if (!strncmp(curTok, \"static\", 5)) push_pcode(\"static_call\", dynamic_static_call, 0);)", "label": "(34, OpenMP, comp_do_parse(cur, tail);)"}
{"predict": "(3, OpenMP, #pragma omp target device(0)); (13, OpenMP, #pragma omp target device(1))", "label": "(3, OpenMP, #pragma omp target device(0)); (13, OpenMP, #pragma omp target device(1))"}
{"predict": "(5, OpenMP, #pragma omp task shared(var, a)); (2, OpenMP, #pragma omp parallel num_threads(2) shared(var, a)); (3, OpenMP, #pragma omp master)", "label": "(4, OpenMP, #pragma omp master); (5, OpenMP, #pragma omp taskgroup); (6, OpenMP, #pragma omp task shared(var, a)); (0, OpenMP, #pragma omp parallel num_threads(2) shared(var, a))"}
{"predict": "(3, MPI, MPI_Comm_size(comm, &size);); (3, MPI, MPI_Comm_rank(comm, &rank);); (13, MPI, MPI_Alltoallv(data_out, counts, displs, MPI_INT, data_in, counts, displs, MPI_INT, comm);)", "label": "(3, MPI, MPI_Comm_size(comm, &size);); (3, MPI, MPI_Comm_rank(comm, &rank);); (13, MPI, MPI_Alltoallv(data_out, counts, displs, MPI_INT, data_in, counts, displs, MPI_INT, comm);)"}
{"predict": "(57, MPI, error = MPI_Barrier(MPI_COMM_WORLD);); (28, MPI, error = MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (63, MPI, error = MPI_Finalize();); (6, MPI, error = MPI_Finalized(&flag);); (15, MPI, MPI_Abort(MPI_COMM_WORLD, error);); (1, MPI, MPI_Error_string(error, err_string, &length);); (17, MPI, error = MPI_Init(NULL, NULL);)", "label": "(11, MPI, MPI_Error_string(error, err_string, &length);); (20, MPI, error = MPI_Init(&argc, &argv);); (27, MPI, error = MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (60, MPI, error = MPI_Finalize();); (50, MPI, error = MPI_Barrier(MPI_COMM_WORLD);); (29, MPI, MPI_Abort(MPI_COMM_WORLD, error);); (3, MPI, error = MPI_Finalized(&flag);)"}
{"predict": "(19, CUDA, cudaDeviceSynchronize();); (34, CUDA, cudaFree(dev_B);); (10, CUDA, cudaMalloc(&dev_A, size_a);); (29, CUDA, cudaFree(dev_A);); (10, CUDA, cudaMalloc(&dev_B, size_b);); (26, CUDA, cudaMemcpy(host_B.data(), dev_B, size_b, cudaMemcpyDeviceToHost);)", "label": "(13, CUDA, cudaMalloc(&dev_B, size_b);); (13, CUDA, cudaDeviceSynchronize();); (10, CUDA, cudaMalloc(&dev_A, size_a);); (27, CUDA, cudaFree(dev_B);); (25, CUDA, cudaFree(dev_A);)"}
{"predict": "(135, OpenMP, #pragma omp target map(tofrom:r.RPtrS[0], r.RPtrS->B)); (143, OpenMP, #pragma omp target map(tofrom:r.ArrS[0], r.ArrS[1], r.ArrS[2], r.ArrS[3], r.ArrS[4], r.ArrS[5], r.ArrS[6], r.ArrS[7], r.ArrS[8], r.ArrS[9])); (130, OpenMP, #pragma omp target map(tofrom:r.ArrS[0])); (232, OpenMP, #pragma omp target map(tofrom:mptr[0][0:2][0])); (140, OpenMP, #pragma omp target map(tofrom:r.ArrS[0], r.ArrS[1], r.ArrS[2], r.ArrS[3], r.ArrS[4], r.ArrS[5], r.ArrS[6], r.ArrS[7], r.ArrS[8], r.ArrS[9], r.ArrS[10], r.ArrS[11], r.ArrS[12], r.ArrS[13], r.ArrS[14], r.ArrS[15], r.ArrS[16], r.ArrS[17], r.ArrS[18], r.ArrS[19], r.ArrS[20], r.ArrS[21], r.ArrS[22], r.ArrS[23], r.ArrS[24], r.ArrS[25], r.ArrS[26], r.ArrS[27], r.ArrS[28], r.ArrS[29], r.ArrS[30], r.ArrS[31], r.ArrS[32], r.ArrS[33], r.ArrS[34], r.ArrS[35], r.ArrS[36], r.ArrS[37], r.ArrS[38], r.Arr(304, OpenMP, #pragma omp target map(tofrom:mptr[", "label": "(35, OpenMP, #pragma omp target teams map(marr[:arg][n:])); (166, OpenMP, #pragma omp target teams map(r.S.Arr[4:5])); (53, OpenMP, #pragma omp target teams map(marr[:arg][:2][:])); (120, OpenMP, #pragma omp target teams map(r.PtrS[0], r.PtrS->B)); (117, OpenMP, #pragma omp target teams map(r.ArrS[0].A, t.ArrS[1].A)); (136, OpenMP, #pragma omp target teams map(r.C, r.D)); (9, OpenMP, #pragma omp target teams map(r)); (80, OpenMP, #pragma omp target teams map(mvla2[:1][:2][:10])); (123, OpenMP, #pragma omp target teams map(r.RPtrS, r.RPtrS->B)); (136, OpenMP, #pragma omp target teams map(r.C) map(r.C)); (21, OpenMP, #pragma omp target teams map(marr[arg:][:arg][:])); (189, OpenMP, #pragma omp target data map(to: t)); (157, OpenMP, #pragma omp target teams map(r.S.Ptr[4:5])); (54, OpenMP, #pragma omp target teams map(marr[:2][2+2-4:][0:5+5])); (35, OpenMP, #pragma omp target teams map(marr[:2][:][1:])); (12, OpenMP, #pragma omp target teams map(marr[arg:][:][:])); (93, OpenMP, #pragma omp target teams map(r.ArrS[:arg].B)); (0, OpenMP, #pragma omp target teams map(marr[2][0:2][0:2])); (80, OpenMP, #pragma omp target teams map(mptr[:1][:2][0:2])); (53, OpenMP, #pragma omp target teams map(marr2[:1][:2][0])); (65, OpenMP, #pragma omp target teams map(mvla2[:1][:2][:11])); (86, OpenMP, #pragma omp target teams map(r.ArrS[:1].B)); (35, OpenMP, #pragma omp target teams map(marr[:1][3:1][:2])); (6, OpenMP, #pragma omp target teams map(marr[arg:])); (29, OpenMP, #pragma omp target teams map(marr[:1][0][:])); (134, OpenMP, #pragma omp target teams map(r.Ptr[3:5])); (74, OpenMP, #pragma omp target teams map(mptr[:1][:][0:2])); (108, OpenMP, #pragma omp target teams map(r.S.Arr[:12])); (125, OpenMP, #pragma omp target teams map(r.A)); (90, OpenMP, #pragma omp target teams map(r.ArrS[0].A, r.ArrS[1].A)); (95, OpenMP, #pragma omp target teams map(r.PtrS, r.PtrS->B)); (164, OpenMP, #pragma omp target teams map(t.D)); (149, OpenMP, #pragma omp target teams map(r.D)); (96, OpenMP, #pragma omp target teams map(r.RPtrS[0], r.RPtrS->B)); (77, OpenMP, #pragma omp target teams map(r.ArrS[0].Arr[1:23])); (98, OpenMP, #pragma omp target teams map(r.RPtrS->A, r.RPtrS->B)); (45, OpenMP, #pragma omp target teams map(mvla[1][2:arg][:])); (142, OpenMP, #pragma omp target data map(to: r.C)); (21, OpenMP, #pragma omp target teams map(marr[:1][3:arg][:2])); (8, OpenMP, #pragma omp target teams map(marr[:2][:][:1])); (37, OpenMP, #pragma omp target teams map(mvla[:2][:arg][:])); (61, OpenMP, #pragma omp target teams map(mptr[:2][:1][0:2])); (71, OpenMP, #pragma omp target teams map(r.ArrS[0].Arr[1:arg])); (142, OpenMP, #pragma omp target data map(to: t.Ptr)); (31, OpenMP, #pragma omp target teams map(mvla[:1][:][0])); (112, OpenMP, #pragma omp target teams map(r.Arr[3:5])); (46, OpenMP, #pragma omp target teams map(mptr[:1][:2-1][2:4-3])); (108, OpenMP, #pragma omp target teams map(r.Arr)); (123, OpenMP, #pragma omp target teams map(r.S.Ptr[:])); (21, OpenMP, #pragma omp target teams map(marr[:1][:2][0])); (-7, OpenMP, #pragma omp target teams map(marr[:2][:1][:])); (125, OpenMP, #pragma omp target teams map(u.B)); (111, OpenMP, #pragma omp target teams map(r.ArrS[3:5].Arr[6:7])); (-16, OpenMP, #pragma omp target teams map(marr[:][:arg][n:])); (97, OpenMP, #pragma omp target teams map(r.C, t.C)); (62, OpenMP, #pragma omp target"}
{"predict": "(13, OpenMP, #pragma omp target map(tofrom: tt) map(to:C[0:10]) nowait); (13, OpenMP, #pragma omp target parallel for)", "label": "(12, OpenMP, #pragma omp target map(tofrom: C) map(to: tt, tt.p1[:10]))"}
{"predict": "(6, CUDA, checkCudaErrors(cudaFree(cusolverData.work_buffer));); (4, CUDA, checkCudaErrors(cudaFreeHost(cusolverData.host_buffer));); (2, CUDA, checkCudaErrors(cudaFree(cusolverData.cusolver_params));)", "label": "(7, CUDA, cudaFree(bufferOffset.W);); (3, CUDA, cudaFree(bufferOffset.srcShiftSum);); (1, CUDA, cudaFree(bufferOffset.dstMean);); (10, CUDA, cudaFree(cusolverData.cusolverBuffer);); (5, CUDA, cudaFree(bufferOffset.J);); (3, CUDA, cudaFree(bufferOffset.r);); (4, CUDA, cudaFree(bufferOffset.calc_buffer);); (-1, CUDA, cudaFree(bufferOffset.LtL);); (-6, CUDA, cudaFree(bufferOffset.srcMean);); (-4, CUDA, cudaFree(bufferOffset.dstShiftSum);); (4, CUDA, cudaFree(cusolverData.cusolverInfo);)"}
{"predict": "(74, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamCreate(&stream));); (71, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (66, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&input, flat_fft_size_bytes));); (90, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(output));); (65, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&output, flat_fft_size_bytes));); (89, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(input));); (65, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(input, input_host.data(), flat_fft_size_bytes, cudaMemcpyHostToDevice));); (82, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamDestroy(stream));)", "label": "(77, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamCreate(&stream));); (71, CUDA, CUDA_CHECK_AND_EXIT(cudaMemset(output, 0b11111111, flat_fft_size_bytes));); (71, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(input, input_host.data(), flat_fft_size_bytes, cudaMemcpyHostToDevice));); (91, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(output));); (70, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (88, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(input));); (62, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&input, flat_fft_size_bytes));); (83, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamDestroy(stream));); (61, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&output, flat_fft_size_bytes));)"}
{"predict": "work_size( kernel, 1, localThreads );(106, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 1, NULL, threads, localThreads, 0, NULL, NULL );); (95, OpenCL, error = clSetKernelArg( kernel, i, sizeof( streams[i] ), &streams[i] );); (21, OpenCL, error = clGetDeviceInfo( device, CL_DEVICE_SINGLE_FP_CONFIG, sizeof( cl_device_fp_config ), &config, NULL );); (90, OpenCL, error = clSetKernelArg( kernel, 3, sizeof( cl_mem ), &streams[2] );); (135, OpenCL, error = clEnqueueUnmapMemObject( queue, streams[2], outDataPtr, 0, NULL, NULL );)", "label": "(108, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 1, NULL, threads, localThreads, 0, NULL, NULL );); (22, OpenCL, error = clGetDeviceInfo( device, CL_DEVICE_SINGLE_FP_CONFIG, sizeof( config ), &config, NULL );); (16, OpenCL, error = clGetDeviceInfo( device, CL_DEVICE_PROFILE, sizeof( kernelSource ), kernelSource, NULL );); (95, OpenCL, error = clSetKernelArg(kernel, (int)i, sizeof( streams[i] ), &streams[i]);); (108, OpenCL, error = clEnqueueReadBuffer( queue, streams[2], true, 0, sizeof( cl_float ) * TEST_SIZE, outData, 0, NULL, NULL );)"}
{"predict": "(11, OpenMP, #pragma omp target data map(tofrom:N_errors,N_target_errors,hostSrcMatA[0:N*M],hostSrcMatB[0:M*P],hostDstMat[0:N*P])); (10, OpenMP, #pragma omp target); (12, OpenMP, #pragma omp task); (54, OpenMP, #pragma omp taskwait)", "label": "(11, OpenMP, #pragma omp parallel for schedule(static,1)); (12, OpenMP, #pragma omp task)"}
{"predict": "(3, CUDA, cudaCheckError(\"Error: cudaMemcpyManaged() failed to copy data to host\");)", "label": "(8, CUDA, safeCall(cudaMemcpy(h_data, data.d_data, sizeof(SiftPoint)*data.numPts, cudaMemcpyDeviceToHost));)"}
{"predict": "(13, SYCL, auto m = sycl::malloc_shared<int>(n, qsort_selector_v);); (15, SYCL, sycl::free(m, qsort_selector_v);)", "label": "(12, TBB, tbb::mutex *m = new tbb::mutex;)"}
{"predict": "(16, OpenMP, #pragma omp parallel for allocate(argc) private(argc, b), firstprivate(c, d), lastprivate(d, f) collapse(N) schedule(static, N) ordered(N) if (parallel : argc) num_threads(N) default(shared) shared(e) reduction(+ : h)); (5, OpenMP, #pragma omp parallel for schedule(dynamic) default(none) copyin(g) linear(a) allocate(a) lastprivate(conditional: d, e, f) order(concurrent)); (10, OpenMP, #pragma omp parallel for)", "label": "(7, OpenMP, #pragma omp parallel for schedule(dynamic) default(none) copyin(g) linear(a) allocate(a) lastprivate(conditional: d, e,f) order(concurrent)); (5, OpenMP, #pragma omp threadprivate(g)); (11, OpenMP, #pragma omp parallel for allocate(argc) private(argc, b), firstprivate(c, d), lastprivate(d, f) collapse(N) schedule(static, N) ordered(N) if (parallel :argc) num_threads(N) default(shared) shared(e) reduction(+ : h))"}
{"predict": "(19, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(19, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(21, TBB, tbb::global_control s(tbb::global_control::max_allowed_parallelism, h_c);); (4, TBB, const unsigned h_c = tbb::tbb_thread::hardware_concurrency();); (1, TBB, TestNullMutex();); (30, TBB, ASSERT(default_ss == tbb::global_control::active_value(tbb::global_control::max_allowed_parallelism), NULL);); (20, TBB, size_t default_ss = tbb::global_control::active_value(tbb::global_control::max_allowed_parallelism);); (18, TBB, TestMultifunctionality();); (2, TBB, tbb::task_scheduler_init init(h_c+1);)", "label": "(31, TBB, ASSERT(default_ss == tbb::global_control::active_value(tbb::global_control::thread_stack_size), NULL);); (3, TBB, const unsigned h_c = tbb::tbb_thread::hardware_concurrency();); (5, TBB, tbb::task_scheduler_init t(h_c+1);); (20, TBB, size_t default_ss = tbb::global_control::active_value(tbb::global_control::thread_stack_size);)"}
{"predict": "(351, OpenCL, err = clEnqueueUnmapMemObject(queue, array, mapped, 0, NULL, NULL);); (316, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(array), &array);); (60, OpenCL, err = clEnqueueWriteBuffer(queue, seed, CL_TRUE, 0, sizeof(cl_uint), &zero, 0, NULL, NULL);); (307, OpenCL, err = clSetKernelArg(kernel, 1, sizeof(cl_uint), &num_of_tests);); (308, OpenCL, err = clSetKernelArg(kernel, 2, sizeof(cl_uint), &mem_size);); (311, OpenCL, err = clEnqueueNDRangeKernel(queue, kernel, dimensions, NULL, globals, locals, 0, NULL, NULL);); (276, OpenCL, err = clSetKernelArg(kernel, 3, sizeof(cl_uint), &x_size);); (278, OpenCL, err = clSetKernelArg(kernel, 5, sizeof(cl_uint), &z_size);); (272, OpenCL, err = clSetKernelArg(kernel, 2, sizeof(cl_uint), &mem_size);); (273, OpenCL, err = clSetKernelArg(kernel, 4, sizeof(cl_uint), &y_size);); (270, OpenCL, err = clSetKernelArg(kernel, 1, sizeof(cl_uint), &num_of_tests);); (345, OpenCL, err = clEnqueueReleaseMemObject(queue, array);); (313, OpenCL, err = clEnqueueReadBuffer(queue, results, CL_TRUE, 0, sizeof(cl_uint), &num_of_failures, 0, NULL, NULL);); (264, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(array), &array);); (333, OpenCL, err = clEnqueueDestroyKernel(queue, kernel);); (264, OpenCL, err = clSetKernelArg(kernel, 3, sizeof(cl_uint), &x_size);); (267, OpenCL, err", "label": "(558, OpenCL, clReleaseKernel(kernel);); (558, OpenCL, clReleaseProgram(program);); (555, OpenCL, clReleaseMemObject(array);)"}
{"predict": "(44, CUDA, HANDLE_ERROR(cudaStreamSynchronize(s1));); (37, CUDA, HANDLE_ERROR(cudaMemcpyAsync(s_d, s_cpu.data(), sizeof(float) * DYN::STATE_DIM, cudaMemcpyHostToDevice, s1));); (54, CUDA, cudaFree(state_deriv_GPU);); (35, CUDA, HANDLE_ERROR(cudaMemcpyAsync(u_d, u.data(), sizeof(float) * DYN::CONTROL_DIM, cudaMemcpyHostToDevice, s1));); (50, CUDA, cudaFree(s_d);); (50, CUDA, cudaFree(u_d);); (27, CUDA, HANDLE_ERROR(cudaMalloc((void**)&state_deriv_GPU, sizeof(float) * DYN::STATE_DIM));); (30, CUDA, HANDLE_ERROR(cudaMemcpyAsync(s_d, s_cpu.data(), sizeof(float) * DYN::STATE_DIM, cudaMemcpyHostToDevice, s1));); (25, CUDA, HANDLE_ERROR(cudaMalloc((void**)&s_d, sizeof(float) * DYN::STATE_DIM));); (25, CUDA, HANDLE_ERROR(cudaMalloc((void**)&u_d, sizeof(float) * DYN::CONTROL_DIM));); (20, CUDA, HANDLE_ERROR(cudaStreamCreate(&s1));)", "label": "(34, CUDA, HANDLE_ERROR(cudaMalloc((void**)&state_deriv_GPU, sizeof(float) * DYN::STATE_DIM));); (37, CUDA, HANDLE_ERROR(cudaMemcpyAsync(s_d, s_gpu.data(), sizeof(float) * DYN::STATE_DIM, cudaMemcpyHostToDevice, s1));); (46, CUDA, CudaCheckError();); (34, CUDA, HANDLE_ERROR(cudaMemcpyAsync(u_d, u.data(), sizeof(float) * DYN::CONTROL_DIM, cudaMemcpyHostToDevice, s1));); (29, CUDA, HANDLE_ERROR(cudaMalloc((void**)&s_d, sizeof(float) * DYN::STATE_DIM));); (27, CUDA, HANDLE_ERROR(cudaMalloc((void**)&u_d, sizeof(float) * DYN::CONTROL_DIM));); (45, CUDA, HANDLE_ERROR(cudaMemcpyAsync(s_gpu.data(), s_d, sizeof(float) * DYN::STATE_DIM, cudaMemcpyDeviceToHost, s1));); (32, CUDA, HANDLE_ERROR(cudaStreamSynchronize(s1));); (19, CUDA, cudaStreamCreate(&s1);)"}
{"predict": "(66, CUDA, cudaStream_t stream = at::cuda::getCurrentCUDAStream();)", "label": "(69, CUDA, cudaStream_t stream = at::cuda::getCurrentCUDAStream();)"}
{"predict": "(24, MPI, MPI_Bcast( &berr, 1, MPI_FLOAT, 0, grid->comm );); (12, MPI, MPI_Allreduce( &s, &berr, 1, MPI_FLOAT, MPI_MAX, grid->comm );)", "label": "(24, MPI, MPI_Allreduce( &s, &berr, 1, MPI_FLOAT, MPI_MAX, grid->comm );)"}
{"predict": "(37, CUDA, cudaDeviceSynchronize();); (8, CUDA, float voxNum = (float)cx::ok(cudaGetDeviceCount());); (16, CUDA, cx::ok(cudaSetDevice(0));)", "label": "(34, CUDA, checkCudaErrors(cudaDeviceSynchronize());)"}
{"predict": "(18, CUDA, checkRuntime(cudaStreamCreate(&stream));); (76, CUDA, checkRuntime(cudaMallocHost(&output_data_host, sizeof(float) * output_numel));); (76, CUDA, checkRuntime(cudaMalloc(&output_data_device, sizeof(float) * output_numel));); (56, CUDA, checkRuntime(cudaMemcpyAsync(input_data_device, input_data_host, input_numel * sizeof(float), cudaMemcpyHostToDevice, stream));); (23, CUDA, checkRuntime(cudaMallocHost(&input_data_host, input_numel * sizeof(float)));); (23, CUDA, checkRuntime(cudaMalloc(&input_data_device, input_numel * sizeof(float)));); (75, CUDA, checkRuntime(cudaStreamSynchronize(stream));); (89, CUDA, checkRuntime(cudaFree(input_data_device));); (87, CUDA, checkRuntime(cudaFreeHost(input_data_host));); (89, CUDA, checkRuntime(cudaFree(output_data_device));); (86, CUDA, checkRuntime(cudaFreeHost(output_data_host));); (-5, CUDA, auto engine = make_nvshared(runtime->deserializeCudaEngine(engine_data.data(), engine_data.size()));); (80, CUDA, checkRuntime(cudaStreamDestroy(stream));); (66, CUDA, checkRuntime(cudaMemcpyAsync(output_data_host, output_data_device, sizeof(float) * output_numel, cudaMemcpyDeviceToHost, stream));); (62, CUDA, checkRuntime(cudaStreamSynchronize(stream));)", "label": "(18, CUDA, checkRuntime(cudaStreamCreate(&stream));); (79, CUDA, checkRuntime(cudaMemcpyAsync(output_data_host, output_data_device, sizeof(float) * output_numel, cudaMemcpyDeviceToHost, stream));); (79, CUDA, checkRuntime(cudaStreamSynchronize(stream));); (57, CUDA, checkRuntime(cudaMemcpyAsync(input_data_device, input_data_host, input_numel * sizeof(float), cudaMemcpyHostToDevice, stream));); (24, CUDA, checkRuntime(cudaMallocHost(&input_data_host, input_numel * sizeof(float)));); (24, CUDA, checkRuntime(cudaMalloc(&input_data_device, input_numel * sizeof(float)));); (89, CUDA, checkRuntime(cudaFree(input_data_device));); (86, CUDA, checkRuntime(cudaFreeHost(input_data_host));); (88, CUDA, checkRuntime(cudaFree(output_data_device));); (62, CUDA, checkRuntime(cudaMalloc(&output_data_device, sizeof(float) * output_numel));); (-5, CUDA, auto engine = make_nvshared(runtime->deserializeCudaEngine(engine_data.data(), engine_data.size()));); (59, CUDA, checkRuntime(cudaMallocHost(&output_data_host, sizeof(float) * output_numel));); (82, CUDA, checkRuntime(cudaFreeHost(output_data_host));); (79, CUDA, checkRuntime(cudaStreamDestroy(stream));)"}
{"predict": "(232, OpenMP, #pragma omp task default(firstprivate)); (75, OpenMP, #pragma omp parallel shared(sa)); (78, OpenMP, #pragma omp task default(none)); (224, OpenMP, #pragma omp task default(shared)); (143, OpenMP, #pragma omp task default(shared)); (154, OpenMP, #pragma omp task default(none)); (120, OpenMP, #pragma omp task default(none)); (230, OpenMP, #pragma omp task default(none)); (125, OpenMP, #pragma omp task default(none)); (133, OpenMP, #pragma omp task default(firstprivate)); (150, OpenMP, #pragma omp task default(none)); (8, OpenMP, #pragma omp task }); (213, OpenMP, #pragma omp task default(firstprivate)); (22, OpenMP, #pragma omp task unknown()); (105, OpenMP, #pragma omp parallel reduction(+ : r)); (234, OpenMP, #pragma omp task default(none)); (32, OpenMP, #pragma omp task default(none)); (50, OpenMP, #pragma omp parallel); (73, OpenMP, #pragma omp parallel shared(a, b)); (3, OpenMP, #pragma omp task]); (137, OpenMP, #pragma omp task default(none)); (33, OpenMP, #pragma omp task default(shared)); (65, OpenMP, #pragma omp task default(shared)); (36, OpenMP, #pragma omp task default(none)); (91, OpenMP, #pragma omp task private(r)); (116, OpenMP, #pragma omp task default(none)); (13, OpenMP, #pragma omp task untied); (75, OpenMP, #pragma omp task default(none)); (64, OpenMP, #pragma omp task default(firstprivate)); (77, OpenMP, #pragma omp parallel shared(a, b, r))", "label": "(174, OpenMP, #pragma omp task mergeable detach(evt)); (19, OpenMP, #pragma omp task unknown()); (14, OpenMP, #pragma omp task }); (154, OpenMP, #pragma omp task untied untied); (106, OpenMP, #pragma omp parallel shared(sa, sb)); (125, OpenMP, #pragma omp for reduction(+ : r)); (109, OpenMP, #pragma omp task firstprivate(r)); (160, OpenMP, #pragma omp task detach(); (-2, OpenMP, #pragma omp task); (104, OpenMP, #pragma omp parallel reduction(+ : r)); (160, OpenMP, #pragma omp task detach(evt) detach(evt)); (161, OpenMP, #pragma omp task detach(evt) mergeable); (60, OpenMP, #pragma omp parallel); (162, OpenMP, #pragma omp task detach(-evt)); (-6, OpenMP, #pragma omp task(); (48, OpenMP, #pragma omp task default(shared)); (-6, OpenMP, #pragma omp task[); (152, OpenMP, #pragma omp task detach(a)); (-6, OpenMP, #pragma omp task]); (58, OpenMP, #pragma omp parallel shared(a, b)); (157, OpenMP, #pragma omp task detach(evt) shared(evt)); (-7, OpenMP, #pragma omp task)); (144, OpenMP, #pragma omp task detach); (148, OpenMP, #pragma omp task detach(cevt) detach(revt)); (136, OpenMP, #pragma omp task mergeable mergeable); (153, OpenMP, #pragma omp task detach(evt) firstprivate(evt)); (19, OpenMP, #pragma omp task default(none)); (141, OpenMP, #pragma omp task detach()); (66, OpenMP, #pragma omp parallel shared(sa))"}
{"predict": "(11, MPI, MPI_Recv_init(buf, 10, MPI_INT, MPI_PROC_NULL, tag, MPI_COMM_WORLD, &r);); (7, MPI, MPI_Send_init(buf, 10, MPI_INT, MPI_PROC_NULL, tag, MPI_COMM_WORLD, &r);)", "label": "(8, MPI, MPI_Start(&r);); (11, MPI, MPI_Send_init(buf, 10, MPI_INT, MPI_PROC_NULL, tag, MPI_COMM_WORLD, &r);); (7, MPI, MPI_Wait(&r, &s);); (4, MPI, MPI_Recv_init(buf, 10, MPI_INT, MPI_PROC_NULL, tag, MPI_COMM_WORLD, &r);); (6, MPI, MPI_Request_free(&r);)"}
{"predict": "(16, CUDA, CUDA_CHECK(cudaGetLastError());)", "label": "(16, CUDA, CUDA_CHECK(cudaGetLastError());)"}
{"predict": "(3, MPI, MPI_Finalized(&isMpiFinalized);); (115, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (33, MPI, MPI_Get_library_version(mpiVersion, &resultLen);); (113, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (111, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)", "label": "(35, MPI, MPI_Get_library_version(mpiVersion, &resultLen);); (132, MPI, ret = MPI_Init_thread(argc, argv, MPI_THREAD_MULTIPLE, &provided);); (146, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (126, MPI, MPI_Initialized(&isMpiInited);); (0, MPI, MPI_Finalized(&isMpiFinalized);)"}
{"predict": "(2, OpenMP, #pragma omp target uses_allocators(omp_null_allocator); (13, OpenMP, #pragma omp target uses_allocators(omp_cgroup_mem_alloc) allocate(omp_cgroup_mem_alloc: a)); (8, OpenMP, #pragma omp target uses_allocators(omp_low_lat_mem_alloc) allocate(omp_low_lat_mem_alloc: a)); (6, OpenMP, #pragma omp target uses_allocators(omp_cgroup_mem_alloc)); (3, OpenMP, #pragma omp target uses_allocators(omp_large_cap_mem_alloc) allocate(omp_large_cap_mem_alloc: a)); (9, OpenMP, #pragma omp target uses_allocators(omp_pteam_mem_alloc) allocate(omp_pteam_mem_alloc: a)); (-2, OpenMP, #pragma omp target uses_allocators(omp_default_mem_alloc); (5, OpenMP, #pragma omp target uses_allocators(omp_pteam_mem_alloc)); (0, OpenMP, #pragma omp target uses_allocators(omp_large_cap_mem_alloc)); (-3, OpenMP, #pragma omp target uses_allocators(omp_default_mem_alloc) allocate(omp_default_mem_alloc: a)); (-5, OpenMP, #pragma omp target uses_allocators(omp_null_allocator) allocate(omp_null_allocator: a)); (3, OpenMP, #pragma omp target uses_allocators(omp_low_lat_mem_alloc))", "label": "(14, OpenMP, #pragma omp parallel reduction(+ : incomplete, ([10])p)); (1, OpenMP, #pragma omp parallel reduction(); (2, OpenMP, #pragma omp parallel reduction(unknown); (9, OpenMP, #pragma omp parallel reduction(inscan, + : a)); (6, OpenMP, #pragma omp parallel reduction(default, + : a)); (1, OpenMP, #pragma omp parallel reduction(default,); (2, OpenMP, #pragma omp parallel reduction(unknown, +: a))"}
{"predict": "(5, OpenMP, int _dev_thread_id = omp_get_thread_num();); (5, OpenMP, int _dev_thread_num = omp_get_num_threads();)", "label": "(13, OpenMP, int _dev_thread_id = omp_get_thread_num();); (11, OpenMP, int _dev_thread_num = omp_get_num_threads();)"}
{"predict": "(14, CUDA, cudaError_t err = cudaMalloc(&m_histogram, sizeof(int) * 256 * maxBatchSize);)", "label": "(14, CUDA, cudaError_t err = cudaMalloc(&m_histogram, sizeof(int) * 256 * maxBatchSize);)"}
{"predict": "(22, OpenMP, #pragma omp target teams distribute parallel for simd if (target: 0)); (69, OpenMP, #pragma omp target teams distribute parallel for simd if(simd :); (13, OpenMP, #pragma omp target teams distribute parallel for simd if (argc))); (16, OpenMP, #pragma omp target teams distribute parallel for simd if (argc > 0? argv[1] : argv[2])); (34, OpenMP, #pragma omp target teams distribute parallel for simd if(parallel :); (50, OpenMP, #pragma omp target teams distribute parallel for simd if(parallel : argc) if (for:argc)); (44, OpenMP, #pragma omp target teams distribute parallel for simd if(parallel : argc) if (argc)); (35, OpenMP, #pragma omp target teams distribute parallel for simd if(parallel : argc); (54, OpenMP, #pragma omp target teams distribute parallel for simd if(parallel : argc) if (parallel:argc)); (59, OpenMP, #pragma omp target teams distribute parallel for simd if(parallel : argc) if (parallel:argc) if(simd : argc)); (3, OpenMP, #pragma omp target teams distribute parallel for simd if (argc))); (24, OpenMP, #pragma omp target teams distribute parallel for simd if(target : argc)); (28, OpenMP, #pragma omp target teams distribute parallel for simd if(target : argc) if (argc)); (1, OpenMP, #pragma omp target teams distribute parallel for simd if (argc))); (46, OpenMP, #pragma omp target teams distribute parallel for simd if(parallel : argc) if (parallel:argc) if(simd : argc) if (target:argc)); (63, OpenMP, #pragma omp target teams distribute parallel for simd if(parallel : argc) if (parallel:argc) if(simd : argc) if (argc)); (68, OpenMP, #pragma omp target teams distribute parallel for simd if(parallel : argc) if (parallel:argc) if(simd : argc) if (argc) if (target:argc)); (54, OpenMP, #pragma omp target teams distribute parallel for simd if(parallel : argc) if (parallel:argc) if(simd", "label": "(137, OpenMP, #pragma omp for schedule(nonmonotonic: auto)); (3, OpenMP, #pragma omp for); (47, OpenMP, #pragma omp for schedule(guided)); (106, OpenMP, #pragma omp for schedule(monotonic: dynamic)); (41, OpenMP, #pragma omp for simd schedule(runtime)); (56, OpenMP, #pragma omp for simd schedule(dynamic)); (107, OpenMP, #pragma omp for simd schedule(monotonic: dynamic)); (46, OpenMP, #pragma omp for simd schedule(guided)); (89, OpenMP, #pragma omp for simd schedule(monotonic: runtime)); (10, OpenMP, #pragma omp for simd schedule(static)); (83, OpenMP, #pragma omp for schedule(monotonic: runtime)); (90, OpenMP, #pragma omp for schedule(monotonic: guided)); (144, OpenMP, #pragma omp for schedule(static, 2) ordered(1)); (28, OpenMP, #pragma omp for schedule(runtime)); (62, OpenMP, #pragma omp for schedule(monotonic: static, 2)); (-1, OpenMP, #pragma omp for schedule(static)); (21, OpenMP, #pragma omp for simd schedule(auto)); (144, OpenMP, #pragma omp for simd schedule(static, 2) ordered); (152, OpenMP, #pragma omp for simd schedule(auto) ordered); (66, OpenMP, #pragma omp for schedule(monotonic: auto)); (105, OpenMP, #pragma omp for schedule(nonmonotonic: dynamic)); (145, OpenMP, #pragma omp for schedule(auto) ordered(1)); (67, OpenMP, #pragma omp for simd schedule(monotonic: auto)); (110, OpenMP, #pragma omp for simd schedule(nonmonotonic: static)); (57, OpenMP, #pragma omp for simd schedule(monotonic: static, 2)); (46, OpenMP, #pragma omp for simd schedule(monotonic: static)); (-2, OpenMP, #pragma omp for schedule(static, 2)); (39, OpenMP, #pragma omp for schedule(monotonic: static)); (113, OpenMP, #pragma omp for simd schedule(nonmonotonic: runtime)); (28, OpenMP, #pragma omp for schedule(dynamic)); (87, OpenMP, #pragma omp for schedule(nonmonotonic: guided)); (149, OpenMP, #pragma omp for simd schedule(runtime) ordered); (97, OpenMP, #pragma omp for simd schedule(nonmonotonic: dynamic)); (157, OpenMP, #pragma omp for simd schedule(guided) ordered); (-25, OpenMP, #pragma omp for simd); (111, OpenMP, #pragma omp for schedule(static) ordered); (69, OpenMP, #pragma omp for simd schedule(monotonic: guided)); (-4, OpenMP, #pragma omp for schedule(auto)); (83, OpenMP, #pragma omp for simd schedule(nonmonotonic: guided)); (156, OpenMP, #pragma omp for schedule(dynamic) ordered(1)); (111, OpenMP, #pragma omp for simd schedule(static) ordered); (144, OpenMP, #pragma omp for schedule(guided) ordered(1)); (-13, OpenMP, #pragma omp for simd schedule(static, 2)); (132, OpenMP, #pragma omp for schedule(runtime) ordered)"}
{"predict": "(52, OpenCL, clReleaseEvent(event);)", "label": "(51, OpenCL, clReleaseEvent(event);)"}
{"predict": "(96, OpenMP, #pragma omp target exit data map(from: u[0:grid_points[0]][0:grid_points[1]][0:grid_points[2]][0:5]) map(delete: u_temp[0:grid_points[0]][0:grid_points[1]][0:grid_points[2]][0:5])); (92, OpenMP, #pragma omp target update from(u[0:grid_points[0]][0:grid_points[1]][0:grid_points[2]][0:5])); (140, OpenMP, #pragma omp target update to(u[0:grid_points[0]][0:grid_points[1]][0:grid_points[2]][0:5])); (87, OpenMP, #pragma omp target enter data map(to: u[0:grid_points[0]][0:grid_points[1]][0:grid_points[2]][0:5])); (87, OpenMP, #pragma omp target map(alloc: u_temp[0:grid_points[0]][0:grid_points[1]][0:grid_points[2]][0:5]))", "label": "(171, OpenMP, #pragma omp for private(j,k,vijk,vp1,vm1)); (397, OpenMP, #pragma omp for private(j,m)); (7, OpenMP, #pragma omp for nowait private(j,k,rho_inv)); (127, OpenMP, #pragma omp for nowait private(j,k,m)); (103, OpenMP, #pragma omp for nowait private(k,m)); (343, OpenMP, #pragma omp for nowait private(j,m)); (151, OpenMP, #pragma omp for private(k,m)); (25, OpenMP, #pragma omp for private(j,k,m)); (38, OpenMP, #pragma omp for private(j,k,uijk,up1,um1)); (282, OpenMP, #pragma omp for private(j,k,wijk,wp1,wm1)); (399, OpenMP, #pragma omp for private(k,m,i))"}
{"predict": "(11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (8, MPI, MPI_Init(&argc, &argv);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (11, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(28, MPI, MPI_Type_free(&ddt);); (7, MPI, MPI_Init(&argc, &argv);); (12, MPI, MPI_Finalize();); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (21, MPI, MPI_DDT_DUMP(ddt);)"}
{"predict": "(58, MPI, MPI_Sendrecv(NULL, 0, MPI_BYTE, partner, tag, NULL, 0, MPI_BYTE, partner, tag, comm, &status);); (37, MPI, MPI_Comm_size(comm, &np);); (47, MPI, MPI_Send(outbufs[j], counts[j], types[j], partner, tag, comm);); (26, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (46, MPI, MPI_Recv(&count, 1, MPI_INT, partner, tag, comm, &status);); (23, MPI, MPI_Comm_get_name(comm, myname, &mynamelen);); (42, MPI, MPI_Send(inbufs[j], counts[j], types[j], partner, tag, comm);); (59, MPI, MPI_Comm_get_errhandler(comm, &errhandler);); (60, MPI, MPI_Error_string(errhandler, &obuf, &bytesize);); (33, MPI, MPI_Comm_rank(comm, &rank);)", "label": "(57, MPI, MPI_Recv(outbufs[j], counts[j], types[j], partner, tag, comm, &status);); (45, MPI, MPI_Type_get_name(types[j], myname, &mynamelen);); (49, MPI, MPI_Send(inbufs[j], counts[j], types[j], partner, tag, comm);); (35, MPI, MPI_Comm_size(comm, &np);); (25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (32, MPI, MPI_Comm_rank(comm, &rank);); (53, MPI, MPI_Get_count(&status, types[j], &count);)"}
{"predict": "(16, MPI, MPI_Init(&argc, &argv);); (22, MPI, MPI_Finalize();)", "label": "(16, MPI, MPI_Init(&argc, &argv);); (20, MPI, MPI_Finalize();)"}
{"predict": "(14, MPI, MPI_Init(&argc, &argv);); (103, MPI, MPI_Finalize();); (72, MPI, t0 = MPI_Wtime();); (73, MPI, t1 = MPI_Wtime();)", "label": "(14, MPI, MPI_Init(&argc, &argv);); (148, MPI, MPI_Finalize();); (72, MPI, t0 = MPI_Wtime();); (73, MPI, t1 = MPI_Wtime();)"}
{"predict": "(12, MPI, MPI_Abort(MPI_COMM_WORLD, MPIT_result);); (7, MPI, MPIT_result = MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);); (10, MPI, MPIT_result = MPI_T_pvar_session_create(session);)", "label": "(11, MPI, MPI_Abort(MPI_COMM_WORLD, MPIT_result);); (12, MPI, MPIT_result = MPI_T_pvar_session_create(session);); (6, MPI, MPIT_result = MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);)"}
{"predict": "(6, CUDA, __shared__ float block_acc[32];)", "label": "(6, CUDA, __shared__ float block_acc[32];)"}
{"predict": "(3, OpenMP, auto maxThreads = omp_get_max_threads();)", "label": "(3, OpenMP, auto maxThreads = omp_get_max_threads();)"}
{"predict": "(18, MPI, MPI_Finalize();); (10, MPI, MPI_Comm_get_parent(&intercomm);); (4, MPI, MPI_Init(NULL, NULL);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (9, MPI, MPI_Comm_disconnect(&intercomm);); (10, MPI, err = MPI_Recv(str, 3, MPI_CHAR, 0, 0, intercomm, MPI_STATUS_IGNORE);); (13, MPI, MPI_Send(\"bye\", 4, MPI_CHAR, 0, 0, intercomm);)", "label": "(6, MPI, MPI_Init(&argc, &argv);); (19, MPI, MPI_Finalize();); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (7, MPI, MPI_Comm_get_parent(&intercomm);); (11, MPI, err = MPI_Recv(str, 4, MPI_CHAR, 3, 0, intercomm, MPI_STATUS_IGNORE);); (8, MPI, err = MPI_Send(\"hi\", 3, MPI_CHAR, 3, 0, intercomm);)"}
{"predict": "(30, CUDA, cudaDeviceSynchronize();)", "label": "(31, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (29, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());)"}
{"predict": "(11, OpenMP, #pragma omp target update from(argc, z)); (4, OpenMP, #pragma omp target update from); (7, OpenMP, #pragma omp target update from (S1)); (0, OpenMP, #pragma omp target update from (); (1, OpenMP, #pragma omp target update from (argc); (5, OpenMP, #pragma omp target update from (argv[1])); (7, OpenMP, #pragma omp target update from(mapptr(i))); (0, OpenMP, #pragma omp target update from (argc > 0? argv[1] : argv[2])); (5, OpenMP, #pragma omp target update from(i)); (-4, OpenMP, #pragma omp target update from ()); (4, OpenMP, #pragma omp target update from(mapptr(mapptr(i))); (-6, OpenMP, #pragma omp target update from (argc); (0, OpenMP, #pragma omp target update from (argv[0])); (-10, OpenMP, #pragma omp target update from)", "label": "(5, OpenMP, #pragma omp task final (argc); (3, OpenMP, #pragma omp task final ()); (8, OpenMP, #pragma omp task final (argv[1]=2)); (4, OpenMP, #pragma omp task final (argc > 0? argv[1] : argv[2])); (8, OpenMP, #pragma omp task final (1 0)); (1, OpenMP, #pragma omp task final (z+argc))); (2, OpenMP, #pragma omp task final (foobool(argc)), final (true)); (6, OpenMP, #pragma omp task final(if(tmain(argc, argv)); (-5, OpenMP, #pragma omp task final (); (2, OpenMP, #pragma omp task final (argc argc)); (-8, OpenMP, #pragma omp task final); (-2, OpenMP, #pragma omp task final (S1))"}
{"predict": "(55, MPI, MPI_Allreduce(MPI_IN_PLACE,&n,1,mpi_int_t,MPI_MAX,grid->comm);); (48, MPI, MPI_Allreduce(MPI_IN_PLACE,&nnzL,1,mpi_int_t,MPI_SUM,grid->comm);)", "label": "(49, MPI, MPI_Allreduce(MPI_IN_PLACE,&n,1,mpi_int_t,MPI_MAX,grid->comm);); (47, MPI, MPI_Allreduce(MPI_IN_PLACE,&nnzL,1,mpi_int_t,MPI_SUM,grid->comm);)"}
{"predict": "(134, OpenCL, err |= clEnqueueUnmapMemObject(queue, dest, dest_host.data(), 0, NULL, NULL);); (107, OpenCL, err = clEnqueueNDRangeKernel(queue, kernels[vecsize], 1, NULL, &global_work_size, &local_work_size, 3, &src1, &src2, &cmp, &dest);); (100, OpenCL, err |= clSetKernelArg(kernels[vecsize], 0, sizeof(cl_mem), &src1);); (130, OpenCL, err |= clEnqueueUnmapMemObject(queue, src1, src1_host.data(), 0, NULL, NULL);); (41, OpenCL, src2 = clCreateBuffer(context, CL_MEM_READ_WRITE, BUFFER_SIZE, NULL, &err);); (43, OpenCL, dest = clCreateBuffer(context, CL_MEM_READ_WRITE, BUFFER_SIZE, NULL, &err);); (100, OpenCL, err |= clSetKernelArg(kernels[vecsize], 2, sizeof(cl_mem), &cmp);); (38, OpenCL, cmp = clCreateBuffer(context, CL_MEM_READ_WRITE, BUFFER_SIZE, NULL, &err);)", "label": "(51, OpenCL, dest = clCreateBuffer( context, CL_MEM_WRITE_ONLY, BUFFER_SIZE, NULL, &err );); (44, OpenCL, src1 = clCreateBuffer( context, CL_MEM_READ_ONLY, BUFFER_SIZE, NULL, &err );); (72, OpenCL, err = clSetKernelArg(kernels[vecsize], 0, sizeof dest, &dest);); (44, OpenCL, src2 = clCreateBuffer( context, CL_MEM_READ_ONLY, BUFFER_SIZE, NULL, &err );); (76, OpenCL, err = clSetKernelArg(kernels[vecsize], 3, sizeof cmp, &cmp);); (71, OpenCL, err = clSetKernelArg(kernels[vecsize], 1, sizeof src1, &src1);); (134, OpenCL, err = clEnqueueNDRangeKernel(queue, kernels[vecsize], 1, NULL, &vector_count, NULL, 0, NULL, NULL);); (42, OpenCL, cmp = clCreateBuffer( context, CL_MEM_READ_ONLY, BUFFER_SIZE, NULL, &err );); (70, OpenCL, err = clSetKernelArg(kernels[vecsize], 2, sizeof src2, &src2);)"}
{"predict": "(15, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (19, MPI, MPI_Type_free(&eq_type);); (17, MPI, MPI_Type_free(&op);); (13, MPI, MPI_Op_create((MPI_User_function *) string_int_compare, 0, &op);); (10, MPI, MPI_Type_commit(&eq_type);); (11, MPI, MPI_Type_create_struct(2, blklens, displs, types, &eq_type);); (13, MPI, MPI_Allreduce(&data, &result, 1, eq_type, op, MPI_COMM_WORLD);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);)", "label": "(17, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (21, MPI, MPI_Reduce(&data, &result, 1, eq_type, op, 0, MPI_COMM_WORLD);); (25, MPI, MPI_Op_free(&op);); (10, MPI, MPI_Type_create_struct(2, blklens, displs, types, &eq_type);); (10, MPI, MPI_Type_commit(&eq_type);); (3, MPI, MPI_Op_create(myop, 0, &op);); (10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (21, MPI, MPI_Type_free(&eq_type);)"}
{"predict": "(4, OpenMP, #pragma omp target)", "label": "(7, OpenMP, #pragma omp critical(add_first)); (3, OpenMP, #pragma omp parallel shared(var1, var2)); (7, OpenMP, #pragma omp atomic); (8, OpenMP, #pragma omp single)"}
{"predict": "(13, TBB, test_extract<tbb::flow::rejecting>();); (13, TBB, test_extract<tbb::flow::queueing>();)", "label": "(9, TBB, test_ports_return_references<tbb::flow::rejecting>();); (9, TBB, lightweight_testing::test<tbb::flow::multifunction_node>(10);); (13, TBB, test_extract<tbb::flow::rejecting>();); (5, TBB, test_ports_return_references<tbb::flow::queueing>();); (12, TBB, test_extract<tbb::flow::queueing>();)"}
{"predict": "(86, MPI, mpirc = MPI_File_read_at(fh, pos, buf, 1, MPI_BYTE, &status);); (59, MPI, MPI_Error_string(mpirc, mpierrstr, &mpierrlen);); (23, MPI, MPI_Bcast(&filesize, 1, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (42, MPI, MPI_Exscan(&chunk_count, &chunk_offset, 1, MPI_UINT64_T, MPI_SUM, MPI_COMM_WORLD);); (55, MPI, mpirc = MPI_File_set_view(fh, disp, MPI_BYTE, MPI_INFO_NULL);); (115, MPI, mpirc = MPI_File_read_at(fh, read_offset, bufstart, read_count, MPI_BYTE, &status);); (82, MPI, MPI_Bcast(&scan, 1, MPI_INT, 0, MPI_COMM_WORLD);); (132, MPI, mpirc = MPI_File_read_at(fh, read_offset, buf, 1, MPI_BYTE, &status);); (141, MPI, mpirc = MPI_File_set_view(fh, disp, MPI_BYTE, MPI_INFO_NULL);); (36, MPI, MPI_Bcast(&chunk_count, 1, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (137, MPI, MPI_Bcast(&filesize, 1, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (4, MPI, MPI_Barrier(MPI_COMM_WORLD);); (134, MPI, MPI_Bcast(&ranks, 1, MPI_INT, 0, MPI_COMM_WORLD);); (127, MPI, MPI_Bcast(&rank, 1, MPI_INT, 0, MPI_COMM_WORLD);); (36, MPI, MPI_Exscan(&chunk_count,", "label": "(64, MPI, MPI_Error_string(mpirc, mpierrstr, &mpierrlen);); (45, MPI, MPI_Exscan(&chunk_count, &chunk_offset, 1, MPI_UINT64_T, MPI_SUM, MPI_COMM_WORLD);); (88, MPI, mpirc = MPI_File_read_at(fh, pos, buf, 1, MPI_CHAR, &status);); (120, MPI, mpirc = MPI_File_read_at(fh, read_offset, bufstart, read_count, MPI_CHAR, &status);); (58, MPI, int mpirc = MPI_File_set_view(fh, disp, MPI_CHAR, MPI_CHAR, datarep, MPI_INFO_NULL);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (19, MPI, MPI_Bcast(&filesize, 1, MPI_UINT64_T, 0, MPI_COMM_WORLD);)"}
{"predict": "(1, CUDA, __shared__ float s_mem[32];)", "label": "(1, CUDA, __shared__ float s_mem[32];)"}
{"predict": "(23, MPI, rc = MPI_Type_free(&type);); (12, MPI, rc = MPI_Type_get_extent(type, &tmp_lb, &attr->child_extent);); (14, MPI, rc = MPI_Type_dup(type, newtype);)", "label": "(15, MPI, rc = MPI_Type_dup(type, newtype);); (11, MPI, rc = MPI_Type_get_extent(type, &tmp_lb, &attr->child_type_extent);); (19, MPI, rc = MPI_Type_free(&type);)"}
{"predict": "(60, OpenMP, #pragma omp parallel for reduction(+ : ret_sum))", "label": "(60, OpenMP, #pragma omp parallel for reduction(+ : ret_sum))"}
{"predict": "(1, TBB, tbb::task_scheduler_init init( tbb::task_scheduler_init::deferred );); (18, TBB, t.set_affinity( *new( tbb::affinity::allocate_root() ) tbb::simple_partitioner() );); (13, TBB, tbb::task::spawn( t );)", "label": "(3, TBB, tbb::task* dummy_root = new( tbb::task::allocate_root() ) tbb::empty_task;); (22, TBB, tbb::task::destroy( *dummy_root );); (13, TBB, tbb::task::spawn_root_and_wait(t);); (16, TBB, tbb::task::enqueue(t);)"}
{"predict": "(33, OpenMP, #pragma omp target update from(z[:10])); (17, OpenMP, #pragma omp target update to(z[:10])); (18, OpenMP, #pragma omp target map(to:k[:5]) map(always, tofrom:x) map(tofrom:y))", "label": "(28, OpenMP, #pragma omp target update from(z[:10])); (17, OpenMP, #pragma omp target update to(z[:10])); (18, OpenMP, #pragma omp target map(to:k[:5]) map(always, tofrom:x) map(tofrom:y))"}
{"predict": "(31, OpenMP, #pragma omp target update from(z[:10])); (17, OpenMP, #pragma omp target map(to:k[:5]) map(always, tofrom:x) map(tofrom:y))", "label": "(28, OpenMP, #pragma omp target update from(z[:10])); (17, OpenMP, #pragma omp target update to(z[:10])); (18, OpenMP, #pragma omp target map(to:k[:5]) map(always, tofrom:x) map(tofrom:y))"}
{"predict": "(1, OpenMP, #pragma omp target map(tofrom:nest))", "label": "(18, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(3, OpenMP, #pragma omp target); (4, OpenMP, #pragma omp teams distribute)", "label": "(4, OpenMP, #pragma omp teams distribute); (2, OpenMP, #pragma omp target)"}
{"predict": "(41, CUDA, return cudaGetLastError();)", "label": "(41, CUDA, return cudaGetLastError();)"}
{"predict": "(21, CUDA, extern __shared__ scalar_t alpha_buffer_sh[];)", "label": "(14, CUDA, extern __shared__ float alpha_buffer_sh[];)"}
{"predict": "(65, MPI, rc = MPI_Type_free(&type);)", "label": "(65, MPI, rc = MPI_Type_free(&type);)"}
{"predict": "(9, MPI, MPI_Init(&argc, &argv);); (35, MPI, MPI_Cart_coords(comm2d, irank, 2, coords);); (38, MPI, MPI_Neighbor_allgather(MPI_IN_PLACE, 0, MPI_DATATYPE_NULL, neighbors, 4, MPI_INT, comm2d);); (47, MPI, MPI_Comm_free(&comm2d);); (31, MPI, MPI_Cart_create(MPI_COMM_WORLD, 2, dims, period, 0, &comm2d);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (32, MPI, MPI_Comm_rank(comm2d, &crank);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (43, MPI, MPI_Finalize();); (24, MPI, MPI_Dims_create(ntasks, 2, dims);); (4, MPI, MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);)", "label": "(37, MPI, MPI_Cart_shift(comm2d, 0, 1, &neighbors[0], &neighbors[1]);); (8, MPI, MPI_Init(&argc, &argv);); (26, MPI, MPI_Finalize();); (38, MPI, MPI_Cart_coords(comm2d, crank, 2, coords);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (28, MPI, MPI_Cart_create(MPI_COMM_WORLD, 2, dims, period, 1, &comm2d);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (42, MPI, MPI_Barrier(comm2d);); (26, MPI, MPI_Comm_rank(comm2d, &crank);); (29, MPI, MPI_Cart_shift(comm2d, 1, 1, &neighbors[2], &neighbors[3]);)"}
{"predict": "(21, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize(), \"KaKjw_to_KKwaj\");); (19, CUDA, qmc_cuda::cuda_check(cudaGetLastError(), \"KaKjw_to_KKwaj\");)", "label": "(21, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize(), \"KaKjw_to_KKwaj\");); (19, CUDA, qmc_cuda::cuda_check(cudaGetLastError(), \"KaKjw_to_KKwaj\");)"}
{"predict": "(6, OpenMP, #pragma omp target update to(a) hint(AMD_fast_fp_atomics)); (10, OpenMP, #pragma omp target update from(a) hint(AMD_fast_fp_atomics)); (6, OpenMP, #pragma omp target update to(a) hint(AMD_fast_fp_atomics)); (1, OpenMP, #pragma omp target update to(a) hint()); (4, OpenMP, #pragma omp target update to(a) hint(AMD_fast_fp_atomics)); (8, OpenMP, #pragma omp target update from(a) hint(AMD_fast_fp_atomics)); (-1, OpenMP, #pragma omp target update to(a) hint(); (0, OpenMP, #pragma omp target update to(a) hint(AMD_fast_fp_atomics)); (5, OpenMP, #pragma omp target update from(a) hint(AMD_fast_fp_atomics))", "label": "(12, OpenMP, #pragma omp atomic hint(1) hint(1)); (7, OpenMP, #pragma omp atomic hint(a); (4, OpenMP, #pragma omp atomic hint(+); (1, OpenMP, #pragma omp atomic hint(); (6, OpenMP, #pragma omp atomic hint(a)); (-3, OpenMP, #pragma omp atomic hint)"}
{"predict": "(62, CUDA, HANDLE_CUDA_ERROR(cudaMallocHost((void**) &C, sizeC));); (202, CUDA, if (C_d) cudaFree(C_d);); (52, CUDA, HANDLE_CUDA_ERROR(cudaMalloc((void**) &C_d, sizeC));); (58, CUDA, HANDLE_CUDA_ERROR(cudaMallocHost((void**) &A, sizeA));); (200, CUDA, if (work) cudaFree(work);); (45, CUDA, HANDLE_CUDA_ERROR(cudaMalloc((void**) &A_d, sizeA));); (201, CUDA, if (A) cudaFreeHost(A);); (202, CUDA, if (C) cudaFreeHost(C);); (180, CUDA, HANDLE_CUDA_ERROR(cudaStreamSynchronize(stream));); (190, CUDA, HANDLE_CUDA_ERROR(cudaStreamDestroy(stream));); (195, CUDA, if (A_d) cudaFree(A_d);)", "label": "(63, CUDA, HANDLE_CUDA_ERROR(cudaMallocHost((void**) &C, sizeof(floatTypeC) * elementsC));); (78, CUDA, HANDLE_CUDA_ERROR(cudaMemcpy2DAsync(A_d, sizeA, A, sizeA, sizeA, 1, cudaMemcpyDefault, nullptr));); (192, CUDA, if (C_d) cudaFree(C_d);); (188, CUDA, if (A) cudaFreeHost(A);); (51, CUDA, HANDLE_CUDA_ERROR(cudaMalloc((void**) &C_d, sizeC));); (49, CUDA, HANDLE_CUDA_ERROR(cudaMalloc((void**) &A_d, sizeA));); (187, CUDA, if (A_d) cudaFree(A_d);); (185, CUDA, if (C) cudaFreeHost(C);); (54, CUDA, HANDLE_CUDA_ERROR(cudaMallocHost((void**) &A, sizeof(floatTypeA) * elementsA));)"}
{"predict": "(18, TBB, tbb::spin_mutex::scoped_lock lock(*rMutex);)", "label": "(18, TBB, tbb::spin_mutex::scoped_lock lock(*rMutex);)"}
{"predict": "(20, CUDA, checkCudaErrors(cudaMalloc((void **)&d_RanksA, N * sizeof(uint)));); (32, CUDA, checkCudaErrors(cudaFree(d_LimitsA));); (21, CUDA, checkCudaErrors(cudaMemset(d_RanksA, 0, N * sizeof(uint)));); (22, CUDA, checkCudaErrors(cudaMalloc((void **)&d_LimitsB, N * sizeof(uint)));); (20, CUDA, checkCudaErrors(cudaMalloc((void **)&d_LimitsA, N * sizeof(uint)));); (18, CUDA, checkCudaErrors(cudaMalloc((void **)&d_RanksB, N * sizeof(uint)));); (20, CUDA, checkCudaErrors(cudaMemset(d_RanksB, 0, N * sizeof(uint)));); (24, CUDA, checkCudaErrors(cudaMalloc((void **)&d_SampleCount, sizeof(uint)));); (30, CUDA, checkCudaErrors(cudaFree(d_RanksA));); (31, CUDA, checkCudaErrors(cudaFree(d_RanksB));); (30, CUDA, checkCudaErrors(cudaFree(d_SampleCount));); (25, CUDA, checkCudaErrors(cudaMalloc((void **)&d_ExscanTemp, N * sizeof(uint)));); (27, CUDA, checkCudaErrors(cudaMemset(d_ExscanTemp, 0, N * sizeof(uint)));); (20, CUDA, checkCudaErrors(cudaMemset(d_LimitsB, 0, N * sizeof(uint)));)", "label": "(21, CUDA, checkCudaErrors(cudaMalloc((void **)&d_LimitsA, MAX_SAMPLE_COUNT * sizeof(uint)));); (19, CUDA, checkCudaErrors(cudaMalloc((void **)&d_RanksB,  MAX_SAMPLE_COUNT * sizeof(uint)));); (17, CUDA, checkCudaErrors(cudaMalloc((void **)&d_RanksA,  MAX_SAMPLE_COUNT * sizeof(uint)));); (19, CUDA, checkCudaErrors(cudaMalloc((void **)&d_LimitsB, MAX_SAMPLE_COUNT * sizeof(uint)));)"}
{"predict": "(100, OpenMP, #pragma omp target exit data map(from:grid[0:n*n])); (77, OpenMP, #pragma omp target teams distribute parallel for collapse(2)); (98, OpenMP, #pragma omp target update from(grid[0:n*n])); (69, OpenMP, #pragma omp target enter data map(to:grid[0:n*n]))", "label": "(79, OpenACC, #pragma acc parallel loop independent); (70, OpenACC, #pragma acc data pcopy(grid[0:n*n])); (86, OpenACC, #pragma acc parallel loop gang); (99, OpenACC, #pragma acc kernels); (89, OpenACC, #pragma acc loop vector)"}
{"predict": "(6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &world_size);)", "label": "(6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &world_size);)"}
{"predict": "(7, MPI, MPI_Init(&argc, &argv);); (24, MPI, MPI_Finalize();); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(25, MPI, MPI_Finalize();); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Init(NULL, NULL);)"}
{"predict": "(126, MPI, MPI_Comm_group( MPI_COMM_WORLD, &world_group );); (144, MPI, MPI_Comm_rank( comm_col, &myrow );); (203, MPI, MPI_Barrier(MPI_COMM_WORLD);); (86, MPI, MPI_Bcast(&inner_block_flag, 1, MPI_INT, root, MPI_COMM_WORLD);); (135, MPI, MPI_Group_incl( world_group, nprow, ranks, &temp_group );); (38, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &my_ID );); (78, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (123, MPI, MPI_Group_incl( world_group, npcol, ranks, &temp_group );); (33, MPI, MPI_Init(&argc,&argv);); (129, MPI, MPI_Comm_create( MPI_COMM_WORLD, temp_group, &comm_col );); (32, MPI, MPI_Comm_size( MPI_COMM_WORLD, &Num_procs );); (119, MPI, MPI_Comm_create( MPI_COMM_WORLD, temp_group, &comm_row );); (241, MPI, MPI_Finalize();); (68, MPI, MPI_Bcast(&order,  1, MPI_INT, root, MPI_COMM_WORLD);); (162, MPI, MPI_Allgather( &mynrows, 1, MPI_INT, mm, 1, MPI_INT, comm_col );); (68, MPI, MPI_Bcast(&nb, 1, MPI_INT, root, MPI_COMM_WORLD);); (122, MPI, MPI_Comm_rank( comm_row, &mycol );); (167, MPI, MPI_Allgather( &myncols, 1, MPI_INT, nn, 1, MPI_INT, comm_row );)", "label": "(124, MPI, MPI_Comm_group( MPI_COMM_WORLD, &world_group );); (143, MPI, MPI_Comm_rank( comm_col, &myrow );); (205, MPI, MPI_Barrier(MPI_COMM_WORLD);); (83, MPI, MPI_Bcast(&inner_block_flag, 1, MPI_INT, root, MPI_COMM_WORLD);); (135, MPI, MPI_Group_incl( world_group, nprow, ranks, &temp_group );); (39, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &my_ID );); (78, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (124, MPI, MPI_Group_incl( world_group, npcol, ranks, &temp_group );); (35, MPI, MPI_Init(&argc,&argv);); (131, MPI, MPI_Comm_create( MPI_COMM_WORLD, temp_group, &comm_col );); (35, MPI, MPI_Comm_size( MPI_COMM_WORLD, &Num_procs );); (121, MPI, MPI_Comm_create( MPI_COMM_WORLD, temp_group, &comm_row );); (245, MPI, MPI_Finalize();); (70, MPI, MPI_Bcast(&order,  1, MPI_INT, root, MPI_COMM_WORLD);); (167, MPI, MPI_Allgather( &mynrows, 1, MPI_INT, mm, 1, MPI_INT, comm_col );); (70, MPI, MPI_Bcast(&nb, 1, MPI_INT, root, MPI_COMM_WORLD);); (127, MPI, MPI_Comm_rank( comm_row, &mycol );); (172, MPI, MPI_Allgather( &myncols, 1, MPI_INT, nn, 1, MPI_INT, comm_row );)"}
{"predict": "(10, OpenMP, #pragma omp target parallel if (Arg)); (3, OpenMP, #pragma omp target parallel if (true)); (5, OpenMP, #pragma omp target parallel if (false)); (0, OpenMP, #pragma omp target parallel if (Arg))", "label": "(6, OpenMP, #pragma omp target teams distribute parallel for if (false)); (9, OpenMP, #pragma omp target teams distribute parallel for if (parallel: Arg)); (0, OpenMP, #pragma omp target teams distribute parallel for if (true))"}
{"predict": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(9, MPI, MPI_Reduce_local(inbuf, inoutbuf, count, datatype, MPI_SUM, stride, datatype);)", "label": "(8, MPI, MPI_Reduce_local_c(inbuf, inoutbuf, count, datatype, MPI_SUM);)"}
{"predict": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(135, MPI, MPI_ERRHAND(MPI_File_read_ordered(c_info->fh, c_info->r_buffer, Locsize, c_info->etype, &stat));); (123, MPI, MPI_ERRHAND(MPI_File_read_at(c_info->fh, Offset, c_info->r_buffer, Locsize, c_info->etype, &stat));); (102, MPI, *time = (MPI_Wtime() - *time) / (i_sample*j_sample);); (137, MPI, MPI_ERRHAND(MPI_File_read_shared(c_info->fh, c_info->r_buffer, Locsize, c_info->etype, &stat));)", "label": "(102, MPI, MPI_Barrier(c_info->File_comm);); (108, MPI, MPI_ERRHAND(GEN_File_read(c_info->fh, c_info->r_buffer, Locsize, c_info->etype, &stat));); (121, MPI, MPI_ERRHAND(GEN_File_read_at(c_info->fh, Offset, c_info->r_buffer, Locsize, c_info->etype, &stat));); (133, MPI, MPI_ERRHAND(GEN_File_read_shared(c_info->fh, c_info->r_buffer, Locsize, c_info->etype, &stat));); (92, MPI, *time = MPI_Wtime();)"}
{"predict": "(14, CUDA, qmc_cuda::cuda_check(cudaFree(d_ptr));); (7, CUDA, qmc_cuda::cuda_check(cudaGetLastError());); (5, CUDA, thrust::device_ptr<thrust::complex<double>> d_ptr = thrust::device_malloc<thrust::complex<double>>(1);)", "label": "(6, CUDA, thrust::device_ptr<thrust::complex<double>> d_ptr = thrust::device_malloc<thrust::complex<double>>(1);); (14, CUDA, thrust::device_free(d_ptr);); (9, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize());); (7, CUDA, qmc_cuda::cuda_check(cudaGetLastError());)"}
{"predict": "(65, OpenMP, #pragma omp distribute parallel for simd private(h)); (70, OpenMP, #pragma omp parallel private(i)); (68, OpenMP, #pragma omp parallel); (25, OpenMP, #pragma omp distribute parallel for simd private(argc); (56, OpenMP, #pragma omp distribute parallel for simd private(e, g)); (36, OpenMP, #pragma omp distribute parallel for simd private(S1)); (4, OpenMP, #pragma omp distribute parallel for simd private); (6, OpenMP, #pragma omp distribute parallel for simd private(); (47, OpenMP, #pragma omp distribute parallel for simd private(argv[1])); (36, OpenMP, #pragma omp distribute parallel for simd private(a, b)); (58, OpenMP, #pragma omp distribute parallel for simd nowait); (66, OpenMP, #pragma omp distribute parallel for simd private(i)); (14, OpenMP, #pragma omp distribute parallel for simd private(argc > 0? argv[1] : argv[2])); (5, OpenMP, #pragma omp distribute parallel for simd private()); (46, OpenMP, #pragma omp distribute parallel for simd private(j)); (23, OpenMP, #pragma omp distribute parallel for simd private(argc,); (73, OpenMP, #pragma omp distribute parallel for simd private(j)); (35, OpenMP, #pragma omp distribute parallel for simd private(argv[1],); (62, OpenMP, #pragma omp parallel shared(i)); (4, OpenMP, #pragma omp distribute parallel for simd private(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (63, OpenMP, #pragma omp parallel shared(i)); (3, OpenMP, #pragma omp distribute parallel for simd private(argc,); (66, OpenMP, #pragma omp parallel private(i)); (7, OpenMP, #pragma omp distribute parallel for simd private(argc)", "label": "(63, OpenMP, #pragma omp distribute parallel for simd private(h)); (86, OpenMP, #pragma omp distribute parallel for simd private(j)); (82, OpenMP, #pragma omp parallel private(i)); (65, OpenMP, #pragma omp distribute parallel for simd nowait); (9, OpenMP, #pragma omp distribute parallel for simd private(); (1, OpenMP, #pragma omp target); (27, OpenMP, #pragma omp distribute parallel for simd private(argc > 0? argv[1] : argv[2])); (1, OpenMP, #pragma omp distribute parallel for simd private); (35, OpenMP, #pragma omp distribute parallel for simd private(S1)); (19, OpenMP, #pragma omp distribute parallel for simd private(argc,); (61, OpenMP, #pragma omp parallel); (12, OpenMP, #pragma omp distribute parallel for simd private(argc); (36, OpenMP, #pragma omp distribute parallel for simd private(a, b)); (25, OpenMP, #pragma omp distribute parallel for simd private(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (39, OpenMP, #pragma omp distribute parallel for simd private(argv[1])); (43, OpenMP, #pragma omp distribute parallel for simd private(e, g)); (2, OpenMP, #pragma omp distribute parallel for simd private()); (60, OpenMP, #pragma omp distribute parallel for simd private(i)); (65, OpenMP, #pragma omp parallel shared(i)); (-12, OpenMP, #pragma omp teams)"}
{"predict": "(13, OpenCL, clReleaseMemObject(objScratch);); (8, OpenCL, clReleaseMemObject(objNrm2);); (2, OpenCL, clReleaseMemObject(objX);)", "label": "(5, OpenCL, clReleaseMemObject(objX);); (12, OpenCL, clReleaseMemObject(objScratch);); (7, OpenCL, clReleaseMemObject(objNrm2);)"}
{"predict": "(4, CUDA, __shared__ unsigned short points[2*MEMWID];)", "label": "(3, CUDA, __shared__ unsigned short points[2*MEMWID];)"}
{"predict": "(13, CUDA, CUDASimulation cudaSimulation(model);)", "label": "(13, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(67, MPI, MPI_Waitall(2, request, status);)", "label": "(67, MPI, MPI_Waitall(2, request, status);)"}
{"predict": "(22, MPI, MPI_Win_get_group(win, &wingroup);); (24, MPI, MPI_Group_free(&wingroup);); (20, MPI, MPI_Win_create(buf, sizeof(int) * 10, sizeof(int), MPI_INFO_NULL, comm, &win);); (18, MPI, MPI_Win_set_errhandler(win, MPI_ERRORS_RETURN);); (23, MPI, MPI_Group_free(&group);); (16, MPI, MPI_Win_set_group(win, group);); (12, MPI, MPI_Comm_group(comm, &group);); (23, MPI, MPI_Win_free(&win);)", "label": "(28, MPI, MPI_Group_free(&wingroup);); (19, MPI, MPI_Win_create(buf, sizeof(int) * 10, sizeof(int), MPI_INFO_NULL, comm, &win);); (28, MPI, MPI_Win_free(&win);); (20, MPI, MPI_Group_compare(group, wingroup, &result);); (25, MPI, MPI_Group_free(&group);); (17, MPI, MPI_Comm_group(comm, &group);); (15, MPI, MPI_Win_get_group(win, &wingroup);)"}
{"predict": "(13, CUDA, ColorSpinorParam cudaParam(cpuParam, *inv_param, QUDA_CUDA_FIELD_LOCATION);)", "label": "(14, CUDA, ColorSpinorParam cudaParam(cpuParam, *inv_param, QUDA_CUDA_FIELD_LOCATION);)"}
{"predict": "(15, OpenMP, #pragma omp target teams); (16, OpenMP, #pragma omp parallel for)", "label": "(15, OpenMP, #pragma omp target parallel for)"}
{"predict": "(41, CUDA, thrust::host_vector<mytype> th_data(h_data, h_data+val);); (27, CUDA, err = cudaLaunchCooperativeKernel((void *)my_remove_if<mytype>, grid, block, args, sizeof(args), cudaLaunchCooperativeKernelDefault);); (49, CUDA, cudaFree(d_idxs);); (22, CUDA, cudaStreamCreate(&str);); (1, CUDA, cudaMalloc(&d_idxs, test_dsize*sizeof(unsigned));); (46, CUDA, cudaEventDestroy(start);); (40, CUDA, cudaEventSynchronize(stop);); (46, CUDA, cudaFree(d_odata);); (13, CUDA, cudaOccupancyMaxActiveBlocksPerMultiprocessor(&numBlkPerSM, my_remove_if<mytype>, nTPB, 0);); (43, CUDA, cudaFree(d_idata);); (17, CUDA, cudaMemcpy(d_idata, h_data, tsize, cudaMemcpyHostToDevice);); (33, CUDA, cudaEventRecord(stop);); (33, CUDA, cudaEventElapsedTime(&et, start, stop);); (3, CUDA, cudaMalloc(&d_odata, tsize);); (3, CUDA, cudaMalloc(&d_idata, tsize);); (30, CUDA, cudaEventCreate(&stop);); (36, CUDA, thrust::remove(th_data.begin(), th_data.end(), -1);); (30, CUDA, cudaEventRecord(start);); (-5, CUDA, cudaError_t err = cudaGetDeviceProperties(&prop, 0);); (23, CUDA, cudaEventCreate(&start);)", "label": "(58, CUDA, thrust::host_vector<mytype> th_data = t_data;); (40, CUDA, thrust::device_vector<mytype> t_data(h_data, h_data+test_dsize);); (49, CUDA, cudaEventElapsedTime(&et, start, stop);); (22, CUDA, cudaStreamCreate(&str);); (27, CUDA, cudaLaunchCooperativeKernel((void *)my_remove_if<mytype>, FIXME);); (8, CUDA, cudaError_t err = cudaGetDeviceProperties(&prop, 0);); (26, CUDA, cudaMemcpy(h_data, d_odata, tsize, cudaMemcpyDeviceToHost);); (49, CUDA, thrust::remove(t_data.begin(), t_data.end(), -1);); (15, CUDA, cudaMemcpy(d_idata, h_data, tsize, cudaMemcpyHostToDevice);); (39, CUDA, cudaEventRecord(stop);); (-1, CUDA, cudaMalloc(&d_idxs, test_dsize*sizeof(unsigned));); (33, CUDA, cudaEventCreate(&start);); (-5, CUDA, cudaMalloc(&d_odata, tsize);); (33, CUDA, cudaEventRecord(start);); (-6, CUDA, cudaMemset(d_odata, 0, tsize);); (-9, CUDA, cudaMalloc(&d_idata, tsize);); (29, CUDA, cudaEventCreate(&stop);); (2, CUDA, cudaOccupancyMaxActiveBlocksPerMultiprocessor(&numBlkPerSM, my_remove_if<mytype>, nTPB, 0);)"}
{"predict": "(106, MPI, MPI_Type_free(recv_types);); (49, MPI, MPI_Type_commit(recv_types + j);)", "label": "(48, MPI, MPI_Type_commit(recv_types + j);)"}
{"predict": "(55, CUDA, HANDLE_ERROR(cudaStreamSynchronize(stream));); (9, CUDA, HANDLE_ERROR(cudaMalloc((void**)&initial_x_d, sizeof(float) * 2 * DYN_T::STATE_DIM));); (36, CUDA, HANDLE_ERROR(cudaMemcpyAsync(trajectory_costs_gpu.data(), cost_trajectories_d, sizeof(float) * 2 * num_rollouts, cudaMemcpyDeviceToHost, stream));); (7, CUDA, HANDLE_ERROR(cudaMalloc((void**)&cost_trajectories_d, sizeof(float) * 2 * num_rollouts));)", "label": "(25, CUDA, HANDLE_ERROR(cudaStreamSynchronize(stream));); (9, CUDA, HANDLE_ERROR(cudaMalloc((void**)&initial_x_d, sizeof(float) * 2 * DYN_T::STATE_DIM));); (9, CUDA, HANDLE_ERROR(cudaMalloc((void**)&cost_trajectories_d, sizeof(float) * 2 * num_rollouts));)"}
{"predict": "(3, CUDA, extern __shared__ float s_buffer[];)", "label": "(6, CUDA, extern __shared__ float s_buffer[];)"}
{"predict": "(23, MPI, MPI_Grequest_complete(*array_of_states);)", "label": "(19, MPI, MPI_Grequest_complete(aio_reqlist[i]->req);)"}
{"predict": "(58, CUDA, cudaEventSynchronize(stop);); (68, CUDA, cudaEventElapsedTime(&cpu_elapsed_time_ms, start, stop);); (107, CUDA, cudaFreeHost(h_cc);); (64, CUDA, checkCudaErrors(cudaGetLastError());); (6, CUDA, cudaMallocHost((void **)&h_a, sizeof(int)*m*n);); (36, CUDA, checkCudaErrors(cudaMalloc((void **)&d_a, sizeof(int)*m*n));); (55, CUDA, checkCudaErrors(cudaMemcpyAsync(h_c, d_c, sizeof(int)*m*k, cudaMemcpyDeviceToHost));); (6, CUDA, cudaMallocHost((void **)&h_c, sizeof(int)*m*k);); (38, CUDA, checkCudaErrors(cudaMemcpyAsync(d_a, h_a, sizeof(int)*m*n, cudaMemcpyHostToDevice));); (6, CUDA, cudaMallocHost((void **)&h_cc, sizeof(int)*m*k);); (26, CUDA, checkCudaErrors(cudaMalloc((void **)&d_c, sizeof(int)*m*k));); (53, CUDA, cudaEventRecord(stop, 0);); (25, CUDA, checkCudaErrors(cudaMalloc((void **)&d_b, sizeof(int)*n*k));); (95, CUDA, cudaFree(d_a);); (95, CUDA, cudaFree(d_b);); (95, CUDA, cudaFree(d_c);); (56, CUDA, checkCudaErrors(cudaMemcpyAsync(d_b, h_b, sizeof(int)*n*k, cudaMemcpyHostToDevice));); (52, CUDA, checkCudaErrors(cudaMemcpyAsync(d_a, h_a, sizeof(int)*m*n, cudaMemcpyHostToDevice));); (53, CUDA, checkCudaErrors(cudaMemcpyAsync(d_b, h_b, sizeof(int)*n*k, cudaMemcpyHostToDevice));); (25, CUDA, checkCudaErrors(cudaMemcpyAsync(d_b, h_b, sizeof(int)*n*k, cudaMemcpyHostToDevice));); (4", "label": "(67, CUDA, cudaEventSynchronize(stop);); (39, CUDA, cudaMalloc((void **) &d_a, sizeof(int)*m*n);); (114, CUDA, cudaFreeHost(h_cc);); (34, CUDA, cudaEventRecord(start, 0);); (60, CUDA, cudaThreadSynchronize();); (36, CUDA, cudaMalloc((void **) &d_b, sizeof(int)*n*k);); (39, CUDA, cudaMemcpy(d_a, h_a, sizeof(int)*m*n, cudaMemcpyHostToDevice);); (103, CUDA, cudaFree(d_a);); (62, CUDA, cudaEventElapsedTime(&gpu_elapsed_time_ms, start, stop);); (3, CUDA, cudaMallocHost((void **) &h_c, sizeof(int)*m*k);); (3, CUDA, cudaMallocHost((void **) &h_cc, sizeof(int)*m*k);); (35, CUDA, cudaMemcpy(d_b, h_b, sizeof(int)*n*k, cudaMemcpyHostToDevice);); (-1, CUDA, cudaMallocHost((void **) &h_b, sizeof(int)*n*k);); (99, CUDA, cudaFree(d_c);); (97, CUDA, cudaFree(d_b);); (66, CUDA, cudaEventElapsedTime(&cpu_elapsed_time_ms, start, stop);); (26, CUDA, cudaMalloc((void **) &d_c, sizeof(int)*m*k);); (49, CUDA, cudaEventRecord(stop, 0);); (45, CUDA, cudaMemcpy(h_c, d_c, sizeof(int)*m*k, cudaMemcpyDeviceToHost);); (95, CUDA, cudaFreeHost(h_b);); (93, CUDA, cudaFreeHost(h_a);); (12, CUDA, cudaEventCreate(&start);); (55, OpenMP, omp_mm(h_a, m, n, h_b, n, k, h_cc);); (-13, CUDA, cudaMallocHost((void **) &h_a, sizeof(int)*m*n);); (10, CUDA, cudaEventCreate(&stop);); (90, CUDA, cudaFreeHost(h_c);)"}
{"predict": "(37, CUDA, cudaMalloc((void **)&result_data, tensor1->size * sizeof(float));)", "label": "(37, CUDA, cudaMalloc((void **)&result_data, tensor1->size * sizeof(float));)"}
{"predict": "(8, OpenMP, #pragma omp parallel firstprivate(a, b, c)); (1, OpenMP, #pragma omp parallel firstprivate(a, b, c))", "label": "(1, OpenMP, #pragma omp parallel firstprivate(a, b, c, e)); (4, OpenMP, #pragma omp parallel firstprivate(a, b, c))"}
{"predict": "(11, CUDA, CUDA_CHECK(cudaMemcpy(M_device, M_host, size, cudaMemcpyHostToDevice));); (37, CUDA, cudaFree(P_device);); (35, CUDA, cudaFree(N_device);); (28, CUDA, CUDA_CHECK(cudaMemcpy(P_host, P_device, size, cudaMemcpyDeviceToHost));); (33, CUDA, cudaFree(M_device);); (8, CUDA, CUDA_CHECK(cudaMemcpy(N_device, N_host, size, cudaMemcpyHostToDevice));); (3, CUDA, CUDA_CHECK(cudaMalloc((void**)&N_device, size));); (1, CUDA, CUDA_CHECK(cudaMalloc((void**)&M_device, size));); (10, CUDA, CUDA_CHECK(cudaMalloc((void**)&P_device, size));;); (23, CUDA, CUDA_CHECK(cudaDeviceSynchronize());)", "label": "(29, CUDA, CUDA_CHECK(cudaMemcpy(P_host, P_device, size, cudaMemcpyDeviceToHost));); (35, CUDA, cudaFree(P_device);); (10, CUDA, CUDA_CHECK(cudaMemcpy(M_device, M_host, size, cudaMemcpyHostToDevice));); (34, CUDA, cudaFree(N_device);); (34, CUDA, cudaFree(M_device);); (25, CUDA, CUDA_CHECK(cudaDeviceSynchronize());); (7, CUDA, CUDA_CHECK(cudaMemcpy(N_device, N_host, size, cudaMemcpyHostToDevice));); (2, CUDA, CUDA_CHECK(cudaMalloc((void**)&N_device, size));); (9, CUDA, CUDA_CHECK(cudaMalloc((void**)&P_device, size));;); (-1, CUDA, CUDA_CHECK(cudaMalloc((void**)&M_device, size));)"}
{"predict": "(11, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)", "label": "(11, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)"}
{"predict": "(3, OpenMP, #pragma omp task default()); (11, OpenMP, #pragma omp parallel default(shared)); (0, OpenMP, #pragma omp task default(); (4, OpenMP, #pragma omp task default(x)); (1, OpenMP, #pragma omp task default(none)", "label": "(17, OpenMP, #pragma omp task default(firstprivate)); (4, OpenMP, #pragma omp task default(shared), default(shared)); (0, OpenMP, #pragma omp task default(); (0, OpenMP, #pragma omp task default()); (5, OpenMP, #pragma omp task default(none)); (1, OpenMP, #pragma omp task default(x)); (7, OpenMP, #pragma omp task default(shared)); (-6, OpenMP, #pragma omp task default); (-4, OpenMP, #pragma omp task default(none)"}
{"predict": "(6, TBB, tbb::parallel_for(blocked_range3d<int>(i,i+1,1,j,j+1,1,k,k+1,1), AssignElement(Array));)", "label": "(4, TBB, const tbb::blocked_range3d<int> r( 0, i, 5, 0, j, 3, 0, k, 1 );); (4, TBB, tbb::parallel_for( r, Striker() );)"}
{"predict": "(31, CUDA, __shared__ int s_bmod;); (31, CUDA, __shared__ double s_lsum[MAXSUPER];)", "label": "(31, CUDA, volatile __shared__ int s_bmod;); (28, CUDA, __shared__ double s_lsum[MAXSUPER];)"}
{"predict": "(52, MPI, MPI_Allreduce(MPI_IN_PLACE,&n,1,mpi_int_t,MPI_MAX,grid->comm);); (50, MPI, MPI_Allreduce(MPI_IN_PLACE,&nnzL,1,mpi_int_t,MPI_SUM,grid->comm);)", "label": "(52, MPI, MPI_Allreduce(MPI_IN_PLACE,&n,1,mpi_int_t,MPI_MAX,grid->comm);); (50, MPI, MPI_Allreduce(MPI_IN_PLACE,&nnzL,1,mpi_int_t,MPI_SUM,grid->comm);)"}
{"predict": "(35, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (52, CUDA, CUDA_CHECK(cudaDeviceReset());); (50, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (23, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (42, CUDA, CUDA_CHECK(cudaFree(d_A));); (17, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)", "label": "(34, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (50, CUDA, CUDA_CHECK(cudaDeviceReset());); (47, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (23, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (41, CUDA, CUDA_CHECK(cudaFree(d_A));); (17, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(33, MPI, err = MPI_Type_size(eviltype, &val);); (135, MPI, MPI_Type_free(&inttype);); (7, MPI, err = MPI_Type_struct(3, blocks, disps, types, &inttype);); (96, MPI, err = MPI_Type_ub(eviltype, &aval);); (131, MPI, MPI_Type_free(&eviltype);); (10, MPI, err = MPI_Type_vector(3, 4, -5, inttype, &eviltype);); (65, MPI, err = MPI_Type_get_extent(eviltype, &lb, &extent);); (107, MPI, err = MPI_Type_get_true_extent(eviltype, &true_lb, &aval);); (55, MPI, err = MPI_Type_lb(eviltype, &aval);); (39, MPI, err = MPI_Type_extent(eviltype, &aval);)", "label": "(11, MPI, err = MPI_Type_struct(3, blocks, disps, types, &inttype);); (32, MPI, err = MPI_Type_size(eviltype, &val);); (134, MPI, MPI_Type_free(&inttype);); (75, MPI, err = MPI_Type_get_extent(eviltype, &lb, &extent);); (96, MPI, err = MPI_Type_ub(eviltype, &aval);); (17, MPI, err = MPI_Type_vector(3, 4, -5, inttype, &eviltype);); (131, MPI, MPI_Type_free(&eviltype);); (108, MPI, err = MPI_Type_get_true_extent(eviltype, &true_lb, &aval);); (40, MPI, err = MPI_Type_extent(eviltype, &aval);); (54, MPI, err = MPI_Type_lb(eviltype, &aval);)"}
{"predict": "(48, OpenMP, lis2mdl_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)", "label": "(48, OpenMP, lis2mdl_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)"}
{"predict": "(4, TBB, tbb::parallel_for( tbb::blocked_range<int>(0,i), Striker() );); (5, TBB, tbb::parallel_for( tbb::blocked_range<int>(0,j), Striker() );); (6, TBB, tbb::parallel_for( tbb::blocked_range<int>(0,k), Striker() );)", "label": "(4, TBB, const tbb::blocked_range3d<int> r( 0, i, 5, 0, j, 3, 0, k, 1 );); (4, TBB, tbb::parallel_for( r, Striker() );)"}
{"predict": "(36, CUDA, CUDASimulation cudaSimulation(model);)", "label": "(36, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(10, OpenMP, #pragma omp distribute parallel for simd lastprivate(g, g1, svar, sfvar)); (8, OpenMP, #pragma omp target); (64, OpenMP, #pragma omp distribute parallel for simd lastprivate(t_var, vec, s_arr, s_arr, var, var)); (8, OpenMP, #pragma omp teams)", "label": "(9, OpenMP, #pragma omp target); (65, OpenMP, #pragma omp distribute parallel for lastprivate(t_var, vec, s_arr, s_arr, var, var, svar)); (9, OpenMP, #pragma omp distribute parallel for lastprivate(g, g1, svar, sfvar)); (7, OpenMP, #pragma omp teams)"}
{"predict": "(22, CUDA, cudaCheck(cudaGetLastError());)", "label": "(45, CUDA, cudaCheck(cudaGetLastError());)"}
{"predict": "(66, OpenMP, iis2mdc_offset_temp_comp_set(&mag_ctx, PROPERTY_ENABLE);)", "label": "(66, OpenMP, iis2mdc_offset_temp_comp_set(&mag_ctx, PROPERTY_ENABLE);)"}
{"predict": "(13, CUDA, CP_host->freeCudaMem();)", "label": "(13, CUDA, CP_host->freeCudaMem();)"}
{"predict": "(87, CUDA, cudaDeviceSynchronize();); (151, CUDA, cudaStreamSynchronize(cuStreams[proc]);); (88, CUDA, if(cudaGetLastError()!=cudaSuccess)printf(\"slsum_bmod_inv_gpu_mrhs_nvshmem error %s\\n\",cudaGetErrorString(cudaGetLastError()));); (138, CUDA, cudaStreamDestroy(cuStreams[i]);); (144, CUDA, cudaFree(d_mynum_u);); (144, CUDA, cudaFree(d_mymaskstart_u);); (138, CUDA, cudaFreeHost(h_nfrecv_u);); (138, CUDA, cudaFree(d_nfrecv_u);); (139, CUDA, cudaFree(d_colnum_u);); (143, CUDA, cudaFree(d_flag_mod_u);); (138, CUDA, cudaFree(d_recv_cnt_u);); (140, CUDA, cudaFree(d_msgnum);); (136, CUDA, cudaFree(d_status_u);); (137, CUDA, cudaFree(d_mynummod_u);); (138, CUDA, cudaFree(d_mymasklength_u);); (134, CUDA, cudaFree(d_statusmod);); (134, CUDA, cudaFree(d_colnummod_u);); (135, CUDA, cudaFree(d_nfrecvmod_u);); (138, CUDA, cudaFree(d_ind_br_dat);); (136, CUDA, cudaFree(d_ind_br_offset);); (130, CUDA, cudaFree(d_lsum_bc_dat);); (135, CUDA, cudaFree(d_indval_loc_bc_dat);); (137, CUDA, cudaFree(d_indval_loc_bc_offset);); (128, CUDA, cudaFree(d_Uinv_bc_dat);); (128, CUDA, cudaFree(d_Uinv_bc_offset);); (134, CUDA, cudaFree(d_Unzval_br_new_", "label": "(117, CUDA, cudaDeviceSetLimit(cudaLimitStackSize, cuattr.localSizeBytes);); (93, CUDA, cudaStreamCreateWithFlags(&stream[i], cudaStreamNonBlocking);); (121, CUDA, cudaOccupancyMaxPotentialBlockSize(&minGridSize, &myblockSize, (const void *) swait_bcrd_u, 0, 0);); (173, CUDA, CUDA_CHECK(cudaGetLastError());); (112, CUDA, cudaFuncGetAttributes(&cuattr, slsum_bmod_inv_gpu_mrhs_nvshmem);); (181, CUDA, CUDA_CHECK(cudaStreamDestroy(stream[i]));)"}
{"predict": "(72, CUDA, getLastCudaError(\"Kernel failed : currentDepositionKernel1D_Order2 [HIP]\");); (101, CUDA, getLastCudaError(\"Kernel failed : currentDepositionKernel1D_Order2 [NVCC]\");)", "label": "(101, CUDA, checkHIPErrors( ::cudaDeviceSynchronize() );)"}
{"predict": "(33, OpenMP, #pragma omp barrier); (21, OpenMP, #pragma omp target map(tofrom:x[:N]) map(to:y[:N],z[:N])); (15, OpenMP, #pragma omp parallel num_threads(THREADS)); (24, OpenMP, #pragma omp atomic read); (28, OpenMP, #pragma omp atomic write); (16, OpenMP, #pragma omp master)", "label": "(27, OpenMP, #pragma omp atomic update); (21, OpenMP, #pragma omp loop order(concurrent)); (29, OpenMP, num_threads = omp_get_num_threads();); (17, OpenMP, #pragma omp target parallel num_threads(THREADS) map(tofrom: x[0:N], num_threads, total_wait_errors) map(to: y[0:N], z[0:N]))"}
{"predict": "(17, OpenMP, #pragma omp target defaultmap(tofrom:scalar)); (2, OpenMP, #pragma omp target defaultmap); (33, OpenMP, #pragma omp target defaultmap(none:scalar) map(g)); (35, OpenMP, #pragma omp target defaultmap(none:scalar) map(g) firstprivate(argc)); (16, OpenMP, #pragma omp target defaultmap(none:scalar) map(g)); (1, OpenMP, #pragma omp target defaultmap()); (10, OpenMP, #pragma omp target defaultmap(none:pointer)); (3, OpenMP, #pragma omp target defaultmap(tofrom)); (2, OpenMP, #pragma omp target defaultmap(tofrom scalar); (7, OpenMP, #pragma omp target defaultmap(none:pointer)); (20, OpenMP, #pragma omp target defaultmap(none:aggregate) defaultmap(none:scalar) map(argc)); (25, OpenMP, #pragma omp target defaultmap(none:aggregate)); (4, OpenMP, #pragma omp target defaultmap(none:pointer) map(g)); (-6, OpenMP, #pragma omp target defaultmap(); (10, OpenMP, #pragma omp target defaultmap(none:pointer) map(g) firstprivate(argc)); (16, OpenMP, #pragma omp target defaultmap(none:pointer) map(g) allocate(g)); (3, OpenMP, #pragma omp target defaultmap(none:pointer) map(g) firstprivate(argc > 0? argv[1] : argv[2]))", "label": "(9, OpenMP, #pragma omp target defaultmap(tofrom:); (4, OpenMP, #pragma omp target defaultmap()); (25, OpenMP, #pragma omp target defaultmap(none:pointer)); (10, OpenMP, #pragma omp target defaultmap(tofrom,); (40, OpenMP, #pragma omp parallel); (-4, OpenMP, #pragma omp target defaultmap); (27, OpenMP, #pragma omp target defaultmap(none:pointer) defaultmap(none:scalar) defaultmap(none:aggregate) map(g) firstprivate(argc)); (8, OpenMP, #pragma omp target defaultmap(scalar:); (11, OpenMP, #pragma omp target defaultmap(tofrom: scalar) defaultmap(tofrom: scalar)); (20, OpenMP, #pragma omp target defaultmap(none:pointer) defaultmap(none:scalar) map(argc)); (1, OpenMP, #pragma omp target defaultmap(tofrom)); (6, OpenMP, #pragma omp target defaultmap(tofrom, scalar); (23, OpenMP, #pragma omp target defaultmap(none:aggregate)); (-10, OpenMP, #pragma omp target defaultmap(); (9, OpenMP, #pragma omp target defaultmap(tofrom : scalar) defaultmap(to: scalar)); (16, OpenMP, #pragma omp target defaultmap(none:pointer) defaultmap(none:scalar) map(g)); (9, OpenMP, #pragma omp target defaultmap(alloc: pointer) defaultmap(to: scalar) defaultmap(firstprivate: pointer)); (20, OpenMP, #pragma omp target defaultmap(none:aggregate) defaultmap(none:scalar) map(argc)); (-11, OpenMP, #pragma omp target defaultmap(tofrom)"}
{"predict": "(139, MPI, MPI_Finalize();); (63, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (17, MPI, MPI_Init(&argc,&argv);); (53, MPI, MPI_Bcast(&vector_length, 1, MPI_LONG, root, MPI_COMM_WORLD);); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (80, MPI, MPI_Barrier(MPI_COMM_WORLD);); (13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)", "label": "(139, MPI, MPI_Finalize();); (63, MPI, MPI_Bcast(&iterations,    1, MPI_INT, root, MPI_COMM_WORLD);); (63, MPI, MPI_Bcast(&vector_length, 1, MPI_LONG, root, MPI_COMM_WORLD);); (17, MPI, MPI_Init(&argc,&argv);); (81, MPI, MPI_Barrier(MPI_COMM_WORLD);); (17, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (15, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(57, OpenMP, #pragma omp target teams distribute parallel for simd collapse (2)); (38, OpenMP, #pragma omp target teams distribute parallel for simd collapse (foobool(argc)), collapse (true), collapse (-5)); (22, OpenMP, #pragma omp target teams distribute parallel for simd collapse (1))); (1, OpenMP, #pragma omp target teams distribute parallel for simd collapse (); (24, OpenMP, #pragma omp target teams distribute parallel for simd collapse (ST); (56, OpenMP, #pragma omp target teams distribute parallel for simd collapse (N)); (3, OpenMP, #pragma omp target teams distribute parallel for simd collapse (argc); (33, OpenMP, #pragma omp target teams distribute parallel for simd collapse (S)); (13, OpenMP, #pragma omp target teams distribute parallel for simd collapse ((ST > 0)? 1 + ST : 2)); (2, OpenMP, #pragma omp target teams distribute parallel for simd collapse ()); (45, OpenMP, #pragma omp target teams distribute parallel for simd collapse (1)); (48, OpenMP, #pragma omp target teams distribute parallel for simd collapse (N))", "label": "(25, OpenMP, #pragma omp target teams distribute parallel for simd collapse (1))); (60, OpenMP, #pragma omp target teams distribute parallel for simd collapse (2)); (4, OpenMP, #pragma omp target teams distribute parallel for simd collapse (); (54, OpenMP, #pragma omp target teams distribute parallel for simd collapse (N)); (36, OpenMP, #pragma omp target teams distribute parallel for simd collapse (S)); (24, OpenMP, #pragma omp target teams distribute parallel for simd collapse ((ST > 0)? 1 + ST : 2)); (-4, OpenMP, #pragma omp target teams distribute parallel for simd collapse); (9, OpenMP, #pragma omp target teams distribute parallel for simd collapse (argc); (28, OpenMP, #pragma omp target teams distribute parallel for simd collapse (foobool(argc)), collapse (true), collapse (-5)); (12, OpenMP, #pragma omp target teams distribute parallel for simd collapse (ST); (43, OpenMP, #pragma omp target teams distribute parallel for simd collapse (1)); (-1, OpenMP, #pragma omp target teams distribute parallel for simd collapse ())"}
{"predict": "(87, OpenCL, clReleaseProgram( program[0] );); (201, OpenCL, clReleaseEvent( copyEvent );); (85, OpenCL, clReleaseKernel( kernel[0] );); (94, OpenCL, clReleaseMemObject( memobjs[0] );); (22, OpenCL, queueStart = clWaitForEvents( 1, &copyEvent );); (70, OpenCL, err = clEnqueueWriteBuffer( queue, memobjs[1], true, 0, num_bytes, inptr, 0, NULL, NULL );); (88, OpenCL, err |= clSetKernelArg( kernel[0], 2, sizeof( cl_mem ), (void *)&memobjs[2] );); (64, OpenCL, clReleaseMemObject(memobjs[0]);); (85, OpenCL, err |= clSetKernelArg( kernel[0], 1, sizeof( cl_mem ), (void *)&memobjs[1] );); (85, OpenCL, err |= clSetKernelArg( kernel[0], 3, sizeof( cl_mem ), (void *)&memobjs[0] );); (93, OpenCL, err = clEnqueueNDRangeKernel( queue, kernel[0], 2, NULL, threads, NULL, 0, NULL, NULL );); (144, OpenCL, clReleaseMemObject( memobjs[1] );); (139, OpenCL, clReleaseMemObject( memobjs[2] );); (66, OpenCL, clReleaseMemObject( memobjs[1] );); (19, OpenCL, submitStart = clWaitForEvents( 1, &copyEvent );); (13, OpenCL, clReleaseEvent( copyEvent );); (67, OpenCL, err = clEnqueueWriteImage( queue, memobjs[2], true, origin, region, 0, 0, inptr, 0, NULL, NULL );); (13, OpenCL, writeStart = clWaitForEvents( 1, &copyEvent );); (12, OpenCL, writeEnd = clWaitForEvents( 1, &copyEvent );); (60, OpenCL, err = clEnqueueCopyImage( queue, memobjs[0],", "label": "(85, OpenCL, clReleaseMemObject( memobjs[2] );); (140, OpenCL, clReleaseEvent(copyEvent);); (103, OpenCL, err = clEnqueueNDRangeKernel( queue, kernel[0], 2, NULL, threads, NULL, 0, NULL, NULL );); (72, OpenCL, err = clEnqueueWriteBuffer( queue, memobjs[1], true, 0, num_bytes, inptr, 0, NULL, NULL );); (87, OpenCL, err = clSetKernelArg( kernel[0], 0, sizeof( cl_mem ), (void *)&memobjs[1] );); (91, OpenCL, clReleaseProgram( program[0] );); (77, OpenCL, clReleaseMemObject( memobjs[0] );); (88, OpenCL, clReleaseKernel( kernel[0] );); (84, OpenCL, err |= clSetKernelArg( kernel[0], 1, sizeof( cl_mem ), (void *)&memobjs[0] );); (48, OpenCL, clReleaseMemObject(memobjs[0]);); (113, OpenCL, err = clEnqueueCopyImage( queue, memobjs[0], memobjs[2], srcPt, destPt, region, 0, NULL, &copyEvent );); (57, OpenCL, clReleaseMemObject(memobjs[1]);); (201, OpenCL, err = clEnqueueReadImage( queue, memobjs[2], true, origin, region2, 0, 0, dst, 0, NULL, NULL );); (125, OpenCL, err = clWaitForEvents( 1, &copyEvent );); (70, OpenCL, clReleaseMemObject( memobjs[1] );)"}
{"predict": "(24, CUDA, cudaEventDestroy(start);); (14, CUDA, cudaMemcpyAsync(h_data, d_data, SIZE * sizeof(float), cudaMemcpyDeviceToHost);); (23, CUDA, cudaFree(d_data);); (4, CUDA, cudaEventRecord(stop);); (4, CUDA, cudaEventSynchronize(stop);); (0, CUDA, cudaMemcpyAsync(d_data, h_data, SIZE * sizeof(float), cudaMemcpyHostToDevice);); (1, CUDA, cudaEventRecord(start);); (4, CUDA, cudaEventElapsedTime(&milliseconds, start, stop);); (-4, CUDA, cudaMalloc((void **)&d_data, SIZE * sizeof(float));); (18, CUDA, cudaEventDestroy(stop);); (-6, CUDA, cudaEventCreate(&stop);); (-4, CUDA, cudaEventCreate(&start);)", "label": "(12, CUDA, cudaEventRecord(stop);); (12, CUDA, cudaEventSynchronize(stop);); (9, CUDA, cudaMemcpy(d_data, h_data, SIZE * sizeof(float), cudaMemcpyHostToDevice);); (7, CUDA, cudaEventRecord(start);); (-1, CUDA, cudaMalloc((void **)&d_data, SIZE * sizeof(float));); (10, CUDA, cudaEventElapsedTime(&milliseconds, start, stop);); (1, CUDA, cudaEventCreate(&stop);); (13, CUDA, cudaMemcpy(h_data, d_data, SIZE * sizeof(float), cudaMemcpyDeviceToHost);); (19, CUDA, cudaFree(d_data);); (-3, CUDA, cudaEventCreate(&start);)"}
{"predict": "(1, OpenMP, #pragma omp for); (7, OpenMP, #pragma omp tile sizes(5))", "label": "(6, OpenMP, #pragma omp tile sizes(5)); (0, OpenMP, #pragma omp for)"}
{"predict": "(139, MPI, MPI_Finalize();); (67, MPI, MPI_Bcast(&iterations,    1, MPI_INT, root, MPI_COMM_WORLD);); (67, MPI, MPI_Bcast(&vector_length, 1, MPI_LONG, root, MPI_COMM_WORLD);); (18, MPI, MPI_Init(&argc,&argv);); (93, MPI, MPI_Barrier(MPI_COMM_WORLD);); (17, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (15, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)", "label": "(143, MPI, MPI_Finalize();); (67, MPI, MPI_Bcast(&iterations,    1, MPI_INT, root, MPI_COMM_WORLD);); (67, MPI, MPI_Bcast(&vector_length, 1, MPI_LONG, root, MPI_COMM_WORLD);); (19, MPI, MPI_Init(&argc,&argv);); (85, MPI, MPI_Barrier(MPI_COMM_WORLD);); (19, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (17, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(10, OpenMP, #pragma omp parallel for); (45, OpenMP, #pragma omp atomic)", "label": "(10, OpenMP, #pragma omp parallel for); (45, OpenMP, #pragma omp atomic)"}
{"predict": "(16, CUDA, tmp[t] = thrust::complex<T>(0.0);); (14, CUDA, __shared__ uninitialized_array<thrust::complex<T>, 256> tmp;)", "label": "(20, CUDA, thrust::complex<T> const* m_ = m + batch * mstride;); (16, CUDA, tmp[t] = thrust::complex<T>(0.0);); (9, CUDA, __shared__ thrust::cuda_cub::core::uninitialized_array<thrust::complex<T>, 64> tmp;)"}
{"predict": "(20, CUDA, __shared__ T B_thread_block_tile[BLOCK_TILE_SIZE_K][BLOCK_TILE_SIZE_X];); (18, CUDA, __shared__ T A_thread_block_tile[BLOCK_TILE_SIZE_Y][BLOCK_TILE_SIZE_K];)", "label": "(19, CUDA, __shared__ T B_thread_block_tile[BLOCK_TILE_SIZE_K][BLOCK_TILE_SIZE_X];); (17, CUDA, __shared__ T A_thread_block_tile[BLOCK_TILE_SIZE_Y][BLOCK_TILE_SIZE_K];)"}
{"predict": "(108, MPI, MPI_Finalize();); (4, MPI, err = MPI_Init(&argc, &argv);); (8, MPI, err = MPI_Comm_size(MPI_COMM_WORLD, &numRanks);); (5, MPI, err = MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(109, MPI, MPI_Finalize();); (5, MPI, err = MPI_Init(&argc, &argv);); (8, MPI, err = MPI_Comm_size(MPI_COMM_WORLD, &numRanks);); (5, MPI, err = MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(74, CUDA, CHECK_CUDA(cudaFree(dX));); (28, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (44, CUDA, CHECK_CUDA(cudaMalloc(&dX, size * sizeof(int32_t)));); (71, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (28, CUDA, CHECK_HIEDNN(hiednnSetCudaStream(handle, 0));); (67, CUDA, CHECK_CUDA(cudaFree(dY));); (40, CUDA, CHECK_CUDA(cudaMalloc(&dY, size * sizeof(int32_t)));)", "label": "(29, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (72, CUDA, CHECK_CUDA(cudaFree(dX));); (43, CUDA, CHECK_CUDA(cudaMalloc(&dX, size * sizeof(int32_t)));); (29, CUDA, CHECK_HIEDNN(hiednnSetCudaStream(handle, 0));); (67, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (69, CUDA, CHECK_CUDA(cudaFree(dY));); (40, CUDA, CHECK_CUDA(cudaMalloc(&dY, size * sizeof(int32_t)));)"}
{"predict": "(29, CUDA, __shared__ float x_s[x_s_sz];); (27, CUDA, __shared__ float w_s[w_s_sz];)", "label": "(29, CUDA, __shared__ float x_s[x_s_sz];); (27, CUDA, __shared__ float w_s[w_s_sz];)"}
{"predict": "(16, SYCL, using namespace cl::sycl::access;); (4, SYCL, cl::sycl::queue queue;)", "label": "(16, SYCL, using namespace cl::sycl::access;); (4, SYCL, cl::sycl::queue queue;)"}
{"predict": "(23, OpenMP, #pragma omp distribute collapse (foobool(argc)), collapse (true), collapse (-5)); (28, OpenMP, #pragma omp distribute collapse (1)); (32, OpenMP, #pragma omp distribute collapse (N)); (17, OpenMP, #pragma omp distribute collapse (S)); (3, OpenMP, #pragma omp distribute collapse ()); (3, OpenMP, #pragma omp distribute collapse (argc); (18, OpenMP, #pragma omp distribute collapse (argv[1]=2)); (1, OpenMP, #pragma omp distribute collapse (ST); (27, OpenMP, #pragma omp distribute collapse (2)); (2, OpenMP, #pragma omp distribute collapse (1)); (4, OpenMP, #pragma omp distribute collapse ((ST > 0)? 1 + ST : 2)); (6, OpenMP, #pragma omp distribute collapse (foobool(1) > 0? 1 : 2)); (-6, OpenMP, #pragma omp distribute collapse ()", "label": "(36, OpenMP, #pragma omp distribute collapse (N)); (9, OpenMP, #pragma omp distribute collapse (argc); (36, OpenMP, #pragma omp distribute collapse (2)); (22, OpenMP, #pragma omp distribute collapse (S)); (2, OpenMP, #pragma omp distribute collapse ()); (-1, OpenMP, #pragma omp distribute collapse (); (9, OpenMP, #pragma omp distribute collapse (1))); (27, OpenMP, #pragma omp distribute collapse (1)); (9, OpenMP, #pragma omp distribute collapse ((ST > 0)? 1 + ST : 2)); (-7, OpenMP, #pragma omp distribute collapse); (3, OpenMP, #pragma omp distribute collapse (ST); (12, OpenMP, #pragma omp distribute collapse (foobool(argc)), collapse (true), collapse (-5))"}
{"predict": "(8, MPI, rc = MPI_Type_get_extent(type, &tmp_lb, &attr->child_type_extent);)", "label": "(8, MPI, rc = MPI_Type_get_extent(type, &tmp_lb, &attr->child_type_extent);)"}
{"predict": "(5, OpenMP, #pragma omp target teams distribute simd reduction(+: sivar))", "label": "(5, OpenMP, #pragma omp target teams distribute simd reduction(+: sivar))"}
{"predict": "(42, MPI, MPI_Bcast( rowind, nnz, mpi_int_t,  0, grid->comm );); (35, MPI, MPI_Bcast( &nnz,   1,   mpi_int_t,  0, grid->comm );); (43, MPI, MPI_Bcast( colptr,  n+1, mpi_int_t,  0, grid->comm );); (53, MPI, MPI_Bcast( nzval,  nnz, SuperLU_MPI_DOUBLE_COMPLEX, 0, grid->comm );); (33, MPI, MPI_Bcast( &m,   1,   mpi_int_t,  0, grid->comm );); (40, MPI, MPI_Bcast( &m,   1,   mpi_int_t,  0, grid->comm );); (34, MPI, MPI_Bcast( nzval,   nnz, SuperLU_MPI_DOUBLE_COMPLEX, 0, grid->comm );); (32, MPI, MPI_Bcast( &n,   1,   mpi_int_t,  0, grid->comm );); (37, MPI, MPI_Bcast( &nnz, 1,   mpi_int_t,  0, grid->comm );); (35, MPI, MPI_Bcast( rowind,  nnz, mpi_int_t,  0, grid->comm );); (30, MPI, MPI_Bcast( colptr, n+1, mpi_int_t,  0, grid->comm );); (25, MPI, MPI_Bcast( &m,   1,   mpi_int_t,  0, grid->comm );)", "label": "(50, MPI, MPI_Bcast( nzval,   nnz, SuperLU_MPI_DOUBLE_COMPLEX, 0, grid->comm );); (38, MPI, MPI_Bcast( rowind, nnz, mpi_int_t,  0, grid->comm );); (50, MPI, MPI_Bcast( colptr,  n+1, mpi_int_t,  0, grid->comm );); (48, MPI, MPI_Bcast( rowind,  nnz, mpi_int_t,  0, grid->comm );); (39, MPI, MPI_Bcast( &m,   1,   mpi_int_t,  0, grid->comm );); (31, MPI, MPI_Bcast( &n,     1,   mpi_int_t,  0, grid->comm );); (29, MPI, MPI_Bcast( &m,     1,   mpi_int_t,  0, grid->comm );); (38, MPI, MPI_Bcast( &nnz, 1,   mpi_int_t,  0, grid->comm );); (32, MPI, MPI_Bcast( colptr, n+1, mpi_int_t,  0, grid->comm );); (28, MPI, MPI_Bcast( &nnz,   1,   mpi_int_t,  0, grid->comm );); (28, MPI, MPI_Bcast( nzval,  nnz, SuperLU_MPI_DOUBLE_COMPLEX, 0, grid->comm );); (33, MPI, MPI_Bcast( &n,   1,   mpi_int_t,  0, grid->comm );)"}
{"predict": "(16, MPI, MPI_Recv(buff, count, type, 0, tag, comm, MPI_STATUS_IGNORE);); (22, MPI, MPI_Request_free(&r);); (10, MPI, MPI_Send(buff, count, type, 1, tag, comm);); (4, MPI, MPI_Comm_rank(comm, &rank);)", "label": "(22, MPI, MPI_Request_free(&r);); (14, MPI, MPI_Recv_init(buff, count, type, 0, tag, comm, &r);); (2, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(55, TBB, tbb::flow::remove_edge( n, r[i] );); (2, TBB, tbb::flow::write_once_node<R> n(g);); (18, TBB, tbb::flow::make_edge( n, r[i] );); (-1, TBB, tbb::flow::graph g;)", "label": "(20, TBB, tbb::flow::make_edge( n, r[i] );); (54, TBB, tbb::flow::remove_edge( n, r[i] );); (1, TBB, tbb::flow::write_once_node<R> n(g);); (-1, TBB, tbb::flow::graph g;)"}
{"predict": "(65, OpenCL, clReleaseMemObject(compressedBuffer.release());); (25, OpenCL, auto clCompressedImage = clCreateImage(context, CL_MEM_READ_WRITE, &imageFormat, &imageDescriptor, nullptr, &retVal);); (62, OpenCL, clReleaseMemObject(clCompressedImage);)", "label": "(64, OpenCL, clReleaseMemObject(clCompressedImage);); (25, OpenCL, auto clCompressedImage = clCreateImage(context, CL_MEM_READ_WRITE, &imageFormat, &imageDescriptor, nullptr, &retVal);)"}
{"predict": "(2, CUDA, extern __shared__ float share_mem[];)", "label": "(7, CUDA, extern __shared__ float share_mem[];)"}
{"predict": "(26, OpenMP, #pragma omp barrier); (6, OpenMP, #pragma omp parallel for)", "label": "(7, OpenMP, #pragma omp parallel for)"}
{"predict": "(24, CUDA, cudaSetDevice(deviceId);); (20, CUDA, cudaGetDeviceCount(&nDevices);); (7, MPI, MPIManager mpi_manager(MPI_COMM_WORLD);)", "label": "(24, CUDA, cudaSetDevice(deviceId);); (20, CUDA, cudaGetDeviceCount(&nDevices);); (6, MPI, MPIManager mpi_manager(MPI_COMM_WORLD);)"}
{"predict": "(76, CUDA, cudaMalloc((void **) &d_diffData, m * c * sizeof(double));); (15, CUDA, cudaMalloc((void **) &d_fcLayer, m * c * sizeof(double));); (15, CUDA, cudaMemcpy(d_fcLayer, fcLayer, m * c * sizeof(double), cudaMemcpyHostToDevice);); (19, CUDA, cudaMalloc((void **) &d_softmaxData, m * c * sizeof(double));); (75, CUDA, cudaMemcpy(d_diffData, diffData, m * c * sizeof(double), cudaMemcpyHostToDevice);); (98, CUDA, cudaFree(d_softmaxData);); (46, CUDA, cudaDeviceSynchronize();); (98, CUDA, cudaFree(d_gradData);); (48, CUDA, cudaMemcpy(result, d_softmaxData, m * c * sizeof(double), cudaMemcpyDeviceToHost);); (95, CUDA, cudaFree(d_diffData);); (65, CUDA, cudaMalloc((void **) &d_gradData, m * c * sizeof(double));); (90, CUDA, cudaMemcpy(result_backward, d_gradData, m * c * sizeof(double), cudaMemcpyDeviceToHost);); (88, CUDA, cudaFree(d_fcLayer);)", "label": "(73, CUDA, cudaMalloc((void**) &d_gradData, m * c * sizeof(double));); (116, CUDA, cudaFree(d_fcLayer);); (118, CUDA, cudaFree(d_diffData);); (116, CUDA, cudaFree(d_gradData);); (12, CUDA, cudaMalloc((void**) &d_fcLayer, m * c * sizeof(double));); (17, CUDA, cudaMalloc((void**) &d_softmaxData, m * c * sizeof(double));); (43, CUDA, cudaDeviceSynchronize();); (11, CUDA, cudaMemcpy(d_fcLayer, fcLayer, m * c * sizeof(double), cudaMemcpyHostToDevice);); (72, CUDA, cudaMalloc((void**) &d_diffData, m * c * sizeof(double));); (109, CUDA, cudaFree(d_softmaxData);); (83, CUDA, cudaMemcpy(result_backward, d_gradData, m * c * sizeof(double), cudaMemcpyDeviceToHost);); (44, CUDA, cudaMemcpy(result, d_softmaxData, m * c * sizeof(double), cudaMemcpyDeviceToHost);); (69, CUDA, cudaMemcpy(d_diffData, diffData, m * c * sizeof(double), cudaMemcpyHostToDevice);)"}
{"predict": "(11, OpenMP, #pragma omp parallel for); (77, OpenMP, #pragma omp atomic)", "label": "(11, OpenMP, #pragma omp parallel for); (77, OpenMP, #pragma omp atomic)"}
{"predict": "IT(143, MPI, MPI_Allreduce(&tmp_error, error_code, 1, MPI_INT, MPI_MAX, fd->comm);); (47, MPI, MPI_Comm_size(fd->comm, &nprocs);); (133, MPI, MPI_Allreduce(&tmp_error, &old_error, 1, MPI_INT, MPI_MIN, fd->comm);); (47, MPI, MPI_Comm_rank(fd->comm, &myrank);); (91, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (71, MPI, MPI_Allgather(&end_offset, 1, ADIO_OFFSET, end_offsets, 1, ADIO_OFFSET, fd->comm);); (70, MPI, MPI_Allgather(&start_offset, 1, ADIO_OFFSET, st_offsets, 1, ADIO_OFFSET, fd->comm);); (315, MPI, MPI_Type_size_x(datatype, &size);); (321, MPI, MPI_Type_size_x(datatype, &buftype_extent);); (313, MPI, MPI_Type_size_x(datatype, &buftype_size);); (292, MPI, MPI_Type_size_x(datatype, &size);); (286, MPI, MPI_Type_size_x(datatype, &etype_size);); (308, MPI, MPI_Type_size_x(datatype, &buftype_size);); (294, MPI, MPI_Type_size_x(datatype, &etype_extent);); (125, MPI, MPI_Allreduce(&tmp_error, error_code, 1, MPI_INT, MPI_MAX, fd->comm);); (304, MPI, MPI_Type_size_x(datatype, &etype_size);); (306, MPI, MPI_Type_size_x(datatype, &etype_extent);); (121, MPI, MPI_Allreduce(&tmp_error, &old_error, 1, MPI_INT, MPI_MIN, fd->comm);); (285, MPI, MPI_Type_size_x(datatype,", "label": "(173, MPI, MPI_Allreduce(&tmp_error, error_code, 1, MPI_INT, MPI_MAX, fd->comm);); (47, MPI, MPI_Comm_size(fd->comm, &nprocs);); (75, MPI, MPI_Allgather(&end_offset, 1, ADIO_OFFSET, end_offsets, 1, ADIO_OFFSET, fd->comm);); (73, MPI, MPI_Allgather(&start_offset, 1, ADIO_OFFSET, st_offsets, 1, ADIO_OFFSET, fd->comm);); (45, MPI, MPI_Comm_rank(fd->comm, &myrank);); (192, MPI, MPI_Type_size_x(datatype, &size);)"}
{"predict": "(15, MPI, MPI_Barrier( grid3d->comm );); (8, MPI, MPI_Allreduce( &SCT->commVolFactor, &cvolFactor, 1, MPI_DOUBLE, MPI_SUM, grid3d->comm );); (8, MPI, MPI_Allreduce( &SCT->commVolRed, &cvolRed, 1, MPI_DOUBLE, MPI_SUM, grid3d->comm );)", "label": "(4, MPI, MPI_Reduce( &SCT->commVolFactor, &cvolFactor,  1, MPI_DOUBLE, MPI_SUM, 0, grid3d->comm );); (5, MPI, MPI_Reduce( &SCT->commVolRed, &cvolRed,  1, MPI_DOUBLE, MPI_SUM, 0, grid3d->comm );)"}
{"predict": "(51, CUDA, CUDA_CHECK(cudaStreamCreate(&stream));); (89, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (1, MPI, MPI_Init(&argc, &argv);); (100, MPI, MPI_Finalize();); (1, MPI, MPI_Comm_size(mpi_comm, &size);); (96, CUDA, CUDA_CHECK(cudaFree(nullptr));); (94, CUDA, CUDA_CHECK(cudaDeviceReset());); (-3, MPI, MPI_Comm_rank(mpi_comm, &rank);); (78, CUDA, CUDA_CHECK(cudaGetLastError());); (91, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (5, CUDA, CUDA_CHECK(cudaGetDeviceCount(&ndevices));)", "label": "(10, CUDA, CUDA_CHECK(cudaGetDeviceCount(&ndevices));); (54, CUDA, CUDA_CHECK(cudaStreamCreate(&stream));); (88, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (8, CUDA, CUDA_CHECK(cudaSetDevice(rank % ndevices));); (-2, MPI, MPI_Init(&argc, &argv);); (113, MPI, MPI_Finalize();); (1, MPI, MPI_Comm_size(mpi_comm, &size);); (103, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (78, CUDA, CUDA_CHECK(cudaGetLastError());); (-3, MPI, MPI_Comm_rank(mpi_comm, &rank);)"}
{"predict": "(11, TBB, tbb::task_group_context g;)", "label": "(5, TBB, if(tbb::is_current_task_group_canceling()) return;)"}
{"predict": "(14, CUDA, CUDASimulation cudaSimulation(model);)", "label": "(14, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(5, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (6, TBB, tbb::tick_count t1 = tbb::tick_count::now();)", "label": "(4, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (5, TBB, tbb::tick_count t1 = tbb::tick_count::now();)"}
{"predict": "(11, MPI, MPI_Init(&argc, &argv);); (45, MPI, MPI_Finalize();); (10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (36, MPI, MPI_Recv(recvbuf.data(), block_size, MPI_INT, 0, rank, MPI_COMM_WORLD, &status);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (31, MPI, MPI_Send(sendbuf.data()+i*block_size, block_size, MPI_INT, i, i, MPI_COMM_WORLD);); (25, MPI, MPI_Send(sendbuf.data()+block_size*i, block_size, MPI_INT, 0, rank, MPI_COMM_WORLD);); (16, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)", "label": "(11, MPI, MPI_Init(&argc, &argv);); (44, MPI, MPI_Finalize();); (10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (35, MPI, MPI_Recv(recvbuf.data(), block_size, MPI_INT, 0, rank, MPI_COMM_WORLD, &status);); (26, MPI, MPI_Send(&sendbuf[i*block_size], block_size, MPI_INT, i, i, MPI_COMM_WORLD);); (20, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(23, MPI, MPI_Allreduce(gFrstCost, gFrstCostAcc, numForests, MPI_DOUBLE, MPI_SUM, grid3d->zscp.comm);)", "label": "(23, MPI, MPI_Reduce(gFrstCost, gFrstCostAcc, numForests, MPI_DOUBLE, MPI_SUM, 0, grid3d->zscp.comm);)"}
{"predict": "(17, MPI, MPI_Finalize();); (2, MPI, MPI_Init(&argc,&argv);)", "label": "(17, MPI, MPI_Finalize();); (2, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(11, CUDA, __shared__ float u_[_N_];); (11, CUDA, __shared__ float r[_N_], k[_N_], v[_N_], w_[_N_], gy[_N_];)", "label": "(11, CUDA, __shared__ float u_[_N_];); (11, CUDA, __shared__ float r[_N_], k[_N_], v[_N_], w_[_N_], gy[_N_];)"}
{"predict": "(12, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(12, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);)", "label": "(7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);)"}
{"predict": "(2, TBB, tbb::enumerable_thread_specific<double> test_locals;); (6, TBB, tbb::task_scheduler_init init(nThreads);)", "label": "(3, TBB, tbb::enumerable_thread_specific<double> test_locals;); (13, TBB, REMARK(\"%d threads, work ratio %d per cent, time %g\", nThreads, csWorkRatio, (tbb::tick_count::now() - t0).seconds());); (0, TBB, tbb::task_scheduler_init init(tbb::task_scheduler_init::deferred);); (5, TBB, tbb::tick_count t0;); (6, TBB, t0 = tbb::tick_count::now();)"}
{"predict": "(43, CUDA, CUDA_CHECK(cudaGetLastError());)", "label": "(43, CUDA, CSC(cudaGetLastError(), STATUS_FAILURE);)"}
{"predict": "(3, MPI, MPI_Init(&argc, &argv);); (105, MPI, MPI_Finalize();); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(2, MPI, MPI_Init(&argc, &argv);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (105, MPI, MPI_Finalize();); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(7, TBB, adder_node outer_node(g, hidden);); (13, TBB, tbb::flow::make_edge(inner_node2, tbb::flow::input_port<1>(outer_node));); (2, TBB, tbb::flow::broadcast_node<int> input(g);); (3, TBB, tbb::flow::make_edge(input, tbb::flow::input_port<0>(outer_node));); (3, TBB, tbb::flow::make_edge(input, tbb::flow::input_port<1>(outer_node));); (14, TBB, tbb::flow::make_edge(outer_node, inner_node3);); (10, TBB, tbb::flow::make_edge(inner_node1, tbb::flow::input_port<0>(outer_node));); (2, TBB, tbb::flow::make_edge(inner_node1, tbb::flow::input_port<1>(outer_node));); (3, TBB, tbb::flow::make_edge(inner_node3, output);); (-4, TBB, tbb::flow::broadcast_node<int> output(g);); (-7, TBB, tbb::flow::graph g;)", "label": "(38, TBB, CHECK_MESSAGE( (tbb::flow::output_port<0>(outer_node).try_get(out) == output.try_get(out)), \"output from outer_node does not match output from graph\");); (13, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<1>(outer_node) == &tbb::flow::input_port<1>(inner_node1)), \"input port 1 of inner_node1 not bound to input port 1 in outer_node\");); (0, TBB, tbb::flow::composite_node<std::tuple<int, int>, std::tuple<int> > outer_node(g);); (8, TBB, outer_node.set_external_ports(base_type::input_ports_type(tbb::flow::input_port<0>(inner_node1), tbb::flow::input_port<1>(inner_node1)), base_type::output_ports_type(tbb::flow::output_port<0>(inner_node3)));); (-1, TBB, typedef tbb::flow::composite_node<std::tuple<int, int>, std::tuple<int> > base_type;); (18, TBB, tbb::flow::make_edge(inner_node2, tbb::flow::input_port<0>(inner_node3));); (18, TBB, tbb::flow::make_edge(inner_node2, tbb::flow::input_port<1>(inner_node3));); (13, TBB, tbb::flow::make_edge(inner_node1, tbb::flow::input_port<0>(inner_node2));); (-7, TBB, tbb::flow::graph g;); (6, TBB, CHECK_MESSAGE( (&tbb::flow::output_port<0>(outer_node) == &tbb::flow::output_port<0>(inner_node3)), \"output port 0 of inner_node3 not bound to output port 0 in outer_node\");); (11, TBB, tbb::flow::make_edge(inner_node1, tbb::flow::input_port<1>(inner_node2));); (2, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<0>(outer_node) == &tbb::flow::input_port<0>(inner_node1)), \"input port 0 of inner_node1 not bound to input port 0 in outer_node\");); (-8, TBB, tbb::flow::broadcast_node<int> input(g);); (-8, TBB, tbb::flow::queue_node<int> output(g);)"}
{"predict": "(16, CUDA, CudaFile aCudaFile( aReader, GPUSPHOptions(), aTemplDir );)", "label": "(16, CUDA, CudaFile aCudaFile( aReader, GPUSPHOptions(), aTemplDir );)"}
{"predict": "(49, OpenMP, #pragma omp target teams distribute simd safelen (2), num_teams (3)); (14, OpenMP, #pragma omp target teams distribute simd safelen (2+2))); (20, OpenMP, #pragma omp target teams distribute simd safelen (foobool(1) > 0? 1 : 2)); (14, OpenMP, #pragma omp target teams distribute simd safelen (4); (30, OpenMP, #pragma omp target teams distribute simd safelen (S1)); (2, OpenMP, #pragma omp target teams distribute simd safelen ()); (23, OpenMP, #pragma omp target teams distribute simd safelen (foobool(argc)), safelen (true), safelen (-5)); (31, OpenMP, #pragma omp target teams distribute simd safelen (foobool(argc)), safelen (argv[1]=2)); (4, OpenMP, #pragma omp target teams distribute simd safelen (2))); (-5, OpenMP, #pragma omp target teams distribute simd safelen); (14, OpenMP, #pragma omp target teams distribute simd safelen (foobool(argc)), safelen (argc)); (42, OpenMP, #pragma omp target teams distribute simd safelen(safelen(tmain<int, char, -1, -2>(argc, argv)); (-8, OpenMP, #pragma omp target teams distribute simd safelen (); (4, OpenMP, #pragma omp target teams distribute simd safelen (foobool(argc)))", "label": "(29, OpenMP, #pragma omp target teams distribute simd safelen (foobool(argc)), safelen (true), safelen (-5)); (46, OpenMP, #pragma omp target teams distribute simd safelen(safelen(tmain<int, char, -1, -2>(argc, argv)); (-1, OpenMP, #pragma omp target teams distribute simd safelen); (19, OpenMP, #pragma omp target teams distribute simd safelen (foobool(1) > 0? 1 : 2)); (9, OpenMP, #pragma omp target teams distribute simd safelen (4); (0, OpenMP, #pragma omp target teams distribute simd safelen (); (27, OpenMP, #pragma omp target teams distribute simd safelen (S1)); (10, OpenMP, #pragma omp target teams distribute simd safelen (2+2))); (1, OpenMP, #pragma omp target teams distribute simd safelen ())"}
{"predict": "(11, OpenMP, #pragma omp parallel for if(num_bins > OMP_LIMIT_COMPUTELIGHT))", "label": "(11, OpenMP, #pragma omp parallel for reduction(+ : entropy))"}
{"predict": "(30, MPI, MPI_Bcast( work, diag_len[p]*nsupc, MPI_FLOAT, pkk, grid->comm );); (27, MPI, MPI_Bcast( work, lwork, MPI_FLOAT, pkk, grid->comm );)", "label": "(28, MPI, MPI_Bcast( work, lwork, MPI_FLOAT, pkk, grid->comm );); (29, MPI, MPI_Bcast( work, diag_len[p], MPI_FLOAT, pkk, grid->comm );)"}
{"predict": "(6, MPI, MPI_Comm_dup(MPI_COMM_SELF, &lwgrp_comm_self);)", "label": "(6, MPI, MPI_Comm_dup(MPI_COMM_SELF, &lwgrp_comm_self);)"}
{"predict": "(5, OpenMP, #pragma omp target teams distribute parallel for private(g, g1, sivar))", "label": "(4, OpenMP, #pragma omp target teams distribute parallel for private(g, g1, sivar))"}
{"predict": "0(70, CUDA, cudaDeviceSynchronize();); (26, CUDA, cudaMalloc((void**)&dataC, sizeof(cufftDoubleComplex) * dims[0] * dims[1] * dims[2]);); (30, CUDA, cudaMemcpy(dataC, inputC, sizeof(cufftDoubleComplex) * dims[0] * dims[1] * dims[2], cudaMemcpyHostToDevice);); (47, CUDA, cudaMalloc(&cuMemAlloc, cuBufferSize);); (27, CUDA, cudaMalloc(&cuMemAlloc, sizeof(double) * 2 * dims[0] * dims[1] * dims[2]);); (98, CUDA, cudaFree(dataC);); (48, CUDA, cudaMalloc(&cuMemAlloc, cuBufferSize);); (24, CUDA, cudaMemcpy(dataC, inputC, sizeof(cufftDoubleComplex) * dims[0] * dims[1] * dims[2], cudaMemcpyHostToDevice);); (48, CUDA, cudaMemcpy(dataC, inputC, sizeof(cufftDoubleComplex) * dims[0] * dims[1] * dims[2], cudaMemcpyHostToDevice);); (96, CUDA, cudaFree(cuMemAlloc);); (44, CUDA, cudaMalloc(&cuMemAlloc, sizeof(double) * 2 * dims[0] * dims[1] * dims[2]);); (44, CUDA, cudaMemcpy(dataC, inputC, sizeof(cufftDoubleComplex) * dims[0] * dims[1] * dims[2], cudaMemcpyHostToDevice);); (47, CUDA, cudaMemcpy(dataC, inputC, sizeof(cufftDoubleComplex) * dims[0] * dims[1] * dims[2], cudaMemcpyHostToDevice);); (44, CUDA, cudaMemcpy(dataC, inputC, sizeof(cufftDoubleComplex) * dims[0] * dims[1] * dims[2], cudaMemcpyHostToDevice);); (91, CUDA, cudaFree(cuMemAlloc);); (45, CUDA, cudaMemcpy(dataC, inputC, sizeof(cufftDoubleComplex) * dims[0] * dims[1] * dims[2], cudaMemcpyHostToDevice););", "label": "(27, CUDA, cudaMalloc((void**)&dataC, sizeof(cufftDoubleComplex) * dims[0] * dims[1] * dims[2]);); (55, CUDA, cudaDeviceSynchronize();); (27, CUDA, cudaMemcpy(dataC, inputC, sizeof(cufftDoubleComplex) * dims[0] * dims[1] * dims[2], cudaMemcpyHostToDevice);); (78, CUDA, cudaFree(dataC);); (1, CUDA, cudaSetDevice(device_id);)"}
{"predict": "(7, CUDA, CUDA_CHECK(cudaMemGetInfo(&free, &total));); (5, CUDA, CUDA_CHECK(cudaMemGetInfo(&prevFree, nullptr));)", "label": "(6, CUDA, cudaCheck(cudaMemGetInfo(&free, &total));)"}
{"predict": "(5, CUDA, cudaMalloc(&curandStates, sizeof(curandState) * batchSize);); (48, CUDA, cudaFree(curandStates);)", "label": "(48, CUDA, cudaFree(curandStates);); (4, CUDA, cudaMalloc(&curandStates, sizeof(curandState_t) * 2 * batchSize);)"}
{"predict": "(16, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (22, MPI, MPI_Error_string(ierr, str, &slen);); (28, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_ARE_FATAL);)", "label": "(29, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_ARE_FATAL);); (23, MPI, MPI_Error_string(ierr, str, &slen);); (14, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(3, TBB, tbb::flow::graph g;)", "label": "(40, TBB, tbb::flow::make_edge(src, gamma);); (40, TBB, tbb::flow::make_edge(gamma, tint);); (40, TBB, tbb::flow::make_edge(tint, write);); (0, TBB, tbb::flow::graph g;)"}
{"predict": "(2, CUDA, extern __shared__ unsigned sidxs[];)", "label": "(3, CUDA, __shared__ unsigned sidxs[nTPB];)"}
{"predict": "(2, MPI, MPI_Init(&argc, &argv);); (41, MPI, MPI_Finalize();); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&me);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD,&np);); (31, MPI, MPI_Allgatherv(&mysizeint,1,MPI_INT,counts.data(),1,MPI_INT,MPI_COMM_WORLD);); (32, MPI, MPI_Allgatherv(&mysizeint,1,MPI_INT,out.data(),1,MPI_CHAR,MPI_COMM_WORLD);); (12, MPI, MPI_Allreduce(&mysize, &totalsize, 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);)", "label": "(19, MPI, MPI_Reduce(&mysize, &totalsize, 1, MPI_INT64_T, MPI_SUM, 0, MPI_COMM_WORLD);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&me);); (0, MPI, MPI_Init(&argc, &argv);); (23, MPI, MPI_Gather(&mysizeint, 1, MPI_INT, &(counts[0]), 1, MPI_INT, 0, MPI_COMM_WORLD);); (37, MPI, MPI_Finalize();); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD,&np);); (29, MPI, MPI_Gatherv(in.c_str(), mysizeint, MPI_CHAR, &(out[0]), &(counts[0]), &(displs[0]), MPI_CHAR, 0, MPI_COMM_WORLD);)"}
{"predict": "(20, OpenMP, #pragma omp teams distribute parallel for order(none); (16, OpenMP, #pragma omp teams distribute parallel for order(concurrent)); (9, OpenMP, #pragma omp teams distribute parallel for order(); (0, OpenMP, #pragma omp teams distribute parallel for order); (6, OpenMP, #pragma omp teams distribute parallel for order()); (16, OpenMP, #pragma omp teams distribute parallel for order(concurrent)); (-5, OpenMP, #pragma omp target)", "label": "(18, OpenMP, #pragma omp teams distribute parallel for order(concurrent); (1, OpenMP, #pragma omp teams distribute parallel for order); (20, OpenMP, #pragma omp teams distribute parallel for order(concurrent)); (-2, OpenMP, #pragma omp target); (10, OpenMP, #pragma omp teams distribute parallel for order(none); (5, OpenMP, #pragma omp teams distribute parallel for order()"}
{"predict": "(13, CUDA, fprintf(stderr, \"CUDA kernel failed : %s\\n\", cudaGetErrorString(err));); (10, CUDA, auto err = cudaGetLastError();)", "label": "(13, CUDA, fprintf(stderr, \"CUDA kernel failed : %s\\n\", cudaGetErrorString(err));); (10, CUDA, auto err = cudaGetLastError();)"}
{"predict": "(9, OpenMP, omp_autotune_init();); (77, OpenMP, omp_set_num_threads(omp_get_num_procs());); (8, OpenMP, omp_set_dynamic(0);); (13, OpenMP, omp_set_num_threads(1);)", "label": "(14, OpenMP, john_omp_fallback(argv);); (18, MPI, mpi_setup(argc, argv);); (78, OpenMP, john_omp_maybe_adjust_or_fallback(argv);); (6, OpenMP, john_omp_init();)"}
{"predict": "(40, MPI, MPI_Comm_split(MPI_COMM_WORLD, randval, i, &newcomm);); (24, MPI, startTime = MPI_Wtime();); (22, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (38, MPI, MPI_Comm_free(&newcomm);); (25, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(44, MPI, MPI_Comm_split(MPI_COMM_WORLD, MPI_UNDEFINED, rank, &newcomm);); (21, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (40, MPI, MPI_Comm_free(&newcomm);); (21, MPI, startTime = MPI_Wtime();); (24, MPI, double rate = MPI_Wtime() - startTime;); (36, MPI, MPI_Comm_split(MPI_COMM_WORLD, 1, rank, &newcomm);)"}
{"predict": "(56, CUDA, flamegpu::CUDASimulation cudaSimulation(model, argc, argv);)", "label": "(56, CUDA, flamegpu::CUDASimulation  cudaSimulation(model, argc, argv);)"}
{"predict": "(2, TBB, using MyTable = oneapi::tbb::concurrent_hash_map<MyKey,MyData>;); (3, TBB, using YourTable1 = oneapi::tbb::concurrent_hash_map<MyKey,MyData>;); (3, TBB, using YourTable2 = oneapi::tbb::concurrent_hash_map<MyKey,MyData>;)", "label": "(36, TBB, typedef tbb::concurrent_hash_map<MyKey,MyData2,YourHashCompare> YourTable2;); (34, TBB, typedef tbb::concurrent_hash_map<MyKey,MyData2,YourHashCompare,MyAllocator> YourTable1;)"}
{"predict": "(32, MPI, mpirc = MPI_Pack((void*) header, 1, MPI_UINT64_T, header_packed, header_size, MPI_BYTE, 0, MPI_COMM_WORLD);); (104, MPI, mpirc = MPI_Recv(user_buf, user_buf_size, MPI_BYTE, 0, 0, MPI_COMM_WORLD, &status);); (124, MPI, mpirc = MPI_Type_get_extent(groups->dt, &lb_group, &extent_group);); (25, MPI, MPI_Bcast(header, header_size, MPI_BYTE, 0, MPI_COMM_WORLD);); (18, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (81, MPI, mpirc = MPI_Type_get_extent(users->dt, &lb_user, &extent_user);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (63, MPI, MPI_Exscan(&count, &offset, 1, MPI_UINT64_T, MPI_SUM, MPI_COMM_WORLD);); (76, MPI, mpirc = MPI_File_set_view(fh, disp, MPI_BYTE, users->dt, datarep, MPI_INFO_NULL);); (9, MPI, MPI_Error_string(mpirc, mpierrstr, &mpierrlen);); (113, MPI, mpirc = MPI_Send(group_buf, group_buf_size, MPI_BYTE, 0, 0, MPI_COMM_WORLD);); (7, MPI, int mpirc = MPI_File_set_view(fh, disp, MPI_BYTE, MPI_UINT64_T, datarep, MPI_INFO_NULL);); (11, MPI, MPI_Error_string(mpirc, mpierrstr, &mpierrlen);); (64, MPI, mpirc = MPI_File_read_at(fh, offset, users->buf, (int)users->count, users->dt, &status);); (107, MPI, MPI_Type_commit(&(groups->dt));); (13, MPI, MPI_Bcast(", "label": "(28, MPI, MPI_Error_string(mpirc, mpierrstr, &mpierrlen);); (142, MPI, MPI_Bcast(groups->buf, (int)groups->count, groups->dt, 0, MPI_COMM_WORLD);); (96, MPI, mpirc = MPI_File_read_at(fh, 0, user_buf, user_buf_size, MPI_BYTE, &status);); (132, MPI, mpirc = MPI_File_read_at(fh, 0, group_buf, group_buf_size, MPI_BYTE, &status);); (102, MPI, MPI_Bcast(users->buf, (int)users->count, users->dt, 0, MPI_COMM_WORLD);); (15, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (28, MPI, mpirc = MPI_File_read_at(fh, 0, header_packed, header_size, MPI_BYTE, &status);); (41, MPI, MPI_Bcast(header, 6, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (60, MPI, MPI_Exscan(&count, &offset, 1, MPI_UINT64_T, MPI_SUM, MPI_COMM_WORLD);); (12, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (16, MPI, int mpirc = MPI_File_set_view(fh, disp, MPI_BYTE, MPI_BYTE, datarep, MPI_INFO_NULL);); (69, MPI, MPI_Type_get_extent(users->dt, &lb_user, &extent_user);); (105, MPI, MPI_Type_get_extent(groups->dt, &lb_group, &extent_group);); (190, MPI, mpirc = MPI_File_read_at(fh, read_offset, buf, read_size, MPI_BYTE, &status);); (74, MPI, mpirc = MPI_File_set_view(fh, disp, MPI_BYTE, MPI_BYTE, datarep, MPI_INFO_NULL);); (159, MPI, MPI_Allreduce(&iters, &all_iters, 1, MPI_UINT64_T, MPI_MAX, MPI_COMM_WORLD);)"}
{"predict": "(22, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (22, MPI, MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);)", "label": "(22, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (22, MPI, MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);)"}
{"predict": "(8, CUDA, CHECK(cudaFree(m_output_boxes_device));); (9, CUDA, CHECK(cudaFree(m_output_scores_device));); (10, CUDA, CHECK(cudaFree(m_output_classes_device));); (5, CUDA, CHECK(cudaFree(m_output_num_device));)", "label": "(8, CUDA, CHECK(cudaFree(m_output_boxes_device));); (9, CUDA, CHECK(cudaFree(m_output_classes_device));); (2, CUDA, CHECK(cudaFree(m_input_resize_device));); (0, CUDA, CHECK(cudaFree(m_input_src_device));); (3, CUDA, CHECK(cudaFree(m_output_num_device));); (0, CUDA, CHECK(cudaFree(m_input_rgb_device));); (3, CUDA, CHECK(cudaFree(m_output_scores_device));)"}
{"predict": "(18, CUDA, extern __shared__ bool kept_bboxinfo_flag[];)", "label": "(19, CUDA, extern __shared__ bool kept_bboxinfo_flag[];)"}
{"predict": "(9, MPI, MPI_Comm_rank(comm_world, &rank_world);)", "label": "(9, MPI, MPI_Comm_rank(comm_world, &rank_world);)"}
{"predict": "(39, MPI, MPI_Request_free(&reqs[i]);)", "label": "(39, MPI, MPI_Request_free(&reqs[i]);)"}
{"predict": "(11, MPI, MPI_Comm_size(comm, &npes);)", "label": "(11, MPI, MPI_Comm_size(comm, &npes);)"}
{"predict": "(14, CUDA, cudaMalloc(&d_A, DSIZE*sizeof(float));); (15, CUDA, cudaMemcpy(d_A, h_A, DSIZE*sizeof(float), cudaMemcpyHostToDevice);); (33, CUDA, cudaFree(d_A);); (16, CUDA, cudaCheckErrors(\"cudaMemcpy H2D failure\");); (32, CUDA, cudaFree(d_C);); (30, CUDA, cudaFree(d_B);); (23, CUDA, cudaMemcpy(h_C, d_C, DSIZE*sizeof(float), cudaMemcpyDeviceToHost);); (10, CUDA, cudaMemcpy(d_B, h_B, DSIZE*sizeof(float), cudaMemcpyHostToDevice);); (7, CUDA, cudaMalloc(&d_B, DSIZE*sizeof(float));); (7, CUDA, cudaMalloc(&d_C, DSIZE*sizeof(float));)", "label": "(15, CUDA, cudaMemcpy(d_A, h_A, DSIZE*sizeof(float), cudaMemcpyHostToDevice);); (20, CUDA, cudaCheckErrors(\"kernel launch failure\");); (8, CUDA, cudaMalloc(&d_A, DSIZE*sizeof(float));)"}
{"predict": "(34, MPI, MPI_Finalize();); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &comm_size);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (8, MPI, vtkh::SetMPICommHandle(MPI_Comm_c2f(MPI_COMM_WORLD));)", "label": "(35, MPI, MPI_Finalize();); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &comm_size);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (8, MPI, vtkh::SetMPICommHandle(MPI_Comm_c2f(MPI_COMM_WORLD));)"}
{"predict": "(32, CUDA, cudaFree(devPtrA);); (18, CUDA, result = cudaMalloc((void**)&devPtrA, M*N*sizeof(*a));)", "label": "(19, CUDA, result = cudaMalloc((void**)&devPtrA, M*N*sizeof(*a));); (30, CUDA, cudaFree(devPtrA);)"}
{"predict": "(6, CUDA, __shared__ uint s_permutations[992];)", "label": "(6, CUDA, __shared__ uint s_permutations[160];)"}
{"predict": "(116, MPI, MPI_Bcast( &berr[j], 1, MPI_FLOAT, 0,  grid3d->zscp.comm);); (109, MPI, MPI_Bcast( &berr[j], 1, MPI_FLOAT, 0,  grid->comm);)", "label": "(110, MPI, MPI_Allreduce( &s, &berr[j], 1, MPI_FLOAT, MPI_MAX, grid->comm );); (111, MPI, MPI_Bcast( &berr[j], 1, MPI_FLOAT, 0,  grid3d->zscp.comm);)"}
{"predict": "(20, TBB, tbb::internal::segmented_iterator<nested_vec, std::pair<int,inner_container> > my_si(my_vec);); (61, TBB, tbb::internal::segmented_iterator<nested_vec, std::pair<int,inner_container> > my_csi(my_vec);)", "label": "(62, TBB, tbb::internal::segmented_iterator<nested_vec, const T> my_csi(my_vec);); (19, TBB, tbb::internal::segmented_iterator<nested_vec, T> my_si(my_vec);)"}
{"predict": "(4, CUDA, __shared__ uchar3 s_img[BLOCK_SIZE];)", "label": "(4, CUDA, __shared__ uint4 s_img[TPB];)"}
{"predict": "(45, MPI, MPI_Open_port(port_info, port);); (45, MPI, MPI_Send(port, PORT_SIZE, MPI_CHAR, members[i], 0, MPI_COMM_WORLD);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (45, MPI, MPI_Intercomm_merge(pgroup_new, 0 /* LOW */, &pgroup);); (55, MPI, MPI_Send(port, PORT_SIZE, MPI_CHAR, members[0], 0, MPI_COMM_WORLD);); (37, MPI, MPI_Comm_dup(MPI_COMM_SELF, &pgroup_new);); (46, MPI, MPI_Comm_free(&pgroup_new);); (48, MPI, MPI_Comm_free(&pgroup_old);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (57, MPI, MPI_Recv(port, PORT_SIZE, MPI_CHAR, members[0], 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (62, MPI, MPI_Comm_dup(pgroup, &pgroup_new);)", "label": "(49, MPI, MPI_Comm_accept(port, MPI_INFO_NULL, 0, pgroup, &pgroup_new);); (53, MPI, MPI_Comm_free(&pgroup_old);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (65, MPI, MPI_Comm_connect(port, MPI_INFO_NULL, 0, MPI_COMM_SELF, &pgroup_new);); (43, MPI, MPI_Send(port, PORT_SIZE, MPI_CHAR, members[i], 0, MPI_COMM_WORLD);); (36, MPI, MPI_Open_port(port_info, port);); (45, MPI, MPI_Comm_free(&pgroup_new);); (49, MPI, MPI_Close_port(port);); (67, MPI, MPI_Comm_connect(port, MPI_INFO_NULL, 0, pgroup, &pgroup_new);)"}
{"predict": "(15, MPI, MPI_Group_translate_ranks(group, ranks.data(), worldRanks.data());); (16, MPI, MPI_Group_free(&group);); (5, MPI, MPI_Comm_group(comm, &group);); (7, MPI, MPI_Group_size(group, &groupSize);); (4, MPI, MPI_Comm_group(MPI_COMM_WORLD, &worldGroup);); (16, MPI, MPI_Group_free(&worldGroup);); (9, MPI, MPI_Barrier(comm);)", "label": "(10, MPI, MPICHECK(MPI_Group_size(group, &groupSize));); (6, MPI, MPICHECK(MPI_Comm_group(comm, &group));); (13, MPI, MPICHECK(MPI_Group_translate_ranks(group, groupSize, ranks.data(), worldGroup, worldRanks.data()));); (13, MPI, MPICHECK(MPI_Group_free(&group));); (2, MPI, MPICHECK(MPI_Comm_group(MPI_COMM_WORLD, &worldGroup));); (12, MPI, MPICHECK(MPI_Group_free(&worldGroup));)"}
{"predict": "(15, OpenMP, #pragma omp taskloop simd private(argc); (35, OpenMP, #pragma omp taskloop simd private(h)); (31, OpenMP, #pragma omp taskloop simd private(argv[1])); (47, OpenMP, #pragma omp taskloop simd private(j)); (20, OpenMP, #pragma omp taskloop simd private(argc > 0? argv[1] : argv[2])); (37, OpenMP, #pragma omp parallel); (21, OpenMP, #pragma omp taskloop simd private(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (36, OpenMP, #pragma omp taskloop simd shared(i)); (2, OpenMP, #pragma omp taskloop simd private()); (1, OpenMP, #pragma omp taskloop simd private(); (36, OpenMP, #pragma omp parallel shared(i)); (42, OpenMP, #pragma omp taskloop simd private(i)); (37, OpenMP, #pragma omp parallel private(i)); (3, OpenMP, #pragma omp taskloop simd private(argc,); (4, OpenMP, #pragma omp taskloop simd private(argc)", "label": "(33, OpenMP, #pragma omp taskloop simd private(argv[1])); (38, OpenMP, #pragma omp taskloop simd private(h)); (13, OpenMP, #pragma omp taskloop simd private(argc); (52, OpenMP, #pragma omp parallel shared(i)); (52, OpenMP, #pragma omp parallel private(i)); (1, OpenMP, #pragma omp taskloop simd private); (15, OpenMP, #pragma omp taskloop simd private(argc > 0? argv[1] : argv[2])); (38, OpenMP, #pragma omp parallel); (41, OpenMP, #pragma omp taskloop simd private(i)); (21, OpenMP, #pragma omp taskloop simd private(a, b)); (32, OpenMP, #pragma omp taskloop simd shared(i)); (13, OpenMP, #pragma omp taskloop simd private(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (15, OpenMP, #pragma omp taskloop simd private(S1)); (5, OpenMP, #pragma omp taskloop simd private(argc,); (-5, OpenMP, #pragma omp taskloop simd private(); (-3, OpenMP, #pragma omp taskloop simd private()); (20, OpenMP, #pragma omp taskloop simd private(e, g, z)); (40, OpenMP, #pragma omp taskloop simd private(j))"}
{"predict": "(6, OpenMP, std::vector<BabelFlow::Payload> comp_outputs( 1 );)", "label": "(6, OpenMP, std::vector<BabelFlow::Payload> comp_outputs( 1 );)"}
{"predict": "(43, MPI, printf(\" MPI library version      : %s\\n\", MPI_VERSION);)", "label": "(43, MPI, printf(\" MPI library version      : %s\\n\", mpi_version());)"}
{"predict": "(16, CUDA, checkCudaErrors(cudaFree(workload->d_data));); (4, CUDA, checkCudaErrors(cudaSetDevice(workload->cudaDeviceID));); (14, CUDA, checkCudaErrors(cudaFreeHost(workload->h_data));)", "label": "(15, CUDA, checkCudaErrors(cudaFree(workload->d_data));); (15, CUDA, checkCudaErrors(cudaFreeHost(workload->h_data));); (3, CUDA, checkCudaErrors(cudaSetDevice(workload->cudaDeviceID));); (14, CUDA, checkCudaErrors(cudaStreamDestroy(workload->stream));)"}
{"predict": "(9, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)", "label": "(9, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(6, CUDA, thrust::device_vector<int> d_num_out(1, 0);); (5, CUDA, thrust::device_vector<bool> d_flags(num_items, false);); (3, CUDA, thrust::host_vector<bool> h_flags(d_flags);); (19, CUDA, CubDebugExit(cudaMemcpy(&h_num_out, d_num_out.data(), sizeof(int), cudaMemcpyDeviceToHost));); (28, CUDA, thrust::device_vector<bool> selection(num_selected, true);); (36, CUDA, thrust::fill(data.begin(), data.end(), false);); (24, CUDA, thrust::device_vector<bool> data(d_flags);); (36, CUDA, thrust::shuffle(data.begin(), data.end(), g);); (36, CUDA, thrust::exclusive_scan(data.begin(), data.end(), data.begin());); (4, CUDA, CubDebugExit(cudaMalloc(&d_tmp_storage, tmp_storage_size));); (36, CUDA, thrust::device_vector<bool> output(num_selected);); (34, CUDA, thrust::copy(data.begin(), data.end(), output.begin());); (46, CUDA, thrust::host_vector<bool> h_output(output);); (43, CUDA, thrust::device_vector<bool> output_perm(output);); (45, CUDA, thrust::sort(output_perm.begin(), output_perm.end());); (48, CUDA, cudaFree(d_tmp_storage);)", "label": "(10, CUDA, int *d_num_out = thrust::raw_pointer_cast(num_out.data());); (10, CUDA, int *d_data = thrust::raw_pointer_cast(data.data());); (24, CUDA, d_tmp_storage = thrust::raw_pointer_cast(tmp_storage.data());); (32, CUDA, thrust::fill(flags.begin(), flags.begin() + num_selected, true);); (3, CUDA, thrust::device_vector<bool> flags(num_items);); (20, CUDA, thrust::device_vector<char> tmp_storage(tmp_storage_size);); (0, CUDA, thrust::device_vector<int> data(num_items);); (29, CUDA, thrust::fill(flags.begin() + num_selected, flags.end(), false);); (26, CUDA, thrust::sequence(data.begin(), data.end());); (28, CUDA, thrust::shuffle(flags.begin(), flags.end(), g);); (-5, CUDA, thrust::device_vector<int> num_out(1);); (36, CUDA, cudaMemcpy(&h_num_out, d_num_out, sizeof(int), cudaMemcpyDeviceToHost);); (0, CUDA, bool *d_flags = thrust::raw_pointer_cast(flags.data());)"}
{"predict": "(26, OpenCL, status = clEnqueueSetKernelArg(queue, kernel, 1, sizeof(cl_mem), &result_buf);)", "label": "(12, OpenCL, status = clSetKernelArg(kernel, 0, sizeof(cl_mem), &test_buf);); (20, OpenCL, status = clSetKernelArg(kernel, 0, sizeof(cl_mem), NULL);)"}
{"predict": "(71, CUDA, checkCudaErrors(cudaEventRecord(stop));); (61, CUDA, checkCudaErrors(cudaEventCreate(&start));); (72, CUDA, checkCudaErrors(cudaEventSynchronize(stop));); (89, CUDA, checkCudaErrors(cudaFreeHost(h_partial_sums));); (89, CUDA, checkCudaErrors(cudaFreeHost(h_result));); (64, CUDA, checkCudaErrors(cudaEventElapsedTime(&inc, start, stop));); (58, CUDA, checkCudaErrors(cudaEventRecord(start));); (46, CUDA, checkCudaErrors(cudaMemset(d_partial_sums, 0, partial_sz));); (42, CUDA, checkCudaErrors(cudaMalloc((void **)&d_partial_sums, partial_sz));); (66, CUDA, checkCudaErrors(cudaMemcpy(h_result, d_data, sz, cudaMemcpyDeviceToHost));); (60, CUDA, checkCudaErrors(cudaMemcpy(d_data, h_data, sz, cudaMemcpyHostToDevice));); (4, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, cuda_device));); (54, CUDA, checkCudaErrors(cudaEventCreate(&stop));); (80, CUDA, checkCudaErrors(cudaFree(d_data));); (80, CUDA, checkCudaErrors(cudaFree(d_partial_sums));); (34, CUDA, checkCudaErrors(cudaMalloc((void **)&d_data, sz));); (75, CUDA, checkCudaErrors(cudaFreeHost(h_data));); (0, CUDA, checkCudaErrors(cudaSetDevice(cuda_device));); (71, CUDA, checkCudaErrors(cudaEventDestroy(start));); (71, CUDA, checkCudaErrors(cudaEventDestroy(stop));); (24, CUDA, checkCudaErrors(cudaMallocHost((void **)&h_result, sz));); (22, CUDA, checkCudaErrors(cudaMallocHost((void **)&h_partial_sums, partial_sz));); (23, CUDA, checkCudaErrors(cudaMallocHost((void **)&h_data, sz));); (63, CUDA, checkCudaErrors(cudaEventDestroy(stop));)", "label": "(76, CUDA, checkCudaErrors(cudaEventRecord(stop, 0));); (15, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, cuda_device));); (94, CUDA, checkCudaErrors(cudaFree(d_partial_sums));); (57, CUDA, checkCudaErrors(cudaEventCreate(&start));); (57, CUDA, checkCudaErrors(cudaEventCreate(&stop));); (73, CUDA, checkCudaErrors(cudaEventElapsedTime(&inc, start, stop));); (66, CUDA, checkCudaErrors(cudaEventRecord(start, 0));); (70, CUDA, checkCudaErrors(cudaEventSynchronize(stop));); (86, CUDA, checkCudaErrors(cudaFreeHost(h_partial_sums));); (84, CUDA, checkCudaErrors(cudaFreeHost(h_result));); (55, CUDA, checkCudaErrors(cudaMalloc((void **)&d_data, sz));); (84, CUDA, checkCudaErrors(cudaFree(d_data));); (18, CUDA, checkCudaErrors(cudaMallocHost((void **)&h_result, sizeof(int)*n_elements));); (79, CUDA, checkCudaErrors(cudaFreeHost(h_data));); (56, CUDA, checkCudaErrors(cudaMemcpy(d_data, h_data, sz, cudaMemcpyHostToDevice));); (-4, CUDA, cuda_device = findCudaDevice(argc, (const char **)argv);); (65, CUDA, checkCudaErrors(cudaMemcpy(h_result, d_data, sz, cudaMemcpyDeviceToHost));); (50, CUDA, checkCudaErrors(cudaMemset(d_partial_sums, 0, partial_sz));); (11, CUDA, checkCudaErrors(cudaMallocHost((void **)&h_data, sizeof(int)*n_elements));); (-5, CUDA, checkCudaErrors(cudaGetDevice(&cuda_device));); (46, CUDA, checkCudaErrors(cudaMalloc((void **)&d_partial_sums, partial_sz));); (48, CUDA, checkCudaErrors(cudaMallocHost((void **)&h_partial_sums, partial_sz));)"}
{"predict": "(7, CUDA, RAFT_CUDA_TRY(cudaStreamSynchronize(stream));)", "label": "(6, CUDA, RAFT_CUDA_TRY(cudaStreamSynchronize(stream));)"}
{"predict": "(1, OpenMP, #pragma omp target parallel num_threads (); (2, OpenMP, #pragma omp target parallel num_threads ()); (10, OpenMP, #pragma omp target parallel num_threads (argc > 0? argv[1] : argv[2])); (5, OpenMP, #pragma omp target parallel num_threads (argc + argc*2)); (13, OpenMP, #pragma omp target parallel num_threads (num_threads(tmain<int, char, -1, -2>(argc, argv)); (7, OpenMP, #pragma omp target parallel num_threads (S1)); (2, OpenMP, #pragma omp target parallel num_threads (argc))); (-2, OpenMP, #pragma omp target parallel num_threads (argc); (1, OpenMP, #pragma omp target parallel num_threads (argc))); (5, OpenMP, #pragma omp target parallel num_threads (foobool(argc)), num_threads (true), num_threads (-5))", "label": "(1, OpenMP, #pragma omp target teams defaultmap); (2, OpenMP, #pragma omp target teams defaultmap(); (9, OpenMP, #pragma omp target teams defaultmap(tofrom)); (12, OpenMP, #pragma omp target teams defaultmap(tofrom,); (3, OpenMP, #pragma omp target teams defaultmap(tofrom); (4, OpenMP, #pragma omp target teams defaultmap (tofrom:); (13, OpenMP, #pragma omp target teams defaultmap(tofrom, scalar); (-2, OpenMP, #pragma omp target teams defaultmap()); (5, OpenMP, #pragma omp target teams defaultmap(tofrom scalar)); (8, OpenMP, #pragma omp target teams defaultmap (scalar:)"}
{"predict": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(31, MPI, MPI_Comm_rank(parent_comm, &parent_rank);); (28, MPI, MPI_Comm_rank(d->comm, &state->my_rank);); (17, MPI, MPI_Comm_size(d->comm, &ranks_comm);); (30, MPI, MPI_Allgather(&parent_rank, 1, MPI_INT, ranklist, 1, MPI_INT, d->comm);)", "label": "(19, MPI, MPI_Comm_size(d->comm, &ranks_comm);); (28, MPI, MPI_Comm_rank(parent_comm, &parent_rank);); (28, MPI, MPI_Allgather(&parent_rank, 1, MPI_INT, ranklist, 1, MPI_INT, d->comm);)"}
{"predict": "(52, CUDA, CHECK(cudaFree(d_y));); (53, CUDA, CHECK(cudaFree(d_z));); (26, CUDA, CHECK(cudaMalloc((void**)&d_y, M));); (39, CUDA, CHECK(cudaMemcpy(h_z, d_z, M, cudaMemcpyDeviceToHost));); (25, CUDA, CHECK(cudaMalloc((void**)&d_z, M));); (48, CUDA, CHECK(cudaFree(d_x));); (27, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice));); (27, CUDA, CHECK(cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice));); (20, CUDA, CHECK(cudaMalloc((void**)&d_x, M));); (33, CUDA, CHECK(cudaGetLastError());); (43, CUDA, CHECK(cudaFree(d_z));)", "label": "(52, CUDA, CHECK(cudaFree(d_y));); (30, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice));); (51, CUDA, CHECK(cudaFree(d_z));); (39, CUDA, CHECK(cudaMemcpy(h_z, d_z, M, cudaMemcpyDeviceToHost));); (22, CUDA, CHECK(cudaMalloc((void**)&d_x, M));); (22, CUDA, CHECK(cudaMalloc((void**)&d_y, M));); (45, CUDA, CHECK(cudaFree(d_x));); (21, CUDA, CHECK(cudaMalloc((void**)&d_z, M));); (30, CUDA, CHECK(cudaGetLastError());); (30, CUDA, CHECK(cudaDeviceSynchronize());); (22, CUDA, CHECK(cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice));)"}
{"predict": "(37, MPI, MPI_Recv(lnzval, len2, MPI_DOUBLE, layer, jb, grid3d->zscp.comm, &status);); (32, MPI, MPI_Send(lnzval, len2, MPI_DOUBLE, 0, jb, grid3d->zscp.comm);)", "label": "(32, MPI, MPI_Send(lnzval, len2, MPI_DOUBLE, 0, jb, grid3d->zscp.comm);); (36, MPI, MPI_Recv(lnzval, len2, MPI_DOUBLE, layer, jb, grid3d->zscp.comm, &status);)"}
{"predict": "(76, OpenMP, #define OP_GOMP_CRITICAL_CPT(TYPE, OP, FLAG))", "label": "(76, OpenMP, #define OP_GOMP_CRITICAL_CPT(TYPE, OP, FLAG))"}
{"predict": "(100, OpenMP, #pragma omp parallel for); (47, OpenMP, #pragma omp simd)", "label": "(101, OpenMP, dims_t comp_dims(ndims, 1);)"}
{"predict": "(17, OpenMP, #pragma omp target parallel num_threads (argv[1]=2)); (10, OpenMP, #pragma omp target parallel num_threads (argc > 0? argv[1] : argv[2])); (5, OpenMP, #pragma omp target parallel num_threads (argc + k)); (17, OpenMP, #pragma omp target parallel num_threads (num_threads(tbb::task_scheduler_init::default_num_threads))); (13, OpenMP, #pragma omp target parallel num_threads (S1)); (10, OpenMP, #pragma omp target parallel num_threads (foobool(argc)), num_threads (true), num_threads (-5)); (4, OpenMP, #pragma omp target parallel num_threads (argc))); (1, OpenMP, #pragma omp target parallel num_threads (argc))); (14, OpenMP, #pragma omp target parallel redef_num_threads (argc, argc)); (-5, OpenMP, #pragma omp target parallel num_threads ()); (-6, OpenMP, #pragma omp target parallel num_threads (); (1, OpenMP, #pragma omp target parallel num_threads (argc))); (5, OpenMP, #pragma omp target parallel num_threads (foobool(1) > 0? 1 : 2))", "label": "(24, OpenMP, #pragma omp target parallel device (3.14)); (9, OpenMP, #pragma omp target parallel device (argc))); (20, OpenMP, #pragma omp target parallel device (-10u)); (5, OpenMP, #pragma omp target parallel device (argc); (12, OpenMP, #pragma omp target parallel device (k), device (argc+1)); (-3, OpenMP, #pragma omp target parallel device); (6, OpenMP, #pragma omp target parallel device (argc > 0? argv[1] : argv[2])); (11, OpenMP, #pragma omp target parallel device (S1)); (12, OpenMP, #pragma omp target parallel device (-2)); (-3, OpenMP, #pragma omp target parallel device ()); (4, OpenMP, #pragma omp target parallel device (argc + argc)); (-7, OpenMP, #pragma omp target parallel device ()"}
{"predict": "(1, CUDA, __shared__ float cache[threadsPerBlock];)", "label": "(1, CUDA, __shared__ float cache[threadsPerBlock];)"}
{"predict": "(33, CUDA, cudaFree( dptrForce );); (33, CUDA, cudaFree( dptrPosMass );)", "label": "(33, CUDA, cudaFree( dptrPosMass );); (33, CUDA, cudaFree( dptrForce );)"}
{"predict": "(64, OpenMP, double stop = omp_get_wtime();); (12, CUDA, CUDA_RT_CALL(cudaMemset(a, 0, nx * ny * sizeof(float)));); (62, CUDA, CUDA_RT_CALL(cudaFree(a));); (10, CUDA, CUDA_RT_CALL(cudaMemset(a_new, 0, nx * ny * sizeof(float)));); (15, CUDA, CUDA_RT_CALL(cudaDeviceSynchronize());); (17, CUDA, CUDA_RT_CALL(cudaGetLastError());); (3, CUDA, CUDA_RT_CALL(cudaMalloc(&a, nx * ny * sizeof(float)));); (54, CUDA, CUDA_RT_CALL(cudaMemcpy(a_ref_h, a, nx * ny * sizeof(float), cudaMemcpyDeviceToHost));); (39, CUDA, CUDA_RT_CALL(cudaMemset(l2_norm_d, 0, sizeof(float)));); (5, CUDA, CUDA_RT_CALL(cudaMalloc(&a_new, nx * ny * sizeof(float)));); (57, CUDA, CUDA_RT_CALL(cudaFree(l2_norm_d));); (60, CUDA, CUDA_RT_CALL(cudaFree(a_new));); (23, CUDA, CUDA_RT_CALL(cudaMallocHost(&l2_norm_h, sizeof(float)));); (23, CUDA, CUDA_RT_CALL(cudaMemcpyAsync(l2_norm_h, l2_norm_d, sizeof(float), cudaMemcpyDeviceToHost));); (13, CUDA, CUDA_RT_CALL(cudaMemset(l2_norm_d, 0, sizeof(float)));); (27, CUDA, CUDA_RT_CALL(cudaMemcpyAsync(a, a_new, nx * ny * sizeof(float), cudaMemcpyDeviceToDevice));); (22, CUDA, CUDA_RT_CALL(cudaMalloc(&l2_norm_d, sizeof(float)));); (46, CUDA, CUDA_RT_CALL(cudaFreeHost(l2_norm_h));); (1, CUDA, CUDA_RT_CALL(cudaGetLastError());); (35, CUDA, CUDA_RT_CALL(", "label": "(63, OpenMP, double stop = omp_get_wtime();); (70, CUDA, CUDA_RT_CALL(cudaFree(a));); (18, CUDA, CUDA_RT_CALL(cudaDeviceSynchronize());); (10, CUDA, CUDA_RT_CALL(cudaMemset(a, 0, nx * ny * sizeof(float)));); (6, CUDA, CUDA_RT_CALL(cudaMalloc(&a, nx * ny * sizeof(float)));); (63, CUDA, CUDA_RT_CALL(cudaFree(l2_norm_d));); (13, CUDA, CUDA_RT_CALL(cudaGetLastError());); (32, CUDA, CUDA_RT_CALL(cudaMemset(l2_norm_d, 0, sizeof(float)));); (36, CUDA, CUDA_RT_CALL(cudaMemcpy(l2_norm_h, l2_norm_d, sizeof(float), cudaMemcpyDeviceToHost));); (5, CUDA, CUDA_RT_CALL(cudaMemset(a_new, 0, nx * ny * sizeof(float)));); (14, CUDA, CUDA_RT_CALL(cudaMallocHost(&l2_norm_h, sizeof(float)));); (54, CUDA, CUDA_RT_CALL(cudaMemcpy(a_ref_h, a, nx * ny * sizeof(float), cudaMemcpyDeviceToHost));); (58, CUDA, CUDA_RT_CALL(cudaFree(a_new));); (-2, CUDA, CUDA_RT_CALL(cudaMalloc(&a_new, nx * ny * sizeof(float)));); (22, OpenMP, double start = omp_get_wtime();); (8, CUDA, CUDA_RT_CALL(cudaMalloc(&l2_norm_d, sizeof(float)));); (51, CUDA, CUDA_RT_CALL(cudaFreeHost(l2_norm_h));)"}
{"predict": "(54, MPI, errval = MPI_Recv(NULL, 0, MPI_INT, src, 100, comm, MPI_STATUS_IGNORE);); (19, MPI, MPI_Comm_size(comm, &size);); (25, MPI, errval = MPI_Send(b1, 10, MPI_INT, dest, 0, comm);); (89, MPI, errval = MPI_Send(b2, 11, MPI_INT, dest, 10, comm);); (58, MPI, errval = MPI_Waitsome(2, r, &outcount, s, &errclass);); (83, MPI, MPI_Send(b1, 10, MPI_INT, dest, 0, comm);); (13, MPI, MPI_Comm_set_errhandler(comm, MPI_ERRORS_RETURN);); (8, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (31, MPI, errval = MPI_Recv(NULL, 0, MPI_INT, src, 100, comm, MPI_STATUS_IGNORE);); (8, MPI, MPI_Comm_rank(comm, &rank);); (85, MPI, MPI_Send(b1, 10, MPI_INT, dest, 0, comm);); (17, MPI, errval = MPI_Irecv(b2, 11, MPI_INT, src, 10, comm, &r[1]);); (85, MPI, MPI_Send(b2, 11, MPI_INT, dest, 10, comm);); (21, MPI, errval = MPI_Recv(NULL, 0, MPI_INT, src, 100, comm, MPI_STATUS_IGNORE);); (14, MPI, errval = MPI_Irecv(b1, 10, MPI_INT, src, 0, comm, &r[0]);)", "label": "(20, MPI, MPI_Comm_size(comm, &size);); (64, MPI, MPI_Error_class(errval, &errclass);); (95, MPI, MPI_Ssend(NULL, 0, MPI_INT, dest, 10, comm);); (39, MPI, errval = MPI_Recv(NULL, 0, MPI_INT, src, 100, comm, MPI_STATUS_IGNORE);); (85, MPI, MPI_Ssend(NULL, 0, MPI_INT, dest, 100, comm);); (59, MPI, errval = MPI_Waitsome(2, r, &outcount, indices, s);); (87, MPI, MPI_Send(b1, 10, MPI_INT, dest, 0, comm);); (25, MPI, errval = MPI_Irecv(b2, 10, MPI_INT, src, 10, comm, &r[1]);); (44, MPI, errval = MPI_Recv(NULL, 0, MPI_INT, src, 10, comm, MPI_STATUS_IGNORE);); (16, MPI, MPI_Comm_set_errhandler(comm, MPI_ERRORS_RETURN);); (16, MPI, errval = MPI_Irecv(b1, 10, MPI_INT, src, 0, comm, &r[0]);); (8, MPI, MPI_Comm_rank(comm, &rank);); (82, MPI, MPI_Send(b2, 11, MPI_INT, dest, 10, comm);)"}
{"predict": "(10, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:gsum) reduction(+:gsum)); (7, OpenMP, #pragma omp target map(tofrom:sum)); (3, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:sum) reduction(+:sum))", "label": "(3, OpenMP, #pragma omp parallel for reduction(+:sum)); (8, OpenMP, #pragma omp target teams distribute parallel for reduction(+:gsum))"}
{"predict": "(35, MPI, MPI_Wait(&(reqs[root]), MPI_STATUS_IGNORE);); (10, MPI, MPI_Info_create(&info);); (10, MPI, MPI_Type_vector(n, 1, stride, MPI_DOUBLE, &vec);); (50, MPI, MPI_Type_free(&vec);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (47, MPI, MPI_Info_free(&info);)", "label": "(35, MPI, MPI_Start(&reqs[root]);); (20, MPI, MPI_Type_vector(n, 1, stride, MPI_DOUBLE, &vec);); (11, MPI, MPI_Info_create(&info);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (32, MPI, MPI_Wait(&reqs[root], MPI_STATUS_IGNORE);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (48, MPI, MPI_Type_free(&vec);); (15, MPI, MPI_Type_commit(&vec);); (39, MPI, MPI_Request_free(&reqs[root]);); (44, MPI, MPI_Info_free(&info);)"}
{"predict": "((13, TBB, tbb::runtime_loader rtl( tbb::runtime_loader::em_status );); (39, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_no_lib );); (8, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_bad_call );); (18, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_bad_arg );); (7, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_ok );); (61, TBB, tbb::runtime_loader rtl( tbb::runtime_loader::em_throw );); (62, TBB, CHECK( code == tbb::runtime_loader::ec_no_lib );); (18, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_no_lib );); (43, TBB, tbb::runtime_loader rtl( tbb::runtime_loader::em_status );); (44, TBB, tbb::runtime_loader::error_code code = tbb::runtime_loader::ec_ok;); (32, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_bad_arg );)", "label": "(172, TBB, CHECK( rtl1.status() == tbb::runtime_loader::ec_bad_ver );); (97, TBB, tbb::task_scheduler_init init( 1 );); (136, TBB, CHECK_EXCEPTION( eid_invalid_multiple_scheduling, tbb::invalid_multiple_scheduling );); (40, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_no_lib );); (8, TBB, tbb::runtime_loader rtl( tbb::runtime_loader::em_status );); (158, TBB, tbb::runtime_loader rtl1( tbb::runtime_loader::em_status );); (158, TBB, CHECK( rtl0.status() == tbb::runtime_loader::ec_ok );); (126, TBB, CHECK_EXCEPTION( eid_bad_last_alloc,              tbb::bad_last_alloc              );); (154, TBB, tbb::runtime_loader rtl0( tbb::runtime_loader::em_status );); (130, TBB, CHECK_EXCEPTION( eid_improper_lock,               tbb::improper_lock               );); (113, TBB, tbb::internal::throw_exception( tbb::internal::id );); (84, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_bad_call );); (96, TBB, tbb::internal::throw_exception( tbb::internal::exception_id( id ) );); (39, TBB, tbb::runtime_loader rtl( tbb::runtime_loader::em_throw );); (133, TBB, tbb::internal::handle_perror( EAGAIN, \"apple\" );); (-1, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_ok );); (37, TBB, tbb::runtime_loader::error_code code = tbb::runtime_loader::ec_ok;); (8, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_bad_arg );); (147, TBB, CHECK( rtl1.status() == tbb::runtime_loader::ec_ok );); (46, TBB, CHECK( code == tbb::runtime_loader::ec_no_lib );); (117, TBB, CHECK_EXCEPTION( eid_missing_wait,                tbb::missing_wait                );); (121, TBB, CHECK_EXCEPTION( eid_user_abort,                  tbb::user_abort                  );); (37, TBB, CHECK( code == tbb::runtime_loader::ec_bad_arg );)"}
{"predict": "(47, OpenMP, #pragma omp parallel for collapse(2)); (133, OpenMP, #pragma omp target update from (u1_real, u1_imag, u_real,u_imag)); (30, OpenMP, #pragma omp parallel for); (49, OpenMP, #pragma omp parallel for collapse(3))", "label": "(31, OpenMP, #pragma omp teams distribute collapse(2)); (47, OpenMP, #pragma omp parallel for collapse(2)); (34, OpenMP, #pragma omp parallel for)"}
{"predict": "(22, OpenMP, #pragma omp distribute parallel for if (argc > 0? argv[1] : argv[2])); (26, OpenMP, #pragma omp distribute parallel for if (S1)); (31, OpenMP, #pragma omp distribute parallel for if (argc argc)); (36, OpenMP, #pragma omp distribute parallel for if(parallel : argc); (3, OpenMP, #pragma omp distribute parallel for if (); (16, OpenMP, #pragma omp distribute parallel for if (argc))); (11, OpenMP, #pragma omp distribute parallel for if (argc); (37, OpenMP, #pragma omp distribute parallel for if(parallel : argc) if (parallel:argc)); (27, OpenMP, #pragma omp distribute parallel for if(parallel : argc) if (argc)); (-4, OpenMP, #pragma omp distribute parallel for if); (-8, OpenMP, #pragma omp target); (36, OpenMP, #pragma omp distribute parallel for if(parallel : argc) if (parallel:argc) if (argc)); (2, OpenMP, #pragma omp distribute parallel for if (argc))); (-9, OpenMP, #pragma omp teams)", "label": "(20, OpenMP, #pragma omp distribute parallel for proc_bind(master); (4, OpenMP, #pragma omp distribute parallel for proc_bind); (13, OpenMP, #pragma omp distribute parallel for proc_bind()); (7, OpenMP, #pragma omp distribute parallel for proc_bind(); (26, OpenMP, #pragma omp distribute parallel for proc_bind(x)); (38, OpenMP, #pragma omp distribute parallel for proc_bind(spread)); (19, OpenMP, #pragma omp distribute parallel for proc_bind(close), proc_bind(spread)); (-4, OpenMP, #pragma omp target); (32, OpenMP, #pragma omp parallel proc_bind(close)); (27, OpenMP, #pragma omp distribute parallel for proc_bind(master)); (-6, OpenMP, #pragma omp teams)"}
{"predict": "(33, MPI, MPI_Bcast(&rc, 1, MPI_INT, 0, scr_storedesc_cntl->comm);)", "label": "(33, MPI, MPI_Bcast(&rc, 1, MPI_INT, 0, scr_storedesc_cntl->comm);)"}
{"predict": "(4, OpenMP, #pragma omp target parallel (); (14, OpenMP, #pragma omp target parallel unknown()); (10, OpenMP, #pragma omp target parallel }); (4, OpenMP, #pragma omp target parallel [); (55, OpenMP, #pragma omp target parallel copyin(pvt)); (3, OpenMP, #pragma omp target parallel )); (6, OpenMP, #pragma omp target parallel)); (48, OpenMP, #pragma omp target parallel default(none)); (-2, OpenMP, #pragma omp target parallel])", "label": "(69, OpenMP, #pragma omp cancellation point parallel); (66, OpenMP, #pragma omp cancellation point taskgroup); (60, OpenMP, #pragma omp cancel for); (57, OpenMP, #pragma omp cancel); (64, OpenMP, #pragma omp cancel parallel); (50, OpenMP, #pragma omp target parallel copyin(pvt)); (-5, OpenMP, #pragma omp target parallel); (-4, OpenMP, #pragma omp target parallel (); (53, OpenMP, #pragma omp cancellation point); (57, OpenMP, #pragma omp cancel taskgroup); (-1, OpenMP, #pragma omp target parallel )); (0, OpenMP, #pragma omp target parallel }); (4, OpenMP, #pragma omp target parallel unknown()); (50, OpenMP, #pragma omp cancellation point for); (-9, OpenMP, #pragma omp target parallel [); (49, OpenMP, #pragma omp cancel sections); (49, OpenMP, #pragma omp cancellation point sections); (-10, OpenMP, #pragma omp target parallel ])"}
{"predict": "(8, CUDA, cudaGetDeviceProperties( &prop, 0 );); (4, CUDA, cudaSetDeviceFlags( cudaDeviceMapHost );); (4, CUDA, cudaSetDevice( 0 );)", "label": "(9, CUDA, cudaGetDeviceProperties( &prop, 0 );); (5, CUDA, cudaSetDeviceFlags( cudaDeviceMapHost );); (3, CUDA, cudaSetDevice( 0 );)"}
{"predict": "(60, MPI, MPI_Finalize();); (0, MPI, MPI_Init(&argc, &argv);); (2, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (0, MPI, MPI_Comm_size(MPI_COMM_WORLD, &P);)", "label": "(1, MPI, MPI_Init(&argc, &argv);); (61, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &P);)"}
{"predict": "(9, CUDA, *pErrorCode = findCudaDevice(commonDataIndex);)", "label": "(43, CUDA, setCommonICUDataPointer(&U_ICUDATA_ENTRY_POINT, FALSE, pErrorCode);)"}
{"predict": "(2, TBB, tbb::global_control control(tbb::global_control::max_allowed_parallelism, num_threads);); (23, TBB, const std::size_t thread_id = tbb::tbb_thread::thread_id();); (0, TBB, tbb::arena big_arena(num_threads);)", "label": "(3, TBB, tbb::task_arena big_arena(static_cast<int>(num_threads));); (28, TBB, int thread_id = tbb::this_task_arena::current_thread_index();); (0, TBB, tbb::global_control concurrency(tbb::global_control::max_allowed_parallelism, num_threads);)"}
{"predict": "(21, OpenMP, #pragma omp parallel for simd collapse (foobool(argc)), collapse (true), collapse (-5)); (36, OpenMP, #pragma omp parallel for simd collapse (N)); (20, OpenMP, #pragma omp parallel for simd collapse (S)); (3, OpenMP, #pragma omp parallel for simd collapse ()); (21, OpenMP, #pragma omp parallel for simd collapse (foobool(1) > 0? 1 : 2)); (33, OpenMP, #pragma omp parallel for simd collapse (2)); (0, OpenMP, #pragma omp parallel for simd collapse (argc); (3, OpenMP, #pragma omp parallel for simd collapse (ST); (27, OpenMP, #pragma omp parallel for simd collapse (2)); (1, OpenMP, #pragma omp parallel for simd collapse (1); (28, OpenMP, #pragma omp parallel for simd collapse (0)); (4, OpenMP, #pragma omp parallel for simd collapse ((ST > 0)? 1 + ST : 2)); (6, OpenMP, #pragma omp parallel for simd collapse (foobool(argc)), collapse (true), collapse (-5))", "label": "(13, OpenMP, #pragma omp parallel for simd collapse (ST); (34, OpenMP, #pragma omp parallel for simd collapse (N)); (20, OpenMP, #pragma omp parallel for simd collapse (foobool(argc)), collapse (true), collapse (-5)); (-1, OpenMP, #pragma omp parallel for simd collapse); (13, OpenMP, #pragma omp parallel for simd collapse ((ST > 0)? 1 + ST : 2)); (19, OpenMP, #pragma omp parallel for simd collapse (S)); (9, OpenMP, #pragma omp parallel for simd collapse (1)); (-3, OpenMP, #pragma omp parallel for simd collapse (); (-2, OpenMP, #pragma omp parallel for simd collapse ()); (1, OpenMP, #pragma omp parallel for simd collapse (argc); (27, OpenMP, #pragma omp parallel for simd collapse (2))"}
{"predict": "(25, TBB, tbb::flow::make_edge( a1, a2 );); (17, TBB, tbb::flow::make_edge( a0, a1 );); (27, TBB, tbb::flow::make_edge( s2, q );); (37, TBB, tbb::flow::remove_edge( s2, q );); (15, TBB, tbb::flow::make_edge( a2, a3 );); (2, TBB, tbb::flow::sequencer_node<int> s(g, seq_inspector());); (3, TBB, tbb::flow::sequencer_node<int> c(g, seq_inspector());); (18, TBB, tbb::flow::make_edge( c, q );); (16, TBB, tbb::flow::make_edge( s, c );); (10, TBB, tbb::flow::make_edge( a0, a3 );); (38, TBB, tbb::flow::sequencer_node<int> s2(g, seq_inspector());); (-5, TBB, tbb::flow::graph g;); (38, TBB, tbb::flow::sequencer_node<int> q(g, seq_inspector());); (38, TBB, tbb::flow::make_edge( s2, q );); (28, TBB, tbb::flow::remove_edge( s2, q );)", "label": "(61, TBB, tbb::flow::make_edge( a1, tbb::flow::input_port<1>(a3) );); (58, TBB, tbb::flow::remove_edge( a2, s2 );); (60, TBB, tbb::flow::make_edge( a3, s2 );); (0, TBB, tbb::flow::function_node<int,int> c(g, tbb::flow::unlimited, cube_body());); (19, TBB, CHECK_MESSAGE( (&std::get<1>(a3.input_ports()) == &tbb::flow::input_port<1>(a3.j)), \"input_port1 of j not bound to input port 1 in composite_node a3\");); (25, TBB, tbb::flow::make_edge( c, tbb::flow::input_port<1>(a0) );); (21, TBB, tbb::flow::queue_node<int> q(g);); (25, TBB, tbb::flow::make_edge( c, tbb::flow::input_port<0>(a1) );); (27, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(a0), tbb::flow::input_port<0>(a2) );); (20, TBB, tbb::flow::make_edge( s, tbb::flow::input_port<0>(a0) );); (16, TBB, tbb::flow::function_node<int,int> s2(g, tbb::flow::unlimited, square_body());); (22, TBB, tbb::flow::make_edge( c, tbb::flow::input_port<1>(a1) );); (24, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(a1), tbb::flow::input_port<1>(a2) );); (9, TBB, CHECK_MESSAGE( (&std::get<0>(a3.input_ports()) == &tbb::flow::input_port<0>(a3.j)), \"input_port 0 of j not bound to input port 0 in composite_node a3\");); (3, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<0>(a2) == &tbb::flow::input_port<0>(a2.j)), \"input_port 0 of j not bound to input port 0 in composite_node a2\");); (24, TBB, tbb::flow::make_edge( s2, q );); (-15, TBB, tbb::flow::graph g;); (-15, TBB, tbb::flow::function_node<int,int> s(g, tbb::flow::unlimited, square_body());); (0, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<1>(a2) == &tbb::flow::input_port<1>(a2.j)), \"input_port 1 of j not bound to input port 1 in composite_node a2\");); (-12, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<0>(a0) == &tbb::flow::input_port<0>(a0.j)), \"input_port 0 of j not bound to input port 0 in composite_node a0\");); (18, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(a2), s2 );); (-12, TBB, CHECK_MESSAGE( (&tbb::flow::output_port<0>(a0) == &a0.f), \"f not bound to output port 0 in composite_node a0\");); (-18, TBB, tbb::flow::function_node<int,int> p(g, tbb::flow::unlimited, passthru_body());); (35, TBB, tbb::flow::remove_edge(s2, q);); (-11, TBB, CHECK_MESSAGE( (&std::get<1>(a0.input_ports()) == &tbb::flow::input_port<1>(a0.j)), \"input_port1 of j not bound to input port 1 in composite_node a1\");); (-17, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<1>(a0) == &tbb::flow::input_port<1>(a0.j)), \"input_port 1 of j not bound to input port 1 in composite_node a0\");); (-14, TBB, CHECK_MESSAGE( (&std::get<0>(a0.input_ports()) == &tbb::flow::input_port<0>(a0.j)), \"input_port 0 of j not bound to input port 0 in composite_node a1\");); (-8, TBB, CHECK_MESSAGE( (&tbb::flow::output_port<0>(a2) == &a2.f), \"f not bound to output port 0 in composite_node a2\");); (3"}
{"predict": "(29, CUDA, CUDASimulation cudaSimulation(model);)", "label": "(29, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(11, TBB, test_concurrency_with_flow_graph(2);); (5, TBB, tbb::task_scheduler_init init( p );)", "label": "(10, TBB, test_extract<tbb::flow::rejecting>();); (10, TBB, test_extract<tbb::flow::queueing>();)"}
{"predict": "(47, CUDA, checkCudaErrors(cudaEventRecord(end, 0));); (57, CUDA, checkCudaErrors(cudaFree(dynamicSMem));); (40, CUDA, checkCudaErrors(cudaEventRecord(start, 0));); (42, CUDA, checkCudaErrors(cudaEventSynchronize(end));); (50, CUDA, checkCudaErrors(cudaEventElapsedTime(&elapsedTime, start, end));); (10, CUDA, checkCudaErrors(cudaOccupancyMaxActiveBlocksPerMultiprocessor(&dynamicSMemUsage, (void *)square, blockSize));); (10, CUDA, checkCudaErrors(cudaEventCreate(&start));); (36, CUDA, checkCudaErrors(cudaEventCreate(&end));); (37, CUDA, checkCudaErrors(cudaEventDestroy(start));); (37, CUDA, checkCudaErrors(cudaEventDestroy(end));)", "label": "(13, CUDA, checkCudaErrors(cudaEventCreate(&start));); (13, CUDA, checkCudaErrors(cudaEventCreate(&end));); (41, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (38, CUDA, checkCudaErrors(cudaEventRecord(end));); (35, CUDA, checkCudaErrors(cudaEventRecord(start));); (50, CUDA, checkCudaErrors(cudaEventElapsedTime(&elapsedTime, start, end));)"}
{"predict": "(3, OpenMP, omp_set_nested(0);); (3, OpenMP, omp_set_num_threads(2);); (4, OpenMP, #pragma omp parallel num_threads(2)); (1, OpenMP, omp_set_dynamic(0);)", "label": "(5, OpenMP, #pragma omp parallel num_threads(2)); (1, OpenMP, omp_set_nested(1);); (1, OpenMP, omp_set_max_active_levels(1);)"}
{"predict": "(3, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (73, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (40, CUDA, EXPECT_EQ(cudaSuccess, cudaMemset(outAccess->sampleData(0), 0xFA, outBufferSize));); (104, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamDestroy(stream));)", "label": "(3, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (73, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (40, CUDA, EXPECT_EQ(cudaSuccess, cudaMemset(outAccess->sampleData(0), 0xFA, outBufferSize));); (104, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamDestroy(stream));)"}
{"predict": "(29, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate(e, g)); (10, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate ()); (36, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate(i)); (20, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate (argc > 0? argv[1] : argv[2])); (2, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate (); (25, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate(S2::S2s)); (30, OpenMP, #pragma omp target teams distribute parallel for simd private(i), firstprivate(i)); (33, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate(j)); (27, OpenMP, #pragma omp target teams distribute parallel for simd shared(i)); (3, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate (argc,); (18, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate(ca)); (12, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate (a, b, c, d, f)); (1, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate (argc); (3, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate (argc, z)); (24, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate(h)); (3, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate (S1)); (3, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate (argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (2, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate (argv[1])); (15, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate(da))", "label": "(25, OpenMP, #pragma omp target teams distribute parallel for simd shared (argv[1])); (28, OpenMP, #pragma omp target teams distribute parallel for simd shared(ca)); (19, OpenMP, #pragma omp target teams distribute parallel for simd shared (S1)); (42, OpenMP, #pragma omp target teams distribute parallel for simd shared(j)); (29, OpenMP, #pragma omp target teams distribute parallel for simd shared(e, g)); (34, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate(i)); (11, OpenMP, #pragma omp target teams distribute parallel for simd shared (argc > 0? argv[1] : argv[2])); (30, OpenMP, #pragma omp target teams distribute parallel for simd private(i), shared(i)); (7, OpenMP, #pragma omp target teams distribute parallel for simd shared (argc,); (22, OpenMP, #pragma omp target teams distribute parallel for simd shared(da)); (3, OpenMP, #pragma omp target teams distribute parallel for simd shared (argc); (24, OpenMP, #pragma omp target teams distribute parallel for simd shared(h, B::x)); (31, OpenMP, #pragma omp target teams distribute parallel for simd shared(i)); (28, OpenMP, #pragma omp target teams distribute parallel for simd private(i)); (-5, OpenMP, #pragma omp target teams distribute parallel for simd shared (); (12, OpenMP, #pragma omp target teams distribute parallel for simd shared(ba)); (-9, OpenMP, #pragma omp target teams distribute parallel for simd shared); (-6, OpenMP, #pragma omp target teams distribute parallel for simd shared ()); (1, OpenMP, #pragma omp target teams distribute parallel for simd shared (argc)); (4, OpenMP, #pragma omp target teams distribute parallel for simd shared (a, b, c, d, f, z))"}
{"predict": "(26, OpenMP, #pragma omp target teams distribute parallel for collapse(3))", "label": "(65, OpenMP, #pragma omp target update to(u1_real, u1_imag))"}
{"predict": "(2, CUDA, __shared__ float siftPoint[128*FMC2H];); (3, CUDA, __shared__ float maxScore[FMC2H];); (3, CUDA, __shared__ float maxScor2[FMC2H];); (3, CUDA, __shared__ int maxIndex[FMC2H];)", "label": "(3, CUDA, __shared__ float maxScore[FMC2H];); (4, CUDA, __shared__ int maxIndex[FMC2H];); (2, CUDA, __shared__ float maxScor2[FMC2H];); (-1, CUDA, __shared__ float siftPoint[128*FMC2H];)"}
{"predict": "(26, MPI, MPI_Bcast(&tmp_val, 1, MPI_INT, 0, fd->comm);)", "label": "(25, MPI, MPI_Bcast(&tmp_val, 1, MPI_INT, 0, fd->comm);)"}
{"predict": "(9, TBB, static tbb::concurrent_set<int> static_init_time_set;)", "label": "(7, TBB, tbb::concurrent_unordered_set<int> static_init_time_set;)"}
{"predict": "(56, OpenMP, #pragma omp simd)", "label": "(56, OpenMP, #pragma omp simd)"}
{"predict": "(136, CUDA, checkCudaErrors(cudaFree(d_idata));); (136, CUDA, checkCudaErrors(cudaFree(d_odata));); (70, CUDA, checkCudaErrors(cudaMalloc((void **)&d_idata, bytes));); (71, CUDA, checkCudaErrors(cudaMalloc((void **)&d_odata, numBlocks * sizeof(T)));)", "label": "(132, CUDA, checkCudaErrors(cudaFree(d_idata));); (71, CUDA, checkCudaErrors(cudaMalloc((void **)&d_idata, bytes));); (131, CUDA, checkCudaErrors(cudaFree(d_odata));); (70, CUDA, checkCudaErrors(cudaMalloc((void **)&d_odata, numBlocks * sizeof(T)));)"}
{"predict": "(144, OpenMP, #pragma omp parallel)", "label": "(17, MPI, MPI_Barrier(world);)"}
{"predict": "(43, CUDA, CUDA_SAFE_CALL(cudaFree(data));)", "label": "(11, OpenMP, #pragma omp parallel for num_threads(N_THREADS))"}
{"predict": "(15, CUDA, extern __shared__ char smem[];)", "label": "(15, CUDA, extern __shared__ char smem[];)"}
{"predict": "(21, OpenCL, retVal = clEnqueueMemcpyINTEL(pCmdQ, true, compressedDeviceMemAlloc1, bufferData, bufferSize, 0, nullptr, nullptr);); (30, OpenCL, retVal = clEnqueueNDRangeKernel(pCmdQ, multiDeviceKernel.get(), 1, nullptr, globalWorkSize, nullptr, 0, nullptr, nullptr);); (15, OpenCL, retVal = clEnqueueMemcpyINTEL(pCmdQ, true, compressedDeviceMemAlloc2, bufferData, bufferSize, 0, nullptr, nullptr);); (37, OpenCL, retVal = clEnqueueMemcpyINTEL(pCmdQ, true, uncompressibleHostMemAlloc, compressedDeviceMemAlloc2, bufferSize, 0, nullptr, nullptr);)", "label": "(31, OpenCL, retVal = clEnqueueNDRangeKernel(pCmdQ, multiDeviceKernel.get(), 1, nullptr, globalWorkSize, nullptr, 0, nullptr, nullptr);); (20, OpenCL, retVal = clEnqueueMemcpyINTEL(pCmdQ, true, compressedDeviceMemAlloc1, bufferData, bufferSize, 0, nullptr, nullptr);); (14, OpenCL, retVal = clEnqueueMemcpyINTEL(pCmdQ, true, compressedDeviceMemAlloc2, bufferData, bufferSize, 0, nullptr, nullptr);); (37, OpenCL, retVal = clEnqueueMemcpyINTEL(pCmdQ, true, uncompressibleHostMemAlloc, compressedDeviceMemAlloc2, bufferSize, 0, nullptr, nullptr);)"}
{"predict": "((13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (90, MPI, MPI_Allreduce(values, sums, 3, MPI_INT64_T, MPI_SUM, MPI_COMM_WORLD);); (81, MPI, MPI_Barrier(MPI_COMM_WORLD);); (21, MPI, MPI_Barrier(MPI_COMM_WORLD);); (82, MPI, MPI_Wtime_ended(&mfu_copy_stats.wtime_ended);); (18, MPI, MPI_Wtime_started(&mfu_copy_stats.wtime_started);)", "label": "(77, MPI, mfu_copy_stats.wtime_ended = MPI_Wtime();); (12, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (24, MPI, mfu_copy_stats.wtime_started = MPI_Wtime();); (89, MPI, MPI_Allreduce(values, sums, 3, MPI_INT64_T, MPI_SUM, MPI_COMM_WORLD);); (120, MPI, MPI_Allreduce(&rc, &all_rc, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);)"}
{"predict": "(160, CUDA, cudaMemcpy(z_cu, z, shape.Z_len * sizeof(DT), cudaMemcpyHostToDevice);); (211, CUDA, cudaFree(x_cu);); (152, CUDA, cudaMalloc(&x_cu, shape.X_len * sizeof(ST));); (155, CUDA, cudaMemcpy(y_cu, y, shape.Y_len * sizeof(ST), cudaMemcpyHostToDevice);); (153, CUDA, cudaMemcpy(x_cu, x, shape.X_len * sizeof(ST), cudaMemcpyHostToDevice);); (150, CUDA, cudaMalloc(&z_cu, shape.Z_len * sizeof(DT));); (148, CUDA, cudaMalloc(&y_cu, shape.Y_len * sizeof(ST));); (198, CUDA, cudaFree(y_cu);); (158, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (200, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (194, CUDA, cudaFree(z_cu);)", "label": "(169, CUDA, cudaMemcpy(z_cu, z, shape.Z_len * sizeof(DT), cudaMemcpyHostToDevice);); (261, CUDA, cudaFree(x_cu);); (162, CUDA, cudaMalloc(&x_cu, shape.X_len * sizeof(ST));); (165, CUDA, cudaMemcpy(y_cu, y, shape.Y_len * sizeof(ST), cudaMemcpyHostToDevice);); (163, CUDA, cudaMemcpy(x_cu, x, shape.X_len * sizeof(ST), cudaMemcpyHostToDevice);); (258, CUDA, cudaFree(y_cu);); (160, CUDA, cudaMalloc(&z_cu, shape.Z_len * sizeof(DT));); (158, CUDA, cudaMalloc(&y_cu, shape.Y_len * sizeof(ST));); (168, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (260, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (254, CUDA, cudaFree(z_cu);)"}
{"predict": "(37, MPI, MPI_Start(&req);); (31, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (35, MPI, MPI_Recv(rbuf, count, MPI_INT, rank, 0, comm, MPI_STATUS_IGNORE);); (50, MPI, MPI_Info_free(&info);); (11, MPI, MPI_Comm_size(comm, &size);); (27, MPI, MPI_Allreduce_init(sbuf, rbuf, count, MPI_INT, MPI_SUM, comm, info, &req);); (29, MPI, MPI_Info_set(info, \"no_indep_rw\", \"true\");); (35, MPI, MPI_Request_free(&req);); (47, MPI, MPI_Comm_free(&comm);); (20, MPI, MPI_Info_create(&info);); (7, MPI, MPI_Comm_rank(comm, &rank);)", "label": "(36, MPI, MPI_Start(&req);); (13, MPI, MPI_Comm_size(comm, &size);); (28, MPI, MPI_Info_create(&info);); (34, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (48, MPI, MPI_Info_free(&info);); (45, MPI, MPI_Request_free(&req);); (27, MPI, MPI_Alltoall_init(sbuf, count, MPI_INT, rbuf, count, MPI_INT, comm, info, &req);); (8, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(28, TBB, tbb::task_arena* result_arena = allocate_and_construct( tbb::task_arena::deferred, arena_args... );); (30, TBB, tbb::task_arena* result_arena = allocate_and_construct( dummy_max_concurrency, dummy_reserved_for_masters, arena_args... );); (16, TBB, tbb::task_arena* result_arena = allocate_and_construct( arena_args... );); (18, TBB, tbb::task_arena* result_arena = allocate_and_construct( tbb::task_arena::automatic, arena_args... );)", "label": "(30, TBB, result_arena = new tbb::task_arena(tmp);); (18, TBB, result_arena = new tbb::task_arena( arena_args... );); (12, TBB, tbb::task_arena* result_arena = nullptr;); (26, TBB, tbb::task_arena tmp(dummy_max_concurrency, dummy_reserved_for_masters);); (19, TBB, result_arena = new tbb::task_arena;)"}
{"predict": "(11, TBB, oneapi::tbb::parallel_for(oneapi::tbb::blocked_range<int>(1, sequence_length + 1), AssignElementToChMap(a_ch_map));)", "label": "(4, TBB, using ch_map = tbb::concurrent_hash_map<int,int>;)"}
{"predict": "(34, MPI, rc = MPI_Type_get_true_extent(obj->DTP_datatype, &true_lb, &true_extent);); (31, MPI, rc = MPI_Type_get_extent(obj->DTP_datatype, &lb, &extent);)", "label": "(32, MPI, rc = MPI_Type_get_true_extent(obj->DTP_datatype, &true_lb, &true_extent);); (22, MPI, rc = MPI_Type_commit(&obj->DTP_datatype);); (32, MPI, rc = MPI_Type_get_extent(obj->DTP_datatype, &lb, &extent);)"}
{"predict": "(13, MPI, MPI_Comm_size(comm, &size);); (73, MPI, err = MPI_Wait(&req, &status);); (33, MPI, MPI_Comm_set_errhandler(comm, MPI_ERRORS_RETURN);); (9, MPI, MPI_Comm_rank(comm, &rank);)", "label": "(73, MPI, err = MPI_Wait(&req, &status);); (13, MPI, MPI_Comm_size(comm, &size);); (34, MPI, MPI_Comm_set_errhandler(comm, MPI_ERRORS_RETURN);); (10, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(2, OpenMP, #pragma omp parallel); (4, OpenMP, #pragma omp parallel master taskloop)", "label": "(2, OpenMP, #pragma omp parallel); (4, OpenMP, #pragma omp parallel master taskloop)"}
{"predict": "(24, MPI, MPI_Barrier(scr_comm_world);); (36, MPI, time_start = MPI_Wtime();); (57, MPI, double time_end = MPI_Wtime();)", "label": "(24, MPI, MPI_Barrier(scr_comm_world);); (60, MPI, double time_end = MPI_Wtime();); (35, MPI, time_start = MPI_Wtime();)"}
{"predict": "(15, OpenMP, #pragma omp target data map(tofrom: i) use_device_ptr(z) use_device_addr(i,j,k[:i])); (8, OpenMP, #pragma omp target data map(tofrom: i) use_device_ptr(k) use_device_addr(i,j))", "label": "(9, OpenMP, #pragma omp target data map(tofrom: i) use_device_ptr(k) use_device_addr(i, j)); (13, OpenMP, #pragma omp target data map(tofrom: i) use_device_ptr(z) use_device_addr(i, j, k[:i]))"}
{"predict": "(24, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (63, MPI, MPI_Allreduce(&counter, &totalNum, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);)", "label": "(2, MPI, MPI_Init(&argc, &argv);); (86, MPI, MPI_Finalize();); (23, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&rank);); (61, MPI, MPI_Barrier(MPI_COMM_WORLD);); (59, MPI, MPI_Reduce(&counter, &totalNum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);)"}
{"predict": "(67, OpenCL, clReleaseMemObject(mem[i]);)", "label": "(67, OpenCL, clReleaseMemObject(mem[i]);)"}
{"predict": "architectures.(155, CUDA, CudaInfo.IsMultiArch = hasMultiArch;); (162, CUDA, CudaInfo.IsSupported = isSupported;); (150, CUDA, CudaInfo.Is64Bit = HostTriple.isArch64Bit();); (152, CUDA, CudaInfo.IsCUDALib =!NoCudaLib;); (153, CUDA, CudaInfo.IsCudaDevice = isCudaDevice;); (155, CUDA, CudaInfo.CudaVersion = Version;); (150, CUDA, CudaInfo.HostTriple = HostTriple;); (153, CUDA, CudaInfo.IncludePath = IncludePath;); (153, CUDA, CudaInfo.LibPath = LibPath;); (152, CUDA, CudaInfo.BinPath = BinPath;); (154, CUDA, CudaInfo.VersionInfo = VersionInfo;); (148, CUDA, CudaInfo.CudaDeviceCount = getCudaDeviceCount();); (148, CUDA, CudaInfo.CudaDeviceName = getCudaDeviceName();); (149, CUDA, CudaInfo.CudaDriverVersion = getCudaDriverVersion();); (145, CUDA, CudaInfo.IsWindows = HostTriple.isOSWindows();)", "label": "(123, CUDA, std::string GpuArchName(CudaArchToString(GpuArch));)"}
{"predict": "(22, CUDA, cudaMallocHost((void**)&src_host, sizeof(float)*size);); (30, CUDA, cudaDeviceSynchronize();); (20, CUDA, cudaMallocHost((void**)&tar_host, sizeof(float)*size);); (48, CUDA, cudaFreeHost(src_host);); (48, CUDA, cudaFreeHost(tar_host);)", "label": "(19, CUDA, cudaMallocHost(&tar_host, size * sizeof(float));); (17, CUDA, cudaMallocHost(&src_host, size * sizeof(float));)"}
{"predict": "(63, OpenMP, #define OP_GOMP_CRITICAL_READ(OP, FLAG))", "label": "(63, OpenMP, #define OP_GOMP_CRITICAL_READ(OP, FLAG))"}
{"predict": "(2, OpenMP, omp_autotune(self, OMP_SCALE);)", "label": "(2, OpenMP, omp_autotune(self, OMP_SCALE);)"}
{"predict": "(3, OpenMP, #pragma omp target data map(tofrom : a[0:2][0:2])); (3, OpenMP, #pragma omp target update from(a[0:2][0:2]))", "label": "(3, OpenMP, #pragma omp parallel num_threads(2)); (12, OpenMP, #pragma omp ordered depend(source)); (7, OpenMP, #pragma omp ordered depend(sink : i - 1, j) depend(sink : i, j - 1)); (1, OpenMP, #pragma omp for ordered(2))"}
{"predict": "(3, OpenMP, #pragma omp parallel); (3, OpenMP, #pragma omp taskloop)", "label": "(3, OpenMP, #pragma omp parallel); (3, OpenMP, #pragma omp taskloop)"}
{"predict": "(10, MPI, MPI_Send(p->buff, p->bufflen, MPI_BYTE, p->prot.nbor, 1, MPI_COMM_WORLD);); (13, MPI, MPI_Wait(&request, &status);); (10, MPI, MPI_Recv(p->buff1, p->bufflen, MPI_BYTE, p->prot.nbor, 1, MPI_COMM_WORLD, &status);); (5, MPI, MPI_Isend(p->buff, p->bufflen, MPI_BYTE, p->prot.nbor, 1, MPI_COMM_WORLD, &request);)", "label": "(9, MPI, MPI_Wait(&request, &status);); (6, MPI, MPI_Irecv(p->buff1, p->bufflen, MPI_BYTE, p->prot.nbor, 1, MPI_COMM_WORLD, &request);); (6, MPI, MPI_Send(p->buff, p->bufflen, MPI_BYTE, p->prot.nbor, 1, MPI_COMM_WORLD);)"}
{"predict": "(23, OpenMP, omp_target_free(d_a, omp_get_default_device());); (15, OpenMP, d_a = (int *)omp_target_alloc(DataSize, omp_get_default_device());); (18, OpenMP, omp_target_disassociate_ptr(p_a, omp_get_default_device());); (14, OpenMP, omp_target_associate_ptr(p_a, d_a, DataSize, 0, omp_get_default_device());)", "label": "(16, OpenMP, omp_target_associate_ptr(p_a, d_a, DataSize, 0, omp_get_default_device());); (21, OpenMP, omp_target_free(d_a, omp_get_default_device());); (12, OpenMP, d_a = (int *)omp_target_alloc(DataSize, omp_get_default_device());); (17, OpenMP, omp_target_disassociate_ptr(p_a, omp_get_default_device());)"}
{"predict": "(10, MPI, MPI_Comm_rank(comm, &par_rank);); (10, MPI, MPI_Comm_size(comm, &par_size);)", "label": "(50, OpenMP, EXPECT_TRUE(check_test_image(output_file, t_apcomp_baseline_dir()));); (9, MPI, MPI_Comm_rank(comm, &par_rank);); (10, MPI, apcomp::mpi_comm(MPI_Comm_c2f(comm));); (8, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(27, MPI, MPI_Wait(&request, &status);); (34, MPI, MPI_Waitall(1, &request, &status);); (44, MPI, MPI_Finalize();); (16, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (11, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (24, MPI, MPI_Grequest_complete(&request);)", "label": "(27, MPI, MPI_Wait(&request, &status);); (40, MPI, MPI_Waitall(1, &request, &status);); (16, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (31, MPI, MPI_Waitsome(1, &request, &outcount, indices, &status);); (19, MPI, MPI_Grequest_start(query_fn, free_fn, cancel_fn, NULL, &request);)"}
{"predict": "(9, OpenMP, omp_set_dynamic(0);); (7, OpenMP, omp_set_num_threads(2);)", "label": "(6, OpenMP, omp_set_dynamic(0);)"}
{"predict": "(14, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize(), \"term_by_term_mat_vec_div\");); (12, CUDA, qmc_cuda::cuda_check(cudaGetLastError(), \"term_by_term_mat_vec_div\");)", "label": "(14, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize());); (12, CUDA, qmc_cuda::cuda_check(cudaGetLastError());)"}
{"predict": "(7, TBB, tbb::flow::queue_node< q3_input_type >  q3(g);); (7, TBB, tbb::flow::make_edge( q3, my_split );)", "label": "(6, TBB, tbb::flow::queue_node< q3_input_type >  q3(g);); (7, TBB, tbb::flow::make_edge( q3, my_split );)"}
{"predict": "(3, CUDA, cudaGetDevice(&device_id);); (3, CUDA, cudaGetDeviceProperties(&device_prop, device_id);)", "label": "(3, CUDA, cudaGetDevice(&device_id);); (3, CUDA, cudaGetDeviceProperties(&device_prop, device_id);)"}
{"predict": "(6, OpenMP, #pragma omp atomic unknown); (14, OpenMP, #pragma omp atomic seq_cst); (2, OpenMP, #pragma omp atomic)", "label": "(3, OpenMP, #pragma omp atomic)"}
{"predict": "(12, MPI, MPI_Pack(sbuf, scount, sdt, packed_buf, outsize, &position, MPI_COMM_WORLD);); (4, MPI, MPI_Type_size(sdt, &outsize);); (15, MPI, timers[t] = MPI_Wtime();); (2, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myself);); (19, MPI, timers[t] = (MPI_Wtime() - timers[t]) / cycles;); (6, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(18, MPI, timers[t] = MPI_Wtime();); (22, MPI, timers[t] = (MPI_Wtime() - timers[t]) / cycles;); (11, MPI, MPI_Pack(sbuf, scount, sdt, packed_buf, outsize, &position, MPI_COMM_WORLD);); (2, MPI, MPI_Type_size(sdt, &outsize);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myself);)"}
{"predict": "(25, OpenMP, #pragma omp target parallel device(global + a) depend(in); (97, OpenMP, #pragma omp target parallel if (0) firstprivate(global) depend(out); (100, OpenMP, #pragma omp target parallel device(global + a) nowait depend(inout)", "label": "(86, OpenMP, #pragma omp target teams device(global + a) nowait depend(inout); (25, OpenMP, #pragma omp target teams device(global + a) depend(in); (100, OpenMP, #pragma omp target teams if (0) firstprivate(global) depend(out)"}
{"predict": "(16, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize(), \"batched_dot_wabn_wban\");); (14, CUDA, qmc_cuda::cuda_check(cudaGetLastError(), \"batched_dot_wabn_wban\");)", "label": "(16, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize(), \"batched_dot_wabn_wban\");); (14, CUDA, qmc_cuda::cuda_check(cudaGetLastError(), \"batched_dot_wabn_wban\");)"}
{"predict": "(48, CUDA, __shared__ TB smemB[cosize_v<BSmemLayout>];); (46, CUDA, __shared__ TA smemA[cosize_v<ASmemLayout>];)", "label": "(48, CUDA, __shared__ TB smemB[cosize_v<BSmemLayout>];); (46, CUDA, __shared__ TA smemA[cosize_v<ASmemLayout>];)"}
{"predict": "(44, CUDA, getLastCudaError(\"Kernel execution failed\");); (46, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (12, CUDA, checkCudaErrors(cudaMalloc((void **)&src, Size.width * Stride * sizeof(float)));); (82, CUDA, checkCudaErrors(cudaFree(src));); (40, CUDA, checkCudaErrors(cudaMemcpy2D(ImgDst, Stride, dst, DeviceStride * sizeof(float), Size.width * sizeof(float), Size.height, cudaMemcpyDeviceToHost));); (79, CUDA, checkCudaErrors(cudaFree(dst));); (9, CUDA, checkCudaErrors(cudaMalloc((void **)&dst, Size.width * Stride * sizeof(float)));)", "label": "(13, CUDA, checkCudaErrors(cudaMallocPitch((void **)&src, &DeviceStride, Size.width * sizeof(float), Size.height));); (79, CUDA, checkCudaErrors(cudaFree(src));); (12, CUDA, checkCudaErrors(cudaMallocPitch((void **)&dst, &DeviceStride, Size.width * sizeof(float), Size.height));); (35, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (40, CUDA, getLastCudaError(\"Kernel execution failed\");); (74, CUDA, checkCudaErrors(cudaFree(dst));)"}
{"predict": "(14, OpenMP, #pragma omp taskloop simd priority(c) num_tasks(a) simdlen(8))", "label": "(14, OpenMP, #pragma omp master taskloop shared(c) num_tasks(a))"}
{"predict": "(35, CUDA, __shared__ _VOLATILE_ float bSmem[16][128+4];); (33, CUDA, __shared__ _VOLATILE_ float aSmem[16][128+4];)", "label": "(35, CUDA, __shared__ _VOLATILE_ float bSmem[16][128+4];); (33, CUDA, __shared__ _VOLATILE_ float aSmem[16][128+4];)"}
{"predict": "(2, OpenMP, #pragma omp parallel); (3, OpenMP, #pragma omp master taskloop simd)", "label": "(4, OpenMP, #pragma omp master taskloop simd); (1, OpenMP, #pragma omp parallel)"}
{"predict": "(59, CUDA, cudaFree(deviceA);); (59, CUDA, cudaFree(deviceC);); (57, CUDA, cudaFree(deviceB);); (38, CUDA, checkCudaErrors(cudaMemcpy(hostC, deviceC, size, cudaMemcpyDeviceToHost));); (24, CUDA, checkCudaErrors(cudaMalloc((void**)&deviceB, size));); (22, CUDA, checkCudaErrors(cudaMalloc((void**)&deviceA, size));); (25, CUDA, checkCudaErrors(cudaMemcpy(deviceA, hostA, size, cudaMemcpyHostToDevice));); (25, CUDA, checkCudaErrors(cudaMemcpy(deviceB, hostB, size, cudaMemcpyHostToDevice));); (20, CUDA, checkCudaErrors(cudaMalloc((void**)&deviceC, size));)", "label": "(28, CUDA, cudaMalloc((void**)&deviceC, size);); (55, CUDA, cudaFree(deviceB);); (40, CUDA, cudaMemcpy(hostC, deviceC, size, cudaMemcpyDeviceToHost);); (54, CUDA, cudaFree(deviceC);); (51, CUDA, cudaFree(deviceA);); (22, CUDA, cudaMalloc((void**)&deviceB, size);); (25, CUDA, cudaMemcpy(deviceA, hostA, size, cudaMemcpyHostToDevice);); (25, CUDA, cudaMemcpy(deviceB, hostB, size, cudaMemcpyHostToDevice);); (18, CUDA, cudaMalloc((void**)&deviceA, size);)"}
{"predict": "(77, MPI, MPI_Comm_size(fd->comm, &nprocs);); (77, MPI, MPI_Comm_rank(fd->comm, &myrank);)", "label": "(77, MPI, MPI_Comm_size(fd->comm, &nprocs);); (77, MPI, MPI_Comm_rank(fd->comm, &myrank);)"}
{"predict": "(15, OpenMP, omp_set_dynamic(0);); (15, OpenMP, omp_set_num_threads(4);); (10, OpenMP, omp_set_default_device(0);); (24, OpenMP, chunk = omp_get_schedule(&ref_sched)[0];); (12, OpenMP, omp_set_schedule(monotonic? omp_sched_monotonic : omp_sched_nonmonotonic, chunk);); (10, OpenMP, sched = omp_get_schedule();); (14, OpenMP, omp_set_schedule(omp_sched_dynamic, 0);)", "label": "(11, OpenMP, ref_sched = str2omp_sched(argv[2]);); (13, OpenMP, omp_get_schedule(&sched, &chunk);)"}
{"predict": "(7, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (7, CUDA, getLastCudaError(\"launch_kernel failed\");)", "label": "(7, CUDA, cudaDeviceSynchronize();); (7, CUDA, getLastCudaError(\"launch_kernel failed\");)"}
{"predict": "(6, TBB, tbb::global_control s(tbb::global_control::max_allowed_parallelism, idx);)", "label": "(6, TBB, tbb::global_control global_limit(tbb::global_control::max_allowed_parallelism, idx);)"}
{"predict": "7, CUDA, __shared__ DT labels[BW * BH * BW];)", "label": "(5, CUDA, __shared__ DT labels[BW * BH * BD];)"}
{"predict": "(11, CUDA, CUDA_CHECK(cudaMalloc(&ctx->d_rx_input, batch_size * 3 * dataset_size));); (12, CUDA, CUDA_CHECK(cudaMalloc(&ctx->d_rx_output, batch_size * 128 * sizeof(float)));); (9, CUDA, CUDA_CHECK(cudaMalloc(&ctx->d_rx_workspace, batch_size * (ctx->algorithm.l2() + 1) * sizeof(float)));); (5, CUDA, CUDA_CHECK(cudaMalloc(&ctx->d_rx_weights,  batch_size * 64 * sizeof(float)));); (2, CUDA, CUDA_CHECK(cudaMalloc(&ctx->d_rx_back_mem, batch_size * (ctx->algorithm.l2() + 1) * sizeof(float)));); (3, CUDA, CUDA_CHECK(cudaMalloc(&ctx->d_rx_mem,     batch_size * (ctx->algorithm.l3() + 64) * sizeof(float)));); (1, CUDA, CUDA_CHECK(cudaMalloc(&ctx->d_rx_scratchpads, ctx->d_scratchpads_size * sizeof(float)));)", "label": "(9, CUDA, CUDA_CHECK(ctx->device_id, cudaMalloc(&ctx->d_rx_dataset, dataset_size));); (15, CUDA, CUDA_CHECK(ctx->device_id, cudaMalloc(&ctx->d_rx_vm_states, batch_size * 2560));); (4, CUDA, CUDA_CHECK(ctx->device_id, cudaHostGetDevicePointer(&ctx->d_rx_dataset, const_cast<void *>(dataset), 0));); (7, CUDA, CUDA_CHECK(ctx->device_id, cudaMemcpy(ctx->d_rx_dataset, dataset, dataset_size, cudaMemcpyHostToDevice));); (11, CUDA, CUDA_CHECK(ctx->device_id, cudaMalloc(&ctx->d_rx_entropy, batch_size * (128 + 2560)));); (12, CUDA, CUDA_CHECK(ctx->device_id, cudaMalloc(&ctx->d_rx_rounding, batch_size * sizeof(uint32_t)));); (8, CUDA, CUDA_CHECK(ctx->device_id, cudaMalloc(&ctx->d_rx_hashes, batch_size * 64));); (6, CUDA, CUDA_CHECK(ctx->device_id, cudaMalloc(&ctx->d_long_state, ctx->d_scratchpads_size));)"}
{"predict": "(39, CUDA, cudaFree(res_dev);); (32, CUDA, cudaMalloc(&res_dev, sizeof(float));); (34, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)", "label": "(39, CUDA, cudaFree(res_dev);); (32, CUDA, cudaMalloc(&res_dev, sizeof(float));); (34, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(8, MPI, err = MPI_Probe(MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &status);); (14, MPI, MPI_Error_string(err, msg, &len);)", "label": "(16, MPI, MPI_Error_string(err, msg, &len);); (8, MPI, err = MPI_Probe(-80, 1, MPI_COMM_WORLD, &status);); (5, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(81, OpenMP, elapsed_time = omp_get_wtime() - start_time;); (40, OpenMP, start_time = omp_get_wtime();); (63, OpenMP, #pragma omp target map(tofrom : conv)); (85, OpenMP, #pragma omp target exit data); (65, OpenMP, #pragma omp target teams distribute parallel for map(tofrom : xold[0 : Ndim], xnew[0 : Ndim],); (45, OpenMP, #pragma omp target enter data); (43, OpenMP, #pragma omp target); (64, OpenMP, #pragma omp teams distribute parallel for reduction(+ : conv))", "label": "(69, OpenMP, #pragma omp target map(tofrom : conv)); (87, OpenMP, elapsed_time = omp_get_wtime() - start_time;); (53, OpenMP, #pragma omp teams distribute parallel for simd); (83, OpenMP, #pragma omp target exit data map(from : xold[0 : Ndim], xnew[0 : Ndim])); (37, OpenMP, start_time = omp_get_wtime();); (43, OpenMP, #pragma omp target enter data map(to : xold[0 : Ndim], xnew[0 : Ndim],); (48, OpenMP, #pragma omp target); (63, OpenMP, #pragma omp teams distribute parallel for simd reduction(+ : conv))"}
{"predict": "(16, CUDA, CUDA_CHECK(cudaGetLastError());)", "label": "(16, CUDA, CUDA_CHECK(cudaGetLastError());)"}
{"predict": "(342, CUDA, DPRINTF(\"Error in synchronizing stream = %s\\n\", cudaGetErrorName(result));); (52, CUDA, result = cudaFree(0);); (54, CUDA, errPtr = cudaGetErrorName(result);); (59, CUDA, result = cudaSetDevice(0);); (335, CUDA, result = cudaStreamSynchronize(stream);); (386, CUDA, DPRINTF(\"Error in destroying cudaStream = %s\\n\", cudaGetErrorName(result));); (95, CUDA, result = cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking);); (396, CUDA, result = cudaDeviceReset();); (376, CUDA, result = cudaStreamDestroy(stream);); (389, CUDA, DPRINTF(\"Error in destroying cudaFree = %s\\n\", cudaGetErrorName(result));); (400, CUDA, result = cudaFree(0);); (43, CUDA, DPRINTF(\"Error in cudaFree = %s\\n\", cudaGetErrorName(result));); (457, CUDA, DPRINTF(\"Error in destroying cudaMalloc = %s\\n\", cudaGetErrorName(result));); (468, CUDA, DPRINTF(\"Error in destroying cudaEvent = %s\\n\", cudaGetErrorName(result));); (460, CUDA, result = cudaEventDestroy(event);); (454, CUDA, result = cudaFree(eventData);); (430, CUDA, DPRINTF(\"Error in destroying cudaMemset = %s\\n\", cudaGetErrorName(result));); (445, CUDA, result = cudaFreeHost(hostData);); (370, CUDA, result = cudaFree(0);); (373, CUDA, result = cudaFreeHost(0);); (396, CUDA, DPRINTF(\"Error in reseting device = %s\\n\", cudaGetErrorName(result));); (414, CUDA, result = cudaMalloc(&eventData, eventDesc.size);); (388, CUDA, DPRINTF(\"Error in destroying cudaMalloc = %s\\n\", cudaGetErrorName(result));); (43, CUDA, result = cudaSetDevice(0);); (392, CUDA, result = cudaMem", "label": "(55, CUDA, result = cudaFree(0);); (56, CUDA, errPtr = cudaGetErrorName(result);); (320, CUDA, cudaFree(outputBufferGPU);); (318, CUDA, cudaFree(inputBufferGPU);); (205, CUDA, result = cudaMalloc(&inputBufferGPU, inputTensorDesc[0].size);); (88, CUDA, result = cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking);); (56, CUDA, result = cudaSetDevice(0);); (211, CUDA, result = cudaMalloc(&outputBufferGPU, outputTensorDesc[0].size);); (324, CUDA, result = cudaStreamDestroy(stream);); (282, CUDA, result = cudaStreamSynchronize(stream);)"}
{"predict": "(58, CUDA, thrust::universal_vector<TB> d_B = h_B;); (47, CUDA, thrust::universal_vector<TC> h_C(m*n);); (55, CUDA, thrust::universal_vector<TC> d_C = h_C;); (50, CUDA, thrust::universal_vector<TB> d_B(n*k);); (46, CUDA, thrust::universal_vector<TA> d_A = h_A;); (40, CUDA, thrust::universal_vector<TA> h_A(m*k);)", "label": "(49, CUDA, thrust::host_vector<TB> h_B(n*k);); (4, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;); (1, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (55, CUDA, thrust::device_vector<TC> d_C = h_C;); (46, CUDA, thrust::host_vector<TC> h_C(m*n);); (52, CUDA, thrust::device_vector<TB> d_B = h_B;); (86, CUDA, thrust::host_vector<TC> cute_result = d_C;); (49, CUDA, thrust::device_vector<TA> d_A = h_A;); (40, CUDA, thrust::host_vector<TA> h_A(m*k);)"}
{"predict": "(6, OpenMP, #pragma omp target map(tofrom : counter_N0)); (7, OpenMP, #pragma omp parallel for)", "label": "(8, OpenMP, #pragma omp taskloop shared(counter_N0)); (5, OpenMP, #pragma omp target data map(tofrom : counter_N0)); (8, OpenMP, #pragma omp target teams distribute parallel for map(tofrom : counter_N0) nowait); (9, OpenMP, #pragma omp atomic update)"}
{"predict": "(167, MPI, MPI_Send(&corner_val,1,MPI_DOUBLE,root,888,MPI_COMM_WORLD);); (232, MPI, MPI_Finalize();); (89, MPI, MPI_Bcast(&m,          1, MPI_INT, root, MPI_COMM_WORLD);); (21, MPI, MPI_Init(&argc,&argv);); (87, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (148, MPI, MPI_Recv(&(ARRAY(0,0)),1,MPI_DOUBLE,my_ID-1,888,MPI_COMM_WORLD,&status);); (193, MPI, MPI_Send(inbuf, jjsize, MPI_DOUBLE, my_ID+1, j, MPI_COMM_WORLD);); (19, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (19, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);); (137, MPI, MPI_Send(&corner_val,1,MPI_DOUBLE,root,888,MPI_COMM_WORLD);); (184, MPI, MPI_Send(outbuf, jjsize, MPI_DOUBLE, my_ID-1, j, MPI_COMM_WORLD);); (16, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (80, MPI, MPI_Bcast(&n,          1, MPI_INT, root, MPI_COMM_WORLD);); (186, MPI, MPI_Recv(inbuf, jjsize, MPI_DOUBLE, my_ID+1, j, MPI_COMM_WORLD,&status);)", "label": "(177, MPI, MPI_Recv(inbuf, jjsize, MPI_DOUBLE, my_ID-1, j, MPI_COMM_WORLD, &status);); (88, MPI, MPI_Bcast(&m,          1, MPI_INT, root, MPI_COMM_WORLD);); (88, MPI, MPI_Bcast(&n,          1, MPI_INT, root, MPI_COMM_WORLD);); (89, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (241, MPI, MPI_Finalize();); (163, MPI, MPI_Send(&(ARRAY(end,j)), 1, MPI_DOUBLE, my_ID+1, j, MPI_COMM_WORLD);); (195, MPI, MPI_Send(&corner_val,1,MPI_DOUBLE,0,888,MPI_COMM_WORLD);); (197, MPI, MPI_Recv(&(ARRAY(0,0)),1,MPI_DOUBLE,root,888,MPI_COMM_WORLD,&status);); (18, MPI, MPI_Init(&argc,&argv);); (183, MPI, MPI_Send(outbuf, jjsize, MPI_DOUBLE, my_ID+1, j, MPI_COMM_WORLD);); (138, MPI, MPI_Barrier(MPI_COMM_WORLD);); (17, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (79, MPI, MPI_Bcast(&grp,        1, MPI_INT, root, MPI_COMM_WORLD);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(10, TBB, oneapi::tbb::blocked_range<utils_t::template data_type<unsigned char, 1>> r(0, data.size());)", "label": "(10, TBB, range_t r = utils_t::make_range((1 << (20 / DimAmount - 1)) - DimAmount, false, &MakeInt, oneapi::tbb::detail::make_index_sequence<DimAmount>());); (1, TBB, using range_t = oneapi::tbb::blocked_nd_range<int, DimAmount>;)"}
{"predict": "(43, MPI, MPI_Send( recvbuf, knsupc * nrhs, MPI_FLOAT, receiver, k, grid3d->zscp.comm);); (51, MPI, MPI_Recv( recvbuf, knsupc * nrhs, MPI_FLOAT, sender, k, grid3d->zscp.comm, &status );)", "label": "(52, MPI, MPI_Recv( recvbuf, knsupc * nrhs, MPI_FLOAT, sender, k, grid3d->zscp.comm, &status );); (42, MPI, MPI_Send( recvbuf, knsupc * nrhs, MPI_FLOAT, receiver, k,  grid3d->zscp.comm);)"}
{"predict": "(33, MPI, MPI_Barrier(scr_comm_world);)", "label": "(33, MPI, MPI_Barrier(scr_comm_world);)"}
{"predict": "(6, CUDA, cudaGridDependencySynchronize();); (27, CUDA, __shared__ typename ACC_TYPE::TempStorage temp_storage[num_token];)", "label": "(27, CUDA, cudaGridDependencySynchronize();); (85, CUDA, cudaTriggerProgrammaticLaunchCompletion();)"}
{"predict": "(16, OpenCL, context = clCreateContext(nullptr, 1, &device, nullptr, nullptr, &error);)", "label": "(16, OpenCL, context = clCreateContext(nullptr, 1, &device, nullptr, nullptr, &error);)"}
{"predict": "(24, OpenCL, clReleaseEvent(event);)", "label": "(24, OpenCL, clReleaseEvent(event);)"}
{"predict": "(25, CUDA, extern __shared__ __align__(sizeof(MoveEltType)) char loadSmemBuffer[];)", "label": "(31, CUDA, __shared__ char loadSmemBuffer[kUpdateKVCacheKernelShmSize];)"}
{"predict": "(6, OpenMP, #pragma omp teams distribute parallel for collapse(3)); (8, OpenMP, #pragma omp simd)", "label": "(6, OpenMP, #pragma omp teams distribute); (7, OpenMP, #pragma omp parallel for); (8, OpenMP, #pragma omp simd); (1, OpenMP, #pragma omp target map (alloc: u0_real,u0_imag,u1_real,u1_imag,twiddle))"}
{"predict": "(17, MPI, MPI_Abort(MPI_COMM_WORLD, MPIT_result);); (11, MPI, MPIT_result = MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);); (15, MPI, MPIT_result = MPI_T_pvar_session_create(session);)", "label": "(16, MPI, MPI_Abort(MPI_COMM_WORLD, MPIT_result);); (17, MPI, MPIT_result = MPI_T_pvar_session_create(session);); (11, MPI, MPIT_result = MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);)"}
{"predict": "(12, OpenCL, void *mappedPtr = clEnqueueMapBuffer(cmdQ.get(), buffer.get(), CL_FALSE, CL_MAP_WRITE, offset, size, 1, &clMapEvent, nullptr, nullptr, &retVal);); (19, OpenCL, retVal = clEnqueueUnmapMemObject(cmdQ.get(), buffer.get(), mappedPtr, 1, &clUnmapEvent, nullptr);)", "label": "(19, OpenCL, retVal = clEnqueueUnmapMemObject(cmdQ.get(), buffer.get(), mappedPtr, 1, &clUnmapEvent, nullptr);); (10, OpenCL, void *mappedPtr = clEnqueueMapBuffer(cmdQ.get(), buffer.get(), CL_FALSE, CL_MAP_WRITE, offset, size, 1, &clMapEvent, nullptr, &retVal);)"}
{"predict": "(258, TBB, benchmark_fixed_n(n,\"reduce_tbb\",[] (size_t N) -> double { return reduce.run_tbb(N); });); (263, TBB, benchmark_fixed_n(n,\"reduce_mytbb\",[] (size_t N) -> double { return reduce.run_mytbb(N); });); (237, TBB, benchmark(N_start,N,\"reduce_tbb\",[] (size_t N) -> double { return reduce.run_tbb(N); });)", "label": "(62, TBB, PRINT( tbb::TBB_runtime_interface_version() );); (68, TBB, while (tracker.get_concurrency() < tbb::task_scheduler_init::default_num_threads()) tbb_pi<double> (N);); (11, TBB, tbb::task_scheduler_init init(tbb::task_scheduler_init::default_num_threads());); (82, TBB, TaskSchedulerTBB::create(0);); (90, TBB, TaskSchedulerTBB::destroy();)"}
{"predict": "(18, OpenMP, #pragma omp target exit data map(from:W[:M])); (15, OpenMP, #pragma omp target update from(W[:M])); (5, OpenMP, #pragma omp target enter data map(to:W[:M])); (5, OpenMP, #pragma omp target map(tofrom:m))", "label": "(7, OpenMP, #pragma omp target data map(tofrom: W[:M])); (12, OpenMP, #pragma omp target teams); (12, OpenMP, #pragma omp distribute parallel for)"}
{"predict": "(65, MPI, MPI_Bcast( ToRecv, nsupers, MPI_INT, 0,  grid3d->zscp.comm);); (28, MPI, MPI_Bcast( &nsupers, 1, MPI_INT, 0,  grid3d->zscp.comm);); (67, MPI, MPI_Bcast( ToSendR[i], Pc * Pr, MPI_INT, 0,  grid3d->zscp.comm);); (69, MPI, MPI_Bcast( ToSendD[i], Pc * Pr, MPI_INT, 0,  grid3d->zscp.comm);); (65, MPI, MPI_Bcast( ToSendR[i], Pr * Pc, MPI_INT, 0,  grid3d->zscp.comm);); (11, MPI, MPI_Bcast( etree, n, MPI_INT, 0,  grid3d->zscp.comm);); (22, MPI, MPI_Bcast( supno, nsupers, MPI_INT, 0,  grid3d->zscp.comm);); (33, MPI, MPI_Bcast( xsup, nsupers + 1, MPI_INT, 0,  grid3d->zscp.comm);); (20, MPI, MPI_Bcast( &myrow, 1, MPI_INT, 0,  grid3d->zscp.comm);); (18, MPI, MPI_Bcast( &mycol, 1, MPI_INT, 0,  grid3d->zscp.comm);); (21, MPI, MPI_Bcast( supno, nsupers, MPI_INT, 0,  grid3d->zscp.comm);); (32, MPI, MPI_Bcast( xsup, nsupers + 1, MPI_INT, 0,  grid3d->zscp.comm);); (74, MPI, MPI_Bcast( &(Llu->nnz_bc), 1, MPI_LONG_LONG_INT, 0,  grid3d->zscp.comm);); (72, MPI, MPI_Bcast( &(Llu->nnz_", "label": "(66, MPI, MPI_Bcast( ToRecv, nsupers, MPI_INT, 0,  grid3d->zscp.comm);); (39, MPI, MPI_Bcast( supno, n, mpi_int_t, 0,  grid3d->zscp.comm);); (27, MPI, MPI_Bcast( &nsupers, 1, mpi_int_t, 0,  grid3d->zscp.comm);); (69, MPI, MPI_Bcast( ToSendR[i], Pc, MPI_INT, 0,  grid3d->zscp.comm);); (11, MPI, MPI_Bcast( etree, n, mpi_int_t, 0,  grid3d->zscp.comm);); (233, MPI, MPI_Bcast(*((void**)&(Llu->fsendx_plist[lk])), Pr * sizeof (int), MPI_BYTE, 0, grid3d->zscp.comm);); (84, MPI, MPI_Bcast( &(Llu->ldalsum), 1, mpi_int_t, 0,  grid3d->zscp.comm);); (61, MPI, MPI_Bcast( ToSendD, nbr, MPI_INT, 0,  grid3d->zscp.comm);); (29, MPI, MPI_Bcast( xsup, nsupers + 1, mpi_int_t, 0,  grid3d->zscp.comm);); (80, MPI, MPI_Bcast( &(Llu->nbsendx), 1, MPI_INT, 0,  grid3d->zscp.comm);); (78, MPI, MPI_Bcast( &(Llu->nfsendx), 1, MPI_INT, 0,  grid3d->zscp.comm);); (45, MPI, MPI_Bcast( bufmax, NBUFFERS, mpi_int_t, 0,  grid3d->zscp.comm);)"}
{"predict": "(9, OpenCL, void *mappedPtr = clEnqueueMapImage(cmdQ.get(), image.get(), CL_TRUE, CL_MAP_READ, &origin[0], &region[0], nullptr, nullptr, 0, nullptr, nullptr, &retVal);); (15, OpenCL, retVal = clEnqueueUnmapMemObject(cmdQ.get(), image.get(), mappedPtr, 0, nullptr, nullptr);)", "label": "(16, OpenCL, retVal = clEnqueueUnmapMemObject(cmdQ.get(), image.get(), mappedPtr, 0, nullptr, nullptr);); (8, OpenCL, void *mappedPtr = clEnqueueMapImage(cmdQ.get(), image.get(), CL_TRUE, CL_MAP_READ, &origin[0], &region[0], nullptr, nullptr, 0, nullptr, nullptr, &retVal);)"}
{"predict": "(11, OpenMP, #pragma omp parallel sections private(i) reduction(+ : sum)); (22, OpenMP, #pragma omp critical); (17, OpenMP, #pragma omp for); (21, OpenMP, #pragma omp section)", "label": "(13, OpenMP, #pragma omp section); (10, OpenMP, #pragma omp parallel); (21, OpenMP, #pragma omp critical)"}
{"predict": "(57, MPI, MPI_Finalize();); (10, MPI, MPI_Init(&argc,&argv);)", "label": "(57, MPI, MPI_Finalize();); (10, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(19, CUDA, cudaMemcpy(d_in1, in1.data(), in1_bytes, cudaMemcpyHostToDevice);); (42, CUDA, cudaFree(d_out);); (15, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (31, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (40, CUDA, cudaFree(d_in2);); (10, CUDA, cudaMalloc((void**)(&d_in1), in1_bytes);); (38, CUDA, cudaFree(d_in1);); (11, CUDA, cudaMalloc((void**)(&d_in2), in2_bytes);); (24, CUDA, assert(cudaMemcpyAsync(out.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);); (12, CUDA, cudaMemcpy(d_in2, in2.data(), in2_bytes, cudaMemcpyHostToDevice);)", "label": "(17, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (14, CUDA, cudaMalloc((void**)(&d_in1), in1_bytes);); (29, CUDA, assert(cudaMemcpyAsync(out.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);); (16, CUDA, cudaMemcpy(d_in1, in1.data(), in1_bytes, cudaMemcpyHostToDevice);); (40, CUDA, cudaFree(d_out);); (38, CUDA, cudaFree(d_in2);); (26, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (35, CUDA, cudaFree(d_in1);); (8, CUDA, cudaMalloc((void**)(&d_in2), in2_bytes);); (11, CUDA, cudaMemcpy(d_in2, in2.data(), in2_bytes, cudaMemcpyHostToDevice);)"}
{"predict": "(30, CUDA, cudaEventRecord(end);); (30, CUDA, cudaEventSynchronize(end);); (25, CUDA, cudaEventRecord(start);); (34, CUDA, cudaEventDestroy(start);); (20, CUDA, cudaEventCreate(&end);); (34, CUDA, cudaEventDestroy(end);); (15, CUDA, cudaEventCreate(&start);); (28, CUDA, cudaEventElapsedTime(&sec, start, end);); (28, CUDA, cudaEventDestroy(end);); (31, CUDA, cudaEventDestroy(start);); (17, CUDA, cudaEventDestroy(K_device);); (15, CUDA, cudaEventDestroy(Q_device);); (15, CUDA, cudaEventDestroy(V_device);)", "label": "(27, CUDA, cudaEventRecord(start);); (31, CUDA, cudaEventRecord(end);); (31, CUDA, cudaEventSynchronize(end);); (23, CUDA, cudaEventCreate(&end);); (21, CUDA, cudaEventCreate(&start);); (31, CUDA, cudaEventElapsedTime(&msec, start, end);)"}
{"predict": "(13, CUDA, cudaMalloc((void **)&d_arr, sizeof(int) * len);); (16, CUDA, cudaMemcpy(d_arr, arr, sizeof(int) * len, cudaMemcpyHostToDevice);); (53, CUDA, cudaFree(d_out);); (11, CUDA, cudaMalloc((void **)&d_out, sizeof(int) * len);); (23, CUDA, cudaMemcpy(out, d_out, sizeof(int) * len, cudaMemcpyDeviceToHost);); (49, CUDA, cudaFree(d_arr);)", "label": "(13, CUDA, cudaMalloc((void **)&d_arr, sizeof(int) * len);); (16, CUDA, cudaMemcpy(d_arr, arr, sizeof(int) * len, cudaMemcpyHostToDevice);); (53, CUDA, cudaFree(d_out);); (11, CUDA, cudaMalloc((void **)&d_out, sizeof(int) * len);); (23, CUDA, cudaMemcpy(out, d_out, sizeof(int) * len, cudaMemcpyDeviceToHost);); (49, CUDA, cudaFree(d_arr);)"}
{"predict": "(17, OpenMP, CUs[i] = omp_get_cu_num();); (10, OpenMP, #pragma omp target teams num_teams(numTeams) thread_limit(72) map(tofrom:Res[0:numTeams*NN], CUs[0:numTeams*NN])); (10, OpenMP, #pragma omp parallel for schedule(static,1)); (12, OpenMP, threads[j] = omp_get_num_threads();); (12, OpenMP, #pragma omp distribute parallel for); (9, OpenMP, lims[j] = omp_get_limit(omp_thread_limit_resource);)", "label": "(11, OpenMP, #pragma omp distribute); (16, OpenMP, #pragma omp parallel for); (13, OpenMP, threads[j] = omp_get_num_threads();); (7, OpenMP, #pragma omp target teams num_teams(numTeams) thread_limit(1024) map (tofrom: CUs, lims, threads, Res)); (16, OpenMP, CUs[i] = omp_ext_get_smid();); (9, OpenMP, lims[j] = omp_get_num_teams();)"}
{"predict": "(20, MPI, rc = MPI_Comm_size(MPI_COMM_WORLD, &size);); (8, MPI, rc = MPI_Comm_get_attr(MPI_COMM_WORLD, MPI_UNIVERSE_SIZE, &v, &flag);)", "label": "(19, MPI, rc = MPI_Comm_size(MPI_COMM_WORLD, &size);); (8, MPI, rc = MPI_Comm_get_attr(MPI_COMM_WORLD, MPI_UNIVERSE_SIZE, &v, &flag);)"}
{"predict": "(9, CUDA, nvDebugExit(cudaSetDevice(i));); (4, CUDA, cudaError_t result = cudaSetDevice(i);); (5, CUDA, nvDebugExit(result);)", "label": "(4, CUDA, cudaError_t result = cudaSetDevice(i);); (6, CUDA, nvDebug(\"*** CUDA Error: %s\\n\", cudaGetErrorString(result));)"}
{"predict": "(201, MPI, MPI_Send(&corner_val,1,MPI_DOUBLE,root,888,MPI_COMM_WORLD);); (95, MPI, MPI_Bcast(&m,         1, MPI_INT, root, MPI_COMM_WORLD);); (253, MPI, MPI_Finalize();); (90, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (22, MPI, MPI_Init(&argc,&argv);); (91, MPI, MPI_Bcast(&n,         1, MPI_INT, root, MPI_COMM_WORLD);); (22, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (17, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);); (85, MPI, MPI_Bcast(&grp,        1, MPI_INT, root, MPI_COMM_WORLD);); (186, MPI, MPI_Recv(&(ARRAY(0,0)),1,MPI_DOUBLE,my_ID-1,888,MPI_COMM_WORLD,&status);); (180, MPI, MPI_Send(outbuf, jjsize, MPI_DOUBLE, my_ID+1, j, MPI_COMM_WORLD);); (18, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (134, MPI, MPI_Send(inbuf, jjsize, MPI_DOUBLE, my_ID-1, j, MPI_COMM_WORLD);); (174, MPI, MPI_Send(&corner_val,1,MPI_DOUBLE,my_ID+1,888,MPI_COMM_WORLD);)", "label": "(180, MPI, MPI_Recv(inbuf, jjsize, MPI_DOUBLE, my_ID-1, j, MPI_COMM_WORLD, &status);); (91, MPI, MPI_Bcast(&m,          1, MPI_INT, root, MPI_COMM_WORLD);); (91, MPI, MPI_Bcast(&n,          1, MPI_INT, root, MPI_COMM_WORLD);); (92, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (244, MPI, MPI_Finalize();); (166, MPI, MPI_Send(&(ARRAY(end,j)), 1, MPI_DOUBLE, my_ID+1, j, MPI_COMM_WORLD);); (198, MPI, MPI_Send(&corner_val,1,MPI_DOUBLE,0,888,MPI_COMM_WORLD);); (200, MPI, MPI_Recv(&(ARRAY(0,0)),1,MPI_DOUBLE,root,888,MPI_COMM_WORLD,&status);); (19, MPI, MPI_Init(&argc,&argv);); (186, MPI, MPI_Send(outbuf, jjsize, MPI_DOUBLE, my_ID+1, j, MPI_COMM_WORLD);); (141, MPI, MPI_Barrier(MPI_COMM_WORLD);); (18, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (82, MPI, MPI_Bcast(&grp,        1, MPI_INT, root, MPI_COMM_WORLD);); (15, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(35, CUDA, CUDA_CHECK(cudaFree(d_work));); (31, CUDA, CUDA_CHECK(cudaDeviceSynchronize());); (13, CUDA, CUDA_CHECK(cudaMalloc(&d_work, workspaceInBytesOnDevice));); (16, CUDA, CUDA_CHECK(cudaMallocHost(&h_work, workspaceInBytesOnHost));)", "label": "(33, CUDA, CUDA_CHECK(cudaFree(d_work));); (14, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_work), workspaceInBytesOnDevice));)"}
{"predict": "(12, OpenMP, #pragma omp target enter data map(alloc : a [0:array_size], b [0:array_size],); (12, OpenMP, #pragma omp target teams distribute parallel for)", "label": "(12, OpenMP, #pragma omp target enter data map(alloc); (4, OpenMP, #pragma omp target map(tofrom : warp_size)); (135, OpenMP, #pragma omp target exit data map(release)"}
{"predict": "(79, OpenCL, clReleaseMemObject(bufS);); (44, OpenCL, clReleaseCommandQueue(queue);); (27, OpenCL, printf( \"clCreateContext() failed with %d\\n\", err );); (76, OpenCL, clReleaseMemObject(bufC);); (73, OpenCL, clReleaseMemObject(bufSB);); (58, OpenCL, err = clWaitForEvents(1, &event);); (66, OpenCL, clReleaseEvent(event);); (39, OpenCL, clReleaseContext(ctx);); (46, OpenCL, err = clEnqueueWriteBuffer(queue, bufSB, CL_TRUE, 0, 64 * sizeof(float), X, 0, NULL, NULL);); (27, OpenCL, printf( \"clCreateCommandQueue() failed with %d\\n\", err );); (44, OpenCL, err = clEnqueueWriteBuffer(queue, bufSA, CL_TRUE, 0, 64 * sizeof(float), A, 0, NULL, NULL);); (46, OpenCL, err = clEnqueueWriteBuffer(queue, bufC, CL_TRUE, 0, 16 * sizeof(float), Y, 0, NULL, NULL);); (40, OpenCL, bufS = clCreateBuffer(ctx, CL_MEM_READ_WRITE, 64 * sizeof(float), NULL, &err);); (17, OpenCL, queue = clCreateCommandQueue(ctx, device, 0, &err);); (14, OpenCL, ctx = clCreateContext(props, 1, &device, NULL, NULL, &err);); (67, OpenCL, clReleaseMemObject(bufSA);); (40, OpenCL, err = clEnqueueWriteBuffer(queue, bufS, CL_TRUE, 0, 64 * sizeof(float), Z, 0, NULL, NULL);); (34, OpenCL, bufSB = clCreateBuffer(ctx, CL_MEM_READ_WRITE, 64 * sizeof(float), NULL, &err);); (54, OpenCL, err = clEnqueueReadBuffer(queue, bufC, CL_TRUE, 0, 16 * sizeof", "label": "(50, OpenCL, bufSA = clCreateBuffer(ctx, CL_MEM_READ_WRITE, sizeof(cl_float), NULL, &err);); (52, OpenCL, bufS  = clCreateBuffer(ctx, CL_MEM_READ_WRITE, sizeof(cl_float), NULL, &err);); (67, OpenCL, err = clWaitForEvents(1, &event);); (55, OpenCL, err = clEnqueueWriteBuffer(queue, bufS, CL_TRUE, 0, sizeof(cl_float), &S, 0, NULL, NULL);); (83, OpenCL, clReleaseMemObject(bufC);); (77, OpenCL, clReleaseEvent(event);); (79, OpenCL, clReleaseMemObject(bufSA);); (68, OpenCL, err = clEnqueueReadBuffer(queue, bufS, CL_TRUE, 0, sizeof(cl_float), &S, 0, NULL, NULL);); (28, OpenCL, clReleaseContext(ctx);); (63, OpenCL, err = clEnqueueReadBuffer(queue, bufSA, CL_TRUE, 0, sizeof(cl_float), &SA, 0, NULL, NULL);); (47, OpenCL, err = clEnqueueWriteBuffer(queue, bufC, CL_TRUE, 0, sizeof(cl_float), &C, 0, NULL, NULL);); (62, OpenCL, err = clEnqueueReadBuffer(queue, bufSB, CL_TRUE, 0, sizeof(cl_float), &SB, 0, NULL, NULL);); (32, OpenCL, clReleaseCommandQueue(queue);); (38, OpenCL, bufSB = clCreateBuffer(ctx, CL_MEM_READ_WRITE, sizeof(cl_float), NULL, &err);); (74, OpenCL, clReleaseMemObject(bufS);); (71, OpenCL, clReleaseMemObject(bufSB);); (19, OpenCL, printf( \"clCreateCommandQueue() failed with %d\\n\", err );); (16, OpenCL, queue = clCreateCommandQueue(ctx, device, 0, &err);); (34, OpenCL, bufC  = clCreateBuffer(ctx, CL_MEM_READ_WRITE, sizeof(cl_float), NULL, &err);); (8, OpenCL, ctx = clCreateContext(props, 1, &device, NULL, NULL, &err);); (9, OpenCL, printf( \"clCreateContext() failed with %d\\n\", err );); (34, OpenCL, err = clEnqueueWriteBuffer(queue, bufSA, CL_TRUE, 0, sizeof(cl_float), &SA, 0, NULL, NULL);); (34, OpenCL, err = clEnqueueWriteBuffer(queue, bufSB, CL_TRUE, 0, sizeof(cl_float), &SB, 0, NULL, NULL);); (51, OpenCL, err = clEnqueueReadBuffer(queue, bufC, CL_TRUE, 0, sizeof(cl_float), &C, 0, NULL, NULL);)"}
{"predict": "(92, CUDA, std::cerr << \"cudaMalloc() failed: \" << cudaGetErrorString(cudaResult) << std::endl;); (176, CUDA, checkCudaErrors(cudaFree(d_output));); (56, CUDA, findCudaDevice(argc, (const char **)argv);); (173, CUDA, checkCudaErrors(cudaFree(d_directions));); (120, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (124, CUDA, checkCudaErrors(cudaMemcpy(h_outputGPU, d_output, n_vectors * n_dimensions * sizeof(float), cudaMemcpyDeviceToHost));); (85, CUDA, cudaResult = cudaMemset(d_directions, 0, n_dimensions * n_directions * sizeof(unsigned int));); (86, CUDA, cudaResult = cudaMemset(d_output, 0, n_vectors * n_dimensions * sizeof(float));); (80, CUDA, cudaResult = cudaMalloc((void **)&d_directions, n_dimensions * n_directions * sizeof(unsigned int));); (166, CUDA, checkCudaErrors(cudaFreeHost(h_outputGPU));); (164, CUDA, checkCudaErrors(cudaFreeHost(h_outputCPU));); (80, CUDA, std::cerr << \"cudaMalloc() failed: \" << cudaGetErrorString(cudaResult) << std::endl;); (76, CUDA, cudaResult = cudaMalloc((void **)&d_output, n_vectors * n_dimensions * sizeof(float));); (160, CUDA, checkCudaErrors(cudaFreeHost(h_directions));)", "label": "(93, CUDA, throw std::runtime_error(cudaGetErrorString(cudaResult));); (57, CUDA, findCudaDevice(argc, (const char **)argv);); (117, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (211, CUDA, checkCudaErrors(cudaFree(d_output));); (209, CUDA, checkCudaErrors(cudaFree(d_directions));)"}
{"predict": "(67, OpenMP, #pragma omp distribute parallel for private(i)); (61, OpenMP, #pragma omp parallel private(i)); (75, OpenMP, #pragma omp parallel); (33, OpenMP, #pragma omp distribute parallel for private(argc)); (56, OpenMP, #pragma omp distribute parallel for private(a, b)); (5, OpenMP, #pragma omp distribute parallel for private); (77, OpenMP, #pragma omp distribute parallel for private(m)); (35, OpenMP, #pragma omp distribute parallel for private(S1)); (3, OpenMP, #pragma omp target); (1, OpenMP, #pragma omp teams); (38, OpenMP, #pragma omp distribute parallel for private(argc > 0? argv[1] : argv[2])); (46, OpenMP, #pragma omp distribute parallel for private(argv[1])); (6, OpenMP, #pragma omp distribute parallel for private(); (66, OpenMP, #pragma omp parallel shared(i)); (68, OpenMP, #pragma omp distribute parallel for private(j)); (13, OpenMP, #pragma omp distribute parallel for private(argc); (18, OpenMP, #pragma omp distribute parallel for private(argc,); (23, OpenMP, #pragma omp distribute parallel for private(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (33, OpenMP, #pragma omp distribute parallel for private(e, g)); (36, OpenMP, #pragma omp distribute parallel for shared(i)); (17, OpenMP, #pragma omp distribute parallel for private(argc,)", "label": "(82, OpenMP, #pragma omp parallel private(i)); (95, OpenMP, #pragma omp distribute parallel for private(m)); (5, OpenMP, #pragma omp target); (26, OpenMP, #pragma omp distribute parallel for private(argc,); (60, OpenMP, #pragma omp distribute parallel for private(h, B::x)); (64, OpenMP, #pragma omp distribute parallel for nowait); (8, OpenMP, #pragma omp distribute parallel for private(); (65, OpenMP, #pragma omp parallel); (16, OpenMP, #pragma omp distribute parallel for private(argc); (40, OpenMP, #pragma omp distribute parallel for private(a, b)); (49, OpenMP, #pragma omp distribute parallel for private(e, g)); (33, OpenMP, #pragma omp distribute parallel for private(S1)); (42, OpenMP, #pragma omp distribute parallel for private(argv[1])); (6, OpenMP, #pragma omp distribute parallel for private()); (-5, OpenMP, #pragma omp distribute parallel for private); (62, OpenMP, #pragma omp distribute parallel for private(i)); (69, OpenMP, #pragma omp distribute parallel for private(j)); (22, OpenMP, #pragma omp distribute parallel for private(argc)); (16, OpenMP, #pragma omp distribute parallel for private(argc > 0? argv[1] : argv[2])); (62, OpenMP, #pragma omp parallel shared(i)); (-12, OpenMP, #pragma omp teams)"}
{"predict": "(13, OpenMP, #pragma omp target parallel for if (S1)); (17, OpenMP, #pragma omp target parallel for if(target : argc) if (argc)); (14, OpenMP, #pragma omp target parallel for if(parallel : argc); (23, OpenMP, #pragma omp target parallel for if(target : argc) if (argc) if (z)); (0, OpenMP, #pragma omp target parallel for if (argc); (3, OpenMP, #pragma omp target parallel for if (argc > 0? argv[1] : argv[2])); (5, OpenMP, #pragma omp target parallel for if (argc argc)); (14, OpenMP, #pragma omp target parallel for if(parallel : argc) if (for:argc)); (1, OpenMP, #pragma omp target parallel for if (argc))); (10, OpenMP, #pragma omp target parallel for if(parallel : argc / z)); (15, OpenMP, #pragma omp target parallel for if(parallel : argc) if (parallel:argc)); (5, OpenMP, #pragma omp target parallel for if (argv[1]=2)); (-4, OpenMP, #pragma omp target parallel for if ()); (0, OpenMP, #pragma omp target parallel for if (argc + z)); (13, OpenMP, #pragma omp target parallel for if(parallel : argc) if (argc) if (for:argc))", "label": "(6, OpenMP, #pragma omp target parallel for device ()); (23, OpenMP, #pragma omp target parallel for device (3.14)); (18, OpenMP, #pragma omp target parallel for device (-2)); (13, OpenMP, #pragma omp target parallel for device (argc), device (argc+1)); (14, OpenMP, #pragma omp target parallel for device (S1)); (5, OpenMP, #pragma omp target parallel for device (argc))); (-4, OpenMP, #pragma omp target parallel for device); (1, OpenMP, #pragma omp target parallel for device (argc); (14, OpenMP, #pragma omp target parallel for device (-10u)); (5, OpenMP, #pragma omp target parallel for device (z + argc)); (-6, OpenMP, #pragma omp target parallel for device (); (1, OpenMP, #pragma omp target parallel for device (argc > 0? argv[1] : argv[2]))"}
{"predict": "(6, OpenMP, #pragma omp target map(from:c))", "label": "(6, OpenMP, #pragma omp target map(from:c))"}
{"predict": "(34, CUDA, volatile __shared__ int s_bmod;); (33, CUDA, volatile __shared__ double s_lsum[MAXSUPER];)", "label": "(35, CUDA, volatile __shared__ int s_bmod;); (32, CUDA, __shared__ double s_lsum[MAXSUPER];)"}
{"predict": "(16, MPI, MPI_Comm_dup(comm, &prg->comm);); (43, MPI, MPI_Comm_rank(prg->comm, &rank);); (44, MPI, MPI_Buffer_attach(prg->values, bytes);)", "label": "(47, MPI, MPI_Ibcast(&(prg->keep_going), 1, MPI_INT, 0, prg->comm, &(prg->bcast_req));); (15, MPI, MPI_Comm_dup(comm, &prg->comm);); (37, MPI, prg->time_start = MPI_Wtime();); (42, MPI, MPI_Comm_rank(prg->comm, &rank);)"}
{"predict": "(27, MPI, MPI_DATATYPE_ALLOC(uval_new, lenv);); (25, MPI, MPI_INT_ALLOC(usub_new, lens);); (60, MPI, MPI_INT_ALLOC(lsub_new, len1);); (58, MPI, MPI_DATATYPE_ALLOC(lnzval_new, len2);)", "label": "(28, MPI, MPI_DATATYPE_ALLOC(uval_new, lenv);); (25, MPI, MPI_INT_ALLOC(usub_new, lens);); (59, MPI, MPI_INT_ALLOC(lsub_new, len1);); (60, MPI, MPI_DATATYPE_ALLOC(lnzval_new, len2);)"}
{"predict": "(27, CUDA, getLastCudaError(\"mergeRanksAndIndicesKernel<<<>>> failed!\\n\");); (17, CUDA, getLastCudaError(\"mergeRanksAndIndicesKernel<<<>>> failed!\\n\");)", "label": "(28, CUDA, getLastCudaError(\"mergeRanksAndIndicesKernel(B)<<<>>> failed\\n\");); (18, CUDA, getLastCudaError(\"mergeRanksAndIndicesKernel(A)<<<>>> failed\\n\");)"}
{"predict": "(13, CUDA, cudaFuncGetAttributes(&attr,ptr);); (6, CUDA, cudaOccupancyMaxPotentialBlockSize(&qblocksize,&mingridsize,(void*)ptr,0,0);)", "label": "(9, CUDA, cudaFuncGetAttributes(&attr,(void*)ptr);); (14, CUDA, fcuda::Check_CudaErroorFun(\"Error collecting kernel information.\");); (5, CUDA, cudaOccupancyMaxPotentialBlockSize(&mingridsize,&qblocksize,(void*)ptr,0,0);)"}
{"predict": "(4, TBB, n_threads = tbb::task_scheduler_init::default_num_threads();); (1, TBB, tbb::global_control control(tbb::global_control::max_allowed_parallelism, n_threads);); (32, TBB, tbb::enumerable_thread_specific<tbb::task_arena*> tls;); (27, TBB, std::vector<tbb::task_arena> arenas(n_arenas);)", "label": "(20, TBB, tbb::global_control control(tbb::global_control::max_allowed_parallelism, n_threads);); (1, TBB, n_threads = tbb::this_task_arena::max_concurrency();); (29, TBB, tbb::enumerable_thread_specific<tbb::task_arena*> tls;); (25, TBB, std::vector<tbb::task_arena> arenas(n_arenas);)"}
{"predict": "(15, OpenCL, int error = clGetDeviceInfo( device, CL_DEVICE_IMAGE2D_MAX_WIDTH, sizeof( maxWidth ), &maxWidth, NULL );); (16, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_GLOBAL_MEM_SIZE, sizeof( memSize ), &memSize, NULL );); (14, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_MAX_MEM_ALLOC_SIZE, sizeof( maxAllocSize ), &maxAllocSize, NULL );)", "label": "(15, OpenCL, int error = clGetDeviceInfo( device, CL_DEVICE_IMAGE2D_MAX_WIDTH, sizeof( maxWidth ), &maxWidth, NULL );); (16, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_GLOBAL_MEM_SIZE, sizeof( memSize ), &memSize, NULL );); (14, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_MAX_MEM_ALLOC_SIZE, sizeof( maxAllocSize ), &maxAllocSize, NULL );)"}
{"predict": "(36, MPI, MPI_Comm_size(intercomm, &size);); (97, MPI, MPI_Finalize();); (55, MPI, MPI_Comm_disconnect(&intercomm);); (48, MPI, MPI_Recv(&data, 1, MPI_INT, RECEIVER_RANK, DATA_TAG, intercomm, &status);); (10, MPI, MPI_Comm_get_parent(&parentcomm);); (44, MPI, MPI_Send(&i, 1, MPI_INT, RECEIVER_RANK, DATA_TAG, intercomm);); (29, MPI, MPI_Comm_rank(intercomm, &rank);); (27, MPI, MPI_Comm_remote_size(intercomm, &rsize);); (70, MPI, MPI_Send(&i, 1, MPI_INT, SENDER_RANK, DATA_TAG, intercomm);)", "label": "(36, MPI, MPI_Comm_size(intercomm, &size);); (56, MPI, MPI_Comm_disconnect(&intercomm);); (102, MPI, MPI_Finalize();); (19, MPI, MPI_Comm_get_parent(&parentcomm);); (47, MPI, MPI_Recv(&data, 1, MPI_INT, RECEIVER_RANK, DATA_TAG, intercomm, &status);); (79, MPI, MPI_Abort(intercomm, 1);); (73, MPI, MPI_Recv(&i, 1, MPI_INT, SENDER_RANK, DATA_TAG, intercomm, &status);); (43, MPI, MPI_Send(&i, 1, MPI_INT, RECEIVER_RANK, DATA_TAG, intercomm);); (29, MPI, MPI_Comm_rank(intercomm, &rank);); (26, MPI, MPI_Comm_remote_size(intercomm, &rsize);); (76, MPI, MPI_Send(&i, 1, MPI_INT, SENDER_RANK, DATA_TAG, intercomm);)"}
{"predict": "(235, MPI, MPI_Wait(&(request[1+4]), MPI_STATUS_IGNORE);); (239, MPI, MPI_Wait(&(request[3+4]), MPI_STATUS_IGNORE);); (320, MPI, MPI_Finalize();); (232, MPI, MPI_Wait(&(request[0+4]), MPI_STATUS_IGNORE);); (235, MPI, MPI_Wait(&(request[2+4]), MPI_STATUS_IGNORE);); (40, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (206, MPI, MPI_Wait(&(request[1]), MPI_STATUS_IGNORE);); (210, MPI, MPI_Wait(&(request[3]), MPI_STATUS_IGNORE);); (206, MPI, MPI_Wait(&(request[2]), MPI_STATUS_IGNORE);); (198, MPI, MPI_Wait(&(request[0]), MPI_STATUS_IGNORE);); (35, MPI, MPI_Init(&argc,&argv);); (131, MPI, MPI_Barrier(MPI_COMM_WORLD);); (35, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);); (196, MPI, MPI_Wait(&(request[2+1]), MPI_STATUS_IGNORE);); (190, MPI, MPI_Wait(&(request[1+1]), MPI_STATUS_IGNORE);); (194, MPI, MPI_Wait(&(request[0+1]), MPI_STATUS_IGNORE);); (253, MPI, MPI_Reduce(&local_norm, &norm, 1, MPI_DTYPE, MPI_SUM, root, MPI_COMM_WORLD);); (124, MPI, MPI_Bcast(&n,          1, MPI_INT, root, MPI_COMM_WORLD);); (124, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (124, MPI, MPI_Bcast(&radius,     1, MPI_INT, root, MPI_COMM_WORLD);); (244, MPI, MPI_Wait(&(request[3+1]), MPI_STATUS_IGNORE);); (238, MPI, MPI_Wait", "label": "(295, MPI, MPI_Wait(&(request[1+4]), MPI_STATUS_IGNORE);); (301, MPI, MPI_Wait(&(request[3+4]), MPI_STATUS_IGNORE);); (234, MPI, MPI_Barrier(MPI_COMM_WORLD);); (128, MPI, MPI_Bcast(&n,          1, MPI_INT, root, MPI_COMM_WORLD);); (43, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (289, MPI, MPI_Wait(&(request[0+4]), MPI_STATUS_IGNORE);); (254, MPI, MPI_Wait(&(request[0]), MPI_STATUS_IGNORE);); (294, MPI, MPI_Wait(&(request[2+4]), MPI_STATUS_IGNORE);); (124, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (36, MPI, MPI_Init(&argc,&argv);); (251, MPI, MPI_Wait(&(request[1]), MPI_STATUS_IGNORE);); (256, MPI, MPI_Wait(&(request[2]), MPI_STATUS_IGNORE);); (365, MPI, MPI_Finalize();); (255, MPI, MPI_Wait(&(request[3]), MPI_STATUS_IGNORE);); (324, MPI, MPI_Reduce(&local_norm, &norm, 1, MPI_DTYPE, MPI_SUM, root, MPI_COMM_WORLD);); (31, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(6, CUDA, extent = make_cudaExtent(30, 40, 0);); (3, CUDA, cudaExtent extent = make_cudaExtent(10, 20, 0);)", "label": "(6, CUDA, extent = make_cudaExtent(30, 40, 0);); (3, CUDA, cudaExtent extent = make_cudaExtent(10, 20, 0);)"}
{"predict": "(13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (13, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (37, MPI, MPI_Comm_free(&cart);)", "label": "(13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (37, MPI, MPI_Comm_free(&cart);); (12, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);)"}
{"predict": "(20, MPI, MPI_Recv_FUN(buff, count, type, 0, tag, comm, MPI_STATUS_IGNORE);); (4, MPI, MPI_Comm_rank(comm, &rank);)", "label": "(20, MPI, MPI_Recv(buff, count, type, 0, tag, comm, MPI_STATUS_IGNORE);); (4, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(42, OpenMP, #pragma omp parallel); (53, OpenMP, #pragma omp master); (54, OpenMP, #pragma omp parallel for firstprivate(number_of_constraints)); (87, OpenMP, #pragma omp critical)", "label": "(55, OpenMP, #pragma omp for schedule(guided, 512)); (52, OpenMP, #pragma omp parallel firstprivate(transformation_matrix, constant_vector, slave_solution_vector, master_solution_vector, p_master_node, p_slave_node))"}
{"predict": "(17, MPI, MPI_Send(&a, 1, MPI_INT, 1, 0, MPI_COMM_WORLD);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (14, MPI, MPI_Recv(&a, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (20, MPI, MPI_Wait(&request, MPI_STATUS_IGNORE);); (15, MPI, MPI_Send(&a, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);); (2, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (13, MPI, MPI_Irecv(&a, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, &request);)", "label": "(24, MPI, MPI_Request_free(&request);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (15, MPI, MPI_Psend_init(&a, 1, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, MPI_INFO_NULL, &request);); (16, MPI, MPI_Precv_init(&a, 1, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_INFO_NULL, &request);); (21, MPI, MPI_Barrier(MPI_COMM_WORLD);); (6, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(165, MPI, MPI_Get_address(read_buf + real_off - off, &addr);); (102, MPI, MPI_Type_get_extent(datatype, &lb, &buftype_extent);); (70, MPI, MPI_Allreduce(&ntimes, &max_ntimes, 1, MPI_INT, MPI_MAX, fd->comm);); (396, MPI, MPI_Get_address(tmp_buf, &addr);); (161, MPI, MPI_Type_size_x(datatype, &size);); (309, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(113, MPI, MPI_Comm_rank(fd->comm, &rank);); (180, MPI, MPI_Get_count(&read_status, MPI_BYTE, &actual_size);); (105, MPI, MPI_Type_get_extent(datatype, &lb, &buftype_extent);); (69, MPI, MPI_Allreduce(&ntimes, &max_ntimes, 1, MPI_INT, MPI_MAX, fd->comm);); (219, MPI, MPI_Get_address(read_buf + req_off - real_off, &addr);)"}
{"predict": "(9, OpenMP, #pragma omp target enter data map(to: m_gate[:nCells])); (16, OpenMP, #pragma omp target teams distribute parallel for); (9, OpenMP, #pragma omp target map(tofrom: after1)); (45, OpenMP, #pragma omp target exit data map(from: Vm[:nCells])); (7, OpenMP, #pragma omp target enter data map(to: Vm[:nCells]))", "label": "(16, OpenMP, #pragma omp target teams distribute parallel for thread_limit(128)); (8, OpenMP, #pragma omp target enter data map(to: Vm[:nCells])); (6, OpenMP, #pragma omp target enter data map(to: m_gate[:nCells]))"}
{"predict": "(6, MPI, MPI_Win_get_attr(win, key[i], &val_p, &flag);)", "label": "(6, MPI, MPI_Win_get_attr(win, key[i], &val_p, &flag);)"}
{"predict": "(25, CUDA, CSC(cudaGetLastError(), STATUS_FAILURE);)", "label": "(23, CUDA, cudaMemcpy(output, data, copySize, cudaMemcpyDeviceToDevice);); (20, CUDA, cudaMemcpy(workspace, transformCoeff, sizeof(int)*nOutputDims,cudaMemcpyHostToDevice );)"}
{"predict": "(20, MPI, MPI_Type_create_resized(mystruct, 0, 100, &vecs[i]);); (15, MPI, MPI_Type_create_struct(3, blockcount, displs, vecs, &mystruct);); (15, MPI, MPI_Type_commit(&mystruct);); (17, MPI, MPI_Type_free(&mystruct);); (21, MPI, MPI_Type_free(&vecs[i]);); (14, MPI, MPI_Type_vector(1, 1, stride, MPI_INT, &vecs[i]);)", "label": "(19, MPI, MPI_Type_commit(&mystruct);); (27, MPI, MPI_Type_create_hvector(0, 1, stride, MPI_INT, &vecs[i]);); (19, MPI, MPI_Type_free(&mystruct);); (8, MPI, MPI_Type_commit(&vecs[i]);); (14, MPI, MPI_Type_create_struct(3, blockcount, displs, vecs, &mystruct);); (5, MPI, MPI_Type_create_hvector(i, 1, stride, MPI_INT, &vecs[i]);); (17, MPI, MPI_Type_free(&vecs[i]);)"}
{"predict": "(28, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (54, CUDA, checkCudaErrors(cudaFree(buffer));); (14, CUDA, checkCudaErrors(cudaMalloc(&info, sizeof(int)));); (51, CUDA, checkCudaErrors(cudaFree(A));); (10, CUDA, checkCudaErrors(cudaMalloc(&buffer, sizeof(double) * bufferSize));); (43, CUDA, checkCudaErrors(cudaFree(info));); (8, CUDA, checkCudaErrors(cudaMalloc(&A, sizeof(double) * lda * n));); (34, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (43, CUDA, checkCudaErrors(cudaFree(A));)", "label": "(14, CUDA, checkCudaErrors(cudaMalloc(&info, sizeof(int)));); (14, CUDA, checkCudaErrors(cudaMalloc(&buffer, sizeof(double) * bufferSize));); (14, CUDA, checkCudaErrors(cudaMalloc(&A, sizeof(double) * lda * n));); (36, CUDA, checkCudaErrors(cusolverDnDpotrs(handle, uplo, n, 1, A, lda, x, n, info));); (37, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (43, CUDA, checkCudaErrors(cudaFree(info));); (45, CUDA, checkCudaErrors(cudaFree(buffer));); (14, CUDA, checkCudaErrors(cudaMemset(info, 0, sizeof(int)));); (46, CUDA, checkCudaErrors(cudaFree(A));)"}
{"predict": "(14, OpenMP, #pragma omp target parallel num_threads(2) uses_allocators(omp_pteam_mem_alloc) allocate(omp_pteam_mem_alloc: tmp) firstprivate(N) map(tofrom: tmp)); (2, OpenMP, #pragma omp target parallel num_threads(2) uses_allocators(omp_pteam_mem_alloc) allocate(omp_pteam_mem_alloc: tmp) firstprivate(N) map(tofrom: tmp)); (11, OpenMP, #pragma omp target parallel num_threads(2) uses_allocators(omp_large_cap_mem_alloc) allocate(omp_large_cap_mem_alloc: tmp) firstprivate(N) map(tofrom: tmp))", "label": "(12, OpenMP, #pragma omp target map(tofrom: N) map(from:tmp)); (2, OpenMP, #pragma omp target map(to: N) map(tofrom: tmp))"}
{"predict": "(4, MPI, MPI_Init(&argc, &argv);); (54, MPI, MPI_Finalize();); (6, MPI, ierr = MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, int ierr = MPI_Comm_size(MPI_COMM_WORLD, &comm_size);)", "label": "(4, MPI, MPI_Init(&argc, &argv);); (52, MPI, MPI_Finalize();); (6, MPI, ierr = MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, int ierr = MPI_Comm_size(MPI_COMM_WORLD, &comm_size);)"}
{"predict": "Output(13, CUDA, state = cudaMemcpy(cudaMins, mins, k * sizeof(float), cudaMemcpyHostToDevice);); (100, CUDA, FastllmCudaFinishOutput(output, cudaOutput);); (17, CUDA, state = cudaMemset(cudaBiasData, 0, k * sizeof(float));); (28, CUDA, float *cudaInput = (float*)FastllmCudaPrepareInput(input);); (30, CUDA, cudaFp16Output = (half *) FastllmCudaMalloc(n * k * sizeof(half));); (30, CUDA, cudaFp16Weight = (half *) FastllmCudaMalloc(k * m * sizeof(half));); (26, CUDA, float *cudaOutput = (float*)FastllmCudaPrepareOutput(output);); (3, CUDA, state = cudaMemcpy(cudaScales, weight.scales.data(), k * sizeof(float), cudaMemcpyHostToDevice);); (12, CUDA, checkCudaErrors(\"Error: CUDA error when moving bias to device!\", state);); (15, CUDA, state = cudaMemcpy(cudaBiasData, (uint8_t *) bias.cudaData, k * sizeof(float), cudaMemcpyDeviceToDevice);); (88, CUDA, FastllmCudaFinishInput(input, cudaInput);); (9, CUDA, state = cudaMalloc(&cudaBiasData, k * sizeof(float));); (2, CUDA, state = cudaMalloc(&cudaMins, k * sizeof(float));); (85, CUDA, FastllmCudaFree(cudaFp16Input);); (85, CUDA, FastllmCudaFree(cudaFp16Weight);); (85, CUDA, FastllmCudaFree(cudaFp16Output);); (1, CUDA, state = cudaMemcpy(cudaBiasData, bias.mins.data(), k * sizeof(float), cudaMemcpyHostToDevice);); (18, CUDA, float *cudaWeight = (float*)FastllmCudaPrepareWeight(weight);)", "label": "(119, CUDA, FastllmCudaFinishOutput(output, cudaOutput);); (39, CUDA, cudaFp16Input = (half *) FastllmCudaMalloc(n * m * sizeof(half));); (21, CUDA, state = cudaMemset(cudaBiasData, 0, k * sizeof(float));); (115, CUDA, FastllmCudaFinishInput(input, cudaInput);); (43, CUDA, cudaFp16Output = (half *) FastllmCudaMalloc(n * k * sizeof(half));); (9, CUDA, state = cudaMemcpy(cudaMins, mins, k * sizeof(float), cudaMemcpyHostToDevice);); (-2, CUDA, state = cudaMalloc(&cudaScales, k * sizeof(float));); (94, CUDA, FastllmCudaFree(cudaFp16Input);); (103, CUDA, FastllmCudaFree(cudaFp16Output);); (32, CUDA, cudaFp16Weight = (half *) FastllmCudaMalloc(k * m * sizeof(half));); (-1, CUDA, state = cudaMalloc(&cudaMins, k * sizeof(float));); (-6, CUDA, state = cudaMemcpy(cudaScales, weight.scales.data(), k * sizeof(float), cudaMemcpyHostToDevice);); (7, CUDA, state = cudaMalloc(&cudaBiasData, k * sizeof(float));); (12, CUDA, checkCudaErrors(\"Error: CUDA error when moving bias to device!\", state);); (7, CUDA, state = cudaMemcpy(cudaBiasData, (uint8_t*)bias.cudaData, k * sizeof(float), cudaMemcpyDeviceToDevice);); (87, CUDA, FastllmCudaFree(cudaFp16Weight);); (18, CUDA, float *cudaOutput = (float*)FastllmCudaPrepareOutput(output);); (16, CUDA, float *cudaInput = (float*)FastllmCudaPrepareInput(input);)"}
{"predict": "(8, TBB, tbb::flow::make_edge(on, *successor);); (3, TBB, tbb::flow::queue_node<int> qout(g);); (3, TBB, tbb::flow::make_edge(qin, on);); (9, TBB, g.reset(tbb::flow::rf_clear_edges);); (-1, TBB, tbb::flow::queue_node<int> qin(g);); (6, TBB, tbb::flow::remove_edge(on, *successor);); (-2, TBB, tbb::flow::graph g;); (-2, TBB, tbb::flow::broadcast_node<int> b(g);); (-4, TBB, tbb::flow::scalar_node<int> on(g, snode_body(4));); (0, TBB, Node *successor = dynamic_cast<Node *>(tbb::flow::output_port<0>(on).my_successors.back());); (-5, TBB, tbb::flow::make_edge(b, qin);); (-7, TBB, tbb::flow::broadcast_node<int> b(g);); (-7, TBB, tbb::flow::graph g;); (-7, TBB, tbb::flow::queue_node<int> qin(g);); (-8, TBB, tbb::flow::scalar_node<int> snode(g, snode_body(4));); (-5, TBB, tbb::flow::make_edge(qout, b);); (1, TBB, tbb::flow::make_edge(on, qout);); (3, TBB, tbb::flow::remove_edge(on, qout);)", "label": "(12, TBB, g.reset(tbb::flow::rf_clear_edges);); (4, TBB, tbb::flow::queue_node<int> qout(g);); (5, TBB, tbb::flow::make_edge(on,qout);); (0, TBB, tbb::flow::graph g;)"}
{"predict": "(34, CUDA, CSC(cudaGetLastError(), STATUS_FAILURE);)", "label": "(34, CUDA, CSC(cudaGetLastError(), STATUS_FAILURE);)"}
{"predict": "(8, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:tmp) num_teams(2) thread_limit(7) uses_allocators(omp_pteam_mem_alloc) allocate(omp_pteam_mem_alloc:tmp) reduction(+:tmp)); (3, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:tmp2) uses_allocators(omp_cgroup_mem_alloc) allocate(omp_cgroup_mem_alloc:tmp2) reduction(+:tmp2))", "label": "(8, OpenMP, #pragma omp target teams distribute parallel for reduction(+:tmp) map(A, N, M) map(tofrom:M))"}
{"predict": "(29, OpenMP, #pragma omp for simd collapse (2)); (19, OpenMP, #pragma omp for simd collapse (foobool(argc)), collapse (true), collapse (-5)); (1, OpenMP, #pragma omp for simd collapse ()); (27, OpenMP, #pragma omp for simd collapse(collapse(tmain<int, char, -1, -2>(argc, argv)); (4, OpenMP, #pragma omp for simd collapse (2+2))); (5, OpenMP, #pragma omp for simd collapse (foobool(1) > 0? 1 : 2)); (1, OpenMP, #pragma omp for simd collapse (4); (2, OpenMP, #pragma omp for simd collapse (foobool(argc)), collapse (argc)); (7, OpenMP, #pragma omp for simd collapse (S1)); (23, OpenMP, #pragma omp for simd collapse (1)); (-8, OpenMP, #pragma omp for simd collapse); (7, OpenMP, #pragma omp for simd collapse (foobool(argc)), collapse (argc+1))", "label": "(12, OpenMP, #pragma omp for simd collapse (foobool(1) > 0? 1 : 2)); (8, OpenMP, #pragma omp for simd collapse (2+2))); (-1, OpenMP, #pragma omp for simd collapse); (16, OpenMP, #pragma omp for simd collapse (S1)); (-1, OpenMP, #pragma omp for simd collapse (); (0, OpenMP, #pragma omp for simd collapse ()); (26, OpenMP, #pragma omp for simd collapse (2)); (10, OpenMP, #pragma omp for simd collapse (foobool(argc)), collapse (true), collapse (-5)); (-1, OpenMP, #pragma omp for simd collapse (4); (21, OpenMP, #pragma omp for simd collapse(collapse(tmain<int, char, -1, -2>(argc, argv))"}
{"predict": "(35, MPI, MPI_Info_get(fd->info, key, &tmp_val, &flag);)", "label": "(35, MPI, MPI_Bcast(&tmp_val, 1, MPI_INT, 0, fd->comm);)"}
{"predict": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(1, TBB, oneapi::tbb::tick_count main_time = oneapi::tbb::tick_count::now();); (4, TBB, oneapi::tbb::tick_count end_time = oneapi::tbb::tick_count::now();); (3, TBB, ASSERT_TRUE(end_time - main_time).seconds() < 10.0);)", "label": "(1, TBB, tbb::task_arena sequential_arena(1);)"}
{"predict": "(168, OpenCL, error = clSetKernelArg(kernel, 2, sizeof(dstPos), &dstPos);); (171, OpenCL, error = clSetKernelArg(kernel, 3, sizeof(dstStride), &dstStride);); (163, OpenCL, error = clSetKernelArg(kernel, 0, localBufferSize, NULL);); (165, OpenCL, error = clSetKernelArg(kernel, 1, sizeof(srcPos), &srcPos);); (170, OpenCL, error = clSetKernelArg(kernel, 4, sizeof(srcStride), &srcStride);)", "label": "(169, OpenCL, error = clSetKernelArg(kernel, 0, sizeof(streams[0]), &streams[0]);); (170, OpenCL, error = clSetKernelArg(kernel, 1, sizeof(streams[1]), &streams[1]);); (184, OpenCL, error = clSetKernelArg(kernel, 7, sizeof(dstStride), &dstStride);); (170, OpenCL, error = clSetKernelArg(kernel, 2, localBufferSize, NULL);); (180, OpenCL, error = clSetKernelArg(kernel, 6, sizeof(srcStride), &srcStride);)"}
{"predict": "(11, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(11, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(4, CUDA, extern __shared__ float shared_val[];)", "label": "(4, CUDA, __shared__ float shared_val[32];)"}
{"predict": "(59, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (84, CUDA, CUDA_CHECK(cudaDeviceReset());); (77, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (40, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (41, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_y), sizeof(data_type) * y.size()));); (39, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_x), sizeof(data_type) * x.size()));); (69, CUDA, CUDA_CHECK(cudaFree(d_y));); (67, CUDA, CUDA_CHECK(cudaFree(d_x));); (65, CUDA, CUDA_CHECK(cudaFree(d_A));); (29, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)", "label": "(60, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (78, CUDA, CUDA_CHECK(cudaDeviceReset());); (75, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (40, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (41, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_y), sizeof(data_type) * y.size()));); (39, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_x), sizeof(data_type) * x.size()));); (67, CUDA, CUDA_CHECK(cudaFree(d_y));); (65, CUDA, CUDA_CHECK(cudaFree(d_x));); (63, CUDA, CUDA_CHECK(cudaFree(d_A));); (30, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(37, TBB, tbb::flow::make_edge(source2, tbb::flow::input_port<1>(merge));); (39, TBB, tbb::flow::make_edge(tbb::flow::output_port<0>(merge), output);); (40, TBB, tbb::flow::make_edge(source1, tbb::flow::input_port<0>(merge));); (1, TBB, tbb::flow::graph g;)", "label": "(37, TBB, tbb::flow::make_edge(source2, tbb::flow::input_port<1>(merge));); (35, TBB, tbb::flow::make_edge(source1, tbb::flow::input_port<0>(merge));); (36, TBB, tbb::flow::make_edge(merge, output);); (-2, TBB, tbb::flow::graph g;)"}
{"predict": "(37, CUDA, CHECK(cudaMalloc(&d_matrix_org, M));); (37, CUDA, CHECK(cudaMalloc(&d_matrix_res, M));); (36, CUDA, CHECK(cudaMemcpy(d_matrix_org, h_matrix_org, M, cudaMemcpyDefault));); (44, CUDA, CHECK(cudaMemcpy(h_matrix_res, d_matrix_res, M, cudaMemcpyDefault));); (41, CUDA, CHECK(cudaEventCreate(&stop));); (48, CUDA, CHECK(cudaEventRecord(stop));); (45, CUDA, CHECK(cudaEventSynchronize(stop));); (40, CUDA, cudaEventQuery(start);); (47, CUDA, CHECK(cudaEventElapsedTime(&curr_time, start, stop));); (87, CUDA, CHECK(cudaFree(d_matrix_org));); (87, CUDA, CHECK(cudaFree(d_matrix_res));); (34, CUDA, CHECK(cudaEventRecord(start));); (30, CUDA, CHECK(cudaEventCreate(&start));); (34, CUDA, CHECK(cudaEventSynchronize(stop));); (3, CUDA, CHECK(cudaMemcpyFromSymbol(&SIZE, c_TILE_DIM, sizeof(int)));)", "label": "(37, CUDA, CHECK(cudaMalloc(&d_matrix_org, M));); (37, CUDA, CHECK(cudaMalloc(&d_matrix_res, M));); (37, CUDA, CHECK(cudaMemcpy(d_matrix_org, h_matrix_org, M, cudaMemcpyDefault));); (42, CUDA, CHECK(cudaEventCreate(&stop));); (50, CUDA, CHECK(cudaEventRecord(stop));); (46, CUDA, CHECK(cudaMemcpy(h_matrix_res, d_matrix_res, M, cudaMemcpyDefault));); (41, CUDA, cudaEventQuery(start);); (48, CUDA, CHECK(cudaEventSynchronize(stop));); (48, CUDA, CHECK(cudaEventElapsedTime(&curr_time, start, stop));); (86, CUDA, CHECK(cudaFree(d_matrix_org));); (86, CUDA, CHECK(cudaFree(d_matrix_res));); (35, CUDA, CHECK(cudaEventRecord(start));); (32, CUDA, CHECK(cudaEventCreate(&start));); (3, CUDA, CHECK(cudaMemcpyFromSymbol(&SIZE, c_TILE_DIM, sizeof(int)));)"}
{"predict": "(13, OpenCL, cl_kernel k0 = clCreateKernel(program, \"compiler_program_global_k0\", &err);); (54, OpenCL, clReleaseKernel(k0);); (54, OpenCL, clReleaseKernel(k1);); (3, OpenCL, cl_kernel k1 = clCreateKernel(program, \"compiler_program_global_k1\", &err);); (53, OpenCL, clReleaseProgram(program);)", "label": "(55, OpenCL, clReleaseProgram(program);); (52, OpenCL, clReleaseKernel(k0);); (52, OpenCL, clReleaseKernel(k1);); (11, OpenCL, cl_kernel k1 = clCreateKernel(program, \"compiler_program_global1\", &err);); (8, OpenCL, cl_kernel k0 = clCreateKernel(program, \"compiler_program_global0\", &err);)"}
{"predict": "(10, OpenMP, #pragma omp target)", "label": "(10, OpenMP, #pragma omp target map(tofrom: a[0:100])); (12, OpenMP, #pragma omp simd)"}
{"predict": "(67, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&device_buffer, input_size_bytes));); (69, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(device_buffer, input.data(), input_size_bytes, cudaMemcpyHostToDevice));); (71, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamSynchronize(stream));); (54, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamCreate(&stream));); (79, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(device_buffer));)", "label": "(66, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&device_buffer, input_size_bytes));); (56, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamCreate(&stream));); (80, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(device_buffer));); (67, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (65, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(device_buffer, input.data(), input_size_bytes, cudaMemcpyHostToDevice));)"}
{"predict": "(26, OpenMP, #pragma omp distribute parallel for simd, private(x);); (16, OpenMP, #pragma omp distribute parallel for simd private(x);); (2, OpenMP, #pragma omp distribute parallel for simd;); (10, OpenMP, #pragma omp distribute parallel for simd linear(x);); (-1, OpenMP, #pragma omp target); (22, OpenMP, #pragma omp distribute parallel for simd, linear(x);); (-3, OpenMP, #pragma omp teams)", "label": "(12, OpenMP, #pragma omp distribute parallel for simd firstprivate(x);); (5, OpenMP, #pragma omp distribute parallel for simd;); (24, OpenMP, #pragma omp distribute parallel for simd, private(x);); (0, OpenMP, #pragma omp target); (15, OpenMP, #pragma omp distribute parallel for simd private(x);); (-1, OpenMP, #pragma omp teams)"}
{"predict": "(88, OpenCL, err = clEnqueueNDRangeKernel(commQueues[0], kernel, 1, NULL, &streamCount, &local_size, numWaitEvents, waitEvents, outEvents);); (89, OpenCL, clReleaseKernel(kernel);); (88, OpenCL, clReleaseProgram(program);); (57, OpenCL, err |= clBuildProgram(program, 0, NULL, includes, NULL, NULL);); (70, OpenCL, clReleaseProgram(program);); (78, OpenCL, cl_kernel kernel = clCreateKernel(program, \"fillBufferU01\", &err);); (47, OpenCL, cl_program program = clCreateProgramWithSource(ctx, 4, sources, NULL, &err);); (81, OpenCL, err |= clSetKernelArg(kernel, 0, sizeof streams, &streams);); (81, OpenCL, err |= clSetKernelArg(kernel, 1, sizeof(number_count_per_stream), &number_count_per_stream);); (81, OpenCL, err |= clSetKernelArg(kernel, 2, sizeof(outBuffer), &outBuffer);); (72, OpenCL, err |= clSetKernelArg(kernel, 3, sizeof(streams[0]), &streams[0]);)", "label": "(95, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(streams), &streams);); (105, OpenCL, clReleaseKernel(kernel);); (85, OpenCL, err = clGetDeviceInfo(dev, CL_DEVICE_MAX_WORK_GROUP_SIZE, sizeof(local_size), &local_size, NULL);); (59, OpenCL, err = clBuildProgram(program, 0, NULL, includes, NULL, NULL);); (103, OpenCL, clReleaseProgram(program);); (48, OpenCL, cl_program program = clCreateProgramWithSource(ctx, 4, sources, NULL, &err);); (96, OpenCL, err = clEnqueueNDRangeKernel(commQueues[0], kernel, 1, NULL, &streamCount, &local_size, numWaitEvents, waitEvents, outEvents);); (89, OpenCL, err |= clSetKernelArg(kernel, 1, sizeof(number_count_per_stream), &number_count_per_stream);); (89, OpenCL, err |= clSetKernelArg(kernel, 2, sizeof(outBuffer), &outBuffer);); (68, OpenCL, cl_kernel kernel = clCreateKernel(program, \"fillBufferU01\", &err);)"}
{"predict": "(3, OpenMP, #pragma omp target)", "label": "(3, OpenMP, #pragma omp target)"}
{"predict": "(29, OpenMP, #pragma omp teams reduction(+:sum)); (26, OpenMP, #pragma omp target map(tofrom: b, sum, t) map(alloc: a)); (28, OpenMP, #pragma omp distribute parallel for reduction(+:sum)); (30, OpenMP, t[i] = omp_get_team_num();)", "label": "(28, OpenMP, #pragma omp teams reduction(+:sum)); (25, OpenMP, #pragma omp target map(tofrom: b, sum, t) map(alloc: a)); (28, OpenMP, #pragma omp distribute parallel for reduction(+:sum)); (30, OpenMP, t[i] = omp_get_team_num();)"}
{"predict": "(39, CUDA, extern __shared__ char shared_memory[];)", "label": "(38, CUDA, extern __shared__ char shared_memory[];)"}
{"predict": "(16, MPI, TLLM_CHECK(MPI_Get_count(&status, kMPI_UINT64_TAG, &count));); (40, MPI, TLLM_CHECK(MPI_Recv(tensor->data(), count, logitsDtype, fastLogitsInfo.draftParticipantId, kMPI_SPEC_DEC_DATA_TAG, worldComm.comm(), MPI_STATUS_IGNORE));); (35, MPI, TLLM_CHECK(MPI_Get_count(&status, logitsDtype, &count));); (22, MPI, MPI_Dims_create(count, 2, dims);); (17, MPI, MPI_Type_size(logitsDtype, &dims[1]);); (38, MPI, TLLM_CHECK(MPI_Recv(nullptr, 0, MPI_BYTE, fastLogitsInfo.draftParticipantId, kMPI_SPEC_DEC_DATA_TAG, worldComm.comm(), MPI_STATUS_IGNORE));); (12, MPI, MPI_Type_size(MPI_UINT64_TAG, &dims[0]);)", "label": "(28, MPI, MPICHECK(MPI_Mrecv(&dims, count, MPI_INT64_T, &msg, &status));); (36, MPI, MPICHECK(MPI_Get_count(&status, MPI_UINT8_T, &count));); (40, MPI, MPICHECK(MPI_Mrecv(tensor->data(), count, MPI_UINT8_T, &msg, &status));); (16, MPI, MPICHECK(MPI_Mrecv(&mpiId, count, MPI_UINT64_T, &msg, &status));); (20, MPI, MPICHECK(MPI_Get_count(&status, MPI_INT64_T, &count));); (11, MPI, MPICHECK(MPI_Get_count(&status, MPI_UINT64_T, &count));)"}
{"predict": "(15, CUDA, extern __shared__ float sdata[];)", "label": "(15, CUDA, extern __shared__ float sdata[];)"}
{"predict": "(11, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(11, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(24, OpenCL, error |= clSetKernelArg(kernel, 1, sizeof(dst), &dst);); (22, OpenCL, error |= clSetKernelArg(kernel, 0, sizeof(cl_mem), &dst);); (13, OpenCL, clKernelWrapper kernel = clCreateKernel(prog, \"test_usersemantic\", &error);)", "label": "(15, OpenCL, clKernelWrapper kernel = clCreateKernel(prog, \"usersemantic_test\", &error);); (22, OpenCL, error |= clSetKernelArg(kernel, 0, sizeof(dst), &dst);)"}
{"predict": "(20, CUDA, getLastCudaError(\"mergeElementaryIntervalsKernel<0> failed\\n\");); (14, CUDA, getLastCudaError(\"mergeElementaryIntervalsKernel<1> failed\\n\");)", "label": "(15, CUDA, getLastCudaError(\"mergeElementaryIntervalsKernel<1> failed\\n\");); (19, CUDA, getLastCudaError(\"mergeElementaryIntervalsKernel<0> failed\\n\");)"}
{"predict": "(87, MPI, MPI_Bcast( a,    nnz, MPI_DOUBLE, 0, grid.comm );); (119, MPI, MPI_Bcast( b, m*nrhs, MPI_DOUBLE, 0, grid.comm );); (226, MPI, MPI_Finalize();); (83, MPI, MPI_Bcast( asub, nnz, mpi_int_t,  0, grid.comm );); (22, MPI, MPI_Init( &argc, &argv );); (114, MPI, MPI_Bcast( xtrue, n*nrhs, MPI_DOUBLE, 0, grid.comm );); (81, MPI, MPI_Bcast( xa,   n+1, mpi_int_t,  0, grid.comm );); (77, MPI, MPI_Bcast( &nnz, 1,   mpi_int_t,  0, grid.comm );)", "label": "(87, MPI, MPI_Bcast( asub, nnz, mpi_int_t,  0, grid.comm );); (224, MPI, MPI_Finalize();); (83, MPI, MPI_Bcast( &nnz, 1,   mpi_int_t,  0, grid.comm );); (83, MPI, MPI_Bcast( a,    nnz, MPI_DOUBLE, 0, grid.comm );); (22, MPI, MPI_Init( &argc, &argv );); (83, MPI, MPI_Bcast( xa,   n+1, mpi_int_t,  0, grid.comm );); (78, MPI, MPI_Bcast( &n,   1,   mpi_int_t,  0, grid.comm );); (76, MPI, MPI_Bcast( &m,   1,   mpi_int_t,  0, grid.comm );)"}
{"predict": "(26, MPI, MPI_Isend(buf, cnt, MPI_INT, thread_num, tag, MPI_COMM_WORLD, &r[thread_num - 1]);); (35, MPI, t = MPI_Wtime() - t;); (28, MPI, MPI_Waitall(wsize - 1, r, MPI_STATUSES_IGNORE);); (30, MPI, t = MPI_Wtime();); (10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (24, MPI, MPI_Wait(&r[thread_num - 1], MPI_STATUS_IGNORE);)", "label": "(26, MPI, MPI_Isend(buf, cnt, MPI_INT, thread_num, tag, MPI_COMM_WORLD, &r[thread_num - 1]);); (36, MPI, t = MPI_Wtime() - t;); (21, MPI, t = MPI_Wtime();); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (29, MPI, MPI_Waitall(wsize - 1, r, MPI_STATUSES_IGNORE);); (23, MPI, MPI_Wait(&r[thread_num - 1], MPI_STATUS_IGNORE);)"}
{"predict": "(14, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate(s) lastprivate(s)); (5, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate(s)); (8, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate(s, )); (14, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate(s) lastprivate(s, )); (1, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate(s)); (8, OpenMP, #pragma omp target teams distribute parallel for simd lastprivate(s) firstprivate(s))", "label": "(8, OpenMP, #pragma omp target parallel for simd order(); (13, OpenMP, #pragma omp target parallel for simd order(concurrent); (15, OpenMP, #pragma omp target parallel for simd order(concurrent)); (2, OpenMP, #pragma omp target parallel for simd order); (-2, OpenMP, #pragma omp target parallel for simd lastprivate(s) firstprivate(s)); (6, OpenMP, #pragma omp target parallel for simd order(none)"}
{"predict": "(7, CUDA, __shared__ block_radix_sort_t::TempStorage temp_storage;)", "label": "(7, CUDA, __shared__ block_radix_sort_t::TempStorage temp_storage;)"}
{"predict": "(3, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (68, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (58, CUDA, EXPECT_EQ(cudaSuccess, cudaMemcpy(inData->basePtr(), inVec.data(), inBufSize, cudaMemcpyHostToDevice));); (72, CUDA, EXPECT_EQ(cudaSuccess, cudaMemcpy(testVec.data(), outData->basePtr(), outBufSize, cudaMemcpyDeviceToHost));); (65, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamDestroy(stream));)", "label": "(68, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (2, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (59, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(inData->basePtr(), inVec.data(), inBufSize, cudaMemcpyHostToDevice));); (72, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(testVec.data(), outData->basePtr(), outBufSize, cudaMemcpyDeviceToHost));); (65, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamDestroy(stream));)"}
{"predict": "(26, CUDA, CUDA_CHECK(cudaMalloc((void **)&dev_max, sizeof(FP) * SEQLEN * DIM));); (26, CUDA, CUDA_CHECK(cudaMalloc((void **)&dev_denom, sizeof(FP) * SEQLEN * DIM));); (45, CUDA, cudaFree(dev_max);); (45, CUDA, cudaFree(dev_denom);); (26, CUDA, CUDA_CHECK(cudaMemcpy(dev_denom, host_denom, sizeof(FP) * SEQLEN * DIM, cudaMemcpyHostToDevice));); (24, CUDA, CUDA_CHECK(cudaMemcpy(dev_max, host_max, sizeof(FP) * SEQLEN * DIM, cudaMemcpyHostToDevice));); (41, CUDA, cudaFree(QK);); (20, CUDA, CUDA_CHECK(cudaMalloc((void **)&QK, sizeof(FP) * SEQLEN * SEQLEN));)", "label": "(26, CUDA, CUDA_CHECK(cudaMalloc((void **)&dev_max, sizeof(FP) * SEQLEN * DIM));); (45, CUDA, cudaFree(dev_max);); (28, CUDA, CUDA_CHECK(cudaMemcpy(dev_denom, host_denom, sizeof(FP) * SEQLEN * DIM, cudaMemcpyHostToDevice));); (24, CUDA, CUDA_CHECK(cudaMalloc((void **)&dev_denom, sizeof(FP) * SEQLEN * DIM));); (24, CUDA, CUDA_CHECK(cudaMalloc((void **)&QK, sizeof(FP) * SEQLEN * SEQLEN));); (24, CUDA, CUDA_CHECK(cudaMemcpy(dev_max, host_max, sizeof(FP) * SEQLEN * DIM, cudaMemcpyHostToDevice));); (39, CUDA, cudaFree(QK);); (40, CUDA, cudaFree(dev_denom);)"}
{"predict": "(16, OpenMP, #pragma omp for schedule(runtime) reduction(+:total) linear(idx:1)); (13, OpenMP, thread_num = omp_get_thread_num();); (9, OpenMP, #pragma omp parallel shared(data, std::cerr, total, idx)); (11, OpenMP, #pragma omp master)", "label": "(19, OpenMP, #pragma omp for schedule(runtime) reduction(+:total) linear(idx:1)); (13, OpenMP, thread_num = omp_get_thread_num();); (15, OpenMP, #pragma omp master); (7, OpenMP, #pragma omp parallel shared(data, std::cerr, total, idx) default(none))"}
{"predict": "(52, MPI, MPI_Type_commit(&types[i+1]);)", "label": "(62, MPI, MPI_Type_commit(newtype);)"}
{"predict": "(62, CUDA, cudaFree(gpu_inputs);); (62, CUDA, cudaFree(gpu_outputs);); (58, CUDA, cudaFree(cpu_inputs);)", "label": "(21, CUDA, $GPU(cudaMemcpy(compare, gpu_outputs, sizeof(typename types<params>::output_t)*count, cudaMemcpyDeviceToHost));); (15, CUDA, $GPU(cudaMalloc((void **)&gpu_outputs, sizeof(typename types<params>::output_t)*count));); (15, CUDA, $GPU(cudaMemset(gpu_outputs, 0, sizeof(typename types<params>::output_t)*count));); (58, CUDA, $GPU(cudaFree(gpu_outputs));)"}
{"predict": "(38, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (40, CUDA, CUDA_CHECK(cudaFree(d_data));)", "label": "(38, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (40, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(443, CUDA, __shared__ int cur_send_num;); (441, CUDA, __shared__ int recv_num, finish_num;)", "label": "(45, CUDA, __shared__ doublecomplex temp2[MAXSUPER];)"}
{"predict": "(10, OpenCL, retVal = clEnqueueMapBuffer(&cmdQ, buffer.get(), CL_TRUE, CL_MAP_WRITE, 0, 1, 0, nullptr, nullptr, nullptr);); (12, OpenCL, retVal = clEnqueueUnmapMemObject(&cmdQ, buffer.get(), mappedPtr, 0, nullptr, nullptr);)", "label": "(10, OpenCL, auto mappedPtr = clEnqueueMapBuffer(&cmdQ, buffer.get(), CL_TRUE, CL_MAP_READ, 0, 1, 0, nullptr, nullptr, &retVal);); (12, OpenCL, retVal = clEnqueueUnmapMemObject(&cmdQ, buffer.get(), mappedPtr, 0, nullptr, nullptr);)"}
{"predict": "(18, OpenMP, #pragma omp target teams if (argc > 0? argv[1] : argv[2])); (33, OpenMP, #pragma omp target teams if(target : argc); (22, OpenMP, #pragma omp target teams if (S1)); (28, OpenMP, #pragma omp target teams if(target : argc argc)); (34, OpenMP, #pragma omp target teams if(target : argc) if (argc)); (36, OpenMP, #pragma omp target teams if(target : argc) if (argc) if (m)); (2, OpenMP, #pragma omp target teams if (); (38, OpenMP, #pragma omp target teams if(target : argc) if (argc) if (m) if (i)); (30, OpenMP, #pragma omp target teams if(target : argc) if (for:argc)); (34, OpenMP, #pragma omp target teams if(target : argc) if (argc) if (m) if (j)); (20, OpenMP, #pragma omp target teams if(target : argc > 0? z : d)); (3, OpenMP, #pragma omp target teams if (argc))); (35, OpenMP, #pragma omp target teams if(target : argc) if (argc) if (m) if (i) if (j)); (2, OpenMP, #pragma omp target teams if (argc))); (13, OpenMP, #pragma omp target teams if(target : argc); (16, OpenMP, #pragma omp target teams if(target : a, b, c, d, f)); (22, OpenMP, #pragma omp target teams if(target : argc) if (argc)); (4, OpenMP, #pragma omp target teams if (argc argc)); (1, OpenMP, #pragma omp target teams if (argc))); (13, OpenMP, #pragma omp target teams if(target : argv[1]=2)); (17, OpenMP, #pragma omp target teams if(target : ba)); (1, OpenMP, #pragma omp target teams if (argc))); (14, OpenMP, #pragma omp target teams if(target : ca)); (2, OpenMP, #pragma omp target teams if (argc > 0? argv[1] : argv[2", "label": "(24, OpenMP, #pragma omp target parallel firstprivate (a, b, c, d, f)); (43, OpenMP, #pragma omp target parallel shared(i)); (46, OpenMP, #pragma omp target parallel firstprivate(j)); (19, OpenMP, #pragma omp target parallel firstprivate (S1)); (6, OpenMP, #pragma omp target parallel firstprivate (); (11, OpenMP, #pragma omp target parallel firstprivate (argc,); (28, OpenMP, #pragma omp target parallel firstprivate(S2::S2s)); (35, OpenMP, #pragma omp target parallel private(i), firstprivate(i)); (12, OpenMP, #pragma omp target parallel firstprivate (argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (29, OpenMP, #pragma omp target parallel firstprivate(e, g)); (30, OpenMP, #pragma omp target parallel firstprivate(h, B::x)); (19, OpenMP, #pragma omp target parallel firstprivate(ca, z)); (16, OpenMP, #pragma omp target parallel firstprivate(ba) allocate(omp_thread_mem_alloc: ba)); (-1, OpenMP, #pragma omp target parallel firstprivate ()); (36, OpenMP, #pragma omp target parallel firstprivate(m)); (-1, OpenMP, #pragma omp target parallel firstprivate (argc); (-8, OpenMP, #pragma omp target parallel firstprivate); (29, OpenMP, #pragma omp target parallel firstprivate(i)); (18, OpenMP, #pragma omp target parallel firstprivate(S2::S2sc)); (7, OpenMP, #pragma omp target parallel firstprivate (argv[1])); (-2, OpenMP, #pragma omp target parallel firstprivate (argc > 0? argv[1] : argv[2])); (11, OpenMP, #pragma omp target parallel firstprivate(da))"}
{"predict": "(8, TBB, now = tbb::tick_count::now();); (8, TBB, tbb::tick_count start = tbb::tick_count::now();); (0, TBB, tbb::global_control control(tbb::global_control::max_allowed_parallelism, nThreads);)", "label": "(8, TBB, start = tbb::tick_count::now();); (0, TBB, tbb::tick_count now;); (8, TBB, now = tbb::tick_count::now();)"}
{"predict": "(5, OpenMP, #pragma omp barrier); (3, OpenMP, #pragma omp parallel); (6, OpenMP, int me = omp_get_thread_num();)", "label": "(17, OpenMP, #pragma omp barrier); (2, OpenMP, #pragma omp parallel); (3, OpenMP, int me = omp_get_thread_num();)"}
{"predict": "(27, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_IMAGE_MAX_ARRAY_SIZE, sizeof( maxArraySize ), &maxArraySize, NULL );); (29, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_GLOBAL_MEM_SIZE, sizeof( memSize ), &memSize, NULL );); (27, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_MAX_MEM_ALLOC_SIZE, sizeof( maxAllocSize ), &maxAllocSize, NULL );); (23, OpenCL, error = clGetDeviceInfo( device, CL_DEVICE_IMAGE2D_MAX_WIDTH, sizeof( maxWidth ), &maxWidth, NULL );)", "label": "(27, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_IMAGE_MAX_ARRAY_SIZE, sizeof( maxArraySize ), &maxArraySize, NULL );); (28, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_GLOBAL_MEM_SIZE, sizeof( memSize ), &memSize, NULL );); (26, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_MAX_MEM_ALLOC_SIZE, sizeof( maxAllocSize ), &maxAllocSize, NULL );); (23, OpenCL, error = clGetDeviceInfo( device, CL_DEVICE_IMAGE2D_MAX_WIDTH, sizeof( maxWidth ), &maxWidth, NULL );)"}
{"predict": "(11, CUDA, __shared__ uninitialized_array<T, COLBS * DIM> sum;)", "label": "(11, CUDA, __shared__ uninitialized_array<T, SUM_SIZE> sum;)"}
{"predict": "(3, TBB, static const size_t number_of_threads = tbb::task_scheduler_init::default_num_threads();); (1, TBB, typedef tbb::enumerable_thread_specific< cache_type > cache_type;); (10, TBB, tbb::global_control s(tbb::global_control::max_allowed_parallelism,number_of_threads);)", "label": "(10, TBB, static const size_t number_of_threads = 8 * tbb::task_scheduler_init::default_num_threads();)"}
{"predict": "(7, CUDA, __shared__ typename BlockReduce::TempStorage temp_storage;)", "label": "(7, CUDA, __shared__ typename BlockReduce::TempStorage temp_storage;)"}
{"predict": "(1, TBB, using namespace tbb::internal;)", "label": "(2, TBB, using namespace tbb::internal;)"}
{"predict": "(2, OpenMP, #pragma omp taskyield untied); (2, OpenMP, #pragma omp taskyield)", "label": "(2, OpenMP, #pragma omp taskyield allocate(argc)); (3, OpenMP, #pragma omp taskyield untied); (3, OpenMP, #pragma omp taskyield)"}
{"predict": "(9, CUDA, cudaDeviceSynchronize();); (17, CUDA, cudaFree(d_ptr);); (1, CUDA, double *d_ptr = thrust::device_malloc(n*sizeof(double));); (9, CUDA, cudaMemcpy(ret_host, d_ptr, sizeof(double), cudaMemcpyDeviceToHost);)", "label": "(14, CUDA, cudaError_t cuerr = cudaMemcpy(ret_host, ret_ptr, (1)*sizeof(double), cudaMemcpyDeviceToHost);); (3, CUDA, double* d_ptr = thrust::raw_pointer_cast(dv_ptr);); (7, CUDA, thrust::device_ptr<double> ret_dev_ptr = thrust::min_element(thrust::device, dv_ptr, dv_ptr+n);); (10, CUDA, double *ret_ptr = thrust::raw_pointer_cast(ret_dev_ptr);); (-1, CUDA, thrust::device_ptr<double> dv_ptr = thrust::device_malloc(n*sizeof(double));); (14, CUDA, thrust::device_free(dv_ptr);)"}
{"predict": "(14, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(14, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(37, MPI, MPI_Pack(a, 1, xpose, buffer, bufsize, &position, MPI_COMM_WORLD);); (55, MPI, MPI_Type_free(&row);); (28, MPI, MPI_Type_commit(&xpose);); (53, MPI, MPI_Type_free(&xpose);); (34, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (25, MPI, MPI_Type_hvector(100, 1, sizeofint, row, &xpose);); (20, MPI, MPI_Type_get_extent(MPI_INT, &tmp_lb, &sizeofint);); (36, MPI, MPI_Unpack(buffer, bufsize, &position, b, 100 * 100, MPI_INT, MPI_COMM_WORLD);); (20, MPI, MPI_Type_vector(100, 1, 100, MPI_INT, &row);); (21, MPI, MPI_Pack_size(1, xpose, MPI_COMM_WORLD, &bufsize);)", "label": "(25, MPI, MPI_Type_get_extent(MPI_INT, &tmp_lb, &sizeofint);); (39, MPI, MPI_Pack(a, 1, xpose, buffer, bufsize, &position, MPI_COMM_WORLD);); (55, MPI, MPI_Type_free(&row);); (27, MPI, MPI_Type_commit(&xpose);); (52, MPI, MPI_Type_free(&xpose);); (28, MPI, MPI_Pack_size(1, xpose, MPI_COMM_WORLD, &bufsize);); (23, MPI, MPI_Type_create_hvector(100, 1, sizeofint, row, &xpose);); (31, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (36, MPI, MPI_Unpack(buffer, bufsize, &position, b, 100 * 100, MPI_INT, MPI_COMM_WORLD);); (19, MPI, MPI_Type_vector(100, 1, 100, MPI_INT, &row);)"}
{"predict": "(34, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (36, CUDA, CUDA_CHECK(cudaFree(d_data));)", "label": "(34, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (36, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(96, OpenMP, #pragma omp parallel reduction(+ : t0, tInit, ratio, vgh_t, val_t, nspheremoves, dNumVGHCalls)); (113, OpenMP, #pragma omp critical); (171, OpenMP, #pragma omp critical reduction(+ : nknots_copy)); (100, OpenMP, const int np = omp_get_num_threads();); (100, OpenMP, const int ip = omp_get_thread_num();); (132, OpenMP, #pragma omp parallel reduction(+ : vgh_t, val_t, nspheremoves, dNumVGHCalls)); (180, OpenMP, #pragma omp atomic read); (181, OpenMP, #pragma omp atomic); (175, OpenMP, #pragma omp parallel reduction(+ : nptcl)); (113, OpenMP, #pragma omp master); (130, OpenMP, #pragma omp atomic write); (85, OpenMP, #pragma omp parallel reduction(+ : t0, tInit, ratio, vgh_t, val_t, nspheremoves, dNumVGHCalls))", "label": "(115, OpenMP, ncrews = omp_get_max_threads();); (98, OpenMP, const int np = omp_get_num_threads();); (189, OpenMP, #pragma omp barrier); (233, OpenMP, double dTotalThreads = omp_get_max_threads();); (268, OpenMP, double nMajorThreads  = omp_get_max_threads();); (137, OpenMP, #pragma omp parallel reduction(+ : vgh_t_loc, v_t_loc, my_vals)); (106, OpenMP, #pragma omp master); (278, OpenMP, cout << \"\\nMPI: \" << nmpi << \" and Threads: \" << omp_get_max_threads();); (92, OpenMP, const int ip = omp_get_thread_num();); (86, OpenMP, #pragma omp parallel reduction(+ : t0, tInit, ratio, vgh_t, val_t, nspheremoves, dNumVGHCalls))"}
{"predict": "(171, CUDA, cudaFree(c_device);); (35, CUDA, cudaDeviceSynchronize();); (36, CUDA, cudaEventRecord(stop, 0);); (36, CUDA, cudaEventSynchronize(stop);); (36, CUDA, cudaEventElapsedTime(&time, start, stop);); (44, CUDA, cudaMemcpy(c_host, c_device, N * sizeof(half), cudaMemcpyDeviceToHost);); (30, CUDA, cudaEventRecord(start, 0);); (34, CUDA, cudaEventDestroy(start);); (34, CUDA, cudaEventDestroy(stop);); (3, CUDA, cudaMalloc((void **)&a_device, N * sizeof(half));); (16, CUDA, cudaMemcpy(b_device, b_host, N * sizeof(half), cudaMemcpyHostToDevice);); (3, CUDA, cudaMemcpy(a_device, a_host, N * sizeof(half), cudaMemcpyHostToDevice);); (24, CUDA, cudaEventCreate(&stop);); (14, CUDA, cudaMalloc((void **)&c_device, N * sizeof(half));); (12, CUDA, cudaMalloc((void **)&b_device, N * sizeof(half));); (20, CUDA, cudaEventCreate(&start);); (14, CUDA, cudaMemcpy(b_device, b_host, N * sizeof(half), cudaMemcpyHostToDevice);)", "label": "(25, CUDA, cudaMalloc((void **)&c_device, N * sizeof(half));); (49, CUDA, cudaEventSynchronize(stop);); (50, CUDA, cudaEventDestroy(start);); (155, CUDA, cudaFree(a_device);); (32, CUDA, cudaDeviceSynchronize();); (155, CUDA, cudaFree(c_device);); (43, CUDA, cudaEventRecord(stop, 0);); (46, CUDA, cudaEventDestroy(stop);); (151, CUDA, cudaFree(b_device);); (33, CUDA, cudaEventRecord(start, 0);); (11, CUDA, cudaMemcpy(b_device, b_host, N * sizeof(half), cudaMemcpyHostToDevice);); (30, CUDA, cudaEventCreate(&stop);); (6, CUDA, cudaMalloc((void **)&b_device, N * sizeof(half));); (-2, CUDA, cudaMalloc((void **)&a_device, N * sizeof(half));); (37, CUDA, cudaEventElapsedTime(&time, start, stop);); (-1, CUDA, cudaMemcpy(a_device, a_host, N * sizeof(half), cudaMemcpyHostToDevice);); (24, CUDA, cudaEventCreate(&start);); (39, CUDA, cudaMemcpy(c_host, c_device, N * sizeof(half), cudaMemcpyDeviceToHost);)"}
{"predict": "(28, CUDA, CHECK(cudaFree(x));); (28, CUDA, CHECK(cudaFree(y));); (28, CUDA, CHECK(cudaFree(z));); (6, CUDA, CHECK(cudaMalloc((void **)&z, M));); (2, CUDA, CHECK(cudaSetDevice(device_id));); (19, CUDA, CHECK(cudaGetLastError());); (19, CUDA, CHECK(cudaDeviceSynchronize());); (0, CUDA, CHECK(cudaMalloc((void **)&x, M));); (20, CUDA, CHECK(cudaMemcpy(y, x, M, cudaMemcpyDeviceToDevice));); (0, CUDA, CHECK(cudaMalloc((void **)&y, M));); (20, CUDA, CHECK(cudaMemcpy(x, y, M, cudaMemcpyDeviceToDevice));)", "label": "(27, CUDA, CHECK(cudaMemPrefetchAsync(z, M, cudaCpuDeviceId, NULL));); (20, CUDA, CHECK(cudaMemPrefetchAsync(x, M, device_id, NULL));); (20, CUDA, CHECK(cudaMemPrefetchAsync(y, M, device_id, NULL));); (7, CUDA, CHECK(cudaMallocManaged((void **)&z, M));); (28, CUDA, CHECK(cudaFree(x));); (-2, CUDA, CHECK(cudaGetDevice(&device_id));); (2, CUDA, CHECK(cudaMallocManaged((void **)&x, M));); (26, CUDA, CHECK(cudaFree(y));); (26, CUDA, CHECK(cudaFree(z));); (14, CUDA, CHECK(cudaMemPrefetchAsync(z, M, device_id, NULL));); (19, CUDA, CHECK(cudaDeviceSynchronize());); (-2, CUDA, CHECK(cudaMallocManaged((void **)&y, M));)"}
{"predict": "(13, CUDA, HANDLE_ERROR( cudaSetDevice( dev ) );); (53, CUDA, HANDLE_ERROR( cudaGraphicsUnmapResources( 1, &resource, 0 ) );); (8, CUDA, HANDLE_ERROR( cudaGLSetGLDevice( dev ) );); (37, CUDA, HANDLE_ERROR( cudaGraphicsMapResources( 1, &resource, 0 ) );); (38, CUDA, HANDLE_ERROR( cudaGraphicsSynchronize( 0 ) );); (3, CUDA, HANDLE_ERROR( cudaGetDeviceProperties( &prop, dev ) );); (10, CUDA, HANDLE_ERROR( cudaGraphicsRegisterResource( &resource, 0 ) );); (38, CUDA, HANDLE_ERROR( cudaGraphicsUnregisterResource( &resource ) );); (3, CUDA, HANDLE_ERROR( cudaSetDeviceFlags( cudaDeviceMapHost ) );)", "label": "(14, CUDA, HANDLE_ERROR( cudaGLSetGLDevice( dev ) );); (51, CUDA, HANDLE_ERROR( cudaGraphicsUnmapResources( 1, &resource, NULL ) );); (39, CUDA, HANDLE_ERROR( cudaGraphicsMapResources( 1, &resource, NULL ) );); (4, CUDA, HANDLE_ERROR( cudaChooseDevice( &dev, &prop ) );)"}
{"predict": "(20, MPI, MPI_Init(&argc, &argv);); (25, MPI, MPI_Finalize();); (19, MPI, MPI_Comm_size(MPI_COMM_WORLD, &num_procs);); (17, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myid);)", "label": "(19, MPI, MPI_Init(&argc, &argv);); (25, MPI, MPI_Finalize();); (19, MPI, MPI_Comm_size(MPI_COMM_WORLD, &num_procs);); (17, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myid);)"}
{"predict": "(26, OpenMP, #pragma omp for private(int)); (47, OpenMP, #pragma omp for private(x, y, z)); (2, OpenMP, #pragma omp for private(); (28, OpenMP, #pragma omp for private(0)); (13, OpenMP, #pragma omp for private(, )); (35, OpenMP, #pragma omp for private(x)); (16, OpenMP, #pragma omp for private()); (38, OpenMP, #pragma omp for private(x, y)); (0, OpenMP, #pragma omp for private(,); (36, OpenMP, #pragma omp parallel shared(x)); (-6, OpenMP, #pragma omp parallel); (1, OpenMP, #pragma omp for private(, ))", "label": "(26, OpenMP, #pragma omp for private(int)); (10, OpenMP, #pragma omp for private(,); (35, OpenMP, #pragma omp for private(x)); (2, OpenMP, #pragma omp for private(); (-2, OpenMP, #pragma omp parallel); (26, OpenMP, #pragma omp for private(0)); (10, OpenMP, #pragma omp for private(, )); (34, OpenMP, #pragma omp for private(x, y)); (13, OpenMP, #pragma omp for private()); (36, OpenMP, #pragma omp for private(x, y, z))"}
{"predict": "(9, TBB, test_extract<tbb::flow::rejecting>();); (3, TBB, tbb::task_scheduler_init init(p);); (5, TBB, test_extract<tbb::flow::queueing>();)", "label": "(7, TBB, test_reserving_nodes<tbb::flow::write_once_node, size_t>();); (3, TBB, tbb::task_scheduler_init init(p);); (8, TBB, test_extract_on_node<tbb::flow::write_once_node, int>();)"}
{"predict": "(14, MPI, MPI_Send(buffer, buf_size, MPI_BYTE, 1, tag1, MPI_COMM_WORLD);); (15, MPI, MPI_Recv(buffer, buf_size, MPI_BYTE, 0, tag2, MPI_COMM_WORLD, &status);); (16, MPI, MPI_Recv(buffer, buf_size, MPI_BYTE, 0, tag1, MPI_COMM_WORLD, &status);); (10, MPI, MPI_Send(buffer, buf_size, MPI_BYTE, 1, tag2, MPI_COMM_WORLD);); (14, MPI, MPI_Wait(&request1, &status);); (14, MPI, MPI_Wait(&request2, &status);); (9, MPI, MPI_Send(buffer, buf_size, MPI_BYTE, 1, tag1, MPI_COMM_WORLD);); (6, MPI, MPI_Recv(buffer, buf_size, MPI_BYTE, 0, tag1, MPI_COMM_WORLD, &status);); (10, MPI, MPI_Request_free(&request1);); (10, MPI, MPI_Request_free(&request2);); (3, MPI, MPI_Send(buffer, buf_size, MPI_BYTE, 1, tag2, MPI_COMM_WORLD);); (3, MPI, MPI_Recv(buffer, buf_size, MPI_BYTE, 0, tag2, MPI_COMM_WORLD, &status);); (3, MPI, MPI_Send(buffer, buf_size, MPI_BYTE, 1, tag1, MPI_COMM_WORLD);); (4, MPI, MPI_Recv(buffer, buf_size, MPI_BYTE, 0, tag1, MPI_COMM_WORLD, &status);); (0, MPI, MPI_Send(buffer, buf_size, MPI_BYTE, 1, tag2, MPI_COMM_WORLD);); (0, MPI, MPI_Send(buffer, buf_size, MPI_BYTE, 1, tag1, MPI_COMM_WORLD);); (5, MPI, MPI_Request_create(buffer, buf_size, MPI_BYTE, 0, tag2, MPI_COMM_WORLD, &request2);); (5, MPI,", "label": "(13, MPI, MPI_Isend(buffer, buf_size, MPI_BYTE, 1, tag1, MPI_COMM_WORLD, &request1);); (15, MPI, MPI_Wait(&request2, &status);); (12, MPI, MPI_Isend(buffer, buf_size, MPI_BYTE, 1, tag2, MPI_COMM_WORLD, &request2);); (16, MPI, MPI_Irecv(buffer, buf_size, MPI_BYTE, 0, tag1, MPI_COMM_WORLD, &request2);); (11, MPI, MPI_Wait(&request1, &status);); (13, MPI, MPI_Irecv(buffer, buf_size, MPI_BYTE, 0, tag2, MPI_COMM_WORLD, &request1);)"}
{"predict": "(15, CUDA, cudaDeviceSynchronize();)", "label": "(15, CUDA, cudaDeviceSynchronize();)"}
{"predict": "(12, OpenMP, #pragma omp parallel for private(i,j,k,index))", "label": "(12, OpenMP, #pragma omp parallel for private(i,j,k) num_threads(dtn(border, 1)))"}
{"predict": "(36, OpenMP, #pragma omp target parallel for map(a, b))", "label": "(36, OpenMP, #pragma omp target parallel for map(a, b))"}
{"predict": "(4, CUDA, extern __shared__ int shared[];)", "label": "(4, CUDA, extern __shared__ int shared[];)"}
{"predict": "(36, OpenMP, omp_target_free(d_a, DefaultDeviceNum);); (16, OpenMP, d_a = (int *)omp_target_alloc(DataSize, DefaultDeviceNum);); (28, OpenMP, omp_target_disassociate_ptr(p_a, DefaultDeviceNum);); (13, OpenMP, omp_target_associate_ptr(p_a, d_a, DataSize, 0, DefaultDeviceNum);); (18, OpenMP, #pragma omp target update to(p_a[:N])); (21, OpenMP, omp_target_h2d(p_a, d_a, DataSize, 0, DefaultDeviceNum);)", "label": "(32, OpenMP, omp_target_free(d_a, DefaultDeviceNum);); (23, OpenMP, #pragma omp target update to(p_a)); (12, OpenMP, d_a = (int *)omp_target_alloc(DataSize, DefaultDeviceNum);); (6, OpenMP, int DefaultDeviceNum = omp_get_default_device();); (14, OpenMP, omp_target_associate_ptr(p_a, d_a, DataSize, 0, DefaultDeviceNum);); (23, OpenMP, omp_target_disassociate_ptr(p_a, DefaultDeviceNum);)"}
{"predict": "(11, TBB, tbb::filter<void, void> one_filter( filter_table[i], i_filter );); (11, TBB, tbb::parallel_pipeline( n_tokens, one_filter, context... );)", "label": "(9, TBB, tbb::parallel_pipeline( n_tokens, one_filter, context... );); (5, TBB, tbb::filter<void, void> one_filter( filter_table[i], i_filter );)"}
{"predict": "(13, MPI, MPI_Bcast(dims, RANK, MPI_INT, 0, comm);); (1, MPI, MPI_Comm_rank(comm, &comm_rank);)", "label": "(13, MPI, MPI_Bcast(dims, 2, MPI_INT, 0, comm);); (2, MPI, MPI_Comm_rank(comm, &comm_rank);)"}
{"predict": "(9, CUDA, CUDA_CHECK(cudaGetVersion(&runtime_version));); (6, CUDA, CUDA_CHECK(cudaDriverGetVersion(&driver_version));)", "label": "(10, CUDA, CHECK_CUDA_ERROR(cudaRuntimeGetVersion(&runtime_version));); (72, CUDA, CHECK_CUDA_ERROR(cudaSetDevice(dev));); (26, CUDA, CHECK_CUDA_ERROR(cudaGetDeviceProperties(&deviceProp, dev));); (3, CUDA, CHECK_CUDA_ERROR(cudaDriverGetVersion(&driver_version));); (20, CUDA, CHECK_CUDA_ERROR(cudaGetDeviceProperties(&deviceProp, i));)"}
{"predict": ", &host_ws_(183, CUDA, cudaMemsetAsync(dv_sum_gpu, 0, batch_size * sizeof(float), cuda_stream);); (183, CUDA, cudaMemsetAsync(dv_max_gpu, -10000.0f, batch_size * sizeof(float), cuda_stream);); (142, CUDA, cudaCheck(cudaMalloc(&k_span_array_device, batch_size * max_num_spans * size_per_head * sizeof(T)));); (142, CUDA, cudaCheck(cudaMalloc(&v_span_array_device, batch_size * max_num_spans * size_per_head * sizeof(T)));); (184, CUDA, cudaCheck(cudaDeviceSynchronize());); (144, CUDA, cudaCheck(cudaMalloc(&dv_mean_gpu, batch_size * sizeof(float)));); (142, CUDA, cudaCheck(cudaMalloc(&dv_max_gpu, batch_size * sizeof(float)));); (139, CUDA, cudaCheck(cudaMalloc(&dv_sum_gpu, batch_size * sizeof(float)));); (140, CUDA, cudaCheck(cudaMalloc(&dv_std_gpu, batch_size * sizeof(float)));); (138, CUDA, cudaCheck(cudaMalloc(&dv_min_gpu, batch_size * sizeof(float)));); (176, CUDA, cudaMemsetAsync(dv_min_gpu, -10000.0f, batch_size * sizeof(float), cuda_stream);); (141, CUDA, cudaCheck(cudaMalloc(&dv_norm_gpu, batch_size * sizeof(float)));)", "label": "(90, CUDA, ASSERT_EQ(cudaSuccess, cudaMallocHost(&host_ws, host_ws_bytes));); (297, CUDA, ASSERT_EQ(cudaSuccess, cudaEventDestroy(ref_start));); (230, CUDA, ASSERT_EQ(cudaSuccess, cudaEventElapsedTime(&ref_time, ref_start, ref_end));); (297, CUDA, ASSERT_EQ(cudaSuccess, cudaEventDestroy(out_start));); (297, CUDA, ASSERT_EQ(cudaSuccess, cudaEventDestroy(out_end));); (249, CUDA, ASSERT_EQ(cudaSuccess, cudaEventElapsedTime(&out_time, out_start, out_end));); (201, CUDA, ASSERT_EQ(cudaSuccess, cudaEventCreate(&ref_end));); (292, CUDA, ASSERT_EQ(cudaSuccess, cudaEventDestroy(ref_end));); (244, CUDA, ASSERT_EQ(cudaSuccess, cudaEventRecord(out_end, cuda_stream));); (199, CUDA, ASSERT_EQ(cudaSuccess, cudaEventCreate(&out_start));); (199, CUDA, ASSERT_EQ(cudaSuccess, cudaEventCreate(&out_end));); (220, CUDA, ASSERT_EQ(cudaSuccess, cudaEventSynchronize(ref_end));); (241, CUDA, ASSERT_EQ(cudaSuccess, cudaEventSynchronize(out_end));); (217, CUDA, ASSERT_EQ(cudaSuccess, cudaEventRecord(ref_end, cuda_stream));); (192, CUDA, ASSERT_EQ(cudaSuccess, cudaEventCreate(&ref_start));)"}
{"predict": "(18, CUDA, extern __shared__ uint32_t sharedMemory[];); (20, CUDA, __shared__ uint32_t s0[1024];)", "label": "(18, CUDA, __shared__ uint32_t sharedMemory[1024];); (29, CUDA, extern __shared__ uint32_t shuffleMem[];)"}
{"predict": "(6, MPI, MPI_Attr_get(comm, lkey[i], &val_p, &flag);)", "label": "(6, MPI, MPI_Comm_get_attr(comm, lkey[i], &val_p, &flag);)"}
{"predict": "(91, CUDA, cudaDeviceSynchronize();); (88, CUDA, checkCudaErrors(cudaGetLastError());); (127, CUDA, texture->Unmap(textureCUDA);)", "label": "(89, CUDA, CUDA(cudaGeneratePointGrid(points, GRID_N, GRID_WORLD_SIZE, apptime()));); (89, CUDA, CUDA(cudaDeviceSynchronize());)"}
{"predict": "(2, OpenMP, #pragma omp target teams distribute parallel for simd num_teams); (4, OpenMP, #pragma omp target teams distribute parallel for simd num_teams (); (30, OpenMP, #pragma omp target teams distribute parallel for simd num_teams (num_teams(tmain<int, -1>(argc)), num_teams (3.14)); (12, OpenMP, #pragma omp target teams distribute parallel for simd num_teams (argc > 0? argv[1] : argv[2])); (22, OpenMP, #pragma omp target teams distribute parallel for simd num_teams (S1)); (27, OpenMP, #pragma omp target teams distribute parallel for simd num_teams (num_teams(tmain<int, 1>(argc)), num_teams (num_teams(tmain<int, -1>(argc))); (12, OpenMP, #pragma omp target teams distribute parallel for simd num_teams (argc + z))); (17, OpenMP, #pragma omp target teams distribute parallel for simd num_teams (argc), num_teams (argc+1)); (20, OpenMP, #pragma omp target teams distribute parallel for simd num_teams (-2)); (5, OpenMP, #pragma omp target teams distribute parallel for simd num_teams (argc))); (-3, OpenMP, #pragma omp target teams distribute parallel for simd num_teams ()); (1, OpenMP, #pragma omp target teams distribute parallel for simd num_teams (argc))); (16, OpenMP, #pragma omp target teams distribute parallel for simd num_teams (argc + argc*z)); (23, OpenMP, #pragma omp target teams distribute parallel for simd num_teams (num_teams(tmain<int, -1>(argc)))", "label": "(29, OpenMP, #pragma omp target teams distribute num_teams (-2)); (4, OpenMP, #pragma omp target teams distribute num_teams (); (30, OpenMP, #pragma omp target teams distribute num_teams (-10u)); (5, OpenMP, #pragma omp target teams distribute num_teams ()); (7, OpenMP, #pragma omp target teams distribute num_teams (argc); (15, OpenMP, #pragma omp target teams distribute num_teams (argc + argc+z)); (17, OpenMP, #pragma omp target teams distribute num_teams (argc), num_teams (argc+1)); (28, OpenMP, #pragma omp target teams distribute num_teams (3.14)); (-6, OpenMP, #pragma omp target teams distribute num_teams); (5, OpenMP, #pragma omp target teams distribute num_teams (argc))); (7, OpenMP, #pragma omp target teams distribute num_teams (argc > 0? argv[1] : argv[2])); (15, OpenMP, #pragma omp target teams distribute num_teams (S1))"}
{"predict": "(14, CUDA, double term = thrust::reduce(temp_vec.begin(), temp_vec.end());); (7, CUDA, thrust::device_vector<double> temp_vec(n);); (12, CUDA, thrust::device_vector<double> temp_vec2(n);); (10, CUDA, thrust::copy(thrust::device, dev_begin, dev_end, temp_vec.begin());); (4, CUDA, thrust::device_vector<double> dv_ptr(n);); (11, CUDA, thrust::exclusive_scan(thrust::device, dev_begin, dev_end, temp_vec2.begin());); (4, CUDA, thrust::device_vector<double> dev_begin(dv_ptr.begin());); (13, CUDA, thrust::device_vector<double> dev_end(dv_ptr.end());)", "label": "(14, CUDA, double term = thrust::reduce(thrust::device, v_temp.begin(), v_temp.end(), 0.0, thrust::plus<double>());); (7, CUDA, thrust::device_vector<double> v_temp(n);); (7, CUDA, double* dv_ptr = thrust::raw_pointer_cast(v_temp.data());)"}
{"predict": "(2, CUDA, extern __shared__ int sums[];)", "label": "(2, CUDA, extern __shared__ int sums[];)"}
{"predict": "(21, MPI, double start_time = MPI_Wtime();)", "label": "(24, MPI, double start_time = MPI_Wtime();); (16, MPI, errcode = MPI_Grequest_complete(aio_reqlist[i]->req);)"}
{"predict": "(2, OpenMP, #pragma omp parallel); (1, OpenMP, #pragma omp single)", "label": "(13, OpenMP, printf(\"main parallel 2 from %d of %d\\n\", omp_get_thread_num(), omp_get_num_threads() );); (1, OpenMP, #pragma omp parallel); (2, OpenMP, printf(\"main parallel 1 from %d of %d\\n\", omp_get_thread_num(), omp_get_num_threads() );)"}
{"predict": "(17, OpenMP, t = omp_get_thread_num();); (7, OpenMP, #pragma omp parallel for)", "label": "(8, OpenMP, #pragma omp parallel for default(none) private(index) shared(count, failed, params, max_threads, local, saved_key, cur_salt, crypt_out)); (15, OpenMP, int t = omp_get_thread_num();)"}
{"predict": "(144, OpenCL, error = clSetKernelArg(kernel, 1, sizeof(tinfo->inBuf), &tinfo->inBuf);)", "label": "(146, OpenCL, error = clSetKernelArg(kernel, 1, sizeof(tinfo->inBuf), &tinfo->inBuf);)"}
{"predict": "(12, MPI, MPI_Info_get(i1, \"key2\", sizeof(value), value, &flag);); (10, MPI, MPI_Info_get(i1, \"key1\", sizeof(value), value, &flag);); (7, MPI, MPI_Info_create(&i1);); (8, MPI, MPI_Info_set(i1, \"key1\", \"value1\");); (18, MPI, MPI_Info_free(&i1);); (11, MPI, MPI_Info_free(&i2);); (5, MPI, MPI_Info_set(i1, \"key2\", \"value2\");); (2, MPI, MPI_Info_create(&i2);)", "label": "(20, MPI, MPI_Info_get(i1, (char *) \"key1\", 64, value, &flag);); (28, MPI, MPI_Info_free(&i1);); (11, MPI, MPI_Info_set(i2, (char *) \"key2\", (char *) \"value2\");); (9, MPI, MPI_Info_set(i1, (char *) \"key1\", (char *) \"value1\");); (11, MPI, MPI_Info_get(i1, (char *) \"key2\", 64, value, &flag);); (25, MPI, MPI_Info_free(&i2);); (3, MPI, MPI_Info_create(&i1);); (3, MPI, MPI_Info_create(&i2);)"}
{"predict": "(36, CUDA, thrust::host_vector<double> A_host(100);); (18, CUDA, thrust::complex<double> BBB = static_cast<thrust::complex<double>>(B[20]);); (31, CUDA, multi::array<double, 2, thrust::device_allocator<double>> Bdev(Adev);); (71, CUDA, thrust::host_vector<int> H(4);); (71, CUDA, thrust::host_vector<int> H2(4);); (54, CUDA, multi::array<double, 2, thrust::device_allocator<double>> C(A);); (12, CUDA, thrust::complex<double> AAA = static_cast<thrust::complex<double>>(A[20]);); (54, CUDA, multi::array<double, 2> B(100, 100, Alloc());); (10, CUDA, thrust::complex<double> CCC = static_cast<thrust::complex<double>>(C[20]);); (33, CUDA, multi::array<double, 2, Alloc> A_dev(3, 3, 0.);); (5, CUDA, thrust::complex<double> V = static_cast<thrust::complex<double>>(A[20]);); (5, CUDA, thrust::complex<double> W = static_cast<thrust::complex<double>>(B[20]);); (3, CUDA, thrust::complex<double> X = static_cast<thrust::complex<double>>(C[20]);); (6, CUDA, std::cout << \"iota? \"<< X.real() <<\" \"<< X.imag() <<\" \"<< std::endl;); (6, CUDA, std::cout << \"iota? \"<< V.real() <<\" \"<< V.imag() <<\" \"<< std::endl;); (2, CUDA, std::cout << \"iota? \"<< A.(143, CUDA, using namespace multi = boost::multi;); (63, CUDA, multi::array<double, 1, Alloc> A_host(100, 99.);); (65, CUDA, multi::array<double, 2, Alloc> A(100,", "label": "(15, CUDA, multi::array<double, 1, thrust::device_allocator<double>> BB(10, 99.);); (34, CUDA, thrust::copy(begin(rotated(A2)[0]), end(rotated(A2)[0]), begin(rotated(B2)[0]));); (16, CUDA, multi::array<std::complex<double>, 1, thrust::device_allocator<std::complex<double> >> BBB_cpy = BBB;); (14, CUDA, multi::array<std::complex<double>, 1, thrust::device_allocator<std::complex<double> >> BBB(10, 99.);); (97, CUDA, thrust::device_vector<int> D = H;); (78, CUDA, multi::array<int, 1, thrust::device_allocator<int>> H2(4, 99); assert(size(H2) == 4);); (69, CUDA, thrust::host_vector<int> H(4);); (21, CUDA, thrust::copy(begin(B), end(B), begin(C));)"}
{"predict": "(8, OpenMP, const std::string comp_name(x.first);)", "label": "(8, OpenMP, const std::string comp_name(x.first);)"}
{"predict": "(52, OpenCL, clReleaseProgram(program);); (9, OpenCL, cl_program program = clCreateProgramWithSource(context, 1, &source, NULL, &status);); (26, OpenCL, status = clBuildProgram(program, 1, &device, buildOptions, NULL, NULL);)", "label": "(27, OpenCL, status = clBuildProgram(program, 1, &device, buildOptions, NULL, NULL);); (9, OpenCL, cl_program program = clCreateProgramWithSource(context,1, &source, NULL, &status);)"}
{"predict": "(15, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)", "label": "(15, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(2, OpenMP, #pragma omp target teams distribute parallel for simd)", "label": "(2, OpenMP, #pragma omp target teams distribute parallel for simd)"}
{"predict": "(34, MPI, MPI_Get(&locbuf[i], 1, MPI_INT, target_rank, i, 1, MPI_INT, win);); (44, MPI, MPI_Win_unlock(rank, win);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (25, MPI, MPI_Barrier(MPI_COMM_WORLD);); (44, MPI, MPI_Win_free(&win);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (26, MPI, MPI_Win_flush_local(target_rank, win);); (21, MPI, MPI_Win_flush(target_rank, win);); (41, MPI, MPI_Free_mem(winbuf);); (13, MPI, MPI_Win_unlock_all(win);); (10, MPI, MPI_Win_lock_all(0, win);)", "label": "(25, MPI, MPI_Win_unlock(rank, win);); (48, MPI, MPI_Win_unlock_all(win);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (31, MPI, MPI_Win_flush_local(target_rank, win);); (45, MPI, MPI_Win_free(&win);); (23, MPI, MPI_Win_lock_all(0, win);); (11, MPI, MPI_Win_create(winbuf, BUFCNT * sizeof(int), sizeof(int), MPI_INFO_NULL, MPI_COMM_WORLD, &win);); (26, MPI, MPI_Get(locbuf, BUFCNT, MPI_INT, target_rank, 0, BUFCNT, MPI_INT, win);); (18, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(9, MPI, return MPI_Wait(&tmprequest, MPI_STATUS_IGNORE);)", "label": "(27, MPI, return MPI_Waitall(num_ep, block_coll_request, MPI_STATUS_IGNORE);); (8, MPI, return MPI_Wait(&tmprequest, MPI_STATUS_IGNORE);)"}
{"predict": "(26, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_IMAGE_MAX_ARRAY_SIZE, sizeof( maxArraySize ), &maxArraySize, NULL );); (27, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_GLOBAL_MEM_SIZE, sizeof( memSize ), &memSize, NULL );); (25, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_MAX_MEM_ALLOC_SIZE, sizeof( maxAllocSize ), &maxAllocSize, NULL );); (21, OpenCL, int error = clGetDeviceInfo( device, CL_DEVICE_IMAGE2D_MAX_WIDTH, sizeof( maxWidth ), &maxWidth, NULL );); (22, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_IMAGE2D_MAX_HEIGHT, sizeof( maxHeight ), &maxHeight, NULL );)", "label": "(28, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_IMAGE_MAX_ARRAY_SIZE, sizeof( maxArraySize ), &maxArraySize, NULL );); (25, OpenCL, int error = clGetDeviceInfo( device, CL_DEVICE_IMAGE2D_MAX_WIDTH, sizeof( maxWidth ), &maxWidth, NULL );); (28, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_GLOBAL_MEM_SIZE, sizeof( memSize ), &memSize, NULL );); (26, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_MAX_MEM_ALLOC_SIZE, sizeof( maxAllocSize ), &maxAllocSize, NULL );); (23, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_IMAGE2D_MAX_HEIGHT, sizeof( maxHeight ), &maxHeight, NULL );)"}
{"predict": "(3, CUDA, extern __shared__ int shist[];)", "label": "(3, CUDA, extern __shared__ int shist[];)"}
{"predict": "(37, OpenMP, #pragma omp barrier); (3, OpenMP, const int nt = omp_get_max_threads();); (5, OpenMP, std::cout << \"num threads  = \" << omp_get_max_threads() << \"\\n\";); (25, OpenMP, #pragma omp parallel); (35, OpenMP, #pragma omp critical); (32, OpenMP, const int me = omp_get_thread_num();)", "label": "(33, OpenMP, #pragma omp barrier); (28, OpenMP, int me    = omp_get_thread_num();); (25, OpenMP, #pragma omp parallel reduction(min:dtmin) reduction(max:dtmax) reduction(+:dtavg)); (-1, OpenMP, int nt = omp_get_max_threads();); (35, OpenMP, #pragma omp critical)"}
{"predict": "(20, CUDA, checkCudaErrors(cudaSetDevice(0));)", "label": "(20, CUDA, checkCudaErrors(cudaSetDevice(0));)"}
{"predict": "(4, TBB, oneapi::tbb::global_control control(oneapi::tbb::global_control::max_allowed_parallelism, p);); (11, TBB, oneapi::tbb::parallel_vector<T, Allocator<T> > v( oneapi::tbb::cache_aligned_allocator<T>( vs3.begin()->second ) );); (14, TBB, oneapi::tbb::parallel_for ( oneapi::tbb::blocked_range<int> (0, N, 10000), parallel_vector_for_body<T, Allocator>( vs ) );); (1, TBB, typedef typename oneapi::tbb::enumerable_thread_specific< std::pair<int, std::vector<T, Allocator<T> >, Allocator< std::pair<int, std::vector<T, Allocator<T> > > ets_tlskey_type;); (26, TBB, oneapi::tbb::parallel_reduce ( vs3.range(1), pvrb );); (37, TBB, typename oneapi::tbb::flattened2d<ets_nokey_type>::iterator i = fvs.begin();); (1, TBB, typedef typename oneapi::tbb::enumerable_thread_specific< std::vector<T, Allocator<T> >, Allocator< std::vector<T, Allocator<T> > > ets_nokey_type;); (44, TBB, typename oneapi::tbb::flattened2d<ets_nokey_type>::const_iterator i = cfvs2.begin();); (17, TBB, oneapi::tbb::flattened2d<ets_nokey_type> fvs2( vs3 );); (35, TBB, typename oneapi::tbb::flattened2d<ets_nokey_type>::const_iterator i = fvs.begin();); (15, TBB, typename oneapi::tbb::flattened2d<ets_nokey_type>::flattened_2d_type cfvs2( vs3 );); (36, TBB, while (i!= fvs.end()) ++i;); (37, TBB, while (i!= cfvs2.end())", "label": "(3, TBB, typedef std::vector<T, oneapi::tbb::tbb_allocator<T> > container_type;); (33, TBB, oneapi::tbb::flattened2d<ets_nokey_type> fvs = flatten2d(vs3);); (11, TBB, typedef typename oneapi::tbb::enumerable_thread_specific< container_type, Allocator<container_type>, oneapi::tbb::ets_key_per_instance > ets_tlskey_type;); (9, TBB, typedef typename oneapi::tbb::enumerable_thread_specific< container_type, Allocator<container_type>, oneapi::tbb::ets_no_key > ets_nokey_type;); (44, TBB, oneapi::tbb::flattened2d<ets_nokey_type> fvs2 = flatten2d(vs3, vs3.begin(), std::next(vs3.begin()));); (23, TBB, oneapi::tbb::parallel_reduce ( vs3.range(1), pvrb );); (0, TBB, oneapi::tbb::global_control gc(oneapi::tbb::global_control::max_allowed_parallelism, p);); (10, TBB, oneapi::tbb::parallel_for ( oneapi::tbb::blocked_range<int> (0, N*p, RANGE_MIN), parallel_vector_for_body<T, Allocator>( vs, allocator_name ) );); (42, TBB, const oneapi::tbb::flattened2d<ets_nokey_type>& cfvs2(fvs2);)"}
{"predict": "(65, OpenMP, #define OP_GOMP_CRITICAL_READ(OP, FLAG))", "label": "(65, OpenMP, #define OP_GOMP_CRITICAL_READ(OP, FLAG))"}
{"predict": "(58, OpenMP, #pragma omp parallel for); (17, OpenMP, gsl_vector_set_zero(dv);); (15, OpenMP, #pragma omp parallel); (16, OpenMP, for(i = 0; i < M; ++i) gsl_vector_set(dv, i, i);)", "label": "(17, OpenMP, s += gsl_linalg_cholesky_decomp_unit(v, dv);)"}
{"predict": "(11, OpenMP, #pragma omp target teams distribute parallel for simd if (parallel: false)); (18, OpenMP, #pragma omp target teams distribute parallel for simd if(simd: true) nontemporal(Arg))", "label": "(19, OpenMP, #pragma omp target teams distribute parallel for simd if (parallel: false)); (10, OpenMP, #pragma omp target teams distribute parallel for simd if(simd: true) nontemporal(Arg))"}
{"predict": "(18, CUDA, cudaSetDevice(currentDevice);)", "label": "(19, CUDA, cudaSetDevice(currentDevice);)"}
{"predict": "(95, OpenMP, #pragma omp distribute simd shared(i)); (81, OpenMP, #pragma omp distribute simd shared(ba)); (55, OpenMP, #pragma omp distribute simd shared(a, b, c, d, f)); (102, OpenMP, #pragma omp distribute simd shared(r)); (11, OpenMP, #pragma omp distribute simd shared); (113, OpenMP, #pragma omp distribute simd firstprivate(i), shared(i)); (8, OpenMP, #pragma omp distribute simd shared(); (30, OpenMP, #pragma omp distribute simd shared(argc, z)); (57, OpenMP, #pragma omp distribute simd shared(argv[1])); (44, OpenMP, #pragma omp distribute simd shared(S1)); (81, OpenMP, #pragma omp distribute simd shared(h, k, B::x)); (68, OpenMP, #pragma omp distribute simd shared(e, g)); (47, OpenMP, #pragma omp distribute simd shared(ca)); (63, OpenMP, #pragma omp distribute simd shared(S2::S2s)); (10, OpenMP, #pragma omp distribute simd shared(argc); (103, OpenMP, #pragma omp parallel private(i)); (12, OpenMP, #pragma omp distribute simd shared(argc > 0? argv[1] : argv[2])); (17, OpenMP, #pragma omp distribute simd shared(argc)); (106, OpenMP, #pragma omp parallel shared(i)); (34, OpenMP, #pragma omp distribute simd shared(2 * 2)); (87, OpenMP, #pragma omp distribute simd private(i), shared(i)); (103, OpenMP, #pragma omp parallel reduction(+ : i)); (91, OpenMP, #pragma omp distribute simd shared(p), private(p)); (66, OpenMP, #pragma omp distribute simd shared(fl)); (74, OpenMP, #pragma omp distribute simd shared(o)); (11, OpenMP, #pragma omp distribute simd shared(argc)); (8, OpenMP, #pragma omp distribute simd shared(argc,); (38,", "label": "(109, OpenMP, #pragma omp distribute parallel for reduction(^ : fl)); (154, OpenMP, #pragma omp distribute parallel for reduction(+ : r)); (117, OpenMP, #pragma omp distribute parallel for reduction(&& : S2::S2sc)); (126, OpenMP, #pragma omp distribute parallel for reduction(+ : h, k, B::x)); (164, OpenMP, #pragma omp distribute parallel for reduction(+ : fl, z)); (39, OpenMP, #pragma omp distribute parallel for reduction(\\)); (6, OpenMP, #pragma omp target); (87, OpenMP, #pragma omp distribute parallel for reduction(+ : ba)); (81, OpenMP, #pragma omp distribute parallel for reduction(max : h.b)); (60, OpenMP, #pragma omp distribute parallel for reduction(&& : argc)); (161, OpenMP, #pragma omp parallel reduction(* : fl)); (28, OpenMP, #pragma omp distribute parallel for reduction(*)); (102, OpenMP, #pragma omp distribute parallel for reduction(&& : S2::S2s)); (6, OpenMP, #pragma omp distribute parallel for reduction +); (85, OpenMP, #pragma omp distribute parallel for reduction(* : ca)); (39, OpenMP, #pragma omp distribute parallel for reduction(| : argc,); (118, OpenMP, #pragma omp distribute parallel for reduction(+ : o)); (125, OpenMP, #pragma omp parallel private(k)); (6, OpenMP, #pragma omp distribute parallel for reduction(); (143, OpenMP, #pragma omp distribute parallel for reduction(max : j)); (44, OpenMP, #pragma omp distribute parallel for reduction(~ : argc)); (138, OpenMP, #pragma omp parallel reduction(min : i)); (143, OpenMP, #pragma omp parallel private(fl)); (26, OpenMP, #pragma omp distribute parallel for reduction(foo : argc); (50, OpenMP, #pragma omp distribute parallel for reduction(^ : S1)); (34, OpenMP, #pragma omp distribute parallel for reduction(|| : argc > 0? argv[1] : argv[2])); (-12, OpenMP, #pragma omp distribute parallel for reduction); (153, OpenMP, #pragma omp distribute parallel for reduction(+ : m)); (76, OpenMP, #pragma omp distribute parallel for reduction(- : da)); (95, OpenMP, #pragma omp distribute parallel for reduction(& : e, g)); (144, OpenMP, #pragma omp distribute parallel for reduction(+ : fl)); (-2, OpenMP, #pragma omp distribute parallel for reduction(-); (47, OpenMP, #pragma omp distribute parallel for reduction(+ : a, b, c, d, f)); (112, OpenMP, #pragma omp distribute parallel for reduction(+ : p), reduction(+ : p)); (151, OpenMP, #pragma omp distribute parallel for reduction(task, + : m)); (49, OpenMP, #pragma omp distribute parallel for reduction(min : a, b, c, d, f)); (122, OpenMP, #pragma omp parallel shared(i)); (102, OpenMP, #pragma omp distribute parallel for private(i), reduction(+ : j), reduction(+ : q)); (-4, OpenMP, #pragma omp distribute parallel for reduction()); (-26, OpenMP, #pragma omp teams)"}
{"predict": "(41, CUDA, cudaError_t result = cudaGetLastError();); (41, CUDA, EXPECT_EQ(result, cudaSuccess) << \" - CUDA error: \" << cudaGetErrorString(result);)", "label": "(41, CUDA, cudaError_t result = cudaGetLastError();); (41, CUDA, EXPECT_EQ(result, cudaSuccess) << \" - CUDA error: \" << cudaGetErrorString(result);)"}
{"predict": "(8, OpenMP, #pragma omp parallel for)", "label": "(8, OpenMP, #pragma omp parallel for)"}
{"predict": "(35, OpenMP, #pragma omp target parallel for); (52, OpenMP, #pragma omp target parallel for firstprivate(c))", "label": "(54, OpenMP, #pragma omp target parallel for map(alloc : c)); (64, OpenMP, #pragma omp barrier); (44, OpenMP, #pragma omp target exit data map(from : a)); (61, OpenMP, #pragma omp target update from(c) nowait); (21, OpenMP, #pragma omp target enter data map(to : a)); (30, OpenMP, #pragma omp target parallel for)"}
{"predict": "(204, MPI, MPI_Bcast(&log2nstarts,      1, MPI_INT,           root, MPI_COMM_WORLD);); (46, MPI, MPI_Comm_size(MPI_COMM_WORLD,&Num_procs);); (300, MPI, MPI_Finalize();); (201, MPI, MPI_Bcast(&log2nproc,        1, MPI_INT,           root, MPI_COMM_WORLD);); (202, MPI, MPI_Bcast(&log2tablesize,    1, MPI_INT,           root, MPI_COMM_WORLD);); (203, MPI, MPI_Bcast(&log2update_ratio, 1, MPI_INT,           root, MPI_COMM_WORLD);); (41, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&my_ID);); (195, MPI, MPI_Bcast(&nupdate,          1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (193, MPI, MPI_Bcast(&nstarts,          1, MPI_INT,           root, MPI_COMM_WORLD);); (194, MPI, MPI_Bcast(&tablesize,        1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (195, MPI, MPI_Bcast(&tablespace,       1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (193, MPI, MPI_Bcast(&loctablesize,     1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (261, MPI, MPI_Reduce(&error, &tot_error, 1, MPI_LONG_LONG_INT, MPI_SUM, root, MPI_COMM_WORLD);); (186, MPI, MPI_Bcast(&update_ratio,     1, MPI_INT,           root, MPI_COMM_WORLD);); (231, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(162, MPI, MPI_Bcast(&log2nstarts,      1, MPI_INT,           root, MPI_COMM_WORLD);); (46, MPI, MPI_Comm_size(MPI_COMM_WORLD,&Num_procs);); (155, MPI, MPI_Bcast(&log2nproc,        1, MPI_INT,           root, MPI_COMM_WORLD);); (303, MPI, MPI_Finalize();); (154, MPI, MPI_Bcast(&log2tablesize,    1, MPI_INT,           root, MPI_COMM_WORLD);); (155, MPI, MPI_Bcast(&log2update_ratio, 1, MPI_INT,           root, MPI_COMM_WORLD);); (42, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&my_ID);); (159, MPI, MPI_Bcast(&nupdate,          1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (153, MPI, MPI_Bcast(&nstarts,          1, MPI_INT,           root, MPI_COMM_WORLD);); (154, MPI, MPI_Bcast(&tablesize,        1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (155, MPI, MPI_Bcast(&tablespace,       1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (153, MPI, MPI_Bcast(&loctablesize,     1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (213, MPI, MPI_Barrier(MPI_COMM_WORLD);); (146, MPI, MPI_Bcast(&update_ratio,     1, MPI_INT,           root, MPI_COMM_WORLD);); (280, MPI, MPI_Reduce(&error, &tot_error, 1, MPI_LONG_LONG_INT, MPI_SUM, root, MPI_COMM_WORLD);)"}
{"predict": "(46, MPI, MPI_Op_free(&dtcmp_reduceop_randroot);); (58, MPI, MPI_Type_free(&dtcmp_type_3uint64t);); (52, MPI, MPI_Op_free(&dtcmp_reduceop_mms_3uint64t);); (62, MPI, MPI_Comm_free(&dtcmp_comm_self);); (47, MPI, MPI_Type_free(&dtcmp_type_3int);)", "label": "(45, MPI, MPI_Op_free(&dtcmp_reduceop_randroot);); (49, MPI, MPI_Op_free(&dtcmp_reduceop_mms_3uint64t);); (64, MPI, MPI_Comm_free(&dtcmp_comm_self);); (52, MPI, MPI_Type_free(&dtcmp_type_3int);); (56, MPI, MPI_Type_free(&dtcmp_type_3uint64t);)"}
{"predict": "(77, OpenMP, __kmpc_omp_task(&loc, gtid, task);); (110, OpenMP, __kmpc_omp_task_with_deps(&loc, gtid, task, ndeps_cnv, dep_list);); (91, OpenMP, __kmpc_omp_task_with_deps(&loc, gtid, task, ndeps_cnv, dep_list);); (74, OpenMP, __kmpc_omp_task_with_deps(&loc, gtid, task, 0, NULL);)", "label": "(70, OpenMP, kmp_gomp_depends_info_t gomp_depends(depend);); (103, OpenMP, __kmpc_omp_wait_deps(&loc, gtid, ndeps, dep_list, 0, NULL);); (77, OpenMP, __kmpc_omp_task(&loc, gtid, task);); (74, OpenMP, __kmpc_omp_task_with_deps(&loc, gtid, task, ndeps_cnv, dep_list, 0, NULL);)"}
{"predict": "(13, MPI, MPI_Comm_size(comm, &size);); (17, MPI, mpi_errno = MPI_Group_difference(g1, g2, NULL);); (21, MPI, MPI_Group_free(&basegroup);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &worldrank);); (15, MPI, MPI_Error_class(mpi_errno, &errclass);); (10, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (18, MPI, MPI_Group_free(&g1);); (10, MPI, MPI_Comm_group(comm, &basegroup);); (15, MPI, MPI_Comm_free(&dupcomm);); (10, MPI, MPI_Comm_group(dupcomm, &g2);); (7, MPI, MPI_Comm_dup(comm, &dupcomm);); (14, MPI, MPI_Group_free(&g2);); (5, MPI, MPI_Comm_rank(comm, &rank);); (7, MPI, MPI_Comm_set_errhandler(dupcomm, MPI_ERRORS_RETURN);); (5, MPI, MPI_Comm_group(comm, &g1);)", "label": "(14, MPI, MPI_Comm_size(comm, &size);); (19, MPI, MPI_Comm_group(dupcomm, &g2);); (28, MPI, MPI_Group_free(&g2);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &worldrank);); (28, MPI, MPI_Comm_free(&dupcomm);); (23, MPI, MPI_Group_free(&basegroup);); (12, MPI, MPI_Comm_group(newcomm, &g1);); (12, MPI, MPI_Comm_dup(comm, &dupcomm);); (15, MPI, mpi_errno = MPI_Group_difference(g1, g2, NULL);); (22, MPI, MPI_Comm_free(&newcomm);); (14, MPI, MPI_Error_class(mpi_errno, &errclass);); (4, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (5, MPI, MPI_Comm_split(comm, 0, size - rank, &newcomm);); (16, MPI, MPI_Group_free(&g1);); (-1, MPI, MPI_Comm_rank(comm, &rank);); (-3, MPI, MPI_Comm_group(comm, &basegroup);)"}
{"predict": "(8, CUDA, HANDLE_ERROR(cudaMalloc((void**)&colored_noise_d, sizeof(float) * full_buffer_size));); (10, CUDA, cudaStreamCreate(&stream);); (18, CUDA, HANDLE_ERROR(cudaStreamSynchronize(stream));)", "label": "(9, CUDA, HANDLE_ERROR(cudaMalloc((void**)&colored_noise_d, sizeof(float) * full_buffer_size));); (11, CUDA, cudaStreamCreate(&stream);); (18, CUDA, HANDLE_ERROR(cudaStreamSynchronize(stream));)"}
{"predict": "(55, OpenMP, #pragma omp target teams distribute collapse(4,, 4)); (87, OpenMP, #pragma omp target teams distribute collapse(2) firstprivate(i)); (43, OpenMP, #pragma omp target teams distribute collapse(4, )); (67, OpenMP, #pragma omp target teams distribute collapse(foo())); (12, OpenMP, #pragma omp target teams distribute collapse(,); (57, OpenMP, #pragma omp target teams distribute collapse(4, 8)); (77, OpenMP, #pragma omp target teams distribute collapse(5 - 5)); (4, OpenMP, #pragma omp target teams distribute collapse()); (6, OpenMP, #pragma omp target teams distribute collapse(, )); (36, OpenMP, #pragma omp target teams distribute collapse(4)); (26, OpenMP, #pragma omp target teams distribute collapse(4,); (78, OpenMP, #pragma omp target teams distribute collapse(5 - 5) firstprivate(i)); (13, OpenMP, #pragma omp target teams distribute collapse 4)); (67, OpenMP, #pragma omp target teams distribute collapse(0)); (3, OpenMP, #pragma omp target teams distribute collapse(, )); (60, OpenMP, #pragma omp target teams distribute collapse(-5)); (33, OpenMP, #pragma omp target teams distribute collapse(4 4)); (67, OpenMP, #pragma omp target teams distribute collapse(5 - 5)); (16, OpenMP, #pragma omp target teams distribute collapse(4); (52, OpenMP, #pragma omp target teams distribute collapse(2.5))", "label": "(41, OpenMP, #pragma omp target teams distribute collapse(4, )); (90, OpenMP, #pragma omp target teams distribute collapse(2) firstprivate(i)); (15, OpenMP, #pragma omp target teams distribute collapse(,); (33, OpenMP, #pragma omp target teams distribute collapse(4,); (74, OpenMP, #pragma omp target teams distribute collapse(-5)); (8, OpenMP, #pragma omp target teams distribute collapse()); (15, OpenMP, #pragma omp target teams distribute collapse(, )); (79, OpenMP, #pragma omp target teams distribute collapse(5 - 5)); (66, OpenMP, #pragma omp target teams distribute collapse(foo())); (73, OpenMP, #pragma omp target teams distribute collapse(0)); (-1, OpenMP, #pragma omp target teams distribute collapse(); (55, OpenMP, #pragma omp target teams distribute collapse(4, 8)); (38, OpenMP, #pragma omp target teams distribute collapse(4 4)); (32, OpenMP, #pragma omp target teams distribute collapse(4)); (-9, OpenMP, #pragma omp target teams distribute collapse); (40, OpenMP, #pragma omp target teams distribute collapse(4,, 4)); (15, OpenMP, #pragma omp target teams distribute collapse(4); (9, OpenMP, #pragma omp target teams distribute collapse 4)); (52, OpenMP, #pragma omp target teams distribute collapse(2.5))"}
{"predict": "(15, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (69, CUDA, checkCudaErrors(cudaFree(cd));); (10, CUDA, checkCudaErrors(cudaMalloc((void**)&cd, sizeof(double)*size));); (65, CUDA, checkCudaErrors(cudaMemcpy(c, cd, sizeof(double)*size, cudaMemcpyDeviceToHost));); (9, CUDA, checkCudaErrors(cudaMemcpy(cd, c, sizeof(double)*size, cudaMemcpyHostToDevice));)", "label": "(64, CUDA, CUDA_SAFE_CALL( cudaMemcpy(c, cd, size*sizeof(double), cudaMemcpyDeviceToHost) );); (67, CUDA, CUDA_SAFE_CALL( cudaDeviceReset() );); (12, CUDA, CUDA_SAFE_CALL( cudaMemset(cd, 0, size*sizeof(double)) );); (8, CUDA, CUDA_SAFE_CALL( cudaMalloc((void**)&cd, size*sizeof(double)) );); (62, CUDA, CUDA_SAFE_CALL( cudaFree(cd) );); (12, CUDA, CUDA_SAFE_CALL( cudaThreadSynchronize() );)"}
{"predict": "(67, CUDA, cudaMalloc(&cuda_mem, size * Mtypes(i));); (67, CUDA, cudaMemcpy(cuda_mem, ptrs[i], size * Mtypes(i), cudaMemcpyHostToDevice);); (68, CUDA, cudaMalloc(&cuda_mem, size * Mtypes(i) * maxGPT);); (68, CUDA, cudaMemcpy(&((int *)cuda_mem)[j*size], ptrs[i], size * Mtypes(i), cudaMemcpyHostToDevice);)", "label": "(71, CUDA, cudaMemcpy(&((int *)cuda_mem)[j*size], ptrs[i], size * Mtypes(i), cudaMemcpyHostToDevice);); (68, CUDA, cudaMalloc(&cuda_mem, size * Mtypes(i) * maxGPT);); (65, CUDA, cudaMemcpy(cuda_mem, ptrs[i], size * Mtypes(i), cudaMemcpyHostToDevice);); (63, CUDA, cudaMalloc(&cuda_mem, size * Mtypes(i));)"}
{"predict": "(3, CUDA, extern __shared__ typename Epilogue::SharedStorage shared_storage[];)", "label": "(3, CUDA, __shared__ typename Epilogue::SharedStorage shared_storage;)"}
{"predict": "(17, CUDA, auto H = thrust::host_vector<T>(A);); (3, CUDA, using T = thrust::complex<double>;)", "label": "(3, CUDA, cudaSetDevice(WORLD.rank());); (11, CUDA, using T = thrust::complex<double>;); (40, CUDA, cudaStreamSynchronize(NULL);); (14, CUDA, thrust::host_vector<T> H = A;)"}
{"predict": "(13, OpenMP, #pragma omp target teams distribute parallel for reduction(max : max_t)); (7, OpenMP, #pragma omp target teams distribute parallel for reduction(min : min_t))", "label": "(9, OpenMP, #pragma omp target teams distribute parallel for reduction(min : min_t)); (6, OpenMP, #pragma omp target data map(to : arr_t[0 : N])); (11, OpenMP, #pragma omp target teams distribute parallel for reduction(max : max_t))"}
{"predict": "(94, OpenCL, error = clEnqueueReadBuffer( queueA, mainBuffer, CL_TRUE, 0, mainSize, actualResults, 0, NULL, NULL );); (21, OpenCL, mainBuffer = clCreateBuffer( context, CL_MEM_READ_WRITE, mainSize, NULL, &error );); (74, OpenCL, clReleaseCommandQueue(queue);)", "label": "(23, OpenCL, mainBuffer = clCreateBuffer( context, CL_MEM_READ_WRITE | CL_MEM_COPY_HOST_PTR, mainSize, mainBufferContents, &error );); (93, OpenCL, error = clEnqueueReadBuffer( queueA, mainBuffer, CL_TRUE, 0, mainSize, actualResults, 0, NULL, NULL );)"}
{"predict": "(20, MPI, err = MPI_Waitany(10, requests, &index, statuses);); (36, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_ARE_FATAL);); (15, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (16, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(20, MPI, err = MPI_Waitany(10, requests, &index, statuses);); (33, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_ARE_FATAL);); (33, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (15, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(66, MPI, MPI_Finalized(&mpi_finalized);); (64, MPI, MPI_Initialized(&mpi_initialized);); (65, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);)", "label": "(69, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (65, MPI, MPI_Finalized(&mpi_finalized);); (63, MPI, MPI_Initialized(&mpi_initialized);)"}
{"predict": "(194, MPI, if ( SendCounts[p] ) MPI_Wait(&send_req[p], &status);); (194, MPI, if ( RecvCounts[p] ) MPI_Wait(&recv_req[p], &status);)", "label": "(166, MPI, if ( SendCounts[p] ) MPI_Wait(&send_req[p], &status);); (166, MPI, if ( RecvCounts[p] ) MPI_Wait(&recv_req[p], &status);)"}
{"predict": "(25, OpenMP, tdefl_create_comp_object(pComp);)", "label": "(3, OpenMP, mz_uint comp_flags = TDEFL_COMPUTE_ADLER32 | tdefl_create_comp_flags_from_zip_params(level, window_bits, strategy);)"}
{"predict": "(40, OpenMP, #pragma omp target simd simdlen(4,, 4)); (57, OpenMP, #pragma omp target simd simdlen(2.5)); (6, OpenMP, #pragma omp target simd simdlen(); (34, OpenMP, #pragma omp target simd simdlen(4)); (38, OpenMP, #pragma omp target simd simdlen(4 4)); (65, OpenMP, #pragma omp target simd simdlen(0)); (69, OpenMP, #pragma omp target simd simdlen(5 - 5)); (60, OpenMP, #pragma omp target simd simdlen(foo())); (3, OpenMP, #pragma omp target simd simdlen(,); (23, OpenMP, #pragma omp target simd simdlen(4,); (5, OpenMP, #pragma omp target simd simdlen 4)); (53, OpenMP, #pragma omp target simd simdlen(4, 8)); (35, OpenMP, #pragma omp target simd simdlen(4,, 8)); (45, OpenMP, #pragma omp target simd simdlen(4, 4, 8)); (58, OpenMP, #pragma omp target simd simdlen(foo())); (12, OpenMP, #pragma omp target simd simdlen(4); (4, OpenMP, #pragma omp target simd simdlen(, )); (64, OpenMP, #pragma omp target simd simdlen(2)); (30, OpenMP, #pragma omp target simd simdlen(4 4 4))", "label": "(3, OpenMP, #pragma omp target simd simdlen); (18, OpenMP, #pragma omp target simd simdlen(, )); (70, OpenMP, #pragma omp target simd simdlen(-5)); (31, OpenMP, #pragma omp target simd simdlen(4,); (72, OpenMP, #pragma omp target simd simdlen(0)); (2, OpenMP, #pragma omp target simd simdlen(); (46, OpenMP, #pragma omp target simd simdlen(4,, 4)); (61, OpenMP, #pragma omp target simd simdlen(foo())); (3, OpenMP, #pragma omp target simd simdlen()); (55, OpenMP, #pragma omp target simd simdlen(2.5)); (5, OpenMP, #pragma omp target simd simdlen(,); (31, OpenMP, #pragma omp target simd simdlen(4)); (68, OpenMP, #pragma omp target simd simdlen(5 - 5)); (26, OpenMP, #pragma omp target simd simdlen(4, )); (33, OpenMP, #pragma omp target simd simdlen(4 4)); (45, OpenMP, #pragma omp target simd simdlen(4, 8)); (13, OpenMP, #pragma omp target simd simdlen(4); (7, OpenMP, #pragma omp target simd simdlen 4))"}
{"predict": "(66, CUDA, checkCudaErrors(cudaMemcpy(d_StockPrice, h_StockPrice, OPT_SZ * sizeof(float), cudaMemcpyHostToDevice));); (50, CUDA, checkCudaErrors(cudaMalloc((void **)&d_StockPrice, OPT_SZ * sizeof(float)));); (67, CUDA, checkCudaErrors(cudaMemcpy(d_OptionYears, h_OptionYears, OPT_SZ * sizeof(float), cudaMemcpyHostToDevice));); (32, CUDA, findCudaDevice(argc, (const char **)argv);); (87, CUDA, getLastCudaError(\"BlackScholesGPU() execution failed.\\n\");); (63, CUDA, checkCudaErrors(cudaMemcpy(d_PutResult, h_PutResult, OPT_SZ * sizeof(float), cudaMemcpyHostToDevice));); (60, CUDA, checkCudaErrors(cudaMemcpy(d_CallResult, h_CallResult, OPT_SZ * sizeof(float), cudaMemcpyHostToDevice));); (44, CUDA, checkCudaErrors(cudaMalloc((void **)&d_PutResult, OPT_SZ * sizeof(float)));); (44, CUDA, checkCudaErrors(cudaMalloc((void **)&d_CallResult, OPT_SZ * sizeof(float)));); (45, CUDA, checkCudaErrors(cudaMalloc((void **)&d_OptionYears, OPT_SZ * sizeof(float)));); (43, CUDA, checkCudaErrors(cudaMalloc((void **)&d_StockPrice, OPT_SZ * sizeof(float)));); (43, CUDA, checkCudaErrors(cudaMalloc((void **)&d_OptionStrike, OPT_SZ * sizeof(float)));); (63, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (79, CUDA, checkCudaErrors(cudaMemcpy(h_PutResultGPU, d_PutResult, OPT_SZ * sizeof(float), cudaMemcpyDeviceToHost));); (77, CUDA, checkCudaErrors(cudaMemcpy(h_CallResultGPU, d_CallResult, OPT_SZ * sizeof(float), cudaMemcpyDeviceToHost));); (121, CUDA, checkCudaErrors(cudaFree(d_StockPrice));); (121, CUDA, checkCudaErrors(cudaFree(d_PutResult));); (121,", "label": "(49, CUDA, checkCudaErrors(cudaMalloc((void **)&d_CallResult,   OPT_SZ));); (155, CUDA, checkCudaErrors(cudaFree(d_OptionYears));); (155, CUDA, checkCudaErrors(cudaFree(d_OptionStrike));); (155, CUDA, checkCudaErrors(cudaFree(d_StockPrice));); (155, CUDA, checkCudaErrors(cudaFree(d_PutResult));); (66, CUDA, checkCudaErrors(cudaMemcpy(d_OptionStrike, h_OptionStrike,  OPT_SZ, cudaMemcpyHostToDevice));); (45, CUDA, checkCudaErrors(cudaMalloc((void **)&d_StockPrice,   OPT_SZ));); (43, CUDA, checkCudaErrors(cudaMalloc((void **)&d_PutResult,    OPT_SZ));); (26, CUDA, findCudaDevice(argc, (const char **)argv);); (68, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (42, CUDA, checkCudaErrors(cudaMalloc((void **)&d_OptionStrike, OPT_SZ));); (149, CUDA, checkCudaErrors(cudaFree(d_CallResult));); (81, CUDA, getLastCudaError(\"BlackScholesGPU() execution failed\\n\");); (98, CUDA, checkCudaErrors(cudaMemcpy(h_CallResultGPU, d_CallResult, OPT_SZ, cudaMemcpyDeviceToHost));); (58, CUDA, checkCudaErrors(cudaMemcpy(d_OptionYears,  h_OptionYears,   OPT_SZ, cudaMemcpyHostToDevice));); (97, CUDA, checkCudaErrors(cudaMemcpy(h_PutResultGPU,  d_PutResult,  OPT_SZ, cudaMemcpyDeviceToHost));); (37, CUDA, checkCudaErrors(cudaMalloc((void **)&d_OptionYears,  OPT_SZ));); (53, CUDA, checkCudaErrors(cudaMemcpy(d_StockPrice,  h_StockPrice,   OPT_SZ, cudaMemcpyHostToDevice));)"}
{"predict": "(12, MPI, MPI_Put(&one, sz, MPI_INT, destRank, j, sz, MPI_INT, win);); (7, MPI, MPI_Win_wait(win);); (13, MPI, t[k].endSync = MPI_Wtime();); (4, MPI, t[k].startOp = MPI_Wtime();); (10, MPI, MPI_Win_complete(win);); (6, MPI, MPI_Barrier(MPI_COMM_WORLD);); (11, MPI, t[k].endOp = MPI_Wtime();); (3, MPI, MPI_Win_post(exposureGroup, 0, win);); (12, MPI, MPI_Win_free(&win);); (4, MPI, MPI_Win_start(accessGroup, 0, win);)", "label": "(20, MPI, t[k].endSync = MPI_Wtime();); (9, MPI, t[k].startOp = MPI_Wtime();); (10, MPI, MPI_Put(&one, sz, MPI_INT, destRank, j, sz, MPI_INT, win);); (12, MPI, t[k].endOp = MPI_Wtime();); (15, MPI, MPI_Win_wait(win);); (2, MPI, MPI_Win_post(exposureGroup, 0, win);); (12, MPI, MPI_Win_complete(win);); (-1, MPI, MPI_Barrier(MPI_COMM_WORLD);); (0, MPI, MPI_Win_start(accessGroup, 0, win);)"}
{"predict": "(17, CUDA, cudaMemsetAsync(nulls.data(), 0xcccc, nulls.size(), rmm::cuda_stream_default.value());)", "label": "(17, CUDA, cudaMemsetAsync(nulls.data(), 0xcccc, nulls.size(), rmm::cuda_stream_default.value());)"}
{"predict": "(57, MPI, errcode = MPI_File_iwrite(fh, buf, nints, MPI_INT, &request);); (51, MPI, errcode = MPI_File_set_view(fh, 0, MPI_INT, MPI_INT, \"native\", MPI_INFO_NULL);); (8, MPI, MPI_Init(&argc, &argv);); (64, MPI, MPI_Barrier(MPI_COMM_WORLD);); (111, MPI, MPI_Finalize();); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (106, MPI, MPI_Allreduce(&errs, &toterrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (54, MPI, MPI_Wait(&request, &status);); (19, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (63, MPI, errcode = MPI_File_open(MPI_COMM_SELF, filename, MPI_MODE_RDONLY, MPI_INFO_NULL, &fh);); (17, MPI, MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);); (63, MPI, errcode = MPI_File_read(fh, buf, nints, MPI_INT, &status);); (13, MPI, MPI_Bcast(filename, len + 10, MPI_CHAR, 0, MPI_COMM_WORLD);); (64, MPI, MPI_File_close(&fh);)", "label": "(29, MPI, MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);); (51, MPI, errcode = MPI_File_set_view(fh, 0, MPI_INT, MPI_INT, \"native\", MPI_INFO_NULL);); (8, MPI, MPI_Init(&argc, &argv);); (59, MPI, MPI_Wait(&request, &status);); (112, MPI, MPI_Finalize();); (25, MPI, MPI_Bcast(filename, len + 10, MPI_CHAR, 0, MPI_COMM_WORLD);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (96, MPI, MPI_Allreduce(&errs, &toterrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (74, MPI, errcode = MPI_File_iread(fh, buf, nints, MPI_INT, &request);); (14, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (47, MPI, errcode = MPI_File_iwrite(fh, buf, nints, MPI_INT, &request);)"}
{"predict": "(13, MPI, MPI_Get(local_b, COUNT, MPI_INT, 1, 0, COUNT, MPI_INT, win[id]);); (30, MPI, MPI_Put(local_b, COUNT, MPI_INT, 0, 0, COUNT, MPI_INT, win[id]);); (6, MPI, MPI_Alloc_mem(COUNT * sizeof(int), MPI_INFO_NULL, &local_b);); (13, MPI, MPI_Win_fence(0, win[id]);); (26, MPI, MPI_Free_mem(local_b);)", "label": "(30, MPI, MPI_Get(local_b, COUNT, MPI_INT, 0, 0, COUNT, MPI_INT, win[id]);); (11, MPI, MPI_Win_fence(0, win[id]);); (43, MPI, MPI_Free_mem(local_b);); (11, MPI, MPI_Put(local_b, COUNT, MPI_INT, 0, 0, COUNT, MPI_INT, win[id]);); (4, MPI, MPI_Alloc_mem(COUNT * sizeof(int), MPI_INFO_NULL, &local_b);)"}
{"predict": "(105, MPI, MPI_Type_free(&typevec);); (105, MPI, MPI_Type_free(&newtype);); (42, MPI, MPI_Type_create_resized(tmptype, 0, extent, &newtype);); (75, MPI, MPI_CHECK(MPI_File_set_view(fh, 0, MPI_INT, newtype, \"native\", MPI_INFO_NULL));); (44, MPI, MPI_Type_free(&tmptype);); (3, MPI, MPI_Init(&argc, &argv);); (74, MPI, MPI_Barrier(MPI_COMM_WORLD);); (36, MPI, MPI_Type_commit(&newtype);); (54, MPI, MPI_Error_class(err, &errorclass);); (96, MPI, MPI_CHECK(MPI_File_close(&fh));); (45, MPI, MPI_Type_free(&newtype);); (30, MPI, MPI_Type_vector(SIZE / 2, 1, 2, MPI_INT, &typevec);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mynod);); (10, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (43, MPI, MPI_Type_create_struct(1, &len, &disp, &typevec, &tmptype);); (98, MPI, MPI_Allreduce(&errs, &toterrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (63, MPI, MPI_CHECK(MPI_File_set_view(fh, 0, MPI_INT, MPI_INT, \"native\", MPI_INFO_NULL));); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (23, MPI, MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);); (65, MPI, err = MPI_File_read_all(fh, buf, SIZE, MPI_INT, &status);); (67, MPI, err = MPI_File_write_all(fh, buf, SIZE, MPI_INT, &status);); (3", "label": "(197, MPI, MPI_CHECK(MPI_File_write_all(fh, buf, SIZE, MPI_INT, &status));); (9, MPI, MPI_Init(&argc, &argv);); (80, MPI, MPI_CHECK(MPI_File_write_all(fh, buf, 1, newtype, &status));); (42, MPI, MPI_Type_vector(SIZE / 2, 1, 2, MPI_INT, &typevec);); (49, MPI, MPI_Type_create_struct(1, &len, &disp, &typevec, &tmptype);); (73, MPI, MPI_CHECK(MPI_File_set_view(fh, 0, MPI_INT, newtype, \"native\", MPI_INFO_NULL));); (67, MPI, MPI_Barrier(MPI_COMM_WORLD);); (107, MPI, MPI_CHECK(MPI_File_close(&fh));); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mynod);); (48, MPI, MPI_Type_free(&tmptype);); (136, MPI, MPI_CHECK(MPI_File_read_at_all(fh, mynod * (SIZE / 2) * sizeof(int), buf, 1, newtype, &status));); (5, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (192, MPI, MPI_CHECK(MPI_File_read_at_all(fh, 0, buf, SIZE, MPI_INT, &status));); (22, MPI, MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);); (54, MPI, MPI_Error_class(err, &errorclass);); (-4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (42, MPI, MPI_Type_commit(&newtype);); (72, MPI, MPI_CHECK(MPI_File_read_at_all(fh, 0, buf, 1, newtype, &status));); (36, MPI, MPI_Type_free(&typevec);); (37, MPI, MPI_Type_create_resized(tmptype, 0, extent, &newtype);); (220, MPI, MPI_Finalize();); (15, MPI, MPI_Bcast(filename, len + 1, MPI_CHAR, 0, MPI_COMM_WORLD);); (206, MPI, MPI_Allreduce(&errs, &toterrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (214, MPI, MPI_Type_free(&newtype);)"}
{"predict": "(10, TBB, tbb::filter_t<void, void> one_filter( filter_table[i], i_filter );); (15, TBB, tbb::atomic<int> counter;); (8, TBB, tbb::parallel_pipeline( n_tokens, one_filter );)", "label": "(10, TBB, tbb::filter_t<void, void> one_filter( filter_table[i], i_filter );); (15, TBB, tbb::atomic<int> counter;); (11, TBB, tbb::parallel_pipeline( n_tokens, one_filter );)"}
{"predict": "(47, CUDA, CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));)", "label": "(47, CUDA, CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));)"}
{"predict": "(34, CUDA, cudaFree(d_inPtr);); (17, CUDA, cudaMalloc((void**)&d_inPtr, size * sizeof(float));); (23, CUDA, cudaMemcpy(&result_gpu, &d_outPtr[0], sizeof(float), cudaMemcpyDeviceToHost);); (17, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);); (30, CUDA, cudaFree(d_outPtr);); (13, CUDA, cudaMalloc((void**)&d_outPtr, size * sizeof(float));)", "label": "(34, CUDA, cudaFree(d_inPtr);); (18, CUDA, cudaMalloc((void**)&d_outPtr, size * sizeof(float));); (24, CUDA, cudaMemcpy(&result_gpu, &d_outPtr[0], sizeof(float), cudaMemcpyDeviceToHost);); (18, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);); (29, CUDA, cudaFree(d_outPtr);); (13, CUDA, cudaMalloc((void**)& d_inPtr, size * sizeof(float));)"}
{"predict": "(58, OpenMP, omp_set_num_threads(nthread_input);); (190, OpenMP, #pragma omp atomic); (131, OpenMP, nthread = omp_get_num_threads();); (184, OpenMP, #pragma omp parallel private(i, j, ran, round, index, my_ID) reduction(+:error)); (55, OpenMP, omp_set_dynamic(0);); (126, OpenMP, my_ID   = omp_get_thread_num();); (185, OpenMP, #pragma omp for); (120, OpenMP, #pragma omp master)", "label": "(202, OpenMP, #pragma omp barrier); (150, OpenMP, nthread = omp_get_num_threads();); (197, OpenMP, #pragma omp for); (146, OpenMP, #pragma omp master); (54, OpenMP, omp_set_num_threads(nthread_input);); (137, OpenMP, #pragma omp parallel private(i, j, ran, round, index, my_ID) reduction(+:error)); (223, OpenMP, #pragma omp atomic); (140, OpenMP, my_ID   = omp_get_thread_num();)"}
{"predict": "(106, MPI, MPI_Type_get_extent(dt_keysat, &keysat_lb, &keysat_extent);); (108, MPI, MPI_Type_get_extent(dt_sat, &sat_lb, &sat_extent);); (17, MPI, MPI_Type_commit(&dt_filepath);); (101, MPI, MPI_Type_get_extent(dt_key, &key_lb, &key_extent);); (12, MPI, MPI_Type_commit(&dt_sat);); (14, MPI, MPI_Type_contiguous((int)bytes, MPI_BYTE, &dt_sat);); (16, MPI, MPI_Type_contiguous((int)chars,  MPI_CHAR, &dt_filepath);); (13, MPI, MPI_Type_commit(&dt_filepath);); (15, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (15, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (9, MPI, MPI_Type_free(&dt_key);); (5, MPI, MPI_Type_commit(&dt_key);)", "label": "(106, MPI, MPI_Type_get_extent(dt_key, &key_lb, &key_extent);); (174, MPI, MPI_Type_free(&dt_key);); (16, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (20, MPI, MPI_Type_commit(&dt_filepath);); (169, MPI, MPI_Type_free(&dt_keysat);); (170, MPI, MPI_Type_free(&dt_filepath);); (107, MPI, MPI_Type_get_extent(dt_sat, &sat_lb, &sat_extent);); (102, MPI, MPI_Type_get_extent(dt_keysat, &keysat_lb, &keysat_extent);); (5, MPI, MPI_Type_contiguous((int)bytes, MPI_BYTE, &dt_sat);); (13, MPI, MPI_Type_contiguous((int)chars, MPI_CHAR, &dt_filepath);); (171, MPI, MPI_Type_free(&dt_sat);)"}
{"predict": "(10, OpenMP, omp_set_teams_thread_limit(iset);); (3, OpenMP, iget = omp_get_num_teams();); (4, OpenMP, omp_set_num_teams(iset);)", "label": "(12, OpenMP, iget = omp_get_teams_thread_limit();); (3, OpenMP, omp_set_num_teams(iset);); (3, OpenMP, iget = omp_get_max_teams();); (8, OpenMP, omp_set_teams_thread_limit(iset);)"}
{"predict": "(18, TBB, test_extract<tbb::flow::queueing>();); (0, TBB, tbb::task_scheduler_init init(p);); (7, TBB, stop = tbb::tick_count::now();); (16, TBB, test_extract<tbb::flow::rejecting>();); (-2, TBB, tbb::tick_count start = tbb::tick_count::now(), stop;)", "label": "(12, TBB, test_resets<int, tbb::flow::queue_node<int> >();); (2, TBB, tbb::task_scheduler_init init(p);); (7, TBB, stop = tbb::tick_count::now();); (18, TBB, test_buffer_extract<tbb::flow::queue_node<int> >().run_tests();); (-3, TBB, tbb::tick_count start = tbb::tick_count::now(), stop;); (8, TBB, test_resets<float, tbb::flow::queue_node<float> >();)"}
{"predict": "(27, CUDA, cudaFree(y);); (16, CUDA, cudaDeviceSynchronize();); (5, CUDA, cudaMallocManaged(&y, N*sizeof(float));); (3, CUDA, cudaMallocManaged(&x, N*sizeof(float));); (22, CUDA, cudaFree(x);)", "label": "(16, CUDA, cudaDeviceSynchronize();); (6, CUDA, cudaMallocManaged(&y, N*sizeof(float));); (4, CUDA, cudaMallocManaged(&x, N*sizeof(float));); (23, CUDA, cudaFree(y);); (21, CUDA, cudaFree(x);)"}
{"predict": "(13, CUDA, cudaError_t err = cudaFree(ptr);)", "label": "(13, CUDA, cudaError_t err = cudaFree(ptr);)"}
{"predict": "(18, OpenMP, #pragma omp target teams distribute parallel for device(i)); (4, OpenMP, #pragma omp target enter data map(to:vecTs[i:1])); (23, OpenMP, #pragma omp target exit data map(from:vecTs[i:1]))", "label": "(17, OpenMP, #pragma omp parallel for num_threads(num_devices)); (10, OpenMP, #pragma omp target teams distribute parallel for); (3, OpenMP, size_t num_devices = omp_get_num_devices();); (17, OpenMP, #pragma omp target teams distribute parallel for device(omp_get_thread_num()))"}
{"predict": "(11, MPI, MPI_Reduce(&time, &max_time, 1, MPI_LONG_LONG, MPI_MAX, 0, MPI_COMM_WORLD);); (1, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (10, MPI, MPI_Reduce(&time, &sum_time, 1, MPI_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (7, MPI, MPI_Reduce(&time, &min_time, 1, MPI_LONG_LONG, MPI_MIN, 0, MPI_COMM_WORLD);)", "label": "(9, MPI, MPI_Reduce(&time, &sum_time, 1, MPI_LONG_LONG, MPI_SUM, 0, comm_);); (6, MPI, MPI_Reduce(&time, &max_time, 1, MPI_LONG_LONG, MPI_MAX, 0, comm_);); (10, MPI, MPI_Comm_size(comm_, &size);); (8, MPI, MPI_Comm_rank(comm_, &rank);); (4, MPI, MPI_Reduce(&time, &min_time, 1, MPI_LONG_LONG, MPI_MIN, 0, comm_);)"}
{"predict": "(135, MPI, MPI_Finalize();); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (4, MPI, MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided);); (13, MPI, MPI_Barrier(MSG_COMM_WORLD);); (58, MPI, MPI_Type_size(type, &type_size);)", "label": "(9, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &MSG_COMM_WORLD);); (4, MPI, MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided);); (63, MPI, MPI_Type_size(type, &type_size);); (16, MPI, MPI_Barrier(MSG_COMM_WORLD);); (130, MPI, MPI_Comm_free(&MSG_COMM_WORLD);); (131, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_rank(MSG_COMM_WORLD, &rank);); (5, MPI, MPI_Comm_size(MSG_COMM_WORLD, &size);)"}
{"predict": "(37, CUDA, cudaFreeHost( hptr );)", "label": "(37, CUDA, cudaFreeHost( hptr );)"}
{"predict": "(27, OpenMP, #pragma omp target simd aligned(B::ib,B:C1+C2)); (2, OpenMP, #pragma omp target simd aligned(B:bfoo())); (16, OpenMP, #pragma omp target simd aligned(X::x : ::z)); (18, OpenMP, #pragma omp target simd aligned(::z)); (10, OpenMP, #pragma omp target simd aligned(B::ib:B:bfoo())); (2, OpenMP, #pragma omp target simd aligned(z:B:bfoo())); (21, OpenMP, #pragma omp target simd aligned(B::bfoo())); (4, OpenMP, #pragma omp target simd aligned(B,rp,::z: X::x)); (15, OpenMP, #pragma omp target simd aligned(B,rp)); (-3, OpenMP, #pragma omp target simd aligned(B::ib:B::bfoo()))", "label": "(17, OpenMP, #pragma omp target simd aligned(X::x : ::z)); (2, OpenMP, #pragma omp target simd aligned(B:bfoo())); (23, OpenMP, #pragma omp target simd aligned(B::bfoo())); (17, OpenMP, #pragma omp target simd aligned(B,rp,::z: X::x)); (24, OpenMP, #pragma omp target simd aligned(B::ib,B:C1+C2)); (6, OpenMP, #pragma omp target simd aligned(z:B:bfoo())); (0, OpenMP, #pragma omp target simd aligned(B::ib:B:bfoo())); (1, OpenMP, #pragma omp target simd aligned(B:B::bfoo())); (14, OpenMP, #pragma omp target simd aligned(::z))"}
{"predict": "(4, OpenMP, __kmpc_omp_task(NULL, 0, task);)", "label": "(10, OpenMP, __kmpc_omp_task(NULL, 0, task);); (2, OpenMP, printf(\"%d: creating task\\n\", omp_get_thread_num());)"}
{"predict": "(25, TBB, tbb::parallel_for_each(blocked_range_with_maps<int>(0,n_cams, visibility_mat, normal_dot_cam_mat), vtask);)", "label": "(25, TBB, tbb::task_scheduler_init init;); (25, TBB, tbb::parallel_for( tbb::blocked_range<int>(0,n_cams), vtask);)"}
{"predict": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(69, CUDA, CubDebugExit(cudaDeviceSynchronize());); (33, CUDA, CubDebugExit(cudaGetDevice(&device_ordinal));); (41, CUDA, CubDebugExit(MaxBlockThreads(max_block_threads, EmptyKernel<void>, block_size));); (36, CUDA, CubDebugExit(cudaDeviceGetAttribute(&sm_count, cudaDevAttrMultiProcessorCount, device_ordinal));)", "label": "(34, CUDA, CubDebugExit(cudaGetDevice(&device_ordinal));); (42, CUDA, CubDebugExit(cudaDeviceGetAttribute(&max_block_threads, cudaDevAttrMaxThreadsPerBlock, device_ordinal));); (40, CUDA, CubDebugExit(cudaDeviceGetAttribute(&sm_count, cudaDevAttrMultiProcessorCount, device_ordinal));); (69, CUDA, retval = CubDebug(cudaDeviceSynchronize());)"}
{"predict": "(91, OpenMP, double tic = omp_get_wtime();); (105, OpenMP, double toc = omp_get_wtime();); (114, OpenMP, double stop = omp_get_wtime();); (8, OpenMP, double start = omp_get_wtime();)", "label": "(92, OpenMP, double tic = omp_get_wtime();); (108, OpenMP, #pragma omp target exit data map(from: u[0:n*n])); (116, OpenMP, double stop = omp_get_wtime();); (103, OpenMP, double toc = omp_get_wtime();); (6, OpenMP, double start = omp_get_wtime();); (80, OpenMP, #pragma omp target enter data map(to: u[0:n*n], u_tmp[0:n*n]))"}
{"predict": "(74, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 3, NULL, threads, NULL, 0, NULL, NULL );); (45, OpenCL, actualSampler = clCreateSampler( context, CL_FALSE, CL_ADDRESS_CLAMP_TO_EDGE, CL_FILTER_NEAREST, &error );); (53, OpenCL, clEnqueueWriteBuffer( queue, results, CL_TRUE, 0, resultValuesSize, resultValues, 0, NULL, NULL );); (56, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_mem ), &read_write_image );); (98, OpenCL, clReleaseMemObject(results);); (96, OpenCL, clReleaseSampler(actualSampler);); (49, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_mem ), &read_only_image );); (63, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_mem ), &results );); (97, OpenCL, clReleaseMemObject(read_write_image);); (49, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_sampler ), &actualSampler );); (70, OpenCL, error = clEnqueueReadBuffer( queue, results, CL_TRUE, 0, resultValuesSize, resultValues, 0, NULL, NULL );); (94, OpenCL, clReleaseMemObject(read_only_image);); (43, OpenCL, cl_mem results = clCreateBuffer( context, 0, imageInfo->width * imageInfo->height * imageInfo->arraySize * sizeof(cl_int), NULL, &error );); (90, OpenCL, clReleaseMemObject(results);); (53, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_mem ), &results );)", "label": "(46, OpenCL, actualSampler = clCreateSampler( context, CL_FALSE, CL_ADDRESS_NONE, CL_FILTER_NEAREST, &error );); (55, OpenCL, clEnqueueWriteBuffer( queue, results, CL_TRUE, 0, resultValuesSize, resultValues, 0, NULL, NULL );); (80, OpenCL, error = clEnqueueReadBuffer( queue, results, CL_TRUE, 0, resultValuesSize, resultValues, 0, NULL, NULL );); (75, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 3, NULL, threads, NULL, 0, NULL, NULL );); (60, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_mem ), &read_write_image );); (98, OpenCL, clReleaseMemObject(results);); (96, OpenCL, clReleaseSampler(actualSampler);); (100, OpenCL, clReleaseMemObject(read_write_image);); (96, OpenCL, clReleaseMemObject(read_only_image);); (51, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_mem ), &read_only_image );); (59, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_mem ), &results );); (56, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_sampler ), &actualSampler );); (38, OpenCL, cl_mem results = clCreateBuffer( context, 0, imageInfo->width * imageInfo->height * imageInfo->arraySize * sizeof(cl_int), NULL, &error);)"}
{"predict": "(41, CUDA, checkCudaErrors(cudaMalloc((void **)&d_a, bufsize));); (80, CUDA, checkCudaErrors(cudaFreeHost(h_a));); (81, CUDA, checkCudaErrors(cudaFreeHost(h_c));); (43, CUDA, checkCudaErrors(cudaGetLastError());); (80, CUDA, checkCudaErrors(cudaFreeHost(h_b));); (32, CUDA, checkCudaErrors(cudaMalloc((void **)&d_c, bufsize));); (76, CUDA, checkCudaErrors(cudaFree(d_c));); (32, CUDA, checkCudaErrors(cudaMalloc((void **)&d_b, bufsize));); (8, CUDA, checkCudaErrors(cudaGetDeviceCount(&np));); (6, CUDA, checkCudaErrors(cudaGetDevice(&rank));); (70, CUDA, checkCudaErrors(cudaFree(d_a));); (70, CUDA, checkCudaErrors(cudaFree(d_b));); (76, CUDA, checkCudaErrors(cudaFreeHost(h_b));); (76, CUDA, checkCudaErrors(cudaFreeHost(h_a));); (18, CUDA, checkCudaErrors(cudaMallocHost((void **)&h_a, bufsize));); (18, CUDA, checkCudaErrors(cudaMallocHost((void **)&h_c, bufsize));); (16, CUDA, checkCudaErrors(cudaMallocHost((void **)&h_b, bufsize));)", "label": "(51, OpenMP, omp_set_num_threads(num_operator);); (16, MPI, MPI_Init(&argc, &argv);); (37, CUDA, cudaMalloc((void**)&d_a, bufsize);); (80, CUDA, cudaFree(d_a);); (24, CUDA, cudaMallocHost((void**)&h_a, bufsize);); (25, CUDA, cudaMallocHost((void**)&h_c, bufsize);); (58, CUDA, cudaDeviceSynchronize();); (78, CUDA, cudaFree(d_c);); (44, OpenMP, #pragma omp parallel); (75, CUDA, cudaFree(d_b);); (30, CUDA, cudaMalloc((void**)&d_b, bufsize);); (43, OpenMP, int i = omp_get_thread_num();); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (16, CUDA, cudaMallocHost((void**)&h_b, bufsize);); (75, CUDA, cudaFreeHost(h_b);); (73, CUDA, cudaFreeHost(h_a);); (25, CUDA, cudaMalloc((void**)&d_c, bufsize);); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &np);); (74, MPI, MPI_Finalize();); (71, CUDA, cudaFreeHost(h_c);)"}
{"predict": "(8, OpenMP, #pragma omp simd private(myind, g_ind), linear(ind), aligned(arr) linear(uval(ref)))", "label": "(7, OpenMP, #pragma omp simd private(myind, g_ind), linear(ind), aligned(arr), linear(uval(ref)))"}
{"predict": "(38, MPI, MPI_Send(&sendbuf, 1, struct_type, rank, 0, MPI_COMM_SELF);); (51, MPI, MPI_Get_elements(&status, struct_type, &count);); (54, MPI, MPI_Type_free(&struct_type);); (54, MPI, MPI_Type_free(&contig);); (33, MPI, MPI_Type_commit(&struct_type);); (33, MPI, MPI_Type_contiguous(3, MPI_BYTE, &contig);); (34, MPI, MPI_Recv(&recvbuf, 1, struct_type, rank, 0, MPI_COMM_SELF, &status);); (27, MPI, MPI_Type_create_struct(3, blens, displs, types, &struct_type);); (36, MPI, MPI_Type_free(&contig);); (34, MPI, MPI_Type_free(&struct_type);); (24, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(29, MPI, MPI_Type_contiguous(1, struct_type, &contig);); (51, MPI, MPI_Type_free(&struct_type);); (26, MPI, MPI_Type_create_struct(3, blens, displs, types, &struct_type);); (27, MPI, MPI_Type_commit(&struct_type);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (26, MPI, MPI_Type_commit(&contig);); (45, MPI, MPI_Get_elements(&status, contig, &count);); (34, MPI, MPI_Sendrecv(&sendbuf, 1, contig, 0, 0, &recvbuf, 1, contig, 0, 0, MPI_COMM_SELF, &status);); (45, MPI, MPI_Type_free(&contig);)"}
{"predict": "(2, MPI, MPI_Init(&argc, &argv);); (13, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (6, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(4, MPI, MPI_Init(&argc, &argv);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (11, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(16, TBB, serial_list_time = (tbb::tick_count::now() - t0).seconds();); (42, TBB, serial_tree_time = (tbb::tick_count::now() - t0).seconds();); (53, TBB, tbb_tree_time = (tbb::tick_count::now() - t0).seconds();); (24, TBB, tbb_list_time = (tbb::tick_count::now() - t0).seconds();); (10, TBB, tbb::tick_count t0 = tbb::tick_count::now();)", "label": "(28, TBB, tbb_list_time = (tbb::tick_count::now() - t0).seconds();); (42, TBB, serial_tree_time = (tbb::tick_count::now() - t0).seconds();); (15, TBB, serial_list_time = (tbb::tick_count::now() - t0).seconds();); (12, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (50, TBB, tbb_tree_time = (tbb::tick_count::now() - t0).seconds();)"}
{"predict": "(62, OpenMP, #pragma omp target parallel for collapse(4, 8)); (57, OpenMP, #pragma omp target parallel for collapse(4, 8, 16)); (80, OpenMP, #pragma omp target parallel for collapse(0)); (71, OpenMP, #pragma omp target parallel for collapse(foo())); (74, OpenMP, #pragma omp target parallel for collapse(-5)); (30, OpenMP, #pragma omp target parallel for collapse(4,); (15, OpenMP, #pragma omp target parallel for collapse(, )); (41, OpenMP, #pragma omp target parallel for collapse(4 4)); (63, OpenMP, #pragma omp target parallel for collapse(2.5)); (6, OpenMP, #pragma omp target parallel for collapse()); (1, OpenMP, #pragma omp target parallel for collapse()); (3, OpenMP, #pragma omp target parallel for collapse(,); (31, OpenMP, #pragma omp target parallel for collapse(4)); (66, OpenMP, #pragma omp target parallel for collapse(5 - 5)); (14, OpenMP, #pragma omp target parallel for collapse 4)); (32, OpenMP, #pragma omp target parallel for collapse(4,, 4)); (40, OpenMP, #pragma omp target parallel for collapse(4,, 8)); (69, OpenMP, #pragma omp target parallel for collapse(2) firstprivate(i)); (6, OpenMP, #pragma omp target parallel for collapse(, ))", "label": "(31, OpenMP, #pragma omp target parallel for collapse(4); (54, OpenMP, #pragma omp target parallel for collapse(4,, 4)); (43, OpenMP, #pragma omp target parallel for collapse(4)); (2, OpenMP, #pragma omp target parallel for collapse); (22, OpenMP, #pragma omp target parallel for collapse 4)); (73, OpenMP, #pragma omp target parallel for collapse(-5)); (3, OpenMP, #pragma omp target parallel for collapse(); (79, OpenMP, #pragma omp target parallel for collapse(5 - 5)); (82, OpenMP, #pragma omp target parallel for collapse(2) firstprivate(i)); (41, OpenMP, #pragma omp target parallel for collapse(4 4)); (60, OpenMP, #pragma omp target parallel for collapse(2.5)); (63, OpenMP, #pragma omp target parallel for collapse(foo())); (29, OpenMP, #pragma omp target parallel for collapse(4, )); (0, OpenMP, #pragma omp target parallel for collapse()); (68, OpenMP, #pragma omp target parallel for collapse(0)); (2, OpenMP, #pragma omp target parallel for collapse(,); (20, OpenMP, #pragma omp target parallel for collapse(4,); (4, OpenMP, #pragma omp target parallel for collapse(, )); (48, OpenMP, #pragma omp target parallel for collapse(4, 8))"}
{"predict": "(32, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (34, CUDA, CUDA_CHECK(cudaFree(d_data));)", "label": "(32, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (34, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(17, CUDA, err = cudaMemcpy(&numErr_h, numErr_d, sizeof(int), cudaMemcpyDeviceToHost);); (10, CUDA, err = cudaDeviceSynchronize();); (3, CUDA, cudaMalloc(&numErr_d, sizeof(int));); (20, CUDA, cudaFree(numErr_d);); (4, CUDA, printf(\"Cuda Main: cudaDeviceSynchronize failed with %s\\n\", cudaGetErrorString(err));)", "label": "(5, CUDA, err = cudaMalloc(&numErr_d, sizeof(int));); (25, CUDA, printf(\"Cuda Main: cudaFree failed with %s\\n\", cudaGetErrorString(err));); (11, CUDA, err = cudaDeviceSynchronize();); (4, CUDA, printf(\"Cuda Main: cudaMemcpy failed with %s\\n\", cudaGetErrorString(err));); (20, CUDA, err = cudaFree(numErr_d);); (13, CUDA, err = cudaMemcpy(&numErr_h, numErr_d, sizeof(int), cudaMemcpyDeviceToHost);); (2, CUDA, cudaFree(numErr_d);)"}
{"predict": "(5, CUDA, __shared__ uint s_Hist[HISTOGRAM256_THREADBLOCK_MEMORY];)", "label": "(5, CUDA, __shared__ uint s_Hist[HISTOGRAM256_THREADBLOCK_MEMORY];)"}
{"predict": "(55, MPI, ascent_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (7, MPI, MPI_Comm_rank(comm, &par_rank);); (7, MPI, MPI_Comm_size(comm, &par_size);)", "label": "(55, MPI, ascent_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (7, MPI, MPI_Comm_rank(comm, &par_rank);); (7, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(17, OpenMP, #pragma omp target teams loop map(tofrom : sum) reduction(+ : sum))", "label": "(17, OpenMP, #pragma omp parallel for default(shared) private(i))"}
{"predict": "(35, CUDA, cudaFree(res_dev);); (28, CUDA, cudaMalloc(&res_dev, sizeof(float));); (30, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)", "label": "(35, CUDA, cudaFree(res_dev);); (28, CUDA, cudaMalloc(&res_dev, sizeof(float));); (30, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(43, MPI, ascent_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (15, MPI, MPI_Comm_rank(comm, &par_rank);); (15, MPI, MPI_Comm_size(comm, &par_size);)", "label": "(43, MPI, ascent_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (15, MPI, MPI_Comm_rank(comm, &par_rank);); (15, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(205, CUDA, CUDA_CHECK(cudaFreeAsync(d_A, stream));); (213, CUDA, CUDA_CHECK(cudaFreeAsync(d_C, stream));); (213, CUDA, CUDA_CHECK(cudaFreeAsync(d_work, stream));); (234, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (227, MPI, MPI_Barrier(MPI_COMM_WORLD);); (127, CUDA, CUDA_CHECK(cudaMemcpyAsync(d_A, h_A.data(), llda * loc_n_a * sizeof(double), cudaMemcpyHostToDevice, stream));); (200, CUDA, CUDA_CHECK(cudaFreeAsync(d_C, stream));); (126, CUDA, CUDA_CHECK(cudaMemcpyAsync(d_C, h_C.data(), lldc * loc_n_c * sizeof(double), cudaMemcpyHostToDevice, stream));); (138, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (38, CUDA, CUDA_CHECK(cudaFree(nullptr));); (36, CUDA, CUDA_CHECK(cudaSetDevice(local_device));); (215, CUDA, CUDA_CHECK(cudaFreeAsync(d_A, stream));); (124, CUDA, CUDA_CHECK(cudaMemcpyAsync(d_C, h_C.data(), lldc * loc_n_c * sizeof(double), cudaMemcpyHostToDevice, stream));); (133, CUDA, CUDA_CHECK(cudaMallocAsync(&d_work, workspaceInBytesOnDevice, stream));); (109, CUDA, CUDA_CHECK(cudaMallocAsync(&d_A, llda * loc_n_a * sizeof(double), stream));); (22, MPI, MPI_Init(nullptr, nullptr);); (203, MPI, MPI_Finalize();); (109, CUDA, CUDA_CHECK(cudaMallocAsync(&d_C, lldc * loc_n_c * sizeof(double), stream));); (22, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (20, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nranks);); (11", "label": "(197, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (109, CUDA, CUDA_CHECK(cudaMemcpyAsync(d_A, h_A.data(), llda * loc_n_a * sizeof(double), cudaMemcpyHostToDevice, stream));); (197, MPI, MPI_Barrier(MPI_COMM_WORLD);); (48, CUDA, CUDA_CHECK(cudaFree(nullptr));); (186, CUDA, CUDA_CHECK(cudaFreeAsync(d_C, stream));); (45, CUDA, CUDA_CHECK(cudaSetDevice(local_device));); (20, MPI, MPI_Init(nullptr, nullptr);); (184, CUDA, CUDA_CHECK(cudaFreeAsync(d_work, stream));); (99, CUDA, CUDA_CHECK(cudaMallocAsync(&d_A, llda * loc_n_a * sizeof(double), stream));); (141, MPI, const double begin = MPI_Wtime();); (179, CUDA, CUDA_CHECK(cudaFreeAsync(d_A, stream));); (33, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (96, CUDA, CUDA_CHECK(cudaMallocAsync(&d_C, lldc * loc_n_c * sizeof(double), stream));); (98, CUDA, CUDA_CHECK(cudaMemcpyAsync(d_C, h_C.data(), lldc * loc_n_c * sizeof(double), cudaMemcpyHostToDevice, stream));); (55, CUDA, CUDA_CHECK(cudaStreamCreate(&stream));); (186, MPI, MPI_Finalize();); (27, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nranks);); (126, CUDA, CUDA_CHECK(cudaMallocAsync(&d_work, workspaceInBytesOnDevice, stream));); (157, MPI, const double end = MPI_Wtime();)"}
{"predict": "(1, CUDA, if (!IsGoogleCudaEnabled()) return;)", "label": "(1, CUDA, if (!IsGoogleCudaEnabled()) return;)"}
{"predict": "(3, CUDA, extern __shared__ unsigned scx1[];)", "label": "(3, CUDA, extern __shared__ unsigned scx1[];)"}
{"predict": "(3, CUDA, extern __shared__ float shared[];)", "label": "(3, CUDA, extern __shared__ float shared[];)"}
{"predict": "(47, MPI, MPI_Startall(2, requests);); (13, MPI, MPI_Init(&argc, &argv);); (62, MPI, MPI_Finalize();); (12, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (12, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (34, MPI, MPI_Barrier(MPI_COMM_WORLD);); (53, MPI, t1 = MPI_Wtime();); (32, MPI, t0 = MPI_Wtime();); (43, MPI, MPI_Waitall(2, requests, statuses);); (44, MPI, MPI_Send_finalize(&requests[0]);); (44, MPI, MPI_Recv_finalize(&requests[1]);)", "label": "(48, MPI, MPI_Startall(2, requests);); (50, MPI, MPI_Waitall(2, requests, statuses);); (57, MPI, t1 = MPI_Wtime();); (12, MPI, MPI_Init(&argc, &argv);); (61, MPI, MPI_Finalize();); (11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (31, MPI, MPI_Barrier(MPI_COMM_WORLD);); (31, MPI, t0 = MPI_Wtime();)"}
{"predict": "(12, OpenCL, retVal = clGetDeviceInfo(device, CL_DEVICE_PROFILE, sizeof(profile), profile, NULL);)", "label": "(12, OpenCL, retVal = clGetDeviceInfo(device, CL_DEVICE_PROFILE, sizeof(profile), profile, NULL);)"}
{"predict": "(19, CUDA, setCudaAccessPolicyWindow(env, nativeObject.accessPolicyWindow);)", "label": "(21, CUDA, setCudaAccessPolicyWindow(env, javaObject, nativeObject.accessPolicyWindow);)"}
{"predict": "(22, CUDA, fptype ret = thrust::norm(totalAmp);)", "label": "(22, CUDA, fptype ret = thrust::norm(totalAmp);)"}
{"predict": "(6, OpenMP, #pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp parallel for)"}
{"predict": "(37, MPI, MPI_Type_commit(newtype);); (33, MPI, MPI_Type_create_struct(num, blocklens, displs, types, newtype);); (34, MPI, MPI_Type_get_true_extent(aligned_type, &true_lb, &true_extent);); (30, MPI, MPI_Type_create_resized(aligned_type, 0, true_extent, newtype);); (25, MPI, MPI_Type_get_extent(oldtypes[i], &lb, &extent);); (34, MPI, MPI_Type_free(&aligned_type);); (25, MPI, MPI_Type_size_x(oldtypes[i], &size);); (28, MPI, MPI_Type_get_true_extent(oldtypes[i], &true_lb, &true_extent);); (20, MPI, MPI_Type_create_hvector(size, 1, extent, oldtypes[i], &aligned_type);)", "label": "(38, MPI, MPI_Type_free(&aligned_type);); (36, MPI, MPI_Type_commit(newtype);); (33, MPI, MPI_Type_create_struct(num, blocklens, displs, types, &aligned_type);); (33, MPI, MPI_Type_create_resized(aligned_type, 0, disp, newtype);); (23, MPI, MPI_Type_get_true_extent(oldtypes[i], &true_lb, &true_extent);)"}
{"predict": "(8, OpenCL, void *mappedPtr = clEnqueueMapImage(cmdQ.get(), image.get(), CL_TRUE, CL_MAP_WRITE, &origin[0], &region[0], nullptr, nullptr, 0, nullptr, nullptr, &retVal);); (14, OpenCL, retVal = clEnqueueUnmapMemObject(cmdQ.get(), image.get(), mappedPtr, 0, nullptr, nullptr);)", "label": "(8, OpenCL, void *mappedPtr = clEnqueueMapImage(cmdQ.get(), image.get(), CL_TRUE, CL_MAP_WRITE, &origin[0], &region[0], nullptr, nullptr, 0, nullptr, nullptr, &retVal);); (14, OpenCL, retVal = clEnqueueUnmapMemObject(cmdQ.get(), image.get(), mappedPtr, 0, nullptr, nullptr);)"}
{"predict": "(13, OpenMP, #pragma omp target parallel for if (argc > 0? argv[1] : argv[2])); (27, OpenMP, #pragma omp target parallel for if(parallel : argc); (22, OpenMP, #pragma omp target parallel for if(parallel : argc) if (parallel:argc)); (17, OpenMP, #pragma omp target parallel for if(parallel : argc / 2)); (24, OpenMP, #pragma omp target parallel for if(parallel : argc) if (argc)); (3, OpenMP, #pragma omp target parallel for if (argc))); (21, OpenMP, #pragma omp target parallel for if(parallel : argc) if (for:argc)); (0, OpenMP, #pragma omp target parallel for if (argc); (7, OpenMP, #pragma omp target parallel for if (S1)); (0, OpenMP, #pragma omp target parallel for if (argc))); (10, OpenMP, #pragma omp target parallel for if(parallel :); (2, OpenMP, #pragma omp target parallel for if (foobool(argc)), if (true)); (6, OpenMP, #pragma omp target parallel for if(parallel : argc + i))", "label": "(29, OpenMP, #pragma omp target simd defaultmap(tofrom, scalar); (1, OpenMP, #pragma omp target simd defaultmap); (9, OpenMP, #pragma omp target simd defaultmap(tofrom); (20, OpenMP, #pragma omp target simd defaultmap(tofrom,); (10, OpenMP, #pragma omp target simd defaultmap(tofrom:); (15, OpenMP, #pragma omp target simd defaultmap(tofrom scalar)); (20, OpenMP, #pragma omp target simd defaultmap(scalar:); (-2, OpenMP, #pragma omp target simd defaultmap(); (9, OpenMP, #pragma omp target simd defaultmap(tofrom)); (-1, OpenMP, #pragma omp target simd defaultmap())"}
{"predict": "(85, MPI, ascent_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (92, MPI, MPI_Barrier(comm);); (16, MPI, MPI_Comm_rank(comm, &par_rank);); (16, MPI, MPI_Comm_size(comm, &par_size);)", "label": "(85, MPI, ascent_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (91, MPI, MPI_Barrier(comm);); (16, MPI, MPI_Comm_rank(comm, &par_rank);); (16, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(7, OpenMP, #pragma omp target map(tofrom:colstat[0:3])); (3, OpenMP, #pragma omp target map(tofrom:colstat[0:3]))", "label": "(2, OpenMP, #pragma omp target map(alloc:colstat[0:NZ])); (5, OpenMP, #pragma omp target map(alloc:colstat[:0])); (9, OpenMP, #pragma omp target update from(colstat))"}
{"predict": "(3, TBB, mask |= mask << 1;)", "label": "(41, TBB, ASSERT( (int)tbb::tbb_thread::hardware_concurrency() == availableProcs, NULL );); (39, TBB, ASSERT( tbb::task_scheduler_init::default_num_threads() == availableProcs, NULL );)"}
{"predict": "(8, CUDA, auto err = thrust::make_counting_iterator(0);); (8, CUDA, auto index = thrust::min_element(err, err + num_items, CompareDeviceArrays<T>());); (3, CUDA, thrust::copy(d_data, d_data + num_items, d_tmp_buffer);); (6, CUDA, thrust::copy(d_reference_begin, d_reference_end, host_reference);)", "label": "(7, CUDA, auto err = thrust::mismatch(thrust::device, d_reference_begin, d_reference_end, d_data);); (2, CUDA, CubDebugExit(cudaMemcpy(d_tmp_buffer, host_reference, sizeof(T) * num_items, cudaMemcpyHostToDevice));); (9, CUDA, const auto index = thrust::distance(d_reference_begin, err.first);)"}
{"predict": "(12, CUDA, CUDA_SAFE_CALL(cudaMalloc(&device_buffer, buffer_size * sizeof(value_type)));); (16, CUDA, CUDA_SAFE_CALL(cudaFree(device_buffer));)", "label": "(12, CUDA, CUDA_SAFE_CALL(cudaMalloc(&device_buffer, buffer_size * sizeof(value_type)));)"}
{"predict": "(31, CUDA, CUDA_CHECK(cudaGetLastError());); (5, CUDA, CUDA_CHECK(cudaMallocManaged(&ptr, size));); (29, CUDA, CUDA_CHECK(cudaFree(ptr));)", "label": "(8, CUDA, cudaSetDevice(devs[0]);); (5, CUDA, CUCH(cudaMallocManaged(&ptr, size), kmcudaMemoryAllocationFailure);); (7, CUDA, CUCH(cudaMemcpy(ptr, (*samples)[0].get(), size, cudaMemcpyDefault), kmcudaMemoryCopyError);)"}
{"predict": "(67, CUDA, CUDA_CHECK(cudaFree(d_B));); (51, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (33, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_B), sizeof(data_type) * B.size()));); (64, CUDA, CUDA_CHECK(cudaDeviceReset());); (67, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (29, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (57, CUDA, CUDA_CHECK(cudaFree(d_A));); (22, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)", "label": "(51, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (33, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_B), sizeof(data_type) * B.size()));); (61, CUDA, CUDA_CHECK(cudaFree(d_B));); (66, CUDA, CUDA_CHECK(cudaDeviceReset());); (63, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (28, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (56, CUDA, CUDA_CHECK(cudaFree(d_A));); (22, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(4, CUDA, __shared__ float local[threads];)", "label": "(3, CUDA, __shared__ float local[threads];)"}
{"predict": "(48, MPI, EXPECT_TRUE(check_test_image(output_file, t_apcomp_baseline_dir()));); (9, MPI, MPI_Comm_rank(comm, &par_rank);); (9, MPI, MPI_Comm_size(comm, &par_size);)", "label": "(47, OpenMP, EXPECT_TRUE(check_test_image(output_file, t_apcomp_baseline_dir()));); (9, MPI, MPI_Comm_rank(comm, &par_rank);); (10, MPI, apcomp::mpi_comm(MPI_Comm_c2f(comm));); (8, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(39, MPI, MPI_Alloc_mem(size, MPI_INFO_NULL, &my_base);); (40, MPI, MPI_Win_allocate(0, 1, MPI_INFO_NULL, shm_comm, my_base, &shm_win);); (42, MPI, MPI_Win_shared_query(shm_win, MPI_PROC_NULL, &query_size, &query_disp_unit, &query_base);); (15, MPI, MPI_Win_shared_query(MPI_WIN_SELF, MPI_PROC_NULL, &query_size, &query_disp_unit, &query_base);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (34, MPI, MPI_Win_allocate(size, 1, MPI_INFO_NULL, shm_comm, NULL, &shm_win);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &shm_nproc);); (65, MPI, MPI_Free_mem(my_base);); (6, MPI, MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, rank, MPI_INFO_NULL, &shm_comm);); (30, MPI, MPI_Win_free(&shm_win);); (18, MPI, MPI_Comm_rank(shm_comm, &shm_rank);); (32, MPI, MPI_Barrier(shm_comm);); (25, MPI, MPI_Alloc_mem(size, MPI_INFO_NULL, &my_base);)", "label": "(28, MPI, MPI_Comm_rank(shm_comm, &shm_rank);); (15, MPI, MPI_Win_shared_query(shm_win, MPI_PROC_NULL, &query_size, &query_disp_unit, &query_base);); (13, MPI, MPI_Win_allocate_shared(size, sizeof(int), MPI_INFO_NULL, MPI_COMM_SELF, &my_base, &shm_win);); (23, MPI, MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, rank, MPI_INFO_NULL, &shm_comm);); (34, MPI, MPI_Win_allocate_shared(0, sizeof(int), MPI_INFO_NULL, shm_comm, &my_base, &shm_win);); (20, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (67, MPI, MPI_Comm_free(&shm_comm);); (15, MPI, MPI_Win_free(&shm_win);); (21, MPI, MPI_Comm_size(shm_comm, &shm_nproc);); (31, MPI, MPI_Win_allocate_shared(size, sizeof(int), MPI_INFO_NULL, shm_comm, &my_base, &shm_win);)"}
{"predict": "(13, CUDA, cudaMalloc((void **)&d_arr, sizeof(int) * len);); (55, CUDA, cudaFree(d_out);); (16, CUDA, cudaMemcpy(d_arr, arr, sizeof(int) * len, cudaMemcpyHostToDevice);); (11, CUDA, cudaMalloc((void **)&d_out, sizeof(int) * len);); (52, CUDA, cudaFree(d_arr);); (22, CUDA, cudaMemcpy(out, d_out, sizeof(int) * len, cudaMemcpyDeviceToHost);)", "label": "(13, CUDA, cudaMalloc((void **)&d_arr, sizeof(int) * len);); (16, CUDA, cudaMemcpy(d_arr, arr, sizeof(int) * len, cudaMemcpyHostToDevice);); (53, CUDA, cudaFree(d_out);); (11, CUDA, cudaMalloc((void **)&d_out, sizeof(int) * len);); (23, CUDA, cudaMemcpy(out, d_out, sizeof(int) * len, cudaMemcpyDeviceToHost);); (49, CUDA, cudaFree(d_arr);)"}
{"predict": "(4, OpenMP, int *shared_ptr = (int *)omp_target_alloc(N * sizeof(int), omp_get_default_device());); (4, OpenMP, #pragma omp target enter data map(to : shared_ptr[:N])); (14, OpenMP, omp_target_free(shared_ptr, omp_get_default_device());)", "label": "(4, OpenMP, int *shared_ptr = llvm_omp_target_alloc_shared(N * sizeof(int), device);); (5, OpenMP, #pragma omp target teams distribute parallel for device(device)); (0, OpenMP, const int device = omp_get_default_device();); (13, OpenMP, omp_target_free(shared_ptr, device);)"}
{"predict": "(72, TBB, tbb::flow::make_edge( b2, b3 );); (50, TBB, tbb::flow::make_edge( b2, b );); (48, TBB, tbb::flow::make_edge( b, b2 );); (70, TBB, tbb::flow::buffer_node<T> b_copy(b);); (3, TBB, tbb::flow::make_edge( b, b2 );); (0, TBB, tbb::flow::buffer_node<T> b2(g);); (0, TBB, tbb::flow::buffer_node<T> b3(g);); (-5, TBB, tbb::flow::graph g;); (-3, TBB, tbb::flow::buffer_node<T> b(g);)", "label": "(52, TBB, tbb::flow::make_edge( b2, b3 );); (50, TBB, tbb::flow::make_edge( b, b2 );); (73, TBB, tbb::flow::buffer_node<T> b_copy(b);); (2, TBB, tbb::flow::buffer_node<T> b3(g);); (0, TBB, tbb::flow::buffer_node<T> b2(g);); (-3, TBB, tbb::flow::graph g;); (-3, TBB, tbb::flow::buffer_node<T> b(g);)"}
{"predict": "(22, OpenCL, clReleaseEvent(outEvent1);); (18, OpenCL, clEnqueueWaitForEvents(mockCmdQueue->getCS(0).getDevice(0), 1, &outEvent1);)", "label": "(22, OpenCL, clReleaseEvent(outEvent1);)"}
{"predict": "(62, CUDA, cudaFree( deviceThreadIDs );); (60, CUDA, cudaFreeHost( hostOut );); (58, CUDA, cudaFree( deviceClockValues );); (56, CUDA, cudaEventDestroy( stop );); (54, CUDA, cudaEventDestroy( start );)", "label": "(59, CUDA, cudaFree( deviceClockValues );); (60, CUDA, cudaFreeHost( hostOut );); (61, CUDA, cudaEventDestroy( stop );); (59, CUDA, cudaEventDestroy( start );); (56, CUDA, cudaFree( deviceThreadIDs );)"}
{"predict": "(63, MPI, t2 = MPI_Wtime();); (57, MPI, t1 = MPI_Wtime();)", "label": "(63, MPI, t2 = MPI_Wtime();); (58, MPI, t1 = MPI_Wtime();)"}
{"predict": "(122, CUDA, int accepted = thrust::count_if(flags.begin(), flags.end(), thrust::identity<bool>());)", "label": "(184, CUDA, long accepted  = thrust::count_if(flags.begin(), flags.end(), thrust::identity<bool>());)"}
{"predict": "(34, MPI, MPI_Comm_size(MPI_COMM_WORLD,&mpi_size);); (34, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&mpi_rank);)", "label": "(34, MPI, MPI_Comm_size(MPI_COMM_WORLD,&mpi_size);); (34, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&mpi_rank);)"}
{"predict": "(6, CUDA, extern __shared__ uint64_t s_QHatModp[];)", "label": "(6, CUDA, extern __shared__ uint64_t s_QHatModp[];)"}
{"predict": "(77, MPI, MPI_Info_set(info_in, \"ordering\", \"none\");); (70, MPI, MPI_Win_create(data, ARRAY_LEN * sizeof(twoint_t), sizeof(twoint_t), info_in, MPI_COMM_WORLD, &win);); (10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (59, MPI, MPI_Accumulate(mine, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_REPLACE, win);); (35, MPI, MPI_Info_set(info_in, \"ordering\", \"default\");); (74, MPI, MPI_Win_fence(0, win);); (58, MPI, MPI_Win_fence(0, win);); (44, MPI, MPI_Accumulate(data, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_REPLACE, win);); (4, MPI, MPI_Info_create(&info_in);); (64, MPI, MPI_Win_free(&win);); (30, MPI, MPI_Win_fence(0, win);); (40, MPI, MPI_Win_flush(0, win);); (55, MPI, MPI_Accumulate(mine_plus, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_REPLACE, win);); (58, MPI, MPI_Info_free(&info_in);); (6, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (1, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (20, MPI, MPI_Accumulate(data, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_REPLACE, win);); (66, MPI, MPI_Info_set(info_in, \"ordering\", \"default\");); (20, MPI, MPI_Info_set(info_in, \"ordering\", \"none\");); (33, MPI, MPI_Info_free(&info_in);); (56, MPI, MPI_", "label": "(81, MPI, MPI_Win_create(data, sizeof(twoint_t) * ARRAY_LEN, 1, info_in, MPI_COMM_WORLD, &win);); (81, MPI, MPI_Info_free(&info_in);); (134, MPI, MPI_Accumulate(mine_plus, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_MAXLOC, win);); (151, MPI, MPI_Accumulate(mine_plus, ARRAY_LEN, MPI_2INT, 0, 0, ARRAY_LEN, MPI_2INT, MPI_MAXLOC, win);); (34, MPI, MPI_Win_fence(0, win);); (56, MPI, MPI_Accumulate(mine, ARRAY_LEN, MPI_2INT, 0, 0, ARRAY_LEN, MPI_2INT, MPI_REPLACE, win);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (63, MPI, MPI_Win_free(&win);); (70, MPI, MPI_Info_create(&info_in);); (70, MPI, MPI_Info_set(info_in, \"accumulate_ordering\", \"none\");); (32, MPI, MPI_Accumulate(mine, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_REPLACE, win);); (181, MPI, MPI_Accumulate(mine, ARRAY_LEN, MPI_2INT, 0, 0, ARRAY_LEN, MPI_2INT, MPI_MINLOC, win);); (31, MPI, MPI_Accumulate(mine_plus, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_REPLACE, win);); (48, MPI, MPI_Accumulate(mine_plus, ARRAY_LEN, MPI_2INT, 0, 0, ARRAY_LEN, MPI_2INT, MPI_REPLACE, win);); (160, MPI, MPI_Accumulate(mine, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_MINLOC, win);); (1, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (20, MPI, MPI_Win_create(data, sizeof(twoint_t) * ARRAY_LEN, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &win);)"}
{"predict": "(49, CUDA, CUDA_CHECK(cudaDeviceSynchronize());); (50, CUDA, CUDA_CHECK(cudaGetLastError());); (54, CUDA, CUDA_CHECK(cudaEventElapsedTime(&milliseconds, start, stop));); (46, CUDA, CUDA_CHECK(cudaEventRecord(stop));); (48, CUDA, CUDA_CHECK(cudaEventSynchronize(stop));); (18, CUDA, CUDA_CHECK(cudaDeviceSetLimit(cudaLimitStackSize, 4 * 1024));); (51, CUDA, CUDA_CHECK(cudaEventDestroy(start));); (22, CUDA, CUDA_CHECK(cudaSetDevice(0));); (18, CUDA, CUDA_CHECK(cudaDeviceSetLimit(cudaLimitMallocHeapSize, heap_size));); (44, CUDA, CUDA_CHECK(cudaEventDestroy(stop));); (14, CUDA, CUDA_CHECK(cudaDeviceSetLimit(cudaLimitStackSize, 1024));); (20, CUDA, CUDA_CHECK(cudaEventRecord(start));); (15, CUDA, CUDA_CHECK(cudaEventCreate(&stop));); (3, CUDA, CUDA_CHECK(cudaMallocManaged(&instances_data, sizeof(CuEVM::evm_instance_t) * num_instances));); (5, CUDA, CUDA_CHECK(cudaSetDevice(0));); (15, CUDA, CUDA_CHECK(cudaEventCreate(&start));)", "label": "(56, CUDA, cudaEventRecord(stop);); (22, CUDA, CUDA_CHECK(cudaDeviceSetLimit(cudaLimitStackSize, 2 * 1024));); (55, CUDA, cudaEventSynchronize(stop);); (58, CUDA, cudaEventDestroy(start);); (5, CUDA, CUDA_CHECK(cudaDeviceReset());); (21, CUDA, CUDA_CHECK(cudaDeviceSynchronize());); (46, CUDA, cudaEventRecord(start);); (55, CUDA, cudaEventDestroy(stop);); (57, CUDA, CUDA_CHECK(cudaGetLastError());); (8, CUDA, cudaDeviceGetLimit(&size_value, cudaLimitStackSize);); (12, CUDA, CUDA_CHECK(cudaDeviceSetLimit(cudaLimitMallocHeapSize, heap_size));); (40, CUDA, cudaEventCreate(&stop);); (46, CUDA, cudaEventElapsedTime(&milliseconds, start, stop);); (37, CUDA, cudaEventCreate(&start);)"}
{"predict": "(19, OpenMP, #pragma omp parallel for reduction(+:s)); (10, OpenMP, #pragma omp parallel for)", "label": "(19, OpenMP, #pragma omp parallel for reduction(+:s)); (10, OpenMP, #pragma omp parallel for)"}
{"predict": "(10, MPI, MPI_Allreduce(input, output, 3, MPI_UINT64_T, MPI_MIN_MAX_SUM, comm);)", "label": "(10, MPI, MPI_Allreduce(input, output, 1, dtcmp_type_3uint64t, dtcmp_reduceop_mms_3uint64t, comm);)"}
{"predict": "(66, MPI, MPI_Finalize();); (7, MPI, MPI_Init(NULL, NULL);); (35, MPI, MPI_Wait(&request, &status);); (36, MPI, end_time = MPI_Wtime();); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (24, MPI, MPI_Ireduce(array1, result, array_size, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD, &request);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (6, MPI, start_time = MPI_Wtime();); (12, MPI, MPI_Bcast(&array_size, 1, MPI_INT, 0, MPI_COMM_WORLD);); (60, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(19, MPI, MPI_Bcast(&array_size, 1, MPI_INT, 0, MPI_COMM_WORLD);); (26, MPI, MPI_Wait(&request, &status);); (41, MPI, end_time = MPI_Wtime();); (66, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (2, MPI, MPI_Init(NULL, NULL);); (2, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (25, MPI, start_time = MPI_Wtime();); (12, MPI, MPI_Bcast(&nr_iters, 1, MPI_INT, 0, MPI_COMM_WORLD);); (6, MPI, MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);)"}
{"predict": "(2, OpenMP, omp_autotune(self, OMP_SCALE);)", "label": "(2, OpenMP, omp_autotune(self, OMP_SCALE);)"}
{"predict": "(129, MPI, *ierr = MPI_Comm_spawn( p2[0], *v3, v4, v5, v6, v7, v8 );); (135, MPI, *ierr = MPI_Comm_spawn_multiple( *v1, p3, v4, v5, v6, v7, v8, v9 );); (54, MPI, MPIU_Free(ptmp2);); (58, MPI, MPIU_Free(p3);); (50, MPI, MPIU_Free(p2);)", "label": "(127, MPI, *ierr = MPI_Comm_spawn_multiple( *v1, p2, p3, v4, v5, *v6, (MPI_Comm)(*v7), (MPI_Comm *)(v8), (int *)v9 );)"}
{"predict": "(96, OpenMP, #pragma omp critical); (9, OpenMP, #pragma omp target map(tofrom : largest, bestScore)); (98, OpenMP, #pragma omp master); (92, OpenMP, #pragma omp critical(c1)); (7, OpenMP, #pragma omp teams reduction(max : largest) reduction(max : bestScore))", "label": "(10, OpenMP, #pragma omp parallel); (14, OpenMP, #pragma omp for); (95, OpenMP, #pragma omp critical)"}
{"predict": "(25, MPI, MPI_Waitall(2, requests, MPI_STATUSES_IGNORE);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (18, MPI, MPI_Recv(&a, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (15, MPI, MPI_Isend(&a, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, &requests[1]);); (16, MPI, MPI_Start(&requests[1]);); (20, MPI, MPI_Send(&a, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);); (13, MPI, MPI_Start(&requests[0]);); (2, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (11, MPI, MPI_Isend(&a, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, &requests[0]);)", "label": "(22, MPI, MPI_Start(&requests[0]);); (17, MPI, MPI_Barrier_init(MPI_COMM_WORLD, MPI_INFO_NULL, &requests[1]);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (14, MPI, MPI_Bcast_init(&a, 1, MPI_INT, 0, MPI_COMM_WORLD, MPI_INFO_NULL, &requests[0]);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (18, MPI, MPI_Start(&requests[1]);); (23, MPI, MPI_Waitall(2, requests, MPI_STATUSES_IGNORE);); (5, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (27, MPI, MPI_Request_free(&requests[0]);); (27, MPI, MPI_Request_free(&requests[1]);)"}
{"predict": "(23, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:sum1,b) reduction(+:sum1) collapse(2)); (10, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:sum1) reduction(+:sum1) collapse(2))", "label": "(11, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:sum1) reduction(+:sum1) collapse(2))"}
{"predict": "(24, TBB, serial_tiled_time = (tbb::tick_count::now() - t0).seconds();); (16, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (27, TBB, tbb_time = (tbb::tick_count::now() - t0).seconds();); (15, TBB, serial_time = (tbb::tick_count::now() - t0).seconds();)", "label": "(16, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (22, TBB, serial_tiled_time = (tbb::tick_count::now() - t0).seconds();); (27, TBB, tbb_time = (tbb::tick_count::now() - t0).seconds();); (15, TBB, serial_time = (tbb::tick_count::now() - t0).seconds();)"}
{"predict": "(31, CUDA, cudaMalloc((void**)&h_aPinned, bytes);); (35, CUDA, cudaMalloc((void**)&h_bCached, bytes);); (22, CUDA, cudaMalloc((void**)&d_a, bytes);); (22, CUDA, cudaMalloc((void**)&d_b, bytes);); (44, CUDA, cudaGetDeviceProperties(&prop, deviceID);); (14, CUDA, cudaMalloc((void**)&h_aCached, bytes);); (10, CUDA, cudaMalloc((void**)&h_bPageable, bytes);); (32, CUDA, cudaMemcpy(d_a, h_aPageable, bytes, cudaMemcpyHostToDevice);); (16, CUDA, cudaMalloc((void**)&h_bPinned, bytes);); (34, CUDA, cudaMemcpy(d_b, h_bPageable, bytes, cudaMemcpyHostToDevice);); (36, CUDA, cudaMemcpy(d_b, h_bCached, bytes, cudaMemcpyHostToDevice);); (26, CUDA, cudaMemcpy(h_bPinned, h_bPageable, bytes, cudaMemcpyHostToHost);); (33, CUDA, cudaMemcpy(d_b, h_bPinned, bytes, cudaMemcpyHostToDevice);)", "label": "(50, CUDA, cudaGetDeviceProperties(&prop, 0);); (125, CUDA, cudaFree(h_aCached);); (29, CUDA, cudaMallocHost((void**)&h_aCached, bytes);); (119, CUDA, cudaFreeHost(h_aPinned);); (119, CUDA, cudaFreeHost(h_bPinned);); (29, CUDA, cudaHostAlloc((void**)&h_bPinned, bytes, cudaHostAllocWriteCombined);); (24, CUDA, cudaMalloc((void**)&d_b, bytes);); (26, CUDA, cudaHostAlloc((void**)&h_aPinned, bytes, cudaHostAllocWriteCombined);); (24, CUDA, cudaMallocHost((void**)&h_bCached, bytes);); (116, CUDA, cudaFree(d_b);); (-1, CUDA, cudaSetDevice(0);); (113, CUDA, cudaFree(d_a);); (115, CUDA, cudaFree(h_bCached);); (-10, CUDA, cudaGetDeviceCount(&count);); (-2, CUDA, cudaGetDevice(&deviceID);); (14, CUDA, cudaMalloc((void**)&d_a, bytes);)"}
{"predict": "(53, OpenCL, clReleaseEvent(copyEvent);); (50, OpenCL, err = clEnqueueNDRangeKernel( queue, copyProgram, 1, NULL, &num_elements, NULL, 0, NULL, NULL );); (30, OpenCL, err = clEnqueueWriteBuffer( queue, streams[0], true, 0, sizeof(cl_int)*num_elements, (void *)int_input_ptr, 0, NULL, &copyEvent );); (108, OpenCL, err = clEnqueueReadBuffer( queue, streams[1], true, 0, sizeof(cl_int)*num_elements, (void *)int_output_ptr, 0, NULL, NULL );); (35, OpenCL, clReleaseMemObject( streams[0] );); (45, OpenCL, clReleaseMemObject( streams[1] );); (24, OpenCL, err = clEnqueueCopyBuffer( queue, streams[0], streams[1], 0, 0, sizeof(cl_int)*num_elements, 0, NULL, &copyEvent );)", "label": "(30, OpenCL, err = clEnqueueWriteBuffer( queue, streams[0], true, 0, sizeof(cl_int)*num_elements, (void *)int_input_ptr, 0, NULL, NULL );); (39, OpenCL, err = clEnqueueCopyBuffer( queue, streams[0], streams[1], 0, 0, sizeof(cl_int)*num_elements, 0, NULL, &copyEvent );); (32, OpenCL, clReleaseMemObject( streams[1] );); (107, OpenCL, err = clEnqueueReadBuffer( queue, streams[1], true, 0, sizeof(cl_int)*num_elements, (void *)int_output_ptr, 0, NULL, NULL );); (48, OpenCL, err = clWaitForEvents( 1, &copyEvent );); (28, OpenCL, clReleaseMemObject( streams[0] );); (49, OpenCL, clReleaseEvent(copyEvent);)"}
{"predict": "(7, MPI, MPI_Init(&argc, &argv);); (17, MPI, MPI_Finalize();); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myid);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &numprocs);); (13, MPI, MPI_Recv(buffer, 10, MPI_INT, left, 0, MPI_COMM_WORLD, &status);); (13, MPI, MPI_Send(buffer, 10, MPI_INT, right, 0, MPI_COMM_WORLD);)", "label": "(9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myid);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &numprocs);); (16, MPI, MPI_Finalize();); (4, MPI, MPI_Init(&argc,&argv);); (12, MPI, MPI_Sendrecv_replace(buffer, 10, MPI_INT, left, 123, right, 123, MPI_COMM_WORLD, &status);)"}
{"predict": "(4, TBB, REMARK(\"is_large_object<int>::value=%d\\n\", tbb::interface6::internal::is_large_object<int>::value);); (4, TBB, REMARK(\"is_large_object<double>::value=%d\\n\", tbb::interface6::internal::is_large_object<double>::value);); (4, TBB, REMARK(\"is_large_object<check_type<int>* >::value=%d\\n\", tbb::interface6::internal::is_large_object<check_type<int>* >::value);); (3, TBB, REMARK(\"is_large_object<check_type<int> >::value=%d\\n\", tbb::interface6::internal::is_large_object<check_type<int> >::value);); (3, TBB, REMARK(\"is_large_object<check_type<short> >::value=%d\\n\", tbb::interface6::internal::is_large_object<check_type<short> >::value);); (2, TBB, REMARK(\"is_large_object<size_t>::value=%d\\n\", tbb::interface6::internal::is_large_object<size_t>::value);)", "label": "(3, TBB, REMARK(\"is_large_object<int>::value=%d\\n\", tbb::interface6::internal::is_large_object<int>::value);); (4, TBB, REMARK(\"is_large_object<int *>::value=%d\\n\", tbb::interface6::internal::is_large_object<int *>::value);); (2, TBB, REMARK(\"is_large_object<double>::value=%d\\n\", tbb::interface6::internal::is_large_object<double>::value);); (4, TBB, REMARK(\"is_large_object<check_type<int>* >::value=%d\\n\", tbb::interface6::internal::is_large_object<check_type<int>* >::value);); (2, TBB, REMARK(\"is_large_object<check_type<int> >::value=%d\\n\", tbb::interface6::internal::is_large_object<check_type<int> >::value);); (3, TBB, REMARK(\"is_large_object<check_type<short> >::value=%d\\n\\n\", tbb::interface6::internal::is_large_object<check_type<short> >::value);); (8, TBB, tbb::task_scheduler_init init(nthread);)"}
{"predict": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(22, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (20, MPI, MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);); (45, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_ARE_FATAL);); (38, MPI, MPI_Error_string(ierr, str, &slen);); (17, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(47, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_ARE_FATAL);); (21, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (21, MPI, MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);); (18, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (38, MPI, MPI_Error_string(ierr, str, &slen);)"}
{"predict": "(53, CUDA, extern __shared__ float s_float[];)", "label": "(53, CUDA, extern __shared__ float s_float[];)"}
{"predict": "(26, OpenMP, #pragma omp barrier); (14, OpenMP, #pragma omp parallel); (16, OpenMP, const int i = omp_get_thread_num();)", "label": "(26, OpenMP, #pragma omp barrier); (19, OpenMP, common::nvtx::range omp_scope(\"interruptible::thread-%d\", i);); (13, OpenMP, omp_set_dynamic(0);); (14, OpenMP, #pragma omp parallel reduction(+ : n_finished) reduction(+ : n_cancelled) num_threads(n_threads)); (12, OpenMP, omp_set_num_threads(n_threads);); (14, OpenMP, auto i = omp_get_thread_num();)"}
{"predict": "(26, MPI, MPI_Send(lnzval, len2, MPI_DOUBLE, receiver, k, grid3d->zscp.comm);)", "label": "(25, MPI, MPI_Send(lnzval, len2, MPI_DOUBLE, receiver, k, grid3d->zscp.comm);)"}
{"predict": "(47, OpenMP, lis2mdl_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)", "label": "(47, OpenMP, lis2mdl_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)"}
{"predict": "(2, OpenMP, #pragma omp for schedule(dynamic, 1))", "label": "(2, OpenMP, #pragma omp for schedule(nonmonotonic: dynamic))"}
{"predict": "(21, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(21, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(15, CUDA, cudaMalloc(&d_ret, sizeof(uint32_t));); (12, CUDA, cudaMalloc(&d_stride, STRIDE_MEM_SIZE);); (33, CUDA, cudaFree(d_stride);); (33, CUDA, cudaFree(d_ret);); (33, CUDA, cudaFree(d_clk);); (26, CUDA, cudaMemcpy(h_clk, d_clk, sizeof(uint32_t) * 32, cudaMemcpyDeviceToHost);); (22, CUDA, cudaDeviceSynchronize();); (15, CUDA, cudaMemcpy(d_stride, h_stride, STRIDE_MEM_SIZE, cudaMemcpyHostToDevice);); (12, CUDA, cudaMalloc(&d_clk, sizeof(uint32_t) * 32);); (16, CUDA, cudaMemcpy(d_clk, h_clk, sizeof(uint32_t) * 32, cudaMemcpyHostToDevice);)", "label": "(35, CUDA, cudaFree(d_ret);); (7, CUDA, cudaMallocHost(&h_stride, STRIDE_MEM_SIZE);); (14, CUDA, cudaMalloc(&d_ret, sizeof(uint32_t));); (31, CUDA, cudaFree(d_stride);); (32, CUDA, cudaFree(d_clk);); (32, CUDA, cudaFreeHost(h_stride);); (25, CUDA, cudaMemcpy(h_clk, d_clk, 32 * sizeof(uint32_t), cudaMemcpyDeviceToHost);); (10, CUDA, cudaMemcpy(d_stride, h_stride, STRIDE_MEM_SIZE, cudaMemcpyHostToDevice);); (7, CUDA, cudaMalloc(&d_stride, STRIDE_MEM_SIZE);); (11, CUDA, cudaMalloc(&d_clk, 32 * sizeof(uint32_t));)"}
{"predict": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(6, OpenMP, #pragma omp target enter data map(alloc:a[0:N])); (7, OpenMP, #pragma omp target map(from:a[0:N], b[0:N])); (21, OpenMP, #pragma omp target exit data map(delete:a[0:N])); (1, OpenMP, #pragma omp target enter data map(to:a[0:N]))", "label": "(5, OpenMP, #pragma omp target enter data map(to:a[:N]))"}
{"predict": "(15, TBB, test_basic<MyCheckedSet>(\"concurrent_unordered_set (checked)\", tbb::internal::true_type());); (19, TBB, test_basic<MyCheckedMultiSet>(\"concurrent_unordered_multiset (checked)\", tbb::internal::true_type());); (12, TBB, test_concurrent<MyCheckedSet>( \"concurrent unordered set (checked)\", tbb::internal::true_type() );); (16, TBB, test_concurrent<MyCheckedMultiSet>( \"concurrent unordered multiset (checked)\", tbb::internal::true_type() );)", "label": "(15, TBB, test_basic<MyCheckedStateSet>(\"concurrent unordered set (checked element state)\", tbb::internal::true_type());)"}
{"predict": "(17, CUDA, extern __shared__ _VOLATILE_ float sMem[];)", "label": "(17, CUDA, extern __shared__ _VOLATILE_ float sMem[];)"}
{"predict": "(9, CUDA, factory = *internal::MakeCUDAExecutorImplementation();)", "label": "(9, CUDA, factory = *internal::MakeCUDAExecutorImplementation();)"}
{"predict": "(26, MPI, MPI_Comm_size(MPI_COMM_WORLD,&Num_procs);); (137, MPI, MPI_Finalize();); (79, OpenMP, omp_set_num_threads(nthread_input);); (23, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&my_ID);); (75, MPI, MPI_Bcast(&iterations,1, MPI_INT, root, MPI_COMM_WORLD);); (20, MPI, MPI_Init(&argc,&argv);); (72, MPI, MPI_Bcast(&offset,1, MPI_LONG, root, MPI_COMM_WORLD);); (70, MPI, MPI_Bcast(&length,1, MPI_LONG, root, MPI_COMM_WORLD);); (114, OpenMP, #pragma omp parallel for simd); (121, MPI, MPI_Barrier(MPI_COMM_WORLD);); (125, MPI, MPI_Reduce(&local_nstream_time, &nstream_time, 1, MPI_DOUBLE, MPI_MAX, root, MPI_COMM_WORLD);); (104, OpenMP, #pragma omp master); (37, OpenMP, omp_set_dynamic(0);)", "label": "(103, OpenMP, printf(\"Number of threads    = %d\\n\", omp_get_max_threads());); (25, MPI, MPI_Comm_size(MPI_COMM_WORLD,&Num_procs);); (107, OpenMP, #pragma omp parallel for simd); (147, MPI, MPI_Finalize();); (76, MPI, MPI_Bcast(&length,        1, MPI_LONG, root, MPI_COMM_WORLD);); (22, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&my_ID);); (77, MPI, MPI_Bcast(&nthread_input, 1, MPI_INT,  root, MPI_COMM_WORLD);); (78, OpenMP, omp_set_num_threads(nthread_input);); (17, MPI, MPI_Init(&argc,&argv);); (73, MPI, MPI_Bcast(&iterations,    1, MPI_INT,  root, MPI_COMM_WORLD);); (114, MPI, MPI_Barrier(MPI_COMM_WORLD);); (70, MPI, MPI_Bcast(&offset,        1, MPI_LONG, root, MPI_COMM_WORLD);)"}
{"predict": "(31, CUDA, CHECK(cudaMalloc(&d_y, size));); (74, CUDA, CHECK(cudaFree(d_y));); (31, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyDefault));); (36, CUDA, CHECK(cudaEventCreate(&stop));); (36, CUDA, CHECK(cudaEventRecord(stop));); (71, CUDA, CHECK(cudaFree(d_x));); (36, CUDA, CHECK(cudaEventSynchronize(stop));); (36, CUDA, CHECK(cudaEventElapsedTime(&curr_time, start, stop));); (24, CUDA, CHECK(cudaEventRecord(start));); (24, CUDA, CHECK(cudaEventSynchronize(start));); (24, CUDA, CHECK(cudaMemcpy(h_y, d_y, size, cudaMemcpyDefault));); (20, CUDA, CHECK(cudaEventCreate(&start));); (14, CUDA, CHECK(cudaMalloc(&d_x, M));)", "label": "(6, CUDA, CHECK(cudaMemcpyFromSymbol(&block_size, BLOCK_DIM, sizeof(real)));); (34, CUDA, CHECK(cudaEventCreate(&stop));); (43, CUDA, CHECK(cudaEventRecord(stop));); (78, CUDA, CHECK(cudaFree(d_y));); (33, CUDA, cudaEventQuery(start);); (41, CUDA, CHECK(cudaEventSynchronize(stop));); (41, CUDA, CHECK(cudaEventElapsedTime(&curr_time, start, stop));); (34, CUDA, CHECK(cudaMemcpy(h_y, d_y, size, cudaMemcpyDefault));); (28, CUDA, CHECK(cudaEventRecord(start));); (20, CUDA, CHECK(cudaMalloc(&d_x, M));); (70, CUDA, CHECK(cudaFree(d_x));); (20, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyDefault));); (30, CUDA, CHECK(cudaGetLastError());); (21, CUDA, CHECK(cudaEventCreate(&start));); (16, CUDA, CHECK(cudaMalloc(&d_y, size));)"}
{"predict": "(23, MPI, MPI_Type_get_extent(fd->filetype, &lb, &filetype_extent);); (29, MPI, MPI_Type_size_x(fd->filetype, &filetype_size);)", "label": "(28, MPI, MPI_Type_get_extent(fd->filetype, &lb, &filetype_extent);); (28, MPI, MPI_Type_size_x(fd->filetype, &filetype_size);)"}
{"predict": "(57, CUDA, CHECK_ERROR(cudaFree(device_params));); (12, CUDA, CHECK_ERROR(cudaMalloc((void **)&device_signals, complex_size_bytes));); (54, CUDA, CHECK_ERROR(cudaFree(device_signals));); (11, CUDA, CHECK_ERROR(cudaMemcpy(device_signals, input_signals, complex_size_bytes, cudaMemcpyHostToDevice));); (47, CUDA, CHECK_ERROR(cudaMemcpy(output_signals, device_signals, complex_size_bytes, cudaMemcpyDeviceToHost));); (32, CUDA, cudaMemcpyFromSymbol(&host_callback_ptr, device_callback_ptr, sizeof(host_callback_ptr));); (17, CUDA, CHECK_ERROR(cudaMalloc((void **)&device_params, sizeof(cb_params)));); (17, CUDA, CHECK_ERROR(cudaMemcpy(device_params, &host_params, sizeof(cb_params), cudaMemcpyHostToDevice));)", "label": "(60, CUDA, CHECK_ERROR(cudaFree(device_params));); (12, CUDA, CHECK_ERROR(cudaMalloc((void **)&device_signals, complex_size_bytes));); (50, CUDA, CHECK_ERROR(cudaMemcpy(output_signals, device_signals, complex_size_bytes, cudaMemcpyDeviceToHost));); (56, CUDA, CHECK_ERROR(cudaFree(device_signals));); (34, CUDA, CHECK_ERROR(cudaMemcpyFromSymbol(&host_callback_ptr, device_callback_ptr, sizeof(host_callback_ptr)));); (9, CUDA, CHECK_ERROR(cudaMemcpy(device_signals, input_signals, complex_size_bytes, cudaMemcpyHostToDevice));); (17, CUDA, CHECK_ERROR(cudaMalloc((void **)&device_params, sizeof(cb_params)));); (17, CUDA, CHECK_ERROR(cudaMemcpy(device_params, &host_params, sizeof(cb_params), cudaMemcpyHostToDevice));)"}
{"predict": "(25, CUDA, cudaDeviceSynchronize();)", "label": "(25, CUDA, cudaDeviceSynchronize();)"}
{"predict": "(15, CUDA, checkCudaRuntime(cudaMemcpyAsync(image_device, image_host, size_image, cudaMemcpyHostToDevice, stream));)", "label": "(15, CUDA, checkCudaRuntime(cudaMemcpyAsync(image_device, image_host, size_image, cudaMemcpyHostToDevice, stream));)"}
{"predict": "(6, CUDA, __shared__ float sub_results[x128::size][bdx][OC_per_warp];)", "label": "(35, CUDA, __shared__ float sub_results[x128::size][32][bdy];)"}
{"predict": "(104, CUDA, checkCudaErrors(cudaGetLastError());); (133, CUDA, checkCudaErrors(cudaFreeArray(cu_3darray));); (90, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (10, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProps, devID));); (3, CUDA, int devID = findCudaDevice(argc, (const char **)argv);); (76, CUDA, checkCudaErrors(cudaCreateTextureObject(&tex, &texRes, &texDescr, NULL));); (130, CUDA, checkCudaErrors(cudaFree(d_data));); (107, CUDA, checkCudaErrors(cudaMemcpy(h_odata, d_data, size, cudaMemcpyDeviceToHost));); (37, CUDA, checkCudaErrors(cudaMalloc((void **)&d_data, size));); (37, CUDA, cudaMemcpy(d_data, h_data, size, cudaMemcpyHostToDevice);); (123, CUDA, checkCudaErrors(cudaDestroyTextureObject(tex));); (124, CUDA, checkCudaErrors(cudaDestroyDevice(devID));); (12, CUDA, checkCudaErrors(cudaMallocArray(&cu_3darray, &channelDesc, make_cudaExtent(width, width, num_faces), cudaArrayCubemap));); (8, CUDA, checkCudaErrors(cudaMalloc((void **)&d_data, size));)", "label": "(59, CUDA, myparms.dstPos = make_cudaPos(0, 0, 0);); (9, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProps, devID));); (56, CUDA, myparms.srcPos = make_cudaPos(0, 0, 0);); (119, CUDA, checkCudaErrors(cudaMemcpy(h_odata, d_data, size, cudaMemcpyDeviceToHost));); (94, CUDA, getLastCudaError(\"warmup Kernel execution failed\");); (60, CUDA, checkCudaErrors(cudaMemcpy3D(&myparms));); (57, CUDA, myparms.extent = make_cudaExtent(width, width, num_faces);); (137, CUDA, checkCudaErrors(cudaFreeArray(cu_3darray));); (76, CUDA, checkCudaErrors(cudaCreateTextureObject(&tex, &texRes, &texDescr, NULL));); (91, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (36, CUDA, checkCudaErrors(cudaMalloc((void **)&d_data, size));); (99, CUDA, getLastCudaError(\"Kernel execution failed\");); (131, CUDA, checkCudaErrors(cudaFree(d_data));); (-10, CUDA, int devID = findCudaDevice(argc, (const char **)argv);); (128, CUDA, checkCudaErrors(cudaDestroyTextureObject(tex));)"}
{"predict": "(80, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (80, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)", "label": "(80, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (80, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)"}
{"predict": "(49, MPI, MPI_Waitall(WINDOW, req, status);); (23, MPI, start = MPI_Wtime();); (50, MPI, end = MPI_Wtime();)", "label": "(48, MPI, MPI_Waitall(WINDOW, req, status);); (23, MPI, start = MPI_Wtime();); (50, MPI, end = MPI_Wtime();)"}
{"predict": "(8, SYCL, return hipsycl::libkernel::sscp::internal::work_group_broadcast<__acpp_input_type>(x, shrd_x);)", "label": "(8, SYCL, return hipsycl::libkernel::sscp::wg_broadcast(sender, x, shrd_x);)"}
{"predict": "(135, OpenMP, #pragma omp distribute parallel for schedule(static)); (67, OpenMP, #pragma omp distribute parallel for dist_schedule(static, ch)); (17, OpenMP, #pragma omp distribute parallel for); (100, OpenMP, #pragma omp distribute parallel for schedule(static, ch)); (4, OpenMP, #pragma omp target); (5, OpenMP, #pragma omp teams); (141, OpenMP, #pragma omp distribute parallel for schedule(static)); (164, OpenMP, #pragma omp distribute parallel for schedule(static, ch)); (7, OpenMP, #pragma omp parallel)", "label": "(169, OpenMP, #pragma omp distribute parallel for schedule(static, ch)); (201, OpenMP, #pragma omp distribute parallel for schedule(dynamic)); (17, OpenMP, #pragma omp distribute parallel for); (59, OpenMP, #pragma omp distribute parallel for dist_schedule(static)); (227, OpenMP, #pragma omp distribute parallel for schedule(dynamic, ch)); (132, OpenMP, #pragma omp distribute parallel for schedule(static)); (10, OpenMP, #pragma omp target); (97, OpenMP, #pragma omp distribute parallel for dist_schedule(static, ch)); (9, OpenMP, #pragma omp teams)"}
{"predict": "(137, CUDA, checkCudaErrors(cudaGetLastError());); (155, CUDA, checkCudaErrors(cudaEventSynchronize(profileEnd));); (130, CUDA, checkCudaErrors(cudaEventRecord(profileEnd, 0));); (107, CUDA, checkCudaErrors(cudaEventCreate(&profileEnd));); (37, CUDA, checkCudaErrors(cudaSetDevice(targetDevice));); (103, CUDA, checkCudaErrors(cudaEventCreate(&profileStart));); (147, CUDA, checkCudaErrors(cudaEventDestroy(profileEnd));); (150, CUDA, checkCudaErrors(cudaEventElapsedTime(&elapsedTimeMS, profileStart, profileEnd));); (143, CUDA, checkCudaErrors(cudaEventDestroy(profileStart));); (35, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, targetDevice));); (102, CUDA, checkCudaErrors(cudaEventRecord(profileStart, 0));); (130, CUDA, checkCudaErrors(cudaEventSynchronize(profileStart));)", "label": "(109, CUDA, checkCudaErrors(cudaEventCreate(&profileStart));); (43, CUDA, checkCudaErrors(cudaSetDevice(targetDevice));); (40, CUDA, targetDevice = findCudaDevice(argc, (const char **)argv);); (193, CUDA, checkCudaErrors(cudaEventDestroy(profileEnd));); (140, CUDA, checkCudaErrors(cudaEventRecord(profileEnd, 0));); (116, CUDA, checkCudaErrors(cudaEventRecord(profileStart, 0));); (186, CUDA, checkCudaErrors(cudaEventDestroy(profileStart));); (61, CUDA, checkCudaErrors(cudaFuncGetAttributes(&funcAttrib, FiniteDifferencesKernel));); (31, CUDA, checkCudaErrors(cudaGetDeviceCount(&deviceCount));); (101, CUDA, checkCudaErrors(cudaEventCreate(&profileEnd));); (176, CUDA, checkCudaErrors(cudaFree(bufferOut));); (171, CUDA, checkCudaErrors(cudaFree(bufferIn));)"}
{"predict": "(38, CUDA, cudaFree(res_dev);); (31, CUDA, cudaMalloc(&res_dev, sizeof(float));); (33, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)", "label": "(38, CUDA, cudaFree(res_dev);); (31, CUDA, cudaMalloc(&res_dev, sizeof(float));); (33, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(19, CUDA, checkCudaErrors(cudaGetLastError());); (17, CUDA, checkCudaErrors(cudaStreamSynchronize(stream));)", "label": "(18, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (18, CUDA, checkCudaErrors(cudaGetLastError());)"}
{"predict": "(8, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (9, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;)", "label": "(8, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (9, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;)"}
{"predict": "(26, OpenMP, #pragma omp task); (13, OpenMP, #pragma omp single); (9, OpenMP, #pragma omp parallel); (23, OpenMP, #pragma omp atomic)", "label": "(29, OpenMP, #pragma omp taskwait); (22, OpenMP, #pragma omp task); (9, OpenMP, #pragma omp parallel); (10, OpenMP, #pragma omp single)"}
{"predict": "(2, SYCL, sycl::queue q = get_queue();); (3, SYCL, int* data = sycl::malloc_device<int>(1024, q);); (1, SYCL, sycl::table<int> table(8);)", "label": "(2, SYCL, sycl::queue q = get_queue();); (3, SYCL, int* data = sycl::malloc_device<int>(1024, q);)"}
{"predict": "(111, MPI, MPI_Allreduce(&have_match, &all_valid, 1, MPI_INT, MPI_LAND, comm);); (48, MPI, MPI_Barrier(comm);); (51, MPI, MPI_Bcast(&index, 1, MPI_INT, 0, comm);); (44, MPI, MPI_Bcast(&index, 1, MPI_INT, rank, comm);); (3, MPI, MPI_Comm_rank(comm, &rank);)", "label": "(7, MPI, MPI_Comm_rank(comm, &rank);); (49, MPI, MPI_Bcast(&num_groups, 1, MPI_INT, 0, comm);)"}
{"predict": "(4, OpenMP, #pragma omp sections); (4, OpenMP, #pragma omp section)", "label": "(4, OpenMP, #pragma omp sections); (49, OpenMP, #pragma omp sections nowait); (28, OpenMP, #pragma omp section)"}
{"predict": "(36, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (38, CUDA, CUDA_CHECK(cudaFree(d_data));)", "label": "(36, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (38, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(31, CUDA, runCUDASimulationContextCreationTestWithConfig(m, CUDAConfig().inLayerwiseParallelism(false));); (31, CUDA, runCUDASimulationContextCreationTestWithConfig(m, CUDAConfig().inLayerwiseParallelism(true));)", "label": "(5, CUDA, c.CUDAConfig().device_id = 0;)"}
{"predict": "(66, MPI, MPI_Gatherv(sendbuf, bytes, MPI_BYTE, recvbuf, counts, disps, MPI_BYTE, 0, MPI_COMM_WORLD);); (13, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (36, MPI, MPI_Gather(&bytes, 1, MPI_INT, counts, 1, MPI_INT, 0, MPI_COMM_WORLD);)", "label": "(65, MPI, MPI_Gatherv(sendbuf, bytes, MPI_BYTE, recvbuf, counts, disps, MPI_BYTE, 0, MPI_COMM_WORLD);); (13, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (36, MPI, MPI_Gather(&bytes, 1, MPI_INT, counts, 1, MPI_INT, 0, MPI_COMM_WORLD);)"}
{"predict": "(31, MPI, MPI_Comm_remote_size(intercomm, &orig_rsize);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (24, MPI, MPI_Comm_split(intercomm, color, key, &newcomm);); (89, MPI, MPI_Comm_free_keyval(&key);); (26, MPI, MPI_Comm_size(newcomm, &new_size);); (24, MPI, MPI_Comm_rank(newcomm, &new_rank);); (19, MPI, MPI_Comm_create_keyval(MPI_NULL_COPY_FN, MPI_NULL_DELETE_FN, &key, (void *) 0);); (25, MPI, MPI_Comm_remote_size(newcomm, &new_rsize);); (6, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (16, MPI, MPI_Comm_rank(intercomm, &orig_rank);)", "label": "(32, MPI, MPI_Comm_test_inter(intercomm, &flag);); (23, MPI, MPI_Comm_rank(intercomm, &key);); (90, MPI, MPI_Comm_free(&intercomm);); (38, MPI, MPI_Comm_size(intercomm, &orig_size);); (38, MPI, MPI_Comm_size(newcomm, &new_size);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (33, MPI, MPI_Comm_remote_size(intercomm, &orig_rsize);); (19, MPI, MPI_Comm_split(intercomm, color, key, &newcomm);); (3, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (31, MPI, MPI_Comm_remote_size(newcomm, &new_rsize);)"}
{"predict": "(5, CUDA, extern __shared__ float shared[];)", "label": "(6, CUDA, extern __shared__ float shared[];)"}
{"predict": "(35, CUDA, cudaFree(control_d);); (33, CUDA, cudaFree(state_der_d);); (24, CUDA, cudaDeviceSynchronize();); (22, CUDA, CudaCheckError();); (30, CUDA, cudaFree(state_d);)", "label": "(34, CUDA, cudaFree(control_d);); (32, CUDA, cudaFree(state_der_d);); (22, CUDA, CudaCheckError();); (29, CUDA, cudaFree(state_d);)"}
{"predict": "(6, CUDA, fcuda::Check_CudaErroorFun(\"DgReduSumUint\");); (6, CUDA, cudaMemcpy(data,datag+inidata,sizeof(unsigned)*ndata,cudaMemcpyDeviceToHost);)", "label": "(7, CUDA, fcuda::Check_CudaErroorFun(\"DgReduSumUint\");); (5, CUDA, cudaMemcpy(data,datag+inidata,sizeof(unsigned)*ndata,cudaMemcpyDeviceToHost);)"}
{"predict": "(37, TBB, oneapi::tbb::spin_mutex::scoped_lock lock (MyMutex);)", "label": "(37, TBB, tbb::spin_mutex::scoped_lock lock (MyMutex);)"}
{"predict": "(25, CUDA, extern __shared__ float sram[];)", "label": "(24, CUDA, extern __shared__ float sram[];)"}
{"predict": "(15, OpenMP, #pragma omp target); (17, OpenMP, #pragma omp teams); (16, OpenMP, #pragma omp distribute parallel for); (13, OpenMP, #pragma omp parallel)", "label": "(14, OpenMP, #pragma omp parallel); (14, OpenMP, #pragma omp task if (task: false))"}
{"predict": "(20, OpenMP, #pragma omp target teams distribute parallel for collapse (2+2))); (1, OpenMP, #pragma omp target teams distribute parallel for collapse); (30, OpenMP, #pragma omp target teams distribute parallel for collapse (foobool(argc)), collapse (true), collapse (-5)); (47, OpenMP, #pragma omp target teams distribute parallel for collapse (2)); (13, OpenMP, #pragma omp target teams distribute parallel for collapse (4); (4, OpenMP, #pragma omp target teams distribute parallel for collapse ()); (24, OpenMP, #pragma omp target teams distribute parallel for collapse (S1)); (38, OpenMP, #pragma omp target teams distribute parallel for collapse(collapse(tmain<int, char, -1, -2>(argc, argv)); (0, OpenMP, #pragma omp target teams distribute parallel for collapse ()); (4, OpenMP, #pragma omp target teams distribute parallel for collapse (2+2))); (13, OpenMP, #pragma omp target teams distribute parallel for collapse (foobool(1) > 0? 1 : 2)); (39, OpenMP, #pragma omp target teams distribute parallel for collapse (0))", "label": "(47, OpenMP, #pragma omp target teams distribute parallel for collapse(collapse(tmain<int, char, -1, -2>(argc, argv)); (4, OpenMP, #pragma omp target teams distribute parallel for collapse (); (7, OpenMP, #pragma omp target teams distribute parallel for collapse ()); (18, OpenMP, #pragma omp target teams distribute parallel for collapse (foobool(1) > 0? 1 : 2)); (13, OpenMP, #pragma omp target teams distribute parallel for collapse (2+2))); (23, OpenMP, #pragma omp target teams distribute parallel for collapse (foobool(argc)), collapse (true), collapse (-5)); (7, OpenMP, #pragma omp target teams distribute parallel for collapse (4); (25, OpenMP, #pragma omp target teams distribute parallel for collapse (S1)); (-7, OpenMP, #pragma omp target teams distribute parallel for collapse); (41, OpenMP, #pragma omp target teams distribute parallel for collapse (2))"}
{"predict": "(32, CUDA, cudaFree(resourceList->outputBufferGPU);); (44, CUDA, cudaStreamDestroy(resourceList->stream);); (26, CUDA, cudaFree(resourceList->inputBufferGPU);)", "label": "(32, CUDA, cudaFree(resourceList->outputBufferGPU);); (27, CUDA, cudaFree(resourceList->inputBufferGPU);); (44, CUDA, cudaStreamDestroy(resourceList->stream);)"}
{"predict": "(76, CUDA, EXPECT_EQ(cudaSuccess, cudaMemcpy(test.data(), output->basePtr(), outBufSize, cudaMemcpyDeviceToHost));); (74, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (2, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (65, CUDA, EXPECT_EQ(cudaSuccess, cudaMemset(input->basePtr(), 0x00, inSampleStride * inAccess->numSamples()));); (65, CUDA, EXPECT_EQ(cudaSuccess, cudaMemset(output->basePtr(), 0x00, outSampleStride * outAccess->numSamples()));); (70, CUDA, EXPECT_EQ(cudaSuccess, cudaMemcpy(testIn.data(), input->basePtr(), inBufSize, cudaMemcpyDeviceToHost));); (62, CUDA, EXPECT_EQ(cudaSuccess, cudaMemset(output->basePtr(), 0x00, outSampleStride * outAccess->numSamples()));); (79, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamDestroy(stream));)", "label": "(77, CUDA, EXPECT_EQ(cudaSuccess, cudaMemcpy(test.data(), output->basePtr(), outBufSize, cudaMemcpyDeviceToHost));); (2, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (64, CUDA, EXPECT_EQ(cudaSuccess, cudaMemset(input->basePtr(), 0xFF, inSampleStride * inAccess->numSamples()));); (72, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (72, CUDA, EXPECT_EQ(cudaSuccess, cudaMemcpy(testIn.data(), input->basePtr(), inBufSize, cudaMemcpyDeviceToHost));); (62, CUDA, EXPECT_EQ(cudaSuccess, cudaMemset(output->basePtr(), 0xFF, outSampleStride * outAccess->numSamples()));); (78, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamDestroy(stream));)"}
{"predict": "(28, OpenMP, #pragma omp target exit data map(from:check3[:ThreadLimit])); (17, OpenMP, #pragma omp target enter data map(to:check1[:ThreadLimit])); (23, OpenMP, #pragma omp target enter data map(to:check3[:ThreadLimit])); (25, OpenMP, #pragma omp target exit data map(from:check2[:ThreadLimit])); (10, OpenMP, #pragma omp parallel); (10, OpenMP, #pragma omp target map(check1[:], check2[:], check3[:], threadLimit)); (24, OpenMP, #pragma omp target exit data map(from:check1[:ThreadLimit])); (18, OpenMP, #pragma omp target enter data map(to:check2[:ThreadLimit]))", "label": "(28, OpenMP, #pragma omp parallel num_threads(NumThreads3)); (29, OpenMP, check3[omp_get_thread_num()] += omp_get_num_threads();); (15, OpenMP, #pragma omp parallel); (10, OpenMP, threadLimit = omp_get_thread_limit();); (6, OpenMP, #pragma omp target teams num_teams(1) thread_limit(ThreadLimit)); (20, OpenMP, check2[omp_get_thread_num()] += omp_get_num_threads();); (16, OpenMP, omp_set_num_threads(NumThreads2);); (12, OpenMP, check1[omp_get_thread_num()] += omp_get_num_threads();)"}
{"predict": "(18, CUDA, getLastCudaError(\"mergeElementaryIntervalsKernel<0> failed\\n\");); (12, CUDA, getLastCudaError(\"mergeElementaryIntervalsKernel<1> failed\\n\");)", "label": "(13, CUDA, getLastCudaError(\"mergeElementaryIntervalsKernel<1> failed\\n\");); (17, CUDA, getLastCudaError(\"mergeElementaryIntervalsKernel<0> failed\\n\");)"}
{"predict": "(55, CUDA, cudaFree(C_reference);); (107, CUDA, result = cudaMemcpy(host_cutlass.data(), C_cutlass, sizeof_C, cudaMemcpyDeviceToHost);); (51, CUDA, result = cudaMemcpy(C_reference, C_cutlass, sizeof_C, cudaMemcpyDeviceToDevice);); (120, CUDA, result = cudaMemcpy(host_reference.data(), C_reference, sizeof_C, cudaMemcpyDeviceToHost);); (53, CUDA, cudaFree(C_cutlass);); (136, CUDA, cudaFree(B);); (136, CUDA, cudaFree(A);); (35, CUDA, cudaFree(A);); (35, CUDA, cudaFree(B);)", "label": "(42, CUDA, cudaFree(B);); (109, CUDA, result = cudaMemcpy(host_cutlass.data(), C_cutlass, sizeof_C, cudaMemcpyDeviceToHost);); (59, CUDA, cudaFree(C_reference);); (52, CUDA, result = cudaMemcpy(C_reference, C_cutlass, sizeof_C, cudaMemcpyDeviceToDevice);); (47, CUDA, cudaFree(C_cutlass);); (119, CUDA, result = cudaMemcpy(host_reference.data(), C_reference, sizeof_C, cudaMemcpyDeviceToHost);); (28, CUDA, cudaFree(A);)"}
{"predict": "(41, OpenMP, #pragma omp teams reduction(+:sum)); (38, OpenMP, #pragma omp target map(tofrom: b, sum, t) map(alloc: a)); (39, OpenMP, #pragma omp distribute parallel for reduction(+:sum)); (42, OpenMP, t[i] = omp_get_team_num();)", "label": "(40, OpenMP, #pragma omp teams reduction(+:sum)); (37, OpenMP, #pragma omp target map(tofrom: b, sum, t) map(alloc: a)); (40, OpenMP, #pragma omp distribute parallel for reduction(+:sum)); (42, OpenMP, t[i] = omp_get_team_num();)"}
{"predict": "(14, CUDA, fprintf(stderr, \"CUDA kernel failed : %s\\n\", cudaGetErrorString(err));); (11, CUDA, err = cudaGetLastError();)", "label": "(14, CUDA, fprintf(stderr, \"CUDA kernel failed : %s\\n\", cudaGetErrorString(err));); (11, CUDA, err = cudaGetLastError();)"}
{"predict": "(4, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (5, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;)", "label": "(4, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (5, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;)"}
{"predict": "(12, CUDA, cudaExtent extent = make_cudaExtent(32, 64, 0);); (12, CUDA, cudaOrigin origin = make_cudaOrigin(0, 0, 0);); (10, CUDA, std::vector<float> data_vec(32 * 64 * 4);); (4, CUDA, CudaCheckError();); (4, CUDA, cudaMemcpy(data_vec.data(), map.data, 32 * 64 * 4 * sizeof(float), cudaMemcpyHostToDevice);)", "label": "(4, CUDA, CudaCheckError();)"}
{"predict": "((82, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &iam );); (257, MPI, MPI_Finalize();); (24, MPI, MPI_Init_thread( &argc, &argv, MPI_THREAD_MULTIPLE, &omp_mpi_level);); (25, MPI, MPI_Comm_size( MPI_COMM_WORLD, &nprocs );)", "label": "(82, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &iam );); (257, MPI, MPI_Finalize();); (24, MPI, MPI_Init_thread( &argc, &argv, MPI_THREAD_MULTIPLE, &omp_mpi_level);); (25, MPI, MPI_Comm_size( MPI_COMM_WORLD, &nprocs );)"}
{"predict": "(344, CUDA, __shared__ int cur_send_num;); (342, CUDA, __shared__ int recv_num, finish_num;)", "label": "(343, CUDA, __shared__ int cur_send_num;); (341, CUDA, __shared__ int recv_num, finish_num;)"}
{"predict": "(4, CUDA, __shared__ float sdata[THREAD_PER_BLOCK];)", "label": "(4, CUDA, __shared__ float sdata[THREAD_PER_BLOCK];)"}
{"predict": "((78, MPI, MPI_Bcast(footer, 7, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (3, MPI, MPI_Barrier(MPI_COMM_WORLD);); (73, MPI, MPI_Bcast(&footer_flag, 1, MPI_INT, 0, MPI_COMM_WORLD);); (161, MPI, MPI_Bcast(&rc, 1, MPI_INT, 0, MPI_COMM_WORLD);); (137, MPI, MPI_Bcast(&block_total, 1, MPI_INT64_T, 0, MPI_COMM_WORLD);); (135, MPI, MPI_Bcast(&block_meta, 1, MPI_INT64_T, 0, MPI_COMM_WORLD);); (136, MPI, MPI_Bcast(&block_size, 1, MPI_INT64_T, 0, MPI_COMM_WORLD);); (16, MPI, MPI_Bcast(&fd, 1, MPI_INT, 0, MPI_COMM_WORLD);); (132, MPI, MPI_Bcast(&version, 1, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (132, MPI, MPI_Bcast(&magic, 1, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (33, MPI, MPI_Bcast(&lseek_rc, 1, MPI_LONG_LONG, 0, MPI_COMM_WORLD);); (16, MPI, MPI_Bcast(&footer_flag, 1, MPI_INT, 0, MPI_COMM_WORLD);); (31, MPI, MPI_Bcast(file_footer, footer_size, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (150, MPI, MPI_Bcast(&size, 1, MPI_LONG_LONG, 0, MPI_COMM_WORLD);); (20, MPI, MPI_", "label": "(10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (76, MPI, MPI_Bcast(&footer_flag, 1, MPI_INT, 0, MPI_COMM_WORLD);); (76, MPI, MPI_Bcast(&footer, 8, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(2, CUDA, extern __shared__ float smem[];)", "label": "(2, CUDA, __shared__ float smem[1024];)"}
{"predict": "(38, CUDA, CUDASimulation cudaSimulation(model);)", "label": "(38, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(29, CUDA, checkCudaErrors(cudaSetDevice(gpuID));)", "label": "(29, CUDA, checkCudaErrors(cudaSetDevice(gpuID));)"}
{"predict": "kernel arg(129, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(cl_mem), &buffer);); (136, OpenCL, err = clEnqueueNDRangeKernel(queue, kernel, dimension, offset, gws, lws, 0, NULL, NULL);); (144, OpenCL, clReleaseMemObject(buffer);); (145, OpenCL, clReleaseKernel(kernel);); (146, OpenCL, clReleaseProgram(program);); (136, OpenCL, clReleaseCommandQueue(queue);); (38, OpenCL, context = clCreateContext(NULL, 1, &device_id, NULL, NULL, &err);); (100, OpenCL, buffer = clCreateBuffer(context, flags, bufferSize, NULL, &err);); (135, OpenCL, clReleaseContext(context);); (63, OpenCL, program = clCreateProgramWithSource(context, 1, &strings, NULL, &err);); (70, OpenCL, err = clBuildProgram(program, 1, &device_id, NULL, NULL, NULL);); (37, OpenCL, queue = clCreateCommandQueue(context, device_id, 0, &err);); (90, OpenCL, kernel = clCreateKernel(program, \"hello\", &err);); (108, OpenCL, void *ptr = clEnqueueMapBuffer(queue, buffer, CL_TRUE, CL_MAP_WRITE, 0, bufferSize, 0, NULL, NULL, &err);); (109, OpenCL, err = clEnqueueUnmapMemObject(queue, buffer, ptr, 0, NULL, NULL);); (137, OpenCL, clReleaseMemObject(buffer);); (142, OpenCL, clReleaseKernel(kernel);); (134, OpenCL, clReleaseProgram(program);)", "label": "(69, OpenCL, program = clCreateProgramWithSource(context, 1, &strings, 0, &err);); (129, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(cl_mem), &buffer);); (190, OpenCL, clReleaseKernel(kernel);); (191, OpenCL, clReleaseCommandQueue(queue);); (189, OpenCL, clReleaseProgram(program);); (139, OpenCL, err = clEnqueueNDRangeKernel(queue, kernel, dimension, offset, gws, lws, 0, 0, nullptr);); (189, OpenCL, clReleaseContext(context);); (101, OpenCL, buffer = clCreateBuffer(context, flags, bufferSize, nullptr, &err);); (115, OpenCL, err = clEnqueueUnmapMemObject(queue, buffer, ptr, 0, nullptr, nullptr);); (106, OpenCL, void *ptr = clEnqueueMapBuffer(queue, buffer, CL_TRUE, CL_MAP_WRITE, 0, bufferSize, 0, nullptr, nullptr, &err);); (38, OpenCL, queue = clCreateCommandQueue(context, device_id, 0, &err);); (140, OpenCL, void *ptr = clEnqueueMapBuffer(queue, buffer, CL_TRUE, CL_MAP_READ, 0, bufferSize, 0, nullptr, nullptr, &err);); (64, OpenCL, err = clBuildProgram(program, 1, &device_id, nullptr, nullptr, nullptr);); (178, OpenCL, clReleaseMemObject(buffer);); (28, OpenCL, context = clCreateContext(0, 1, &device_id, NULL, NULL, &err);); (84, OpenCL, kernel = clCreateKernel(program, \"hello\", &err);)"}
{"predict": "(6, MPI, MPI_Comm_size(comm, &nproc);); (4, MPI, MPI_Comm_rank(comm, &rank);)", "label": "(6, MPI, MPI_Comm_size(comm, &nproc);); (4, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(17, TBB, tbb::tick_count t1 = tbb::tick_count::now();); (10, TBB, tbb::tick_count ts_0 = tbb::tick_count::now();); (3, TBB, c.set_core_type(tbb::info::core_types().back());); (3, TBB, c.set_numa_id(tbb::info::numa_nodes()[0]);); (3, TBB, tbb::task_arena::constraints c;); (10, TBB, tbb::tick_count t2 = tbb::tick_count::now();); (16, TBB, double v4 = tbb::global_control::active_value(tbb::global_control::max_allowed_parallelism);); (14, TBB, tbb::tick_count t3 = tbb::tick_count::now();); (16, TBB, double v2 = tbb::global_control::active_value(tbb::global_control::max_allowed_parallelism);); (1, TBB, tbb::task_arena a(c);); (7, TBB, tbb::tick_count t4 = tbb::tick_count::now();); (13, TBB, double v3 = tbb::global_control::active_value(tbb::global_control::max_allowed_parallelism);); (1, TBB, tbb::tick_count ts_1 = tbb::tick_count::now();); (10, TBB, tbb::tick_count t5 = tbb::tick_count::now();); (13, TBB, double v0 = tbb::global_control::active_value(tbb::global_control::max_allowed_parallelism);)", "label": "(27, TBB, tbb::tick_count t1 = tbb::tick_count::now();); (23, TBB, reducePiExample<tbb::auto_partitioner>(num_intervals, gs);); (6, TBB, c.set_max_concurrency(std::min(8, tbb::info::default_concurrency(c)));); (29, TBB, double v3 = deterministicReducePiExample<tbb::simple_partitioner>(num_intervals, gs);); (27, TBB, tbb::tick_count t3 = tbb::tick_count::now();); (-1, TBB, tbb::task_arena::constraints c;); (11, TBB, tbb::tick_count ts_1 = tbb::tick_count::now();); (18, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (27, TBB, tbb::tick_count t5 = tbb::tick_count::now();); (25, TBB, double v4 = deterministicReducePiExample<tbb::static_partitioner>(num_intervals, gs);); (5, TBB, tbb::tick_count ts_0 = tbb::tick_count::now();); (-2, TBB, tbb::task_arena a(c);); (-6, TBB, c.set_core_type(tbb::info::core_types().back());); (20, TBB, tbb::tick_count t4 = tbb::tick_count::now();); (14, TBB, double v1 = reducePiExample<tbb::simple_partitioner>(num_intervals, gs);); (15, TBB, double v2 = reducePiExample<tbb::static_partitioner>(num_intervals, gs);); (-11, TBB, c.set_numa_id(tbb::info::numa_nodes()[0]);); (12, TBB, tbb::tick_count t2 = tbb::tick_count::now();); (8, TBB, double v0 = reducePiExample<tbb::auto_partitioner>(num_intervals, gs);)"}
{"predict": "(10, CUDA, cuda_safe_call(cudaStreamCreate(&stream));); (7, CUDA, cuda_safe_call(cudaMalloc((void **)&d_ptrA, N * sizeof(double)));); (7, CUDA, cuda_safe_call(cudaMalloc((void **)&d_ptrB, N * sizeof(double)));); (14, CUDA, cuda_safe_call(cudaStreamSynchronize(stream));)", "label": "(18, CUDA, cuda_safe_call(cudaStreamSynchronize(stream));); (7, CUDA, cuda_safe_call(cudaMalloc(&d_ptrB, N * sizeof(double)));); (9, CUDA, cuda_safe_call(cudaStreamCreate(&stream));); (4, CUDA, cuda_safe_call(cudaMalloc(&d_ptrA, N * sizeof(double)));)"}
{"predict": "(13, CUDA, __shared__ alignas(alignof(float4)) float Bs[BLOCK_SIZE][BLOCK_SIZE];); (8, CUDA, __shared__ alignas(alignof(float4)) float As[BLOCK_SIZE][BLOCK_SIZE];)", "label": "(13, CUDA, __shared__ alignas(alignof(float4)) float Bs[BLOCK_SIZE][BLOCK_SIZE];); (8, CUDA, __shared__ alignas(alignof(float4)) float As[BLOCK_SIZE][BLOCK_SIZE];)"}
{"predict": "(78, OpenMP, lis2mdl_offset_temp_comp_set(&mag_ctx, PROPERTY_ENABLE);)", "label": "(78, OpenMP, lis2mdl_offset_temp_comp_set(&mag_ctx, PROPERTY_ENABLE);)"}
{"predict": "(9, OpenMP, num = omp_get_device_num();); (1, OpenMP, unsigned num_devices = omp_get_num_devices();); (4, OpenMP, omp_set_device(i);)", "label": "(2, OpenMP, const int num_devices = omp_get_num_devices();); (6, OpenMP, #pragma omp target map(from : num) device(i)); (6, OpenMP, num = omp_get_device_num();)"}
{"predict": "(55, OpenCL, clReleaseEvent(clEvent);)", "label": "(55, OpenCL, clReleaseEvent(clEvent);)"}
{"predict": "(15, CUDA, auto err = cudaGetDeviceProperties(&props, dev);); (44, CUDA, cudaDeviceCanAccessPeer(&access, dev1, dev2);); (4, CUDA, cudaSetDevice(0);); (56, CUDA, auto err = cudaDeviceEnablePeerAccess(odev, 0);); (52, CUDA, auto err = cudaDeviceEnablePeerAccess(dev, 0);)", "label": "(47, CUDA, cudaDeviceCanAccessPeer(&access, dev1, dev2);); (53, CUDA, cudaSetDevice(dev);); (57, CUDA, auto err = cudaDeviceEnablePeerAccess(odev, 0);); (14, CUDA, auto err = cudaGetDeviceProperties(&props, dev);); (9, CUDA, INFO(\"failed to cudaSetDevice(%d)\\n\", dev);); (-2, CUDA, cudaGetDeviceCount(reinterpret_cast<int *>(&device));)"}
{"predict": "(19, TBB, tbb::spin_mutex::scoped_lock lock(critical_section);); (8, TBB, tbb::spin_mutex::scoped_lock lock(my_mutex);)", "label": "(9, TBB, oneapi::tbb::spin_mutex::scoped_lock l(locks[u]);); (20, TBB, oneapi::tbb::spin_mutex::scoped_lock l(locks[v]);)"}
{"predict": "(17, MPI, MPI_Bcast(&nccl_id, sizeof(nccl_id), MPI_BYTE, 0, MPI_COMM_WORLD);); (6, MPI, MPI_Init(NULL, NULL);); (18, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(6, MPI, MPI_CHECK(MPI_Init(NULL, NULL));); (15, MPI, MPI_CHECK(MPI_Bcast((void *)&nccl_id, sizeof(nccl_id), MPI_BYTE, 0, MPI_COMM_WORLD));); (16, CUDA, cudaSetDevice(rank);)"}
{"predict": "(22, CUDA, __shared__ float v[_N_], r[_N_], k[_N_], gy[_N_];)", "label": "(22, CUDA, __shared__ float v[_N_], r[_N_], k[_N_], gy[_N_];)"}
{"predict": "(93, OpenMP, #pragma omp target teams distribute parallel for); (86, OpenMP, #pragma omp target parallel for)", "label": "(93, OpenMP, #pragma omp target teams distribute parallel for); (86, OpenMP, #pragma omp target parallel for)"}
{"predict": "(49, MPI, MPI_Put(&localbuf[i], 1, MPI_INT, trank, rank + i * wsize, 1, MPI_INT, win);); (53, MPI, MPI_Win_unlock(trank, win);); (44, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, trank, 0, win);); (47, MPI, MPI_Put(&localbuf[i], 1, MPI_INT, trank, rank + i * wsize, 1, MPI_INT, win);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (40, MPI, MPI_Put(&localbuf[i], 1, MPI_INT, trank, rank + i * wsize, 1, MPI_INT, win);); (96, MPI, MPI_Win_unlock(trank, win);); (3, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (43, MPI, MPI_Barrier(MPI_COMM_WORLD);); (8, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, trank, 0, win);); (33, MPI, MPI_Put(&localbuf2[i * NBLOCK], NBLOCK, MPI_INT, trank, NELM * wsize + NBLOCK * (rank + i * wsize), NBLOCK, MPI_INT, win);); (34, MPI, MPI_Put(&localbuf2[i * NBLOCK], NBLOCK, MPI_INT, trank, NELM * wsize + NBLOCK * (rank + (i + NELM) * wsize), NBLOCK, MPI_INT, win);); (4, MPI, MPI_Win_create(rmabuf, windowsize * sizeof(int), sizeof(int), MPI_INFO_NULL, MPI_COMM_WORLD, &win);); (42, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, trank, 0, win);)", "label": "(12, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (208, MPI, MPI_Win_free(&win);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (46, MPI, MPI_Put(&localbuf[i], 1, MPI_INT, trank, rank + i * wsize, 1, MPI_INT, win);); (44, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, trank, 0, win);); (45, MPI, MPI_Put(&localbuf[i], 1, MPI_INT, trank, rank + (i + NELM) * wsize, 1, MPI_INT, win);); (45, MPI, MPI_Win_unlock(trank, win);); (127, MPI, MPI_Get(&vals[i], 1, MPI_INT, trank, i, 1, MPI_INT, win);); (183, MPI, MPI_Accumulate(&localbuf[i], 1, MPI_INT, trank, rank + i * wsize, 1, MPI_INT, MPI_SUM, win);); (37, MPI, MPI_Barrier(MPI_COMM_WORLD);); (6, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(14, CUDA, thrust::device_vector<Element> d_src = h_src;); (8, CUDA, thrust::host_vector<Element> h_src(kNumel);)", "label": "(14, CUDA, thrust::device_vector<Element> d_src = h_src;); (8, CUDA, thrust::host_vector<Element> h_src(kNumel);)"}
{"predict": "(51, CUDA, cudaFree(res_dev);); (44, CUDA, cudaMalloc(&res_dev, sizeof(float));); (46, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)", "label": "(51, CUDA, cudaFree(res_dev);); (44, CUDA, cudaMalloc(&res_dev, sizeof(float));); (46, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(165, OpenMP, #pragma omp simd); (195, OpenMP, #pragma omp atomic capture); (190, OpenMP, #pragma omp master); (77, OpenMP, #pragma omp parallel default (shared)); (85, OpenMP, #pragma omp taskloop private (i,ii,jj,lk,lsub,nsupr,lusup,t1,t2,Uinv,nn,lbstart,lbend,thread_id1) untied); (223, OpenMP, #pragma omp atomic); (195, OpenMP, thread_id1 = omp_get_thread_num();); (76, OpenMP, #pragma omp single); (144, OpenMP, #pragma omp simd reduction(+:temp)); (185, OpenMP, #pragma omp parallel default (shared) private(i)); (210, OpenMP, #pragma omp atomic update); (140, OpenMP, #pragma omp parallel default(shared)); (145, OpenMP, #pragma omp atomic read); (212, OpenMP, #pragma omp atomic read capture); (215, OpenMP, #pragma omp atomic hint(6)); (183, OpenMP, #pragma omp critical)", "label": "(82, OpenMP, #pragma\tomp\ttaskloop firstprivate (stat) private (thread_id1,Uinv,nn,lbstart,lbend,ub,temp,rtemp_loc,ik,lk1,gik,gikcol,usub,uval,lsub,lusup,iknsupc,il,i,irow,bmod_tmp,p,ii,jj,t1,t2,j,ikfrow,iklrow,dest,y,uptr,fnz,nsupr,nroot_send_tmp) untied nogroup); (126, OpenMP, #pragma omp simd); (146, OpenMP, #pragma omp atomic capture); (75, OpenMP, #pragma\tomp\tparallel for private (thread_id1,Uinv,nn,lbstart,lbend,ub,temp,rtemp_loc,ik,lk1,gik,gikcol,usub,uval,lsub,lusup,iknsupc,il,i,irow,bmod_tmp,p,ii,jj,t1,t2,j,ikfrow,iklrow,dest,y,uptr,fnz,nsupr))"}
{"predict": "(48, CUDA, CubDebugExit(cudaMemcpy(d_flags, h_flags, sizeof(unsigned char) * num_items, cudaMemcpyHostToDevice));); (46, CUDA, CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));)", "label": "(48, CUDA, CubDebugExit(cudaMemcpy(d_flags, h_flags, sizeof(unsigned char) * num_items, cudaMemcpyHostToDevice));); (46, CUDA, CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));)"}
{"predict": "(6, CUDA, __shared__ uint s_key[SHARED_SIZE_LIMIT];); (6, CUDA, __shared__ uint s_val[SHARED_SIZE_LIMIT];)", "label": "(6, CUDA, __shared__ uint s_key[SHARED_SIZE_LIMIT];); (6, CUDA, __shared__ uint s_val[SHARED_SIZE_LIMIT];)"}
{"predict": "_add_script_line(p, \"%(244, OpenMP, comp_add_script_line(p, \"set $%s=%s\\n\", p->pVar, p->pValue);); (334, OpenMP, comp_add_script_line(p, \"check_success=%s\\n\", p->pCheckSuccess);); (336, OpenMP, comp_add_script_line(p, \"(342, OpenMP, comp_add_script_line(p, \"error=%s\\n\", p->pError);); (335, OpenMP, comp_add_script_line(p, \"status=%s\\n\", p->pStatus);); (240, OpenMP, comp_add_script_line(p, \"set $%s=%s\\n\", p->pKey1, p->pValue1);); (241, OpenMP, comp_add_script_line(p, \"set $%s=%s\\n\", p->pKey2, p->pValue2);)", "label": "(31, OpenMP, comp_add_script_line(\"Flag=MGF_SALT_AS_HEX_%s\\n\", tmp);); (19, OpenMP, comp_do_parse(0, nSyms-1);); (222, OpenMP, comp_add_script_line(\"Func=DynamicFunc__append_keys_pad20\\n\"); if (use_inp1) len_comp += 20; else len_comp2 += 20; }); (58, OpenMP, if (bNeedS) comp_add_script_line(\"Flag=MGF_SALTED\\n\");); (153, OpenMP, comp_add_script_line(\"Func=DynamicFunc__LargeHash_OUTMode_raw\\n\");); (18, OpenMP, comp_add_script_line(\"Expression=dynamic=%s\\n\", p->pExpr);); (306, OpenMP, comp_add_script_line(\"Flag=MGF_INPBASE64b\\n\");); (307, OpenMP,"}
{"predict": "(45, MPI, MPI_Type_free(&dt_vector);); (15, MPI, MPI_Type_vector(COUNT, 1, noncontig_stride, MPI_INT, &dt_vector);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (30, MPI, MPI_Type_free(&dt_noncontig);); (33, MPI, MPI_Op_free(&op);); (11, MPI, MPI_Type_commit(&dt_vector);); (8, MPI, MPI_Type_create_hvector(COUNT, 1, sizeof(int) * noncontig_stride, dt_vector, &dt_noncontig);)", "label": "(38, MPI, MPI_Op_free(&op);); (16, MPI, MPI_Type_free(&dt_vector);); (35, MPI, MPI_Type_free(&dt_noncontig);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (13, MPI, MPI_Type_commit(&dt_noncontig);); (9, MPI, MPI_Type_vector(1, 1, noncontig_stride, MPI_INT, &dt_vector);); (9, MPI, MPI_Type_create_resized(dt_vector, 0, sizeof(int) * noncontig_stride, &dt_noncontig);)"}
{"predict": "(7, CUDA, __shared__ int s_data[32][32 + 1];)", "label": "(6, CUDA, __shared__ int s_data[32][33];)"}
{"predict": "(13, CUDA, CUDA_CHECK(cudaGetDeviceProperties(&props, 0));)", "label": "(13, CUDA, CUDA_CHECK(cudaGetDeviceProperties(&props, 0));)"}
{"predict": "(6, CUDA, st = ucc_mc_alloc_device_mem(4096 * 9, 0, NULL);)", "label": "(6, CUDA, st = cudaMalloc(&test_ptr, TEST_ALLOC_SIZE);); (11, CUDA, cudaFree(test_ptr);)"}
{"predict": "(32, TBB, tbb_time = (tbb::tick_count::now() - t0).seconds();); (25, TBB, serial_tiled_time = (tbb::tick_count::now() - t0).seconds();); (18, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (17, TBB, serial_time = (tbb::tick_count::now() - t0).seconds();)", "label": "(19, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (25, TBB, serial_tiled_time = (tbb::tick_count::now() - t0).seconds();); (30, TBB, tbb_time = (tbb::tick_count::now() - t0).seconds();); (18, TBB, serial_time = (tbb::tick_count::now() - t0).seconds();)"}
{"predict": "(38, CUDA, flamegpu::CUDASimulation cudaSimulation(model);)", "label": "(52, CUDA, flamegpu::CUDASimulation cudaSimulation(model);)"}
{"predict": "(7, OpenMP, #pragma omp sections private(t_var, vec, s_arr, var)); (8, OpenMP, #pragma omp section)", "label": "(8, OpenMP, #pragma omp section); (6, OpenMP, #pragma omp parallel)"}
{"predict": "(25, MPI, MPI_Type_get_true_extent(key, &key_true_lb, &key_true_extent);); (85, MPI, MPI_Allreduce(counts, all_counts, 2, MPI_UINT64_T, MPI_SUM, comm);); (18, MPI, MPI_Type_get_extent(keysat, &keysat_lb, &keysat_extent);); (13, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(25, MPI, MPI_Type_get_true_extent(key, &key_true_lb, &key_true_extent);); (15, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (83, MPI, MPI_Allreduce(counts, all_counts, 2, MPI_UINT64_T, MPI_SUM, comm);); (18, MPI, MPI_Type_get_extent(keysat, &keysat_lb, &keysat_extent);)"}
{"predict": "(39, CUDA, checkCudaErrors(cudaFree(bLines_d));); (24, CUDA, checkCudaErrors(cudaMalloc((void **)&bLines_d, N_LINES * sizeof(BezierLine)));); (35, CUDA, checkCudaErrors(cudaFreeVertexMem());); (27, CUDA, checkCudaErrors(cudaGetLastError());)", "label": "(24, CUDA, checkCudaErrors(cudaMalloc((void **)&bLines_d, N_LINES * sizeof(BezierLine)));); (35, CUDA, checkCudaErrors(cudaFree(bLines_d));)"}
{"predict": "(7, CUDA, __shared__ float shared_sum2[32];); (5, CUDA, __shared__ float shared_sum[32];)", "label": "(7, CUDA, __shared__ float shared_sum2[32];); (5, CUDA, __shared__ float shared_sum[32];)"}
{"predict": "(24, OpenMP, #pragma omp target exit data map(from : c[:size]))", "label": "(2, OpenMP, #pragma omp parallel for schedule(static))"}
{"predict": "(234, OpenMP, #pragma omp parallel for simd lastprivate(ii)); (220, OpenMP, #pragma omp parallel for simd firstprivate(ii)); (241, OpenMP, #pragma omp parallel for simd collapse(2)); (5, OpenMP, #pragma omp parallel for simd); (232, OpenMP, #pragma omp parallel for simd private(ii)); (223, OpenMP, #pragma omp parallel for simd linear(ii))", "label": "(235, OpenMP, #pragma omp parallel for simd private(ii)); (5, OpenMP, #pragma omp parallel for simd); (257, OpenMP, #pragma omp parallel for simd collapse(2)); (226, OpenMP, #pragma omp parallel for simd linear(ii)); (237, OpenMP, #pragma omp parallel for simd lastprivate(ii)); (220, OpenMP, #pragma omp parallel for simd firstprivate(ii))"}
{"predict": "(12, TBB, tbb::parallel_sort(iter, iter + n, *compare );)", "label": "(12, TBB, tbb::parallel_sort(iter, iter + n, *compare );)"}
{"predict": "(10, TBB, double serial_time = (tbb::tick_count::now() - t0).seconds();); (7, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (16, TBB, double tbb_time = (tbb::tick_count::now() - t1).seconds();); (13, TBB, tbb::tick_count t1 = tbb::tick_count::now();)", "label": "(10, TBB, double serial_time = (tbb::tick_count::now() - t0).seconds();); (7, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (16, TBB, double tbb_time = (tbb::tick_count::now() - t1).seconds();); (13, TBB, tbb::tick_count t1 = tbb::tick_count::now();)"}
{"predict": "(24, CUDA, CHECK(cudaMalloc(&d_B, M));); (24, CUDA, CHECK(cudaMemcpy(d_A, h_A, M, cudaMemcpyHostToDevice));); (42, CUDA, CHECK(cudaFree(d_B));); (19, CUDA, CHECK(cudaMalloc(&d_A, M));); (28, CUDA, CHECK(cudaMemcpy(h_B, d_B, M, cudaMemcpyDeviceToHost));); (38, CUDA, CHECK(cudaFree(d_A));)", "label": "(24, CUDA, CHECK(cudaMalloc(&d_B, M));); (24, CUDA, CHECK(cudaMemcpy(d_A, h_A, M, cudaMemcpyHostToDevice));); (42, CUDA, CHECK(cudaFree(d_B));); (20, CUDA, CHECK(cudaMalloc(&d_A, M));); (28, CUDA, CHECK(cudaMemcpy(h_B, d_B, M, cudaMemcpyDeviceToHost));); (38, CUDA, CHECK(cudaFree(d_A));)"}
{"predict": "(5, CUDA, HANDLE_ERROR(cudaStreamDestroy(stream));); (3, CUDA, HANDLE_ERROR(cudaStreamCreate(&stream));)", "label": "(4, CUDA, HANDLE_ERROR(cudaStreamCreate(&stream));); (15, CUDA, HANDLE_ERROR(cudaStreamDestroy(stream));)"}
{"predict": "(5, CUDA, __shared__ uint data[MERGE_THREADBLOCK_SIZE];)", "label": "(12, CUDA, __shared__ uint data[MERGE_THREADBLOCK_SIZE];)"}
{"predict": "(2, OpenMP, #pragma omp parallel num_threads(2)); (5, OpenMP, omp_set_lock(0);); (6, OpenMP, omp_unset_lock(0);); (8, OpenMP, omp_destroy_lock(0);)", "label": "(2, OpenMP, #pragma omp critical)"}
{"predict": "(38, OpenCL, retVal = clReleaseMemObject(clBuffer);)", "label": "(38, OpenCL, retVal = clReleaseMemObject(clBuffer);)"}
{"predict": "(26, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&data, size_bytes));); (49, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (47, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (17, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamCreate(&stream));); (54, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(data));); (42, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceCanAccessPeer(&canAccessPeer, 0, GPUS[Arch], GPUS[Arch]));); (24, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpyAsync(data, data, size_bytes, cudaMemcpyHostToDevice, stream));)", "label": "(26, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&data, size_bytes));); (19, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamCreate(&stream));); (48, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (46, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (53, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(data));); (53, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamDestroy(stream));)"}
{"predict": "(30, OpenMP, x = omp_target_pointer(px, 0);); (28, OpenMP, px = omp_get_address(x);); (13, OpenMP, #pragma omp target map(tofrom : px)); (6, OpenMP, #pragma omp target enter data map(alloc : x [0:count])); (14, OpenMP, #pragma omp target exit data map(delete : x [0:count]))", "label": "(15, OpenMP, #pragma omp target map(from:px)); (7, OpenMP, #pragma omp target data map(to:x[0:count])); (21, OpenMP, #pragma omp target data use_device_ptr(x)); (26, OpenMP, #pragma omp target data use_device_addr(x[0:0]))"}
{"predict": "(65, CUDA, CHECKED_CALL(cudaEventRecord(stop, 0));); (48, CUDA, CHECKED_CALL(cudaDeviceSetLimit(cudaLimitStackSize, 128));); (63, CUDA, CHECKED_CALL(cudaEventSynchronize(stop));); (87, CUDA, CHECKED_CALL(cudaEventElapsedTime(&time, start, stop));); (32, CUDA, CHECKED_CALL(cudaMalloc((void **)&d_nresults, THREADS * sizeof(*d_nresults)));); (55, CUDA, CHECKED_CALL(cudaEventRecord(start, 0));); (5, CUDA, int dev = findCudaDevice(argc, (const char **)argv);); (84, CUDA, CHECKED_CALL(cudaEventDestroy(start));); (84, CUDA, CHECKED_CALL(cudaEventDestroy(stop));); (47, CUDA, CHECKED_CALL(cudaGetLastError());); (2, CUDA, CHECKED_CALL(cudaGetDeviceProperties(&deviceProp, dev));); (44, CUDA, CHECKED_CALL(cudaDeviceSynchronize());); (23, CUDA, CHECKED_CALL(cudaEventCreate(&stop));); (21, CUDA, CHECKED_CALL(cudaEventCreate(&start));)", "label": "(59, CUDA, CHECKED_CALL(cudaEventRecord(start, 0));); (52, CUDA, CHECKED_CALL(cudaDeviceSetLimit(cudaLimitStackSize, 8192));); (62, CUDA, CHECKED_CALL(cudaGetLastError());); (41, CUDA, CHECKED_CALL(cudaEventCreate(&start));); (36, CUDA, CHECKED_CALL(cudaSetDevice(devID));); (88, CUDA, CHECKED_CALL(cudaEventDestroy(start));); (89, CUDA, CHECKED_CALL(cudaFree(d_result));); (61, CUDA, CHECKED_CALL(cudaDeviceSynchronize());); (6, CUDA, cudaGetDeviceProperties(&deviceProp, devID);); (3, CUDA, int devID = findCudaDevice(argc, (const char **)argv);); (78, CUDA, CHECKED_CALL(cudaEventElapsedTime(&time, start, stop));); (56, CUDA, CHECKED_CALL(cudaEventRecord(stop, 0));); (84, CUDA, CHECKED_CALL(cudaFree(d_nresults));); (30, CUDA, CHECKED_CALL(cudaMalloc((void **)&d_nresults, THREADS * sizeof(*d_nresults)));); (80, CUDA, CHECKED_CALL(cudaEventDestroy(stop));); (30, CUDA, CHECKED_CALL(cudaEventCreate(&stop));)"}
{"predict": "(6, OpenMP, #pragma omp target teams num_teams(numBlocks) thread_limit(128)); (7, OpenMP, #pragma omp distribute parallel for collapse(2))", "label": "(4, OpenACC, #pragma acc enter data create(in1[:w*h], in2[:h*w], out[:w*h])); (12, OpenACC, #pragma acc loop); (8, OpenACC, #pragma acc update device(in1[lower*w:(upper-lower)*w],in2[lower*w:(upper-lower)*w]) async(block%2)); (17, OpenACC, #pragma acc wait); (7, OpenACC, #pragma acc parallel loop present(in1,in2, out) async(block%2)); (13, OpenACC, #pragma acc update self(out[lower*w:(upper-lower)*w]) async(block%2)); (15, OpenACC, #pragma acc exit data delete(in1, in2, out))"}
{"predict": "(55, CUDA, checkCudaErrors(cudaMemcpy(&siftData.numPts, memoryTmp+iAlignUp(4, CUDA, checkCudaErrors(cudaMalloc(&d_PointCounterAddr, sizeof(unsigned int)));); (44, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (4, CUDA, checkCudaErrors(cudaMemcpy(&siftData.numPts, memoryTmp+iAlignUp(4, CUDA), sizeof(unsigned int), cudaMemcpyDeviceToHost));); (4, CUDA, checkCudaErrors(cudaMemcpy(&fstPts, memoryTmp+iAlignUp(3, CUDA), sizeof(unsigned int), cudaMemcpyDeviceToHost));); (52, CUDA, checkCudaErrors(cudaDestroyTextureObject(texObj));); (30, CUDA, checkCudaErrors(cudaCreateTextureObject(&texObj, &resDesc, &texDesc, NULL));); (52, CUDA, checkCudaErrors(cudaFree(d_PointCounterAddr));)", "label": "(5, CUDA, safeCall(cudaGetSymbolAddress((void**)&d_PointCounterAddr, d_PointCounter));); (57, CUDA, safeCall(cudaMemcpy(&totPts, &d_PointCounterAddr[2*octave+1], sizeof(int), cudaMemcpyDeviceToHost));); (5, CUDA, safeCall(cudaMemcpy(&fstPts, &d_PointCounterAddr[2*octave-1], sizeof(int), cudaMemcpyDeviceToHost));); (50, CUDA, safeCall(cudaDestroyTextureObject(texObj));); (32, CUDA, cudaCreateTextureObject(&texObj, &resDesc, &texDesc, NULL);)"}
{"predict": "(52, TBB, tbb::flow::make_edge( q2, q3 );); (50, TBB, tbb::flow::make_edge( q, q2 );); (3, TBB, tbb::flow::queue_node<T> q2(g);); (3, TBB, tbb::flow::queue_node<T> q3(g);); (48, TBB, tbb::flow::make_edge( q, q2 );); (-3, TBB, tbb::flow::graph g;); (68, TBB, tbb::flow::queue_node<T> q_copy(q);); (-5, TBB, tbb::flow::queue_node<T> q(g);)", "label": "(4, TBB, tbb::flow::queue_node<T> q2(g);); (51, TBB, tbb::flow::make_edge( q2, q3 );); (3, TBB, tbb::flow::queue_node<T> q3(g);); (48, TBB, tbb::flow::make_edge( q, q2 );); (-2, TBB, tbb::flow::graph g;); (68, TBB, tbb::flow::queue_node<T> q_copy(q);); (-3, TBB, tbb::flow::queue_node<T> q(g);)"}
{"predict": "(13, MPI, MPI_Win_create(buf, sizeof(int), sizeof(int), MPI_INFO_NULL, comm, &win);); (24, MPI, MPI_Win_set_attr(win, key[2], &attrval[2]);); (24, MPI, MPI_Win_set_attr(win, key[0], &attrval[0]);); (60, MPI, MPI_Win_free_keyval(&key[i]);); (23, MPI, MPI_Win_set_attr(win, key[1], &attrval[1]);); (27, MPI, MPI_Win_delete_attr(win, key[0]);); (27, MPI, MPI_Win_delete_attr(win, key[1]);); (27, MPI, MPI_Win_delete_attr(win, key[2]);)", "label": "(27, MPI, MPI_Win_set_attr(win, key[0], &attrval[0]);); (12, MPI, MPI_Win_create(buf, sizeof(int), sizeof(int), MPI_INFO_NULL, comm, &win);); (29, MPI, MPI_Win_delete_attr(win, key[0]);); (30, MPI, MPI_Win_delete_attr(win, key[2]);); (58, MPI, MPI_Win_free_keyval(&key[i]);); (20, MPI, MPI_Win_set_attr(win, key[2], &attrval[2]);); (58, MPI, MPI_Win_free(&win);); (19, MPI, MPI_Win_set_attr(win, key[1], &attrval[1]);); (24, MPI, MPI_Win_delete_attr(win, key[1]);)"}
{"predict": "(11, OpenMP, #pragma omp parallel for); (38, OpenMP, #pragma omp atomic)", "label": "(11, OpenMP, #pragma omp parallel for); (38, OpenMP, #pragma omp atomic)"}
{"predict": "(19, MPI, MPI_Comm_rank(comm, &rank);)", "label": "(19, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(79, MPI, MPI_Info_set(spawn_info, \"initial_errhandler\", \"mpi_errors_are_fatal\");); (28, MPI, MPI_Init(&argc, &argv);); (78, MPI, MPI_Comm_spawn(argv[0], &spawn_argv[1], 1, spawn_info, 0, MPI_COMM_SELF, &icomm, spawn_err);); (84, MPI, MPI_Error_string(rc, estr, &slen);); (89, MPI, rc = MPI_Finalize();); (72, MPI, MPI_Info_create(&spawn_info);); (23, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (88, MPI, MPI_Info_free(&spawn_info);); (76, MPI, rc = MPI_Comm_set_errhandler(MPI_COMM_SELF, MPI_ERRORS_ARE_FATAL);); (90, MPI, rc = MPI_Error_string(MPI_SUCCESS, estr, &slen);); (39, MPI, MPI_Info_get(MPI_INFO_ENV, \"mpi_initial_errhandler\", MPI_MAX_INFO_VAL, init_errh_info, &flag);); (70, MPI, MPI_Info_set(spawn_info, \"mpi_initial_errhandler\", \"mpi_errors_return\");); (62, MPI, MPI_Comm_get_errhandler(MPI_COMM_SELF, &errh);); (92, MPI, MPI_Error_string(rc, estr, &slen);); (80, MPI, rc = MPI_Error_string(MPI_SUCCESS, estr, &slen);); (82, MPI, rc = MPI_Error_string(MPI_FAILURE, estr, &slen);)", "label": "(28, MPI, MPI_Init(&argc, &argv);); (82, MPI, MPI_Comm_disconnect(&icomm);); (76, MPI, MPI_Comm_spawn(argv[0], &spawn_argv[1], 1, spawn_info, 0, MPI_COMM_WORLD, &icomm, spawn_err);); (26, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (83, MPI, rc = MPI_Comm_set_errhandler(MPI_COMM_SELF, MPI_ERRORS_ARE_FATAL);); (43, MPI, MPI_Info_get(MPI_INFO_ENV, \"mpi_initial_errhandler\", MPI_MAX_INFO_VAL, init_errh_info, &flag);); (68, MPI, MPI_Info_set(spawn_info, \"mpi_initial_errhandler\", \"mpi_errors_return\");); (90, MPI, rc = MPI_Finalize();); (74, MPI, MPI_Barrier(icomm);); (80, MPI, MPI_Error_string(rc, estr, &slen);); (49, MPI, MPI_Comm_get_errhandler(MPI_COMM_SELF, &errh);); (62, MPI, MPI_Info_create(&spawn_info);); (93, MPI, rc = MPI_Error_string(MPI_ERR_WIN, estr, &slen);)"}
{"predict": "(3, OpenMP, #pragma omp target map(tofrom: threadCount)); (4, OpenMP, #pragma omp metadirective)", "label": "(3, OpenMP, #pragma omp target map(tofrom: threadCount)); (4, OpenMP, #pragma omp metadirective)"}
{"predict": "(4, OpenMP, xomp_check_device(devID);); (4, OpenMP, xomp_get_deviceInfo(&major, &minor, devID);)", "label": "(14, OpenMP, printf(\"Error: xomp_get_maxThreadBlocksPerMultiprocessor(): unhandled Compute Capability numbers%d.%d \\n\", major, minor);); (3, CUDA, major = xomp_getCudaDeviceProp(devID)-> major;); (3, CUDA, minor = xomp_getCudaDeviceProp(devID)-> minor;)"}
{"predict": "(27, MPI, MPI_Comm_dup(MPI_COMM_SELF, &self_dup);); (16, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &comm);); (33, MPI, MPI_Comm_free(&self_dup);); (19, MPI, MPI_Barrier(comm);); (31, MPI, MPI_Comm_free(&comm);); (1, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (22, MPI, MPI_Comm_dup(comm, &self_dup);)", "label": "(26, MPI, MPI_Comm_dup(comms[*(int *) arg], &comm);); (4, MPI, MPI_Comm_rank(comms[*(int *) arg], &rank);); (20, MPI, MPI_Comm_free(&self_dup);); (24, MPI, MPI_Comm_free(&comm);); (13, MPI, MPI_Comm_dup(MPI_COMM_SELF, &self_dup);)"}
{"predict": "(75, CUDA, HANDLE_ERROR(cudaFree(costs_dev_only_d));); (75, CUDA, HANDLE_ERROR(cudaFree(costs_host_only_d));); (75, CUDA, HANDLE_ERROR(cudaFree(baseline_and_normalizer_d));); (9, CUDA, HANDLE_ERROR(cudaStreamCreate(&stream));); (27, CUDA, HANDLE_ERROR(cudaMalloc((void**)&costs_dev_only_d, sizeof(float) * num_rollouts));); (27, CUDA, HANDLE_ERROR(cudaMalloc((void**)&costs_host_only_d, sizeof(float) * num_rollouts));); (57, CUDA, HANDLE_ERROR(cudaStreamSynchronize(stream));); (26, CUDA, HANDLE_ERROR(cudaMalloc((void**)&baseline_and_normalizer_d, sizeof(float2)));)", "label": "(12, CUDA, cudaStreamCreate(&stream);); (37, CUDA, HANDLE_ERROR(cudaStreamSynchronize(stream));); (17, CUDA, HANDLE_ERROR(cudaMalloc((void**)&costs_dev_only_d, sizeof(float) * num_rollouts));); (15, CUDA, HANDLE_ERROR(cudaMalloc((void**)&baseline_and_normalizer_d, sizeof(float2)));); (16, CUDA, HANDLE_ERROR(cudaMalloc((void**)&costs_host_only_d, sizeof(float) * num_rollouts));)"}
{"predict": "(2, CUDA, __shared__ typename BlockReduce<float>::TempStorage m_storage;); (2, CUDA, __shared__ typename BlockReduce<float>::TempStorage v_storage;)", "label": "(2, CUDA, __shared__ typename BlockReduce<float>::TempStorage v_storage;); (0, CUDA, __shared__ typename BlockReduce<float>::TempStorage m_storage;)"}
{"predict": "(36, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (41, TBB, reduce_time = (tbb::tick_count::now() - t0).seconds();); (48, TBB, graph_time = (tbb::tick_count::now() - t0).seconds();); (32, TBB, serial_time = (tbb::tick_count::now() - t0).seconds();)", "label": "(33, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (41, TBB, reduce_time = (tbb::tick_count::now() - t0).seconds();); (47, TBB, graph_time = (tbb::tick_count::now() - t0).seconds();); (32, TBB, serial_time = (tbb::tick_count::now() - t0).seconds();)"}
{"predict": "(6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &world_size);)", "label": "(6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &world_size);)"}
{"predict": "(8, OpenMP, rec = get_decomp_record(code);)", "label": "(8, OpenMP, rec = get_decomp_record(code);)"}
{"predict": "(10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (4, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)", "label": "(10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(6, CUDA, __shared__ float sdata[THREAD_PER_BLOCK], sdata2[THREAD_PER_BLOCK];); (6, CUDA, __shared__ float mean, var;)", "label": "(6, CUDA, __shared__ float sdata[THREAD_PER_BLOCK];); (8, CUDA, __shared__ float var;); (5, CUDA, __shared__ float sdata2[THREAD_PER_BLOCK];); (5, CUDA, __shared__ float mean;)"}
{"predict": "(38, TBB, TestTypeDefinitionPresence2(flow::multifunction_node<intpair, intpair> );); (38, TBB, TestTypeDefinitionPresence2(flow::join_node<intpair, tbb::flow::key_matching<int> > );); (38, TBB, TestTypeDefinitionPresence2(flow::join_node<intpair, tbb::flow::simple_matching<int> > );); (38, TBB, TestTypeDefinitionPresence2(flow::join_node<intpair, tbb::flow::auto_matching<int> > );); (39, TBB, TestTypeDefinitionPresence3(flow::broadcast_node<intpair, tbb::flow::reserving> );); (40, TBB, TestTypeDefinitionPresence3(flow::multifunction_node<int, int, tbb::flow::rejecting> );); (37, TBB, TestTypeDefinitionPresence2(flow::continue_node<int, tbb::flow::rejecting> );); (36, TBB, TestTypeDefinitionPresence2(flow::continue_node<int, tbb::flow::reserving> );); (37, TBB, TestTypeDefinitionPresence3(flow::join_node<intpair, tbb::flow::key_matching<int>, tbb::flow::reserving> );); (37, TBB, TestTypeDefinitionPresence3(flow::join_node<intpair, tbb::flow::simple_matching<int>, tbb::flow::reserving> );); (36, TBB, TestTypeDefinitionPresence3(flow::join_node<intpair, tbb::flow::auto_matching<int>, tbb::flow::reserving> );); (40, TBB, TestTypeDefinitionPresence3(flow::broadcast_node<int, tbb::flow::reserving> );); (38, TBB, TestTypeDefinitionPresence3(flow::broadcast_node<intpair, tbb::flow::rejecting> );); (34, TBB, TestTypeDefinitionPresence2(flow::split_node<intpair, tbb::flow::reserving> );); (34, TBB, TestTypeDefinitionPresence2(flow::split_node<intpair", "label": "(103, TBB, TestFuncDefinitionPresence( parallel_deterministic_reduce, (const tbb::blocked_range<int>&, Body2&, const tbb::simple_partitioner&, tbb::task_group_context&), void );); (32, TBB, TestTypeDefinitionPresence3(flow::async_node<int, int, tbb::flow::queueing_lightweight> );); (96, TBB, TestFuncDefinitionPresence( parallel_for_each, (const intarray&, const Body1&, tbb::task_group_context&), void );); (93, TBB, TestFuncDefinitionPresence( parallel_invoke, (const Body&, const Body&, tbb::task_group_context&), void );); (93, TBB, TestFuncDefinitionPresence( parallel_do, (const intarray&, const Body1a&, tbb::task_group_context&), void );); (27, TBB, TestTypeDefinitionPresence3(flow::multifunction_node<int, intpair, tbb::flow::queueing> );); (30, TBB, TestTypeDefinitionPresence2(flow::join_node<intpair, tbb::flow::key_matching<int> > );); (82, TBB, TestFuncDefinitionPresence( parallel_scan, (const tbb::blocked_range<int>&, const int&, const Body3a&, const Body1b&), int );); (75, TBB, TestFuncDefinitionPresence( parallel_for, (const tbb::blocked_range<int>&, const Body2&, const tbb::simple_partitioner&), void );); (79, TBB, TestFuncDefinitionPresence( parallel_scan, (const tbb::blocked_range2d<int>&, Body3&, const tbb::auto_partitioner&), void );); (74, TBB, TestFuncDefinitionPresence( parallel_reduce, (const tbb::blocked_range<int>&, const int&, const Body2a&, const Body1b&), int );); (23, TBB, TestTypeDefinitionPresence2(flow::continue_node<int, tbb::flow::lightweight> );); (73, TBB, TestFuncDefinitionPresence( parallel_reduce, (const tbb::blocked_range<int>&, Body2&, tbb::affinity_partitioner&), void );); (74, TBB, TestFuncDefinitionPresence( parallel_deterministic_reduce, (const tbb::blocked_range<int>&, Body2&, const tbb::static_partitioner&), void );); (14, TBB, typedef tbb::flow::tuple<int, int> intpair;); (33, TBB, TestTypeDefinitionPresence2(flow::composite_node<tbb::flow::tuple<int>, tbb::flow::tuple<int> > );); (15, TBB, TestTypeDefinitionPresence3(flow::function_node<int, int, tbb::flow::rejecting> );); (82, TBB, TestFuncDefinitionPresence("}
{"predict": "(235, OpenMP, return __kmp_omp_parse_dynamic_schedule(ptr, &chunk);); (203, OpenMP, return __kmp_omp_parse_static_schedule(ptr);); (216, OpenMP, return __kmp_omp_parse_guided_schedule(ptr);); (182, OpenMP, return __kmp_omp_parse_dynamic_schedule(NULL, &chunk);); (187, OpenMP, return __kmp_omp_parse_guided_schedule(NULL);); (230, OpenMP, return __kmp_omp_parse_static_schedule(ptr, &chunk);); (182, OpenMP, return __kmp_omp_parse_static_schedule(NULL);); (208, OpenMP, return __kmp_omp_parse_dynamic_schedule(NULL);); (194, OpenMP, return __kmp_omp_parse_nonmonotonic_dynamic_schedule(ptr);); (194, OpenMP, return __kmp_omp_parse_nonmonotonic_dynamic_schedule(NULL);); (188, OpenMP, return __kmp_omp_parse_nonmonotonic_guided_schedule(NULL);); (194, OpenMP, return __kmp_omp_parse_nonmonotonic_guided_schedule(ptr);); (164, OpenMP, return __kmp_omp_parse_auto_dynamic_schedule(NULL);); (164, OpenMP, return __kmp_omp_parse_auto_dynamic_schedule(ptr);); (167, OpenMP, return __kmp_omp_parse_auto_guided_schedule(NULL);); (173, OpenMP, return __kmp_omp_parse_guided_schedule(ptr, &chunk);); (160, OpenMP, return __kmp_omp_parse_auto_dynamic_schedule(NULL, &chunk);); (177, OpenMP, return __kmp_omp_parse_static_schedule(ptr, &chunk);); (166, OpenMP, return __kmp_omp_parse_guided_schedule(NULL, &chunk);); (160", "label": "(30, OpenMP, __kmp_omp_schedule_restore();)"}
{"predict": "(12, MPI, MPI_Comm_rank(team.comm, &rank);); (12, MPI, MPI_Comm_size(team.comm, &nprocs);)", "label": "(12, MPI, MPI_Comm_rank(team.comm, &rank);); (12, MPI, MPI_Comm_size(team.comm, &nprocs);)"}
{"predict": "(9, OpenCL, err |= clBuildProgram(program, 0, NULL, NULL, NULL, NULL);)", "label": "(9, OpenCL, kernel_info->kernel = clCreateKernel(program, kernel_info->kernel_name, &err);)"}
{"predict": "(17, OpenCL, error = clEnqueueNDRangeKernel(cmdq, kernel_verify_lists, 1, NULL, &numLists, NULL, 0, NULL, NULL);); (10, OpenCL, error = clEnqueueUnmapMemObject(cmdq, num_correct, pNumCorrect, 0,NULL,NULL);); (4, OpenCL, cl_int* pNumCorrect = (cl_int*) clEnqueueMapBuffer(cmdq, num_correct, CL_TRUE, CL_MAP_WRITE, 0, sizeof(cl_int), 0, NULL, NULL, &error);)", "label": "(6, OpenCL, cl_int *pNumCorrect = (cl_int*) clEnqueueMapBuffer(cmdq, num_correct, CL_TRUE, CL_MAP_READ | CL_MAP_WRITE, 0, sizeof(cl_int), 0, NULL,NULL, &error);); (10, OpenCL, error = clEnqueueUnmapMemObject(cmdq, num_correct, pNumCorrect, 0,NULL,NULL);); (15, OpenCL, pNumCorrect = (cl_int*) clEnqueueMapBuffer(cmdq, num_correct, CL_TRUE, CL_MAP_READ | CL_MAP_WRITE, 0, sizeof(cl_int), 0, NULL,NULL, &error);); (11, OpenCL, error = clEnqueueNDRangeKernel(cmdq, kernel_verify_lists, 1, NULL, &numLists, NULL, 0, NULL, NULL);)"}
{"predict": "(8, TBB, tbb::task_scheduler_init init(MaxThread);)", "label": "(8, TBB, tbb::task_scheduler_init init_market_p_plus_one(MaxThread + 1);)"}
{"predict": "(7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &comm_size);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &comm_rank);); (58, MPI, MPI_Comm_split(MPI_COMM_WORLD, 0, rand(), &comm);); (66, MPI, MPI_Comm_free(&comm);); (56, MPI, MPI_Comm_rank(comm, &comm_rank);); (52, MPI, MPI_Comm_split(MPI_COMM_WORLD, (comm_rank == comm_size - 1)? 0 : 1, 0, &comm);); (3, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &comm_rank);)", "label": "(7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &comm_size);); (69, MPI, MPI_Comm_split(MPI_COMM_WORLD, 0, rand(), &comm);); (57, MPI, MPI_Comm_split(MPI_COMM_WORLD, (comm_rank == comm_size - 1)? 0 : 1, 0, &comm);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &comm_rank);); (58, MPI, MPI_Comm_free(&comm);); (7, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (30, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(1, TBB, oneapi::tbb::flow::graph g;)", "label": "(1, TBB, oneapi::tbb::flow::graph g;)"}
{"predict": "(20, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (20, CUDA, checkCudaErrors(cudaMemcpy(array, dArray, size, cudaMemcpyDeviceToHost));); (17, CUDA, checkCudaErrors(cudaFree(dArray));); (9, CUDA, checkCudaErrors(cudaMalloc(&dArray, size));); (10, CUDA, checkCudaErrors(cudaMemcpy(dArray, array, size, cudaMemcpyHostToDevice));)", "label": "(12, CUDA, checkCudaErrors(cudaMemcpy(dArray, array, size, cudaMemcpyHostToDevice));); (10, CUDA, checkCudaErrors(cudaMalloc(&dArray, size));); (19, CUDA, checkCudaErrors(cudaFree(dArray));); (17, CUDA, checkCudaErrors(cudaMemcpy(array, dArray, size, cudaMemcpyDeviceToHost));)"}
{"predict": "(35, TBB, tbb::affinity_partitioner aff_p;); (7, TBB, c.set_core_type(tbb::info::core_types().back());); (6, TBB, tbb::task_arena::constraints c;); (50, TBB, double static_time = (tbb::tick_count::now() - t0).seconds();); (26, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (31, TBB, t0 = tbb::tick_count::now();); (3, TBB, c.set_numa_id(tbb::info::numa_nodes()[0]);); (28, TBB, double affinity_time = (tbb::tick_count::now() - t0).seconds();); (38, TBB, double auto_time = (tbb::tick_count::now() - t0).seconds();); (0, TBB, tbb::task_arena arena(c);)", "label": "(9, TBB, c.set_core_type(tbb::info::core_types().back());); (6, TBB, tbb::task_arena::constraints c;); (36, TBB, tbb::affinity_partitioner aff_p;); (5, TBB, c.set_numa_id(tbb::info::numa_nodes()[0]);); (26, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (46, TBB, double static_time = (tbb::tick_count::now() - t0).seconds();); (5, TBB, c.set_max_concurrency(std::min(8, tbb::info::default_concurrency(c)));); (36, TBB, double affinity_time = (tbb::tick_count::now() - t0).seconds();); (31, TBB, t0 = tbb::tick_count::now();); (25, TBB, double auto_time = (tbb::tick_count::now() - t0).seconds();); (2, TBB, tbb::task_arena arena(c);)"}
{"predict": "(185, CUDA, checkRuntime(cudaMallocHost(&output_data_host, sizeof(float) * output_numel));); (185, CUDA, checkRuntime(cudaMalloc(&output_data_device, sizeof(float) * output_numel));); (78, CUDA, checkRuntime(cudaStreamSynchronize(stream));); (17, CUDA, checkRuntime(cudaStreamCreate(&stream));); (57, CUDA, checkRuntime(cudaMemcpyAsync(input_data_device, input_data_host, input_numel * sizeof(float), cudaMemcpyHostToDevice, stream));); (181, CUDA, checkRuntime(cudaFree(input_data_device));); (182, CUDA, checkRuntime(cudaFreeHost(input_data_host));); (179, CUDA, checkRuntime(cudaFree(output_data_device));); (176, CUDA, checkRuntime(cudaFreeHost(output_data_host));); (170, CUDA, checkRuntime(cudaStreamDestroy(stream));); (17, CUDA, checkRuntime(cudaMallocHost(&input_data_host, input_numel * sizeof(float)));); (17, CUDA, checkRuntime(cudaMalloc(&input_data_device, input_numel * sizeof(float)));); (64, CUDA, checkRuntime(cudaMallocHost(&output_data_host, sizeof(float) * output_numel));); (-6, CUDA, auto engine = make_nvshared(runtime->deserializeCudaEngine(engine_data.data(), engine_data.size()));); (164, CUDA, checkRuntime(cudaFreeHost(output_data_host));)", "label": "(18, CUDA, checkRuntime(cudaStreamCreate(&stream));); (80, CUDA, checkRuntime(cudaMemcpyAsync(output_data_host, output_data_device, sizeof(float) * output_numel, cudaMemcpyDeviceToHost, stream));); (80, CUDA, checkRuntime(cudaStreamSynchronize(stream));); (58, CUDA, checkRuntime(cudaMemcpyAsync(input_data_device, input_data_host, input_numel * sizeof(float), cudaMemcpyHostToDevice, stream));); (24, CUDA, checkRuntime(cudaMallocHost(&input_data_host, input_numel * sizeof(float)));); (24, CUDA, checkRuntime(cudaMalloc(&input_data_device, input_numel * sizeof(float)));); (191, CUDA, checkRuntime(cudaFree(input_data_device));); (188, CUDA, checkRuntime(cudaFreeHost(input_data_host));); (190, CUDA, checkRuntime(cudaFree(output_data_device));); (63, CUDA, checkRuntime(cudaMalloc(&output_data_device, sizeof(float) * output_numel));); (-5, CUDA, auto engine = make_nvshared(runtime->deserializeCudaEngine(engine_data.data(), engine_data.size()));); (60, CUDA, checkRuntime(cudaMallocHost(&output_data_host, sizeof(float) * output_numel));); (184, CUDA, checkRuntime(cudaFreeHost(output_data_host));); (181, CUDA, checkRuntime(cudaStreamDestroy(stream));)"}
{"predict": "(10, MPI, MPI_Type_create_struct(1, b, d, MPI_INT, &newtype);); (13, MPI, MPI_Type_free_keyval(&key);); (14, MPI, MPI_Type_set_attr(MPI_INT, key, NULL);)", "label": "(17, MPI, MPI_Sendrecv(b, 1, newtype, 0, 0, d, 0, newtype, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (10, MPI, MPI_Type_commit(&newtype);); (8, MPI, MPI_Type_create_hvector(0, 1, 10, MPI_DOUBLE, &newtype);); (9, MPI, MPI_Type_free(&newtype);); (10, MPI, MPI_Type_indexed(0, b, d, MPI_DOUBLE, &newtype);)"}
{"predict": "(16, CUDA, thrust::fill(out_values.begin(), out_values.end(), static_cast<ValueT>(0.));); (12, CUDA, thrust::sequence(data_source.begin(), data_source.end(), re);); (16, CUDA, thrust::device_ptr<KeyT> dev_out_keys = thrust::device_malloc<KeyT>(elements);); (16, CUDA, thrust::device_ptr<ValueT> dev_out_values = thrust::device_malloc<ValueT>(elements);); (4, CUDA, thrust::host_vector<KeyT> host_keys(elements);); (4, CUDA, thrust::host_vector<ValueT> host_values(elements);); (16, CUDA, thrust::fill(dev_out_keys.get_pointer(), dev_out_keys.get_pointer() + elements, static_cast<KeyT>(0.));); (16, CUDA, thrust::fill(dev_out_values.get_pointer(), dev_out_values.get_pointer() + elements, static_cast<ValueT>(0.));); (11, CUDA, thrust::device_ptr<const KeyT> dev_in_keys = thrust::device_pointer_cast(in_keys.data());); (11, CUDA, thrust::device_ptr<const ValueT> dev_in_values = thrust::device_pointer_cast(in_values.data());); (12, CUDA, thrust::host_vector<KeyT> out_keys = dev_out_keys;); (12, CUDA, thrust::host_vector<ValueT> out_values = dev_out_values;); (13, CUDA, thrust::device_free(dev_out_keys.get_pointer());); (13, CUDA, thrust::device_free(dev_out_values.get_pointer());); (5, CUDA, thrust::host_vector<ValueT> in_values(elements);); (3, CUDA, thrust::device_vector<KeyT> in_keys(elements);); (8, CUDA, thrust::sort_by_key(dev_in_keys, dev_in_keys + elements, dev_in_values, CustomLess {});)", "label": "(9, CUDA, thrust::device_vector<std::uint8_t> data_source(elements);); (12, CUDA, thrust::sequence(data_source.begin(), data_source.end());); (12, CUDA, thrust::shuffle(data_source.begin(), data_source.end(), re);); (5, CUDA, thrust::default_random_engine re;); (15, CUDA, thrust::device_vector<ValueT> in_values(data_source);); (17, CUDA, thrust::host_vector<KeyT> host_keys(in_keys);); (17, CUDA, thrust::host_vector<ValueT> host_values(in_values);); (13, CUDA, thrust::device_vector<ValueT> out_values(elements);); (8, CUDA, thrust::device_vector<KeyT> out_keys(elements);); (6, CUDA, thrust::device_vector<KeyT> in_keys(data_source);)"}
{"predict": "(13, OpenMP, #pragma omp parallel for); (44, OpenMP, #pragma omp critical)", "label": "(13, OpenMP, #pragma omp parallel for); (72, OpenMP, #pragma omp atomic); (43, OpenMP, #pragma omp critical)"}
{"predict": "(85, OpenCL, sub_buf = clCreateSubBuffer(buf[1], 0, CL_BUFFER_CREATE_TYPE_REGION, &region, &error );); (87, OpenCL, void * map_ptr = clEnqueueMapBuffer(queue, buf[0], 1, CL_MAP_READ, 0, 64, 0, NULL, NULL, NULL);)", "label": "(84, OpenCL, sub_buf = clCreateSubBuffer(buf[1], 0, CL_BUFFER_CREATE_TYPE_REGION, &region, &error );); (87, OpenCL, void * map_ptr = clEnqueueMapBuffer(queue, buf[0], 1, CL_MAP_READ, 0, 64, 0, NULL, NULL, NULL);)"}
{"predict": "(1, OpenMP, #pragma omp parallel num_threads(2) cancel(1)); (7, OpenMP, #pragma omp cancel parallel); (8, OpenMP, #pragma omp cancellation point parallel); (0, OpenMP, #pragma omp master)", "label": "(1, OpenMP, #pragma omp parallel num_threads(2)); (10, OpenMP, #pragma omp cancellation point parallel); (3, OpenMP, #pragma omp cancel parallel)"}
{"predict": "(35, OpenMP, #pragma omp target teams distribute parallel for is_device_ptr(DATA) nowait); (13, OpenMP, #pragma omp target teams distribute parallel for is_device_ptr(DATA)); (36, OpenMP, #pragma omp taskwait); (1, OpenMP, #pragma omp target enter data map(alloc:DATA[0:N])); (32, OpenMP, #pragma omp target exit data map(release:DATA[0:N])); (16, OpenMP, #pragma omp target teams distribute parallel for is_device_ptr(DATA)); (18, OpenMP, t1=omp_get_wtime();); (34, OpenMP, #pragma omp target update from(DATA[0:N]))", "label": "(4, OpenMP, omp_set_default_device(0);); (33, OpenMP, #pragma omp target teams distribute parallel for is_device_ptr(DATA) nowait); (15, OpenMP, #pragma omp target teams distribute parallel for is_device_ptr(DATA)); (35, OpenMP, #pragma omp taskwait); (19, OpenMP, t1=omp_get_wtime();); (4, OpenMP, #pragma omp parallel for); (-1, OpenMP, DATA = (double*)   omp_target_alloc(N*sizeof(double), omp_get_default_device()) ;); (22, OpenMP, t2=omp_get_wtime();)"}
{"predict": "(8, CUDA, __shared__ double sum[BLOCK_SIZE];)", "label": "(8, CUDA, __shared__ double sum[block_size];)"}
{"predict": "(62, CUDA, CHECK(cudaPeekAtLastError());)", "label": "(62, CUDA, CHECK(cudaPeekAtLastError());)"}
{"predict": "(42, CUDA, cudaFree(d_inPtr);); (27, CUDA, cudaMalloc((void**)&d_outPtr, size * sizeof(float));); (33, CUDA, cudaMemcpy(&result_gpu, &d_outPtr[0], sizeof(float), cudaMemcpyDeviceToHost);); (27, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);); (37, CUDA, cudaFree(d_outPtr);); (22, CUDA, cudaMalloc((void**)& d_inPtr, size * sizeof(float));)", "label": "(42, CUDA, cudaFree(d_inPtr);); (27, CUDA, cudaMalloc((void**)&d_outPtr, size * sizeof(float));); (32, CUDA, cudaMemcpy(&result_gpu, &d_outPtr[0], sizeof(float), cudaMemcpyDeviceToHost);); (27, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);); (37, CUDA, cudaFree(d_outPtr);); (22, CUDA, cudaMalloc((void**)& d_inPtr, size * sizeof(float));)"}
{"predict": "(88, CUDA, checkCudaErrors(cudaFreeAsync(ptrs[i], stream));); (73, CUDA, checkCudaErrors(cudaStreamSynchronize(stream));); (19, CUDA, checkCudaErrors(cudaSetDevice(shm->devices[id]));); (86, CUDA, checkCudaErrors(cudaEventDestroy(events[i]));); (17, CUDA, checkCudaErrors(cudaStreamCreate(&stream));); (17, CUDA, checkCudaErrors(cudaGetDeviceProperties(&prop, shm->devices[id]));); (81, CUDA, checkCudaErrors(cudaFreeAsync(events[i], stream));)", "label": "(21, CUDA, checkCudaErrors(cudaSetDevice(shm->devices[id]));); (54, CUDA, checkCudaErrors(cudaStreamWaitEvent(stream, events[bufferId], 0));); (71, CUDA, checkCudaErrors(cudaStreamWaitEvent(stream, events[id], 0));); (56, CUDA, checkCudaErrors(cudaGetLastError());); (92, CUDA, checkCudaErrors(cudaStreamDestroy(stream));); (18, CUDA, checkCudaErrors(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));); (86, CUDA, checkCudaErrors(cudaIpcCloseMemHandle(ptrs[i]));); (86, CUDA, checkCudaErrors(cudaEventDestroy(events[i]));); (53, CUDA, checkCudaErrors(cudaEventRecord(events[bufferId], stream));); (68, CUDA, checkCudaErrors(cudaStreamSynchronize(stream));); (12, CUDA, checkCudaErrors(cudaGetDeviceProperties(&prop, shm->devices[id]));)"}
{"predict": "(21, OpenCL, retVal = clReleaseMemObject(smallBuffer);); (5, OpenCL, cl_mem smallBuffer = clCreateBuffer(clContext, flags, size, hostPtr, &retVal);); (24, OpenCL, EXPECT_EQ(clReleaseContext(context), CL_SUCCESS);)", "label": "(21, OpenCL, retVal = clReleaseMemObject(smallBuffer);); (5, OpenCL, cl_mem smallBuffer = clCreateBuffer(clContext, flags, size, hostPtr, &retVal);); (24, OpenCL, EXPECT_EQ(clReleaseContext(context), CL_SUCCESS);)"}
{"predict": "(13, CUDA, cudaCheck(cudaDeviceSynchronize());); (13, CUDA, cudaCheck(cudaGetLastError());)", "label": "(13, CUDA, cudaCheck(cudaGetLastError());)"}
{"predict": "(90, MPI, MPI_Waitall(2, request, status);); (96, MPI, MPI_Recv(buf_recv, shuffile_mpi_buf_size, MPI_BYTE, rank_recv, 0, comm, &status[0]);); (74, MPI, MPI_Send(buf_send, nread, MPI_BYTE, rank_send, 0, comm);); (66, MPI, MPI_Irecv(buf_recv, shuffile_mpi_buf_size, MPI_BYTE, rank_recv, 0, comm, &request[0]);); (67, MPI, MPI_Isend(buf_send, shuffile_mpi_buf_size, MPI_BYTE, rank_send, 0, comm, &request[1]);); (80, MPI, MPI_Send(buf_send, shuffile_mpi_buf_size, MPI_BYTE, rank_send, 0, comm);); (86, MPI, MPI_Recv(buf_recv, shuffile_mpi_buf_size, MPI_BYTE, rank_recv, 0, comm, &status[1]);)", "label": "(82, MPI, MPI_Wait(&request[1], &status[1]);); (90, MPI, MPI_Wait(&request[0], &status[0]);); (90, MPI, MPI_Get_count(&status[0], MPI_BYTE, &nwrite);); (66, MPI, MPI_Irecv(buf_recv, shuffile_mpi_buf_size, MPI_BYTE, rank_recv, 0, comm, &request[0]);); (77, MPI, MPI_Isend(buf_send, nread, MPI_BYTE, rank_send, 0, comm, &request[1]);)"}
{"predict": "(16, CUDA, CUDASimulation cudaSimulation(model);)", "label": "(16, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(66, CUDA, CHECK_CUDART(cudaFree(indata));); (19, CUDA, CHECK_CUDART(cudaMemcpy(indata, &inval, sizeof(T), cudaMemcpyHostToDevice));); (64, CUDA, CHECK_CUDART(cudaFree(outdata));); (13, CUDA, CHECK_CUDART(cudaMalloc((void**)&indata, sizeof(T)));); (13, CUDA, CHECK_CUDART(cudaMalloc((void**)&outdata, sizeof(T)));); (60, CUDA, CHECK_CUDART(cudaMemcpy(&outval, outdata, sizeof(T), cudaMemcpyDeviceToHost));)", "label": "(65, CUDA, CHECK_CUDART(cudaFree(indata));); (19, CUDA, CHECK_CUDART(cudaMemcpy(indata, &inval, sizeof(T), cudaMemcpyHostToDevice));); (61, CUDA, CHECK_CUDART(cudaMemcpy(&outval, outdata, sizeof(T), cudaMemcpyDeviceToHost));); (14, CUDA, CHECK_CUDART(cudaMalloc((void**)&indata, sizeof(T)));); (14, CUDA, CHECK_CUDART(cudaMalloc((void**)&outdata, sizeof(T)));); (59, CUDA, CHECK_CUDART(cudaFree(outdata));)"}
{"predict": "(174, OpenCL, clGetDeviceInfo(vkGPU->device, CL_DEVICE_VENDOR_ID, sizeof(cl_int), &vendorID, 0);); (76, CUDA, res = cudaMalloc((void**)&buffer, bufferSize);); (94, OpenCL, buffer = clCreateBuffer(vkGPU->context, CL_MEM_READ_WRITE, bufferSize, 0, &res);)", "label": "(162, OpenCL, clGetDeviceInfo(vkGPU->device, CL_DEVICE_VENDOR_ID, sizeof(cl_int), &vendorID, 0);); (77, CUDA, res = cudaMalloc((void**)&buffer, bufferSize);); (86, OpenCL, buffer = clCreateBuffer(vkGPU->context, CL_MEM_READ_WRITE, bufferSize, 0, &res);)"}
{"predict": "(114, OpenMP, #pragma omp teams distribute reduction(+ : fl)); (109, OpenMP, #pragma omp teams distribute reduction(+ : r)); (60, OpenMP, #pragma omp teams distribute reduction(max : h.b)); (74, OpenMP, #pragma omp teams distribute reduction(&& : S2::S2s)); (125, OpenMP, #pragma omp teams distribute reduction(+ : m)); (75, OpenMP, #pragma omp teams distribute reduction(& : e, g)); (76, OpenMP, #pragma omp teams distribute reduction(+ : h, k, B::x)); (40, OpenMP, #pragma omp teams distribute reduction(min : a, b, c, d, f)); (100, OpenMP, #pragma omp teams distribute reduction(+ : o)); (90, OpenMP, #pragma omp teams distribute reduction(&& : S2::S2sc)); (53, OpenMP, #pragma omp teams distribute reduction(- : da)); (7, OpenMP, #pragma omp teams distribute reduction); (109, OpenMP, #pragma omp teams distribute reduction(+ : fl)); (63, OpenMP, #pragma omp teams distribute reduction(+ : o)); (70, OpenMP, #pragma omp teams distribute reduction(&& : S2::S2)); (9, OpenMP, #pragma omp teams distribute reduction(); (7, OpenMP, #pragma omp teams distribute reduction()); (73, OpenMP, #pragma omp teams distribute private(i), reduction(+ : j), reduction(+ : q)); (104, OpenMP, #pragma omp teams distribute reduction(+ : fl)); (14, OpenMP, #pragma omp teams distribute reduction(\\)); (49, OpenMP, #pragma omp teams distribute reduction(+ : ba)); (95, OpenMP, #pragma omp teams distribute reduction(+ : fl)); (37, OpenMP, #pragma omp teams distribute reduction(* : ca)); (30, OpenMP, #pragma omp teams distribute reduction(~ : argc)); (83, OpenMP, #pragma omp teams distribute reduction(+ : p), reduction(+ : p)); (57, OpenMP, #pragma omp teams distribute reduction(&& : S2::S2)); (40, OpenMP, #pragma omp teams distribute reduction(&& : argc)); (3,", "label": "(113, OpenMP, #pragma omp teams distribute reduction(task, + : m)); (15, OpenMP, #pragma omp teams distribute reduction +); (29, OpenMP, #pragma omp teams distribute reduction(\\)); (37, OpenMP, #pragma omp teams distribute reduction(|| : argc > 0? argv[1] : argv[2])); (106, OpenMP, #pragma omp teams distribute reduction(+ : m)); (20, OpenMP, #pragma omp teams distribute reduction()); (6, OpenMP, #pragma omp target); (42, OpenMP, #pragma omp teams distribute reduction(^ : S1)); (44, OpenMP, #pragma omp teams distribute reduction(+ : a, b, c, d, f)); (67, OpenMP, #pragma omp teams distribute reduction(&& : S2::S2sc)); (57, OpenMP, #pragma omp teams distribute reduction(- : da)); (32, OpenMP, #pragma omp teams distribute reduction(~ : argc)); (10, OpenMP, #pragma omp teams distribute reduction(-); (45, OpenMP, #pragma omp teams distribute reduction(max : h.b)); (5, OpenMP, #pragma omp teams distribute reduction(); (75, OpenMP, #pragma omp parallel private(k)); (18, OpenMP, #pragma omp teams distribute reduction(foo : argc); (38, OpenMP, #pragma omp teams distribute reduction(min : a, b, c, d, f)); (70, OpenMP, #pragma omp teams distribute private(i), reduction(+ : j), reduction(+ : q)); (79, OpenMP, #pragma omp teams distribute reduction(+ : r)); (41, OpenMP, #pragma omp teams distribute reduction(+ : ba)); (25, OpenMP, #pragma omp teams distribute reduction(&& : argc)); (81, OpenMP, #pragma omp teams distribute reduction(max : j)); (78, OpenMP, #pragma omp parallel reduction(min : i)); (4, OpenMP, #pragma omp teams distribute reduction(*)); (67, OpenMP, #pragma omp teams distribute reduction(+ : p), reduction(+ : p)); (44, OpenMP, #pragma omp teams distribute reduction(^ : fl)); (58, OpenMP, #pragma omp teams distribute reduction(+ : o)); (54, OpenMP, #pragma omp teams distribute reduction(+ : h, k, B::x)); (35, OpenMP, #pragma omp teams distribute reduction(* : ca)); (76, OpenMP, #pragma omp teams distribute reduction(+ : fl, z)); (48, OpenMP, #pragma omp teams distribute reduction(& : e, g)); (68, OpenMP, #pragma omp parallel shared(i)); (4, OpenMP, #pragma omp teams distribute reduction(| : argc,); (-21, OpenMP, #pragma omp teams distribute reduction); (38, OpenMP, #pragma omp teams distribute reduction(&& : S2::S2s))"}
{"predict": "(3, CUDA, HANDLE_ERROR(cudaGetDeviceProperties(&prop, whichDevice));); (3, CUDA, HANDLE_ERROR(cudaGetDevice(&whichDevice));)", "label": "(4, CUDA, checkCudaErrors(cudaGetDeviceProperties(&prop, whichDevice));); (2, CUDA, checkCudaErrors(cudaGetDevice(&whichDevice));)"}
{"predict": "(4, OpenMP, #pragma omp parallel for)", "label": "(4, OpenMP, #pragma omp parallel for)"}
{"predict": "(21, CUDA, extern __shared__ float entire_buffer[];)", "label": "(21, CUDA, extern __shared__ float entire_buffer[];)"}
{"predict": "(30, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (24, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (55, MPI, MPI_Finalize();); (52, MPI, MPI_T_finalize();); (23, MPI, MPI_T_init_thread(required, &provided);); (36, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(29, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (24, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (55, MPI, MPI_Finalize();); (52, MPI, MPI_T_finalize();); (23, MPI, MPI_T_init_thread(required, &provided);); (36, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(56, TBB, ets_type vvs( typename tbb::enumerable_thread_specific<T, Allocator<T> >::instance(), minus_one, minus_one );); (56, TBB, tbb::parallel_reduce( vvs.range(1), pvrb2 );); (2, TBB, typedef std::vector<T, Allocator<T> > container_type;); (3, TBB, typedef tbb::enumerable_thread_specific< container_type, Allocator<container_type> > ets_type;); (53, TBB, tbb::flattened2d<ets_type> fvvs = flatten2d(vvs);); (3, TBB, tbb::global_control s(tbb::global_control::max_allowed_parallelism, p);); (42, TBB, tbb::flattened2d<ets_type>::iterator i = fvs.begin();); (43, TBB, while (i!= fvs.end()) ++i;); (24, TBB, tbb::parallel_reduce( vs.range(1), pvrb );); (2, TBB, tbb::flattened2d<ets_type> fvs = flatten2d(vs);)", "label": "(30, TBB, tbb::parallel_reduce ( vs.range(1), pvrb );); (60, TBB, tbb::parallel_for ( tbb::blocked_range<int> (0, N, RANGE_MIN), parallel_vector_for_body<T,Allocator>( vvs, allocator_name ) );); (1, TBB, typedef std::vector<T, tbb::tbb_allocator<T> > container_type;); (5, TBB, tbb::task_scheduler_init init(p);); (10, TBB, if (Verbose && t == 0) t0 = tbb::tick_count::now();); (64, TBB, tbb::flattened2d<ets_type> fvvs = flatten2d(vvs);); (59, TBB, tbb::parallel_reduce ( vvs.range(1), pvrb2 );); (-5, TBB, tbb::tick_count t0;); (30, TBB, tbb::flattened2d<ets_type> fvs = flatten2d(vs);); (50, TBB, ets_type vvs( typename container_type::size_type(25), minus_one, tbb::tbb_allocator<T>() );); (-6, TBB, typedef tbb::enumerable_thread_specific< container_type, Allocator<container_type> > ets_type;); (66, TBB, REMARK(\"done\\nparallel %s, %d, %g, %g\\n\", test_name, p, result_value, ( tbb::tick_count::now() - t0).seconds());)"}
{"predict": "(1, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)", "label": "(1, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)"}
{"predict": "(14, OpenMP, #pragma omp parallel for)", "label": "(14, OpenMP, #pragma omp parallel for)"}
{"predict": "(12, OpenCL, status = clEnqueueNDRangeKernel(pCmdQ, mockKernel.mockMultiDeviceKernel, testedWorkDim, nullptr, globalWorkSize, localWorkSize, 0, nullptr, nullptr);)", "label": "(12, OpenCL, status = clEnqueueNDRangeKernel(pCmdQ, mockKernel.mockMultiDeviceKernel, testedWorkDim, nullptr, globalWorkSize, localWorkSize, 0, nullptr, nullptr);)"}
{"predict": "(23, CUDA, CSC(cudaGetLastError(), STATUS_FAILURE);)", "label": "(23, CUDA, CSC(cudaGetLastError(), STATUS_FAILURE);)"}
{"predict": "(37, TBB, inc_functor<OutputType> body_copy = tbb::flow::copy_body< inc_functor<OutputType> >( exe_node );); (22, TBB, tbb::flow::make_edge( exe_node, *receivers[r] );); (8, TBB, tbb::flow::continue_node< OutputType > exe_node( g, cf );); (33, TBB, tbb::flow::remove_edge( exe_node, *receivers[r] );); (3, TBB, exe_node.register_predecessor( &fake_sender );); (0, TBB, tbb::flow::graph g;); (43, TBB, g.reset(tbb::flow::rf_reset_bodies);)", "label": "(46, TBB, g.reset(tbb::flow::rf_reset_bodies);); (21, TBB, tbb::flow::make_edge( exe_node, *receivers[r] );); (7, TBB, tbb::flow::continue_node< OutputType > exe_node( g, cf );); (22, TBB, utils::NativeParallelFor( p, parallel_puts<tbb::flow::continue_msg>(exe_node) );); (36, TBB, body_copy = tbb::flow::copy_body< inc_functor<OutputType> >( exe_node );); (-1, TBB, tbb::flow::graph g;); (29, TBB, tbb::flow::remove_edge( exe_node, *receivers[r] );); (5, TBB, tbb::detail::d2::register_predecessor(exe_node, fake_sender);)"}
{"predict": "(45, MPI, MPI_Finalize();); (6, MPI, MPI_Init(&argc, &argv);); (39, MPI, auto t1 = MPI_Wtime();); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (21, MPI, MPI_Barrier(MPI_COMM_WORLD);); (19, MPI, auto t0 = MPI_Wtime();)", "label": "(6, MPI, MPI_Init(&argc, &argv);); (47, MPI, MPI_Finalize();); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (38, MPI, double t1 = MPI_Wtime();); (20, MPI, MPI_Barrier(MPI_COMM_WORLD);); (20, MPI, double t0 = MPI_Wtime();)"}
{"predict": "(1, TBB, tbb::task_arena a(2);)", "label": "(1, TBB, tbb::task_arena a(1);); (13, TBB, CHECK(tbb::this_task_arena::max_concurrency() == 2);)"}
{"predict": "(16, OpenMP, #pragma omp teams distribute parallel for proc_bind(master); (3, OpenMP, #pragma omp teams distribute parallel for proc_bind); (37, OpenMP, #pragma omp teams distribute parallel for proc_bind(spread)); (21, OpenMP, #pragma omp teams distribute parallel for proc_bind(x)); (25, OpenMP, #pragma omp teams distribute parallel for proc_bind(close), proc_bind(spread)); (16, OpenMP, #pragma omp teams distribute parallel for proc_bind(master)); (28, OpenMP, #pragma omp teams distribute parallel for proc_bind(master)); (2, OpenMP, #pragma omp teams distribute parallel for proc_bind(); (10, OpenMP, #pragma omp teams distribute parallel for proc_bind(close)); (28, OpenMP, #pragma omp teams distribute parallel for proc_bind(close)); (-4, OpenMP, #pragma omp target); (18, OpenMP, #pragma omp teams distribute parallel for proc_bind(spread))", "label": "(31, OpenMP, #pragma omp teams distribute parallel for proc_bind(master)); (25, OpenMP, #pragma omp teams distribute parallel for proc_bind(x)); (4, OpenMP, #pragma omp teams distribute parallel for proc_bind); (34, OpenMP, #pragma omp teams distribute parallel for proc_bind(spread)); (6, OpenMP, #pragma omp teams distribute parallel for proc_bind(); (9, OpenMP, #pragma omp teams distribute parallel for proc_bind()); (16, OpenMP, #pragma omp teams distribute parallel for proc_bind(close), proc_bind(spread)); (-2, OpenMP, #pragma omp target); (27, OpenMP, #pragma omp parallel proc_bind(close)); (9, OpenMP, #pragma omp teams distribute parallel for proc_bind(master)"}
{"predict": "(42, CUDA, cudaFree(res_dev);); (35, CUDA, cudaMalloc(&res_dev, sizeof(float));); (37, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)", "label": "(42, CUDA, cudaFree(res_dev);); (35, CUDA, cudaMalloc(&res_dev, sizeof(float));); (37, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(11, CUDA, extern __shared__ __align__(sizeof(double)) unsigned char buf_[];)", "label": "(11, CUDA, extern __shared__ __align__(sizeof(double)) unsigned char buf_[];)"}
{"predict": "(131, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (34, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (138, MPI, MPI_Finalize();); (18, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (30, MPI, MPI_Barrier(MPI_COMM_WORLD);); (124, MPI, MPI_Test(&req, &completed, MPI_STATUS_IGNORE);); (30, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (131, MPI, MPI_Allreduce(MPI_IN_PLACE, total_done_skipped_failed, sizeof(total_done_skipped_failed)/sizeof(int), MPI_INT, MPI_MAX, MPI_COMM_WORLD);); (24, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(21, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (193, MPI, MPI_Finalize();); (25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (23, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (124, MPI, MPI_Test(&req, &completed, MPI_STATUS_IGNORE);)"}
{"predict": "(87, MPI, Mpi.Instance.mpi_state = MPI_STATE_POST_FINALIZED;); (40, MPI, MPI_Barrier(Mpi.CommWorld);)", "label": "(40, MPI, mpi_errno = Mpi.Finalize();); (86, MPI, Mpi.CompleteFinalize();)"}
{"predict": "(44, MPI, MPI_T_finalize();); (19, MPI, MPI_T_init_thread(required, &provided);)", "label": "(44, MPI, MPI_T_finalize();); (19, MPI, MPI_T_init_thread(required, &provided);)"}
{"predict": "(151, MPI, MPI_Allgather(&end_offset, 1, ADIO_OFFSET, end_offsets, 1, ADIO_OFFSET, fd->comm);); (57, MPI, MPI_Comm_rank(fd->comm, &myrank);); (147, MPI, MPI_Allgather(&start_offset, 1, ADIO_OFFSET, st_offsets, 1, ADIO_OFFSET, fd->comm);); (401, MPI, MPI_Type_size_x(datatype, &size);); (103, MPI, MPI_Type_size_x(datatype, &buftype_size);)", "label": "(141, MPI, MPI_Allgather(&end_offset, 1, ADIO_OFFSET, end_offsets, 1, ADIO_OFFSET, fd->comm);); (59, MPI, MPI_Comm_rank(fd->comm, &myrank);); (138, MPI, MPI_Allgather(&start_offset, 1, ADIO_OFFSET, st_offsets, 1, ADIO_OFFSET, fd->comm);); (395, MPI, MPI_Type_size_x(datatype, &size);); (94, MPI, MPI_Type_size_x(datatype, &buftype_size);)"}
{"predict": "(41, OpenMP, lis2mdl_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)", "label": "(41, OpenMP, lis2mdl_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)"}
{"predict": "(75, OpenMP, __kmp_gomp_task(&loc, gtid, task, 0, NULL, 0, NULL);); (112, OpenMP, #pragma omp taskwait); (64, OpenMP, __kmp_gomp_task(&loc, gtid, task, ndeps_cnv, dep_list, 0, depend);); (102, OpenMP, __kmp_omp_task(&loc, gtid, task, 0, NULL, 0, depend);)", "label": "(67, OpenMP, kmp_gomp_depends_info_t gomp_depends(depend);); (100, OpenMP, __kmpc_omp_wait_deps(&loc, gtid, ndeps, dep_list, 0, NULL);); (74, OpenMP, __kmpc_omp_task(&loc, gtid, task);); (71, OpenMP, __kmpc_omp_task_with_deps(&loc, gtid, task, ndeps_cnv, dep_list, 0, NULL);)"}
{"predict": "(10, MPI, MPI_Comm c_comm = MPI_Comm_f2c(*comm);)", "label": "(10, MPI, MPI_Comm c_comm = MPI_Comm_f2c(*comm);)"}
{"predict": "(91, OpenMP, kernel_endtime=omp_get_wtime();); (84, OpenMP, double sync_time=omp_get_wtime()-sync_starttime;); (85, OpenMP, rank0_printf_timestamp(\"Finishing iteration %6li, time %lf sec\\n\", itime, sync_time);); (91, OpenMP, rank0_printf_timestamp(\"Kernel launch time %lf sec\\n\", kernel_endtime-kernel_starttime);); (144, OpenMP, rank0_printf_timestamp(\"Finishing kernel(104, OpenMP, #pragma omp target enter data map(to: m_gate[:nCells])); (104, OpenMP, #pragma omp target enter data map(to: Vm[:nCells])); (147, OpenMP, #pragma omp target exit data map(from: m_gate[:nCells])); (80, OpenMP, #pragma omp target map(tofrom:sync_starttime)); (137, OpenMP, #pragma omp target exit data map(from: Vm[:nCells])); (94, OpenMP, kernel_runtime=omp_get_wtime()-kernel_starttime;); (133, OpenMP, #pragma omp target update from(Vm[:nCells])); (14, OpenMP, double sync_starttime=omp_get_wtime();); (98, OpenMP, #pragma omp target map(tofrom:kernel_starttime)); (134, OpenMP, #pragma omp target update to(m_gate[:nCells])); (64, OpenMP, #pragma omp target enter data map(to: m_gate[:nCells], Vm[:nCells])); (13, OpenMP, #pragma omp target map(tofrom:last_printf_timestamp)); (85, OpenMP, #pragma omp target map(tofrom:kernel_endtime)); (70, OpenMP, #pragma omp target launch); (62, OpenMP, #pragma omp target data map(to: m_gate[:nCells], Vm[:nCells])); (103, OpenMP, #pragma omp target teams distribute parallel for); (105, OpenMP, #pragma omp target parallel for); (70, OpenMP, #pragma", "label": "(162, OpenMP, #pragma omp target exit data map(delete:Vm[:nCells])); (66, OpenMP, #pragma omp target enter data map(to: m_gate[:nCells])); (149, OpenMP, #pragma omp target update from (m_gate[0:nCells])); (65, OpenMP, #pragma omp target enter data map(to: Vm[:nCells])); (102, OpenMP, #pragma omp target teams distribute parallel for); (156, OpenMP, #pragma omp target exit data map(delete:m_gate[:nCells]))"}
{"predict": "(6, MPI, MPI_Type_get_attr(type, key[i], &val_p, &flag);)", "label": "(6, MPI, MPI_Type_get_attr(type, key[i], &val_p, &flag);)"}
{"predict": "(7, OpenMP, #pragma omp parallel)", "label": "(7, OpenMP, #pragma omp for nowait schedule(runtime))"}
{"predict": "(9, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(9, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(6, CUDA, extern __shared__ float s_data[];)", "label": "(6, CUDA, extern __shared__ float s_data[];)"}
{"predict": "(37, MPI, MPI_Info_get_nkeys(infos[i], &nkeys);); (8, MPI, MPI_Info_create(&(infos[i] = MPI_Info_f2c(i)));); (55, MPI, MPI_Info_get(infos[i], key, sizeof(value), value, &flag);); (71, MPI, MPI_Info_free(&(infos[i]));); (22, MPI, MPI_Info_get_nkeys(infos[0], &nkeys);)", "label": "(10, MPI, MPI_Info_create(&infos[i]);); (57, MPI, MPI_Info_get(infos[i], key, sizeof(value), value, &flag);); (47, MPI, MPI_Info_get_nthkey(infos[i], j, key);); (22, MPI, MPI_Info_get_nkeys(infos[0], &nkeys);); (33, MPI, MPI_Info_get_nkeys(infos[i], &nkeys);); (71, MPI, MPI_Info_free(&infos[i]);)"}
{"predict": "(5, OpenMP, #pragma omp parallel for)", "label": "(5, OpenMP, #pragma omp parallel for)"}
{"predict": "(10, MPI, MPI_Reduce_scatter_block_c(sendbuf, recvbuf, count, datatype, MPI_SUM, comm);); (12, MPI, MPI_Ireduce_scatter_block_c(sendbuf, recvbuf, count, datatype, MPI_SUM, comm, &req);); (12, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);)", "label": "(14, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (12, MPI, MPI_Ireduce_scatter_block_c(sendbuf, recvbuf, count, datatype, MPI_SUM, comm, &req);); (8, MPI, MPI_Reduce_scatter_block_c(sendbuf, recvbuf, count, datatype, MPI_SUM, comm);)"}
{"predict": "(29, CUDA, checkCudaErrors(cudaGetDeviceCount(&GPU_N));); (120, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, whichGPUs[i]));); (100, CUDA, checkCudaErrors(cufftCreate(&plan_input));); (38, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, i));); (113, CUDA, cudaSetDevice(whichGPUs[i]);); (105, CUDA, cudaSetDevice(0);)", "label": "(29, CUDA, checkCudaErrors(cudaGetDeviceCount(&GPU_N));); (219, CUDA, checkCudaErrors(cufftXtFree(d_out_filter_kernel));); (120, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, whichGPUs[i]));); (216, CUDA, checkCudaErrors(cufftXtFree(d_out_signal));); (100, CUDA, checkCudaErrors(cufftCreate(&plan_input));); (40, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, i));); (211, CUDA, checkCudaErrors(cufftXtFree(d_signal));); (216, CUDA, checkCudaErrors(cufftDestroy(plan_input));); (210, CUDA, checkCudaErrors(cufftXtFree(d_filter_kernel));)"}
{"predict": "(63, MPI, err = MPI_Get(recvBuf, 1, originType, source, 0, 1, originType, win);); (58, MPI, MPI_Win_fence(0, win);); (32, MPI, MPI_Comm_size(comm, &size);); (58, MPI, err = MPI_Put(srcBuf, 1, originType, dest, 0, 1, originType, win);); (25, MPI, MPI_Type_indexed(2, counts, displs, MPI_INT, &originType);); (25, MPI, MPI_Type_commit(&originType);); (76, MPI, MPI_Win_free(&win);); (55, MPI, MPI_Win_set_errhandler(win, MPI_ERRORS_RETURN);); (76, MPI, MPI_Type_free(&originType);); (47, MPI, MPI_Win_create(recvBuf, totsize * extent, extent, MPI_INFO_NULL, comm, &win);); (50, MPI, MPI_Win_fence(0, win);); (20, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (16, MPI, MPI_Comm_get_extent(comm, &tmp_lb, &extent);); (44, MPI, MPI_Win_set_errhandler(win, MPI_ERRORS_RETURN);); (36, MPI, MPI_Comm_rank(comm, &rank);)", "label": "(56, MPI, MPI_Type_get_extent(MPI_INT, &tmp_lb, &extent);); (33, MPI, MPI_Comm_size(comm, &size);); (55, MPI, MPI_Win_create(recvBuf, totsize * extent, extent, MPI_INFO_NULL, comm, &win);); (24, MPI, MPI_Type_indexed(2, counts, displs, MPI_INT, &originType);); (54, MPI, MPI_Win_fence(0, win);); (23, MPI, MPI_Type_commit(&originType);); (77, MPI, MPI_Win_free(&win);); (75, MPI, MPI_Type_free(&originType);); (58, MPI, err = MPI_Win_fence(0, win);); (53, MPI, MPI_Win_set_errhandler(win, MPI_ERRORS_RETURN);); (33, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (22, MPI, MPI_Comm_rank(comm, &rank);); (52, MPI, err = MPI_Put(srcBuf, 1, originType, dest, 0, totsize, MPI_INT, win);)"}
{"predict": "(18, OpenMP, #pragma omp target parallel firstprivate(i)); (13, OpenMP, #pragma omp target parallel firstprivate(argv[1])); (16, OpenMP, #pragma omp target parallel firstprivate(j)); (19, OpenMP, #pragma omp target parallel firstprivate(m)); (7, OpenMP, #pragma omp target parallel firstprivate(a, b, c, d, f)); (1, OpenMP, #pragma omp target parallel firstprivate(argc); (3, OpenMP, #pragma omp target parallel firstprivate(S1)); (6, OpenMP, #pragma omp target parallel firstprivate(argv[1], z)); (1, OpenMP, #pragma omp target parallel firstprivate(argc > 0? argv[1] : argv[2])); (8, OpenMP, #pragma omp target parallel shared(i)); (13, OpenMP, #pragma omp target parallel firstprivate(e, g)); (3, OpenMP, #pragma omp target parallel firstprivate(2 * 2))", "label": "(8, OpenMP, #pragma omp task private(); (17, OpenMP, #pragma omp task private(ca)); (20, OpenMP, #pragma omp task private(threadvar, B::x)); (10, OpenMP, #pragma omp task private(argc argv)); (17, OpenMP, #pragma omp task private(e, g)); (31, OpenMP, #pragma omp task private(m)); (1, OpenMP, #pragma omp task private); (9, OpenMP, #pragma omp task private(argv[1])); (1, OpenMP, #pragma omp task private()); (19, OpenMP, #pragma omp task private(j)); (4, OpenMP, #pragma omp task private(S1)); (0, OpenMP, #pragma omp task private(argc,); (5, OpenMP, #pragma omp task private(ba) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (7, OpenMP, #pragma omp task private(S2::S2s)); (5, OpenMP, #pragma omp task private(da)); (-3, OpenMP, #pragma omp task private(argc > 0? argv[1] : argv[2])); (11, OpenMP, #pragma omp task private(i)); (6, OpenMP, #pragma omp task shared(i), private(i)); (14, OpenMP, #pragma omp task private(i) allocate(omp_thread_mem_alloc: i)); (6, OpenMP, #pragma omp task firstprivate(i) private(i)); (10, OpenMP, #pragma omp task firstprivate(i)); (-11, OpenMP, #pragma omp task private(argc); (-7, OpenMP, #pragma omp task private(z, a, b, c, d, f))"}
{"predict": "(26, OpenMP, omp_target_free(d_a, omp_get_default_device());); (14, OpenMP, d_a = (int *)omp_target_alloc(DataSize, omp_get_default_device());); (15, OpenMP, omp_target_associate_ptr(p_a, d_a, DataSize, 0, omp_get_default_device());); (17, OpenMP, #pragma omp target update to(p_a[0:N])); (19, OpenMP, omp_target_disassociate_ptr(p_a, omp_get_default_device());)", "label": "(25, OpenMP, omp_target_free(d_a, omp_get_default_device());); (12, OpenMP, d_a = (int *)omp_target_alloc(DataSize, omp_get_default_device());); (18, OpenMP, #pragma omp target update to(p_a)); (20, OpenMP, omp_target_disassociate_ptr(p_a, omp_get_default_device());); (11, OpenMP, omp_target_associate_ptr(p_a, d_a, DataSize, 0, omp_get_default_device());)"}
{"predict": "(45, MPI, err = MPI_Type_size(MPI_COMM_WORLD, &size);); (26, MPI, err = MPI_Type_vector(2, 1, 1, MPI_INT, &vectype);); (77, MPI, MPI_Type_free(&newtype);); (33, MPI, err = MPI_Type_create_blockindexed(count, 1, disp, MPI_INT, &newtype);); (49, MPI, MPI_Type_size(MPI_INT, &int_size);); (55, MPI, MPI_Type_commit(&newtype);); (29, MPI, MPI_Type_free(&vectype);)", "label": "(35, MPI, err = MPI_Type_create_indexed_block(count, 1, disp, vectype, &newtype);); (42, MPI, MPI_Type_size(MPI_INT, &int_size);); (58, MPI, MPI_Type_commit(&newtype);); (24, MPI, err = MPI_Type_vector(2, 1, 2, MPI_INT, &vectype);); (41, MPI, err = MPI_Type_size(newtype, &size);); (74, MPI, MPI_Type_free(&newtype);); (72, MPI, MPI_Type_free(&vectype);)"}
{"predict": "(44, OpenCL, clReleaseEvent(retEvent);)", "label": "(44, OpenCL, clReleaseEvent(retEvent);)"}
{"predict": "(3, CUDA, EXPECT_EQ(cudaMalloc(&_junk, junk_size), cudaSuccess);); (24, CUDA, ASSERT_EQ(cudaEventRecord(wm.ready, 0), cudaSuccess);); (20, CUDA, ASSERT_EQ(cudaEventQuery(wm.ready), cudaErrorNotReady);); (16, CUDA, cudaMemsetAsync(base, i, 64, 0);); (28, CUDA, ASSERT_EQ(cudaDeviceSynchronize(), cudaSuccess);); (20, CUDA, cvcuda::WorkspaceMemAllocator wa(wm, cudaStream_t(0));); (24, CUDA, EXPECT_PTR_EQ(wa.get(32), base);); (41, CUDA, cvcuda::WorkspaceMemAllocator wa(wm, std::nullopt, std::nullopt);)", "label": "(24, CUDA, ASSERT_EQ(cudaEventRecord(wm.ready, 0), cudaSuccess);); (25, CUDA, cvcuda::WorkspaceMemAllocator wa(wm, cudaStream_t(0));); (51, CUDA, EXPECT_EQ(cudaEventQuery(wm.ready), cudaSuccess);); (1, CUDA, ASSERT_EQ(cudaMalloc(&_junk, junk_size), cudaSuccess);); (14, CUDA, ASSERT_EQ(cudaMemset(junk.get(), i, junk_size), cudaSuccess);); (6, CUDA, ASSERT_EQ(cudaEventCreateWithFlags(&wm.ready, cudaEventDisableTiming), cudaSuccess);); (26, CUDA, ASSERT_EQ(cudaDeviceSynchronize(), cudaSuccess);); (50, CUDA, ASSERT_EQ(cudaEventDestroy(wm.ready), cudaSuccess);); (19, CUDA, EXPECT_EQ(cudaEventQuery(wm.ready), cudaErrorNotReady);)"}
{"predict": "(99, CUDA, CHECK_CUDA(cudaFree(dUpdates));); (64, CUDA, CHECK_CUDA(cudaMalloc(&dy, dataSize * sizeof(int32_t)));); (95, CUDA, CHECK_CUDA(cudaFree(dy));); (38, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (91, CUDA, CHECK_CUDA(cudaFree(dIndices));); (39, CUDA, CHECK_HIEDNN(hiednnSetCudaStream(handle, 0));); (90, CUDA, CHECK_CUDA(cudaFree(dx));); (56, CUDA, CHECK_CUDA(cudaMalloc(&dUpdates, updatesSize * sizeof(int32_t)));); (53, CUDA, CHECK_CUDA(cudaMalloc(&dx, dataSize * sizeof(int32_t)));); (54, CUDA, CHECK_CUDA(cudaMalloc(&dIndices, indicesSize * sizeof(int32_t)));); (86, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));)", "label": "(65, CUDA, CHECK_CUDA(cudaMalloc(&dy, dataSize * sizeof(int32_t)));); (100, CUDA, CHECK_CUDA(cudaFree(dUpdates));); (62, CUDA, CHECK_CUDA(cudaMalloc(&dUpdates, updatesSize * sizeof(int32_t)));); (94, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (37, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (97, CUDA, CHECK_CUDA(cudaFree(dy));); (56, CUDA, CHECK_CUDA(cudaMalloc(&dx, dataSize * sizeof(int32_t)));); (93, CUDA, CHECK_CUDA(cudaFree(dIndices));); (36, CUDA, CHECK_HIEDNN(hiednnSetCudaStream(handle, 0));); (90, CUDA, CHECK_CUDA(cudaFree(dx));); (53, CUDA, CHECK_CUDA(cudaMalloc(&dIndices, indicesSize * sizeof(int32_t)));)"}
{"predict": "(17, TBB, tbb::flow::make_edge( src3, dest3 );); (17, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(dest3), counters3[i] );); (3, TBB, tbb::flow::make_edge( src, dest );); (3, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(dest), counters[i] );); (29, TBB, tbb::flow::make_edge( src2, dest2 );); (37, TBB, tbb::flow::source_node<T> src_copy(src);); (12, TBB, tbb::flow::source_node<T> src3(g, snode_body<T>() );); (12, TBB, tbb::flow::function_node<T,bool> dest3(g, tbb::flow::unlimited, b3 );); (14, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(dest3), counters3[i] );); (26, TBB, tbb::flow::function_node<T,bool,tbb::flow::rejecting> dest2(g, tbb::flow::serial, b2 );); (23, TBB, tbb::flow::source_node<T> src2(g, snode_body<T>() );); (10, TBB, tbb::flow::make_edge( src3, dest3 );); (-6, TBB, tbb::flow::graph g;); (14, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(dest2), counters2[i] );); (19, TBB, tbb::flow::make_edge( src2, dest2 );); (-9, TBB, tbb::flow::source_node<T> src(g, snode_body<T>() );)", "label": "(28, TBB, tbb::flow::input_node<T> src2(g, my_input_body<T>() );); (18, TBB, tbb::flow::function_node<T,bool> dest3(g, tbb::flow::unlimited, b3 );); (31, TBB, tbb::flow::function_node<T,bool,tbb::flow::rejecting> dest2(g, tbb::flow::serial, b2 );); (3, TBB, tbb::flow::make_edge( src, dest );); (0, TBB, tbb::flow::input_node<T> src(g, my_input_body<T>() );); (29, TBB, tbb::flow::make_edge( src2, dest2 );); (36, TBB, tbb::flow::input_node<T> src_copy(src);); (13, TBB, tbb::flow::make_edge( src3, dest3 );); (-5, TBB, tbb::flow::graph g;); (6, TBB, tbb::flow::input_node<T> src3(g, my_input_body<T>() );)"}
{"predict": "(26, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&output_data, output_size_bytes));); (41, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (17, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&input_data, input_size_bytes));); (37, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (45, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(output_data));); (43, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(input_data));)", "label": "(26, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&output_data, output_size_bytes));); (18, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&input_data, input_size_bytes));); (48, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(output_data));); (39, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (37, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (44, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(input_data));)"}
{"predict": "(85, CUDA, cudaFuncGetAttributes(&cuattr, slsum_fmod_inv_gpu_mrhs_nvshmem );); (181, CUDA, CUDA_CHECK(cudaGetLastError());); (131, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream[i]));); (85, CUDA, cudaOccupancyMaxPotentialBlockSize(&minGridSize,&myblockSize,(const void *) sready_x,sizeof(float)*nrhs,0,0 );); (76, CUDA, cudaDeviceSetLimit(cudaLimitStackSize, cuattr.localSizeBytes);); (78, CUDA, cudaStreamCreateWithFlags(&stream[i],cudaStreamNonBlocking);); (136, CUDA, CUDA_CHECK(cudaGetLastError());)", "label": "(85, CUDA, cudaDeviceSetLimit(cudaLimitStackSize, cuattr.localSizeBytes);); (140, CUDA, CUDA_CHECK(cudaDeviceSynchronize());); (78, CUDA, cudaStreamCreateWithFlags(&stream[i], cudaStreamNonBlocking);); (135, CUDA, CUDA_CHECK(cudaGetLastError());); (80, CUDA, cudaFuncGetAttributes(&cuattr, slsum_fmod_inv_gpu_mrhs_nvshmem);); (138, CUDA, CUDA_CHECK(cudaStreamDestroy(stream[i]));); (82, CUDA, cudaOccupancyMaxPotentialBlockSize(&minGridSize,&myblockSize,(const void *) swait_bcrd,0,0 );)"}
{"predict": "(1, CUDA, extern __shared__ unsigned int s_data[];)", "label": "(2, CUDA, __shared__ volatile unsigned int s_data[WARP_SIZE*2];)"}
{"predict": "(29, OpenCL, error |= clSetKernelArg(kernel, 1, sizeof(value), &value);); (27, OpenCL, error |= clSetKernelArg(kernel, 0, sizeof(dst), &dst);)", "label": "(29, OpenCL, error |= clSetKernelArg(kernel, 1, sizeof(value), &value);); (27, OpenCL, error |= clSetKernelArg(kernel, 0, sizeof(dst), &dst);)"}
{"predict": "(8, MPI, tim::settings::mpi_init()       = false;); (202, MPI, tim::settings::mpi_finalize()   = true;)", "label": "(8, MPI, tim::settings::mpi_init()       = false;); (8, MPI, tim::settings::mpi_finalize()   = false;)"}
{"predict": "(18, OpenMP, #pragma omp parallel); (1, OpenMP, int nThreads = omp_get_max_threads();)", "label": "(19, OpenMP, #pragma omp parallel); (0, OpenMP, int nThreads = omp_get_max_threads();)"}
{"predict": "(8, TBB, oneapi::tbb::parallel_for( oneapi::tbb::blocked_range<std::size_t>(0,10000*s,s), GrowToAtLeast<MyVector>(use_two_arg_form, v, copy_from), oneapi::tbb::simple_partitioner() );); (4, TBB, using MyVector = oneapi::tbb::concurrent_vector<move_support_tests::Foo, MyAllocator>;)", "label": "(4, TBB, using MyVector = oneapi::tbb::concurrent_vector<move_support_tests::Foo, MyAllocator>;); (8, TBB, oneapi::tbb::parallel_for(oneapi::tbb::blocked_range<std::size_t>(0, 10000*s, s), GrowToAtLeast<MyVector>(use_two_arg_form, v, copy_from), oneapi::tbb::simple_partitioner());)"}
{"predict": "(75, MPI, MPI_Group_free(&g3);); (44, MPI, MPI_Group_translate_ranks(selfgroup, 3, rin, g45, rout);); (49, MPI, MPI_Group_translate_ranks(MPI_GROUP_WORLD, 1, &myrank, g45, rout);); (74, MPI, MPI_Group_free(&g45);); (42, MPI, MPI_Group_translate_ranks(MPI_GROUP_WORLD, 1, &myrank, g3, rout);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myrank);); (83, MPI, MPI_Group_excl(g1, 3, ranks, &g3);); (80, MPI, MPI_Group_incl(g1, 3, ranks, &g2);); (73, MPI, MPI_Group_translate_ranks(MPI_GROUP_WORLD, g1size, lranks, g6, rout);); (76, MPI, MPI_Group_free(&g6);); (28, MPI, MPI_Group_translate_ranks(MPI_GROUP_WORLD, 3, rin, g1, rout);); (19, MPI, MPI_Group_range_incl(g1, 1, range, &g45);); (20, MPI, MPI_Group_translate_ranks(MPI_GROUP_WORLD, 3, rin, g2, rout);); (14, MPI, MPI_Group_incl(g1, 3, ranks, &g45);); (10, MPI, MPI_Group_translate_ranks(MPI_GROUP_WORLD, 3, rin, g1, rout);); (17, MPI, MPI_Group_translate_ranks(MPI_GROUP_WORLD, 1, &myrank, g2, rout);); (30, MPI, MPI_Group_rank(MPI_GROUP_WORLD, &rank);); (74, MPI, MPI_Group_excl(g1, g1size, lranks, &g6);); (74, MPI, MPI_Group_excl(g1, g1size, lranks, &g7);", "label": "(84, MPI, MPI_Group_excl(g1, g1size, lranks, &g6);); (27, MPI, MPI_Group_size(g2, &size);); (41, MPI, MPI_Group_translate_ranks(g2, 3, rin, g1, rout);); (70, MPI, MPI_Group_free(&selfgroup);); (105, MPI, MPI_Group_free(&g2);); (24, MPI, MPI_Group_rank(g2, &rank);); (117, MPI, MPI_Group_compare(MPI_GROUP_EMPTY, g4, &result);); (123, MPI, MPI_Group_free(&g45);); (113, MPI, MPI_Group_union(g4, g5, &g45);); (105, MPI, MPI_Group_range_excl(g1, 1, range, &g5);); (109, MPI, MPI_Group_range_incl(g1, 1, range, &g4);); (118, MPI, MPI_Group_free(&g5);); (-1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (12, MPI, MPI_Group_incl(g1, 3, ranks, &g2);); (114, MPI, MPI_Group_free(&g4);); (140, MPI, MPI_Group_free(&g1);); (72, MPI, MPI_Group_free(&g6);); (62, MPI, MPI_Group_size(g1, &g1size);); (34, MPI, MPI_Comm_group(MPI_COMM_SELF, &selfgroup);); (-10, MPI, MPI_Comm_group(MPI_COMM_WORLD, &g1);); (-10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myrank);); (41, MPI, MPI_Group_translate_ranks(g1, size, rin, selfgroup, rout);); (32, MPI, MPI_Group_translate_ranks(selfgroup, 1, rin, g1, rout);)"}
{"predict": "(60, MPI, MPI_Finalize();); (0, MPI, MPI_Init(&argc, &argv);); (1, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (33, MPI, MPI_Allgather(in.data(), local_size, MPI_DOUBLE, result.data(), sizes.data(), dspls.data(), MPI_DOUBLE, MPI_COMM_WORLD);); (47, MPI, MPI_Allgather(in_padded.data(), max_size, MPI_DOUBLE, result_padded.data(), max_size, MPI_DOUBLE, MPI_COMM_WORLD);); (19, MPI, MPI_Padded(in.data(), local_size, MPI_DOUBLE, in_padded.data(), max_size);); (-3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &P);)", "label": "(1, MPI, MPI_Init(&argc, &argv);); (58, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &P);)"}
{"predict": "(7, CUDA, extern __shared__ __align__(sizeof(float)) uchar s[];)", "label": "(7, CUDA, extern __shared__ __align__(sizeof(float)) uchar s[];)"}
{"predict": "(4, OpenMP, #pragma omp cancel sections); (23, OpenMP, #pragma omp for); (0, OpenMP, #pragma omp section); (5, OpenMP, #pragma omp cancel for); (18, OpenMP, #pragma omp parallel sections); (10, OpenMP, #pragma omp cancel for); (3, OpenMP, #pragma omp cancel parallel); (15, OpenMP, #pragma omp cancel for); (2, OpenMP, #pragma omp cancel)", "label": "(5, OpenMP, #pragma omp barrier); (23, OpenMP, #pragma omp task); (18, OpenMP, #pragma omp for); (0, OpenMP, #pragma omp cancel parallel if(flag)); (4, OpenMP, #pragma omp sections); (7, OpenMP, #pragma omp section); (22, OpenMP, #pragma omp parallel sections); (-6, OpenMP, #pragma omp parallel); (18, OpenMP, #pragma omp cancel taskgroup); (32, OpenMP, #pragma omp parallel for reduction(+: r)); (0, OpenMP, #pragma omp cancel sections); (11, OpenMP, #pragma omp cancel for)"}
{"predict": "(31, OpenCL, clReleaseMemObject(buffer);)", "label": "(31, OpenCL, clReleaseMemObject(buffer);)"}
{"predict": "(12, TBB, tbb::parallel_for(tbb::blocked_range<size_t>(0,n,n/10),AddOne<Counter<M> >(counter));); (12, TBB, tbb::tick_count t1 = tbb::tick_count::now();)", "label": "(5, TBB, tbb::profiling::set_name(counter.mutex, name);); (11, TBB, tbb::parallel_for(tbb::blocked_range<size_t>(0,n,n/10),AddOne<Counter<M> >(counter));); (11, TBB, tbb::tick_count t1 = tbb::tick_count::now();)"}
{"predict": "(231, MPI, MPI_Finalize();); (35, MPI, MPI_Init_thread( &argc, &argv, MPI_THREAD_MULTIPLE, &omp_mpi_level);)", "label": "(232, MPI, MPI_Finalize();); (35, MPI, MPI_Init_thread( &argc, &argv, MPI_THREAD_MULTIPLE, &omp_mpi_level);)"}
{"predict": "(36, OpenMP, #pragma omp teams distribute parallel for simd linear (argc > 0? argv[1] : argv[2])); (31, OpenMP, #pragma omp teams distribute parallel for simd linear (argc : 5)); (5, OpenMP, #pragma omp teams distribute parallel for simd linear); (50, OpenMP, #pragma omp teams distribute parallel for simd linear(e, g)); (24, OpenMP, #pragma omp teams distribute parallel for simd linear (argc :); (12, OpenMP, #pragma omp teams distribute parallel for simd linear ()); (40, OpenMP, #pragma omp teams distribute parallel for simd linear(a, b : B::ib)); (3, OpenMP, #pragma omp teams distribute parallel for simd linear (); (14, OpenMP, #pragma omp teams distribute parallel for simd linear (argc); (26, OpenMP, #pragma omp teams distribute parallel for simd linear (S1)); (32, OpenMP, #pragma omp teams distribute parallel for simd linear (argv[1])); (4, OpenMP, #pragma omp teams distribute parallel for simd linear (argc); (45, OpenMP, #pragma omp teams distribute parallel for simd linear(h)); (13, OpenMP, #pragma omp teams distribute parallel for simd linear (argc,); (47, OpenMP, #pragma omp teams distribute parallel for simd linear(i)); (1, OpenMP, #pragma omp teams distribute parallel for simd linear (ref())); (44, OpenMP, #pragma omp teams distribute parallel for simd linear(j))", "label": "(34, OpenMP, #pragma omp teams distribute parallel for simd linear (argc : 5) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (19, OpenMP, #pragma omp teams distribute parallel for simd linear (argc); (27, OpenMP, #pragma omp teams distribute parallel for simd linear (argc > 0? argv[1] : argv[2])); (43, OpenMP, #pragma omp teams distribute parallel for simd linear (argv[1])); (38, OpenMP, #pragma omp teams distribute parallel for simd linear (a, b:B::ib)); (55, OpenMP, #pragma omp teams distribute parallel for simd linear(i)); (19, OpenMP, #pragma omp teams distribute parallel for simd linear (argc,); (8, OpenMP, #pragma omp teams distribute parallel for simd linear ()); (30, OpenMP, #pragma omp teams distribute parallel for simd linear (S1)); (-2, OpenMP, #pragma omp teams distribute parallel for simd linear); (41, OpenMP, #pragma omp teams distribute parallel for simd linear(e, g)); (-5, OpenMP, #pragma omp target); (-1, OpenMP, #pragma omp teams distribute parallel for simd linear (); (42, OpenMP, #pragma omp teams distribute parallel for simd linear(h))"}
{"predict": "(146, MPI, MPI_Bcast(&size2,      1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (146, MPI, MPI_Bcast(&radius,     1, MPI_INT,           root, MPI_COMM_WORLD);); (322, MPI, MPI_Finalize();); (140, MPI, MPI_Bcast(&lsize2,     1, MPI_INT,           root, MPI_COMM_WORLD);); (138, MPI, MPI_Bcast(&lsize,      1, MPI_INT,           root, MPI_COMM_WORLD);); (139, MPI, MPI_Bcast(&size,       1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (295, MPI, MPI_Reduce(&vector_sum, &check_sum, 1, MPI_DOUBLE, MPI_SUM, root, MPI_COMM_WORLD);); (41, MPI, MPI_Init(&argc,&argv);); (237, MPI, MPI_Barrier(MPI_COMM_WORLD);); (41, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (137, MPI, MPI_Bcast(&iterations, 1, MPI_INT,           root, MPI_COMM_WORLD);); (38, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)", "label": "(134, MPI, MPI_Bcast(&size2,      1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (134, MPI, MPI_Bcast(&radius,     1, MPI_INT,           root, MPI_COMM_WORLD);); (316, MPI, MPI_Finalize();); (129, MPI, MPI_Bcast(&lsize2,     1, MPI_INT,           root, MPI_COMM_WORLD);); (127, MPI, MPI_Bcast(&lsize,      1, MPI_INT,           root, MPI_COMM_WORLD);); (128, MPI, MPI_Bcast(&size,       1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (290, MPI, MPI_Reduce(&vector_sum, &check_sum, 1, MPI_DOUBLE, MPI_SUM, root, MPI_COMM_WORLD);); (41, MPI, MPI_Init(&argc,&argv);); (243, MPI, MPI_Barrier(MPI_COMM_WORLD);); (41, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (126, MPI, MPI_Bcast(&iterations, 1, MPI_INT,           root, MPI_COMM_WORLD);); (38, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(15, OpenMP, #pragma omp target); (16, OpenMP, #pragma omp teams); (17, OpenMP, #pragma omp distribute parallel for); (20, OpenMP, #pragma omp atomic)", "label": "(18, OpenMP, #pragma omp parallel for); (14, OpenMP, #pragma omp target map(tofrom: Inner, Outer, wayout)); (14, OpenMP, #pragma omp teams num_teams(1  ) thread_limit(tl))"}
{"predict": "(6, OpenMP, #pragma omp parallel for firstprivate(length, v_cut))", "label": "(6, OpenMP, #pragma omp parallel for firstprivate(length, v_cut))"}
{"predict": "(138, MPI, ascent_opt[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (22, MPI, MPI_Comm_rank(comm, &par_rank);); (22, MPI, MPI_Comm_size(comm, &par_size);); (169, MPI, ascent_opt[\"mpi_comm\"] = MPI_Comm_f2c(flow::Workspace::default_mpi_comm());); (133, MPI, ascent_opt[\"mpi_comm\"] = MPI_Comm_c2f(MPI_COMM_WORLD);); (195, MPI, MPI_Barrier(comm);); (164, MPI, ascent_opt[\"mpi_comm\"] = MPI_Comm_f2c(comm);)", "label": "(166, MPI, opts[\"mpi_comm\"] = MPI_Comm_c2f(MPI_COMM_WORLD);); (171, MPI, MPI_Barrier(comm);); (21, MPI, MPI_Comm_rank(comm, &par_rank);); (21, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(53, MPI, MPI_Finalize();); (25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (20, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (48, MPI, MPI_T_finalize();); (20, MPI, MPI_T_init_thread(required, &provided);); (32, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (20, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (51, MPI, MPI_Finalize();); (48, MPI, MPI_T_finalize();); (19, MPI, MPI_T_init_thread(required, &provided);); (32, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(71, CUDA, checkCudaErrors( ::cudaGetLastError() );)", "label": "(101, CUDA, checkHIPErrors( ::cudaDeviceSynchronize() );)"}
{"predict": "(33, OpenMP, #pragma omp parallel if(Gc>0.0 && c>0 && Sc>0.0)); (5, OpenMP, #pragma omp parallel); (23, OpenMP, #pragma omp target if(Ga>0.0 && a>0 && Sa>0.0))", "label": "(32, OpenMP, #pragma omp parallel if(Gc>0.0 && c>0 && Sc>0.0)); (22, OpenMP, #pragma omp target if(Ga>0.0 && a>0 && Sa>0.0))"}
{"predict": "(7, CUDA, __shared__ block_radix_sort_t::TempStorage temp_storage;)", "label": "(7, CUDA, __shared__ block_radix_sort_t::TempStorage temp_storage;)"}
{"predict": "(18, OpenMP, #pragma omp target teams distribute if(cond) firstprivate(fp) reduction(+:rd) map(alloc:map))", "label": "(18, OpenMP, #pragma omp target teams distribute if(cond) firstprivate(fp) reduction(+:rd) map(alloc:map))"}
{"predict": "(51, CUDA, thrust::fill(counts_out.begin(), counts_out.end(), 814920U);); (42, CUDA, thrust::device_vector<uint64_t> bins_in(NUM_ITEMS);); (58, CUDA, thrust::host_vector<uint32_t> host_counts(NUM_ITEMS);); (42, CUDA, thrust::device_vector<uint32_t> increments_in(num_increments);); (47, CUDA, thrust::device_vector<uint32_t> counts_out(NUM_ITEMS);)", "label": "(14, CUDA, thrust::host_vector<uint32_t> h_increments(num_increments);); (43, CUDA, thrust::device_vector<uint32_t> increments_in(num_increments);); (53, CUDA, thrust::fill(counts_out.begin(), counts_out.end(), 814920U);); (40, CUDA, thrust::device_vector<uint32_t> bins_in(num_increments);); (41, CUDA, thrust::device_vector<uint32_t> counts_out(NUM_ITEMS);); (47, CUDA, thrust::host_vector<uint32_t> host_counts(num_increments);); (7, CUDA, thrust::host_vector<uint32_t> h_bins(num_increments);)"}
{"predict": "(21, OpenMP, #pragma omp target update from(a[:n])); (12, OpenMP, #pragma omp target teams distribute parallel for); (10, OpenMP, #pragma omp target parallel for)", "label": "(21, OpenMP, #pragma omp target update from(a[:n])); (12, OpenMP, #pragma omp target teams distribute parallel for map(to:b[:n])); (15, OpenMP, #pragma omp target teams distribute parallel for)"}
{"predict": "(28, CUDA, gpuErrchk(cudaFree(d_int));); (4, CUDA, gpuErrchk(cudaPointerGetAttributes(&attributes, d_int));); (1, CUDA, gpuErrchk(cudaMalloc(&d_int, sizeof(int)));)", "label": "(6, CUDA, gpuErrchk(cudaPointerGetAttributes(&attributes, d_int));); (28, CUDA, gpuErrchk(cudaFree(d_int));); (1, CUDA, gpuErrchk(cudaMalloc(&d_int, sizeof(int)));)"}
{"predict": "(2, CUDA, __shared__ float2 cellImagePos;); (2, CUDA, __shared__ bool isContained;); (2, CUDA, __shared__ float4 color;)", "label": "(4, CUDA, __shared__ float3 color;); (2, CUDA, __shared__ bool isContained;); (0, CUDA, __shared__ float2 cellImagePos;)"}
{"predict": "(15, CUDA, checkCudaErrors(cudaGetLastError());)", "label": "(12, CUDA, cudaMemset(d_sums, 0, sizeof(Type) * nkeys * ncols);)"}
{"predict": "(16, OpenMP, #pragma omp section); (11, OpenMP, #pragma omp parallel default(none) shared(a)); (17, OpenMP, int thread_nr = omp_get_thread_num();); (10, OpenMP, #pragma omp sections)", "label": "(11, OpenMP, omp_set_num_threads(nr_threads);); (12, OpenMP, #pragma omp parallel default(none) shared(a)); (13, OpenMP, #pragma omp sections); (7, OpenMP, omp_set_dynamic(0);); (13, OpenMP, #pragma omp section); (15, OpenMP, int thread_nr = omp_get_thread_num();)"}
{"predict": "(10, CUDA, extern __shared__ uint64_t buffer[];)", "label": "(10, CUDA, extern __shared__ uint64_t buffer[];)"}
{"predict": "(2, CUDA, extern __shared__ float shared[];)", "label": "(22, CUDA, __shared__ float shared[forward_threads];)"}
{"predict": "(8, CUDA, cudaDeviceSynchronize();); (8, CUDA, cudaMalloc((void **)&sm_o, sizeof(float) * m * m);); (34, CUDA, cudaFree(sm_o);)", "label": "(12, CUDA, cudaDeviceSynchronize();); (7, CUDA, cudaMalloc((void **)&sm_o, sizeof(float) * m * m);); (34, CUDA, cudaFree(sm_o);)"}
{"predict": "(66, CUDA, CUDA_CHECK(cudaFree(d_AP));); (66, CUDA, CUDA_CHECK(cudaFree(d_A));); (45, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (30, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_AP), sizeof(data_type) * AP.size()));); (57, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (29, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (52, CUDA, CUDA_CHECK(cudaFree(d_A));); (23, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)", "label": "(46, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (32, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_AP), sizeof(data_type) * AP.size()));); (63, CUDA, CUDA_CHECK(cudaDeviceReset());); (60, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (30, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (53, CUDA, CUDA_CHECK(cudaFree(d_AP));); (53, CUDA, CUDA_CHECK(cudaFree(d_A));); (22, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(49, OpenCL, OCL_CALL(clReleaseEvent, exec_event);); (49, OpenCL, OCL_CALL(clReleaseCommandQueue, profiling_queue);); (10, OpenCL, profiling_queue = clCreateCommandQueue(ctx, devices[0], 0, &status);)", "label": "(40, OpenCL, clReleaseEvent(exec_event);); (11, OpenCL, profiling_queue = clCreateCommandQueue(ctx, device, CL_QUEUE_PROFILING_ENABLE, &status);); (47, OpenCL, clReleaseCommandQueue(profiling_queue);)"}
{"predict": "(140, MPI, MPI_Win_unlock_all(win);); (150, MPI, MPI_Barrier(comm);); (148, MPI, MPI_Win_flush_all(win);); (20, MPI, MPI_Comm_size(comm, &size);); (78, MPI, MPI_Win_lock(MPI_LOCK_SHARED, target_rank, MPI_MODE_NOCHECK, win);); (133, MPI, MPI_Win_flush_local_all(win);); (130, MPI, MPI_Win_flush_local(target_rank, win);); (73, MPI, MPI_Barrier(comm);); (134, MPI, MPI_Win_unlock_all(win);); (74, MPI, MPI_Win_flush(target_rank, win);); (12, MPI, MPI_Comm_rank(comm, &rank);); (67, MPI, MPI_Win_lock_all(0, win);)", "label": "(163, MPI, MPI_Win_unlock(rank, win);); (22, MPI, MPI_Comm_size(comm, &size);); (125, MPI, MPI_Barrier(comm);); (134, MPI, MPI_Win_unlock(target_rank, win);); (135, MPI, MPI_Win_unlock_all(win);); (76, MPI, MPI_Win_lock(MPI_LOCK_SHARED, target_rank, 0, win);); (120, MPI, MPI_Win_flush_all(win);); (76, MPI, MPI_Win_lock_all(0, win);); (150, MPI, MPI_Win_lock(MPI_LOCK_SHARED, rank, 0, win);); (111, MPI, MPI_Win_flush_local_all(win);); (12, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(38, CUDA, return cudaGetLastError();)", "label": "(39, CUDA, return cudaGetLastError();)"}
{"predict": "(19, MPI, MPIT_result = MPI_T_pvar_session_create(&session);); (12, MPI, MPIT_result = MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);)", "label": "(19, MPI, MPIT_result = MPI_T_pvar_session_create(&session);); (12, MPI, MPIT_result = MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);)"}
{"predict": "(77, CUDA, getLastCudaError(\"Kernel execution failed\");); (32, CUDA, checkCudaErrors(cudaMalloc((void **) &dData, size));); (127, CUDA, checkCudaErrors(cudaFreeArray(cuArray));); (64, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (125, CUDA, checkCudaErrors(cudaFree(dData));); (53, CUDA, checkCudaErrors(cudaBindTextureToArray(tex, cuArray));)", "label": "(127, CUDA, checkCudaErrors(cudaFreeArray(cuArray));); (33, CUDA, checkCudaErrors(cudaMalloc((void **) &dData, size));); (56, CUDA, checkCudaErrors(cudaBindTextureToArray(tex, cuArray, channelDesc));); (63, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (71, CUDA, getLastCudaError(\"Kernel execution failed\");); (-3, CUDA, int devID = findCudaDevice(argc, (const char **) argv);); (120, CUDA, checkCudaErrors(cudaFree(dData));)"}
{"predict": "(7, MPI, MPI_Comm_group(cFile, &group);); (10, MPI, MPI_Group_free(&group);); (4, MPI, MPI_Comm_set_group(cFile, wgroup);); (0, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (10, MPI, MPI_Group_free(&wgroup);); (2, MPI, MPI_Comm_group(MPI_COMM_WORLD, &wgroup);); (5, MPI, MPI_Group_compare(group, wgroup, &result);)", "label": "(9, MPI, MPI_Group_compare(group, wgroup, &result);); (5, MPI, MPI_File_get_group(cFile, &group);); (0, MPI, MPI_File cFile = MPI_File_f2c(*file);); (4, MPI, MPI_Comm_group(MPI_COMM_WORLD, &wgroup);); (12, MPI, MPI_Group_free(&wgroup);); (10, MPI, MPI_Group_free(&group);)"}
{"predict": "(13, CUDA, cudaStreamCreate(&streams[i]);); (73, CUDA, cudaCheckErrors(\"Kernel c failed\");); (56, CUDA, cudaCheckErrors(\"Kernel b failed\");); (33, CUDA, cudaMalloc((void**) &d_x, N * sizeof(float));); (34, CUDA, cudaCheckErrors(\"cudaMalloc failed\");); (65, CUDA, cudaCheckErrors(\"Kernel d failed\");); (91, CUDA, cudaDeviceSynchronize();); (106, CUDA, cudaMemcpyAsync(h_y, d_y, N, cudaMemcpyDeviceToHost, streams[0]);); (70, CUDA, cudaCheckErrors(\"Launching graph failed\");); (59, CUDA, cudaCheckErrors(\"Event record failed\");); (60, CUDA, cudaStreamWaitEvent(streams[1], event1);); (64, CUDA, cudaStreamWaitEvent(streams[0], event2);); (46, CUDA, cudaStreamBeginCapture(streams[0], cudaStreamCaptureModeGlobal);); (60, CUDA, cudaCheckErrors(\"Event wait failed\");); (122, CUDA, cudaCheckErrors(\"Finishing memcpy failed\");); (55, CUDA, cudaCheckErrors(\"Stream capture end failed\");); (35, CUDA, cudaCheckErrors(\"Kernel a failed\");); (29, CUDA, cudaMemcpy(d_y, h_y, N, cudaMemcpyHostToDevice);); (30, CUDA, cudaCheckErrors(\"cudaMemcpy failed\");); (25, CUDA, cudaMemcpy(d_x, h_x, N, cudaMemcpyHostToDevice);); (55, CUDA, cudaEventRecord(event1, streams[1]);); (10, CUDA, cudaMalloc((void**) &d_y, N * sizeof(float));); (68, CUDA, cudaGraphGetNodes(graph, nodes, &numNodes);); (13, CUDA, cudaCheckErrors(\"cudaMalloc failed\");); (46, CUDA, cudaStreamEndCapture(streams[0], &graph);); (63, CUDA, cudaGraphInstantiate(graph, NULL, NULL, 0);); (60, CUDA, cudaEventRecord(event2, streams[0]);); (118, CUDA, cudaCheckErrors(\"Finishing memcpy failed\");", "label": "(75, CUDA, cudaCheckErrors(\"Kernel c failed\");); (99, CUDA, cudaCheckErrors(\"Launching graph failed\");); (13, CUDA, cudaStreamCreateWithFlags(&streams[i], cudaStreamNonBlocking);); (63, CUDA, cudaCheckErrors(\"Event record failed\");); (56, CUDA, cudaCheckErrors(\"Stream begin capture failed\");); (31, CUDA, cudaMalloc((void**) &d_x, N * sizeof(float));); (44, CUDA, cudaGraphCreate(FIXME, 0);); (62, CUDA, cudaCheckErrors(\"Kernel b failed\");); (104, CUDA, cudaDeviceSynchronize();); (98, CUDA, cudaGraphGetNodes(graph, nodes, &numNodes);); (121, CUDA, cudaMemcpy(h_y, d_y, N, cudaMemcpyDeviceToHost);); (73, CUDA, cudaCheckErrors(\"Kernel d failed\");); (89, CUDA, cudaStreamSynchronize(streams[0]);); (58, CUDA, cudaStreamWaitEvent(streams[1], event1);); (72, CUDA, cudaStreamEndCapture(streams[0], &graph);); (48, CUDA, cudaCheckErrors(\"Kernel a failed\");); (64, CUDA, cudaStreamWaitEvent(streams[0], event2);); (-11, CUDA, cudaEventCreateWithFlags(&event1, cudaEventDisableTiming);); (-11, CUDA, cudaEventCreateWithFlags(&event2, cudaEventDisableTiming);); (53, CUDA, cudaCheckErrors(\"Event wait failed\");); (88, CUDA, cudaCheckErrors(\"Graph get nodes failed\");); (44, CUDA, cudaEventRecord(event1, streams[0]);); (19, CUDA, cudaMemcpy(d_y, h_y, N, cudaMemcpyHostToDevice);); (54, CUDA, cudaEventRecord(event2, streams[1]);); (16, CUDA, cudaMemcpy(d_x, h_x, N, cudaMemcpyHostToDevice);); (107, CUDA, cudaCheckErrors(\"Finishing memcpy failed\");); (11, CUDA, cudaMalloc((void**) &d_y, N * sizeof(float));); (32, CUDA, cudaStreamBeginCapture(streams[0], cudaStreamCaptureModeGlobal);); (91, CUDA, cudaGraphLaunch(instance, streams[0]);); (63, CUDA, cudaCheckErrors(\"instantiating graph failed\");); (8, CUDA, cudaCheckErrors(\"cudaMalloc failed\");); (56, CUDA, cudaCheckErrors(\"Stream end capture failed\");); (59, CUDA, cudaGraphInstantiate(FIXME, graph, NULL, NULL, 0);); (66, CUDA, cudaGraphLaunch(FIXME, streams[0]);)"}
{"predict": "(19, MPI, MPI_Reduce(outbuf, inbuf, count, type, op, 0, group->comm);)", "label": "(19, MPI, MPI_Reduce_local((void*)inbuf, outbuf, count, type, op);)"}
{"predict": "(34, CUDA, __shared__ _VOLATILE_ float aSmem2[8][128+4];); (32, CUDA, __shared__ _VOLATILE_ float bSmem1[8][128+4];); (33, CUDA, __shared__ _VOLATILE_ float bSmem2[8][128+4];); (30, CUDA, __shared__ _VOLATILE_ float aSmem1[8][128+4];)", "label": "(35, CUDA, __shared__ _VOLATILE_ float aSmem2[8][128+4];); (33, CUDA, __shared__ _VOLATILE_ float bSmem1[8][128+4];); (34, CUDA, __shared__ _VOLATILE_ float bSmem2[8][128+4];); (30, CUDA, __shared__ _VOLATILE_ float aSmem1[8][128+4];)"}
{"predict": "(3, CUDA, extern __shared__ unsigned sfdatx[];)", "label": "(3, CUDA, extern __shared__ unsigned sfdatx[];)"}
{"predict": "(5, MPI, MPI_Init(&argc, &argv);); (42, MPI, MPI_Finalize();)", "label": "(9, MPI, test_integration_sin<2,1,double> t1( mpi.vm()[\"hsize\"].as<double>() );); (31, MPI, test_integration_sin<2,4,double> t2( mpi.vm()[\"hsize\"].as<double>() );); (14, MPI, test_integration_sin<2,2,double> t2( mpi.vm()[\"hsize\"].as<double>() );); (2, MPI, std::cout << \"Order = \" << mpi.vm()[\"order\"].as<int>() << \" / \" << FEELPP_MESH_MAX_ORDER << \"\\n\";); (20, MPI, test_integration_sin<2,3,double> t2( mpi.vm()[\"hsize\"].as<double>() );); (37, MPI, test_integration_sin<2,5,double> t2( mpi.vm()[\"hsize\"].as<double>() );)"}
{"predict": "(15, CUDA, FastllmCudaFree(pointers);); (9, CUDA, uint8_t ** pointers = (uint8_t**)FastllmCudaMalloc(sizeof(uint8_t*) * part);); (12, CUDA, cudaMemcpy(pointers, cpuPointers, sizeof(uint8_t*) * part, cudaMemcpyHostToDevice);)", "label": "(16, CUDA, FastllmCudaFree(pointers);); (12, CUDA, cudaMemcpy(pointers, cpuPointers, sizeof(uint8_t*) * part, cudaMemcpyHostToDevice);); (6, CUDA, uint8_t ** pointers = (uint8_t**)FastllmCudaMalloc(sizeof(uint8_t*) * part);)"}
{"predict": "(6, MPI, MPI_Init(&argc, &argv);); (64, MPI, MPI_Finalize();); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (16, MPI, errs = MPI_Wait(&request, MPI_STATUS_IGNORE);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (15, MPI, errs = MPI_Send(\"No Errors\", 10, MPI_CHAR, 3, 0, MPI_COMM_WORLD);); (22, MPI, errs = MPI_Recv(buf, 10, MPI_CHAR, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (36, MPI, errs = MPI_Send(\"No Errors\", 10, MPI_CHAR, 0, 0, MPI_COMM_WORLD);); (22, MPI, errs = MPI_Irecv(buf, 10, MPI_CHAR, 0, 0, MPI_COMM_WORLD, &request);); (4, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(30, MPI, errs = MPI_Irecv(buf, 10, MPI_CHAR, 0, 0, MPI_COMM_WORLD, &request);); (5, MPI, MPI_Init(&argc, &argv);); (65, MPI, MPI_Finalize();); (18, MPI, errs = MPI_Issend(\"No Errors\", 10, MPI_CHAR, 3, 0, MPI_COMM_WORLD, &request);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (16, MPI, errs += MPI_Wait(&request, MPI_STATUS_IGNORE);); (2, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (5, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (34, MPI, errs = MPI_Issend(\"No Errors\", 10, MPI_CHAR, 2, 0, MPI_COMM_WORLD, &request);)"}
{"predict": "(16, OpenMP, #pragma omp target teams distribute parallel for if(target: i<5) nowait depend(in: global)); (5, OpenMP, #pragma omp target teams distribute parallel for if(target: i<5) nowait); (1, OpenMP, #pragma omp target teams distribute parallel for nowait); (8, OpenMP, #pragma omp target teams distribute parallel for if(target: i<5) nowait depend(inout: global)); (13, OpenMP, #pragma omp target teams distribute parallel for if(target: i<5) nowait depend(in: global) depend(out: global)); (0, OpenMP, #pragma omp target teams distribute parallel for nowait depend(out: global)); (8, OpenMP, #pragma omp target teams distribute parallel for if(target: i<5) nowait depend(in: global) depend(in: global))", "label": "(7, OpenMP, #pragma omp target teams distribute parallel for simd order(); (15, OpenMP, #pragma omp target teams distribute parallel for simd order(concurrent)); (-1, OpenMP, #pragma omp target teams distribute parallel for simd nowait nowait); (7, OpenMP, #pragma omp target teams distribute parallel for simd order(none); (9, OpenMP, #pragma omp target teams distribute parallel for simd order(concurrent); (-1, OpenMP, #pragma omp target teams distribute parallel for simd order)"}
{"predict": "(24, CUDA, CHECK_CUDA_RETURN(cudaGetLastError());)", "label": "(24, CUDA, CHECK_CUDA_RETURN(cudaGetLastError());)"}
{"predict": "(8, TBB, tbb::tick_count t2 = tbb::tick_count::now();); (3, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (3, TBB, tbb::tick_count t1 = tbb::tick_count::now();); (10, TBB, ASSERT( num_threads > 0, \"tbb::thread::hardware_concurrency() has returned an incorrect value\" );); (8, TBB, int num_threads = tbb::tbb_thread::hardware_concurrency();)", "label": "(7, TBB, tbb::tick_count t1 = tbb::tick_count::now();); (14, TBB, ASSERT( num_threads > 0, \"tbb::thread::hardware_concurrency() has returned an incorrect value\" );); (3, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (11, TBB, int num_threads = tbb::tbb_thread::hardware_concurrency();); (5, TBB, tbb::tick_count t2 = tbb::tick_count::now();)"}
{"predict": "(267, OpenMP, #pragma omp distribute parallel for simd lastprivate(ii)); (32, OpenMP, #pragma omp distribute parallel for simd); (5, OpenMP, #pragma omp target); (270, OpenMP, #pragma omp distribute parallel for simd firstprivate(ii)); (2, OpenMP, #pragma omp distribute parallel for simd); (322, OpenMP, #pragma omp distribute parallel for simd collapse(2)); (4, OpenMP, #pragma omp teams); (230, OpenMP, #pragma omp distribute parallel for simd private(ii))", "label": "(311, OpenMP, #pragma omp distribute parallel for simd firstprivate(ii)); (332, OpenMP, #pragma omp distribute parallel for simd lastprivate(ii)); (6, OpenMP, #pragma omp distribute parallel for simd); (322, OpenMP, #pragma omp distribute parallel for simd private(ii)); (313, OpenMP, #pragma omp distribute parallel for simd linear(ii)); (1, OpenMP, #pragma omp target); (347, OpenMP, #pragma omp distribute parallel for simd collapse(2)); (0, OpenMP, #pragma omp teams)"}
{"predict": "(27, CUDA, cudaFree(d_Z_);); (30, CUDA, cudaFree(d_V_);); (33, CUDA, cudaFree(d_info_M_);); (18, CUDA, cudaFree(d_X_);); (21, CUDA, cudaFree(d_M_);); (23, CUDA, cudaFree(d_L_);); (25, CUDA, cudaFree(d_U_);); (12, CUDA, cudaFree(d_y_);); (12, CUDA, cudaFree(d_res_);)", "label": "(17, CUDA, cudaFree(devx_);); (27, CUDA, cudaFree(d_T_);); (24, CUDA, cudaFree(d_P_);); (24, CUDA, cudaFree(d_Q_);); (12, CUDA, cudaFree(devr_);); (1, CUDA, cudaFree(d_work_);)"}
{"predict": "(8, OpenMP, num_teams[j] = omp_get_num_teams();); (8, OpenMP, num_threads[j] = omp_get_num_threads();); (6, OpenMP, #pragma omp target teams distribute parallel for)", "label": "(7, OpenMP, printf(\"#pragma omp target teams distribute parallel for thread_limit(4)\\n\");); (10, OpenMP, num_teams[j] = omp_get_num_teams();); (13, OpenMP, printf(\"#pragma omp target teams distribute parallel for\\n\");); (7, OpenMP, num_threads[j] = omp_get_num_threads();); (4, OpenMP, #pragma omp target teams distribute parallel for)"}
{"predict": "(18, OpenCL, clReleaseContext_ExpectAndReturn(make_context(0), CL_SUCCESS);)", "label": "(19, OpenCL, clReleaseContext_ExpectAndReturn(make_context(0), CL_SUCCESS);)"}
{"predict": "(64, MPI, *time -= MPI_Wtime();); (42, MPI, MPI_Type_size(c_info->s_data_type, &s_size);); (73, MPI, *time += MPI_Wtime();); (42, MPI, MPI_Type_size(c_info->r_data_type, &r_size);)", "label": "(63, MPI, *time -= MPI_Wtime();); (43, MPI, MPI_Type_size(c_info->r_data_type, &r_size);); (41, MPI, MPI_Type_size(c_info->s_data_type, &s_size);); (73, MPI, *time += MPI_Wtime();)"}
{"predict": "(13, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(exe_node), receivers[r] );); (27, TBB, tbb::flow::remove_edge( tbb::flow::output_port<0>(exe_node), receivers[r] );); (3, TBB, tbb::flow::multifunction_node< InputType, OutputTuple, tbb::flow::rejecting > exe_node( g, tbb::flow::unlimited, body );); (3, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(exe_node), *reinterpret_cast< tbb::flow::sender<InputType>* >(&exe_node) );); (-1, TBB, tbb::flow::graph g;)", "label": "(14, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(exe_node), receivers[r] );); (1, TBB, typedef typename tbb::flow::tuple_element<0,OutputTuple>::type OutputType;); (27, TBB, tbb::flow::remove_edge( tbb::flow::output_port<0>(exe_node), receivers[r] );); (3, TBB, tbb::flow::multifunction_node< InputType, OutputTuple, tbb::flow::rejecting > exe_node( g, tbb::flow::unlimited, body );); (1, TBB, tbb::flow::graph g;)"}
{"predict": "(11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &nodeID);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nr_nodes);); (73, MPI, MPI_Allgather(&local_n0, 1, MPI_INT, slice_width, 1, MPI_INT, MPI_COMM_WORLD);)", "label": "(11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &nodeID);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nr_nodes);); (73, MPI, MPI_Allgather(&local_n0, 1, MPI_INT, slice_width, 1, MPI_INT, MPI_COMM_WORLD);)"}
{"predict": "(286, OpenMP, #pragma omp teams distribute simd lastprivate(ii)); (222, OpenMP, #pragma omp teams distribute simd collapse(2)); (230, OpenMP, #pragma omp teams distribute simd firstprivate(ii)); (2, OpenMP, #pragma omp teams distribute simd); (22, OpenMP, #pragma omp teams distribute simd linear(ii)); (3, OpenMP, #pragma omp target)", "label": "(266, OpenMP, #pragma omp teams distribute simd firstprivate(ii)); (6, OpenMP, #pragma omp teams distribute simd); (270, OpenMP, #pragma omp teams distribute simd private(ii)); (275, OpenMP, #pragma omp teams distribute simd lastprivate(ii)); (287, OpenMP, #pragma omp teams distribute simd collapse(2)); (1, OpenMP, #pragma omp target)"}
{"predict": "(84, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (84, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)", "label": "(84, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (84, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)"}
{"predict": "(6, OpenMP, #pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp parallel for)"}
{"predict": "(8, MPI, dray::dray::mpi_comm(MPI_Comm_c2f(comm));)", "label": "(20, MPI, int rank = dray::dray::mpi_rank();); (9, MPI, dray::dray::mpi_comm(MPI_Comm_c2f(comm));); (19, MPI, int size = dray::dray::mpi_size();)"}
{"predict": "(106, MPI, MPI_Barrier(MPI_COMM_WORLD);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (104, MPI, MPI_Barrier(MPI_COMM_WORLD);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(251, MPI, MPI_Waitall( num_procs_to_send, send_reqs, MPI_STATUSES_IGNORE);); (252, MPI, MPI_Waitall( num_procs_to_recv, recv_reqs, MPI_STATUSES_IGNORE);); (160, MPI, MPI_Barrier( grid3d->comm );); (350, MPI, MPI_Bcast( B, m_loc * nrhs, MPI_DOUBLE, 0, grid3d->zscp.comm);); (129, MPI, MPI_Allgather( &m_loc, 1, MPI_INT, row_counts_int, 1, MPI_INT, grid3d->comm );); (241, MPI, MPI_Waitall( num_procs_to_send, send_reqs, MPI_STATUSES_IGNORE);); (118, MPI, MPI_Allgather( &m_loc, 1, MPI_INT, b_counts_int, 1, MPI_INT, grid3d->comm );); (246, MPI, MPI_Recv( NULL, 0, MPI_BYTE, src, tag, grid3d->comm, &recv_status );); (107, MPI, MPI_Alltoall( &m_loc, 1, MPI_INT,procs_to_send_list, 1, MPI_INT, grid3d->comm );); (107, MPI, MPI_Alltoall( &m_loc, 1, MPI_INT,procs_recv_from_list, 1, MPI_INT, grid3d->comm );); (325, MPI, MPI_Bcast( Btmp, m_loc * nrhs, MPI_DOUBLE, 0, grid3d->zscp.comm);); (115, MPI, MPI_Allgather( &m_loc, 1, MPI_INT, recv_count_list, 1, MPI_INT, grid3d->comm );); (104, MPI, MPI_Allgather( &m_loc, 1, MPI_INT, send_count_list, 1, MPI_INT, grid3d->comm );); (233, MPI, MPI_Send( NULL", "label": "(270, MPI, MPI_Wait(&recv_req, &recv_status);)"}
{"predict": "(21, MPI, MPI_Type_get_extent(keysat, &keysat_lb, &keysat_extent);); (23, MPI, MPI_Type_get_true_extent(key, &key_true_lb, &key_true_extent);); (100, MPI, MPI_Allreduce(MPI_IN_PLACE, splitters, ranks * serial_search_threshold, type_int_with_key, MPI_BYTE, lwgcomm);)", "label": "(24, MPI, MPI_Type_get_true_extent(key, &key_true_lb, &key_true_extent);); (204, MPI, MPI_Type_free(&type_int_with_key);); (280, MPI, MPI_Comm_rank(comm, &comm_rank);); (17, MPI, MPI_Type_get_extent(keysat, &keysat_lb, &keysat_extent);)"}
{"predict": "(14, MPI, MPI_Comm_call_errhandler( comm, MPI_ERR_OTHER );); (7, MPI, MPI_Comm_set_errhandler( comm, newerr );); (13, MPI, MPI_Comm_free_errhandler( &newerr );); (3, MPI, MPI_Comm_create_errhandler( myerrors, &newerr );)", "label": "(10, MPI, MPI_Comm_call_errhandler( comm, MPI_ERR_OTHER );); (8, MPI, MPI_Comm_set_errhandler( comm, newerr );); (14, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_create_errhandler( eh, &newerr );); (1, MPI, MPI_Init( &argc, &argv );); (6, MPI, MPI_Errhandler_free( &newerr );)"}
{"predict": "(11, OpenMP, #pragma omp parallel for)", "label": "(11, OpenMP, #pragma omp parallel for)"}
{"predict": "(32, MPI, MPI_Comm_free(&merge1);); (35, MPI, MPI_Comm_free(&inter);); (28, MPI, err = MPI_Barrier(inter);); (7, MPI, MPI_Comm_rank(parent, &rank);); (8, MPI, MPI_Comm_split(parent, first, rank, &intra);); (25, MPI, err = MPI_Intercomm_merge(inter, first, &merge1);); (30, MPI, MPI_Comm_free(&intra);); (20, MPI, MPI_Barrier(MPI_COMM_WORLD);); (8, MPI, err = MPI_Intercomm_create(intra, 0, intra, MPI_COMM_NULL, tag, &inter);)", "label": "(38, MPI, MPI_Comm_disconnect(&parent);); (27, MPI, err = MPI_Intercomm_merge(inter, 0, &merge1);); (34, MPI, MPI_Comm_free(&intra);); (31, MPI, MPI_Comm_free(&merge1);); (6, MPI, err = MPI_Intercomm_merge(parent, 1, &intra);); (30, MPI, MPI_Comm_free(&inter);); (25, MPI, err = MPI_Barrier(merge1);); (18, MPI, err = MPI_Barrier(inter);); (21, MPI, MPI_Comm_rank(merge1, &rank);); (7, MPI, err = MPI_Intercomm_create(intra, 0, MPI_COMM_NULL, 0, tag, &inter);); (1, MPI, MPI_Comm_rank(intra, &rank);); (10, MPI, err = MPI_Intercomm_create(MPI_COMM_WORLD, 0, intra, 0, tag, &inter);)"}
{"predict": "(132, MPI, MPI_Type_free(&send_type);); (20, MPI, MPI_Alltoall(send_size, 1, MPI_COUNT, recv_size, 1, MPI_COUNT, fd->comm);); (130, MPI, MPI_Type_free(&recv_type);); (133, MPI, MPI_Send(send_buf, send_count, MPI_BYTE, i, ADIOI_COLL_TAG(i, iter), fd->comm);); (103, MPI, MPI_Waitall(nprocs_send + nprocs_recv, requests, statuses);)", "label": "(94, MPI, MPI_Type_commit(&send_type);); (137, MPI, MPI_Waitall(nprocs_send, requests + nprocs_recv, MPI_STATUSES_IGNORE);); (19, MPI, MPI_Alltoall(send_size, 1, MPI_COUNT, recv_size, 1, MPI_COUNT, fd->comm);); (118, MPI, MPI_Get_count_c(&statuses[j], MPI_BYTE, &count_recved);); (93, MPI, MPI_Type_free(&send_type);)"}
{"predict": "(87, OpenMP, #pragma omp task); (88, OpenMP, #pragma omp taskgroup)", "label": "(101, OpenMP, #pragma omp task); (102, OpenMP, #pragma omp taskgroup)"}
{"predict": "(17, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize());); (15, CUDA, qmc_cuda::cuda_check(cudaGetLastError());)", "label": "(17, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize());); (15, CUDA, qmc_cuda::cuda_check(cudaGetLastError());)"}
{"predict": "(30, MPI, MPI_Finalize();); (9, MPI, MPI_Get_processor_name(mach_name,&mach_len);); (23, MPI, MPI_Allreduce(&local,&global,1,MPI_INT,MPI_SUM,MPI_COMM_WORLD);); (17, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(28, MPI, MPI_Finalize();); (8, MPI, MPI_Get_processor_name(mach_name,&mach_len);); (22, MPI, MPI_Allreduce(&local,&global,1,MPI_INT,MPI_SUM,MPI_COMM_WORLD);)"}
{"predict": "(21, OpenMP, #pragma omp distribute parallel for if (parallel: Arg)); (0, OpenMP, #pragma omp target); (1, OpenMP, #pragma omp distribute parallel for if (true)); (19, OpenMP, #pragma omp distribute parallel for if (false)); (6, OpenMP, #pragma omp distribute parallel for if (Arg)); (-4, OpenMP, #pragma omp teams)", "label": "(3, OpenMP, #pragma omp distribute parallel for if (true)); (11, OpenMP, #pragma omp distribute parallel for if (false)); (19, OpenMP, #pragma omp distribute parallel for if (Arg)); (-2, OpenMP, #pragma omp target); (-2, OpenMP, #pragma omp teams)"}
{"predict": "(3, OpenMP, #pragma omp parallel for)", "label": "(3, OpenMP, #pragma omp parallel for)"}
{"predict": "(19, OpenCL, clReleaseEvent(event);)", "label": "(19, OpenCL, clReleaseEvent(event);)"}
{"predict": "(10, MPI, MPI_Init(&argc, &argv);); (10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (58, MPI, MPI_Finalize();); (126, MPI, MPI_Comm_split( MPI_COMM_WORLD, world_mpi_rank!= world_server_mpi_rank, 0, &file_mpi_comm);); (175, MPI, MPI_Allreduce(MPI_IN_PLACE, &nerrors, 1, MPI_INT, MPI_MAX, MPI_COMM_WORLD);); (123, MPI, MPI_Comm_free(&file_mpi_comm);)", "label": "(10, MPI, MPI_Init(&argc, &argv);); (206, MPI, MPI_Finalize();); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (185, MPI, MPI_Barrier(MPI_COMM_WORLD);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);)"}
{"predict": "(9, OpenMP, #pragma omp target teams distribute parallel for firstprivate(t_var, vec, s_arr, var)); (7, OpenMP, #pragma omp target enter data map(to : t_var) nowait); (13, OpenMP, #pragma omp target exit data map(from : t_var) nowait)", "label": "(8, OpenMP, #pragma omp master taskloop lastprivate(t_var, vec, s_arr, s_arr, var, var))"}
{"predict": "(16, OpenMP, #pragma omp target exit data map(from : a)); (10, OpenMP, #pragma omp target teams distribute parallel for); (8, OpenMP, #pragma omp target enter data map(to : a))", "label": "(11, OpenMP, #pragma omp target teams distribute parallel for map(tofrom : a[ : n]) map(to : b[ : n]))"}
{"predict": "(8, OpenMP, #pragma omp target update CLAUSE(present: i)); (6, OpenMP, #pragma omp target enter data map(alloc: i))", "label": "(8, OpenMP, #pragma omp target update CLAUSE(present: i)); (6, OpenMP, #pragma omp target enter data map(alloc: i)); (7, OpenMP, #pragma omp target exit data map(delete: i))"}
{"predict": "(151, OpenMP, #pragma omp for collapse(2))", "label": "(163, OpenMP, #pragma omp for collapse(2))"}
{"predict": "(58, OpenMP, #pragma omp distribute parallel for firstprivate(h)); (49, OpenMP, #pragma omp distribute parallel for firstprivate(argv[1])); (70, OpenMP, #pragma omp distribute parallel for lastprivate(g) firstprivate(g)); (8, OpenMP, #pragma omp distribute parallel for firstprivate); (6, OpenMP, #pragma omp target); (32, OpenMP, #pragma omp distribute parallel for firstprivate(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (33, OpenMP, #pragma omp distribute parallel for firstprivate(S1)); (63, OpenMP, #pragma omp parallel private(i)); (67, OpenMP, #pragma omp distribute parallel for firstprivate(j)); (37, OpenMP, #pragma omp distribute parallel for firstprivate(a, b)); (4, OpenMP, #pragma omp distribute parallel for firstprivate(); (14, OpenMP, #pragma omp distribute parallel for firstprivate(argc); (1, OpenMP, #pragma omp distribute parallel for firstprivate()); (61, OpenMP, #pragma omp parallel); (37, OpenMP, #pragma omp distribute parallel for firstprivate(k, e, g)); (6, OpenMP, #pragma omp distribute parallel for firstprivate(argc,); (66, OpenMP, #pragma omp distribute parallel for firstprivate(i)); (50, OpenMP, #pragma omp distribute parallel for firstprivate(i)); (42, OpenMP, #pragma omp distribute parallel for firstprivate(B::x)); (63, OpenMP, #pragma omp parallel shared(i)); (13, OpenMP, #pragma omp distribute parallel for firstprivate(argc > 0? argv[1] : argv[2])); (5, OpenMP, #pragma omp distribute parallel for firstprivate(argc)", "label": "(38, OpenMP, #pragma omp distribute parallel for firstprivate(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (78, OpenMP, #pragma omp parallel private(i)); (56, OpenMP, #pragma omp distribute parallel for firstprivate(e, g)); (30, OpenMP, #pragma omp distribute parallel for firstprivate(argc > 0? argv[1] : argv[2])); (68, OpenMP, #pragma omp distribute parallel for firstprivate(i)); (1, OpenMP, #pragma omp target); (57, OpenMP, #pragma omp distribute parallel for firstprivate(k, h)); (59, OpenMP, #pragma omp parallel); (40, OpenMP, #pragma omp distribute parallel for firstprivate(a, b)); (93, OpenMP, #pragma omp parallel reduction(+ : i)); (3, OpenMP, #pragma omp distribute parallel for firstprivate(); (71, OpenMP, #pragma omp distribute parallel for firstprivate(j)); (41, OpenMP, #pragma omp distribute parallel for firstprivate(argv[1])); (5, OpenMP, #pragma omp distribute parallel for firstprivate()); (9, OpenMP, #pragma omp distribute parallel for firstprivate(argc); (78, OpenMP, #pragma omp distribute parallel for lastprivate(g) firstprivate(g)); (27, OpenMP, #pragma omp distribute parallel for firstprivate(S1)); (11, OpenMP, #pragma omp distribute parallel for firstprivate(argc,); (60, OpenMP, #pragma omp parallel shared(i)); (-11, OpenMP, #pragma omp distribute parallel for firstprivate); (-13, OpenMP, #pragma omp teams)"}
{"predict": "(18, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (18, MPI, MPI_Comm_size(MPI_COMM_WORLD, &world_size);)", "label": "(25, MPI, MPI_Comm_free(&comms[x]);); (18, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &comms[x]);)"}
{"predict": "(4, OpenCL, clReleaseDevice_ExpectAndReturn(make_device_id(1), CL_SUCCESS);); (2, OpenCL, clReleaseDevice_ExpectAndReturn(make_device_id(0), CL_SUCCESS);)", "label": "(5, OpenCL, clReleaseContext_ExpectAndReturn(make_context(0), CL_SUCCESS);); (3, OpenCL, clCreateContext_StubWithCallback(clCreateContext_testContextNonNullProperties);)"}
{"predict": "(37, OpenMP, #pragma omp sections firstprivate(0)); (23, OpenMP, #pragma omp sections firstprivate()); (2, OpenMP, #pragma omp sections firstprivate(); (10, OpenMP, #pragma omp sections firstprivate(,); (48, OpenMP, #pragma omp sections firstprivate(x, y)); (41, OpenMP, #pragma omp sections firstprivate(x)); (14, OpenMP, #pragma omp sections firstprivate(, )); (33, OpenMP, #pragma omp sections firstprivate(int)); (37, OpenMP, #pragma omp sections firstprivate(x, y, z)); (-5, OpenMP, #pragma omp parallel); (46, OpenMP, #pragma omp sections lastprivate(x) firstprivate(x)); (1, OpenMP, #pragma omp sections firstprivate(,); (15, OpenMP, #pragma omp sections firstprivate(int)); (47, OpenMP, #pragma omp sections lastprivate(x, y, z) firstprivate(x, y, z)); (22, OpenMP, #pragma omp sections firstprivate(foo: x))", "label": "(5, OpenMP, #pragma omp sections firstprivate(); (24, OpenMP, #pragma omp sections firstprivate()); (35, OpenMP, #pragma omp sections firstprivate(0)); (51, OpenMP, #pragma omp sections lastprivate(x, y, z) firstprivate(x, y, z)); (45, OpenMP, #pragma omp sections lastprivate(x, y) firstprivate(x, y)); (39, OpenMP, #pragma omp sections lastprivate(x) firstprivate(x)); (-4, OpenMP, #pragma omp parallel); (6, OpenMP, #pragma omp sections firstprivate(,); (11, OpenMP, #pragma omp sections firstprivate(, )); (22, OpenMP, #pragma omp sections firstprivate(int))"}
{"predict": "(3, OpenMP, #pragma omp target enter data map(to: i)); (4, OpenMP, #pragma omp target enter data map(always, to: i)); (3, OpenMP, #pragma omp target enter data map(delete: i)); (3, OpenMP, #pragma omp target enter data map(release: i)); (2, OpenMP, #pragma omp target enter data map(to, always, delete, release: i))", "label": "(10, OpenMP, #pragma omp target update to(i) nowait (3.14) device (-10u)); (3, OpenMP, #pragma omp target nowait update to(i)); (7, OpenMP, #pragma omp target update to(i) nowait device (-10u)); (5, OpenMP, #pragma omp target update to(i) nowait (argc))); (7, OpenMP, #pragma omp target update to(i) nowait nowait); (7, OpenMP, #pragma omp target update nowait to(i) nowait); (-3, OpenMP, #pragma omp nowait target update to(i)); (-1, OpenMP, #pragma omp target update nowait() to(i)); (-1, OpenMP, #pragma omp target update to(i) nowait()"}
{"predict": "(7, OpenACC, #pragma acc loop gang worker vector); (5, OpenACC, #pragma acc parallel deviceptr( position,cell_keys ))", "label": "(7, OpenACC, #pragma acc loop gang worker vector); (5, OpenACC, #pragma acc parallel deviceptr(position,cell_keys))"}
{"predict": "(6, TBB, tbb::flow::queue_node< q3_input_type >  q3(g);); (8, TBB, tbb::flow::make_edge( my_indexer, q3 );)", "label": "(6, TBB, tbb::flow::queue_node< q3_input_type >  q3(g);); (11, TBB, ASSERT(tbb::flow::input_port<0>(my_indexer).predecessor_count() == 0, NULL);); (7, TBB, tbb::flow::make_edge(my_indexer, q3);)"}
{"predict": "(79, TBB, tbb::task::enqueue(*t, p);); (81, TBB, tbb::task::enqueue(*t);); (75, TBB, const tbb::priority_t p = getPriorityByInt(i);); (7, TBB, FFTask* t = new( tbb::task::allocate_root() ) FFTask(&flag);); (5, TBB, tbb::task::enqueue(*t);); (3, TBB, tbb::global_control c(tbb::global_control::max_allowed_parallelism, 1);); (77, TBB, tbb::atomic<bool> signalToLeave;); (60, TBB, tbb::task_scheduler_init tsi(1, 0, /*blocking=*/true);); (107, TBB, tbb::atomic<bool> signalToLeave;); (40, TBB, tbb::task::enqueue(*t, tbb::task::priority::normal);); (32, TBB, tbb::task::enqueue(*t, tbb::task::priority::normal);); (56, TBB, tbb::atomic<int> counter;)", "label": "(5, TBB, tbb::atomic<bool> taskDoneFlag;); (85, TBB, tbb::task::enqueue(*t, p);); (9, TBB, tbb::task::enqueue(*w);); (13, TBB, FFTask* t = new( tbb::task::allocate_root() ) FFTask(&flag);); (81, TBB, const tbb::priority_t p = getPriorityByInt(i);); (98, TBB, tbb::atomic<int> counter;); (11, TBB, tbb::task::enqueue(*t);); (108, TBB, tbb::atomic<bool> signalToLeave;); (-4, TBB, tbb::atomic<int> flag;); (1, TBB, WaiterTask* w = new( tbb::task::allocate_root() ) WaiterTask(&taskDoneFlag);); (3, TBB, tbb::global_control c(tbb::global_control::max_allowed_parallelism, 1);)"}
{"predict": "(25, MPI, ascent_opt[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (7, MPI, MPI_Comm_rank(comm, &par_rank);); (7, MPI, MPI_Comm_size(comm, &par_size);)", "label": "(8, MPI, MPI_Comm_rank(comm, &par_rank);); (24, MPI, open_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (7, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(13, OpenMP, #pragma omp critical)", "label": "(5, OpenMP, #pragma omp workshare)"}
{"predict": "(6, OpenCL, retVal = clEnqueueMemcpyINTEL(this->pCmdQ, true, unifiedMemoryPtr, input.data(), dataSize, 0, nullptr, nullptr);)", "label": "(6, OpenCL, retVal = clEnqueueMemcpyINTEL(this->pCmdQ, true, unifiedMemoryPtr, input.data(), dataSize, 0, nullptr, nullptr);)"}
{"predict": "(87, OpenMP, #pragma omp master taskloop private(i), reduction(+ : j), reduction(+ : q)); (101, OpenMP, #pragma omp master taskloop reduction(+ : r)); (79, OpenMP, #pragma omp master taskloop reduction(&& : S2::S2s)); (64, OpenMP, #pragma omp master taskloop reduction(min : a, b, c, d, f)); (72, OpenMP, #pragma omp master taskloop reduction(& : e, g)); (110, OpenMP, #pragma omp parallel private(fl)); (76, OpenMP, #pragma omp master taskloop reduction(&& : S2::S2sc)); (79, OpenMP, #pragma omp master taskloop reduction(+ : h, k, B::x)); (97, OpenMP, #pragma omp master taskloop reduction(+ : fl)); (133, OpenMP, #pragma omp master taskloop reduction(task, + : m)); (91, OpenMP, #pragma omp parallel reduction(min : i)); (84, OpenMP, #pragma omp master taskloop reduction(+ : o)); (37, OpenMP, #pragma omp master taskloop reduction(~ : argc)); (79, OpenMP, #pragma omp master taskloop reduction(&& : S2::S2s)); (105, OpenMP, #pragma omp master taskloop reduction(+ : m)); (46, OpenMP, #pragma omp master taskloop reduction(max : h.b)); (5, OpenMP, #pragma omp master taskloop reduction(); (115, OpenMP, #pragma omp parallel shared(i)); (57, OpenMP, #pragma omp master taskloop reduction(+ : ba)); (100, OpenMP, #pragma omp master taskloop reduction(+ : fl)); (88, OpenMP, #pragma omp master taskloop reduction(+ : p), reduction(+ : p)); (10, OpenMP, #pragma omp master taskloop reduction(\\)); (78, OpenMP, #pragma omp master taskloop private(i), reduction(+ : j), reduction(+ : q)); (5, OpenMP, #pragma omp master taskloop reduction(foo : argc); (2, OpenMP, #pragma omp master taskloop reduction(*)); (26, OpenMP,", "label": "(15, OpenMP, #pragma omp master taskloop reduction +); (65, OpenMP, #pragma omp master taskloop reduction(- : da)); (61, OpenMP, #pragma omp master taskloop reduction(* : ca)); (54, OpenMP, #pragma omp master taskloop reduction(max : h.b)); (47, OpenMP, #pragma omp master taskloop reduction(+ : a, b, c, d, f)); (55, OpenMP, #pragma omp master taskloop reduction(+ : ba)); (30, OpenMP, #pragma omp master taskloop reduction(| : argc,); (84, OpenMP, #pragma omp master taskloop reduction(+ : p), reduction(+ : p)); (46, OpenMP, #pragma omp master taskloop reduction(min : a, b, c, d, f)); (33, OpenMP, #pragma omp master taskloop reduction(~ : argc)); (38, OpenMP, #pragma omp master taskloop reduction(^ : S1)); (73, OpenMP, #pragma omp master taskloop reduction(+ : o)); (90, OpenMP, #pragma omp master taskloop reduction(max : j)); (11, OpenMP, #pragma omp master taskloop reduction()); (95, OpenMP, #pragma omp parallel reduction(* : fl)); (63, OpenMP, #pragma omp master taskloop reduction(& : e, g)); (65, OpenMP, #pragma omp master taskloop reduction(+ : h, k, B::x)); (73, OpenMP, #pragma omp parallel private(k)); (96, OpenMP, #pragma omp master taskloop reduction(+ : m)); (20, OpenMP, #pragma omp master taskloop reduction(|| : argc > 0? argv[1] : argv[2])); (-2, OpenMP, #pragma omp master taskloop reduction(); (24, OpenMP, #pragma omp master taskloop reduction(&& : argc, z)); (53, OpenMP, #pragma omp master taskloop reduction(&& : S2::S2sc)); (46, OpenMP, #pragma omp master taskloop reduction(^ : fl)); (6, OpenMP, #pragma omp master taskloop reduction(\\)); (-13, OpenMP, #pragma omp master taskloop reduction); (7, OpenMP, #pragma omp master taskloop reduction(foo : argc); (74, OpenMP, #pragma omp parallel reduction(min : i)); (77, OpenMP, #pragma omp parallel private(fl)); (91, OpenMP, #pragma omp master taskloop reduction(task, + : m)); (-9, OpenMP, #pragma omp master taskloop reduction(-); (86, OpenMP, #pragma omp master taskloop nogroup reduction(+ : m)); (65, OpenMP, #pragma omp master taskloop reduction(+ : r)); (-6, OpenMP, #pragma omp master taskloop reduction(*)); (53, OpenMP, #pragma omp master taskloop private(i), reduction(+ : j), reduction(+ : q)); (65, OpenMP, #pragma omp parallel shared(i)); (36, OpenMP, #pragma omp master taskloop reduction(&& : S2::S2s)); (69, OpenMP, #pragma omp master taskloop reduction(+ : fl))"}
{"predict": "(34, CUDA, __shared__ float4 pillarSumSM[max_voxels];); (34, CUDA, __shared__ int4 cordsSM[max_voxels];); (30, CUDA, __shared__ float4 pillarSM[max_voxels][max_points];); (28, CUDA, __shared__ unsigned int pointsNumSM[max_voxels];); (30, CUDA, __shared__ float4 pillarOutSM[max_voxels][max_points][features_size];)", "label": "(34, CUDA, __shared__ int pointsNumSM[4];); (30, CUDA, __shared__ float4 pillarSM[4][64];); (30, CUDA, __shared__ float4 pillarSumSM[4];); (30, CUDA, __shared__ int4 cordsSM[4];); (31, CUDA, __shared__ float pillarOutSM[4][64][11];)"}
{"predict": "(2, OpenMP, omp_set_default_mem_alloc(omp_default_mem_alloc);); (2, OpenMP, void *p = omp_alloc(1024 * 1024, omp_default_mem_alloc);); (11, OpenMP, omp_free(p, omp_default_mem_alloc);)", "label": "(1, OpenMP, omp_allocator_handle_t default_alloc = omp_get_default_allocator();); (4, OpenMP, int *p = (int *)omp_alloc(123456*sizeof(int));)"}
{"predict": "(11, OpenMP, #pragma omp for reduction(inscan, + : a[:n], b)); (43, OpenMP, #pragma omp scan inclusive(a[:n], b)); (40, OpenMP, #pragma omp scan exclusive(a[:n], b))", "label": "(194, OpenMP, #pragma omp scan exclusive(a[:n], b)); (44, OpenMP, #pragma omp scan inclusive(a[:n], b)); (9, OpenMP, #pragma omp for reduction(inscan, +:a[:n], b))"}
{"predict": "(10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &np);); (30, MPI, err = MPI_File_write(fh, buf, MAXLEN, MPI_INT, &status);); (31, MPI, err = MPI_File_close(&fh);); (67, MPI, err = MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_WRONLY, MPI_INFO_NULL, &fh);); (78, MPI, err = MPI_File_open(MPI_COMM_SELF, filename, MPI_MODE_RDONLY, MPI_INFO_NULL, &fh);); (65, MPI, err = MPI_File_set_view(fh, 0, MPI_INT, filetype, \"native\", MPI_INFO_NULL);); (75, MPI, err = MPI_File_read(fh, buf, MAXLEN, MPI_INT, &status);); (55, MPI, MPI_Type_commit(&filetype);); (52, MPI, MPI_Type_indexed(num, blocklen, addrs, MPI_INT, &filetype);); (45, MPI, MPI_Type_create_struct(num, blocklen, addrs, types, &filetype);); (55, MPI, MPI_Type_free(&filetype);); (2, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (62, MPI, err = MPI_File_write_all(fh, buf, buflen, MPI_INT, &status);)", "label": "(11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &np);); (68, MPI, err = MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_WRONLY, MPI_INFO_NULL, &fh);); (73, MPI, err = MPI_File_write_all(fh, buf, buflen, MPI_INT, &status);); (29, MPI, err = MPI_File_close(&fh);); (60, MPI, MPI_Type_commit(&filetype);); (24, MPI, err = MPI_File_write(fh, buf, MAXLEN, MPI_INT, &status);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (65, MPI, err = MPI_File_set_view(fh, 0, MPI_INT, filetype, \"native\", MPI_INFO_NULL);); (45, MPI, MPI_Type_create_hindexed(num, blocklen, addrs, MPI_INT, &filetype);); (76, MPI, err = MPI_File_open(MPI_COMM_SELF, filename, MPI_MODE_RDONLY, MPI_INFO_NULL, &fh);); (40, MPI, MPI_Type_indexed(num, blocklen, indices, MPI_INT, &filetype);); (67, MPI, MPI_Type_free(&filetype);); (76, MPI, err = MPI_File_read(fh, buf, MAXLEN, MPI_INT, &status);); (23, MPI, MPI_Barrier(MPI_COMM_WORLD);); (42, MPI, MPI_Type_create_struct(num, blocklen, addrs, types, &filetype);)"}
{"predict": "(18, OpenMP, #pragma omp parallel sections default(none)); (1, OpenMP, #pragma omp parallel sections default); (6, OpenMP, #pragma omp parallel sections default(shared), default(shared)); (2, OpenMP, #pragma omp parallel sections default()); (4, OpenMP, #pragma omp parallel sections default(x)); (-1, OpenMP, #pragma omp parallel sections default()", "label": "(5, OpenMP, #pragma omp parallel sections default()); (2, OpenMP, #pragma omp parallel sections default(); (7, OpenMP, #pragma omp parallel sections default(shared), default(shared)); (4, OpenMP, #pragma omp parallel sections default(none); (7, OpenMP, #pragma omp parallel sections default(x)); (16, OpenMP, #pragma omp parallel sections default(none)); (-5, OpenMP, #pragma omp parallel sections default); (21, OpenMP, #pragma omp parallel sections default(shared))"}
{"predict": "(17, CUDA, __shared__ float s_a[2][BK][BM];); (17, CUDA, __shared__ float s_b[2][BK][BN];)", "label": "(16, CUDA, __shared__ float s_a[2][BK][BM];); (16, CUDA, __shared__ float s_b[2][BK][BN];)"}
{"predict": "(17, OpenMP, volatile  int_t thread_id = omp_get_thread_num();)", "label": "(17, OpenMP, volatile  int_t thread_id = omp_get_thread_num();)"}
{"predict": "(9, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (10, TBB, tbb::parallel_for(tbb::blocked_range<size_t>(0,n,n/10),AddOne<Invariant<M,8> >(invariant));); (10, TBB, tbb::tick_count t1 = tbb::tick_count::now();)", "label": "(8, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (9, TBB, tbb::tick_count t1 = tbb::tick_count::now();); (7, TBB, tbb::parallel_for(tbb::blocked_range<size_t>(0,n,n/100),TwiddleInvariant<Invariant<M,8> >(invariant));)"}
{"predict": "(35, MPI, MPI_Wait(&request, &status);); (25, MPI, errcode = MPI_File_iwrite_at_all(fh, off, buffer, BUFSIZE, MPI_INT, &request);); (13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (34, MPI, errcode = MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_RDONLY, info, &fh);); (24, MPI, MPI_Barrier(MPI_COMM_WORLD);); (28, MPI, errcode = MPI_File_close(&fh);); (10, MPI, MPI_Info_set(info, \"ind_wr_buffer_size\", \"1024\");); (14, MPI, errcode = MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_WRONLY, info, &fh);); (33, MPI, errcode = MPI_File_iread_at_all(fh, off, buf2, BUFSIZE, MPI_INT, &request);); (12, MPI, MPI_Info_create(&info);); (34, MPI, MPI_Request_free(&request);); (10, MPI, MPI_Info_set(info, \"ind_rd_buffer_size\", \"1024\");); (32, MPI, MPI_Info_free(&info);)", "label": "(19, MPI, MPI_Info_set(info, \"cb_nodes\", \"1\");); (25, MPI, errcode = MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_WRONLY | MPI_MODE_CREATE, info, &fh);); (27, MPI, errcode = MPI_File_iwrite_at_all(fh, off, buffer, BUFSIZE, MPI_INT, &request);); (30, MPI, errcode = MPI_File_close(&fh);); (28, MPI, MPI_Wait(&request, &status);); (12, MPI, MPI_Info_create(&info);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (33, MPI, errcode = MPI_File_iread_at_all(fh, off, buf2, BUFSIZE, MPI_INT, &request);); (10, MPI, MPI_Info_set(info, \"romio_cb_write\", \"enable\");); (43, MPI, MPI_Info_free(&info);); (27, MPI, errcode = MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_RDONLY, info, &fh);)"}
{"predict": "(110, OpenMP, #pragma omp barrier); (76, OpenMP, nthread = omp_get_num_threads();); (94, OpenMP, #pragma omp for); (71, OpenMP, #pragma omp parallel private(j,iter))", "label": "(109, OpenMP, #pragma omp barrier); (75, OpenMP, nthread = omp_get_num_threads();); (92, OpenMP, #pragma omp for); (71, OpenMP, #pragma omp master); (68, OpenMP, #pragma omp parallel private(j,iter))"}
{"predict": "(14, OpenMP, #pragma omp target parallel for)", "label": "(14, OpenMP, #pragma omp target parallel for)"}
{"predict": "(16, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);)", "label": "(16, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &scri_rank);); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &scri_ranks);)"}
{"predict": "(255, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 2, NULL, threads, NULL, 0, NULL, NULL );); (200, OpenCL, error = clEnqueueUnmapMemObject( queue, mapped, inputValues, 0, NULL, NULL );); (109, OpenCL, error = clEnqueueMapImage( queue, image, CL_TRUE, CL_MAP_WRITE, imageOrigin, imageRegion, imageInfo->rowPitch, 0, 0, inputValues, 0, NULL, NULL, &mapped );); (162, OpenCL, error = clEnqueueUnmapMemObject( queue, image, mapped, 0, NULL, NULL );); (224, OpenCL, error = clEnqueueReadImage( queue, image, CL_TRUE, imageOrigin, imageRegion, 0, 0, resultValues, 0, NULL, NULL );); (102, OpenCL, clEnqueueUnmapMemObject( queue, mapped, inputValues, 0, NULL, NULL );); (106, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 2, NULL, threads, NULL, 0, NULL, NULL );); (150, OpenCL, error = clEnqueueMapImage( queue, image, CL_TRUE, CL_MAP_READ, imageOrigin, imageRegion, 0, 0, NULL, 0, NULL, NULL, &mapped );); (156, OpenCL, error = clEnqueueUnmapMemObject( queue, mapped, NULL, 0, NULL, NULL );); (101, OpenCL, error = clEnqueueMapImage( queue, image, CL_TRUE, CL_MAP_WRITE, imageOrigin, imageRegion, 0, 0, inputValues, 0, NULL, NULL, &mapped );); (18, OpenCL, clEnqueueUnmapMemObject( queue, image, imageValues, 0, NULL, NULL );); (143, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 2, NULL, threads, NULL, 0, NULL, NULL );); (133, OpenCL, error = clEnqueueWriteImage( queue, image, mem_flag_types[mem_flag_index], image", "label": "(242, OpenCL, error = clSetKernelArg( kernel, 0, sizeof( cl_mem ), &inputStream );); (228, OpenCL, error = clSetKernelArg( kernel, 2, sizeof( int ), &lod );); (215, OpenCL, error = clSetKernelArg( kernel, 1, sizeof( cl_mem ), &image );); (244, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 1, NULL, threads, NULL, 0, NULL, NULL );); (260, OpenCL, error = clEnqueueReadImage( queue, image, CL_TRUE, origin, region, gEnablePitch? imageInfo->rowPitch : 0, 0, resultValues, 0, NULL, NULL );)"}
{"predict": "(9, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &iam );)", "label": "(9, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &iam );)"}
{"predict": "(36, MPI, MPI_File_close(&c_info->fh);)", "label": "(36, MPI, MPI_File_close(&c_info->fh);)"}
{"predict": "(8, OpenMP, #pragma omp parallel); (14, OpenMP, #pragma omp critical)", "label": "(8, OpenMP, #pragma omp parallel); (10, OpenMP, #pragma omp for); (14, OpenMP, #pragma omp critical)"}
{"predict": "(38, OpenMP, std::cout << \"Number of threads    = \" << omp_get_max_threads() << std::endl;)", "label": "(38, OpenMP, std::cout << \"Number of threads    = \" << omp_get_max_threads() << std::endl;)"}
{"predict": "(32, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (34, CUDA, CUDA_CHECK(cudaFree(d_data));)", "label": "(32, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (34, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(7, SYCL, sycl::accessor a(A, cgh, sycl::write_only, sycl::noinit);); (16, SYCL, sycl::accessor b(B, cgh, sycl::write_only, sycl::noinit);)", "label": "(16, SYCL, auto b = B.get_access<sycl::access_mode::write>(cgh);); (5, SYCL, auto a = A.get_access<sycl::access_mode::write>(cgh);)"}
{"predict": "(20, OpenCL, clReleaseMemObject(mobjA_);); (16, OpenCL, clReleaseMemObject(mobjX_);)", "label": "(20, OpenCL, clReleaseMemObject(mobjA_);); (16, OpenCL, clReleaseMemObject(mobjX_);)"}
{"predict": "(7, TBB, tbb::flow::queue_node<T> q2(g);); (7, TBB, tbb::flow::make_edge( q, q2 );); (8, TBB, tbb::flow::queue_node<T> q3(g);); (8, TBB, tbb::flow::make_edge( q2, q3 );); (38, TBB, tbb::flow::make_edge( q, q2 );); (38, TBB, tbb::flow::remove_edge( q, q2 );); (-3, TBB, tbb::flow::graph g;); (-3, TBB, tbb::flow::queue_node<T> q(g);); (65, TBB, tbb::flow::remove_edge( q,  q2 );)", "label": "(4, TBB, tbb::flow::queue_node<T> q2(g);); (70, TBB, tbb::flow::make_edge( q2, q3 );); (67, TBB, tbb::flow::queue_node<T> q3(g);); (37, TBB, tbb::flow::make_edge( q, q2 );); (56, TBB, tbb::flow::remove_edge( q, q2 );); (87, TBB, tbb::flow::remove_edge( q,  q2 );); (-3, TBB, tbb::flow::queue_node<T> q(g);); (-5, TBB, tbb::flow::graph g;)"}
{"predict": "(39, OpenCL, retVal = clReleaseCommandQueue(cmdQ);); (13, OpenCL, cl_command_queue cmdQ = clCreateCommandQueueWithProperties(context.get(), clDevice, properties, &retVal);); (1, OpenCL, clGetDeviceInfo(clDevice, CL_DEVICE_SLICE_COUNT_INTEL, sizeof(size_t), &maxSliceCount, nullptr);)", "label": "(14, OpenCL, cl_command_queue cmdQ = clCreateCommandQueueWithProperties(context.get(), clDevice, properties, &retVal);); (38, OpenCL, retVal = clReleaseCommandQueue(cmdQ);); (1, OpenCL, clGetDeviceInfo(clDevice, CL_DEVICE_SLICE_COUNT_INTEL, sizeof(size_t), &maxSliceCount, nullptr);)"}
{"predict": "(20, OpenMP, #pragma omp parallel for schedule (guided)); (121, OpenMP, const int th=omp_get_thread_num();)", "label": "(205, OpenMP, const int th=omp_get_thread_num();); (19, OpenMP, #pragma omp parallel for schedule (guided))"}
{"predict": "(86, MPI, MPI_Finalize();); (7, MPI, MPI_Init(&argc,&argv);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)", "label": "(7, MPI, MPI_Init(&argc, &argv);); (85, MPI, MPI_Finalize();); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (57, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(10, CUDA, os << \"cudaMemcpyToSymbol(d_merged\" << T::name << \"GroupStartID, mergedGroupStartID, sizeBytes);); (13, CUDA, os << \"mergedGroupStartID = static_cast<unsigned int *>(nvtxNameCudaAgnostheticMemory(\"mergedGroupStartID\"));)", "label": "(14, CUDA, os << \"__device__ \";); (8, CUDA, os << \"__device__ __constant__ \";)"}
{"predict": "(11, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:v[:N]))", "label": "(12, OpenMP, #pragma omp target teams); (9, OpenMP, #pragma omp target update to(vec.v[0:N])); (16, OpenMP, #pragma omp target update from(vec.v[0:N])); (10, OpenMP, #pragma omp distribute parallel for)"}
{"predict": "(17, OpenMP, #pragma omp target exit data map(from: data)); (12, OpenMP, #pragma omp target enter data map(to: data)); (15, OpenMP, #pragma omp target teams distribute parallel for)", "label": "(12, OpenMP, #pragma omp target teams distribute parallel for)"}
{"predict": "(2, OpenCL, cl_mem notUsedBuffer = clCreateBuffer(clContext, flags, size, hostPtr, &retVal);); (27, OpenCL, EXPECT_EQ(clReleaseContext(context), CL_SUCCESS);); (22, OpenCL, retVal = clReleaseMemObject(buffer);); (3, OpenCL, cl_mem buffer = clCreateBuffer(clContext, flags, size, hostPtr, &retVal);); (18, OpenCL, retVal = clReleaseMemObject(subBuffer);); (10, OpenCL, cl_mem subBuffer = clCreateSubBuffer(buffer, flags, CL_BUFFER_CREATE_TYPE_REGION, &region, &retVal);); (23, OpenCL, EXPECT_EQ(clReleaseMemObject(notUsedBuffer), CL_SUCCESS);)", "label": "(21, OpenCL, retVal = clReleaseMemObject(subBuffer);); (26, OpenCL, retVal = clReleaseMemObject(notUsedBuffer);); (28, OpenCL, EXPECT_EQ(clReleaseContext(context), CL_SUCCESS);); (-2, OpenCL, cl_mem notUsedBuffer = clCreateBuffer(clContext, flags, size, hostPtr, &retVal);); (20, OpenCL, retVal = clReleaseMemObject(buffer);); (0, OpenCL, cl_mem buffer = clCreateBuffer(clContext, flags, size, hostPtr, &retVal);); (8, OpenCL, cl_mem subBuffer = clCreateSubBuffer(buffer, flags, CL_BUFFER_CREATE_TYPE_REGION, &region, &retVal);)"}
{"predict": "(35, MPI, rc = MPI_Reduce(cinbuf, coutbuf, 3, MPI_CHAR, MPI_BXOR, 0, comm);); (24, MPI, MPI_Comm_size(comm, &size);); (66, MPI, rc = MPI_Reduce(scinbuf, scoutbuf, 3, MPI_SIGNED_CHAR, MPI_BXOR, 0, comm);); (94, MPI, rc = MPI_Reduce(ucinbuf, ucoutbuf, 3, MPI_UNSIGNED_CHAR, MPI_BXOR, 0, comm);); (15, MPI, MPI_Comm_set_errhandler(comm, MPI_ERRORS_RETURN);); (13, MPI, MPI_Comm_rank(comm, &rank);)", "label": "(25, MPI, MPI_Comm_size(comm, &size);); (97, MPI, rc = MPI_Reduce(ucinbuf, ucoutbuf, 3, MPI_UNSIGNED_CHAR, MPI_BXOR, 0, comm);); (156, MPI, rc = MPI_Reduce(sinbuf, soutbuf, 3, MPI_SHORT, MPI_BXOR, 0, comm);); (125, MPI, rc = MPI_Reduce(cinbuf, coutbuf, 3, MPI_BYTE, MPI_BXOR, 0, comm);); (274, MPI, rc = MPI_Reduce(linbuf, loutbuf, 3, MPI_LONG, MPI_BXOR, 0, comm);); (63, MPI, rc = MPI_Reduce(scinbuf, scoutbuf, 3, MPI_SIGNED_CHAR, MPI_BXOR, 0, comm);); (31, MPI, rc = MPI_Reduce(cinbuf, coutbuf, 3, MPI_CHAR, MPI_BXOR, 0, comm);); (335, MPI, rc = MPI_Reduce(llinbuf, lloutbuf, 3, MPI_LONG_LONG, MPI_BXOR, 0, comm);); (14, MPI, MPI_Comm_set_errhandler(comm, MPI_ERRORS_RETURN);); (299, MPI, rc = MPI_Reduce(ulinbuf, uloutbuf, 3, MPI_UNSIGNED_LONG, MPI_BXOR, 0, comm);); (238, MPI, rc = MPI_Reduce(iinbuf, ioutbuf, 3, MPI_INT, MPI_BXOR, 0, comm);); (13, MPI, MPI_Comm_rank(comm, &rank);); (206, MPI, rc = MPI_Reduce(uinbuf, uoutbuf, 3, MPI_UNSIGNED, MPI_BXOR, 0, comm);); (175, MPI, rc = MPI_Reduce(usinbuf, usoutbuf, 3, MPI_UNSIGNED_SHORT, MPI_BXOR, 0, comm);)"}
{"predict": "(18, CUDA, ColorSpinorParam cudaParam(cpuParam, *inv_param, QUDA_CUDA_FIELD_LOCATION);)", "label": "(19, CUDA, ColorSpinorParam cudaParam(cpuParam, *inv_param, QUDA_CUDA_FIELD_LOCATION);)"}
{"predict": "(160, OpenMP, lsm303ah_mg_offset_temp_comp_set(&dev_ctx_mg, PROPERTY_ENABLE);)", "label": "(145, OpenMP, lsm303ah_mg_offset_temp_comp_set(&dev_ctx_mg, PROPERTY_ENABLE);)"}
{"predict": "(87, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (87, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)", "label": "(87, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (87, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)"}
{"predict": "(22, CUDA, nvinfer1::ICudaEngine* engine = builder->buildCudaEngine(*network);)", "label": "(22, CUDA, nvinfer1::ICudaEngine* engine = builder->buildCudaEngine(*network);)"}
{"predict": "(36, OpenCL, streams[1] = clCreateBuffer(context, CL_MEM_READ_WRITE, length, NULL, &err);); (58, OpenCL, err |= clSetKernelArg(kernel, 1, sizeof(streams[1]), &streams[1]);); (56, OpenCL, err |= clSetKernelArg(kernel, 0, sizeof(streams[0]), &streams[0]);); (57, OpenCL, err |= clSetKernelArg(kernel, 2, sizeof(sampler), &sampler);)", "label": "(58, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(streams[0]), &streams[0]);); (58, OpenCL, err |= clSetKernelArg(kernel, 1, sizeof(streams[1]), &streams[1]);); (58, OpenCL, err |= clSetKernelArg(kernel, 2, sizeof(sampler), &sampler);); (33, OpenCL, streams[1] = clCreateBuffer(context, CL_MEM_READ_WRITE, length, NULL, &err);)"}
{"predict": "(41, CUDA, cudaDeviceSynchronize();)", "label": "(41, CUDA, cudaDeviceSynchronize();)"}
{"predict": "(7, OpenMP, #pragma omp atomic)", "label": "(7, OpenMP, #pragma omp atomic)"}
{"predict": "(21, CUDA, CUDA_CHECK_ERROR(cudaMemcpy(&max, tempCfl, sizeof(float), cudaMemcpyDeviceToHost));); (21, CUDA, CUDA_CHECK_ERROR(cudaFree(tempCfl));)", "label": "(21, CUDA, CUDA_SAFE_CALL(cudaMemcpy(&max, tempCfl, sizeof(float), cudaMemcpyDeviceToHost));)"}
{"predict": "(24, CUDA, cudaSetDevice(deviceId);); (20, CUDA, cudaGetDeviceCount(&nDevices);); (13, MPI, MPIManager mpi_manager(MPI_COMM_WORLD);)", "label": "(24, CUDA, cudaSetDevice(deviceId);); (20, CUDA, cudaGetDeviceCount(&nDevices);); (9, MPI, MPIManager mpi_manager(MPI_COMM_WORLD);)"}
{"predict": "(18, OpenMP, #pragma omp distribute parallel for private(argc, b), firstprivate(argv, c), lastprivate(d, f) collapse(2) schedule(auto) if (argc) num_threads(a) default(shared) shared(e) reduction(+ : h) dist_schedule(static, b)); (4, OpenMP, #pragma omp target); (5, OpenMP, #pragma omp distribute parallel for schedule(guided, argc) default(none) copyin(g) dist_schedule(static, a) private(a) shared(argc)); (14, OpenMP, #pragma omp teams)", "label": "(5, OpenMP, #pragma omp threadprivate(g)); (21, OpenMP, #pragma omp distribute parallel for private(argc, b), firstprivate(argv, c), lastprivate(d, f) collapse(2) schedule(auto) if (argc) num_threads(a) default(shared) shared(e) reduction(+ : h) dist_schedule(static, b)); (7, OpenMP, #pragma omp distribute parallel for schedule(guided, argc) default(none) copyin(g) dist_schedule(static, a) private(a) shared(argc)); (4, OpenMP, #pragma omp target); (4, OpenMP, #pragma omp teams)"}
{"predict": "(33, MPI, mpi_errno = MPI_Comm_create(MPI_COMM_WORLD, i, MPI_INFO_NULL, MPI_ERRORS_RETURN, &comm_hdls[i]);); (31, MPI, mpi_errno = MPI_Waitall(block, req, error_status);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (20, MPI, mpi_errno = MPI_Comm_create(MPI_COMM_WORLD, i, MPI_INFO_NULL, MPI_ERRORS_RETURN, &comm_hdls[i]);); (45, MPI, mpi_errno = MPI_Comm_create(MPI_COMM_WORLD, i, MPI_INFO_NULL, MPI_ERRORS_RETURN, &comm_hdls[i]);); (18, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (22, MPI, mpi_errno = MPI_Waitall(block, req, error_status);); (10, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (47, MPI, mpi_errno = MPI_Waitall(block, req, MPI_STATUSES_IGNORE);); (17, MPI, MPI_Comm_idup(MPI_COMM_WORLD, &comm_hdls[i], &req[block++]);); (28, MPI, mpi_errno = MPI_Wait(&req[j], MPI_STATUSES_IGNORE);)"}
{"predict": "(13, TBB, double serial_time = (tbb::tick_count::now() - t0).seconds();); (10, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (19, TBB, double tbb_time = (tbb::tick_count::now() - t1).seconds();); (16, TBB, tbb::tick_count t1 = tbb::tick_count::now();)", "label": "(13, TBB, double serial_time = (tbb::tick_count::now() - t0).seconds();); (10, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (19, TBB, double tbb_time = (tbb::tick_count::now() - t1).seconds();); (16, TBB, tbb::tick_count t1 = tbb::tick_count::now();)"}
{"predict": "(3, CUDA, extern __shared__ double sdat[];)", "label": "(3, CUDA, extern __shared__ double sdat[];)"}
{"predict": "(131, OpenMP, #pragma omp simd); (144, OpenMP, #pragma omp atomic capture); (136, OpenMP, #pragma omp master); (76, OpenMP, #pragma omp parallel default (shared)); (85, OpenMP, #pragma omp taskloop private (nn,lbstart,lbend,thread_id1,rtemp_loc,nroot_send_tmp) untied); (139, OpenMP, #pragma omp atomic); (77, OpenMP, #pragma omp simd aligned(rtemp_loc:alignof(double))); (95, OpenMP, #pragma omp parallel default (shared)); (204, OpenMP, #pragma omp atomic update); (205, OpenMP, #pragma omp atomic); (72, OpenMP, #pragma omp parallel default(shared) private(nn)); (227, OpenMP, #pragma omp atomic capture); (125, OpenMP, #pragma omp atomic read); (123, OpenMP, #pragma omp atomic write); (224, OpenMP, #pragma omp critical); (74, OpenMP, #pragma omp master); (192, OpenMP, #pragma omp atomic)", "label": "(82, OpenMP, #pragma\tomp\ttaskloop firstprivate (stat) private (thread_id1,Uinv,nn,lbstart,lbend,ub,temp,rtemp_loc,ik,lk1,gik,gikcol,usub,uval,lsub,lusup,iknsupc,il,i,irow,bmod_tmp,p,ii,jj,t1,t2,j,ikfrow,iklrow,dest,y,uptr,fnz,nsupr,nroot_send_tmp) untied nogroup); (126, OpenMP, #pragma omp simd); (141, OpenMP, #pragma omp atomic capture); (75, OpenMP, #pragma\tomp\tparallel for private (thread_id1,Uinv,nn,lbstart,lbend,ub,temp,rtemp_loc,ik,lk1,gik,gikcol,usub,uval,lsub,lusup,iknsupc,il,i,irow,bmod_tmp,p,ii,jj,t1,t2,j,ikfrow,iklrow,dest,y,uptr,fnz,nsupr))"}
{"predict": "(21, CUDA, CUDA_SAFE_CALL(cudaFree(input.data()));)", "label": "(8, CUDA, auto input   = thrust::host_vector<coordinate<T>>(d_input);)"}
{"predict": "(19, TBB, my_thread_array = tbb::cache_aligned_allocator<padded_ipc_worker>().allocate( my_n_thread );)", "label": "(20, TBB, my_thread_array = tbb::cache_aligned_allocator<padded_ipc_worker>().allocate( my_n_thread );); (14, TBB, my_n_thread = tbb::detail::r1::AvailableHwConcurrency();); (25, TBB, my_waker = tbb::cache_aligned_allocator<ipc_waker>().allocate(1);); (27, TBB, my_stopper = tbb::cache_aligned_allocator<ipc_stopper>().allocate(1);)"}
{"predict": "(81, OpenMP, #pragma omp teams distribute parallel for num_teams(20) num_threads(512)); (77, OpenMP, #pragma omp teams distribute parallel for num_teams(20) thread_limit(768)); (35, OpenMP, #pragma omp distribute parallel for num_threads(256)); (15, OpenMP, #pragma omp distribute parallel for); (45, OpenMP, #pragma omp distribute parallel for num_threads(128)); (62, OpenMP, #pragma omp teams num_teams(20) thread_limit(768)); (14, OpenMP, #pragma omp target teams num_teams(20) thread_limit(768)); (75, OpenMP, #pragma omp teams num_teams(20)); (43, OpenMP, #pragma omp teams num_teams(20)); (53, OpenMP, #pragma omp distribute parallel for num_threads(128)); (12, OpenMP, #pragma omp teams)", "label": "(19, OpenMP, #pragma omp distribute parallel for); (17, OpenMP, #pragma omp target); (45, OpenMP, #pragma omp distribute parallel for num_threads(128)); (72, OpenMP, #pragma omp teams distribute parallel for thread_limit(512) num_threads(128)); (22, OpenMP, #pragma omp target teams thread_limit(64)); (76, OpenMP, #pragma omp teams distribute parallel for); (29, OpenMP, #pragma omp teams)"}
{"predict": "(68, CUDA, checkCudaErrors(cudaMalloc((void **)&d_idata, bytes));); (107, CUDA, checkCudaErrors(cudaFree(d_idata));); (107, CUDA, checkCudaErrors(cudaFree(d_odata));); (67, CUDA, checkCudaErrors(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));); (62, CUDA, checkCudaErrors(cudaMalloc((void **)&d_odata, numBlocks * sizeof(float)));); (6, CUDA, checkCudaErrors(cudaGetDeviceProperties(&prop, device));); (95, CUDA, checkCudaErrors(cudaFreeHost(h_odata));)", "label": "(68, CUDA, checkCudaErrors(cudaMalloc((void **)&d_odata, numBlocks * sizeof(float)));); (105, CUDA, cudaFree(d_odata);); (65, CUDA, checkCudaErrors(cudaMalloc((void **)&d_idata, bytes));); (10, CUDA, checkCudaErrors(cudaGetDeviceProperties(&prop, device));); (8, CUDA, checkCudaErrors(cudaSetDevice(device));); (100, CUDA, cudaFree(d_idata);); (65, CUDA, checkCudaErrors(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));)"}
{"predict": "(20, CUDA, cudaFree(res_dev);); (13, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);); (9, CUDA, cudaMalloc(&res_dev, sizeof(float));)", "label": "(20, CUDA, cudaFree(res_dev);); (10, CUDA, cudaMalloc(&res_dev, sizeof(float));); (12, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(2, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (2, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(2, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (2, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(13, TBB, return tbb::parallel_deterministic_reduce(r, my_gain, std::plus<>());); (11, TBB, auto r = tbb::blocked_range<size_t>(0, end, 1000);)", "label": "(14, TBB, return tbb::parallel_reduce(range, 0, accum, std::plus<>());); (3, TBB, auto range = tbb::blocked_range<size_t>(UL(0), end);)"}
{"predict": "(12, CUDA, cudaDeviceSynchronize();)", "label": "(12, CUDA, cudaDeviceSynchronize();)"}
{"predict": "(11, MPI, MPI_Type_size_x(type, &typesize);)", "label": "(11, MPI, MPI_Type_size_x(type, &typesize);)"}
{"predict": "(2, MPI, MPI_Init(&argc, &argv);); (14, MPI, MPI_Finalize();)", "label": "(15, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(79, CUDA, flamegpu::CUDASimulation cudaSimulation(model, argc, (const char **)argv);)", "label": "(80, CUDA, flamegpu::CUDASimulation cudaSimulation(model);)"}
{"predict": "(16, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);)", "label": "(16, MPI, MPI_Comm_size(MPI_COMM_WORLD,&mpi_size);); (16, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&mpi_rank);)"}
{"predict": "(65, MPI, MPI_Pack(check_pack_buf, 1, datatype, pack_buf, pack_size, &pos, obj.comm);); (60, MPI, MPI_Unpack(unpack_buf, pack_size, &pos, check_buf, 1, datatype, obj.comm);); (4, MPI, MPI_Type_size(datatype, &type_size);); (54, MPI, MPI_Pack(check_buf, 1, datatype, pack_buf, bufsize, &pos, obj.comm);)", "label": "(63, MPI, MPI_Unpack(check_pack_buf, pack_size, &pos, unpack_buf, 1, datatype, MPI_COMM_WORLD);); (6, MPI, MPI_Type_size_c(datatype, &type_size);); (34, MPI, MPI_Pack_size(1, datatype, MPI_COMM_WORLD, &pack_size);); (54, MPI, MPI_Pack(check_unpack_buf, 1, datatype, pack_buf, pack_size, &pos, MPI_COMM_WORLD);)"}
{"predict": "(2, OpenMP, #pragma omp parallel for)", "label": "(4, OpenMP, #pragma omp parallel for)"}
{"predict": "NDRangeKernel( \\\"%s\\\" ) launch failed\",(111, OpenCL, err = clEnqueueWriteBuffer( queue, pThreadData->m_input_buffer, CL_TRUE, 0, num_elements * vectorSize, (void *)(pThreadData->m_input_ptr), 0, NULL, NULL );); (64, OpenCL, clReleaseMemObject(pThreadData->m_input_buffer);); (64, OpenCL, pThreadData->m_input_buffer = clCreateBuffer(context, CL_MEM_READ_WRITE, num_elements * vectorSize, NULL, &err);); (62, OpenCL, clReleaseMemObject(pThreadData->m_result_buffer);); (60, OpenCL, pThreadData->m_result_buffer = clCreateBuffer(context, CL_MEM_READ_WRITE, num_elements * vectorSize, NULL, &err);); (144, OpenCL, clReleaseKernel(pThreadData->m_kernel);); (60, OpenCL, clReleaseMemObject(pThreadData->m_temp_buffer);); (140, OpenCL, clReleaseProgram(pThreadData->m_program);)", "label": "(113, OpenCL, err = clEnqueueWriteBuffer( queue, pThreadData->m_streams[1], CL_FALSE, 0, pThreadData->m_type_size*num_elements * inputBVecSize, (void *)pThreadData->m_input_ptr[1], 0, NULL, NULL );); (110, OpenCL, err = clEnqueueWriteBuffer(queue, pThreadData->m_streams[0], CL_FALSE, 0, pThreadData->m_type_size*num_elements * inputAVecSize, (void *)pThreadData->m_input_ptr[0], 0, NULL, NULL);); (117, OpenCL, err = clEnqueueReadBuffer( queue, pThreadData->m_streams[2], CL_TRUE, 0, pThreadData->m_type_size*num_elements * vectorSize, (void *)pThreadData->m_output_ptr, 0, NULL, NULL );); (113, OpenCL, err = clEnqueueNDRangeKernel( queue, pThreadData->m_kernel[i], 1, NULL, threads, NULL, 0, NULL, NULL );)"}
{"predict": "(127, OpenMP, stencil_time = omp_get_wtime() - stencil_time;); (112, OpenMP, if (iter==1) stencil_time = omp_get_wtime();); (134, OpenMP, std::cout << \"L1 norm(113, OpenMP, #pragma omp target teams distribute parallel for simd collapse(2))", "label": "(114, OpenMP, if (iter==1) stencil_time = omp_get_wtime();); (59, OpenMP, std::cout << \"Number of threads (max)   = \" << omp_get_max_threads() << std::endl;); (125, OpenMP, stencil_time = omp_get_wtime() - stencil_time;)"}
{"predict": "(38, CUDA, cudaMemcpy(d_in, tensor.data(), in_bytes, cudaMemcpyHostToDevice);); (48, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (33, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (82, CUDA, cudaFree(d_in);); (44, CUDA, assert(cudaMemcpyAsync(tensor_arg.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);); (29, CUDA, cudaMalloc((void**)(&d_in), in_bytes);); (80, CUDA, cudaFree(d_out);)", "label": "(36, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (83, CUDA, cudaFree(d_in);); (83, CUDA, cudaFree(d_out);); (32, CUDA, cudaMalloc((void**)(&d_in), in_bytes);); (45, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (33, CUDA, cudaMemcpy(d_in, tensor.data(), in_bytes, cudaMemcpyHostToDevice);); (42, CUDA, assert(cudaMemcpyAsync(tensor_arg.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);)"}
{"predict": "(38, MPI, MPI_Type_free(&newtype);); (38, MPI, MPI_Type_free(&types[0]);); (27, MPI, MPI_Recv(buf, count, newtype, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (22, MPI, MPI_Send(MPI_BOTTOM, count, MPI_INTEGER, 1, 0, MPI_COMM_WORLD);); (14, MPI, MPI_Type_commit(&newtype);); (15, MPI, MPI_Type_create_struct(2, lens, displs, types, &newtype);); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (32, MPI, MPI_Send(MPI_BOTTOM, 1, newtype, 0, 0, MPI_COMM_WORLD);)", "label": "(30, MPI, MPI_Send(MPI_BOTTOM, 1, newtype, 1, 0, MPI_COMM_WORLD);); (23, MPI, MPI_Type_commit(&newtype);); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (20, MPI, MPI_Type_create_struct(2, lens, displs, types, &newtype);); (15, MPI, MPI_Get_address(&count, &displs[0]);); (27, MPI, MPI_Recv(buf, 6, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (35, MPI, MPI_Type_free(&newtype);); (15, MPI, types[1] = MPI_Type_f2c(*ftype);)"}
{"predict": "(46, OpenMP, #pragma omp target simd linear(i : i + 4)); (38, OpenMP, #pragma omp target simd linear(h, C::x)); (45, OpenMP, #pragma omp target simd linear(j)); (30, OpenMP, #pragma omp target simd linear(argv[1])); (23, OpenMP, #pragma omp target simd linear(argc : 5)); (35, OpenMP, #pragma omp target simd linear(i)); (40, OpenMP, #pragma omp target simd linear(i : z)); (13, OpenMP, #pragma omp target simd linear(argc,); (4, OpenMP, #pragma omp target simd linear); (13, OpenMP, #pragma omp target simd linear(argc); (33, OpenMP, #pragma omp parallel); (42, OpenMP, #pragma omp target simd linear(i : 0)); (3, OpenMP, #pragma omp target simd linear()); (23, OpenMP, #pragma omp target simd linear(a, b : B::ib)); (13, OpenMP, #pragma omp target simd linear(argc > 0? argv[1] : argv[2])); (33, OpenMP, #pragma omp target simd linear(i : 4)); (4, OpenMP, #pragma omp target simd linear(argc); (36, OpenMP, #pragma omp target simd linear(i : i)); (38, OpenMP, #pragma omp target simd linear(i)); (23, OpenMP, #pragma omp target simd linear(e, g)); (5, OpenMP, #pragma omp target simd linear(S1)); (13, OpenMP, #pragma omp target simd linear(S2::S2s)); (38, OpenMP, #pragma omp target simd linear(i : i + z)); (3, OpenMP, #pragma omp target simd linear(argc :); (39, OpenMP, #pragma omp target simd linear(i : i - z)); (30, OpenMP, #pragma omp target simd linear(i : 8)); (33, OpenMP, #pragma omp target simd linear(i : i + 8", "label": "(11, OpenMP, #pragma omp taskloop simd linear(f) linear(f)); (54, OpenMP, #pragma omp taskloop simd linear(ref(j))); (35, OpenMP, #pragma omp taskloop simd linear(a, b)); (26, OpenMP, #pragma omp taskloop simd linear (argc > 0? argv[1] : argv[2])); (27, OpenMP, #pragma omp taskloop simd linear (argc, z)); (10, OpenMP, #pragma omp taskloop simd linear (); (39, OpenMP, #pragma omp taskloop simd linear(h, C::x)); (16, OpenMP, #pragma omp taskloop simd linear (foo())); (49, OpenMP, #pragma omp taskloop simd linear(i)); (38, OpenMP, #pragma omp parallel); (17, OpenMP, #pragma omp taskloop simd linear (argc,); (2, OpenMP, #pragma omp taskloop simd linear); (13, OpenMP, #pragma omp taskloop simd linear (argc); (37, OpenMP, #pragma omp taskloop simd linear(val(i))); (19, OpenMP, #pragma omp taskloop simd linear (S1)); (24, OpenMP, #pragma omp taskloop simd linear (argv[1])); (3, OpenMP, #pragma omp taskloop simd linear (val); (4, OpenMP, #pragma omp taskloop simd linear (ref())); (34, OpenMP, #pragma omp taskloop simd linear(uval(i) : 4)); (24, OpenMP, #pragma omp taskloop simd linear(val(e, g))); (-3, OpenMP, #pragma omp taskloop simd linear ())"}
{"predict": "(2, OpenMP, #pragma omp taskyield unknown); (1, OpenMP, #pragma omp taskyield untied); (2, OpenMP, #pragma omp taskyield); (3, OpenMP, #pragma omp taskyield allocate(argc))", "label": "(3, OpenMP, #pragma omp taskyield untied); (0, OpenMP, #pragma omp taskyield); (2, OpenMP, #pragma omp taskyield unknown)"}
{"predict": "(13, CUDA, checkCudaErrors(cudaFuncGetAttributes(&attr,ptr));); (6, CUDA, checkCudaErrors(cudaOccupancyMaxPotentialBlockSize(&qblocksize,&mingridsize,(void*)ptr,0,0));)", "label": "(9, CUDA, cudaFuncGetAttributes(&attr,(void*)ptr);); (14, CUDA, fcuda::Check_CudaErroorFun(\"Error collecting kernel information.\");); (5, CUDA, cudaOccupancyMaxPotentialBlockSize(&mingridsize,&qblocksize,(void*)ptr,0,0);)"}
{"predict": "(54, TBB, tbb::flow::make_edge( b2, b3 );); (50, TBB, tbb::flow::make_edge( b, b2 );); (3, TBB, tbb::flow::broadcast_node<T> b3(g);); (3, TBB, tbb::flow::broadcast_node<T> b2(g);); (75, TBB, tbb::flow::broadcast_node<T> b_copy(b);); (-3, TBB, tbb::flow::graph g;); (-3, TBB, tbb::flow::broadcast_node<T> b(g);)", "label": "(52, TBB, tbb::flow::make_edge( b2, b3 );); (50, TBB, tbb::flow::make_edge( b, b2 );); (73, TBB, tbb::flow::buffer_node<T> b_copy(b);); (2, TBB, tbb::flow::buffer_node<T> b3(g);); (0, TBB, tbb::flow::buffer_node<T> b2(g);); (-3, TBB, tbb::flow::graph g;); (-3, TBB, tbb::flow::buffer_node<T> b(g);)"}
{"predict": "(132, OpenCL, err |= clSetKernelArg(kernel, 2, sizeof(streams[2]), &streams[2]);); (130, OpenCL, err |= clSetKernelArg(kernel, 0, sizeof(streams[0]), &streams[0]);); (131, OpenCL, err |= clSetKernelArg(kernel, 1, sizeof(streams[1]), &streams[1]);); (134, OpenCL, err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, threads, NULL, 0, NULL, NULL);)", "label": "(132, OpenCL, err |= clSetKernelArg(kernel, 1, sizeof(cl_mem), &streams[1]);); (132, OpenCL, err |= clSetKernelArg(kernel, 2, sizeof(cl_mem), &streams[2]);); (132, OpenCL, err |= clSetKernelArg(kernel, 3, sizeof(cl_mem), &streams[3]);); (128, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(cl_mem), &streams[0]);)"}
{"predict": "(12, OpenMP, #pragma omp parallel default(shared)); (37, OpenMP, #pragma omp parallel num_threads(x)); (23, OpenMP, #pragma omp parallel default(firstprivate)); (17, OpenMP, #pragma omp parallel default(none)); (28, OpenMP, #pragma omp parallel default(none)); (36, OpenMP, #pragma omp parallel default(shared))", "label": "(19, OpenMP, #pragma omp parallel default(shared)); (11, OpenMP, #pragma omp parallel); (38, OpenMP, #pragma omp parallel num_threads(x)); (23, OpenMP, #pragma omp parallel default(none)); (29, OpenMP, #pragma omp parallel default(firstprivate))"}
{"predict": "(3, TBB, typedef oneapi::tbb::blocked_range3d<row_type,col_type> range_type;); (19, TBB, AssertSameType( r.cols(), oneapi::tbb::blocked_range<col_type>( coli, colj, 1 ));); (17, TBB, AssertSameType( r.rows(), oneapi::tbb::blocked_range<row_type>( rowi, rowj, 1 ));); (20, TBB, range_type r2(r,oneapi::tbb::split());)", "label": "(24, TBB, range_type r2(r,tbb::split());); (2, TBB, typedef tbb::blocked_range2d<row_type,col_type> range_type;); (18, TBB, AssertSameType( r.cols(), tbb::blocked_range<col_type>( coli, colj, 1 ));); (16, TBB, AssertSameType( r.rows(), tbb::blocked_range<row_type>( rowi, rowj, 1 ));)"}
{"predict": "(15, OpenMP, #pragma omp flush); (25, OpenMP, #pragma omp critical); (7, OpenMP, #pragma omp parallel private(sum0)); (11, OpenMP, #pragma omp for nowait); (7, OpenMP, #pragma omp master)", "label": "(13, OpenMP, #pragma omp for private(sum0) schedule(static,1)); (7, OpenMP, #pragma omp parallel); (22, OpenMP, #pragma omp critical); (13, OpenMP, #pragma omp flush)"}
{"predict": ", 1(20, MPI, MPI_CALL(MPI_Comm_free(&local_comm));); (200, MPI, double stop = MPI_Wtime();); (209, CUDA, CUDA_RT_CALL(cudaFree(a));); (-2, MPI, MPI_CALL(MPI_Comm_rank(MPI_COMM_WORLD, &rank));); (18, CUDA, CUDA_RT_CALL(cudaFree(0));); (16, CUDA, CUDA_RT_CALL(cudaSetDevice(local_rank%num_devices));); (10, CUDA, CUDA_RT_CALL(cudaGetDeviceCount(&num_devices));); (18, CUDA, CUDA_RT_CALL(cudaMalloc(&a, nx * (chunk_size + 2) * sizeof(real)));); (20, CUDA, CUDA_RT_CALL(cudaMalloc(&a_new, nx * (chunk_size + 2) * sizeof(real)));); (23, CUDA, CUDA_RT_CALL(cudaMemset(a_new, 0, nx * (chunk_size + 2) * sizeof(real)));); (31, CUDA, CUDA_RT_CALL(cudaMallocHost(&a_h, nx * ny * sizeof(real)));); (31, CUDA, CUDA_RT_CALL(cudaMallocHost(&a_ref_h, nx * ny * sizeof(real)));); (153, CUDA, CUDA_RT_CALL(cudaGetLastError());); (184, CUDA, CUDA_RT_CALL(cudaFree(l2_norm_d));); (193, CUDA, CUDA_RT_CALL(cudaFreeHost(a_h));); (184, CUDA, CUDA_RT_CALL(cudaFreeHost(a_ref_h));); (13, CUDA, CUDA_RT_CALL(cudaMallocHost(&l2_norm_h, sizeof(real)));)", "label": "(118, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&reset_l2norm_done, cudaEventDisableTiming));); (143, MPI, MPI_CALL(MPI_Barrier(MPI_COMM_WORLD));); (21, MPI, MPI_CALL(MPI_Comm_free(&local_comm));); (249, CUDA, CUDA_RT_CALL(cudaStreamDestroy(compute_stream));); (201, MPI, double stop = MPI_Wtime();); (244, CUDA, CUDA_RT_CALL(cudaEventDestroy(push_done));); (257, CUDA, CUDA_RT_CALL(cudaFree(a));); (-4, MPI, MPI_CALL(MPI_Comm_rank(MPI_COMM_WORLD, &rank));); (94, CUDA, CUDA_RT_CALL(cudaDeviceSynchronize());); (18, CUDA, CUDA_RT_CALL(cudaFree(0));); (16, CUDA, CUDA_RT_CALL(cudaSetDevice(local_rank%num_devices));); (58, CUDA, CUDA_RT_CALL(cudaMalloc(&a, nx * (chunk_size + 2) * sizeof(real)));); (243, CUDA, CUDA_RT_CALL(cudaFree(l2_norm_d));); (138, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(push_stream, reset_l2norm_done, 0));); (100, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&push_prep_done, cudaEventDisableTiming));); (28, CUDA, CUDA_RT_CALL(cudaMallocHost(&a_h, nx * ny * sizeof(real)));); (25, CUDA, CUDA_RT_CALL(cudaMallocHost(&a_ref_h, nx * ny * sizeof(real)));); (84, CUDA"}
{"predict": "(27, TBB, tbb::parallel_for( tbb::blocked_range<int> (0, N, 10000), ParallelScalarBody<T>(sums ) );); (13, TBB, tbb::combinable<T> assigned;); (31, TBB, ParallelScalarBody<T> my_body( finit_combinable );); (10, TBB, tbb::combinable<T> sums;); (31, TBB, tbb::parallel_for( tbb::blocked_range<int> (0, N, 10000), my_body );); (23, TBB, tbb::parallel_for( tbb::blocked_range<int> (0, N, 10000), ParallelScalarBodyNoInit<T>( finit_combinable ) );); (2, TBB, tbb::affinity_partitioner aff;); (3, TBB, tbb::parallel_scalar_init init( aff );); (26, TBB, tbb::parallel_for( tbb::blocked_range<int> (0, N, 10000), ParallelScalarBodyNoInit<T>( sums ) );); (27, TBB, tbb::combinable<T> finit_combinable( my_finit_decl );); (-5, TBB, tbb::tick_count t0;); (15, TBB, if (Verbose && t == 0) t0 = tbb::tick_count::now();)", "label": "(30, TBB, tbb::combinable<T> finit_combinable(my_finit_decl);); (32, TBB, tbb::parallel_for( tbb::blocked_range<int>( 0, N, 10000 ), ParallelScalarBodyNoInit<T>( finit_combinable ) );); (24, TBB, if (Verbose && t == 0) t0 = tbb::tick_count::now();); (31, TBB, tbb::parallel_for( tbb::blocked_range<int>( 0, N, 10000 ), ParallelScalarBody<T>( sums ) );); (40, TBB, tbb::combinable<T> assigned;); (-2, TBB, tbb::task_scheduler_init init(tbb::task_scheduler_init::deferred);); (7, TBB, tbb::tick_count t0;); (21, TBB, tbb::combinable<T> sums;)"}
{"predict": "(13, OpenMP, omp_set_num_threads(nr_threads);); (39, OpenMP, #pragma omp parallel default(none) shared(a)); (16, OpenMP, #pragma omp sections); (22, OpenMP, int thread_nr = omp_get_thread_num();); (13, OpenMP, #pragma omp section); (8, OpenMP, #pragma omp parallel default(none) shared(a, sum))", "label": "(12, OpenMP, omp_set_num_threads(nr_threads);); (13, OpenMP, #pragma omp parallel default(none) shared(a)); (14, OpenMP, #pragma omp sections); (8, OpenMP, omp_set_dynamic(0);); (14, OpenMP, #pragma omp section); (16, OpenMP, int thread_nr = omp_get_thread_num();)"}
{"predict": "(12, CUDA, extern __shared__ float _shared_centroids[];)", "label": "(12, CUDA, extern __shared__ float _shared_centroids[];)"}
{"predict": "(21, CUDA, extern __shared__ float entire_buffer[];)", "label": "(21, CUDA, extern __shared__ float entire_buffer[];)"}
{"predict": "(7, OpenMP, #pragma omp flush acq_rel); (12, OpenMP, #pragma omp flush (a)); (2, OpenMP, #pragma omp flush); (4, OpenMP, #pragma omp flush release); (0, OpenMP, #pragma omp flush acquire)", "label": "(5, OpenMP, #pragma omp flush acquire); (5, OpenMP, #pragma omp flush release); (1, OpenMP, #pragma omp flush); (4, OpenMP, #pragma omp flush(a)); (0, OpenMP, #pragma omp flush acq_rel)"}
{"predict": "(67, CUDA, CUDA_CHECK(cudaFree(d_A));); (41, CUDA, CUDA_CHECK(cudaMemcpyAsync(d_A, h_A, sizeof(data_type) * lda * n, cudaMemcpyHostToDevice, stream));); (66, CUDA, CUDA_CHECK(cudaFree(d_info));); (34, CUDA, CUDA_CHECK(cudaMalloc(&d_info, sizeof(int)));); (44, CUDA, CUDA_CHECK(cudaMemcpyAsync(&h_info, d_info, sizeof(int), cudaMemcpyDeviceToHost, stream));); (44, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (30, CUDA, CUDA_CHECK(cudaMalloc(&d_A, sizeof(data_type) * lda * n));); (62, CUDA, CUDA_CHECK(cudaFree(d_A_inv));); (29, CUDA, CUDA_CHECK(cudaMalloc(&d_A_inv, sizeof(data_type) * lda * n));); (33, CUDA, CUDA_CHECK(cudaMemcpyAsync(d_info, &h_info, sizeof(int), cudaMemcpyHostToDevice, stream));); (55, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (23, CUDA, CUDA_CHECK(cudaStreamCreate(&stream));); (57, CUDA, CUDA_CHECK(cudaDeviceReset());)", "label": "(43, CUDA, CUDA_CHECK(cudaMemcpy(d_A_inv, d_A, sizeof(data_type) * lda * n, cudaMemcpyDeviceToDevice));); (47, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (69, CUDA, CUDA_CHECK(cudaFree(d_info));); (34, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * lda * n));); (34, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A_inv), sizeof(data_type) * lda * n));); (65, CUDA, CUDA_CHECK(cudaFree(d_A_inv));); (58, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (32, CUDA, CUDA_CHECK(cudaMalloc(&d_info, sizeof(int)));); (34, CUDA, CUDA_CHECK(cudaMemcpy(d_A, h_A, sizeof(data_type) * lda * n, cudaMemcpyHostToDevice));); (42, CUDA, CUDA_CHECK(cudaMemcpy(&h_info, d_info, sizeof(int), cudaMemcpyDeviceToHost));); (34, CUDA, CUDA_CHECK(cudaMemset(d_info, 0, sizeof(int)));); (58, CUDA, CUDA_CHECK(cudaFree(d_A));); (20, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(34, CUDA, thrust::complex<T2>* vb_(vb + (nc0 + Q) * nwalk);); (40, CUDA, thrust::complex<T> const* v2_(v1 + Qm * nchol_max * nwalk);); (27, CUDA, thrust::complex<T> const* v1_(v1 + Q * nchol_max * nwalk);); (41, CUDA, thrust::complex<T2>* vb_(vb + (nc0 + Qm) * nwalk);); (52, CUDA, thrust::complex<T> ialpha(0.0, 1.0);)", "label": "(31, CUDA, thrust::complex<T> const* v1_(v1 + Q * nchol_max * nwalk);); (31, CUDA, thrust::complex<T> const* v2_(v1 + Qm * nchol_max * nwalk);); (52, CUDA, thrust::complex<T2> ialpha(alpha * thrust::complex<T2>(0.0, 1.0));); (27, CUDA, thrust::complex<T2>* vb_(vb + nc0 * nwalk);); (46, CUDA, thrust::complex<T2>* vb_(vb + (nc0 + nc) * nwalk);)"}
{"predict": "(83, OpenACC, #pragma acc update host(state[(nz + 2 * hs) * (nx + 2 * hs) * NUM_VARS])); (45, OpenACC, #pragma acc data copyin(state_tmp[(nz + 2 * hs) * (nx + 2 * hs) * NUM_VARS], hy_dens_cell[nz + 2 * hs], hy_dens_theta_cell[nz + 2 * hs], hy_dens_int[nz + 1], hy_dens_theta_int[nz + 1], hy_pressure_int[nz + 1]))", "label": "(83, OpenACC, #pragma acc update host(state[(nz + 2 * hs) * (nx + 2 * hs) * NUM_VARS])); (45, OpenACC, #pragma acc data copyin(state_tmp[(nz + 2 * hs) * (nx + 2 * hs) * NUM_VARS], hy_dens_cell[nz + 2 * hs], hy_dens_theta_cell[nz + 2 * hs], hy_dens_int[nz + 1], hy_dens_theta_int[nz + 1], hy_pressure_int[nz + 1]))"}
{"predict": "(22, CUDA, cudaStreamSynchronize(self_data.stream());)", "label": "(32, MPI, CALL(MPI_Waitall(static_cast<int>(reqs.size()), reqs.data(), MPI_STATUSES_IGNORE));); (20, MPI, CALL(MPI_Waitall(reqs.size(), reqs.data(), MPI_STATUSES_IGNORE));)"}
{"predict": "(30, MPI, MPI_Bcast(data, num_elements, MPI_INT, 0, MPI_COMM_WORLD);); (33, MPI, MPI_Allreduce(&my_bcast_time, &total_my_bcast_time, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);); (25, MPI, MPI_Barrier(MPI_COMM_WORLD);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (26, MPI, MPI_Allreduce(&mpi_bcast_time, &total_mpi_bcast_time, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);); (22, MPI, MPI_Barrier(MPI_COMM_WORLD);); (25, MPI, MPI_Bcast(&mpi_bcast_time, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);); (15, MPI, MPI_Barrier(MPI_COMM_WORLD);); (13, MPI, MPI_Bcast(&my_bcast_time, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);)", "label": "(28, MPI, total_my_bcast_time += MPI_Wtime();); (31, MPI, total_mpi_bcast_time -= MPI_Wtime();); (45, MPI, MPI_Finalize();); (32, MPI, total_mpi_bcast_time += MPI_Wtime();); (5, MPI, MPI_Init(NULL, NULL);); (28, MPI, MPI_Bcast(data, num_elements, MPI_INT, 0, MPI_COMM_WORLD);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (17, MPI, total_my_bcast_time -= MPI_Wtime();); (15, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(5, CUDA, __shared__ float Bs[block_size][block_size];); (3, CUDA, __shared__ float As[block_size][block_size];)", "label": "(4, CUDA, __shared__ float Bs[block_size][block_size];); (2, CUDA, __shared__ float As[block_size][block_size];)"}
{"predict": "(11, MPI, MPI_Type_get_true_extent(item, &lb, &extent);)", "label": "(11, MPI, MPI_Type_get_extent(item, &lb, &extent);)"}
{"predict": "(47, CUDA, env.engine.reset(runtime->deserializeCudaEngine(env.serializedEngine->data(), env.serializedEngine->size()));); (19, CUDA, TrtUniquePtr<ICudaStream> profileStream(sample::createCudaStream(sample::gLogger.getTRTLogger()));); (55, CUDA, checkCudaRuntime(cudaSetDevice(0));)", "label": "(46, CUDA, env.engine.reset(runtime->deserializeCudaEngine(env.serializedEngine->data(), env.serializedEngine->size()));); (19, CUDA, auto profileStream = samplesCommon::makeCudaStream();)"}
{"predict": "(30, CUDA, (void) cudaFree( p );)", "label": "(28, CUDA, cudaFree( p );)"}
{"predict": "(38, OpenMP, #pragma omp target teams is_device_ptr(h)); (28, OpenMP, #pragma omp target teams is_device_ptr(aa)); (45, OpenMP, #pragma omp target teams is_device_ptr(rh)); (16, OpenMP, #pragma omp target teams is_device_ptr(k)); (32, OpenMP, #pragma omp target teams is_device_ptr(raa)); (18, OpenMP, #pragma omp target teams is_device_ptr(z)); (45, OpenMP, #pragma omp target teams is_device_ptr(da))", "label": "(49, OpenMP, #pragma omp target teams is_device_ptr(da)); (23, OpenMP, #pragma omp target teams is_device_ptr(z)); (27, OpenMP, #pragma omp target teams is_device_ptr(aa)); (16, OpenMP, #pragma omp target teams is_device_ptr(k)); (30, OpenMP, #pragma omp target teams is_device_ptr(raa)); (39, OpenMP, #pragma omp target teams is_device_ptr(rh)); (33, OpenMP, #pragma omp target teams is_device_ptr(h))"}
{"predict": "(11, MPI, tim::mpi::barrier(tim::mpi::comm_world_v);); (11, MPI, tim::mpi::finalize();)", "label": "(35, MPI, _measurements = get_measure()->mpi_get();)"}
{"predict": "(42, MPI, MPI_T_finalize();); (17, MPI, MPI_T_init_thread(required, &provided);)", "label": "(42, MPI, MPI_T_finalize();); (17, MPI, MPI_T_init_thread(required, &provided);)"}
{"predict": "(5, CUDA, __shared__ float reduce_smem[NUM_WARPS];)", "label": "(5, CUDA, __shared__ float reduce_smem[NUM_WARPS];)"}
{"predict": "(3, TBB, tbb::parallel_for( tbb::blocked_range<int>(0,m,m/nthread), GrowBy<MyVector>(v) );)", "label": "(3, TBB, tbb::parallel_for( tbb::blocked_range<int>(0,m,1000), GrowBy(v) );)"}
{"predict": "(77, OpenMP, #pragma omp parallel reduction(+ : r)); (44, OpenMP, #pragma omp parallel reduction(max : h.b)); (78, OpenMP, #pragma omp parallel private(fl)); (52, OpenMP, #pragma omp parallel reduction(&& : S2::S2s)); (73, OpenMP, #pragma omp parallel reduction(+ : fl)); (69, OpenMP, #pragma omp parallel reduction(+ : fl)); (53, OpenMP, #pragma omp parallel reduction(+ : o)); (18, OpenMP, #pragma omp parallel reduction(foo : argc); (35, OpenMP, #pragma omp parallel reduction(+ : ba)); (50, OpenMP, #pragma omp parallel reduction(& : e, g)); (47, OpenMP, #pragma omp parallel reduction(&& : S2::S2sc)); (43, OpenMP, #pragma omp parallel reduction(&& : S2::S2s)); (15, OpenMP, #pragma omp parallel reduction(\\)); (48, OpenMP, #pragma omp parallel reduction(+ : p), reduction(+ : p)); (20, OpenMP, #pragma omp parallel reduction(~ : argc)); (59, OpenMP, #pragma omp parallel reduction(min : i)); (30, OpenMP, #pragma omp parallel reduction(min : a, b, c, d, f)); (24, OpenMP, #pragma omp parallel reduction(|| : argc > 0? argv[1] : argv[2])); (1, OpenMP, #pragma omp parallel reduction(); (13, OpenMP, #pragma omp parallel reduction(&& : argc)); (60, OpenMP, #pragma omp parallel reduction(+ : m)); (56, OpenMP, #pragma omp parallel reduction(+ : fl)); (1, OpenMP, #pragma omp parallel reduction(argc); (48, OpenMP, #pragma omp parallel shared(i)); (15, OpenMP, #pragma omp parallel reduction(~ : S1)); (44, OpenMP, #pragma omp parallel reduction(+ : h, k, B::x)); (17, OpenMP, #pragma omp parallel reduction(min : a, b, c, d, f)); (33, OpenMP, #pragma omp parallel reduction(+ : o)); (53,", "label": "(54, OpenMP, #pragma omp parallel reduction(&& : S2::S2sc)); (25, OpenMP, #pragma omp parallel reduction(foo : argc); (44, OpenMP, #pragma omp parallel reduction(* : ca)); (78, OpenMP, #pragma omp for reduction(- : fl)); (28, OpenMP, #pragma omp parallel reduction(~ : argc)); (53, OpenMP, #pragma omp parallel reduction(+ : h, k, B::x)); (82, OpenMP, #pragma omp parallel reduction(task, + : m)); (79, OpenMP, #pragma omp parallel reduction(+ : m)); (36, OpenMP, #pragma omp parallel reduction(+ : ba)); (39, OpenMP, #pragma omp parallel reduction(- : da)); (59, OpenMP, #pragma omp parallel reduction(+ : r)); (5, OpenMP, #pragma omp parallel reduction(); (50, OpenMP, #pragma omp parallel private(i), reduction(+ : j), reduction(+ : q)); (47, OpenMP, #pragma omp parallel reduction(+ : o)); (-2, OpenMP, #pragma omp parallel); (37, OpenMP, #pragma omp parallel reduction(&& : S2::S2s)); (2, OpenMP, #pragma omp parallel reduction(-); (17, OpenMP, #pragma omp parallel reduction(&& : argc, z)); (-4, OpenMP, #pragma omp parallel reduction); (52, OpenMP, #pragma omp parallel shared(i)); (44, OpenMP, #pragma omp parallel private(k)); (21, OpenMP, #pragma omp parallel reduction(max : h.b)); (6, OpenMP, #pragma omp parallel reduction(| : argc,); (15, OpenMP, #pragma omp parallel reduction(+ : a, b, c, d, f)); (49, OpenMP, #pragma omp parallel reduction(max : j)); (5, OpenMP, #pragma omp parallel reduction(|| : argc > 0? argv[1] : argv[2])); (14, OpenMP, #pragma omp parallel reduction(min : a, b, c, d, f)); (45, OpenMP, #pragma omp parallel reduction(min : i)); (-4, OpenMP, #pragma omp parallel reduction(\\)); (-9, OpenMP, #pragma omp parallel reduction()); (60, OpenMP, #pragma omp parallel reduction(+:gptr[:argc])); (45, OpenMP, #pragma omp for private(fl)); (-10, OpenMP, #pragma omp parallel reduction(*)); (17, OpenMP, #pragma omp parallel reduction(^ : fl)); (2, OpenMP, #pragma omp parallel reduction(^ : S1)); (30, OpenMP, #pragma omp parallel reduction(+ : p), reduction(+ : p)); (20, OpenMP, #pragma omp parallel reduction(& : e, g))"}
{"predict": "(3, TBB, static const size_t number_of_threads = tbb::task_scheduler_init::default_num_threads();); (1, TBB, typedef tbb::concurrent_lru_cache<cache_type,tbb::cache_aligned_allocator<cache_type>,tbb::simple_lru_policy> cache_type;)", "label": "(10, TBB, static const size_t number_of_threads = 4 * tbb::task_scheduler_init::default_num_threads();)"}
{"predict": "(9, CUDA, cuda_safe_call(cudaMalloc((void**) &dZ, N * sizeof(double)));); (7, CUDA, cuda_safe_call(cudaMalloc((void**) &dX, N * sizeof(double)));); (7, CUDA, cuda_safe_call(cudaMalloc((void**) &dY, N * sizeof(double)));)", "label": "(8, CUDA, cuda_safe_call(cudaMalloc((void**) &dX, N * sizeof(double)));); (8, CUDA, cuda_safe_call(cudaMalloc((void**) &dY, N * sizeof(double)));); (8, CUDA, cuda_safe_call(cudaMalloc((void**) &dZ, N * sizeof(double)));); (11, CUDA, cuda_safe_call(cudaDeviceSynchronize());)"}
{"predict": "(8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);)", "label": "(8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);)"}
{"predict": "(15, CUDA, cudaMalloc(&buffersD2D[d], numElems * sizeof(int));); (9, CUDA, cudaEventCreateWithFlags(&flag, cudaEventDisableTiming);); (7, CUDA, cudaStreamCreate(&stream0[d]);); (14, CUDA, cudaMemset(buffersD2D[d], 0, numElems * sizeof(int));); (37, CUDA, cudaStreamSynchronize(stream1[j]);); (10, CUDA, cudaMalloc(&buffers[d], numElems * sizeof(int));); (8, CUDA, cudaMemset(buffers[d], 0, numElems * sizeof(int));); (89, CUDA, cudaEventDestroy(stop[d]);); (83, CUDA, cudaEventSynchronize(stop[d]);); (13, CUDA, cudaStreamCreate(&stream1[d]);); (75, CUDA, cudaEventRecord(stop[d], stream1[j]);); (8, CUDA, cudaSetDevice(d);); (78, CUDA, cudaEventRecord(start[d], stream0[i]);); (85, CUDA, cudaEventDestroy(start[d]);); (81, CUDA, cudaEventElapsedTime(&time_ms, start[d], stop[d]);); (76, CUDA, cudaEventSynchronize(start[d]);); (86, CUDA, cudaStreamDestroy(stream1[d]);); (84, CUDA, cudaStreamDestroy(stream0[d]);); (6, CUDA, cudaStreamSynchronize(stream0[i]);); (7, CUDA, cudaEventCreate(&stop[d]);); (4, CUDA, cudaEventCreate(&start[d]);); (1, CUDA, cudaSetDevice(j);); (6, CUDA, cudaMemset(buffersD2D[d], 0, numElems * sizeof(int));); (8, CUDA, cudaSetDevice(i);)", "label": "(147, CUDA, cudaStreamDestroy(stream0[d]);); (16, CUDA, cudaMalloc(&buffersD2D[d], numElems * sizeof(int));); (141, CUDA, cudaEventDestroy(start[d]);); (41, CUDA, cudaDeviceEnablePeerAccess(i, 0);); (7, CUDA, cudaCheckError();); (136, CUDA, cudaFree(buffersD2D[d]);); (35, CUDA, cudaDeviceEnablePeerAccess(j, 0);); (138, CUDA, cudaEventDestroy(stop[d]);); (25, CUDA, cudaSetDevice(i);); (106, CUDA, cudaDeviceDisablePeerAccess(i);); (139, CUDA, cudaStreamDestroy(stream1[d]);); (83, CUDA, cudaEventRecord(stop[i], stream0[i]);); (10, CUDA, cudaEventCreate(&stop[d]);); (140, CUDA, cudaFreeHost((void *)flag);); (79, CUDA, cudaStreamWaitEvent(stream0[i], stop[j], 0);); (3, CUDA, cudaMemset(buffersD2D[d], 0, numElems * sizeof(int));); (35, CUDA, cudaStreamSynchronize(stream1[j]);); (52, CUDA, cudaStreamWaitEvent(stream1[j], start[i], 0);); (-2, CUDA, cudaMemset(buffers[d], 0, numElems * sizeof(int));); (-4, CUDA, cudaMalloc(&buffers[d], numElems * sizeof(int));); (-10, CUDA, cudaHostAlloc((void **)&flag, sizeof(*flag), cudaHostAllocPortable);); (119, CUDA, cudaFree(buffers[d]);); (28, CUDA, cudaStreamSynchronize(stream0[i]);); (-9, CUDA, cudaSetDevice(d);); (0, CUDA, cudaStreamCreateWithFlags(&stream0[d], cudaStreamNonBlocking);); (13, CUDA, cudaDeviceCanAccessPeer(&access, i, j);); (42, CUDA, cudaEventRecord(start[i], stream0[i]);); (16, CUDA, cudaSetDevice(j);); (64, CUDA, cudaEventRecord(stop[j], stream1[j]);); (74, CUDA, cudaEventElapsedTime(&time_ms, start[i], stop[i]);); (-10, CUDA, cudaEventCreate(&start[d]);); (-5, CUDA, cudaStreamCreateWithFlags(&stream1[d], cudaStreamNonBlocking);); (81, CUDA, cudaDeviceDisablePeerAccess(j);)"}
{"predict": "(34, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(5, OpenACC, #pragma acc parallel loop)"}
{"predict": "(13, OpenMP, #pragma omp master taskloop collapse (1)); (33, OpenMP, #pragma omp master taskloop collapse (N)); (17, OpenMP, #pragma omp master taskloop collapse ((ST > 0)? 1 + ST : 2)); (21, OpenMP, #pragma omp master taskloop collapse (foobool(argc)), collapse (true), collapse (-5)); (1, OpenMP, #pragma omp master taskloop collapse ()); (21, OpenMP, #pragma omp master taskloop collapse (S)); (1, OpenMP, #pragma omp master taskloop collapse (argc); (30, OpenMP, #pragma omp master taskloop collapse (2)); (6, OpenMP, #pragma omp master taskloop collapse (ST); (-6, OpenMP, #pragma omp master taskloop collapse); (1, OpenMP, #pragma omp master taskloop collapse (ST)", "label": "(10, OpenMP, #pragma omp master taskloop collapse (argc); (1, OpenMP, #pragma omp master taskloop collapse); (13, OpenMP, #pragma omp master taskloop collapse (1)); (14, OpenMP, #pragma omp master taskloop collapse ((ST > 0)? 1 + ST : 2)); (31, OpenMP, #pragma omp master taskloop collapse (N)); (32, OpenMP, #pragma omp master taskloop collapse (2)); (16, OpenMP, #pragma omp master taskloop collapse (foobool(argc)), collapse (true), collapse (-5)); (-3, OpenMP, #pragma omp master taskloop collapse (); (-2, OpenMP, #pragma omp master taskloop collapse ()); (15, OpenMP, #pragma omp master taskloop collapse (S)); (3, OpenMP, #pragma omp master taskloop collapse (ST)"}
{"predict": "(5, CUDA, __shared__ float reduce_smem[NUM_WARPS];)", "label": "(5, CUDA, __shared__ float reduce_smem[NUM_WARPS];)"}
{"predict": "(107, CUDA, auto& gemm_plan = make_gemm_plan(parsed_args);); (159, CUDA, cudaDeviceSynchronize();); (100, CUDA, cudaStreamCreate(&stream);); (166, CUDA, cudaFree(work_buffer);); (164, CUDA, cudaFreeHost(host_buffer);); (160, CUDA, cudaFree(gemm_result);); (154, CUDA, cudaFree(A);); (155, CUDA, cudaFree(B);); (155, CUDA, cudaFree(C);); (153, CUDA, cudaFree(D);); (92, CUDA, cudaMallocHost(&host_buffer, work_size);); (92, CUDA, cudaMalloc(&work_buffer, work_size);); (153, CUDA, cudaFree(workspace);); (150, CUDA, cudaFree(gemv_result);); (145, CUDA, cudaFree(BatchedGemmResult);); (102, CUDA, cudaMalloc(&gemm_result, sizeof(ElementC));); (143, CUDA, cudaFree(Accumulators);); (98, CUDA, cudaMalloc(&B, shape.Z_len * sizeof(ElementB));); (143, CUDA, cudaFree(ElementEpilogue);); (100, CUDA, cudaMalloc(&C, shape.Z_len * sizeof(ElementC));)", "label": "(161, CUDA, thrust::device_vector<ElementA> d_A = h_A;); (183, CUDA, cuda_err = cudaDeviceSynchronize();); (148, CUDA, thrust::host_vector<ElementB> h_B(N_size * K_size * L_size);); (184, CUDA, std::cerr << cudaGetErrorString(cuda_err) << \"\\n\";); (148, CUDA, thrust::host_vector<ElementD> h_D(M_size * N_size * L_size);); (157, CUDA, thrust::device_vector<ElementB> d_B = h_B;); (158, CUDA, thrust::device_vector<ElementD> cutlass_result = h_D;); (144, CUDA, thrust::host_vector<ElementC> h_C(M_size * N_size * L_size);); (213, CUDA, thrust::host_vector<ElementD> h_cutlass_result = cutlass_result;); (156, CUDA, thrust::device_vector<ElementD> reference_result = h_D;); (153, CUDA, thrust::device_vector<ElementC> d_C = h_C;); (138, CUDA, thrust::host_vector<ElementA> h_A(M_size * K_size * L_size);)"}
{"predict": "(9, CUDA, gpuErrchk(cudaDeviceReset());); (3, CUDA, gpuErrchk(cudaFree(0));); (1, CUDA, gpuErrchk(cudaSetDevice(0));); (14, CUDA, gpuErrchk(cudaGetDeviceCount(&deviceCount));)", "label": "(18, CUDA, gpuErrchk(cudaGetDeviceCount(&deviceCount));); (1, CUDA, gpuErrchk(cudaSetDevice(0));); (2, CUDA, gpuErrchk(cudaFree(0));); (7, CUDA, gpuErrchk(cudaDeviceReset());)"}
{"predict": "(36, MPI, MPI_Error_string(ERR_CODE, aux_string, &L);)", "label": "(36, MPI, MPI_Error_string(ERR_CODE, aux_string, &L);)"}
{"predict": "(40, OpenMP, #pragma omp target teams); (145, OpenMP, #pragma omp target update from(u21i, u31i, u41i, u51i)); (37, OpenMP, #pragma omp distribute private(k, j, i, m, zeta, eta, xi)); (21, OpenMP, #pragma omp distribute private(k, j, i, m)); (140, OpenMP, #pragma omp target update from(u21j, u31j, u41j, u51j)); (74, OpenMP, #pragma omp distribute private(k, j, i, u21, q)); (132, OpenMP, #pragma omp target update from(u21k, u31k, u41k, u51k)); (135, OpenMP, #pragma omp target update from(u21m1, u31m1, u41m1, u51m1)); (138, OpenMP, #pragma omp target update from(u21l, u31l, u41l, u51l)); (94, OpenMP, #pragma omp distribute private(k, j, i, m)); (65, OpenMP, #pragma omp target teams private(u21, q)); (13, OpenMP, #pragma omp target teams private(k, j, i, m, zeta, eta, xi)); (31, OpenMP, #pragma omp target update to(rsd[m][k][j][i])); (122, OpenMP, #pragma omp target update from(u21i, u31i, u41i, u51i)); (125, OpenMP, #pragma omp target update from(u21l, u31l, u41l, u51l)); (128, OpenMP, #pragma omp target update from(u21m1, u31m1, u41m1, u51m1)); (61, OpenMP, #pragma omp target update to(flux_G[m][k][j][i])); (104, OpenMP, #pragma omp target teams shared(u", "label": "(22, OpenMP, #pragma omp distribute); (17, OpenMP, #pragma omp target data map(alloc: frct, rsd, ce, flux_G)); (19, OpenMP, #pragma omp target teams)"}
{"predict": "(89, CUDA, CUDA_RT_CALL(cudaFree(a));); (22, CUDA, CUDA_RT_CALL(cudaDeviceSynchronize());); (27, CUDA, CUDA_RT_CALL(cudaMemset(a, 0, nx * ny * sizeof(real)));); (83, CUDA, CUDA_RT_CALL(cudaFree(l2_norm_d));); (24, CUDA, CUDA_RT_CALL(cudaMemset(a_new, 0, nx * ny * sizeof(real)));); (60, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(compute_stream, push_bottom_done, 0));); (16, CUDA, CUDA_RT_CALL(cudaStreamCreate(&push_bottom_stream));); (76, CUDA, CUDA_RT_CALL(cudaStreamSynchronize(compute_stream));); (62, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(push_bottom_stream, compute_done, 0));); (8, CUDA, CUDA_RT_CALL(cudaStreamCreate(&compute_stream));); (63, CUDA, CUDA_RT_CALL(cudaEventRecord(push_bottom_done, push_bottom_stream));); (66, CUDA, CUDA_RT_CALL(cudaEventSynchronize(compute_done));); (77, CUDA, CUDA_RT_CALL(cudaStreamDestroy(compute_stream));); (66, CUDA, CUDA_RT_CALL(cudaEventSynchronize(push_top_done));); (78, CUDA, CUDA_RT_CALL(cudaEventDestroy(compute_done));); (67, CUDA, CUDA_RT_CALL(cudaEventSynchronize(push_bottom_done));); (40, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(push_top_stream, compute_done, 0));); (74, CUDA, CUDA_RT_CALL(cudaEventDestroy(push_bottom_done));); (74, CUDA, CUDA_RT_CALL(cudaEventDestroy(push_top_done));); (77, CUDA, CUDA_RT_CALL(cudaFree(a_new));); (75, CUDA, CUDA_RT_CALL(cudaFreeHost(l2_norm_h));); (13, CUDA, CUDA", "label": "(22, CUDA, CUDA_RT_CALL(cudaMemset(a_new, 0, nx * ny * sizeof(real)));); (100, CUDA, CUDA_RT_CALL(cudaStreamDestroy(compute_stream));); (105, CUDA, CUDA_RT_CALL(cudaFree(a));); (23, CUDA, CUDA_RT_CALL(cudaDeviceSynchronize());); (100, CUDA, CUDA_RT_CALL(cudaFree(l2_norm_d));); (27, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&push_top_done, cudaEventDisableTiming));); (24, CUDA, CUDA_RT_CALL(cudaStreamCreate(&push_bottom_stream));); (26, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&push_bottom_done, cudaEventDisableTiming));); (49, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(compute_stream, push_bottom_done, 0));); (67, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(push_bottom_stream, compute_done, 0));); (69, CUDA, CUDA_RT_CALL(cudaEventRecord(push_bottom_done, push_bottom_stream));); (89, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_top_stream));); (86, CUDA, CUDA_RT_CALL(cudaEventDestroy(compute_done));); (81, CUDA, CUDA_RT_CALL(cudaMemcpy(a_ref_h, a, nx * ny * sizeof(real), cudaMemcpyDeviceToHost));); (82, CUDA, CUDA_RT_CALL(cudaEventDestroy(push_bottom_done));); (14, CUDA, CUDA_RT_CALL(cudaStreamCreate(&push_top_stream));); (81, CUDA, CUDA_RT_CALL(cudaEventDestroy(push_top_done));); (65, CUDA, CUDA_RT_CALL(cudaStreamSynchronize(compute_stream));); (44, CUDA, CUDA_RT_CALL(cudaEventRecord(compute_done, compute_stream));); (17, CUDA, CUDA_RT_CALL(cudaMallocHost(&l2_norm_h, sizeof(real)));); (54, CUDA, CUDA_RT_CALL(cudaEventRecord(push_top_done, push_top_stream));); (33, CUDA, CUDA_RT_CALL(cudaMemsetAsync(l2_norm_d, 0, sizeof(real), compute_stream));); (6, CUDA, CUDA_RT_CALL(cudaStreamCreate(&compute_stream));); (-2, CUDA, CUDA_RT_CALL(cudaMemset(a, 0, nx * ny * sizeof(real)));); (7, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&compute_done, cudaEventDisableTiming));); (81, CUDA, CUDA_RT_CALL(cudaFree(a_new));); (45, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(push_top_stream, compute_done, 0));); (72, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_bottom_stream));); (28, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(compute_stream, push_top_done, 0));); (6, CUDA, CUDA_RT_CALL(cudaMalloc(&l2_norm_d, sizeof(real)));); (21, MPI, double start = MPI_Wtime();); (-13, CUDA, CUDA_RT_CALL(cudaMalloc(&a, nx * ny * sizeof(real)));); (71, CUDA, CUDA_RT_CALL(cudaFreeHost(l2_norm_h));); (-14, CUDA, CUDA_RT_CALL(cudaMalloc(&a_new, nx * ny * sizeof(real)));)"}
{"predict": "(29, MPI, MPI_Bcast( work, diag_len[p]*knsupc, MPI_DOUBLE, pkk, grid->comm );); (26, MPI, MPI_Bcast( work, lwork, MPI_DOUBLE, pkk, grid->comm );)", "label": "(30, MPI, MPI_Bcast( work, diag_len[p], MPI_DOUBLE, pkk, grid->comm );); (27, MPI, MPI_Bcast( work, lwork, MPI_DOUBLE, pkk, grid->comm );)"}
{"predict": "(4, CUDA, const T** input_ptrs = GetCudaDeviceArrayOnDevice(&input_ptr_data);)", "label": "(4, CUDA, const T** input_ptrs = GetCudaDeviceArrayOnDevice(&input_ptr_data);)"}
{"predict": "(13, MPI, MPI_Allreduce(bins_ptr, global_ptr, bins_size, MPI_DOUBLE, MPI_SUM, mpi_comm);); (17, MPI, MPI_Allreduce(bins_ptr, global_ptr, bins_size, MPI_DOUBLE, MPI_MIN, mpi_comm);); (10, MPI, MPI_Comm mpi_comm = MPI_Comm_f2c(flow::Workspace::default_mpi_comm());); (18, MPI, MPI_Allreduce(bins_ptr, global_ptr, bins_size, MPI_DOUBLE, MPI_MAX, mpi_comm);)", "label": "(17, MPI, MPI_Allreduce(bins_ptr, global_ptr, bins_size, MPI_DOUBLE, MPI_MIN, mpi_comm);); (12, MPI, MPI_Allreduce(bins_ptr, global_ptr, bins_size, MPI_DOUBLE, MPI_SUM, mpi_comm);); (19, MPI, MPI_Allreduce(bins_ptr, global_ptr, bins_size, MPI_DOUBLE, MPI_MAX, mpi_comm);); (2, MPI, MPI_Comm mpi_comm = MPI_Comm_f2c(flow::Workspace::default_mpi_comm());)"}
{"predict": "(11, OpenMP, #pragma omp parallel for schedule (guided))", "label": "(71, OpenMP, const int th=omp_get_thread_num();); (10, OpenMP, #pragma omp parallel for schedule (guided))"}
{"predict": "(16, MPI, MPI_Dims_create(tests[k].size, tests[k].dim, dims);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);)", "label": "(10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (15, MPI, MPI_Dims_create(tests[k].size, tests[k].dim, dims);)"}
{"predict": "(18, CUDA, __shared__ float b_smem[BK * BN];); (16, CUDA, __shared__ float a_smem[BM * BK];)", "label": "(17, CUDA, __shared__ float a_smem[BM * BK];); (17, CUDA, __shared__ float b_smem[BK * BN];)"}
{"predict": "(83, OpenCL, err = clReleaseMemObject(image_buffer);); (85, OpenCL, err = clReleaseMemObject(buffer);)", "label": "(86, OpenCL, err = clReleaseMemObject(buffer);); (82, OpenCL, err = clReleaseMemObject(image_buffer);)"}
{"predict": "(12, CUDA, cudaMallocHost((void**)&h_a, bufsize);); (66, CUDA, cudaFree(d_c);); (42, CUDA, cudaDeviceSynchronize();); (11, CUDA, cudaMallocHost((void**)&h_c, bufsize);); (62, CUDA, cudaFree(d_b);); (9, CUDA, cudaMallocHost((void**)&h_b, bufsize);); (17, CUDA, cudaMalloc((void**)&d_a, bufsize);); (60, CUDA, cudaFree(d_a);); (56, CUDA, cudaFreeHost(h_b);); (56, CUDA, cudaFreeHost(h_a);); (56, CUDA, cudaFreeHost(h_c);); (14, CUDA, cudaMalloc((void**)&d_c, bufsize);); (14, CUDA, cudaMalloc((void**)&d_b, bufsize);)", "label": "(12, CUDA, cudaMallocHost((void**)&h_a, bufsize);); (13, CUDA, cudaMallocHost((void**)&h_c, bufsize);); (43, CUDA, cudaDeviceSynchronize();); (20, CUDA, cudaMalloc((void**)&d_a, bufsize);); (63, CUDA, cudaFree(d_c);); (61, CUDA, cudaFree(d_b);); (7, CUDA, cudaMallocHost((void**)&h_b, bufsize);); (64, CUDA, cudaFreeHost(h_b);); (64, CUDA, cudaFreeHost(h_c);); (56, CUDA, cudaFree(d_a);); (60, CUDA, cudaFreeHost(h_a);); (14, CUDA, cudaMalloc((void**)&d_c, bufsize);); (12, CUDA, cudaMalloc((void**)&d_b, bufsize);)"}
{"predict": "(43, OpenMP, lis2mdl_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)", "label": "(43, OpenMP, lis2mdl_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)"}
{"predict": "(3, TBB, ValueType r1 = tbb::parallel_reduce( m_range, I, Accumulator(), Sum(), partitioner );)", "label": "(3, TBB, ValueType r1 = tbb::parallel_reduce( m_range, I, Accumulator(), Sum(), partitioner );)"}
{"predict": "(13, CUDA, __shared__ float r[_N_], k[_N_], u[_N_], w[_N_];)", "label": "(11, CUDA, __shared__ float r[_N_], k[_N_], u[_N_], w[_N_];)"}
{"predict": "(49, CUDA, flamegpu::CUDASimulation cudaSimulation(model);)", "label": "(50, CUDA, flamegpu::CUDASimulation cudaSimulation(model, argc, argv);)"}
{"predict": "(81, CUDA, cudaError_t result = cudaDeviceSynchronize();); (12, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;); (10, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);)", "label": "(12, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (13, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;); (80, CUDA, cudaError_t result = cudaDeviceSynchronize();)"}
{"predict": "(22, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)", "label": "(22, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(11, MPI, MPI_Type_get_extent(int_with_key, &lb, &extent);)", "label": "(11, MPI, MPI_Type_get_extent(int_with_key, &lb, &extent);)"}
{"predict": "(35, OpenCL, retVal = clEnqueueNDRangeKernel(clCommandQ, clKernel, 1, offset, gws, nullptr, 0, nullptr, nullptr);); (14, OpenCL, retVal = clEnqueueNDRangeKernel(clCommandQ, clKernel, 1, offset, gws, nullptr, 1, &eventBlocking, &eventOut);); (39, OpenCL, clReleaseEvent(eventOut);); (10, OpenCL, clCommandQ = clCreateCommandQueue(context->getDevice(0), properties, &retVal);); (37, OpenCL, clReleaseEvent(callbackEvent);); (39, OpenCL, clReleaseCommandQueue(clCommandQ);)", "label": "(15, OpenCL, retVal = clEnqueueNDRangeKernel(clCommandQ, clKernel, 1, offset, gws, nullptr, 1, &eventBlocking, &eventOut);); (13, OpenCL, clCommandQ = clCreateCommandQueue(context, devices[0], properties, &retVal);); (38, OpenCL, clReleaseEvent(eventOut);); (31, OpenCL, clWaitForEvents(1, &callbackEvent);); (37, OpenCL, clReleaseCommandQueue(clCommandQ);); (28, OpenCL, clWaitForEvents(1, &eventOut);)"}
{"predict": "(38, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (40, CUDA, CUDA_CHECK(cudaFree(d_data));)", "label": "(38, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (40, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(12, OpenMP, #pragma omp target)", "label": "(12, OpenMP, #pragma omp target)"}
{"predict": "(10, MPI, MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);)", "label": "(10, MPI, MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);)"}
{"predict": "(34, MPI, MPI_Type_free(&type);); (16, MPI, MPI_Type_get_extent(type, &lb, &extent);); (7, MPI, err = MPI_Type_indexed(3, len, disp, MPI_INT, &type);)", "label": "(33, MPI, MPI_Type_free(&type);); (16, MPI, MPI_Type_get_extent(type, &lb, &extent);); (7, MPI, err = MPI_Type_indexed(3, len, disp, MPI_INT, &type);)"}
{"predict": "(23, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (17, MPI, MPI_Alltoallw_c(sendbuf, counts, displs, types, recvbuf, counts, displs, types, comm);)", "label": "(23, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (17, MPI, MPI_Alltoallw_c(sendbuf, counts, displs, types, recvbuf, counts, displs, types, comm);)"}
{"predict": "(8, OpenMP, #pragma omp target exit data map(delete: i)); (4, OpenMP, #pragma omp target enter data map(alloc: i)); (5, OpenMP, #pragma omp target map(present, delete: i))", "label": "(3, OpenMP, #pragma omp target enter data map(alloc:i)); (6, OpenMP, #pragma omp target data map(present, alloc: i)); (7, OpenMP, #pragma omp target exit data map(delete:i))"}
{"predict": "(9, MPI, errcode = MPI_Grequest_complete(aio_req->req);)", "label": "(10, MPI, errcode = MPI_Grequest_complete(aio_req->req);)"}
{"predict": "(15, OpenMP, #pragma omp task private(i)); (10, OpenMP, #pragma omp single); (7, OpenMP, #pragma omp parallel); (16, OpenMP, #pragma omp master)", "label": "(20, OpenMP, #pragma omp flush); (13, OpenMP, #pragma omp task private(sum) shared(result, known_sum)); (7, OpenMP, #pragma omp parallel); (8, OpenMP, #pragma omp single); (21, OpenMP, #pragma omp critical)"}
{"predict": "(20, OpenCL, clReleaseMemObject(mobjAP_);); (16, OpenCL, clReleaseMemObject(mobjX_);)", "label": "(20, OpenCL, clReleaseMemObject(mobjAP_);); (16, OpenCL, clReleaseMemObject(mobjX_);)"}
{"predict": "(40, OpenCL, TEST_OCL_CHECK(clReleaseCommandQueue(ocl_queue));)", "label": "(27, OpenCL, TEST_OCL_CHECK(clReleaseCommandQueue(ocl_queue));)"}
{"predict": "(9, MPI, MPI_Comm_rank(comm, &comm_rank);); (10, MPI, MPI_Comm_size(comm, &comm_size);)", "label": "(9, MPI, MPI_Comm_rank(comm, &comm_rank);); (10, MPI, MPI_Comm_size(comm, &comm_size);)"}
{"predict": "(45, CUDA, cudaFree(d_messages);); (43, CUDA, cudaFree(d_secret_keys);); (41, CUDA, cudaFree(d_signatures);); (25, CUDA, cudaMemcpy(d_secret_keys, secret_keys.data(), length_secret_key * batch, cudaMemcpyHostToDevice);); (17, CUDA, cudaMalloc(reinterpret_cast<void**>(&d_secret_keys), length_secret_key * batch);); (32, CUDA, cudaMemcpy(signatures.data(), d_signatures, length_signature_aligned * batch, cudaMemcpyDeviceToHost);); (15, CUDA, cudaMalloc(reinterpret_cast<void**>(&d_messages), message_size * batch);); (21, CUDA, cudaMemcpy(d_messages, messages.data(), message_size * batch, cudaMemcpyHostToDevice);); (12, CUDA, cudaMalloc(reinterpret_cast<void**>(&d_signatures), length_signature_aligned * batch);)", "label": "(36, CUDA, cudaMemcpy(signatures.data(), d_signatures, length_signature_aligned * batch, cudaMemcpyDeviceToHost);); (40, CUDA, cudaFree(d_secret_keys);); (26, CUDA, cudaMemcpy(d_secret_keys, secret_keys.data(), length_secret_key * batch, cudaMemcpyHostToDevice);); (26, CUDA, cudaMemcpy(d_messages, messages.data(), message_size * batch, cudaMemcpyHostToDevice);); (39, CUDA, cudaFree(d_messages);); (17, CUDA, cudaMalloc(reinterpret_cast<void**>(&d_secret_keys), length_secret_key * batch);); (15, CUDA, cudaMalloc(reinterpret_cast<void**>(&d_signatures), length_signature_aligned * batch);); (35, CUDA, cudaFree(d_signatures);); (15, CUDA, cudaMalloc(reinterpret_cast<void**>(&d_messages), message_size * batch);)"}
{"predict": "(19, OpenMP, #pragma omp parallel for)", "label": "(19, OpenMP, #pragma omp parallel for)"}
{"predict": "(13, MPI, MPI_Recv(&factor, 1, MPI_INT, root, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (4, MPI, MPI_Init(&argc, &argv);); (26, MPI, MPI_Finalize();); (2, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (2, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (14, MPI, MPI_Wait(&request, MPI_STATUS_IGNORE);); (2, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (9, MPI, MPI_Send(&factor, 1, MPI_INT, root, tag, MPI_COMM_WORLD);)", "label": "(27, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Init(NULL, NULL);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (20, MPI, MPI_Wait(&request, MPI_STATUS_IGNORE);); (6, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (9, MPI, MPI_Bcast(&factor, 1, MPI_INT, root, MPI_COMM_WORLD);)"}
{"predict": "(3, OpenMP, #pragma omp target); (4, OpenMP, #pragma omp distribute parallel for simd); (2, OpenMP, #pragma omp teams)", "label": "(3, OpenMP, #pragma omp target); (4, OpenMP, #pragma omp distribute parallel for simd); (2, OpenMP, #pragma omp teams)"}
{"predict": "(6, OpenMP, #pragma omp target teams distribute parallel for)", "label": "(5, OpenMP, #pragma omp target teams distribute parallel for)"}
{"predict": "(43, MPI, err = MPI_Type_size(eviltype, &val);); (98, MPI, MPI_Type_free(&inttype);); (98, MPI, MPI_Type_free(&tmptype);); (4, MPI, err = MPI_Type_contiguous(4, MPI_BYTE, &tmptype);); (97, MPI, MPI_Type_free(&eviltype);); (17, MPI, err = MPI_Type_create_resized(tmptype, 6, -9, &inttype);); (74, MPI, err = MPI_Type_get_true_extent(eviltype, &true_lb, &aval);); (25, MPI, err = MPI_Type_vector(3, 4, -5, inttype, &eviltype);); (51, MPI, err = MPI_Type_get_extent(eviltype, &lb, &extent);)", "label": "(43, MPI, err = MPI_Type_size(eviltype, &val);); (100, MPI, MPI_Type_free(&inttype);); (19, MPI, err = MPI_Type_create_resized(tmptype, 6, -9, &inttype);); (55, MPI, err = MPI_Type_get_extent(eviltype, &lb, &extent);); (28, MPI, err = MPI_Type_vector(3, 4, 5, inttype, &eviltype);); (95, MPI, MPI_Type_free(&tmptype);); (3, MPI, err = MPI_Type_contiguous(4, MPI_BYTE, &tmptype);); (95, MPI, MPI_Type_free(&eviltype);); (71, MPI, err = MPI_Type_get_true_extent(eviltype, &true_lb, &aval);)"}
{"predict": "(79, CUDA, CHECK(cudaFree(d_y));); (56, CUDA, CHECK(cudaMemcpy(NL, d_NL, N*M*sizeof(int), cudaMemcpyDefault));); (56, CUDA, CHECK(cudaMemcpy(NN, d_NN, N*sizeof(int), cudaMemcpyDefault));); (46, CUDA, CHECK(cudaMemcpy(d_y, y.data(), N*sizeof(real), cudaMemcpyDefault));); (32, CUDA, CHECK(cudaMalloc(&d_NL, N*M*sizeof(int)));); (30, CUDA, CHECK(cudaMalloc(&d_NN, N*sizeof(int)));); (72, CUDA, CHECK(cudaFree(d_NN));); (46, CUDA, CHECK(cudaMemcpy(d_x, x.data(), N*sizeof(real), cudaMemcpyDefault));); (32, CUDA, CHECK(cudaMalloc(&d_y, N*sizeof(real)));); (67, CUDA, CHECK(cudaFree(d_x));); (30, CUDA, CHECK(cudaMalloc(&d_x, N*sizeof(real)));); (64, CUDA, CHECK(cudaFree(d_NL));)", "label": "(78, CUDA, CHECK(cudaFree(d_y));); (74, CUDA, CHECK(cudaFree(d_NN));); (56, CUDA, CHECK(cudaMemcpy(NL, d_NL, N*M*sizeof(int), cudaMemcpyDefault));); (34, CUDA, CHECK(cudaMalloc(&d_NL, N*M*sizeof(int)));); (32, CUDA, CHECK(cudaMalloc(&d_NN, N*sizeof(int)));); (46, CUDA, CHECK(cudaMemcpy(d_y, y.data(), N*sizeof(real), cudaMemcpyDefault));); (51, CUDA, CHECK(cudaMemcpy(NN, d_NN, N*sizeof(int), cudaMemcpyDefault));); (33, CUDA, CHECK(cudaMalloc(&d_y, N*sizeof(real)));); (69, CUDA, CHECK(cudaFree(d_x));); (30, CUDA, CHECK(cudaMalloc(&d_x, N*sizeof(real)));); (66, CUDA, CHECK(cudaFree(d_NL));)"}
{"predict": "(35, MPI, MPI_Recv(rmsg2, 2, MPI_DOUBLE, src, tag, comm, MPI_STATUS_IGNORE);); (32, MPI, MPI_Recv(rmsg3, 17, MPI_CHAR, src, tag, comm, MPI_STATUS_IGNORE);); (30, MPI, MPI_Recv(rmsg1, 6, MPI_CHAR, src, tag, comm, MPI_STATUS_IGNORE);); (35, MPI, MPI_Buffer_detach(&bbuf, &bsize);); (55, MPI, MPI_Send(NULL, 0, MPI_CHAR, dest, tag, comm);); (25, MPI, MPI_Pack(msg2, 2, MPI_DOUBLE, buf, bufsize, &s1, comm);); (23, MPI, MPI_Pack(msg1, 6, MPI_CHAR, buf, bufsize, &s1, comm);); (24, MPI, MPI_Pack(msg3, 17, MPI_CHAR, buf, bufsize, &s2, comm);); (4, MPI, MPI_Comm_rank(comm, &rank);); (47, MPI, MPI_Barrier(comm);); (42, MPI, MPI_Buffer_attach(bbuf, bsize);); (50, MPI, MPI_Send(NULL, 0, MPI_DOUBLE, dest, tag, comm);); (20, MPI, MPI_Bsend(buf, bufsize, MPI_CHAR, dest, tag, comm);); (17, MPI, MPI_Bsend(buf, bufsize, MPI_DOUBLE, dest, tag, comm);)", "label": "(19, MPI, MPI_Pack_size(2, MPI_DOUBLE, comm, &s2);); (38, MPI, MPI_Recv(rmsg2, 10, MPI_DOUBLE, src, tag, comm, MPI_STATUS_IGNORE);); (21, MPI, MPI_Buffer_attach(buf, bufsize);); (56, MPI, MPI_Buffer_detach(&bbuf, &bsize);); (34, MPI, MPI_Recv(rmsg1, 7, MPI_CHAR, src, tag, comm, MPI_STATUS_IGNORE);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (27, MPI, MPI_Bsend(msg2, 2, MPI_DOUBLE, dest, tag, comm);); (13, MPI, MPI_Pack_size(17, MPI_CHAR, comm, &s3);); (24, MPI, MPI_Bsend(msg1, 7, MPI_CHAR, dest, tag, comm);); (25, MPI, MPI_Bsend(msg3, 17, MPI_CHAR, dest, tag, comm);); (8, MPI, MPI_Pack_size(7, MPI_CHAR, comm, &s1);); (29, MPI, MPI_Recv(rmsg3, 17, MPI_CHAR, src, tag, comm, MPI_STATUS_IGNORE);)"}
{"predict": "(10, SYCL, ::sycl::device device;)", "label": "(9, SYCL, sycl::device device;)"}
{"predict": "(70, OpenMP, #pragma omp interop init(prefer_type(1,2,3,4,5,6), targetsync : I)); (90, OpenMP, #pragma omp interop init(prefer_type(\"cuda\",\"cuda_driver\",\"opencl\",\"sycl\",\"hip\",\"level_zero\"), targetsync : I)); (36, OpenMP, #pragma omp interop device(dev) depend(inout : ap) use(I)); (4, OpenMP, #pragma omp interop init(target : IRef)); (63, OpenMP, #pragma omp interop device(dev) depend(inout : ap) destroy(I)); (47, OpenMP, #pragma omp interop device(dev) depend(inout : ap) init(targetsync : I)); (34, OpenMP, #pragma omp interop device(dev) depend(inout : ap)); (21, OpenMP, #pragma omp interop use(IRef)); (14, OpenMP, #pragma omp interop destroy(I)); (24, OpenMP, #pragma omp interop use(I)); (65, OpenMP, #pragma omp interop init(prefer_type(2,4,6,1), targetsync : I)); (15, OpenMP, #pragma omp interop init(targetsync : I)); (18, OpenMP, #pragma omp interop destroy(IRef)); (23, OpenMP, #pragma omp interop use(CI)); (74, OpenMP, #pragma omp interop init(prefer_type(\"cuda\",\"cuda_driver\",\"opencl\",\"sycl\",\"hip\",\"level_zero\"), targetsync : IRef)); (69, OpenMP, #pragma omp interop destroy(IRef))", "label": "(140, OpenMP, #pragma omp interop use(I) use(J)); (129, OpenMP, #pragma omp interop init(target:I) init(targetsync:J)); (24, OpenMP, #pragma omp interop init(target:IRef)); (31, OpenMP, #pragma omp interop destroy(IRef)); (71, OpenMP, #pragma omp interop device(dev) depend(inout:ap) destroy(I)); (159, OpenMP, #pragma omp interop destroy(I) use(J)); (4, OpenMP, #pragma omp interop use(I)); (141, OpenMP, #pragma omp interop destroy(I) destroy(J)); (47, OpenMP, #pragma omp interop device(dev) depend(inout:ap) init(targetsync:I)); (7, OpenMP, #pragma omp interop destroy(I)); (-2, OpenMP, #pragma omp interop init(target:I)); (127, OpenMP, #pragma omp interop init(target:I) use(J)); (29, OpenMP, #pragma omp interop use(CI)); (84, OpenMP, #pragma omp interop init(prefer_type(2,4,6,1),targetsync:I)); (14, OpenMP, #pragma omp interop use(IRef)); (50, OpenMP, #pragma omp interop device(dev) depend(inout:ap) use(I)); (71, OpenMP, #pragma omp interop init(prefer_type(1,2,3,4,5,6),targetsync:I)); (145, OpenMP, #pragma omp interop init(target:I) destroy(J)); (-14, OpenMP, #pragma omp interop init()"}
{"predict": "(36, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (38, CUDA, CUDA_CHECK(cudaFree(d_data));)", "label": "(36, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (38, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(87, CUDA, cudaFreeHost( outHost );); (85, CUDA, cudaFree( texDevice );)", "label": "(86, CUDA, cudaFreeHost( outHost );); (84, CUDA, cudaFree( texDevice );)"}
{"predict": "(31, OpenMP, #pragma omp teams reduction(+:sum)); (28, OpenMP, #pragma omp target map(tofrom: b, sum, t) map(alloc: a)); (31, OpenMP, #pragma omp distribute parallel for reduction(+:sum)); (33, OpenMP, t[i] = omp_get_team_num();)", "label": "(31, OpenMP, #pragma omp teams reduction(+:sum)); (28, OpenMP, #pragma omp target map(tofrom: b, sum, t) map(alloc: a)); (31, OpenMP, #pragma omp distribute parallel for reduction(+:sum)); (33, OpenMP, t[i] = omp_get_team_num();)"}
{"predict": "(91, OpenMP, _num_threads = omp_get_num_threads();)", "label": "(91, OpenMP, const int NumThreads = omp_get_max_threads();)"}
{"predict": "(56, CUDA, checkCudaErrors(cudaMemset(d_Output, 0, QRNG_DIMENSIONS * N * sizeof(float)));); (51, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (30, CUDA, checkCudaErrors(cudaMalloc((void **)&d_Output, QRNG_DIMENSIONS * N * sizeof(float)));); (122, CUDA, checkCudaErrors(cudaFree(d_Output));); (18, CUDA, int devID = findCudaDevice(argc, (const char **)argv);); (60, CUDA, checkCudaErrors(cudaMemcpy(h_OutputGPU, d_Output, QRNG_DIMENSIONS * N * sizeof(float), cudaMemcpyDeviceToHost));); (18, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, devID));)", "label": "(44, CUDA, checkCudaErrors(cudaMemset(d_Output, 0, QRNG_DIMENSIONS * N * sizeof(float)));); (20, CUDA, int dev = findCudaDevice(argc, (const char **)argv);); (49, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (123, CUDA, checkCudaErrors(cudaFree(d_Output));); (18, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, dev));); (28, CUDA, checkCudaErrors(cudaMalloc((void **)&d_Output, QRNG_DIMENSIONS * N * sizeof(float)));); (60, CUDA, checkCudaErrors(cudaMemcpy(h_OutputGPU, d_Output, QRNG_DIMENSIONS * N * sizeof(float), cudaMemcpyDeviceToHost));)"}
{"predict": "(243, OpenMP, nthreads = omp_get_num_threads();); (231, OpenMP, #pragma omp parallel firstprivate(niter) private(step)); (215, OpenMP, #pragma omp parallel); (232, OpenMP, #pragma omp master)", "label": "(249, OpenMP, nthreads = omp_get_num_threads();); (234, OpenMP, #pragma omp parallel firstprivate(niter) private(step)); (214, OpenMP, #pragma omp parallel); (237, OpenMP, #pragma omp master)"}
{"predict": "(6, MPI, MPI_Init(&argc, &argv);); (33, MPI, MPI_Finalize();); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (21, MPI, MPI_Allreduce(&errs, &toterrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);)", "label": "(6, MPI, MPI_Init(&argc, &argv);); (32, MPI, MPI_Finalize();); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (20, MPI, MPI_Reduce(&errs, &toterrs, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);)"}
{"predict": "(588, MPI, MPI_Allreduce(MPI_IN_PLACE, &nmethods_used, 1, MPI_INT, MPI_MAX, MPI_COMM_WORLD);); (163, MPI, MPI_Allgather(MPI_IN_PLACE, 0, MPI_DATATYPE_NULL, allhoststrings, &len, MPI_CHAR, MPI_COMM_WORLD);); (214, MPI, MPI_Allreduce(MPI_IN_PLACE, &nlocalranks, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);); (580, MPI, MPI_Allreduce(MPI_IN_PLACE, &n2D1Cprottable, 1, MPI_INT, MPI_MAX, MPI_COMM_WORLD);); (576, MPI, MPI_Allreduce(MPI_IN_PLACE, &n2Dprottable, 1, MPI_INT, MPI_MAX, MPI_COMM_WORLD);); (196, MPI, MPI_Allreduce(MPI_IN_PLACE, &numhosts, 1, MPI_INT, MPI_MAX, MPI_COMM_WORLD);); (577, MPI, MPI_Allreduce(MPI_IN_PLACE, &n2D1Cprottable, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);); (164, MPI, MPI_Allgather(MPI_IN_PLACE, 0, MPI_DATATYPE_NULL, NULL, &len, MPI_CHAR, MPI_COMM_WORLD);); (168, MPI, MPI_Allgather(hoststring, len + 1, MPI_CHAR, allhoststrings, &len, MPI_CHAR, MPI_COMM_WORLD);); (495, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (493, MPI, MPI_Comm_set_errhandler(local_comm, MPI_ERRORS_RETURN);); (495, MPI, MPI_Comm_set_errhandler(leader_comm, MPI_ERRORS_RETURN);); (188, MPI, MPI_Allreduce(MPI_IN_PLACE, &nlocalranks, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD", "label": "(150, MPI, MPI_Type_commit(&mydt);); (154, MPI, MPI_Op_free(&myop);); (154, MPI, MPI_Type_free(&mydt);); (146, MPI, MPI_Type_contiguous(sizeof(comm_method_string_conversion_t), MPI_BYTE, &mydt);); (147, MPI, MPI_Op_create(myfn, 1, &myop);)"}
{"predict": "(14, MPI, MPI_Attr_put(windows[i], keyval_copy, NULL);); (8, MPI, MPI_Alloc_mem(DATA_SIZE, MPI_INFO_NULL, &base_ptr[i]);); (16, MPI, MPI_Free_mem(base_ptr[i]);); (12, MPI, MPI_Win_free(&windows[i]);); (7, MPI, MPI_Win_create(base_ptr[i], DATA_SIZE, ELEM_SIZE, MPI_INFO_NULL, MPI_COMM_WORLD, &windows[i]);); (5, MPI, MPI_Keyval_create(MPI_NULL_COPY_FN, MPI_NULL_DELETE_FN, &keyval, NULL);); (8, MPI, MPI_Attr_put(windows[i], keyval, NULL);)", "label": "(8, MPI, MPI_Alloc_mem(DATA_SZ, MPI_INFO_NULL, &base_ptr[0]);); (20, MPI, MPI_Win_free_keyval(&keyval);); (21, MPI, MPI_Win_free(&windows[1]);); (6, MPI, MPI_Win_create(base_ptr[0], DATA_SZ, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &windows[0]);); (12, MPI, MPI_Win_set_attr(windows[0], keyval, NULL);); (5, MPI, MPI_Alloc_mem(DATA_SZ, MPI_INFO_NULL, &base_ptr[1]);); (14, MPI, MPI_Free_mem(base_ptr[0]);); (15, MPI, MPI_Win_free_keyval(&keyval_copy);); (16, MPI, MPI_Free_mem(base_ptr[1]);); (4, MPI, MPI_Win_create_keyval(MPI_WIN_NULL_COPY_FN, delete_fn, &keyval, NULL);); (9, MPI, MPI_Win_free(&windows[0]);); (0, MPI, MPI_Win_create(base_ptr[1], DATA_SZ, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &windows[1]);); (5, MPI, MPI_Win_set_attr(windows[1], keyval, NULL);)"}
{"predict": "(47, CUDA, thrust::fill( std::begin(range), std::end(range), 0.0 );); (42, CUDA, thrust::host_vector<double> A(order*order);); (53, CUDA, thrust::fill( std::begin(range), std::end(range), 1.0 );); (39, CUDA, thrust::host_vector<double> B(order*order);)", "label": "(46, CUDA, thrust::fill(thrust::device, B.begin(), B.end(), 0.0);); (44, CUDA, thrust::sequence(thrust::device, A.begin(), A.end() );); (40, CUDA, thrust::device_vector<double> A(order*order);); (40, CUDA, thrust::device_vector<double> B(order*order);)"}
{"predict": "(3, CUDA, extern __shared__ char shared_memory[];)", "label": "(2, CUDA, extern __shared__ char shared_memory[];)"}
{"predict": "(3, CUDA, extern __shared__ float s_data[];)", "label": "(5, CUDA, extern __shared__ float s_data[];)"}
{"predict": "(148, OpenMP, #pragma omp sections reduction(&& : S2::S2sc)); (104, OpenMP, #pragma omp sections reduction(- : da)); (234, OpenMP, #pragma omp sections reduction(+ : m)); (32, OpenMP, #pragma omp sections reduction(\\)); (20, OpenMP, #pragma omp sections reduction()); (105, OpenMP, #pragma omp sections reduction(&& : S2::S2s)); (162, OpenMP, #pragma omp sections private(i), reduction(+ : j), reduction(+ : q)); (13, OpenMP, #pragma omp sections reduction(); (38, OpenMP, #pragma omp sections reduction(foo : argc); (106, OpenMP, #pragma omp sections reduction(& : e, g)); (130, OpenMP, #pragma omp sections reduction(+ : h, k, B::x)); (135, OpenMP, #pragma omp sections reduction(+ : p), reduction(+ : p)); (107, OpenMP, #pragma omp sections reduction(&& : S3::S3s)); (58, OpenMP, #pragma omp sections reduction(* : ca)); (2, OpenMP, #pragma omp sections reduction); (3, OpenMP, #pragma omp sections reduction); (37, OpenMP, #pragma omp sections reduction(|| : argc > 0? argv[1] : argv[2])); (10, OpenMP, #pragma omp sections reduction(\\)); (130, OpenMP, #pragma omp parallel private(k)); (112, OpenMP, #pragma omp sections reduction(+ : o)); (55, OpenMP, #pragma omp sections reduction(+ : ba)); (37, OpenMP, #pragma omp sections reduction(&& : argc)); (166, OpenMP, #pragma omp parallel reduction(min : i)); (144, OpenMP, #pragma omp sections reduction(+ : fl)); (75, OpenMP, #pragma omp sections reduction(&& : S3::S3sc)); (158, OpenMP, #pragma omp parallel shared(i)); (105, OpenMP, #pragma omp sections reduction(&& : S3::S3s)); (33, OpenMP, #pragma omp sections reduction(&& : argc)); (", "label": "(58, OpenMP, #pragma omp sections reduction(|| : argc > 0? argv[1] : argv[2])); (102, OpenMP, #pragma omp sections reduction(- : da)); (131, OpenMP, #pragma omp sections reduction(+ : o)); (75, OpenMP, #pragma omp sections reduction(+ : a, b, c, d, f)); (160, OpenMP, #pragma omp sections reduction(+ : fl)); (83, OpenMP, #pragma omp sections reduction(max : h.b)); (117, OpenMP, #pragma omp sections reduction(& : e, g)); (146, OpenMP, #pragma omp sections reduction(+ : r)); (20, OpenMP, #pragma omp sections reduction(-); (129, OpenMP, #pragma omp sections private(i), reduction(+ : j), reduction(+ : q)); (164, OpenMP, #pragma omp sections reduction(+ : m)); (102, OpenMP, #pragma omp sections reduction(&& : S2::S2s)); (61, OpenMP, #pragma omp sections reduction(^ : S1)); (80, OpenMP, #pragma omp sections reduction(+ : ba)); (154, OpenMP, #pragma omp parallel reduction(* : fl)); (-3, OpenMP, #pragma omp parallel); (67, OpenMP, #pragma omp sections reduction(min : a, b, c, d, f)); (111, OpenMP, #pragma omp sections reduction(+ : h, k, B::x)); (0, OpenMP, #pragma omp sections reduction +); (-6, OpenMP, #pragma omp sections reduction); (122, OpenMP, #pragma omp parallel private(k)); (87, OpenMP, #pragma omp sections reduction(^ : fl)); (31, OpenMP, #pragma omp sections reduction(| : argc,); (120, OpenMP, #pragma omp sections reduction(+ : p), reduction(+ : p)); (154, OpenMP, #pragma omp sections reduction(task, + : m)); (13, OpenMP, #pragma omp sections reduction(*)); (132, OpenMP, #pragma omp parallel reduction(min : i)); (136, OpenMP, #pragma omp parallel private(fl)); (40, OpenMP, #pragma omp sections reduction(&& : argc, z)); (69, OpenMP, #pragma omp sections reduction(* : ca)); (3, OpenMP, #pragma omp sections reduction()); (87, OpenMP, #pragma omp sections reduction(&& : S2::S2sc)); (127, OpenMP, #pragma omp sections reduction(max : j)); (-10, OpenMP, #pragma omp sections reduction(); (9, OpenMP, #pragma omp sections reduction(\\)); (13, OpenMP, #pragma omp sections reduction(foo : argc); (121, OpenMP, #pragma omp parallel shared(i)); (26, OpenMP, #pragma omp sections reduction(~ : argc))"}
{"predict": "(14, CUDA, cuda::CUDABlas *blas = new cuda::CUDABlas(cuda_executor);)", "label": "(14, CUDA, cuda::CUDABlas *blas = new cuda::CUDABlas(cuda_executor);)"}
{"predict": "(12, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize(), \"strided_determinant_from_getrf_gpu\");); (10, CUDA, qmc_cuda::cuda_check(cudaGetLastError(), \"strided_determinant_from_getrf_gpu\");)", "label": "(13, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize());); (11, CUDA, qmc_cuda::cuda_check(cudaGetLastError());)"}
{"predict": "(13, MPI, MPI_Abort(MPI_COMM_WORLD, code);); (10, MPI, MPI_Error_string(code, msg, &resultLen);); (8, MPI, MPI_Error_class(code, &class);)", "label": "(11, MPI, MPI_Error_string(code, msg, &resultLen);); (12, MPI, MPI_Abort(MPI_COMM_WORLD, code);); (8, MPI, MPI_Error_class(code, &class);)"}
{"predict": "(2, CUDA, __shared__ uchar as[18][66];)", "label": "(2, CUDA, __shared__ uchar as[18][66];)"}
{"predict": "(46, OpenMP, #pragma omp target parallel for is_device_ptr(h)); (34, OpenMP, #pragma omp target parallel for is_device_ptr(raa)); (54, OpenMP, #pragma omp target parallel for is_device_ptr(da)); (25, OpenMP, #pragma omp target parallel for is_device_ptr(aa)); (46, OpenMP, #pragma omp target parallel for is_device_ptr(rh)); (28, OpenMP, #pragma omp target parallel for is_device_ptr(k)); (14, OpenMP, #pragma omp target parallel for is_device_ptr(z))", "label": "(25, OpenMP, #pragma omp target parallel for is_device_ptr(z)); (36, OpenMP, #pragma omp target parallel for is_device_ptr(raa)); (29, OpenMP, #pragma omp target parallel for is_device_ptr(aa)); (46, OpenMP, #pragma omp target parallel for is_device_ptr(rh)); (39, OpenMP, #pragma omp target parallel for is_device_ptr(h)); (14, OpenMP, #pragma omp target parallel for is_device_ptr(k)); (49, OpenMP, #pragma omp target parallel for is_device_ptr(da))"}
{"predict": "(8, OpenMP, #pragma omp for collapse(2)); (3, OpenMP, #pragma omp target map(tofrom: is_larger)); (7, OpenMP, #pragma omp parallel)", "label": "(11, OpenMP, #pragma omp ordered); (3, OpenMP, #pragma omp parallel); (13, OpenMP, #pragma omp critical); (5, OpenMP, #pragma omp for private(i,j) schedule(static,1) collapse(2) ordered)"}
{"predict": "(5, OpenMP, #pragma omp section); (3, OpenMP, #pragma omp parallel); (33, OpenMP, #pragma omp sections private(g, sivar)); (60, OpenMP, #pragma omp sections private(t_var, vec, s_arr, var, sivar)); (67, OpenMP, #pragma omp sections private(A::x, B::x))", "label": "(60, OpenMP, #pragma omp sections lastprivate(t_var, vec, s_arr, var, sivar)); (4, OpenMP, #pragma omp section); (2, OpenMP, #pragma omp parallel); (28, OpenMP, #pragma omp sections lastprivate(g, sivar)); (65, OpenMP, #pragma omp sections lastprivate(CONDITIONAL A::x, B::x))"}
{"predict": "(14, CUDA, cudaMemset(d_ret, 0, sizeof(void *) * THREAD);); (35, CUDA, cudaFree(d_clk);); (17, CUDA, cudaMemcpy(d_ptr, h_ptr, sizeof(void *) * THREAD, cudaMemcpyHostToDevice);); (32, CUDA, cudaFree(d_ret);); (30, CUDA, cudaFree(d_ptr);); (24, CUDA, cudaMemcpy(h_clk, d_clk, sizeof(uint32_t) * THREAD, cudaMemcpyDeviceToHost);); (2, CUDA, cudaMalloc(&d_ret, sizeof(void *) * THREAD);); (0, CUDA, cudaMalloc(&d_ptr, sizeof(void *) * THREAD);); (1, CUDA, cudaMalloc(&d_clk, sizeof(uint32_t) * THREAD);); (22, CUDA, cudaDeviceSynchronize();)", "label": "(10, CUDA, cudaMallocHost(&h_ptr, THREAD * sizeof(void *));); (29, CUDA, cudaFree(d_ptr);); (25, CUDA, cudaMemcpy(h_clk, d_clk, THREAD * sizeof(uint32_t), cudaMemcpyDeviceToHost);); (28, CUDA, cudaFree(d_ret);); (12, CUDA, cudaMemcpy(d_ptr, h_ptr, THREAD * sizeof(void *), cudaMemcpyHostToDevice);); (27, CUDA, cudaFree(d_clk);); (-1, CUDA, cudaMalloc(&d_ptr, THREAD * sizeof(void *));); (26, CUDA, cudaFreeHost(h_ptr);); (-1, CUDA, cudaMalloc(&d_clk, THREAD * sizeof(uint32_t));); (-3, CUDA, cudaMalloc(&d_ret, sizeof(void *));)"}
{"predict": "(2, MPI, MPI_Init(&argc, &argv);); (13, MPI, MPI_Finalize();)", "label": "(14, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(21, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(exe_node), *receivers[r] );); (10, TBB, harness_multifunction_executor<InputType, OutputType>::execute_count = Offset;); (6, TBB, cf.body = tbb::flow::serial;); (6, TBB, tbb::flow::multifunction_node< InputType, OutputType, tbb::flow::rejecting > exe_node( g, lc, cf );); (6, TBB, tbb::flow::multifunction_node< InputType, InputType, tbb::flow::rejecting > body_copy( exe_node );); (6, TBB, body_copy.reset( tbb::flow::rf_reset_bodies );); (47, TBB, tbb::flow::remove_edge( tbb::flow::output_port<0>(exe_node), *receivers[r] );); (-2, TBB, tbb::flow::graph g;)", "label": "(69, TBB, g.reset(tbb::flow::rf_reset_bodies);); (28, TBB, tbb::flow::make_edge( senders[s], exe_node );); (19, TBB, tbb::flow::make_edge( exe_node, *receivers[r] );); (61, TBB, inc_functor body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (66, TBB, body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (0, TBB, tbb::flow::graph g;); (44, TBB, tbb::flow::remove_edge( exe_node, *receivers[r] );); (4, TBB, tbb::flow::function_node< InputType, OutputType > exe_node( g, lc, cf );)"}
{"predict": "(4, TBB, oneapi::tbb::global_control s(oneapi::tbb::global_control::max_allowed_parallelism, p);); (18, TBB, typedef typename oneapi::tbb::combinable< ContainerType > CombinableType;); (18, TBB, oneapi::tbb::parallel_for( oneapi::tbb::blocked_range<int> (0, N, 10000), ParallelVectorForBody<T>( vs ) );); (0, TBB, typedef std::vector<T, oneapi::tbb::tbb_allocator<T> > ContainerType;); (20, TBB, if (Verbose && t == 0) t0 = oneapi::tbb::tick_count::now();)", "label": "(8, TBB, oneapi::tbb::global_control gc(oneapi::tbb::global_control::max_allowed_parallelism, p);); (23, TBB, oneapi::tbb::parallel_for( oneapi::tbb::blocked_range<int> (0, N, 10000), ParallelVectorForBody<T>( vs ) );); (0, TBB, typedef std::vector<T, oneapi::tbb::tbb_allocator<T> > ContainerType;); (17, TBB, typedef typename oneapi::tbb::combinable< ContainerType > CombinableType;)"}
{"predict": "(16, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(exe_node), receivers[r] );); (62, TBB, body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (5, TBB, harness_multifunction_executor<InputType, OutputType>::execute_count = Offset;); (5, TBB, global_execute_count = Offset;); (44, TBB, tbb::flow::remove_edge( tbb::flow::output_port<0>(exe_node), receivers[r] );); (56, TBB, inc_functor body_copy;); (0, TBB, tbb::flow::multifunction_node< InputType, OutputType, tbb::flow::rejecting > exe_node( g, lc, cf );); (1, TBB, tbb::flow::graph g;)", "label": "(16, TBB, tbb::flow::make_edge( exe_node, receivers[r] );); (23, TBB, tbb::flow::make_edge( senders[s], exe_node );); (43, TBB, tbb::flow::remove_edge( exe_node, receivers[r] );); (53, TBB, inc_functor body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (1, TBB, tbb::flow::graph g;); (6, TBB, tbb::flow::function_node< InputType, OutputType > exe_node( g, lc, cf );)"}
{"predict": "(58, CUDA, cudaEventSynchronize(stop);); (62, CUDA, cudaEventDestroy(start);); (25, CUDA, cudaMalloc((void **)&d_C_ref, m * n * sizeof(float));); (35, CUDA, cudaMemcpy(C, d_C, m * n * sizeof(float), cudaMemcpyDeviceToHost);); (57, CUDA, cudaEventRecord(stop, 0);); (61, CUDA, cudaEventDestroy(stop);); (50, CUDA, cudaEventRecord(start, 0);); (39, CUDA, cudaMemcpy(d_C_ref, C_ref, m * n * sizeof(float), cudaMemcpyHostToDevice);); (56, CUDA, cudaEventElapsedTime(&elapsed_time, start, stop);); (17, CUDA, cudaMalloc((void **)&d_B, k * n * sizeof(float));); (51, CUDA, cudaEventCreate(&stop);); (15, CUDA, cudaMalloc((void **)&d_A, m * k * sizeof(float));); (22, CUDA, cudaMemcpy(d_A, A, m * k * sizeof(float), cudaMemcpyHostToDevice);); (17, CUDA, cudaMemcpy(d_B, B, k * n * sizeof(float), cudaMemcpyHostToDevice);); (43, CUDA, cudaEventCreate(&start);)", "label": "(29, CUDA, cudaMalloc((void **)&d_C_ref, m * n * sizeof(float));); (68, CUDA, cudaEventElapsedTime(&elapsed_time, start, stop);); (30, CUDA, cudaMemcpy(d_A, A, m * k * sizeof(float), cudaMemcpyHostToDevice);); (32, CUDA, cudaMemcpy(d_C_ref, C_ref, m * n * sizeof(float), cudaMemcpyHostToDevice);); (36, CUDA, cudaMemcpy(C, d_C, m * n * sizeof(float), cudaMemcpyDeviceToHost);); (61, CUDA, cudaEventRecord(stop);); (61, CUDA, cudaEventSynchronize(stop);); (15, CUDA, cudaMalloc((void **)&d_B, k * n * sizeof(float));); (13, CUDA, cudaMalloc((void **)&d_A, m * k * sizeof(float));); (52, CUDA, cudaEventRecord(start);); (13, CUDA, cudaMalloc((void **)&d_C, m * n * sizeof(float));); (23, CUDA, cudaMemcpy(d_C, C, m * n * sizeof(float), cudaMemcpyHostToDevice);); (21, CUDA, cudaMemcpy(d_B, B, k * n * sizeof(float), cudaMemcpyHostToDevice);); (46, CUDA, cudaEventCreate(&stop);); (44, CUDA, cudaEventCreate(&start);)"}
{"predict": "(6, OpenMP, #pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp parallel for)"}
{"predict": "(8, CUDA, A[blockIdx.z * stride + i * lda + i] = thrust::complex<T>(1.0, 0.0);); (11, CUDA, A[blockIdx.z * stride + j * lda + i] = thrust::complex<T>(0.0, 0.0);)", "label": "(8, CUDA, A[blockIdx.z * stride + i * lda + i] = thrust::complex<T>(1.0, 0.0);); (11, CUDA, A[blockIdx.z * stride + j * lda + i] = thrust::complex<T>(0.0, 0.0);)"}
{"predict": "(57, CUDA, cudaFree(d_t_right);); (25, CUDA, cudaMalloc((void**)(&d_t_right), t_right_bytes);); (55, CUDA, cudaFree(d_t_result);); (22, CUDA, cudaMalloc((void**)(&d_t_left), t_left_bytes);); (39, CUDA, assert(cudaMemcpyAsync(t_result_gpu.data(), d_t_result, t_result_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);); (40, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (50, CUDA, cudaFree(d_t_left);); (20, CUDA, cudaMalloc((void**)(&d_t_result), t_result_bytes);); (21, CUDA, cudaMemcpy(d_t_right, t_right.data(), t_right_bytes, cudaMemcpyHostToDevice);); (19, CUDA, cudaMemcpy(d_t_left, t_left.data(), t_left_bytes, cudaMemcpyHostToDevice);)", "label": "(25, CUDA, cudaMalloc((void**)(&d_t_right), t_right_bytes);); (25, CUDA, cudaMalloc((void**)(&d_t_result), t_result_bytes);); (50, CUDA, cudaFree((void*)d_t_left);); (26, CUDA, cudaMemcpy(d_t_right, t_right.data(), t_right_bytes, cudaMemcpyHostToDevice);); (20, CUDA, cudaMalloc((void**)(&d_t_left), t_left_bytes);); (23, CUDA, cudaMemcpy(d_t_left, t_left.data(), t_left_bytes, cudaMemcpyHostToDevice);); (48, CUDA, cudaFree((void*)d_t_result);); (46, CUDA, cudaFree((void*)d_t_right);); (36, CUDA, cudaMemcpy(t_result_gpu.data(), d_t_result, t_result_bytes, cudaMemcpyDeviceToHost);)"}
{"predict": "(35, MPI, MPI_Comm_rank(adio_fh->comm, &myrank);); (109, MPI, MPI_Barrier(adio_fh->comm);)", "label": "(37, MPI, MPI_Comm_rank(adio_fh->comm, &myrank);); (24, MPI, MPI_Bcast(&tmp_whence, 1, MPI_INT, 0, adio_fh->comm);); (12, MPI, MPI_Bcast(&tmp_offset, 1, ADIO_OFFSET, 0, adio_fh->comm);); (110, MPI, MPI_Barrier(adio_fh->comm);)"}
{"predict": "(10, OpenMP, #pragma omp target update to(data)); (1, OpenMP, #pragma omp target parallel num_threads(32) map(from); (18, OpenMP, #pragma omp atomic read); (8, OpenMP, #pragma omp atomic write); (1, OpenMP, #pragma omp target enter data map(to)", "label": "(22, OpenMP, #pragma omp flush(out)); (17, OpenMP, #pragma omp atomic read); (10, OpenMP, #pragma omp atomic write); (-1, OpenMP, #pragma omp target parallel num_threads(64) map(tofrom); (6, OpenMP, #pragma omp flush(data))"}
{"predict": "(11, TBB, tbb::task_scheduler_init init(nthread);)", "label": "(11, TBB, tbb::task_scheduler_init init(nthread);)"}
{"predict": "(34, MPI, MPI_Finalize();); (2, MPI, MPI_Init(&argc, &argv);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (30, MPI, MPI_Barrier(comm_node);); (28, MPI, MPI_Comm_free(&comm_node);); (5, MPI, MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, rank_node, MPI_INFO_NULL, &comm_node);); (0, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (8, MPI, MPI_Get_processor_name(procname, &namelen);); (2, MPI, MPI_Comm_rank(comm_node, &rank_node);); (2, MPI, MPI_Comm_size(comm_node, &size_node);)", "label": "(12, MPI, MPI_Comm_size(comm_node, &size_node);); (1, MPI, MPI_Init(&argc, &argv);); (32, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (13, MPI, MPI_Get_processor_name(procname, &namelen);); (5, MPI, MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, 0, MPI_INFO_NULL, &comm_node);); (-1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (6, MPI, MPI_Comm_rank(comm_node, &rank_node);); (19, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(12, OpenMP, #pragma omp parallel for)", "label": "(12, OpenMP, #pragma omp parallel for)"}
{"predict": "(35, CUDA, HANDLE_ERROR( cudaEventRecord( stop ) );); (35, CUDA, HANDLE_ERROR( cudaEventSynchronize( stop ) );); (50, CUDA, HANDLE_ERROR( cudaFreeHost( a ) );); (51, CUDA, HANDLE_ERROR( cudaFreeHost( b ) );); (51, CUDA, HANDLE_ERROR( cudaFreeHost( partial_c ) );); (3, CUDA, HANDLE_ERROR( cudaEventCreate( &stop ) );); (47, CUDA, HANDLE_ERROR( cudaFree( dev_a ) );); (47, CUDA, HANDLE_ERROR( cudaFree( dev_b ) );); (3, CUDA, HANDLE_ERROR( cudaEventRecord( start ) );); (27, CUDA, HANDLE_ERROR( cudaMemcpyAsync( dev_a, a, size*sizeof(float), cudaMemcpyHostToDevice ) );); (48, CUDA, HANDLE_ERROR( cudaEventDestroy( start ) );); (27, CUDA, HANDLE_ERROR( cudaMemcpyAsync( dev_b, b, size*sizeof(float), cudaMemcpyHostToDevice ) );); (45, CUDA, HANDLE_ERROR( cudaEventDestroy( stop ) );); (46, CUDA, HANDLE_ERROR( cudaFree( dev_partial_c ) );); (-4, CUDA, HANDLE_ERROR( cudaEventCreate( &start ) );); (16, CUDA, HANDLE_ERROR( cudaMalloc( (void**)&dev_b, size*sizeof(float) ) );); (14, CUDA, HANDLE_ERROR( cudaMalloc( (void**)&dev_a, size*sizeof(float) ) );); (15, CUDA, HANDLE_ERROR( cudaMalloc( (void**)&dev_partial_c, blocksPerGrid*sizeof(float) ) );)", "label": "(56, CUDA, HANDLE_ERROR( cudaEventDestroy( start ) );); (6, CUDA, HANDLE_ERROR( cudaEventCreate( &stop ) );); (55, CUDA, HANDLE_ERROR( cudaEventDestroy( stop ) );); (36, CUDA, HANDLE_ERROR( cudaThreadSynchronize() );); (20, CUDA, HANDLE_ERROR( cudaHostGetDevicePointer( &dev_b, b, 0 ) );); (18, CUDA, HANDLE_ERROR( cudaHostGetDevicePointer( &dev_a, a, 0 ) );); (28, CUDA, HANDLE_ERROR( cudaEventRecord( start, 0 ) );); (33, CUDA, HANDLE_ERROR( cudaEventRecord( stop, 0 ) );); (33, CUDA, HANDLE_ERROR( cudaEventSynchronize( stop ) );); (43, CUDA, HANDLE_ERROR( cudaFreeHost( b ) );); (41, CUDA, HANDLE_ERROR( cudaFreeHost( a ) );); (42, CUDA, HANDLE_ERROR( cudaFreeHost( partial_c ) );); (-6, CUDA, HANDLE_ERROR( cudaEventCreate( &start ) );)"}
{"predict": "(13, MPI, mpi_err = MPI_Type_commit(&type);); (10, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(13, MPI, mpi_err = MPI_Type_commit(&type);); (9, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(7, TBB, tbb::blocked_range<RangeType> range2( range, tbb::split() );)", "label": "(7, TBB, RangeType range2( range, tbb::split() );)"}
{"predict": "(20, MPI, MPI_Allreduce( & tmp, & result, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD );)", "label": "(20, MPI, MPI_Allreduce( & tmp, & result, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD );)"}
{"predict": "(3, CUDA, __shared__ float local[threads];)", "label": "(3, CUDA, __shared__ float local[threads];)"}
{"predict": "(3, CUDA, __shared__ float s_variance;); (1, CUDA, __shared__ float s_mean;)", "label": "(9, CUDA, __shared__ float s_variance;); (7, CUDA, __shared__ float s_mean;)"}
{"predict": "(96, MPI, MPI_File_open(MPI_COMM_WORLD, file_name.c_str(), MPI_MODE_CREATE | MPI_MODE_WRONLY, MPI_INFO_NULL, &file);); (103, MPI, MPI_File_write(file, &size_array, 1, MPI_INT, status);); (107, MPI, MPI_File_close(&file);); (100, MPI, MPI_File_set_view(file, 0, MPI_INT, MPI_INT, \"native\", MPI_INFO_NULL);); (101, MPI, MPI_File_write(file, &pos_values[0], data_size[rank], MPI_DOUBLE, status);); (95, MPI, MPI_File_open(MPI_COMM_WORLD, file_name.c_str(), MPI_MODE_RDONLY, MPI_INFO_NULL, &file);); (97, MPI, MPI_File_read(file, &size_array, 1, MPI_INT, status);); (92, MPI, MPI_Status status;); (94, MPI, MPI_File_set_view(file, 4, MPI_DOUBLE, MPI_DOUBLE, \"native\", MPI_INFO_NULL);); (93, MPI, MPI_File_read(file, &pos_values[0], data_size[rank], MPI_DOUBLE, status);)", "label": "(99, MPI, MPI_File_open(MPI_COMM_WORLD, file_name.c_str(), MPI_MODE_CREATE | MPI_MODE_WRONLY, MPI_INFO_NULL, &file);); (105, MPI, MPI_File_write(file, &size_array, 1, MPI_INT, &status);); (110, MPI, MPI_File_close(&file);); (102, MPI, MPI_File_seek(file, mpi_offset, MPI_SEEK_SET);); (107, MPI, MPI_File_write(file, &pos_values[0], data_size[rank], MPI_DOUBLE, &status);)"}
{"predict": "(16, CUDA, __shared__ float Bsh[_M_ * _K_];)", "label": "(16, CUDA, extern __shared__ volatile float Bsh[];)"}
{"predict": "(27, OpenMP, #pragma omp target update from(pi)); (8, OpenMP, #pragma omp target enter data map(to : i) nowait); (28, OpenMP, #pragma omp target exit data map(delete : i)); (29, OpenMP, #pragma omp target update from(i)); (19, OpenMP, #pragma omp target teams)", "label": "(8, MPI, MPI_Init(&argc, &argv);); (28, MPI, MPI_Finalize();)"}
{"predict": "(20, CUDA, cudaStream_t stream = at::cuda::getCurrentCUDAStream();)", "label": "(20, CUDA, cudaStream_t stream = at::cuda::getCurrentCUDAStream();)"}
{"predict": "(16, CUDA, std::cerr << \"CUDA error: \" << cudaGetErrorString(cudaError) << std::endl;); (89, CUDA, CUDA_CHECK(cudaDeviceSynchronize());); (87, CUDA, CUDA_CHECK(cudaGetLastError());); (56, CUDA, CUDA_CHECK(cudaMalloc(&p.workspace, p.workspace_size()));)", "label": "(85, CUDA, cudaDeviceSynchronize();); (59, CUDA, cudaMalloc(&p.workspace, p.workspace_size());); (114, CUDA, CUDA_CHECK(cudaEventSynchronize(events[1]));); (112, CUDA, CUDA_CHECK(cudaEventRecord(events[1]));); (84, CUDA, CUDA_CHECK(cudaFuncSetAttribute(kernel_fn, cudaFuncAttributeMaxDynamicSharedMemorySize, int(smem_bytes)));); (114, CUDA, CUDA_CHECK(cudaEventElapsedTime(&runtime_ms, events[0], events[1]));); (9, CUDA, std::cerr << \"FATAL: \" #FN \" failed: \" << cudaGetErrorString(cudaError) << std::endl;); (102, CUDA, CUDA_CHECK(cudaEventCreate(&event));); (103, CUDA, CUDA_CHECK(cudaEventRecord(events[0]));)"}
{"predict": "(20, CUDA, cudaMemcpy2DAsyncBatch(pointers, total * 3, cpuPointers, total * 3, widths[0] * heights[0], cudaMemcpyDeviceToDevice);); (23, CUDA, FastllmCudaFree(pointers);); (17, CUDA, cudaMemcpy2DAsyncBatch(pointers, total * 3, cpuPointers, total * 3, widths[0] * heights[0], cudaMemcpyHostToDevice);); (1, CUDA, uint8_t ** pointers = (uint8_t**)FastllmCudaMalloc(sizeof(uint8_t*) * total * 3);)", "label": "(19, CUDA, cudaMemcpy(pointers, cpuPointers, sizeof(uint8_t*) * total * 3, cudaMemcpyHostToDevice);); (6, CUDA, uint8_t ** pointers = (uint8_t**)FastllmCudaMalloc(sizeof(uint8_t*) * total * 3);); (20, CUDA, FastllmCudaFree(pointers);)"}
{"predict": "(96, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ j - 1 ], 0, MPI_COMM_WORLD );); (10, MPI, MPI_Comm_size( MPI_COMM_WORLD, &Parallel::nprocs );); (10, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &Parallel::pid );); (93, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ j + 1 ], 1, MPI_COMM_WORLD );); (122, MPI, MPI_Finalize();); (113, MPI, MPI_Allgather( &tmp, 1, MPI_DOUBLE, x.data(), 1, MPI_DOUBLE, MPI_COMM_WORLD );); (9, MPI, MPI_Init( &argc, &argv );); (117, MPI, MPI_Allreduce( &s, &global_s, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD );); (102, MPI, MPI_Recv( A[ 3 ].data(), N + 1, MPI_DOUBLE, activep[ j - 1 ], 0, MPI_COMM_WORLD, &status );); (92, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ j + 1 ], 1, MPI_COMM_WORLD );); (100, MPI, MPI_Recv( A[ 4 ].data(), N + 1, MPI_DOUBLE, activep[ j + 1 ], 1, MPI_COMM_WORLD, &status );); (94, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ j - 1 ], 1, MPI_COMM_WORLD );)", "label": "(106, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ j + 1 ], 0, MPI_COMM_WORLD );); (104, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ j - 1 ], 1, MPI_COMM_WORLD );); (108, MPI, MPI_Recv( A[ 3 ].data(), N + 1, MPI_DOUBLE, activep[ j - 1 ], 0, MPI_COMM_WORLD, &status );); (7, MPI, MPI_Comm_size( MPI_COMM_WORLD, &Parallel::nprocs );); (97, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ numactivep - 2 ], 1, MPI_COMM_WORLD );); (135, MPI, MPI_Allgather( &tmp, 1, MPI_DOUBLE, x.data(), 1, MPI_DOUBLE, MPI_COMM_WORLD);); (187, MPI, MPI_Finalize();); (157, MPI, MPI_Allgather( &tmp, 1, MPI_DOUBLE, x.data(), 1, MPI_DOUBLE, MPI_COMM_WORLD );); (1, MPI, MPI_Init( &argc, &argv );); (2, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &Parallel::pid );); (101, MPI, MPI_Recv( A[ 4 ].data(), N + 1, MPI_DOUBLE, activep[ j + 1 ], 1, MPI_COMM_WORLD, &status );); (86, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ 1 ], 0, MPI_COMM_WORLD );)"}
{"predict": "(37, OpenMP, #pragma omp parallel num_threads(x)); (25, OpenMP, #pragma omp parallel default(shared)); (17, OpenMP, #pragma omp parallel default(none)); (10, OpenMP, #pragma omp parallel); (28, OpenMP, #pragma omp parallel default(firstprivate))", "label": "(25, OpenMP, #pragma omp parallel default(shared)); (10, OpenMP, #pragma omp parallel); (37, OpenMP, #pragma omp parallel num_threads(x)); (15, OpenMP, #pragma omp parallel default(none)); (28, OpenMP, #pragma omp parallel default(firstprivate))"}
{"predict": "(38, CUDA, __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];); (40, CUDA, __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];)", "label": "(41, CUDA, __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];); (36, CUDA, __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];)"}
{"predict": "(4, CUDA, __shared__ unsigned char sharedHistogram[NUM_OF_THREADS][cBuckets];)", "label": "(12, CUDA, __shared__ unsigned char sharedHistogram[NUM_THREADS][cBuckets];)"}
{"predict": "(6, TBB, tbb::flow::make_edge(bnode, fnode);); (1, TBB, tbb::flow::sequencer_node<int> bnode(g, seq_body());); (21, TBB, tbb::flow::make_edge(bnode, tbb::flow::input_port<0>(jnode));); (19, TBB, tbb::flow::join_node<tbb::flow::tuple<int>, tbb::flow::reserving> jnode(g);); (34, TBB, g.reset(tbb::flow::rf_clear_edges);); (-2, TBB, tbb::flow::function_node<int> fnode(g, tbb::flow::serial, serial_fn_body<int>(serial_fn_state0));); (-5, TBB, tbb::flow::graph g;)", "label": "(25, TBB, tbb::flow::make_edge(bnode, tbb::flow::input_port<0>(jnode));); (7, TBB, tbb::flow::make_edge(bnode, fnode);); (22, TBB, tbb::flow::join_node<std::tuple<int,int>,tbb::flow::reserving> jnode(g);); (-1, TBB, tbb::flow::sequencer_node<int> bnode(g, seq_body());); (-3, TBB, tbb::flow::graph g;); (32, TBB, g.reset(tbb::flow::rf_clear_edges);); (-2, TBB, tbb::flow::function_node<int> fnode(g, tbb::flow::serial, serial_fn_body<int>(serial_fn_state0));); (15, TBB, tbb::flow::remove_edge(bnode, fnode);)"}
{"predict": "(67, TBB, utility::report_elapsed_time((oneapi::tbb::tick_count::now() - main_time).seconds());); (50, TBB, oneapi::tbb::flow::make_edge(the_source, the_value_pool);); (52, TBB, oneapi::tbb::flow::make_edge(the_value_pool, *(bins[i]));); (48, TBB, oneapi::tbb::flow::make_edge(the_bin_buffer, *(bins[i]));); (30, TBB, oneapi::tbb::tick_count main_time = oneapi::tbb::tick_count::now();); (34, TBB, oneapi::tbb::flow::graph g;); (56, TBB, oneapi::tbb::flow::make_edge(the_value_pool, the_writer);)", "label": "(34, TBB, oneapi::tbb::tick_count start = oneapi::tbb::tick_count::now();); (69, TBB, utility::report_elapsed_time((oneapi::tbb::tick_count::now() - start).seconds());); (49, TBB, oneapi::tbb::flow::make_edge(the_source, the_value_pool);); (53, TBB, oneapi::tbb::flow::make_edge(the_value_pool, *(bins[i]));); (44, TBB, oneapi::tbb::flow::graph g;); (31, TBB, oneapi::tbb::global_control c(oneapi::tbb::global_control::max_allowed_parallelism, p);)"}
{"predict": "(36, OpenMP, #pragma omp target teams distribute parallel for simd safelen (S)); (17, OpenMP, #pragma omp target teams distribute parallel for simd safelen (ST); (5, OpenMP, #pragma omp target teams distribute parallel for simd safelen ()); (3, OpenMP, #pragma omp target teams distribute parallel for simd safelen (); (26, OpenMP, #pragma omp target teams distribute parallel for simd safelen (foobool(argc)), safelen (true), safelen (-5)); (4, OpenMP, #pragma omp target teams distribute parallel for simd safelen ()); (47, OpenMP, #pragma omp target teams distribute parallel for simd safelen (N)); (42, OpenMP, #pragma omp target teams distribute parallel for simd safelen (4)); (30, OpenMP, #pragma omp target teams distribute parallel for simd safelen (foobool(1) > 0? 1 : 2)); (10, OpenMP, #pragma omp target teams distribute parallel for simd safelen ((ST > 0)? 1 + ST : 2)); (13, OpenMP, #pragma omp target teams distribute parallel for simd safelen (1))); (17, OpenMP, #pragma omp target teams distribute parallel for simd safelen (foobool(argc)), safelen (argc))", "label": "(37, OpenMP, #pragma omp target teams distribute parallel for simd safelen (S)); (17, OpenMP, #pragma omp target teams distribute parallel for simd safelen (ST); (20, OpenMP, #pragma omp target teams distribute parallel for simd safelen (1))); (51, OpenMP, #pragma omp target teams distribute parallel for simd safelen (N)); (46, OpenMP, #pragma omp target teams distribute parallel for simd safelen (4)); (2, OpenMP, #pragma omp target teams distribute parallel for simd safelen (); (20, OpenMP, #pragma omp target teams distribute parallel for simd safelen ((ST > 0)? 1 + ST : 2)); (26, OpenMP, #pragma omp target teams distribute parallel for simd safelen (foobool(argc)), safelen (true), safelen (-5)); (3, OpenMP, #pragma omp target teams distribute parallel for simd safelen ()); (5, OpenMP, #pragma omp target teams distribute parallel for simd safelen (argc); (-7, OpenMP, #pragma omp target teams distribute parallel for simd safelen)"}
{"predict": "(13, CUDA, extern __shared__ char shared_mem[];)", "label": "(13, CUDA, extern __shared__ char shared_mem[];)"}
{"predict": "(14, MPI, MPI_Abort(MPI_COMM_WORLD, 127);)", "label": "(14, MPI, MPI_Abort(MPI_COMM_WORLD, 127);)"}
{"predict": "(51, CUDA, cuda_safe_call(cudaStreamSynchronize(stream));)", "label": "(51, CUDA, cuda_safe_call(cudaStreamSynchronize(stream));)"}
{"predict": "(34, OpenCL, TEST_OCL_CHECK(clReleaseCommandQueue(interop_ocl_queue));)", "label": "(34, OpenCL, TEST_OCL_CHECK(clReleaseCommandQueue(interop_ocl_queue));)"}
{"predict": "(16, OpenMP, #pragma omp parallel); (16, OpenMP, #pragma omp taskloop simd private(argc, b), firstprivate(c, d), lastprivate(d, f) collapse(N) shared(g) if (c) final(d) mergeable priority(f) simdlen(N) nogroup num_tasks(N)); (5, OpenMP, #pragma omp taskloop simd allocate(d) if(taskloop: argc > N) default(shared) untied priority(N) safelen(N) linear(c), aligned(ptr) grainsize(N) reduction(+:g) in_reduction(+: d)); (3, OpenMP, #pragma omp taskgroup task_reduction(+: d) allocate(d))", "label": "(15, OpenMP, #pragma omp taskloop simd private(argc, b), firstprivate(c, d), lastprivate(d, f) collapse(N) shared(g) if (c) final(d) mergeable priority(f) simdlen(N) nogroup num_tasks(N)); (13, OpenMP, #pragma omp parallel); (5, OpenMP, #pragma omp taskloop simd allocate(d) if(taskloop: argc > N) default(shared) untied priority(N) safelen(N) linear(c) aligned(ptr) grainsize(N) reduction(+:g) in_reduction(+: d)); (3, OpenMP, #pragma omp taskgroup task_reduction(+: d) allocate(d))"}
{"predict": "(10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (43, MPI, mpi_errno = MPI_Comm_free(&stream_comm);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (16, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (43, MPI, mpi_errno = MPI_Comm_free(&stream_comm);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (15, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(18, MPI, *ierr = MPI_Info_delete( (MPI_Info)(*v1), p2 );)", "label": "(18, MPI, *ierr = MPI_Info_delete( (MPI_Info)(*v1), p2 );)"}
{"predict": "(2, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (11, TBB, tbb::tick_count t1 = tbb::tick_count::now();)", "label": "(2, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (11, TBB, tbb::tick_count t1 = tbb::tick_count::now();)"}
{"predict": "(13, CUDA, A_[j] = static_cast<thrust::complex<T1>>(B_[j]);)", "label": "(12, CUDA, A_[j] = static_cast<thrust::complex<T1>>(B_[j]);)"}
{"predict": "(15, OpenMP, #pragma omp for linear (argc : 5) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (37, OpenMP, #pragma omp for linear(j)); (21, OpenMP, #pragma omp for linear (argv[1])); (15, OpenMP, #pragma omp for linear (S1)); (27, OpenMP, #pragma omp parallel); (2, OpenMP, #pragma omp for linear (); (18, OpenMP, #pragma omp for linear (a, b : B::ib)); (35, OpenMP, #pragma omp for linear(v:j)); (27, OpenMP, #pragma omp for linear(v:i)); (10, OpenMP, #pragma omp for linear (argc > 0? argv[1] : argv[2])); (1, OpenMP, #pragma omp for linear (argc); (34, OpenMP, #pragma omp for linear(v:j) ordered(1)); (30, OpenMP, #pragma omp for linear(i) ordered(1)); (23, OpenMP, #pragma omp for linear(i)); (3, OpenMP, #pragma omp for linear (argc :); (3, OpenMP, #pragma omp for linear (argc); (24, OpenMP, #pragma omp for linear(v:j) linear(i)); (15, OpenMP, #pragma omp for linear(h)); (1, OpenMP, #pragma omp for linear (argc,)", "label": "(7, OpenMP, #pragma omp for linear (); (24, OpenMP, #pragma omp for linear (argv[1])); (17, OpenMP, #pragma omp for linear (S1)); (12, OpenMP, #pragma omp for linear (argc > 0? argv[1] : argv[2])); (1, OpenMP, #pragma omp for linear); (22, OpenMP, #pragma omp for linear(e, g, k)); (27, OpenMP, #pragma omp parallel); (33, OpenMP, #pragma omp for linear(j)); (3, OpenMP, #pragma omp for linear (argc); (34, OpenMP, #pragma omp for linear(v:j)); (-1, OpenMP, #pragma omp for linear ()); (2, OpenMP, #pragma omp for linear (argc,); (11, OpenMP, #pragma omp for linear (a, b:B::ib)); (34, OpenMP, #pragma omp for linear(i) ordered(1)); (15, OpenMP, #pragma omp for linear(h)); (2, OpenMP, #pragma omp for linear (argc : 5) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (21, OpenMP, #pragma omp for linear(v:i)); (14, OpenMP, #pragma omp for linear(i))"}
{"predict": "(16, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(16, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(8, CUDA, cudaStreamSynchronize(LU_v1->A_gpu.cuStreams[0]);)", "label": "(8, CUDA, cudaStreamSynchronize(LU_v1->A_gpu.cuStreams[0]);)"}
{"predict": "(31, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(31, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(5, CUDA, __shared__ float reduce_smem[NUM_WARPS];)", "label": "(5, CUDA, __shared__ float reduce_smem[NUM_WARPS];)"}
{"predict": "(16, CUDA, extern __shared__ float buffer[];)", "label": "(16, CUDA, extern __shared__ float buffer[];)"}
{"predict": "(85, MPI, MPI_Bcast(&min_source, 1, MPI_INT, source, scr_comm_world);); (96, MPI, MPI_Allreduce(&min_depth, &min_global, 1, MPI_INT, MPI_MIN, scr_comm_world);); (76, MPI, MPI_Allreduce(&source_rank, &source, 1, MPI_INT, MPI_MIN, scr_comm_world);); (95, MPI, MPI_Allreduce(&max_depth, &max_global, 1, MPI_INT, MPI_MAX, scr_comm_world);); (145, MPI, MPI_Barrier(scr_comm_world);)", "label": "(85, MPI, MPI_Bcast(&min_source, 1, MPI_INT, source, scr_comm_world);); (96, MPI, MPI_Allreduce(&min_depth, &min_global, 1, MPI_INT, MPI_MIN, scr_comm_world);); (76, MPI, MPI_Allreduce(&source_rank, &source, 1, MPI_INT, MPI_MIN, scr_comm_world);); (95, MPI, MPI_Allreduce(&max_depth, &max_global, 1, MPI_INT, MPI_MAX, scr_comm_world);); (168, MPI, MPI_Barrier(scr_comm_world);)"}
{"predict": "(37, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&d_ref, size * sizeof(DataType)));); (35, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&d_out, size * sizeof(DataType)));); (55, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(d_out));); (55, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(states));); (55, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(d_ref));); (26, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (20, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (24, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(h_out.data(), d_out, size * sizeof(DataType), cudaMemcpyDeviceToHost));); (14, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&states, block_dim * grid_size * sizeof(RNG)));); (41, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(h_ref.data(), d_ref, size * sizeof(DataType), cudaMemcpyDeviceToHost));)", "label": "(8, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc((void**)&d_out, size * sizeof(DataType)));); (32, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(h_out.data(), d_out, size * sizeof(DataType), cudaMemcpyDeviceToHost));); (28, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (35, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc((void**)&d_ref, size * sizeof(DataType)));); (48, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(d_out));); (16, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc((void**)&states, block_dim * grid_size * sizeof(RNG)));); (19, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (44, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(states));); (45, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(d_ref));); (39, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(h_ref.data(), d_ref, size * sizeof(DataType), cudaMemcpyDeviceToHost));)"}
{"predict": "(36, OpenCL, err = clSetKernelArg(scan_kernel, 1, dyn_mem_size, NULL);); (31, OpenCL, err = clSetKernelArg(scan_kernel, 0, sizeof(buffer), &buffer);)", "label": "(36, OpenCL, err = clSetKernelArg(scan_kernel, 1, dyn_mem_size, NULL);); (31, OpenCL, err = clSetKernelArg(scan_kernel, 0, sizeof(buffer), &buffer);)"}
{"predict": "(13, MPI, err = MPI_Type_create_struct(1, &blen, &displ, &type, &tmptype);); (31, MPI, MPI_Type_free(&tmptype);); (20, MPI, err = MPI_Type_create_resized(tmptype, 0, aext * sizeof(int), datatype);); (12, MPI, MPI_Type_free(&type);)", "label": "(22, MPI, err = MPI_Type_create_resized(tmptype, 0, aext * sizeof(int), datatype);); (28, MPI, MPI_Type_free(&tmptype);); (12, MPI, err = MPI_Type_create_struct(1, &blen, &displ, &type, &tmptype);)"}
{"predict": "(13, OpenMP, #pragma omp atomic update seq_cst); (4, OpenMP, #pragma omp atomic seq_cst)", "label": "(4, OpenMP, #pragma omp atomic seq_cst); (8, OpenMP, #pragma omp atomic seq_cst seq_cst); (10, OpenMP, #pragma omp atomic update seq_cst)"}
{"predict": "(12, CUDA, img[idx] = cudaAlphaBlend(img[idx], color);)", "label": "(12, CUDA, img[idx] = cudaAlphaBlend(img[idx], color);)"}
{"predict": "(26, MPI, MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);); (43, MPI, err = MPI_File_close(&fh);); (4, MPI, MPI_Init(&argc, &argv);); (24, MPI, MPI_Bcast(filename, len + 10, MPI_CHAR, 0, MPI_COMM_WORLD);); (145, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (14, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (34, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(25, MPI, MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);); (40, MPI, err = MPI_File_close(&fh);); (4, MPI, MPI_Init(&argc, &argv);); (143, MPI, MPI_Finalize();); (22, MPI, MPI_Bcast(filename, len + 10, MPI_CHAR, 0, MPI_COMM_WORLD);); (2, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (39, MPI, MPI_Barrier(MPI_COMM_WORLD);); (12, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(105, MPI, MPI_Reduce(ldinbuf, ldoutbuf, 3, MPI_LONG_DOUBLE, MPI_MAX, 0, comm);); (13, MPI, MPI_Comm_size(comm, &size);); (24, MPI, MPI_Reduce(cinbuf, coutbuf, 3, MPI_CHAR, MPI_MAX, 0, comm);); (49, MPI, MPI_Reduce(scinbuf, scoutbuf, 3, MPI_SIGNED_CHAR, MPI_MAX, 0, comm);); (72, MPI, MPI_Reduce(ucinbuf, ucoutbuf, 3, MPI_UNSIGNED_CHAR, MPI_MAX, 0, comm);); (8, MPI, MPI_Comm_rank(comm, &rank);); (133, MPI, MPI_Reduce(llinbuf, lloutbuf, 3, MPI_LONG_LONG, MPI_MAX, 0, comm);)", "label": "(106, MPI, MPI_Reduce(ldinbuf, ldoutbuf, 3, MPI_LONG_DOUBLE, MPI_MAX, 0, comm);); (13, MPI, MPI_Comm_size(comm, &size);); (24, MPI, MPI_Reduce(cinbuf, coutbuf, 3, MPI_CHAR, MPI_MAX, 0, comm);); (49, MPI, MPI_Reduce(scinbuf, scoutbuf, 3, MPI_SIGNED_CHAR, MPI_MAX, 0, comm);); (73, MPI, MPI_Reduce(ucinbuf, ucoutbuf, 3, MPI_UNSIGNED_CHAR, MPI_MAX, 0, comm);); (8, MPI, MPI_Comm_rank(comm, &rank);); (132, MPI, MPI_Reduce(llinbuf, lloutbuf, 3, MPI_LONG_LONG, MPI_MAX, 0, comm);)"}
{"predict": "(18, OpenMP, #pragma omp distribute parallel for if (argc > 0? argv[1] : argv[2])); (3, OpenMP, #pragma omp distribute parallel for if); (11, OpenMP, #pragma omp distribute parallel for if (argc); (20, OpenMP, #pragma omp distribute parallel for if (S1)); (35, OpenMP, #pragma omp distribute parallel for if(parallel : argc); (21, OpenMP, #pragma omp distribute parallel for if (argc argc)); (25, OpenMP, #pragma omp distribute parallel for if(parallel : argc) if (argc)); (30, OpenMP, #pragma omp distribute parallel for if(parallel : argc) if (parallel:argc)); (-5, OpenMP, #pragma omp target); (2, OpenMP, #pragma omp distribute parallel for if ()); (35, OpenMP, #pragma omp distribute parallel for if(parallel : argc) if (parallel:argc) if (argc)); (-6, OpenMP, #pragma omp distribute parallel for if (); (3, OpenMP, #pragma omp distribute parallel for if (foobool(argc)), if (true)); (-9, OpenMP, #pragma omp teams)", "label": "(19, OpenMP, #pragma omp distribute parallel for proc_bind(master); (3, OpenMP, #pragma omp distribute parallel for proc_bind); (12, OpenMP, #pragma omp distribute parallel for proc_bind()); (6, OpenMP, #pragma omp distribute parallel for proc_bind(); (25, OpenMP, #pragma omp distribute parallel for proc_bind(x)); (37, OpenMP, #pragma omp distribute parallel for proc_bind(spread)); (18, OpenMP, #pragma omp distribute parallel for proc_bind(close), proc_bind(spread)); (-5, OpenMP, #pragma omp target); (31, OpenMP, #pragma omp parallel proc_bind(close)); (26, OpenMP, #pragma omp distribute parallel for proc_bind(master)); (-7, OpenMP, #pragma omp teams)"}
{"predict": "(4, OpenCL, retVal = clEnqueueAcquireGLObjects(pCmdQ, 1, &glBuffer, 1, &retEvent, nullptr);); (26, OpenCL, retVal = clReleaseMemObject(glBuffer);); (13, OpenCL, retVal = clEnqueueReleaseGLObjects(pCmdQ, 1, &glBuffer, 1, &retEvent, nullptr);); (27, OpenCL, retVal = clReleaseEvent(retEvent);); (-1, OpenCL, auto glBuffer = clCreateFromGLBuffer(&context, 0, bufferId, &retVal);); (13, OpenCL, retVal = clReleaseEvent(retEvent);)", "label": "(19, OpenCL, retVal = clEnqueueReleaseGLObjects(commandQueue, 1, &glBuffer, 0, nullptr, &retEvent);); (30, OpenCL, retVal = clReleaseMemObject(glBuffer);); (3, OpenCL, auto glBuffer = clCreateFromGLBuffer(&context, 0, bufferId, &retVal);); (1, OpenCL, auto commandQueue = clCreateCommandQueue(&context, context.getDevice(0), 0, &retVal);); (12, OpenCL, retVal = clReleaseEvent(retEvent);); (1, OpenCL, retVal = clEnqueueAcquireGLObjects(commandQueue, 1, &glBuffer, 0, nullptr, &retEvent);); (23, OpenCL, retVal = clReleaseCommandQueue(commandQueue);)"}
{"predict": "(24, MPI, MPI_Type_set_attr(type, key[0], &attrval[0]);); (25, MPI, MPI_Type_set_attr(type, key[2], &attrval[2]);); (22, MPI, MPI_Type_set_attr(type, key[1], &attrval[1]);); (55, MPI, MPI_Type_free_keyval(&key[i]);); (24, MPI, MPI_Type_delete_attr(type, key[0]);); (24, MPI, MPI_Type_delete_attr(type, key[1]);); (24, MPI, MPI_Type_delete_attr(type, key[2]);)", "label": "(21, MPI, MPI_Type_set_attr(type, key[2], &attrval[2]);); (22, MPI, MPI_Type_set_attr(type, key[0], &attrval[0]);); (56, MPI, MPI_Type_free_keyval(&key[i]);); (19, MPI, MPI_Type_set_attr(type, key[1], &attrval[1]);); (23, MPI, MPI_Type_delete_attr(type, key[0]);); (23, MPI, MPI_Type_delete_attr(type, key[1]);); (23, MPI, MPI_Type_delete_attr(type, key[2]);)"}
{"predict": "(5, CUDA, __shared__ float reduce_smem[NUM_WARPS];)", "label": "(5, CUDA, __shared__ float reduce_smem[NUM_WARPS];)"}
{"predict": "(8, TBB, tbb::enumerable_thread_specific<HyperedgeWeight> deltas(0);)", "label": "(8, TBB, tbb::enumerable_thread_specific<HyperedgeWeight> deltas(0);)"}
{"predict": "(8, OpenCL, retVal = clEnqueueMemcpyINTEL(commandQueues[0][0].get(), true, deviceMemAlloc, buffer.data(), bufferSize, 0, nullptr, nullptr);); (16, OpenCL, retVal = clEnqueueMemcpyINTEL(commandQueues[0][0].get(), true, hostMemAlloc, deviceMemAlloc, bufferSize, 0, nullptr, nullptr);)", "label": "(8, OpenCL, retVal = clEnqueueMemcpyINTEL(commandQueues[0][0].get(), true, deviceMemAlloc, buffer.data(), bufferSize, 0, nullptr, nullptr);); (16, OpenCL, retVal = clEnqueueMemcpyINTEL(commandQueues[0][0].get(), true, hostMemAlloc, deviceMemAlloc, bufferSize, 0, nullptr, nullptr);)"}
{"predict": "(43, MPI, MPI_Recv(receiveBuffer.data(), msgsize, MPI_INT, 1, MPI_ANY_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (7, MPI, MPI_Init(&argc, &argv);); (54, MPI, MPI_Finalize();); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (45, MPI, MPI_Probe(0, MPI_ANY_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (34, MPI, MPI_Send(message.data(), msgsize, MPI_INT, 1, tag, MPI_COMM_WORLD);); (45, MPI, MPI_Recv(receiveBuffer.data(), msgsize, MPI_INT, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status);); (40, MPI, MPI_Send(message.data(), msgsize, MPI_INT, 0, 1, MPI_COMM_WORLD);); (34, MPI, MPI_Recv(receiveBuffer.data(), msgsize, MPI_INT, 1, MPI_ANY_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (2, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (45, MPI, MPI_Send(message.data(), msgsize, MPI_INT, 0, 2, MPI_COMM_WORLD);)", "label": "(42, MPI, MPI_Recv(receiveBuffer.data(), msgsize, MPI_INT, 1, MPI_ANY_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (7, MPI, MPI_Init(&argc, &argv);); (48, MPI, MPI_Send(message.data(), msgsize, MPI_INT, 0, 1, MPI_COMM_WORLD);); (54, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (33, MPI, MPI_Send(message.data(), msgsize, MPI_INT, 1, tag, MPI_COMM_WORLD);); (8, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (41, MPI, MPI_Recv(receiveBuffer.data(), msgsize, MPI_INT, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status);)"}
{"predict": "(3, MPI, MPI_Init(&argc, &argv);); (39, MPI, MPI_Finalize();); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (5, CUDA, CUDA_CHECK(cudaGetDeviceCount(&ndevices));); (3, CUDA, CUDA_CHECK(cudaSetDevice(rank % ndevices));)", "label": "(9, CUDA, CUDA_CHECK(cudaGetDeviceCount(&ndevices));); (9, CUDA, CUDA_CHECK(cudaSetDevice(rank % ndevices));); (0, MPI, MPI_Init(&argc, &argv);); (37, MPI, MPI_Finalize();); (1, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)"}
{"predict": "(3, CUDA, extern __shared__ char smem[];)", "label": "(2, CUDA, extern __shared__ char smem[];)"}
{"predict": "(11, CUDA, __shared__ FP sV[Bc][dim];); (13, CUDA, __shared__ FP sSafeE[Br][Bc];); (11, CUDA, __shared__ FP sQK[Br][Bc];); (13, CUDA, __shared__ FP sNewO[Br][dim];); (7, CUDA, __shared__ FP sK[Bc][dim];); (5, CUDA, __shared__ FP sQ[Br][dim];); (9, CUDA, __shared__ FP sMax[Br];); (11, CUDA, __shared__ FP sDenom[Br];); (7, CUDA, __shared__ FP sO[Br][dim];)", "label": "(12, CUDA, __shared__ FP sV[Bc][dim];); (20, CUDA, __shared__ FP sMax[Br];); (9, CUDA, __shared__ FP sK[Bc][dim];); (11, CUDA, __shared__ FP sQK[Br][Bc];); (14, CUDA, __shared__ FP sSafeE[Br][Bc];); (8, CUDA, __shared__ FP sO[Br][dim];); (10, CUDA, __shared__ FP sNewO[Br][dim];); (3, CUDA, __shared__ FP sQ[Br][dim];); (12, CUDA, __shared__ FP sDenom[Br];)"}
{"predict": "(7, OpenMP, #pragma omp parallel for)", "label": "(7, OpenMP, #pragma omp parallel for)"}
{"predict": "(12, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (13, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;)", "label": "(12, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (13, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;)"}
{"predict": "(30, OpenCL, clCreateCommandQueueWithProperties_StubWithCallback(clCreateCommandQueueWithProperties_testCommandQueueFromSpecifiedContext);); (43, OpenCL, clReleaseCommandQueue_ExpectAndReturn(make_command_queue(0), CL_SUCCESS);); (44, OpenCL, clReleaseMemObject_ExpectAndReturn(expected, CL_SUCCESS);); (31, OpenCL, clReleaseEvent_StubWithCallback(clReleaseEvent_testCopyHostToBuffer);); (26, OpenCL, clEnqueueMapBuffer_StubWithCallback(clEnqueueMapBuffer_testCopyHostToBuffer);); (26, OpenCL, clEnqueueUnmapMemObject_StubWithCallback(clEnqueueUnmapMemObject_testCopyHostToBuffer);); (17, OpenCL, clCreateBuffer_StubWithCallback(clCreateBuffer_testBufferConstructorContextIterator);); (40, OpenCL, clReleaseContext_ExpectAndReturn(make_context(0), CL_SUCCESS);); (21, OpenCL, clReleaseDevice_ExpectAndReturn(make_device_id(0), CL_SUCCESS);); (17, OpenCL, clEnqueueNDRangeKernel_StubWithCallback(clEnqueueNDRangeKernel_testCopyHostToBuffer);)", "label": "(16, OpenCL, clCreateBuffer_StubWithCallback(clCreateBuffer_testBufferConstructorContextIterator);); (35, OpenCL, clReleaseCommandQueue_ExpectAndReturn(make_command_queue(0), CL_SUCCESS);); (43, OpenCL, clReleaseMemObject_ExpectAndReturn(expected, CL_SUCCESS);); (24, OpenCL, clCreateCommandQueueWithProperties_StubWithCallback(clCreateCommandQueueWithProperties_testCommandQueueFromSpecifiedContext);); (31, OpenCL, clReleaseEvent_StubWithCallback(clReleaseEvent_testCopyHostToBuffer);); (41, OpenCL, clReleaseContext_ExpectAndReturn(make_context(0), CL_SUCCESS);); (26, OpenCL, clEnqueueMapBuffer_StubWithCallback(clEnqueueMapBuffer_testCopyHostToBuffer);); (26, OpenCL, clEnqueueUnmapMemObject_StubWithCallback(clEnqueueUnmapMemObject_testCopyHostToBuffer);)"}
{"predict": "(35, MPI, MPI_Cart_shift(cart_comm, 0, 1, &source, &destination);); (32, MPI, MPI_Cart_create(MPI_COMM_WORLD, ndims, dims, periods, 1, &cart_comm);); (56, MPI, MPI_Finalize();); (10, MPI, MPI_Init(&argc, &argv);); (51, MPI, t1 = MPI_Wtime();); (10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (46, MPI, MPI_Comm_free(&cart_comm);); (27, MPI, MPI_Comm_rank(cart_comm, &cart_id);); (29, MPI, MPI_Barrier(MPI_COMM_WORLD);); (30, MPI, t0 = MPI_Wtime();)", "label": "(33, MPI, MPI_Cart_shift(cart_comm, 0, 1, &source, &destination);); (52, MPI, t1 = MPI_Wtime();); (12, MPI, MPI_Init(&argc, &argv);); (57, MPI, MPI_Finalize();); (11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (27, MPI, MPI_Cart_create(MPI_COMM_WORLD, ndims, dims, periods, 1, &cart_comm);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (52, MPI, MPI_Comm_free(&cart_comm);); (27, MPI, MPI_Comm_rank(cart_comm, &cart_id);); (29, MPI, MPI_Barrier(MPI_COMM_WORLD);); (29, MPI, t0 = MPI_Wtime();)"}
{"predict": "(8, MPI, MPI_Comm_rank(mpi_comm, &rank);); (6, MPI, MPI_Comm mpi_comm = MPI_Comm_f2c(vtkh::GetMPICommHandle());); (56, MPI, MPI_Bcast(&entropy, 1, MPI_DOUBLE, 0, mpi_comm);)", "label": "(8, MPI, MPI_Comm_rank(mpi_comm, &rank);); (6, MPI, MPI_Comm mpi_comm = MPI_Comm_f2c(vtkh::GetMPICommHandle());); (56, MPI, MPI_Bcast(&entropy, 1, MPI_DOUBLE, 0, mpi_comm);)"}
{"predict": "(104, CUDA, cudaCheck(cudaFree(d_inp));); (63, CUDA, cudaCheck(cudaMalloc(&d_dvaccum, B * T * C * sizeof(float)));); (102, CUDA, cudaCheck(cudaFree(d_qkvr));); (60, CUDA, cudaCheck(cudaMalloc(&d_dpreatt, B * NH * T * T * sizeof(float)));); (101, CUDA, cudaCheck(cudaFree(d_vaccum));); (54, CUDA, cudaCheck(cudaMalloc(&d_dqkvr, B * T * 3 * C * sizeof(float)));); (61, CUDA, cudaCheck(cudaMalloc(&d_datt, B * NH * T * T * sizeof(float)));); (100, CUDA, cudaCheck(cudaFree(d_preatt));); (50, CUDA, cudaCheck(cudaMalloc(&d_dinp, B * T * 3 * C * sizeof(float)));); (101, CUDA, cudaCheck(cudaFree(d_dvaccum));); (101, CUDA, cudaCheck(cudaFree(d_dinp));); (100, CUDA, cudaCheck(cudaFree(d_dqkvr));); (47, CUDA, cudaCheck(cudaMalloc(&d_dvaccum, B * T * C * sizeof(float)));); (48, CUDA, cudaCheck(cudaMalloc(&d_dout, B * T * C * sizeof(float)));); (100, CUDA, cudaCheck(cudaFree(d_datt));); (49, CUDA, cudaCheck(cudaMemcpy(d_dout, dout, B * T * C * sizeof(float), cudaMemcpyHostToDevice));); (45, CUDA, cudaCheck(cudaMalloc(&d_dvaccum, B * T * C * sizeof(float)));); (100, CUDA, cudaCheck(cudaFree(d_dpreatt));); (34, CUDA, cudaCheck(cudaMalloc(&d_qkvr, B * T * 3 * C * sizeof(float)));); (44, CUDA, cudaCheck(cudaMalloc(&d_datt, B * NH * T * T * sizeof(float)));); (34", "label": "(68, CUDA, cudaCheck(cudaMemset(d_dinp, 0, B * T * 3 * C * sizeof(float)));); (60, CUDA, cudaCheck(cudaMalloc(&d_dpreatt, B * NH * T * T * sizeof(float)));); (157, CUDA, cudaCheck(cudaFree(d_inp));); (69, CUDA, cudaCheck(cudaMemset(d_dvaccum, 0, B * T * C * sizeof(float)));); (86, CUDA, cudaCheck(cudaMemcpy(h_dinp, d_dinp, B * T * 3 * C * sizeof(float), cudaMemcpyDeviceToHost));); (162, CUDA, cudaCheck(cudaFree(d_dpreatt));); (159, CUDA, cudaCheck(cudaFree(d_dinp));); (159, CUDA, cudaCheck(cudaFree(d_dqkvr));); (152, CUDA, cudaCheck(cudaFree(d_qkvr));); (23, CUDA, cudaCheck(cudaMalloc(&d_att, B * NH * T * T * sizeof(float)));); (158, CUDA, cudaCheck(cudaFree(d_datt));); (58, CUDA, cudaCheck(cudaMemset(d_dqkvr, 0, B * T * 3 * C * sizeof(float)));); (149, CUDA, cudaCheck(cudaFree(d_preatt));); (50, CUDA, cudaCheck(cudaMalloc(&d_dvaccum, B * T * C * sizeof(float)));); (50, CUDA, cudaCheck(cudaMalloc(&d_dout, B * T * C * sizeof(float)));); (15, CUDA, cudaCheck(cudaMalloc(&d_qkvr, B * T * 3 * C * sizeof(float)));); (17, CUDA, cudaCheck(cudaMalloc(&d_vaccum, B * T * C * sizeof(float)));); (14, CUDA, cudaCheck(cudaMalloc(&d_preatt, B * NH * T * T * sizeof(float)));); (48, CUDA, cudaCheck(cudaMemcpy(d_dout, dout, B * T * C * sizeof(float), cudaMemcpyHostToDevice));); (144, CUDA, cudaCheck(cudaFree(d_vaccum));); (40, CUDA, cudaCheck(cudaMalloc(&d_dqkvr, B * T * 3 * C * sizeof(float)));); (38, CUDA, cudaCheck(cudaMalloc(&d_dinp, B * T * 3 * C * sizeof(float)));); (49, CUDA, cudaCheck(cudaMemset(d_datt, 0, B * NH * T * T * sizeof(float)));); (141, CUDA, cudaCheck(cudaFree(d_out));); (145, CUDA, cudaCheck(cudaFree(d_dvaccum));); (9, CUDA, cudaCheck(cudaMalloc(&d_out, B * T * C * sizeof(float)));); (36, CUDA, cudaCheck(cudaMalloc(&d_datt, B * NH * T * T * sizeof(float)));); (2, CUDA, cudaCheck(cudaMalloc(&d_inp, B * T * 3 * C * sizeof(float)));); (134, CUDA, cudaCheck(cudaFree(d_att));); (41, CUDA, cudaCheck(cudaMemset(d_dpreatt, 0, B * NH * T * T * sizeof(float)));); (6, CUDA, cudaCheck(cudaMemcpy(d_inp, inp, B * T * 3 * C * sizeof(float), cudaMemcpyHostToDevice));); (139, CUDA, cudaCheck(cudaFree(d_dout));)"}
{"predict": "(25, CUDA, CHECK(cudaFree(x));); (25, CUDA, CHECK(cudaFree(y));); (25, CUDA, CHECK(cudaFree(z));); (5, CUDA, CHECK(cudaMallocManaged((void **)&z, M));); (3, CUDA, CHECK(cudaMallocManaged((void **)&x, M));); (18, CUDA, CHECK(cudaDeviceSynchronize());); (2, CUDA, CHECK(cudaMallocManaged((void **)&y, M));)", "label": "(9, CUDA, CHECK(cudaMallocManaged((void **)&z, M));); (23, CUDA, CHECK(cudaFree(x));); (5, CUDA, CHECK(cudaMallocManaged((void **)&x, M));); (22, CUDA, CHECK(cudaFree(y));); (22, CUDA, CHECK(cudaFree(z));); (16, CUDA, CHECK(cudaDeviceSynchronize());); (2, CUDA, CHECK(cudaMallocManaged((void **)&y, M));)"}
{"predict": "(52, MPI, MPI_Allreduce(rwork, temprwork, A->ncol, MPI_DOUBLE, MPI_SUM, grid->comm);); (25, MPI, MPI_Allreduce(&value, &tempvalue, 1, MPI_DOUBLE, MPI_MAX, grid->comm);)", "label": "(26, MPI, MPI_Allreduce(&value, &tempvalue, 1, MPI_DOUBLE, MPI_MAX, grid->comm);); (50, MPI, MPI_Allreduce(rwork, temprwork, A->ncol, MPI_DOUBLE, MPI_SUM, grid->comm);)"}
{"predict": "(17, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (59, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_ARE_FATAL);); (25, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(17, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (26, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(26, OpenCL, clReleaseMemObject(scratchBuff);); (20, OpenCL, clReleaseMemObject(mobjDP_);); (15, OpenCL, clReleaseMemObject(mobjY_);); (10, OpenCL, clReleaseMemObject(mobjX_);)", "label": "(25, OpenCL, clReleaseMemObject(scratchBuff);); (20, OpenCL, clReleaseMemObject(mobjDP_);); (15, OpenCL, clReleaseMemObject(mobjY_);); (10, OpenCL, clReleaseMemObject(mobjX_);)"}
{"predict": "(5, TBB, oneapi::tbb::global_control c(oneapi::tbb::global_control::max_allowed_parallelism, nthreads);); (22, TBB, static oneapi::tbb::affinity_partitioner g_ap;); (21, TBB, static oneapi::tbb::simple_partitioner s_ap;)", "label": "(5, TBB, oneapi::tbb::global_control c(oneapi::tbb::global_control::max_allowed_parallelism, nthreads);); (21, TBB, static oneapi::tbb::affinity_partitioner g_ap;)"}
{"predict": "(16, SYCL, using namespace cl::sycl::access;); (4, SYCL, cl::sycl::queue queue;)", "label": "(16, SYCL, using namespace cl::sycl::access;); (4, SYCL, cl::sycl::queue queue;)"}
{"predict": "(37, OpenMP, omp_target_free(d_a, DefaultDeviceNum);); (15, OpenMP, d_a = (int *)omp_target_alloc(DataSize, DefaultDeviceNum);); (25, OpenMP, omp_target_disassociate_ptr(p_a, DefaultDeviceNum);); (16, OpenMP, omp_target_associate_ptr(p_a, d_a, DataSize, 0, DefaultDeviceNum);); (28, OpenMP, omp_target_delete_mem(d_a, DefaultDeviceNum);); (18, OpenMP, omp_target_update_to_device(p_a, d_a, DataSize, 0, DefaultDeviceNum);)", "label": "(33, OpenMP, omp_target_free(d_a, DefaultDeviceNum);); (15, OpenMP, d_a = (int *)omp_target_alloc(DataSize, DefaultDeviceNum);); (23, OpenMP, #pragma omp target update to(d_a)); (5, OpenMP, int DefaultDeviceNum = omp_get_default_device();); (16, OpenMP, omp_target_associate_ptr(p_a, d_a, DataSize, 0, DefaultDeviceNum);); (24, OpenMP, omp_target_disassociate_ptr(p_a, DefaultDeviceNum);)"}
{"predict": "(50, OpenCL, clReleaseEvent(ev[i]);); (24, OpenCL, buf_data[0] = clEnqueueMapBuffer(queue, buf[0], CL_TRUE, CL_MAP_WRITE, 0, BUFFERSIZE*sizeof(int), 1, &ev[0], NULL, NULL);); (30, OpenCL, clEnqueueNDRangeKernel(queue, kernel, 1, NULL, globals, locals, 2, &ev[0], &ev[2]);); (13, OpenCL, clEnqueueWriteBuffer(queue, buf[0], CL_TRUE, 0, BUFFERSIZE*sizeof(int), (void *)cpu_src, 1, &ev[0], &ev[1]);)", "label": "(51, OpenCL, clReleaseEvent(ev[i]);); (24, OpenCL, clEnqueueNDRangeKernel(queue, kernel, 1, NULL, globals, locals, 2, &ev[0], &ev[2]);); (30, OpenCL, buf_data[0] = clEnqueueMapBuffer(queue, buf[0], CL_FALSE, 0, 0, BUFFERSIZE*sizeof(int), 1, &ev[2], NULL, NULL);); (14, OpenCL, clEnqueueWriteBuffer(queue, buf[0], CL_FALSE, 0, BUFFERSIZE*sizeof(int), (void *)cpu_src, 1, &ev[0], &ev[1]);)"}
{"predict": "(52, MPI, MPI_Allreduce(MPI_IN_PLACE, &(flag), 1, mpi_int_t, MPI_MAX, grid3d->zscp.comm);); (53, MPI, MPI_Allreduce(MPI_IN_PLACE, &(gridid), 1, mpi_int_t, MPI_MAX, grid3d->zscp.comm);); (104, MPI, MPI_Allreduce(MPI_IN_PLACE, myusize, nsupers, mpi_int_t, MPI_MAX, grid);); (102, MPI, MPI_Allreduce(MPI_IN_PLACE, mylsize, nsupers, mpi_int_t, MPI_MAX, grid);); (50, MPI, MPI_Bcast( &(size), 1, mpi_int_t, gridid,  grid3d->zscp.comm);)", "label": "(50, MPI, MPI_Allreduce(MPI_IN_PLACE, &(flag), 1, mpi_int_t, MPI_MAX, grid3d->zscp.comm);); (105, MPI, MPI_Allreduce(  MPI_IN_PLACE, myusize, nsupers, mpi_int_t, MPI_MAX, grid->comm );); (102, MPI, MPI_Allreduce( MPI_IN_PLACE, mylsize, nsupers, mpi_int_t, MPI_MAX, grid->comm );); (50, MPI, MPI_Bcast( &(size), 1, mpi_int_t, gridid,  grid3d->zscp.comm);); (48, MPI, MPI_Allreduce(MPI_IN_PLACE, &(gridid), 1, mpi_int_t, MPI_MAX, grid3d->zscp.comm);)"}
{"predict": "(39, CUDA, checkCudaErrors(cudaGetLastError());); (42, CUDA, checkCudaErrors(cudaEventRecord(ev2, stream));); (40, CUDA, checkCudaErrors(cudaEventSynchronize(ev2));); (63, CUDA, checkCudaErrors(cudaFree(ringbuf));); (36, CUDA, checkCudaErrors(cudaEventRecord(ev1, stream));); (20, CUDA, checkCudaErrors(cudaEventCreate(&ev1));); (38, CUDA, checkCudaErrors(cudaEventElapsedTime(&elapse, ev1, ev2));); (17, CUDA, checkCudaErrors(cudaEventCreate(&ev2));); (59, CUDA, checkCudaErrors(cudaFree(gpustack));); (15, CUDA, checkCudaErrors(cudaEventRecord(ev1, stream));); (11, CUDA, checkCudaErrors(cudaMalloc((void **)&ringbuf, sizeof(*ringbuf)));)", "label": "(15, CUDA, checkCudaErrors(cudaMalloc((void **)&ringbuf, sizeof(qsortRingbuf)));); (28, CUDA, checkCudaErrors(cudaEventRecord(ev1));); (40, CUDA, checkCudaErrors(cudaEventRecord(ev2));); (38, CUDA, checkCudaErrors(cudaGetLastError());); (61, CUDA, checkCudaErrors(cudaFree(ringbuf));); (38, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (21, CUDA, checkCudaErrors(cudaEventCreate(&ev1));); (21, CUDA, checkCudaErrors(cudaEventCreate(&ev2));); (58, CUDA, checkCudaErrors(cudaFree(gpustack));)"}
{"predict": "(12, MPI, MPI_Comm_group(MPI_COMM_WORLD, &world);); (29, MPI, rc = MPI_Scatter(sendbuf, 10, MPI_CHAR, rbuf, 10, MPI_CHAR, 0, newcomm);); (90, MPI, MPI_Group_free(&newgroup);); (33, MPI, MPI_Error_class(rc, &errclass);); (87, MPI, MPI_Comm_free(&newcomm);); (5, MPI, MPI_Init(&argc, &argv);); (80, MPI, MPI_Group_free(&world);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (50, MPI, rc = MPI_Scatter(sendbuf, 100000, MPI_CHAR, rbuf, 100000, MPI_CHAR, 0, newcomm);); (76, MPI, MPI_Finalize();); (10, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (7, MPI, MPI_Comm_create_group(MPI_COMM_WORLD, newgroup, 0, &newcomm);); (4, MPI, MPI_Group_excl(world, 1, deadprocs, &newgroup);); (18, MPI, rc = MPI_Reduce(&errs, &toterrs, 1, MPI_INT, MPI_SUM, 0, newcomm);); (1, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(21, MPI, MPI_Comm_create_group(MPI_COMM_WORLD, newgroup, 0, &newcomm);); (19, MPI, MPI_Group_excl(world, 1, deadprocs, &newgroup);); (7, MPI, MPI_Init(&argc, &argv);); (82, MPI, MPI_Group_free(&newgroup);); (84, MPI, MPI_Finalize();); (53, MPI, rc = MPI_Scatter(sendbuf, 100000, MPI_CHAR, rbuf, 100000, MPI_CHAR, 0, MPI_COMM_WORLD);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (76, MPI, MPI_Comm_free(&newcomm);); (3, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (76, MPI, MPI_Group_free(&world);); (25, MPI, rc = MPI_Scatter(sendbuf, 10, MPI_CHAR, rbuf, 10, MPI_CHAR, 0, MPI_COMM_WORLD);); (4, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (26, MPI, MPI_Error_class(rc, &errclass);); (5, MPI, MPI_Comm_group(MPI_COMM_WORLD, &world);)"}
{"predict": "(5, OpenMP, #pragma omp parallel num_threads(8) shared(var)); (13, OpenMP, omp_destroy_nest_lock(&lock);); (1, OpenMP, omp_init_nest_lock(&lock);)", "label": "(6, OpenMP, #pragma omp parallel num_threads(8) shared(var)); (7, OpenMP, omp_set_nest_lock(&lock);); (9, OpenMP, omp_unset_nest_lock(&lock);); (1, OpenMP, omp_init_nest_lock(&lock);); (13, OpenMP, omp_destroy_nest_lock(&lock);)"}
{"predict": "(8, CUDA, sim->CUDAConfig().device_id = device;)", "label": "(8, CUDA, sim->CUDAConfig().device_id = device;)"}
{"predict": "(202, OpenMP, #pragma omp parallel); (206, OpenMP, #pragma omp for schedule(dynamic,2) nowait)", "label": "(203, OpenMP, #pragma omp parallel); (206, OpenMP, #pragma omp for schedule(dynamic,2) nowait)"}
{"predict": "(18, OpenMP, #pragma omp parallel for)", "label": "(19, OpenMP, #pragma omp parallel for)"}
{"predict": "(26, OpenCL, test_clEnqueueCopyBuffer(&clEnqueueCopyBufferData[i]);); (40, OpenCL, test_clEnqueueUnmapMemObject(&clEnqueueUnmapMemObjectData[i]);); (15, OpenCL, test_clEnqueueFillBuffer(&clEnqueueFillBufferData[i]);); (27, OpenCL, test_clEnqueueMapBuffer(&clEnqueueMapBufferData[i]);); (4, OpenCL, test_clEnqueueReadBuffer(&clEnqueueReadBufferData[i]);); (1, OpenCL, test_clEnqueueWriteBuffer(&clEnqueueWriteBufferData[i]);); (23, OpenCL, test_clEnqueueCopyBufferRect(&clEnqueueCopyBufferRectData[i]);); (10, OpenCL, test_clEnqueueReadBufferRect(&clEnqueueReadBufferRectData[i]);)", "label": "(24, OpenCL, test_clEnqueueCopyBuffer(&clEnqueueCopyBufferData[i]);); (27, OpenCL, test_clEnqueueCopyBufferRect(&clEnqueueCopyBufferRectData[i]);); (6, OpenCL, test_clEnqueueWriteBuffer(&clEnqueueWriteBufferData[i]);); (41, OpenCL, test_clEnqueueUnmapMemObject(&clEnqueueUnmapMemObjectData[i]);); (8, OpenCL, test_clEnqueueReadBufferRect(&clEnqueueReadBufferRectData[i]);); (-1, OpenCL, test_clEnqueueReadBuffer(&clEnqueueReadBufferData[i]);); (14, OpenCL, test_clEnqueueFillBuffer(&clEnqueueFillBufferData[i]);); (25, OpenCL, test_clEnqueueMapBuffer(&clEnqueueMapBufferData[i]);); (8, OpenCL, test_clEnqueueWriteBufferRect(&clEnqueueWriteBufferRectData[i]);)"}
{"predict": "(6, OpenMP, #pragma omp parallel sections reduction(task,*:f)); (14, OpenMP, #pragma omp parallel sections default(none), private(argc, b) firstprivate(argv) if (argc > 0) num_threads(ee) copyin(a) proc_bind(spread) reduction(| : c, d) reduction(* : e) lastprivate(argv)); (6, OpenMP, #pragma omp section); (1, OpenMP, #pragma omp parallel); (14, OpenMP, #pragma omp section)", "label": "(7, OpenMP, #pragma omp parallel sections reduction(task,*:f)); (14, OpenMP, #pragma omp parallel sections default(none), private(argc, b) firstprivate(argv) if (argc > 0) num_threads(ee) copyin(a) proc_bind(spread) reduction(| : c, d) reduction(* : e) lastprivate(argv)); (2, OpenMP, #pragma omp threadprivate(a)); (16, OpenMP, #pragma omp section)"}
{"predict": "(28, OpenMP, double et = omp_get_wtime();); (6, OpenMP, double st = omp_get_wtime();)", "label": "(15, OpenMP, #pragma omp target teams distribute parallel for collapse(2) depend(out:image[y_start]) nowait); (6, OpenMP, double st = omp_get_wtime();); (28, OpenMP, double et = omp_get_wtime();); (25, OpenMP, #pragma omp taskwait); (5, OpenMP, #pragma omp target data map(alloc:image[0:width*height])); (18, OpenMP, #pragma omp target update from(image[block*block_size:block_size]))"}
{"predict": "(36, MPI, MPI_Type_size(c_info->s_data_type, &s_size);); (36, MPI, MPI_Type_size(c_info->r_data_type, &r_size);)", "label": "(36, MPI, MPI_Type_size(c_info->s_data_type, &s_size);); (36, MPI, MPI_Type_size(c_info->r_data_type, &r_size);)"}
{"predict": "(21, CUDA, PyObject* shape = PyCudaImage_GetShape(self, closure);)", "label": "(13, CUDA, PyObject* shape = PyCudaImage_GetShape(self, closure);)"}
{"predict": "(33, MPI, MPI_Type_commit(&newtype);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (43, MPI, MPI_Send(buf, 1, newtype, 1, i, MPI_COMM_WORLD);); (73, MPI, MPI_Recv(&errs, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (68, MPI, MPI_Type_free(&newtype);); (23, MPI, MPI_Type_vector(i, 128, 129, MPI_CHAR, &newtype);); (76, MPI, MPI_Send(&errs, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);); (36, MPI, MPI_Recv(buf, 1, newtype, 0, i, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)", "label": "(31, MPI, MPI_Type_commit(&newtype);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (42, MPI, MPI_Send(buf, 1, newtype, 1, i, MPI_COMM_WORLD);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (66, MPI, MPI_Type_free(&newtype);); (24, MPI, MPI_Type_vector(i, 128, 129, MPI_CHAR, &newtype);); (70, MPI, MPI_Recv(&recv_errs, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (76, MPI, MPI_Send(&errs, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);); (39, MPI, MPI_Recv(buf, 1, newtype, 0, i, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)"}
{"predict": "(144, MPI, MPI_Type_get_extent(datatype, &lb, &extent);); (136, MPI, MPI_Type_size_x(datatype, &size);); (139, MPI, MPI_Type_get_extent(datatype, &lb, &bufextent);); (70, MPI, MPI_Comm_rank(fd->comm, &myrank);); (196, MPI, MPI_Type_free(&client_comm_dtype_arr[i]);); (367, MPI, MPI_Allreduce(&clients_agg_count, &all_clients_agg_count, 1, MPI_INT, MPI_SUM, fd->comm);); (195, MPI, MPI_Type_free(&agg_comm_dtype_arr[i]);); (314, MPI, MPI_Type_free(&client_comm_dtype_arr[0]);); (357, MPI, MPI_Allreduce(&aggregators_done, &all_aggregators_done, 1, MPI_INT, MPI_MIN, fd->comm);); (122, MPI, MPI_Type_get_extent(datatype, &lb, &extent);); (187, MPI, MPI_Type_free(&agg_dtype);); (123, MPI, MPI_Type_size_x(datatype, &size);); (355, MPI, MPI_Type_free(&client_comm_dtype_arr[i]);); (22, MPI, MPI_Comm_size(fd->comm, &nprocs);); (183, MPI, MPI_Type_free(&client_dtype);); (349, MPI, MPI_Type_free(&client_comm_dtype_arr[0]);); (116, MPI, MPI_Type_size_x(datatype, &size);); (178, MPI, MPI_Type_free(&client_dtype_arr[i]);); (172, MPI, MPI_Type_free(&datatype);); (108, MPI, MPI_Allgather(st_end_offset, 2, ADIO_OFFSET, all_st_end_offsets, 2, ADIO_OFFSET, fd->comm);); (164, MPI, MPI_", "label": "(106, MPI, MPI_Allgather(st_end_offset, 2, ADIO_OFFSET, all_st_end_offsets, 2, ADIO_OFFSET, fd->comm);); (71, MPI, MPI_Comm_rank(fd->comm, &myrank);); (527, MPI, MPI_Type_free(&client_comm_dtype_arr[i]);); (587, MPI, MPI_Type_free(&fd->file_realm_types[0]);); (67, MPI, MPI_Comm_size(fd->comm, &nprocs);); (589, MPI, MPI_Type_free(&fd->file_realm_types[i]);); (155, MPI, MPI_Type_get_extent(datatype, &lb, &extent);); (307, MPI, MPI_Type_size_x(agg_comm_dtype_arr[i], &size);); (301, MPI, MPI_Type_free(&agg_dtype);)"}
{"predict": "(46, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (97, MPI, MPI_Alltoall(sendcounts, 1, MPI_INT, recvcounts, 1, MPI_INT, MPI_COMM_WORLD);); (40, MPI, MPI_Type_free(&keytype);)", "label": "(99, MPI, MPI_Alltoall(sendcounts, 1, MPI_INT, recvcounts, 1, MPI_INT, MPI_COMM_WORLD);); (46, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (40, MPI, MPI_Type_free(&keytype);)"}
{"predict": "(13, OpenMP, #pragma omp parallel default(none) shared(data, x, y, z, repeats))", "label": "(13, OpenMP, #pragma omp parallel default(none) shared(data,x,y,z,repeats))"}
{"predict": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(231, MPI, MPI_Finalize();); (23, MPI, MPI_Init(&argc,&argv);); (95, MPI, MPI_Barrier(MPI_COMM_WORLD);); (23, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (76, MPI, MPI_Bcast(&btype,         1, MPI_INTEGER, root, MPI_COMM_WORLD);); (78, MPI, MPI_Bcast(&iterations,    1, MPI_INTEGER, root, MPI_COMM_WORLD);); (20, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);); (73, MPI, MPI_Bcast(&vector_length, 1, MPI_INTEGER, root, MPI_COMM_WORLD);); (145, MPI, MPI_Reduce(&total, &total_sum, 1, MPI_INTEGER, MPI_SUM, root, MPI_COMM_WORLD);)", "label": "(207, MPI, MPI_Reduce(&total, &total_sum, 1, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD);); (230, MPI, MPI_Finalize();); (79, MPI, MPI_Bcast(&iterations,    1, MPI_INT, root, MPI_COMM_WORLD);); (21, MPI, MPI_Init(&argc,&argv);); (76, MPI, MPI_Bcast(&vector_length, 1, MPI_INT, root, MPI_COMM_WORLD);); (77, MPI, MPI_Bcast(&btype,         1, MPI_INT, root, MPI_COMM_WORLD);); (93, MPI, MPI_Barrier(MPI_COMM_WORLD);); (19, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (17, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(36, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (38, CUDA, CUDA_CHECK(cudaFree(d_data));)", "label": "(36, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (38, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(44, MPI, MPI_Buffer_detach(&bptr, &bl);); (19, MPI, MPI_Send(a, 10, MPI_INT, i, 27 + j, MPI_COMM_WORLD);); (15, MPI, MPI_Buffer_attach(buf, BUFSIZE);); (23, MPI, MPI_Recv(b, 10, MPI_INT, i, 27 + j, MPI_COMM_WORLD, &status);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)", "label": "(44, MPI, MPI_Buffer_detach(&bptr, &bl);); (24, MPI, MPI_Recv(b, 10, MPI_INT, i, 27 + j, MPI_COMM_WORLD, &status);); (14, MPI, MPI_Bsend(a, 10, MPI_INT, 0, 27 + j, MPI_COMM_WORLD);); (7, MPI, MPI_Buffer_attach(buf, BUFSIZE);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)"}
{"predict": "(3, CUDA, __shared__ int sharedMemory[BLOCK_SIZE][BLOCK_SIZE + 1];)", "label": "(2, CUDA, __shared__ int sharedMemory [BLOCK_SIZE] [BLOCK_SIZE + 1];)"}
{"predict": "(42, MPI, MPI_Recv(x_external, n_recv, MPI_DOUBLE, neighbors[i], MPI_MY_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (56, MPI, MPI_Send(sendBuffer, n_send, MPI_DOUBLE, neighbors[i], MPI_MY_TAG, MPI_COMM_WORLD);); (13, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(57, MPI, MPI_Send(sendBuffer, n_send, MPI_DOUBLE, neighbors[i], MPI_MY_TAG, MPI_COMM_WORLD);); (15, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (36, MPI, MPI_Irecv(x_external, n_recv, MPI_DOUBLE, neighbors[i], MPI_MY_TAG, MPI_COMM_WORLD, request+i);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(15, OpenMP, #pragma omp for nowait); (18, OpenMP, fprintf(stderr,\"Thread nr %d exits first for construct to release.\\n\",rank);); (5, OpenMP, #pragma omp parallel); (12, OpenMP, fprintf(stderr,\"Thread nr %d enters second for to release.\\n\",rank);); (8, OpenMP, #pragma omp for); (4, OpenMP, rank = omp_get_thread_num();)", "label": "(5, OpenMP, #pragma omp parallel num_threads(4)); (11, OpenMP, #pragma omp for schedule(static) nowait); (8, OpenMP, rank = omp_get_thread_num();); (16, OpenMP, #pragma omp atomic); (20, OpenMP, #pragma omp for schedule(static))"}
{"predict": "(5, OpenMP, #pragma omp parallel for)", "label": "(4, OpenMP, #pragma omp parallel for)"}
{"predict": "(8, OpenMP, #pragma omp ordered depend(source)); (2, OpenMP, #pragma omp ordered depend); (3, OpenMP, #pragma omp ordered depend()); (13, OpenMP, #pragma omp ordered depend(sink : i + 20)); (7, OpenMP, #pragma omp ordered depend(sink)); (10, OpenMP, #pragma omp ordered depend(sink : i + 20, source)); (1, OpenMP, #pragma omp ordered depend(source)); (3, OpenMP, #pragma omp ordered depend(sink)", "label": "(1, OpenMP, #pragma omp parallel for ordered ordered); (15, OpenMP, #pragma omp parallel for order(concurrent)); (8, OpenMP, #pragma omp parallel for order(none); (4, OpenMP, #pragma omp parallel for order(); (0, OpenMP, #pragma omp parallel for order); (8, OpenMP, #pragma omp parallel for order(concurrent)"}
{"predict": "(14, OpenMP, printf(\"Thread %d of %d\\n\", omp_get_thread_num(), omp_get_num_threads());)", "label": "(14, OpenMP, printf(\"Thread %d of %d\\n\", omp_get_thread_num(), omp_get_num_threads());)"}
{"predict": "(16, CUDA, cuda_st = cudaGetDeviceCount(&num_devices);)", "label": "(43, CUDA, CUDA_CHECK_GOTO(cudaGetDevice(&self->device), free_mpool, status);); (15, CUDA, cuda_st = cudaGetDeviceCount(&num_devices);)"}
{"predict": "(13, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (24, CUDA, assert(cudaMemcpyAsync(out.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);); (11, CUDA, cudaMalloc((void**)(&d_in1), in1_bytes);); (9, CUDA, cudaMemcpy(d_in1, in1.data(), in1_bytes, cudaMemcpyHostToDevice);); (40, CUDA, cudaFree(d_out);); (38, CUDA, cudaFree(d_in1);)", "label": "(25, CUDA, assert(cudaMemcpyAsync(out.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);); (11, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (9, CUDA, cudaMalloc((void**)(&d_in1), in1_bytes);); (11, CUDA, cudaMemcpy(d_in1, in1.data(), in1_bytes, cudaMemcpyHostToDevice);); (39, CUDA, cudaFree(d_out);); (21, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (36, CUDA, cudaFree(d_in1);)"}
{"predict": "(11, OpenMP, #pragma omp teams default(none), private(argc,b) num_teams(f) firstprivate(argv) reduction(| : c, d) reduction(* : e) thread_limit(f+g)); (6, OpenMP, #pragma omp target); (5, OpenMP, #pragma omp teams)", "label": "(14, OpenMP, #pragma omp teams default(none), private(argc,b) num_teams(f) firstprivate(argv) reduction(| : c, d) reduction(* : e) thread_limit(f+g)); (6, OpenMP, #pragma omp target); (2, OpenMP, #pragma omp threadprivate(a)); (5, OpenMP, #pragma omp teams)"}
{"predict": "(341, CUDA, __shared__ int cur_send_num;); (339, CUDA, __shared__ int recv_num, finish_num;)", "label": "(341, CUDA, __shared__ int cur_send_num;); (339, CUDA, __shared__ int recv_num, finish_num;)"}
{"predict": "(46, TBB, tbb::flow::make_edge( q2, q3 );); (70, TBB, tbb::flow::make_edge( q2, q3 );); (39, TBB, tbb::flow::make_edge( q, q2 );); (5, TBB, tbb::flow::queue_node<T> q2(g);); (5, TBB, tbb::flow::queue_node<T> q3(g);); (6, TBB, tbb::flow::make_edge( q, q2 );); (39, TBB, tbb::flow::remove_edge( q, q2 );); (-3, TBB, tbb::flow::graph g;); (55, TBB, tbb::flow::remove_edge( q,  q2 );); (70, TBB, tbb::flow::remove_edge( q2, q3 );); (-4, TBB, tbb::flow::queue_node<T> q(g);); (86, TBB, tbb::flow::remove_edge( q,  q2 );)", "label": "(4, TBB, tbb::flow::queue_node<T> q2(g);); (70, TBB, tbb::flow::make_edge( q2, q3 );); (67, TBB, tbb::flow::queue_node<T> q3(g);); (37, TBB, tbb::flow::make_edge( q, q2 );); (56, TBB, tbb::flow::remove_edge( q, q2 );); (87, TBB, tbb::flow::remove_edge( q,  q2 );); (-3, TBB, tbb::flow::queue_node<T> q(g);); (-5, TBB, tbb::flow::graph g;)"}
{"predict": "(23, OpenCL, clReleaseEvent(event);)", "label": "(23, OpenCL, clReleaseEvent(event);)"}
{"predict": "(243, MPI, MPI_Finalize();); (24, MPI, MPI_Comm_rank(comm, &mpi_rank);); (22, MPI, MPI_Comm_size(comm, &mpi_size);); (21, MPI, MPI_Get_processor_name(mpi_name, &mpi_namelen);)", "label": "(25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (25, MPI, MPI_Get_processor_name(mpi_name, &mpi_namelen);); (22, MPI, MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);)"}
{"predict": "(43, OpenCL, clReleaseMemObject(imageDesc.mem_object);); (14, OpenCL, imageDesc.mem_object = clCreateBuffer(&context, CL_MEM_READ_WRITE, imageDesc.image_height * imageDesc.image_width, nullptr, &retVal);)", "label": "(43, OpenCL, clReleaseMemObject(imageDesc.mem_object);); (14, OpenCL, imageDesc.mem_object = clCreateBuffer(&context, CL_MEM_READ_WRITE, imageDesc.image_height * imageDesc.image_width, nullptr, &retVal);)"}
{"predict": "(60, OpenMP, omp_target_free(y1, omp_get_default_device());); (40, OpenMP, #pragma omp teams distribute parallel for default(shared) collapse(3)); (40, OpenMP, #pragma omp target is_device_ptr(x1, y1) map(tofrom: or[0:m3k*m2k*m1k]) map(tofrom: os[0:m3j*m2j*m1j])); (36, OpenMP, omp_target_associate_ptr(y1, x1, m3k*m2k*m1k*sizeof(double), 0, omp_get_default_device());)", "label": "(114, OpenMP, omp_target_free(y1, omp_get_default_device());); (41, OpenMP, #pragma omp teams distribute parallel for default(shared) collapse(3)); (39, OpenMP, #pragma omp target is_device_ptr(x1, y1) map(tofrom: or[0:m3k*m2k*m1k]) map(tofrom: os[0:m3j*m2j*m1j])); (13, OpenMP, y1 = (double*)omp_target_alloc(m3k*m2k*m1k*sizeof(double), omp_get_default_device());)"}
{"predict": "(9, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)", "label": "(9, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(17, CUDA, extern __shared__ _VOLATILE_ float sMem[];)", "label": "(17, CUDA, extern __shared__ _VOLATILE_ float sMem[];)"}
{"predict": "(1, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)", "label": "(1, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)"}
