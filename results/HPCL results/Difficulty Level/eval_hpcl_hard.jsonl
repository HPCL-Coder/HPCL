{"predict": "(114, CUDA, cudaFree(gpuTransposeMatrix);); (44, CUDA, cudaMemcpy(gpuMatrix, Matrix, NUM * sizeof(float), cudaMemcpyHostToDevice);); (18, CUDA, cudaEventCreate(&start);); (109, CUDA, cudaFree(gpuMatrix);); (51, CUDA, cudaEventSynchronize(stop);); (44, CUDA, cudaEventRecord(stop, NULL);); (46, CUDA, cudaEventElapsedTime(&eventMs, start, stop);); (79, CUDA, cudaMemcpy(TransposeMatrix, gpuTransposeMatrix, NUM * sizeof(float), cudaMemcpyDeviceToHost);); (6, CUDA, cudaGetDeviceProperties(&devProp, 0);); (32, CUDA, cudaMalloc((void**)&gpuMatrix, NUM * sizeof(float));); (33, CUDA, cudaMalloc((void**)&gpuTransposeMatrix, NUM * sizeof(float));); (36, CUDA, cudaEventRecord(start, NULL);); (106, CUDA, cudaEventDestroy(start);); (106, CUDA, cudaEventDestroy(stop);)", "label": "(53, CUDA, cudaEventSynchronize(stop);); (86, CUDA, cudaMemcpy(TransposeMatrix, gpuTransposeMatrix, NUM * sizeof(float), cudaMemcpyDeviceToHost);); (9, CUDA, cudaGetDeviceProperties(&devProp, 0);); (44, CUDA, cudaMemcpy(gpuMatrix, Matrix, NUM * sizeof(float), cudaMemcpyHostToDevice);); (114, CUDA, cudaFree(gpuMatrix);); (32, CUDA, cudaMalloc((void**)&gpuMatrix, NUM * sizeof(float));); (49, CUDA, cudaEventElapsedTime(&eventMs, start, stop);); (36, CUDA, cudaEventRecord(start, NULL);); (112, CUDA, cudaFree(gpuTransposeMatrix);); (42, CUDA, cudaEventRecord(stop, NULL);); (10, CUDA, cudaEventCreate(&stop);); (28, CUDA, cudaMalloc((void**)&gpuTransposeMatrix, NUM * sizeof(float));); (6, CUDA, cudaEventCreate(&start);)"}
{"predict": "(58, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 1, NULL, threads, localThreads, 0, NULL, NULL );); (46, OpenCL, error = clSetKernelArg(kernel, 0, sizeof( streams[0] ), &streams[0] );); (47, OpenCL, error = clSetKernelArg(kernel, 1, sizeof( streams[1] ), &streams[1] );); (59, OpenCL, error = clEnqueueReadBuffer( queue, streams[1], true, 0, sizeof( cl_double ) * TEST_SIZE, outData, 0, NULL, NULL );)", "label": "(58, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 1, NULL, threads, localThreads, 0, NULL, NULL );); (46, OpenCL, error = clSetKernelArg( kernel, 0, sizeof( streams[ 0 ] ), &streams[0] );); (60, OpenCL, error = clEnqueueReadBuffer( queue, streams[1], true, 0, sizeof( cl_double ) * TEST_SIZE, outData, 0, NULL, NULL );); (46, OpenCL, error = clSetKernelArg( kernel, 1, sizeof( streams[ 1 ] ), &streams[1] );)"}
{"predict": "(143, CUDA, cudaCheck(cudaFree(d_inp));); (34, CUDA, cudaCheck(cudaMalloc(&d_att, B * T * C * sizeof(float)));); (37, CUDA, cudaCheck(cudaMalloc(&d_dpreatt, B * T * C * sizeof(float)));); (30, CUDA, cudaCheck(cudaMalloc(&d_qkvr, B * T * 3 * C * sizeof(float)));); (33, CUDA, cudaCheck(cudaMalloc(&d_dinp, B * T * 3 * C * sizeof(float)));); (34, CUDA, cudaCheck(cudaMalloc(&d_dqkvr, B * T * 3 * C * sizeof(float)));); (34, CUDA, cudaCheck(cudaMalloc(&d_datt, B * T * C * sizeof(float)));); (137, CUDA, cudaCheck(cudaFree(d_dpreatt));); (134, CUDA, cudaCheck(cudaFree(d_qkvr));); (135, CUDA, cudaCheck(cudaFree(d_att));); (25, CUDA, cudaCheck(cudaMalloc(&d_preatt, B * T * C * sizeof(float)));); (29, CUDA, cudaCheck(cudaMalloc(&d_dout, B * T * C * sizeof(float)));); (133, CUDA, cudaCheck(cudaFree(d_dinp));); (134, CUDA, cudaCheck(cudaFree(d_dqkvr));); (135, CUDA, cudaCheck(cudaFree(d_scratch));); (20, CUDA, cudaCheck(cudaMalloc(&d_out, B * T * C * sizeof(float)));); (126, CUDA, cudaCheck(cudaFree(d_preatt));); (129, CUDA, cudaCheck(cudaFree(d_dout));); (133, CUDA, cudaCheck(cudaFree(d_out));); (20, CUDA, cudaCheck(cudaMalloc(&d_scratch, cuda_att_get_workspace_size(B, T, C, NH)));); (12, CUDA, cudaCheck(cudaMalloc(&d_inp, B * T * 3 * C * sizeof(float)));); (20, CUDA, cudaCheck(cudaMemcpy(d_inp, qkv, B * T * 3 * C * sizeof(float), cudaMemcpyHostToDevice));); (129, CUDA, cudaCheck(cudaFree(d_scratch));); (122, CUDA, cuda", "label": "(45, CUDA, cudaCheck(cudaMemset(d_dinp, 0, B * T * 3 * C * sizeof(float)));); (47, CUDA, cudaCheck(cudaMalloc(&d_dpreatt, B * NH * T * T * sizeof(float)));); (99, CUDA, cudaCheck(cudaFree(d_inp));); (38, CUDA, cudaCheck(cudaMemset(d_att, 0, B * NH * T * T * sizeof(float)));); (105, CUDA, cudaCheck(cudaFree(d_dpreatt));); (102, CUDA, cudaCheck(cudaFree(d_dinp));); (102, CUDA, cudaCheck(cudaFree(d_dqkvr));); (28, CUDA, cudaCheck(cudaMemset(d_qkvr, 0, B * T * 3 * C * sizeof(float)));); (31, CUDA, cudaCheck(cudaMemset(d_preatt, 0, B * NH * T * T * sizeof(float)));); (93, CUDA, cudaCheck(cudaFree(d_qkvr));); (30, CUDA, cudaCheck(cudaMalloc(&d_att, B * NH * T * T * sizeof(float)));); (99, CUDA, cudaCheck(cudaFree(d_datt));); (35, CUDA, cudaCheck(cudaMemset(d_dqkvr, 0, B * T * 3 * C * sizeof(float)));); (24, CUDA, cudaCheck(cudaMemset(d_out, 0, B * T * C * sizeof(float)));); (90, CUDA, cudaCheck(cudaFree(d_preatt));); (27, CUDA, cudaCheck(cudaMalloc(&d_dout, B * T * C * sizeof(float)));); (18, CUDA, cudaCheck(cudaMalloc(&d_qkvr, B * T * 3 * C * sizeof(float)));); (16, CUDA, cudaCheck(cudaMemcpy(d_inp, qkv, B * T * 3 * C * sizeof(float), cudaMemcpyHostToDevice));); (35, CUDA, cudaCheck(cudaMemset(d_scratch, 0, B * T * C * sizeof(float)));); (19, CUDA, cudaCheck(cudaMalloc(&d_preatt, B * NH * T * T * sizeof(float)));); (91, CUDA, cudaCheck(cudaFree(d_scratch));); (22, CUDA, cudaCheck(cudaMemcpy(d_dout, dout, B * T * C * sizeof(float), cudaMemcpyHostToDevice));); (81, CUDA, cudaCheck(cudaFree(d_out));); (23, CUDA, cudaCheck(cudaMalloc(&d_dqkvr, B * T * 3 * C * sizeof(float)));); (20, CUDA, cudaCheck(cudaMalloc(&d_dinp, B * T * 3 * C * sizeof(float)));); (26, CUDA, cudaCheck(cudaMemset(d_datt, 0, B * NH * T * T * sizeof(float)));); (26, CUDA, cudaCheck(cudaMalloc(&d_scratch, B * T * C * sizeof(float)));); (9, CUDA, cudaCheck(cudaMalloc(&d_out, B * T * C * sizeof(float)));); (22, CUDA, cudaCheck(cudaMalloc(&d_datt, B * NH * T * T * sizeof(float)));); (3, CUDA, cudaCheck(cudaMalloc(&d_inp, B * T * 3 * C * sizeof(float)));); (75, CUDA, cudaCheck(cudaFree(d_att));); (18, CUDA, cudaCheck(cudaMemset(d_dpreatt, 0, B * NH * T * T * sizeof(float)));); (74, CUDA, cudaCheck(cudaFree(d_dout));)"}
{"predict": "(46, MPI, MPI_Buffer_detach(&bptr, &bl);); (26, MPI, MPI_Recv(b, 10, MPI_INT, i, 27 + j, MPI_COMM_WORLD, &status);); (15, MPI, MPI_Bsend(a, 10, MPI_INT, 0, 27 + j, MPI_COMM_WORLD);); (7, MPI, MPI_Buffer_attach(buf, BUFSIZE);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)", "label": "(48, MPI, MPI_Buffer_detach(&bptr, &bl);); (17, MPI, MPI_Start(&request);); (27, MPI, MPI_Recv(b, 10, MPI_INT, i, 27 + j, MPI_COMM_WORLD, &status);); (8, MPI, MPI_Buffer_attach(buf, BUFSIZE);); (15, MPI, MPI_Wait(&request, &status);); (15, MPI, MPI_Request_free(&request);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (6, MPI, MPI_Bsend_init(a, 10, MPI_INT, 0, 27 + j, MPI_COMM_WORLD, &request);)"}
{"predict": "(112, MPI, MPI_Allreduce(&run_time, &max_run_time, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);); (31, MPI, auto err = MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided);); (37, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (120, MPI, MPI_Finalize();); (35, MPI, MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);); (111, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(31, MPI, auto err = MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided);); (119, MPI, MPI_Finalize();); (37, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (40, MPI, ascent_opt[\"mpi_comm\"] = MPI_Comm_c2f(MPI_COMM_WORLD);); (111, MPI, MPI_Reduce(&run_time, &max_run_time, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);); (33, MPI, MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);)"}
{"predict": "(25, CUDA, __shared__ FP sSafeE[Br][Bc];); (12, CUDA, __shared__ FP sO[Br][dim];); (9, CUDA, __shared__ FP sMax[Br];); (11, CUDA, __shared__ FP sDenom[Br];); (3, CUDA, __shared__ FP sV[Bc][dim];); (14, CUDA, __shared__ FP sQK[Br][Bc];); (-1, CUDA, __shared__ FP sK[Bc][dim];); (-3, CUDA, __shared__ FP sQ[Br][dim];)", "label": "(12, CUDA, __shared__ FP sV[Bc][dim];); (19, CUDA, __shared__ FP sMax[Br];); (9, CUDA, __shared__ FP sK[Bc][dim];); (12, CUDA, __shared__ FP sQK[Br][Bc];); (13, CUDA, __shared__ FP sSafeE[Br][Bc];); (9, CUDA, __shared__ FP sO[Br][dim];); (4, CUDA, __shared__ FP sQ[Br][dim];); (12, CUDA, __shared__ FP sDenom[Br];)"}
{"predict": "(22, CUDA, CHECK_CUDA(cudaSetDevice(device_id_));)", "label": "(22, CUDA, CudaCheck(cudaSetDevice(device_id_));)"}
{"predict": "(68, OpenMP, #pragma omp parallel private(i)); (62, OpenMP, #pragma omp parallel shared(i)); (4, OpenMP, #pragma omp parallel); (24, OpenMP, #pragma omp for simd lastprivate(argc,); (28, OpenMP, #pragma omp for simd lastprivate(argc > 0 ? argv[1] : argv[2])); (44, OpenMP, #pragma omp for simd lastprivate(argv[1])); (31, OpenMP, #pragma omp for simd lastprivate(argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (2, OpenMP, #pragma omp for simd lastprivate(); (6, OpenMP, #pragma omp for simd lastprivate()); (8, OpenMP, #pragma omp for simd lastprivate(argc); (43, OpenMP, #pragma omp for simd lastprivate(e, g)); (31, OpenMP, #pragma omp for simd lastprivate(conditional: argc) lastprivate(conditional:); (49, OpenMP, #pragma omp for simd lastprivate(h)); (57, OpenMP, #pragma omp for simd lastprivate(i)); (0, OpenMP, #pragma omp for simd lastprivate(argc,); (36, OpenMP, #pragma omp for simd lastprivate(z, a, b)); (23, OpenMP, #pragma omp for simd lastprivate(S1)); (59, OpenMP, #pragma omp parallel shared(i)); (50, OpenMP, #pragma omp for simd lastprivate(i)); (44, OpenMP, #pragma omp for simd lastprivate(B::x)); (37, OpenMP, #pragma omp for simd lastprivate(argv[1], argv[2])); (26, OpenMP, #pragma omp for simd lastprivate(foo:argc)); (51, OpenMP, #pragma omp for simd lastprivate(j))", "label": "(69, OpenMP, #pragma omp parallel private(i)); (42, OpenMP, #pragma omp master taskloop lastprivate(a, b)); (49, OpenMP, #pragma omp master taskloop lastprivate(z, e, g)); (16, OpenMP, #pragma omp master taskloop lastprivate(argc); (2, OpenMP, #pragma omp parallel); (18, OpenMP, #pragma omp master taskloop lastprivate(argc,); (25, OpenMP, #pragma omp master taskloop lastprivate(argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (0, OpenMP, #pragma omp master taskloop lastprivate); (47, OpenMP, #pragma omp master taskloop lastprivate(h)); (18, OpenMP, #pragma omp master taskloop lastprivate(argc > 0 ? argv[1] : argv[2])); (64, OpenMP, #pragma omp master taskloop lastprivate(i)); (51, OpenMP, #pragma omp master taskloop allocate(omp_thread_mem_alloc: i) lastprivate(i)); (27, OpenMP, #pragma omp master taskloop lastprivate(S1)); (57, OpenMP, #pragma omp master taskloop lastprivate(j)); (-3, OpenMP, #pragma omp master taskloop lastprivate(); (32, OpenMP, #pragma omp master taskloop lastprivate(argv[1])); (-1, OpenMP, #pragma omp master taskloop lastprivate()); (18, OpenMP, #pragma omp master taskloop lastprivate(conditional: argc) lastprivate(conditional:); (50, OpenMP, #pragma omp parallel shared(i))"}
{"predict": "(189, OpenMP, comp_add_script_line(\"Flag=MGF_INPUT_EXTRA_BYTE\\n\");); (10, OpenMP, comp_add_script_line(\"Func=DynamicFunc__append_input2_from_input\\n\");); (28, OpenMP, comp_add_script_line(\"Func=DynamicFunc__append_input%d_to_output1_FINAL\\n\", side);); (2, OpenMP, comp_add_script_line(\"Side=%d\\n\", side);)", "label": "(39, OpenMP, comp_add_script_line(\"Func=DynamicFunc__\" #C \"_crypt_input%d_to_output1_FINAL\\n\",side);); (18, OpenMP, else                 comp_add_script_line(\"Func=DynamicFunc__append_salt%s\\n\", side==2?\"2\":\"\");); (9, OpenMP, comp_add_script_line(\"Func=DynamicFunc__clean_input2_kwik\\n\");); (61, OpenMP, comp_add_script_line(\"MaxInputLen=110\\n\");); (5, OpenMP, comp_add_script_line(\"Func=DynamicFunc__set_input_len_%d\\n\", keys_hex_len);)"}
{"predict": "(17, CUDA, ColorSpinorParam cudaParam(cpuParam, inv_param, QUDA_CUDA_FIELD_LOCATION);)", "label": "(17, CUDA, ColorSpinorParam cudaParam(cpuParam, inv_param, QUDA_CUDA_FIELD_LOCATION);)"}
{"predict": "(43, MPI, MPI_Isend(&size_send, 1, MPI_INT, rank_send, 0, comm, &request[1]);); (45, MPI, MPI_Waitall(num_req, request, status);); (25, MPI, MPI_Irecv(&size_recv, 1, MPI_INT, rank_recv, 0, comm, &request[0]);)", "label": "(55, MPI, MPI_Irecv(buf_recv, size_recv, MPI_BYTE, rank_recv, 0, comm, &request[num_req]);); (25, MPI, MPI_Irecv(&size_recv, 1, MPI_INT, rank_recv, 0, comm, &request[num_req]);); (60, MPI, MPI_Isend(buf_send, size_send, MPI_BYTE, rank_send, 0, comm, &request[num_req]);); (38, MPI, MPI_Isend(&size_send, 1, MPI_INT, rank_send, 0, comm, &request[num_req]);); (41, MPI, MPI_Waitall(num_req, request, status);)"}
{"predict": "(40, OpenMP, #pragma omp distribute simd safelen ((ST > 0) ? 1 + ST : 2)); (27, OpenMP, #pragma omp distribute simd safelen (ST); (53, OpenMP, #pragma omp distribute simd safelen (S)); (63, OpenMP, #pragma omp distribute simd safelen (argv[1]=2)); (74, OpenMP, #pragma omp distribute simd safelen (N)); (6, OpenMP, #pragma omp distribute simd safelen (); (43, OpenMP, #pragma omp distribute simd safelen (foobool(argc)), safelen (true), safelen (-5)); (10, OpenMP, #pragma omp distribute simd safelen ()); (14, OpenMP, #pragma omp distribute simd safelen (argc); (63, OpenMP, #pragma omp distribute simd safelen (4)); (-5, OpenMP, #pragma omp distribute simd safelen); (-7, OpenMP, #pragma omp target); (22, OpenMP, #pragma omp distribute simd safelen (1))); (-7, OpenMP, #pragma omp teams)", "label": "(11, OpenMP, #pragma omp distribute simd safelen (); (71, OpenMP, #pragma omp distribute simd safelen (4)); (3, OpenMP, #pragma omp distribute simd safelen); (19, OpenMP, #pragma omp distribute simd safelen (argc); (51, OpenMP, #pragma omp distribute simd safelen (S)); (44, OpenMP, #pragma omp distribute simd safelen (foobool(argc)), safelen (true), safelen (-5)); (72, OpenMP, #pragma omp distribute simd safelen (N)); (27, OpenMP, #pragma omp distribute simd safelen (1))); (58, OpenMP, #pragma omp distribute simd safelen (argv[1]=2)); (-6, OpenMP, #pragma omp target); (18, OpenMP, #pragma omp distribute simd safelen (ST); (29, OpenMP, #pragma omp distribute simd safelen ((ST > 0) ? 1 + ST : 2)); (5, OpenMP, #pragma omp distribute simd safelen ()); (-9, OpenMP, #pragma omp teams)"}
{"predict": "(21, OpenMP, #pragma omp parallel for simd private(argc > 0 ? argv[1] : argv[2])); (23, OpenMP, #pragma omp parallel for simd private(argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (55, OpenMP, #pragma omp parallel for simd private(j)); (11, OpenMP, #pragma omp parallel for simd private(argc); (4, OpenMP, #pragma omp parallel); (47, OpenMP, #pragma omp parallel for simd private(i)); (12, OpenMP, #pragma omp parallel for simd private(argc,); (17, OpenMP, #pragma omp parallel for simd private(S1)); (26, OpenMP, #pragma omp parallel for simd private(argv[1])); (19, OpenMP, #pragma omp parallel for simd private(a, b)); (21, OpenMP, #pragma omp parallel for simd private(e, g, z)); (35, OpenMP, #pragma omp parallel for simd copyprivate(h)); (47, OpenMP, #pragma omp parallel for simd private(); (2, OpenMP, #pragma omp parallel for simd private()); (26, OpenMP, #pragma omp parallel for simd private(h)); (41, OpenMP, #pragma omp parallel for simd shared(i)); (7, OpenMP, #pragma omp parallel for simd private(argc,)", "label": "(33, OpenMP, #pragma omp parallel for simd private(argv[1])); (29, OpenMP, #pragma omp parallel for simd private(z, a, b)); (47, OpenMP, #pragma omp parallel for simd private(i)); (15, OpenMP, #pragma omp parallel for simd private(argc,); (23, OpenMP, #pragma omp parallel for simd private(S1)); (50, OpenMP, #pragma omp parallel shared(i)); (33, OpenMP, #pragma omp parallel for simd private(h)); (49, OpenMP, #pragma omp parallel private(i)); (-2, OpenMP, #pragma omp parallel); (12, OpenMP, #pragma omp parallel for simd private(argc > 0 ? argv[1] : argv[2])); (26, OpenMP, #pragma omp parallel for simd private(e, g)); (-2, OpenMP, #pragma omp parallel for simd private); (30, OpenMP, #pragma omp parallel for simd nowait); (44, OpenMP, #pragma omp parallel for simd private(j)); (10, OpenMP, #pragma omp parallel for simd private(argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (0, OpenMP, #pragma omp parallel for simd private(argc); (-4, OpenMP, #pragma omp parallel for simd private()"}
{"predict": "(54, MPI, MPI_Comm_size(comm, &size);); (52, MPI, MPI_Win_create(target.buf, target.maxbufsize, base_type_size, MPI_INFO_NULL, comm, &win);); (67, MPI, MPI_Barrier(comm);); (79, MPI, MPI_Win_free(&win);); (26, MPI, MPI_Type_get_extent(dtp.DTP_base_type, &lb, &extent);); (47, MPI, MPI_Comm_rank(comm, &rank);)", "label": "(52, MPI, MPI_Comm_size(comm, &size);); (62, MPI, MPI_Barrier(comm);); (52, MPI, MPI_Win_create(target.buf, target.maxbufsize, base_type_size, MPI_INFO_NULL, comm, &win);); (69, MPI, MPI_Win_free(&win);); (25, MPI, MPI_Type_get_extent(dtp.DTP_base_type, &lb, &extent);); (46, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(114, OpenMP, plasma_time_t start = omp_get_wtime();); (116, OpenMP, plasma_time_t stop = omp_get_wtime();)", "label": "(117, OpenMP, plasma_time_t stop = omp_get_wtime();); (113, OpenMP, plasma_time_t start = omp_get_wtime();)"}
{"predict": "(33, OpenMP, #pragma omp target parallel for simd linear(argv[1])); (63, OpenMP, #pragma omp target parallel for simd linear(j)); (37, OpenMP, #pragma omp target parallel for simd linear(z, h)); (31, OpenMP, #pragma omp target parallel for simd linear(a, b : B::ib)); (43, OpenMP, #pragma omp parallel); (39, OpenMP, #pragma omp target parallel for simd linear(i)); (25, OpenMP, #pragma omp target parallel for simd linear(S1)); (50, OpenMP, #pragma omp target parallel for simd linear(v : j)); (36, OpenMP, #pragma omp target parallel for simd linear(i : 4)); (15, OpenMP, #pragma omp target parallel for simd linear(argc : 5) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (18, OpenMP, #pragma omp target parallel for simd linear(argc > 0 ? argv[1] : argv[2])); (5, OpenMP, #pragma omp target parallel for simd linear(argc); (-5, OpenMP, #pragma omp target parallel for simd linear); (-1, OpenMP, #pragma omp target parallel for simd linear(); (47, OpenMP, #pragma omp target parallel for simd linear(v : i)); (23, OpenMP, #pragma omp target parallel for simd linear(e, g)); (5, OpenMP, #pragma omp target parallel for simd linear(argc,); (7, OpenMP, #pragma omp target parallel for simd linear(argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (13, OpenMP, #pragma omp target parallel for simd linear()); (37, OpenMP, #pragma omp target parallel for simd allocate(omp_thread_mem_alloc: v) linear(v : i))", "label": "(32, OpenMP, #pragma omp target parallel for simd linear(a, b : B::ib)); (23, OpenMP, #pragma omp target parallel for simd linear(argc : 5) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (4, OpenMP, #pragma omp target parallel for simd linear); (12, OpenMP, #pragma omp target parallel for simd linear(argc); (40, OpenMP, #pragma omp target parallel for simd linear(i)); (33, OpenMP, #pragma omp target parallel for simd linear(e, g, z)); (45, OpenMP, #pragma omp target parallel for simd linear(v : i)); (34, OpenMP, #pragma omp target parallel for simd linear(h)); (39, OpenMP, #pragma omp parallel); (12, OpenMP, #pragma omp target parallel for simd linear(argc > 0 ? argv[1] : argv[2])); (-1, OpenMP, #pragma omp target parallel for simd linear(); (7, OpenMP, #pragma omp target parallel for simd linear(argc,); (49, OpenMP, #pragma omp target parallel for simd linear(v : j)); (22, OpenMP, #pragma omp target parallel for simd linear(argv[1])); (13, OpenMP, #pragma omp target parallel for simd linear(S1)); (42, OpenMP, #pragma omp target parallel for simd linear(j)); (-4, OpenMP, #pragma omp target parallel for simd linear())"}
{"predict": "(39, OpenMP, #pragma omp target simd collapse(4, )); (30, OpenMP, #pragma omp target simd collapse(4); (63, OpenMP, #pragma omp target simd collapse(4, 8)); (74, OpenMP, #pragma omp target simd collapse(foo())); (77, OpenMP, #pragma omp target simd collapse(0)); (13, OpenMP, #pragma omp target simd collapse(, )); (17, OpenMP, #pragma omp target simd collapse(,); (6, OpenMP, #pragma omp target simd collapse()); (-3, OpenMP, #pragma omp target simd collapse); (51, OpenMP, #pragma omp target simd collapse(4, , 4)); (73, OpenMP, #pragma omp target simd collapse(5 - 5)); (63, OpenMP, #pragma omp target simd collapse(2.5)); (4, OpenMP, #pragma omp target simd collapse(,); (-6, OpenMP, #pragma omp target simd collapse(); (28, OpenMP, #pragma omp target simd collapse(4)); (66, OpenMP, #pragma omp target simd collapse(-5)); (19, OpenMP, #pragma omp target simd collapse(4,); (29, OpenMP, #pragma omp target simd collapse(4 4))", "label": "(41, OpenMP, #pragma omp target simd collapse(4, )); (12, OpenMP, #pragma omp target simd collapse()); (48, OpenMP, #pragma omp target simd collapse(4 4)); (63, OpenMP, #pragma omp target simd collapse(4, 8)); (82, OpenMP, #pragma omp target simd collapse(5 - 5)); (4, OpenMP, #pragma omp target simd collapse(); (15, OpenMP, #pragma omp target simd collapse(, )); (10, OpenMP, #pragma omp target simd collapse(,); (74, OpenMP, #pragma omp target simd collapse(0)); (36, OpenMP, #pragma omp target simd collapse(4)); (45, OpenMP, #pragma omp target simd collapse(4, , 4)); (63, OpenMP, #pragma omp target simd collapse(foo())); (66, OpenMP, #pragma omp target simd collapse(-5)); (23, OpenMP, #pragma omp target simd collapse(4,); (17, OpenMP, #pragma omp target simd collapse(4); (11, OpenMP, #pragma omp target simd collapse 4)); (-11, OpenMP, #pragma omp target simd collapse); (53, OpenMP, #pragma omp target simd collapse(2.5))"}
{"predict": "(97, OpenMP, nthreads = omp_get_num_threads();); (82, OpenMP, #pragma omp parallel firstprivate(niter) private(step)); (62, OpenMP, #pragma omp parallel); (85, OpenMP, #pragma omp master)", "label": "(97, OpenMP, nthreads = omp_get_num_threads();); (82, OpenMP, #pragma omp parallel firstprivate(niter) private(step)); (62, OpenMP, #pragma omp parallel); (85, OpenMP, #pragma omp master)"}
{"predict": "(61, OpenCL, TEST_OCL_CHECK(clReleaseEvent(read_buffer_event));); (61, OpenCL, TEST_OCL_CHECK(clReleaseEvent(eltwise_event));); (39, OpenCL, TEST_OCL_CHECK(clReleaseEvent(write_buffer_event));)", "label": "(62, OpenCL, TEST_OCL_CHECK(clReleaseEvent(read_buffer_event));); (64, OpenCL, TEST_OCL_CHECK(clReleaseCommandQueue(ocl_queue));); (52, OpenCL, TEST_OCL_CHECK(clWaitForEvents(1, &read_buffer_event));); (61, OpenCL, TEST_OCL_CHECK(clReleaseEvent(eltwise_event));); (59, OpenCL, TEST_OCL_CHECK(clReleaseEvent(write_buffer_event));)"}
{"predict": "(56, MPI, MPI_Finalize();); (22, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (30, MPI, MPI_Bcast(&params, 1, *params_type, root, MPI_COMM_WORLD);); (14, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (10, MPI, MPI_Init_thread(NULL, NULL, MPI_THREAD_FUNNELED, &thread_level);)", "label": "(56, MPI, MPI_Finalize();); (22, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (29, MPI, MPI_Bcast(&params, 1, *params_type, root, MPI_COMM_WORLD);); (14, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (10, MPI, MPI_Init_thread(NULL, NULL, MPI_THREAD_FUNNELED, &thread_level);)"}
{"predict": "(150, MPI, MPI_Exscan(&max_offset, out_starting_pos, 1, MPI_UINT64_T, MPI_MAX, MPI_COMM_WORLD);); (148, MPI, MPI_Allreduce(&rc, &all_rc, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (14, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);)", "label": "(141, MPI, MPI_Exscan(&max_offset, &starting_pos, 1, MPI_UINT64_T, MPI_MAX, MPI_COMM_WORLD);); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (258, MPI, MPI_Allreduce(&pos, &max_pos, 1, MPI_UINT64_T, MPI_MAX, MPI_COMM_WORLD);); (13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(7, CUDA, cudaStreamCreate(&s);); (41, CUDA, cudaEventRecord(end, s);); (41, CUDA, cudaEventSynchronize(end);); (30, CUDA, cudaEventRecord(begin, s);); (65, CUDA, cudaStreamDestroy(s);); (30, CUDA, cudaEventElapsedTime(&time, begin, end);); (4, CUDA, cudaEventCreate(&end);); (4, CUDA, cudaEventCreate(&begin);)", "label": "(72, CUDA, cudaStreamDestroy(s);); (5, CUDA, cudaStreamCreate(&s);); (31, CUDA, cudaEventRecord(end, s);); (31, CUDA, cudaEventSynchronize(end);); (23, CUDA, cudaEventRecord(begin, s);); (66, CUDA, cudaEventDestroy(end);); (3, CUDA, cudaEventCreate(&end);); (63, CUDA, cudaEventDestroy(begin);); (0, CUDA, cudaEventCreate(&begin);); (27, CUDA, cudaEventElapsedTime(&time, begin, end);)"}
{"predict": "(7, OpenMP, int tid = omp_get_thread_num();)", "label": "(7, OpenMP, int tid = omp_get_thread_num();)"}
{"predict": "(15, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (12, CUDA, cudaMalloc((void**)(&d_in1), in1_bytes);); (15, CUDA, cudaMemcpy(d_in1, in1.data(), in1_bytes, cudaMemcpyHostToDevice);); (12, CUDA, cudaMalloc((void**)(&d_in2), in2_bytes);); (14, CUDA, cudaMemcpy(d_in2, in2.data(), in2_bytes, cudaMemcpyHostToDevice);); (94, CUDA, cudaFree(d_in1);); (95, CUDA, cudaFree(d_out);); (93, CUDA, cudaFree(d_in2);); (28, CUDA, assert(cudaStreamSynchronize(context.device().stream()) == cudaSuccess);); (25, CUDA, assert(cudaMemcpyAsync(out.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, context.device().stream()) == cudaSuccess);)", "label": "(27, CUDA, assert(cudaMemcpy(out.data(), d_out, out_bytes, cudaMemcpyDeviceToHost) == cudaSuccess);); (15, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (12, CUDA, cudaMalloc((void**)(&d_in1), in1_bytes);); (69, CUDA, assert(cudaMemcpyAsync(out.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, context.device().stream()) == cudaSuccess);); (14, CUDA, cudaMemcpy(d_in1, in1.data(), in1_bytes, cudaMemcpyHostToDevice);); (10, CUDA, cudaMalloc((void**)(&d_in2), in2_bytes);); (67, CUDA, assert(cudaStreamSynchronize(context.device().stream()) == cudaSuccess);); (12, CUDA, cudaMemcpy(d_in2, in2.data(), in2_bytes, cudaMemcpyHostToDevice);)"}
{"predict": "(6, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)", "label": "(6, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(65, CUDA, cudaFree( outGPU );); (63, CUDA, cudaEventDestroy( evStop );); (64, CUDA, cudaFree( inGPU );); (60, CUDA, cudaEventDestroy( evStart );)", "label": "(65, CUDA, cudaFree( outGPU );); (63, CUDA, cudaEventDestroy( evStop );); (64, CUDA, cudaFree( inGPU );); (60, CUDA, cudaEventDestroy( evStart );)"}
{"predict": "(2, CUDA, __shared__ float siftPoint[128];); (3, CUDA, __shared__ float maxScore[FMC2H];); (3, CUDA, __shared__ float maxScor2[FMC2H];); (3, CUDA, __shared__ int maxIndex[FMC2H];)", "label": "(3, CUDA, __shared__ float maxScore[FMC2H];); (1, CUDA, __shared__ float siftPoint[128];); (2, CUDA, __shared__ float maxScor2[FMC2H];); (2, CUDA, __shared__ int maxIndex[FMC2H];)"}
{"predict": "(174, OpenMP, #pragma omp simd); (112, OpenMP, #pragma\tomp\ttaskloop private (lptr1,luptr1,nlb1,thread_id1,lsub1,lusup1,nsupr1,Linv,nn,lbstart,lbend,luptr_tmp1,nbrow,lb,lptr1_tmp,rtemp_loc,nbrow_ref,lptr,nbrow1,ik,rel,lk,iknsupc,il,i,irow,fmod_tmp,ikcol,p,ii,jj,t1,t2,j) untied); (229, OpenMP, #pragma omp simd lastprivate(irow))", "label": "(174, OpenMP, #pragma omp simd); (112, OpenMP, #pragma\tomp\ttaskloop private (lptr1,luptr1,nlb1,thread_id1,lsub1,lusup1,nsupr1,Linv,nn,lbstart,lbend,luptr_tmp1,nbrow,lb,lptr1_tmp,rtemp_loc,nbrow_ref,lptr,nbrow1,ik,rel,lk,iknsupc,il,i,irow,fmod_tmp,ikcol,p,ii,jj,t1,t2,j) untied); (229, OpenMP, #pragma omp simd lastprivate(irow))"}
{"predict": "(18, CUDA, thrust::device_vector<float> dev_B(Brow*Bcol);); (12, CUDA, thrust::host_vector<float>       A(Arow*Acol);); (14, CUDA, thrust::device_vector<float> dev_C(Crow*Ccol);); (32, CUDA, cudaDeviceSynchronize();); (10, CUDA, thrust::host_vector<float>       B(Brow*Bcol);); (10, CUDA, thrust::host_vector<float>       C(Crow*Ccol);); (11, CUDA, thrust::device_vector<float> dev_A(Arow*Acol);)", "label": "(18, CUDA, thrust::device_vector<float> dev_B(Brow*Bcol);); (12, CUDA, thrust::host_vector<float>       A(Arow*Acol);); (14, CUDA, thrust::device_vector<float> dev_C(Crow*Ccol);); (32, CUDA, cudaDeviceSynchronize();); (10, CUDA, thrust::host_vector<float>       B(Brow*Bcol);); (10, CUDA, thrust::host_vector<float>       C(Crow*Ccol);); (11, CUDA, thrust::device_vector<float> dev_A(Arow*Acol);)"}
{"predict": "(14, MPI, MPI_Init(&argc, &argv);); (147, MPI, err = MPI_File_close(&fh);); (152, MPI, err = MPI_File_set_view(fh, 0, MPI_BYTE, filetype, \"native\", MPI_INFO_NULL);); (23, MPI, MPI_Dims_create(nprocs, 2, psize);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (95, MPI, err = MPI_Type_commit(&buftype);); (66, MPI, err = MPI_Type_free(&subType);); (10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (52, MPI, err = MPI_Type_create_subarray(2, gsize, count, start, MPI_ORDER_C, MPI_BYTE, &subType);); (65, MPI, err = MPI_Type_commit(&filetype);); (141, MPI, err = MPI_File_write_all(fh, buf, buf_len, MPI_BYTE, &status);); (8, MPI, MPI_Finalize();); (66, MPI, err = MPI_Type_free(&filetype);); (46, MPI, err = MPI_Type_commit(&subType);); (133, MPI, err = MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_RDONLY, MPI_INFO_NULL, &fh);); (93, MPI, err = MPI_Type_free(&buftype);); (59, MPI, err = MPI_Type_create_struct(NVARS, array_of_blocklengths, array_of_displacements, array_of_types, &buftype);); (132, MPI, err = MPI_File_read_all(fh, buf, buf_len, MPI_BYTE, &status);); (125, MPI, MPI_Barrier(MPI_COMM_WORLD);); (56, MPI, err = MPI_Type_commit(&buftype);)", "label": "(14, MPI, MPI_Init(&argc, &argv);); (130, MPI, err = MPI_File_iwrite_all(fh, buf, 1, buftype, &req);); (100, MPI, err = MPI_Type_commit(&buftype);); (56, MPI, err = MPI_Type_create_subarray(2, gsize, count, start, MPI_ORDER_C, MPI_BYTE, &subType);); (57, MPI, err = MPI_Type_commit(&subType);); (68, MPI, err = MPI_Type_commit(&filetype);); (69, MPI, err = MPI_Type_free(&subType);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (115, MPI, err = MPI_File_write_all(fh, buf, 1, buftype, &status);); (125, MPI, err = MPI_File_read_all(fh, buf, 1, buftype, &status);); (129, MPI, err = MPI_File_iread_all(fh, buf, 1, buftype, &req);); (121, MPI, MPI_Wait(&req, &status);); (8, MPI, MPI_Finalize();); (106, MPI, err = MPI_File_set_view(fh, 0, MPI_BYTE, filetype, \"native\", MPI_INFO_NULL);); (134, MPI, err = MPI_Type_free(&buftype);); (131, MPI, err = MPI_Type_free(&filetype);); (127, MPI, MPI_File_close(&fh);); (10, MPI, err = MPI_Dims_create(nprocs, 2, psize);); (109, MPI, err = MPI_File_seek(fh, 0, MPI_SEEK_SET);)"}
{"predict": "(126, OpenMP, #pragma omp distribute parallel for simd private(fl)); (72, OpenMP, #pragma omp distribute parallel for simd private(da)); (135, OpenMP, #pragma omp distribute parallel for simd private(fl)); (50, OpenMP, #pragma omp distribute parallel for simd private(a, b, c, d, f)); (61, OpenMP, #pragma omp distribute parallel for simd private(ba)); (102, OpenMP, #pragma omp distribute parallel for simd private(p)); (112, OpenMP, #pragma omp distribute parallel for simd private(r)); (121, OpenMP, #pragma omp distribute parallel for simd reduction(+:fl)); (4, OpenMP, #pragma omp target); (13, OpenMP, #pragma omp distribute parallel for simd private(); (94, OpenMP, #pragma omp distribute parallel for simd private(h)); (32, OpenMP, #pragma omp distribute parallel for simd private(argc, z)); (37, OpenMP, #pragma omp distribute parallel for simd private(argc > 0 ? argv[1] : argv[2])); (14, OpenMP, #pragma omp distribute parallel for simd private()); (52, OpenMP, #pragma omp distribute parallel for simd private(ca)); (62, OpenMP, #pragma omp distribute parallel for simd private(S2::S2s)); (94, OpenMP, #pragma omp distribute parallel for simd private(B::x)); (34, OpenMP, #pragma omp distribute parallel for simd private(S1)); (15, OpenMP, #pragma omp distribute parallel for simd private(argc); (83, OpenMP, #pragma omp distribute parallel for simd private(e, g)); (88, OpenMP, #pragma omp distribute parallel for simd private(k)); (114, OpenMP, #pragma omp distribute parallel for simd private(fl)); (18, OpenMP, #pragma omp distribute parallel for simd private(argc,); (57, OpenMP, #pragma omp distribute parallel for simd private(S2::S2sc)); (-7, OpenMP, #pragma omp distribute parallel for simd private); (96, OpenMP, #pragma omp distribute parallel for simd firstprivate(fl), private(fl)); (-7, OpenMP, #pragma omp teams)", "label": "(109, OpenMP, #pragma omp distribute parallel for simd reduction(^ : fl)); (118, OpenMP, #pragma omp distribute parallel for simd reduction(&& : S2::S2sc)); (132, OpenMP, #pragma omp distribute parallel for simd reduction(+ : z, o)); (21, OpenMP, #pragma omp distribute parallel for simd reduction(); (45, OpenMP, #pragma omp distribute parallel for simd reduction(foo : argc); (140, OpenMP, #pragma omp distribute parallel for simd reduction(+ : p), reduction(+ : p)); (6, OpenMP, #pragma omp target); (87, OpenMP, #pragma omp distribute parallel for simd reduction(+ : ba)); (131, OpenMP, #pragma omp distribute parallel for simd private(i), reduction(+ : j), reduction(+ : q)); (146, OpenMP, #pragma omp distribute parallel for simd reduction(+ : r)); (161, OpenMP, #pragma omp parallel reduction(* : fl)); (151, OpenMP, #pragma omp distribute parallel for simd reduction(max : j)); (112, OpenMP, #pragma omp distribute parallel for simd reduction(& : e, g)); (172, OpenMP, #pragma omp distribute parallel for simd reduction(task, + : m)); (166, OpenMP, #pragma omp distribute parallel for simd reduction(+ : m)); (127, OpenMP, #pragma omp parallel private(k)); (63, OpenMP, #pragma omp distribute parallel for simd reduction(+ : a, b, c, d, f)); (67, OpenMP, #pragma omp distribute parallel for simd reduction(min : a, b, c, d, f)); (71, OpenMP, #pragma omp distribute parallel for simd reduction(max : h.b)); (55, OpenMP, #pragma omp distribute parallel for simd reduction(^ : S1)); (109, OpenMP, #pragma omp distribute parallel for simd reduction(+ : h, k, B::x)); (48, OpenMP, #pragma omp distribute parallel for simd reduction(&& : argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (17, OpenMP, #pragma omp distribute parallel for simd reduction(*)); (136, OpenMP, #pragma omp parallel reduction(min : i)); (20, OpenMP, #pragma omp distribute parallel for simd reduction(\\)); (140, OpenMP, #pragma omp parallel private(fl)); (8, OpenMP, #pragma omp distribute parallel for simd reduction()); (72, OpenMP, #pragma omp distribute parallel for simd reduction(* : ca)); (140, OpenMP, #pragma omp distribute parallel for simd reduction(+ : fl)); (30, OpenMP, #pragma omp distribute parallel for simd reduction(|| : argc > 0 ? argv[1] : argv[2])); (-1, OpenMP, #pragma omp distribute parallel for simd reduction(-); (73, OpenMP, #pragma omp distribute parallel for simd reduction(- : da)); (-13, OpenMP, #pragma omp distribute parallel for simd reduction +); (-19, OpenMP, #pragma omp distribute parallel for simd reduction); (20, OpenMP, #pragma omp distribute parallel for simd reduction(| : argc,); (123, OpenMP, #pragma omp parallel shared(i)); (78, OpenMP, #pragma omp distribute parallel for simd reduction(&& : S2::S2s)); (27, OpenMP, #pragma omp distribute parallel for simd reduction(~ : argc)); (-25, OpenMP, #pragma omp teams)"}
{"predict": "(28, TBB, tbb::combinable<T> finit_combinable(my_finit_decl);); (22, TBB, if (Verbose && t == 0) t0 = tbb::tick_count::now();); (45, TBB, tbb::combinable<T> moved1(std::move(sums));); (38, TBB, tbb::combinable<T> copy_constructed(sums);); (50, TBB, tbb::combinable<T> moved2;); (46, TBB, tbb::combinable<T> moved(finit_combinable);); (-2, TBB, tbb::task_scheduler_init init(tbb::task_scheduler_init::deferred);); (38, TBB, tbb::combinable<T> assigned;); (17, TBB, tbb::combinable<T> sums;); (-4, TBB, tbb::tick_count t0;)", "label": "(41, TBB, tbb::combinable<T> copy_constructed(sums);); (31, TBB, tbb::combinable<T> finit_combinable(my_finit_decl);); (31, TBB, tbb::parallel_for( tbb::blocked_range<int>( 0, N, 10000 ), ParallelScalarBodyNoInit<T>( finit_combinable ) );); (19, TBB, if (Verbose && t == 0) t0 = tbb::tick_count::now();); (22, TBB, tbb::parallel_for( tbb::blocked_range<int>( 0, N, 10000 ), ParallelScalarBody<T>( sums ) );); (40, TBB, tbb::combinable<T> assigned;); (-3, TBB, tbb::task_scheduler_init init(tbb::task_scheduler_init::deferred);); (44, TBB, tbb::combinable<T> moved1(std::move(sums));); (2, TBB, tbb::tick_count t0;); (16, TBB, tbb::combinable<T> sums;); (45, TBB, tbb::combinable<T> moved2;)"}
{"predict": "(94, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 1, NULL, &threadSize, &groupSize, 0, NULL, NULL );); (80, OpenCL, error = clSetKernelArg( kernel, 0, sizeof( streams[ 0 ] ), &streams[0] );); (81, OpenCL, error = clSetKernelArg( kernel, 1, sizeof( streams[ 1 ] ), &streams[1] );); (116, OpenCL, clReleaseMemObject(streams[2]);); (82, OpenCL, error = clSetKernelArg( kernel, 2, sizeof( streams[ 2 ] ), &streams[2] );); (95, OpenCL, error = clEnqueueReadBuffer( queue, streams[2], true, 0, outStride * count, outData, 0, NULL, NULL );); (111, OpenCL, clReleaseMemObject(streams[1]);); (109, OpenCL, clReleaseMemObject(streams[0]);)", "label": "(119, OpenCL, clReleaseMemObject( streams[2] );); (80, OpenCL, error = clSetKernelArg(kernel, 0, sizeof( streams[0] ), &streams[0] );); (119, OpenCL, clReleaseProgram( program );); (82, OpenCL, error = clSetKernelArg(kernel, 2, sizeof( streams[2] ), &streams[2] );); (114, OpenCL, clReleaseMemObject( streams[1] );); (115, OpenCL, clReleaseKernel( kernel );); (77, OpenCL, error = clSetKernelArg(kernel, 1, sizeof( streams[1] ), &streams[1] );); (92, OpenCL, error = clEnqueueReadBuffer( queue, streams[2], CL_TRUE, 0, outStride * count, outData, 0, NULL, NULL );); (86, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 1, NULL, &threadSize, &groupSize, 0, NULL, NULL );); (108, OpenCL, clReleaseMemObject( streams[0] );)"}
{"predict": "(31, MPI, MPI_Ireduce(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, root, comm, &req);); (13, MPI, MPI_Reduce(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, root, comm);); (63, MPI, MPI_Iallreduce(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, comm, &req);); (45, MPI, MPI_Allreduce(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, comm);); (82, MPI, MPI_Reduce_scatter(buf, recvbuf, recvcounts, MPI_INT, MPI_SUM, comm);); (92, MPI, MPI_Ireduce_scatter(buf, recvbuf, recvcounts, MPI_INT, MPI_SUM, comm, &req);); (24, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (18, MPI, MPI_Request req;)", "label": "(169, MPI, MPI_Scan(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (28, MPI, MPI_Ireduce(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD, &req);); (82, MPI, MPI_Reduce_scatter(buf, recvbuf, recvcounts, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (45, MPI, MPI_Allreduce(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (124, MPI, MPI_Reduce_scatter_block(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (59, MPI, MPI_Iallreduce(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD, &req);); (100, MPI, MPI_Ireduce_scatter(buf, recvbuf, recvcounts, MPI_INT, MPI_SUM, MPI_COMM_WORLD, &req);); (23, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (3, MPI, MPI_Reduce(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD);); (187, MPI, MPI_Exscan(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (139, MPI, MPI_Ireduce_scatter_block(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD, &req);); (171, MPI, MPI_Iscan(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD, &req);); (198, MPI, MPI_Iexscan(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD, &req);)"}
{"predict": "(63, OpenMP, #pragma omp distribute simd aligned(z)); (4, OpenMP, #pragma omp distribute simd aligned(); (115, OpenMP, #pragma omp distribute simd aligned(x : 1, x : 2)); (122, OpenMP, #pragma omp distribute simd aligned(x, y, z) aligned(y, z)); (49, OpenMP, #pragma omp distribute simd aligned(x, y, z)); (6, OpenMP, #pragma omp distribute simd aligned(,); (80, OpenMP, #pragma omp distribute simd aligned(x :)); (72, OpenMP, #pragma omp distribute simd aligned(x :, )); (14, OpenMP, #pragma omp distribute simd aligned()); (33, OpenMP, #pragma omp distribute simd aligned(x)); (90, OpenMP, #pragma omp distribute simd aligned(x : 1)); (22, OpenMP, #pragma omp distribute simd aligned(0)); (0, OpenMP, #pragma omp distribute simd aligned(, )); (52, OpenMP, #pragma omp distribute simd aligned(x : 1)); (82, OpenMP, #pragma omp distribute simd aligned(x : 1, y, z : 1)); (-11, OpenMP, #pragma omp target); (92, OpenMP, #pragma omp distribute simd aligned(x : 1, y)); (-12, OpenMP, #pragma omp teams)", "label": "(19, OpenMP, #pragma omp distribute simd aligned(, )); (75, OpenMP, #pragma omp distribute simd aligned(x :)); (80, OpenMP, #pragma omp distribute simd aligned(x :, )); (9, OpenMP, #pragma omp distribute simd aligned(,); (-2, OpenMP, #pragma omp target); (82, OpenMP, #pragma omp distribute simd aligned(x : 1)); (52, OpenMP, #pragma omp distribute simd aligned(x, y, z)); (24, OpenMP, #pragma omp distribute simd aligned(int)); (62, OpenMP, #pragma omp distribute simd aligned(z)); (-4, OpenMP, #pragma omp distribute simd aligned(); (40, OpenMP, #pragma omp distribute simd aligned(x, y)); (32, OpenMP, #pragma omp distribute simd aligned(x)); (80, OpenMP, #pragma omp distribute simd aligned(x : 2 * 2)); (12, OpenMP, #pragma omp distribute simd aligned()); (23, OpenMP, #pragma omp distribute simd aligned(0)); (83, OpenMP, #pragma omp distribute simd aligned(x : 1, y)); (88, OpenMP, #pragma omp distribute simd aligned(x : 1, y, z : 1)); (108, OpenMP, #pragma omp distribute simd aligned(x) aligned(z, x)); (116, OpenMP, #pragma omp distribute simd aligned(x, y, z) aligned(y, z)); (-16, OpenMP, #pragma omp teams)"}
{"predict": "(59, MPI, MPI_Comm_rank(intracomm, &icrank);); (146, MPI, MPI_Comm_free(&intercomm);); (71, MPI, MPI_Comm_size(MPI_COMM_WORLD, &psize);); (152, MPI, MPI_Comm_free(&intracomm3);); (10, MPI, MPI_Comm_get_parent(&parentcomm);); (35, MPI, MPI_Send(&i, 1, MPI_INT, i, 0, intercomm);); (88, MPI, MPI_Comm_split(intracomm, (isChild ? 0 : 1), wrank, &intracomm3);); (55, MPI, MPI_Comm_size(intracomm, &icsize);); (23, MPI, MPI_Comm_rank(intercomm, &rank);); (21, MPI, MPI_Comm_remote_size(intercomm, &rsize);); (119, MPI, MPI_Comm_split(intracomm, (isChild ? 1 : 0), wrank, &intracomm2);); (81, MPI, MPI_Comm_rank(intracomm3, &icrank);); (141, MPI, MPI_Comm_free(&intracomm2);); (49, MPI, MPI_Barrier(intercomm);); (24, MPI, MPI_Recv(&i, 1, MPI_INT, 0, 0, intercomm, &status);); (53, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (134, MPI, MPI_Comm_free(&intracomm);); (11, MPI, MPI_Comm_size(intercomm, &size);); (117, MPI, MPI_Comm_rank(intracomm2, &icrank);); (114, MPI, MPI_Comm_size(intracomm2, &icsize);); (96, MPI, MPI_Comm_size(intracomm3, &csize);); (41, MPI, MPI_Comm_incl(intercomm, rsize, rank, &intracomm);); (133, MPI, MPI_Comm_free(&parentcomm);); (28, MPI, MPI_Comm_incl(intercomm, size, rank, &intracomm);); (43, MPI, MPI_Comm_rank(intracomm, &icrank);)", "label": "(65, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (56, MPI, MPI_Intercomm_merge(intercomm, isChild, &intracomm);); (161, MPI, MPI_Comm_free(&intracomm3);); (95, MPI, MPI_Comm_rank(intracomm2, &icrank);); (12, MPI, MPI_Comm_get_parent(&parentcomm);); (34, MPI, MPI_Send(&i, 1, MPI_INT, i, 0, intercomm);); (141, MPI, MPI_Ssend(&errs, 1, MPI_INT, 0, 1, intercomm);); (56, MPI, MPI_Comm_size(intracomm, &icsize);); (146, MPI, MPI_Recv(&err, 1, MPI_INT, i, 1, intercomm, MPI_STATUS_IGNORE);); (117, MPI, MPI_Intercomm_merge(intercomm, 0, &intracomm3);); (65, MPI, MPI_Comm_remote_size(parentcomm, &psize);); (80, MPI, MPI_Intercomm_merge(intercomm, !isChild, &intracomm2);); (149, MPI, MPI_Comm_free(&intracomm);); (102, MPI, MPI_Comm_remote_size(intercomm, &csize);); (83, MPI, MPI_Comm_size(intracomm2, &icsize);); (49, MPI, MPI_Comm_rank(intracomm, &icrank);); (150, MPI, MPI_Comm_free(&intercomm);); (32, MPI, MPI_Recv(&i, 1, MPI_INT, 0, 0, intercomm, &status);); (114, MPI, MPI_Comm_rank(intracomm3, &icrank);); (10, MPI, MPI_Comm_rank(intercomm, &rank);); (7, MPI, MPI_Comm_remote_size(intercomm, &rsize);); (7, MPI, MPI_Comm_size(intercomm, &size);); (109, MPI, MPI_Comm_size(intracomm3, &icsize);); (139, MPI, MPI_Comm_free(&intracomm2);); (150, MPI, MPI_Finalize();)"}
{"predict": "(109, CUDA, cudaStat1 = cudaMemcpy(d_csrRowPtrA, csrRowPtrA, sizeof(int)*(m + 1), cudaMemcpyHostToDevice);); (244, CUDA, if (d_csrColIndC) cudaFree(d_csrColIndC);); (219, CUDA, cudaStat1 = cudaMemcpy(csrRowPtrC, d_csrRowPtrC, sizeof(int)*(m + 1), cudaMemcpyDeviceToHost);); (102, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrRowPtrC, sizeof(int)*(m + 1));); (97, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrValA, sizeof(float)*nnzA);); (112, CUDA, cudaStat1 = cudaMemcpy(d_csrValA, csrValA, sizeof(float)*nnzA, cudaMemcpyHostToDevice);); (231, CUDA, if (d_csrRowPtrA) cudaFree(d_csrRowPtrA);); (248, CUDA, if (stream) cudaStreamDestroy(stream);); (235, CUDA, if (d_csrRowPtrC) cudaFree(d_csrRowPtrC);); (177, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrValC, sizeof(float) * nnzC);); (237, CUDA, if (d_csrValC) cudaFree(d_csrValC);); (159, CUDA, cudaStat1 = cudaDeviceSynchronize();); (81, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrRowPtrA, sizeof(int)*(m + 1));); (212, CUDA, cudaStat1 = cudaMemcpy(csrColIndC, d_csrColIndC, sizeof(int)*nnzC, cudaMemcpyDeviceToHost);); (83, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrColIndA, sizeof(int)*nnzA);); (40, CUDA, cudaStat1 = cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking);); (97, CUDA, cudaStat1 = cudaMemcpy(d_csrColIndA, csrColIndA, sizeof(int)*nnzA, cudaMemcpyHostToDevice);); (129, CUDA, cudaStat1 = cudaMalloc((void**)&", "label": "(106, CUDA, cudaStat1 = cudaMemcpy(d_csrRowPtrA, csrRowPtrA, sizeof(int)*(m + 1), cudaMemcpyHostToDevice);); (234, CUDA, if (d_csrColIndC) cudaFree(d_csrColIndC);); (211, CUDA, cudaStat1 = cudaMemcpy(csrRowPtrC, d_csrRowPtrC, sizeof(int)*(m + 1), cudaMemcpyDeviceToHost);); (99, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrRowPtrC, sizeof(int)*(m + 1));); (94, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrValA, sizeof(float)*nnzA);); (109, CUDA, cudaStat1 = cudaMemcpy(d_csrValA, csrValA, sizeof(float)*nnzA, cudaMemcpyHostToDevice);); (221, CUDA, if (d_csrRowPtrA) cudaFree(d_csrRowPtrA);); (237, CUDA, if (stream) cudaStreamDestroy(stream);); (225, CUDA, if (d_csrRowPtrC) cudaFree(d_csrRowPtrC);); (170, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrValC, sizeof(float) * nnzC);); (227, CUDA, if (d_csrValC) cudaFree(d_csrValC);); (154, CUDA, cudaStat1 = cudaDeviceSynchronize();); (78, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrRowPtrA, sizeof(int)*(m + 1));); (204, CUDA, cudaStat1 = cudaMemcpy(csrColIndC, d_csrColIndC, sizeof(int)*nnzC, cudaMemcpyDeviceToHost);); (80, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrColIndA, sizeof(int)*nnzA);); (41, CUDA, cudaStat1 = cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking);); (94, CUDA, cudaStat1 = cudaMemcpy(d_csrColIndA, csrColIndA, sizeof(int)*nnzA, cudaMemcpyHostToDevice);); (125, CUDA, cudaStat1 = cudaMalloc((void**)&d_work, lworkInBytes);); (211, CUDA, if (d_csrColIndA) cudaFree(d_csrColIndA);); (202, CUDA, cudaStat1 = cudaMemcpy(csrValC, d_csrValC, sizeof(float)*nnzC, cudaMemcpyDeviceToHost);); (155, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrColIndC, sizeof(int) * nnzC);); (210, CUDA, if (d_csrValA) cudaFree(d_csrValA);); (228, CUDA, cudaDeviceReset();)"}
{"predict": "(61, MPI, MPI_Send(sbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD);); (106, MPI, MPI_Barrier(MPI_COMM_WORLD);); (155, MPI, MPI_Wait(&rreq, MPI_STATUS_IGNORE);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (54, MPI, t1 = MPI_Wtime();); (168, MPI, MPI_Sendrecv(MPI_BOTTOM, 0, MPI_BYTE, partner, 0, MPI_BOTTOM, 0, MPI_BYTE, partner, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (33, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (75, MPI, MPI_Recv(rbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (59, MPI, t1 = MPI_Wtime() - t1;); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (46, MPI, MPI_Barrier(MPI_COMM_WORLD);); (52, MPI, MPI_Wait(&rreq, MPI_STATUS_IGNORE);); (39, MPI, MPI_Irecv(rbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD, &rreq);); (153, MPI, MPI_Send(sbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD);); (167, MPI, MPI_Recv(rbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (71, MPI, MPI_Sendrecv(sbuf, len, MPI_BYTE, partner, k, rbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (133, MPI, MPI_Wait(&rreq, MPI_STATUS_IGNORE);); (173, MPI, MPI_Reduce(times, &avg_times, 3 * MAXTESTS, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);); (173, MPI, MPI_Reduce(times, &max_times, 3 * MAXTESTS, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);); (62, MPI, MPI_Sendrecv(MPI_BOTTOM, 0, MPI_BYTE, partner, 0, MPI_BOTTOM, 0, MPI_BYTE, partner, 0,", "label": "(13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (60, MPI, MPI_Irecv(rbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD, &rreq);); (57, MPI, t1 = MPI_Wtime();); (60, MPI, MPI_Wait(&rreq, MPI_STATUS_IGNORE);); (146, MPI, MPI_Recv(rbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (57, MPI, MPI_Send(sbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (34, MPI, MPI_Barrier(MPI_COMM_WORLD);); (57, MPI, t1 = MPI_Wtime() - t1;); (8, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (24, MPI, MPI_Abort(MPI_COMM_WORLD, 2);)"}
{"predict": "(20, TBB, tbb::flow::make_edge( n, r[i] );); (54, TBB, tbb::flow::remove_edge( n, r[i] );); (1, TBB, tbb::flow::write_once_node<R> n(g);); (-1, TBB, tbb::flow::graph g;)", "label": "(20, TBB, tbb::flow::make_edge( n, r[i] );); (54, TBB, tbb::flow::remove_edge( n, r[i] );); (1, TBB, tbb::flow::write_once_node<R> n(g);); (-1, TBB, tbb::flow::graph g;)"}
{"predict": "(46, OpenMP, #pragma omp distribute parallel for schedule (guided, foobool(argc)), schedule (static, true), schedule (dynamic, -5)); (54, OpenMP, #pragma omp distribute parallel for schedule (static, argv[1]=2)); (30, OpenMP, #pragma omp distribute parallel for schedule (guided, 4); (47, OpenMP, #pragma omp distribute parallel for schedule (guided, S1)); (9, OpenMP, #pragma omp distribute parallel for schedule ()); (33, OpenMP, #pragma omp distribute parallel for schedule (static, 2+2))); (17, OpenMP, #pragma omp distribute parallel for schedule (guided,); (35, OpenMP, #pragma omp distribute parallel for schedule (dynamic, foobool(1) > 0 ? 1 : 2 -z)); (13, OpenMP, #pragma omp distribute parallel for schedule (static,); (-4, OpenMP, #pragma omp distribute parallel for schedule); (-5, OpenMP, #pragma omp target); (5, OpenMP, #pragma omp distribute parallel for schedule (); (-6, OpenMP, #pragma omp teams)", "label": "(16, OpenMP, #pragma omp distribute parallel for schedule (auto); (45, OpenMP, #pragma omp distribute parallel for schedule (guided, foobool(argc)), schedule (static, true), schedule (dynamic, -5)); (48, OpenMP, #pragma omp distribute parallel for schedule (guided, S1)); (52, OpenMP, #pragma omp distribute parallel for schedule (static, argv[1]=2)); (0, OpenMP, #pragma omp distribute parallel for schedule); (31, OpenMP, #pragma omp distribute parallel for schedule (static, 2+2))); (2, OpenMP, #pragma omp distribute parallel for schedule (); (5, OpenMP, #pragma omp distribute parallel for schedule ()); (32, OpenMP, #pragma omp distribute parallel for schedule (dynamic, foobool(1) > 0 ? 1 : 2 - z)); (23, OpenMP, #pragma omp distribute parallel for schedule (guided, 4); (51, OpenMP, #pragma omp distribute parallel for schedule(dynamic, schedule(tmain<int, char, -1, -2>(argc, argv)); (17, OpenMP, #pragma omp distribute parallel for schedule (runtime, 3)); (-10, OpenMP, #pragma omp target); (11, OpenMP, #pragma omp distribute parallel for schedule (auto,); (6, OpenMP, #pragma omp distribute parallel for schedule (auto_dynamic); (-12, OpenMP, #pragma omp teams)"}
{"predict": "(98, CUDA, cudaFree(deviceDiagonalResult);); (20, CUDA, cudaMalloc((void**)&deviceMatrix, width * height * sizeof(float));); (34, CUDA, cudaMemcpy(deviceMatrix, hostMatrix, width * height * sizeof(float), cudaMemcpyHostToDevice);); (94, CUDA, cudaFree(deviceToeplitzResult);); (18, CUDA, cudaMalloc((void**)&deviceToeplitzResult, sizeof(bool));); (93, CUDA, cudaFree(deviceSymmetricResult);); (45, CUDA, cudaMemcpy(hostDiagonalResult, deviceDiagonalResult, sizeof(bool), cudaMemcpyDeviceToHost);); (89, CUDA, cudaFree(deviceSkewSymmetricResult);); (31, CUDA, cudaMemcpy(deviceSymmetricResult, hostSymmetricResult, sizeof(bool), cudaMemcpyHostToDevice);); (32, CUDA, cudaMemcpy(deviceSkewSymmetricResult, hostSkewSymmetricResult, sizeof(bool), cudaMemcpyHostToDevice);); (84, CUDA, cudaFree(deviceIdempotentResult);); (41, CUDA, cudaMemcpy(hostSymmetricResult, deviceSymmetricResult, sizeof(bool), cudaMemcpyDeviceToHost);); (42, CUDA, cudaMemcpy(hostSkewSymmetricResult, deviceSkewSymmetricResult, sizeof(bool), cudaMemcpyDeviceToHost);); (11, CUDA, cudaMalloc((void**)&deviceDiagonalResult, sizeof(bool));); (12, CUDA, cudaMalloc((void**)&deviceSymmetricResult, sizeof(bool));); (23, CUDA, cudaMemcpy(deviceDiagonalResult, hostDiagonalResult, sizeof(bool), cudaMemcpyHostToDevice);); (82, CUDA, cudaFree(deviceMatrix);); (38, CUDA, cudaMemcpy(hostToeplitzResult, deviceToeplitzResult, sizeof(bool), cudaMemcpyDeviceToHost);); (79, CUDA, cudaFree(deviceMatrix);); (81, CUDA, cudaFree(deviceDiagonalResult);); (57, CUDA, cudaMemcpy(hostIdempotentResult, deviceIdempotentResult, sizeof(bool), cudaMemcpyDeviceToHost);); (13, CUDA, cudaMalloc((void**)&deviceSkewSymmetricResult, sizeof(bool));); (14, CUDA, cudaMalloc((void**)&deviceIdempotentResult, sizeof(bool));); (27, CUDA, cudaMemcpy(deviceIdempotentResult, hostIdempotentResult, sizeof(bool), cudaMemcpyHostToDevice);)", "label": "(21, CUDA, cudaMalloc((void**)&deviceDiagonalResult, sizeof(bool));); (55, CUDA, cudaMemcpy(hostDiagonalResult, deviceDiagonalResult, sizeof(bool), cudaMemcpyDeviceToHost);); (99, CUDA, cudaFree(deviceSymmetricResult);); (20, CUDA, cudaMalloc((void**)&deviceSkewSymmetricResult, sizeof(bool));); (54, CUDA, cudaMemcpy(hostSkewSymmetricResult, deviceSkewSymmetricResult, sizeof(bool), cudaMemcpyDeviceToHost);); (17, CUDA, cudaMalloc((void**)&deviceSymmetricResult, sizeof(bool));); (33, CUDA, cudaMemcpy(deviceMatrix, hostMatrix, width * height * sizeof(float), cudaMemcpyHostToDevice);); (37, CUDA, cudaMemcpy(deviceIdempotentResult, hostIdempotentResult, sizeof(bool), cudaMemcpyHostToDevice);); (11, CUDA, cudaMalloc((void**)&deviceMatrix, width * height * sizeof(float));); (31, CUDA, cudaMemcpy(deviceToeplitzResult, hostToeplitzResult, sizeof(bool), cudaMemcpyHostToDevice);); (49, CUDA, cudaMemcpy(hostIdempotentResult, deviceIdempotentResult, sizeof(bool), cudaMemcpyDeviceToHost);); (91, CUDA, cudaFree(deviceSkewSymmetricResult);); (31, CUDA, cudaMemcpy(deviceSkewSymmetricResult, hostSkewSymmetricResult, sizeof(bool), cudaMemcpyHostToDevice);); (44, CUDA, cudaMemcpy(hostSymmetricResult, deviceSymmetricResult, sizeof(bool), cudaMemcpyDeviceToHost);); (28, CUDA, cudaMemcpy(deviceSymmetricResult, hostSymmetricResult, sizeof(bool), cudaMemcpyHostToDevice);); (85, CUDA, cudaFree(deviceDiagonalResult);); (4, CUDA, cudaMalloc((void**)&deviceToeplitzResult, sizeof(bool));); (82, CUDA, cudaFree(deviceToeplitzResult);); (85, CUDA, cudaFree(deviceIdempotentResult);); (36, CUDA, cudaMemcpy(hostToeplitzResult, deviceToeplitzResult, sizeof(bool), cudaMemcpyDeviceToHost);); (78, CUDA, cudaFree(deviceMatrix);); (20, CUDA, cudaMemcpy(deviceDiagonalResult, hostDiagonalResult, sizeof(bool), cudaMemcpyHostToDevice);); (2, CUDA, cudaMalloc((void**)&deviceIdempotentResult, sizeof(bool));)"}
{"predict": "(60, OpenCL, err = clEnqueueWriteBuffer( queue, streams[ii], CL_TRUE, 0, ptrSizes[i]*num_elements, inptr[i], 0, NULL, &writeEvent );); (147, OpenCL, err = clSetKernelArg( kernel[i], 1, sizeof( streams[ii+1] ), &streams[ii+1] );); (64, OpenCL, clReleaseEvent(writeEvent);); (60, OpenCL, clReleaseMemObject( streams[ii] );); (26, OpenCL, clReleaseMemObject( streams[ii+1] );); (60, OpenCL, err = clWaitForEvents( 1, &writeEvent );); (116, OpenCL, err = clSetKernelArg( kernel[i], 0, sizeof( streams[ii] ), &streams[ii] );); (50, OpenCL, clReleaseMemObject( streams[ii+1] );); (142, OpenCL, err = clSetKernelArg( kernel[i], 2, sizeof( threads ), &threads );); (155, OpenCL, err = clEnqueueNDRangeKernel( queue, kernel[i], 1, NULL, threads, NULL, 0, NULL, NULL );); (164, OpenCL, clReleaseProgram( program[i] );); (162, OpenCL, clReleaseKernel( kernel[i] );); (171, OpenCL, clReleaseMemObject(outptr[i]);); (50, OpenCL, print_error( err, \" clEnqueueWriteBuffer failed\" );)", "label": "(57, OpenCL, err = clEnqueueWriteBuffer( queue, streams[ii], false, 0, ptrSizes[i]*num_elements, inptr[i], 0, NULL, &writeEvent );); (132, OpenCL, err = clSetKernelArg( kernel[i], 0, sizeof( cl_mem ), (void *)&streams[ii] );); (49, OpenCL, clReleaseMemObject(streams[ii]);); (135, OpenCL, clReleaseProgram( program[i] );); (123, OpenCL, clReleaseMemObject(streams[ii+1]);); (55, OpenCL, clReleaseMemObject( streams[ii+1] );); (140, OpenCL, err = clEnqueueNDRangeKernel( queue, kernel[i], 1, NULL, threads, NULL, 0, NULL, NULL );); (52, OpenCL, clReleaseMemObject( streams[ii] );); (64, OpenCL, clReleaseEvent(writeEvent);); (59, OpenCL, err = clWaitForEvents( 1, &writeEvent );); (127, OpenCL, clReleaseKernel( kernel[i] );); (123, OpenCL, err |= clSetKernelArg( kernel[i], 1, sizeof( cl_mem ), (void *)&streams[ii+1] );); (148, OpenCL, err = clEnqueueReadBuffer( queue, streams[ii+1], true, 0, outPtrSizes[i]*num_elements, outptr[i], 0, NULL, NULL );)"}
{"predict": "(15, OpenMP, #pragma omp target teams distribute parallel for simd linear (); (28, OpenMP, #pragma omp target teams distribute parallel for simd linear (argc,); (46, OpenMP, #pragma omp target teams distribute parallel for simd linear(i)); (31, OpenMP, #pragma omp target teams distribute parallel for simd linear (argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (51, OpenMP, #pragma omp target teams distribute parallel for simd linear(j)); (34, OpenMP, #pragma omp target teams distribute parallel for simd linear (S1)); (14, OpenMP, #pragma omp target teams distribute parallel for simd linear ()); (42, OpenMP, #pragma omp target teams distribute parallel for simd linear(i : 4)); (35, OpenMP, #pragma omp target teams distribute parallel for simd linear (argv[1])); (24, OpenMP, #pragma omp target teams distribute parallel for simd linear (argc > 0 ? argv[1] : argv[2])); (3, OpenMP, #pragma omp target teams distribute parallel for simd linear); (41, OpenMP, #pragma omp target teams distribute parallel for simd linear(i) ordered(1)); (12, OpenMP, #pragma omp target teams distribute parallel for simd linear (argc)", "label": "(15, OpenMP, #pragma omp target teams distribute parallel for simd linear (); (11, OpenMP, #pragma omp target teams distribute parallel for simd linear); (34, OpenMP, #pragma omp target teams distribute parallel for simd linear (S1)); (37, OpenMP, #pragma omp target teams distribute parallel for simd linear (a, b)); (39, OpenMP, #pragma omp target teams distribute parallel for simd linear (argv[1])); (44, OpenMP, #pragma omp target teams distribute parallel for simd linear(h, C::x)); (23, OpenMP, #pragma omp target teams distribute parallel for simd linear (argc > 0 ? argv[1] : argv[2])); (15, OpenMP, #pragma omp target teams distribute parallel for simd linear (argc); (10, OpenMP, #pragma omp target teams distribute parallel for simd linear ()); (24, OpenMP, #pragma omp target teams distribute parallel for simd linear (argc)); (36, OpenMP, #pragma omp target teams distribute parallel for simd linear(e, g)); (15, OpenMP, #pragma omp target teams distribute parallel for simd linear (argc,)"}
{"predict": "(56, OpenCL, err = clEnqueueWriteBuffer(queue, bufY, CL_TRUE, 0, (lenY*sizeof(cl_float)), Y, 0, NULL, NULL);); (82, OpenCL, clReleaseMemObject(bufX);); (44, OpenCL, clReleaseCommandQueue(queue);); (28, OpenCL, printf( \"clCreateContext() failed with %d\\n\", err );); (75, OpenCL, clReleaseEvent(event);); (32, OpenCL, printf( \"clCreateCommandQueue() failed with %d\\n\", err );); (29, OpenCL, queue = clCreateCommandQueue(ctx, device, 0, &err);); (46, OpenCL, bufY = clCreateBuffer(ctx, CL_MEM_READ_WRITE, (lenY*sizeof(cl_float)), NULL, &err);); (30, OpenCL, clReleaseContext(ctx);); (73, OpenCL, clReleaseMemObject(bufY);); (42, OpenCL, bufX = clCreateBuffer(ctx, CL_MEM_READ_WRITE, (lenX*sizeof(cl_float)), NULL, &err);); (44, OpenCL, err = clEnqueueWriteBuffer(queue, bufX, CL_TRUE, 0, (lenX*sizeof(cl_float)), X, 0, NULL, NULL);); (17, OpenCL, ctx = clCreateContext(props, 1, &device, NULL, NULL, &err);); (53, OpenCL, err = clWaitForEvents(1, &event);)", "label": "(56, OpenCL, err = clEnqueueWriteBuffer(queue, bufY, CL_TRUE, 0, (lenY*sizeof(cl_float)), Y, 0, NULL, NULL);); (82, OpenCL, clReleaseMemObject(bufX);); (44, OpenCL, clReleaseCommandQueue(queue);); (28, OpenCL, printf( \"clCreateContext() failed with %d\\n\", err );); (75, OpenCL, clReleaseEvent(event);); (32, OpenCL, printf( \"clCreateCommandQueue() failed with %d\\n\", err );); (29, OpenCL, queue = clCreateCommandQueue(ctx, device, 0, &err);); (46, OpenCL, bufY = clCreateBuffer(ctx, CL_MEM_READ_WRITE, (lenY*sizeof(cl_float)), NULL, &err);); (30, OpenCL, clReleaseContext(ctx);); (73, OpenCL, clReleaseMemObject(bufY);); (42, OpenCL, bufX = clCreateBuffer(ctx, CL_MEM_READ_ONLY, (lenX*sizeof(cl_float)), NULL, &err);); (44, OpenCL, err = clEnqueueWriteBuffer(queue, bufX, CL_TRUE, 0, (lenX*sizeof(cl_float)), X, 0, NULL, NULL);); (17, OpenCL, ctx = clCreateContext(props, 1, &device, NULL, NULL, &err);); (53, OpenCL, err = clWaitForEvents(1, &event);)"}
{"predict": "(73, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (39, CUDA, EXPECT_EQ(cudaSuccess, cudaMemcpy(inData->basePtr(), inVec.data(), inBufSize, cudaMemcpyHostToDevice));); (52, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (71, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamDestroy(stream));)", "label": "(39, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(inData->basePtr(), inVec.data(), inBufSize, cudaMemcpyHostToDevice));); (71, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (71, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamDestroy(stream));); (51, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));)"}
{"predict": "(48, OpenMP, std::cout << \"Number of threads    = \" << omp_get_max_threads() << std::endl;)", "label": "(48, OpenMP, std::cout << \"Number of threads    = \" << omp_get_max_threads() << std::endl;)"}
{"predict": "(33, MPI, merr = MPI_Unpublish_name(serv_name, MPI_INFO_NULL, port_name);); (24, MPI, merr = MPI_Publish_name(serv_name, MPI_INFO_NULL, port_name);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (24, MPI, MPI_Error_string(merr, errmsg, &msglen);); (66, MPI, MPI_Error_class(merr, &mclass);); (39, MPI, merr = MPI_Lookup_name(serv_name, MPI_INFO_NULL, port_name_out);); (17, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(35, MPI, merr = MPI_Unpublish_name(serv_name, MPI_INFO_NULL, port_name);); (24, MPI, merr = MPI_Publish_name(serv_name, MPI_INFO_NULL, port_name);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (25, MPI, MPI_Error_string(merr, errmsg, &msglen);); (41, MPI, merr = MPI_Lookup_name(serv_name, MPI_INFO_NULL, port_name_out);); (64, MPI, MPI_Error_class(merr, &mclass);); (16, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (25, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(55, TBB, tbb::concurrent_bounded_queue<int> iq4;); (22, TBB, tbb::concurrent_bounded_queue<int> iq2;); (35, TBB, tbb::concurrent_bounded_queue<int> iq3;); (1, TBB, tbb::concurrent_bounded_queue<int> iq1;)", "label": "(55, TBB, tbb::concurrent_bounded_queue<int> iq4;); (22, TBB, tbb::concurrent_bounded_queue<int> iq2;); (35, TBB, tbb::concurrent_bounded_queue<int> iq3;); (3, TBB, tbb::concurrent_bounded_queue<int> iq1;)"}
{"predict": "(105, CUDA, cudaError_t cudaErr = cudaStreamCreate(&streamArray[i]);); (240, CUDA, cudaFree(devPtrC[i]);); (237, CUDA, cudaFree(devPtrB[i]);); (115, CUDA, cudaErr = cudaMemcpy(devPtrB[i], B, matrixSizeB * sizeof(B[0]), cudaMemcpyHostToDevice);); (234, CUDA, cudaFree(devPtrA[i]);); (112, CUDA, cudaErr = cudaMemcpy(devPtrA[i], A, matrixSizeA * sizeof(A[0]), cudaMemcpyHostToDevice);); (191, CUDA, cudaErr = cudaDeviceSynchronize();); (113, CUDA, cudaErr = cudaMemcpy(devPtrC[i], C, matrixSizeC * sizeof(C[0]), cudaMemcpyHostToDevice);); (222, CUDA, cudaFree(devPtrC_dev);); (219, CUDA, cudaFree(devPtrA_dev);); (216, CUDA, cudaFree(devPtrB_dev);); (175, CUDA, cudaErr = cudaMemcpy(C, devPtrC[i], matrixSizeC * sizeof(C[0]), cudaMemcpyDeviceToHost);); (105, CUDA, cudaErr = cudaMemcpy(devPtrA_dev, devPtrA, opts.N * sizeof(*devPtrA), cudaMemcpyHostToDevice);); (105, CUDA, cudaErr = cudaMemcpy(devPtrB_dev, devPtrB, opts.N * sizeof(*devPtrB), cudaMemcpyHostToDevice);); (173, CUDA, cudaErr = cudaStreamDestroy(streamArray[i]);); (104, CUDA, cudaErr = cudaMemcpy(devPtrC_dev, devPtrC, opts.N * sizeof(*devPtrC), cudaMemcpyHostToDevice);); (95, CUDA, cudaErr = cudaStreamCreate(&streamArray[i]);); (198, CUDA, cudaDeviceReset();)", "label": "(193, CUDA, cudaError_t cudaStatus = cudaDeviceSynchronize();); (103, CUDA, cudaError_t cudaErr = cudaStreamCreate(&streamArray[i]);); (163, CUDA, cudaError_t cudaStatus = cudaGetLastError();)"}
{"predict": "(164, MPI, MPI_Waitall(nprocs_send, send_req, statuses);); (29, MPI, MPI_Alltoall(count, 1, MPI_COUNT, recv_size, 1, MPI_COUNT, fd->comm);)", "label": "(65, MPI, MPI_Type_commit(recv_types + j);); (30, MPI, MPI_Alltoall(recv_size, 1, MPI_COUNT, send_size, 1, MPI_COUNT, fd->comm);)"}
{"predict": "(40, SYCL, sycl::accessor A(d_A, h, sycl::no_init);); (82, SYCL, prk::SYCL::print_exception_details(e);); (39, SYCL, sycl::accessor B(d_B, h, sycl::read_only);); (39, SYCL, sycl::accessor C(d_C, h, sycl::read_only);); (86, SYCL, sycl::accessor A(d_A, h, sycl::read_only);); (36, SYCL, sycl::accessor A(d_A, h);); (86, SYCL, sycl::accessor B(d_B, h, sycl::no_init);); (86, SYCL, sycl::accessor C(d_C, h, sycl::no_init);)", "label": "(39, SYCL, sycl::accessor B(d_B, h, sycl::read_only);); (37, SYCL, sycl::accessor A(d_A, h);); (38, SYCL, sycl::accessor C(d_C, h, sycl::read_only);); (81, SYCL, prk::SYCL::print_exception_details(e);)"}
{"predict": "(3, MPI, MPI_Init(nullptr, nullptr);); (100, MPI, MPI_Finalize();); (3, MPI, dray::dray::mpi_comm(MPI_Comm_c2f(comm));)", "label": "(3, MPI, MPI_Init(nullptr, nullptr);); (100, MPI, MPI_Finalize();); (3, MPI, dray::dray::mpi_comm(MPI_Comm_c2f(comm));)"}
{"predict": "(66, OpenMP, omp_target_free(min, omp_get_default_device() );); (67, OpenMP, omp_target_free(dbl_min, omp_get_default_device() );); (64, OpenMP, omp_target_free(max, omp_get_default_device() );); (28, OpenMP, #pragma omp atomic compare); (65, OpenMP, omp_target_free(dbl_max, omp_get_default_device() );); (33, OpenMP, #pragma omp atomic compare hint(ompx_fast_fp_atomics)); (25, OpenMP, #pragma omp target teams distribute parallel for  map(tofrom:min[0:1],max[0:1])); (16, OpenMP, #pragma omp target teams distribute parallel for  map(tofrom:min[0:1],max[0:1],dbl_min[0:1],dbl_max[0:1]))", "label": "(24, OpenMP, double *dbl_min = (double*) omp_target_alloc(sizeof(double)*1, omp_get_default_device() );); (22, OpenMP, int *max = (int*) omp_target_alloc(sizeof(int)*1, omp_get_default_device() );); (23, OpenMP, double *dbl_max = (double*) omp_target_alloc(sizeof(double)*1, omp_get_default_device() );); (33, OpenMP, #pragma omp atomic compare); (6, OpenMP, #pragma omp target teams distribute parallel for); (38, OpenMP, #pragma omp atomic compare hint(ompx_fast_fp_atomics)); (50, OpenMP, #pragma omp target teams distribute parallel for  map(tofrom:min2,max2))"}
{"predict": "(246, CUDA, result.error = cudaEventRecord(events[0]);); (267, CUDA, std::cerr << \"cudaEventSynchronize() failed: \" << cudaGetErrorString(result.error) << std::endl;); (274, CUDA, std::cerr << \"cudaEventElapsed() failed: \" << cudaGetErrorString(result.error) << std::endl;); (245, CUDA, std::cerr << \"cudaEventRecord() failed: \" << cudaGetErrorString(result.error) << std::endl;); (255, CUDA, result.error = cudaEventRecord(events[1]);); (281, CUDA, (void)cudaEventDestroy(event);); (234, CUDA, std::cerr << \"cudaEventCreate() failed: \" << cudaGetErrorString(result.error) << std::endl;); (267, CUDA, result.error = cudaEventElapsedTime(&runtime_ms, events[0], events[1]);); (258, CUDA, result.error = cudaEventSynchronize(events[1]);); (229, CUDA, result.error = cudaEventCreate(&event);)", "label": "(202, CUDA, result.error = cudaEventRecord(events[0]);); (223, CUDA, std::cerr << \"cudaEventSynchronize() failed: \" << cudaGetErrorString(result.error) << std::endl;); (230, CUDA, std::cerr << \"cudaEventElapsed() failed: \" << cudaGetErrorString(result.error) << std::endl;); (201, CUDA, std::cerr << \"cudaEventRecord() failed: \" << cudaGetErrorString(result.error) << std::endl;); (211, CUDA, result.error = cudaEventRecord(events[1]);); (237, CUDA, (void)cudaEventDestroy(event);); (190, CUDA, std::cerr << \"cudaEventCreate() failed: \" << cudaGetErrorString(result.error) << std::endl;); (223, CUDA, result.error = cudaEventElapsedTime(&runtime_ms, events[0], events[1]);); (214, CUDA, result.error = cudaEventSynchronize(events[1]);); (185, CUDA, result.error = cudaEventCreate(&event);)"}
{"predict": "(57, MPI, MPI_Type_get_extent(datatype, &lb, &buftype_extent);); (43, MPI, MPI_Type_size_x(fd->filetype, &filetype_size);); (56, MPI, MPI_Type_size_x(datatype, &buftype_size);); (77, MPI, MPI_Comm_size(fd->comm, &nprocs);); (93, MPI, MPI_Comm_rank(fd->comm, &rank);)", "label": "(44, MPI, MPI_Type_size_x(fd->filetype, &filetype_size);); (52, MPI, MPI_Comm_rank(fd->comm, &rank);); (52, MPI, MPI_Comm_size(fd->comm, &nprocs);); (56, MPI, MPI_Type_get_extent(datatype, &lb, &buftype_extent);); (54, MPI, MPI_Type_size_x(datatype, &buftype_size);)"}
{"predict": "(144, MPI, MPI_Bcast(&size2,      1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (144, MPI, MPI_Bcast(&radius,     1, MPI_INT,           root, MPI_COMM_WORLD);); (304, MPI, MPI_Finalize();); (139, MPI, MPI_Bcast(&lsize2,     1, MPI_INT,           root, MPI_COMM_WORLD);); (137, MPI, MPI_Bcast(&lsize,      1, MPI_INT,           root, MPI_COMM_WORLD);); (138, MPI, MPI_Bcast(&size,       1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (278, MPI, MPI_Reduce(&vector_sum, &check_sum, 1, MPI_DOUBLE, MPI_SUM, root, MPI_COMM_WORLD);); (44, MPI, MPI_Init(&argc,&argv);); (229, MPI, MPI_Barrier(MPI_COMM_WORLD);); (44, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (136, MPI, MPI_Bcast(&iterations, 1, MPI_INT,           root, MPI_COMM_WORLD);); (41, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)", "label": "(146, MPI, MPI_Bcast(&size2,      1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (146, MPI, MPI_Bcast(&radius,     1, MPI_INT,           root, MPI_COMM_WORLD);); (303, MPI, MPI_Finalize();); (141, MPI, MPI_Bcast(&lsize2,     1, MPI_INT,           root, MPI_COMM_WORLD);); (139, MPI, MPI_Bcast(&lsize,      1, MPI_INT,           root, MPI_COMM_WORLD);); (140, MPI, MPI_Bcast(&size,       1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (277, MPI, MPI_Reduce(&vector_sum, &check_sum, 1, MPI_DOUBLE, MPI_SUM, root, MPI_COMM_WORLD);); (44, MPI, MPI_Init(&argc,&argv);); (230, MPI, MPI_Barrier(MPI_COMM_WORLD);); (44, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (138, MPI, MPI_Bcast(&iterations, 1, MPI_INT,           root, MPI_COMM_WORLD);); (41, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(68, OpenMP, #pragma omp parallel for)", "label": "(251, OpenMP, #pragma omp parallel for shared(curdat, inc, m_count))"}
{"predict": "(31, OpenMP, #pragma omp target parallel for simd collapse(4); (64, OpenMP, #pragma omp target parallel for simd collapse(4, 8)); (39, OpenMP, #pragma omp target parallel for simd collapse(4, )); (49, OpenMP, #pragma omp target parallel for simd collapse(4, , 4)); (80, OpenMP, #pragma omp target parallel for simd collapse(5 - 5)); (6, OpenMP, #pragma omp target parallel for simd collapse(); (65, OpenMP, #pragma omp target parallel for simd collapse(2.5)); (8, OpenMP, #pragma omp target parallel for simd collapse()); (-1, OpenMP, #pragma omp target parallel for simd collapse); (72, OpenMP, #pragma omp target parallel for simd collapse(-5)); (36, OpenMP, #pragma omp target parallel for simd collapse(4)); (20, OpenMP, #pragma omp target parallel for simd collapse(4,); (43, OpenMP, #pragma omp target parallel for simd collapse(4, 4, 4)); (66, OpenMP, #pragma omp target parallel for simd collapse(0)); (3, OpenMP, #pragma omp target parallel for simd collapse(,); (10, OpenMP, #pragma omp target parallel for simd collapse 4)); (57, OpenMP, #pragma omp target parallel for simd collapse(foo())); (21, OpenMP, #pragma omp target parallel for simd collapse(4, )); (33, OpenMP, #pragma omp target parallel for simd collapse(4 4))", "label": "(21, OpenMP, #pragma omp target parallel for simd collapse(, )); (54, OpenMP, #pragma omp target parallel for simd collapse(4, , 4)); (34, OpenMP, #pragma omp target parallel for simd collapse(4,); (79, OpenMP, #pragma omp target parallel for simd collapse(0)); (46, OpenMP, #pragma omp target parallel for simd collapse(4 4)); (85, OpenMP, #pragma omp target parallel for simd collapse(2) firstprivate(i)); (72, OpenMP, #pragma omp target parallel for simd collapse(-5)); (6, OpenMP, #pragma omp target parallel for simd collapse()); (-3, OpenMP, #pragma omp target parallel for simd collapse); (57, OpenMP, #pragma omp target parallel for simd collapse(4, 8)); (7, OpenMP, #pragma omp target parallel for simd collapse(,); (63, OpenMP, #pragma omp target parallel for simd collapse(foo())); (74, OpenMP, #pragma omp target parallel for simd collapse(5 - 5)); (57, OpenMP, #pragma omp target parallel for simd collapse(2.5)); (17, OpenMP, #pragma omp target parallel for simd collapse(4); (11, OpenMP, #pragma omp target parallel for simd collapse 4)); (-7, OpenMP, #pragma omp target parallel for simd collapse(); (24, OpenMP, #pragma omp target parallel for simd collapse(4, )); (27, OpenMP, #pragma omp target parallel for simd collapse(4))"}
{"predict": "(82, CUDA, cudaDeviceSynchronize();); (14, CUDA, checkCudaErrors(cudaMalloc((void **)&d_in_signal, mem_size));); (73, CUDA, checkCudaErrors(cufftExecZ2Z(plan, (cufftDoubleComplex *)d_in_signal, (cufftDoubleComplex *)d_output_signal, direction));); (18, CUDA, checkCudaErrors(cudaMalloc((void **)&d_output_signal, mem_size));); (166, CUDA, checkCudaErrors(cufftDestroy(plan));); (19, CUDA, cudaDeviceSynchronize();); (66, CUDA, checkCudaErrors(cufftExecC2C(plan, (cufftComplex *)d_in_signal, (cufftComplex *)d_output_signal, direction));); (2, CUDA, checkCudaErrors(cudaSetDevice(0));); (168, CUDA, cudaDeviceReset();)", "label": "(83, CUDA, cudaDeviceSynchronize();); (20, CUDA, checkCudaErrors(cudaMalloc((void **)&d_output_signal, mem_size));); (73, CUDA, checkCudaErrors(cufftExecZ2Z(plan, (cufftDoubleComplex *)d_in_signal, (cufftDoubleComplex *)d_output_signal, direction));); (69, CUDA, checkCudaErrors(cufftExecC2C(plan, (cufftComplex *)d_in_signal, (cufftComplex *)d_output_signal, direction));); (155, CUDA, checkCudaErrors(cufftDestroy(plan));); (9, CUDA, checkCudaErrors(cudaMalloc((void **)&d_in_signal, mem_size));); (163, CUDA, cudaDeviceReset();)"}
{"predict": "(5, MPI, MPI_Get_library_version(version, &versionlen);); (1, MPI, MPI_Init(&argc, &argv);); (121, MPI, MPI_Finalize();)", "label": "(5, MPI, MPI_Get_library_version(version, &versionlen);)"}
{"predict": "(102, MPI, MPI_Allgather(st_end_offset, 2, ADIO_OFFSET, all_st_end_offsets, 2, ADIO_OFFSET, fd->comm);); (66, MPI, MPI_Comm_rank(fd->comm, &myrank);); (63, MPI, MPI_Comm_size(fd->comm, &nprocs);); (407, MPI, MPI_Type_free(&client_comm_dtype_arr[i]);); (457, MPI, MPI_Type_free(&fd->file_realm_types[0]);); (462, MPI, MPI_Type_free(&fd->file_realm_types[i]);); (394, MPI, MPI_Type_free(&agg_comm_dtype_arr[i]);); (507, MPI, MPI_Free_mem(my_mem_view_state_arr[i].mem_region);); (313, MPI, MPI_Type_get_extent(datatype, &lb, &extent);); (504, MPI, MPI_Free_mem(agg_file_view_state_arr[i].mem_region);); (314, MPI, MPI_Type_size_x(datatype, &size);); (496, MPI, MPI_Free_mem(client_file_view_state_arr[i].mem_region);); (393, MPI, MPI_Type_free(&client_comm_dtype_arr[i]);); (453, MPI, MPI_Type_free(&fd->file_realm_types[fd->file_realm_types_size - 1]);); (412, MPI, MPI_Irecv(&client_comm_sz_arr[i], 1, ADIO_OFFSET, i, i, fd->comm, &client_comm_requests[i]);); (386, MPI, MPI_Type_free(&fd->file_realm_types[i - 1]);); (512, MPI, MPI_Request_free(&client_comm_requests[i]);); (392, MPI, MPI_Irecv(&agg_comm_sz_arr[i], 1, ADIO_OFFSET, i, i, fd->comm, &agg_comm_requests[i]);); (443, MPI, MPI_Type_free(&fd->file_realm_types[fd->file_realm_types_size - 2]);); (494, MPI, MPI_Request_free(&agg_comm_requests[i]);); (434, MPI, MPI_Alltoall(agg_comm_sz_arr, 1, ADIO_OFFSET, fd->file_realm_types, 1, ADIO_OFFSET, fd->comm);); (2", "label": "(101, MPI, MPI_Allgather(st_end_offset, 2, ADIO_OFFSET, all_st_end_offsets, 2, ADIO_OFFSET, fd->comm);); (66, MPI, MPI_Comm_rank(fd->comm, &myrank);); (522, MPI, MPI_Type_free(&client_comm_dtype_arr[i]);); (582, MPI, MPI_Type_free(&fd->file_realm_types[0]);); (62, MPI, MPI_Comm_size(fd->comm, &nprocs);); (584, MPI, MPI_Type_free(&fd->file_realm_types[i]);); (150, MPI, MPI_Type_get_extent(datatype, &lb, &extent);); (302, MPI, MPI_Type_size_x(agg_comm_dtype_arr[i], &size);); (296, MPI, MPI_Type_free(&agg_dtype);)"}
{"predict": "(29, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (37, TBB, reduce_time = (tbb::tick_count::now() - t0).seconds();); (43, TBB, graph_time = (tbb::tick_count::now() - t0).seconds();); (28, TBB, serial_time = (tbb::tick_count::now() - t0).seconds();)", "label": "(29, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (37, TBB, reduce_time = (tbb::tick_count::now() - t0).seconds();); (43, TBB, graph_time = (tbb::tick_count::now() - t0).seconds();); (28, TBB, serial_time = (tbb::tick_count::now() - t0).seconds();)"}
{"predict": "(19, CUDA, cudaMemcpy(poscellh,poscell+pini,sizeof(float4)*np,cudaMemcpyDeviceToHost);); (19, CUDA, cudaMemcpy(checkh,check+pini,sizeof(byte)*np,cudaMemcpyDeviceToHost);)", "label": "(19, CUDA, cudaMemcpy(poscellh,poscell+pini,sizeof(float4)*np,cudaMemcpyDeviceToHost);); (19, CUDA, cudaMemcpy(checkh,check+pini,sizeof(byte)*np,cudaMemcpyDeviceToHost);)"}
{"predict": "(3, OpenMP, #pragma omp parallel for default(none) private(idx) shared(count, sapH_cur_salt, saved_plain, crypt_key))", "label": "(3, OpenMP, #pragma omp parallel for default(none) private(idx) shared(count, sapH_cur_salt, saved_plain, crypt_key))"}
{"predict": "(18, CUDA, YAKSURI_CUDAI_CUDA_ERR_CHKANDJUMP(cerr, rc, fn_fail);); (8, CUDA, uintptr_t iov_unpack_threshold = yaksuri_cudai_get_iov_unpack_threshold(info);); (28, CUDA, cudaMemcpyAsync(iov[i].iov_base, sbuf, iov[i].iov_len, cudaMemcpyDefault, stream);); (52, CUDA, cerr = cudaSetDevice(cur_device);); (42, CUDA, cerr = cudaGetDevice(&cur_device);); (32, CUDA, rc = yaksuri_cudai_md_alloc(type);); (43, CUDA, cerr = cudaSetDevice(target);)", "label": "(9, CUDA, uintptr_t iov_unpack_threshold = yaksuri_cudai_get_iov_unpack_threshold(info);); (29, CUDA, cudaMemcpyAsync(iov[i].iov_base, sbuf, iov[i].iov_len, cudaMemcpyDefault, stream);); (16, CUDA, YAKSURI_CUDAI_CUDA_ERR_CHKANDJUMP(cerr, rc, fn_fail);); (52, CUDA, cerr = cudaSetDevice(cur_device);); (42, CUDA, cerr = cudaGetDevice(&cur_device);); (32, CUDA, rc = yaksuri_cudai_md_alloc(type);); (43, CUDA, cerr = cudaSetDevice(target);)"}
{"predict": "(18, MPI, MPI_Ticker_test();); (6, MPI, t1 = MPI_Wtime();); (32, MPI, tick = MPI_Wtick();); (5, MPI, t2 = MPI_Wtime();)", "label": "(8, MPI, t1 = MPI_Wtime();); (6, MPI, MPI_Init(&argc, &argv);); (38, MPI, MPI_Finalize();); (31, MPI, tick = MPI_Wtick();); (5, MPI, t2 = MPI_Wtime();)"}
{"predict": "(30, OpenCL, err = clGetDeviceInfo(device, CL_DEVICE_EXTENSIONS, 0, NULL, &len);); (43, OpenCL, err = clGetDeviceInfo(device, CL_DEVICE_EXTENSIONS, len, extensions, NULL);); (54, OpenCL, err = clGetDeviceInfo(device, CL_DRIVER_EXTENSIONS, len, extensions, NULL);)", "label": "(29, OpenCL, err = clGetDeviceInfo(device, CL_DEVICE_EXTENSIONS, 0, NULL, &len);); (42, OpenCL, err = clGetDeviceInfo(device, CL_DEVICE_EXTENSIONS, len, extensions, NULL);)"}
{"predict": "(4, MPI, MPI_Finalize();); (135, MPI, MPI_Send(x.data(), chunk, MPI_DOUBLE, 0, 42, comm);); (0, MPI, MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided);)", "label": "(2, MPI, MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided);); (136, MPI, MPI_Send(x.data(), chunk, MPI_DOUBLE, 0, 42, comm);); (2, MPI, MPI_Finalize();); (65, MPI, MPI_Allgather(&chunk, 1, MPI_INT, &domain[1], 1, MPI_INT, comm);)"}
{"predict": "(114, MPI, MPI_Comm_dup(lo_comm, &dup_comm);); (125, MPI, MPI_Attr_put(dup_comm, key_3, (void *) (MPI_Aint) (2 * world_rank));); (135, MPI, MPI_Comm_free(&dup_comm_world);); (142, MPI, MPI_Keyval_free(&key_3);); (138, MPI, MPI_Comm_free(&rev_comm);); (115, MPI, MPI_Attr_put(lo_comm, key_1, (void *) (MPI_Aint) world_rank);); (104, MPI, MPI_Keyval_create(MPI_NULL_COPY_FN, MPI_NULL_DELETE_FN, &key_1, &value);); (169, MPI, MPI_Group_free(&world_group);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (43, MPI, MPI_Group_free(&world_group);); (114, MPI, MPI_Attr_put(lo_comm, key_3, (void *) (MPI_Aint) world_rank);); (107, MPI, MPI_Keyval_create(MPI_NULL_COPY_FN, delete_fn, &key_3, &value);); (135, MPI, MPI_Keyval_free(&key_1);); (95, MPI, MPI_Comm_set_attr(lo_comm, key_1, (void *) (MPI_Aint) world_rank);); (55, MPI, MPI_Comm_free(&dup_comm_world);); (133, MPI, MPI_Group_free(&lo_group);); (155, MPI, MPI_Comm_free(&world_comm);); (65, MPI, MPI_Barrier(world_comm);); (34, MPI, MPI_Comm_create(world_comm, lo_group, &rev_comm);); (128, MPI, MPI_Comm_free(&dup_comm);); (-2, MPI, MPI_Comm_size(MPI_COMM_WORLD, &world_size);); (93, MPI, MPI_Comm_set_attr(lo_comm, key_3, (void *) (MPI_Aint) (2 * world_rank));); (146, MPI, MPI_Keyval_free(&key_3);); (15, MPI, MPI_Comm_group(dup_comm_world, &world_group);); (71, MPI, MPI_Comm_free(&lo_comm);); (76, MPI, MPI_Comm_dup(lo", "label": "(172, MPI, MPI_Comm_split(dup_comm_world, color, key, &split_comm);); (120, MPI, MPI_Attr_put(lo_comm, key_1, (void *) (MPI_Aint) world_rank);); (111, MPI, MPI_Keyval_create(copy_fn, delete_fn, &key_1, &value);); (142, MPI, MPI_Abort(MPI_COMM_WORLD, 3005);); (118, MPI, MPI_Attr_put(lo_comm, key_3, (void *) 0);); (238, MPI, MPI_Comm_free(&split_comm);); (210, MPI, MPI_Comm_compare(world_comm, rev_comm, &result);); (28, MPI, MPI_Comm_create(dup_comm_world, world_group, &world_comm);); (28, MPI, MPI_Comm_rank(world_comm, &rank);); (2, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (114, MPI, MPI_Comm_dup(lo_comm, &dup_comm);); (146, MPI, MPI_Keyval_free(&key_3);); (230, MPI, MPI_Comm_free(&rev_comm);); (135, MPI, MPI_Attr_get(dup_comm, key_3, (void **) &vvalue, &flag);); (210, MPI, MPI_Comm_compare(world_comm, lo_comm, &result);); (190, MPI, MPI_Abort(MPI_COMM_WORLD, 3011);); (71, MPI, MPI_Abort(MPI_COMM_WORLD, 3003);); (228, MPI, MPI_Group_free(&world_group);); (155, MPI, MPI_Comm_size(split_comm, &size);); (-7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &world_size);); (194, MPI, MPI_Comm_create(world_comm, rev_group, &rev_comm);); (135, MPI, MPI_Keyval_free(&key_1);); (157, MPI, MPI_Abort(MPI_COMM_WORLD, 3009);); (190, MPI, MPI_Group_range_incl(world_group, 1, ranges, &rev_group);); (92, MPI, MPI_Keyval_create(MPI_NULL_COPY_FN, MPI_NULL_DELETE_FN, &key_3, &value);); (129, MPI, MPI_Abort(MPI_COMM_WORLD, 3008);); (148, MPI, MPI_Comm_rank(split_comm, &rank);); (201, MPI, MPI_Abort(MPI_COMM_WORLD, 3013);); (173, MPI, MPI_Comm_compare(lo_comm, dup_comm, &result);); (47, MPI, MPI_Abort(MPI_COMM_WORLD, 3002);); (152, MPI, MPI_Barrier(split_comm);); (40, MPI, MPI_Comm_rank(lo_comm, &rank);); (106, MPI, MPI_Abort(MPI_COMM_WORLD, 3004);); (213, MPI, MPI_Group_free(&rev_group);); (163, MPI, MPI_Abort(MPI_COMM_WORLD, 3010);); (205, MPI, MPI_Comm_free(&dup_comm_world);); (4, MPI, MPI_Abort(MPI_COMM_WORLD, 3001);); (213, MPI, MPI_Comm_free(&dup_comm);); (41, MPI, MPI_Barrier(lo_comm);); (210, MPI, MPI_Comm_free(&lo_comm);); (91, MPI, MPI_Attr_get(dup_comm, key_1, (void **) &vvalue, &flag);); (179, MPI, MPI_Abort(MPI_COMM_WORLD, 3012);)"}
{"predict": "(103, CUDA, extern __shared__ __align__(sizeof(float2)) char smem_[];)", "label": "(16, CUDA, extern __shared__ __align__(sizeof(float2)) char smem_[];)"}
{"predict": "(195, OpenMP, double stop = omp_get_wtime();); (78, CUDA, CUDA_RT_CALL(cudaDeviceSynchronize());); (29, CUDA, CUDA_RT_CALL(cudaFree(0));); (89, CUDA, CUDA_RT_CALL(cudaDeviceCanAccessPeer(&canAccessPeer, dev_id, top));); (248, CUDA, CUDA_RT_CALL(cudaFreeHost(a_h));); (30, CUDA, CUDA_RT_CALL(cudaMallocHost(&a_ref_h, nx * ny * sizeof(float)));); (72, CUDA, CUDA_RT_CALL(cudaGetLastError());); (94, CUDA, CUDA_RT_CALL(cudaDeviceCanAccessPeer(&canAccessPeer, dev_id, bottom));); (95, CUDA, CUDA_RT_CALL(cudaDeviceEnablePeerAccess(bottom, 0));); (232, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_bottom_stream[dev_id]));); (84, CUDA, CUDA_RT_CALL(cudaDeviceEnablePeerAccess(top, 0));); (75, CUDA, CUDA_RT_CALL(cudaMalloc(l2_norm_d + dev_id, sizeof(float)));); (45, CUDA, CUDA_RT_CALL(cudaMalloc(a + dev_id, nx * (chunk_size[dev_id] + 2) * sizeof(float)));); (47, CUDA, CUDA_RT_CALL(cudaMemset(a[dev_id], 0, nx * (chunk_size[dev_id] + 2) * sizeof(float)));); (14, CUDA, CUDA_RT_CALL(cudaGetDeviceCount(&num_devices));); (227, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_top_stream[dev_id]));); (149, CUDA, CUDA_RT_CALL(cudaStreamSynchronize(compute_stream[dev_id]));); (228, CUDA, CUDA_RT_CALL(cudaFreeHost(l2_norm_h[dev_id]));); (230, CUDA, CUDA_RT_CALL(cudaFree(a_new[dev_id]));); (11, CUDA, CUDA_RT_CALL(cudaSetDevice(dev_id));); (40, CUDA, CUDA_RT_CALL(cudaMalloc(a_new + dev_id, nx * (chunk_size[dev_id] + 2) * sizeof(float)));); (224, CUDA, CUDA_RT_CALL(cudaStreamDestroy(compute_stream[dev_id]));); (62, CUDA, CUDA_RT_CALL(cudaStreamCreate(compute_stream + dev_id));); (63, CUDA, CUDA_RT_CALL", "label": "(207, OpenMP, double stop = omp_get_wtime();); (146, CUDA, CUDA_RT_CALL(cudaStreamSynchronize(push_top_stream[bottom]));); (77, CUDA, CUDA_RT_CALL(cudaDeviceSynchronize());); (28, CUDA, CUDA_RT_CALL(cudaFree(0));); (88, CUDA, CUDA_RT_CALL(cudaDeviceCanAccessPeer(&canAccessPeer, dev_id, top));); (250, CUDA, CUDA_RT_CALL(cudaFreeHost(a_h));); (29, CUDA, CUDA_RT_CALL(cudaMallocHost(&a_ref_h, nx * ny * sizeof(float)));); (71, CUDA, CUDA_RT_CALL(cudaGetLastError());); (91, CUDA, CUDA_RT_CALL(cudaDeviceCanAccessPeer(&canAccessPeer, dev_id, bottom));); (92, CUDA, CUDA_RT_CALL(cudaDeviceEnablePeerAccess(bottom, 0));); (235, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_bottom_stream[dev_id]));); (83, CUDA, CUDA_RT_CALL(cudaDeviceEnablePeerAccess(top, 0));); (74, CUDA, CUDA_RT_CALL(cudaMalloc(l2_norm_d + dev_id, sizeof(float)));); (44, CUDA, CUDA_RT_CALL(cudaMalloc(a + dev_id, nx * (chunk_size[dev_id] + 2) * sizeof(float)));); (157, CUDA, CUDA_RT_CALL(cudaStreamSynchronize(compute_stream[dev_id]));); (45, CUDA, CUDA_RT_CALL(cudaMemset(a[dev_id], 0, nx * (chunk_size[dev_id] + 2) * sizeof(float)));); (132, CUDA, CUDA_RT_CALL(cudaStreamSynchronize(push_bottom_stream[top]));); (9, CUDA, CUDA_RT_CALL(cudaGetDeviceCount(&num_devices));); (228, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_top_stream[dev_id]));); (11, CUDA, CUDA_RT_CALL(cudaSetDevice(dev_id));); (38, CUDA, CUDA_RT_CALL(cudaMalloc(a_new + dev_id, nx * (chunk_size[dev_id] + 2) * sizeof(float)));); (228, CUDA, CUDA_RT_CALL(cudaFreeHost(l2_norm_h[dev_id]));); (230, CUDA, CUDA_RT_CALL(cudaFree(a_new[dev_id]));); (59, CUDA, CUDA_RT_CALL(cudaStreamCreate(compute_stream + dev_id));); (60, CUDA, CUDA_RT_CALL(cudaStreamCreate(push_bottom_stream + dev_id));); (36, CUDA, CUDA_RT_CALL(cudaMemset(a_new[dev_id], 0, nx * (chunk_size[dev_id] + 2) * sizeof(float)));); (230, CUDA, CUDA_RT_CALL(cudaFreeHost(a_ref_h));); (9, CUDA, CUDA_RT_CALL(cudaMallocHost(&a_h, nx * ny * sizeof(float)));); (219, CUDA, CUDA_RT_CALL(cudaStreamDestroy(compute_stream[dev_id]));); (102, OpenMP, double start = omp_get_wtime();); (57, CUDA, CUDA_RT_CALL(cudaMallocHost(l2_norm_h + dev_id, sizeof(float)));); (222, CUDA, CUDA_RT_CALL(cudaFree(a[dev_id]));); (218, CUDA, CUDA_RT_CALL(cudaFree(l2_norm_d[dev_id]));); (50, CUDA, CUDA_RT_CALL(cudaStreamCreate(push_top_stream + dev_id));)"}
{"predict": "(267, CUDA, checkCuda(cudaGetLastError());); (267, CUDA, checkCuda(cudaStreamSynchronize(FunCallStream));); (97, CUDA, checkCuda(cudaMemcpyAsync(A_gpu->scubufs[streamId].usub_IndirectJ1, indirectJ1, ncols * sizeof(int_t), cudaMemcpyHostToDevice, FunCallStream));); (95, CUDA, checkCuda(cudaMemcpyAsync(A_gpu->scubufs[streamId].usub_IndirectJ2, indirectJ2, ncols * sizeof(int_t), cudaMemcpyHostToDevice, FunCallStream));); (286, CUDA, checkCuda(cudaMemcpy(indirectJ3, A_gpu->scubufs[streamId].usub_IndirectJ3, nnz_col * sizeof(int_t), cudaMemcpyDeviceToHost));); (283, CUDA, checkCuda(cudaMemcpy(indirectJ1, A_gpu->scubufs[streamId].usub_IndirectJ1, ncols * sizeof(int_t), cudaMemcpyDeviceToHost));); (283, CUDA, checkCuda(cudaMemcpy(indirectJ2, A_gpu->scubufs[streamId].usub_IndirectJ2, ncols * sizeof(int_t), cudaMemcpyDeviceToHost));); (81, CUDA, checkCuda(cudaMemcpyAsync(A_gpu->scubufs[streamId].lsub, lsub, lsub_len * sizeof(int_t), cudaMemcpyHostToDevice, FunCallStream));); (276, CUDA, t = SuperLU_timer_() - t;); (82, CUDA, checkCuda(cudaMemcpyAsync(A_gpu->scubufs[streamId].usub, usub, usub_len * sizeof(int_t), cudaMemcpyHostToDevice, FunCallStream));); (268, CUDA, checkCuda(cudaMemcpy(indirectJ3, A_gpu->scubufs[streamId].usub_IndirectJ3_host, nnz_col * sizeof(int_t), cudaMemcpyDeviceToHost));); (77, CUDA, checkCuda(cudaMemcpyAsync(A_gpu->scubufs[streamId].xsup, xsup, nprow * sizeof(int_t), cudaMemcpyHostToDevice, FunCallStream));); (271, CUDA, checkCuda(cudaMemcpy(indirectJ2, A_gpu->scubufs[streamId].usub_IndirectJ2", "label": "(93, CUDA, cudaEventRecord(A_gpu->ePCIeH2D[k0], FunCallStream);); (231, CUDA, cudaEventRecord(A_gpu->GemmStart[k0], FunCallStream);); (239, CUDA, cudaStreamSynchronize(FunCallStream);)"}
{"predict": "(46, MPI, MPI_Win_unlock(1, win);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (43, MPI, MPI_Type_free(&vector_dtp);); (66, MPI, MPI_Free_mem(tar_buf);); (43, MPI, MPI_Win_flush(1, win);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (37, MPI, MPI_Win_lock(MPI_LOCK_SHARED, 1, 0, win);); (37, MPI, MPI_Type_commit(&vector_dtp);); (39, MPI, MPI_Put(orig_buf, 1, vector_dtp, 1, 0, 1, vector_dtp, win);); (61, MPI, MPI_Win_free(&win);); (43, MPI, MPI_Barrier(MPI_COMM_WORLD);); (27, MPI, MPI_Win_create(tar_buf, sizeof(long_double_int_t) * DATA_SIZE, sizeof(long_double_int_t), MPI_INFO_NULL, MPI_COMM_WORLD, &win);)", "label": "(49, MPI, MPI_Win_unlock(1, win);); (32, MPI, MPI_Win_unlock(rank, win);); (45, MPI, MPI_Win_lock(MPI_LOCK_SHARED, 1, 0, win);); (68, MPI, MPI_Type_free(&vector_dtp);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (64, MPI, MPI_Win_free(&win);); (41, MPI, MPI_Accumulate(orig_buf, 1, vector_dtp, 1, 0, 1, vector_dtp, MPI_MAXLOC, win);); (14, MPI, MPI_Type_commit(&vector_dtp);); (64, MPI, MPI_Free_mem(orig_buf);); (65, MPI, MPI_Free_mem(tar_buf);); (17, MPI, MPI_Win_lock(MPI_LOCK_SHARED, rank, 0, win);); (22, MPI, MPI_Barrier(MPI_COMM_WORLD);); (1, MPI, MPI_Alloc_mem(sizeof(long_double_int_t) * DATA_SIZE, MPI_INFO_NULL, &orig_buf);)"}
{"predict": "(67, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (73, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(test.data(), dstDev->basePtr(), test.size(), cudaMemcpyDeviceToHost));); (60, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(srcDev->basePtr(), srcVec.data(), srcVec.size(), cudaMemcpyHostToDevice));); (61, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (65, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamDestroy(stream));)", "label": "(74, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(test.data(), dstDev->basePtr(), test.size(), cudaMemcpyDeviceToHost));); (66, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (50, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(srcDev->basePtr(), srcVec.data(), srcVec.size(), cudaMemcpyHostToDevice));); (60, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (64, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamDestroy(stream));)"}
{"predict": "(142, OpenCL, error = clSetKernelArg(kernel, 1, sizeof(tinfo->inBuf), &tinfo->inBuf);); (172, OpenCL, error = clSetKernelArg(kernel, 3, sizeof(cl_float), &ulps);); (198, OpenCL, error = clSetKernelArg(kernel, 5, sizeof(ftz), &ftz);); (190, OpenCL, error = clSetKernelArg(kernel, 4, sizeof(copysign_test), &copysign_test);); (135, OpenCL, error = clSetKernelArg(kernel, 4, sizeof(cl_int), &isNextafter);); (141, OpenCL, error = clSetKernelArg(kernel, 5, sizeof(cl_int), &isFDim);); (135, OpenCL, error = clSetKernelArg(kernel, 6, sizeof(cl_int), &skipNanInf);); (156, OpenCL, error = clSetKernelArg(kernel, 3, sizeof(cl_int), &isFDim);); (126, OpenCL, error = clSetKernelArg(kernel, 0, sizeof(tinfo->outBuf[j]), &tinfo->outBuf[j]);); (133, OpenCL, error = clSetKernelArg(kernel, 1, sizeof(tinfo->inBuf), &tinfo->inBuf);); (134, OpenCL, error = clSetKernelArg(kernel, 2, sizeof(tinfo->inBuf2), &tinfo->inBuf2);); (156, OpenCL, error = clSetKernelArg(kernel, 6, sizeof(cl_int), &skipNanInf);); (184, OpenCL, error = clSetKernelArg(kernel, 7, sizeof(cl_uint), &job_id);); (204, OpenCL, error = clSetKernelArg(kernel, 9, sizeof(cl_uint), &thread_id);); (143, OpenCL, error = clSetKernelArg(kernel, 4, sizeof(cl_int), &copysign_test);); (170, OpenCL, error = clSetKernelArg(kernel, 8, sizeof(cl_float), &ulps);); (182, OpenCL, error = clSetKernelArg(kernel, 10, sizeof(cl", "label": "(142, OpenCL, error = clSetKernelArg(kernel, 1, sizeof(tinfo->inBuf), &tinfo->inBuf);)"}
{"predict": "(13, CUDA, __shared__ int32_t smemExpertCount[NumThreads];); (32, CUDA, cudaGridDependencySynchronize();); (13, CUDA, __shared__ typename Scan::TempStorage tempStorage;); (29, CUDA, __shared__ int32_t warpHistogram[NumWarps];); (153, CUDA, __shared__ int32_t smemExpertCounts[NumThreadsPerCluster];); (29, CUDA, __shared__ int32_t tokenCounts[NumThreads];); (155, CUDA, __shared__ typename Scan::TempStorage tempStoragePerWarps[NumWarps];); (164, CUDA, __shared__ int32_t idx[NumThreads];); (172, CUDA, __shared__ int32_t counts[NumBlocksPerCluster];); (16, CUDA, __shared__ int32_t warpRanks[NumWarps];); (142, CUDA, __shared__ int32_t smemIndices[NumThreadsPerCluster];); (23, CUDA, __shared__ int32_t expandedIndices[MaxExpandedIdxPerThread];); (152, CUDA, __shared__ int32_t smemOffset[NumThreadsPerCluster];); (162, CUDA, __shared__ typename Scan::TempStorage tempStoragePerCta[NumCta];); (157, CUDA, __shared__ int32_t smemExpertIndex[NumThreadsPerCluster];); (16, CUDA, __shared__ typename Scan::TempStorage tempStoragePerThread[NumThreads];)", "label": "(10, CUDA, __shared__ typename Scan::TempStorage tempStorage;); (33, CUDA, cudaGridDependencySynchronize();); (4, CUDA, __shared__ int32_t __attribute((aligned(128))) smemExpertCount[NumThreads];); (4, CUDA, __shared__ int32_t __attribute((aligned(128))) smemExpertOffset[NumThreads];)"}
{"predict": "(9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &np);); (157, MPI, MPI_Type_free(&stride_dt);); (134, MPI, t1 = MPI_Wtime();); (156, MPI, MPI_Type_free(&trans_dt);); (90, MPI, MPI_Type_size(MPI_DOUBLE,&dsize);); (20, MPI, MPI_Finalize();); (100, MPI, MPI_Type_commit(&trans_dt);); (85, MPI, MPI_Type_create_hvector(block_order, 1, dsize, stride_dt, &trans_dt);); (17, MPI, MPI_Bcast(&iterations, 1, MPI_INT, 0, MPI_COMM_WORLD);); (85, MPI, MPI_Type_free(&stride_dt);); (88, MPI, MPI_Type_vector(block_order, 1, block_order, MPI_DOUBLE, &stride_dt);); (-3, MPI, MPI_Init_thread(&argc, &argv, requested, &provided);); (100, MPI, t0 = MPI_Wtime();); (59, MPI, MPI_Barrier(MPI_COMM_WORLD);); (64, MPI, MPI_Abort(MPI_COMM_WORLD,99);); (-3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (144, MPI, MPI_Finalized(&provided);)", "label": "(9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &np);); (142, MPI, t1 = MPI_Wtime();); (20, MPI, MPI_Finalize();); (98, MPI, MPI_Type_commit(&trans_dt);); (144, MPI, MPI_Type_free(&stride_dt);); (159, MPI, MPI_Allreduce(MPI_IN_PLACE, &abserr, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);); (2, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (90, MPI, MPI_Type_size(MPI_DOUBLE,&dsize);); (141, MPI, MPI_Type_free(&trans_dt);); (70, MPI, MPI_Abort(MPI_COMM_WORLD,99);); (-5, MPI, if (provided < requested) MPI_Abort(MPI_COMM_WORLD,provided);); (89, MPI, MPI_Type_hvector(block_order, 1, dsize, stride_dt, &trans_dt);); (83, MPI, MPI_Type_vector(block_order, 1, block_order, MPI_DOUBLE, &stride_dt);); (54, MPI, MPI_Barrier(MPI_COMM_WORLD);); (96, MPI, t0 = MPI_Wtime();); (-11, MPI, MPI_Init_thread(&argc, &argv, requested, &provided);)"}
{"predict": "(55, CUDA, cudaMalloc((void**)&result_data, axis_size * sizeof(float));); (52, CUDA, cudaMalloc((void**)&result_data, tensor->size * sizeof(float));)", "label": "(55, CUDA, cudaMalloc((void**)&result_data, axis_size * sizeof(float));); (52, CUDA, cudaMalloc((void**)&result_data, tensor->size * sizeof(float));)"}
{"predict": "(51, MPI, MPI_T_category_get_pvars(i, numPvars, pvarIndex);); (28, MPI, MPI_T_category_get_cvars(i, numCvars, cvarIndex);); (3, MPI, MPI_T_category_get_num(&numCat);); (72, MPI, MPI_T_category_get_categories(i, numSubcat, subcatIndex);)", "label": "(51, MPI, MPI_T_category_get_pvars(i, numPvars, pvarIndex);); (28, MPI, MPI_T_category_get_cvars(i, numCvars, cvarIndex);); (3, MPI, MPI_T_category_get_num(&numCat);); (72, MPI, MPI_T_category_get_categories(i, numSubcat, subcatIndex);)"}
{"predict": "(34, MPI, MPI_Bcast( rowind, nnz, mpi_int_t,  0, grid->comm );); (31, MPI, MPI_Bcast( &nnz,   1,   mpi_int_t,  0, grid->comm );); (45, MPI, MPI_Bcast( colptr,  n+1, mpi_int_t,  0, grid->comm );); (43, MPI, MPI_Bcast( rowind,  nnz, mpi_int_t,  0, grid->comm );); (34, MPI, MPI_Bcast( &m,   1,   mpi_int_t,  0, grid->comm );); (26, MPI, MPI_Bcast( &n,     1,   mpi_int_t,  0, grid->comm );); (24, MPI, MPI_Bcast( &m,     1,   mpi_int_t,  0, grid->comm );); (33, MPI, MPI_Bcast( &nnz, 1,   mpi_int_t,  0, grid->comm );); (27, MPI, MPI_Bcast( colptr, n+1, mpi_int_t,  0, grid->comm );); (24, MPI, MPI_Bcast( nzval,  nnz, MPI_FLOAT, 0, grid->comm );); (29, MPI, MPI_Bcast( &n,   1,   mpi_int_t,  0, grid->comm );); (34, MPI, MPI_Bcast( nzval,   nnz, MPI_FLOAT, 0, grid->comm );)", "label": "(33, MPI, MPI_Bcast( nzval,  nnz, MPI_FLOAT, 0, grid->comm );); (33, MPI, MPI_Bcast( rowind, nnz, mpi_int_t,  0, grid->comm );); (45, MPI, MPI_Bcast( colptr,  n+1, mpi_int_t,  0, grid->comm );); (43, MPI, MPI_Bcast( rowind,  nnz, mpi_int_t,  0, grid->comm );); (34, MPI, MPI_Bcast( &m,   1,   mpi_int_t,  0, grid->comm );); (26, MPI, MPI_Bcast( &n,     1,   mpi_int_t,  0, grid->comm );); (24, MPI, MPI_Bcast( &m,     1,   mpi_int_t,  0, grid->comm );); (33, MPI, MPI_Bcast( &nnz, 1,   mpi_int_t,  0, grid->comm );); (27, MPI, MPI_Bcast( colptr, n+1, mpi_int_t,  0, grid->comm );); (23, MPI, MPI_Bcast( &nnz,   1,   mpi_int_t,  0, grid->comm );); (29, MPI, MPI_Bcast( &n,   1,   mpi_int_t,  0, grid->comm );); (34, MPI, MPI_Bcast( nzval,   nnz, MPI_FLOAT, 0, grid->comm );)"}
{"predict": "(40, OpenMP, #pragma omp master taskloop simd reduction(|| : argc > 0 ? argv[1] : argv[2])); (59, OpenMP, #pragma omp master taskloop simd reduction(max : h.b)); (76, OpenMP, #pragma omp master taskloop simd reduction(+ : h, k, B::x)); (83, OpenMP, #pragma omp master taskloop simd private(i), reduction(+ : j), reduction(+ : q)); (59, OpenMP, #pragma omp master taskloop simd reduction(+ : ba)); (13, OpenMP, #pragma omp master taskloop simd reduction()); (60, OpenMP, #pragma omp master taskloop simd reduction(^ : z, fl)); (16, OpenMP, #pragma omp master taskloop simd reduction(-); (90, OpenMP, #pragma omp master taskloop simd reduction(+ : r)); (75, OpenMP, #pragma omp master taskloop simd reduction(+ : o)); (52, OpenMP, #pragma omp master taskloop simd reduction(- : da)); (43, OpenMP, #pragma omp master taskloop simd reduction(+ : a, b, c, d, f)); (3, OpenMP, #pragma omp master taskloop simd reduction(); (13, OpenMP, #pragma omp master taskloop simd reduction(\\)); (83, OpenMP, #pragma omp parallel reduction(* : fl)); (68, OpenMP, #pragma omp master taskloop simd reduction(+ : p), reduction(+ : p)); (62, OpenMP, #pragma omp master taskloop simd reduction(&& : S2::S2s)); (87, OpenMP, #pragma omp master taskloop simd reduction(max : j)); (67, OpenMP, #pragma omp master taskloop simd reduction(+ : p), reduction(+ : P::p)); (52, OpenMP, #pragma omp master taskloop simd reduction(&& : S2::S2sc)); (11, OpenMP, #pragma omp master taskloop simd reduction(foo : argc); (4, OpenMP, #pragma omp master taskloop simd reduction(*)); (28, OpenMP, #pragma omp master taskloop simd reduction(+ : c)); (1, OpenMP, #pragma omp master taskloop simd reduction()); (13, OpenMP, #pragma omp master taskloop simd reduction(~", "label": "(45, OpenMP, #pragma omp master taskloop simd reduction(&& : argc, z)); (101, OpenMP, #pragma omp master taskloop simd reduction(max : j)); (67, OpenMP, #pragma omp master taskloop simd reduction(^ : fl)); (111, OpenMP, #pragma omp master taskloop simd reduction(+ : m)); (20, OpenMP, #pragma omp master taskloop simd reduction()); (46, OpenMP, #pragma omp master taskloop simd reduction(+ : a, b, c, d, f)); (48, OpenMP, #pragma omp master taskloop simd reduction(min : a, b, c, d, f)); (53, OpenMP, #pragma omp master taskloop simd reduction(+ : ba)); (34, OpenMP, #pragma omp master taskloop simd reduction(~ : argc)); (100, OpenMP, #pragma omp parallel reduction(* : fl)); (17, OpenMP, #pragma omp master taskloop simd reduction(*)); (1, OpenMP, #pragma omp master taskloop simd reduction); (63, OpenMP, #pragma omp master taskloop simd reduction(&& : S2::S2sc)); (68, OpenMP, #pragma omp master taskloop simd reduction(+ : h, k, B::x)); (106, OpenMP, #pragma omp master taskloop simd reduction(task, + : m)); (75, OpenMP, #pragma omp parallel private(k)); (62, OpenMP, #pragma omp master taskloop simd reduction(& : e, g)); (67, OpenMP, #pragma omp master taskloop simd reduction(+ : o)); (-3, OpenMP, #pragma omp master taskloop simd reduction +); (72, OpenMP, #pragma omp master taskloop simd reduction(+ : p), reduction(+ : p)); (43, OpenMP, #pragma omp master taskloop simd reduction(* : ca)); (9, OpenMP, #pragma omp master taskloop simd reduction(\\)); (14, OpenMP, #pragma omp master taskloop simd reduction(| : argc,); (78, OpenMP, #pragma omp parallel reduction(min : i)); (81, OpenMP, #pragma omp parallel private(fl)); (72, OpenMP, #pragma omp master taskloop simd reduction(+ : r)); (46, OpenMP, #pragma omp master taskloop simd reduction(&& : S2::S2s)); (30, OpenMP, #pragma omp master taskloop simd reduction(max : h.b)); (20, OpenMP, #pragma omp master taskloop simd reduction(^ : S1)); (10, OpenMP, #pragma omp master taskloop simd reduction(|| : argc > 0 ? argv[1] : argv[2])); (-12, OpenMP, #pragma omp master taskloop simd reduction(); (56, OpenMP, #pragma omp master taskloop simd private(i), reduction(+ : j), reduction(+ : q)); (85, OpenMP, #pragma omp master taskloop simd reduction(+ : m) nogroup); (-12, OpenMP, #pragma omp master taskloop simd reduction(-); (66, OpenMP, #pragma omp parallel shared(i)); (-2, OpenMP, #pragma omp master taskloop simd reduction(foo : argc); (30, OpenMP, #pragma omp master taskloop simd reduction(- : da)); (69, OpenMP, #pragma omp master taskloop simd reduction(+ : fl))"}
{"predict": "(41, MPI, MPI_Type_commit(&columntype);); (63, MPI, MPI_Finalize();); (35, MPI, MPI_Type_create_hvector(8, 1, sizeof(int), columntype, &columntype);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (37, MPI, MPI_Send(&array[0][1], 1, columntype, 1, 1, MPI_COMM_WORLD);); (-1, MPI, MPI_Init(NULL, NULL);); (44, MPI, MPI_Type_free(&columntype);)", "label": "(39, MPI, MPI_Type_commit(&columntype);); (7, MPI, MPI_Init(&argc, &argv);); (61, MPI, MPI_Finalize();); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (34, MPI, MPI_Type_vector(8, 1, 8, MPI_INT, &columntype);); (45, MPI, MPI_Type_free(&columntype);); (37, MPI, MPI_Send(&array[0][1], 1, columntype, 1, 1, MPI_COMM_WORLD);)"}
{"predict": "(41, OpenMP, #pragma omp teams distribute parallel for simd aligned (argv[1])); (19, OpenMP, #pragma omp teams distribute parallel for simd aligned (argc); (6, OpenMP, #pragma omp teams distribute parallel for simd aligned); (26, OpenMP, #pragma omp teams distribute parallel for simd aligned (*argc > 0 ? argv[1] : argv[2])); (42, OpenMP, #pragma omp teams distribute parallel for simd aligned(e, g)); (44, OpenMP, #pragma omp teams distribute parallel for simd aligned(h)); (50, OpenMP, #pragma omp parallel); (5, OpenMP, #pragma omp teams distribute parallel for simd aligned (); (45, OpenMP, #pragma omp teams distribute parallel for simd aligned(i)); (64, OpenMP, #pragma omp teams distribute parallel for simd aligned(f:j)); (13, OpenMP, #pragma omp teams distribute parallel for simd aligned (argc,); (46, OpenMP, #pragma omp teams distribute parallel for simd aligned(v:16)); (26, OpenMP, #pragma omp teams distribute parallel for simd aligned (S1)); (4, OpenMP, #pragma omp teams distribute parallel for simd aligned ()); (-7, OpenMP, #pragma omp target); (52, OpenMP, #pragma omp teams distribute parallel for simd aligned(f)); (12, OpenMP, #pragma omp teams distribute parallel for simd aligned (argc : 5))", "label": "(45, OpenMP, #pragma omp teams distribute parallel for simd aligned(e, g)); (7, OpenMP, #pragma omp teams distribute parallel for simd aligned); (14, OpenMP, #pragma omp teams distribute parallel for simd aligned ()); (9, OpenMP, #pragma omp teams distribute parallel for simd aligned (); (45, OpenMP, #pragma omp teams distribute parallel for simd aligned(h)); (51, OpenMP, #pragma omp parallel); (18, OpenMP, #pragma omp teams distribute parallel for simd aligned (argc,); (66, OpenMP, #pragma omp teams distribute parallel for simd aligned(f:j)); (33, OpenMP, #pragma omp teams distribute parallel for simd aligned (argv[1])); (24, OpenMP, #pragma omp teams distribute parallel for simd aligned (argc : 5)); (43, OpenMP, #pragma omp teams distribute parallel for simd aligned(i)); (56, OpenMP, #pragma omp teams distribute parallel for simd aligned(f)); (17, OpenMP, #pragma omp teams distribute parallel for simd aligned (*argc > 0 ? argv[1] : argv[2])); (-6, OpenMP, #pragma omp target); (47, OpenMP, #pragma omp teams distribute parallel for simd aligned(v:16)); (22, OpenMP, #pragma omp teams distribute parallel for simd aligned (S1)); (4, OpenMP, #pragma omp teams distribute parallel for simd aligned (argc)"}
{"predict": "(7, MPI, MPI_Init(&argc, &argv);); (48, MPI, MPI_Type_free(&vector);); (65, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (40, MPI, MPI_Type_vector(8, 1, 6, MPI_INT, &vector);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (40, MPI, MPI_Type_commit(&vector);)", "label": "(52, MPI, MPI_Type_free(&vector);); (6, MPI, MPI_Init(&argc, &argv);); (42, MPI, MPI_Type_create_resized(vector, 0, sizeof(int), &vector);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (64, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (37, MPI, MPI_Type_vector(8, 1, 6, MPI_INT, &vector);); (38, MPI, MPI_Type_commit(&vector);); (57, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(35, MPI, MPI_Recv(recv_buff, msg_size, MPI_INT, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (54, MPI, MPI_Finalize();); (30, MPI, MPI_Send(send_buff, msg_size, MPI_INT, 1, 0, MPI_COMM_WORLD);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (2, MPI, MPI_Init(NULL, NULL);); (48, MPI, MPI_Recv(recv_buff, msg_size, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (46, MPI, MPI_Send(send_buff, msg_size, MPI_INT, 0, 0, MPI_COMM_WORLD);); (25, MPI, double start_time = MPI_Wtime();); (48, MPI, double end_time = MPI_Wtime();); (2, MPI, MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);)", "label": "(59, MPI, double end_time = MPI_Wtime();); (60, MPI, MPI_Finalize();); (47, MPI, MPI_Recv(recv_buff, msg_size, MPI_INT, 1, TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (17, MPI, MPI_Bcast(&msg_size, 1, MPI_INT, 0, MPI_COMM_WORLD);); (2, MPI, MPI_Init(NULL, NULL);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (13, MPI, MPI_Bcast(&nr_msgs, 1, MPI_INT, 0, MPI_COMM_WORLD);); (48, MPI, MPI_Ssend(send_buff, msg_size, MPI_INT, 0, TAG, MPI_COMM_WORLD);); (24, MPI, double start_time = MPI_Wtime();); (31, MPI, MPI_Recv(recv_buff, msg_size, MPI_INT, 0, TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (28, MPI, MPI_Ssend(send_buff, msg_size, MPI_INT, 1, TAG, MPI_COMM_WORLD);); (0, MPI, MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);)"}
{"predict": "(55, CUDA, CUDA_CHECK(cudaMallocHost((void**)&tmp_ptr, nsupers * sizeof(int_t)));); (141, CUDA, CUDA_CHECK(cudaEventCreate(A_gpu->GemmEnd[nsupers-1]));); (141, CUDA, CUDA_CHECK(cudaEventCreate(A_gpu->ScatterEnd[nsupers-1]));); (151, CUDA, CUDA_CHECK(cudaStreamCreate(&(A_gpu->funCallStreams[streamId])));); (135, CUDA, CUDA_CHECK(cudaEventCreate(A_gpu->ePCIeD2H_Start[nsupers-1]));); (135, CUDA, CUDA_CHECK(cudaEventCreate(A_gpu->ePCIeD2H_End[nsupers-1]));); (131, CUDA, CUDA_CHECK(cudaEventCreate(A_gpu->ePCIeH2D[nsupers-1]));); (46, CUDA, CUDA_CHECK(cudaMallocHost((void**)&tmp_ptr, remain_l_max * sizeof(double)));); (52, CUDA, CUDA_CHECK(cudaMallocHost((void**)&tmp_ptr, bigu_size * sizeof(double)));); (150, CUDA, CUDA_CHECK(cudaStreamCreate(&(A_gpu->CopyStream)));); (41, CUDA, CUDA_CHECK(cudaMallocHost((void**)&tmp_ptr, nsupers * sizeof(Remain_info_t)));); (46, CUDA, CUDA_CHECK(cudaMallocHost((void**)&tmp_ptr, buffer_size * sizeof(double)));); (138, CUDA, CUDA_CHECK(cudaEventCreate(A_gpu->ePCIeD2H_End[streamId]));); (141, CUDA, CUDA_CHECK(cudaEventCreate(A_gpu->GemmStart[streamId]));); (137, CUDA, CUDA_CHECK(cudaEventCreate(A_gpu->ePCIeD2H_Start[streamId]));); (38, CUDA, CUDA_CHECK(cudaMallocHost((void**)&tmp_ptr, A_host->bufmax[0] * sizeof(int_t)));); (53, CUDA, CUDA_CHECK(cudaMallocHost((void**)&tmp_ptr, mcb * sizeof(Ublock_info_t)));); (25, CUDA, CUDA_CHECK(cudaMalloc((void**)&(A_gpu->d_nnz_per_supc), nsupers * sizeof(int_t)));); (133, CUDA, CUDA_CHECK(cudaEventCreate(A_gpu->ePCIeH2D[streamId]));); (140", "label": "(61, CUDA, checkCudaErrors(cudaMallocHost(  &tmp_ptr,  remain_l_max * sizeof(double) )) ;); (50, CUDA, checkCudaErrors(cudaMallocHost(  &tmp_ptr, (n) * sizeof(int_t) )) ;); (256, CUDA, checkCudaErrors(cudaMemcpy( (A_gpu->local_u_blk_infoVec), local_u_blk_infoVec, cum_num_u_blocks * sizeof(local_u_blk_info_t), cudaMemcpyHostToDevice)) ;); (476, CUDA, checkCudaErrors(cudaMemcpy( (A_gpu->UrowindVec), indtemp1, u_ind_len * sizeof(int_t), cudaMemcpyHostToDevice)) ;); (62, CUDA, checkCudaErrors(cudaMallocHost ( &tmp_ptr, sizeof(double) * (A_host->bufmax[1])));); (67, CUDA, checkCudaErrors(cudaMallocHost ( &tmp_ptr, sizeof(int_t) * (A_host->bufmax[2])));); (81, CUDA, checkCudaErrors(cudaMalloc(  &tmp_ptr,  buffer_size * sizeof(double))) ;); (179, CUDA, checkCudaErrors(cudaMemcpy( (A_gpu->local_l_blk_infoVec), local_l_blk_infoVec, cum_num_l_blocks * sizeof(local_l_blk_info_t), cudaMemcpyHostToDevice)) ;); (51, CUDA, checkCudaErrors(cudaMallocHost(  &tmp_ptr, mcb * sizeof(Ublock_info_t) )) ;); (174, CUDA, checkCudaErrors(cudaMalloc(  &tmp_ptr,  cum_num_l_blocks * sizeof(local_l_blk_info_t))) ;); (477, CUDA, checkCudaErrors(cudaMemcpy( (A_gpu->UrowindPtr), temp_UrowindPtr, u_k * sizeof(int_t), cudaMemcpyHostToDevice))"}
{"predict": "(53, OpenMP, #pragma omp teams distribute shared (j)); (62, OpenMP, #pragma omp teams distribute shared(i)); (15, OpenMP, #pragma omp teams distribute shared (argc); (34, OpenMP, #pragma omp teams distribute shared (argv[1])); (46, OpenMP, #pragma omp teams distribute shared(e, g)); (16, OpenMP, #pragma omp teams distribute shared (argc,); (21, OpenMP, #pragma omp teams distribute shared (argc > 0 ? argv[1] : argv[2])); (46, OpenMP, #pragma omp teams distribute shared(h, B::x)); (55, OpenMP, #pragma omp teams distribute firstprivate(i), shared(i)); (18, OpenMP, #pragma omp teams distribute shared (argc)); (22, OpenMP, #pragma omp teams distribute shared (S1)); (45, OpenMP, #pragma omp teams distribute shared(i)); (1, OpenMP, #pragma omp teams distribute shared (); (12, OpenMP, #pragma omp teams distribute shared (argc,); (26, OpenMP, #pragma omp teams distribute shared(ca)); (23, OpenMP, #pragma omp teams distribute shared(ba)); (30, OpenMP, #pragma omp teams distribute shared(da)); (-7, OpenMP, #pragma omp target); (12, OpenMP, #pragma omp teams distribute shared (argc))); (-2, OpenMP, #pragma omp teams distribute shared ()); (47, OpenMP, #pragma omp teams distribute shared(j)); (32, OpenMP, #pragma omp teams distribute firstprivate(i)); (26, OpenMP, #pragma omp teams distribute shared(g)); (33, OpenMP, #pragma omp teams distribute private(i)); (15, OpenMP, #pragma omp teams distribute shared (a, b, c, d, f)); (47, OpenMP, #pragma omp teams distribute firstprivate(i) shared(i))", "label": "(26, OpenMP, #pragma omp teams distribute shared (argc)); (22, OpenMP, #pragma omp teams distribute shared (argc > 0 ? argv[1] : argv[2])); (39, OpenMP, #pragma omp teams distribute shared(ca)); (41, OpenMP, #pragma omp teams distribute shared(da)); (16, OpenMP, #pragma omp teams distribute shared (argc,); (3, OpenMP, #pragma omp teams distribute shared); (5, OpenMP, #pragma omp teams distribute shared (); (49, OpenMP, #pragma omp teams distribute firstprivate(i), shared(i)); (27, OpenMP, #pragma omp teams distribute shared (argv[1])); (-2, OpenMP, #pragma omp target); (55, OpenMP, #pragma omp teams distribute shared(j)); (57, OpenMP, #pragma omp teams distribute firstprivate(i)); (17, OpenMP, #pragma omp teams distribute shared (S1)); (37, OpenMP, #pragma omp teams distribute shared(h, B::x)); (18, OpenMP, #pragma omp teams distribute shared (a, b, c, d, f)); (2, OpenMP, #pragma omp teams distribute shared (argc); (46, OpenMP, #pragma omp teams distribute shared(i)); (21, OpenMP, #pragma omp teams distribute shared(ba)); (35, OpenMP, #pragma omp teams distribute private(i), shared(i)); (28, OpenMP, #pragma omp teams distribute shared(e, g)); (39, OpenMP, #pragma omp teams distribute private(i)); (-7, OpenMP, #pragma omp teams distribute shared ())"}
{"predict": "(132, MPI, MPI_Waitall(k, request, status);)", "label": "(133, MPI, MPI_Waitall(k, request, status);); (90, MPI, MPI_Reduce_local(recvbuf, tempbuf, count, type, op);)"}
{"predict": "(68, CUDA, checkCudaErrors(cudaMemset(d_Kernel, 0, DATA_SIZE));); (109, CUDA, checkCudaErrors(cudaFree(d_Kernel));); (73, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (40, CUDA, findCudaDevice(argc, (const char **)argv);); (51, CUDA, checkCudaErrors(cudaMalloc((void **)&d_Kernel, DATA_SIZE));); (104, CUDA, checkCudaErrors(cudaFree(d_Data));); (49, CUDA, checkCudaErrors(cudaMalloc((void **)&d_Data, DATA_SIZE));)", "label": "(68, CUDA, checkCudaErrors(cudaMemset(d_Kernel, 0, DATA_SIZE));); (110, CUDA, checkCudaErrors(cudaFree(d_Kernel));); (40, CUDA, findCudaDevice(argc, (const char **)argv);); (72, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (49, CUDA, checkCudaErrors(cudaMalloc((void **)&d_Kernel, DATA_SIZE));); (105, CUDA, checkCudaErrors(cudaFree(d_Data));); (48, CUDA, checkCudaErrors(cudaMalloc((void **)&d_Data, DATA_SIZE));)"}
{"predict": "(94, MPI, MPI_Barrier(MPI_COMM_WORLD);); (35, MPI, MPI_Error_string(mpirc, mpierrstr, &mpierrlen);); (70, MPI, mpirc = MPI_File_close(&fh);); (8, MPI, double start_read = MPI_Wtime();); (38, MPI, mpirc = MPI_File_read(fh, &version_packed, 1, MPI_UINT64_T, &status);); (25, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (18, MPI, int mpirc = MPI_File_set_view(fh, disp, MPI_UINT64_T, MPI_UINT64_T, datarep, MPI_INFO_NULL);); (75, MPI, double end_read = MPI_Wtime();)", "label": "(39, MPI, MPI_Error_string(mpirc, mpierrstr, &mpierrlen);); (98, MPI, MPI_Barrier(MPI_COMM_WORLD);); (22, MPI, int mpirc = MPI_File_open(MPI_COMM_WORLD, (char*)name, amode, MPI_INFO_NULL, &fh);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (79, MPI, double end_read = MPI_Wtime();); (3, MPI, double start_read = MPI_Wtime();); (67, MPI, mpirc = MPI_File_close(&fh);); (30, MPI, mpirc = MPI_File_set_view(fh, disp, MPI_BYTE, MPI_BYTE, datarep, MPI_INFO_NULL);); (47, MPI, MPI_Bcast(&version, 1, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (36, MPI, mpirc = MPI_File_read_at(fh, 0, &version_packed, 8, MPI_BYTE, &status);)"}
{"predict": "(267, MPI, MPI_Allreduce(&tmp_error, error_code, 1, MPI_INT, MPI_MAX, fd->comm);); (65, MPI, MPI_Comm_size(fd->comm, &nprocs);); (93, MPI, MPI_Allgather(&end_offset, 1, ADIO_OFFSET, end_offsets, 1, ADIO_OFFSET, fd->comm);); (91, MPI, MPI_Allgather(&start_offset, 1, ADIO_OFFSET, st_offsets, 1, ADIO_OFFSET, fd->comm);); (63, MPI, MPI_Comm_rank(fd->comm, &myrank);); (284, MPI, MPI_Type_size_x(datatype, &size);)", "label": "(191, MPI, MPI_Allreduce(&tmp_error, error_code, 1, MPI_INT, MPI_MAX, fd->comm);); (65, MPI, MPI_Comm_size(fd->comm, &nprocs);); (93, MPI, MPI_Allgather(&end_offset, 1, ADIO_OFFSET, end_offsets, 1, ADIO_OFFSET, fd->comm);); (91, MPI, MPI_Allgather(&start_offset, 1, ADIO_OFFSET, st_offsets, 1, ADIO_OFFSET, fd->comm);); (63, MPI, MPI_Comm_rank(fd->comm, &myrank);); (210, MPI, MPI_Type_size_x(datatype, &size);)"}
{"predict": "(103, OpenMP, #pragma omp for firstprivate(numElemReg)); (36, OpenMP, #pragma omp parallel); (65, OpenMP, #pragma omp for firstprivate(numElemReg)); (75, OpenMP, #pragma omp for nowait firstprivate(numElemReg)); (36, OpenMP, #pragma omp for firstprivate(numElemReg))", "label": "(106, OpenMP, #pragma omp parallel for firstprivate(numElemReg)); (53, OpenMP, #pragma omp for firstprivate(numElemReg)); (65, OpenMP, #pragma omp for nowait firstprivate(numElemReg, eosvmin)); (34, OpenMP, #pragma omp parallel); (74, OpenMP, #pragma omp for nowait firstprivate(numElemReg, eosvmax)); (36, OpenMP, #pragma omp for nowait firstprivate(numElemReg))"}
{"predict": "(38, MPI, MPI_Comm_size(comm, &mpi_size);); (45, MPI, rc = MPI_Bcast(&(cont->amode), 1, MPI_INT, 0, comm);); (11, MPI, MPI_Barrier(comm);)", "label": "(40, MPI, MPI_Error_class(*error_code, &rc);); (12, MPI, MPI_Comm_size(comm, &mpi_size);); (42, MPI, MPI_Bcast(&rc, 1, MPI_INT, 0, comm);)"}
{"predict": "(77, CUDA, cudaError result = cudaDeviceSynchronize();); (30, CUDA, Tensor tensor_S = make_tensor(make_gmem_ptr(thrust::raw_pointer_cast(d_S.data())), make_layout(tensor_shape));); (14, CUDA, thrust::host_vector<Element> h_S(size(tensor_shape));); (22, CUDA, thrust::device_vector<Element> d_D = h_D;); (80, CUDA, std::cerr << \"CUDA Runtime error: \" << cudaGetErrorString(result) << std::endl;); (11, CUDA, thrust::host_vector<Element> h_D(size(tensor_shape));); (26, CUDA, Tensor tensor_D = make_tensor(make_gmem_ptr(thrust::raw_pointer_cast(d_D.data())), make_layout(tensor_shape));); (17, CUDA, thrust::device_vector<Element> d_S = h_S;)", "label": "(81, CUDA, cudaError result = cudaDeviceSynchronize();); (30, CUDA, Tensor tensor_S = make_tensor(make_gmem_ptr(thrust::raw_pointer_cast(d_S.data())), make_layout(tensor_shape));); (14, CUDA, thrust::host_vector<Element> h_S(size(tensor_shape));); (22, CUDA, thrust::device_vector<Element> d_D = h_D;); (79, CUDA, std::cerr << \"CUDA Runtime error: \" << cudaGetErrorString(result) << std::endl;); (12, CUDA, thrust::host_vector<Element> h_D(size(tensor_shape));); (26, CUDA, Tensor tensor_D = make_tensor(make_gmem_ptr(thrust::raw_pointer_cast(d_D.data())), make_layout(tensor_shape));); (17, CUDA, thrust::device_vector<Element> d_S = h_S;)"}
{"predict": "(2, CUDA, extern __shared__ float4 siftParts1[];); (2, CUDA, extern __shared__ float4 siftParts2[];)", "label": "(3, CUDA, __shared__ float siftParts2[16*64];); (1, CUDA, __shared__ float siftParts1[17*64];)"}
{"predict": "(53, CUDA, __shared__ bool isAccepted;); (53, CUDA, __shared__ T sSumVal;)", "label": "(52, CUDA, __shared__ bool isAccepted;); (52, CUDA, __shared__ T sSumVal;)"}
{"predict": "(53, MPI, MPI_Free_mem(dst_buf);); (9, MPI, MPI_Alloc_mem(sizeof(double) * nproc * MAXELEMS, MPI_INFO_NULL, &dst_buf);); (31, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, target, 0, dst_win);); (49, MPI, MPI_Win_free(&dst_win);); (14, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, me, 0, dst_win);); (17, MPI, MPI_Barrier(MPI_COMM_WORLD);); (27, MPI, MPI_Win_unlock(target, dst_win);); (12, MPI, MPI_Win_unlock(me, dst_win);)", "label": "(54, MPI, MPI_Free_mem(dst_buf);); (9, MPI, MPI_Alloc_mem(sizeof(double) * nproc * MAXELEMS, MPI_INFO_NULL, &dst_buf);); (30, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, target, 0, dst_win);); (50, MPI, MPI_Win_free(&dst_win);); (13, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, me, 0, dst_win);); (17, MPI, MPI_Win_unlock(me, dst_win);); (18, MPI, MPI_Barrier(MPI_COMM_WORLD);); (28, MPI, MPI_Win_unlock(target, dst_win);)"}
{"predict": "(28, OpenMP, #pragma omp target parallel for simd private(S1)); (45, OpenMP, #pragma omp target parallel for simd private(i)); (8, OpenMP, #pragma omp target parallel for simd private(); (19, OpenMP, #pragma omp target parallel for simd private(argc > 0 ? argv[1] : argv[2])); (3, OpenMP, #pragma omp target parallel for simd private); (29, OpenMP, #pragma omp target parallel for simd private(argv[1])); (25, OpenMP, #pragma omp target parallel for simd private(a, b)); (30, OpenMP, #pragma omp target parallel for simd private(e, g)); (35, OpenMP, #pragma omp parallel); (42, OpenMP, #pragma omp parallel private(i)); (14, OpenMP, #pragma omp target parallel for simd private(argc, z)); (2, OpenMP, #pragma omp target parallel for simd private()); (28, OpenMP, #pragma omp target parallel for simd private(h, B::x)); (47, OpenMP, #pragma omp target parallel for simd private(m)); (36, OpenMP, #pragma omp parallel shared(i)); (37, OpenMP, #pragma omp target parallel for simd private(j)); (7, OpenMP, #pragma omp target parallel for simd private(argc,); (-3, OpenMP, #pragma omp target parallel for simd private); (3, OpenMP, #pragma omp target parallel for simd private(argc))", "label": "(22, OpenMP, #pragma omp target parallel for simd private(argc > 0 ? argv[1] : argv[2])); (36, OpenMP, #pragma omp target parallel for simd private(e, g)); (44, OpenMP, #pragma omp target parallel for simd private(i)); (49, OpenMP, #pragma omp target parallel for simd private(j)); (47, OpenMP, #pragma omp parallel private(i)); (54, OpenMP, #pragma omp target parallel for simd private(m)); (4, OpenMP, #pragma omp target parallel for simd private(); (36, OpenMP, #pragma omp parallel); (32, OpenMP, #pragma omp target parallel for simd private(h, B::x)); (22, OpenMP, #pragma omp target parallel for simd private(a, b)); (24, OpenMP, #pragma omp target parallel for simd private(argv[1])); (17, OpenMP, #pragma omp target parallel for simd private(S1)); (38, OpenMP, #pragma omp parallel shared(i)); (12, OpenMP, #pragma omp target parallel for simd private(argc, z)); (2, OpenMP, #pragma omp target parallel for simd private(argc); (-2, OpenMP, #pragma omp target parallel for simd private()); (-9, OpenMP, #pragma omp target parallel for simd private); (2, OpenMP, #pragma omp target parallel for simd private(argc,)"}
{"predict": "(81, MPI, MPI_Finalize();)", "label": "(81, MPI, MPI_Finalize();)"}
{"predict": "(210, MPI, MPI_Reduce(&total, &total_sum, 1, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD);); (233, MPI, MPI_Finalize();); (83, MPI, MPI_Bcast(&iterations,    1, MPI_INT, root, MPI_COMM_WORLD);); (25, MPI, MPI_Init(&argc,&argv);); (80, MPI, MPI_Bcast(&vector_length, 1, MPI_INT, root, MPI_COMM_WORLD);); (81, MPI, MPI_Bcast(&btype,         1, MPI_INT, root, MPI_COMM_WORLD);); (98, MPI, MPI_Barrier(MPI_COMM_WORLD);); (23, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (21, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)", "label": "(211, MPI, MPI_Reduce(&total, &total_sum, 1, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD);); (234, MPI, MPI_Finalize();); (83, MPI, MPI_Bcast(&iterations,    1, MPI_INT, root, MPI_COMM_WORLD);); (25, MPI, MPI_Init(&argc,&argv);); (80, MPI, MPI_Bcast(&vector_length, 1, MPI_INT, root, MPI_COMM_WORLD);); (81, MPI, MPI_Bcast(&btype,         1, MPI_INT, root, MPI_COMM_WORLD);); (97, MPI, MPI_Barrier(MPI_COMM_WORLD);); (23, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (21, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(24, MPI, err = MPI_Alltoall(recv_size, 1, MPI_COUNT, send_size, 1, MPI_COUNT, fd->comm);); (87, MPI, MPI_Alltoall(send_size, 1, MPI_COUNT, recv_size, 1, MPI_COUNT, fd->comm);); (22, MPI, int ret = MPI_Alltoall(send_size, 1, MPI_COUNT, recv_size, 1, MPI_COUNT, fd->comm);); (80, MPI, MPI_Alltoallv_c(all_send_buf, send_size, sdispls, MPI_BYTE, all_recv_buf, recv_size, rdispls, MPI_BYTE, fd->comm);); (109, MPI, MPI_Alltoall(send_size, 1, MPI_COUNT, recv_size, 1, MPI_COUNT, fd->comm);); (21, MPI, MPI_Alltoall(recv_size, 1, MPI_COUNT, send_size, 1, MPI_COUNT, fd->comm);); (86, MPI, MPI_Alltoallv_c(all_send_buf, send_size, sdispls, MPI_BYTE, all_recv_buf, recv_size, rdispls, MPI_BYTE, fd->comm);)", "label": "(121, MPI, gpfsmpio_prof_cw[GPFSMPIO_CIO_T_DEXCH_SORT] += MPI_Wtime() - io_time;); (90, MPI, gpfsmpio_prof_cw[GPFSMPIO_CIO_T_DEXCH_NET] += MPI_Wtime() - io_time;); (23, MPI, int ret = MPI_Alltoall(recv_size, 1, MPI_COUNT, send_size, 1, MPI_COUNT, fd->comm);); (19, MPI, io_time = MPI_Wtime();); (24, MPI, gpfsmpio_prof_cw[GPFSMPIO_CIO_T_DEXCH_RECV_EXCH] += MPI_Wtime() - io_time;); (132, MPI, gpfsmpio_prof_cw[GPFSMPIO_CIO_T_DEXCH_SIEVE] += MPI_Wtime() - io_time;); (75, MPI, gpfsmpio_prof_cw[GPFSMPIO_CIO_T_DEXCH_SETUP] += MPI_Wtime() - io_time;)"}
{"predict": "(44, MPI, MPI_Buffer_detach(&bptr, &bl);); (24, MPI, MPI_Irecv(b, 10, MPI_INT, i, 27 + j, MPI_COMM_WORLD, &request);); (14, MPI, MPI_Buffer_attach(buf, BUFSIZE);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (13, MPI, MPI_Bsend(a, 10, MPI_INT, 0, 27 + j, MPI_COMM_WORLD);); (21, MPI, MPI_Wait(&request, &status);)", "label": "(45, MPI, MPI_Buffer_detach(&bptr, &bl);); (25, MPI, MPI_Recv(b, 10, MPI_INT, i, 27 + j, MPI_COMM_WORLD, &status);); (8, MPI, MPI_Buffer_attach(buf, BUFSIZE);); (14, MPI, MPI_Wait(&request, &status);); (12, MPI, MPI_Ibsend(a, 10, MPI_INT, 0, 27 + j, MPI_COMM_WORLD, &request);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)"}
{"predict": "(42, OpenMP, #pragma omp for lastprivate(conditional: s,argc) lastprivate(conditional:); (75, OpenMP, #pragma omp parallel private(i)); (46, OpenMP, #pragma omp for lastprivate(S1)); (14, OpenMP, #pragma omp for lastprivate()); (54, OpenMP, #pragma omp for lastprivate(h)); (4, OpenMP, #pragma omp for lastprivate); (67, OpenMP, #pragma omp for lastprivate(j)); (46, OpenMP, #pragma omp for lastprivate(argv[1])); (23, OpenMP, #pragma omp for lastprivate(argc > 0 ? argv[1] : argv[2])); (58, OpenMP, #pragma omp parallel shared(i)); (14, OpenMP, #pragma omp for lastprivate(argc,); (27, OpenMP, #pragma omp for lastprivate(argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (2, OpenMP, #pragma omp for lastprivate(); (22, OpenMP, #pragma omp for lastprivate(argc) allocate(omp_thread_mem_alloc: argc)); (33, OpenMP, #pragma omp for lastprivate(k, a, b)); (-7, OpenMP, #pragma omp parallel); (49, OpenMP, #pragma omp for lastprivate(i)); (61, OpenMP, #pragma omp for lastprivate(i = 2)); (36, OpenMP, #pragma omp for lastprivate(e, g)); (23, OpenMP, #pragma omp for lastprivate(conditional: argc) lastprivate(conditional:); (53, OpenMP, #pragma omp parallel reduction(+ : i)); (43, OpenMP, #pragma omp for lastprivate)", "label": "(32, OpenMP, #pragma omp for lastprivate(argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (73, OpenMP, #pragma omp parallel private(i)); (22, OpenMP, #pragma omp for lastprivate(argc,); (45, OpenMP, #pragma omp for lastprivate(a, b)); (12, OpenMP, #pragma omp for lastprivate()); (2, OpenMP, #pragma omp parallel); (69, OpenMP, #pragma omp for lastprivate(j)); (60, OpenMP, #pragma omp for lastprivate(i)); (4, OpenMP, #pragma omp for lastprivate(); (11, OpenMP, #pragma omp for lastprivate(argc); (30, OpenMP, #pragma omp for lastprivate(foo:argc)); (-3, OpenMP, #pragma omp for lastprivate); (32, OpenMP, #pragma omp for lastprivate(conditional: argc,s) lastprivate(conditional:); (15, OpenMP, #pragma omp for lastprivate(argc > 0 ? argv[1] : argv[2])); (46, OpenMP, #pragma omp for lastprivate(h)); (37, OpenMP, #pragma omp for lastprivate(argv[1])); (40, OpenMP, #pragma omp for lastprivate(k, e, g)); (56, OpenMP, #pragma omp parallel shared(i)); (18, OpenMP, #pragma omp for lastprivate(S1))"}
{"predict": "(23, CUDA, Sema::CUDAVarTarget VarTarget = SemaRef.getCUDAVarTarget(Var);); (23, CUDA, Sema::CUDAUserTarget UserTarget = SemaRef.getCUDAUserTarget(Loc);)", "label": "(25, CUDA, auto UserTarget = SemaRef.IdentifyCUDATarget(FD);); (23, CUDA, auto VarTarget = SemaRef.IdentifyCUDATarget(Var);)"}
{"predict": "(33, MPI, MPI_Win_fence(0, win);); (45, MPI, MPI_Win_unlock(0, win);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (12, MPI, MPI_Win_create(data, sizeof(twoint_t), sizeof(int), MPI_INFO_NULL, MPI_COMM_WORLD, &win);); (30, MPI, MPI_Accumulate(&mine, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_MAXLOC, win);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (111, MPI, MPI_Win_free(&win);); (108, MPI, MPI_Win_detach(win, data);); (73, MPI, MPI_Win_fence(MPI_MODE_NOPRECEDE, win);); (93, MPI, MPI_Win_lock(MPI_LOCK_SHARED, 0, MPI_MODE_NOCHECK, win);); (41, MPI, MPI_Win_fence(MPI_MODE_NOSUCCEED, win);); (37, MPI, MPI_Accumulate(&mine, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_MINLOC, win);); (108, MPI, MPI_Free_mem(data);); (2, MPI, MPI_Alloc_mem(sizeof(twoint_t), MPI_INFO_NULL, &data);); (23, MPI, MPI_Win_lock(MPI_LOCK_SHARED, 0, MPI_MODE_NOCHECK, win);); (18, MPI, MPI_Win_unlock(0, win);); (58, MPI, MPI_Win_detach(win, data);); (107, MPI, MPI_Barrier(MPI_COMM_WORLD);); (58, MPI, MPI_Win_create(data, sizeof(twoint_t), sizeof(int), MPI_INFO_NULL, MPI_COMM_WORLD, &win);)", "label": "(119, MPI, MPI_Free_mem(data);); (30, MPI, MPI_Win_fence(0, win);); (73, MPI, MPI_Win_fence(MPI_MODE_NOSUCCEED, win);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (12, MPI, MPI_Win_fence(MPI_MODE_NOPRECEDE, win);); (110, MPI, MPI_Win_free(&win);); (28, MPI, MPI_Accumulate(&mine, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_MAXLOC, win);); (5, MPI, MPI_Alloc_mem(sizeof(twoint_t), MPI_INFO_NULL, &data);); (71, MPI, MPI_Win_lock(MPI_LOCK_SHARED, 0, MPI_MODE_NOCHECK, win);); (72, MPI, MPI_Win_unlock(0, win);); (73, MPI, MPI_Barrier(MPI_COMM_WORLD);); (4, MPI, MPI_Win_create(data, me == 0 ? sizeof(twoint_t) : 0, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &win);); (41, MPI, MPI_Accumulate(&mine, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_MINLOC, win);)"}
{"predict": "(44, SYCL, double *d_A = sycl::malloc_device<double>(length, q);); (53, SYCL, double *d_C = sycl::malloc_device<double>(length, q);); (78, SYCL, sycl::free(d_A, q);); (30, SYCL, prk::SYCL::print_device_platform(q);); (60, SYCL, sycl::free(d_B, q);); (77, SYCL, sycl::free(d_C, q);); (48, SYCL, double *d_B = sycl::malloc_device<double>(length, q);); (24, SYCL, sycl::queue q(sycl::default_selector_v);); (45, SYCL, double *d_A = sycl::malloc_device<double>(length, q);)", "label": "(80, SYCL, sycl::free(d_A, q);); (32, SYCL, prk::SYCL::print_device_platform(q);); (51, SYCL, double *d_C = sycl::malloc_device<double>(length, q);); (76, SYCL, sycl::free(d_B, q);); (74, SYCL, sycl::free(d_C, q);); (47, SYCL, double *d_B = sycl::malloc_device<double>(length, q);); (26, SYCL, sycl::queue q(sycl::default_selector_v);); (44, SYCL, double *d_A = sycl::malloc_device<double>(length, q);)"}
{"predict": "(64, OpenMP, #pragma omp distribute parallel for collapse(2)); (146, OpenMP, #pragma omp target teams map (alloc: u, qs, rho_i) \\\n(24, OpenMP, #pragma omp target teams map (alloc: rsd, rho_i, u, qs) \\\n    num_teams(nz)); (28, OpenMP, #pragma omp distribute parallel for collapse(3)); (85, OpenMP, #pragma omp distribute parallel for collapse(2) private (k, j, i, tmp, u21i, u31i, u41i, u51i, u21im1, u31im1, u41im1, u51im1)); (173, OpenMP, #pragma omp target teams map (alloc: u, rsd, rho_i, qs) \\\n(98, OpenMP, #pragma omp distribute parallel for collapse(2)); (123, OpenMP, #pragma omp target teams map (alloc: flux_G, rsd, u, rho_i) \\\n(50, OpenMP, #pragma omp target teams map (alloc: rho_i, u, qs, flux_G) \\\n    num_teams(nz-2) thread_limit(num_workers3)); (133, OpenMP, #pragma omp target teams map (alloc: qs, rho_i, u, flux_G) \\\n(191, OpenMP, #pragma omp target update of(u[0:5][0:2][0:2][0:2], qs[0:2][0:2][0:2], rho_i[0:2][0:2][0:2])); (18, OpenMP, #pragma omp distribute parallel for collapse(2) private (tmp)); (119, OpenMP, #pragma omp target teams map (alloc: u, rho_i, qs, flux_G) \\\n(137, OpenMP, #pragma omp target teams map (alloc: u, rho_i, qs) \\\n(46, OpenMP, #pragma omp target teams map (alloc: u, rho_i) \\\n    num_teams(nz-2)); (179, OpenMP, #pragma omp target update of(rsd[0:2][0:2][0:2][0:2", "label": "(569, OpenMP, #pragma omp teams distribute parallel for collapse(3) private(i,j,k) num_teams(jend - jst)); (26, OpenMP, #pragma omp distribute parallel for collapse (3) private(tmp)); (106, OpenMP, #pragma omp simd simdlen(32) private (tmp, u21i, u31i, u41i, u51i, u21im1, u31im1, u41im1, u51im1)); (21, OpenMP, #pragma omp target teams); (15, OpenMP, #pragma omp target data); (59, OpenMP, #pragma omp distribute)"}
{"predict": "(156, OpenMP, ism303dac_mg_offset_temp_comp_set(&dev_ctx_mg, PROPERTY_ENABLE);)", "label": "(162, OpenMP, ism303dac_mg_offset_temp_comp_set(&dev_ctx_mg, PROPERTY_ENABLE);)"}
{"predict": "(14, OpenMP, #pragma omp teams distribute parallel for simd shared ()); (34, OpenMP, #pragma omp teams distribute parallel for simd shared (argv[1])); (51, OpenMP, #pragma omp teams distribute parallel for simd private(i), shared(i)); (14, OpenMP, #pragma omp teams distribute parallel for simd shared (argc); (3, OpenMP, #pragma omp target); (42, OpenMP, #pragma omp teams distribute parallel for simd shared(e, g)); (59, OpenMP, #pragma omp teams distribute parallel for simd shared(j)); (4, OpenMP, #pragma omp teams distribute parallel for simd shared (); (12, OpenMP, #pragma omp teams distribute parallel for simd shared (argc,); (14, OpenMP, #pragma omp teams distribute parallel for simd shared (argc > 0 ? argv[1] : argv[2])); (31, OpenMP, #pragma omp teams distribute parallel for simd shared(ca)); (45, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(i), shared(i)); (32, OpenMP, #pragma omp teams distribute parallel for simd shared(da)); (49, OpenMP, #pragma omp teams distribute parallel for simd shared(i)); (18, OpenMP, #pragma omp teams distribute parallel for simd shared (a, b, c, d, f)); (53, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(i)); (43, OpenMP, #pragma omp teams distribute parallel for simd private(i)); (17, OpenMP, #pragma omp teams distribute parallel for simd shared(ba)); (8, OpenMP, #pragma omp teams distribute parallel for simd shared (argc, z)); (18, OpenMP, #pragma omp teams distribute parallel for simd shared (S1)); (-11, OpenMP, #pragma omp teams distribute parallel for simd shared); (27, OpenMP, #pragma omp teams distribute parallel for simd shared(h, B::x)); (-1, OpenMP, #pragma omp teams distribute parallel for simd shared (argc); (35, OpenMP, #pragma omp teams distribute parallel for simd shared(si))", "label": "(11, OpenMP, #pragma omp teams distribute simd shared (); (43, OpenMP, #pragma omp teams distribute simd shared(da)); (30, OpenMP, #pragma omp teams distribute simd shared (a, b, c, d, f)); (5, OpenMP, #pragma omp teams distribute simd shared); (34, OpenMP, #pragma omp teams distribute simd shared(ba)); (2, OpenMP, #pragma omp target); (35, OpenMP, #pragma omp teams distribute simd shared(ca)); (55, OpenMP, #pragma omp teams distribute simd shared(i)); (57, OpenMP, #pragma omp teams distribute simd shared(j)); (17, OpenMP, #pragma omp teams distribute simd shared (argc)); (43, OpenMP, #pragma omp teams distribute simd private(i), shared(i)); (24, OpenMP, #pragma omp teams distribute simd shared (argv[1])); (17, OpenMP, #pragma omp teams distribute simd shared (S1)); (43, OpenMP, #pragma omp teams distribute simd firstprivate(i), shared(i)); (9, OpenMP, #pragma omp teams distribute simd shared (argc > 0 ? argv[1] : argv[2])); (35, OpenMP, #pragma omp teams distribute simd shared(h, B::x)); (43, OpenMP, #pragma omp teams distribute simd private(i)); (51, OpenMP, #pragma omp teams distribute simd firstprivate(i)); (2, OpenMP, #pragma omp teams distribute simd shared (argc,); (28, OpenMP, #pragma omp teams distribute simd shared(e, g, z)); (-6, OpenMP, #pragma omp teams distribute simd shared ()); (-4, OpenMP, #pragma omp teams distribute simd shared (argc)"}
{"predict": "(25, MPI, MPI_Init(&argc, &argv);); (201, MPI, MPI_Finalize();); (25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myid);); (209, MPI, MPI_Barrier(MPI_COMM_WORLD);); (23, MPI, MPI_Comm_size(MPI_COMM_WORLD, &num_procs);)", "label": "(25, MPI, MPI_Init(&argc, &argv);); (79, MPI, MPI_Finalize();); (25, MPI, MPI_Comm_size(MPI_COMM_WORLD, &num_procs);); (23, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myid);)"}
{"predict": "(70, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(h)); (45, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argv[1])); (7, OpenMP, #pragma omp target); (63, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(S2::S2sc)); (14, OpenMP, #pragma omp teams distribute parallel for simd firstprivate ()); (9, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (); (72, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(i)); (43, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(ba)); (16, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argc,); (19, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argc > 0 ? argv[1] : argv[2])); (59, OpenMP, #pragma omp teams distribute parallel for simd private(i), firstprivate(i)); (66, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(j)); (20, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (26, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (S1)); (29, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (a, b, c, d, f)); (40, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(da, z)); (43, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(S2::S2s)); (-1, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argc); (70, OpenMP, #pragma omp teams distribute parallel for simd lastprivate(argc), firstprivate(argc)); (57, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(i) map(i)); (26, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argv[1])); (36, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(ca)); (-9,", "label": "(34, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (85, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(j)); (7, OpenMP, #pragma omp target); (59, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(S2::S2s)); (10, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (); (21, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argc,); (36, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (a, b, c, d, f)); (47, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(ca)); (66, OpenMP, #pragma omp teams distribute parallel for simd private(i), firstprivate(i)); (13, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argc); (40, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(ba, z)); (59, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(h)); (26, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (S1)); (33, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argv[1])); (52, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(S2::S2sc)); (63, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(i)); (2, OpenMP, #pragma omp teams distribute parallel for simd firstprivate ()); (-7, OpenMP, #pragma omp teams distribute parallel for simd firstprivate); (40, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(da)); (72, OpenMP, #pragma omp teams distribute parallel for simd lastprivate(argc), firstprivate(argc)); (10, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argc > 0 ? argv[1] : argv[2]))"}
{"predict": "(20, MPI, tim::settings::mpi_init()       = false;); (20, MPI, tim::settings::mpi_finalize()   = false;)", "label": "(20, MPI, tim::settings::mpi_init()       = false;); (20, MPI, tim::settings::mpi_finalize()   = false;)"}
{"predict": "(32, CUDA, checkCudaErrors(cudaMallocHost(&b, N_ROWS * sizeof(double)));); (27, CUDA, int dev = findCudaDevice(argc, (const char **)argv);); (67, CUDA, checkCudaErrors(cudaMalloc(&d_A, sizeof(float) * N_ROWS * N_ROWS));); (97, CUDA, checkCudaErrors(cudaFree(d_x_new));); (99, CUDA, checkCudaErrors(cudaFreeHost(b));); (93, CUDA, checkCudaErrors(cudaFree(d_A));); (64, CUDA, checkCudaErrors(cudaMalloc(&d_x, sizeof(double) * N_ROWS));); (62, CUDA, checkCudaErrors(cudaMalloc(&d_b, sizeof(double) * N_ROWS));); (60, CUDA, checkCudaErrors(cudaStreamCreateWithFlags(&stream1, cudaStreamNonBlocking));); (91, CUDA, checkCudaErrors(cudaFree(d_x));); (57, CUDA, checkCudaErrors(cudaMalloc(&d_x_new, sizeof(double) * N_ROWS));); (59, CUDA, checkCudaErrors(cudaMemsetAsync(d_x, 0, sizeof(double) * N_ROWS, stream1));); (24, CUDA, checkCudaErrors(cudaMallocHost(&A, N_ROWS * N_ROWS * sizeof(float)));); (90, CUDA, checkCudaErrors(cudaFree(d_b));); (88, CUDA, checkCudaErrors(cudaFree(d_x_new));); (86, CUDA, checkCudaErrors(cudaFreeHost(A));)", "label": "(32, CUDA, checkCudaErrors(cudaMallocHost(&b, N_ROWS * sizeof(double)));); (27, CUDA, int dev = findCudaDevice(argc, (const char **)argv);); (67, CUDA, checkCudaErrors(cudaMalloc(&d_A, sizeof(float) * N_ROWS * N_ROWS));); (99, CUDA, checkCudaErrors(cudaFree(d_x_new));); (101, CUDA, checkCudaErrors(cudaFreeHost(b));); (95, CUDA, checkCudaErrors(cudaFree(d_A));); (64, CUDA, checkCudaErrors(cudaMalloc(&d_x, sizeof(double) * N_ROWS));); (92, CUDA, checkCudaErrors(cudaFree(d_b));); (93, CUDA, checkCudaErrors(cudaFree(d_x));); (59, CUDA, checkCudaErrors(cudaMalloc(&d_b, sizeof(double) * N_ROWS));); (57, CUDA, checkCudaErrors(cudaStreamCreateWithFlags(&stream1, cudaStreamNonBlocking));); (60, CUDA, checkCudaErrors(cudaMalloc(&d_x_new, sizeof(double) * N_ROWS));); (61, CUDA, checkCudaErrors(cudaMemsetAsync(d_x, 0, sizeof(double) * N_ROWS, stream1));); (21, CUDA, checkCudaErrors(cudaMallocHost(&A, N_ROWS * N_ROWS * sizeof(float)));); (90, CUDA, checkCudaErrors(cudaFreeHost(A));)"}
{"predict": "(90, MPI, MPI_Allreduce( &problem, &any_problem, 1, MPI_INT, MPI_LOR, smpi->world() );)", "label": "(96, MPI, MPI_Finalize();); (88, MPI, MPI_Allreduce( &problem, &any_problem, 1, MPI_INT, MPI_LOR, smpi->world() );)"}
{"predict": "(4, CUDA, __shared__ float4 buffer2[M7H*NUM];); (2, CUDA, __shared__ float4 buffer1[M7W*NDIM/4];)", "label": "(5, CUDA, __shared__ float4 buffer2[M7H*NUM];); (3, CUDA, __shared__ float4 buffer1[M7W*NDIM/4];)"}
{"predict": "(103, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (107, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (116, MPI, MPI_Barrier(comm);); (-1, MPI, MPI_Init(&argc, &argv);); (99, CUDA, CUDA_CHECK(cudaMemcpyAsync(src, src_host.data(), src_host.size() * sizeof(int), cudaMemcpyDefault, stream));); (71, CUDA, CUDA_CHECK(cudaStreamCreate(&stream));); (133, MPI, MPI_Finalize();); (87, MPI, MPI_Allreduce(&my_io_max, &global_io_max, 1, MPI_UINT64_T, MPI_MAX, comm);); (96, CUDA, CUDA_CHECK(cudaMemsetAsync(dst, 0, global_io_max * sizeof(int), stream));); (-4, MPI, MPI_Comm_size(comm, &size);); (125, CUDA, CUDA_CHECK(cudaFree(dst));); (96, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (-7, MPI, MPI_Comm_rank(comm, &rank);); (121, CUDA, CUDA_CHECK(cudaFree(src));); (127, MPI, MPI_Barrier(comm);)", "label": "(15, CUDA, CUDA_CHECK(cudaGetDeviceCount(&ndevices));); (77, CUDA, CUDA_CHECK(cudaStreamCreate(&stream));); (5, MPI, MPI_Comm_size(comm, &size);); (119, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (12, CUDA, CUDA_CHECK(cudaSetDevice(rank % ndevices));); (-3, MPI, MPI_Init(&argc, &argv);); (90, MPI, MPI_Allreduce(&my_io_max, &global_io_max, 1, MPI_UNSIGNED_LONG_LONG, MPI_MAX, comm);); (116, CUDA, CUDA_CHECK(cudaMemcpy(dst_host.data(), dst, dst_host_expected.size() * sizeof(int), cudaMemcpyDefault));); (139, MPI, MPI_Finalize();); (118, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (94, CUDA, CUDA_CHECK(cudaMemsetAsync(dst, 0, dst_host_expected.size(), stream));); (92, CUDA, CUDA_CHECK(cudaMemcpyAsync(src, src_host.data(), src_host.size() * sizeof(int), cudaMemcpyDefault, stream));); (-6, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(11, TBB, tbb::flow::multifunction_node< InputType, OutputType > exe_node( g, lc, cf );); (49, TBB, tbb::flow::remove_edge( tbb::flow::output_port<0>(exe_node), *receivers[r] );); (64, TBB, g.reset(tbb::flow::rf_reset_bodies);); (20, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(exe_node), *receivers[r] );); (60, TBB, inc_functor body_copy = tbb::flow::copy_body<inc_functor>(exe_node);); (25, TBB, tbb::flow::make_edge( *senders.back(), exe_node );); (-1, TBB, tbb::flow::graph g;)", "label": "(66, TBB, g.reset(tbb::flow::rf_reset_bodies);); (20, TBB, tbb::flow::make_edge( exe_node, *receivers[r] );); (58, TBB, inc_functor body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (64, TBB, body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (1, TBB, tbb::flow::graph g;); (45, TBB, tbb::flow::remove_edge( exe_node, *receivers[r] );); (24, TBB, tbb::flow::make_edge( *senders.back(), exe_node );); (4, TBB, tbb::flow::function_node< InputType, OutputType > exe_node( g, lc, cf );)"}
{"predict": "(44, OpenCL, clReleaseCommandQueue(queue);); (28, OpenCL, printf( \"clCreateContext() failed with %d\\n\", err );); (85, OpenCL, clReleaseMemObject(bufX);); (85, OpenCL, clReleaseMemObject(bufA);); (79, OpenCL, clReleaseEvent(event);); (30, OpenCL, printf( \"clCreateCommandQueue() failed with %d\\n\", err );); (27, OpenCL, queue = clCreateCommandQueue(ctx, device, 0, &err);); (29, OpenCL, clReleaseContext(ctx);); (78, OpenCL, clReleaseMemObject(bufY);); (18, OpenCL, ctx = clCreateContext(props, 1, &device, NULL, NULL, &err);); (63, OpenCL, err = clWaitForEvents(1, &event);)", "label": "(87, OpenCL, clReleaseMemObject(bufX);); (43, OpenCL, clReleaseCommandQueue(queue);); (27, OpenCL, printf( \"clCreateContext() failed with %d\\n\", err );); (85, OpenCL, clReleaseMemObject(bufA);); (79, OpenCL, clReleaseEvent(event);); (30, OpenCL, printf( \"clCreateCommandQueue() failed with %d\\n\", err );); (27, OpenCL, queue = clCreateCommandQueue(ctx, device, 0, &err);); (29, OpenCL, clReleaseContext(ctx);); (78, OpenCL, clReleaseMemObject(bufY);); (18, OpenCL, ctx = clCreateContext(props, 1, &device, NULL, NULL, &err);); (63, OpenCL, err = clWaitForEvents(1, &event);)"}
{"predict": "(6, OpenMP, #pragma omp parallel master taskloop simd linear (); (21, OpenMP, #pragma omp parallel master taskloop simd linear (argc : 5) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (38, OpenMP, #pragma omp parallel master taskloop simd linear(uval(i))); (22, OpenMP, #pragma omp parallel master taskloop simd linear (S1)); (16, OpenMP, #pragma omp parallel master taskloop simd linear (argc,); (2, OpenMP, #pragma omp parallel master taskloop simd linear ()); (23, OpenMP, #pragma omp parallel master taskloop simd linear (argv[1])); (32, OpenMP, #pragma omp parallel master taskloop simd linear(ref(e, g))); (37, OpenMP, #pragma omp parallel master taskloop simd linear(i)); (23, OpenMP, #pragma omp parallel master taskloop simd linear(v:16)); (3, OpenMP, #pragma omp parallel master taskloop simd linear (argc); (15, OpenMP, #pragma omp parallel master taskloop simd linear (a, b : B::ib)); (7, OpenMP, #pragma omp parallel master taskloop simd linear (argc > 0 ? argv[1] : argv[2])); (31, OpenMP, #pragma omp parallel master taskloop simd linear(v:j)); (9, OpenMP, #pragma omp parallel master taskloop simd linear (argc, z : 5)); (-8, OpenMP, #pragma omp parallel); (-1, OpenMP, #pragma omp parallel master taskloop simd linear (argc,)", "label": "(17, OpenMP, #pragma omp parallel for simd linear (argc : 5) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (39, OpenMP, #pragma omp parallel for simd linear(j)); (41, OpenMP, #pragma omp parallel for simd linear(v:j)); (34, OpenMP, #pragma omp parallel for simd linear(v:i)); (27, OpenMP, #pragma omp parallel for simd linear(i, z)); (4, OpenMP, #pragma omp parallel for simd linear ()); (13, OpenMP, #pragma omp parallel for simd linear (S1)); (6, OpenMP, #pragma omp parallel for simd linear (argc,); (-3, OpenMP, #pragma omp parallel); (36, OpenMP, #pragma omp parallel for simd linear(i)); (17, OpenMP, #pragma omp parallel for simd linear(e, g)); (12, OpenMP, #pragma omp parallel for simd linear (a, b:B::ib)); (-1, OpenMP, #pragma omp parallel for simd linear (argc); (12, OpenMP, #pragma omp parallel for simd linear (argv[1])); (15, OpenMP, #pragma omp parallel for simd linear(h)); (-8, OpenMP, #pragma omp parallel for simd linear (); (-1, OpenMP, #pragma omp parallel for simd linear (argc > 0 ? argv[1] : argv[2]))"}
{"predict": "(49, MPI, MPI_Allreduce(MPI_IN_PLACE,&n,1,mpi_int_t,MPI_MAX,grid->comm);); (47, MPI, MPI_Allreduce(MPI_IN_PLACE,&nnzL,1,mpi_int_t,MPI_SUM,grid->comm);)", "label": "(49, MPI, MPI_Allreduce(MPI_IN_PLACE,&n,1,mpi_int_t,MPI_MAX,grid->comm);); (47, MPI, MPI_Allreduce(MPI_IN_PLACE,&nnzL,1,mpi_int_t,MPI_SUM,grid->comm);)"}
{"predict": "(17, MPI, MPI_Comm_size(MPI_COMM_WORLD,&totpes);); (39, MPI, MPI_Cart_create(MPI_COMM_WORLD,3,numpes,isperiodic,reorder,&comm_cart);); (39, MPI, MPI_Comm_rank(comm_cart,&mype);); (20, MPI, MPI_Finalize();); (17, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&rank);); (72, MPI, MPI_Barrier(comm_cart);); (36, MPI, MPI_Cart_coords(comm_cart,mype,3,pe_coords);); (93, MPI, MPI_Allreduce(rates_l,rates_g,4,MPI_DOUBLE,MPI_MIN,comm_cart);); (31, MPI, MPI_Dims_create(totpes,3,numpes);); (18, MPI, MPI_Bcast(filename, 256, MPI_CHAR, 0, MPI_COMM_WORLD);); (113, MPI, MPI_Allreduce(MPI_IN_PLACE,&nerrs,1,MPI_INT,MPI_SUM, MPI_COMM_WORLD);); (2, MPI, MPI_Init(&argc,&argv);); (100, MPI, MPI_Comm_free(&comm_cart);); (87, MPI, MPI_Allreduce(rates_l,rates_g,4,MPI_DOUBLE,MPI_MAX,comm_cart);); (103, MPI, MPI_Reduce(&malloc_size, &sum_size, 1, MPI_OFFSET, MPI_SUM, 0, MPI_COMM_WORLD);)", "label": "(17, MPI, MPI_Comm_size(MPI_COMM_WORLD,&totpes);); (39, MPI, MPI_Cart_create(MPI_COMM_WORLD,3,numpes,isperiodic,reorder,&comm_cart);); (39, MPI, MPI_Comm_rank(comm_cart,&mype);); (19, MPI, MPI_Finalize();); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&rank);); (72, MPI, MPI_Barrier(comm_cart);); (36, MPI, MPI_Cart_coords(comm_cart,mype,3,pe_coords);); (94, MPI, MPI_Allreduce(t, t_g, 20, MPI_DOUBLE, MPI_MAX, comm_cart);); (31, MPI, MPI_Dims_create(totpes,3,numpes);); (18, MPI, MPI_Bcast(filename, 256, MPI_CHAR, 0, MPI_COMM_WORLD);); (115, MPI, MPI_Allreduce(MPI_IN_PLACE, &nerrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (5, MPI, MPI_Init(&argc,&argv);); (102, MPI, MPI_Comm_free(&comm_cart);); (87, MPI, MPI_Allreduce(rates_l, rates_g, 4, MPI_DOUBLE, MPI_MIN, comm_cart);); (105, MPI, MPI_Reduce(&malloc_size, &sum_size, 1, MPI_OFFSET, MPI_SUM, 0, MPI_COMM_WORLD);)"}
{"predict": "(116, MPI, MPI_Type_commit(&mpi_word);); (122, MPI, MPI_Allgather(iterstring,1,mpi_word, catstring,1,mpi_word, MPI_COMM_WORLD);); (155, MPI, MPI_Finalize();); (63, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (111, MPI, MPI_Type_contiguous(proc_length,MPI_CHAR, &mpi_word);); (18, MPI, MPI_Init(&argc,&argv);); (60, MPI, MPI_Bcast(&length,     1, MPI_LONG, root, MPI_COMM_WORLD);); (111, MPI, MPI_Barrier(MPI_COMM_WORLD);); (17, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (15, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)", "label": "(115, MPI, MPI_Type_commit(&mpi_word);); (122, MPI, MPI_Allgather(iterstring,1,mpi_word, catstring,1,mpi_word, MPI_COMM_WORLD);); (154, MPI, MPI_Finalize();); (63, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (110, MPI, MPI_Type_contiguous(proc_length,MPI_CHAR, &mpi_word);); (17, MPI, MPI_Init(&argc,&argv);); (61, MPI, MPI_Bcast(&length,     1, MPI_LONG, root, MPI_COMM_WORLD);); (110, MPI, MPI_Barrier(MPI_COMM_WORLD);); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(39, TBB, tbb::flow::make_edge(buffer1, tbb::flow::input_port<0>(join));); (21, TBB, tbb::flow::make_edge(buffer3, tbb::flow::input_port<2>(join));); (38, TBB, tbb::flow::make_edge(function, join);); (14, TBB, tbb::flow::queue_node<int> buffer2(g);); (39, TBB, tbb::flow::make_edge(buffer2, tbb::flow::input_port<1>(join));); (-3, TBB, tbb::task_arena arena(1);); (15, TBB, tbb::flow::join_node<tuple_type, tbb::flow::reserving> join(g);); (9, TBB, tbb::flow::queue_node<int> buffer1(g);); (-5, TBB, tbb::flow::graph g;); (13, TBB, tbb::flow::make_edge(buffer1, tbb::flow::input_port<0>(join));); (34, TBB, tbb::flow::make_edge(join, function);); (8, TBB, tbb::flow::queue_node<int> buffer3(g);)", "label": "(42, TBB, tbb::flow::make_edge(buffer2, tbb::flow::input_port<1>(join));); (43, TBB, tbb::flow::make_edge(join, function);); (19, TBB, tbb::flow::join_node<tuple_type, tbb::flow::reserving> join(g);); (38, TBB, tbb::flow::make_edge(buffer1, tbb::flow::input_port<0>(join));); (-3, TBB, tbb::task_arena arena(1);); (-1, TBB, tbb::flow::graph g;); (11, TBB, tbb::flow::queue_node<int> buffer1(g);); (12, TBB, tbb::flow::queue_node<int> buffer3(g);); (35, TBB, tbb::flow::make_edge(buffer3, tbb::flow::input_port<2>(join));); (9, TBB, tbb::flow::queue_node<int> buffer2(g);)"}
{"predict": "(18, OpenMP, varLimit[i] = omp_get_var_limit(0);); (12, OpenMP, #pragma omp target teams distribute thread_limit(thdLim)); (26, OpenMP, varLimitHuge[i] = omp_get_var_limit(0);); (42, OpenMP, constLimitHuge[i] = omp_get_const_limit(0);); (32, OpenMP, constLimit[i] = omp_get_const_limit(0);); (11, OpenMP, #pragma omp parallel for)", "label": "(13, OpenMP, fprintf(stderr, \"#pragma omp target teams distribute thread_limit(thdLim) 27\\n\");); (43, OpenMP, #pragma omp target teams distribute thread_limit(1024)); (22, OpenMP, fprintf(stderr, \"#pragma omp target teams distribute thread_limit(thdLim) 1024\\n\");); (31, OpenMP, #pragma omp target teams distribute thread_limit(27)); (34, OpenMP, constLimit[i] = omp_get_num_threads();); (9, OpenMP, #pragma omp target teams distribute thread_limit(thdLim)); (42, OpenMP, constLimitHuge[i] = omp_get_num_threads();); (21, OpenMP, varLimitHuge[i] = omp_get_num_threads();); (9, OpenMP, varLimit[i] = omp_get_num_threads();)"}
{"predict": "(27, MPI, MPI_Gather( vecin, 1, vec, vecout, n, MPI_DOUBLE, root, comm );); (10, MPI, MPI_Init( &argc, &argv );); (45, MPI, MPI_Gather( NULL, 0, MPI_BYTE, NULL, 0, MPI_BYTE, 0, MPI_COMM_WORLD );); (9, MPI, MPI_Comm_size( comm, &size );); (21, MPI, MPI_Type_vector( n, 1, stride, MPI_DOUBLE, &vec );); (5, MPI, MPI_Comm_rank( comm, &rank );); (37, MPI, MPI_Type_free( &vec );); (42, MPI, MPI_Finalize();); (17, MPI, MPI_Type_commit( &vec );)", "label": "(13, MPI, MPI_Comm_size( comm, &size );); (22, MPI, MPI_Type_commit( &vec );); (26, MPI, MPI_Gather( vecin, 1, vec, vecout, n, MPI_DOUBLE, root, comm );); (46, MPI, MPI_Finalize();); (5, MPI, MPI_Init( &argc, &argv );); (35, MPI, MPI_Type_free( &vec );); (6, MPI, MPI_Comm_rank( comm, &rank );); (15, MPI, MPI_Type_vector( n, 1, stride, MPI_DOUBLE, &vec );); (39, MPI, MPI_Gather( NULL, 0, MPI_BYTE, NULL, 0, MPI_BYTE, 0, MPI_COMM_WORLD );)"}
{"predict": "(18, MPI, MPI_Comm_rank(shm_comm, &shm_rank);); (24, MPI, MPI_Win_shared_query(shm_win, 0, &size, &disp_unit, &base);); (15, MPI, MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, rank, MPI_INFO_NULL, &shm_comm);); (11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (60, MPI, MPI_Win_unlock_all(shm_win);); (37, MPI, MPI_Win_lock_all(0, shm_win);); (57, MPI, MPI_Win_free(&shm_win);); (11, MPI, MPI_Comm_size(shm_comm, &shm_nproc);)", "label": "(18, MPI, MPI_Comm_rank(shm_comm, &shm_rank);); (41, MPI, MPI_Win_lock_all(MPI_MODE_NOCHECK, shm_win);); (24, MPI, MPI_Win_shared_query(shm_win, MPI_PROC_NULL, &size, &disp_unit, &base);); (13, MPI, MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, rank, MPI_INFO_NULL, &shm_comm);); (10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (58, MPI, MPI_Win_unlock_all(shm_win);); (58, MPI, MPI_Win_free(&shm_win);); (58, MPI, MPI_Comm_free(&shm_comm);); (10, MPI, MPI_Comm_size(shm_comm, &shm_nproc);); (40, MPI, MPI_Barrier(shm_comm);); (38, MPI, MPI_Win_sync(shm_win);)"}
{"predict": "(107, CUDA, printf(\"cudaGetDeviceProperties returned %d\\n-> %s\\n\", (int)error_id, cudaGetErrorString(error_id));); (56, CUDA, cudaError_t error_id = cudaGetDeviceCount(&deviceCount);); (81, CUDA, checkCudaErrors(cudaSetDevice(currentDevice));); (91, CUDA, cudaError_t error_id = cudaGetDeviceProperties(&deviceProp, currentDevice);); (53, CUDA, printf(\"cudaGetDeviceCount returned %d\\n-> %s\\n\", (int)error_id, cudaGetErrorString(error_id));); (97, CUDA, printf(\"cudaGetDeviceProperties returned %d\\n-> %s\\n\", (int)error_id, cudaGetErrorString(error_id));); (105, CUDA, checkCudaErrors(cudaSetDevice(0));)", "label": "(267, CUDA, cudaSetDevice(nDevice);); (108, CUDA, printf(\"cudaGetDeviceProperties returned %d\\n-> %s\\n\", (int)error_id, cudaGetErrorString(error_id));); (54, CUDA, cudaError_t error_id = cudaGetDeviceCount(&deviceCount);); (98, CUDA, fprintf(stderr, \"Error: device is running in <Compute Mode Prohibited>, no threads can use ::cudaSetDevice().\\n\");); (56, CUDA, printf(\"cudaGetDeviceCount returned %d\\n-> %s\\n\", (int)error_id, cudaGetErrorString(error_id));); (88, CUDA, cudaError_t error_id = cudaGetDeviceProperties(&deviceProp, currentDevice);); (96, CUDA, checkCudaErrors(cudaSetDevice(currentDevice));)"}
{"predict": "(18, OpenMP, int id = omp_get_thread_num();); (14, OpenMP, #pragma omp parallel); (6, OpenMP, #pragma omp target map(inParallel, numThreads, threadNum, check1[:], check2[:])); (18, OpenMP, check1[id] += omp_get_num_threads() + omp_in_parallel();); (20, OpenMP, #pragma omp atomic); (1, OpenMP, #pragma omp parallel); (66, OpenMP, #pragma omp target map(inParallel, numThreads, threadNum, check1[:], check2[:]))", "label": "(77, OpenMP, #pragma omp parallel num_threads(NumThreads1)); (14, OpenMP, #pragma omp parallel); (26, OpenMP, int nestedThreadNum = omp_get_thread_num();); (7, OpenMP, inParallel = omp_in_parallel();); (22, OpenMP, int nestedNumThreads = omp_get_num_threads();); (19, OpenMP, int nestedInParallel = omp_in_parallel();); (12, OpenMP, check1[id] += omp_get_num_threads() + omp_in_parallel();); (4, OpenMP, numThreads = omp_get_num_threads();); (21, OpenMP, #pragma omp atomic); (3, OpenMP, threadNum = omp_get_thread_num();); (-2, OpenMP, #pragma omp target map(inParallel, numThreads, threadNum, check1[:], check2[:])); (6, OpenMP, int id = omp_get_thread_num();)"}
{"predict": "(78, CUDA, err = cudaEventRecord(stop, 0);); (85, CUDA, err = cudaEventDestroy(start);); (73, CUDA, err = cudaFree(d_C);); (55, CUDA, fprintf(stderr, \"Failed to copy vector B to device (error code %s)!\\n\", cudaGetErrorString(err));); (81, CUDA, err = cudaEventSynchronize(stop);); (62, CUDA, fprintf(stderr, \"Failed to copy result vector from device to host (error code %s)!\\n\", cudaGetErrorString(err));); (73, CUDA, err = cudaFree(d_B);); (66, CUDA, err = cudaMemcpy(C, d_C, length, cudaMemcpyDeviceToHost);); (79, CUDA, err = cudaEventDestroy(stop);); (9, CUDA, err = cudaEventRecord(start, 0);); (46, CUDA, fprintf(stderr, \"Failed to allocate device vector C (error code %s)!\\n\", cudaGetErrorString(err));); (12, CUDA, err = cudaMalloc(&d_B, length);); (70, CUDA, err = cudaFree(d_A);); (39, CUDA, fprintf(stderr, \"Failed to allocate device vector B (error code %s)!\\n\", cudaGetErrorString(err));); (6, CUDA, err = cudaMalloc(&d_A, length);); (73, CUDA, err = cudaEventElapsedTime(elapsed_time, start, stop);); (30, CUDA, err = cudaMemcpy(d_B, B, length, cudaMemcpyHostToDevice);); (2, CUDA, err = cudaMalloc(&d_C, length);); (35, CUDA, fprintf(stderr, \"Failed to allocate device vector A (error code %s)!\\n\", cudaGetErrorString(err));); (30, CUDA, err = cudaMemcpy(d_A, A, length, cudaMemcpyHostToDevice);); (64, CUDA, err = cudaFree(d_C);); (18, CUDA, fprintf(stderr, \"Failed to copy vector A to device (error code %s)!\\n\", cudaGetErrorString(err));); (-12, CUDA, err = cudaEventCreate(&stop);); (-14, CUDA, err = cudaEventCreate(&start);)", "label": "(88, CUDA, cudaEventSynchronize(stop);); (70, CUDA, err = cudaGetLastError();); (19, CUDA, err = cudaMalloc(&d_A, length);); (12, CUDA, cudaEventRecord(start, 0);); (78, CUDA, err = cudaFree(d_C);); (87, CUDA, cudaEventDestroy(start);); (74, CUDA, err = cudaFree(d_B);); (21, CUDA, err = cudaMalloc(&d_B, length);); (78, CUDA, cudaEventRecord(stop, 0);); (32, CUDA, err = cudaMemcpy(d_A, A, length, cudaMemcpyHostToDevice);); (80, CUDA, cudaEventElapsedTime(elapsed_time, start, stop);); (1, CUDA, cudaEventCreate(&start);); (66, CUDA, err = cudaFree(d_A);); (30, CUDA, err = cudaMemcpy(d_B, B, length, cudaMemcpyHostToDevice);); (16, CUDA, err = cudaMalloc(&d_C, length);); (78, CUDA, cudaEventDestroy(stop);); (-3, CUDA, cudaEventCreate(&stop);); (57, CUDA, err = cudaMemcpy(C, d_C, length, cudaMemcpyDeviceToHost);)"}
{"predict": "(17, MPI, MPI_Comm_size(comm, &size);); (28, MPI, MPI_Abort(comm, 1);); (15, MPI, MPI_Comm_rank(comm, &rank);)", "label": "(29, MPI, MPI_Abort(comm, 1);); (15, MPI, MPI_Comm_size(comm, &size);); (15, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(152, OpenMP, #pragma omp distribute parallel for lastprivate(n) firstprivate(n)); (173, OpenMP, #pragma omp distribute parallel for lastprivate(j)); (138, OpenMP, #pragma omp distribute parallel for lastprivate(h)); (25, OpenMP, #pragma omp distribute parallel for lastprivate(argc); (53, OpenMP, #pragma omp distribute parallel for lastprivate(ba, k)); (80, OpenMP, #pragma omp distribute parallel for lastprivate(S2::S2s)); (148, OpenMP, #pragma omp distribute parallel for private(xa), lastprivate(xa)); (75, OpenMP, #pragma omp distribute parallel for lastprivate(S2::S2sc)); (131, OpenMP, #pragma omp distribute parallel for lastprivate(B::x)); (116, OpenMP, #pragma omp distribute parallel for lastprivate(i)); (53, OpenMP, #pragma omp distribute parallel for lastprivate(ca)); (44, OpenMP, #pragma omp distribute parallel for lastprivate(2 * 2)); (10, OpenMP, #pragma omp distribute parallel for lastprivate(argc,); (-6, OpenMP, #pragma omp target); (139, OpenMP, #pragma omp distribute parallel for lastprivate(si)); (-5, OpenMP, #pragma omp distribute parallel for lastprivate); (148, OpenMP, #pragma omp distribute parallel for lastprivate(cond) firstprivate(cond)); (27, OpenMP, #pragma omp distribute parallel for lastprivate(a, b, c, d, f)); (-6, OpenMP, #pragma omp distribute parallel for lastprivate()); (37, OpenMP, #pragma omp distribute parallel for lastprivate(da)); (60, OpenMP, #pragma omp distribute parallel for lastprivate(e, g)); (72, OpenMP, #pragma omp distribute parallel for safelen(5)); (65, OpenMP, #pragma omp distribute parallel for lastprivate(m)); (102, OpenMP, #pragma omp distribute parallel for lastprivate(xa)); (15, OpenMP, #pragma omp distribute parallel for lastprivate(argc > 0 ? argv[1] : argv[2])); (74, OpenMP, #pragma omp distribute parallel for lastprivate(S2::S2sc)); (36, OpenMP, #pragma", "label": "(82, OpenMP, #pragma omp distribute parallel for lastprivate(xa)); (15, OpenMP, #pragma omp distribute parallel for lastprivate(); (59, OpenMP, #pragma omp distribute parallel for lastprivate(2 * 2)); (73, OpenMP, #pragma omp distribute parallel for lastprivate(da)); (93, OpenMP, #pragma omp distribute parallel for safelen(5)); (137, OpenMP, #pragma omp distribute parallel for lastprivate(j)); (25, OpenMP, #pragma omp distribute parallel for lastprivate(argc,); (2, OpenMP, #pragma omp target); (109, OpenMP, #pragma omp distribute parallel for lastprivate(B::x)); (62, OpenMP, #pragma omp distribute parallel for lastprivate(ca)); (138, OpenMP, #pragma omp distribute parallel for firstprivate(m) lastprivate(m)); (116, OpenMP, #pragma omp distribute parallel for lastprivate(i)); (142, OpenMP, #pragma omp distribute parallel for lastprivate(n) firstprivate(n)); (94, OpenMP, #pragma omp distribute parallel for lastprivate(m)); (32, OpenMP, #pragma omp distribute parallel for lastprivate(S1)); (97, OpenMP, #pragma omp distribute parallel for lastprivate(h)); (-5, OpenMP, #pragma omp distribute parallel for lastprivate); (39, OpenMP, #pragma omp distribute parallel for lastprivate(argv[1])); (18, OpenMP, #pragma omp distribute parallel for lastprivate(argc > 0 ? argv[1] : argv[2])); (32, OpenMP, #pragma omp distribute parallel for lastprivate(a, b, c, d, f)); (140, OpenMP, #pragma omp distribute parallel for lastprivate(si)); (20, OpenMP, #pragma omp distribute parallel for lastprivate(argc)); (80, OpenMP, #pragma omp distribute parallel for lastprivate(e, g)); (3, OpenMP, #pragma omp distribute parallel for lastprivate(argc); (42, OpenMP, #pragma omp distribute parallel for lastprivate(ba)); (62, OpenMP, #pragma omp distribute parallel for lastprivate(S2::S2s)); (66, OpenMP, #pragma omp distribute parallel for lastprivate(S2::S2sc)); (95, OpenMP, #pragma omp distribute parallel for private(xa), lastprivate(xa)); (-7, OpenMP, #pragma omp distribute parallel for lastprivate()); (-19, OpenMP, #pragma omp teams)"}
{"predict": "(103, OpenMP, printf(\"Number of threads    = %d\\n\", omp_get_max_threads());); (24, MPI, MPI_Comm_size(MPI_COMM_WORLD,&Num_procs);); (150, MPI, MPI_Finalize();); (77, MPI, MPI_Bcast(&length,        1, MPI_LONG, root, MPI_COMM_WORLD);); (21, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&my_ID);); (74, MPI, MPI_Bcast(&nthread_input, 1, MPI_INT,  root, MPI_COMM_WORLD);); (76, OpenMP, omp_set_num_threads(nthread_input);); (17, MPI, MPI_Init(&argc,&argv);); (71, MPI, MPI_Bcast(&iterations,    1, MPI_INT,  root, MPI_COMM_WORLD);); (113, MPI, MPI_Barrier(MPI_COMM_WORLD);); (68, MPI, MPI_Bcast(&offset,        1, MPI_LONG, root, MPI_COMM_WORLD);)", "label": "(101, OpenMP, printf(\"Number of threads    = %d\\n\", omp_get_max_threads());); (24, MPI, MPI_Comm_size(MPI_COMM_WORLD,&Num_procs);); (148, MPI, MPI_Finalize();); (104, OpenMP, #pragma omp parallel for); (74, MPI, MPI_Bcast(&length,        1, MPI_LONG, root, MPI_COMM_WORLD);); (21, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&my_ID);); (75, MPI, MPI_Bcast(&nthread_input, 1, MPI_INT,  root, MPI_COMM_WORLD);); (76, OpenMP, omp_set_num_threads(nthread_input);); (16, MPI, MPI_Init(&argc,&argv);); (71, MPI, MPI_Bcast(&iterations,    1, MPI_INT,  root, MPI_COMM_WORLD);); (113, MPI, MPI_Barrier(MPI_COMM_WORLD);); (68, MPI, MPI_Bcast(&offset,        1, MPI_LONG, root, MPI_COMM_WORLD);)"}
{"predict": "(19, TBB, tbb::global_control c(tbb::global_control::max_allowed_parallelism, p);); (32, TBB, tbb::flow::input_edge<0>(b).try_put(tbb::flow::continue_msg());); (10, TBB, tbb::flow::graph g;); (54, TBB, tbb::flow::input_edge<0>(n).try_put(tbb::flow::continue_msg());); (16, TBB, tbb::flow::input_edge<0>(b).try_put(tbb::flow::tuple<int>(0));); (79, TBB, tbb::flow::input_edge<0>(a).try_put(tbb::flow::tuple<int>(0));); (89, TBB, tbb::flow::input_edge<1>(a).try_put(tbb::flow::tuple<int>(1));); (12, TBB, tbb::flow::input_edge<0>(b).try_put(tbb::flow::tuple<int>(1));); (139, TBB, tbb::flow::input_edge<2>(o).try_put(tbb::flow::tuple<int>(1));); (169, TBB, tbb::flow::input_edge<2>(a).try_put(tbb::flow::tuple<int>(2));); (178, TBB, tbb::flow::input_edge<3>(o).try_put(tbb::flow::tuple<int>(3));); (195, TBB, utility::report_elapsed_time((tbb::tick_count::now() - start).seconds());); (-4, TBB, tbb::tick_count start = tbb::tick_count::now();); (58, TBB, tbb::flow::input_edge<1>(n).try_put(tbb::flow::tuple<int>(1));); (85, TBB, tbb::flow::input_edge<1>(a).try_put(tbb::flow::tuple<int>(0));); (139, TBB, tbb::flow::input_edge<1>(a).try_put(tbb::flow::tuple<int>(1));); (159, TBB, tbb::flow::input_edge<1>(o).try_put(tbb::flow::tuple<int>(1));); (1", "label": "(720, TBB, utility::report_elapsed_time((oneapi::tbb::tick_count::now() - start).seconds());); (15, TBB, oneapi::tbb::global_control c(oneapi::tbb::global_control::max_allowed_parallelism, p);); (19, TBB, oneapi::tbb::flow::graph g;); (11, TBB, oneapi::tbb::tick_count start = oneapi::tbb::tick_count::now();)"}
{"predict": "(57, CUDA, CHECK(cudaStreamDestroy(streams[n]));); (42, CUDA, CHECK(cudaStreamCreate(&(streams[n])));); (88, CUDA, CHECK(cudaFree(d_y));); (18, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice));); (87, CUDA, CHECK(cudaFree(d_z));); (58, CUDA, CHECK(cudaMallocHost(&h_x2, M));); (81, CUDA, CHECK(cudaFreeHost(h_y2));); (81, CUDA, CHECK(cudaFreeHost(h_z2));); (67, CUDA, CHECK(cudaStreamCreate(&(streams[i])));); (9, CUDA, CHECK(cudaMalloc(&d_y, M));); (76, CUDA, CHECK(cudaStreamDestroy(streams[i]));); (65, CUDA, CHECK(cudaMallocHost(&h_z2, M));); (4, CUDA, CHECK(cudaMalloc(&d_x, M));); (59, CUDA, CHECK(cudaMallocHost(&h_y2, M));); (8, CUDA, CHECK(cudaMalloc(&d_z, M));); (7, CUDA, CHECK(cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice));); (81, CUDA, CHECK(cudaFree(d_x));); (64, CUDA, CHECK(cudaStreamCreate(&(streams[num])));); (43, CUDA, CHECK(cudaMallocHost(&h_z2, M));); (78, CUDA, CHECK(cudaFreeHost(h_x2));); (48, CUDA, CHECK(cudaStreamDestroy(streams[num]));); (69, CUDA, CHECK(cudaFreeHost(h_z2));); (75, CUDA, CHECK(cudaFreeHost(h_y2));); (39, CUDA, CHECK(cudaMallocHost(&h_y2, M));); (3, CUDA, CHECK(cudaMemcpy(d_z, h_z, M, cudaMemcpyHostToDevice));); (36, CUDA, CHECK(cudaMallocHost(&h_x2, M));)", "label": "(47, CUDA, CHECK(cudaStreamCreate(&(streams[n])));); (84, CUDA, CHECK(cudaStreamDestroy(streams[i]));); (94, CUDA, CHECK(cudaFree(d_y));); (19, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice));); (93, CUDA, CHECK(cudaFree(d_z));); (53, CUDA, CHECK(cudaStreamDestroy(streams[n]));); (58, CUDA, CHECK(cudaMallocHost(&h_x2, M));); (82, CUDA, CHECK(cudaFreeHost(h_y2));); (82, CUDA, CHECK(cudaFreeHost(h_z2));); (66, CUDA, CHECK(cudaStreamCreate(&(streams[i])));); (10, CUDA, CHECK(cudaMalloc(&d_y, M));); (8, CUDA, CHECK(cudaMalloc(&d_x, M));); (83, CUDA, CHECK(cudaFree(d_x));); (53, CUDA, CHECK(cudaMallocHost(&h_z2, M));); (7, CUDA, CHECK(cudaMalloc(&d_z, M));); (73, CUDA, CHECK(cudaFreeHost(h_x2));); (7, CUDA, CHECK(cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice));); (48, CUDA, CHECK(cudaMallocHost(&h_y2, M));)"}
{"predict": "(93, CUDA, cudaFree(gpuTransposeMatrix);); (19, CUDA, cudaGetDeviceProperties(&devProp, 0);); (81, CUDA, cudaFree(gpuMatrix);); (43, CUDA, cudaMalloc((void**)&gpuMatrix, NUM * sizeof(float));); (43, CUDA, cudaMalloc((void**)&gpuTransposeMatrix, NUM * sizeof(float));)", "label": "(20, CUDA, cudaGetDeviceProperties(&devProp, 0);); (81, CUDA, cudaFree(gpuMatrix);); (43, CUDA, cudaMalloc((void**)&gpuMatrix, NUM * sizeof(float));); (81, CUDA, cudaFree(gpuTransposeMatrix);); (43, CUDA, cudaMalloc((void**)&gpuTransposeMatrix, NUM * sizeof(float));)"}
{"predict": "(8, OpenMP, #pragma omp taskloop); (6, OpenMP, #pragma omp parallel); (306, OpenMP, #pragma omp taskloop linear(ii)); (260, OpenMP, #pragma omp taskloop collapse(2)); (275, OpenMP, #pragma omp taskloop lastprivate(ii)); (284, OpenMP, #pragma omp taskloop firstprivate(ii)); (299, OpenMP, #pragma omp taskloop private(ii))", "label": "(286, OpenMP, #pragma omp taskloop lastprivate(ii)); (280, OpenMP, #pragma omp taskloop private(ii)); (5, OpenMP, #pragma omp parallel); (5, OpenMP, #pragma omp taskloop); (317, OpenMP, #pragma omp taskloop collapse(2)); (265, OpenMP, #pragma omp taskloop firstprivate(ii)); (270, OpenMP, #pragma omp taskloop linear(ii))"}
{"predict": "(109, OpenCL, retVal = clReleaseKernel(kernel);); (34, OpenCL, context = clCreateContext(nullptr, 1, &device, nullptr, nullptr, &retVal);); (110, OpenCL, retVal = clReleaseCommandQueue(cmdQ);); (107, OpenCL, retVal = clReleaseProgram(pProgram);); (109, OpenCL, retVal = clReleaseContext(context);); (57, OpenCL, kernel = clCreateKernel(pProgram, \"CopyBuffer\", &retVal);); (34, OpenCL, cmdQ = clCreateCommandQueue(context, device, properties, &retVal);)", "label": "(104, OpenCL, retVal = clReleaseKernel(kernel);); (34, OpenCL, context = clCreateContext(nullptr, 1, &device, nullptr, nullptr, &retVal);); (108, OpenCL, retVal = clReleaseCommandQueue(cmdQ);); (110, OpenCL, retVal = clReleaseContext(context);); (57, OpenCL, kernel = clCreateKernel(pProgram, \"CopyBuffer\", &retVal);); (102, OpenCL, retVal = clReleaseProgram(pProgram);); (33, OpenCL, cmdQ = clCreateCommandQueue(context, device, properties, &retVal);)"}
{"predict": "(319, MPI, MPI_Win_free(&rma_winy);); (137, MPI, MPI_Bcast(&n,          1, MPI_INT, root, MPI_COMM_WORLD);); (255, MPI, MPI_Barrier(MPI_COMM_WORLD);); (316, MPI, MPI_Win_free(&rma_winx);); (136, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (42, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (322, MPI, MPI_Finalize();); (135, MPI, MPI_Bcast(&RADIUS, 1, MPI_INT, root, MPI_COMM_WORLD);); (36, MPI, MPI_Init(&argc,&argv);); (316, MPI, MPI_Reduce(&local_norm, &norm, 1, MPI_DTYPE, MPI_SUM, root, MPI_COMM_WORLD);); (292, MPI, MPI_Win_unlock_all(rma_winx);); (292, MPI, MPI_Win_unlock_all(rma_winy);); (261, MPI, MPI_Win_lock_all(0, rma_winx);); (26, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);); (290, MPI, MPI_Win_lock_all(0, rma_winy);)", "label": "(210, MPI, MPI_Info_create(&rma_winfo);); (212, MPI, MPI_Info_set(rma_winfo, \"no_locks\", \"true\");); (370, MPI, MPI_Info_free(&rma_winfo);); (371, MPI, MPI_Finalize();); (128, MPI, MPI_Bcast(&n,          1, MPI_INT, root, MPI_COMM_WORLD);); (128, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (267, MPI, MPI_Win_fence(MPI_MODE_NOSTORE, rma_winx);); (39, MPI, MPI_Init(&argc,&argv);); (232, MPI, MPI_Barrier(MPI_COMM_WORLD);); (321, MPI, MPI_Reduce(&local_norm, &norm, 1, MPI_DTYPE, MPI_SUM, root, MPI_COMM_WORLD);); (38, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (234, MPI, MPI_Win_fence(MPI_MODE_NOSTORE, rma_winy);); (35, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(145, CUDA, checkCudaErrors(cudaEventSynchronize(stop_event));); (112, CUDA, checkCudaErrors(cudaEventRecord(start_event, 0));); (31, CUDA, checkCudaErrors(cudaGetDeviceProperties(&prop[i], i));); (133, CUDA, checkCudaErrors(cudaMemcpyPeer(&time_memcpy, gpuid[0], &time_memcpy, gpuid[1], sizeof(float)));); (103, CUDA, checkCudaErrors(cudaMemcpy(g0, h0, buf_size, cudaMemcpyHostToDevice));); (184, CUDA, checkCudaErrors(cudaEventDestroy(start_event));); (140, CUDA, checkCudaErrors(cudaEventElapsedTime(&time_memcpy, start_event, stop_event));); (105, CUDA, checkCudaErrors(cudaEventRecord(stop_event, 0));); (134, CUDA, checkCudaErrors(cudaEventSynchronize(start_event));); (118, CUDA, checkCudaErrors(cudaMemcpyPeer(g1, gpuid[0], h0, gpuid[1], buf_size));); (199, CUDA, checkCudaErrors(cudaFreeHost(h0));); (109, CUDA, checkCudaErrors(cudaMemcpy(g1, h0, buf_size, cudaMemcpyHostToDevice));); (137, CUDA, checkCudaErrors(cudaEventDestroy(stop_event));); (76, CUDA, checkCudaErrors(cudaMalloc(&g1, buf_size));); (8, CUDA, checkCudaErrors(cudaGetDeviceCount(&gpu_n));); (93, CUDA, checkCudaErrors(cudaEventCreate(&start_event));); (105, CUDA, checkCudaErrors(cudaMemcpyPeer(g0, gpuid[1], h0, gpuid[0], buf_size));); (75, CUDA, checkCudaErrors(cudaMalloc(&g0, buf_size));); (90, CUDA, checkCudaErrors(cudaEventCreate(&stop_event));); (184, CUDA, checkCudaErrors(cudaFree(g1));); (106, CUDA, checkCudaErrors(cudaMemcpy(g0, g1, buf_size, cudaMemcpyDeviceToDevice));); (179, CUDA, checkCudaErrors(cudaFree(g0));); (42, CUDA, checkCudaErrors(cudaDeviceCanAccessPeer(&can_access_peer", "label": "(46, CUDA, checkCudaErrors(cudaDeviceCanAccessPeer(&can_access_peer, i, j));); (99, CUDA, checkCudaErrors(cudaEventCreateWithFlags(&stop_event, eventflags));); (101, CUDA, checkCudaErrors(cudaEventRecord(start_event, 0));); (107, CUDA, checkCudaErrors(cudaMemcpy(g1, g0, buf_size, cudaMemcpyDefault));); (190, CUDA, checkCudaErrors(cudaFreeHost(h0));); (142, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (156, CUDA, checkCudaErrors(cudaMemcpy(h0, g0, buf_size, cudaMemcpyDefault));); (109, CUDA, checkCudaErrors(cudaEventRecord(stop_event, 0));); (6, CUDA, checkCudaErrors(cudaGetDeviceCount(&gpu_n));); (66, CUDA, checkCudaErrors(cudaSetDevice(gpuid[0]));); (181, CUDA, checkCudaErrors(cudaFree(g0));); (20, CUDA, checkCudaErrors(cudaGetDeviceProperties(&prop[i], i));); (74, CUDA, checkCudaErrors(cudaMalloc(&g0, buf_size));); (176, CUDA, checkCudaErrors(cudaEventDestroy(stop_event));); (62, CUDA, checkCudaErrors(cudaDeviceEnablePeerAccess(gpuid[1], 0));); (97, CUDA, checkCudaErrors(cudaMemcpy(g0, g1, buf_size, cudaMemcpyDefault));); (102, CUDA, checkCudaErrors(cudaEventElapsedTime(&time_memcpy, start_event, stop_event));); (171, CUDA, checkCudaErrors(cudaEventDestroy(start_event));); (59, CUDA, checkCudaErrors(cudaSetDevice(gpuid[1]));); (70, CUDA, checkCudaErrors(cudaMalloc(&g1, buf_size));); (58, CUDA, checkCudaErrors(cudaDeviceEnablePeerAccess(gpuid[0], 0));); (163, CUDA, checkCudaErrors(cudaDeviceDisablePeerAccess(gpuid[0]));); (160, CUDA, checkCudaErrors(cudaDeviceDisablePeerAccess(gpuid[1]));); (76, CUDA, checkCudaErrors(cudaEventCreateWithFlags(&start_event, eventflags));); (173, CUDA, checkCudaErrors(cudaSetDevice(i));); (107, CUDA, checkCudaErrors(cudaMemcpy(g0, h0, buf_size, cudaMemcpyDefault));); (91, CUDA, checkCudaErrors(cudaEventSynchronize(stop_event));); (166, CUDA, checkCudaErrors(cudaFree(g1));)"}
{"predict": "(17, MPI, MPI_Barrier(MPI_COMM_WORLD);); (269, MPI, DCMP_GLOBAL_STATE = DCMP_STATE_ENDED;); (26, MPI, MPI_Type_contiguous(8, MPI_UINT64_T, &type_8uint64);); (301, MPI, MPI_Type_free(&type_8uint64);); (255, MPI, MPI_Allreduce(&rc, &all_rc, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);); (30, MPI, MPI_Type_commit(&type_8uint64);); (266, MPI, DCMP_GLOBAL_STATE = DCMP_STATE_REDUCE;); (21, MPI, time(&time_ended);); (287, MPI, MPI_Type_contiguous(4, MPI_UINT64_T, &type_4uint64);); (28, MPI, MPI_Allreduce(&src_mtime,      &all_src_mtime,      1, type_8uint64, MPI_MAX, MPI_COMM_WORLD);); (28, MPI, MPI_Allreduce(&src_mtime_nsec, &all_src_mtime_nsec, 1, type_8uint64, MPI_MAX, MPI_COMM_WORLD);); (287, MPI, MPI_Type_free(&type_4uint64);); (25, MPI, MPI_Allreduce(&dst_mtime,      &all_dst_mtime,      1, type_8uint64, MPI_MAX, MPI_COMM_WORLD);); (25, MPI, MPI_Allreduce(&dst_mtime_nsec, &all_dst_mtime_nsec, 1, type_8uint64, MPI_MAX, MPI_COMM_WORLD);); (15, MPI, MPI_Type_contiguous(4, MPI_UINT64_T, &type_4uint64);); (16, MPI, MPI_Type_commit(&type_4uint64);); (256, MPI, DCMP_GLOBAL_STATE = DCMP_STATE_STARTED;); (18, MPI, MPI_Allreduce(&src_mode, &all_src_mode, 1, type_4uint64, MPI_MAX, MPI_COMM_WORLD);); (18, MPI, MPI_Allreduce(&dst_mode, &all_dst_mode, 1, type_4uint64, MPI_MAX, MPI_COMM_WORLD);); (282, MPI, MPI_Barrier(MPI_COMM_WORLD);", "label": "(17, MPI, MPI_Barrier(MPI_COMM_WORLD);); (26, MPI, double start_compare = MPI_Wtime();); (199, MPI, double end_compare = MPI_Wtime();)"}
{"predict": "(102, MPI, MPI_Send(&report, sizeof(imb_p2p_report_t), MPI_BYTE, 0, 1, MPI_COMM_WORLD);); (71, MPI, MPI_Isend(send_buffers[i], size, MPI_BYTE, neighbours[i], 0, MPI_COMM_WORLD, &requests[NUMBER_OF_NEIGHBORS + i]);); (75, MPI, MPI_Waitall((NUMBER_OF_NEIGHBORS * 2), requests, MPI_STATUSES_IGNORE);); (80, MPI, time = MPI_Wtime() - time;); (66, MPI, time = MPI_Wtime();); (110, MPI, MPI_Recv(&report, sizeof(imb_p2p_report_t), MPI_BYTE, source_rank, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (69, MPI, MPI_Irecv(recv_buffers[i], size, MPI_BYTE, neighbours[i], 0, MPI_COMM_WORLD, &requests[i]);)", "label": "(75, MPI, MPI_Waitall((NUMBER_OF_NEIGHBORS * 2), requests, MPI_STATUSES_IGNORE);); (97, MPI, time = MPI_Wtime() - time;); (71, MPI, MPI_Isend(send_buffers[i], size, MPI_BYTE, neighbours[i], 0, MPI_COMM_WORLD, &requests[NUMBER_OF_NEIGHBORS + i]);); (78, MPI, time = MPI_Wtime();); (101, MPI, MPI_Send(&report, sizeof(imb_p2p_report_t), MPI_BYTE, 0, 1, MPI_COMM_WORLD);); (65, MPI, MPI_Irecv(recv_buffers[i], size, MPI_BYTE, neighbours[i], 0, MPI_COMM_WORLD, &requests[i]);); (108, MPI, MPI_Recv(&report, sizeof(imb_p2p_report_t), MPI_BYTE, source_rank, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)"}
{"predict": "(73, CUDA, cudaFree( in );); (59, CUDA, cudaEventRecord( evStop );); (37, CUDA, cudaEventRecord( evStart );); (72, CUDA, cudaFree( out );)", "label": "(59, CUDA, cudaEventRecord( evStop );); (75, CUDA, cudaFree( in );); (72, CUDA, cudaEventDestroy( evStart );); (52, CUDA, cudaEventRecord( evStart );); (73, CUDA, cudaFree( out );); (70, CUDA, cudaEventDestroy( evStop );)"}
{"predict": "(40, CUDA, checkCudaErrors(cudaMemset(d_Output, 0, QRNG_DIMENSIONS * N * sizeof(float)));); (44, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (124, CUDA, checkCudaErrors(cudaFree(d_Output));)", "label": "(40, CUDA, checkCudaErrors(cudaMemset(d_Output, 0, QRNG_DIMENSIONS * N * sizeof(float)));); (44, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (124, CUDA, checkCudaErrors(cudaFree(d_Output));)"}
{"predict": "(66, MPI, MPI_Type_free(&parent_type);); (37, MPI, MPI_Pack(array, 1, parent_type, buf, sizeoftype, &position, MPI_COMM_WORLD);); (11, MPI, MPI_Pack_external_size((char *) \"external32\", 1, MPI_INT, &sizeofint);); (21, MPI, MPI_Type_vector(10, 2, 2, MPI_INT, &parent_type);); (45, MPI, MPI_Unpack(buf, sizeoftype, &position, array, 1, parent_type, MPI_COMM_WORLD);); (22, MPI, MPI_Type_commit(&parent_type);); (19, MPI, MPI_Pack_external_size((char *) \"external32\", 1, parent_type, &sizeoftype);)", "label": "(12, MPI, MPI_Pack_external_size((char *) \"external32\", 1, MPI_INT, &sizeofint);); (66, MPI, MPI_Type_free(&parent_type);); (36, MPI, MPI_Pack_external((char *) \"external32\", array, 1, parent_type, buf, sizeoftype, &position);); (46, MPI, MPI_Unpack_external((char *) \"external32\", buf, sizeoftype, &position, array, 1, parent_type);); (21, MPI, MPI_Pack_external_size((char *) \"external32\", 1, parent_type, &sizeoftype);); (18, MPI, MPI_Type_commit(&parent_type);); (15, MPI, MPI_Type_vector(10, 2, 2, MPI_INT, &parent_type);)"}
{"predict": "(7, SYCL, TA * h_a = sycl::malloc_host<TA>( nelems, q);); (19, SYCL, TA * A = sycl::malloc_device<TA>( nelems, q);); (7, SYCL, TB * h_b = sycl::malloc_host<TB>( nelems, q);); (17, SYCL, TB * B = sycl::malloc_device<TB>( nelems, q);); (50, SYCL, sycl::free(h_a, q);); (24, SYCL, sycl::free(h_b, q);); (48, SYCL, sycl::free(A, q);); (48, SYCL, sycl::free(B, q);); (-1, SYCL, TC * h_c = sycl::malloc_host<TC>( nelems, q);); (12, SYCL, TC * C = sycl::malloc_device<TC>( nelems, q);); (44, SYCL, sycl::free(h_c, q);); (71, SYCL, auto isA_fp16 = (typeid(TA) == typeid(sycl::half));); (85, SYCL, auto isC_fp16 = (typeid(TC) == typeid(sycl::half));); (78, SYCL, auto isB_fp16 = (typeid(TB) == typeid(sycl::half));)", "label": "(82, SYCL, auto isA_fp16 = (typeid(TA) == typeid(sycl::half));); (28, SYCL, sycl::free(h_a, q);); (6, SYCL, auto h_c = sycl::malloc_host<TC>( nelems, q);); (4, SYCL, auto h_b = sycl::malloc_host<TB>( nelems, q);); (111, SYCL, sycl::free(h_c, q);); (14, SYCL, auto  A = sycl::malloc_device<TA>( nelems, q);); (94, SYCL, auto isC_fp16 = (typeid(TC) == typeid(sycl::half));); (23, SYCL, sycl::free(h_b, q);); (49, SYCL, sycl::free(A, q);); (47, SYCL, sycl::free(B, q);); (-4, SYCL, auto h_a = sycl::malloc_host<TA>( nelems, q);); (80, SYCL, auto isB_fp16 = (typeid(TB) == typeid(sycl::half));); (9, SYCL, auto  C = sycl::malloc_device<TC>( nelems, q);); (7, SYCL, auto  B = sycl::malloc_device<TB>( nelems, q);); (41, SYCL, sycl::free(C, q);)"}
{"predict": "(33, MPI, MPI_Type_free(&column[j]);); (18, MPI, MPI_Type_commit(&xpose[j]);); (34, MPI, MPI_Type_free(&xpose[j]);); (18, MPI, t[i] = MPI_Wtime() - ttmp;); (14, MPI, MPI_Type_vector(ncols, 1, 1, MPI_DOUBLE, &column[0]);); (16, MPI, MPI_Type_commit(&column[j]);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (0, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (15, MPI, ttmp = MPI_Wtime();); (11, MPI, MPI_Type_create_hvector(nrows, 1, sizeof(double), &column[0], &xpose[0]);)", "label": "(24, MPI, t[i] = MPI_Wtime() - ttmp;); (19, MPI, MPI_Type_commit(&xpose[j]);); (34, MPI, MPI_Type_free(&column[j]);); (32, MPI, MPI_Type_free(&xpose[j]);); (11, MPI, ttmp = MPI_Wtime();); (13, MPI, MPI_Type_vector(nrows, 1, ncols, MPI_INT, &column[j]);); (13, MPI, MPI_Type_create_hvector(ncols, 1, sizeof(int), column[j], &xpose[j]);)"}
{"predict": "(3, CUDA, __shared__ float buffer[128];); (3, CUDA, __shared__ float sums[4];); (0, CUDA, __shared__ float gauss[16];)", "label": "(3, CUDA, __shared__ float buffer[128];); (3, CUDA, __shared__ float sums[4];); (0, CUDA, __shared__ float gauss[16];)"}
{"predict": "(14, MPI, rc = MPI_Type_get_extent(type, &tmp_lb, &attr->child_type_extent);); (151, MPI, rc = MPI_Type_free(&type);)", "label": "(14, MPI, rc = MPI_Type_get_extent(type, &tmp_lb, &attr->child_type_extent);); (127, MPI, rc = MPI_Type_free(&type);)"}
{"predict": "(134, OpenCL, error = clSetKernelArg(kernel, 1, sizeof(tinfo->inBuf), &tinfo->inBuf);)", "label": "(134, OpenCL, error = clSetKernelArg(kernel, 1, sizeof(tinfo->inBuf), &tinfo->inBuf);)"}
{"predict": "(8, MPI, MPI_Finalize();); (0, MPI, MPI_Init(&argc,&argv);)", "label": "(8, MPI, MPI_Finalize();); (0, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(19, CUDA, CUASSERT(cudaMemsetAsync(sortClassValidCountPtr, 0, N * sizeof(int), stream));)", "label": "(21, CUDA, CUASSERT(cudaMemsetAsync(sortClassValidCountPtr, 0, N * sizeof(int), stream));); (19, CUDA, CUASSERT(cudaMemsetAsync(argMaxBBoxPtr, 0, N * samples * 4 * sizeof(float), stream));); (20, CUDA, CUASSERT(cudaMemsetAsync(sortClassPosPtr, 0, N * (NClass + 1) * sizeof(int), stream));); (20, CUDA, CUASSERT(cudaMemsetAsync(sortClassSampleIdxPtr, 0, N * (samples + 1) * sizeof(int), stream));); (15, CUDA, CUASSERT(cudaMemsetAsync(argMaxScorePtr, 0, N * samples * sizeof(float), stream));)"}
{"predict": "(52, OpenCL, err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &global, NULL, 0, NULL, NULL);); (28, OpenCL, clMemWrapper in = clCreateBuffer(context, CL_MEM_READ_ONLY, bytes, NULL, &err);); (30, OpenCL, err = clEnqueueWriteBuffer(queue, in, CL_TRUE, 0, bytes, &h_in[0], 0, NULL, NULL);); (39, OpenCL, clKernelWrapper kernel = clCreateKernel(prog, spvName, &err);); (42, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(cl_mem), &in);); (48, OpenCL, err = clEnqueueReadBuffer(queue, in, CL_TRUE, 0, bytes, &h_out[0], 0, NULL, NULL);)", "label": "(32, OpenCL, err = clEnqueueWriteBuffer(queue, in, CL_TRUE, 0, bytes, &h_in[0], 0, NULL, NULL);); (44, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(cl_mem), &in);); (51, OpenCL, err = clEnqueueReadBuffer(queue, in, CL_TRUE, 0, bytes, &h_out[0], 0, NULL, NULL);); (26, OpenCL, clMemWrapper in = clCreateBuffer(context, CL_MEM_READ_WRITE, bytes, NULL, &err);); (38, OpenCL, clKernelWrapper kernel = clCreateKernel(prog, spvName, &err);); (44, OpenCL, err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &global, NULL, 0, NULL, NULL);)"}
{"predict": "(54, TBB, tbb::detail::d1::segmented_iterator<nested_vec, const T> my_csi(my_vec);); (19, TBB, tbb::detail::d1::segmented_iterator<nested_vec, T> my_si(my_vec);)", "label": "(61, TBB, tbb::internal::segmented_iterator<nested_vec, const T> my_csi(my_vec);); (18, TBB, tbb::internal::segmented_iterator<nested_vec, T> my_si(my_vec);)"}
{"predict": "(25, OpenCL, cl_command_queue queue = clCreateCommandQueueWithProperties(ctx, ctx->device, 0, &err);); (33, OpenCL, cl_mem buf = clEnqueueMapBuffer(queue, data, CL_TRUE, CL_MAP_READ_WRITE, 0, sz, 0, NULL, NULL, &err);); (55, OpenCL, err = clEnqueueCopyBuffer(queue, buf, mem, origin, region, 0, NULL, NULL);); (69, OpenCL, clReleaseCommandQueue(queue);); (39, OpenCL, clReleaseMemObject(buf);)", "label": "(61, OpenCL, clReleaseMemObject(mem);); (33, OpenCL, cl_mem buf = clCreateBuffer(ctx, CL_MEM_USE_HOST_PTR, sz, data, &err);); (23, OpenCL, ctx->image_queue = clCreateCommandQueueWithProperties(ctx, ctx->devices[0], 0, &err);); (55, OpenCL, err = clEnqueueCopyBufferToImage(ctx->image_queue, buf, mem, 0, origin, region, 0, NULL, NULL);); (38, OpenCL, clReleaseMemObject(buf);)"}
{"predict": "(145, OpenMP, #pragma omp distribute parallel for lastprivate(n) firstprivate(n)); (174, OpenMP, #pragma omp distribute parallel for firstprivate(si)); (115, OpenMP, #pragma omp distribute parallel for private(xa), firstprivate(xa)); (109, OpenMP, #pragma omp distribute parallel for firstprivate(h, B::x)); (156, OpenMP, #pragma omp distribute parallel for firstprivate(m)); (68, OpenMP, #pragma omp distribute parallel for firstprivate(da)); (120, OpenMP, #pragma omp distribute parallel for lastprivate(g) firstprivate(g)); (6, OpenMP, #pragma omp distribute parallel for firstprivate); (11, OpenMP, #pragma omp distribute parallel for firstprivate()); (130, OpenMP, #pragma omp distribute parallel for firstprivate(i)); (49, OpenMP, #pragma omp distribute parallel for firstprivate(2 * 2)); (91, OpenMP, #pragma omp distribute parallel for firstprivate(e, g)); (95, OpenMP, #pragma omp distribute parallel for firstprivate(h)); (74, OpenMP, #pragma omp distribute parallel for firstprivate(xa)); (10, OpenMP, #pragma omp distribute parallel for firstprivate(argc); (160, OpenMP, #pragma omp distribute parallel for firstprivate(da, k)); (17, OpenMP, #pragma omp distribute parallel for firstprivate(argc,); (38, OpenMP, #pragma omp distribute parallel for firstprivate(argv[1])); (130, OpenMP, #pragma omp distribute parallel for firstprivate(j)); (140, OpenMP, #pragma omp distribute parallel for private(xa), firstprivate(xa)); (31, OpenMP, #pragma omp distribute parallel for firstprivate(a, b, c, d, f)); (59, OpenMP, #pragma omp distribute parallel for firstprivate(S2::S2sc)); (10, OpenMP, #pragma omp distribute parallel for firstprivate(argc > 0 ? argv[1] : argv[2])); (118, OpenMP, #pragma omp distribute parallel for lastprivate(g) firstprivate(g)); (78, OpenMP, #pragma omp distribute parallel for firstprivate(S2::S2s)); (-14, OpenMP, #pragma omp target", "label": "(51, OpenMP, #pragma omp distribute parallel for firstprivate(a, b, c, d, f)); (86, OpenMP, #pragma omp distribute parallel for firstprivate(S2::S2s)); (158, OpenMP, #pragma omp parallel private(i)); (94, OpenMP, #pragma omp distribute parallel for safelen(5)); (98, OpenMP, #pragma omp distribute parallel for firstprivate(e, g)); (170, OpenMP, #pragma omp distribute parallel for firstprivate(si)); (70, OpenMP, #pragma omp distribute parallel for firstprivate(da)); (75, OpenMP, #pragma omp distribute parallel for firstprivate(xa)); (28, OpenMP, #pragma omp distribute parallel for firstprivate(argc > 0 ? argv[1] : argv[2])); (113, OpenMP, #pragma omp distribute parallel for firstprivate(i)); (-1, OpenMP, #pragma omp target); (30, OpenMP, #pragma omp distribute parallel for firstprivate(argc)); (133, OpenMP, #pragma omp distribute parallel for lastprivate(n) firstprivate(n)); (99, OpenMP, #pragma omp distribute parallel for firstprivate(k, h, B::x)); (111, OpenMP, #pragma omp parallel); (151, OpenMP, #pragma omp parallel reduction(+ : i)); (0, OpenMP, #pragma omp distribute parallel for firstprivate(); (44, OpenMP, #pragma omp distribute parallel for firstprivate(2 * 2)); (53, OpenMP, #pragma omp distribute parallel for firstprivate(ca)); (114, OpenMP, #pragma omp distribute parallel for firstprivate(j)); (36, OpenMP, #pragma omp distribute parallel for firstprivate(argv[1])); (96, OpenMP, #pragma omp distribute parallel for private(xa), firstprivate(xa)); (70, OpenMP, #pragma omp distribute parallel for firstprivate(S2::S2sc)); (-2, OpenMP, #pragma omp distribute parallel for firstprivate()); (2, OpenMP, #pragma omp distribute parallel for firstprivate(argc); (114, OpenMP, #pragma omp distribute parallel for lastprivate(g) firstprivate(g)); (20, OpenMP, #pragma omp distribute parallel for firstprivate(S1)); (4, OpenMP, #pragma omp distribute parallel for firstprivate(argc,); (79, OpenMP, #pragma omp distribute parallel for firstprivate(m)); (37, OpenMP, #pragma omp distribute parallel for firstprivate(ba)); (-19, OpenMP, #pragma omp distribute parallel for firstprivate); (-21, OpenMP, #pragma omp teams)"}
{"predict": "(71, OpenMP, const int th=omp_get_thread_num();); (10, OpenMP, #pragma omp parallel for schedule (guided))", "label": "(71, OpenMP, const int th=omp_get_thread_num();); (10, OpenMP, #pragma omp parallel for schedule (guided))"}
{"predict": "(103, MPI, MPI_Send(buf_send, nread, MPI_BYTE, rank_recv, 0, comm);); (141, MPI, MPI_Waitall(2, request, status);); (76, MPI, MPI_Recv(buf_recv, shuffile_mpi_buf_size, MPI_BYTE, rank_send, 0, comm, status);); (98, MPI, MPI_Send(buf_send, count, MPI_BYTE, rank_recv, 0, comm);); (108, MPI, nwrite = (int) (write_pos - lseek(fd, 0, SEEK_CUR));)", "label": "(101, MPI, MPI_Wait(&request[1], &status[1]);); (110, MPI, MPI_Wait(&request[0], &status[0]);); (110, MPI, MPI_Get_count(&status[0], MPI_BYTE, &nwrite);); (74, MPI, MPI_Irecv(buf_recv, shuffile_mpi_buf_size, MPI_BYTE, rank_recv, 0, comm, &request[0]);); (96, MPI, MPI_Isend(buf_send, nread, MPI_BYTE, rank_send, 0, comm, &request[1]);)"}
{"predict": "(46, MPI, MPI_Comm_free(&pgroup_old);); (70, MPI, MPI_Recv(port, PORT_SIZE, MPI_CHAR, members[i], 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (38, MPI, MPI_Open_port(port_info, port);); (45, MPI, MPI_Intercomm_merge(pgroup_new, 0 /* LOW */ , &pgroup);); (57, MPI, MPI_Comm_connect(port, MPI_INFO_NULL, 0, MPI_COMM_WORLD, &pgroup_new);); (1, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (45, MPI, MPI_Send(port, PORT_SIZE, MPI_CHAR, members[i], 0, MPI_COMM_WORLD);); (76, MPI, MPI_Barrier(pgroup);); (27, MPI, MPI_Comm_split(MPI_COMM_WORLD, 0, members[0], &pgroup_new);); (45, MPI, MPI_Comm_free(&pgroup_new);)", "label": "(48, MPI, MPI_Comm_accept(port, MPI_INFO_NULL, 0, pgroup, &pgroup_new);); (52, MPI, MPI_Comm_free(&pgroup_old);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (64, MPI, MPI_Comm_connect(port, MPI_INFO_NULL, 0, MPI_COMM_SELF, &pgroup_new);); (42, MPI, MPI_Send(port, PORT_SIZE, MPI_CHAR, members[i], 0, MPI_COMM_WORLD);); (35, MPI, MPI_Open_port(port_info, port);); (44, MPI, MPI_Comm_free(&pgroup_new);); (48, MPI, MPI_Close_port(port);); (66, MPI, MPI_Comm_connect(port, MPI_INFO_NULL, 0, pgroup, &pgroup_new);)"}
{"predict": "(119, MPI, MPI_Type_commit(&mpi_word);); (125, MPI, MPI_Allgather(iterstring,1,mpi_word, catstring,1,mpi_word, MPI_COMM_WORLD);); (156, MPI, MPI_Finalize();); (62, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (115, MPI, MPI_Type_contiguous(proc_length,MPI_CHAR, &mpi_word);); (18, MPI, MPI_Init(&argc,&argv);); (59, MPI, MPI_Bcast(&length,     1, MPI_LONG, root, MPI_COMM_WORLD);); (115, MPI, MPI_Barrier(MPI_COMM_WORLD);); (17, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (15, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)", "label": "(113, MPI, MPI_Type_commit(&mpi_word);); (120, MPI, MPI_Allgather(iterstring,1,mpi_word, catstring,1,mpi_word, MPI_COMM_WORLD);); (152, MPI, MPI_Finalize();); (61, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (108, MPI, MPI_Type_contiguous(proc_length,MPI_CHAR, &mpi_word);); (17, MPI, MPI_Init(&argc,&argv);); (59, MPI, MPI_Bcast(&length,     1, MPI_LONG, root, MPI_COMM_WORLD);); (108, MPI, MPI_Barrier(MPI_COMM_WORLD);); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(56, CUDA, checkCudaErrors(cufftPlan2d(&planc2r, DIM, DIM, CUFFT_C2R));); (39, CUDA, checkCudaErrors(cudaMalloc((void **)&dvfield, sizeof(cData) * PDS));); (40, CUDA, checkCudaErrors(cudaMemcpy(dvfield, hvfield, sizeof(cData) * PDS, cudaMemcpyHostToDevice));); (67, CUDA, checkCudaErrors(cufftPlan2d(&planr2c, DIM, DIM, CUFFT_R2C));); (57, CUDA, getLastCudaError(\"fluidsGLES\");); (37, CUDA, checkCudaErrors(cudaMalloc((void **)&cvfield, sizeof(cData) * PDS));); (68, CUDA, checkCudaErrors(cudaFree(cvfield));); (49, CUDA, checkCudaErrors(cudaMalloc1DTextureRef(&tRef, tex));); (65, CUDA, checkCudaErrors(cudaFree(dvfield));); (36, CUDA, checkCudaErrors(cudaMemset(cvfield, 0, sizeof(cData) * PDS));); (61, CUDA, checkCudaErrors(cufftPlanDestroy(planr2c));); (59, CUDA, checkCudaErrors(cufftPlanDestroy(planc2r));); (34, CUDA, checkCudaErrors(cudaMalloc((void **)&dvfield, sizeof(cData) * PDS));); (58, CUDA, checkCudaErrors(cudaFree(particles));)", "label": "(56, CUDA, checkCudaErrors(cufftPlan2d(&planc2r, DIM, DIM, CUFFT_C2R));); (54, CUDA, checkCudaErrors(cufftPlan2d(&planr2c, DIM, DIM, CUFFT_R2C));); (37, CUDA, checkCudaErrors(cudaMallocPitch((void **)&dvfield, &tPitch, sizeof(cData)*DIM, DIM));); (38, CUDA, checkCudaErrors(cudaMemcpy(dvfield, hvfield, sizeof(cData) * DS, cudaMemcpyHostToDevice));); (40, CUDA, checkCudaErrors(cudaMalloc((void **)&vyfield, sizeof(cData) * PDS));); (38, CUDA, checkCudaErrors(cudaMalloc((void **)&vxfield, sizeof(cData) * PDS));)"}
{"predict": "(141, CUDA, cudaCheck(cudaMemcpy(dembs, acts.emb, B * C_emb * sizeof(float), cudaMemcpyDeviceToDevice));); (52, CUDA, cudaCheck(cudaMalloc(&params_memory_gpu, num_parameters * sizeof(float)));); (217, CUDA, cudaCheck(cudaFree(acts_memory_gpu));); (138, CUDA, cudaCheck(cudaMemcpy(resets, acts.h_gn1, B * C * H * W * sizeof(float), cudaMemcpyDeviceToDevice));); (141, CUDA, cudaCheck(cudaMemcpy(acts_memory_gpu, &acts, sizeof(acts), cudaMemcpyHostToDevice));); (52, CUDA, cudaCheck(cudaMalloc(&grads_memory_gpu, num_parameters * sizeof(float)));); (128, CUDA, cudaCheck(cudaMemset(resets, 0, B * C * H * W * sizeof(float)));); (133, CUDA, cudaCheck(cudaMemcpy(resets, acts.h_1, B * C * H_out * W_out * sizeof(float), cudaMemcpyDeviceToDevice));); (141, CUDA, cudaCheck(cudaMemcpy(hud, acts.h_ud, B * C * H_out * W_out * sizeof(float), cudaMemcpyDeviceToDevice));); (152, CUDA, cudaCheck(cudaMemcpy(outs, acts.out, B * C_out * H_out * W_out * sizeof(float), cudaMemcpyDeviceToDevice));); (136, CUDA, cudaCheck(cudaMemcpy(resets, acts.h_2, B * C * H_out * W_out * sizeof(float), cudaMemcpyDeviceToDevice));); (158, CUDA, cudaCheck(cudaMemcpy(dxs, acts.dx, B * C * H * W * sizeof(float), cudaMemcpyDeviceToDevice));); (143, CUDA, cudaCheck(cudaMemcpy(emb_outs, acts.emb_out, B * C_out * H_out * W_out * sizeof(float), cudaMemcpyDeviceToDevice));); (115, CUDA, cudaCheck(cudaMemset(resets, 0, B * C * H_out * W_out * sizeof(float)));); (130, CUDA, cudaCheck(cudaMemcpy(resets, acts.h, B * C * H * W * sizeof(float), cudaMemcpyDeviceToDevice));)", "label": "(125, CUDA, cudaCheck(cudaMalloc(&acts_memory_gpu, num_acts_params * sizeof(float)));); (257, CUDA, cudaCheck(cudaFree(acts_memory_gpu));); (173, CUDA, cudaCheck(cudaMalloc(&back_acts_memory_gpu, num_back_acts * sizeof(float)));); (254, CUDA, cudaCheck(cudaFree(params_memory_gpu));); (259, CUDA, cudaCheck(cudaFree(acts.emb));); (53, CUDA, cudaCheck(cudaMemset(grads_memory_gpu, 0, num_parameters * sizeof(float)));); (170, CUDA, cudaCheck(cudaMemset(back_acts_memory_gpu, 0, num_back_acts * sizeof(float)));); (182, CUDA, cudaCheck(cudaMemcpy(back_acts.dout, debug_states.dout, B * C_out * H_out * W_out * sizeof(float), cudaMemcpyHostToDevice));); (254, CUDA, cudaCheck(cudaFree(acts.input));); (117, CUDA, cudaCheck(cudaMemset(acts_memory_gpu, 0, num_acts_params * sizeof(float)));); (43, CUDA, cudaCheck(cudaMemcpy(params_memory_gpu, params_memory_cpu, num_parameters * sizeof(float), cudaMemcpyHostToDevice));); (249, CUDA, cudaCheck(cudaFree(back_acts_memory_gpu));); (122, CUDA, cudaCheck(cudaMemcpy(acts.input, debug_states.input, state_sizes[0] * sizeof(float), cudaMemcpyHostToDevice));); (122, CUDA, cudaCheck(cudaMemcpy(acts.emb, debug_states.emb, state_sizes[1] * sizeof(float), cudaMemcpyHostToDevice));); (43, CUDA, cudaCheck(cudaMalloc(&grads_memory_gpu, num_parameters * sizeof(float)));); (116, CUDA, cudaCheck(cudaMalloc(&acts.input, B * C * H * W * sizeof(float)));); (36, CUDA, cudaCheck(cudaMalloc(&params_memory_gpu, num_parameters * sizeof(float)));); (115, CUDA, cudaCheck(cudaMalloc(&acts.emb, B * C_emb * sizeof(float)));); (241, CUDA, cudaCheck(cudaFree(grads_memory_gpu));)"}
{"predict": "(72, MPI, MPI_T_pvar_stop(session->timers[10]);); (56, MPI, MPI_T_pvar_start(session->timers[11]);); (87, MPI, MPI_T_pvar_stop(session->timers[11]);); (36, MPI, MPI_T_pvar_start(session->timers[9]);)", "label": "(73, MPI, Irecv_time -= MPI_Wtime();); (55, MPI, Isend_time += MPI_Wtime();); (88, MPI, Irecv_time += MPI_Wtime();); (35, MPI, Isend_time -= MPI_Wtime();)"}
{"predict": "(65, OpenMP, #pragma omp parallel for if(n>OMP_MIN_SIZE)); (9, MPI, MPI_Init_thread( &argc , &argv , MPI_THREAD_FUNNELED , &provided );); (148, MPI, MPI_Finalize();); (9, MPI, MPI_Comm_rank( MPI_COMM_WORLD , &rank );); (131, OpenMP, #pragma omp parallel for if(n>OMP_MIN_SIZE) schedule(static,1)); (61, OpenMP, #pragma omp parallel for if(0)); (31, OpenMP, printf( \"# starting test: OpenMP ON... \\n\" );); (3, MPI, MPI_Abort( MPI_COMM_WORLD , 1 );); (29, MPI, fprintf( stderr , \"starting test: OpenMP ON... \\n\" );)", "label": "(8, MPI, MPI_Comm_rank( MPI_COMM_WORLD , &rank );); (364, MPI, MPI_Finalize();); (34, OpenMP, fprintf( stderr , \"starting test: OpenMP ON with %d threads... \\n\", omp_get_max_threads() );); (164, OpenMP, #pragma omp parallel if(n>OMP_MIN_SIZE)); (168, OpenMP, #pragma omp for private(i,j,n4)); (63, OpenMP, #pragma omp parallel for if(n>OMP_MIN_SIZE)); (1, MPI, MPI_Init_thread( &argc , &argv , MPI_THREAD_FUNNELED , &provided );); (28, OpenMP, printf( \"# starting test: OpenMP ON with %d threads... \\n\", omp_get_max_threads() );)"}
{"predict": "(12, MPI, err = MPI_Type_struct(3, blocks, disps, types, &eviltype);); (22, MPI, err = MPI_Type_size(eviltype, &val);); (66, MPI, err = MPI_Type_get_extent(eviltype, &lb, &extent);); (123, MPI, MPI_Type_free(&eviltype);); (85, MPI, err = MPI_Type_ub(eviltype, &aval);); (99, MPI, err = MPI_Type_get_true_extent(eviltype, &true_lb, &aval);); (32, MPI, err = MPI_Type_extent(eviltype, &aval);); (46, MPI, err = MPI_Type_lb(eviltype, &aval);)", "label": "(23, MPI, err = MPI_Type_size(eviltype, &val);); (11, MPI, err = MPI_Type_struct(3, blocks, disps, types, &eviltype);); (66, MPI, err = MPI_Type_get_extent(eviltype, &lb, &extent);); (86, MPI, err = MPI_Type_ub(eviltype, &aval);); (121, MPI, MPI_Type_free(&eviltype);); (99, MPI, err = MPI_Type_get_true_extent(eviltype, &true_lb, &aval);); (32, MPI, err = MPI_Type_extent(eviltype, &aval);); (46, MPI, err = MPI_Type_lb(eviltype, &aval);)"}
{"predict": "(48, MPI, MPI_Win_start(group, 0, win);); (84, MPI, MPI_Win_free(&win);); (14, MPI, MPI_Comm_split(MPI_COMM_WORLD, (rank < 2), rank, &CommDeuce);); (58, MPI, MPI_Win_post(group, 0, win);); (23, MPI, MPI_Comm_group(CommDeuce, &comm_group);); (77, MPI, MPI_Comm_free(&CommDeuce);); (58, MPI, MPI_Recv(B, SIZE, MPI_INT, 0, 100, CommDeuce, MPI_STATUS_IGNORE);); (33, MPI, MPI_Win_allocate(0, 1, MPI_INFO_NULL, CommDeuce, &base_ptr, &win);); (44, MPI, MPI_Win_allocate(SIZE * sizeof(int), sizeof(int), MPI_INFO_NULL, CommDeuce, &B, &win);); (39, MPI, MPI_Send(A, SIZE, MPI_INT, 1, 100, CommDeuce);); (6, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (73, MPI, MPI_Group_free(&comm_group);); (14, MPI, i = MPI_Alloc_mem(SIZE * sizeof(int), MPI_INFO_NULL, &A);); (-2, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (43, MPI, MPI_Win_complete(win);); (70, MPI, MPI_Group_free(&group);); (37, MPI, MPI_Win_create(A, SIZE * sizeof(int), sizeof(int), MPI_INFO_NULL, CommDeuce, &win);); (18, MPI, MPI_Group_incl(comm_group, 1, &destrank, &group);); (55, MPI, MPI_Win_lock(MPI_LOCK_SHARED, rank, 0, win);); (52, MPI, MPI_Win_create(NULL, 0, 1, MPI_INFO_NULL, CommDeuce, &win);)", "label": "(67, MPI, MPI_Win_unlock(rank, win);); (28, MPI, i = MPI_Alloc_mem(SIZE * sizeof(int), MPI_INFO_NULL, &B);); (45, MPI, MPI_Win_start(group, 0, win);); (92, MPI, MPI_Win_free(&win);); (13, MPI, MPI_Comm_split(MPI_COMM_WORLD, (rank < 2), rank, &CommDeuce);); (66, MPI, MPI_Win_post(group, 0, win);); (44, MPI, MPI_Send(B, SIZE, MPI_INT, 1, 100, MPI_COMM_WORLD);); (34, MPI, MPI_Win_allocate(0, 1, MPI_INFO_NULL, CommDeuce, &base_ptr, &win);); (47, MPI, MPI_Win_allocate(SIZE * sizeof(int), sizeof(int), MPI_INFO_NULL, CommDeuce, &B, &win);); (63, MPI, MPI_Recv(A, SIZE, MPI_INT, 0, 100, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (63, MPI, MPI_Win_wait(win);); (41, MPI, MPI_Free_mem(B);); (37, MPI, MPI_Win_complete(win);); (35, MPI, MPI_Put(A, SIZE, MPI_INT, 1, 0, SIZE, MPI_INT, win);); (0, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (5, MPI, i = MPI_Alloc_mem(SIZE * sizeof(int), MPI_INFO_NULL, &A);); (82, MPI, MPI_Comm_free(&CommDeuce);); (77, MPI, MPI_Group_free(&comm_group);); (-9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (7, MPI, MPI_Comm_group(CommDeuce, &comm_group);); (-10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (75, MPI, MPI_Free_mem(A);); (71, MPI, MPI_Group_free(&group);); (23, MPI, MPI_Group_incl(comm_group, 1, &destrank, &group);); (38, MPI, MPI_Win_create(B, SIZE * sizeof(int), sizeof(int), MPI_INFO_NULL, CommDeuce, &win);)"}
{"predict": "(58, MPI, MPI_Start(&req);); (13, MPI, MPI_Comm_rank(comm, &myrank);); (76, MPI, MPI_Type_free(&stype[i][j]);); (51, MPI, MPI_Info_create(&info);); (55, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (72, MPI, MPI_Type_free(&xtype[i][j]);); (56, MPI, MPI_Request_free(&req);); (6, MPI, MPI_Comm_size(comm, &p);); (57, MPI, MPI_Info_free(&info);)", "label": "(58, MPI, MPI_Start(&req);); (13, MPI, MPI_Comm_rank(comm, &myrank);); (76, MPI, MPI_Type_free(&stype[i][j]);); (50, MPI, MPI_Info_create(&info);); (55, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (72, MPI, MPI_Type_free(&xtype[i][j]);); (56, MPI, MPI_Request_free(&req);); (6, MPI, MPI_Comm_size(comm, &p);); (55, MPI, MPI_Info_free(&info);)"}
{"predict": "(189, OpenCL, clGetDeviceInfo(vkGPU->device, CL_DEVICE_VENDOR_ID, sizeof(cl_int), &vendorID, 0);); (86, CUDA, res = cudaMalloc((void**)&buffer, bufferSize);); (95, OpenCL, buffer = clCreateBuffer(vkGPU->context, CL_MEM_READ_WRITE, bufferSize, 0, &res);); (170, CUDA, cudaFree(buffer);); (83, CUDA, res = cudaMalloc((void**)&kernel, kernelSize);); (93, OpenCL, kernel = clCreateBuffer(vkGPU->context, CL_MEM_READ_WRITE, kernelSize, 0, &res);); (165, OpenCL, clReleaseMemObject(kernel);); (162, CUDA, cudaFree(kernel);); (76, OpenCL, buffer = clCreateBuffer(vkGPU->context, CL_MEM_READ_WRITE, bufferSize, 0, &res);); (166, CUDA, cudaFree(0);); (160, OpenCL, clReleaseMemObject(buffer);); (78, CUDA, res = cudaMalloc((void**)&buffer, bufferSize);); (154, CUDA, cudaFree(dll);); (68, CUDA, res = cudaMalloc((void**)&dll, kernelSize);); (163, CUDA, cudaFree(dll);); (73, OpenCL, kernel = clCreateBuffer(vkGPU->context, CL_MEM_READ_WRITE, kernelSize, 0, &res);); (128, CUDA, res = cudaMalloc((void**)&buffer, bufferSize);); (157, OpenCL, clReleaseCommandQueue(vkGPU->queue);); (66, CUDA, dll = cudaMalloc((void*)kernelSize, kernel);); (152, CUDA, cudaFree(buffer);); (123, OpenCL, buffer = clCreateBuffer(vkGPU->context, CL_MEM_READ_WRITE, bufferSize, 0, &res);); (150, OpenCL, clReleaseContext(vkGPU->context);); (125, CUDA, res = cudaMalloc((void**)&kernel, kernelSize);); (65, OpenCL, clEnqueueWriteBuffer(vkGPU->queue, kernel, CL_TRUE, 0, kernelSize, (uint8_t*)kernelData, 0,", "label": "(161, OpenCL, clGetDeviceInfo(vkGPU->device, CL_DEVICE_VENDOR_ID, sizeof(cl_int), &vendorID, 0);); (81, CUDA, res = cudaMalloc((void**)&buffer, bufferSize);); (90, OpenCL, buffer = clCreateBuffer(vkGPU->context, CL_MEM_READ_WRITE, bufferSize, 0, &res);)"}
{"predict": "(25, MPI, MPI_Comm_size(intercomm, &size);); (71, MPI, MPI_Send(&i, 1, MPI_INT, 0, 0, intercomm);); (87, MPI, MPI_Finalize();); (34, MPI, MPI_Send(&i, 1, MPI_INT, i, 0, intercomm);); (47, MPI, MPI_Comm_get_name(intercomm, cname, &rlen);); (75, MPI, MPI_Recv(&i, 1, MPI_INT, 0, 0, intercomm, &status);); (10, MPI, MPI_Comm_get_parent(&parentcomm);); (36, MPI, MPI_Recv(&i, 1, MPI_INT, i, 0, intercomm, &status);); (76, MPI, MPI_Comm_disconnect(&intercomm);); (17, MPI, MPI_Comm_rank(intercomm, &rank);); (14, MPI, MPI_Comm_remote_size(intercomm, &rsize);); (66, MPI, MPI_Ssend(&errs, 1, MPI_INT, 0, 1, intercomm);); (34, MPI, MPI_Abort(intercomm, 1);)", "label": "(26, MPI, MPI_Comm_size(intercomm, &size);); (84, MPI, MPI_Comm_disconnect(&intercomm);); (91, MPI, MPI_Finalize();); (11, MPI, MPI_Comm_get_parent(&parentcomm);); (66, MPI, MPI_Recv(&i, 1, MPI_INT, 0, 0, intercomm, &status);); (32, MPI, MPI_Send(&i, 1, MPI_INT, i, 0, intercomm);); (52, MPI, MPI_Comm_get_name(intercomm, cname, &rlen);); (69, MPI, MPI_Ssend(&errs, 1, MPI_INT, 0, 1, intercomm);); (35, MPI, MPI_Recv(&err, 1, MPI_INT, i, 1, intercomm, MPI_STATUS_IGNORE);); (18, MPI, MPI_Comm_rank(intercomm, &rank);); (15, MPI, MPI_Comm_remote_size(intercomm, &rsize);)"}
{"predict": "(46, OpenMP, #pragma omp teams distribute parallel for collapse (foobool(argc)), collapse (true), collapse (-5)); (4, OpenMP, #pragma omp teams distribute parallel for collapse (); (6, OpenMP, #pragma omp teams distribute parallel for collapse ()); (25, OpenMP, #pragma omp teams distribute parallel for collapse (ST); (28, OpenMP, #pragma omp teams distribute parallel for collapse (1)); (38, OpenMP, #pragma omp teams distribute parallel for collapse ((ST > 0) ? 1 + ST : 2)); (7, OpenMP, #pragma omp teams distribute parallel for collapse (argc); (49, OpenMP, #pragma omp teams distribute parallel for collapse (argv[1]=2)); (63, OpenMP, #pragma omp teams distribute parallel for collapse (N)); (65, OpenMP, #pragma omp teams distribute parallel for collapse (2)); (37, OpenMP, #pragma omp teams distribute parallel for collapse (S)); (-10, OpenMP, #pragma omp target); (-10, OpenMP, #pragma omp teams distribute parallel for collapse)", "label": "(31, OpenMP, #pragma omp teams distribute parallel for collapse (1)); (35, OpenMP, #pragma omp teams distribute parallel for collapse ((ST > 0) ? 1 + ST : 2)); (1, OpenMP, #pragma omp teams distribute parallel for collapse); (10, OpenMP, #pragma omp teams distribute parallel for collapse ()); (44, OpenMP, #pragma omp distribute collapse (S)); (21, OpenMP, #pragma omp teams distribute parallel for collapse (ST); (62, OpenMP, #pragma omp teams distribute parallel for collapse (N)); (13, OpenMP, #pragma omp teams distribute parallel for collapse (argc); (65, OpenMP, #pragma omp teams distribute parallel for collapse (2)); (49, OpenMP, #pragma omp teams distribute parallel for collapse (argv[1]=2)); (-8, OpenMP, #pragma omp target); (-3, OpenMP, #pragma omp teams distribute parallel for collapse (); (32, OpenMP, #pragma omp teams distribute parallel for collapse (foobool(argc)), collapse (true), collapse (-5))"}
{"predict": "(45, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (47, MPI, double start = MPI_Wtime();); (52, MPI, MPI_Allrunning(&total_count, &all_offsets, rank, ranks);); (42, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (192, MPI, MPI_Allreduce(&rc, &all_rc, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);); (50, MPI, double now = MPI_Wtime();); (26, MPI, MPI_Barrier(MPI_COMM_WORLD);); (167, MPI, MPI_Irecv(&shmid, 1, MPI_INT, right, 0, MPI_COMM_WORLD, &request[0]);); (169, MPI, MPI_Waitall(2, request, status);); (164, MPI, MPI_Isend(&shmid, 1, MPI_INT, left,  0, MPI_COMM_WORLD, &request[1]);); (35, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(45, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (47, MPI, double start = MPI_Wtime();); (237, MPI, double now = MPI_Wtime();); (41, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(16, CUDA, __shared__ thrust::cuda_cub::core::uninitialized_array<thrust::complex<T>, 1024> cache;); (29, CUDA, cache[id] += static_cast<thrust::complex<T>>(A_[(a * nchol + i) * nocc + b] * A_[(b * nchol + i) * nocc + a]);); (21, CUDA, thrust::complex<T2> const* A_(Tab + blockIdx.x * nocc * nocc * nchol);); (18, CUDA, cache[id]              = thrust::complex<T>(0.0);); (18, CUDA, thrust::complex<T> alp = static_cast<thrust::complex<T>>(alpha);)", "label": "(16, CUDA, __shared__ uninitialized_array<thrust::complex<T>, 1024> cache;); (29, CUDA, cache[id] += static_cast<thrust::complex<T>>(A_[(a * nchol + i) * nocc + b] * A_[(b * nchol + i) * nocc + a]);); (21, CUDA, thrust::complex<T2> const* A_(Tab + blockIdx.x * nocc * nocc * nchol);); (18, CUDA, cache[id]              = thrust::complex<T>(0.0);); (18, CUDA, thrust::complex<T> alp = static_cast<thrust::complex<T>>(alpha);)"}
{"predict": "(159, CUDA, CUDA_CHECK(cudaFree(d_B_array));); (159, CUDA, CUDA_CHECK(cudaFree(d_C_array));); (167, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (155, CUDA, CUDA_CHECK(cudaFree(d_A_array));); (158, CUDA, CUDA_CHECK(cudaFree(d_A[i]));); (159, CUDA, CUDA_CHECK(cudaFree(d_C[i]));); (123, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (164, CUDA, CUDA_CHECK(cudaDeviceReset());); (155, CUDA, CUDA_CHECK(cudaFree(d_B[i]));); (76, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)", "label": "(169, CUDA, CUDA_CHECK(cudaFree(d_B_array));); (169, CUDA, CUDA_CHECK(cudaFree(d_C_array));); (177, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (165, CUDA, CUDA_CHECK(cudaFree(d_A_array));); (168, CUDA, CUDA_CHECK(cudaFree(d_A[i]));); (169, CUDA, CUDA_CHECK(cudaFree(d_C[i]));); (133, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (174, CUDA, CUDA_CHECK(cudaDeviceReset());); (165, CUDA, CUDA_CHECK(cudaFree(d_B[i]));); (75, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(15, MPI, MPI_Type_get_true_extent(inkey, &key_true_lb, &key_true_extent);); (18, MPI, MPI_Type_get_true_extent(inkeysat, &keysat_true_lb, &keysat_true_extent);)", "label": "(19, MPI, MPI_Type_get_true_extent(inkeysat, &keysat_true_lb, &keysat_true_extent);); (76, MPI, MPI_Type_dup(inkey,    &values->key);); (13, MPI, MPI_Type_get_true_extent(inkey, &key_true_lb, &key_true_extent);); (75, MPI, MPI_Type_dup(inkeysat, &values->keysat);)"}
{"predict": "(31, MPI, error_code = MPI_Comm_test_inter(comm, &flag);); (56, MPI, MPI_Comm_dup(comm, &dupcomm);); (129, MPI, MPI_Comm_rank(dupcomm, &rank);); (139, MPI, MPI_Barrier(dupcomm);); (67, MPI, MPI_Allreduce(&amode, &tmp_amode, 1, MPI_INT, ADIO_same_amode, dupcomm);)", "label": "(31, MPI, error_code = MPI_Comm_test_inter(comm, &flag);); (56, MPI, MPI_Comm_dup(comm, &dupcomm);); (126, MPI, MPI_Comm_rank(dupcomm, &rank);); (136, MPI, MPI_Barrier(dupcomm);); (67, MPI, MPI_Allreduce(&amode, &tmp_amode, 1, MPI_INT, ADIO_same_amode, dupcomm);)"}
{"predict": "(14, MPI, MPI_Bcast( &nbases, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (28, MPI, MPI_Bcast( &iphysdim, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (26, MPI, MPI_Bcast( &icelldim, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (38, MPI, MPI_Bcast( &nzones, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );)", "label": "(224, MPI, MPI_Bcast( &bocotype, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (190, MPI, MPI_Bcast( &nbocos, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (202, MPI, MPI_Bcast( &location, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (38, MPI, MPI_Bcast( &nzones, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (84, MPI, MPI_Bcast( zonename, 33, MPI_CHAR, Parallel::serverid, MPI_COMM_WORLD );); (295, MPI, MPI_Bcast( range.data(), npnts * index_dim, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (219, MPI, MPI_Bcast( &ptset_type, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (251, MPI, MPI_Bcast( pnts.data(), npnts * index_dim, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (81, MPI, MPI_Bcast( isize.data(), index_dim * 3, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (113, MPI, MPI_Bcast( &zoneType, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (220, MPI, MPI_Bcast( normalIndex.data(), index_dim, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (216, MPI, MPI_Bcast( &npnts, 1, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (289, MPI, MPI_Bcast( donor_range.data(), npnts * index_dim, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (164, MPI, MPI_Bcast( pcoor, nNodes, MPI_DOUBLE, Parallel::serverid, MPI_COMM_WORLD );); (145, MPI, MPI_Bcast( &dataType, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (283, MPI, MPI_Bcast( connectname, 33, MPI_CHAR, Parallel::serverid, MPI_COMM_WORLD );); (243, MPI, MPI_Bcast( normalList.data(), nNodes * iphysdim * sizeof( double ), MPI_CHAR, Parallel::serverid, MPI_COMM_WORLD );); (-3, MPI, MPI_Bcast( &nbases, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (205, MPI, MPI_Bcast( boconame, 33, MPI_CHAR, Parallel::serverid, MPI_COMM_WORLD );); (139, MPI, MPI_Bcast( coordname, 33, MPI_CHAR, Parallel::serverid, MPI_COMM_WORLD );); (208, MPI, MPI_Bcast( &normalListSize, 1, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (256, MPI, MPI_Bcast( &n1to1, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (6, MPI, MPI_Bcast( &icelldim, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (6, MPI, MPI_Bcast( &iphysdim, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (205, MPI, MPI_Bcast( &ndataset, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (274, MPI, MPI_Bcast( donorname, 33, MPI_CHAR, Parallel::serverid, MPI_COMM_WORLD );); (104, MPI, MPI_Bcast( &ncoords, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (199, MPI, MPI_Bcast( &normalDataType, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );)"}
{"predict": "(191, MPI, MPI_Wait( &send_req[p], &status);); (191, MPI, MPI_Wait( &send_req[procs+p], &status);); (170, MPI, MPI_Recv( itemp, it, mpi_int_t, p, p, grid->comm, &status );)", "label": "(191, MPI, MPI_Wait( &send_req[p], &status);); (191, MPI, MPI_Wait( &send_req[procs+p], &status);); (170, MPI, MPI_Recv( itemp, it, mpi_int_t, p, p, grid->comm, &status );)"}
{"predict": "(19, OpenMP, #pragma omp simd aligned (argc : 5)); (10, OpenMP, #pragma omp simd aligned (argc); (17, OpenMP, #pragma omp simd aligned (*argc > 0 ? argv[1] : argv[2])); (22, OpenMP, #pragma omp simd aligned(e, g)); (27, OpenMP, #pragma omp parallel); (29, OpenMP, #pragma omp simd aligned(f)); (23, OpenMP, #pragma omp simd aligned(i)); (34, OpenMP, #pragma omp simd aligned(f:j)); (4, OpenMP, #pragma omp simd aligned ()); (25, OpenMP, #pragma omp simd aligned(v:16)); (7, OpenMP, #pragma omp simd aligned (S1)); (8, OpenMP, #pragma omp simd aligned (argv[1])); (28, OpenMP, #pragma omp simd aligned)", "label": "(25, OpenMP, #pragma omp simd aligned(e, g)); (18, OpenMP, #pragma omp simd aligned (argc : 5)); (29, OpenMP, #pragma omp simd aligned(i)); (5, OpenMP, #pragma omp simd aligned (); (17, OpenMP, #pragma omp simd aligned (S1)); (28, OpenMP, #pragma omp parallel); (40, OpenMP, #pragma omp simd aligned(f:j)); (10, OpenMP, #pragma omp simd aligned (*argc > 0 ? argv[1] : argv[2])); (6, OpenMP, #pragma omp simd aligned (argc,); (14, OpenMP, #pragma omp simd aligned (argv[1])); (-4, OpenMP, #pragma omp simd aligned); (-1, OpenMP, #pragma omp simd aligned ()); (25, OpenMP, #pragma omp simd aligned(v:16)); (28, OpenMP, #pragma omp simd aligned(f)); (14, OpenMP, #pragma omp simd aligned(h)); (-3, OpenMP, #pragma omp simd aligned (argc)"}
{"predict": "(24, CUDA, extern __shared__ char smem_[];); (26, CUDA, __shared__ half2 smem[VECS_PER_CTA];)", "label": "(24, CUDA, extern __shared__ char smem_[];); (26, CUDA, __shared__ half2 smem_red[VECS_PER_CTA][WARPS];)"}
{"predict": "(89, OpenCL, err = clGetDeviceInfo(devices[i], CL_DEVICE_NAME, sizeof(devicename), devicename, NULL);); (69, OpenCL, err = clGetDeviceInfo(devices[i], CL_DEVICE_TYPE, sizeof(device_type), &device_type, NULL);); (79, OpenCL, err = clBuildProgram(plan->program, 1, &devices[i], NULL, NULL, NULL);); (57, OpenCL, plan->program = clCreateProgramWithSource(context, 1, &source_str, NULL, &err);); (113, OpenCL, plan->program = clCreateProgramWithSource(context, 1, &source_str, NULL, &err);); (88, OpenCL, clReleaseProgram(plan->program);)", "label": "(76, OpenCL, err = clBuildProgram(plan->program, 1, &devices[i], coptions, NULL, NULL);); (59, OpenCL, plan->program = clCreateProgramWithSource(context, 1, (const char**) &source_str, NULL, &err);); (68, OpenCL, err = clGetDeviceInfo(devices[i], CL_DEVICE_TYPE, sizeof(device_type), &device_type, NULL);); (88, OpenCL, err = clGetDeviceInfo(devices[i], CL_DEVICE_NAME, sizeof(devicename), devicename, NULL);)"}
{"predict": "(22, OpenMP, #pragma omp parallel master taskloop simd collapse (foobool(argc)), collapse (true), collapse (-5)); (35, OpenMP, #pragma omp parallel master taskloop simd collapse (2)); (4, OpenMP, #pragma omp parallel master taskloop simd collapse ()); (10, OpenMP, #pragma omp parallel master taskloop simd collapse (ST); (11, OpenMP, #pragma omp parallel master taskloop simd collapse (1))); (-3, OpenMP, #pragma omp parallel master taskloop simd collapse); (26, OpenMP, #pragma omp parallel master taskloop simd collapse (1)); (28, OpenMP, #pragma omp parallel master taskloop simd collapse (N)); (9, OpenMP, #pragma omp parallel master taskloop simd collapse ((ST > 0) ? 1 + ST : 2)); (15, OpenMP, #pragma omp parallel master taskloop simd collapse (S)); (5, OpenMP, #pragma omp parallel master taskloop simd collapse (1 + ST))", "label": "(13, OpenMP, #pragma omp parallel master taskloop simd collapse (ST); (5, OpenMP, #pragma omp parallel master taskloop simd collapse ()); (15, OpenMP, #pragma omp parallel master taskloop simd collapse ((ST > 0) ? 1 + ST : 2)); (1, OpenMP, #pragma omp parallel master taskloop simd collapse (); (20, OpenMP, #pragma omp parallel master taskloop simd collapse (S)); (10, OpenMP, #pragma omp parallel master taskloop simd collapse (1))); (27, OpenMP, #pragma omp parallel master taskloop simd collapse (1)); (30, OpenMP, #pragma omp parallel master taskloop simd collapse (2)); (14, OpenMP, #pragma omp parallel master taskloop simd collapse (foobool(argc)), collapse (true), collapse (-5)); (-7, OpenMP, #pragma omp parallel master taskloop simd collapse); (0, OpenMP, #pragma omp parallel master taskloop simd collapse (argc); (24, OpenMP, #pragma omp parallel master taskloop simd collapse (N))"}
{"predict": "(53, OpenMP, #pragma omp distribute private(i)); (21, OpenMP, #pragma omp parallel private (argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (73, OpenMP, #pragma omp teams reduction(+:i)); (31, OpenMP, #pragma omp parallel private(i)); (36, OpenMP, #pragma omp teams private(i)); (15, OpenMP, #pragma omp parallel private (argc > 0 ? argv[1] : argv[2])); (59, OpenMP, #pragma omp distribute reduction(+:i)); (23, OpenMP, #pragma omp parallel private (S1)); (47, OpenMP, #pragma omp teams firstprivate(i)); (64, OpenMP, #pragma omp distribute private(j)); (34, OpenMP, #pragma omp distribute private(i)); (24, OpenMP, #pragma omp parallel private(ba) allocate(omp_thread_mem_alloc: ba)); (26, OpenMP, #pragma omp parallel private(da)); (57, OpenMP, #pragma omp distribute private(g)); (19, OpenMP, #pragma omp parallel private (argv[1])); (30, OpenMP, #pragma omp teams); (25, OpenMP, #pragma omp parallel private(e, g)); (-2, OpenMP, #pragma omp target); (-2, OpenMP, #pragma omp parallel private); (40, OpenMP, #pragma omp distribute firstprivate(i)); (-2, OpenMP, #pragma omp parallel private (); (14, OpenMP, #pragma omp parallel private (a, b, c, d, f)); (24, OpenMP, #pragma omp parallel shared(i)); (17, OpenMP, #pragma omp parallel private(ca))", "label": "(35, OpenMP, #pragma omp distribute private(e, g)); (51, OpenMP, #pragma omp parallel private(i)); (5, OpenMP, #pragma omp distribute private); (12, OpenMP, #pragma omp distribute private (argc,); (9, OpenMP, #pragma omp distribute private (argc); (36, OpenMP, #pragma omp target); (19, OpenMP, #pragma omp distribute private (argv[1])); (12, OpenMP, #pragma omp distribute private (argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (9, OpenMP, #pragma omp distribute private (argc > 0 ? argv[1] : argv[2])); (14, OpenMP, #pragma omp distribute private (a, b, c, d, f)); (17, OpenMP, #pragma omp distribute private(ba)); (39, OpenMP, #pragma omp distribute private(j)); (-1, OpenMP, #pragma omp distribute private ()); (24, OpenMP, #pragma omp distribute private(h)); (44, OpenMP, #pragma omp teams reduction(+:i)); (18, OpenMP, #pragma omp distribute private(S2::S2s)); (43, OpenMP, #pragma omp distribute private(i)); (14, OpenMP, #pragma omp distribute private(da)); (-9, OpenMP, #pragma omp distribute private (); (2, OpenMP, #pragma omp distribute private (S1)); (29, OpenMP, #pragma omp teams private(i)); (8, OpenMP, #pragma omp distribute private(ca)); (17, OpenMP, #pragma omp distribute); (22, OpenMP, #pragma omp distribute firstprivate(i)); (30, OpenMP, #pragma omp teams firstprivate(i)); (17, OpenMP, #pragma omp teams)"}
{"predict": "(26, MPI, MPI_Barrier(params.comm);); (2, MPI, MPI_Init(&argc, &argv);); (164, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_size(params.comm, &size);); (3, MPI, MPI_Comm_rank(params.comm, &rank);); (23, MPI, MPI_Flush(MPI_STDOUT);)", "label": "(3, MPI, MPI_Init(&argc, &argv);); (303, MPI, MPI_Finalize();); (23, MPI, MPI_Barrier(MPI_COMM_WORLD);); (222, MPI, MPI_Allreduce(&local_opt_worst_time, &opt_worst_time, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);)"}
{"predict": "(97, MPI, MPI_Query_thread(&omp_mpi_level);); (38, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (33, MPI, MPI_Init_thread(&argc, &argv, required, &provided);)", "label": "(97, MPI, MPI_Query_thread(&omp_mpi_level);); (38, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (33, MPI, MPI_Init_thread(&argc, &argv, required, &provided);)"}
{"predict": "(431, MPI, start_exch = MPI_Wtime();); (146, MPI, start_comm_time = MPI_Wtime();); (159, MPI, end_comm_time = MPI_Wtime();); (486, MPI, start_write_time = MPI_Wtime();); (492, MPI, end_write_time = MPI_Wtime();); (529, MPI, end_exch = MPI_Wtime();)", "label": "(391, MPI, start_exch = MPI_Wtime();); (143, MPI, start_comm_time = MPI_Wtime();); (156, MPI, end_comm_time = MPI_Wtime();); (435, MPI, start_write_time = MPI_Wtime();); (442, MPI, end_write_time = MPI_Wtime();); (495, MPI, end_exch = MPI_Wtime();)"}
{"predict": "(25, MPI, MPI_Isend(sendbuf, BUF_COUNT, MPI_INT, i, 1, MPI_COMM_WORLD, &sendreqs[i]);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (24, MPI, MPI_Waitall(nprocs, sendreqs, sendstats);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (40, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (42, MPI, MPI_Waitall(nprocs, sendreqs, sendstats);); (13, MPI, MPI_Barrier(MPI_COMM_WORLD);); (21, MPI, MPI_Isend(sendbuf, BUF_COUNT, MPI_INT, i, 0, MPI_COMM_WORLD, &sendreqs[i]);)"}
{"predict": "(321, CUDA, cudaStat = cudaMemset(d_info_geqrf, 0, sizeof(int));); (48, CUDA, cudaStat = cudaSetDevice(localDeviceId);); (257, CUDA, cudaStat = cudaMalloc((void**)&d_work_geqrf, workspaceInBytesOnDevice_geqrf);); (23, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rankId);); (209, CUDA, cudaStat = cudaMalloc((void**)&d_tau, sizeof(double) * colsA);); (326, CUDA, cudaStat = cudaMemcpyAsync(&h_info_geqrf, d_info_geqrf, sizeof(int), cudaMemcpyDeviceToHost, localStream);); (231, CUDA, cudaStat = cudaMemset(d_info_geqrf, -1, sizeof(int));); (216, CUDA, cudaStat = cudaMalloc((void**)&d_A, sizeof(double) * lda * colsA);); (319, CUDA, cudaStat = cudaStreamSynchronize(localStream);); (227, CUDA, cudaStat = cudaMalloc((void**)&d_info_geqrf, sizeof(int));); (328, MPI, MPI_Barrier(MPI_COMM_WORLD);); (42, CUDA, cudaStat = cudaFree(0);); (17, MPI, MPI_Comm_size(MPI_COMM_WORLD, &rankSize);); (246, CUDA, cudaStat = cudaMallocHost((void**)&h_work_geqrf, workspaceInBytesOnHost_geqrf);); (222, CUDA, cudaStat = cudaMemset(d_tau, 0, sizeof(double) * colsA);); (313, CUDA, cudaStat = cudaMemcpyAsync(d_A, h_A, sizeof(double) * lda * colsA, cudaMemcpyHostToDevice, localStream);); (219, CUDA, cudaStat = cudaMemset(d_A, 0, sizeof(double) * lda * colsA);); (287, CUDA, cudaStat = cudaMalloc((void**)&d_QR, sizeof(double) * lda * colsA);); (332, CUDA, cudaStat = cudaFree(d_QR);); (307, CUDA, cudaStat = cudaMemcpyAsync(&h_tau, d_tau, sizeof(double) * colsA, cudaMemcpyDeviceToHost, localStream);); (321, CUDA, cuda", "label": "(358, CUDA, cudaStat = cudaMemcpy(h_A, d_global_Q, sizeof(double) * lda * colsA, cudaMemcpyDeviceToHost);); (196, CUDA, cudaStat = cudaMalloc((void**)&d_info_geqrf, sizeof(int));); (27, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rankId);); (429, CUDA, cudaStat = cudaFree(d_tau);); (-1, MPI, MPI_Init(NULL, NULL);); (274, CUDA, cudaStat = cudaMemcpyAsync(&h_info_geqrf, d_info_geqrf, sizeof(int), cudaMemcpyDeviceToHost, localStream);); (471, CUDA, cudaStat = cudaStreamDestroy(localStream);); (474, MPI, MPI_Barrier(MPI_COMM_WORLD);); (42, CUDA, cudaStat = cudaSetDevice(localDeviceId);); (43, CUDA, cudaStat = cudaFree(0);); (194, CUDA, cudaStat = cudaMemset(d_info_geqrf, 0, sizeof(int));); (428, CUDA, cudaStat = cudaFree(d_work_geqrf);); (178, CUDA, cudaStat = cudaMalloc((void**)&d_tau, localColsA * sizeof(double));); (301, CUDA, cudaStat = cudaMalloc((void**)&d_global_Q, lda * colsA * sizeof(double));); (213, CUDA, cudaStat = cudaMalloc((void**)&d_work_geqrf, workspaceInBytesOnDevice_geqrf);); (431, CUDA, cudaStat = cudaFree(d_info_geqrf);); (12, MPI, MPI_Comm_size(MPI_COMM_WORLD, &rankSize);); (154, CUDA, cudaStat = cudaMalloc((void**)&d_A, localColsA * LLDA * sizeof(double));); (305, CUDA, cudaStat = cudaMemcpy(d_global_R, d_global_Q, sizeof(double) * lda * colsA, cudaMemcpyDeviceToDevice);); (302, CUDA, cudaStat = cudaMemcpy(d_global_Q, h_A, sizeof(double) * lda * colsA, cudaMemcpyHostToDevice);); (53, CUDA, cudaStat = cudaStreamCreate(&localStream);); (463, MPI, MPI_Finalize();); (403, CUDA, cudaStat = cudaFree(d_A);); (293, CUDA, cudaStat = cudaMalloc((void**)&d_global_R, lda * colsA * sizeof(double));); (294, CUDA, cudaStat = cudaMalloc((void**)&d_global_tau, colsA * sizeof(double));)"}
{"predict": "(103, CUDA, cudaFree( dptrForce[i] );); (99, CUDA, cudaFree( dptrPosMass[i] );); (23, CUDA, cudaSetDevice( oldDevice );)", "label": "(98, CUDA, cudaFree( dptrPosMass[i] );); (100, CUDA, cudaSetDevice( oldDevice );); (97, CUDA, cudaFree( dptrForce[i] );)"}
{"predict": "(90, OpenMP, #pragma omp parallel master taskloop simd firstprivate(m)); (127, OpenMP, #pragma omp parallel private(i)); (44, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argv[1])); (133, OpenMP, #pragma omp parallel master taskloop simd firstprivate(B::x)); (35, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (1, OpenMP, #pragma omp parallel); (75, OpenMP, #pragma omp parallel master taskloop simd safelen(5)); (65, OpenMP, #pragma omp parallel master taskloop simd firstprivate(S2::S2sc)); (112, OpenMP, #pragma omp parallel master taskloop simd lastprivate(g) firstprivate(g)); (83, OpenMP, #pragma omp parallel master taskloop simd private(xa), firstprivate(xa)); (123, OpenMP, #pragma omp parallel reduction(+ : i)); (68, OpenMP, #pragma omp parallel master taskloop simd firstprivate(e, g)); (35, OpenMP, #pragma omp parallel master taskloop simd firstprivate(2 * 2)); (95, OpenMP, #pragma omp parallel shared(xa)); (115, OpenMP, #pragma omp parallel master taskloop simd firstprivate(si)); (78, OpenMP, #pragma omp parallel master taskloop simd firstprivate(h)); (46, OpenMP, #pragma omp parallel master taskloop simd firstprivate(ca)); (48, OpenMP, #pragma omp parallel master taskloop simd firstprivate(da)); (1, OpenMP, #pragma omp parallel master taskloop simd firstprivate(); (85, OpenMP, #pragma omp parallel master taskloop simd firstprivate(j)); (57, OpenMP, #pragma omp parallel master taskloop simd firstprivate(S2::S2s)); (-6, OpenMP, #pragma omp parallel master taskloop simd firstprivate); (2, OpenMP, #pragma omp parallel master taskloop simd firstprivate()); (27, OpenMP, #pragma omp parallel master taskloop simd firstprivate(ba)); (", "label": "(22, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argc); (78, OpenMP, #pragma omp parallel master taskloop simd safelen(5)); (81, OpenMP, #pragma omp parallel master taskloop simd firstprivate(e, g)); (125, OpenMP, #pragma omp parallel private(i)); (87, OpenMP, #pragma omp parallel master taskloop simd firstprivate(h)); (53, OpenMP, #pragma omp parallel master taskloop simd firstprivate(ca)); (48, OpenMP, #pragma omp parallel master taskloop simd firstprivate(ba)); (27, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (38, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argv[1])); (90, OpenMP, #pragma omp parallel master taskloop simd firstprivate(i)); (-1, OpenMP, #pragma omp parallel); (64, OpenMP, #pragma omp parallel master taskloop simd firstprivate(S2::S2sc)); (128, OpenMP, #pragma omp parallel master taskloop simd firstprivate(B::x)); (119, OpenMP, #pragma omp parallel reduction(+ : i)); (88, OpenMP, #pragma omp parallel shared(xa)); (23, OpenMP, #pragma omp parallel master taskloop simd firstprivate(S1)); (14, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argc > 0 ? argv[1] : argv[2])); (78, OpenMP, #pragma omp parallel master taskloop simd private(xa), firstprivate(xa)); (53, OpenMP, #pragma omp parallel master taskloop simd firstprivate(S2::S2s)); (125, OpenMP, #pragma omp parallel master taskloop simd firstprivate(si)); (-6, OpenMP, #pragma omp parallel master taskloop simd firstprivate(); (-3, OpenMP, #pragma omp parallel master taskloop simd firstprivate()); (65, OpenMP, #pragma omp parallel master taskloop simd firstprivate(m)); (84, OpenMP, #pragma omp parallel master taskloop simd firstprivate(j)); (18, OpenMP, #pragma omp parallel master taskloop simd firstprivate(a, b, c, d, f)); (86, OpenMP, #pragma omp parallel master taskloop simd lastprivate(g) firstprivate(g)); (89, OpenMP, #pragma omp parallel master taskloop simd lastprivate(n) firstprivate(n)); (-1, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argc,); (39, OpenMP, #pragma omp parallel master taskloop simd firstprivate(xa)); (21, OpenMP, #pragma omp parallel master taskloop simd firstprivate(2 * 2)); (32, OpenMP, #pragma omp parallel master taskloop simd firstprivate(da)); (-21, OpenMP, #pragma omp parallel master taskloop simd firstprivate)"}
{"predict": "(91, OpenMP, comp_error(\"comp_do_parse()\", \"invalid token\");)", "label": "(34, OpenMP, comp_do_parse(cur, tail);)"}
{"predict": "(11, MPI, MPI_Error_string(error, err_string, &length);); (38, MPI, MPI_Abort(MPI_COMM_WORLD, error);); (20, MPI, error = MPI_Init(&argc, &argv);); (65, MPI, error = MPI_Finalize();); (5, MPI, error = MPI_Finalized(&flag);); (25, MPI, error = MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (52, MPI, error = MPI_Barrier(MPI_COMM_WORLD);)", "label": "(11, MPI, MPI_Error_string(error, err_string, &length);); (20, MPI, error = MPI_Init(&argc, &argv);); (27, MPI, error = MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (60, MPI, error = MPI_Finalize();); (50, MPI, error = MPI_Barrier(MPI_COMM_WORLD);); (29, MPI, MPI_Abort(MPI_COMM_WORLD, error);); (3, MPI, error = MPI_Finalized(&flag);)"}
{"predict": "(74, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamCreate(&stream));); (71, CUDA, CUDA_CHECK_AND_EXIT(cudaMemset(output, 0b11111111, flat_fft_size_bytes));); (71, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(input, input_host.data(), flat_fft_size_bytes, cudaMemcpyHostToDevice));); (91, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(output));); (89, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(input));); (65, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&input, flat_fft_size_bytes));); (65, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&output, flat_fft_size_bytes));); (83, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamDestroy(stream));)", "label": "(77, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamCreate(&stream));); (71, CUDA, CUDA_CHECK_AND_EXIT(cudaMemset(output, 0b11111111, flat_fft_size_bytes));); (71, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(input, input_host.data(), flat_fft_size_bytes, cudaMemcpyHostToDevice));); (91, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(output));); (70, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (88, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(input));); (62, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&input, flat_fft_size_bytes));); (83, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamDestroy(stream));); (61, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&output, flat_fft_size_bytes));)"}
{"predict": "(160, OpenCL, error = clSetKernelArg( kernel, 0, sizeof( streams[0] ), &streams[0] );); (161, OpenCL, error = clSetKernelArg( kernel, 1, sizeof( streams[1] ), &streams[1] );); (106, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 1, NULL, threads, localThreads, 0, NULL, NULL );); (162, OpenCL, error = clSetKernelArg( kernel, 2, sizeof( streams[2] ), &streams[2] );); (93, OpenCL, error = clSetKernelArg( kernel, i, sizeof( streams[i] ), &streams[i] );); (110, OpenCL, error = clEnqueueReadBuffer( queue, streams[2], CL_TRUE, 0, sizeof( cl_float ) * TEST_SIZE, outData, 0, NULL, NULL );); (19, OpenCL, error = clGetDeviceInfo( device, CL_DEVICE_SINGLE_FP_CONFIG, sizeof( config ), &config, NULL );); (12, OpenCL, error = clGetDeviceInfo( device, CL_DEVICE_PROFILE, sizeof( kernelSource ), kernelSource, NULL );)", "label": "(108, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 1, NULL, threads, localThreads, 0, NULL, NULL );); (22, OpenCL, error = clGetDeviceInfo( device, CL_DEVICE_SINGLE_FP_CONFIG, sizeof( config ), &config, NULL );); (16, OpenCL, error = clGetDeviceInfo( device, CL_DEVICE_PROFILE, sizeof( kernelSource ), kernelSource, NULL );); (95, OpenCL, error = clSetKernelArg(kernel, (int)i, sizeof( streams[i] ), &streams[i]);); (108, OpenCL, error = clEnqueueReadBuffer( queue, streams[2], true, 0, sizeof( cl_float ) * TEST_SIZE, outData, 0, NULL, NULL );)"}
{"predict": "(11, OpenMP, #pragma omp target); (11, OpenMP, #pragma omp teams distribute parallel for); (39, OpenMP, #pragma omp task shared(copiedSrcMatA)); (33, OpenMP, #pragma omp task shared(matrixBAllocated, deviceSrcMatB)); (73, OpenMP, #pragma omp task shared(matrixAAllocated, deviceSrcMatA)); (43, OpenMP, #pragma omp taskwait); (71, OpenMP, #pragma omp task shared(matrixCAllocated, deviceDstMat)); (100, OpenMP, #pragma omp target teams distribute parallel for); (9, OpenMP, #pragma omp team)", "label": "(11, OpenMP, #pragma omp parallel for schedule(static,1)); (12, OpenMP, #pragma omp task)"}
{"predict": "(61, OpenCL, clSetKernelArg(kernel, 2, sizeof(cl_mem), &array);); (266, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(cl_uint), &size);); (216, OpenCL, err = clSetKernelArg(kernel, 1, sizeof(cl_uint), &dim);); (61, OpenCL, clSetKernelArg(kernel, 3, sizeof(cl_uint), &min_dim);); (233, OpenCL, err = clSetKernelArg(kernel, 4, sizeof(cl_uint), &max_dim);); (255, OpenCL, err = clSetKernelArg(kernel, 5, sizeof(cl_uint), &extra_dim);); (55, OpenCL, clSetKernelArg(kernel, 1, sizeof(cl_uint), &dimensions);); (265, OpenCL, err = clSetKernelArg(kernel, 6, sizeof(cl_uint), &size_increase_per_iteration);); (54, OpenCL, clSetKernelArg(kernel, 0, sizeof(cl_ulong), &global_size);); (55, OpenCL, clSetKernelArg(kernel, 2, sizeof(cl_uint), &max_dim);); (222, OpenCL, err = clSetKernelArg(kernel, 3, sizeof(cl_uint), &min_dim);); (254, OpenCL, err = clSetKernelArg(kernel, 7, sizeof(cl_uint), &explicit_local);); (50, OpenCL, clSetKernelArg(kernel, 0, sizeof(cl_ulong), &min_global_size);); (52, OpenCL, clSetKernelArg(kernel, 1, sizeof(cl_uint), &max_dim);); (218, OpenCL, err = clSetKernelArg(kernel, 2, sizeof(cl_uint), &max_x_size);); (220, OpenCL, err = clSetKernelArg(kernel, 5, sizeof(cl_uint), &max_z_size);); (217, OpenCL, err = clSetKernelArg(kernel, 4, sizeof(cl_uint), &max_y_size);); (221, OpenCL, err = clSetKernelArg(kernel, 8, sizeof(cl_uint), &use_atomics);); (44, OpenCL, clSetKernelArg(kernel, 0, sizeof(cl_ulong), &max_global_size););", "label": "(558, OpenCL, clReleaseKernel(kernel);); (558, OpenCL, clReleaseProgram(program);); (555, OpenCL, clReleaseMemObject(array);)"}
{"predict": "(37, CUDA, cudaDeviceSynchronize();); (7, CUDA, cx::ok(cudaSetDevice(0));)", "label": "(34, CUDA, checkCudaErrors(cudaDeviceSynchronize());)"}
{"predict": "(18, CUDA, checkRuntime(cudaStreamCreate(&stream));); (80, CUDA, checkRuntime(cudaStreamSynchronize(stream));); (57, CUDA, checkRuntime(cudaMemcpyAsync(input_data_device, input_data_host, input_numel * sizeof(float), cudaMemcpyHostToDevice, stream));); (88, CUDA, checkRuntime(cudaFreeHost(input_data_host));); (86, CUDA, checkRuntime(cudaFree(output_data_device));); (64, CUDA, checkRuntime(cudaMalloc(&output_data_device, output_numel * sizeof(float)));); (62, CUDA, checkRuntime(cudaMallocHost(&output_data_host, output_numel * sizeof(float)));); (-5, CUDA, auto engine = make_nvshared(runtime->deserializeCudaEngine(engine_data.data(), engine_data.size()));); (56, CUDA, checkRuntime(cudaMemcpyAsync(output_data_host, output_data_device, output_numel * sizeof(float), cudaMemcpyDeviceToHost, stream));); (80, CUDA, checkRuntime(cudaFree(input_data_device));); (50, CUDA, checkRuntime(cudaMalloc(&input_data_device, input_numel * sizeof(float)));); (48, CUDA, checkRuntime(cudaMallocHost(&input_data_host, input_numel * sizeof(float)));); (79, CUDA, checkRuntime(cudaFreeHost(output_data_host));); (75, CUDA, checkRuntime(cudaStreamDestroy(stream));)", "label": "(18, CUDA, checkRuntime(cudaStreamCreate(&stream));); (79, CUDA, checkRuntime(cudaMemcpyAsync(output_data_host, output_data_device, sizeof(float) * output_numel, cudaMemcpyDeviceToHost, stream));); (79, CUDA, checkRuntime(cudaStreamSynchronize(stream));); (57, CUDA, checkRuntime(cudaMemcpyAsync(input_data_device, input_data_host, input_numel * sizeof(float), cudaMemcpyHostToDevice, stream));); (24, CUDA, checkRuntime(cudaMallocHost(&input_data_host, input_numel * sizeof(float)));); (24, CUDA, checkRuntime(cudaMalloc(&input_data_device, input_numel * sizeof(float)));); (89, CUDA, checkRuntime(cudaFree(input_data_device));); (86, CUDA, checkRuntime(cudaFreeHost(input_data_host));); (88, CUDA, checkRuntime(cudaFree(output_data_device));); (62, CUDA, checkRuntime(cudaMalloc(&output_data_device, sizeof(float) * output_numel));); (-5, CUDA, auto engine = make_nvshared(runtime->deserializeCudaEngine(engine_data.data(), engine_data.size()));); (59, CUDA, checkRuntime(cudaMallocHost(&output_data_host, sizeof(float) * output_numel));); (82, CUDA, checkRuntime(cudaFreeHost(output_data_host));); (79, CUDA, checkRuntime(cudaStreamDestroy(stream));)"}
{"predict": "(145, OpenMP, #pragma omp task firstprivate(r)); (68, OpenMP, #pragma omp task default(shared)); (80, OpenMP, #pragma omp parallel shared(sa)); (65, OpenMP, #pragma omp task default(none)); (132, OpenMP, #pragma omp task private(r)); (139, OpenMP, #pragma omp parallel reduction(+ : r)); (175, OpenMP, #pragma omp task if(argc) default(none)); (66, OpenMP, #pragma omp task sa(a, b)); (122, OpenMP, #pragma omp for reduction(+ : r)); (140, OpenMP, #pragma omp task shared(a, b, sa, sb)); (134, OpenMP, #pragma omp task private(a, b)); (152, OpenMP, #pragma omp task if(argc)); (137, OpenMP, #pragma omp task firstprivate(a, b, r)); (159, OpenMP, #pragma omp task default(none) firstprivate(a, b, r)); (131, OpenMP, #pragma omp task shared(sa, sb)); (33, OpenMP, #pragma omp task default(shared)); (139, OpenMP, #pragma omp parallel private(a, b)); (160, OpenMP, #pragma omp task default(firstprivate)); (165, OpenMP, #pragma omp task default(none) private(a, b, r)); (37, OpenMP, #pragma omp task); (124, OpenMP, #pragma omp parallel shared(a, b, sa, sb)); (156, OpenMP, #pragma omp task default(shared) firstprivate(a, b, r));(-3, OpenMP, #pragma omp task unknown); (-4, OpenMP, #pragma omp task); (55, OpenMP, #pragma omp task default(none) shared(a, b, sa, sb)); (126, OpenMP, #pragma omp task default(firstprivate) firstprivate(a, b, r)); (150, OpenMP, #pragma omp task default(none) shared(a, b, sa, sb)); (44, OpenMP, #pragma omp task default(none) firstprivate(a, b, r)); (115, OpenMP, #", "label": "(174, OpenMP, #pragma omp task mergeable detach(evt)); (19, OpenMP, #pragma omp task unknown()); (14, OpenMP, #pragma omp task }); (154, OpenMP, #pragma omp task untied untied); (106, OpenMP, #pragma omp parallel shared(sa, sb)); (125, OpenMP, #pragma omp for reduction(+ : r)); (109, OpenMP, #pragma omp task firstprivate(r)); (160, OpenMP, #pragma omp task detach(); (-2, OpenMP, #pragma omp task); (104, OpenMP, #pragma omp parallel reduction(+ : r)); (160, OpenMP, #pragma omp task detach(evt) detach(evt)); (161, OpenMP, #pragma omp task detach(evt) mergeable); (60, OpenMP, #pragma omp parallel); (162, OpenMP, #pragma omp task detach(-evt)); (-6, OpenMP, #pragma omp task(); (48, OpenMP, #pragma omp task default(shared)); (-6, OpenMP, #pragma omp task[); (152, OpenMP, #pragma omp task detach(a)); (-6, OpenMP, #pragma omp task]); (58, OpenMP, #pragma omp parallel shared(a, b)); (157, OpenMP, #pragma omp task detach(evt) shared(evt)); (-7, OpenMP, #pragma omp task)); (144, OpenMP, #pragma omp task detach); (148, OpenMP, #pragma omp task detach(cevt) detach(revt)); (136, OpenMP, #pragma omp task mergeable mergeable); (153, OpenMP, #pragma omp task detach(evt) firstprivate(evt)); (19, OpenMP, #pragma omp task default(none)); (141, OpenMP, #pragma omp task detach()); (66, OpenMP, #pragma omp parallel shared(sa))"}
{"predict": "(133, MPI, MPI_Init_thread(argc, argv, MPI_THREAD_MULTIPLE, &provided);); (34, MPI, MPI_Get_library_version(mpiVersion, &resultLen);); (152, MPI, MPI_Comm_size(MPI_COMM_WORLD, &commSize);); (152, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &commRank);); (138, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(35, MPI, MPI_Get_library_version(mpiVersion, &resultLen);); (132, MPI, ret = MPI_Init_thread(argc, argv, MPI_THREAD_MULTIPLE, &provided);); (146, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (126, MPI, MPI_Initialized(&isMpiInited);); (0, MPI, MPI_Finalized(&isMpiFinalized);)"}
{"predict": "(34, OpenMP, #pragma omp for simd dispatch); (14, OpenMP, #pragma omp for simd linear); (76, OpenMP, #pragma omp for simd linear(2)); (55, OpenMP, #pragma omp for simd linear(group:)); (41, OpenMP, #pragma omp for simd linear(x)); (2, OpenMP, #pragma omp for simd); (84, OpenMP, #pragma omp for simd linear(concurrent:2)); (62, OpenMP, #pragma omp for simd linear(access:)); (32, OpenMP, #pragma omp for simd linear()); (95, OpenMP, #pragma omp for simd linear(uval(i))); (50, OpenMP, #pragma omp for simd linear(val(i):2)); (72, OpenMP, #pragma omp for simd linear(group:2)); (13, OpenMP, #pragma omp for simd linear()); (55, OpenMP, #pragma omp for simd linear(uval(i))); (-1, OpenMP, #pragma omp for simd linear(group); (73, OpenMP, #pragma omp for simd linear(concurrent:); (15, OpenMP, #pragma omp for simd linear(x:2)); (-7, OpenMP, #pragma omp for simd linear(); (62, OpenMP, #pragma omp for simd linear(concurrent:)); (4, OpenMP, #pragma omp for simd dispatch)", "label": "(137, OpenMP, #pragma omp for schedule(nonmonotonic: auto)); (3, OpenMP, #pragma omp for); (47, OpenMP, #pragma omp for schedule(guided)); (106, OpenMP, #pragma omp for schedule(monotonic: dynamic)); (41, OpenMP, #pragma omp for simd schedule(runtime)); (56, OpenMP, #pragma omp for simd schedule(dynamic)); (107, OpenMP, #pragma omp for simd schedule(monotonic: dynamic)); (46, OpenMP, #pragma omp for simd schedule(guided)); (89, OpenMP, #pragma omp for simd schedule(monotonic: runtime)); (10, OpenMP, #pragma omp for simd schedule(static)); (83, OpenMP, #pragma omp for schedule(monotonic: runtime)); (90, OpenMP, #pragma omp for schedule(monotonic: guided)); (144, OpenMP, #pragma omp for schedule(static, 2) ordered(1)); (28, OpenMP, #pragma omp for schedule(runtime)); (62, OpenMP, #pragma omp for schedule(monotonic: static, 2)); (-1, OpenMP, #pragma omp for schedule(static)); (21, OpenMP, #pragma omp for simd schedule(auto)); (144, OpenMP, #pragma omp for simd schedule(static, 2) ordered); (152, OpenMP, #pragma omp for simd schedule(auto) ordered); (66, OpenMP, #pragma omp for schedule(monotonic: auto)); (105, OpenMP, #pragma omp for schedule(nonmonotonic: dynamic)); (145, OpenMP, #pragma omp for schedule(auto) ordered(1)); (67, OpenMP, #pragma omp for simd schedule(monotonic: auto)); (110, OpenMP, #pragma omp for simd schedule(nonmonotonic: static)); (57, OpenMP, #pragma omp for simd schedule(monotonic: static, 2)); (46, OpenMP, #pragma omp for simd schedule(monotonic: static)); (-2, OpenMP, #pragma omp for schedule(static, 2)); (39, OpenMP, #pragma omp for schedule(monotonic: static)); (113, OpenMP, #pragma omp for simd schedule(nonmonotonic: runtime)); (28, OpenMP, #pragma omp for schedule(dynamic)); (87, OpenMP, #pragma omp for schedule(nonmonotonic: guided)); (149, OpenMP, #pragma omp for simd schedule(runtime) ordered); (97, OpenMP, #pragma omp for simd schedule(nonmonotonic: dynamic)); (157, OpenMP, #pragma omp for simd schedule(guided) ordered); (-25, OpenMP, #pragma omp for simd); (111, OpenMP, #pragma omp for schedule(static) ordered); (69, OpenMP, #pragma omp for simd schedule(monotonic: guided)); (-4, OpenMP, #pragma omp for schedule(auto)); (83, OpenMP, #pragma omp for simd schedule(nonmonotonic: guided)); (156, OpenMP, #pragma omp for schedule(dynamic) ordered(1)); (111, OpenMP, #pragma omp for simd schedule(static) ordered); (144, OpenMP, #pragma omp for schedule(guided) ordered(1)); (-13, OpenMP, #pragma omp for simd schedule(static, 2)); (132, OpenMP, #pragma omp for schedule(runtime) ordered)"}
{"predict": "(46, OpenMP, #pragma omp for private(j,k,m)); (105, OpenMP, #pragma omp for private(j,k,m,rho_inv,uijk,up1,um1,vijk,vp1,vm1,wijk,wp1,wm1)); (8, OpenMP, #pragma omp for private(j,k,m)); (126, OpenMP, #pragma omp for private(j,k,m,rho_inv,uijk,up1,um1)); (30, OpenMP, #pragma omp for private(j,k,m)); (143, OpenMP, #pragma omp for private(j,k,m,rho_inv,uijk,up1,um1,vijk,vp1,vm1)); (13, OpenMP, #pragma omp for private(j,k))", "label": "(171, OpenMP, #pragma omp for private(j,k,vijk,vp1,vm1)); (397, OpenMP, #pragma omp for private(j,m)); (7, OpenMP, #pragma omp for nowait private(j,k,rho_inv)); (127, OpenMP, #pragma omp for nowait private(j,k,m)); (103, OpenMP, #pragma omp for nowait private(k,m)); (343, OpenMP, #pragma omp for nowait private(j,m)); (151, OpenMP, #pragma omp for private(k,m)); (25, OpenMP, #pragma omp for private(j,k,m)); (38, OpenMP, #pragma omp for private(j,k,uijk,up1,um1)); (282, OpenMP, #pragma omp for private(j,k,wijk,wp1,wm1)); (399, OpenMP, #pragma omp for private(k,m,i))"}
{"predict": "(8, MPI, MPI_Init(&argc, &argv);); (24, MPI, MPI_Type_free(&ddt);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (10, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)", "label": "(28, MPI, MPI_Type_free(&ddt);); (7, MPI, MPI_Init(&argc, &argv);); (12, MPI, MPI_Finalize();); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (21, MPI, MPI_DDT_DUMP(ddt);)"}
{"predict": "(38, MPI, MPI_Comm_size(comm, &np);); (56, MPI, MPI_Recv(inbufs[j], count, types[j], partner, tag, comm, &status);); (44, MPI, MPI_Type_get_name(types[j], myname, &mynamelen);); (40, MPI, MPI_Comm_rank(comm, &rank);); (12, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (45, MPI, MPI_Send(outbufs[j], count, types[j], partner, tag, comm);); (52, MPI, MPI_Get_count(&status, types[j], &count);)", "label": "(57, MPI, MPI_Recv(outbufs[j], counts[j], types[j], partner, tag, comm, &status);); (45, MPI, MPI_Type_get_name(types[j], myname, &mynamelen);); (49, MPI, MPI_Send(inbufs[j], counts[j], types[j], partner, tag, comm);); (35, MPI, MPI_Comm_size(comm, &np);); (25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (32, MPI, MPI_Comm_rank(comm, &rank);); (53, MPI, MPI_Get_count(&status, types[j], &count);)"}
{"predict": "(14, MPI, MPI_Init(&argc, &argv);); (148, MPI, MPI_Finalize();); (72, MPI, t0 = MPI_Wtime();); (73, MPI, t1 = MPI_Wtime();)", "label": "(14, MPI, MPI_Init(&argc, &argv);); (148, MPI, MPI_Finalize();); (72, MPI, t0 = MPI_Wtime();); (73, MPI, t1 = MPI_Wtime();)"}
{"predict": "(49, MPI, MPI_Allreduce(MPI_IN_PLACE,&n,1,mpi_int_t,MPI_MAX,grid->comm);); (47, MPI, MPI_Allreduce(MPI_IN_PLACE,&nnzL,1,mpi_int_t,MPI_SUM,grid->comm);)", "label": "(49, MPI, MPI_Allreduce(MPI_IN_PLACE,&n,1,mpi_int_t,MPI_MAX,grid->comm);); (47, MPI, MPI_Allreduce(MPI_IN_PLACE,&nnzL,1,mpi_int_t,MPI_SUM,grid->comm);)"}
{"predict": "(52, OpenCL, src2 = clCreateBuffer(context, CL_MEM_READ_WRITE, BUFFER_SIZE, NULL, &err);); (75, OpenCL, err = clSetKernelArg(kernels[vecsize], 1, sizeof(src1), &src1);); (147, OpenCL, err = clSetKernelArg(kernels[vecsize], 3, sizeof(cmp), &cmp);); (48, OpenCL, dest = clCreateBuffer(context, CL_MEM_READ_WRITE, BUFFER_SIZE, NULL, &err);); (74, OpenCL, err = clSetKernelArg(kernels[vecsize], 2, sizeof(src2), &src2);); (75, OpenCL, err = clSetKernelArg(kernels[vecsize], 3, sizeof(cmp), &cmp);); (138, OpenCL, err = clEnqueueNDRangeKernel(queue, kernels[vecsize], 1, NULL, &size, NULL, 0, NULL, NULL);); (44, OpenCL, src1 = clCreateBuffer(context, CL_MEM_READ_WRITE, BUFFER_SIZE, NULL, &err);); (150, OpenCL, err = clEnqueueReadBuffer(queue, dest, CL_TRUE, 0, BUFFER_SIZE, dest_host.data(), 0, NULL, NULL);); (130, OpenCL, err = clSetKernelArg(kernels[vecsize], 1, sizeof(src1), &src1);); (68, OpenCL, err = clSetKernelArg(kernels[vecsize], 0, sizeof(dest), &dest);); (43, OpenCL, cmp = clCreateBuffer(context, CL_MEM_READ_WRITE, BUFFER_SIZE, NULL, &err);); (144, OpenCL, err = clSetKernelArg(kernels[vecsize], 2, sizeof(src2), &src2);); (138, OpenCL, err = clEnqueueWriteBuffer(queue, src1, CL_FALSE, 0, BUFFER_SIZE, src1_host.data(), 0, NULL, NULL);); (142, OpenCL, err = clEnqueueWriteBuffer(queue, src2, CL_FALSE, 0, BUFFER_SIZE, src2_host.data(), 0, NULL, NULL);); (66, OpenCL, err = clSetKernelArg(kernels[vecsize],", "label": "(51, OpenCL, dest = clCreateBuffer( context, CL_MEM_WRITE_ONLY, BUFFER_SIZE, NULL, &err );); (44, OpenCL, src1 = clCreateBuffer( context, CL_MEM_READ_ONLY, BUFFER_SIZE, NULL, &err );); (72, OpenCL, err = clSetKernelArg(kernels[vecsize], 0, sizeof dest, &dest);); (44, OpenCL, src2 = clCreateBuffer( context, CL_MEM_READ_ONLY, BUFFER_SIZE, NULL, &err );); (76, OpenCL, err = clSetKernelArg(kernels[vecsize], 3, sizeof cmp, &cmp);); (71, OpenCL, err = clSetKernelArg(kernels[vecsize], 1, sizeof src1, &src1);); (134, OpenCL, err = clEnqueueNDRangeKernel(queue, kernels[vecsize], 1, NULL, &vector_count, NULL, 0, NULL, NULL);); (42, OpenCL, cmp = clCreateBuffer( context, CL_MEM_READ_ONLY, BUFFER_SIZE, NULL, &err );); (70, OpenCL, err = clSetKernelArg(kernels[vecsize], 2, sizeof src2, &src2);)"}
{"predict": "(62, MPI, MPI_Error_string(mpirc, mpierrstr, &mpierrlen);); (24, MPI, MPI_Bcast(&filesize, 1, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (118, MPI, mpirc = MPI_File_read_at(fh, read_offset, bufstart, read_count, MPI_CHAR, &status);); (83, MPI, MPI_Error_string(mpirc, mpierrstr, &mpierrlen);); (42, MPI, MPI_Exscan(&chunk_count, &chunk_offset, 1, MPI_UINT64_T, MPI_SUM, MPI_COMM_WORLD);); (56, MPI, int mpirc = MPI_File_set_view(fh, disp, MPI_CHAR, MPI_CHAR, datarep, MPI_INFO_NULL);); (155, MPI, MPI_Exscan(&read_count, &bufoffset, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);)", "label": "(64, MPI, MPI_Error_string(mpirc, mpierrstr, &mpierrlen);); (45, MPI, MPI_Exscan(&chunk_count, &chunk_offset, 1, MPI_UINT64_T, MPI_SUM, MPI_COMM_WORLD);); (88, MPI, mpirc = MPI_File_read_at(fh, pos, buf, 1, MPI_CHAR, &status);); (120, MPI, mpirc = MPI_File_read_at(fh, read_offset, bufstart, read_count, MPI_CHAR, &status);); (58, MPI, int mpirc = MPI_File_set_view(fh, disp, MPI_CHAR, MPI_CHAR, datarep, MPI_INFO_NULL);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (19, MPI, MPI_Bcast(&filesize, 1, MPI_UINT64_T, 0, MPI_COMM_WORLD);)"}
{"predict": "(9, MPI, MPI_Init(&argc, &argv);); (34, MPI, MPI_Cart_coords(comm2d, crank, 2, coords);); (46, MPI, MPI_Comm_free(&comm2d);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (27, MPI, MPI_Cart_create(MPI_COMM_WORLD, 2, dims, period, 1, &comm2d);); (30, MPI, MPI_Cart_get(comm2d, 2, dims, period, &crank);); (27, MPI, MPI_Cart_neighbors(comm2d, 4, neighbors);); (41, MPI, MPI_Finalize();)", "label": "(37, MPI, MPI_Cart_shift(comm2d, 0, 1, &neighbors[0], &neighbors[1]);); (8, MPI, MPI_Init(&argc, &argv);); (26, MPI, MPI_Finalize();); (38, MPI, MPI_Cart_coords(comm2d, crank, 2, coords);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (28, MPI, MPI_Cart_create(MPI_COMM_WORLD, 2, dims, period, 1, &comm2d);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (42, MPI, MPI_Barrier(comm2d);); (26, MPI, MPI_Comm_rank(comm2d, &crank);); (29, MPI, MPI_Cart_shift(comm2d, 1, 1, &neighbors[2], &neighbors[3]);)"}
{"predict": "(216, CUDA, if (C_d) cudaFree(C_d);); (213, CUDA, if (A) cudaFreeHost(A);); (53, CUDA, HANDLE_CUDA_ERROR(cudaMalloc((void**) &C_d, sizeC));); (51, CUDA, HANDLE_CUDA_ERROR(cudaMalloc((void**) &A_d, sizeA));); (59, CUDA, HANDLE_CUDA_ERROR(cudaHostAlloc((void**) &C, sizeC, cudaHostAllocDefault));); (211, CUDA, if (C) cudaFreeHost(C);); (57, CUDA, HANDLE_CUDA_ERROR(cudaHostAlloc((void**) &A, sizeA, cudaHostAllocDefault));); (161, CUDA, HANDLE_CUDA_ERROR(cudaDeviceSynchronize());); (181, CUDA, HANDLE_CUDA_ERROR(cudaPlanDestroy(plan));); (208, CUDA, if (A_d) cudaFree(A_d);); (156, CUDA, HANDLE_CUDA_ERROR(cudaProfilerStart());); (169, CUDA, minTimeCUTENSOR = std::min(minTimeCUTENSOR, (cudaProfilerStopAndElapsedTime()[0]/1000.);); (164, CUDA, HANDLE_CUDA_ERROR(cudaProfilerStop());); (166, CUDA, HANDLE_CUDA_ERROR(cudaStreamDestroy(stream));); (18, CUDA, HANDLE_CUDA_ERROR(cudaSetDevice(0));); (151, CUDA, HANDLE_CUDA_ERROR(cudaStreamCreate(&stream));); (179, CUDA, HANDLE_CUDA_ERROR(cudaFreeHost(A));); (183, CUDA, HANDLE_CUDA_ERROR(cudaFree(C_d));); (179, CUDA, HANDLE_CUDA_ERROR(cudaFreeHost(C));); (18, CUDA, HANDLE_CUDA_ERROR(cudaMallocHost((void**) &C, sizeC));); (173, CUDA, HANDLE_CUDA_ERROR(cudaFree(A_d));); (143, CUDA, HANDLE_CUDA_ERROR(cudaPlanGetLastError());); (41, CUDA, HANDLE_CUDA_ERROR(cudaMemset(A_d, 0, sizeA));); (43, CUDA, HANDLE_CUDA_ERROR(cudaMemset(C_d, 0, sizeC));); (167, CUDA, HANDLE_CUDA_ERROR(cudaFreeHost(hostData));); (176, CUDA, HANDLE_CUDA_ERROR(cudaFree(A));); (173, CUDA, HANDLE_CUDA_ERROR(cudaFree(C));); (5", "label": "(63, CUDA, HANDLE_CUDA_ERROR(cudaMallocHost((void**) &C, sizeof(floatTypeC) * elementsC));); (78, CUDA, HANDLE_CUDA_ERROR(cudaMemcpy2DAsync(A_d, sizeA, A, sizeA, sizeA, 1, cudaMemcpyDefault, nullptr));); (192, CUDA, if (C_d) cudaFree(C_d);); (188, CUDA, if (A) cudaFreeHost(A);); (51, CUDA, HANDLE_CUDA_ERROR(cudaMalloc((void**) &C_d, sizeC));); (49, CUDA, HANDLE_CUDA_ERROR(cudaMalloc((void**) &A_d, sizeA));); (187, CUDA, if (A_d) cudaFree(A_d);); (185, CUDA, if (C) cudaFreeHost(C);); (54, CUDA, HANDLE_CUDA_ERROR(cudaMallocHost((void**) &A, sizeof(floatTypeA) * elementsA));)"}
{"predict": "(102, OpenMP, #pragma omp parallel for private(i,j)); (71, OpenMP, #pragma omp parallel for private(i,j,x,y))", "label": "(79, OpenACC, #pragma acc parallel loop independent); (70, OpenACC, #pragma acc data pcopy(grid[0:n*n])); (86, OpenACC, #pragma acc parallel loop gang); (99, OpenACC, #pragma acc kernels); (89, OpenACC, #pragma acc loop vector)"}
{"predict": "(124, MPI, MPI_Comm_group( MPI_COMM_WORLD, &world_group );); (142, MPI, MPI_Comm_rank( comm_col, &myrow );); (204, MPI, MPI_Barrier(MPI_COMM_WORLD);); (83, MPI, MPI_Bcast(&inner_block_flag, 1, MPI_INT, root, MPI_COMM_WORLD);); (134, MPI, MPI_Group_incl( world_group, nprow, ranks, &temp_group );); (40, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &my_ID );); (78, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (123, MPI, MPI_Group_incl( world_group, npcol, ranks, &temp_group );); (36, MPI, MPI_Init(&argc,&argv);); (130, MPI, MPI_Comm_create( MPI_COMM_WORLD, temp_group, &comm_col );); (36, MPI, MPI_Comm_size( MPI_COMM_WORLD, &Num_procs );); (120, MPI, MPI_Comm_create( MPI_COMM_WORLD, temp_group, &comm_row );); (244, MPI, MPI_Finalize();); (71, MPI, MPI_Bcast(&order,  1, MPI_INT, root, MPI_COMM_WORLD);); (166, MPI, MPI_Allgather( &mynrows, 1, MPI_INT, mm, 1, MPI_INT, comm_col );); (71, MPI, MPI_Bcast(&nb, 1, MPI_INT, root, MPI_COMM_WORLD);); (126, MPI, MPI_Comm_rank( comm_row, &mycol );); (171, MPI, MPI_Allgather( &myncols, 1, MPI_INT, nn, 1, MPI_INT, comm_row );)", "label": "(124, MPI, MPI_Comm_group( MPI_COMM_WORLD, &world_group );); (143, MPI, MPI_Comm_rank( comm_col, &myrow );); (205, MPI, MPI_Barrier(MPI_COMM_WORLD);); (83, MPI, MPI_Bcast(&inner_block_flag, 1, MPI_INT, root, MPI_COMM_WORLD);); (135, MPI, MPI_Group_incl( world_group, nprow, ranks, &temp_group );); (39, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &my_ID );); (78, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (124, MPI, MPI_Group_incl( world_group, npcol, ranks, &temp_group );); (35, MPI, MPI_Init(&argc,&argv);); (131, MPI, MPI_Comm_create( MPI_COMM_WORLD, temp_group, &comm_col );); (35, MPI, MPI_Comm_size( MPI_COMM_WORLD, &Num_procs );); (121, MPI, MPI_Comm_create( MPI_COMM_WORLD, temp_group, &comm_row );); (245, MPI, MPI_Finalize();); (70, MPI, MPI_Bcast(&order,  1, MPI_INT, root, MPI_COMM_WORLD);); (167, MPI, MPI_Allgather( &mynrows, 1, MPI_INT, mm, 1, MPI_INT, comm_col );); (70, MPI, MPI_Bcast(&nb, 1, MPI_INT, root, MPI_COMM_WORLD);); (127, MPI, MPI_Comm_rank( comm_row, &mycol );); (172, MPI, MPI_Allgather( &myncols, 1, MPI_INT, nn, 1, MPI_INT, comm_row );)"}
{"predict": "(104, MPI, MPI_Barrier(c_info->File_comm);); (124, MPI, MPI_Error_handler(c_info->fh, &stat);); (135, MPI, MPI_File_read_ordered(c_info->fh, c_info->r_buffer, Locsize, c_info->etype, &stat);); (153, MPI, MPI_File_read_all(c_info->fh, c_info->r_buffer, Locsize, c_info->etype, &stat);); (115, MPI, MPI_File_read(c_info->fh, c_info->r_buffer, Locsize, c_info->etype, &stat);); (129, MPI, MPI_File_read_at_ordered(c_info->fh, Offset, c_info->r_buffer, Locsize, c_info->etype, &stat);)", "label": "(102, MPI, MPI_Barrier(c_info->File_comm);); (108, MPI, MPI_ERRHAND(GEN_File_read(c_info->fh, c_info->r_buffer, Locsize, c_info->etype, &stat));); (121, MPI, MPI_ERRHAND(GEN_File_read_at(c_info->fh, Offset, c_info->r_buffer, Locsize, c_info->etype, &stat));); (133, MPI, MPI_ERRHAND(GEN_File_read_shared(c_info->fh, c_info->r_buffer, Locsize, c_info->etype, &stat));); (92, MPI, *time = MPI_Wtime();)"}
{"predict": "(84, OpenMP, #pragma omp parallel private(i)); (62, OpenMP, #pragma omp distribute parallel for simd private(h)); (4, OpenMP, #pragma omp target); (25, OpenMP, #pragma omp distribute parallel for simd private(argc,); (64, OpenMP, #pragma omp distribute parallel for simd nowait); (8, OpenMP, #pragma omp distribute parallel for simd private(); (65, OpenMP, #pragma omp parallel); (16, OpenMP, #pragma omp distribute parallel for simd private(argc); (40, OpenMP, #pragma omp distribute parallel for simd private(a, b)); (49, OpenMP, #pragma omp distribute parallel for simd private(e, g)); (33, OpenMP, #pragma omp distribute parallel for simd private(S1)); (42, OpenMP, #pragma omp distribute parallel for simd private(argv[1])); (6, OpenMP, #pragma omp distribute parallel for simd private()); (-5, OpenMP, #pragma omp distribute parallel for simd private); (63, OpenMP, #pragma omp distribute parallel for simd private(i)); (72, OpenMP, #pragma omp distribute parallel for simd private(j)); (22, OpenMP, #pragma omp distribute parallel for simd private(argc)); (16, OpenMP, #pragma omp distribute parallel for simd private(argc > 0 ? argv[1] : argv[2])); (65, OpenMP, #pragma omp parallel shared(i)); (-12, OpenMP, #pragma omp teams)", "label": "(63, OpenMP, #pragma omp distribute parallel for simd private(h)); (86, OpenMP, #pragma omp distribute parallel for simd private(j)); (82, OpenMP, #pragma omp parallel private(i)); (65, OpenMP, #pragma omp distribute parallel for simd nowait); (9, OpenMP, #pragma omp distribute parallel for simd private(); (1, OpenMP, #pragma omp target); (27, OpenMP, #pragma omp distribute parallel for simd private(argc > 0 ? argv[1] : argv[2])); (1, OpenMP, #pragma omp distribute parallel for simd private); (35, OpenMP, #pragma omp distribute parallel for simd private(S1)); (19, OpenMP, #pragma omp distribute parallel for simd private(argc,); (61, OpenMP, #pragma omp parallel); (12, OpenMP, #pragma omp distribute parallel for simd private(argc); (36, OpenMP, #pragma omp distribute parallel for simd private(a, b)); (25, OpenMP, #pragma omp distribute parallel for simd private(argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (39, OpenMP, #pragma omp distribute parallel for simd private(argv[1])); (43, OpenMP, #pragma omp distribute parallel for simd private(e, g)); (2, OpenMP, #pragma omp distribute parallel for simd private()); (60, OpenMP, #pragma omp distribute parallel for simd private(i)); (65, OpenMP, #pragma omp parallel shared(i)); (-12, OpenMP, #pragma omp teams)"}
{"predict": "(2, CUDA, __shared__ unsigned int points[2*MEMWID];)", "label": "(3, CUDA, __shared__ unsigned short points[2*MEMWID];)"}
{"predict": "(58, CUDA, thrust::host_vector<mytype> th_data = t_data;); (40, CUDA, thrust::device_vector<mytype> t_data(h_data, h_data+test_dsize);); (49, CUDA, cudaEventElapsedTime(&et, start, stop);); (29, CUDA, cudaLaunchCooperativeKernel((void *)my_remove_if<mytype>, FIXME);); (24, CUDA, cudaStreamCreate(&str);); (28, CUDA, cudaEventRecord(start);); (35, CUDA, thrust::remove(t_data.begin(), t_data.end(), -1);); (17, CUDA, cudaMemcpy(d_idata, h_data, tsize, cudaMemcpyHostToDevice);); (22, CUDA, cudaError_t err = cudaGetLastError();); (24, CUDA, cudaMemcpy(h_data, d_odata, tsize, cudaMemcpyDeviceToHost);); (41, CUDA, thrust::remove(t_data.begin(), t_data.end(), 0);); (-5, CUDA, cudaError_t err = cudaGetDeviceProperties(&prop, 0);); (20, CUDA, cudaEventRecord(stop);); (-8, CUDA, cudaMalloc(&d_idxs, test_dsize*sizeof(unsigned));); (16, CUDA, cudaEventCreate(&start);); (28, CUDA, cudaEventElapsedTime(&et, start, stop);); (-1, CUDA, cudaMalloc(&d_odata, tsize);); (4, CUDA, cudaOccupancyMaxActiveBlocksPerMultiprocessor(&numBlkPerSM, my_remove_if<mytype>, nTPB, 0);); (-11, CUDA, cudaMalloc(&d_idata, tsize);); (21, CUDA, cudaEventSynchronize(stop);); (-6, CUDA, cudaEventCreate(&stop);); (41, CUDA, thrust::device_vector<mytype> t_data(th_data);)", "label": "(58, CUDA, thrust::host_vector<mytype> th_data = t_data;); (40, CUDA, thrust::device_vector<mytype> t_data(h_data, h_data+test_dsize);); (49, CUDA, cudaEventElapsedTime(&et, start, stop);); (22, CUDA, cudaStreamCreate(&str);); (27, CUDA, cudaLaunchCooperativeKernel((void *)my_remove_if<mytype>, FIXME);); (8, CUDA, cudaError_t err = cudaGetDeviceProperties(&prop, 0);); (26, CUDA, cudaMemcpy(h_data, d_odata, tsize, cudaMemcpyDeviceToHost);); (49, CUDA, thrust::remove(t_data.begin(), t_data.end(), -1);); (15, CUDA, cudaMemcpy(d_idata, h_data, tsize, cudaMemcpyHostToDevice);); (39, CUDA, cudaEventRecord(stop);); (-1, CUDA, cudaMalloc(&d_idxs, test_dsize*sizeof(unsigned));); (33, CUDA, cudaEventCreate(&start);); (-5, CUDA, cudaMalloc(&d_odata, tsize);); (33, CUDA, cudaEventRecord(start);); (-6, CUDA, cudaMemset(d_odata, 0, tsize);); (-9, CUDA, cudaMalloc(&d_idata, tsize);); (29, CUDA, cudaEventCreate(&stop);); (2, CUDA, cudaOccupancyMaxActiveBlocksPerMultiprocessor(&numBlkPerSM, my_remove_if<mytype>, nTPB, 0);)"}
{"predict": "(47, MPI, MPI_Type_commit(recv_types + j);)", "label": "(48, MPI, MPI_Type_commit(recv_types + j);)"}
{"predict": "(67, CUDA, cudaEventSynchronize(stop);); (39, CUDA, cudaMalloc((void **) &d_a, sizeof(int)*m*n);); (113, CUDA, cudaFreeHost(h_cc);); (34, CUDA, cudaEventRecord(start, 0);); (60, CUDA, cudaThreadSynchronize();); (36, CUDA, cudaMalloc((void **) &d_b, sizeof(int)*n*k);); (39, CUDA, cudaMemcpy(d_a, h_a, sizeof(int)*m*n, cudaMemcpyHostToDevice);); (102, CUDA, cudaFree(d_a);); (62, CUDA, cudaEventElapsedTime(&gpu_elapsed_time_ms, start, stop);); (3, CUDA, cudaMallocHost((void **) &h_c, sizeof(int)*m*k);); (3, CUDA, cudaMallocHost((void **) &h_cc, sizeof(int)*m*k);); (35, CUDA, cudaMemcpy(d_b, h_b, sizeof(int)*n*k, cudaMemcpyHostToDevice);); (-1, CUDA, cudaMallocHost((void **) &h_b, sizeof(int)*n*k);); (97, CUDA, cudaFree(d_c);); (99, CUDA, cudaFree(d_b);); (65, CUDA, cudaEventElapsedTime(&cpu_elapsed_time_ms, start, stop);); (26, CUDA, cudaMalloc((void **) &d_c, sizeof(int)*m*k);); (49, CUDA, cudaEventRecord(stop, 0);); (45, CUDA, cudaMemcpy(h_c, d_c, sizeof(int)*m*k, cudaMemcpyDeviceToHost);); (94, CUDA, cudaFreeHost(h_b);); (92, CUDA, cudaFreeHost(h_a);); (12, CUDA, cudaEventCreate(&start);); (-12, CUDA, cudaMallocHost((void **) &h_a, sizeof(int)*m*n);); (11, CUDA, cudaEventCreate(&stop);); (90, CUDA, cudaFreeHost(h_c);)", "label": "(67, CUDA, cudaEventSynchronize(stop);); (39, CUDA, cudaMalloc((void **) &d_a, sizeof(int)*m*n);); (114, CUDA, cudaFreeHost(h_cc);); (34, CUDA, cudaEventRecord(start, 0);); (60, CUDA, cudaThreadSynchronize();); (36, CUDA, cudaMalloc((void **) &d_b, sizeof(int)*n*k);); (39, CUDA, cudaMemcpy(d_a, h_a, sizeof(int)*m*n, cudaMemcpyHostToDevice);); (103, CUDA, cudaFree(d_a);); (62, CUDA, cudaEventElapsedTime(&gpu_elapsed_time_ms, start, stop);); (3, CUDA, cudaMallocHost((void **) &h_c, sizeof(int)*m*k);); (3, CUDA, cudaMallocHost((void **) &h_cc, sizeof(int)*m*k);); (35, CUDA, cudaMemcpy(d_b, h_b, sizeof(int)*n*k, cudaMemcpyHostToDevice);); (-1, CUDA, cudaMallocHost((void **) &h_b, sizeof(int)*n*k);); (99, CUDA, cudaFree(d_c);); (97, CUDA, cudaFree(d_b);); (66, CUDA, cudaEventElapsedTime(&cpu_elapsed_time_ms, start, stop);); (26, CUDA, cudaMalloc((void **) &d_c, sizeof(int)*m*k);); (49, CUDA, cudaEventRecord(stop, 0);); (45, CUDA, cudaMemcpy(h_c, d_c, sizeof(int)*m*k, cudaMemcpyDeviceToHost);); (95, CUDA, cudaFreeHost(h_b);); (93, CUDA, cudaFreeHost(h_a);); (12, CUDA, cudaEventCreate(&start);); (55, OpenMP, omp_mm(h_a, m, n, h_b, n, k, h_cc);); (-13, CUDA, cudaMallocHost((void **) &h_a, sizeof(int)*m*n);); (10, CUDA, cudaEventCreate(&stop);); (90, CUDA, cudaFreeHost(h_c);)"}
{"predict": "(31, CUDA, volatile __shared__ int s_bmod;); (28, CUDA, __shared__ double s_lsum[MAXSUPER];)", "label": "(31, CUDA, volatile __shared__ int s_bmod;); (28, CUDA, __shared__ double s_lsum[MAXSUPER];)"}
{"predict": "(52, MPI, MPI_Allreduce(MPI_IN_PLACE,&n,1,mpi_int_t,MPI_MAX,grid->comm);); (50, MPI, MPI_Allreduce(MPI_IN_PLACE,&nnzL,1,mpi_int_t,MPI_SUM,grid->comm);)", "label": "(52, MPI, MPI_Allreduce(MPI_IN_PLACE,&n,1,mpi_int_t,MPI_MAX,grid->comm);); (50, MPI, MPI_Allreduce(MPI_IN_PLACE,&nnzL,1,mpi_int_t,MPI_SUM,grid->comm);)"}
{"predict": "(11, MPI, err = MPI_Type_struct(3, blocks, disps, types, &inttype);); (32, MPI, err = MPI_Type_size(eviltype, &val);); (95, MPI, err = MPI_Type_ub(eviltype, &aval);); (134, MPI, MPI_Type_free(&inttype);); (19, MPI, err = MPI_Type_vector(3, 4, -5, inttype, &eviltype);); (73, MPI, err = MPI_Type_get_extent(eviltype, &lb, &extent);); (131, MPI, MPI_Type_free(&eviltype);); (108, MPI, err = MPI_Type_get_true_extent(eviltype, &true_lb, &aval);); (40, MPI, err = MPI_Type_extent(eviltype, &aval);); (54, MPI, err = MPI_Type_lb(eviltype, &aval);)", "label": "(11, MPI, err = MPI_Type_struct(3, blocks, disps, types, &inttype);); (32, MPI, err = MPI_Type_size(eviltype, &val);); (134, MPI, MPI_Type_free(&inttype);); (75, MPI, err = MPI_Type_get_extent(eviltype, &lb, &extent);); (96, MPI, err = MPI_Type_ub(eviltype, &aval);); (17, MPI, err = MPI_Type_vector(3, 4, -5, inttype, &eviltype);); (131, MPI, MPI_Type_free(&eviltype);); (108, MPI, err = MPI_Type_get_true_extent(eviltype, &true_lb, &aval);); (40, MPI, err = MPI_Type_extent(eviltype, &aval);); (54, MPI, err = MPI_Type_lb(eviltype, &aval);)"}
{"predict": "(48, OpenMP, lis2mdl_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)", "label": "(48, OpenMP, lis2mdl_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)"}
{"predict": "(116, CUDA, cudaFuncGetAttributes(&cuattr, slsum_bmod_inv_gpu_mrhs_nvshmem);); (116, CUDA, cudaDeviceSetLimit(cudaLimitStackSize, cuattr.localSizeBytes);); (121, CUDA, cudaOccupancyMaxPotentialBlockSize(&minGridSize, &myblockSize, (const void *) swait_bcrd_u, 0, 0);); (91, CUDA, cudaStreamCreateWithFlags(&stream[i], cudaStreamNonBlocking);); (172, CUDA, CUDA_CHECK(cudaGetLastError());); (181, CUDA, CUDA_CHECK(cudaStreamDestroy(stream[i]));)", "label": "(117, CUDA, cudaDeviceSetLimit(cudaLimitStackSize, cuattr.localSizeBytes);); (93, CUDA, cudaStreamCreateWithFlags(&stream[i], cudaStreamNonBlocking);); (121, CUDA, cudaOccupancyMaxPotentialBlockSize(&minGridSize, &myblockSize, (const void *) swait_bcrd_u, 0, 0);); (173, CUDA, CUDA_CHECK(cudaGetLastError());); (112, CUDA, cudaFuncGetAttributes(&cuattr, slsum_bmod_inv_gpu_mrhs_nvshmem);); (181, CUDA, CUDA_CHECK(cudaStreamDestroy(stream[i]));)"}
{"predict": "(20, OpenMP, #pragma omp parallel for num_threads(THREADS)); (25, OpenMP, #pragma omp atomic); (30, OpenMP, printf(\"Thread %d: Wait error! Index: %d | Expected: %d | Received: %d\\n\", omp_get_thread_num(), rand_indexes[omp_get_thread_num()], 1 + (y[rand_indexes[omp_get_thread_num()] ]*z[rand_indexes[omp_get_thread_num()] ]), x[rand_indexes[omp_get_thread_num()] ]);); (28, OpenMP, #pragma omp barrier); (17, OpenMP, num_threads = omp_get_num_threads();)", "label": "(27, OpenMP, #pragma omp atomic update); (21, OpenMP, #pragma omp loop order(concurrent)); (29, OpenMP, num_threads = omp_get_num_threads();); (17, OpenMP, #pragma omp target parallel num_threads(THREADS) map(tofrom: x[0:N], num_threads, total_wait_errors) map(to: y[0:N], z[0:N]))"}
{"predict": "(139, MPI, MPI_Finalize();); (63, MPI, MPI_Bcast(&iterations,    1, MPI_INT, root, MPI_COMM_WORLD);); (63, MPI, MPI_Bcast(&vector_length, 1, MPI_LONG, root, MPI_COMM_WORLD);); (17, MPI, MPI_Init(&argc,&argv);); (81, MPI, MPI_Barrier(MPI_COMM_WORLD);); (17, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (15, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)", "label": "(139, MPI, MPI_Finalize();); (63, MPI, MPI_Bcast(&iterations,    1, MPI_INT, root, MPI_COMM_WORLD);); (63, MPI, MPI_Bcast(&vector_length, 1, MPI_LONG, root, MPI_COMM_WORLD);); (17, MPI, MPI_Init(&argc,&argv);); (81, MPI, MPI_Barrier(MPI_COMM_WORLD);); (17, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (15, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(25, OpenMP, #pragma omp target teams distribute parallel for simd collapse (1))); (5, OpenMP, #pragma omp target teams distribute parallel for simd collapse (); (27, OpenMP, #pragma omp target teams distribute parallel for simd collapse ((ST > 0) ? 1 + ST : 2)); (13, OpenMP, #pragma omp target teams distribute parallel for simd collapse (argc); (57, OpenMP, #pragma omp target teams distribute parallel for simd collapse (2)); (-3, OpenMP, #pragma omp target teams distribute parallel for simd collapse); (51, OpenMP, #pragma omp target teams distribute parallel for simd collapse (N)); (46, OpenMP, #pragma omp target teams distribute parallel for simd collapse (1)); (32, OpenMP, #pragma omp target teams distribute parallel for simd collapse (S)); (27, OpenMP, #pragma omp target teams distribute parallel for simd collapse (foobool(argc)), collapse (true), collapse (-5)); (0, OpenMP, #pragma omp target teams distribute parallel for simd collapse ()); (10, OpenMP, #pragma omp target teams distribute parallel for simd collapse (ST)", "label": "(25, OpenMP, #pragma omp target teams distribute parallel for simd collapse (1))); (60, OpenMP, #pragma omp target teams distribute parallel for simd collapse (2)); (4, OpenMP, #pragma omp target teams distribute parallel for simd collapse (); (54, OpenMP, #pragma omp target teams distribute parallel for simd collapse (N)); (36, OpenMP, #pragma omp target teams distribute parallel for simd collapse (S)); (24, OpenMP, #pragma omp target teams distribute parallel for simd collapse ((ST > 0) ? 1 + ST : 2)); (-4, OpenMP, #pragma omp target teams distribute parallel for simd collapse); (9, OpenMP, #pragma omp target teams distribute parallel for simd collapse (argc); (28, OpenMP, #pragma omp target teams distribute parallel for simd collapse (foobool(argc)), collapse (true), collapse (-5)); (12, OpenMP, #pragma omp target teams distribute parallel for simd collapse (ST); (43, OpenMP, #pragma omp target teams distribute parallel for simd collapse (1)); (-1, OpenMP, #pragma omp target teams distribute parallel for simd collapse ())"}
{"predict": "(74, OpenCL, err = clEnqueueWriteBuffer( queue, memobjs[1], true, 0, num_bytes, inptr, 0, NULL, NULL );); (88, OpenCL, err |= clSetKernelArg( kernel[0], 3, sizeof( cl_mem ), (void *)&memobjs[2] );); (100, OpenCL, clReleaseProgram( program[0] );); (102, OpenCL, clReleaseKernel( kernel[0] );); (84, OpenCL, err = clSetKernelArg( kernel[0], 0, sizeof( cl_mem ), (void *)&memobjs[0] );); (225, OpenCL, clReleaseMemObject(memobjs[0]);); (141, OpenCL, err = clEnqueueCopyImage( queue, memobjs[0], memobjs[2], srcPt, destPt, region, 0, NULL, &copyEvent );); (82, OpenCL, err |= clSetKernelArg( kernel[0], 2, sizeof( cl_mem ), (void *)&memobjs[1] );); (21, OpenCL, clReleaseMemObject(memobjs[1]);); (223, OpenCL, clReleaseMemObject(memobjs[2]);); (87, OpenCL, err = clEnqueueNDRangeKernel( queue, kernel[0], 2, NULL, threads, NULL, 0, NULL, NULL );); (76, OpenCL, err |= clSetKernelArg( kernel[0], 1, sizeof( cl_int ), (void *)&subh );); (74, OpenCL, err = clSetKernelArg( kernel[0], 0, sizeof( cl_mem ), (void *)&memobjs[0] );); (18, OpenCL, clReleaseMemObject(memobjs[0]);)", "label": "(85, OpenCL, clReleaseMemObject( memobjs[2] );); (140, OpenCL, clReleaseEvent(copyEvent);); (103, OpenCL, err = clEnqueueNDRangeKernel( queue, kernel[0], 2, NULL, threads, NULL, 0, NULL, NULL );); (72, OpenCL, err = clEnqueueWriteBuffer( queue, memobjs[1], true, 0, num_bytes, inptr, 0, NULL, NULL );); (87, OpenCL, err = clSetKernelArg( kernel[0], 0, sizeof( cl_mem ), (void *)&memobjs[1] );); (91, OpenCL, clReleaseProgram( program[0] );); (77, OpenCL, clReleaseMemObject( memobjs[0] );); (88, OpenCL, clReleaseKernel( kernel[0] );); (84, OpenCL, err |= clSetKernelArg( kernel[0], 1, sizeof( cl_mem ), (void *)&memobjs[0] );); (48, OpenCL, clReleaseMemObject(memobjs[0]);); (113, OpenCL, err = clEnqueueCopyImage( queue, memobjs[0], memobjs[2], srcPt, destPt, region, 0, NULL, &copyEvent );); (57, OpenCL, clReleaseMemObject(memobjs[1]);); (201, OpenCL, err = clEnqueueReadImage( queue, memobjs[2], true, origin, region2, 0, 0, dst, 0, NULL, NULL );); (125, OpenCL, err = clWaitForEvents( 1, &copyEvent );); (70, OpenCL, clReleaseMemObject( memobjs[1] );)"}
{"predict": "(143, MPI, MPI_Finalize();); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);); (19, MPI, MPI_Init(&argc,&argv);)", "label": "(143, MPI, MPI_Finalize();); (67, MPI, MPI_Bcast(&iterations,    1, MPI_INT, root, MPI_COMM_WORLD);); (67, MPI, MPI_Bcast(&vector_length, 1, MPI_LONG, root, MPI_COMM_WORLD);); (19, MPI, MPI_Init(&argc,&argv);); (85, MPI, MPI_Barrier(MPI_COMM_WORLD);); (19, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (17, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(108, MPI, MPI_Finalize();); (5, MPI, err = MPI_Init(&argc, &argv);); (8, MPI, err = MPI_Comm_size(MPI_COMM_WORLD, &numRanks);); (5, MPI, err = MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(109, MPI, MPI_Finalize();); (5, MPI, err = MPI_Init(&argc, &argv);); (8, MPI, err = MPI_Comm_size(MPI_COMM_WORLD, &numRanks);); (5, MPI, err = MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(29, CUDA, __shared__ float x_s[x_s_sz];); (27, CUDA, __shared__ float w_s[w_s_sz];)", "label": "(29, CUDA, __shared__ float x_s[x_s_sz];); (27, CUDA, __shared__ float w_s[w_s_sz];)"}
{"predict": "(25, OpenMP, #pragma omp distribute collapse (S)); (28, OpenMP, #pragma omp distribute collapse (4)); (33, OpenMP, #pragma omp distribute collapse (N)); (11, OpenMP, #pragma omp distribute collapse (ST); (3, OpenMP, #pragma omp distribute collapse ()); (12, OpenMP, #pragma omp distribute collapse (1))); (-3, OpenMP, #pragma omp distribute collapse (); (5, OpenMP, #pragma omp distribute collapse (argc); (16, OpenMP, #pragma omp distribute collapse (foobool(argc)), collapse (true), collapse (-5)); (-3, OpenMP, #pragma omp distribute collapse (argc); (28, OpenMP, #pragma omp distribute collapse (2)); (15, OpenMP, #pragma omp distribute collapse (1))", "label": "(36, OpenMP, #pragma omp distribute collapse (N)); (9, OpenMP, #pragma omp distribute collapse (argc); (36, OpenMP, #pragma omp distribute collapse (2)); (22, OpenMP, #pragma omp distribute collapse (S)); (2, OpenMP, #pragma omp distribute collapse ()); (-1, OpenMP, #pragma omp distribute collapse (); (9, OpenMP, #pragma omp distribute collapse (1))); (27, OpenMP, #pragma omp distribute collapse (1)); (9, OpenMP, #pragma omp distribute collapse ((ST > 0) ? 1 + ST : 2)); (-7, OpenMP, #pragma omp distribute collapse); (3, OpenMP, #pragma omp distribute collapse (ST); (12, OpenMP, #pragma omp distribute collapse (foobool(argc)), collapse (true), collapse (-5))"}
{"predict": "(39, MPI, MPI_Bcast( rowind, nnz, mpi_int_t,  0, grid->comm );); (36, MPI, MPI_Bcast( &nnz,   1,   mpi_int_t,  0, grid->comm );); (50, MPI, MPI_Bcast( colptr,  n+1, mpi_int_t,  0, grid->comm );); (48, MPI, MPI_Bcast( rowind,  nnz, mpi_int_t,  0, grid->comm );); (39, MPI, MPI_Bcast( &m,   1,   mpi_int_t,  0, grid->comm );); (31, MPI, MPI_Bcast( &n,     1,   mpi_int_t,  0, grid->comm );); (29, MPI, MPI_Bcast( &m,     1,   mpi_int_t,  0, grid->comm );); (38, MPI, MPI_Bcast( &nnz, 1,   mpi_int_t,  0, grid->comm );); (32, MPI, MPI_Bcast( colptr, n+1, mpi_int_t,  0, grid->comm );); (29, MPI, MPI_Bcast( nzval,  nnz, SuperLU_MPI_DOUBLE_COMPLEX, 0, grid->comm );); (34, MPI, MPI_Bcast( &n,   1,   mpi_int_t,  0, grid->comm );); (39, MPI, MPI_Bcast( nzval,   nnz, SuperLU_MPI_DOUBLE_COMPLEX, 0, grid->comm );)", "label": "(50, MPI, MPI_Bcast( nzval,   nnz, SuperLU_MPI_DOUBLE_COMPLEX, 0, grid->comm );); (38, MPI, MPI_Bcast( rowind, nnz, mpi_int_t,  0, grid->comm );); (50, MPI, MPI_Bcast( colptr,  n+1, mpi_int_t,  0, grid->comm );); (48, MPI, MPI_Bcast( rowind,  nnz, mpi_int_t,  0, grid->comm );); (39, MPI, MPI_Bcast( &m,   1,   mpi_int_t,  0, grid->comm );); (31, MPI, MPI_Bcast( &n,     1,   mpi_int_t,  0, grid->comm );); (29, MPI, MPI_Bcast( &m,     1,   mpi_int_t,  0, grid->comm );); (38, MPI, MPI_Bcast( &nnz, 1,   mpi_int_t,  0, grid->comm );); (32, MPI, MPI_Bcast( colptr, n+1, mpi_int_t,  0, grid->comm );); (28, MPI, MPI_Bcast( &nnz,   1,   mpi_int_t,  0, grid->comm );); (28, MPI, MPI_Bcast( nzval,  nnz, SuperLU_MPI_DOUBLE_COMPLEX, 0, grid->comm );); (33, MPI, MPI_Bcast( &n,   1,   mpi_int_t,  0, grid->comm );)"}
{"predict": "(20, TBB, tbb::flow::make_edge( n, r[i] );); (54, TBB, tbb::flow::remove_edge( n, r[i] );); (1, TBB, tbb::flow::write_once_node<R> n(g);); (-1, TBB, tbb::flow::graph g;)", "label": "(20, TBB, tbb::flow::make_edge( n, r[i] );); (54, TBB, tbb::flow::remove_edge( n, r[i] );); (1, TBB, tbb::flow::write_once_node<R> n(g);); (-1, TBB, tbb::flow::graph g;)"}
{"predict": "(11, OpenMP, #pragma omp parallel for); (77, OpenMP, #pragma omp atomic)", "label": "(11, OpenMP, #pragma omp parallel for); (77, OpenMP, #pragma omp atomic)"}
{"predict": "(124, MPI, MPI_Allreduce(&tmp_error, error_code, 1, MPI_INT, MPI_MAX, fd->comm);); (47, MPI, MPI_Comm_size(fd->comm, &nprocs);); (75, MPI, MPI_Allgather(&end_offset, 1, ADIO_OFFSET, end_offsets, 1, ADIO_OFFSET, fd->comm);); (73, MPI, MPI_Allgather(&start_offset, 1, ADIO_OFFSET, st_offsets, 1, ADIO_OFFSET, fd->comm);); (45, MPI, MPI_Comm_rank(fd->comm, &myrank);); (213, MPI, MPI_Type_size_x(datatype, &size);)", "label": "(173, MPI, MPI_Allreduce(&tmp_error, error_code, 1, MPI_INT, MPI_MAX, fd->comm);); (47, MPI, MPI_Comm_size(fd->comm, &nprocs);); (75, MPI, MPI_Allgather(&end_offset, 1, ADIO_OFFSET, end_offsets, 1, ADIO_OFFSET, fd->comm);); (73, MPI, MPI_Allgather(&start_offset, 1, ADIO_OFFSET, st_offsets, 1, ADIO_OFFSET, fd->comm);); (45, MPI, MPI_Comm_rank(fd->comm, &myrank);); (192, MPI, MPI_Type_size_x(datatype, &size);)"}
{"predict": "(54, CUDA, CUDA_CHECK(cudaStreamCreate(&stream));); (106, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (0, MPI, MPI_Init(&argc, &argv);); (103, MPI, MPI_Finalize();); (59, CUDA, CUDA_CHECK(cudaSetDevice(rank % ndevices));); (67, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (-2, MPI, MPI_Comm_rank(mpi_comm, &rank);); (-2, MPI, MPI_Comm_size(mpi_comm, &size);); (103, CUDA, CUDA_CHECK(cudaFree(nullptr));); (37, CUDA, CUDA_CHECK(cudaMalloc(nullptr, 0));)", "label": "(10, CUDA, CUDA_CHECK(cudaGetDeviceCount(&ndevices));); (54, CUDA, CUDA_CHECK(cudaStreamCreate(&stream));); (88, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (8, CUDA, CUDA_CHECK(cudaSetDevice(rank % ndevices));); (-2, MPI, MPI_Init(&argc, &argv);); (113, MPI, MPI_Finalize();); (1, MPI, MPI_Comm_size(mpi_comm, &size);); (103, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (78, CUDA, CUDA_CHECK(cudaGetLastError());); (-3, MPI, MPI_Comm_rank(mpi_comm, &rank);)"}
{"predict": "(3, TBB, tbb::enumerable_thread_specific<double> test_locals;); (13, TBB, REMARK(\"%d threads, work ratio %d per cent, time %g\", nThreads, csWorkRatio, (tbb::tick_count::now() - t0).seconds());); (0, TBB, tbb::task_scheduler_init init(tbb::task_scheduler_init::deferred);); (5, TBB, tbb::tick_count t0;); (6, TBB, t0 = tbb::tick_count::now();)", "label": "(3, TBB, tbb::enumerable_thread_specific<double> test_locals;); (13, TBB, REMARK(\"%d threads, work ratio %d per cent, time %g\", nThreads, csWorkRatio, (tbb::tick_count::now() - t0).seconds());); (0, TBB, tbb::task_scheduler_init init(tbb::task_scheduler_init::deferred);); (5, TBB, tbb::tick_count t0;); (6, TBB, t0 = tbb::tick_count::now();)"}
{"predict": "(105, MPI, MPI_Finalize();); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (2, MPI, MPI_Init(NULL, NULL);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);)", "label": "(2, MPI, MPI_Init(&argc, &argv);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (105, MPI, MPI_Finalize();); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(27, CUDA, cudaMalloc((void**)&dataC, sizeof(cufftDoubleComplex) * dims[0] * dims[1] * dims[2]);); (55, CUDA, cudaDeviceSynchronize();); (27, CUDA, cudaMemcpy(dataC, inputC, sizeof(cufftDoubleComplex) * dims[0] * dims[1] * dims[2], cudaMemcpyHostToDevice);); (78, CUDA, cudaFree(dataC);); (1, CUDA, cudaSetDevice(device_id);)", "label": "(27, CUDA, cudaMalloc((void**)&dataC, sizeof(cufftDoubleComplex) * dims[0] * dims[1] * dims[2]);); (55, CUDA, cudaDeviceSynchronize();); (27, CUDA, cudaMemcpy(dataC, inputC, sizeof(cufftDoubleComplex) * dims[0] * dims[1] * dims[2], cudaMemcpyHostToDevice);); (78, CUDA, cudaFree(dataC);); (1, CUDA, cudaSetDevice(device_id);)"}
{"predict": "(9, OpenMP, john_omp_autotune_init();); (14, OpenMP, omp_set_num_threads(DEFOMP);); (77, OpenMP, omp_reinit();); (18, MPI, mpi_autodetect();); (121, MPI, MPI_Allreduce(MPI_IN_PLACE, &s, 1, MPI_SIZE_T, MPI_SUM, mpi_state);)", "label": "(14, OpenMP, john_omp_fallback(argv);); (18, MPI, mpi_setup(argc, argv);); (78, OpenMP, john_omp_maybe_adjust_or_fallback(argv);); (6, OpenMP, john_omp_init();)"}
{"predict": "(23, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (38, MPI, MPI_Comm_split(MPI_COMM_WORLD, rank % 2, randval, &newcomm);); (46, MPI, MPI_Comm_free(&newcomm);); (21, MPI, startTime = MPI_Wtime();); (27, MPI, double rate = MPI_Wtime() - startTime;); (38, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(44, MPI, MPI_Comm_split(MPI_COMM_WORLD, MPI_UNDEFINED, rank, &newcomm);); (21, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (40, MPI, MPI_Comm_free(&newcomm);); (21, MPI, startTime = MPI_Wtime();); (24, MPI, double rate = MPI_Wtime() - startTime;); (36, MPI, MPI_Comm_split(MPI_COMM_WORLD, 1, rank, &newcomm);)"}
{"predict": "(28, MPI, MPI_Error_string(mpirc, mpierrstr, &mpierrlen);); (117, MPI, MPI_Type_get_extent(groups->dt, &lb_group, &extent_group);); (199, MPI, mpirc = MPI_File_set_view(fh, disp, MPI_BYTE, MPI_BYTE, datarep, MPI_INFO_NULL);); (21, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (18, MPI, int mpirc = MPI_File_set_view(fh, disp, MPI_BYTE, MPI_BYTE, datarep, MPI_INFO_NULL);); (81, MPI, MPI_Type_get_extent(users->dt, &lb_user, &extent_user);); (162, MPI, MPI_Bcast(groups->buf, (int)groups->count, groups->dt, 0, MPI_COMM_WORLD);); (30, MPI, mpirc = MPI_Bcast(header_packed, header_size, MPI_BYTE, 0, MPI_COMM_WORLD);); (90, MPI, mpirc = MPI_File_read_at(fh, 0, user_buf, user_buf_size, MPI_BYTE, &status);); (151, MPI, MPI_Bcast(users->buf, (int)users->count, users->dt, 0, MPI_COMM_WORLD);); (112, MPI, mpirc = MPI_File_set_view(fh, disp, MPI_BYTE, MPI_BYTE, datarep, MPI_INFO_NULL);); (15, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (105, MPI, MPI_Type_get_extent(groups->dt, &lb_group, &extent_group);); (88, MPI, MPI_Barrier(MPI_COMM_WORLD);); (119, MPI, MPI_Type_free(&groups->dt);); (65, MPI, mpirc = MPI_File_set_view(fh, disp, MPI_BYTE, MPI_BYTE, datarep, MPI_INFO_NULL);); (11, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (115, MPI, MPI_Type_free(&users->dt);); (42, MPI, MPI_Bcast(&header[0], 6, MPI_UINT64_T, 0, MPI_COMM_WORLD);)", "label": "(28, MPI, MPI_Error_string(mpirc, mpierrstr, &mpierrlen);); (142, MPI, MPI_Bcast(groups->buf, (int)groups->count, groups->dt, 0, MPI_COMM_WORLD);); (96, MPI, mpirc = MPI_File_read_at(fh, 0, user_buf, user_buf_size, MPI_BYTE, &status);); (132, MPI, mpirc = MPI_File_read_at(fh, 0, group_buf, group_buf_size, MPI_BYTE, &status);); (102, MPI, MPI_Bcast(users->buf, (int)users->count, users->dt, 0, MPI_COMM_WORLD);); (15, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (28, MPI, mpirc = MPI_File_read_at(fh, 0, header_packed, header_size, MPI_BYTE, &status);); (41, MPI, MPI_Bcast(header, 6, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (60, MPI, MPI_Exscan(&count, &offset, 1, MPI_UINT64_T, MPI_SUM, MPI_COMM_WORLD);); (12, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (16, MPI, int mpirc = MPI_File_set_view(fh, disp, MPI_BYTE, MPI_BYTE, datarep, MPI_INFO_NULL);); (69, MPI, MPI_Type_get_extent(users->dt, &lb_user, &extent_user);); (105, MPI, MPI_Type_get_extent(groups->dt, &lb_group, &extent_group);); (190, MPI, mpirc = MPI_File_read_at(fh, read_offset, buf, read_size, MPI_BYTE, &status);); (74, MPI, mpirc = MPI_File_set_view(fh, disp, MPI_BYTE, MPI_BYTE, datarep, MPI_INFO_NULL);); (159, MPI, MPI_Allreduce(&iters, &all_iters, 1, MPI_UINT64_T, MPI_MAX, MPI_COMM_WORLD);)"}
{"predict": "(18, CUDA, __shared__ bool kept_bboxinfo_flag[TSIZE];)", "label": "(19, CUDA, extern __shared__ bool kept_bboxinfo_flag[];)"}
{"predict": "(112, MPI, MPI_Bcast( &berr[j], 1, MPI_FLOAT, 0,  grid3d->zscp.comm);); (109, MPI, MPI_Allreduce( &s, &berr[j], 1, MPI_FLOAT, MPI_MAX, grid->comm );)", "label": "(110, MPI, MPI_Allreduce( &s, &berr[j], 1, MPI_FLOAT, MPI_MAX, grid->comm );); (111, MPI, MPI_Bcast( &berr[j], 1, MPI_FLOAT, 0,  grid3d->zscp.comm);)"}
{"predict": "(21, TBB, tbb::detail::d1::segmented_iterator<nested_vec, T> my_si;); (62, TBB, tbb::detail::d1::segmented_iterator<nested_vec, const T> my_csi;)", "label": "(62, TBB, tbb::internal::segmented_iterator<nested_vec, const T> my_csi(my_vec);); (19, TBB, tbb::internal::segmented_iterator<nested_vec, T> my_si(my_vec);)"}
{"predict": "(53, MPI, MPI_Comm_free(&pgroup_old);); (46, MPI, MPI_Comm_get_info(pgroup, &port_info);); (63, MPI, MPI_Send(port, PORT_SIZE, MPI_CHAR, members[i], 0, MPI_COMM_WORLD);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (41, MPI, MPI_Open_port(port_info, port);); (50, MPI, MPI_Comm_free(&pgroup_new);); (35, MPI, MPI_Intercomm_merge(pgroup_new, 0 /* LOW */ , &pgroup);); (1, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (36, MPI, MPI_Close_port(port);); (73, MPI, MPI_Barrier(pgroup);); (30, MPI, MPI_Comm_spawn(\"pgroup_daemon\", NULL, 1, MPI_INFO_NULL, 0, MPI_COMM_WORLD, &pgroup_new, NULL);)", "label": "(49, MPI, MPI_Comm_accept(port, MPI_INFO_NULL, 0, pgroup, &pgroup_new);); (53, MPI, MPI_Comm_free(&pgroup_old);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (65, MPI, MPI_Comm_connect(port, MPI_INFO_NULL, 0, MPI_COMM_SELF, &pgroup_new);); (43, MPI, MPI_Send(port, PORT_SIZE, MPI_CHAR, members[i], 0, MPI_COMM_WORLD);); (36, MPI, MPI_Open_port(port_info, port);); (45, MPI, MPI_Comm_free(&pgroup_new);); (49, MPI, MPI_Close_port(port);); (67, MPI, MPI_Comm_connect(port, MPI_INFO_NULL, 0, pgroup, &pgroup_new);)"}
{"predict": "(6, OpenMP, #pragma omp taskloop simd private); (25, OpenMP, #pragma omp taskloop simd private(argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (30, OpenMP, #pragma omp taskloop simd private(a, b)); (37, OpenMP, #pragma omp taskloop simd private(h)); (48, OpenMP, #pragma omp parallel shared(i)); (48, OpenMP, #pragma omp parallel private(i)); (4, OpenMP, #pragma omp taskloop simd private(); (9, OpenMP, #pragma omp taskloop simd private(argc); (37, OpenMP, #pragma omp parallel); (11, OpenMP, #pragma omp taskloop simd private(argc,); (14, OpenMP, #pragma omp taskloop simd private(argc > 0 ? argv[1] : argv[2])); (50, OpenMP, #pragma omp taskloop simd private(j)); (15, OpenMP, #pragma omp taskloop simd private(argc) allocate(omp_thread_mem_alloc: argc)); (19, OpenMP, #pragma omp taskloop simd private(S1)); (-1, OpenMP, #pragma omp taskloop simd private()); (27, OpenMP, #pragma omp taskloop simd shared(i)); (35, OpenMP, #pragma omp taskloop simd private(i)); (20, OpenMP, #pragma omp taskloop simd private(e, g, z))", "label": "(33, OpenMP, #pragma omp taskloop simd private(argv[1])); (38, OpenMP, #pragma omp taskloop simd private(h)); (13, OpenMP, #pragma omp taskloop simd private(argc); (52, OpenMP, #pragma omp parallel shared(i)); (52, OpenMP, #pragma omp parallel private(i)); (1, OpenMP, #pragma omp taskloop simd private); (15, OpenMP, #pragma omp taskloop simd private(argc > 0 ? argv[1] : argv[2])); (38, OpenMP, #pragma omp parallel); (41, OpenMP, #pragma omp taskloop simd private(i)); (21, OpenMP, #pragma omp taskloop simd private(a, b)); (32, OpenMP, #pragma omp taskloop simd shared(i)); (13, OpenMP, #pragma omp taskloop simd private(argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (15, OpenMP, #pragma omp taskloop simd private(S1)); (5, OpenMP, #pragma omp taskloop simd private(argc,); (-5, OpenMP, #pragma omp taskloop simd private(); (-3, OpenMP, #pragma omp taskloop simd private()); (20, OpenMP, #pragma omp taskloop simd private(e, g, z)); (40, OpenMP, #pragma omp taskloop simd private(j))"}
{"predict": "(37, OpenCL, clReleaseMemObject(test_buf);); (25, OpenCL, status = clSetKernelArg(kernel, 1, sizeof(cl_mem), &result_buf);); (18, OpenCL, status = clSetKernelArg(kernel, 0, sizeof(cl_mem), &test_buf);); (86, OpenCL, clReleaseMemObject(result_buf);)", "label": "(12, OpenCL, status = clSetKernelArg(kernel, 0, sizeof(cl_mem), &test_buf);); (20, OpenCL, status = clSetKernelArg(kernel, 0, sizeof(cl_mem), NULL);)"}
{"predict": "(19, MPI, MPI_Comm_size(d->comm, &ranks_comm);); (28, MPI, MPI_Comm_rank(parent_comm, &parent_rank);); (28, MPI, MPI_Allgather(&parent_rank, 1, MPI_INT, ranklist, 1, MPI_INT, d->comm);)", "label": "(19, MPI, MPI_Comm_size(d->comm, &ranks_comm);); (28, MPI, MPI_Comm_rank(parent_comm, &parent_rank);); (28, MPI, MPI_Allgather(&parent_rank, 1, MPI_INT, ranklist, 1, MPI_INT, d->comm);)"}
{"predict": "(144, OpenMP, comp_val += zp_comp.get_elem(comp_idle_off);)", "label": "(101, OpenMP, dims_t comp_dims(ndims, 1);)"}
{"predict": "(28, MPI, errval = MPI_Irecv(b2, 10, MPI_INT, src, 10, comm, &r[1]);); (59, MPI, MPI_Error_class(errval, &errclass);); (94, MPI, MPI_Ssend(NULL, 0, MPI_INT, dest, 100, comm);); (17, MPI, MPI_Comm_size(comm, &size);); (81, MPI, MPI_Barrier(comm);); (38, MPI, errval = MPI_Recv(NULL, 0, MPI_INT, src, 100, comm, MPI_STATUS_IGNORE);); (58, MPI, errval = MPI_Waitsome(2, r, &outcount, indices, MPI_STATUS_IGNORE);); (84, MPI, MPI_Ssend(b1, 10, MPI_INT, dest, 0, comm);); (84, MPI, MPI_Ssend(b1, 10, MPI_INT, dest, 0, comm);); (14, MPI, MPI_Comm_set_errhandler(comm, MPI_ERRORS_RETURN);); (15, MPI, errval = MPI_Irecv(b1, 10, MPI_INT, src, 0, comm, &r[0]);); (23, MPI, MPI_Barrier(comm);); (42, MPI, errval = MPI_Recv(NULL, 0, MPI_INT, src, 100, comm, &s[0]);); (7, MPI, MPI_Comm_rank(comm, &rank);); (45, MPI, errval = MPI_Recv(&b1[10], 10, MPI_INT, src, 0, comm, &s[1]);)", "label": "(20, MPI, MPI_Comm_size(comm, &size);); (64, MPI, MPI_Error_class(errval, &errclass);); (95, MPI, MPI_Ssend(NULL, 0, MPI_INT, dest, 10, comm);); (39, MPI, errval = MPI_Recv(NULL, 0, MPI_INT, src, 100, comm, MPI_STATUS_IGNORE);); (85, MPI, MPI_Ssend(NULL, 0, MPI_INT, dest, 100, comm);); (59, MPI, errval = MPI_Waitsome(2, r, &outcount, indices, s);); (87, MPI, MPI_Send(b1, 10, MPI_INT, dest, 0, comm);); (25, MPI, errval = MPI_Irecv(b2, 10, MPI_INT, src, 10, comm, &r[1]);); (44, MPI, errval = MPI_Recv(NULL, 0, MPI_INT, src, 10, comm, MPI_STATUS_IGNORE);); (16, MPI, MPI_Comm_set_errhandler(comm, MPI_ERRORS_RETURN);); (16, MPI, errval = MPI_Irecv(b1, 10, MPI_INT, src, 0, comm, &r[0]);); (8, MPI, MPI_Comm_rank(comm, &rank);); (82, MPI, MPI_Send(b2, 11, MPI_INT, dest, 10, comm);)"}
{"predict": "(33, MPI, MPI_Start(&reqs[root]);); (52, MPI, MPI_Type_free(&vec);); (20, MPI, MPI_Type_vector(n, 1, stride, MPI_DOUBLE, &vec);); (8, MPI, MPI_Info_create(&info);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (31, MPI, MPI_Wait(&reqs[root], MPI_STATUS_IGNORE);); (47, MPI, MPI_Info_free(&info);); (17, MPI, MPI_Type_commit(&vec);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (41, MPI, MPI_Request_free(&reqs[root]);)", "label": "(35, MPI, MPI_Start(&reqs[root]);); (20, MPI, MPI_Type_vector(n, 1, stride, MPI_DOUBLE, &vec);); (11, MPI, MPI_Info_create(&info);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (32, MPI, MPI_Wait(&reqs[root], MPI_STATUS_IGNORE);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (48, MPI, MPI_Type_free(&vec);); (15, MPI, MPI_Type_commit(&vec);); (39, MPI, MPI_Request_free(&reqs[root]);); (44, MPI, MPI_Info_free(&info);)"}
{"predict": "(172, TBB, CHECK( rtl1.status() == tbb::runtime_loader::ec_bad_ver );); (97, TBB, tbb::task_scheduler_init init( 1 );); (136, TBB, CHECK_EXCEPTION( eid_invalid_multiple_scheduling, tbb::invalid_multiple_scheduling );); (40, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_no_lib );); (8, TBB, tbb::runtime_loader rtl( tbb::runtime_loader::em_status );); (157, TBB, tbb::runtime_loader rtl1( tbb::runtime_loader::em_status );); (157, TBB, CHECK( rtl0.status() == tbb::runtime_loader::ec_ok );); (126, TBB, CHECK_EXCEPTION( eid_bad_last_alloc,              tbb::bad_last_alloc              );); (153, TBB, tbb::runtime_loader rtl0( tbb::runtime_loader::em_status );); (130, TBB, CHECK_EXCEPTION( eid_improper_lock,               tbb::improper_lock               );); (113, TBB, tbb::internal::throw_exception( tbb::internal::id );); (84, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_bad_call );); (96, TBB, tbb::internal::throw_exception( tbb::internal::exception_id( id ) );); (39, TBB, tbb::runtime_loader rtl( tbb::runtime_loader::em_throw );); (132, TBB, tbb::internal::handle_perror( EAGAIN, \"apple\" );); (-1, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_ok );); (37, TBB, tbb::runtime_loader::error_code code = tbb::runtime_loader::ec_ok;); (8, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_bad_arg );); (146, TBB, CHECK( rtl1.status() == tbb::runtime_loader::ec_ok );); (46, TBB, CHECK( code == tbb::runtime_loader::ec_no_lib );); (117, TBB, CHECK_EXCEPTION( eid_missing_wait,                tbb::missing_wait", "label": "(172, TBB, CHECK( rtl1.status() == tbb::runtime_loader::ec_bad_ver );); (97, TBB, tbb::task_scheduler_init init( 1 );); (136, TBB, CHECK_EXCEPTION( eid_invalid_multiple_scheduling, tbb::invalid_multiple_scheduling );); (40, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_no_lib );); (8, TBB, tbb::runtime_loader rtl( tbb::runtime_loader::em_status );); (158, TBB, tbb::runtime_loader rtl1( tbb::runtime_loader::em_status );); (158, TBB, CHECK( rtl0.status() == tbb::runtime_loader::ec_ok );); (126, TBB, CHECK_EXCEPTION( eid_bad_last_alloc,              tbb::bad_last_alloc              );); (154, TBB, tbb::runtime_loader rtl0( tbb::runtime_loader::em_status );); (130, TBB, CHECK_EXCEPTION( eid_improper_lock,               tbb::improper_lock               );); (113, TBB, tbb::internal::throw_exception( tbb::internal::id );); (84, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_bad_call );); (96, TBB, tbb::internal::throw_exception( tbb::internal::exception_id( id ) );); (39, TBB, tbb::runtime_loader rtl( tbb::runtime_loader::em_throw );); (133, TBB, tbb::internal::handle_perror( EAGAIN, \"apple\" );); (-1, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_ok );); (37, TBB, tbb::runtime_loader::error_code code = tbb::runtime_loader::ec_ok;); (8, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_bad_arg );); (147, TBB, CHECK( rtl1.status() == tbb::runtime_loader::ec_ok );); (46, TBB, CHECK( code == tbb::runtime_loader::ec_no_lib );); (117, TBB, CHECK_EXCEPTION( eid_missing_wait,                tbb::missing_wait                );); (121, TBB, CHECK_EXCEPTION( eid_user_abort,                  tbb::user_abort                  );); (37, TBB, CHECK( code == tbb::runtime_loader::ec_bad_arg );)"}
{"predict": "(47, OpenMP, #pragma omp parallel for); (34, OpenMP, #pragma omp parallel for collapse(2)); (85, OpenMP, #pragma omp parallel for collapse(2))", "label": "(31, OpenMP, #pragma omp teams distribute collapse(2)); (47, OpenMP, #pragma omp parallel for collapse(2)); (34, OpenMP, #pragma omp parallel for)"}
{"predict": "(109, CUDA, LogError(\"openCommonICUData() failed to map file %s\\n\", ourPathBuffer);); (109, CUDA, LogError(\"openCommonICUData() failed to map file %s\\n\", path);)", "label": "(43, CUDA, setCommonICUDataPointer(&U_ICUDATA_ENTRY_POINT, FALSE, pErrorCode);)"}
{"predict": "(28, TBB, const std::size_t thread_id = tbb::this_task_arena::current_thread_index();); (1, TBB, tbb::task_arena big_arena(num_threads);)", "label": "(3, TBB, tbb::task_arena big_arena(static_cast<int>(num_threads));); (28, TBB, int thread_id = tbb::this_task_arena::current_thread_index();); (0, TBB, tbb::global_control concurrency(tbb::global_control::max_allowed_parallelism, num_threads);)"}
{"predict": "(17, OpenMP, #pragma omp parallel for simd collapse ((ST > 0) ? 1 + ST : 2)); (22, OpenMP, #pragma omp parallel for simd collapse (S)); (12, OpenMP, #pragma omp parallel for simd collapse (1)); (29, OpenMP, #pragma omp parallel for simd collapse (N)); (2, OpenMP, #pragma omp parallel for simd collapse ()); (31, OpenMP, #pragma omp parallel for simd collapse (2)); (-2, OpenMP, #pragma omp parallel for simd collapse (); (5, OpenMP, #pragma omp parallel for simd collapse (ST); (-7, OpenMP, #pragma omp parallel for simd collapse); (8, OpenMP, #pragma omp parallel for simd collapse (foobool(argc)), collapse (true), collapse (-5)); (14, OpenMP, #pragma omp parallel for simd collapse (C))", "label": "(13, OpenMP, #pragma omp parallel for simd collapse (ST); (34, OpenMP, #pragma omp parallel for simd collapse (N)); (20, OpenMP, #pragma omp parallel for simd collapse (foobool(argc)), collapse (true), collapse (-5)); (-1, OpenMP, #pragma omp parallel for simd collapse); (13, OpenMP, #pragma omp parallel for simd collapse ((ST > 0) ? 1 + ST : 2)); (19, OpenMP, #pragma omp parallel for simd collapse (S)); (9, OpenMP, #pragma omp parallel for simd collapse (1)); (-3, OpenMP, #pragma omp parallel for simd collapse (); (-2, OpenMP, #pragma omp parallel for simd collapse ()); (1, OpenMP, #pragma omp parallel for simd collapse (argc); (27, OpenMP, #pragma omp parallel for simd collapse (2))"}
{"predict": "(3, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (74, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (40, CUDA, EXPECT_EQ(cudaSuccess, cudaMemset(outAccess->sampleData(0), 0xFA, outBufferSize));); (72, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamDestroy(stream));)", "label": "(3, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (73, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (40, CUDA, EXPECT_EQ(cudaSuccess, cudaMemset(outAccess->sampleData(0), 0xFA, outBufferSize));); (104, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamDestroy(stream));)"}
{"predict": "(27, OpenMP, #pragma omp target teams distribute parallel for simd shared(ba)); (18, OpenMP, #pragma omp target teams distribute parallel for simd shared (argc, z)); (15, OpenMP, #pragma omp target teams distribute parallel for simd shared (argc,); (27, OpenMP, #pragma omp target teams distribute parallel for simd shared(ca)); (37, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate(i)); (28, OpenMP, #pragma omp target teams distribute parallel for simd shared(da)); (31, OpenMP, #pragma omp target teams distribute parallel for simd shared(h, B::x)); (13, OpenMP, #pragma omp target teams distribute parallel for simd shared (argc)); (28, OpenMP, #pragma omp target teams distribute parallel for simd shared(e, g)); (29, OpenMP, #pragma omp target teams distribute parallel for simd shared(i)); (12, OpenMP, #pragma omp target teams distribute parallel for simd shared (S1)); (3, OpenMP, #pragma omp target teams distribute parallel for simd shared ()); (15, OpenMP, #pragma omp target teams distribute parallel for simd shared (a, b, c, d, f)); (27, OpenMP, #pragma omp target teams distribute parallel for simd shared(j)); (-4, OpenMP, #pragma omp target teams distribute parallel for simd shared (); (6, OpenMP, #pragma omp target teams distribute parallel for simd shared (argc > 0 ? argv[1] : argv[2])); (6, OpenMP, #pragma omp target teams distribute parallel for simd shared (argc)); (26, OpenMP, #pragma omp target teams distribute parallel for simd shared)", "label": "(25, OpenMP, #pragma omp target teams distribute parallel for simd shared (argv[1])); (28, OpenMP, #pragma omp target teams distribute parallel for simd shared(ca)); (19, OpenMP, #pragma omp target teams distribute parallel for simd shared (S1)); (42, OpenMP, #pragma omp target teams distribute parallel for simd shared(j)); (29, OpenMP, #pragma omp target teams distribute parallel for simd shared(e, g)); (34, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate(i)); (11, OpenMP, #pragma omp target teams distribute parallel for simd shared (argc > 0 ? argv[1] : argv[2])); (30, OpenMP, #pragma omp target teams distribute parallel for simd private(i), shared(i)); (7, OpenMP, #pragma omp target teams distribute parallel for simd shared (argc,); (22, OpenMP, #pragma omp target teams distribute parallel for simd shared(da)); (3, OpenMP, #pragma omp target teams distribute parallel for simd shared (argc); (24, OpenMP, #pragma omp target teams distribute parallel for simd shared(h, B::x)); (31, OpenMP, #pragma omp target teams distribute parallel for simd shared(i)); (28, OpenMP, #pragma omp target teams distribute parallel for simd private(i)); (-5, OpenMP, #pragma omp target teams distribute parallel for simd shared (); (12, OpenMP, #pragma omp target teams distribute parallel for simd shared(ba)); (-9, OpenMP, #pragma omp target teams distribute parallel for simd shared); (-6, OpenMP, #pragma omp target teams distribute parallel for simd shared ()); (1, OpenMP, #pragma omp target teams distribute parallel for simd shared (argc)); (4, OpenMP, #pragma omp target teams distribute parallel for simd shared (a, b, c, d, f, z))"}
{"predict": "(2, CUDA, __shared__ float siftPoint[128*FMC2H];); (2, CUDA, __shared__ float maxScore[FMC2H];); (2, CUDA, __shared__ float maxScor2[FMC2H];); (2, CUDA, __shared__ int maxIndex[FMC2H];)", "label": "(3, CUDA, __shared__ float maxScore[FMC2H];); (4, CUDA, __shared__ int maxIndex[FMC2H];); (2, CUDA, __shared__ float maxScor2[FMC2H];); (-1, CUDA, __shared__ float siftPoint[128*FMC2H];)"}
{"predict": "(73, CUDA, checkCudaErrors(cudaMalloc((void **)&d_idata, bytes));); (130, CUDA, cudaFree(d_odata);); (72, CUDA, checkCudaErrors(cudaMalloc((void **)&d_odata, numBlocks * sizeof(T)));); (127, CUDA, cudaFree(d_idata);)", "label": "(132, CUDA, checkCudaErrors(cudaFree(d_idata));); (71, CUDA, checkCudaErrors(cudaMalloc((void **)&d_idata, bytes));); (131, CUDA, checkCudaErrors(cudaFree(d_odata));); (70, CUDA, checkCudaErrors(cudaMalloc((void **)&d_odata, numBlocks * sizeof(T)));)"}
{"predict": "(16, MPI, MPI_Barrier(world);)", "label": "(17, MPI, MPI_Barrier(world);)"}
{"predict": "(15, CUDA, extern __shared__ char smem[];)", "label": "(15, CUDA, extern __shared__ char smem[];)"}
{"predict": "(122, MPI, MPI_Allreduce(&rc, &all_rc, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);); (129, MPI, MPI_Barrier(MPI_COMM_WORLD);); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (88, MPI, MPI_Allreduce(values, sums, 3, MPI_INT64_T, MPI_SUM, MPI_COMM_WORLD);); (76, MPI, mfu_copy_stats.wtime_ended = MPI_Wtime();); (21, MPI, mfu_copy_stats.wtime_started = MPI_Wtime();)", "label": "(77, MPI, mfu_copy_stats.wtime_ended = MPI_Wtime();); (12, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (24, MPI, mfu_copy_stats.wtime_started = MPI_Wtime();); (89, MPI, MPI_Allreduce(values, sums, 3, MPI_INT64_T, MPI_SUM, MPI_COMM_WORLD);); (120, MPI, MPI_Allreduce(&rc, &all_rc, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);)"}
{"predict": "(36, MPI, MPI_Start(&req);); (13, MPI, MPI_Comm_size(comm, &size);); (28, MPI, MPI_Info_create(&info);); (34, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (48, MPI, MPI_Info_free(&info);); (28, MPI, MPI_Allreduce_init(sbuf, rbuf, count, MPI_INT, MPI_SUM, comm, info, &req);); (44, MPI, MPI_Request_free(&req);); (8, MPI, MPI_Comm_rank(comm, &rank);)", "label": "(36, MPI, MPI_Start(&req);); (13, MPI, MPI_Comm_size(comm, &size);); (28, MPI, MPI_Info_create(&info);); (34, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (48, MPI, MPI_Info_free(&info);); (45, MPI, MPI_Request_free(&req);); (27, MPI, MPI_Alltoall_init(sbuf, count, MPI_INT, rbuf, count, MPI_INT, comm, info, &req);); (8, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(13, MPI, MPI_Comm_size(comm, &size);); (71, MPI, err = MPI_Wait(&req, &status);); (11, MPI, MPI_Comm_rank(comm, &rank);); (32, MPI, MPI_Comm_set_errhandler(comm, MPI_ERRORS_RETURN);)", "label": "(73, MPI, err = MPI_Wait(&req, &status);); (13, MPI, MPI_Comm_size(comm, &size);); (34, MPI, MPI_Comm_set_errhandler(comm, MPI_ERRORS_RETURN);); (10, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(24, MPI, MPI_Barrier(scr_comm_world);); (36, MPI, time_start = MPI_Wtime();); (60, MPI, double time_end = MPI_Wtime();)", "label": "(24, MPI, MPI_Barrier(scr_comm_world);); (60, MPI, double time_end = MPI_Wtime();); (35, MPI, time_start = MPI_Wtime();)"}
{"predict": "(2, MPI, MPI_Init(&argc, &argv);); (86, MPI, MPI_Finalize();); (23, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&rank);); (61, MPI, MPI_Barrier(MPI_COMM_WORLD);); (59, MPI, MPI_Reduce(&counter, &totalNum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);)", "label": "(2, MPI, MPI_Init(&argc, &argv);); (86, MPI, MPI_Finalize();); (23, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&rank);); (61, MPI, MPI_Barrier(MPI_COMM_WORLD);); (59, MPI, MPI_Reduce(&counter, &totalNum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);)"}
{"predict": "(228, CUDA, CudaArch GpuArch = static_cast<CudaArch>(Arch);); (225, CUDA, int Arch = cudaGetDeviceAttribute(CUDA_DEV_ATTR_COMPUTE_MODEL, 0);)", "label": "(123, CUDA, std::string GpuArchName(CudaArchToString(GpuArch));)"}
{"predict": "(6, CUDA, __shared__ DT labels[BW * BH * BD];)", "label": "(5, CUDA, __shared__ DT labels[BW * BH * BD];)"}
{"predict": "(67, OpenMP, #pragma omp target map(tofrom : conv)); (86, OpenMP, elapsed_time = omp_get_wtime() - start_time;); (52, OpenMP, #pragma omp teams distribute parallel for simd); (82, OpenMP, #pragma omp target exit data map(from : xold[0 : Ndim], xnew[0 : Ndim])); (37, OpenMP, start_time = omp_get_wtime();); (42, OpenMP, #pragma omp target enter data map(to : xold[0 : Ndim], xnew[0 : Ndim],); (47, OpenMP, #pragma omp target); (63, OpenMP, #pragma omp teams distribute parallel for simd reduction(+ : conv))", "label": "(69, OpenMP, #pragma omp target map(tofrom : conv)); (87, OpenMP, elapsed_time = omp_get_wtime() - start_time;); (53, OpenMP, #pragma omp teams distribute parallel for simd); (83, OpenMP, #pragma omp target exit data map(from : xold[0 : Ndim], xnew[0 : Ndim])); (37, OpenMP, start_time = omp_get_wtime();); (43, OpenMP, #pragma omp target enter data map(to : xold[0 : Ndim], xnew[0 : Ndim],); (48, OpenMP, #pragma omp target); (63, OpenMP, #pragma omp teams distribute parallel for simd reduction(+ : conv))"}
{"predict": "(55, CUDA, result = cudaFree(0);); (56, CUDA, errPtr = cudaGetErrorName(result);); (292, CUDA, DPRINTF(\"Error in synchronizing stream = %s\\n\", cudaGetErrorName(result));); (199, CUDA, result = cudaMalloc(&inputBufferGPU, inputTensorDesc[0].size);); (89, CUDA, result = cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking);); (57, CUDA, result = cudaSetDevice(0);); (205, CUDA, result = cudaMalloc(&outputBufferGPU, outputTensorDesc[0].size);); (285, CUDA, result = cudaStreamSynchronize(stream);); (121, CUDA, DPRINTF(\"Error in getting numOutputTensors = %d\\n\", err);); (268, CUDA, DPRINTF(\"Error in creating cudaMalloc = %s\\n\", cudaGetErrorName(result));)", "label": "(55, CUDA, result = cudaFree(0);); (56, CUDA, errPtr = cudaGetErrorName(result);); (320, CUDA, cudaFree(outputBufferGPU);); (318, CUDA, cudaFree(inputBufferGPU);); (205, CUDA, result = cudaMalloc(&inputBufferGPU, inputTensorDesc[0].size);); (88, CUDA, result = cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking);); (56, CUDA, result = cudaSetDevice(0);); (211, CUDA, result = cudaMalloc(&outputBufferGPU, outputTensorDesc[0].size);); (324, CUDA, result = cudaStreamDestroy(stream);); (282, CUDA, result = cudaStreamSynchronize(stream);)"}
{"predict": "(49, CUDA, thrust::host_vector<TB> h_B(n*k);); (4, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;); (1, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (55, CUDA, thrust::device_vector<TC> d_C = h_C;); (46, CUDA, thrust::host_vector<TC> h_C(m*n);); (52, CUDA, thrust::device_vector<TB> d_B = h_B;); (86, CUDA, thrust::host_vector<TC> cute_result = d_C;); (49, CUDA, thrust::device_vector<TA> d_A = h_A;); (40, CUDA, thrust::host_vector<TA> h_A(m*k);)", "label": "(49, CUDA, thrust::host_vector<TB> h_B(n*k);); (4, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;); (1, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (55, CUDA, thrust::device_vector<TC> d_C = h_C;); (46, CUDA, thrust::host_vector<TC> h_C(m*n);); (52, CUDA, thrust::device_vector<TB> d_B = h_B;); (86, CUDA, thrust::host_vector<TC> cute_result = d_C;); (49, CUDA, thrust::device_vector<TA> d_A = h_A;); (40, CUDA, thrust::host_vector<TA> h_A(m*k);)"}
{"predict": "(201, MPI, MPI_Send(&corner_val,1,MPI_DOUBLE,my_ID,888,MPI_COMM_WORLD);); (88, MPI, MPI_Bcast(&m,          1, MPI_INT, root, MPI_COMM_WORLD);); (90, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (165, MPI, MPI_Send(&(ARRAY(end,j)), 1, MPI_DOUBLE, my_ID+1, j, MPI_COMM_WORLD);); (240, MPI, MPI_Finalize();); (85, MPI, MPI_Bcast(&n,          1, MPI_INT, root, MPI_COMM_WORLD);); (19, MPI, MPI_Init(&argc,&argv);); (170, MPI, MPI_Recv(inbuf, jjsize, MPI_DOUBLE, my_ID-1, j, MPI_COMM_WORLD, &status);); (184, MPI, MPI_Send(outbuf, jjsize, MPI_DOUBLE, my_ID+1, j, MPI_COMM_WORLD);); (195, MPI, MPI_Recv(&(ARRAY(0,0)),1,MPI_DOUBLE,root,888,MPI_COMM_WORLD,&status);); (138, MPI, MPI_Barrier(MPI_COMM_WORLD);); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (79, MPI, MPI_Bcast(&grp,        1, MPI_INT, root, MPI_COMM_WORLD);); (13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)", "label": "(177, MPI, MPI_Recv(inbuf, jjsize, MPI_DOUBLE, my_ID-1, j, MPI_COMM_WORLD, &status);); (88, MPI, MPI_Bcast(&m,          1, MPI_INT, root, MPI_COMM_WORLD);); (88, MPI, MPI_Bcast(&n,          1, MPI_INT, root, MPI_COMM_WORLD);); (89, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (241, MPI, MPI_Finalize();); (163, MPI, MPI_Send(&(ARRAY(end,j)), 1, MPI_DOUBLE, my_ID+1, j, MPI_COMM_WORLD);); (195, MPI, MPI_Send(&corner_val,1,MPI_DOUBLE,0,888,MPI_COMM_WORLD);); (197, MPI, MPI_Recv(&(ARRAY(0,0)),1,MPI_DOUBLE,root,888,MPI_COMM_WORLD,&status);); (18, MPI, MPI_Init(&argc,&argv);); (183, MPI, MPI_Send(outbuf, jjsize, MPI_DOUBLE, my_ID+1, j, MPI_COMM_WORLD);); (138, MPI, MPI_Barrier(MPI_COMM_WORLD);); (17, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (79, MPI, MPI_Bcast(&grp,        1, MPI_INT, root, MPI_COMM_WORLD);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(35, TBB, tbb::task_arena init(tbb::task_arena::automatic);); (113, TBB, tbb::task_scheduler_init init(tbb::task_scheduler_init::default_num_threads());); (8, TBB, tbb::task_scheduler_init init(tbb::task_scheduler_init::default_num_threads());); (108, TBB, tbb::task_arena limited(128);); (33, TBB, tbb::enumerable_thread_specific<double> results;); (33, TBB, tbb::mutex::scoped_lock lock(results_mutex);); (17, TBB, tbb::mutex results_mutex;); (35, TBB, std::fstream fs;); (112, TBB, TaskScheduler::destroy();); (31, TBB, tbb::task_reduce<double> reduce;); (111, TBB, TaskScheduler::create();)", "label": "(62, TBB, PRINT( tbb::TBB_runtime_interface_version() );); (68, TBB, while (tracker.get_concurrency() < tbb::task_scheduler_init::default_num_threads()) tbb_pi<double> (N);); (11, TBB, tbb::task_scheduler_init init(tbb::task_scheduler_init::default_num_threads());); (82, TBB, TaskSchedulerTBB::create(0);); (90, TBB, TaskSchedulerTBB::destroy();)"}
{"predict": "(66, MPI, MPI_Bcast( ToRecv, nsupers, MPI_INT, 0,  grid3d->zscp.comm);); (39, MPI, MPI_Bcast( supno, n, mpi_int_t, 0,  grid3d->zscp.comm);); (27, MPI, MPI_Bcast( &nsupers, 1, mpi_int_t, 0,  grid3d->zscp.comm);); (69, MPI, MPI_Bcast( ToSendR[i], Pc, MPI_INT, 0,  grid3d->zscp.comm);); (11, MPI, MPI_Bcast( etree, n, mpi_int_t, 0,  grid3d->zscp.comm);); (233, MPI, MPI_Bcast(*((void**)&(Llu->fsendx_plist[lk])), Pr * sizeof (int), MPI_BYTE, 0, grid3d->zscp.comm);); (84, MPI, MPI_Bcast( &(Llu->ldalsum), 1, mpi_int_t, 0,  grid3d->zscp.comm);); (61, MPI, MPI_Bcast( ToSendD, nbr, MPI_INT, 0,  grid3d->zscp.comm);); (29, MPI, MPI_Bcast( xsup, nsupers + 1, mpi_int_t, 0,  grid3d->zscp.comm);); (80, MPI, MPI_Bcast( &(Llu->nbsendx), 1, MPI_INT, 0,  grid3d->zscp.comm);); (78, MPI, MPI_Bcast( &(Llu->nfsendx), 1, MPI_INT, 0,  grid3d->zscp.comm);); (45, MPI, MPI_Bcast( bufmax, NBUFFERS, mpi_int_t, 0,  grid3d->zscp.comm);)", "label": "(66, MPI, MPI_Bcast( ToRecv, nsupers, MPI_INT, 0,  grid3d->zscp.comm);); (39, MPI, MPI_Bcast( supno, n, mpi_int_t, 0,  grid3d->zscp.comm);); (27, MPI, MPI_Bcast( &nsupers, 1, mpi_int_t, 0,  grid3d->zscp.comm);); (69, MPI, MPI_Bcast( ToSendR[i], Pc, MPI_INT, 0,  grid3d->zscp.comm);); (11, MPI, MPI_Bcast( etree, n, mpi_int_t, 0,  grid3d->zscp.comm);); (233, MPI, MPI_Bcast(*((void**)&(Llu->fsendx_plist[lk])), Pr * sizeof (int), MPI_BYTE, 0, grid3d->zscp.comm);); (84, MPI, MPI_Bcast( &(Llu->ldalsum), 1, mpi_int_t, 0,  grid3d->zscp.comm);); (61, MPI, MPI_Bcast( ToSendD, nbr, MPI_INT, 0,  grid3d->zscp.comm);); (29, MPI, MPI_Bcast( xsup, nsupers + 1, mpi_int_t, 0,  grid3d->zscp.comm);); (80, MPI, MPI_Bcast( &(Llu->nbsendx), 1, MPI_INT, 0,  grid3d->zscp.comm);); (78, MPI, MPI_Bcast( &(Llu->nfsendx), 1, MPI_INT, 0,  grid3d->zscp.comm);); (45, MPI, MPI_Bcast( bufmax, NBUFFERS, mpi_int_t, 0,  grid3d->zscp.comm);)"}
{"predict": "(16, OpenMP, CUs[i] = omp_get_thread_num();); (10, OpenMP, #pragma omp target teams distribute num_teams(numTeams) thread_limit(1024) map(from:Res[0:numTeams*NN], CUs[0:numTeams*NN])); (11, OpenMP, int j = omp_get_team_num();); (11, OpenMP, if (j < N) lims[j] = omp_get_team_size(j);); (8, OpenMP, #pragma omp target data map(from:Res[0:numTeams*NN], CUs[0:numTeams*NN])); (10, OpenMP, threads[j] = omp_get_num_threads();)", "label": "(11, OpenMP, #pragma omp distribute); (16, OpenMP, #pragma omp parallel for); (13, OpenMP, threads[j] = omp_get_num_threads();); (7, OpenMP, #pragma omp target teams num_teams(numTeams) thread_limit(1024) map (tofrom: CUs, lims, threads, Res)); (16, OpenMP, CUs[i] = omp_ext_get_smid();); (9, OpenMP, lims[j] = omp_get_num_teams();)"}
{"predict": "(206, MPI, MPI_Recv(&(ARRAY(0,0)),1,MPI_DOUBLE,root,888,MPI_COMM_WORLD,&status);); (248, MPI, MPI_Send(&corner_val,1,MPI_DOUBLE,0,888,MPI_COMM_WORLD);); (90, MPI, MPI_Bcast(&m,          1, MPI_INT, root, MPI_COMM_WORLD);); (92, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (164, MPI, MPI_Send(&(ARRAY(end,j)), 1, MPI_DOUBLE, my_ID+1, j, MPI_COMM_WORLD);); (243, MPI, MPI_Recv(inbuf, jjsize, MPI_DOUBLE, my_ID-1, j, MPI_COMM_WORLD, &status);); (167, MPI, MPI_Send(outbuf, jjsize, MPI_DOUBLE, my_ID+1, j, MPI_COMM_WORLD);); (203, MPI, MPI_Send(&ARRAY(end,j), 1, MPI_DOUBLE, root, 888, MPI_COMM_WORLD);); (11, MPI, MPI_Init(&argc,&argv);); (185, MPI, MPI_Recv(inbuf, jjsize, MPI_DOUBLE, my_ID-1, j, MPI_COMM_WORLD, &status);); (87, MPI, MPI_Bcast(&n,          1, MPI_INT, root, MPI_COMM_WORLD);); (235, MPI, MPI_Recv(&(ARRAY(0,0)),1,MPI_DOUBLE,my_ID-1,888,MPI_COMM_WORLD,&status);); (148, MPI, MPI_Send(&(ARRAY(start,j)), 1, MPI_DOUBLE, my_ID+1, j, MPI_COMM_WORLD);); (230, MPI, MPI_Send(outbuf, jjsize, MPI_DOUBLE, my_ID-1, j, MPI_COMM_WORLD);); (173, MPI, MPI_Recv(outbuf, jjsize, MPI_DOUBLE, my_ID+1, j, MPI_COMM_WORLD, &status);); (242, MPI, MPI_Finalize();); (13, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (79, MPI, MPI_Bcast(&grp,        1, MPI_INT, root, MPI_COMM_WORLD);); (10, MPI", "label": "(180, MPI, MPI_Recv(inbuf, jjsize, MPI_DOUBLE, my_ID-1, j, MPI_COMM_WORLD, &status);); (91, MPI, MPI_Bcast(&m,          1, MPI_INT, root, MPI_COMM_WORLD);); (91, MPI, MPI_Bcast(&n,          1, MPI_INT, root, MPI_COMM_WORLD);); (92, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (244, MPI, MPI_Finalize();); (166, MPI, MPI_Send(&(ARRAY(end,j)), 1, MPI_DOUBLE, my_ID+1, j, MPI_COMM_WORLD);); (198, MPI, MPI_Send(&corner_val,1,MPI_DOUBLE,0,888,MPI_COMM_WORLD);); (200, MPI, MPI_Recv(&(ARRAY(0,0)),1,MPI_DOUBLE,root,888,MPI_COMM_WORLD,&status);); (19, MPI, MPI_Init(&argc,&argv);); (186, MPI, MPI_Send(outbuf, jjsize, MPI_DOUBLE, my_ID+1, j, MPI_COMM_WORLD);); (141, MPI, MPI_Barrier(MPI_COMM_WORLD);); (18, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (82, MPI, MPI_Bcast(&grp,        1, MPI_INT, root, MPI_COMM_WORLD);); (15, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(78, OpenCL, clReleaseMemObject(bufS);); (44, OpenCL, clReleaseCommandQueue(queue);); (27, OpenCL, printf( \"clCreateContext() failed with %d\\n\", err );); (73, OpenCL, clReleaseMemObject(bufC);); (71, OpenCL, clReleaseMemObject(bufSB);); (69, OpenCL, clReleaseMemObject(bufSA);); (64, OpenCL, clReleaseEvent(event);); (30, OpenCL, printf( \"clCreateCommandQueue() failed with %d\\n\", err );); (27, OpenCL, queue = clCreateCommandQueue(ctx, device, 0, &err);); (45, OpenCL, bufSB = clCreateBuffer(ctx, CL_MEM_READ_WRITE, (2 * sizeof(float)), NULL, &err);); (29, OpenCL, clReleaseContext(ctx);); (45, OpenCL, bufS = clCreateBuffer(ctx, CL_MEM_READ_WRITE, (2 * sizeof(float)), NULL, &err);); (39, OpenCL, bufSA = clCreateBuffer(ctx, CL_MEM_READ_WRITE, (2 * sizeof(float)), NULL, &err);); (59, OpenCL, err = clEnqueueReadBuffer(queue, bufS, CL_TRUE, 0, (2 * sizeof(float)), s, 0, NULL, NULL);); (53, OpenCL, err = clWaitForEvents(1, &event);); (36, OpenCL, bufC = clCreateBuffer(ctx, CL_MEM_READ_WRITE, (2 * sizeof(float)), NULL, &err);); (15, OpenCL, ctx = clCreateContext(props, 1, &device, NULL, NULL, &err);); (40, OpenCL, err = clEnqueueWriteBuffer(queue, bufSA, CL_TRUE, 0, (2 * sizeof(float)), sa, 0, NULL, NULL);); (40, OpenCL, err = clEnqueueWriteBuffer(queue, bufSB, CL_TRUE, 0, (2 * sizeof(float)), sb, 0, NULL, NULL);)", "label": "(50, OpenCL, bufSA = clCreateBuffer(ctx, CL_MEM_READ_WRITE, sizeof(cl_float), NULL, &err);); (52, OpenCL, bufS  = clCreateBuffer(ctx, CL_MEM_READ_WRITE, sizeof(cl_float), NULL, &err);); (67, OpenCL, err = clWaitForEvents(1, &event);); (55, OpenCL, err = clEnqueueWriteBuffer(queue, bufS, CL_TRUE, 0, sizeof(cl_float), &S, 0, NULL, NULL);); (83, OpenCL, clReleaseMemObject(bufC);); (77, OpenCL, clReleaseEvent(event);); (79, OpenCL, clReleaseMemObject(bufSA);); (68, OpenCL, err = clEnqueueReadBuffer(queue, bufS, CL_TRUE, 0, sizeof(cl_float), &S, 0, NULL, NULL);); (28, OpenCL, clReleaseContext(ctx);); (63, OpenCL, err = clEnqueueReadBuffer(queue, bufSA, CL_TRUE, 0, sizeof(cl_float), &SA, 0, NULL, NULL);); (47, OpenCL, err = clEnqueueWriteBuffer(queue, bufC, CL_TRUE, 0, sizeof(cl_float), &C, 0, NULL, NULL);); (62, OpenCL, err = clEnqueueReadBuffer(queue, bufSB, CL_TRUE, 0, sizeof(cl_float), &SB, 0, NULL, NULL);); (32, OpenCL, clReleaseCommandQueue(queue);); (38, OpenCL, bufSB = clCreateBuffer(ctx, CL_MEM_READ_WRITE, sizeof(cl_float), NULL, &err);); (74, OpenCL, clReleaseMemObject(bufS);); (71, OpenCL, clReleaseMemObject(bufSB);); (19, OpenCL, printf( \"clCreateCommandQueue() failed with %d\\n\", err );); (16, OpenCL, queue = clCreateCommandQueue(ctx, device, 0, &err);); (34, OpenCL, bufC  = clCreateBuffer(ctx, CL_MEM_READ_WRITE, sizeof(cl_float), NULL, &err);); (8, OpenCL, ctx = clCreateContext(props, 1, &device, NULL, NULL, &err);); (9, OpenCL, printf( \"clCreateContext() failed with %d\\n\", err );); (34, OpenCL, err = clEnqueueWriteBuffer(queue, bufSA, CL_TRUE, 0, sizeof(cl_float), &SA, 0, NULL, NULL);); (34, OpenCL, err = clEnqueueWriteBuffer(queue, bufSB, CL_TRUE, 0, sizeof(cl_float), &SB, 0, NULL, NULL);); (51, OpenCL, err = clEnqueueReadBuffer(queue, bufC, CL_TRUE, 0, sizeof(cl_float), &C, 0, NULL, NULL);)"}
{"predict": "(171, CUDA, checkCudaErrors(cudaFree(d_directions));); (57, CUDA, findCudaDevice(argc, (const char **)argv);); (91, CUDA, throw std::runtime_error(cudaGetErrorString(cudaResult));); (170, CUDA, checkCudaErrors(cudaFree(d_output));); (121, CUDA, checkCudaErrors(cudaDeviceSynchronize());)", "label": "(93, CUDA, throw std::runtime_error(cudaGetErrorString(cudaResult));); (57, CUDA, findCudaDevice(argc, (const char **)argv);); (117, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (211, CUDA, checkCudaErrors(cudaFree(d_output));); (209, CUDA, checkCudaErrors(cudaFree(d_directions));)"}
{"predict": "(96, OpenMP, #pragma omp distribute parallel for private(m)); (84, OpenMP, #pragma omp distribute parallel for private(j)); (80, OpenMP, #pragma omp parallel private(i)); (66, OpenMP, #pragma omp distribute parallel for nowait); (10, OpenMP, #pragma omp distribute parallel for private(); (2, OpenMP, #pragma omp target); (28, OpenMP, #pragma omp distribute parallel for private(argc > 0 ? argv[1] : argv[2])); (2, OpenMP, #pragma omp distribute parallel for private); (36, OpenMP, #pragma omp distribute parallel for private(S1)); (20, OpenMP, #pragma omp distribute parallel for private(argc,); (62, OpenMP, #pragma omp parallel); (13, OpenMP, #pragma omp distribute parallel for private(argc); (37, OpenMP, #pragma omp distribute parallel for private(a, b)); (41, OpenMP, #pragma omp distribute parallel for private(argv[1])); (50, OpenMP, #pragma omp distribute parallel for private(h, B::x)); (44, OpenMP, #pragma omp distribute parallel for private(e, g)); (3, OpenMP, #pragma omp distribute parallel for private()); (60, OpenMP, #pragma omp distribute parallel for private(i)); (21, OpenMP, #pragma omp distribute parallel for private(argc)); (62, OpenMP, #pragma omp parallel shared(i)); (-12, OpenMP, #pragma omp teams)", "label": "(82, OpenMP, #pragma omp parallel private(i)); (95, OpenMP, #pragma omp distribute parallel for private(m)); (5, OpenMP, #pragma omp target); (26, OpenMP, #pragma omp distribute parallel for private(argc,); (60, OpenMP, #pragma omp distribute parallel for private(h, B::x)); (64, OpenMP, #pragma omp distribute parallel for nowait); (8, OpenMP, #pragma omp distribute parallel for private(); (65, OpenMP, #pragma omp parallel); (16, OpenMP, #pragma omp distribute parallel for private(argc); (40, OpenMP, #pragma omp distribute parallel for private(a, b)); (49, OpenMP, #pragma omp distribute parallel for private(e, g)); (33, OpenMP, #pragma omp distribute parallel for private(S1)); (42, OpenMP, #pragma omp distribute parallel for private(argv[1])); (6, OpenMP, #pragma omp distribute parallel for private()); (-5, OpenMP, #pragma omp distribute parallel for private); (62, OpenMP, #pragma omp distribute parallel for private(i)); (69, OpenMP, #pragma omp distribute parallel for private(j)); (22, OpenMP, #pragma omp distribute parallel for private(argc)); (16, OpenMP, #pragma omp distribute parallel for private(argc > 0 ? argv[1] : argv[2])); (62, OpenMP, #pragma omp parallel shared(i)); (-12, OpenMP, #pragma omp teams)"}
{"predict": "(33, CUDA, __shared__ double s_lsum[MAXSUPER];); (34, CUDA, volatile __shared__ int s_bmod;)", "label": "(35, CUDA, volatile __shared__ int s_bmod;); (32, CUDA, __shared__ double s_lsum[MAXSUPER];)"}
{"predict": "(33, TBB, tbb::enumerable_thread_specific<tbb::task_arena*> tls;); (29, TBB, tbb::global_control control(tbb::global_control::max_allowed_parallelism, n_threads);); (27, TBB, std::vector<tbb::task_arena> arenas(n_arenas);); (-1, TBB, n_threads = tbb::task_scheduler_init::default_num_threads();); (33, TBB, tbb::enumerable_thread_specific<tbb::task_arena*> tls;)", "label": "(20, TBB, tbb::global_control control(tbb::global_control::max_allowed_parallelism, n_threads);); (1, TBB, n_threads = tbb::this_task_arena::max_concurrency();); (29, TBB, tbb::enumerable_thread_specific<tbb::task_arena*> tls;); (25, TBB, std::vector<tbb::task_arena> arenas(n_arenas);)"}
{"predict": "(15, OpenCL, int error = clGetDeviceInfo( device, CL_DEVICE_IMAGE2D_MAX_WIDTH, sizeof( maxWidth ), &maxWidth, NULL );); (16, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_GLOBAL_MEM_SIZE, sizeof( memSize ), &memSize, NULL );); (14, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_MAX_MEM_ALLOC_SIZE, sizeof( maxAllocSize ), &maxAllocSize, NULL );)", "label": "(15, OpenCL, int error = clGetDeviceInfo( device, CL_DEVICE_IMAGE2D_MAX_WIDTH, sizeof( maxWidth ), &maxWidth, NULL );); (16, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_GLOBAL_MEM_SIZE, sizeof( memSize ), &memSize, NULL );); (14, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_MAX_MEM_ALLOC_SIZE, sizeof( maxAllocSize ), &maxAllocSize, NULL );)"}
{"predict": "(36, MPI, MPI_Comm_size(intercomm, &size);); (56, MPI, MPI_Comm_disconnect(&intercomm);); (102, MPI, MPI_Finalize();); (19, MPI, MPI_Comm_get_parent(&parentcomm);); (47, MPI, MPI_Recv(&data, 1, MPI_INT, RECEIVER_RANK, DATA_TAG, intercomm, &status);); (79, MPI, MPI_Abort(intercomm, 1);); (73, MPI, MPI_Recv(&i, 1, MPI_INT, SENDER_RANK, DATA_TAG, intercomm, &status);); (43, MPI, MPI_Send(&i, 1, MPI_INT, RECEIVER_RANK, DATA_TAG, intercomm);); (29, MPI, MPI_Comm_rank(intercomm, &rank);); (26, MPI, MPI_Comm_remote_size(intercomm, &rsize);); (76, MPI, MPI_Send(&i, 1, MPI_INT, SENDER_RANK, DATA_TAG, intercomm);)", "label": "(36, MPI, MPI_Comm_size(intercomm, &size);); (56, MPI, MPI_Comm_disconnect(&intercomm);); (102, MPI, MPI_Finalize();); (19, MPI, MPI_Comm_get_parent(&parentcomm);); (47, MPI, MPI_Recv(&data, 1, MPI_INT, RECEIVER_RANK, DATA_TAG, intercomm, &status);); (79, MPI, MPI_Abort(intercomm, 1);); (73, MPI, MPI_Recv(&i, 1, MPI_INT, SENDER_RANK, DATA_TAG, intercomm, &status);); (43, MPI, MPI_Send(&i, 1, MPI_INT, RECEIVER_RANK, DATA_TAG, intercomm);); (29, MPI, MPI_Comm_rank(intercomm, &rank);); (26, MPI, MPI_Comm_remote_size(intercomm, &rsize);); (76, MPI, MPI_Send(&i, 1, MPI_INT, SENDER_RANK, DATA_TAG, intercomm);)"}
{"predict": "(236, MPI, MPI_Barrier(MPI_COMM_WORLD);); (293, MPI, MPI_Wait(&(request[0+4]), MPI_STATUSES_IGNORE);); (129, MPI, MPI_Bcast(&n,          1, MPI_INT, root, MPI_COMM_WORLD);); (44, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (298, MPI, MPI_Wait(&(request[3+4]), MPI_STATUSES_IGNORE);); (255, MPI, MPI_Wait(&(request[0]), MPI_STATUS_IGNORE);); (126, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (288, MPI, MPI_Wait(&(request[1+4]), MPI_STATUSES_IGNORE);); (37, MPI, MPI_Init(&argc,&argv);); (252, MPI, MPI_Wait(&(request[1]), MPI_STATUS_IGNORE);); (291, MPI, MPI_Wait(&(request[2+4]), MPI_STATUSES_IGNORE);); (256, MPI, MPI_Wait(&(request[2]), MPI_STATUS_IGNORE);); (365, MPI, MPI_Finalize();); (255, MPI, MPI_Wait(&(request[3]), MPI_STATUS_IGNORE);); (324, MPI, MPI_Reduce(&local_norm, &norm, 1, MPI_DTYPE, MPI_SUM, root, MPI_COMM_WORLD);); (31, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)", "label": "(295, MPI, MPI_Wait(&(request[1+4]), MPI_STATUS_IGNORE);); (301, MPI, MPI_Wait(&(request[3+4]), MPI_STATUS_IGNORE);); (234, MPI, MPI_Barrier(MPI_COMM_WORLD);); (128, MPI, MPI_Bcast(&n,          1, MPI_INT, root, MPI_COMM_WORLD);); (43, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (289, MPI, MPI_Wait(&(request[0+4]), MPI_STATUS_IGNORE);); (254, MPI, MPI_Wait(&(request[0]), MPI_STATUS_IGNORE);); (294, MPI, MPI_Wait(&(request[2+4]), MPI_STATUS_IGNORE);); (124, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (36, MPI, MPI_Init(&argc,&argv);); (251, MPI, MPI_Wait(&(request[1]), MPI_STATUS_IGNORE);); (256, MPI, MPI_Wait(&(request[2]), MPI_STATUS_IGNORE);); (365, MPI, MPI_Finalize();); (255, MPI, MPI_Wait(&(request[3]), MPI_STATUS_IGNORE);); (324, MPI, MPI_Reduce(&local_norm, &norm, 1, MPI_DTYPE, MPI_SUM, root, MPI_COMM_WORLD);); (31, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(48, OpenMP, #pragma omp parallel for)", "label": "(55, OpenMP, #pragma omp for schedule(guided, 512)); (52, OpenMP, #pragma omp parallel firstprivate(transformation_matrix, constant_vector, slave_solution_vector, master_solution_vector, p_master_node, p_slave_node))"}
{"predict": "(106, MPI, MPI_Type_get_extent(datatype, &lb, &buftype_extent);); (115, MPI, MPI_Comm_rank(fd->comm, &rank);); (70, MPI, MPI_Allreduce(&ntimes, &max_ntimes, 1, MPI_INT, MPI_MAX, fd->comm);); (188, MPI, MPI_Get_address(read_buf + real_off - off, &addr);); (108, MPI, MPI_Barrier(fd->comm);); (174, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(113, MPI, MPI_Comm_rank(fd->comm, &rank);); (180, MPI, MPI_Get_count(&read_status, MPI_BYTE, &actual_size);); (105, MPI, MPI_Type_get_extent(datatype, &lb, &buftype_extent);); (69, MPI, MPI_Allreduce(&ntimes, &max_ntimes, 1, MPI_INT, MPI_MAX, fd->comm);); (219, MPI, MPI_Get_address(read_buf + req_off - real_off, &addr);)"}
{"predict": "(4, MPI, MPI_Init(&argc, &argv);); (52, MPI, MPI_Finalize();); (6, MPI, ierr = MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, int ierr = MPI_Comm_size(MPI_COMM_WORLD, &comm_size);)", "label": "(4, MPI, MPI_Init(&argc, &argv);); (52, MPI, MPI_Finalize();); (6, MPI, ierr = MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, int ierr = MPI_Comm_size(MPI_COMM_WORLD, &comm_size);)"}
{"predict": "(30, CUDA, cudaFp16Input = (half *) FastllmCudaMalloc(n * m * sizeof(half));); (114, CUDA, FastllmCudaFinishOutput(output, cudaOutput);); (30, CUDA, cudaFp16Output = (half *) FastllmCudaMalloc(n * k * sizeof(half));); (125, CUDA, FastllmCudaFree(cudaFp16Input);); (1)", "label": "(119, CUDA, FastllmCudaFinishOutput(output, cudaOutput);); (39, CUDA, cudaFp16Input = (half *) FastllmCudaMalloc(n * m * sizeof(half));); (21, CUDA, state = cudaMemset(cudaBiasData, 0, k * sizeof(float));); (115, CUDA, FastllmCudaFinishInput(input, cudaInput);); (43, CUDA, cudaFp16Output = (half *) FastllmCudaMalloc(n * k * sizeof(half));); (9, CUDA, state = cudaMemcpy(cudaMins, mins, k * sizeof(float), cudaMemcpyHostToDevice);); (-2, CUDA, state = cudaMalloc(&cudaScales, k * sizeof(float));); (94, CUDA, FastllmCudaFree(cudaFp16Input);); (103, CUDA, FastllmCudaFree(cudaFp16Output);); (32, CUDA, cudaFp16Weight = (half *) FastllmCudaMalloc(k * m * sizeof(half));); (-1, CUDA, state = cudaMalloc(&cudaMins, k * sizeof(float));); (-6, CUDA, state = cudaMemcpy(cudaScales, weight.scales.data(), k * sizeof(float), cudaMemcpyHostToDevice);); (7, CUDA, state = cudaMalloc(&cudaBiasData, k * sizeof(float));); (12, CUDA, checkCudaErrors(\"Error: CUDA error when moving bias to device!\", state);); (7, CUDA, state = cudaMemcpy(cudaBiasData, (uint8_t*)bias.cudaData, k * sizeof(float), cudaMemcpyDeviceToDevice);); (87, CUDA, FastllmCudaFree(cudaFp16Weight);); (18, CUDA, float *cudaOutput = (float*)FastllmCudaPrepareOutput(output);); (16, CUDA, float *cudaInput = (float*)FastllmCudaPrepareInput(input);)"}
{"predict": "(201, OpenCL, error = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &globalWorkgroupSize, &localWorkgroupSize, 0, NULL, NULL);); (153, OpenCL, error = clSetKernelArg(kernel, 4, sizeof(dstStride), &dstStride);); (148, OpenCL, error = clSetKernelArg(kernel, 2, sizeof(numElementsPerLineInt), &numElementsPerLineInt);); (151, OpenCL, error = clSetKernelArg(kernel, 5, sizeof(totalLines), &totalLines);); (148, OpenCL, error = clSetKernelArg(kernel, 3, sizeof(lineCopiesPerWorkItemInt), &lineCopiesPerWorkItemInt);); (142, OpenCL, error = clSetKernelArg(kernel, 0, sizeof(inBuffer), &inBuffer);); (143, OpenCL, error = clSetKernelArg(kernel, 1, sizeof(outBuffer), &outBuffer);); (150, OpenCL, error = clSetKernelArg(kernel, 6, sizeof(numberOfLocalWorkgroups), &numberOfLocalWorkgroups);); (146, OpenCL, error = clSetKernelArg(kernel, 2, sizeof(dstMargin), &dstMargin);); (139, OpenCL, error = clEnqueueWriteBuffer(queue, outBufferWrapper, CL_TRUE, 0, outBufferSize, outBufferCopy, 0, NULL, NULL);); (142, OpenCL, error = clSetKernelArg(kernel, 1, sizeof(srcStride), &srcStride);); (134, OpenCL, clMemWrapper outBufferWrapper = clCreateBuffer(context, CL_MEM_READ_WRITE, outBufferSize, NULL, &error);); (141, OpenCL, error = clSetKernelArg(kernel, 3, sizeof(srcMargin), &srcMargin);); (131, OpenCL, clMemWrapper inBufferWrapper = clCreateBuffer(context, CL_MEM_READ_ONLY, inBufferSize, NULL, &error);); (148, OpenCL, error = clEnqueueReadBuffer(queue, outBufferWrapper, CL_TRUE, 0, outBufferSize, outBuffer, 0, NULL, NULL);); (135, OpenCL, error = clEnqueueWriteBuffer", "label": "(169, OpenCL, error = clSetKernelArg(kernel, 0, sizeof(streams[0]), &streams[0]);); (170, OpenCL, error = clSetKernelArg(kernel, 1, sizeof(streams[1]), &streams[1]);); (184, OpenCL, error = clSetKernelArg(kernel, 7, sizeof(dstStride), &dstStride);); (170, OpenCL, error = clSetKernelArg(kernel, 2, localBufferSize, NULL);); (180, OpenCL, error = clSetKernelArg(kernel, 6, sizeof(srcStride), &srcStride);)"}
{"predict": "(12, OpenCL, cl_kernel k1 = clCreateKernel(program, \"compiler_program_global1\", &err);); (10, OpenCL, cl_kernel k0 = clCreateKernel(program, \"compiler_program_global0\", &err);)", "label": "(55, OpenCL, clReleaseProgram(program);); (52, OpenCL, clReleaseKernel(k0);); (52, OpenCL, clReleaseKernel(k1);); (11, OpenCL, cl_kernel k1 = clCreateKernel(program, \"compiler_program_global1\", &err);); (8, OpenCL, cl_kernel k0 = clCreateKernel(program, \"compiler_program_global0\", &err);)"}
{"predict": "(65, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&device_buffer, input_size_bytes));); (68, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(device_buffer, input.data(), input_size_bytes, cudaMemcpyHostToDevice));); (55, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamCreate(&stream));); (78, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(device_buffer));); (64, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());)", "label": "(66, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&device_buffer, input_size_bytes));); (56, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamCreate(&stream));); (80, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(device_buffer));); (67, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (65, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(device_buffer, input.data(), input_size_bytes, cudaMemcpyHostToDevice));)"}
{"predict": "(95, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(streams), &streams);); (105, OpenCL, clReleaseKernel(kernel);); (85, OpenCL, err = clGetDeviceInfo(dev, CL_DEVICE_MAX_WORK_GROUP_SIZE, sizeof(local_size), &local_size, NULL);); (59, OpenCL, err = clBuildProgram(program, 0, NULL, includes, NULL, NULL);); (103, OpenCL, clReleaseProgram(program);); (48, OpenCL, cl_program program = clCreateProgramWithSource(ctx, 4, sources, NULL, &err);); (96, OpenCL, err = clEnqueueNDRangeKernel(commQueues[0], kernel, 1, NULL, &streamCount, &local_size, numWaitEvents, waitEvents, outEvents);); (89, OpenCL, err |= clSetKernelArg(kernel, 1, sizeof(number_count_per_stream), &number_count_per_stream);); (89, OpenCL, err |= clSetKernelArg(kernel, 2, sizeof(outBuffer), &outBuffer);); (68, OpenCL, cl_kernel kernel = clCreateKernel(program, \"fillBufferU01\", &err);)", "label": "(95, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(streams), &streams);); (105, OpenCL, clReleaseKernel(kernel);); (85, OpenCL, err = clGetDeviceInfo(dev, CL_DEVICE_MAX_WORK_GROUP_SIZE, sizeof(local_size), &local_size, NULL);); (59, OpenCL, err = clBuildProgram(program, 0, NULL, includes, NULL, NULL);); (103, OpenCL, clReleaseProgram(program);); (48, OpenCL, cl_program program = clCreateProgramWithSource(ctx, 4, sources, NULL, &err);); (96, OpenCL, err = clEnqueueNDRangeKernel(commQueues[0], kernel, 1, NULL, &streamCount, &local_size, numWaitEvents, waitEvents, outEvents);); (89, OpenCL, err |= clSetKernelArg(kernel, 1, sizeof(number_count_per_stream), &number_count_per_stream);); (89, OpenCL, err |= clSetKernelArg(kernel, 2, sizeof(outBuffer), &outBuffer);); (68, OpenCL, cl_kernel kernel = clCreateKernel(program, \"fillBufferU01\", &err);)"}
{"predict": "(87, MPI, MPI_Bcast( asub, nnz, mpi_int_t,  0, grid.comm );); (231, MPI, MPI_Finalize();); (83, MPI, MPI_Bcast( &nnz, 1,   mpi_int_t,  0, grid.comm );); (83, MPI, MPI_Bcast( a,    nnz, MPI_DOUBLE, 0, grid.comm );); (22, MPI, MPI_Init( &argc, &argv );); (83, MPI, MPI_Bcast( xa,   n+1, mpi_int_t,  0, grid.comm );); (78, MPI, MPI_Bcast( &n,   1,   mpi_int_t,  0, grid.comm );); (76, MPI, MPI_Bcast( &m,   1,   mpi_int_t,  0, grid.comm );)", "label": "(87, MPI, MPI_Bcast( asub, nnz, mpi_int_t,  0, grid.comm );); (224, MPI, MPI_Finalize();); (83, MPI, MPI_Bcast( &nnz, 1,   mpi_int_t,  0, grid.comm );); (83, MPI, MPI_Bcast( a,    nnz, MPI_DOUBLE, 0, grid.comm );); (22, MPI, MPI_Init( &argc, &argv );); (83, MPI, MPI_Bcast( xa,   n+1, mpi_int_t,  0, grid.comm );); (78, MPI, MPI_Bcast( &n,   1,   mpi_int_t,  0, grid.comm );); (76, MPI, MPI_Bcast( &m,   1,   mpi_int_t,  0, grid.comm );)"}
{"predict": "(45, CUDA, __shared__ doublecomplex temp2[MAXSUPER];); (46, CUDA, __shared__ int cur_send_num;); (74, CUDA, __shared__ doublecomplex rtemp_loc[MAXSUPER*nrhs*2];); (107, CUDA, __threadfence();); (155, CUDA, __shared__ int recv_num, finish_num;); (38, CUDA, __shared__ doublecomplex rhs[MAXSUPER*nrhs];)", "label": "(45, CUDA, __shared__ doublecomplex temp2[MAXSUPER];)"}
{"predict": "(27, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_IMAGE_MAX_ARRAY_SIZE, sizeof( maxArraySize ), &maxArraySize, NULL );); (28, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_GLOBAL_MEM_SIZE, sizeof( memSize ), &memSize, NULL );); (26, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_MAX_MEM_ALLOC_SIZE, sizeof( maxAllocSize ), &maxAllocSize, NULL );); (23, OpenCL, error = clGetDeviceInfo( device, CL_DEVICE_IMAGE2D_MAX_WIDTH, sizeof( maxWidth ), &maxWidth, NULL );)", "label": "(27, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_IMAGE_MAX_ARRAY_SIZE, sizeof( maxArraySize ), &maxArraySize, NULL );); (28, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_GLOBAL_MEM_SIZE, sizeof( memSize ), &memSize, NULL );); (26, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_MAX_MEM_ALLOC_SIZE, sizeof( maxAllocSize ), &maxAllocSize, NULL );); (23, OpenCL, error = clGetDeviceInfo( device, CL_DEVICE_IMAGE2D_MAX_WIDTH, sizeof( maxWidth ), &maxWidth, NULL );)"}
{"predict": "(14, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(14, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(112, OpenMP, #pragma omp master); (95, OpenMP, #pragma omp parallel reduction(+ : t0, tInit, ratio, vgh_t, val_t, nspheremoves, dNumVGHCalls)); (152, OpenMP, #pragma omp barrier); (139, OpenMP, #pragma omp nested); (156, OpenMP, myComm->reduce(&val_t, &vgh_t, 1, MPI_DOUBLE, MPI_MAX, 0);); (182, OpenMP, double tFinal = omp_get_wtime();); (111, OpenMP, #pragma omp critical); (22, OpenMP, int np     = omp_get_max_threads();); (94, OpenMP, const int ip = omp_get_thread_num();); (130, OpenMP, #pragma omp private(tt, vgh_t_loc, v_t_loc, my_accepted, my_vals)); (143, OpenMP, #pragma omp master dNumVGHCalls += omp_get_num_threads();); (152, OpenMP, myComm->reduce(&my_vals, &nvals, 1, MPI_INT, MPI_SUM, 0);); (150, OpenMP, myComm->reduce(&my_accepted, &naccepted, 1, MPI_INT, MPI_SUM, 0);); (111, OpenMP, #pragma omp parallel reduction(+ : tt, vgh_t_loc, v_t_loc, my_accepted, my_vals)); (83, OpenMP, #pragma omp parallel); (17, OpenMP, int naccepted = 0;); (108, OpenMP, #pragma omp critical(t0)); (111, OpenMP, t0 += omp_get_wtime();); (169, OpenMP, double tstep = (omp_get_wtime() - t0) / nsteps;); (111, OpenMP, #pragma omp parallel reduction(+ : naccepted)); (172, OpenMP, cout << \" t_init = \" << tInit << \" and t_final = \" << tFinal << \" and t_step = \" << tstep << endl;); (87, OpenMP, #pragma omp master initClock.restart();); (105, OpenMP, #pragma omp critical(tInit)); (164, Open", "label": "(115, OpenMP, ncrews = omp_get_max_threads();); (98, OpenMP, const int np = omp_get_num_threads();); (189, OpenMP, #pragma omp barrier); (233, OpenMP, double dTotalThreads = omp_get_max_threads();); (268, OpenMP, double nMajorThreads  = omp_get_max_threads();); (137, OpenMP, #pragma omp parallel reduction(+ : vgh_t_loc, v_t_loc, my_vals)); (106, OpenMP, #pragma omp master); (278, OpenMP, cout << \"\\nMPI: \" << nmpi << \" and Threads: \" << omp_get_max_threads();); (92, OpenMP, const int ip = omp_get_thread_num();); (86, OpenMP, #pragma omp parallel reduction(+ : t0, tInit, ratio, vgh_t, val_t, nspheremoves, dNumVGHCalls))"}
{"predict": "(49, CUDA, cudaDeviceSynchronize();); (45, CUDA, cudaEventDestroy(start);); (40, CUDA, cudaEventRecord(stop, 0);); (46, CUDA, cudaEventDestroy(stop);); (41, CUDA, cudaEventElapsedTime(&time, start, stop);); (10, CUDA, cudaMalloc((void **)&a_device, N * sizeof(half));); (24, CUDA, cudaDeviceSynchronize();); (40, CUDA, cudaEventSynchronize(stop);); (59, CUDA, cudaEventCreate(&start);); (148, CUDA, cudaFree(a_device);); (56, CUDA, cudaEventCreate(&stop);); (7, CUDA, cudaMalloc((void **)&b_device, N * sizeof(half));); (31, CUDA, cudaEventRecord(start, 0);); (41, CUDA, cudaMemcpy(c_host, c_device, N * sizeof(half), cudaMemcpyDeviceToHost);); (143, CUDA, cudaFree(c_device);); (13, CUDA, cudaMalloc((void **)&c_device, N * sizeof(half));); (140, CUDA, cudaFree(b_device);); (24, CUDA, cudaEventDestroy(stop);); (1, CUDA, cudaMemcpy(a_device, a_host, N * sizeof(half), cudaMemcpyHostToDevice);); (29, CUDA, cudaEventSynchronize(start);); (-3, CUDA, cudaMemcpy(b_device, b_host, N * sizeof(half), cudaMemcpyHostToDevice);)", "label": "(25, CUDA, cudaMalloc((void **)&c_device, N * sizeof(half));); (49, CUDA, cudaEventSynchronize(stop);); (50, CUDA, cudaEventDestroy(start);); (155, CUDA, cudaFree(a_device);); (32, CUDA, cudaDeviceSynchronize();); (155, CUDA, cudaFree(c_device);); (43, CUDA, cudaEventRecord(stop, 0);); (46, CUDA, cudaEventDestroy(stop);); (151, CUDA, cudaFree(b_device);); (33, CUDA, cudaEventRecord(start, 0);); (11, CUDA, cudaMemcpy(b_device, b_host, N * sizeof(half), cudaMemcpyHostToDevice);); (30, CUDA, cudaEventCreate(&stop);); (6, CUDA, cudaMalloc((void **)&b_device, N * sizeof(half));); (-2, CUDA, cudaMalloc((void **)&a_device, N * sizeof(half));); (37, CUDA, cudaEventElapsedTime(&time, start, stop);); (-1, CUDA, cudaMemcpy(a_device, a_host, N * sizeof(half), cudaMemcpyHostToDevice);); (24, CUDA, cudaEventCreate(&start);); (39, CUDA, cudaMemcpy(c_host, c_device, N * sizeof(half), cudaMemcpyDeviceToHost);)"}
{"predict": "(19, MPI, MPI_Init(&argc, &argv);); (25, MPI, MPI_Finalize();); (19, MPI, MPI_Comm_size(MPI_COMM_WORLD, &num_procs);); (17, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myid);)", "label": "(19, MPI, MPI_Init(&argc, &argv);); (25, MPI, MPI_Finalize();); (19, MPI, MPI_Comm_size(MPI_COMM_WORLD, &num_procs);); (17, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myid);)"}
{"predict": "(71, CUDA, CHECK_CUDA_ERROR(cudaDeviceSetCacheConfig(cudaFuncCachePreferShared));); (71, CUDA, CHECK_CUDA_ERROR(cudaDeviceSetCacheConfig(cudaFuncCachePreferL2));); (9, CUDA, CHECK_CUDA_ERROR(cudaRuntimeGetVersion(&runtime_version));); (6, CUDA, CHECK_CUDA_ERROR(cudaDriverGetVersion(&driver_version));); (70, CUDA, CHECK_CUDA_ERROR(cudaDeviceSetCacheConfig(cudaFuncCachePreferTex));); (70, CUDA, CHECK_CUDA_ERROR(cudaDeviceSetCacheConfig(cudaFuncCachePreferL1));); (20, CUDA, CHECK_CUDA_ERROR(cudaGetDeviceProperties(&deviceProp, dev));)", "label": "(10, CUDA, CHECK_CUDA_ERROR(cudaRuntimeGetVersion(&runtime_version));); (72, CUDA, CHECK_CUDA_ERROR(cudaSetDevice(dev));); (26, CUDA, CHECK_CUDA_ERROR(cudaGetDeviceProperties(&deviceProp, dev));); (3, CUDA, CHECK_CUDA_ERROR(cudaDriverGetVersion(&driver_version));); (20, CUDA, CHECK_CUDA_ERROR(cudaGetDeviceProperties(&deviceProp, i));)"}
{"predict": "(163, CUDA, AsStatus status = cudaOp->Execute(tensors);); (192, CUDA, cudaFree(host_data);); (171, CUDA, cudaMemcpyAsync(v_span_data, request_num_spans.data(), batch_size * sizeof(int), cudaMemcpyHostToDevice, cuda_stream);); (187, CUDA, cudaFree(dv_ptr);); (171, CUDA, cudaMalloc(&dv_ptr, sizeof(T) * hidden_size * batch_size);); (155, CUDA, AsStatus status = cudaOp->SetBatchSize(tensors, batch_size);); (162, CUDA, cudaMemcpyAsync(q_data, tensors.at(q_name)->Data(), sizeof(T) * hidden_size * batch_size, cudaMemcpyDeviceToDevice, cuda_stream);); (173, CUDA, return cudaOp->GetOutput(tensors, out_name);); (153, CUDA, cudaMemcpyAsync(k_span_data, tensors.at(k_span_name)->Data(), sizeof(int) * batch_size * max_num_spans, cudaMemcpyDeviceToDevice, cuda_stream);); (160, CUDA, cudaMemcpyAsync(span_pool_data, tensors.at(span_pool_name)->Data(), sizeof(T) * 2 * num_spans * span_bytes, cudaMemcpyDeviceToDevice, cuda_stream);); (173, CUDA, cudaFree(dk_ptr);); (150, CUDA, cudaMalloc(&dk_ptr, sizeof(T) * hidden_size * batch_size);); (116, CUDA, cudaMallocHost(&host_data, sizeof(T) * hidden_size * batch_size);); (161, CUDA, cudaMemcpyAsync(dk_ptr, k_span_data, sizeof(int) * batch_size * max_num_spans, cudaMemcpyDeviceToDevice, cuda_stream);); (167, CUDA, cudaMemcpyAsync(dv_ptr, v_span_data, sizeof(int) * batch_size * max_num_spans, cudaMemcpyDeviceToDevice, cuda_stream);); (123, CUDA, AsKernelPtr cudaOp = GetCudaKernel<SpanAttnV2<T>>(param);); (165, CUDA, cudaMemcpyAsync(dv_ptr, tensors.at(v_span_name)->Data(), sizeof(int) * batch_size * max_num_spans, cudaMemcpyDeviceToDevice, cuda_stream);); (155, CUDA, cudaMemcpyAsync(dk_ptr, request_num_spans.data(), batch_size * sizeof(int), cudaMemcpyHostToDevice, cuda_stream);); (119,", "label": "(90, CUDA, ASSERT_EQ(cudaSuccess, cudaMallocHost(&host_ws, host_ws_bytes));); (297, CUDA, ASSERT_EQ(cudaSuccess, cudaEventDestroy(ref_start));); (230, CUDA, ASSERT_EQ(cudaSuccess, cudaEventElapsedTime(&ref_time, ref_start, ref_end));); (297, CUDA, ASSERT_EQ(cudaSuccess, cudaEventDestroy(out_start));); (297, CUDA, ASSERT_EQ(cudaSuccess, cudaEventDestroy(out_end));); (249, CUDA, ASSERT_EQ(cudaSuccess, cudaEventElapsedTime(&out_time, out_start, out_end));); (201, CUDA, ASSERT_EQ(cudaSuccess, cudaEventCreate(&ref_end));); (292, CUDA, ASSERT_EQ(cudaSuccess, cudaEventDestroy(ref_end));); (244, CUDA, ASSERT_EQ(cudaSuccess, cudaEventRecord(out_end, cuda_stream));); (199, CUDA, ASSERT_EQ(cudaSuccess, cudaEventCreate(&out_start));); (199, CUDA, ASSERT_EQ(cudaSuccess, cudaEventCreate(&out_end));); (220, CUDA, ASSERT_EQ(cudaSuccess, cudaEventSynchronize(ref_end));); (241, CUDA, ASSERT_EQ(cudaSuccess, cudaEventSynchronize(out_end));); (217, CUDA, ASSERT_EQ(cudaSuccess, cudaEventRecord(ref_end, cuda_stream));); (192, CUDA, ASSERT_EQ(cudaSuccess, cudaEventCreate(&ref_start));)"}
{"predict": "(18, CUDA, __shared__ uint32_t sharedMemory[1024];); (39, CUDA, __shared__ uint32_t shuffleMem[1024];)", "label": "(18, CUDA, __shared__ uint32_t sharedMemory[1024];); (29, CUDA, extern __shared__ uint32_t shuffleMem[];)"}
{"predict": "(90, CUDA, CUDA(cudaAnimatePoints(points, make_float2(0.0f, 0.0f), 0.017f));)", "label": "(89, CUDA, CUDA(cudaGeneratePointGrid(points, GRID_N, GRID_WORLD_SIZE, apptime()));); (89, CUDA, CUDA(cudaDeviceSynchronize());)"}
{"predict": "(146, OpenCL, error = clSetKernelArg(kernel, 1, sizeof(tinfo->inBuf), &tinfo->inBuf);)", "label": "(146, OpenCL, error = clSetKernelArg(kernel, 1, sizeof(tinfo->inBuf), &tinfo->inBuf);)"}
{"predict": "(73, CUDA, thrust::host_vector<int> H(4);); (16, CUDA, thrust_array<double, 1> thrust_array = get_thrust_array<Alloc>(100, 11.);); (107, CUDA, thrust::device_vector<int> D = H;); (71, CUDA, thrust::host_vector<int> H2(2);)", "label": "(15, CUDA, multi::array<double, 1, thrust::device_allocator<double>> BB(10, 99.);); (34, CUDA, thrust::copy(begin(rotated(A2)[0]), end(rotated(A2)[0]), begin(rotated(B2)[0]));); (16, CUDA, multi::array<std::complex<double>, 1, thrust::device_allocator<std::complex<double> >> BBB_cpy = BBB;); (14, CUDA, multi::array<std::complex<double>, 1, thrust::device_allocator<std::complex<double> >> BBB(10, 99.);); (97, CUDA, thrust::device_vector<int> D = H;); (78, CUDA, multi::array<int, 1, thrust::device_allocator<int>> H2(4, 99); assert(size(H2) == 4);); (69, CUDA, thrust::host_vector<int> H(4);); (21, CUDA, thrust::copy(begin(B), end(B), begin(C));)"}
{"predict": "(28, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_IMAGE_MAX_ARRAY_SIZE, sizeof( maxArraySize ), &maxArraySize, NULL );); (26, OpenCL, int error = clGetDeviceInfo( device, CL_DEVICE_IMAGE2D_MAX_WIDTH, sizeof( maxWidth ), &maxWidth, NULL );); (28, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_GLOBAL_MEM_SIZE, sizeof( memSize ), &memSize, NULL );); (26, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_MAX_MEM_ALLOC_SIZE, sizeof( maxAllocSize ), &maxAllocSize, NULL );); (23, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_IMAGE2D_MAX_HEIGHT, sizeof( maxHeight ), &maxHeight, NULL );)", "label": "(28, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_IMAGE_MAX_ARRAY_SIZE, sizeof( maxArraySize ), &maxArraySize, NULL );); (25, OpenCL, int error = clGetDeviceInfo( device, CL_DEVICE_IMAGE2D_MAX_WIDTH, sizeof( maxWidth ), &maxWidth, NULL );); (28, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_GLOBAL_MEM_SIZE, sizeof( memSize ), &memSize, NULL );); (26, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_MAX_MEM_ALLOC_SIZE, sizeof( maxAllocSize ), &maxAllocSize, NULL );); (23, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_IMAGE2D_MAX_HEIGHT, sizeof( maxHeight ), &maxHeight, NULL );)"}
{"predict": "(4, OpenMP, int nt = omp_get_max_threads();); (35, OpenMP, #pragma omp barrier); (25, OpenMP, #pragma omp parallel); (6, OpenMP, std::cout << \"num iterations = \" << iterations << \"\\n\";); (35, OpenMP, #pragma omp critical); (-1, OpenMP, int me = omp_get_thread_num();)", "label": "(33, OpenMP, #pragma omp barrier); (28, OpenMP, int me    = omp_get_thread_num();); (25, OpenMP, #pragma omp parallel reduction(min:dtmin) reduction(max:dtmax) reduction(+:dtavg)); (-1, OpenMP, int nt = omp_get_max_threads();); (35, OpenMP, #pragma omp critical)"}
{"predict": "(45, OpenMP, #pragma omp distribute parallel for simd dist_schedule (static,); (78, OpenMP, #pragma omp distribute parallel for simd dist_schedule (static), dist_schedule (static, 0)); (66, OpenMP, #pragma omp distribute parallel for simd dist_schedule (argc))); (10, OpenMP, #pragma omp distribute parallel for simd dist_schedule); (29, OpenMP, #pragma omp distribute parallel for simd dist_schedule ()); (46, OpenMP, #pragma omp distribute parallel for simd dist_schedule (static); (100, OpenMP, #pragma omp distribute parallel for simd dist_schedule (static, z)); (104, OpenMP, #pragma omp distribute parallel for simd dist_schedule (static, fl)); (14, OpenMP, #pragma omp distribute parallel for simd dist_schedule (); (65, OpenMP, #pragma omp distribute parallel for simd dist_schedule (static, S1)); (91, OpenMP, #pragma omp distribute parallel for simd dist_schedule (static, r)); (95, OpenMP, #pragma omp distribute parallel for simd dist_schedule (static, qa[i])); (24, OpenMP, #pragma omp distribute parallel for simd dist_schedule (static); (39, OpenMP, #pragma omp distribute parallel for simd dist_schedule (static,); (6, OpenMP, #pragma omp target); (68, OpenMP, #pragma omp distribute parallel for simd dist_schedule (static, da[i])); (96, OpenMP, #pragma omp distribute parallel for simd dist_schedule (static, p)); (16, OpenMP, #pragma omp distribute parallel for simd dist_schedule (static,); (103, OpenMP, #pragma omp distribute parallel for simd dist_schedule (static, fl)); (26, OpenMP, #pragma omp distribute parallel for simd dist_schedule (static, argc > 0 ? argv[1] : argv[2])); (-3, OpenMP, #pragma omp teams)", "label": "(109, OpenMP, #pragma omp distribute parallel for reduction(^ : fl)); (154, OpenMP, #pragma omp distribute parallel for reduction(+ : r)); (117, OpenMP, #pragma omp distribute parallel for reduction(&& : S2::S2sc)); (126, OpenMP, #pragma omp distribute parallel for reduction(+ : h, k, B::x)); (164, OpenMP, #pragma omp distribute parallel for reduction(+ : fl, z)); (39, OpenMP, #pragma omp distribute parallel for reduction(\\)); (6, OpenMP, #pragma omp target); (87, OpenMP, #pragma omp distribute parallel for reduction(+ : ba)); (81, OpenMP, #pragma omp distribute parallel for reduction(max : h.b)); (60, OpenMP, #pragma omp distribute parallel for reduction(&& : argc)); (161, OpenMP, #pragma omp parallel reduction(* : fl)); (28, OpenMP, #pragma omp distribute parallel for reduction(*)); (102, OpenMP, #pragma omp distribute parallel for reduction(&& : S2::S2s)); (6, OpenMP, #pragma omp distribute parallel for reduction +); (85, OpenMP, #pragma omp distribute parallel for reduction(* : ca)); (39, OpenMP, #pragma omp distribute parallel for reduction(| : argc,); (118, OpenMP, #pragma omp distribute parallel for reduction(+ : o)); (125, OpenMP, #pragma omp parallel private(k)); (6, OpenMP, #pragma omp distribute parallel for reduction(); (143, OpenMP, #pragma omp distribute parallel for reduction(max : j)); (44, OpenMP, #pragma omp distribute parallel for reduction(~ : argc)); (138, OpenMP, #pragma omp parallel reduction(min : i)); (143, OpenMP, #pragma omp parallel private(fl)); (26, OpenMP, #pragma omp distribute parallel for reduction(foo : argc); (50, OpenMP, #pragma omp distribute parallel for reduction(^ : S1)); (34, OpenMP, #pragma omp distribute parallel for reduction(|| : argc > 0 ? argv[1] : argv[2])); (-12, OpenMP, #pragma omp distribute parallel for reduction); (153, OpenMP, #pragma omp distribute parallel for reduction(+ : m)); (76, OpenMP, #pragma omp distribute parallel for reduction(- : da)); (95, OpenMP, #pragma omp distribute parallel for reduction(& : e, g)); (144, OpenMP, #pragma omp distribute parallel for reduction(+ : fl)); (-2, OpenMP, #pragma omp distribute parallel for reduction(-); (47, OpenMP, #pragma omp distribute parallel for reduction(+ : a, b, c, d, f)); (112, OpenMP, #pragma omp distribute parallel for reduction(+ : p), reduction(+ : p)); (151, OpenMP, #pragma omp distribute parallel for reduction(task, + : m)); (49, OpenMP, #pragma omp distribute parallel for reduction(min : a, b, c, d, f)); (122, OpenMP, #pragma omp parallel shared(i)); (102, OpenMP, #pragma omp distribute parallel for private(i), reduction(+ : j), reduction(+ : q)); (-4, OpenMP, #pragma omp distribute parallel for reduction()); (-26, OpenMP, #pragma omp teams)"}
{"predict": "(8, OpenMP, #pragma omp parallel for)", "label": "(8, OpenMP, #pragma omp parallel for)"}
{"predict": "(58, OpenMP, #pragma omp target parallel for map(alloc : c)); (37, OpenMP, #pragma omp target parallel for map(tofrom : a) map(alloc : b)); (46, OpenMP, #pragma omp target exit data map(from : a)); (17, OpenMP, #pragma omp enter data map(alloc : a)); (7, OpenMP, int c[X];); (59, OpenMP, #pragma omp update target map(alloc : c))", "label": "(54, OpenMP, #pragma omp target parallel for map(alloc : c)); (64, OpenMP, #pragma omp barrier); (44, OpenMP, #pragma omp target exit data map(from : a)); (61, OpenMP, #pragma omp target update from(c) nowait); (21, OpenMP, #pragma omp target enter data map(to : a)); (30, OpenMP, #pragma omp target parallel for)"}
{"predict": "(161, MPI, MPI_Bcast(&log2nstarts,      1, MPI_INT,           root, MPI_COMM_WORLD);); (46, MPI, MPI_Comm_size(MPI_COMM_WORLD,&Num_procs);); (154, MPI, MPI_Bcast(&log2nproc,        1, MPI_INT,           root, MPI_COMM_WORLD);); (302, MPI, MPI_Finalize();); (153, MPI, MPI_Bcast(&log2tablesize,    1, MPI_INT,           root, MPI_COMM_WORLD);); (154, MPI, MPI_Bcast(&log2update_ratio, 1, MPI_INT,           root, MPI_COMM_WORLD);); (42, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&my_ID);); (158, MPI, MPI_Bcast(&nupdate,          1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (152, MPI, MPI_Bcast(&nstarts,          1, MPI_INT,           root, MPI_COMM_WORLD);); (153, MPI, MPI_Bcast(&tablesize,        1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (154, MPI, MPI_Bcast(&tablespace,       1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (152, MPI, MPI_Bcast(&loctablesize,     1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (212, MPI, MPI_Barrier(MPI_COMM_WORLD);); (145, MPI, MPI_Bcast(&update_ratio,     1, MPI_INT,           root, MPI_COMM_WORLD);); (279, MPI, MPI_Reduce(&error, &tot_error, 1, MPI_LONG_LONG_INT, MPI_SUM, root, MPI_COMM_WORLD);)", "label": "(162, MPI, MPI_Bcast(&log2nstarts,      1, MPI_INT,           root, MPI_COMM_WORLD);); (46, MPI, MPI_Comm_size(MPI_COMM_WORLD,&Num_procs);); (155, MPI, MPI_Bcast(&log2nproc,        1, MPI_INT,           root, MPI_COMM_WORLD);); (303, MPI, MPI_Finalize();); (154, MPI, MPI_Bcast(&log2tablesize,    1, MPI_INT,           root, MPI_COMM_WORLD);); (155, MPI, MPI_Bcast(&log2update_ratio, 1, MPI_INT,           root, MPI_COMM_WORLD);); (42, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&my_ID);); (159, MPI, MPI_Bcast(&nupdate,          1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (153, MPI, MPI_Bcast(&nstarts,          1, MPI_INT,           root, MPI_COMM_WORLD);); (154, MPI, MPI_Bcast(&tablesize,        1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (155, MPI, MPI_Bcast(&tablespace,       1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (153, MPI, MPI_Bcast(&loctablesize,     1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (213, MPI, MPI_Barrier(MPI_COMM_WORLD);); (146, MPI, MPI_Bcast(&update_ratio,     1, MPI_INT,           root, MPI_COMM_WORLD);); (280, MPI, MPI_Reduce(&error, &tot_error, 1, MPI_LONG_LONG_INT, MPI_SUM, root, MPI_COMM_WORLD);)"}
{"predict": "(58, MPI, MPI_Type_free(&dtcmp_type_3uint64t);); (64, MPI, MPI_Comm_free(&dtcmp_comm_self);); (48, MPI, MPI_Op_free(&dtcmp_reduceop_mms_3uint64t);); (42, MPI, MPI_Op_free(&dtcmp_reduceop_randroot);)", "label": "(45, MPI, MPI_Op_free(&dtcmp_reduceop_randroot);); (49, MPI, MPI_Op_free(&dtcmp_reduceop_mms_3uint64t);); (64, MPI, MPI_Comm_free(&dtcmp_comm_self);); (52, MPI, MPI_Type_free(&dtcmp_type_3int);); (56, MPI, MPI_Type_free(&dtcmp_type_3uint64t);)"}
{"predict": "(67, OpenMP, __kmpc_omp_task(&loc, gtid, task);); (104, OpenMP, __kmpc_omp_wait_deps(&loc, gtid, ndeps, dep_list, 0, NULL);); (77, OpenMP, __kmpc_omp_task(&loc, gtid, task);); (73, OpenMP, kmp_gomp_depends_info_t gomp_depends(depend);); (105, OpenMP, __kmpc_omp_complete_deps(&loc, gtid, ndeps_cnv, dep_list);)", "label": "(70, OpenMP, kmp_gomp_depends_info_t gomp_depends(depend);); (103, OpenMP, __kmpc_omp_wait_deps(&loc, gtid, ndeps, dep_list, 0, NULL);); (77, OpenMP, __kmpc_omp_task(&loc, gtid, task);); (74, OpenMP, __kmpc_omp_task_with_deps(&loc, gtid, task, ndeps_cnv, dep_list, 0, NULL);)"}
{"predict": "(31, OpenMP, #pragma omp target teams distribute collapse(4); (65, OpenMP, #pragma omp target teams distribute collapse(4, 8)); (39, OpenMP, #pragma omp target teams distribute collapse(4, )); (6, OpenMP, #pragma omp target teams distribute collapse(); (82, OpenMP, #pragma omp target teams distribute collapse(2) firstprivate(i)); (86, OpenMP, #pragma omp target teams distribute collapse(5 - 5)); (72, OpenMP, #pragma omp target teams distribute collapse(-5)); (39, OpenMP, #pragma omp target teams distribute collapse(4)); (75, OpenMP, #pragma omp target teams distribute collapse(0)); (14, OpenMP, #pragma omp target teams distribute collapse(, )); (49, OpenMP, #pragma omp target teams distribute collapse(4, , 4)); (20, OpenMP, #pragma omp target teams distribute collapse(4,); (59, OpenMP, #pragma omp target teams distribute collapse(2.5)); (6, OpenMP, #pragma omp target teams distribute collapse(,); (-9, OpenMP, #pragma omp target teams distribute collapse); (60, OpenMP, #pragma omp target teams distribute collapse(foo())); (28, OpenMP, #pragma omp target teams distribute collapse(4 4)); (12, OpenMP, #pragma omp target teams distribute collapse(4); (41, OpenMP, #pragma omp target teams distribute collapse(4, 4)); (-9, OpenMP, #pragma omp target teams distribute collapse()", "label": "(41, OpenMP, #pragma omp target teams distribute collapse(4, )); (90, OpenMP, #pragma omp target teams distribute collapse(2) firstprivate(i)); (15, OpenMP, #pragma omp target teams distribute collapse(,); (33, OpenMP, #pragma omp target teams distribute collapse(4,); (74, OpenMP, #pragma omp target teams distribute collapse(-5)); (8, OpenMP, #pragma omp target teams distribute collapse()); (15, OpenMP, #pragma omp target teams distribute collapse(, )); (79, OpenMP, #pragma omp target teams distribute collapse(5 - 5)); (66, OpenMP, #pragma omp target teams distribute collapse(foo())); (73, OpenMP, #pragma omp target teams distribute collapse(0)); (-1, OpenMP, #pragma omp target teams distribute collapse(); (55, OpenMP, #pragma omp target teams distribute collapse(4, 8)); (38, OpenMP, #pragma omp target teams distribute collapse(4 4)); (32, OpenMP, #pragma omp target teams distribute collapse(4)); (-9, OpenMP, #pragma omp target teams distribute collapse); (40, OpenMP, #pragma omp target teams distribute collapse(4, , 4)); (15, OpenMP, #pragma omp target teams distribute collapse(4); (9, OpenMP, #pragma omp target teams distribute collapse 4)); (52, OpenMP, #pragma omp target teams distribute collapse(2.5))"}
{"predict": "(71, CUDA, cudaMemcpy(&((int *)cuda_mem)[j*size], ptrs[i], size * Mtypes(i), cudaMemcpyHostToDevice);); (65, CUDA, cudaMalloc(&cuda_mem, size * Mtypes(i) * maxGPT);); (67, CUDA, cudaMemcpy(cuda_mem, ptrs[i], size * Mtypes(i), cudaMemcpyHostToDevice);); (63, CUDA, cudaMalloc(&cuda_mem, size * Mtypes(i));)", "label": "(71, CUDA, cudaMemcpy(&((int *)cuda_mem)[j*size], ptrs[i], size * Mtypes(i), cudaMemcpyHostToDevice);); (68, CUDA, cudaMalloc(&cuda_mem, size * Mtypes(i) * maxGPT);); (65, CUDA, cudaMemcpy(cuda_mem, ptrs[i], size * Mtypes(i), cudaMemcpyHostToDevice);); (63, CUDA, cudaMalloc(&cuda_mem, size * Mtypes(i));)"}
{"predict": "(94, OpenCL, error = clEnqueueReadBuffer( queueB, mainBuffer, CL_TRUE, 0, mainSize, actualResults, 0, NULL, NULL );); (22, OpenCL, mainBuffer = clCreateBuffer( context, CL_MEM_READ_WRITE | CL_MEM_COPY_HOST_PTR, mainSize, mainBufferContents, &error );)", "label": "(23, OpenCL, mainBuffer = clCreateBuffer( context, CL_MEM_READ_WRITE | CL_MEM_COPY_HOST_PTR, mainSize, mainBufferContents, &error );); (93, OpenCL, error = clEnqueueReadBuffer( queueA, mainBuffer, CL_TRUE, 0, mainSize, actualResults, 0, NULL, NULL );)"}
{"predict": "(68, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (65, MPI, MPI_Finalized(&mpi_finalized);); (63, MPI, MPI_Initialized(&mpi_initialized);)", "label": "(69, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (65, MPI, MPI_Finalized(&mpi_finalized);); (63, MPI, MPI_Initialized(&mpi_initialized);)"}
{"predict": "(167, MPI, if ( SendCounts[p] ) MPI_Wait(&send_req[p], &status);); (167, MPI, if ( RecvCounts[p] ) MPI_Wait(&recv_req[p], &status);)", "label": "(166, MPI, if ( SendCounts[p] ) MPI_Wait(&send_req[p], &status);); (166, MPI, if ( RecvCounts[p] ) MPI_Wait(&recv_req[p], &status);)"}
{"predict": "(19, OpenMP, #pragma omp target simd simdlen(, )); (31, OpenMP, #pragma omp target simd simdlen(4,); (55, OpenMP, #pragma omp target simd simdlen(4, 8)); (77, OpenMP, #pragma omp target simd simdlen(5 - 5)); (-1, OpenMP, #pragma omp target simd simdlen); (10, OpenMP, #pragma omp target simd simdlen(,); (1, OpenMP, #pragma omp target simd simdlen(); (68, OpenMP, #pragma omp target simd simdlen(0)); (3, OpenMP, #pragma omp target simd simdlen()); (30, OpenMP, #pragma omp target simd simdlen(4, )); (49, OpenMP, #pragma omp target simd simdlen(2.5)); (61, OpenMP, #pragma omp target simd simdlen(foo())); (32, OpenMP, #pragma omp target simd simdlen(4)); (52, OpenMP, #pragma omp target simd simdlen(4, 0)); (2, OpenMP, #pragma omp target simd simdlen(, )); (33, OpenMP, #pragma omp target simd simdlen(4 4)); (63, OpenMP, #pragma omp target simd simdlen(-5)); (41, OpenMP, #pragma omp target simd simdlen(4, , 4))", "label": "(3, OpenMP, #pragma omp target simd simdlen); (18, OpenMP, #pragma omp target simd simdlen(, )); (70, OpenMP, #pragma omp target simd simdlen(-5)); (31, OpenMP, #pragma omp target simd simdlen(4,); (72, OpenMP, #pragma omp target simd simdlen(0)); (2, OpenMP, #pragma omp target simd simdlen(); (46, OpenMP, #pragma omp target simd simdlen(4, , 4)); (61, OpenMP, #pragma omp target simd simdlen(foo())); (3, OpenMP, #pragma omp target simd simdlen()); (55, OpenMP, #pragma omp target simd simdlen(2.5)); (5, OpenMP, #pragma omp target simd simdlen(,); (31, OpenMP, #pragma omp target simd simdlen(4)); (68, OpenMP, #pragma omp target simd simdlen(5 - 5)); (26, OpenMP, #pragma omp target simd simdlen(4, )); (33, OpenMP, #pragma omp target simd simdlen(4 4)); (45, OpenMP, #pragma omp target simd simdlen(4, 8)); (13, OpenMP, #pragma omp target simd simdlen(4); (7, OpenMP, #pragma omp target simd simdlen 4))"}
{"predict": "(150, CUDA, checkCudaErrors(cudaFree(d_StockPrice));); (147, CUDA, checkCudaErrors(cudaFree(d_OptionYears));); (152, CUDA, checkCudaErrors(cudaFree(d_PutResult));); (45, CUDA, checkCudaErrors(cudaMalloc((void **)&h_OptionYears,   OPT_SZ));); (66, CUDA, checkCudaErrors(cudaMemcpy(d_StockPrice,   h_StockPrice,    OPT_SZ, cudaMemcpyHostToDevice));); (145, CUDA, checkCudaErrors(cudaFree(d_CallResult));); (40, CUDA, checkCudaErrors(cudaMalloc((void **)&h_PutResultCPU,  OPT_SZ));); (102, CUDA, checkCudaErrors(cudaMemcpy(h_CallResultGPU, d_CallResult, OPT_SZ, cudaMemcpyDeviceToHost));); (85, CUDA, getLastCudaError(\"BlackScholesGPU() execution failed\\n\");); (62, CUDA, checkCudaErrors(cudaMemcpy(d_OptionYears,  h_OptionYears,   OPT_SZ, cudaMemcpyHostToDevice));); (35, CUDA, checkCudaErrors(cudaMalloc((void **)&h_StockPrice,    OPT_SZ));); (35, CUDA, checkCudaErrors(cudaMalloc((void **)&h_OptionStrike,  OPT_SZ));); (57, CUDA, checkCudaErrors(cudaMemcpy(d_OptionStrike, h_OptionStrike,  OPT_SZ, cudaMemcpyHostToDevice));); (96, CUDA, checkCudaErrors(cudaMemcpy(h_PutResultGPU,  d_PutResult,   OPT_SZ, cudaMemcpyDeviceToHost));); (48, CUDA, checkCudaErrors(cudaMemcpy(d_CallResult,   h_CallResultCPU, OPT_SZ, cudaMemcpyHostToDevice));); (139, CUDA, checkCudaErrors(cudaFree(d_OptionStrike));); (37, CUDA, checkCudaErrors(cudaMalloc((void **)&h_PutResult,   OPT_SZ));); (135, CUDA, checkCudaErrors(cudaFree(h_OptionYears));); (33, CUDA, checkCudaErrors(cudaMalloc((void **)&h_CallResult,  OPT_SZ));); (50, CUDA, checkCudaErrors(cudaMemcpy(d_StockPrice,   h_StockPrice,    OPT_SZ, cudaMemcpyHostToDevice));); (137,", "label": "(49, CUDA, checkCudaErrors(cudaMalloc((void **)&d_CallResult,   OPT_SZ));); (155, CUDA, checkCudaErrors(cudaFree(d_OptionYears));); (155, CUDA, checkCudaErrors(cudaFree(d_OptionStrike));); (155, CUDA, checkCudaErrors(cudaFree(d_StockPrice));); (155, CUDA, checkCudaErrors(cudaFree(d_PutResult));); (66, CUDA, checkCudaErrors(cudaMemcpy(d_OptionStrike, h_OptionStrike,  OPT_SZ, cudaMemcpyHostToDevice));); (45, CUDA, checkCudaErrors(cudaMalloc((void **)&d_StockPrice,   OPT_SZ));); (43, CUDA, checkCudaErrors(cudaMalloc((void **)&d_PutResult,    OPT_SZ));); (26, CUDA, findCudaDevice(argc, (const char **)argv);); (68, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (42, CUDA, checkCudaErrors(cudaMalloc((void **)&d_OptionStrike, OPT_SZ));); (149, CUDA, checkCudaErrors(cudaFree(d_CallResult));); (81, CUDA, getLastCudaError(\"BlackScholesGPU() execution failed\\n\");); (98, CUDA, checkCudaErrors(cudaMemcpy(h_CallResultGPU, d_CallResult, OPT_SZ, cudaMemcpyDeviceToHost));); (58, CUDA, checkCudaErrors(cudaMemcpy(d_OptionYears,  h_OptionYears,   OPT_SZ, cudaMemcpyHostToDevice));); (97, CUDA, checkCudaErrors(cudaMemcpy(h_PutResultGPU,  d_PutResult,  OPT_SZ, cudaMemcpyDeviceToHost));); (37, CUDA, checkCudaErrors(cudaMalloc((void **)&d_OptionYears,  OPT_SZ));); (53, CUDA, checkCudaErrors(cudaMemcpy(d_StockPrice,  h_StockPrice,   OPT_SZ, cudaMemcpyHostToDevice));)"}
{"predict": "(29, MPI, MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);); (51, MPI, errcode = MPI_File_set_view(fh, 0, MPI_INT, MPI_INT, \"native\", MPI_INFO_NULL);); (8, MPI, MPI_Init(&argc, &argv);); (111, MPI, MPI_Finalize();); (26, MPI, MPI_Bcast(filename, len + 10, MPI_CHAR, 0, MPI_COMM_WORLD);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (95, MPI, MPI_Allreduce(&errs, &toterrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (50, MPI, errcode = MPI_File_iwrite(fh, buf, nints, MPI_INT, &request);); (74, MPI, errcode = MPI_File_iread(fh, buf, nints, MPI_INT, &request);); (3, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(29, MPI, MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);); (51, MPI, errcode = MPI_File_set_view(fh, 0, MPI_INT, MPI_INT, \"native\", MPI_INFO_NULL);); (8, MPI, MPI_Init(&argc, &argv);); (59, MPI, MPI_Wait(&request, &status);); (112, MPI, MPI_Finalize();); (25, MPI, MPI_Bcast(filename, len + 10, MPI_CHAR, 0, MPI_COMM_WORLD);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (96, MPI, MPI_Allreduce(&errs, &toterrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (74, MPI, errcode = MPI_File_iread(fh, buf, nints, MPI_INT, &request);); (14, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (47, MPI, errcode = MPI_File_iwrite(fh, buf, nints, MPI_INT, &request);)"}
{"predict": "(31, MPI, MPI_Accumulate(local_b, COUNT, MPI_INT, 0, 0, COUNT, MPI_INT, MPI_SUM, win[id]);); (13, MPI, MPI_Accumulate(MPI_IN_PLACE, COUNT, MPI_INT, 1, 0, COUNT, MPI_INT, MPI_SUM, win[id]);); (6, MPI, MPI_Alloc_mem(COUNT * sizeof(int), MPI_INFO_NULL, &local_b);); (13, MPI, MPI_Win_fence(0, win[id]);); (42, MPI, MPI_Free_mem(local_b);)", "label": "(30, MPI, MPI_Get(local_b, COUNT, MPI_INT, 0, 0, COUNT, MPI_INT, win[id]);); (11, MPI, MPI_Win_fence(0, win[id]);); (43, MPI, MPI_Free_mem(local_b);); (11, MPI, MPI_Put(local_b, COUNT, MPI_INT, 0, 0, COUNT, MPI_INT, win[id]);); (4, MPI, MPI_Alloc_mem(COUNT * sizeof(int), MPI_INFO_NULL, &local_b);)"}
{"predict": "(62, MPI, MPI_Barrier(MPI_COMM_WORLD);); (46, MPI, MPI_Type_create_struct(1, &len, &disp, &typevec, &tmptype);); (9, MPI, MPI_Init(&argc, &argv);); (115, MPI, MPI_CHECK(MPI_File_close(&fh));); (135, MPI, MPI_CHECK(MPI_File_write_all(fh, buf, SIZE, MPI_INT, &status));); (45, MPI, MPI_Type_free(&typevec);); (58, MPI, MPI_Error_class(err, &errorclass);); (130, MPI, MPI_CHECK(MPI_File_read_all(fh, buf, SIZE, MPI_INT, &status));); (8, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (32, MPI, MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);); (16, MPI, MPI_Finalize();); (28, MPI, MPI_Bcast(filename, len + 1, MPI_CHAR, 0, MPI_COMM_WORLD);); (44, MPI, MPI_Type_commit(&newtype);); (64, MPI, MPI_CHECK(MPI_File_set_view(fh, 0, MPI_INT, newtype, \"native\", MPI_INFO_NULL));); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mynod);); (102, MPI, MPI_CHECK(MPI_File_write_all(fh, buf, SIZE, MPI_INT, &status));); (38, MPI, MPI_Type_free(&tmptype);); (-2, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (72, MPI, MPI_CHECK(MPI_File_read_all(fh, buf, SIZE, MPI_INT, &status));); (38, MPI, MPI_Type_create_resized(tmptype, 0, extent, &newtype);); (98, MPI, MPI_CHECK(MPI_File_close(&fh));); (113, MPI, MPI_CHECK(MPI_File_set_view(fh, 0, MPI_INT, MPI_INT, \"native\", MPI_INFO_NULL));); (130, MPI, MPI_Allreduce(&errs, &toterrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (41, MPI, MPI_Type_free(&newtype);); (26, MPI, MPI_Type_vector(SIZE / 2,", "label": "(197, MPI, MPI_CHECK(MPI_File_write_all(fh, buf, SIZE, MPI_INT, &status));); (9, MPI, MPI_Init(&argc, &argv);); (80, MPI, MPI_CHECK(MPI_File_write_all(fh, buf, 1, newtype, &status));); (42, MPI, MPI_Type_vector(SIZE / 2, 1, 2, MPI_INT, &typevec);); (49, MPI, MPI_Type_create_struct(1, &len, &disp, &typevec, &tmptype);); (73, MPI, MPI_CHECK(MPI_File_set_view(fh, 0, MPI_INT, newtype, \"native\", MPI_INFO_NULL));); (67, MPI, MPI_Barrier(MPI_COMM_WORLD);); (107, MPI, MPI_CHECK(MPI_File_close(&fh));); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mynod);); (48, MPI, MPI_Type_free(&tmptype);); (136, MPI, MPI_CHECK(MPI_File_read_at_all(fh, mynod * (SIZE / 2) * sizeof(int), buf, 1, newtype, &status));); (5, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (192, MPI, MPI_CHECK(MPI_File_read_at_all(fh, 0, buf, SIZE, MPI_INT, &status));); (22, MPI, MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);); (54, MPI, MPI_Error_class(err, &errorclass);); (-4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (42, MPI, MPI_Type_commit(&newtype);); (72, MPI, MPI_CHECK(MPI_File_read_at_all(fh, 0, buf, 1, newtype, &status));); (36, MPI, MPI_Type_free(&typevec);); (37, MPI, MPI_Type_create_resized(tmptype, 0, extent, &newtype);); (220, MPI, MPI_Finalize();); (15, MPI, MPI_Bcast(filename, len + 1, MPI_CHAR, 0, MPI_COMM_WORLD);); (206, MPI, MPI_Allreduce(&errs, &toterrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (214, MPI, MPI_Type_free(&newtype);)"}
{"predict": "(199, OpenMP, #pragma omp barrier); (147, OpenMP, nthread = omp_get_num_threads();); (183, OpenMP, #pragma omp for); (142, OpenMP, #pragma omp master); (54, OpenMP, omp_set_num_threads(nthread_input);); (137, OpenMP, #pragma omp parallel private(i, j, ran, round, index, my_ID) reduction(+:error)); (214, OpenMP, #pragma omp atomic); (144, OpenMP, #pragma omp parallel for private(i, j, ran, round, index) reduction(+:error)); (133, OpenMP, my_ID   = omp_get_thread_num();)", "label": "(202, OpenMP, #pragma omp barrier); (150, OpenMP, nthread = omp_get_num_threads();); (197, OpenMP, #pragma omp for); (146, OpenMP, #pragma omp master); (54, OpenMP, omp_set_num_threads(nthread_input);); (137, OpenMP, #pragma omp parallel private(i, j, ran, round, index, my_ID) reduction(+:error)); (223, OpenMP, #pragma omp atomic); (140, OpenMP, my_ID   = omp_get_thread_num();)"}
{"predict": "(171, MPI, MPI_Type_free(&dt_key);); (171, MPI, MPI_Type_free(&dt_keysat);); (116, MPI, MPI_Type_get_extent(dt_sat, &sat_lb, &sat_extent);); (105, MPI, MPI_Type_get_extent(dt_key, &key_lb, &key_extent);); (20, MPI, MPI_Type_commit(&dt_filepath);); (145, MPI, MPI_Type_free(&dt_filepath);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (17, MPI, MPI_Type_contiguous((int)chars, MPI_CHAR, &dt_filepath);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (162, MPI, MPI_Type_free(&dt_sat);); (101, MPI, MPI_Type_get_extent(dt_keysat, &keysat_lb, &keysat_extent);); (7, MPI, MPI_Type_contiguous((int)bytes, MPI_BYTE, &dt_sat);)", "label": "(106, MPI, MPI_Type_get_extent(dt_key, &key_lb, &key_extent);); (174, MPI, MPI_Type_free(&dt_key);); (16, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (20, MPI, MPI_Type_commit(&dt_filepath);); (169, MPI, MPI_Type_free(&dt_keysat);); (170, MPI, MPI_Type_free(&dt_filepath);); (107, MPI, MPI_Type_get_extent(dt_sat, &sat_lb, &sat_extent);); (102, MPI, MPI_Type_get_extent(dt_keysat, &keysat_lb, &keysat_extent);); (5, MPI, MPI_Type_contiguous((int)bytes, MPI_BYTE, &dt_sat);); (13, MPI, MPI_Type_contiguous((int)chars, MPI_CHAR, &dt_filepath);); (171, MPI, MPI_Type_free(&dt_sat);)"}
{"predict": "(61, MPI, MPI_Type_size(type, &type_size);); (16, MPI, MPI_Comm_set_errhandler(MSG_COMM_WORLD, MPI_ERRORS_RETURN);); (10, MPI, MPI_Comm_rank(MSG_COMM_WORLD, &rank);); (10, MPI, MPI_Comm_size(MSG_COMM_WORLD, &size);); (2, MPI, MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided);); (129, MPI, MPI_Finalize();); (51, MPI, MPI_Barrier(MSG_COMM_WORLD);); (119, MPI, MPI_Op_free(&op);); (56, MPI, MPI_Type_free(&type);)", "label": "(9, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &MSG_COMM_WORLD);); (4, MPI, MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided);); (63, MPI, MPI_Type_size(type, &type_size);); (16, MPI, MPI_Barrier(MSG_COMM_WORLD);); (130, MPI, MPI_Comm_free(&MSG_COMM_WORLD);); (131, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_rank(MSG_COMM_WORLD, &rank);); (5, MPI, MPI_Comm_size(MSG_COMM_WORLD, &size);)"}
{"predict": "(91, OpenMP, double tic = omp_get_wtime();); (108, OpenMP, #pragma omp target exit data map(from: u[0:n*n])); (116, OpenMP, double stop = omp_get_wtime();); (103, OpenMP, double toc = omp_get_wtime();); (5, OpenMP, double start = omp_get_wtime();); (79, OpenMP, #pragma omp target enter data map(to: u[0:n*n], u_tmp[0:n*n]))", "label": "(92, OpenMP, double tic = omp_get_wtime();); (108, OpenMP, #pragma omp target exit data map(from: u[0:n*n])); (116, OpenMP, double stop = omp_get_wtime();); (103, OpenMP, double toc = omp_get_wtime();); (6, OpenMP, double start = omp_get_wtime();); (80, OpenMP, #pragma omp target enter data map(to: u[0:n*n], u_tmp[0:n*n]))"}
{"predict": "(46, OpenCL, actualSampler = clCreateSampler( context, CL_FALSE, CL_ADDRESS_NONE, CL_FILTER_NEAREST, &error );); (55, OpenCL, clEnqueueWriteBuffer( queue, results, CL_TRUE, 0, resultValuesSize, resultValues, 0, NULL, NULL );); (76, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 3, NULL, threads, NULL, 0, NULL, NULL );); (57, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_mem ), &read_write_image );); (99, OpenCL, clReleaseMemObject(results);); (97, OpenCL, clReleaseSampler(actualSampler);); (93, OpenCL, clReleaseMemObject(read_write_image);); (60, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_mem ), &read_only_image );); (61, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_mem ), &results );); (40, OpenCL, cl_mem results = clCreateBuffer( context, 0, imageInfo->width * imageInfo->height * imageInfo->arraySize * sizeof(cl_int), NULL, &error);); (56, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_sampler ), &actualSampler );); (72, OpenCL, error = clEnqueueReadBuffer( queue, results, CL_TRUE, 0, resultValuesSize, resultValues, 0, NULL, NULL );); (93, OpenCL, clReleaseMemObject(read_only_image);)", "label": "(46, OpenCL, actualSampler = clCreateSampler( context, CL_FALSE, CL_ADDRESS_NONE, CL_FILTER_NEAREST, &error );); (55, OpenCL, clEnqueueWriteBuffer( queue, results, CL_TRUE, 0, resultValuesSize, resultValues, 0, NULL, NULL );); (80, OpenCL, error = clEnqueueReadBuffer( queue, results, CL_TRUE, 0, resultValuesSize, resultValues, 0, NULL, NULL );); (75, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 3, NULL, threads, NULL, 0, NULL, NULL );); (60, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_mem ), &read_write_image );); (98, OpenCL, clReleaseMemObject(results);); (96, OpenCL, clReleaseSampler(actualSampler);); (100, OpenCL, clReleaseMemObject(read_write_image);); (96, OpenCL, clReleaseMemObject(read_only_image);); (51, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_mem ), &read_only_image );); (59, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_mem ), &results );); (56, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_sampler ), &actualSampler );); (38, OpenCL, cl_mem results = clCreateBuffer( context, 0, imageInfo->width * imageInfo->height * imageInfo->arraySize * sizeof(cl_int), NULL, &error);)"}
{"predict": "(28, TBB, tbb_list_time = (tbb::tick_count::now() - t0).seconds();); (42, TBB, serial_tree_time = (tbb::tick_count::now() - t0).seconds();); (15, TBB, serial_list_time = (tbb::tick_count::now() - t0).seconds();); (12, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (50, TBB, tbb_tree_time = (tbb::tick_count::now() - t0).seconds();)", "label": "(28, TBB, tbb_list_time = (tbb::tick_count::now() - t0).seconds();); (42, TBB, serial_tree_time = (tbb::tick_count::now() - t0).seconds();); (15, TBB, serial_list_time = (tbb::tick_count::now() - t0).seconds();); (12, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (50, TBB, tbb_tree_time = (tbb::tick_count::now() - t0).seconds();)"}
{"predict": "(31, OpenMP, #pragma omp target parallel for collapse(4); (65, OpenMP, #pragma omp target parallel for collapse(4, 8)); (39, OpenMP, #pragma omp target parallel for collapse(4, )); (6, OpenMP, #pragma omp target parallel for collapse(); (80, OpenMP, #pragma omp target parallel for collapse(5 - 5)); (88, OpenMP, #pragma omp target parallel for collapse(2) firstprivate(i)); (72, OpenMP, #pragma omp target parallel for collapse(-5)); (30, OpenMP, #pragma omp target parallel for collapse(4)); (75, OpenMP, #pragma omp target parallel for collapse(0)); (-6, OpenMP, #pragma omp target parallel for collapse); (63, OpenMP, #pragma omp target parallel for collapse(2.5)); (66, OpenMP, #pragma omp target parallel for collapse(foo())); (0, OpenMP, #pragma omp target parallel for collapse()); (69, OpenMP, #pragma omp target parallel for collapse(0.5)); (28, OpenMP, #pragma omp target parallel for collapse(4, , 4)); (2, OpenMP, #pragma omp target parallel for collapse(,); (36, OpenMP, #pragma omp target parallel for collapse(4, 8, 12)); (57, OpenMP, #pragma omp target parallel for collapse(4 % 4)); (39, OpenMP, #pragma omp target parallel for collapse(4, ,))", "label": "(31, OpenMP, #pragma omp target parallel for collapse(4); (54, OpenMP, #pragma omp target parallel for collapse(4, , 4)); (43, OpenMP, #pragma omp target parallel for collapse(4)); (2, OpenMP, #pragma omp target parallel for collapse); (22, OpenMP, #pragma omp target parallel for collapse 4)); (73, OpenMP, #pragma omp target parallel for collapse(-5)); (3, OpenMP, #pragma omp target parallel for collapse(); (79, OpenMP, #pragma omp target parallel for collapse(5 - 5)); (82, OpenMP, #pragma omp target parallel for collapse(2) firstprivate(i)); (41, OpenMP, #pragma omp target parallel for collapse(4 4)); (60, OpenMP, #pragma omp target parallel for collapse(2.5)); (63, OpenMP, #pragma omp target parallel for collapse(foo())); (29, OpenMP, #pragma omp target parallel for collapse(4, )); (0, OpenMP, #pragma omp target parallel for collapse()); (68, OpenMP, #pragma omp target parallel for collapse(0)); (2, OpenMP, #pragma omp target parallel for collapse(,); (20, OpenMP, #pragma omp target parallel for collapse(4,); (4, OpenMP, #pragma omp target parallel for collapse(, )); (48, OpenMP, #pragma omp target parallel for collapse(4, 8))"}
{"predict": "(201, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (109, CUDA, CUDA_CHECK(cudaMemcpyAsync(d_A, h_A.data(), llda * loc_n_a * sizeof(double), cudaMemcpyHostToDevice, stream));); (201, MPI, MPI_Barrier(MPI_COMM_WORLD);); (48, CUDA, CUDA_CHECK(cudaFree(nullptr));); (190, CUDA, CUDA_CHECK(cudaFreeAsync(d_C, stream));); (45, CUDA, CUDA_CHECK(cudaSetDevice(local_device));); (20, MPI, MPI_Init(nullptr, nullptr);); (188, CUDA, CUDA_CHECK(cudaFreeAsync(d_work, stream));); (99, CUDA, CUDA_CHECK(cudaMallocAsync(&d_A, llda * loc_n_a * sizeof(double), stream));); (142, MPI, const double begin = MPI_Wtime();); (183, CUDA, CUDA_CHECK(cudaFreeAsync(d_A, stream));); (33, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (96, CUDA, CUDA_CHECK(cudaMallocAsync(&d_C, lldc * loc_n_c * sizeof(double), stream));); (98, CUDA, CUDA_CHECK(cudaMemcpyAsync(d_C, h_C.data(), lldc * loc_n_c * sizeof(double), cudaMemcpyHostToDevice, stream));); (55, CUDA, CUDA_CHECK(cudaStreamCreate(&stream));); (190, MPI, MPI_Finalize();); (27, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nranks);); (127, CUDA, CUDA_CHECK(cudaMallocAsync(&d_work, workspaceInBytesOnDevice, stream));); (161, MPI, const double end = MPI_Wtime();)", "label": "(197, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (109, CUDA, CUDA_CHECK(cudaMemcpyAsync(d_A, h_A.data(), llda * loc_n_a * sizeof(double), cudaMemcpyHostToDevice, stream));); (197, MPI, MPI_Barrier(MPI_COMM_WORLD);); (48, CUDA, CUDA_CHECK(cudaFree(nullptr));); (186, CUDA, CUDA_CHECK(cudaFreeAsync(d_C, stream));); (45, CUDA, CUDA_CHECK(cudaSetDevice(local_device));); (20, MPI, MPI_Init(nullptr, nullptr);); (184, CUDA, CUDA_CHECK(cudaFreeAsync(d_work, stream));); (99, CUDA, CUDA_CHECK(cudaMallocAsync(&d_A, llda * loc_n_a * sizeof(double), stream));); (141, MPI, const double begin = MPI_Wtime();); (179, CUDA, CUDA_CHECK(cudaFreeAsync(d_A, stream));); (33, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (96, CUDA, CUDA_CHECK(cudaMallocAsync(&d_C, lldc * loc_n_c * sizeof(double), stream));); (98, CUDA, CUDA_CHECK(cudaMemcpyAsync(d_C, h_C.data(), lldc * loc_n_c * sizeof(double), cudaMemcpyHostToDevice, stream));); (55, CUDA, CUDA_CHECK(cudaStreamCreate(&stream));); (186, MPI, MPI_Finalize();); (27, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nranks);); (126, CUDA, CUDA_CHECK(cudaMallocAsync(&d_work, workspaceInBytesOnDevice, stream));); (157, MPI, const double end = MPI_Wtime();)"}
{"predict": "(16, MPI, MPI_Win_shared_query(shm_win, MPI_PROC_NULL, &query_size, &query_disp_unit, &query_base);); (30, MPI, MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, rank, MPI_INFO_NULL, &shm_comm);); (44, MPI, MPI_Win_free(&shm_win);); (36, MPI, MPI_Win_allocate(0, 1, MPI_INFO_NULL, shm_comm, &my_base, &shm_win);); (22, MPI, MPI_Comm_size(shm_comm, &shm_nproc);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (19, MPI, MPI_Comm_rank(shm_comm, &shm_rank);); (29, MPI, MPI_Win_allocate(size, sizeof(int), MPI_INFO_NULL, shm_comm, &my_base, &shm_win);)", "label": "(28, MPI, MPI_Comm_rank(shm_comm, &shm_rank);); (15, MPI, MPI_Win_shared_query(shm_win, MPI_PROC_NULL, &query_size, &query_disp_unit, &query_base);); (13, MPI, MPI_Win_allocate_shared(size, sizeof(int), MPI_INFO_NULL, MPI_COMM_SELF, &my_base, &shm_win);); (23, MPI, MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, rank, MPI_INFO_NULL, &shm_comm);); (34, MPI, MPI_Win_allocate_shared(0, sizeof(int), MPI_INFO_NULL, shm_comm, &my_base, &shm_win);); (20, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (67, MPI, MPI_Comm_free(&shm_comm);); (15, MPI, MPI_Win_free(&shm_win);); (21, MPI, MPI_Comm_size(shm_comm, &shm_nproc);); (31, MPI, MPI_Win_allocate_shared(size, sizeof(int), MPI_INFO_NULL, shm_comm, &my_base, &shm_win);)"}
{"predict": "(52, TBB, tbb::flow::make_edge( b2, b3 );); (50, TBB, tbb::flow::make_edge( b, b2 );); (73, TBB, tbb::flow::buffer_node<T> b_copy(b);); (2, TBB, tbb::flow::buffer_node<T> b3(g);); (0, TBB, tbb::flow::buffer_node<T> b2(g);); (-3, TBB, tbb::flow::graph g;); (-3, TBB, tbb::flow::buffer_node<T> b(g);)", "label": "(52, TBB, tbb::flow::make_edge( b2, b3 );); (50, TBB, tbb::flow::make_edge( b, b2 );); (73, TBB, tbb::flow::buffer_node<T> b_copy(b);); (2, TBB, tbb::flow::buffer_node<T> b3(g);); (0, TBB, tbb::flow::buffer_node<T> b2(g);); (-3, TBB, tbb::flow::graph g;); (-3, TBB, tbb::flow::buffer_node<T> b(g);)"}
{"predict": "(58, CUDA, cudaFree( deviceThreadIDs );); (60, CUDA, cudaFreeHost( hostOut );); (56, CUDA, cudaFree( deviceClockValues );)", "label": "(59, CUDA, cudaFree( deviceClockValues );); (60, CUDA, cudaFreeHost( hostOut );); (61, CUDA, cudaEventDestroy( stop );); (59, CUDA, cudaEventDestroy( start );); (56, CUDA, cudaFree( deviceThreadIDs );)"}
{"predict": "(45, MPI, MPI_Accumulate(mine, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_REPLACE, win);); (46, MPI, MPI_Accumulate(mine_plus, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_REPLACE, win);); (82, MPI, MPI_Win_unlock_all(win);); (43, MPI, MPI_Win_fence(0, win);); (71, MPI, MPI_Info_set(info_in, \"no_locks\", \"true\");); (82, MPI, MPI_Win_lock_all(0, win);); (12, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (58, MPI, MPI_Accumulate(mine, ARRAY_LEN, MPI_2INT, 0, 0, ARRAY_LEN, MPI_2INT, MPI_REPLACE, win);); (59, MPI, MPI_Accumulate(mine_plus, ARRAY_LEN, MPI_2INT, 0, 0, ARRAY_LEN, MPI_2INT, MPI_REPLACE, win);); (69, MPI, MPI_Win_free(&win);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (69, MPI, MPI_Win_create(data, ARRAY_LEN * sizeof(twoint_t), sizeof(twoint_t), MPI_INFO_NULL, MPI_COMM_WORLD, &win);); (42, MPI, MPI_Win_unlock(0, win);); (73, MPI, MPI_Win_create(data, ARRAY_LEN * sizeof(twoint_t), sizeof(twoint_t), info_in, MPI_COMM_WORLD, &win);); (36, MPI, MPI_Accumulate(mine_plus, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_REPLACE, win);); (2, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (64, MPI, MPI_Info_free(&info_in);); (24, MPI, MPI_Win_fence(0, win);); (6, MPI, MPI_Info_set(info_in, \"no_locks\", \"false\");); (39, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, 0, 0, win);", "label": "(81, MPI, MPI_Win_create(data, sizeof(twoint_t) * ARRAY_LEN, 1, info_in, MPI_COMM_WORLD, &win);); (81, MPI, MPI_Info_free(&info_in);); (134, MPI, MPI_Accumulate(mine_plus, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_MAXLOC, win);); (151, MPI, MPI_Accumulate(mine_plus, ARRAY_LEN, MPI_2INT, 0, 0, ARRAY_LEN, MPI_2INT, MPI_MAXLOC, win);); (34, MPI, MPI_Win_fence(0, win);); (56, MPI, MPI_Accumulate(mine, ARRAY_LEN, MPI_2INT, 0, 0, ARRAY_LEN, MPI_2INT, MPI_REPLACE, win);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (63, MPI, MPI_Win_free(&win);); (70, MPI, MPI_Info_create(&info_in);); (70, MPI, MPI_Info_set(info_in, \"accumulate_ordering\", \"none\");); (32, MPI, MPI_Accumulate(mine, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_REPLACE, win);); (181, MPI, MPI_Accumulate(mine, ARRAY_LEN, MPI_2INT, 0, 0, ARRAY_LEN, MPI_2INT, MPI_MINLOC, win);); (31, MPI, MPI_Accumulate(mine_plus, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_REPLACE, win);); (48, MPI, MPI_Accumulate(mine_plus, ARRAY_LEN, MPI_2INT, 0, 0, ARRAY_LEN, MPI_2INT, MPI_REPLACE, win);); (160, MPI, MPI_Accumulate(mine, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_MINLOC, win);); (1, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (20, MPI, MPI_Win_create(data, sizeof(twoint_t) * ARRAY_LEN, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &win);)"}
{"predict": "(19, MPI, MPI_Bcast(&array_size, 1, MPI_INT, 0, MPI_COMM_WORLD);); (26, MPI, MPI_Wait(&request, &status);); (41, MPI, end_time = MPI_Wtime();); (66, MPI, MPI_Finalize();); (12, MPI, MPI_Bcast(&nr_iters, 1, MPI_INT, 0, MPI_COMM_WORLD);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (26, MPI, start_time = MPI_Wtime();); (2, MPI, MPI_Init(NULL, NULL);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (8, MPI, MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);)", "label": "(19, MPI, MPI_Bcast(&array_size, 1, MPI_INT, 0, MPI_COMM_WORLD);); (26, MPI, MPI_Wait(&request, &status);); (41, MPI, end_time = MPI_Wtime();); (66, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (2, MPI, MPI_Init(NULL, NULL);); (2, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (25, MPI, start_time = MPI_Wtime();); (12, MPI, MPI_Bcast(&nr_iters, 1, MPI_INT, 0, MPI_COMM_WORLD);); (6, MPI, MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);)"}
{"predict": "(143, MPI, *v8 = MPI_Comm_c2f(comm);); (129, MPI, MPIU_Free(pargs);); (145, MPI, *ierr = MPI_Comm_spawn_multiple( (MPI_Info)(*v5), p2, (MPI_Info)(*v6), *v7, comm );); (16, MPI, MPIU_Free(ptmp2);); (135, MPI, MPIU_Free(p3);); (16, MPI, MPIU_Free(p2);)", "label": "(127, MPI, *ierr = MPI_Comm_spawn_multiple( *v1, p2, p3, v4, v5, *v6, (MPI_Comm)(*v7), (MPI_Comm *)(v8), (int *)v9 );)"}
{"predict": "(10, OpenMP, #pragma omp parallel); (96, OpenMP, #pragma omp critical)", "label": "(10, OpenMP, #pragma omp parallel); (14, OpenMP, #pragma omp for); (95, OpenMP, #pragma omp critical)"}
{"predict": "(30, OpenCL, err = clEnqueueWriteBuffer( queue, streams[0], true, 0, sizeof(cl_int)*num_elements, (void *)int_input_ptr, 0, NULL, NULL );); (37, OpenCL, err = clEnqueueCopyBuffer( queue, streams[0], streams[1], 0, 0, sizeof(cl_int)*num_elements, 0, NULL, &copyEvent );); (47, OpenCL, err = clWaitForEvents( 1, &copyEvent );); (105, OpenCL, err = clEnqueueReadBuffer( queue, streams[1], true, 0, sizeof(cl_int)*num_elements, (void *)int_output_ptr, 0, NULL, NULL );); (50, OpenCL, clReleaseEvent(copyEvent);); (38, OpenCL, clReleaseMemObject( streams[0] );); (38, OpenCL, clReleaseMemObject( streams[1] );)", "label": "(30, OpenCL, err = clEnqueueWriteBuffer( queue, streams[0], true, 0, sizeof(cl_int)*num_elements, (void *)int_input_ptr, 0, NULL, NULL );); (39, OpenCL, err = clEnqueueCopyBuffer( queue, streams[0], streams[1], 0, 0, sizeof(cl_int)*num_elements, 0, NULL, &copyEvent );); (32, OpenCL, clReleaseMemObject( streams[1] );); (107, OpenCL, err = clEnqueueReadBuffer( queue, streams[1], true, 0, sizeof(cl_int)*num_elements, (void *)int_output_ptr, 0, NULL, NULL );); (48, OpenCL, err = clWaitForEvents( 1, &copyEvent );); (28, OpenCL, clReleaseMemObject( streams[0] );); (49, OpenCL, clReleaseEvent(copyEvent);)"}
{"predict": "(53, CUDA, extern __shared__ float s_float[];)", "label": "(53, CUDA, extern __shared__ float s_float[];)"}
{"predict": "(47, OpenMP, lis2mdl_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)", "label": "(47, OpenMP, lis2mdl_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)"}
{"predict": "(108, OpenMP, #pragma omp parallel for simd); (26, MPI, MPI_Comm_size(MPI_COMM_WORLD,&Num_procs);); (150, MPI, MPI_Finalize();); (106, OpenMP, #pragma omp barrier); (76, MPI, MPI_Bcast(&length,        1, MPI_LONG, root, MPI_COMM_WORLD);); (23, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&my_ID);); (77, MPI, MPI_Bcast(&nthread_input, 1, MPI_INT,  root, MPI_COMM_WORLD);); (78, OpenMP, omp_set_num_threads(nthread_input);); (18, MPI, MPI_Init(&argc,&argv);); (73, MPI, MPI_Bcast(&iterations,    1, MPI_INT,  root, MPI_COMM_WORLD);); (74, MPI, MPI_Bcast(&offset,        1, MPI_LONG, root, MPI_COMM_WORLD);); (113, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(103, OpenMP, printf(\"Number of threads    = %d\\n\", omp_get_max_threads());); (25, MPI, MPI_Comm_size(MPI_COMM_WORLD,&Num_procs);); (107, OpenMP, #pragma omp parallel for simd); (147, MPI, MPI_Finalize();); (76, MPI, MPI_Bcast(&length,        1, MPI_LONG, root, MPI_COMM_WORLD);); (22, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&my_ID);); (77, MPI, MPI_Bcast(&nthread_input, 1, MPI_INT,  root, MPI_COMM_WORLD);); (78, OpenMP, omp_set_num_threads(nthread_input);); (17, MPI, MPI_Init(&argc,&argv);); (73, MPI, MPI_Bcast(&iterations,    1, MPI_INT,  root, MPI_COMM_WORLD);); (114, MPI, MPI_Barrier(MPI_COMM_WORLD);); (70, MPI, MPI_Bcast(&offset,        1, MPI_LONG, root, MPI_COMM_WORLD);)"}
{"predict": "(121, CUDA, checkCudaErrors(cudaFreeArray(cu_3darray));); (94, CUDA, getLastCudaError(\"warmup Kernel execution failed\");); (119, CUDA, checkCudaErrors(cudaMemcpy(h_odata, d_data, size, cudaMemcpyDeviceToHost));); (60, CUDA, myparms.dstPos = make_cudaPos(0, 0, 0);); (64, CUDA, checkCudaErrors(cudaCreateTextureObject(&tex, &texRes, &texDescr, NULL));); (103, CUDA, cudaDeviceSynchronize();); (57, CUDA, myparms.srcPos = make_cudaPos(0, 0, 0);); (98, CUDA, getLastCudaError(\"Kernel execution failed\");); (58, CUDA, checkCudaErrors(cudaMemcpy3D(&myparms));); (-9, CUDA, int devID = findCudaDevice(argc, (const char **)argv);); (37, CUDA, checkCudaErrors(cudaMalloc((void **)&d_data, size));); (63, CUDA, myparms.extent = make_cudaExtent(width, width, num_faces);); (109, CUDA, checkCudaErrors(cudaFree(d_data));); (124, CUDA, checkCudaErrors(cudaDestroyTextureObject(tex));); (3, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProps, devID));)", "label": "(59, CUDA, myparms.dstPos = make_cudaPos(0, 0, 0);); (9, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProps, devID));); (56, CUDA, myparms.srcPos = make_cudaPos(0, 0, 0);); (119, CUDA, checkCudaErrors(cudaMemcpy(h_odata, d_data, size, cudaMemcpyDeviceToHost));); (94, CUDA, getLastCudaError(\"warmup Kernel execution failed\");); (60, CUDA, checkCudaErrors(cudaMemcpy3D(&myparms));); (57, CUDA, myparms.extent = make_cudaExtent(width, width, num_faces);); (137, CUDA, checkCudaErrors(cudaFreeArray(cu_3darray));); (76, CUDA, checkCudaErrors(cudaCreateTextureObject(&tex, &texRes, &texDescr, NULL));); (91, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (36, CUDA, checkCudaErrors(cudaMalloc((void **)&d_data, size));); (99, CUDA, getLastCudaError(\"Kernel execution failed\");); (131, CUDA, checkCudaErrors(cudaFree(d_data));); (-10, CUDA, int devID = findCudaDevice(argc, (const char **)argv);); (128, CUDA, checkCudaErrors(cudaDestroyTextureObject(tex));)"}
{"predict": "(51, MPI, MPI_Waitall(WINDOW, req, status);); (23, MPI, start = MPI_Wtime();); (48, MPI, end = MPI_Wtime();)", "label": "(48, MPI, MPI_Waitall(WINDOW, req, status);); (23, MPI, start = MPI_Wtime();); (50, MPI, end = MPI_Wtime();)"}
{"predict": "(62, OpenMP, #pragma omp distribute parallel for dist_schedule(static)); (18, OpenMP, #pragma omp distribute parallel for); (102, OpenMP, #pragma omp distribute parallel for dist_schedule(static, ch)); (134, OpenMP, #pragma omp distribute parallel for schedule(static)); (165, OpenMP, #pragma omp distribute parallel for schedule(static, ch)); (11, OpenMP, #pragma omp target); (196, OpenMP, #pragma omp distribute parallel for schedule(dynamic)); (224, OpenMP, #pragma omp distribute parallel for schedule(dynamic, ch)); (9, OpenMP, #pragma omp teams)", "label": "(169, OpenMP, #pragma omp distribute parallel for schedule(static, ch)); (201, OpenMP, #pragma omp distribute parallel for schedule(dynamic)); (17, OpenMP, #pragma omp distribute parallel for); (59, OpenMP, #pragma omp distribute parallel for dist_schedule(static)); (227, OpenMP, #pragma omp distribute parallel for schedule(dynamic, ch)); (132, OpenMP, #pragma omp distribute parallel for schedule(static)); (10, OpenMP, #pragma omp target); (97, OpenMP, #pragma omp distribute parallel for dist_schedule(static, ch)); (9, OpenMP, #pragma omp teams)"}
{"predict": "(10, CUDA, checkCudaErrors(cudaGetDeviceCount(&deviceCount));); (110, CUDA, checkCudaErrors(cudaEventCreate(&profileStart));); (192, CUDA, checkCudaErrors(cudaFree(bufferIn));); (40, CUDA, checkCudaErrors(cudaSetDevice(targetDevice));); (64, CUDA, checkCudaErrors(cudaFuncGetAttributes(&funcAttrib, FiniteDifferencesKernel));); (115, CUDA, checkCudaErrors(cudaEventRecord(profileStart, 0));); (138, CUDA, checkCudaErrors(cudaEventRecord(profileEnd, 0));); (102, CUDA, checkCudaErrors(cudaEventCreate(&profileEnd));); (173, CUDA, checkCudaErrors(cudaEventElapsedTime(&elapsedTimeMS, profileStart, profileEnd));); (160, CUDA, checkCudaErrors(cudaEventSynchronize(profileEnd));); (177, CUDA, checkCudaErrors(cudaMemcpy(output, bufferSrc, volumeSize * sizeof(float), cudaMemcpyDeviceToHost));)", "label": "(109, CUDA, checkCudaErrors(cudaEventCreate(&profileStart));); (43, CUDA, checkCudaErrors(cudaSetDevice(targetDevice));); (40, CUDA, targetDevice = findCudaDevice(argc, (const char **)argv);); (193, CUDA, checkCudaErrors(cudaEventDestroy(profileEnd));); (140, CUDA, checkCudaErrors(cudaEventRecord(profileEnd, 0));); (116, CUDA, checkCudaErrors(cudaEventRecord(profileStart, 0));); (186, CUDA, checkCudaErrors(cudaEventDestroy(profileStart));); (61, CUDA, checkCudaErrors(cudaFuncGetAttributes(&funcAttrib, FiniteDifferencesKernel));); (31, CUDA, checkCudaErrors(cudaGetDeviceCount(&deviceCount));); (101, CUDA, checkCudaErrors(cudaEventCreate(&profileEnd));); (176, CUDA, checkCudaErrors(cudaFree(bufferOut));); (171, CUDA, checkCudaErrors(cudaFree(bufferIn));)"}
{"predict": "(65, MPI, MPI_Gatherv(sendbuf, bytes, MPI_BYTE, recvbuf, counts, disps, MPI_BYTE, 0, MPI_COMM_WORLD);); (13, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (36, MPI, MPI_Gather(&bytes, 1, MPI_INT, counts, 1, MPI_INT, 0, MPI_COMM_WORLD);)", "label": "(65, MPI, MPI_Gatherv(sendbuf, bytes, MPI_BYTE, recvbuf, counts, disps, MPI_BYTE, 0, MPI_COMM_WORLD);); (13, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (36, MPI, MPI_Gather(&bytes, 1, MPI_INT, counts, 1, MPI_INT, 0, MPI_COMM_WORLD);)"}
{"predict": "(31, MPI, MPI_Comm_test_inter(intercomm, &flag);); (38, MPI, MPI_Comm_size(intercomm, &orig_size);); (54, MPI, MPI_Comm_size(newcomm, &new_size);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (6, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (28, MPI, MPI_Comm_remote_size(intercomm, &orig_rsize);); (20, MPI, MPI_Comm_split(intercomm, color, key, &newcomm);); (14, MPI, MPI_Comm_rank(intercomm, &key);); (82, MPI, MPI_Comm_free(&intercomm);)", "label": "(32, MPI, MPI_Comm_test_inter(intercomm, &flag);); (23, MPI, MPI_Comm_rank(intercomm, &key);); (90, MPI, MPI_Comm_free(&intercomm);); (38, MPI, MPI_Comm_size(intercomm, &orig_size);); (38, MPI, MPI_Comm_size(newcomm, &new_size);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (33, MPI, MPI_Comm_remote_size(intercomm, &orig_rsize);); (19, MPI, MPI_Comm_split(intercomm, color, key, &newcomm);); (3, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (31, MPI, MPI_Comm_remote_size(newcomm, &new_rsize);)"}
{"predict": "(24, CUDA, extern __shared__ float sram[];)", "label": "(24, CUDA, extern __shared__ float sram[];)"}
{"predict": "(18, OpenMP, #pragma omp parallel for); (26, OpenMP, threadLimit = omp_get_thread_num();); (21, OpenMP, check3[omp_get_thread_num()] += omp_get_num_threads();); (8, OpenMP, #pragma omp target map(check1[:], check2[:], check3[:], threadLimit)); (25, OpenMP, #pragma omp target update from(check3[ : ])); (12, OpenMP, #pragma omp target teams thread_limit(ThreadLimit) map(check1[:], check2[:], check3[:], threadLimit)); (15, OpenMP, check2[omp_get_thread_num()] += omp_get_num_threads();); (20, OpenMP, check1[omp_get_thread_num()] += omp_get_num_threads();); (10, OpenMP, #pragma omp target update from(check2[ : ])); (24, OpenMP, #pragma omp target update from(check1[ : ]))", "label": "(28, OpenMP, #pragma omp parallel num_threads(NumThreads3)); (29, OpenMP, check3[omp_get_thread_num()] += omp_get_num_threads();); (15, OpenMP, #pragma omp parallel); (10, OpenMP, threadLimit = omp_get_thread_limit();); (6, OpenMP, #pragma omp target teams num_teams(1) thread_limit(ThreadLimit)); (20, OpenMP, check2[omp_get_thread_num()] += omp_get_num_threads();); (16, OpenMP, omp_set_num_threads(NumThreads2);); (12, OpenMP, check1[omp_get_thread_num()] += omp_get_num_threads();)"}
{"predict": "(42, CUDA, cudaFree(B);); (109, CUDA, result = cudaMemcpy(host_cutlass.data(), C_cutlass, sizeof_C, cudaMemcpyDeviceToHost);); (59, CUDA, cudaFree(C_reference);); (52, CUDA, result = cudaMemcpy(C_reference, C_cutlass, sizeof_C, cudaMemcpyDeviceToDevice);); (47, CUDA, cudaFree(C_cutlass);); (119, CUDA, result = cudaMemcpy(host_reference.data(), C_reference, sizeof_C, cudaMemcpyDeviceToHost);); (28, CUDA, cudaFree(A);)", "label": "(42, CUDA, cudaFree(B);); (109, CUDA, result = cudaMemcpy(host_cutlass.data(), C_cutlass, sizeof_C, cudaMemcpyDeviceToHost);); (59, CUDA, cudaFree(C_reference);); (52, CUDA, result = cudaMemcpy(C_reference, C_cutlass, sizeof_C, cudaMemcpyDeviceToDevice);); (47, CUDA, cudaFree(C_cutlass);); (119, CUDA, result = cudaMemcpy(host_reference.data(), C_reference, sizeof_C, cudaMemcpyDeviceToHost);); (28, CUDA, cudaFree(A);)"}
{"predict": "(82, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &iam );); (257, MPI, MPI_Finalize();); (24, MPI, MPI_Init_thread( &argc, &argv, MPI_THREAD_MULTIPLE, &omp_mpi_level);); (25, MPI, MPI_Comm_size( MPI_COMM_WORLD, &nprocs );)", "label": "(82, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &iam );); (257, MPI, MPI_Finalize();); (24, MPI, MPI_Init_thread( &argc, &argv, MPI_THREAD_MULTIPLE, &omp_mpi_level);); (25, MPI, MPI_Comm_size( MPI_COMM_WORLD, &nprocs );)"}
{"predict": "(9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (76, MPI, MPI_Bcast(footer, 7, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (208, MPI, MPI_Barrier(MPI_COMM_WORLD);); (182, MPI, MPI_Bcast(&decomp_state, 1, MPI_INT, 0, MPI_COMM_WORLD);)", "label": "(10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (76, MPI, MPI_Bcast(&footer_flag, 1, MPI_INT, 0, MPI_COMM_WORLD);); (76, MPI, MPI_Bcast(&footer, 8, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(181, OpenCL, err = clReleaseKernel(kernel);); (185, OpenCL, err = clReleaseMemObject(buffer);); (180, OpenCL, err = clReleaseProgram(program);); (40, OpenCL, context = clCreateContext(NULL, 1, &device_id, NULL, NULL, &err);); (148, OpenCL, void *ptr = (int *)clEnqueueMapBuffer(queue, buffer, CL_TRUE, CL_MAP_READ, offset, bufferSize, 0, NULL, NULL, &err);); (72, OpenCL, err = clBuildProgram(program, 1, &device_id, NULL, NULL, NULL);); (172, OpenCL, err = clReleaseCommandQueue(queue);); (169, OpenCL, err = clReleaseContext(context);); (46, OpenCL, queue = clCreateCommandQueue(context, device_id, 0, &err);); (94, OpenCL, kernel = clCreateKernel(program, \"hello\", &err);); (137, OpenCL, err = clEnqueueNDRangeKernel(queue, kernel, dimension, offset, gws, lws, 0, NULL, NULL);); (114, OpenCL, err = clEnqueueUnmapMemObject(queue, buffer, ptr, 0, NULL, NULL);); (97, OpenCL, buffer = clCreateBuffer(context, flags, bufferSize, NULL, &err);); (170, OpenCL, err = clReleaseMemObject(buffer);); (56, OpenCL, program = clCreateProgramWithSource(context, 1, &strings, NULL, &err);); (20, OpenCL, abort();); (139, OpenCL, ptr = (int *)clEnqueueMapBuffer(queue, buffer, CL_TRUE, CL_MAP_READ, offset, bufferSize, 0, NULL, NULL, &err);); (86, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(cl_mem), &buffer);); (163, OpenCL, err = clReleaseQueue(queue);); (159, OpenCL, err = clReleaseContext(context);); (142, OpenCL, clReleaseKernel(kernel);); (137, OpenCL, clReleaseProgram(program);); (15", "label": "(69, OpenCL, program = clCreateProgramWithSource(context, 1, &strings, 0, &err);); (129, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(cl_mem), &buffer);); (190, OpenCL, clReleaseKernel(kernel);); (191, OpenCL, clReleaseCommandQueue(queue);); (189, OpenCL, clReleaseProgram(program);); (139, OpenCL, err = clEnqueueNDRangeKernel(queue, kernel, dimension, offset, gws, lws, 0, 0, nullptr);); (189, OpenCL, clReleaseContext(context);); (101, OpenCL, buffer = clCreateBuffer(context, flags, bufferSize, nullptr, &err);); (115, OpenCL, err = clEnqueueUnmapMemObject(queue, buffer, ptr, 0, nullptr, nullptr);); (106, OpenCL, void *ptr = clEnqueueMapBuffer(queue, buffer, CL_TRUE, CL_MAP_WRITE, 0, bufferSize, 0, nullptr, nullptr, &err);); (38, OpenCL, queue = clCreateCommandQueue(context, device_id, 0, &err);); (140, OpenCL, void *ptr = clEnqueueMapBuffer(queue, buffer, CL_TRUE, CL_MAP_READ, 0, bufferSize, 0, nullptr, nullptr, &err);); (64, OpenCL, err = clBuildProgram(program, 1, &device_id, nullptr, nullptr, nullptr);); (178, OpenCL, clReleaseMemObject(buffer);); (28, OpenCL, context = clCreateContext(0, 1, &device_id, NULL, NULL, &err);); (84, OpenCL, kernel = clCreateKernel(program, \"hello\", &err);)"}
{"predict": "(77, CUDA, cuda_safe_call(cudaFree(d_ptrB));); (7, CUDA, cuda_safe_call(cudaMalloc(&d_ptrB, N * sizeof(double)));); (9, CUDA, cuda_safe_call(cudaStreamCreate(&stream));); (5, CUDA, cuda_safe_call(cudaMalloc(&d_ptrA, N * sizeof(double)));); (72, CUDA, cuda_safe_call(cudaFree(d_ptrA));); (12, CUDA, cuda_safe_call(cudaStreamSynchronize(stream));)", "label": "(18, CUDA, cuda_safe_call(cudaStreamSynchronize(stream));); (7, CUDA, cuda_safe_call(cudaMalloc(&d_ptrB, N * sizeof(double)));); (9, CUDA, cuda_safe_call(cudaStreamCreate(&stream));); (4, CUDA, cuda_safe_call(cudaMalloc(&d_ptrA, N * sizeof(double)));)"}
{"predict": "(57, CUDA, cudaError_t err = cudaDeviceEnablePeerAccess(odev, 0);); (44, CUDA, cudaDeviceCanAccessPeer(&access, dev1, dev2);); (53, CUDA, err = cudaSetDevice(dev);); (13, CUDA, cudaError_t err = cudaGetDeviceProperties(&props, dev);); (-1, CUDA, cudaGetDeviceCount(reinterpret_cast<int *>(&device));)", "label": "(47, CUDA, cudaDeviceCanAccessPeer(&access, dev1, dev2);); (53, CUDA, cudaSetDevice(dev);); (57, CUDA, auto err = cudaDeviceEnablePeerAccess(odev, 0);); (14, CUDA, auto err = cudaGetDeviceProperties(&props, dev);); (9, CUDA, INFO(\"failed to cudaSetDevice(%d)\\n\", dev);); (-2, CUDA, cudaGetDeviceCount(reinterpret_cast<int *>(&device));)"}
{"predict": "(11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (209, MPI, MPI_Win_free(&win);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (44, MPI, MPI_Put(&localbuf[i], 1, MPI_INT, trank, rank + i * wsize, 1, MPI_INT, win);); (42, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, trank, 0, win);); (43, MPI, MPI_Put(&localbuf[i], 1, MPI_INT, trank, rank + (i + NELM) * wsize, 1, MPI_INT, win);); (43, MPI, MPI_Win_unlock(trank, win);); (135, MPI, MPI_Get(&vals[i], 1, MPI_INT, trank, i, 1, MPI_INT, win);); (181, MPI, MPI_Accumulate(&localbuf[i], 1, MPI_INT, trank, rank + i * wsize, 1, MPI_INT, MPI_SUM, win);); (35, MPI, MPI_Barrier(MPI_COMM_WORLD);); (4, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(12, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (208, MPI, MPI_Win_free(&win);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (46, MPI, MPI_Put(&localbuf[i], 1, MPI_INT, trank, rank + i * wsize, 1, MPI_INT, win);); (44, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, trank, 0, win);); (45, MPI, MPI_Put(&localbuf[i], 1, MPI_INT, trank, rank + (i + NELM) * wsize, 1, MPI_INT, win);); (45, MPI, MPI_Win_unlock(trank, win);); (127, MPI, MPI_Get(&vals[i], 1, MPI_INT, trank, i, 1, MPI_INT, win);); (183, MPI, MPI_Accumulate(&localbuf[i], 1, MPI_INT, trank, rank + i * wsize, 1, MPI_INT, MPI_SUM, win);); (37, MPI, MPI_Barrier(MPI_COMM_WORLD);); (6, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(144, OpenMP, #pragma omp simd); (87, OpenMP, #pragma\tomp\ttaskloop private (thread_id1,nn,lbstart,lbend) untied); (77, OpenMP, #pragma\tomp master); (221, OpenMP, #pragma omp simd lastprivate(irow)); (81, OpenMP, #pragma omp parallel default (shared)); (174, OpenMP, #pragma omp simd private (irow,ii,jj,t1,t2,Uinv,nn,lbstart,lbend,thread_id1) lastprivate(irow)); (42, OpenMP, #pragma omp parallel default(shared)); (142, OpenMP, #pragma omp simd private (irow,ii,t1,t2,Uinv,nn,lbstart,lbend) lastprivate(irow)); (76, OpenMP, #pragma\tomp taskloop private (thread_id1,nn,lbstart,lbend))", "label": "(82, OpenMP, #pragma\tomp\ttaskloop firstprivate (stat) private (thread_id1,Uinv,nn,lbstart,lbend,ub,temp,rtemp_loc,ik,lk1,gik,gikcol,usub,uval,lsub,lusup,iknsupc,il,i,irow,bmod_tmp,p,ii,jj,t1,t2,j,ikfrow,iklrow,dest,y,uptr,fnz,nsupr,nroot_send_tmp) untied nogroup); (126, OpenMP, #pragma omp simd); (146, OpenMP, #pragma omp atomic capture); (75, OpenMP, #pragma\tomp\tparallel for private (thread_id1,Uinv,nn,lbstart,lbend,ub,temp,rtemp_loc,ik,lk1,gik,gikcol,usub,uval,lsub,lusup,iknsupc,il,i,irow,bmod_tmp,p,ii,jj,t1,t2,j,ikfrow,iklrow,dest,y,uptr,fnz,nsupr))"}
{"predict": "(24, MPI, MPI_Type_get_true_extent(key, &key_true_lb, &key_true_extent);); (84, MPI, MPI_Allreduce(counts, all_counts, 2, MPI_UINT64_T, MPI_SUM, comm);); (19, MPI, MPI_Type_get_extent(keysat, &keysat_lb, &keysat_extent);); (14, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(25, MPI, MPI_Type_get_true_extent(key, &key_true_lb, &key_true_extent);); (15, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (83, MPI, MPI_Allreduce(counts, all_counts, 2, MPI_UINT64_T, MPI_SUM, comm);); (18, MPI, MPI_Type_get_extent(keysat, &keysat_lb, &keysat_extent);)"}
{"predict": "(235, OpenMP, #pragma omp parallel for simd linear(ii)); (224, OpenMP, #pragma omp parallel for simd private(ii)); (254, OpenMP, #pragma omp parallel for simd lastprivate(ii)); (277, OpenMP, #pragma omp parallel for simd collapse(2)); (3, OpenMP, #pragma omp parallel for simd); (227, OpenMP, #pragma omp parallel for simd firstprivate(ii))", "label": "(235, OpenMP, #pragma omp parallel for simd private(ii)); (5, OpenMP, #pragma omp parallel for simd); (257, OpenMP, #pragma omp parallel for simd collapse(2)); (226, OpenMP, #pragma omp parallel for simd linear(ii)); (237, OpenMP, #pragma omp parallel for simd lastprivate(ii)); (220, OpenMP, #pragma omp parallel for simd firstprivate(ii))"}
{"predict": "(14, CUDA, CHECKED_CALL(cudaGetDeviceProperties(&deviceProp, devID));); (63, CUDA, CHECKED_CALL(cudaEventRecord(stop, 0));); (55, CUDA, CHECKED_CALL(cudaEventRecord(start, 0));); (63, CUDA, CHECKED_CALL(cudaEventElapsedTime(&time, start, stop));); (46, CUDA, CHECKED_CALL(cudaFuncSetAttribute(test_interval_newton<T>, cudaFuncAttributeMaxDynamicSharedMemorySize, 4 * 1024));); (99, CUDA, CHECKED_CALL(cudaFree(d_result));); (10, CUDA, int devID = findCudaDevice(argc, (const char **)argv);); (56, CUDA, CHECKED_CALL(cudaEventSynchronize(stop));); (32, CUDA, CHECKED_CALL(cudaMalloc((void **)&d_nresults, THREADS * sizeof(*d_nresults)));); (88, CUDA, CHECKED_CALL(cudaFree(d_nresults));); (46, CUDA, CHECKED_CALL(cudaEventCreate(&start));); (46, CUDA, CHECKED_CALL(cudaEventCreate(&stop));)", "label": "(59, CUDA, CHECKED_CALL(cudaEventRecord(start, 0));); (52, CUDA, CHECKED_CALL(cudaDeviceSetLimit(cudaLimitStackSize, 8192));); (62, CUDA, CHECKED_CALL(cudaGetLastError());); (41, CUDA, CHECKED_CALL(cudaEventCreate(&start));); (36, CUDA, CHECKED_CALL(cudaSetDevice(devID));); (88, CUDA, CHECKED_CALL(cudaEventDestroy(start));); (89, CUDA, CHECKED_CALL(cudaFree(d_result));); (61, CUDA, CHECKED_CALL(cudaDeviceSynchronize());); (6, CUDA, cudaGetDeviceProperties(&deviceProp, devID);); (3, CUDA, int devID = findCudaDevice(argc, (const char **)argv);); (78, CUDA, CHECKED_CALL(cudaEventElapsedTime(&time, start, stop));); (56, CUDA, CHECKED_CALL(cudaEventRecord(stop, 0));); (84, CUDA, CHECKED_CALL(cudaFree(d_nresults));); (30, CUDA, CHECKED_CALL(cudaMalloc((void **)&d_nresults, THREADS * sizeof(*d_nresults)));); (80, CUDA, CHECKED_CALL(cudaEventDestroy(stop));); (30, CUDA, CHECKED_CALL(cudaEventCreate(&stop));)"}
{"predict": "(35, CUDA, checkCudaErrors(cudaCreateTextureObject(&texObj, &resDesc, &texDesc, NULL));); (6, CUDA, checkCudaErrors(cudaMemset(d_PointCounterAddr, 0, sizeof(unsigned int)));); (55, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (1, CUDA, checkCudaErrors(cudaGetSymbolAddress((void**)&d_PointCounterAddr, d_PointCounter));); (4, CUDA, checkCudaErrors(cudaMemcpyFromSymbol(&fstPts, d_PointCounterAddr, sizeof(unsigned int), 0, cudaMemcpyDeviceToHost));); (53, CUDA, checkCudaErrors(cudaFreeTextureObject(texObj));); (3, CUDA, checkCudaErrors(cudaMemcpyFromSymbol(&totPts, d_PointCounterAddr, sizeof(unsigned int), sizeof(unsigned int), cudaMemcpyDeviceToHost));)", "label": "(5, CUDA, safeCall(cudaGetSymbolAddress((void**)&d_PointCounterAddr, d_PointCounter));); (57, CUDA, safeCall(cudaMemcpy(&totPts, &d_PointCounterAddr[2*octave+1], sizeof(int), cudaMemcpyDeviceToHost));); (5, CUDA, safeCall(cudaMemcpy(&fstPts, &d_PointCounterAddr[2*octave-1], sizeof(int), cudaMemcpyDeviceToHost));); (50, CUDA, safeCall(cudaDestroyTextureObject(texObj));); (32, CUDA, cudaCreateTextureObject(&texObj, &resDesc, &texDesc, NULL);)"}
{"predict": "(52, TBB, tbb::flow::make_edge( q2, q3 );); (50, TBB, tbb::flow::make_edge( q, q2 );); (72, TBB, tbb::flow::queue_node<T> q_copy(q);); (3, TBB, tbb::flow::queue_node<T> q(g);); (3, TBB, tbb::flow::queue_node<T> q2(g);); (3, TBB, tbb::flow::queue_node<T> q3(g);); (-2, TBB, tbb::flow::graph g;); (83, TBB, tbb::flow::remove_edge( q, q2 );)", "label": "(4, TBB, tbb::flow::queue_node<T> q2(g);); (51, TBB, tbb::flow::make_edge( q2, q3 );); (3, TBB, tbb::flow::queue_node<T> q3(g);); (48, TBB, tbb::flow::make_edge( q, q2 );); (-2, TBB, tbb::flow::graph g;); (68, TBB, tbb::flow::queue_node<T> q_copy(q);); (-3, TBB, tbb::flow::queue_node<T> q(g);)"}
{"predict": "(11, OpenMP, #pragma omp parallel for); (38, OpenMP, #pragma omp atomic)", "label": "(11, OpenMP, #pragma omp parallel for); (38, OpenMP, #pragma omp atomic)"}
{"predict": "(82, MPI, MPI_Comm_disconnect(&icomm);); (28, MPI, MPI_Init(&argc, &argv);); (84, MPI, MPI_Error_string(rc, estr, &slen);); (73, MPI, MPI_Comm_spawn(argv[0], &spawn_argv[1], 1, spawn_info, 0, MPI_COMM_WORLD, &icomm, spawn_err);); (55, MPI, MPI_Comm_get_errhandler(MPI_COMM_SELF, &errh);); (76, MPI, MPI_Barrier(icomm);); (43, MPI, MPI_Info_get(MPI_INFO_ENV, \"mpi_initial_errhandler\", MPI_MAX_INFO_VAL, init_errh_info, &flag);); (80, MPI, rc = MPI_Comm_set_errhandler(MPI_COMM_SELF, MPI_ERRORS_ARE_FATAL);); (23, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (87, MPI, rc = MPI_Finalize();); (93, MPI, rc = MPI_Error_string(MPI_ERR_WIN, estr, &slen);); (62, MPI, MPI_Info_set(spawn_info, \"mpi_initial_errhandler\", \"mpi_errors_return\");); (43, MPI, MPI_Info_get(MPI_INFO_ENV, \"mpi_initial_errhandler\", MPI_MAX_INFO_VAL - 1, init_errh_info, &flag);); (67, MPI, MPI_Info_create(&spawn_info);); (57, MPI, MPI_Info_free(&spawn_info);)", "label": "(28, MPI, MPI_Init(&argc, &argv);); (82, MPI, MPI_Comm_disconnect(&icomm);); (76, MPI, MPI_Comm_spawn(argv[0], &spawn_argv[1], 1, spawn_info, 0, MPI_COMM_WORLD, &icomm, spawn_err);); (26, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (83, MPI, rc = MPI_Comm_set_errhandler(MPI_COMM_SELF, MPI_ERRORS_ARE_FATAL);); (43, MPI, MPI_Info_get(MPI_INFO_ENV, \"mpi_initial_errhandler\", MPI_MAX_INFO_VAL, init_errh_info, &flag);); (68, MPI, MPI_Info_set(spawn_info, \"mpi_initial_errhandler\", \"mpi_errors_return\");); (90, MPI, rc = MPI_Finalize();); (74, MPI, MPI_Barrier(icomm);); (80, MPI, MPI_Error_string(rc, estr, &slen);); (49, MPI, MPI_Comm_get_errhandler(MPI_COMM_SELF, &errh);); (62, MPI, MPI_Info_create(&spawn_info);); (93, MPI, rc = MPI_Error_string(MPI_ERR_WIN, estr, &slen);)"}
{"predict": "(33, CUDA, HANDLE_ERROR(cudaMalloc((void**)&baseline_and_normalizer_d, sizeof(float2)));); (42, CUDA, HANDLE_ERROR(cudaStreamSynchronize(stream));); (29, CUDA, HANDLE_ERROR(cudaMalloc((void**)&costs_host_only_d, sizeof(float) * num_rollouts));); (27, CUDA, HANDLE_ERROR(cudaMalloc((void**)&costs_dev_only_d, sizeof(float) * num_rollouts));); (74, CUDA, HANDLE_ERROR(cudaFree(baseline_and_normalizer_d));); (72, CUDA, HANDLE_ERROR(cudaFree(costs_host_only_d));); (70, CUDA, HANDLE_ERROR(cudaFree(costs_dev_only_d));); (0, CUDA, HANDLE_ERROR(cudaStreamCreate(&stream));)", "label": "(12, CUDA, cudaStreamCreate(&stream);); (37, CUDA, HANDLE_ERROR(cudaStreamSynchronize(stream));); (17, CUDA, HANDLE_ERROR(cudaMalloc((void**)&costs_dev_only_d, sizeof(float) * num_rollouts));); (15, CUDA, HANDLE_ERROR(cudaMalloc((void**)&baseline_and_normalizer_d, sizeof(float2)));); (16, CUDA, HANDLE_ERROR(cudaMalloc((void**)&costs_host_only_d, sizeof(float) * num_rollouts));)"}
{"predict": "(33, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (41, TBB, reduce_time = (tbb::tick_count::now() - t0).seconds();); (47, TBB, graph_time = (tbb::tick_count::now() - t0).seconds();); (32, TBB, serial_time = (tbb::tick_count::now() - t0).seconds();)", "label": "(33, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (41, TBB, reduce_time = (tbb::tick_count::now() - t0).seconds();); (47, TBB, graph_time = (tbb::tick_count::now() - t0).seconds();); (32, TBB, serial_time = (tbb::tick_count::now() - t0).seconds();)"}
{"predict": "(10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (31, MPI, MPI_Barrier(MPI_COMM_WORLD);); (4, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)", "label": "(10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(102, TBB, TestFuncDefinitionPresence( parallel_deterministic_reduce, (const tbb::blocked_range<int>&, Body2&, const tbb::simple_partitioner&, tbb::task_group_context&), void );); (33, TBB, TestTypeDefinitionPresence3(flow::async_node<int, int, tbb::flow::queueing_lightweight> );); (99, TBB, TestFuncDefinitionPresence( parallel_for_each, (const intarray&, const Body1&, tbb::task_group_context&), void );); (97, TBB, TestFuncDefinitionPresence( parallel_invoke, (const Body&, const Body&, tbb::task_group_context&), void );); (97, TBB, TestFuncDefinitionPresence( parallel_do, (const intarray&, const Body1a&, tbb::task_group_context&), void );); (28, TBB, TestTypeDefinitionPresence3(flow::multifunction_node<int, intpair, tbb::flow::queueing> );); (31, TBB, TestTypeDefinitionPresence2(flow::join_node<intpair, tbb::flow::key_matching<int> > );); (94, TBB, TestFuncDefinitionPresence( parallel_reduce, (const tbb::blocked_range<int>&, Body2&, const tbb::auto_partitioner&, tbb::task_group_context&), void );); (88, TBB, TestFuncDefinitionPresence( tbb::flow::make_edge, (tbb::flow::sender<Msg>&, tbb::flow::receiver<Msg>&), void );); (88, TBB, TestFuncDefinitionPresence( tbb::flow::remove_edge, (tbb::flow::sender<Msg>&, tbb::flow::receiver<Msg>&), void );); (24, TBB, TestTypeDefinitionPresence2(flow::continue_node<int, tbb::flow::lightweight> );); (40, TBB, TestTypeDefinitionPresence3(flow::function_node<int, int, tbb::flow::rejecting> );); (87, TBB, TestFuncDefinitionPresence( tbb::flow::join, (const tbb::flow::join_policy&, const typename tbb::flow::tuple<int>::type&, const typename tbb::flow::tuple<int>::type&), typename tbb::flow::tuple<int>::type );); (", "label": "(103, TBB, TestFuncDefinitionPresence( parallel_deterministic_reduce, (const tbb::blocked_range<int>&, Body2&, const tbb::simple_partitioner&, tbb::task_group_context&), void );); (32, TBB, TestTypeDefinitionPresence3(flow::async_node<int, int, tbb::flow::queueing_lightweight> );); (96, TBB, TestFuncDefinitionPresence( parallel_for_each, (const intarray&, const Body1&, tbb::task_group_context&), void );); (93, TBB, TestFuncDefinitionPresence( parallel_invoke, (const Body&, const Body&, tbb::task_group_context&), void );); (93, TBB, TestFuncDefinitionPresence( parallel_do, (const intarray&, const Body1a&, tbb::task_group_context&), void );); (27, TBB, TestTypeDefinitionPresence3(flow::multifunction_node<int, intpair, tbb::flow::queueing> );); (30, TBB, TestTypeDefinitionPresence2(flow::join_node<intpair, tbb::flow::key_matching<int> > );); (82, TBB, TestFuncDefinitionPresence( parallel_scan, (const tbb::blocked_range<int>&, const int&, const Body3a&, const Body1b&), int );); (75, TBB, TestFuncDefinitionPresence( parallel_for, (const tbb::blocked_range<int>&, const Body2&, const tbb::simple_partitioner&), void );); (79, TBB, TestFuncDefinitionPresence( parallel_scan, (const tbb::blocked_range2d<int>&, Body3&, const tbb::auto_partitioner&), void );); (74, TBB, TestFuncDefinitionPresence( parallel_reduce, (const tbb::blocked_range<int>&, const int&, const Body2a&, const Body1b&), int );); (23, TBB, TestTypeDefinitionPresence2(flow::continue_node<int, tbb::flow::lightweight> );); (73, TBB, TestFuncDefinitionPresence( parallel_reduce, (const tbb::blocked_range<int>&, Body2&, tbb::affinity_partitioner&), void );); (74, TBB, TestFuncDefinitionPresence( parallel_deterministic_reduce, (const tbb::blocked_range<int>&, Body2&, const tbb::static_partitioner&), void );); (14, TBB, typedef tbb::flow::tuple<int, int> intpair;); (33, TBB, TestTypeDefinitionPresence2(flow::composite_node<tbb::flow::tuple<int>, tbb::flow::tuple<int> > );); (15, TBB, TestTypeDefinitionPresence3(flow::function_node<int, int, tbb::flow::rejecting> );); (82, TBB, TestFuncDefinitionPresence( parallel_for, (int, int, const Body1&, const tbb::auto_partitioner&, tbb::task_group_context&), void );); (68, TBB, TestFuncDefinitionPresence( parallel_deterministic_reduce, (const tbb::blocked_range<int>&, const int&, const Body2a&, const Body1b&), int );); (75, TBB, TestFuncDefinitionPresence( parallel_pipeline, (size_t, const tbb::filter_t<void,void>&), void );); (81, TBB, TestFuncDefinitionPresence( parallel_reduce, (const tbb::blocked_range<int>&, Body2&, const tbb::auto_partitioner&, tbb::task_group_context&), void );); (5, TBB, TestFuncDefinitionPresence( flow::make_edge, (tbb::flow::sender<Msg>&, tbb::flow::receiver<Msg>&), void );); (82, TBB, TestFuncDefinitionPresence( parallel_deterministic_reduce, (const tbb::blocked_range<int>&, Body2&, tbb::task_group_context&), void );); (4, TBB, TestFuncDefinitionPresence( flow::remove_edge, (tbb::flow::sender<Msg>&, tbb::flow::receiver<Msg>&), void );); (76, TBB, TestFuncDefinitionPresence( parallel_for, (int, int, const Body1&, tbb::task_group_context&), void );); (77, TBB, TestFuncDefinitionPresence( parallel_reduce, (const tbb::blocked_range<int>&, Body2&, tbb::task_group_context&), void );); (9, TBB, TestTypeDefinitionPresence2(flow::join_node<intpair, tbb::flow::reserving> );)"}
{"predict": "(153, OpenMP, __kmp_omp_schedule_set(h, sched, chunk);)", "label": "(30, OpenMP, __kmp_omp_schedule_restore();)"}
{"predict": "(7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &comm_size);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &comm_rank);); (57, MPI, MPI_Comm_split(MPI_COMM_WORLD, (comm_rank == comm_size - 1) ? 0 : 1, 0, &comm);); (68, MPI, MPI_Comm_split(MPI_COMM_WORLD, 0, rand(), &comm);); (8, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (31, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)", "label": "(7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &comm_size);); (69, MPI, MPI_Comm_split(MPI_COMM_WORLD, 0, rand(), &comm);); (57, MPI, MPI_Comm_split(MPI_COMM_WORLD, (comm_rank == comm_size - 1) ? 0 : 1, 0, &comm);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &comm_rank);); (58, MPI, MPI_Comm_free(&comm);); (7, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (30, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(18, CUDA, checkRuntime(cudaStreamCreate(&stream));); (80, CUDA, checkRuntime(cudaMemcpyAsync(output_data_host, output_data_device, sizeof(float) * output_numel, cudaMemcpyDeviceToHost, stream));); (80, CUDA, checkRuntime(cudaStreamSynchronize(stream));); (58, CUDA, checkRuntime(cudaMemcpyAsync(input_data_device, input_data_host, input_numel * sizeof(float), cudaMemcpyHostToDevice, stream));); (24, CUDA, checkRuntime(cudaMallocHost(&input_data_host, input_numel * sizeof(float)));); (24, CUDA, checkRuntime(cudaMalloc(&input_data_device, input_numel * sizeof(float)));); (171, CUDA, checkRuntime(cudaFree(input_data_device));); (168, CUDA, checkRuntime(cudaFreeHost(input_data_host));); (170, CUDA, checkRuntime(cudaFree(output_data_device));); (63, CUDA, checkRuntime(cudaMalloc(&output_data_device, sizeof(float) * output_numel));); (-5, CUDA, auto engine = make_nvshared(runtime->deserializeCudaEngine(engine_data.data(), engine_data.size()));); (60, CUDA, checkRuntime(cudaMallocHost(&output_data_host, sizeof(float) * output_numel));); (164, CUDA, checkRuntime(cudaFreeHost(output_data_host));); (161, CUDA, checkRuntime(cudaStreamDestroy(stream));)", "label": "(18, CUDA, checkRuntime(cudaStreamCreate(&stream));); (80, CUDA, checkRuntime(cudaMemcpyAsync(output_data_host, output_data_device, sizeof(float) * output_numel, cudaMemcpyDeviceToHost, stream));); (80, CUDA, checkRuntime(cudaStreamSynchronize(stream));); (58, CUDA, checkRuntime(cudaMemcpyAsync(input_data_device, input_data_host, input_numel * sizeof(float), cudaMemcpyHostToDevice, stream));); (24, CUDA, checkRuntime(cudaMallocHost(&input_data_host, input_numel * sizeof(float)));); (24, CUDA, checkRuntime(cudaMalloc(&input_data_device, input_numel * sizeof(float)));); (191, CUDA, checkRuntime(cudaFree(input_data_device));); (188, CUDA, checkRuntime(cudaFreeHost(input_data_host));); (190, CUDA, checkRuntime(cudaFree(output_data_device));); (63, CUDA, checkRuntime(cudaMalloc(&output_data_device, sizeof(float) * output_numel));); (-5, CUDA, auto engine = make_nvshared(runtime->deserializeCudaEngine(engine_data.data(), engine_data.size()));); (60, CUDA, checkRuntime(cudaMallocHost(&output_data_host, sizeof(float) * output_numel));); (184, CUDA, checkRuntime(cudaFreeHost(output_data_host));); (181, CUDA, checkRuntime(cudaStreamDestroy(stream));)"}
{"predict": "(13, OpenMP, #pragma omp parallel for); (45, OpenMP, #pragma omp critical); (71, OpenMP, #pragma omp atomic)", "label": "(13, OpenMP, #pragma omp parallel for); (72, OpenMP, #pragma omp atomic); (43, OpenMP, #pragma omp critical)"}
{"predict": "(3, OpenMP, omp_set_default_device(0);); (32, OpenMP, #pragma omp target teams distribute parallel for is_device_ptr(DATA) nowait); (15, OpenMP, #pragma omp target teams distribute parallel for is_device_ptr(DATA)); (35, OpenMP, #pragma omp taskwait); (18, OpenMP, t1=omp_get_wtime();); (3, OpenMP, #pragma omp parallel for); (-2, OpenMP, DATA = (double*)   omp_target_alloc(N*sizeof(double), omp_get_default_device()) ;); (22, OpenMP, t2=omp_get_wtime();)", "label": "(4, OpenMP, omp_set_default_device(0);); (33, OpenMP, #pragma omp target teams distribute parallel for is_device_ptr(DATA) nowait); (15, OpenMP, #pragma omp target teams distribute parallel for is_device_ptr(DATA)); (35, OpenMP, #pragma omp taskwait); (19, OpenMP, t1=omp_get_wtime();); (4, OpenMP, #pragma omp parallel for); (-1, OpenMP, DATA = (double*)   omp_target_alloc(N*sizeof(double), omp_get_default_device()) ;); (22, OpenMP, t2=omp_get_wtime();)"}
{"predict": "(21, CUDA, checkCudaErrors(cudaSetDevice(shm->devices[id]));); (74, CUDA, checkCudaErrors(cudaStreamSynchronize(stream));); (88, CUDA, checkCudaErrors(cudaEventDestroy(events[i]));); (86, CUDA, checkCudaErrors(cudaFree(ptrs[i]));); (18, CUDA, checkCudaErrors(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));); (54, CUDA, checkCudaErrors(cudaEventRecord(events[bufferId], stream));); (57, CUDA, checkCudaErrors(cudaEventSynchronize(shm->event[0]));); (14, CUDA, checkCudaErrors(cudaGetDeviceProperties(&prop, shm->devices[id]));); (87, CUDA, checkCudaErrors(cudaStreamDestroy(stream));); (66, CUDA, checkCudaErrors(cudaEventRecord(shm->event[0], stream));)", "label": "(21, CUDA, checkCudaErrors(cudaSetDevice(shm->devices[id]));); (54, CUDA, checkCudaErrors(cudaStreamWaitEvent(stream, events[bufferId], 0));); (71, CUDA, checkCudaErrors(cudaStreamWaitEvent(stream, events[id], 0));); (56, CUDA, checkCudaErrors(cudaGetLastError());); (92, CUDA, checkCudaErrors(cudaStreamDestroy(stream));); (18, CUDA, checkCudaErrors(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));); (86, CUDA, checkCudaErrors(cudaIpcCloseMemHandle(ptrs[i]));); (86, CUDA, checkCudaErrors(cudaEventDestroy(events[i]));); (53, CUDA, checkCudaErrors(cudaEventRecord(events[bufferId], stream));); (68, CUDA, checkCudaErrors(cudaStreamSynchronize(stream));); (12, CUDA, checkCudaErrors(cudaGetDeviceProperties(&prop, shm->devices[id]));)"}
{"predict": "(70, MPI, MPI_Irecv(buf_recv, shuffile_mpi_buf_size, MPI_BYTE, rank_recv, 0, comm, &request[0]);); (71, MPI, MPI_Waitall(2, request, status);); (81, MPI, MPI_Send(buf_send, nread, MPI_BYTE, rank_recv, 0, comm);); (88, MPI, nwrite = status[0].bytes_received;)", "label": "(82, MPI, MPI_Wait(&request[1], &status[1]);); (90, MPI, MPI_Wait(&request[0], &status[0]);); (90, MPI, MPI_Get_count(&status[0], MPI_BYTE, &nwrite);); (66, MPI, MPI_Irecv(buf_recv, shuffile_mpi_buf_size, MPI_BYTE, rank_recv, 0, comm, &request[0]);); (77, MPI, MPI_Isend(buf_send, nread, MPI_BYTE, rank_send, 0, comm, &request[1]);)"}
{"predict": "(162, OpenCL, clGetDeviceInfo(vkGPU->device, CL_DEVICE_VENDOR_ID, sizeof(cl_int), &vendorID, 0);); (77, CUDA, res = cudaMalloc((void**)&buffer, bufferSize);); (86, OpenCL, buffer = clCreateBuffer(vkGPU->context, CL_MEM_READ_WRITE, bufferSize, 0, &res);)", "label": "(162, OpenCL, clGetDeviceInfo(vkGPU->device, CL_DEVICE_VENDOR_ID, sizeof(cl_int), &vendorID, 0);); (77, CUDA, res = cudaMalloc((void**)&buffer, bufferSize);); (86, OpenCL, buffer = clCreateBuffer(vkGPU->context, CL_MEM_READ_WRITE, bufferSize, 0, &res);)"}
{"predict": "(15, OpenMP, #pragma omp teams distribute reduction +); (105, OpenMP, #pragma omp teams distribute reduction(+ : m)); (66, OpenMP, #pragma omp teams distribute reduction(- : da)); (58, OpenMP, #pragma omp teams distribute reduction(+ : ba)); (49, OpenMP, #pragma omp teams distribute reduction(+ : a, b, c, d, f)); (79, OpenMP, #pragma omp teams distribute reduction(&& : S2::S2sc)); (73, OpenMP, #pragma omp teams distribute reduction(&& : S2::S2s)); (95, OpenMP, #pragma omp teams distribute reduction(+ : fl)); (26, OpenMP, #pragma omp teams distribute reduction(| : argc,); (85, OpenMP, #pragma omp parallel private(k)); (62, OpenMP, #pragma omp teams distribute reduction(^ : z, fl)); (68, OpenMP, #pragma omp teams distribute reduction(&& : S1)); (38, OpenMP, #pragma omp teams distribute reduction(min : a, b, c, d, f)); (44, OpenMP, #pragma omp teams distribute reduction(max : h.b)); (78, OpenMP, #pragma omp parallel reduction(min : i)); (80, OpenMP, #pragma omp teams distribute reduction(max : j)); (46, OpenMP, #pragma omp teams distribute reduction(+ : h, k, B::x)); (71, OpenMP, #pragma omp teams distribute private(i), reduction(+ : j), reduction(+ : q)); (28, OpenMP, #pragma omp teams distribute reduction(^ : S1)); (19, OpenMP, #pragma omp teams distribute reduction(|| : argc > 0 ? argv[1] : argv[2])); (1, OpenMP, #pragma omp target); (70, OpenMP, #pragma omp teams distribute reduction(+ : p), reduction(+ : p)); (65, OpenMP, #pragma omp teams distribute reduction(+ : h)); (31, OpenMP, #pragma omp teams distribute reduction(* : ca)); (21, OpenMP, #pragma omp teams distribute reduction(~ : argc)); (-2, OpenMP, #pragma omp teams distribute reduction(); (36, OpenMP, #pragma omp teams distribute reduction(&& : argc)); (", "label": "(113, OpenMP, #pragma omp teams distribute reduction(task, + : m)); (15, OpenMP, #pragma omp teams distribute reduction +); (29, OpenMP, #pragma omp teams distribute reduction(\\)); (37, OpenMP, #pragma omp teams distribute reduction(|| : argc > 0 ? argv[1] : argv[2])); (106, OpenMP, #pragma omp teams distribute reduction(+ : m)); (20, OpenMP, #pragma omp teams distribute reduction()); (6, OpenMP, #pragma omp target); (42, OpenMP, #pragma omp teams distribute reduction(^ : S1)); (44, OpenMP, #pragma omp teams distribute reduction(+ : a, b, c, d, f)); (67, OpenMP, #pragma omp teams distribute reduction(&& : S2::S2sc)); (57, OpenMP, #pragma omp teams distribute reduction(- : da)); (32, OpenMP, #pragma omp teams distribute reduction(~ : argc)); (10, OpenMP, #pragma omp teams distribute reduction(-); (45, OpenMP, #pragma omp teams distribute reduction(max : h.b)); (5, OpenMP, #pragma omp teams distribute reduction(); (75, OpenMP, #pragma omp parallel private(k)); (18, OpenMP, #pragma omp teams distribute reduction(foo : argc); (38, OpenMP, #pragma omp teams distribute reduction(min : a, b, c, d, f)); (70, OpenMP, #pragma omp teams distribute private(i), reduction(+ : j), reduction(+ : q)); (79, OpenMP, #pragma omp teams distribute reduction(+ : r)); (41, OpenMP, #pragma omp teams distribute reduction(+ : ba)); (25, OpenMP, #pragma omp teams distribute reduction(&& : argc)); (81, OpenMP, #pragma omp teams distribute reduction(max : j)); (78, OpenMP, #pragma omp parallel reduction(min : i)); (4, OpenMP, #pragma omp teams distribute reduction(*)); (67, OpenMP, #pragma omp teams distribute reduction(+ : p), reduction(+ : p)); (44, OpenMP, #pragma omp teams distribute reduction(^ : fl)); (58, OpenMP, #pragma omp teams distribute reduction(+ : o)); (54, OpenMP, #pragma omp teams distribute reduction(+ : h, k, B::x)); (35, OpenMP, #pragma omp teams distribute reduction(* : ca)); (76, OpenMP, #pragma omp teams distribute reduction(+ : fl, z)); (48, OpenMP, #pragma omp teams distribute reduction(& : e, g)); (68, OpenMP, #pragma omp parallel shared(i)); (4, OpenMP, #pragma omp teams distribute reduction(| : argc,); (-21, OpenMP, #pragma omp teams distribute reduction); (38, OpenMP, #pragma omp teams distribute reduction(&& : S2::S2s))"}
{"predict": "(21, CUDA, extern __shared__ float entire_buffer[];)", "label": "(21, CUDA, extern __shared__ float entire_buffer[];)"}
{"predict": "(27, TBB, tbb::parallel_reduce( vs.range(1), pvrb );); (63, TBB, tbb::parallel_for ( tbb::blocked_range<int> (0, N, RANGE_MIN), parallel_vector_for_body<T,Allocator>( vvs, allocator_name ) );); (66, TBB, tbb::parallel_reduce( vvs.range(1), pvrb2 );); (33, TBB, tbb::flattened2d<ets_type> fvs = flatten2d(vs);); (53, TBB, ets_type vvs( typename tbb::zero_allocator<T>::value, 25 );); (57, TBB, tbb::flattened2d<ets_type> fvvs = flatten2d(vvs);); (1, TBB, typedef std::vector<T, tbb::tbb_allocator<T> > container_type;); (1, TBB, typedef typename tbb::enumerable_thread_specific< container_type, Allocator<container_type> > ets_type;); (11, TBB, if (Verbose && t == 0) t0 = tbb::tick_count::now();); (-4, TBB, tbb::task_scheduler_init init(p);); (-4, TBB, tbb::tick_count t0;); (66, TBB, REMARK(\"done\\nparallel %s with allocator %s on %d thread(s) -- %g [sec]\\n\", test_name, allocator_name, p, ( tbb::tick_count::now() - t0).seconds());); (19, TBB, tbb::flattened2d<ets_type> fvs2 = flatten2d(vs2);); (19, TBB, tbb::flattened2d<ets_type> fvs3 = flatten2d(vs3);)", "label": "(30, TBB, tbb::parallel_reduce ( vs.range(1), pvrb );); (60, TBB, tbb::parallel_for ( tbb::blocked_range<int> (0, N, RANGE_MIN), parallel_vector_for_body<T,Allocator>( vvs, allocator_name ) );); (1, TBB, typedef std::vector<T, tbb::tbb_allocator<T> > container_type;); (5, TBB, tbb::task_scheduler_init init(p);); (10, TBB, if (Verbose && t == 0) t0 = tbb::tick_count::now();); (64, TBB, tbb::flattened2d<ets_type> fvvs = flatten2d(vvs);); (59, TBB, tbb::parallel_reduce ( vvs.range(1), pvrb2 );); (-5, TBB, tbb::tick_count t0;); (30, TBB, tbb::flattened2d<ets_type> fvs = flatten2d(vs);); (50, TBB, ets_type vvs( typename container_type::size_type(25), minus_one, tbb::tbb_allocator<T>() );); (-6, TBB, typedef tbb::enumerable_thread_specific< container_type, Allocator<container_type> > ets_type;); (66, TBB, REMARK(\"done\\nparallel %s, %d, %g, %g\\n\", test_name, p, result_value, ( tbb::tick_count::now() - t0).seconds());)"}
{"predict": "(21, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (26, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (127, MPI, MPI_Test(&req, &completed, MPI_STATUS_IGNORE);); (235, MPI, MPI_Finalize();); (23, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (173, MPI, MPI_Allreduce(MPI_IN_PLACE, &duration, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);); (30, MPI, MPI_Barrier(MPI_COMM_WORLD);); (219, MPI, MPI_Reduce(&iters, &min_iters, 1, MPI_UINT64_T, MPI_MIN, 0, MPI_COMM_WORLD);); (220, MPI, MPI_Reduce(&iters, &max_iters, 1, MPI_UINT64_T, MPI_MAX, 0, MPI_COMM_WORLD);); (16, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (214, MPI, MPI_Reduce(&iters, &sum_iters, 1, MPI_UINT64_T, MPI_SUM, 0, MPI_COMM_WORLD);); (112, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (209, MPI, MPI_Reduce(&iters, &avg_iters, 1, MPI_UINT64_T, MPI_SUM, 0, MPI_COMM_WORLD);); (132, MPI, double duration = std::chrono::duration<double>(end - begin).count();); (162, MPI, MPI_Allreduce(MPI_IN_PLACE, &latency, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);); (160, MPI, MPI_Allreduce(MPI_IN_PLACE, &bw, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);)", "label": "(21, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (193, MPI, MPI_Finalize();); (25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (23, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (124, MPI, MPI_Test(&req, &completed, MPI_STATUS_IGNORE);)"}
{"predict": "(129, MPI, MPI_Allgather(&end_offset, 1, ADIO_OFFSET, end_offsets, 1, ADIO_OFFSET, fd->comm);); (60, MPI, MPI_Comm_rank(fd->comm, &myrank);); (126, MPI, MPI_Allgather(&start_offset, 1, ADIO_OFFSET, st_offsets, 1, ADIO_OFFSET, fd->comm);); (128, MPI, MPI_Allgather(&count_sizes, 1, ADIO_OFFSET, count_sizes, 1, ADIO_OFFSET, fd->comm);)", "label": "(141, MPI, MPI_Allgather(&end_offset, 1, ADIO_OFFSET, end_offsets, 1, ADIO_OFFSET, fd->comm);); (59, MPI, MPI_Comm_rank(fd->comm, &myrank);); (138, MPI, MPI_Allgather(&start_offset, 1, ADIO_OFFSET, st_offsets, 1, ADIO_OFFSET, fd->comm);); (395, MPI, MPI_Type_size_x(datatype, &size);); (94, MPI, MPI_Type_size_x(datatype, &buftype_size);)"}
{"predict": "(67, OpenMP, kmp_gomp_depends_info_t gomp_depends(depend);); (101, OpenMP, __kmpc_omp_wait_deps(&loc, gtid, ndeps, dep_list, 0, NULL);); (74, OpenMP, __kmpc_omp_task(&loc, gtid, task);); (71, OpenMP, __kmpc_omp_task_with_deps(&loc, gtid, task, ndeps_cnv, dep_list, 0, NULL);)", "label": "(67, OpenMP, kmp_gomp_depends_info_t gomp_depends(depend);); (100, OpenMP, __kmpc_omp_wait_deps(&loc, gtid, ndeps, dep_list, 0, NULL);); (74, OpenMP, __kmpc_omp_task(&loc, gtid, task);); (71, OpenMP, __kmpc_omp_task_with_deps(&loc, gtid, task, ndeps_cnv, dep_list, 0, NULL);)"}
{"predict": "(221, OpenMP, omp_target_free(m_gate, host);); (222, OpenMP, omp_target_free(Vm, host);); (141, OpenMP, #pragma omp target teams distribute parallel for thread_limit(128) map(to: m_gate[:nCells], Vm[:nCells]) map(tofrom: syncarray)); (168, OpenMP, #pragma omp target teams distribute parallel for thread_limit(128) map(tofrom: m_gate[:nCells], Vm[:nCells])); (64, OpenMP, #pragma omp target enter data map(to: m_gate[:nCells], Vm[:nCells])); (214, OpenMP, omp_target_free(syncarray, host);); (19, OpenMP, rank0_printf_timestamp(\"OpenMP kernel using %d threads\\n\", omp_get_max_threads());); (68, OpenMP, double* syncarray = (double*)omp_target_alloc(4096* sizeof(double), host);); (129, OpenMP, #pragma omp target teams distribute parallel for thread_limit(128)); (212, OpenMP, omp_target_free(Vm, host);); (137, OpenMP, #pragma omp target update to(m_gate[:nCells], Vm[:nCells])); (141, OpenMP, #pragma omp target update from(m_gate[:nCells], Vm[:nCells])); (57, OpenMP, #pragma omp target enter data map(to: syncarray[:4096])); (201, OpenMP, omp_target_free(m_gate, host);); (171, OpenMP, #pragma omp target teams distribute parallel for thread_limit(128) map(from: syncarray[:4096])); (11, OpenMP, double host = omp_get_initial_device();); (14, OpenMP, rank0_printf_timestamp(\"Rush Larsen %s\\n\", VARIANT_DESC);); (45, OpenMP, #pragma omp target enter data map(to: Vm[:nCells])); (42, OpenMP, #pragma omp target enter data map(to: m_gate[:nCells])); (201, OpenMP, omp_target_free(syncarray, host);); (171, OpenMP, #pragma omp target teams distribute parallel for thread_limit(128) map", "label": "(162, OpenMP, #pragma omp target exit data map(delete:Vm[:nCells])); (66, OpenMP, #pragma omp target enter data map(to: m_gate[:nCells])); (149, OpenMP, #pragma omp target update from (m_gate[0:nCells])); (65, OpenMP, #pragma omp target enter data map(to: Vm[:nCells])); (102, OpenMP, #pragma omp target teams distribute parallel for); (156, OpenMP, #pragma omp target exit data map(delete:m_gate[:nCells]))"}
{"predict": "(26, MPI, MPI_Info_get_nkeys(infos[0], &nkeys);); (38, MPI, MPI_Info_get_nkeys(infos[i], &nkeys);); (75, MPI, MPI_Info_free(&infos[i]);); (8, MPI, MPI_Info_create(&infos[i]);)", "label": "(10, MPI, MPI_Info_create(&infos[i]);); (57, MPI, MPI_Info_get(infos[i], key, sizeof(value), value, &flag);); (47, MPI, MPI_Info_get_nthkey(infos[i], j, key);); (22, MPI, MPI_Info_get_nkeys(infos[0], &nkeys);); (33, MPI, MPI_Info_get_nkeys(infos[i], &nkeys);); (71, MPI, MPI_Info_free(&infos[i]);)"}
{"predict": "(29, CUDA, checkCudaErrors(cudaGetDeviceCount(&GPU_N));); (219, CUDA, checkCudaErrors(cufftXtFree(d_out_filter_kernel));); (120, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, whichGPUs[i]));); (216, CUDA, checkCudaErrors(cufftXtFree(d_out_signal));); (100, CUDA, checkCudaErrors(cufftCreate(&plan_input));); (39, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, i));); (211, CUDA, checkCudaErrors(cufftXtFree(d_signal));); (216, CUDA, checkCudaErrors(cufftDestroy(plan_input));); (209, CUDA, checkCudaErrors(cufftXtFree(d_filter_kernel));)", "label": "(29, CUDA, checkCudaErrors(cudaGetDeviceCount(&GPU_N));); (219, CUDA, checkCudaErrors(cufftXtFree(d_out_filter_kernel));); (120, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, whichGPUs[i]));); (216, CUDA, checkCudaErrors(cufftXtFree(d_out_signal));); (100, CUDA, checkCudaErrors(cufftCreate(&plan_input));); (40, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, i));); (211, CUDA, checkCudaErrors(cufftXtFree(d_signal));); (216, CUDA, checkCudaErrors(cufftDestroy(plan_input));); (210, CUDA, checkCudaErrors(cufftXtFree(d_filter_kernel));)"}
{"predict": "(58, MPI, MPI_Win_fence(0, win);); (30, MPI, MPI_Type_commit(&originType);); (40, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (58, MPI, err = MPI_Win_fence(0, win);); (80, MPI, MPI_Win_free(&win);); (26, MPI, MPI_Type_indexed(2, counts, displs, MPI_INT, &originType);); (77, MPI, MPI_Type_free(&originType);); (26, MPI, MPI_Type_get_extent(MPI_INT, &tmp_lb, &extent);); (8, MPI, MPI_Comm_size(comm, &size);); (52, MPI, MPI_Win_create(recvBuf, totsize * extent, extent, MPI_INFO_NULL, comm, &win);); (7, MPI, MPI_Comm_rank(comm, &rank);); (53, MPI, MPI_Win_fence(0, win);); (48, MPI, MPI_Win_set_errhandler(win, MPI_ERRORS_RETURN);); (53, MPI, err = MPI_Put(srcBuf, 1, originType, dest, 0, totsize, MPI_INT, win);)", "label": "(56, MPI, MPI_Type_get_extent(MPI_INT, &tmp_lb, &extent);); (33, MPI, MPI_Comm_size(comm, &size);); (55, MPI, MPI_Win_create(recvBuf, totsize * extent, extent, MPI_INFO_NULL, comm, &win);); (24, MPI, MPI_Type_indexed(2, counts, displs, MPI_INT, &originType);); (54, MPI, MPI_Win_fence(0, win);); (23, MPI, MPI_Type_commit(&originType);); (77, MPI, MPI_Win_free(&win);); (75, MPI, MPI_Type_free(&originType);); (58, MPI, err = MPI_Win_fence(0, win);); (53, MPI, MPI_Win_set_errhandler(win, MPI_ERRORS_RETURN);); (33, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (22, MPI, MPI_Comm_rank(comm, &rank);); (52, MPI, err = MPI_Put(srcBuf, 1, originType, dest, 0, totsize, MPI_INT, win);)"}
{"predict": "(26, MPI, err = MPI_Type_vector(count, 2, 3, MPI_INT, &vectype);); (45, MPI, err = MPI_Type_size(newtype, &size);); (76, MPI, MPI_Type_free(&newtype);); (32, MPI, err = MPI_Type_create_indexed_block(count, 1, disp, vectype, &newtype);); (22, MPI, MPI_Type_size(MPI_INT, &int_size);); (55, MPI, MPI_Type_commit(&newtype);); (73, MPI, MPI_Type_free(&vectype);)", "label": "(35, MPI, err = MPI_Type_create_indexed_block(count, 1, disp, vectype, &newtype);); (42, MPI, MPI_Type_size(MPI_INT, &int_size);); (58, MPI, MPI_Type_commit(&newtype);); (24, MPI, err = MPI_Type_vector(2, 1, 2, MPI_INT, &vectype);); (41, MPI, err = MPI_Type_size(newtype, &size);); (74, MPI, MPI_Type_free(&newtype);); (72, MPI, MPI_Type_free(&vectype);)"}
{"predict": "(8, MPI, tim::settings::mpi_init()       = false;); (8, MPI, tim::settings::mpi_finalize()   = false;)", "label": "(8, MPI, tim::settings::mpi_init()       = false;); (8, MPI, tim::settings::mpi_finalize()   = false;)"}
{"predict": "(18, OpenMP, #pragma omp parallel); (0, OpenMP, int nThreads = omp_get_max_threads();)", "label": "(19, OpenMP, #pragma omp parallel); (0, OpenMP, int nThreads = omp_get_max_threads();)"}
{"predict": "(77, MPI, MPI_Group_free(&g1);); (105, MPI, MPI_Group_range_incl(g1, 1, range, &g4);); (24, MPI, MPI_Group_incl(g1, 3, ranks, &g45);); (72, MPI, MPI_Group_size(g1, &g1size);); (167, MPI, MPI_Group_compare(g7, g1, &result);); (147, MPI, MPI_Group_range_excl(g1, 1, range, &g5);); (35, MPI, MPI_Group_translate_ranks(g45, 3, rin, selfgroup, rout);); (111, MPI, MPI_Group_compare(g4, g5, &result);); (107, MPI, MPI_Group_range_excl(g1, 1, range, &g6);); (16, MPI, MPI_Group_size(g45, &size);); (110, MPI, MPI_Group_compare(g6, g7, &result);); (136, MPI, MPI_Group_range_incl(g1, 1, range, &g4);); (75, MPI, MPI_Group_excl(g1, g1size, lranks, &g6);); (135, MPI, MPI_Group_compare(g45, g6, &result);); (-4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myrank);); (106, MPI, MPI_Group_compare(g5, g6, &result);); (74, MPI, MPI_Group_free(&g45);); (19, MPI, MPI_Group_rank(g45, &rank);); (33, MPI, MPI_Group_translate_ranks(selfgroup, size, rin, g1, rout);); (-11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (148, MPI, MPI_Group_free(&g2);); (148, MPI, MPI_Group_free(&g3);); (153, MPI, MPI_Group_free(&g7);); (95, MPI, MPI_Group_compare(g1, MPI_GROUP_EMPTY, &result);); (143, MPI, MPI_Group_free(&g4);); (145, MPI, MPI_Group_free(&g6);); (3", "label": "(84, MPI, MPI_Group_excl(g1, g1size, lranks, &g6);); (27, MPI, MPI_Group_size(g2, &size);); (41, MPI, MPI_Group_translate_ranks(g2, 3, rin, g1, rout);); (70, MPI, MPI_Group_free(&selfgroup);); (105, MPI, MPI_Group_free(&g2);); (24, MPI, MPI_Group_rank(g2, &rank);); (117, MPI, MPI_Group_compare(MPI_GROUP_EMPTY, g4, &result);); (123, MPI, MPI_Group_free(&g45);); (113, MPI, MPI_Group_union(g4, g5, &g45);); (105, MPI, MPI_Group_range_excl(g1, 1, range, &g5);); (109, MPI, MPI_Group_range_incl(g1, 1, range, &g4);); (118, MPI, MPI_Group_free(&g5);); (-1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (12, MPI, MPI_Group_incl(g1, 3, ranks, &g2);); (114, MPI, MPI_Group_free(&g4);); (140, MPI, MPI_Group_free(&g1);); (72, MPI, MPI_Group_free(&g6);); (62, MPI, MPI_Group_size(g1, &g1size);); (34, MPI, MPI_Comm_group(MPI_COMM_SELF, &selfgroup);); (-10, MPI, MPI_Comm_group(MPI_COMM_WORLD, &g1);); (-10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myrank);); (41, MPI, MPI_Group_translate_ranks(g1, size, rin, selfgroup, rout);); (32, MPI, MPI_Group_translate_ranks(selfgroup, 1, rin, g1, rout);)"}
{"predict": "(232, MPI, MPI_Finalize();); (35, MPI, MPI_Init_thread( &argc, &argv, MPI_THREAD_MULTIPLE, &omp_mpi_level);)", "label": "(232, MPI, MPI_Finalize();); (35, MPI, MPI_Init_thread( &argc, &argv, MPI_THREAD_MULTIPLE, &omp_mpi_level);)"}
{"predict": "(19, OpenMP, #pragma omp teams distribute parallel for simd linear (foo()); (56, OpenMP, #pragma omp teams distribute parallel for simd linear(val(e, g))); (37, OpenMP, #pragma omp teams distribute parallel for simd linear (val argc); (33, OpenMP, #pragma omp teams distribute parallel for simd linear (val (argc,); (6, OpenMP, #pragma omp teams distribute parallel for simd linear (); (20, OpenMP, #pragma omp teams distribute parallel for simd linear ()); (57, OpenMP, #pragma omp teams distribute parallel for simd linear(uval(i))); (15, OpenMP, #pragma omp teams distribute parallel for simd linear (argc); (44, OpenMP, #pragma omp teams distribute parallel for simd linear (argv[1])); (57, OpenMP, #pragma omp teams distribute parallel for simd linear(ref(j))); (27, OpenMP, #pragma omp teams distribute parallel for simd linear (S1)); (-6, OpenMP, #pragma omp target); (47, OpenMP, #pragma omp teams distribute parallel for simd linear(addrs(e, g))); (31, OpenMP, #pragma omp teams distribute parallel for simd linear (argc : 5)); (-4, OpenMP, #pragma omp teams distribute parallel for simd linear); (6, OpenMP, #pragma omp teams distribute parallel for simd linear (argc > 0 ? argv[1] : argv[2])); (4, OpenMP, #pragma omp teams distribute parallel for simd linear (argc,)", "label": "(34, OpenMP, #pragma omp teams distribute parallel for simd linear (argc : 5) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (19, OpenMP, #pragma omp teams distribute parallel for simd linear (argc); (27, OpenMP, #pragma omp teams distribute parallel for simd linear (argc > 0 ? argv[1] : argv[2])); (43, OpenMP, #pragma omp teams distribute parallel for simd linear (argv[1])); (38, OpenMP, #pragma omp teams distribute parallel for simd linear (a, b:B::ib)); (55, OpenMP, #pragma omp teams distribute parallel for simd linear(i)); (19, OpenMP, #pragma omp teams distribute parallel for simd linear (argc,); (8, OpenMP, #pragma omp teams distribute parallel for simd linear ()); (30, OpenMP, #pragma omp teams distribute parallel for simd linear (S1)); (-2, OpenMP, #pragma omp teams distribute parallel for simd linear); (41, OpenMP, #pragma omp teams distribute parallel for simd linear(e, g)); (-5, OpenMP, #pragma omp target); (-1, OpenMP, #pragma omp teams distribute parallel for simd linear (); (42, OpenMP, #pragma omp teams distribute parallel for simd linear(h))"}
{"predict": "(131, MPI, MPI_Bcast(&size2,      1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (131, MPI, MPI_Bcast(&radius,     1, MPI_INT,           root, MPI_COMM_WORLD);); (301, MPI, MPI_Finalize();); (127, MPI, MPI_Bcast(&lsize2,     1, MPI_INT,           root, MPI_COMM_WORLD);); (125, MPI, MPI_Bcast(&lsize,      1, MPI_INT,           root, MPI_COMM_WORLD);); (126, MPI, MPI_Bcast(&size,       1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (275, MPI, MPI_Reduce(&vector_sum, &check_sum, 1, MPI_DOUBLE, MPI_SUM, root, MPI_COMM_WORLD);); (41, MPI, MPI_Init(&argc,&argv);); (228, MPI, MPI_Barrier(MPI_COMM_WORLD);); (41, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (124, MPI, MPI_Bcast(&iterations, 1, MPI_INT,           root, MPI_COMM_WORLD);); (38, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)", "label": "(134, MPI, MPI_Bcast(&size2,      1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (134, MPI, MPI_Bcast(&radius,     1, MPI_INT,           root, MPI_COMM_WORLD);); (316, MPI, MPI_Finalize();); (129, MPI, MPI_Bcast(&lsize2,     1, MPI_INT,           root, MPI_COMM_WORLD);); (127, MPI, MPI_Bcast(&lsize,      1, MPI_INT,           root, MPI_COMM_WORLD);); (128, MPI, MPI_Bcast(&size,       1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (290, MPI, MPI_Reduce(&vector_sum, &check_sum, 1, MPI_DOUBLE, MPI_SUM, root, MPI_COMM_WORLD);); (41, MPI, MPI_Init(&argc,&argv);); (243, MPI, MPI_Barrier(MPI_COMM_WORLD);); (41, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (126, MPI, MPI_Bcast(&iterations, 1, MPI_INT,           root, MPI_COMM_WORLD);); (38, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(44, CUDA, thrust::device_vector<uint32_t> bins_in(NUM_ITEMS);); (52, CUDA, thrust::fill(counts_out.begin(), counts_out.end(), 814920U);); (11, CUDA, thrust::host_vector<uint32_t> h_bins(num_increments);); (11, CUDA, thrust::host_vector<uint32_t> h_increments(num_increments);); (47, CUDA, thrust::device_vector<uint32_t> increments_in(num_increments);); (49, CUDA, thrust::device_vector<uint32_t> counts_out(NUM_ITEMS);); (58, CUDA, thrust::host_vector<uint32_t> host_counts(NUM_ITEMS);)", "label": "(14, CUDA, thrust::host_vector<uint32_t> h_increments(num_increments);); (43, CUDA, thrust::device_vector<uint32_t> increments_in(num_increments);); (53, CUDA, thrust::fill(counts_out.begin(), counts_out.end(), 814920U);); (40, CUDA, thrust::device_vector<uint32_t> bins_in(num_increments);); (41, CUDA, thrust::device_vector<uint32_t> counts_out(NUM_ITEMS);); (47, CUDA, thrust::host_vector<uint32_t> host_counts(num_increments);); (7, CUDA, thrust::host_vector<uint32_t> h_bins(num_increments);)"}
{"predict": "(10, CUDA, extern __shared__ uint64_t buffer[];)", "label": "(10, CUDA, extern __shared__ uint64_t buffer[];)"}
{"predict": "(81, MPI, MPI_Win_lock_all(0, win);); (144, MPI, MPI_Win_unlock_all(win);); (14, MPI, MPI_Comm_size(comm, &size);); (77, MPI, MPI_Win_lock(MPI_LOCK_SHARED, target_rank, 0, win);); (131, MPI, MPI_Win_unlock(target_rank, win);); (157, MPI, MPI_Win_free(&win);); (64, MPI, MPI_Barrier(comm);); (118, MPI, MPI_Win_flush_local_all(win);); (76, MPI, MPI_Win_shared_query(win, target_rank, &targetcount, &targettype, (void *) &result.buf);); (15, MPI, MPI_Comm_rank(comm, &rank);); (129, MPI, MPI_Win_lock(MPI_LOCK_SHARED, orig_rank, 0, win);); (67, MPI, MPI_Win_flush_all(win);); (19, MPI, MPI_Type_size(base_type, &base_type_size);); (148, MPI, MPI_Win_detach(win, orig.buf);); (148, MPI, MPI_Win_detach(win, result.buf);); (67, MPI, MPI_Win_flush(orig_rank, win);); (140, MPI, MPI_Barrier(comm);); (119, MPI, MPI_Win_unlock(orig_rank, win);); (68, MPI, MPI_Win_unlock_all(win);); (139, MPI, MPI_Type_free(&resulttype);); (137, MPI, MPI_Type_free(&targettype);); (19, MPI, MPI_Type_commit(&resulttype);); (17, MPI, MPI_Type_contiguous(count, base_type, &resulttype);); (132, MPI, MPI_Type_free(&origtype);); (61, MPI, MPI_Win_lock_all(0, win);); (141, MPI, MPI_Win_detach(win, target.buf);); (10, MPI, MPI_Type_contiguous(count, base_type, &targettype);); (12, MPI, MPI_Type_commit(&targettype);); (5, MPI, MPI_Type_contiguous(count, base_type, &origtype);); (128, MPI, MPI_Win_free(&resultwin);); (13", "label": "(163, MPI, MPI_Win_unlock(rank, win);); (22, MPI, MPI_Comm_size(comm, &size);); (125, MPI, MPI_Barrier(comm);); (134, MPI, MPI_Win_unlock(target_rank, win);); (135, MPI, MPI_Win_unlock_all(win);); (76, MPI, MPI_Win_lock(MPI_LOCK_SHARED, target_rank, 0, win);); (120, MPI, MPI_Win_flush_all(win);); (76, MPI, MPI_Win_lock_all(0, win);); (150, MPI, MPI_Win_lock(MPI_LOCK_SHARED, rank, 0, win);); (111, MPI, MPI_Win_flush_local_all(win);); (12, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(34, CUDA, checkCudaErrors(cudaMalloc((void **) &dData, size));); (57, CUDA, checkCudaErrors(cudaBindTextureToArray(tex, cuArray));); (72, CUDA, getLastCudaError(\"Kernel execution failed\");); (124, CUDA, checkCudaErrors(cudaFreeArray(cuArray));); (122, CUDA, checkCudaErrors(cudaFree(dData));)", "label": "(127, CUDA, checkCudaErrors(cudaFreeArray(cuArray));); (33, CUDA, checkCudaErrors(cudaMalloc((void **) &dData, size));); (56, CUDA, checkCudaErrors(cudaBindTextureToArray(tex, cuArray, channelDesc));); (63, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (71, CUDA, getLastCudaError(\"Kernel execution failed\");); (-3, CUDA, int devID = findCudaDevice(argc, (const char **) argv);); (120, CUDA, checkCudaErrors(cudaFree(dData));)"}
{"predict": "(94, CUDA, cudaCheckErrors(\"Graph instantiation failed\");); (14, CUDA, cudaStreamCreateWithFlags(&streams[i], cudaStreamNonBlocking);); (46, CUDA, cudaGraphCreate(&graph, 0);); (62, CUDA, cudaCheckErrors(\"Event record failed\");); (54, CUDA, cudaCheckErrors(\"Stream begin capture failed\");); (30, CUDA, cudaMalloc((void**) &d_x, N * sizeof(float));); (64, CUDA, cudaCheckErrors(\"Kernel c failed\");); (97, CUDA, cudaGraphLaunch(instance, 0);); (114, CUDA, cudaDeviceSynchronize();); (116, CUDA, cudaMemcpy(h_y, d_y, N, cudaMemcpyDeviceToHost);); (98, CUDA, cudaCheckErrors(\"Graph launch failed\");); (85, CUDA, cudaCheckErrors(\"Kernel b failed\");); (56, CUDA, cudaCheckErrors(\"Kernel a failed\");); (19, CUDA, cudaMemcpy(d_y, h_y, N, cudaMemcpyHostToDevice);); (67, CUDA, cudaCheckErrors(\"Stream wait event failed\");); (71, CUDA, cudaGraphGetNodes(graph, nodes, &numNodes);); (49, CUDA, cudaStreamWaitEvent(streams[1], event1);); (69, CUDA, cudaStreamEndCapture(streams[0], &graph);); (76, CUDA, cudaGraphInstantiate(&instance, graph, NULL, NULL, 0);); (51, CUDA, cudaStreamWaitEvent(streams[0], event2);); (63, CUDA, cudaCheckErrors(\"Kernel d failed\");); (66, CUDA, cudaStreamSynchronize(streams[0]);); (-10, CUDA, cudaEventCreateWithFlags(&event1, cudaEventDisableTiming);); (-10, CUDA, cudaEventCreateWithFlags(&event2, cudaEventDisableTiming);); (40, CUDA, cudaCheckErrors(\"Stream begin capture failed\");); (52, CUDA, cudaEventRecord(event1, streams[0]);); (13, CUDA, cudaMemcpy(d_x, h_x, N, cudaMemcpyHostToDevice);); (55, CUDA, cudaEventRecord(event2, streams[1]);); (21, CUDA, cudaMalloc((void**) &d_y, N * sizeof(float)););", "label": "(75, CUDA, cudaCheckErrors(\"Kernel c failed\");); (99, CUDA, cudaCheckErrors(\"Launching graph failed\");); (13, CUDA, cudaStreamCreateWithFlags(&streams[i], cudaStreamNonBlocking);); (63, CUDA, cudaCheckErrors(\"Event record failed\");); (56, CUDA, cudaCheckErrors(\"Stream begin capture failed\");); (31, CUDA, cudaMalloc((void**) &d_x, N * sizeof(float));); (44, CUDA, cudaGraphCreate(FIXME, 0);); (62, CUDA, cudaCheckErrors(\"Kernel b failed\");); (104, CUDA, cudaDeviceSynchronize();); (98, CUDA, cudaGraphGetNodes(graph, nodes, &numNodes);); (121, CUDA, cudaMemcpy(h_y, d_y, N, cudaMemcpyDeviceToHost);); (73, CUDA, cudaCheckErrors(\"Kernel d failed\");); (89, CUDA, cudaStreamSynchronize(streams[0]);); (58, CUDA, cudaStreamWaitEvent(streams[1], event1);); (72, CUDA, cudaStreamEndCapture(streams[0], &graph);); (48, CUDA, cudaCheckErrors(\"Kernel a failed\");); (64, CUDA, cudaStreamWaitEvent(streams[0], event2);); (-11, CUDA, cudaEventCreateWithFlags(&event1, cudaEventDisableTiming);); (-11, CUDA, cudaEventCreateWithFlags(&event2, cudaEventDisableTiming);); (53, CUDA, cudaCheckErrors(\"Event wait failed\");); (88, CUDA, cudaCheckErrors(\"Graph get nodes failed\");); (44, CUDA, cudaEventRecord(event1, streams[0]);); (19, CUDA, cudaMemcpy(d_y, h_y, N, cudaMemcpyHostToDevice);); (54, CUDA, cudaEventRecord(event2, streams[1]);); (16, CUDA, cudaMemcpy(d_x, h_x, N, cudaMemcpyHostToDevice);); (107, CUDA, cudaCheckErrors(\"Finishing memcpy failed\");); (11, CUDA, cudaMalloc((void**) &d_y, N * sizeof(float));); (32, CUDA, cudaStreamBeginCapture(streams[0], cudaStreamCaptureModeGlobal);); (91, CUDA, cudaGraphLaunch(instance, streams[0]);); (63, CUDA, cudaCheckErrors(\"instantiating graph failed\");); (8, CUDA, cudaCheckErrors(\"cudaMalloc failed\");); (56, CUDA, cudaCheckErrors(\"Stream end capture failed\");); (59, CUDA, cudaGraphInstantiate(FIXME, graph, NULL, NULL, 0);); (66, CUDA, cudaGraphLaunch(FIXME, streams[0]);)"}
{"predict": "(5, MPI, MPI_Init( &argc, &argv );); (42, MPI, MPI_Finalize();)", "label": "(9, MPI, test_integration_sin<2,1,double> t1( mpi.vm()[\"hsize\"].as<double>() );); (31, MPI, test_integration_sin<2,4,double> t2( mpi.vm()[\"hsize\"].as<double>() );); (14, MPI, test_integration_sin<2,2,double> t2( mpi.vm()[\"hsize\"].as<double>() );); (2, MPI, std::cout << \"Order = \" << mpi.vm()[\"order\"].as<int>() << \" / \" << FEELPP_MESH_MAX_ORDER << \"\\n\";); (20, MPI, test_integration_sin<2,3,double> t2( mpi.vm()[\"hsize\"].as<double>() );); (37, MPI, test_integration_sin<2,5,double> t2( mpi.vm()[\"hsize\"].as<double>() );)"}
{"predict": "(20, MPI, errs = MPI_Isend(\"No Errors\", 10, MPI_CHAR, 3, 0, MPI_COMM_WORLD, &request);); (64, MPI, MPI_Finalize();); (8, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (3, MPI, MPI_Init(0, 0);); (44, MPI, errs = MPI_Isend(\"More Errors\", 10, MPI_CHAR, 2, 0, MPI_COMM_WORLD, &request);); (22, MPI, errs += MPI_Wait(&request, MPI_STATUS_IGNORE);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (5, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (2, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(30, MPI, errs = MPI_Irecv(buf, 10, MPI_CHAR, 0, 0, MPI_COMM_WORLD, &request);); (5, MPI, MPI_Init(&argc, &argv);); (65, MPI, MPI_Finalize();); (18, MPI, errs = MPI_Issend(\"No Errors\", 10, MPI_CHAR, 3, 0, MPI_COMM_WORLD, &request);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (16, MPI, errs += MPI_Wait(&request, MPI_STATUS_IGNORE);); (2, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (5, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (34, MPI, errs = MPI_Issend(\"No Errors\", 10, MPI_CHAR, 2, 0, MPI_COMM_WORLD, &request);)"}
{"predict": "(330, OpenMP, #pragma omp distribute parallel for simd lastprivate(ii) linear(jj) collapse(2)); (317, OpenMP, #pragma omp distribute parallel for simd shared(ii)); (334, OpenMP, #pragma omp parallel); (356, OpenMP, #pragma omp distribute parallel for simd collapse(2)); (307, OpenMP, #pragma omp distribute parallel for simd private(ii)); (3, OpenMP, #pragma omp distribute parallel for simd); (318, OpenMP, #pragma omp distribute parallel for simd linear(ii))", "label": "(311, OpenMP, #pragma omp distribute parallel for simd firstprivate(ii)); (332, OpenMP, #pragma omp distribute parallel for simd lastprivate(ii)); (6, OpenMP, #pragma omp distribute parallel for simd); (322, OpenMP, #pragma omp distribute parallel for simd private(ii)); (313, OpenMP, #pragma omp distribute parallel for simd linear(ii)); (1, OpenMP, #pragma omp target); (347, OpenMP, #pragma omp distribute parallel for simd collapse(2)); (0, OpenMP, #pragma omp teams)"}
{"predict": "(77, MPI, MPI_Allgather(&count, 1, MPI_LONG, slice_width, 1, MPI_INT, MPI_COMM_WORLD);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &nodeID);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nr_nodes);)", "label": "(11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &nodeID);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nr_nodes);); (73, MPI, MPI_Allgather(&local_n0, 1, MPI_INT, slice_width, 1, MPI_INT, MPI_COMM_WORLD);)"}
{"predict": "(266, OpenMP, #pragma omp teams distribute simd lastprivate(ii) linear(jj) collapse(2)); (277, OpenMP, #pragma omp teams distribute simd firstprivate(ii)); (5, OpenMP, #pragma omp teams distribute simd); (265, OpenMP, #pragma omp teams distribute simd shared(ii)); (272, OpenMP, #pragma omp teams distribute simd private(ii)); (276, OpenMP, #pragma omp teams distribute simd lastprivate(ii))", "label": "(266, OpenMP, #pragma omp teams distribute simd firstprivate(ii)); (6, OpenMP, #pragma omp teams distribute simd); (270, OpenMP, #pragma omp teams distribute simd private(ii)); (275, OpenMP, #pragma omp teams distribute simd lastprivate(ii)); (287, OpenMP, #pragma omp teams distribute simd collapse(2)); (1, OpenMP, #pragma omp target)"}
{"predict": "(6, OpenMP, #pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp parallel for)"}
{"predict": "(6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (104, MPI, MPI_Barrier(MPI_COMM_WORLD);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (104, MPI, MPI_Barrier(MPI_COMM_WORLD);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(191, MPI, MPI_Waitall(num_procs_to_recv, recv_reqs, recv_status);); (149, MPI, MPI_Isend(Btmp + p * nrhs * m_loc, m_loc * nrhs, MPI_DOUBLE, dest, tag, grid3d->zscp.comm, &send_reqs[p]);); (186, MPI, MPI_Isend(B1 + p * nrhs * m_loc, m_loc * nrhs, MPI_DOUBLE, dest, tag, grid3d->zscp.comm, &send_reqs[p]);); (133, MPI, MPI_Irecv(Btmp + p * nrhs * m_loc, m_loc * nrhs, MPI_DOUBLE, src, tag, grid3d->zscp.comm, &recv_reqs[p]);); (190, MPI, MPI_Irecv(B1 + p * nrhs * m_loc, m_loc * nrhs, MPI_DOUBLE, src, tag, grid3d->zscp.comm, &recv_reqs[p]);); (143, MPI, MPI_Waitall(num_procs_to_send, send_reqs, send_status);); (348, MPI, MPI_Barrier(grid3d->zscp.comm);); (107, MPI, MPI_Alltoall(row_counts_int, 1, MPI_INT, x_send_counts, 1, MPI_INT, grid3d->zscp.comm);); (167, MPI, MPI_Abort(grid3d->zscp.comm, -1);); (339, MPI, MPI_Gather(B, m_loc * nrhs, MPI_DOUBLE, B2d, A2d->m_loc * nrhs, MPI_DOUBLE, 0, grid3d->zscp.comm);); (103, MPI, MPI_Alltoall(b_counts_int, 1, MPI_INT, m_loc_3d_counts, 1, MPI_INT, grid3d->zscp.comm);); (135, MPI, MPI_Status send_status(MPI_SUCCESS);); (162, MPI, MPI_Gather(B, m_loc * nrhs, MPI_DOUBLE, B2d, A2d->m_loc * nrhs, MPI_DOUBLE, 0, grid3d->zscp.comm);); (156, MPI, MPI_Comm_rank(grid3d->zscp.comm, &iam);); (94, MPI,", "label": "(270, MPI, MPI_Wait(&recv_req, &recv_status);)"}
{"predict": "(24, MPI, MPI_Type_get_true_extent(key, &key_true_lb, &key_true_extent);); (20, MPI, MPI_Type_get_extent(keysat, &keysat_lb, &keysat_extent);); (26, MPI, MPI_Type_commit(&type_int_with_key);); (150, MPI, MPI_Type_free(&type_int_with_key);)", "label": "(24, MPI, MPI_Type_get_true_extent(key, &key_true_lb, &key_true_extent);); (204, MPI, MPI_Type_free(&type_int_with_key);); (280, MPI, MPI_Comm_rank(comm, &comm_rank);); (17, MPI, MPI_Type_get_extent(keysat, &keysat_lb, &keysat_extent);)"}
{"predict": "(11, OpenMP, #pragma omp parallel for)", "label": "(11, OpenMP, #pragma omp parallel for)"}
{"predict": "(134, MPI, MPI_Waitall(nprocs_send, requests + nprocs_recv, MPI_STATUSES_IGNORE);); (19, MPI, MPI_Alltoall(send_size, 1, MPI_COUNT, recv_size, 1, MPI_COUNT, fd->comm);); (92, MPI, MPI_Type_commit(&send_type);); (99, MPI, MPI_Type_free(&send_type);); (118, MPI, MPI_Get_count(&statuses[j], MPI_BYTE, &count_recved);)", "label": "(94, MPI, MPI_Type_commit(&send_type);); (137, MPI, MPI_Waitall(nprocs_send, requests + nprocs_recv, MPI_STATUSES_IGNORE);); (19, MPI, MPI_Alltoall(send_size, 1, MPI_COUNT, recv_size, 1, MPI_COUNT, fd->comm);); (118, MPI, MPI_Get_count_c(&statuses[j], MPI_BYTE, &count_recved);); (93, MPI, MPI_Type_free(&send_type);)"}
{"predict": "(3, OpenMP, #pragma omp parallel for)", "label": "(3, OpenMP, #pragma omp parallel for)"}
{"predict": "(10, MPI, MPI_Init(&argc, &argv);); (61, MPI, MPI_Finalize();); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);)", "label": "(10, MPI, MPI_Init(&argc, &argv);); (206, MPI, MPI_Finalize();); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (185, MPI, MPI_Barrier(MPI_COMM_WORLD);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);)"}
{"predict": "(89, OpenMP, #pragma omp parallel private(i)); (77, OpenMP, #pragma omp distribute parallel for firstprivate(j)); (5, OpenMP, #pragma omp distribute parallel for firstprivate); (16, OpenMP, #pragma omp distribute parallel for firstprivate()); (35, OpenMP, #pragma omp distribute parallel for firstprivate(argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (70, OpenMP, #pragma omp distribute parallel for firstprivate(g)); (24, OpenMP, #pragma omp distribute parallel for firstprivate(argc,); (37, OpenMP, #pragma omp distribute parallel for firstprivate(S1)); (59, OpenMP, #pragma omp parallel); (3, OpenMP, #pragma omp distribute parallel for firstprivate(); (37, OpenMP, #pragma omp distribute parallel for firstprivate(a, b)); (64, OpenMP, #pragma omp distribute parallel for firstprivate(h)); (23, OpenMP, #pragma omp distribute parallel for firstprivate(argc > 0 ? argv[1] : argv[2])); (43, OpenMP, #pragma omp distribute parallel for firstprivate(e, k)); (46, OpenMP, #pragma omp distribute parallel for firstprivate(argv[1])); (5, OpenMP, #pragma omp distribute parallel for firstprivate(argc); (69, OpenMP, #pragma omp distribute parallel for lastprivate(g) firstprivate(g)); (-12, OpenMP, #pragma omp target); (83, OpenMP, #pragma omp distribute parallel for firstprivate(i)); (46, OpenMP, #pragma omp distribute parallel for firstprivate(v)); (26, OpenMP, #pragma omp distribute parallel for firstprivate(argv[1], argv[2])); (52, OpenMP, #pragma omp parallel shared(i)); (-13, OpenMP, #pragma omp teams)", "label": "(38, OpenMP, #pragma omp distribute parallel for firstprivate(argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (78, OpenMP, #pragma omp parallel private(i)); (56, OpenMP, #pragma omp distribute parallel for firstprivate(e, g)); (30, OpenMP, #pragma omp distribute parallel for firstprivate(argc > 0 ? argv[1] : argv[2])); (68, OpenMP, #pragma omp distribute parallel for firstprivate(i)); (1, OpenMP, #pragma omp target); (57, OpenMP, #pragma omp distribute parallel for firstprivate(k, h)); (59, OpenMP, #pragma omp parallel); (40, OpenMP, #pragma omp distribute parallel for firstprivate(a, b)); (93, OpenMP, #pragma omp parallel reduction(+ : i)); (3, OpenMP, #pragma omp distribute parallel for firstprivate(); (71, OpenMP, #pragma omp distribute parallel for firstprivate(j)); (41, OpenMP, #pragma omp distribute parallel for firstprivate(argv[1])); (5, OpenMP, #pragma omp distribute parallel for firstprivate()); (9, OpenMP, #pragma omp distribute parallel for firstprivate(argc); (78, OpenMP, #pragma omp distribute parallel for lastprivate(g) firstprivate(g)); (27, OpenMP, #pragma omp distribute parallel for firstprivate(S1)); (11, OpenMP, #pragma omp distribute parallel for firstprivate(argc,); (60, OpenMP, #pragma omp parallel shared(i)); (-11, OpenMP, #pragma omp distribute parallel for firstprivate); (-13, OpenMP, #pragma omp teams)"}
{"predict": "(5, TBB, tbb::atomic<bool> taskDoneFlag;); (85, TBB, tbb::task::enqueue(*t, p);); (83, TBB, const tbb::priority_t p = getPriorityByInt(i);); (8, TBB, tbb::task::enqueue(*w);); (11, TBB, FFTask* t = new( tbb::task::allocate_root() ) FFTask(&flag);); (97, TBB, tbb::atomic<int> counter;); (11, TBB, tbb::task::enqueue(*t);); (108, TBB, tbb::atomic<bool> signalToLeave;); (-4, TBB, tbb::atomic<int> flag;); (1, TBB, WaiterTask* w = new( tbb::task::allocate_root() ) WaiterTask(&taskDoneFlag);); (3, TBB, tbb::global_control c(tbb::global_control::max_allowed_parallelism, 1);)", "label": "(5, TBB, tbb::atomic<bool> taskDoneFlag;); (85, TBB, tbb::task::enqueue(*t, p);); (9, TBB, tbb::task::enqueue(*w);); (13, TBB, FFTask* t = new( tbb::task::allocate_root() ) FFTask(&flag);); (81, TBB, const tbb::priority_t p = getPriorityByInt(i);); (98, TBB, tbb::atomic<int> counter;); (11, TBB, tbb::task::enqueue(*t);); (108, TBB, tbb::atomic<bool> signalToLeave;); (-4, TBB, tbb::atomic<int> flag;); (1, TBB, WaiterTask* w = new( tbb::task::allocate_root() ) WaiterTask(&taskDoneFlag);); (3, TBB, tbb::global_control c(tbb::global_control::max_allowed_parallelism, 1);)"}
{"predict": "(48, OpenMP, #pragma omp master taskloop reduction(^ : S1)); (79, OpenMP, #pragma omp master taskloop reduction(&& : S2::S2sc)); (37, OpenMP, #pragma omp master taskloop reduction(+ : a, b, c, d, f)); (17, OpenMP, #pragma omp master taskloop reduction(-); (71, OpenMP, #pragma omp master taskloop reduction(+ : h, k, B::x)); (83, OpenMP, #pragma omp master taskloop private(i), reduction(+ : j), reduction(+ : q)); (100, OpenMP, #pragma omp master taskloop reduction(max : j)); (103, OpenMP, #pragma omp parallel reduction(* : fl)); (44, OpenMP, #pragma omp master taskloop reduction(max : h.b)); (62, OpenMP, #pragma omp master taskloop reduction(&& : S2::S2s)); (50, OpenMP, #pragma omp master taskloop reduction(min : a, b, c, d, f)); (70, OpenMP, #pragma omp parallel private(k)); (71, OpenMP, #pragma omp master taskloop reduction(+ : o)); (16, OpenMP, #pragma omp master taskloop reduction(\\)); (93, OpenMP, #pragma omp master taskloop reduction(+ : r)); (3, OpenMP, #pragma omp master taskloop reduction()); (86, OpenMP, #pragma omp parallel shared(i)); (94, OpenMP, #pragma omp master taskloop reduction(+ : fl)); (103, OpenMP, #pragma omp master taskloop reduction(task, + : m)); (22, OpenMP, #pragma omp master taskloop reduction(+ : ba)); (-6, OpenMP, #pragma omp master taskloop reduction); (105, OpenMP, #pragma omp master taskloop reduction(+ : m)); (60, OpenMP, #pragma omp master taskloop reduction(+ : p), reduction(+ : p)); (22, OpenMP, #pragma omp master taskloop reduction(* : ca)); (42, OpenMP, #pragma omp master taskloop reduction(+ : e, g)); (29, OpenMP, #pragma omp master taskloop reduction(^ : fl)); (55, OpenMP, #pragma", "label": "(15, OpenMP, #pragma omp master taskloop reduction +); (65, OpenMP, #pragma omp master taskloop reduction(- : da)); (61, OpenMP, #pragma omp master taskloop reduction(* : ca)); (54, OpenMP, #pragma omp master taskloop reduction(max : h.b)); (47, OpenMP, #pragma omp master taskloop reduction(+ : a, b, c, d, f)); (55, OpenMP, #pragma omp master taskloop reduction(+ : ba)); (30, OpenMP, #pragma omp master taskloop reduction(| : argc,); (84, OpenMP, #pragma omp master taskloop reduction(+ : p), reduction(+ : p)); (46, OpenMP, #pragma omp master taskloop reduction(min : a, b, c, d, f)); (33, OpenMP, #pragma omp master taskloop reduction(~ : argc)); (38, OpenMP, #pragma omp master taskloop reduction(^ : S1)); (73, OpenMP, #pragma omp master taskloop reduction(+ : o)); (90, OpenMP, #pragma omp master taskloop reduction(max : j)); (11, OpenMP, #pragma omp master taskloop reduction()); (95, OpenMP, #pragma omp parallel reduction(* : fl)); (63, OpenMP, #pragma omp master taskloop reduction(& : e, g)); (65, OpenMP, #pragma omp master taskloop reduction(+ : h, k, B::x)); (73, OpenMP, #pragma omp parallel private(k)); (96, OpenMP, #pragma omp master taskloop reduction(+ : m)); (20, OpenMP, #pragma omp master taskloop reduction(|| : argc > 0 ? argv[1] : argv[2])); (-2, OpenMP, #pragma omp master taskloop reduction(); (24, OpenMP, #pragma omp master taskloop reduction(&& : argc, z)); (53, OpenMP, #pragma omp master taskloop reduction(&& : S2::S2sc)); (46, OpenMP, #pragma omp master taskloop reduction(^ : fl)); (6, OpenMP, #pragma omp master taskloop reduction(\\)); (-13, OpenMP, #pragma omp master taskloop reduction); (7, OpenMP, #pragma omp master taskloop reduction(foo : argc); (74, OpenMP, #pragma omp parallel reduction(min : i)); (77, OpenMP, #pragma omp parallel private(fl)); (91, OpenMP, #pragma omp master taskloop reduction(task, + : m)); (-9, OpenMP, #pragma omp master taskloop reduction(-); (86, OpenMP, #pragma omp master taskloop nogroup reduction(+ : m)); (65, OpenMP, #pragma omp master taskloop reduction(+ : r)); (-6, OpenMP, #pragma omp master taskloop reduction(*)); (53, OpenMP, #pragma omp master taskloop private(i), reduction(+ : j), reduction(+ : q)); (65, OpenMP, #pragma omp parallel shared(i)); (36, OpenMP, #pragma omp master taskloop reduction(&& : S2::S2s)); (69, OpenMP, #pragma omp master taskloop reduction(+ : fl))"}
{"predict": "(34, CUDA, __shared__ int4 cordsSM[64];); (31, CUDA, __shared__ float4 pillarSumSM[64];); (29, CUDA, __shared__ float4 pillarSM[64][32];); (27, CUDA, __shared__ int pointsNumSM[64];); (66, CUDA, __shared__ float pillarOutSM[64][32][11];)", "label": "(34, CUDA, __shared__ int pointsNumSM[4];); (30, CUDA, __shared__ float4 pillarSM[4][64];); (30, CUDA, __shared__ float4 pillarSumSM[4];); (30, CUDA, __shared__ int4 cordsSM[4];); (31, CUDA, __shared__ float pillarOutSM[4][64][11];)"}
{"predict": "(11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &np);); (78, MPI, err = MPI_File_open(MPI_COMM_SELF, filename, MPI_MODE_RDONLY, MPI_INFO_NULL, &fh);); (27, MPI, err = MPI_File_write(fh, buf, MAXLEN, MPI_INT, &status);); (30, MPI, err = MPI_File_close(&fh);); (57, MPI, MPI_Type_commit(&filetype);); (66, MPI, err = MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_WRONLY, MPI_INFO_NULL, &fh);); (57, MPI, err = MPI_File_set_view(fh, 0, MPI_INT, filetype, \"native\", MPI_INFO_NULL);); (67, MPI, err = MPI_File_write_all(fh, buf, buflen, MPI_INT, &status);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (45, MPI, MPI_Type_create_hindexed(num, blocklen, addrs, MPI_INT, &filetype);); (76, MPI, err = MPI_File_read(fh, buf, MAXLEN, MPI_INT, &status);); (48, MPI, MPI_Type_create_struct(num, blocklen, indices, types, &filetype);); (43, MPI, MPI_Type_indexed(num, blocklen, indices, MPI_INT, &filetype);); (66, MPI, MPI_Type_free(&filetype);)", "label": "(11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &np);); (68, MPI, err = MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_WRONLY, MPI_INFO_NULL, &fh);); (73, MPI, err = MPI_File_write_all(fh, buf, buflen, MPI_INT, &status);); (29, MPI, err = MPI_File_close(&fh);); (60, MPI, MPI_Type_commit(&filetype);); (24, MPI, err = MPI_File_write(fh, buf, MAXLEN, MPI_INT, &status);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (65, MPI, err = MPI_File_set_view(fh, 0, MPI_INT, filetype, \"native\", MPI_INFO_NULL);); (45, MPI, MPI_Type_create_hindexed(num, blocklen, addrs, MPI_INT, &filetype);); (76, MPI, err = MPI_File_open(MPI_COMM_SELF, filename, MPI_MODE_RDONLY, MPI_INFO_NULL, &fh);); (40, MPI, MPI_Type_indexed(num, blocklen, indices, MPI_INT, &filetype);); (67, MPI, MPI_Type_free(&filetype);); (76, MPI, err = MPI_File_read(fh, buf, MAXLEN, MPI_INT, &status);); (23, MPI, MPI_Barrier(MPI_COMM_WORLD);); (42, MPI, MPI_Type_create_struct(num, blocklen, addrs, types, &filetype);)"}
{"predict": "(27, MPI, errcode = MPI_File_iwrite_at_all(fh, off, buffer, BUFSIZE, MPI_INT, &request);); (33, MPI, errcode = MPI_File_iread_at_all(fh, off, buf2, BUFSIZE, MPI_INT, &request);); (13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (14, MPI, MPI_File_set_errhandler(fh, MPI_ERRORS_RETURN);); (28, MPI, MPI_Wait(&request, &status);); (20, MPI, errcode = MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_CREATE | MPI_MODE_RDWR, info, &fh);); (30, MPI, errcode = MPI_File_close(&fh);); (25, MPI, MPI_File_seek_shared(fh, off, MPI_SEEK_SET);); (10, MPI, MPI_File_info_create(&info);); (24, MPI, MPI_File_seek(fh, off, MPI_SEEK_SET);); (38, MPI, MPI_File_free(&fh);); (38, MPI, MPI_File_info_free(&info);)", "label": "(19, MPI, MPI_Info_set(info, \"cb_nodes\", \"1\");); (25, MPI, errcode = MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_WRONLY | MPI_MODE_CREATE, info, &fh);); (27, MPI, errcode = MPI_File_iwrite_at_all(fh, off, buffer, BUFSIZE, MPI_INT, &request);); (30, MPI, errcode = MPI_File_close(&fh);); (28, MPI, MPI_Wait(&request, &status);); (12, MPI, MPI_Info_create(&info);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (33, MPI, errcode = MPI_File_iread_at_all(fh, off, buf2, BUFSIZE, MPI_INT, &request);); (10, MPI, MPI_Info_set(info, \"romio_cb_write\", \"enable\");); (43, MPI, MPI_Info_free(&info);); (27, MPI, errcode = MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_RDONLY, info, &fh);)"}
{"predict": "(109, OpenMP, #pragma omp barrier); (75, OpenMP, nthread = omp_get_num_threads();); (92, OpenMP, #pragma omp for); (71, OpenMP, #pragma omp master); (7, OpenMP, #pragma omp parallel private(j,iter))", "label": "(109, OpenMP, #pragma omp barrier); (75, OpenMP, nthread = omp_get_num_threads();); (92, OpenMP, #pragma omp for); (71, OpenMP, #pragma omp master); (68, OpenMP, #pragma omp parallel private(j,iter))"}
{"predict": "(252, OpenCL, error = clSetKernelArg( kernel, 0, sizeof( cl_mem ), &inputStream );); (234, OpenCL, error = clSetKernelArg( kernel, 2, sizeof( int ), &lod );); (253, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 2, NULL, threads, NULL, 0, NULL, NULL );); (217, OpenCL, error = clSetKernelArg( kernel, 1, sizeof( cl_mem ), &image );); (269, OpenCL, error = clEnqueueReadImage( queue, image, CL_TRUE, origin, region, gEnablePitch ? imageInfo->rowPitch : 0, gEnablePitch ? imageInfo->slicePitch : 0, resultValues, 0, NULL, NULL );); (209, OpenCL, clReleaseMemObject(image);); (254, OpenCL, error = clSetKernelArg( kernel, 3, sizeof( int ), &lod );); (229, OpenCL, error = clSetKernelArg( kernel, 2, sizeof( cl_mem ), &mipImage );); (205, OpenCL, image = clCreateImage( context, mem_flag_types[mem_flag_index], imageInfo->format, &image_desc, NULL, &error );); (231, OpenCL, error = clSetKernelArg( kernel, 1, sizeof( cl_mem ), &inputStream2 );); (213, OpenCL, error = clSetKernelArg( kernel, 0, sizeof( cl_mem ), &image );); (227, OpenCL, error = clSetKernelArg( kernel, 4, sizeof( int ), &lod );); (207, OpenCL, error = clSetKernelArg( kernel, 1, sizeof( cl_mem ), &inputStream );); (217, OpenCL, mipImage = clCreateImage( context, CL_MEM_READ_ONLY, imageInfo->format, &mip_desc, NULL, &error );); (242, OpenCL, error = clEnqueueWriteBuffer( queue, inputStream, CL_TRUE, 0, inputValues, 0, NULL, NULL );); (207, OpenCL, error = clSetKernelArg( kernel,", "label": "(242, OpenCL, error = clSetKernelArg( kernel, 0, sizeof( cl_mem ), &inputStream );); (228, OpenCL, error = clSetKernelArg( kernel, 2, sizeof( int ), &lod );); (215, OpenCL, error = clSetKernelArg( kernel, 1, sizeof( cl_mem ), &image );); (244, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 1, NULL, threads, NULL, 0, NULL, NULL );); (260, OpenCL, error = clEnqueueReadImage( queue, image, CL_TRUE, origin, region, gEnablePitch ? imageInfo->rowPitch : 0, 0, resultValues, 0, NULL, NULL );)"}
{"predict": "(38, OpenMP, std::cout << \"Number of threads    = \" << omp_get_max_threads() << std::endl;)", "label": "(38, OpenMP, std::cout << \"Number of threads    = \" << omp_get_max_threads() << std::endl;)"}
{"predict": "(4, TBB, tbb::flow::queue_node<T> q2(g);); (70, TBB, tbb::flow::make_edge( q2, q3 );); (67, TBB, tbb::flow::queue_node<T> q3(g);); (37, TBB, tbb::flow::make_edge( q, q2 );); (56, TBB, tbb::flow::remove_edge( q, q2 );); (87, TBB, tbb::flow::remove_edge( q,  q2 );); (-3, TBB, tbb::flow::queue_node<T> q(g);); (-5, TBB, tbb::flow::graph g;)", "label": "(4, TBB, tbb::flow::queue_node<T> q2(g);); (70, TBB, tbb::flow::make_edge( q2, q3 );); (67, TBB, tbb::flow::queue_node<T> q3(g);); (37, TBB, tbb::flow::make_edge( q, q2 );); (56, TBB, tbb::flow::remove_edge( q, q2 );); (87, TBB, tbb::flow::remove_edge( q,  q2 );); (-3, TBB, tbb::flow::queue_node<T> q(g);); (-5, TBB, tbb::flow::graph g;)"}
{"predict": "(201, OpenMP, const int th=omp_get_thread_num();); (19, OpenMP, #pragma omp parallel for schedule (guided))", "label": "(205, OpenMP, const int th=omp_get_thread_num();); (19, OpenMP, #pragma omp parallel for schedule (guided))"}
{"predict": "(86, MPI, MPI_Finalize();); (7, MPI, MPI_Init(&argc,&argv);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (57, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(7, MPI, MPI_Init(&argc, &argv);); (85, MPI, MPI_Finalize();); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (57, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(24, MPI, MPI_Comm_size(comm, &size);); (67, MPI, rc = MPI_Reduce(scinbuf, scoutbuf, 3, MPI_SIGNED_CHAR, MPI_BXOR, 0, comm);); (197, MPI, rc = MPI_Reduce(iinbuf, ioutbuf, 3, MPI_INT, MPI_BXOR, 0, comm);); (213, MPI, rc = MPI_Reduce(usinbuf, usoutbuf, 3, MPI_UNSIGNED_SHORT, MPI_BXOR, 0, comm);); (229, MPI, rc = MPI_Reduce(uinbuf, uoutbuf, 3, MPI_UNSIGNED, MPI_BXOR, 0, comm);); (33, MPI, rc = MPI_Reduce(cinbuf, coutbuf, 3, MPI_CHAR, MPI_BXOR, 0, comm);); (243, MPI, rc = MPI_Reduce(ulinbuf, uloutbuf, 3, MPI_UNSIGNED_LONG, MPI_BXOR, 0, comm);); (175, MPI, rc = MPI_Reduce(linbuf, loutbuf, 3, MPI_LONG, MPI_BXOR, 0, comm);); (256, MPI, rc = MPI_Reduce(uinbuf, uoutbuf, 3, MPI_INT, MPI_BXOR, 0, comm);); (124, MPI, rc = MPI_Reduce(ucinbuf, ucoutbuf, 3, MPI_UNSIGNED_CHAR, MPI_BXOR, 0, comm);); (18, MPI, MPI_Comm_set_errhandler(comm, MPI_ERRORS_RETURN);); (132, MPI, rc = MPI_Reduce(sinbuf, soutbuf, 3, MPI_SHORT, MPI_BXOR, 0, comm);); (22, MPI, MPI_Comm_rank(comm, &rank);); (39, MPI, rc = MPI_Reduce(cinbuf, coutbuf, 3, MPI_CHAR, MPI_BXOR, 1, comm);)", "label": "(25, MPI, MPI_Comm_size(comm, &size);); (97, MPI, rc = MPI_Reduce(ucinbuf, ucoutbuf, 3, MPI_UNSIGNED_CHAR, MPI_BXOR, 0, comm);); (156, MPI, rc = MPI_Reduce(sinbuf, soutbuf, 3, MPI_SHORT, MPI_BXOR, 0, comm);); (125, MPI, rc = MPI_Reduce(cinbuf, coutbuf, 3, MPI_BYTE, MPI_BXOR, 0, comm);); (274, MPI, rc = MPI_Reduce(linbuf, loutbuf, 3, MPI_LONG, MPI_BXOR, 0, comm);); (63, MPI, rc = MPI_Reduce(scinbuf, scoutbuf, 3, MPI_SIGNED_CHAR, MPI_BXOR, 0, comm);); (31, MPI, rc = MPI_Reduce(cinbuf, coutbuf, 3, MPI_CHAR, MPI_BXOR, 0, comm);); (335, MPI, rc = MPI_Reduce(llinbuf, lloutbuf, 3, MPI_LONG_LONG, MPI_BXOR, 0, comm);); (14, MPI, MPI_Comm_set_errhandler(comm, MPI_ERRORS_RETURN);); (299, MPI, rc = MPI_Reduce(ulinbuf, uloutbuf, 3, MPI_UNSIGNED_LONG, MPI_BXOR, 0, comm);); (238, MPI, rc = MPI_Reduce(iinbuf, ioutbuf, 3, MPI_INT, MPI_BXOR, 0, comm);); (13, MPI, MPI_Comm_rank(comm, &rank);); (206, MPI, rc = MPI_Reduce(uinbuf, uoutbuf, 3, MPI_UNSIGNED, MPI_BXOR, 0, comm);); (175, MPI, rc = MPI_Reduce(usinbuf, usoutbuf, 3, MPI_UNSIGNED_SHORT, MPI_BXOR, 0, comm);)"}
{"predict": "(19, CUDA, ColorSpinorParam cudaParam(cpuParam, *inv_param, QUDA_CUDA_FIELD_LOCATION);)", "label": "(19, CUDA, ColorSpinorParam cudaParam(cpuParam, *inv_param, QUDA_CUDA_FIELD_LOCATION);)"}
{"predict": "(144, OpenMP, lsm303ah_mg_offset_temp_comp_set(&dev_ctx_mg, PROPERTY_ENABLE);)", "label": "(145, OpenMP, lsm303ah_mg_offset_temp_comp_set(&dev_ctx_mg, PROPERTY_ENABLE);)"}
{"predict": "(34, MPI, MPI_Irecv(&error_status[j], 1, MPI_STATUS_IGNORE, req[j], &req[j]);); (21, MPI, MPI_Comm_idup(MPI_COMM_WORLD, &comm_hdls[i], &req[i]);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (20, MPI, mpi_errno = MPI_Waitall(block, req, MPI_STATUSES_IGNORE);); (45, MPI, MPI_Isend(&block, 1, MPI_INT, MPI_PROC_NULL, 0, MPI_COMM_WORLD, &req[i]);); (48, MPI, mpi_errno = MPI_Wait(&req[i], MPI_STATUS_IGNORE);); (15, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (22, MPI, mpi_errno = MPI_Waitall(block, req, error_status);); (10, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (47, MPI, mpi_errno = MPI_Waitall(block, req, MPI_STATUSES_IGNORE);); (17, MPI, MPI_Comm_idup(MPI_COMM_WORLD, &comm_hdls[i], &req[block++]);); (28, MPI, mpi_errno = MPI_Wait(&req[j], MPI_STATUSES_IGNORE);)"}
{"predict": "(80, OpenMP, #pragma\tomp\ttaskloop private (thread_id1,rtemp_loc,nroot_send_tmp) untied); (138, OpenMP, #pragma omp simd); (86, OpenMP, #pragma omp parallel); (145, OpenMP, #pragma omp atomic capture); (163, OpenMP, #pragma omp master); (175, OpenMP, thread_id1=omp_get_thread_num();); (74, OpenMP, #pragma omp parallel default (shared)); (133, OpenMP, #pragma\tomp\ttaskloop private (thread_id1,rtemp_loc,nroot_send_tmp) untied if (Nchunk>10)); (69, OpenMP, #pragma omp master); (164, OpenMP, msg_cnt=omp_get_msgcnt(thread_id1);); (160, OpenMP, msg_vol=omp_get_msgvol(thread_id1);); (84, OpenMP, #pragma\tomp\ttaskloop private (thread_id1,rtemp_loc,nroot_send_tmp) untied nogroup); (77, OpenMP, #pragma\tomp\ttaskloop private (thread_id1,rtemp_loc,nroot_send_tmp) untied if (Nchunk>20))", "label": "(82, OpenMP, #pragma\tomp\ttaskloop firstprivate (stat) private (thread_id1,Uinv,nn,lbstart,lbend,ub,temp,rtemp_loc,ik,lk1,gik,gikcol,usub,uval,lsub,lusup,iknsupc,il,i,irow,bmod_tmp,p,ii,jj,t1,t2,j,ikfrow,iklrow,dest,y,uptr,fnz,nsupr,nroot_send_tmp) untied nogroup); (126, OpenMP, #pragma omp simd); (141, OpenMP, #pragma omp atomic capture); (75, OpenMP, #pragma\tomp\tparallel for private (thread_id1,Uinv,nn,lbstart,lbend,ub,temp,rtemp_loc,ik,lk1,gik,gikcol,usub,uval,lsub,lusup,iknsupc,il,i,irow,bmod_tmp,p,ii,jj,t1,t2,j,ikfrow,iklrow,dest,y,uptr,fnz,nsupr))"}
{"predict": "(82, OpenMP, #pragma omp teams distribute parallel for num_teams(20) num_threads(512)); (18, OpenMP, #pragma omp distribute parallel for); (50, OpenMP, #pragma omp distribute parallel for num_threads(64)); (34, OpenMP, #pragma omp teams num_teams(20) thread_limit(768)); (74, OpenMP, #pragma omp teams distribute parallel for num_teams(20) thread_limit(128)); (44, OpenMP, #pragma omp distribute parallel for num_threads(256)); (12, OpenMP, #pragma omp target); (5, OpenMP, #pragma omp teams num_teams(20) thread_limit(128)); (33, OpenMP, #pragma omp distribute parallel for num_threads(96)); (18, OpenMP, #pragma omp teams num_teams(20) thread_limit(64))", "label": "(19, OpenMP, #pragma omp distribute parallel for); (17, OpenMP, #pragma omp target); (45, OpenMP, #pragma omp distribute parallel for num_threads(128)); (72, OpenMP, #pragma omp teams distribute parallel for thread_limit(512) num_threads(128)); (22, OpenMP, #pragma omp target teams thread_limit(64)); (76, OpenMP, #pragma omp teams distribute parallel for); (29, OpenMP, #pragma omp teams)"}
{"predict": "(13, CUDA, checkCudaErrors(cudaGetDeviceProperties(&prop, device));); (104, CUDA, checkCudaErrors(cudaFree(d_idata));); (66, CUDA, checkCudaErrors(cudaMalloc((void **)&d_idata, bytes));); (66, CUDA, checkCudaErrors(cudaMalloc((void **)&d_odata, numBlocks * sizeof(float)));); (99, CUDA, checkCudaErrors(cudaFree(d_odata));); (64, CUDA, checkCudaErrors(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));)", "label": "(68, CUDA, checkCudaErrors(cudaMalloc((void **)&d_odata, numBlocks * sizeof(float)));); (105, CUDA, cudaFree(d_odata);); (65, CUDA, checkCudaErrors(cudaMalloc((void **)&d_idata, bytes));); (10, CUDA, checkCudaErrors(cudaGetDeviceProperties(&prop, device));); (8, CUDA, checkCudaErrors(cudaSetDevice(device));); (100, CUDA, cudaFree(d_idata);); (65, CUDA, checkCudaErrors(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));)"}
{"predict": "(62, MPI, MPI_Unpack(pack_buf, pack_size, &pos, buf, 1, datatype, MPI_COMM_SELF);); (35, MPI, MPI_Type_size_x(datatype, &type_size);); (55, MPI, MPI_Pack(check_pack_buf, 1, datatype, pack_buf, pack_size, &pos, MPI_COMM_SELF);); (32, MPI, MPI_Pack_size(1, datatype, MPI_COMM_SELF, &pack_size);)", "label": "(63, MPI, MPI_Unpack(check_pack_buf, pack_size, &pos, unpack_buf, 1, datatype, MPI_COMM_WORLD);); (6, MPI, MPI_Type_size_c(datatype, &type_size);); (34, MPI, MPI_Pack_size(1, datatype, MPI_COMM_WORLD, &pack_size);); (54, MPI, MPI_Pack(check_unpack_buf, 1, datatype, pack_buf, pack_size, &pos, MPI_COMM_WORLD);)"}
{"predict": "(172, OpenCL, err = clEnqueueNDRangeKernel( queue, pThreadData->m_kernel, 1, NULL, threads, NULL, 0, NULL, NULL );); (115, OpenCL, err = clEnqueueWriteBuffer( queue, pThreadData->m_input_buf[ 1 ], CL_TRUE, 0, num_elements * vectorSize * sizeof(cl_ushort), pThreadData->m_input_ptr[ 1 ], 0, NULL, NULL );); (192, OpenCL, err = clEnqueueReadBuffer( queue, pThreadData->m_output_buf, CL_TRUE, 0, num_elements * vectorSize * sizeof(cl_ushort), pThreadData->m_output_ptr, 0, NULL, NULL );); (119, OpenCL, err = clEnqueueReadBuffer( queue, pThreadData->m_output_buf, CL_TRUE, 0, num_elements * vectorSize * sizeof(cl_ushort), pThreadData->m_output_ptr, 0, NULL, NULL );); (151, OpenCL, err = clEnqueueWriteBuffer( queue, pThreadData->m_input_buf[ 0 ], CL_TRUE, 0, num_elements * vectorSize * sizeof(cl_ushort), pThreadData->m_input_ptr[ 0 ], 0, NULL, NULL );); (110, OpenCL, err = clEnqueueWriteBuffer( queue, pThreadData->m_input_buf[ 0 ], CL_TRUE, 0, num_elements * vectorSize * sizeof(cl_ushort), pThreadData->m_input_ptr[ 0 ], 0, NULL, NULL );); (181, OpenCL, err = clEnqueueReadBuffer( queue, pThreadData->m_aux_buf, CL_TRUE, 0, num_elements * vectorSize * sizeof(cl_ushort), pThreadData->m_aux_ptr, 0, NULL, NULL );); (62, OpenCL, clCreateKernelsInProgram( pThreadData->m_program, 1, &(pThreadData->m_kernel) );); (181, OpenCL, err = clEnqueueReadBuffer( queue, pThreadData->m_err_count_buf, CL_TRUE, 0, sizeof(cl_uint), &pThreadData->m_err_count, 0, NULL, NULL );); (110, OpenCL, err", "label": "(113, OpenCL, err = clEnqueueWriteBuffer( queue, pThreadData->m_streams[1], CL_FALSE, 0, pThreadData->m_type_size*num_elements * inputBVecSize, (void *)pThreadData->m_input_ptr[1], 0, NULL, NULL );); (110, OpenCL, err = clEnqueueWriteBuffer(queue, pThreadData->m_streams[0], CL_FALSE, 0, pThreadData->m_type_size*num_elements * inputAVecSize, (void *)pThreadData->m_input_ptr[0], 0, NULL, NULL);); (117, OpenCL, err = clEnqueueReadBuffer( queue, pThreadData->m_streams[2], CL_TRUE, 0, pThreadData->m_type_size*num_elements * vectorSize, (void *)pThreadData->m_output_ptr, 0, NULL, NULL );); (113, OpenCL, err = clEnqueueNDRangeKernel( queue, pThreadData->m_kernel[i], 1, NULL, threads, NULL, 0, NULL, NULL );)"}
{"predict": "(113, OpenMP, if (iter==1) stencil_time = omp_get_wtime();); (125, OpenMP, stencil_time = omp_get_wtime() - stencil_time;); (111, OpenMP, #pragma omp target teams distribute parallel for simd collapse(2))", "label": "(114, OpenMP, if (iter==1) stencil_time = omp_get_wtime();); (59, OpenMP, std::cout << \"Number of threads (max)   = \" << omp_get_max_threads() << std::endl;); (125, OpenMP, stencil_time = omp_get_wtime() - stencil_time;)"}
{"predict": "(36, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (83, CUDA, cudaFree(d_in);); (83, CUDA, cudaFree(d_out);); (32, CUDA, cudaMalloc((void**)(&d_in), in_bytes);); (45, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (33, CUDA, cudaMemcpy(d_in, tensor.data(), in_bytes, cudaMemcpyHostToDevice);); (42, CUDA, assert(cudaMemcpyAsync(tensor_arg.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);)", "label": "(36, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (83, CUDA, cudaFree(d_in);); (83, CUDA, cudaFree(d_out);); (32, CUDA, cudaMalloc((void**)(&d_in), in_bytes);); (45, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (33, CUDA, cudaMemcpy(d_in, tensor.data(), in_bytes, cudaMemcpyHostToDevice);); (42, CUDA, assert(cudaMemcpyAsync(tensor_arg.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);)"}
{"predict": "(47, OpenMP, #pragma omp for simd linear(i : 4)); (14, OpenMP, #pragma omp for simd linear ()); (10, OpenMP, #pragma omp for simd linear); (11, OpenMP, #pragma omp for simd linear (); (38, OpenMP, #pragma omp for simd linear(h, C::x)); (12, OpenMP, #pragma omp for simd linear (argc); (13, OpenMP, #pragma omp for simd linear (argc > 0 ? argv[1] : argv[2])); (40, OpenMP, #pragma omp parallel); (37, OpenMP, #pragma omp for simd linear(i : j)); (22, OpenMP, #pragma omp for simd linear (S1)); (12, OpenMP, #pragma omp for simd linear (argc,); (14, OpenMP, #pragma omp for simd linear (argc, z)); (24, OpenMP, #pragma omp for simd linear (argv[1])); (15, OpenMP, #pragma omp for simd linear (a, b)); (25, OpenMP, #pragma omp for simd linear (argv[1], argc)); (37, OpenMP, #pragma omp for simd linear(i)); (15, OpenMP, #pragma omp for simd linear (argc)); (11, OpenMP, #pragma omp for simd linear (argc,)", "label": "(11, OpenMP, #pragma omp taskloop simd linear(f) linear(f)); (54, OpenMP, #pragma omp taskloop simd linear(ref(j))); (35, OpenMP, #pragma omp taskloop simd linear(a, b)); (26, OpenMP, #pragma omp taskloop simd linear (argc > 0 ? argv[1] : argv[2])); (27, OpenMP, #pragma omp taskloop simd linear (argc, z)); (10, OpenMP, #pragma omp taskloop simd linear (); (39, OpenMP, #pragma omp taskloop simd linear(h, C::x)); (16, OpenMP, #pragma omp taskloop simd linear (foo())); (49, OpenMP, #pragma omp taskloop simd linear(i)); (38, OpenMP, #pragma omp parallel); (17, OpenMP, #pragma omp taskloop simd linear (argc,); (2, OpenMP, #pragma omp taskloop simd linear); (13, OpenMP, #pragma omp taskloop simd linear (argc); (37, OpenMP, #pragma omp taskloop simd linear(val(i))); (19, OpenMP, #pragma omp taskloop simd linear (S1)); (24, OpenMP, #pragma omp taskloop simd linear (argv[1])); (3, OpenMP, #pragma omp taskloop simd linear (val); (4, OpenMP, #pragma omp taskloop simd linear (ref())); (34, OpenMP, #pragma omp taskloop simd linear(uval(i) : 4)); (24, OpenMP, #pragma omp taskloop simd linear(val(e, g))); (-3, OpenMP, #pragma omp taskloop simd linear ())"}
{"predict": "(52, TBB, tbb::flow::make_edge( b2, b3 );); (50, TBB, tbb::flow::make_edge( b, b2 );); (73, TBB, tbb::flow::buffer_node<T> b_copy(b);); (2, TBB, tbb::flow::buffer_node<T> b3(g);); (0, TBB, tbb::flow::buffer_node<T> b2(g);); (-3, TBB, tbb::flow::graph g;); (-3, TBB, tbb::flow::buffer_node<T> b(g);)", "label": "(52, TBB, tbb::flow::make_edge( b2, b3 );); (50, TBB, tbb::flow::make_edge( b, b2 );); (73, TBB, tbb::flow::buffer_node<T> b_copy(b);); (2, TBB, tbb::flow::buffer_node<T> b3(g);); (0, TBB, tbb::flow::buffer_node<T> b2(g);); (-3, TBB, tbb::flow::graph g;); (-3, TBB, tbb::flow::buffer_node<T> b(g);)"}
{"predict": "(131, OpenCL, err |= clSetKernelArg(kernel, 2, sizeof streams[2], &streams[2]);); (131, OpenCL, err |= clSetKernelArg(kernel, 3, sizeof streams[3], &streams[3]);); (129, OpenCL, err = clSetKernelArg(kernel, 0, sizeof streams[0], &streams[0]);); (129, OpenCL, err |= clSetKernelArg(kernel, 1, sizeof streams[1], &streams[1]);)", "label": "(132, OpenCL, err |= clSetKernelArg(kernel, 1, sizeof(cl_mem), &streams[1]);); (132, OpenCL, err |= clSetKernelArg(kernel, 2, sizeof(cl_mem), &streams[2]);); (132, OpenCL, err |= clSetKernelArg(kernel, 3, sizeof(cl_mem), &streams[3]);); (128, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(cl_mem), &streams[0]);)"}
{"predict": "(3, TBB, typedef tbb::blocked_range2d<row_type,col_type> range_type;); (19, TBB, AssertSameType( r.cols(), tbb::blocked_range<col_type>( coli, colj, 1 ));); (22, TBB, range_type r2(r,tbb::split());); (16, TBB, AssertSameType( r.rows(), tbb::blocked_range<row_type>( rowi, rowj, 1 ));)", "label": "(24, TBB, range_type r2(r,tbb::split());); (2, TBB, typedef tbb::blocked_range2d<row_type,col_type> range_type;); (18, TBB, AssertSameType( r.cols(), tbb::blocked_range<col_type>( coli, colj, 1 ));); (16, TBB, AssertSameType( r.rows(), tbb::blocked_range<row_type>( rowi, rowj, 1 ));)"}
{"predict": "(150, MPI, MPI_CALL(MPI_Barrier(MPI_COMM_WORLD));); (22, MPI, MPI_CALL(MPI_Comm_free(&local_comm));); (244, CUDA, CUDA_RT_CALL(cudaStreamDestroy(compute_stream));); (204, MPI, double stop = MPI_Wtime();); (239, CUDA, CUDA_RT_CALL(cudaFree(a));); (-2, MPI, MPI_CALL(MPI_Comm_rank(MPI_COMM_WORLD, &rank));); (92, CUDA, CUDA_RT_CALL(cudaDeviceSynchronize());); (18, CUDA, CUDA_RT_CALL(cudaFree(0));); (16, CUDA, CUDA_RT_CALL(cudaSetDevice(local_rank%num_devices));); (60, CUDA, CUDA_RT_CALL(cudaMalloc(&a, nx * (chunk_size + 2) * sizeof(real)));); (235, CUDA, CUDA_RT_CALL(cudaFree(l2_norm_d));); (32, CUDA, CUDA_RT_CALL(cudaMallocHost(&a_h, nx * ny * sizeof(real)));); (29, CUDA, CUDA_RT_CALL(cudaMallocHost(&a_ref_h, nx * ny * sizeof(real)));); (85, CUDA, CUDA_RT_CALL(cudaGetLastError());); (226, CUDA, CUDA_RT_CALL(cudaEventDestroy(compute_done));); (56, CUDA, CUDA_RT_CALL(cudaMalloc(&a_new, nx * (chunk_size + 2) * sizeof(real)));); (-11, MPI, MPI_CALL(MPI_Comm_size(MPI_COMM_WORLD, &size));); (178, CUDA, CUDA_RT_CALL(cudaStreamSynchronize(compute_stream));); (149, CUDA, CUDA_RT_CALL(cudaEventRecord(compute_done, compute_stream));); (-12, CUDA, CUDA_RT_CALL(cudaGetDeviceCount(&num_devices));); (92, CUDA, CUDA_RT_CALL(cudaMallocHost(&l2_norm_h, sizeof(real)));); (133, CUDA, CUDA_RT_CALL(cudaMemsetAsync(l2_norm_d, 0, sizeof(real), compute_stream));); (83, CUDA, CUDA_RT_CALL(cudaMalloc(&l2_norm_d, sizeof(real)));); (-22, MPI, MPI_CALL(MPI_Init(&argc, &argv));); (233, CUDA, CUDA_RT_CALL(cudaFreeHost(a_ref_h));); (177, MPI, MPI_CALL(MPI_Allreduce(l2_norm_h, &l2_norm, 1", "label": "(118, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&reset_l2norm_done, cudaEventDisableTiming));); (143, MPI, MPI_CALL(MPI_Barrier(MPI_COMM_WORLD));); (21, MPI, MPI_CALL(MPI_Comm_free(&local_comm));); (249, CUDA, CUDA_RT_CALL(cudaStreamDestroy(compute_stream));); (201, MPI, double stop = MPI_Wtime();); (244, CUDA, CUDA_RT_CALL(cudaEventDestroy(push_done));); (257, CUDA, CUDA_RT_CALL(cudaFree(a));); (-4, MPI, MPI_CALL(MPI_Comm_rank(MPI_COMM_WORLD, &rank));); (94, CUDA, CUDA_RT_CALL(cudaDeviceSynchronize());); (18, CUDA, CUDA_RT_CALL(cudaFree(0));); (16, CUDA, CUDA_RT_CALL(cudaSetDevice(local_rank%num_devices));); (58, CUDA, CUDA_RT_CALL(cudaMalloc(&a, nx * (chunk_size + 2) * sizeof(real)));); (243, CUDA, CUDA_RT_CALL(cudaFree(l2_norm_d));); (138, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(push_stream, reset_l2norm_done, 0));); (100, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&push_prep_done, cudaEventDisableTiming));); (28, CUDA, CUDA_RT_CALL(cudaMallocHost(&a_h, nx * ny * sizeof(real)));); (25, CUDA, CUDA_RT_CALL(cudaMallocHost(&a_ref_h, nx * ny * sizeof(real)));); (84, CUDA, CUDA_RT_CALL(cudaGetLastError());); (145, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(compute_stream, push_prep_done, 0));); (89, CUDA, CUDA_RT_CALL(cudaStreamCreateWithPriority(&compute_stream, cudaStreamDefault, leastPriority));); (51, CUDA, CUDA_RT_CALL(cudaMalloc(&a_new, nx * (chunk_size + 2) * sizeof(real)));); (230, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_stream));); (-17, MPI, MPI_CALL(MPI_Comm_size(MPI_COMM_WORLD, &size));); (170, CUDA, CUDA_RT_CALL(cudaStreamSynchronize(compute_stream));); (125, CUDA, CUDA_RT_CALL(cudaEventRecord(reset_l2norm_done, compute_stream));); (-18, CUDA, CUDA_RT_CALL(cudaGetDeviceCount(&num_devices));); (102, CUDA, CUDA_RT_CALL(cudaMallocHost(&l2_norm_h, sizeof(real)));); (121, CUDA, CUDA_RT_CALL(cudaMemsetAsync(l2_norm_d, 0, sizeof(real), compute_stream));); (-27, MPI, MPI_CALL(MPI_Init(&argc, &argv));); (46, CUDA, CUDA_RT_CALL(cudaMemset(a_new, 0, nx * (chunk_size + 2) * sizeof(real)));); (76, CUDA, CUDA_RT_CALL(cudaDeviceGetStreamPriorityRange(&leastPriority, &greatestPriority));); (219, CUDA, CUDA_RT_CALL(cudaEventDestroy(push_prep_done));); (94, CUDA, CUDA_RT_CALL(cudaMalloc(&l2_norm_d, sizeof(real)));); (-12, MPI, MPI_CALL(MPI_Comm_rank(local_comm, &local_rank));); (233, CUDA, CUDA_RT_CALL(cudaFreeHost(a_ref_h));); (159, MPI, MPI_CALL(MPI_Allreduce(l2_norm_h, &l2_norm, 1, MPI_REAL_TYPE, MPI_SUM, MPI_COMM_WORLD));); (109, MPI, double start = MPI_Wtime();); (79, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&push_done, cudaEventDisableTiming));); (210, CUDA, CUDA_RT_CALL(cudaEventDestroy(reset_l2norm_done));); (215, CUDA, CUDA_RT_CALL(cudaFreeHost(l2_norm_h));); (120, CUDA, CUDA_RT_CALL(cudaEventRecord(push_prep_done, push_stream));)"}
{"predict": "(42, CUDA, CUDA_CHECK(cudaMemcpyAsync(d_A, h_A, lda * n * sizeof(data_type), cudaMemcpyHostToDevice, stream));); (35, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A_inv), lda * n * sizeof(data_type)));); (64, CUDA, CUDA_CHECK(cudaFree(d_A_inv));); (32, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_info), sizeof(int)));); (41, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (44, CUDA, CUDA_CHECK(cudaMemcpyAsync(&h_info, d_info, sizeof(int), cudaMemcpyDeviceToHost, stream));); (60, CUDA, CUDA_CHECK(cudaFree(d_info));); (58, CUDA, CUDA_CHECK(cudaFree(d_A));); (36, CUDA, CUDA_CHECK(cudaMemcpyAsync(d_info, &h_info, sizeof(int), cudaMemcpyHostToDevice, stream));); (26, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), lda * n * sizeof(data_type)));); (54, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (20, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)", "label": "(43, CUDA, CUDA_CHECK(cudaMemcpy(d_A_inv, d_A, sizeof(data_type) * lda * n, cudaMemcpyDeviceToDevice));); (47, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (69, CUDA, CUDA_CHECK(cudaFree(d_info));); (34, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * lda * n));); (34, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A_inv), sizeof(data_type) * lda * n));); (65, CUDA, CUDA_CHECK(cudaFree(d_A_inv));); (58, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (32, CUDA, CUDA_CHECK(cudaMalloc(&d_info, sizeof(int)));); (34, CUDA, CUDA_CHECK(cudaMemcpy(d_A, h_A, sizeof(data_type) * lda * n, cudaMemcpyHostToDevice));); (42, CUDA, CUDA_CHECK(cudaMemcpy(&h_info, d_info, sizeof(int), cudaMemcpyDeviceToHost));); (34, CUDA, CUDA_CHECK(cudaMemset(d_info, 0, sizeof(int)));); (58, CUDA, CUDA_CHECK(cudaFree(d_A));); (20, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(29, CUDA, thrust::complex<T2>* vb_(vb + Q * nchol_max * nwalk);); (26, CUDA, thrust::complex<T> const* v1_(v1 + Q * nchol_max * nwalk);); (27, CUDA, thrust::complex<T> const* v2_(v1 + Qm * nchol_max * nwalk);); (36, CUDA, thrust::complex<T> ialpha(-0.0, 1.0);)", "label": "(31, CUDA, thrust::complex<T> const* v1_(v1 + Q * nchol_max * nwalk);); (31, CUDA, thrust::complex<T> const* v2_(v1 + Qm * nchol_max * nwalk);); (52, CUDA, thrust::complex<T2> ialpha(alpha * thrust::complex<T2>(0.0, 1.0));); (27, CUDA, thrust::complex<T2>* vb_(vb + nc0 * nwalk);); (46, CUDA, thrust::complex<T2>* vb_(vb + (nc0 + nc) * nwalk);)"}
{"predict": "(85, MPI, MPI_Finalize();); (35, MPI, tim::mpi::mpi_gather(_measurements);)", "label": "(35, MPI, _measurements = get_measure()->mpi_get();)"}
{"predict": "(21, CUDA, cudaMalloc(&buffersD2D[d], numElems * sizeof(int));); (155, CUDA, cudaEventDestroy(start[d]);); (94, CUDA, cudaDeviceDisablePeer(i);); (54, CUDA, cudaStreamWaitEvent(stream1[j], start[i], 0);); (121, CUDA, cudaFree(buffersD2D[d]);); (83, CUDA, cudaEventRecord(stop[j], stream1[j]);); (85, CUDA, cudaEventSynchronize(stop[j]);); (118, CUDA, cudaFreeHost((void *)flag);); (151, CUDA, cudaEventDestroy(stop[d]);); (45, CUDA, cudaStreamWaitEvent(stream0[i], start[i], 0);); (155, CUDA, cudaStreamDestroy(stream1[d]);); (34, CUDA, cudaDeviceEnablePeer(i, j);); (148, CUDA, cudaFree(buffers[d]);); (150, CUDA, cudaStreamDestroy(stream0[d]);); (-4, CUDA, cudaCheckError();); (9, CUDA, cudaEventCreate(&stop[d]);); (5, CUDA, cudaMemset(buffers[d], 0, numElems * sizeof(int));); (91, CUDA, cudaDeviceDisablePeer(j);); (138, CUDA, cudaEventDestroy(flag[d]);); (148, CUDA, cudaFreeHost((void *)flag);); (118, CUDA, cudaFree(buffersD2D[d]);); (-14, CUDA, cudaMalloc(&buffers[d], numElems * sizeof(int));); (24, CUDA, cudaSetDevice(i);); (76, CUDA, cudaEventElapsedTime(&time_ms, start[i], stop[j]);); (-16, CUDA, cudaEventCreate(&start[d]);); (31, CUDA, cudaStreamCreateWithFlags(&stream1[d], cudaStreamNonBlocking);); (-17, CUDA, cudaSetDevice(d);); (11, CUDA, cudaSetDevice(i);); (44, CUDA, cudaEventRecord(start[i], stream0[i]);); (26, CUDA, cudaDeviceEnablePeer(j, i);); (137, CUDA, cudaFree(buffers[d]);); (140, CUDA, cudaFree(buffersD2D[d]);); (1", "label": "(147, CUDA, cudaStreamDestroy(stream0[d]);); (16, CUDA, cudaMalloc(&buffersD2D[d], numElems * sizeof(int));); (141, CUDA, cudaEventDestroy(start[d]);); (41, CUDA, cudaDeviceEnablePeerAccess(i, 0);); (7, CUDA, cudaCheckError();); (136, CUDA, cudaFree(buffersD2D[d]);); (35, CUDA, cudaDeviceEnablePeerAccess(j, 0);); (138, CUDA, cudaEventDestroy(stop[d]);); (25, CUDA, cudaSetDevice(i);); (106, CUDA, cudaDeviceDisablePeerAccess(i);); (139, CUDA, cudaStreamDestroy(stream1[d]);); (83, CUDA, cudaEventRecord(stop[i], stream0[i]);); (10, CUDA, cudaEventCreate(&stop[d]);); (140, CUDA, cudaFreeHost((void *)flag);); (79, CUDA, cudaStreamWaitEvent(stream0[i], stop[j], 0);); (3, CUDA, cudaMemset(buffersD2D[d], 0, numElems * sizeof(int));); (35, CUDA, cudaStreamSynchronize(stream1[j]);); (52, CUDA, cudaStreamWaitEvent(stream1[j], start[i], 0);); (-2, CUDA, cudaMemset(buffers[d], 0, numElems * sizeof(int));); (-4, CUDA, cudaMalloc(&buffers[d], numElems * sizeof(int));); (-10, CUDA, cudaHostAlloc((void **)&flag, sizeof(*flag), cudaHostAllocPortable);); (119, CUDA, cudaFree(buffers[d]);); (28, CUDA, cudaStreamSynchronize(stream0[i]);); (-9, CUDA, cudaSetDevice(d);); (0, CUDA, cudaStreamCreateWithFlags(&stream0[d], cudaStreamNonBlocking);); (13, CUDA, cudaDeviceCanAccessPeer(&access, i, j);); (42, CUDA, cudaEventRecord(start[i], stream0[i]);); (16, CUDA, cudaSetDevice(j);); (64, CUDA, cudaEventRecord(stop[j], stream1[j]);); (74, CUDA, cudaEventElapsedTime(&time_ms, start[i], stop[i]);); (-10, CUDA, cudaEventCreate(&start[d]);); (-5, CUDA, cudaStreamCreateWithFlags(&stream1[d], cudaStreamNonBlocking);); (81, CUDA, cudaDeviceDisablePeerAccess(j);)"}
{"predict": "(13, OpenMP, #pragma omp master taskloop collapse (ST); (3, OpenMP, #pragma omp master taskloop collapse (); (23, OpenMP, #pragma omp master taskloop collapse (S)); (12, OpenMP, #pragma omp master taskloop collapse (1)); (31, OpenMP, #pragma omp master taskloop collapse (N)); (1, OpenMP, #pragma omp master taskloop collapse ()); (17, OpenMP, #pragma omp master taskloop collapse (foobool(argc)), collapse (true), collapse (-5)); (30, OpenMP, #pragma omp master taskloop collapse (2)); (5, OpenMP, #pragma omp master taskloop collapse (argc); (8, OpenMP, #pragma omp master taskloop collapse ((ST > 0) ? 1 + ST : 2)); (-8, OpenMP, #pragma omp master taskloop collapse)", "label": "(10, OpenMP, #pragma omp master taskloop collapse (argc); (1, OpenMP, #pragma omp master taskloop collapse); (13, OpenMP, #pragma omp master taskloop collapse (1)); (14, OpenMP, #pragma omp master taskloop collapse ((ST > 0) ? 1 + ST : 2)); (31, OpenMP, #pragma omp master taskloop collapse (N)); (32, OpenMP, #pragma omp master taskloop collapse (2)); (16, OpenMP, #pragma omp master taskloop collapse (foobool(argc)), collapse (true), collapse (-5)); (-3, OpenMP, #pragma omp master taskloop collapse (); (-2, OpenMP, #pragma omp master taskloop collapse ()); (15, OpenMP, #pragma omp master taskloop collapse (S)); (3, OpenMP, #pragma omp master taskloop collapse (ST)"}
{"predict": "(218, CUDA, result.error = cudaEventRecord(events[0]);); (239, CUDA, std::cerr << \"cudaEventSynchronize() failed: \" << cudaGetErrorString(result.error) << std::endl;); (246, CUDA, std::cerr << \"cudaEventElapsed() failed: \" << cudaGetErrorString(result.error) << std::endl;); (217, CUDA, std::cerr << \"cudaEventRecord() failed: \" << cudaGetErrorString(result.error) << std::endl;); (228, CUDA, result.error = cudaEventRecord(events[1]);); (253, CUDA, (void)cudaEventDestroy(event);); (206, CUDA, std::cerr << \"cudaEventCreate() failed: \" << cudaGetErrorString(result.error) << std::endl;); (232, CUDA, result.error = cudaEventElapsedTime(&runtime_ms, events[0], events[1]);); (223, CUDA, result.error = cudaEventSynchronize(events[1]);); (201, CUDA, result.error = cudaEventCreate(&event);)", "label": "(161, CUDA, thrust::device_vector<ElementA> d_A = h_A;); (183, CUDA, cuda_err = cudaDeviceSynchronize();); (148, CUDA, thrust::host_vector<ElementB> h_B(N_size * K_size * L_size);); (184, CUDA, std::cerr << cudaGetErrorString(cuda_err) << \"\\n\";); (148, CUDA, thrust::host_vector<ElementD> h_D(M_size * N_size * L_size);); (157, CUDA, thrust::device_vector<ElementB> d_B = h_B;); (158, CUDA, thrust::device_vector<ElementD> cutlass_result = h_D;); (144, CUDA, thrust::host_vector<ElementC> h_C(M_size * N_size * L_size);); (213, CUDA, thrust::host_vector<ElementD> h_cutlass_result = cutlass_result;); (156, CUDA, thrust::device_vector<ElementD> reference_result = h_D;); (153, CUDA, thrust::device_vector<ElementC> d_C = h_C;); (138, CUDA, thrust::host_vector<ElementA> h_A(M_size * K_size * L_size);)"}
{"predict": "(18, OpenMP, #pragma omp target teams); (203, OpenMP, #pragma omp target update from(flux_G[:,:,:][0:stencil_size])); (18, OpenMP, #pragma omp distribute); (237, OpenMP, #pragma omp target update from(rsd[:,:,:][0:stencil_size])); (162, OpenMP, #pragma omp target teams distribute parallel for); (233, OpenMP, #pragma omp target update from(frct[:,:,:][0:stencil_size])); (18, OpenMP, #pragma omp distribute private(k,j,i,m)); (20, OpenMP, #pragma omp target teams private(k,j,i,m)); (149, OpenMP, #pragma omp target teams distribute parallel for private(k,j,i,m,tmp,u21i,u31i,u41i,u51i,u21im1,u(21, OpenMP, #pragma omp distribute private(k,j,i,m,zeta,eta,xi)); (191, OpenMP, #pragma omp target update from(rsd[:,:,:][stencil_size:]); (222, OpenMP, #pragma omp target update to(flux_G[:,:,:][0:stencil_size])); (17, OpenMP, #pragma omp distribute private(k,j,i,m,zeta,eta,xi)); (116, OpenMP, #pragma omp target teams distribute parallel for private(k,j,i,m,tmp,u21j,u31j,u41j,u51j,u21jm1,u(225, OpenMP, #pragma omp target update to(frct[:,:,:][0:stencil_size])); (181, OpenMP, #pragma omp target teams distribute parallel for private(k,j,i,m,tmp,u21k,u31k,u41k,u51k,u21km1,u(160, OpenMP, #pragma omp target teams distribute parallel for private(k,j,i,m,tmp)); (171, OpenMP, #pragma omp target teams distribute parallel for private(j,i,m,tmp)); (154, OpenMP, #pragma omp target teams distribute parallel for private(i,m,tmp)); (22, OpenMP, #pragma omp target teams distribute parallel for); (11, OpenMP, #pragma omp distribute private(k)); (152, OpenMP, #pragma", "label": "(22, OpenMP, #pragma omp distribute); (17, OpenMP, #pragma omp target data map(alloc: frct, rsd, ce, flux_G)); (19, OpenMP, #pragma omp target teams)"}
{"predict": "(22, CUDA, CUDA_RT_CALL(cudaMemset(a_new, 0, nx * ny * sizeof(real)));); (100, CUDA, CUDA_RT_CALL(cudaStreamDestroy(compute_stream));); (105, CUDA, CUDA_RT_CALL(cudaFree(a));); (23, CUDA, CUDA_RT_CALL(cudaDeviceSynchronize());); (100, CUDA, CUDA_RT_CALL(cudaFree(l2_norm_d));); (27, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&push_top_done, cudaEventDisableTiming));); (24, CUDA, CUDA_RT_CALL(cudaStreamCreate(&push_bottom_stream));); (26, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&push_bottom_done, cudaEventDisableTiming));); (49, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(compute_stream, push_bottom_done, 0));); (67, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(push_bottom_stream, compute_done, 0));); (69, CUDA, CUDA_RT_CALL(cudaEventRecord(push_bottom_done, push_bottom_stream));); (89, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_top_stream));); (86, CUDA, CUDA_RT_CALL(cudaEventDestroy(compute_done));); (81, CUDA, CUDA_RT_CALL(cudaMemcpy(a_ref_h, a, nx * ny * sizeof(real), cudaMemcpyDeviceToHost));); (82, CUDA, CUDA_RT_CALL(cudaEventDestroy(push_bottom_done));); (14, CUDA, CUDA_RT_CALL(cudaStreamCreate(&push_top_stream));); (81, CUDA, CUDA_RT_CALL(cudaEventDestroy(push_top_done));); (65, CUDA, CUDA_RT_CALL(cudaStreamSynchronize(compute_stream));); (44, CUDA, CUDA_RT_CALL(cudaEventRecord(compute_done, compute_stream));); (17, CUDA, CUDA_RT_CALL(cudaMallocHost(&l2_norm_h, sizeof(real)));); (54, CUDA, CUDA_RT_CALL(cudaEventRecord(push_top_done, push_top_stream));); (33, CUDA, CUDA_RT_CALL(cudaMemsetAsync(l2_norm_d, 0, sizeof(real), compute_stream));); (6, CUDA, CUDA_RT_CALL(cudaStreamCreate(&compute_stream));); (-2, CUDA, CUDA_RT_CALL(cudaMemset(a, 0, nx * ny * sizeof(real)));); (7, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&compute_done,", "label": "(22, CUDA, CUDA_RT_CALL(cudaMemset(a_new, 0, nx * ny * sizeof(real)));); (100, CUDA, CUDA_RT_CALL(cudaStreamDestroy(compute_stream));); (105, CUDA, CUDA_RT_CALL(cudaFree(a));); (23, CUDA, CUDA_RT_CALL(cudaDeviceSynchronize());); (100, CUDA, CUDA_RT_CALL(cudaFree(l2_norm_d));); (27, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&push_top_done, cudaEventDisableTiming));); (24, CUDA, CUDA_RT_CALL(cudaStreamCreate(&push_bottom_stream));); (26, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&push_bottom_done, cudaEventDisableTiming));); (49, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(compute_stream, push_bottom_done, 0));); (67, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(push_bottom_stream, compute_done, 0));); (69, CUDA, CUDA_RT_CALL(cudaEventRecord(push_bottom_done, push_bottom_stream));); (89, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_top_stream));); (86, CUDA, CUDA_RT_CALL(cudaEventDestroy(compute_done));); (81, CUDA, CUDA_RT_CALL(cudaMemcpy(a_ref_h, a, nx * ny * sizeof(real), cudaMemcpyDeviceToHost));); (82, CUDA, CUDA_RT_CALL(cudaEventDestroy(push_bottom_done));); (14, CUDA, CUDA_RT_CALL(cudaStreamCreate(&push_top_stream));); (81, CUDA, CUDA_RT_CALL(cudaEventDestroy(push_top_done));); (65, CUDA, CUDA_RT_CALL(cudaStreamSynchronize(compute_stream));); (44, CUDA, CUDA_RT_CALL(cudaEventRecord(compute_done, compute_stream));); (17, CUDA, CUDA_RT_CALL(cudaMallocHost(&l2_norm_h, sizeof(real)));); (54, CUDA, CUDA_RT_CALL(cudaEventRecord(push_top_done, push_top_stream));); (33, CUDA, CUDA_RT_CALL(cudaMemsetAsync(l2_norm_d, 0, sizeof(real), compute_stream));); (6, CUDA, CUDA_RT_CALL(cudaStreamCreate(&compute_stream));); (-2, CUDA, CUDA_RT_CALL(cudaMemset(a, 0, nx * ny * sizeof(real)));); (7, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&compute_done, cudaEventDisableTiming));); (81, CUDA, CUDA_RT_CALL(cudaFree(a_new));); (45, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(push_top_stream, compute_done, 0));); (72, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_bottom_stream));); (28, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(compute_stream, push_top_done, 0));); (6, CUDA, CUDA_RT_CALL(cudaMalloc(&l2_norm_d, sizeof(real)));); (21, MPI, double start = MPI_Wtime();); (-13, CUDA, CUDA_RT_CALL(cudaMalloc(&a, nx * ny * sizeof(real)));); (71, CUDA, CUDA_RT_CALL(cudaFreeHost(l2_norm_h));); (-14, CUDA, CUDA_RT_CALL(cudaMalloc(&a_new, nx * ny * sizeof(real)));)"}
{"predict": "(71, OpenMP, const int th=omp_get_thread_num();); (10, OpenMP, #pragma omp parallel for schedule (guided))", "label": "(71, OpenMP, const int th=omp_get_thread_num();); (10, OpenMP, #pragma omp parallel for schedule (guided))"}
{"predict": "(43, MPI, err = MPI_Type_size(eviltype, &val);); (99, MPI, MPI_Type_free(&inttype);); (56, MPI, err = MPI_Type_get_extent(eviltype, &lb, &extent);); (96, MPI, MPI_Type_free(&tmptype);); (6, MPI, err = MPI_Type_contiguous(4, MPI_BYTE, &tmptype);); (16, MPI, err = MPI_Type_create_resized(tmptype, 6, -9, &inttype);); (95, MPI, MPI_Type_free(&eviltype);); (73, MPI, err = MPI_Type_get_true_extent(eviltype, &true_lb, &aval);); (24, MPI, err = MPI_Type_vector(3, 4, -5, inttype, &eviltype);)", "label": "(43, MPI, err = MPI_Type_size(eviltype, &val);); (100, MPI, MPI_Type_free(&inttype);); (19, MPI, err = MPI_Type_create_resized(tmptype, 6, -9, &inttype);); (55, MPI, err = MPI_Type_get_extent(eviltype, &lb, &extent);); (28, MPI, err = MPI_Type_vector(3, 4, 5, inttype, &eviltype);); (95, MPI, MPI_Type_free(&tmptype);); (3, MPI, err = MPI_Type_contiguous(4, MPI_BYTE, &tmptype);); (95, MPI, MPI_Type_free(&eviltype);); (71, MPI, err = MPI_Type_get_true_extent(eviltype, &true_lb, &aval);)"}
{"predict": "(34, MPI, MPI_Brecv(&rmsg2[0], 2, MPI_DOUBLE, src, tag + 2, comm, MPI_STATUS_IGNORE);); (13, MPI, MPI_Comm_rank(comm, &rank);); (33, MPI, MPI_Brecv(rmsg1, s1, MPI_CHAR, src, tag + 1, comm, MPI_STATUS_IGNORE);); (26, MPI, MPI_Buffer_attach(buf, bufsize);); (33, MPI, MPI_Brecv(rmsg3, s3, MPI_CHAR, src, tag + 3, comm, MPI_STATUS_IGNORE);); (25, MPI, MPI_Bsend(msg1, s1, MPI_CHAR, dest, tag + 1, comm);); (12, MPI, MPI_Pack_size(1, msg2, 2, MPI_DOUBLE, comm, &s2);); (25, MPI, MPI_Bsend(msg2, 2, MPI_DOUBLE, dest, tag + 2, comm);); (10, MPI, MPI_Pack_size(1, msg1, s1, MPI_CHAR, comm, &s1);); (51, MPI, MPI_Buffer_detach(&bbuf, &bsize);); (23, MPI, MPI_Bsend(msg3, s3, MPI_CHAR, dest, tag + 3, comm);); (8, MPI, MPI_Pack_size(1, msg3, s3, MPI_CHAR, comm, &s3);)", "label": "(19, MPI, MPI_Pack_size(2, MPI_DOUBLE, comm, &s2);); (38, MPI, MPI_Recv(rmsg2, 10, MPI_DOUBLE, src, tag, comm, MPI_STATUS_IGNORE);); (21, MPI, MPI_Buffer_attach(buf, bufsize);); (56, MPI, MPI_Buffer_detach(&bbuf, &bsize);); (34, MPI, MPI_Recv(rmsg1, 7, MPI_CHAR, src, tag, comm, MPI_STATUS_IGNORE);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (27, MPI, MPI_Bsend(msg2, 2, MPI_DOUBLE, dest, tag, comm);); (13, MPI, MPI_Pack_size(17, MPI_CHAR, comm, &s3);); (24, MPI, MPI_Bsend(msg1, 7, MPI_CHAR, dest, tag, comm);); (25, MPI, MPI_Bsend(msg3, 17, MPI_CHAR, dest, tag, comm);); (8, MPI, MPI_Pack_size(7, MPI_CHAR, comm, &s1);); (29, MPI, MPI_Recv(rmsg3, 17, MPI_CHAR, src, tag, comm, MPI_STATUS_IGNORE);)"}
{"predict": "(44, CUDA, checkCudaErrors(cudaMemset(d_Output, 0, QRNG_DIMENSIONS * N * sizeof(float)));); (20, CUDA, int dev = findCudaDevice(argc, (const char **)argv);); (50, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (123, CUDA, checkCudaErrors(cudaFree(d_Output));); (19, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, dev));); (29, CUDA, checkCudaErrors(cudaMalloc((void **)&d_Output, QRNG_DIMENSIONS * N * sizeof(float)));); (60, CUDA, checkCudaErrors(cudaMemcpy(h_OutputGPU, d_Output, QRNG_DIMENSIONS * N * sizeof(float), cudaMemcpyDeviceToHost));)", "label": "(44, CUDA, checkCudaErrors(cudaMemset(d_Output, 0, QRNG_DIMENSIONS * N * sizeof(float)));); (20, CUDA, int dev = findCudaDevice(argc, (const char **)argv);); (49, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (123, CUDA, checkCudaErrors(cudaFree(d_Output));); (18, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, dev));); (28, CUDA, checkCudaErrors(cudaMalloc((void **)&d_Output, QRNG_DIMENSIONS * N * sizeof(float)));); (60, CUDA, checkCudaErrors(cudaMemcpy(h_OutputGPU, d_Output, QRNG_DIMENSIONS * N * sizeof(float), cudaMemcpyDeviceToHost));)"}
{"predict": "(252, OpenMP, nthreads = omp_get_num_threads();); (237, OpenMP, #pragma omp parallel firstprivate(niter) private(step)); (217, OpenMP, #pragma omp parallel); (240, OpenMP, #pragma omp master)", "label": "(249, OpenMP, nthreads = omp_get_num_threads();); (234, OpenMP, #pragma omp parallel firstprivate(niter) private(step)); (214, OpenMP, #pragma omp parallel); (237, OpenMP, #pragma omp master)"}
{"predict": "(214, MPI, MPI_Send(&sbuf, 1, MPI_INT, 0, 99, leader_comm);); (181, MPI, MPI_Recv(&rbuf, 1, MPI_INT, peer, 99, leader_comm, &status);); (234, MPI, MPI_Bcast(methods_used, n, MPI_UNSIGNED_CHAR, 0, MPI_COMM_WORLD);); (102, MPI, MPI_Bcast(&hoststring, len, MPI_CHAR, 0, leader_comm);); (68, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (35, MPI, MPI_Comm_free(&local_comm);); (216, MPI, MPI_Recv(&rbuf, 1, MPI_INT, speer, 99, leader_comm, &status);); (34, MPI, MPI_Comm_free(&leader_comm);); (22, MPI, MPI_Comm_size(leader_comm, &nleaderranks);); (112, MPI, MPI_Wait(&rreq, &status);); (62, MPI, MPI_Alloc_mem((numhosts * sizeof(int) * (hpmp_myrank?1:numhosts)), MPI_INFO_NULL, &methods_used);); (108, MPI, MPI_Wait(&sreq, &status);); (222, MPI, MPI_Free_mem(methods_used);); (131, MPI, MPI_Send(&sbuf, 1, MPI_INT, rpeer, 99, leader_comm);); (53, MPI, MPI_Comm_rank(leader_comm, &myleaderrank);); (162, MPI, MPI_Send(&sbuf, 1, MPI_INT, peer, 99, leader_comm);); (26, MPI, MPI_Comm_rank(local_comm, &mylocalrank);); (98, MPI, MPI_Bcast(&nlocalranks, 1, MPI_INT, 0, leader_comm);); (165, MPI, MPI_Recv(&rbuf, 1, MPI_INT, speer, 99, leader_comm, &status);); (105, MPI, MPI_Irecv(&rbuf, 1, MPI_INT, rpeer, 99, leader_comm, &rreq);); (169, MPI, MPI_Isend(&", "label": "(150, MPI, MPI_Type_commit(&mydt);); (154, MPI, MPI_Op_free(&myop);); (154, MPI, MPI_Type_free(&mydt);); (146, MPI, MPI_Type_contiguous(sizeof(comm_method_string_conversion_t), MPI_BYTE, &mydt);); (147, MPI, MPI_Op_create(myfn, 1, &myop);)"}
{"predict": "(46, CUDA, thrust::fill(thrust::device, B.begin(), B.end(), 0.0);); (44, CUDA, thrust::sequence(thrust::device, A.begin(), A.end() );); (40, CUDA, thrust::device_vector<double> A(order*order);); (40, CUDA, thrust::device_vector<double> B(order*order);)", "label": "(46, CUDA, thrust::fill(thrust::device, B.begin(), B.end(), 0.0);); (44, CUDA, thrust::sequence(thrust::device, A.begin(), A.end() );); (40, CUDA, thrust::device_vector<double> A(order*order);); (40, CUDA, thrust::device_vector<double> B(order*order);)"}
{"predict": "(11, TBB, tbb::flow::multifunction_node< InputType, OutputType > exe_node( g, lc, cf );); (49, TBB, tbb::flow::remove_edge( tbb::flow::output_port<0>(exe_node), *receivers[r] );); (66, TBB, tbb::flow::reset( tbb::flow::rf_reset_bodies );); (27, TBB, tbb::flow::make_edge( senders[s], exe_node );); (5, TBB, tbb::flow::graph g;); (17, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(exe_node), *receivers[r] );); (60, TBB, inc_functor body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (44, TBB, tbb::flow::remove_edge( tbb::flow::output_port<0>(exe_node), *receivers[r] );)", "label": "(69, TBB, g.reset(tbb::flow::rf_reset_bodies);); (28, TBB, tbb::flow::make_edge( senders[s], exe_node );); (19, TBB, tbb::flow::make_edge( exe_node, *receivers[r] );); (61, TBB, inc_functor body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (66, TBB, body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (0, TBB, tbb::flow::graph g;); (44, TBB, tbb::flow::remove_edge( exe_node, *receivers[r] );); (4, TBB, tbb::flow::function_node< InputType, OutputType > exe_node( g, lc, cf );)"}
{"predict": "(11, TBB, tbb::flow::multifunction_node< InputType, OutputType > exe_node( g, lc, cf );); (15, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(exe_node), receivers[r] );); (43, TBB, tbb::flow::remove_edge( tbb::flow::output_port<0>(exe_node), receivers[r] );); (21, TBB, tbb::flow::make_edge( senders[s], exe_node );); (52, TBB, inc_functor body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (0, TBB, tbb::flow::graph g;)", "label": "(16, TBB, tbb::flow::make_edge( exe_node, receivers[r] );); (23, TBB, tbb::flow::make_edge( senders[s], exe_node );); (43, TBB, tbb::flow::remove_edge( exe_node, receivers[r] );); (53, TBB, inc_functor body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (1, TBB, tbb::flow::graph g;); (6, TBB, tbb::flow::function_node< InputType, OutputType > exe_node( g, lc, cf );)"}
{"predict": "(35, MPI, MPI_Comm_rank(adio_fh->comm, &myrank);); (111, MPI, MPI_Barrier(adio_fh->comm);)", "label": "(37, MPI, MPI_Comm_rank(adio_fh->comm, &myrank);); (24, MPI, MPI_Bcast(&tmp_whence, 1, MPI_INT, 0, adio_fh->comm);); (12, MPI, MPI_Bcast(&tmp_offset, 1, ADIO_OFFSET, 0, adio_fh->comm);); (110, MPI, MPI_Barrier(adio_fh->comm);)"}
{"predict": "(108, MPI, MPI_File_write(file, &size_array, 1, MPI_INT, &status);); (102, MPI, MPI_File_open(MPI_COMM_WORLD, file_name.c_str(), MPI_MODE_CREATE | MPI_MODE_WRONLY, MPI_INFO_NULL, &file);); (111, MPI, MPI_File_close(&file);); (105, MPI, MPI_File_seek(file, mpi_offset, MPI_SEEK_SET);); (108, MPI, MPI_File_write(file, &pos_values[0], data_size[rank], MPI_DOUBLE, &status);)", "label": "(99, MPI, MPI_File_open(MPI_COMM_WORLD, file_name.c_str(), MPI_MODE_CREATE | MPI_MODE_WRONLY, MPI_INFO_NULL, &file);); (105, MPI, MPI_File_write(file, &size_array, 1, MPI_INT, &status);); (110, MPI, MPI_File_close(&file);); (102, MPI, MPI_File_seek(file, mpi_offset, MPI_SEEK_SET);); (107, MPI, MPI_File_write(file, &pos_values[0], data_size[rank], MPI_DOUBLE, &status);)"}
{"predict": "(116, CUDA, CUDA_CHECK(cudaEventRecord(events[1]));); (106, CUDA, CUDA_CHECK(cudaEventCreate(&event));); (116, CUDA, CUDA_CHECK(cudaEventElapsedTime(&runtime_ms, events[0], events[1]));); (103, CUDA, CUDA_CHECK(cudaEventRecord(events[0]));); (111, CUDA, CUDA_CHECK(cudaEventSynchronize(events[1]));); (56, CUDA, CUDA_CHECK(cudaMalloc(&p.workspace, p.workspace_size()));); (12, CUDA, CUDA_CHECK(cudaSetDevice(0));); (13, CUDA, std::cerr << \"CUDA error: \" << cudaGetErrorString(cudaError) << std::endl;); (106, CUDA, CUDA_CHECK(cudaEventSynchronize(events[0]));)", "label": "(85, CUDA, cudaDeviceSynchronize();); (59, CUDA, cudaMalloc(&p.workspace, p.workspace_size());); (114, CUDA, CUDA_CHECK(cudaEventSynchronize(events[1]));); (112, CUDA, CUDA_CHECK(cudaEventRecord(events[1]));); (84, CUDA, CUDA_CHECK(cudaFuncSetAttribute(kernel_fn, cudaFuncAttributeMaxDynamicSharedMemorySize, int(smem_bytes)));); (114, CUDA, CUDA_CHECK(cudaEventElapsedTime(&runtime_ms, events[0], events[1]));); (9, CUDA, std::cerr << \"FATAL: \" #FN \" failed: \" << cudaGetErrorString(cudaError) << std::endl;); (102, CUDA, CUDA_CHECK(cudaEventCreate(&event));); (103, CUDA, CUDA_CHECK(cudaEventRecord(events[0]));)"}
{"predict": "(104, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ j + 1 ], 0, MPI_COMM_WORLD );); (98, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ 1 ], 1, MPI_COMM_WORLD );); (168, MPI, MPI_Finalize();); (8, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &Parallel::pid );); (5, MPI, MPI_Init( &argc, &argv );); (103, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ j - 1 ], 1, MPI_COMM_WORLD );); (93, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ 0 ], 0, MPI_COMM_WORLD );); (3, MPI, MPI_Comm_size( MPI_COMM_WORLD, &Parallel::nprocs );); (103, MPI, MPI_Recv( A[ 3 ].data(), N + 1, MPI_DOUBLE, activep[ j - 1 ], 0, MPI_COMM_WORLD, &status );); (109, MPI, MPI_Recv( A[ 4 ].data(), N + 1, MPI_DOUBLE, activep[ j + 1 ], 1, MPI_COMM_WORLD, &status );); (97, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ numactivep - 2 ], 1, MPI_COMM_WORLD );); (160, MPI, MPI_Barrier( MPI_COMM_WORLD );); (91, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ 1 ], 0, MPI_COMM_WORLD );); (101, MPI, MPI_Recv( A[ 3 ].data(), N + 1, MPI_DOUBLE, activep[ 0 ], 1, MPI_COMM_WORLD, &status );); (105, MPI, MPI_Recv( A[ 4 ].data(), N + 1, MPI_DOUBLE, activep[ numactivep - 1 ], 0, MPI_COMM_WORLD, &status );); (150, MPI, MPI_Allgather( &", "label": "(106, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ j + 1 ], 0, MPI_COMM_WORLD );); (104, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ j - 1 ], 1, MPI_COMM_WORLD );); (108, MPI, MPI_Recv( A[ 3 ].data(), N + 1, MPI_DOUBLE, activep[ j - 1 ], 0, MPI_COMM_WORLD, &status );); (7, MPI, MPI_Comm_size( MPI_COMM_WORLD, &Parallel::nprocs );); (97, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ numactivep - 2 ], 1, MPI_COMM_WORLD );); (135, MPI, MPI_Allgather( &tmp, 1, MPI_DOUBLE, x.data(), 1, MPI_DOUBLE, MPI_COMM_WORLD);); (187, MPI, MPI_Finalize();); (157, MPI, MPI_Allgather( &tmp, 1, MPI_DOUBLE, x.data(), 1, MPI_DOUBLE, MPI_COMM_WORLD );); (1, MPI, MPI_Init( &argc, &argv );); (2, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &Parallel::pid );); (101, MPI, MPI_Recv( A[ 4 ].data(), N + 1, MPI_DOUBLE, activep[ j + 1 ], 1, MPI_COMM_WORLD, &status );); (86, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ 1 ], 0, MPI_COMM_WORLD );)"}
{"predict": "(34, TBB, oneapi::tbb::tick_count start = oneapi::tbb::tick_count::now();); (69, TBB, utility::report_elapsed_time((oneapi::tbb::tick_count::now() - start).seconds());); (49, TBB, oneapi::tbb::flow::make_edge(the_source, the_value_pool);); (53, TBB, oneapi::tbb::flow::make_edge(the_value_pool, *(bins[i]));); (44, TBB, oneapi::tbb::flow::graph g;); (31, TBB, oneapi::tbb::global_control c(oneapi::tbb::global_control::max_allowed_parallelism, p);)", "label": "(34, TBB, oneapi::tbb::tick_count start = oneapi::tbb::tick_count::now();); (69, TBB, utility::report_elapsed_time((oneapi::tbb::tick_count::now() - start).seconds());); (49, TBB, oneapi::tbb::flow::make_edge(the_source, the_value_pool);); (53, TBB, oneapi::tbb::flow::make_edge(the_value_pool, *(bins[i]));); (44, TBB, oneapi::tbb::flow::graph g;); (31, TBB, oneapi::tbb::global_control c(oneapi::tbb::global_control::max_allowed_parallelism, p);)"}
{"predict": "(37, OpenMP, #pragma omp target teams distribute parallel for simd safelen (S)); (17, OpenMP, #pragma omp target teams distribute parallel for simd safelen (ST); (20, OpenMP, #pragma omp target teams distribute parallel for simd safelen (1))); (51, OpenMP, #pragma omp target teams distribute parallel for simd safelen (N)); (46, OpenMP, #pragma omp target teams distribute parallel for simd safelen (4)); (2, OpenMP, #pragma omp target teams distribute parallel for simd safelen (); (20, OpenMP, #pragma omp target teams distribute parallel for simd safelen ((ST > 0) ? 1 + ST : 2)); (25, OpenMP, #pragma omp target teams distribute parallel for simd safelen (foobool(argc)), safelen (true), safelen (-5)); (3, OpenMP, #pragma omp target teams distribute parallel for simd safelen ()); (5, OpenMP, #pragma omp target teams distribute parallel for simd safelen (argc); (-7, OpenMP, #pragma omp target teams distribute parallel for simd safelen)", "label": "(37, OpenMP, #pragma omp target teams distribute parallel for simd safelen (S)); (17, OpenMP, #pragma omp target teams distribute parallel for simd safelen (ST); (20, OpenMP, #pragma omp target teams distribute parallel for simd safelen (1))); (51, OpenMP, #pragma omp target teams distribute parallel for simd safelen (N)); (46, OpenMP, #pragma omp target teams distribute parallel for simd safelen (4)); (2, OpenMP, #pragma omp target teams distribute parallel for simd safelen (); (20, OpenMP, #pragma omp target teams distribute parallel for simd safelen ((ST > 0) ? 1 + ST : 2)); (26, OpenMP, #pragma omp target teams distribute parallel for simd safelen (foobool(argc)), safelen (true), safelen (-5)); (3, OpenMP, #pragma omp target teams distribute parallel for simd safelen ()); (5, OpenMP, #pragma omp target teams distribute parallel for simd safelen (argc); (-7, OpenMP, #pragma omp target teams distribute parallel for simd safelen)"}
{"predict": "(7, OpenMP, #pragma omp for linear (); (24, OpenMP, #pragma omp for linear (argv[1])); (17, OpenMP, #pragma omp for linear (S1)); (12, OpenMP, #pragma omp for linear (argc > 0 ? argv[1] : argv[2])); (24, OpenMP, #pragma omp for linear(e, g, k)); (26, OpenMP, #pragma omp parallel); (33, OpenMP, #pragma omp for linear(j)); (3, OpenMP, #pragma omp for linear (argc); (34, OpenMP, #pragma omp for linear(v:j)); (-1, OpenMP, #pragma omp for linear ()); (2, OpenMP, #pragma omp for linear (argc,); (15, OpenMP, #pragma omp for linear (argv[1], e)); (34, OpenMP, #pragma omp for linear(i) ordered(1)); (2, OpenMP, #pragma omp for linear (argc : 5) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (15, OpenMP, #pragma omp for linear(v:i)); (18, OpenMP, #pragma omp for linear(i))", "label": "(7, OpenMP, #pragma omp for linear (); (24, OpenMP, #pragma omp for linear (argv[1])); (17, OpenMP, #pragma omp for linear (S1)); (12, OpenMP, #pragma omp for linear (argc > 0 ? argv[1] : argv[2])); (1, OpenMP, #pragma omp for linear); (22, OpenMP, #pragma omp for linear(e, g, k)); (27, OpenMP, #pragma omp parallel); (33, OpenMP, #pragma omp for linear(j)); (3, OpenMP, #pragma omp for linear (argc); (34, OpenMP, #pragma omp for linear(v:j)); (-1, OpenMP, #pragma omp for linear ()); (2, OpenMP, #pragma omp for linear (argc,); (11, OpenMP, #pragma omp for linear (a, b:B::ib)); (34, OpenMP, #pragma omp for linear(i) ordered(1)); (15, OpenMP, #pragma omp for linear(h)); (2, OpenMP, #pragma omp for linear (argc : 5) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (21, OpenMP, #pragma omp for linear(v:i)); (14, OpenMP, #pragma omp for linear(i))"}
{"predict": "(85, MPI, MPI_Bcast(&min_source, 1, MPI_INT, source, scr_comm_world);); (96, MPI, MPI_Allreduce(&min_depth, &min_global, 1, MPI_INT, MPI_MIN, scr_comm_world);); (76, MPI, MPI_Allreduce(&source_rank, &source, 1, MPI_INT, MPI_MIN, scr_comm_world);); (95, MPI, MPI_Allreduce(&max_depth, &max_global, 1, MPI_INT, MPI_MAX, scr_comm_world);); (168, MPI, MPI_Barrier(scr_comm_world);)", "label": "(85, MPI, MPI_Bcast(&min_source, 1, MPI_INT, source, scr_comm_world);); (96, MPI, MPI_Allreduce(&min_depth, &min_global, 1, MPI_INT, MPI_MIN, scr_comm_world);); (76, MPI, MPI_Allreduce(&source_rank, &source, 1, MPI_INT, MPI_MIN, scr_comm_world);); (95, MPI, MPI_Allreduce(&max_depth, &max_global, 1, MPI_INT, MPI_MAX, scr_comm_world);); (168, MPI, MPI_Barrier(scr_comm_world);)"}
{"predict": "(25, MPI, MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);); (40, MPI, err = MPI_File_close(&fh);); (4, MPI, MPI_Init(&argc, &argv);); (143, MPI, MPI_Finalize();); (22, MPI, MPI_Bcast(filename, len + 10, MPI_CHAR, 0, MPI_COMM_WORLD);); (2, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (39, MPI, MPI_Barrier(MPI_COMM_WORLD);); (12, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(25, MPI, MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);); (40, MPI, err = MPI_File_close(&fh);); (4, MPI, MPI_Init(&argc, &argv);); (143, MPI, MPI_Finalize();); (22, MPI, MPI_Bcast(filename, len + 10, MPI_CHAR, 0, MPI_COMM_WORLD);); (2, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (39, MPI, MPI_Barrier(MPI_COMM_WORLD);); (12, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(106, MPI, MPI_Reduce(ldinbuf, ldoutbuf, 3, MPI_LONG_DOUBLE, MPI_MAX, 0, comm);); (13, MPI, MPI_Comm_size(comm, &size);); (24, MPI, MPI_Reduce(cinbuf, coutbuf, 3, MPI_CHAR, MPI_MAX, 0, comm);); (49, MPI, MPI_Reduce(scinbuf, scoutbuf, 3, MPI_SIGNED_CHAR, MPI_MAX, 0, comm);); (73, MPI, MPI_Reduce(ucinbuf, ucoutbuf, 3, MPI_UNSIGNED_CHAR, MPI_MAX, 0, comm);); (8, MPI, MPI_Comm_rank(comm, &rank);); (132, MPI, MPI_Reduce(llinbuf, lloutbuf, 3, MPI_LONG_LONG, MPI_MAX, 0, comm);)", "label": "(106, MPI, MPI_Reduce(ldinbuf, ldoutbuf, 3, MPI_LONG_DOUBLE, MPI_MAX, 0, comm);); (13, MPI, MPI_Comm_size(comm, &size);); (24, MPI, MPI_Reduce(cinbuf, coutbuf, 3, MPI_CHAR, MPI_MAX, 0, comm);); (49, MPI, MPI_Reduce(scinbuf, scoutbuf, 3, MPI_SIGNED_CHAR, MPI_MAX, 0, comm);); (73, MPI, MPI_Reduce(ucinbuf, ucoutbuf, 3, MPI_UNSIGNED_CHAR, MPI_MAX, 0, comm);); (8, MPI, MPI_Comm_rank(comm, &rank);); (132, MPI, MPI_Reduce(llinbuf, lloutbuf, 3, MPI_LONG_LONG, MPI_MAX, 0, comm);)"}
{"predict": "(12, CUDA, __shared__ FP sV[Bc][dim];); (20, CUDA, __shared__ FP sMax[Br];); (9, CUDA, __shared__ FP sK[Bc][dim];); (11, CUDA, __shared__ FP sQK[Br][Bc];); (14, CUDA, __shared__ FP sSafeE[Br][Bc];); (8, CUDA, __shared__ FP sO[Br][dim];); (10, CUDA, __shared__ FP sNewO[Br][dim];); (3, CUDA, __shared__ FP sQ[Br][dim];); (12, CUDA, __shared__ FP sDenom[Br];)", "label": "(12, CUDA, __shared__ FP sV[Bc][dim];); (20, CUDA, __shared__ FP sMax[Br];); (9, CUDA, __shared__ FP sK[Bc][dim];); (11, CUDA, __shared__ FP sQK[Br][Bc];); (14, CUDA, __shared__ FP sSafeE[Br][Bc];); (8, CUDA, __shared__ FP sO[Br][dim];); (10, CUDA, __shared__ FP sNewO[Br][dim];); (3, CUDA, __shared__ FP sQ[Br][dim];); (12, CUDA, __shared__ FP sDenom[Br];)"}
{"predict": "(68, CUDA, cudaCheck(cudaMemset(d_dinp, 0, B * T * 3 * C * sizeof(float)));); (60, CUDA, cudaCheck(cudaMalloc(&d_dpreatt, B * NH * T * T * sizeof(float)));); (158, CUDA, cudaCheck(cudaFree(d_inp));); (69, CUDA, cudaCheck(cudaMemset(d_dvaccum, 0, B * T * C * sizeof(float)));); (86, CUDA, cudaCheck(cudaMemcpy(h_dinp, d_dinp, B * T * 3 * C * sizeof(float), cudaMemcpyDeviceToHost));); (163, CUDA, cudaCheck(cudaFree(d_dpreatt));); (160, CUDA, cudaCheck(cudaFree(d_dinp));); (160, CUDA, cudaCheck(cudaFree(d_dqkvr));); (153, CUDA, cudaCheck(cudaFree(d_qkvr));); (23, CUDA, cudaCheck(cudaMalloc(&d_att, B * NH * T * T * sizeof(float)));); (159, CUDA, cudaCheck(cudaFree(d_datt));); (58, CUDA, cudaCheck(cudaMemset(d_dqkvr, 0, B * T * 3 * C * sizeof(float)));); (150, CUDA, cudaCheck(cudaFree(d_preatt));); (50, CUDA, cudaCheck(cudaMalloc(&d_dvaccum, B * T * C * sizeof(float)));); (50, CUDA, cudaCheck(cudaMalloc(&d_dout, B * T * C * sizeof(float)));); (15, CUDA, cudaCheck(cudaMalloc(&d_qkvr, B * T * 3 * C * sizeof(float)));); (17, CUDA, cudaCheck(cudaMalloc(&d_vaccum, B * T * C * sizeof(float)));); (14, CUDA, cudaCheck(cudaMalloc(&d_preatt, B * NH * T * T * sizeof(float)));); (48, CUDA, cudaCheck(cudaMemcpy(d_dout, dout, B * T * C * sizeof(float), cudaMemcpyHostToDevice));); (145, CUDA, cudaCheck(cudaFree(d_vaccum));); (40, CUDA, cudaCheck(cudaMalloc(&d_dqkvr, B * T * 3 * C * sizeof(float)));); (38,", "label": "(68, CUDA, cudaCheck(cudaMemset(d_dinp, 0, B * T * 3 * C * sizeof(float)));); (60, CUDA, cudaCheck(cudaMalloc(&d_dpreatt, B * NH * T * T * sizeof(float)));); (157, CUDA, cudaCheck(cudaFree(d_inp));); (69, CUDA, cudaCheck(cudaMemset(d_dvaccum, 0, B * T * C * sizeof(float)));); (86, CUDA, cudaCheck(cudaMemcpy(h_dinp, d_dinp, B * T * 3 * C * sizeof(float), cudaMemcpyDeviceToHost));); (162, CUDA, cudaCheck(cudaFree(d_dpreatt));); (159, CUDA, cudaCheck(cudaFree(d_dinp));); (159, CUDA, cudaCheck(cudaFree(d_dqkvr));); (152, CUDA, cudaCheck(cudaFree(d_qkvr));); (23, CUDA, cudaCheck(cudaMalloc(&d_att, B * NH * T * T * sizeof(float)));); (158, CUDA, cudaCheck(cudaFree(d_datt));); (58, CUDA, cudaCheck(cudaMemset(d_dqkvr, 0, B * T * 3 * C * sizeof(float)));); (149, CUDA, cudaCheck(cudaFree(d_preatt));); (50, CUDA, cudaCheck(cudaMalloc(&d_dvaccum, B * T * C * sizeof(float)));); (50, CUDA, cudaCheck(cudaMalloc(&d_dout, B * T * C * sizeof(float)));); (15, CUDA, cudaCheck(cudaMalloc(&d_qkvr, B * T * 3 * C * sizeof(float)));); (17, CUDA, cudaCheck(cudaMalloc(&d_vaccum, B * T * C * sizeof(float)));); (14, CUDA, cudaCheck(cudaMalloc(&d_preatt, B * NH * T * T * sizeof(float)));); (48, CUDA, cudaCheck(cudaMemcpy(d_dout, dout, B * T * C * sizeof(float), cudaMemcpyHostToDevice));); (144, CUDA, cudaCheck(cudaFree(d_vaccum));); (40, CUDA, cudaCheck(cudaMalloc(&d_dqkvr, B * T * 3 * C * sizeof(float)));); (38, CUDA, cudaCheck(cudaMalloc(&d_dinp, B * T * 3 * C * sizeof(float)));); (49, CUDA, cudaCheck(cudaMemset(d_datt, 0, B * NH * T * T * sizeof(float)));); (141, CUDA, cudaCheck(cudaFree(d_out));); (145, CUDA, cudaCheck(cudaFree(d_dvaccum));); (9, CUDA, cudaCheck(cudaMalloc(&d_out, B * T * C * sizeof(float)));); (36, CUDA, cudaCheck(cudaMalloc(&d_datt, B * NH * T * T * sizeof(float)));); (2, CUDA, cudaCheck(cudaMalloc(&d_inp, B * T * 3 * C * sizeof(float)));); (134, CUDA, cudaCheck(cudaFree(d_att));); (41, CUDA, cudaCheck(cudaMemset(d_dpreatt, 0, B * NH * T * T * sizeof(float)));); (6, CUDA, cudaCheck(cudaMemcpy(d_inp, inp, B * T * 3 * C * sizeof(float), cudaMemcpyHostToDevice));); (139, CUDA, cudaCheck(cudaFree(d_dout));)"}
{"predict": "(26, MPI, MPI_Allreduce(&value, &tempvalue, 1, MPI_DOUBLE, MPI_MAX, grid->comm);); (50, MPI, MPI_Allreduce(rwork, temprwork, A->ncol, MPI_DOUBLE, MPI_SUM, grid->comm);)", "label": "(26, MPI, MPI_Allreduce(&value, &tempvalue, 1, MPI_DOUBLE, MPI_MAX, grid->comm);); (50, MPI, MPI_Allreduce(rwork, temprwork, A->ncol, MPI_DOUBLE, MPI_SUM, grid->comm);)"}
{"predict": "(50, MPI, MPI_Allreduce(MPI_IN_PLACE, &(flag), 1, mpi_int_t, MPI_MAX, grid3d->zscp.comm);); (105, MPI, MPI_Allreduce(  MPI_IN_PLACE, myusize, nsupers, mpi_int_t, MPI_MAX, grid->comm );); (102, MPI, MPI_Allreduce( MPI_IN_PLACE, mylsize, nsupers, mpi_int_t, MPI_MAX, grid->comm );); (50, MPI, MPI_Bcast( &(size), 1, mpi_int_t, gridid,  grid3d->zscp.comm);); (48, MPI, MPI_Allreduce(MPI_IN_PLACE, &(gridid), 1, mpi_int_t, MPI_MAX, grid3d->zscp.comm);)", "label": "(50, MPI, MPI_Allreduce(MPI_IN_PLACE, &(flag), 1, mpi_int_t, MPI_MAX, grid3d->zscp.comm);); (105, MPI, MPI_Allreduce(  MPI_IN_PLACE, myusize, nsupers, mpi_int_t, MPI_MAX, grid->comm );); (102, MPI, MPI_Allreduce( MPI_IN_PLACE, mylsize, nsupers, mpi_int_t, MPI_MAX, grid->comm );); (50, MPI, MPI_Bcast( &(size), 1, mpi_int_t, gridid,  grid3d->zscp.comm);); (48, MPI, MPI_Allreduce(MPI_IN_PLACE, &(gridid), 1, mpi_int_t, MPI_MAX, grid3d->zscp.comm);)"}
{"predict": "(21, MPI, MPI_Comm_create_group(MPI_COMM_WORLD, newgroup, 0, &newcomm);); (19, MPI, MPI_Group_excl(world, 1, deadprocs, &newgroup);); (7, MPI, MPI_Init(&argc, &argv);); (82, MPI, MPI_Group_free(&newgroup);); (84, MPI, MPI_Finalize();); (53, MPI, rc = MPI_Scatter(sendbuf, 100000, MPI_CHAR, rbuf, 100000, MPI_CHAR, 0, MPI_COMM_WORLD);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (76, MPI, MPI_Comm_free(&newcomm);); (3, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (76, MPI, MPI_Group_free(&world);); (25, MPI, rc = MPI_Scatter(sendbuf, 10, MPI_CHAR, rbuf, 10, MPI_CHAR, 0, MPI_COMM_WORLD);); (4, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (26, MPI, MPI_Error_class(rc, &errclass);); (5, MPI, MPI_Comm_group(MPI_COMM_WORLD, &world);)", "label": "(21, MPI, MPI_Comm_create_group(MPI_COMM_WORLD, newgroup, 0, &newcomm);); (19, MPI, MPI_Group_excl(world, 1, deadprocs, &newgroup);); (7, MPI, MPI_Init(&argc, &argv);); (82, MPI, MPI_Group_free(&newgroup);); (84, MPI, MPI_Finalize();); (53, MPI, rc = MPI_Scatter(sendbuf, 100000, MPI_CHAR, rbuf, 100000, MPI_CHAR, 0, MPI_COMM_WORLD);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (76, MPI, MPI_Comm_free(&newcomm);); (3, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (76, MPI, MPI_Group_free(&world);); (25, MPI, rc = MPI_Scatter(sendbuf, 10, MPI_CHAR, rbuf, 10, MPI_CHAR, 0, MPI_COMM_WORLD);); (4, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (26, MPI, MPI_Error_class(rc, &errclass);); (5, MPI, MPI_Comm_group(MPI_COMM_WORLD, &world);)"}
{"predict": "(203, OpenMP, #pragma omp parallel); (206, OpenMP, #pragma omp for schedule(dynamic,2) nowait)", "label": "(203, OpenMP, #pragma omp parallel); (206, OpenMP, #pragma omp for schedule(dynamic,2) nowait)"}
{"predict": "(24, OpenCL, test_clEnqueueCopyBuffer(&clEnqueueCopyBufferData[i]);); (27, OpenCL, test_clEnqueueCopyBufferRect(&clEnqueueCopyBufferRectData[i]);); (6, OpenCL, test_clEnqueueWriteBuffer(&clEnqueueWriteBufferData[i]);); (41, OpenCL, test_clEnqueueUnmapMemObject(&clEnqueueUnmapMemObjectData[i]);); (8, OpenCL, test_clEnqueueReadBufferRect(&clEnqueueReadBufferRectData[i]);); (-1, OpenCL, test_clEnqueueReadBuffer(&clEnqueueReadBufferData[i]);); (14, OpenCL, test_clEnqueueFillBuffer(&clEnqueueFillBufferData[i]);); (25, OpenCL, test_clEnqueueMapBuffer(&clEnqueueMapBufferData[i]);); (8, OpenCL, test_clEnqueueWriteBufferRect(&clEnqueueWriteBufferRectData[i]);)", "label": "(24, OpenCL, test_clEnqueueCopyBuffer(&clEnqueueCopyBufferData[i]);); (27, OpenCL, test_clEnqueueCopyBufferRect(&clEnqueueCopyBufferRectData[i]);); (6, OpenCL, test_clEnqueueWriteBuffer(&clEnqueueWriteBufferData[i]);); (41, OpenCL, test_clEnqueueUnmapMemObject(&clEnqueueUnmapMemObjectData[i]);); (8, OpenCL, test_clEnqueueReadBufferRect(&clEnqueueReadBufferRectData[i]);); (-1, OpenCL, test_clEnqueueReadBuffer(&clEnqueueReadBufferData[i]);); (14, OpenCL, test_clEnqueueFillBuffer(&clEnqueueFillBufferData[i]);); (25, OpenCL, test_clEnqueueMapBuffer(&clEnqueueMapBufferData[i]);); (8, OpenCL, test_clEnqueueWriteBufferRect(&clEnqueueWriteBufferRectData[i]);)"}
{"predict": "(31, MPI, MPI_Type_commit(&newtype);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (42, MPI, MPI_Send(buf, 1, newtype, 1, i, MPI_COMM_WORLD);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (66, MPI, MPI_Type_free(&newtype);); (24, MPI, MPI_Type_vector(i, 128, 129, MPI_CHAR, &newtype);); (70, MPI, MPI_Recv(&recv_errs, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (76, MPI, MPI_Send(&errs, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);); (39, MPI, MPI_Recv(buf, 1, newtype, 0, i, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)", "label": "(31, MPI, MPI_Type_commit(&newtype);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (42, MPI, MPI_Send(buf, 1, newtype, 1, i, MPI_COMM_WORLD);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (66, MPI, MPI_Type_free(&newtype);); (24, MPI, MPI_Type_vector(i, 128, 129, MPI_CHAR, &newtype);); (70, MPI, MPI_Recv(&recv_errs, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (76, MPI, MPI_Send(&errs, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);); (39, MPI, MPI_Recv(buf, 1, newtype, 0, i, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)"}
{"predict": "(106, MPI, MPI_Allgather(st_end_offset, 2, ADIO_OFFSET, all_st_end_offsets, 2, ADIO_OFFSET, fd->comm);); (71, MPI, MPI_Comm_rank(fd->comm, &myrank);); (527, MPI, MPI_Type_free(&client_comm_dtype_arr[i]);); (587, MPI, MPI_Type_free(&fd->file_realm_types[0]);); (67, MPI, MPI_Comm_size(fd->comm, &nprocs);); (589, MPI, MPI_Type_free(&fd->file_realm_types[i]);); (155, MPI, MPI_Type_get_extent(datatype, &lb, &extent);); (307, MPI, MPI_Type_size_x(agg_comm_dtype_arr[i], &size);); (301, MPI, MPI_Type_free(&agg_dtype);)", "label": "(106, MPI, MPI_Allgather(st_end_offset, 2, ADIO_OFFSET, all_st_end_offsets, 2, ADIO_OFFSET, fd->comm);); (71, MPI, MPI_Comm_rank(fd->comm, &myrank);); (527, MPI, MPI_Type_free(&client_comm_dtype_arr[i]);); (587, MPI, MPI_Type_free(&fd->file_realm_types[0]);); (67, MPI, MPI_Comm_size(fd->comm, &nprocs);); (589, MPI, MPI_Type_free(&fd->file_realm_types[i]);); (155, MPI, MPI_Type_get_extent(datatype, &lb, &extent);); (307, MPI, MPI_Type_size_x(agg_comm_dtype_arr[i], &size);); (301, MPI, MPI_Type_free(&agg_dtype);)"}
{"predict": "(207, MPI, MPI_Reduce(&total, &total_sum, 1, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD);); (230, MPI, MPI_Finalize();); (79, MPI, MPI_Bcast(&iterations,    1, MPI_INT, root, MPI_COMM_WORLD);); (21, MPI, MPI_Init(&argc,&argv);); (76, MPI, MPI_Bcast(&vector_length, 1, MPI_INT, root, MPI_COMM_WORLD);); (77, MPI, MPI_Bcast(&btype,         1, MPI_INT, root, MPI_COMM_WORLD);); (93, MPI, MPI_Barrier(MPI_COMM_WORLD);); (19, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (17, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)", "label": "(207, MPI, MPI_Reduce(&total, &total_sum, 1, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD);); (230, MPI, MPI_Finalize();); (79, MPI, MPI_Bcast(&iterations,    1, MPI_INT, root, MPI_COMM_WORLD);); (21, MPI, MPI_Init(&argc,&argv);); (76, MPI, MPI_Bcast(&vector_length, 1, MPI_INT, root, MPI_COMM_WORLD);); (77, MPI, MPI_Bcast(&btype,         1, MPI_INT, root, MPI_COMM_WORLD);); (93, MPI, MPI_Barrier(MPI_COMM_WORLD);); (19, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (17, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(44, MPI, MPI_Buffer_detach(&bptr, &bl);); (24, MPI, MPI_Recv(b, 10, MPI_INT, i, 27 + j, MPI_COMM_WORLD, &status);); (14, MPI, MPI_Bsend(a, 10, MPI_INT, 0, 27 + j, MPI_COMM_WORLD);); (7, MPI, MPI_Buffer_attach(buf, BUFSIZE);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)", "label": "(44, MPI, MPI_Buffer_detach(&bptr, &bl);); (24, MPI, MPI_Recv(b, 10, MPI_INT, i, 27 + j, MPI_COMM_WORLD, &status);); (14, MPI, MPI_Bsend(a, 10, MPI_INT, 0, 27 + j, MPI_COMM_WORLD);); (7, MPI, MPI_Buffer_attach(buf, BUFSIZE);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)"}
{"predict": "(4, TBB, tbb::flow::queue_node<T> q2(g);); (70, TBB, tbb::flow::make_edge( q2, q3 );); (67, TBB, tbb::flow::queue_node<T> q3(g);); (37, TBB, tbb::flow::make_edge( q, q2 );); (56, TBB, tbb::flow::remove_edge( q, q2 );); (87, TBB, tbb::flow::remove_edge( q,  q2 );); (-3, TBB, tbb::flow::queue_node<T> q(g);); (-5, TBB, tbb::flow::graph g;)", "label": "(4, TBB, tbb::flow::queue_node<T> q2(g);); (70, TBB, tbb::flow::make_edge( q2, q3 );); (67, TBB, tbb::flow::queue_node<T> q3(g);); (37, TBB, tbb::flow::make_edge( q, q2 );); (56, TBB, tbb::flow::remove_edge( q, q2 );); (87, TBB, tbb::flow::remove_edge( q,  q2 );); (-3, TBB, tbb::flow::queue_node<T> q(g);); (-5, TBB, tbb::flow::graph g;)"}
{"predict": "(25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (25, MPI, MPI_Get_processor_name(mpi_name, &mpi_namelen);); (22, MPI, MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);)", "label": "(25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (25, MPI, MPI_Get_processor_name(mpi_name, &mpi_namelen);); (22, MPI, MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);)"}
{"predict": "(114, OpenMP, omp_target_free(y1, omp_get_default_device());); (41, OpenMP, #pragma omp teams distribute parallel for default(shared) collapse(3)); (39, OpenMP, #pragma omp target is_device_ptr(x1, y1) map(tofrom: or[0:m3k*m2k*m1k]) map(tofrom: os[0:m3j*m2j*m1j])); (13, OpenMP, y1 = (double*)omp_target_alloc(m3k*m2k*m1k*sizeof(double), omp_get_default_device());)", "label": "(114, OpenMP, omp_target_free(y1, omp_get_default_device());); (41, OpenMP, #pragma omp teams distribute parallel for default(shared) collapse(3)); (39, OpenMP, #pragma omp target is_device_ptr(x1, y1) map(tofrom: or[0:m3k*m2k*m1k]) map(tofrom: os[0:m3j*m2j*m1j])); (13, OpenMP, y1 = (double*)omp_target_alloc(m3k*m2k*m1k*sizeof(double), omp_get_default_device());)"}
{"predict": "(17, CUDA, extern __shared__ _VOLATILE_ float sMem[];)", "label": "(17, CUDA, extern __shared__ _VOLATILE_ float sMem[];)"}
