{"predict": "(27, CUDA, dim3 grid = dim3(Gc, Gr);) (28, CUDA, dim3 block = dim3(Bc, Br);) (30, CUDA, flash_attention_v1_kernel<<<grid, block>>>(Q, K, V, O, dev_max, dev_denom, SEQLEN, sm_scale);)", "label": "(20, CUDA, CUDA_CHECK(cudaMalloc((void **)&dev_max, sizeof(FP) * SEQLEN * DIM));); (39, CUDA, cudaFree(dev_max);); (22, CUDA, CUDA_CHECK(cudaMemcpy(dev_denom, host_denom, sizeof(FP) * SEQLEN * DIM, cudaMemcpyHostToDevice));); (18, CUDA, CUDA_CHECK(cudaMalloc((void **)&dev_denom, sizeof(FP) * SEQLEN * DIM));); (18, CUDA, CUDA_CHECK(cudaMalloc((void **)&QK, sizeof(FP) * SEQLEN * SEQLEN));); (18, CUDA, CUDA_CHECK(cudaMemcpy(dev_max, host_max, sizeof(FP) * SEQLEN * DIM, cudaMemcpyHostToDevice));); (33, CUDA, cudaFree(QK);); (34, CUDA, cudaFree(dev_denom);)"}
{"predict": "(23, OpenMP, #pragma omp critical(out))", "label": "(32, OpenMP, #pragma omp critical(cerr)); (15, OpenMP, #pragma omp critical(out))"}
{"predict": "(20, MPI, MTest_Start_thread(RunTest, (void *) (long) i);)", "label": "(31, MPI, MPI_T_finalize();); (4, MPI, MPI_T_init_thread(required, &provided);); (6, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(19, OpenMP, #pragma omp parallel for reduction(+:totalSum))", "label": "(19, OpenMP, #pragma omp parallel for simd reduction(+:totalSum) schedule(guided) aligned(data: 64) num_threads(N_THREADS)); (10, OpenMP, #pragma omp parallel for num_threads(N_THREADS))"}
{"predict": "(10, OpenMP, #pragma omp parallel for reduction(+:printf_count))", "label": "(15, CUDA, cudaMemcpy(printfbuf_local, bufptr, CUPRINTF_MAX_LEN, cudaMemcpyDeviceToHost);)"}
{"predict": "(3, OpenMP, #pragma omp parallel for)", "label": "(26, OpenMP, #pragma omp parallel sections firstprivate(int)); (10, OpenMP, #pragma omp parallel sections firstprivate(,); (14, OpenMP, #pragma omp parallel sections firstprivate(, )); (34, OpenMP, #pragma omp parallel sections lastprivate(x) firstprivate(x)); (17, OpenMP, #pragma omp parallel sections firstprivate()); (36, OpenMP, #pragma omp parallel sections lastprivate(x, y) firstprivate(x, y)); (25, OpenMP, #pragma omp parallel sections firstprivate(0)); (-3, OpenMP, #pragma omp parallel sections firstprivate(); (37, OpenMP, #pragma omp parallel sections lastprivate(x, y, z) firstprivate(x, y, z))"}
{"predict": "(12, MPI, MPI_Scatter(a, 10, MPI_INT, b, 10, MPI_INT, 0, MPI_COMM_WORLD, &status);)", "label": "(29, MPI, MPI_Recv(NULL, 0, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &status);); (16, MPI, MPI_Isend(a, 0, MPI_INT, rank, 0, MPI_COMM_WORLD, &request);); (24, MPI, MPI_Wait(&request, &status);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (24, MPI, MPI_Isend(NULL, 0, MPI_INT, rank, 0, MPI_COMM_WORLD, &request);); (14, MPI, MPI_Get_count(&status, MPI_INT, &count);); (12, MPI, MPI_Recv(b, 1, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &status);)"}
{"predict": "(3, OpenCL, cl_mem_flags flags = CL_MEM_READ_WRITE | CL_MEM_WRITE_ONLY;); (4, OpenCL, flags |= CL_MEM_ACCESS_FLAGS_UNRESTRICTED_INTEL;)", "label": "(14, OpenCL, retVal = clReleaseMemObject(image);)"}
{"predict": "(58, OpenMP, #pragma omp parallel for collapse(2))", "label": "(60, OpenMP, #pragma omp parallel for)"}
{"predict": "(17, CUDA, cudaMemcpy(anchorsDev, _anchorsDev, 4 * params.anchorsRatioCount * params.anchorsScaleCount * sizeof(float), cudaMemcpyDeviceToDevice);)", "label": "(17, CUDA, CHECK(cudaMalloc((void**) &anchorsDev, 4 * params.anchorsRatioCount * params.anchorsScaleCount * sizeof(float)));)"}
{"predict": "(2, CUDA, __global__ void computeDynamicsTestKernel(DYNAMICS_T* model, float* state, float* control, float* state_der, int count) {)", "label": "(3, CUDA, extern __shared__ float entire_buffer[];)"}
{"predict": "(17, CUDA, float res_dev; cudaMalloc((void**)&res_dev, sizeof(float));) (20, CUDA, bf16_##expr##_function_kernel<<<1, 1, 0, 0>>>(a, res_dev);) (22, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);) (24, CUDA, cudaFree(res_dev);)", "label": "(24, CUDA, cudaFree(res_dev);); (17, CUDA, cudaMalloc(&res_dev, sizeof(float));); (19, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(20, OpenCL, clFinish(cmdQ.get());)", "label": "(16, OpenCL, auto b1 = clCreateBuffer(context, CL_MEM_READ_WRITE, 20, nullptr, nullptr);); (24, OpenCL, void *ptr2 = clEnqueueMapBuffer(&cmdQ, b2, CL_TRUE, CL_MAP_READ, 0, 8, 0, nullptr, nullptr, nullptr);); (32, OpenCL, clReleaseMemObject(b2);); (17, OpenCL, void *ptr1 = clEnqueueMapBuffer(&cmdQ, b1, CL_FALSE, CL_MAP_READ, 0, 8, 1, &gatingEvent, nullptr, nullptr);); (29, OpenCL, clReleaseMemObject(b1);); (31, OpenCL, clReleaseEvent(gatingEvent);); (25, OpenCL, clEnqueueUnmapMemObject(pCmdQ, b2, ptr2, 0, nullptr, nullptr);); (12, OpenCL, auto gatingEvent = clCreateUserEvent(context, nullptr);); (13, OpenCL, clEnqueueUnmapMemObject(&cmdQ, b1, ptr1, 0, nullptr, nullptr);); (8, OpenCL, auto b2 = clCreateBuffer(context, CL_MEM_READ_WRITE, 20, nullptr, nullptr);)"}
{"predict": "(3, CUDA, cudaMalloc((void**)&TransposeMatrix, NUM * sizeof(float));) (6, CUDA, cudaMalloc((void**)&gpuTransposeMatrix, NUM * sizeof(float));) (12, CUDA, cudaGetDeviceProperties(&devProp, 0);) (14, CUDA, cudaEventCreate(&start);) (15, CUDA, cudaEventCreate(&stop);) (30, CUDA, cudaEventRecord(start, NULL);) (33, CUDA, cudaMalloc((void**)&gpuMatrix, NUM * sizeof(float));) (36, CUDA, cudaMalloc((void**)&gpuTransposeMatrix, NUM * sizeof(float));) (38, CUDA, cudaEventRecord(start, NULL);) (42, CUDA, cudaMemcpy(gpuMatrix, Matrix, NUM * sizeof(float), cudaMemcpyHostToDevice);) (45, CUDA, cudaEventRecord(stop, NULL);) (46, CUDA, cudaEventSynchronize(stop);) (47, CUDA, cudaEventElapsedTime(&eventMs, start, stop);) (52, CUDA, cudaEventRecord(start, NULL);) (55, CUDA, matrixTranspose <<<dimGrid, dimBlock>>>(gpuTransposeMatrix, gpuMatrix, WIDTH);) (58, CUDA, cudaEventRecord(stop, NULL);) (59, CUDA, cudaEventSynchronize(stop);) (65, CUDA, cudaEventElapsedTime(&eventMs, start, stop);) (68, CUDA, cudaEventRecord(start, NULL);) (72, CUDA, cudaMemcpy(TransposeMatrix, gpuTransposeMatrix, NUM * sizeof(float), cudaMemcpyDeviceToHost);) (75, CUDA, cudaEventRecord(stop, NULL);) (76, CUDA, cudaEventSynchronize(stop);) (82, CUDA, cudaEventElapsedTime(&eventMs, start, stop);) (85, CUDA, cudaEventRecord(start, NULL);) (89, CUDA, cudaFree(gpuMatrix);) (90, CUDA, cudaFree(gpuTransposeMatrix);) (111, CUDA, free(Matrix);) (112, CUDA, free(TransposeMatrix);) (113, CUDA, free(cpuTransposeMatrix);)", "label": "(53, CUDA, cudaEventSynchronize(stop);); (86, CUDA, cudaMemcpy(TransposeMatrix, gpuTransposeMatrix, NUM * sizeof(float), cudaMemcpyDeviceToHost);); (9, CUDA, cudaGetDeviceProperties(&devProp, 0);); (44, CUDA, cudaMemcpy(gpuMatrix, Matrix, NUM * sizeof(float), cudaMemcpyHostToDevice);); (114, CUDA, cudaFree(gpuMatrix);); (32, CUDA, cudaMalloc((void**)&gpuMatrix, NUM * sizeof(float));); (49, CUDA, cudaEventElapsedTime(&eventMs, start, stop);); (36, CUDA, cudaEventRecord(start, NULL);); (112, CUDA, cudaFree(gpuTransposeMatrix);); (42, CUDA, cudaEventRecord(stop, NULL);); (10, CUDA, cudaEventCreate(&stop);); (28, CUDA, cudaMalloc((void**)&gpuTransposeMatrix, NUM * sizeof(float));); (6, CUDA, cudaEventCreate(&start);)"}
{"predict": "(27, CUDA, auto srcData = cudaMallocManaged(&srcVec, srcBufSize * sizeof(T));) (29, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpyAsync(srcData, srcVec.data(), srcBufSize * sizeof(T), cudaMemcpyHostToDevice, stream));) (31, CUDA, auto dstData = cudaMallocManaged(&dstVec, dstBufSize * sizeof(T));) (55, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(testVec.data(), dstData, dstBufSize * sizeof(T), cudaMemcpyDeviceToHost));)", "label": "(56, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (4, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (35, CUDA, ASSERT_EQ(cudaSuccess, cudaMemsetAsync(dstData->basePtr(), 0, dstBufSize * sizeof(T), stream));); (54, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamDestroy(stream));)"}
{"predict": "(14, MPI, if (mfu_rank == 0) { MFU_LOG(MFU_LOG_INFO, \"Extracting file data\"); } MPI_Bcast(&rc, 1, MPI_INT, 0, MPI_COMM_WORLD);)", "label": "(38, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (44, MPI, MPI_Allgather(&item_offset, 1, MPI_INT, rank_disps, 1, MPI_INT, MPI_COMM_WORLD);); (80, MPI, reduce_start = MPI_Wtime();)"}
{"predict": "(27, CUDA, __syncthreads();)", "label": "(58, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 1, NULL, threads, localThreads, 0, NULL, NULL );); (46, OpenCL, error = clSetKernelArg( kernel, 0, sizeof( streams[ 0 ] ), &streams[0] );); (60, OpenCL, error = clEnqueueReadBuffer( queue, streams[1], true, 0, sizeof( cl_double ) * TEST_SIZE, outData, 0, NULL, NULL );); (46, OpenCL, error = clSetKernelArg( kernel, 1, sizeof( streams[ 1 ] ), &streams[1] );)"}
{"predict": "(24, CUDA, cudaMemcpy(d_A, A, m * k * sizeof(float), cudaMemcpyHostToDevice);) (25, CUDA, cudaMemcpy(d_B, B, k * n * sizeof(float), cudaMemcpyHostToDevice);) (27, CUDA, cudaMemcpy(C, d_C, m * n * sizeof(float), cudaMemcpyDeviceToHost);)", "label": "(26, CUDA, cudaMemcpy(d_A, A, m * k * sizeof(float), cudaMemcpyHostToDevice);); (56, CUDA, cudaFree(d_A);); (31, CUDA, cudaMemcpy(C, d_C, m * n * sizeof(float), cudaMemcpyDeviceToHost);); (55, CUDA, cudaFree(d_B);); (55, CUDA, cudaFree(d_C);); (17, CUDA, cudaMalloc((void **)&d_B, k * n * sizeof(float));); (15, CUDA, cudaMalloc((void **)&d_A, m * k * sizeof(float));); (16, CUDA, cudaMalloc((void **)&d_C, m * n * sizeof(float));); (20, CUDA, cudaMemcpy(d_C, C, m * n * sizeof(float), cudaMemcpyHostToDevice);); (18, CUDA, cudaMemcpy(d_B, B, k * n * sizeof(float), cudaMemcpyHostToDevice);)"}
{"predict": "inp, dqkv, \"dqkv\", B * T * 3 * C);\n52:         // reset d_out, which is used as a buffer\n53:         cudaCheck(cudaMemcpy(d_out, out, B * T * C * sizeof(float), cudaMemcpyHostToDevice));\n54:     }\n55: \n56:     // free memory\n57:     cublasCheck(cublasDestroy(cublas_handle));\n58:     free(qkv);\n59:     free(out);\n60:     free(dout);\n61:     free(dqkv);\n62:     cudaFree(d_inp);\n63:     cudaFree(d_qkvr);\n64:     cudaFree(d_out);\n65:     cudaFree(d_preatt);\n66:     cudaFree(d_att);\n67:     cudaFree(d_dout);\n68:     cudaFree(d_dinp);\n69:     cudaFree(d_dqkvr);\n70:     cudaFree(d_dpreatt);\n71:     cudaFree(d_datt);\n72:     cudaFree(d_scratch);\n73: \n74:     return 0;\n75: }", "label": "(45, CUDA, cudaCheck(cudaMemset(d_dinp, 0, B * T * 3 * C * sizeof(float)));); (47, CUDA, cudaCheck(cudaMalloc(&d_dpreatt, B * NH * T * T * sizeof(float)));); (99, CUDA, cudaCheck(cudaFree(d_inp));); (38, CUDA, cudaCheck(cudaMemset(d_att, 0, B * NH * T * T * sizeof(float)));); (105, CUDA, cudaCheck(cudaFree(d_dpreatt));); (102, CUDA, cudaCheck(cudaFree(d_dinp));); (102, CUDA, cudaCheck(cudaFree(d_dqkvr));); (28, CUDA, cudaCheck(cudaMemset(d_qkvr, 0, B * T * 3 * C * sizeof(float)));); (31, CUDA, cudaCheck(cudaMemset(d_preatt, 0, B * NH * T * T * sizeof(float)));); (93, CUDA, cudaCheck(cudaFree(d_qkvr));); (30, CUDA, cudaCheck(cudaMalloc(&d_att, B * NH * T * T * sizeof(float)));); (99, CUDA, cudaCheck(cudaFree(d_datt));); (35, CUDA, cudaCheck(cudaMemset(d_dqkvr, 0, B * T * 3 * C * sizeof(float)));); (24, CUDA, cudaCheck(cudaMemset(d_out, 0, B * T * C * sizeof(float)));); (90, CUDA, cudaCheck(cudaFree(d_preatt));); (27, CUDA, cudaCheck(cudaMalloc(&d_dout, B * T * C * sizeof(float)));); (18, CUDA, cudaCheck(cudaMalloc(&d_qkvr, B * T * 3 * C * sizeof(float)));); (16, CUDA, cudaCheck(cudaMemcpy(d_inp, qkv, B * T * 3 * C * sizeof(float), cudaMemcpyHostToDevice));); (35, CUDA, cudaCheck(cudaMemset(d_scratch, 0, B * T * C * sizeof(float)));); (19, CUDA, cudaCheck(cudaMalloc(&d_preatt, B * NH * T * T * sizeof(float)));); (91, CUDA, cudaCheck(cudaFree(d_scratch));); (22, CUDA, cudaCheck(cudaMemcpy(d_dout, dout, B * T * C * sizeof(float), cudaMemcpyHostToDevice));); (81, CUDA, cudaCheck(cudaFree(d_out));); (23, CUDA, cudaCheck(cudaMalloc(&d_dqkvr, B * T * 3 * C * sizeof(float)));); (20, CUDA, cudaCheck(cudaMalloc(&d_dinp, B * T * 3 * C * sizeof(float)));); (26, CUDA, cudaCheck(cudaMemset(d_datt, 0, B * NH * T * T * sizeof(float)));); (26, CUDA, cudaCheck(cudaMalloc(&d_scratch, B * T * C * sizeof(float)));); (9, CUDA, cudaCheck(cudaMalloc(&d_out, B * T * C * sizeof(float)));); (22, CUDA, cudaCheck(cudaMalloc(&d_datt, B * NH * T * T * sizeof(float)));); (3, CUDA, cudaCheck(cudaMalloc(&d_inp, B * T * 3 * C * sizeof(float)));); (75, CUDA, cudaCheck(cudaFree(d_att));); (18, CUDA, cudaCheck(cudaMemset(d_dpreatt, 0, B * NH * T * T * sizeof(float)));); (74, CUDA, cudaCheck(cudaFree(d_dout));)"}
{"predict": "(12, OpenMP, #pragma omp parallel for private(i, k))\n(20, OpenMP, #pragma omp parallel for private(k))", "label": "(48, MPI, MPI_Buffer_detach(&bptr, &bl);); (17, MPI, MPI_Start(&request);); (27, MPI, MPI_Recv(b, 10, MPI_INT, i, 27 + j, MPI_COMM_WORLD, &status);); (8, MPI, MPI_Buffer_attach(buf, BUFSIZE);); (15, MPI, MPI_Wait(&request, &status);); (15, MPI, MPI_Request_free(&request);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (6, MPI, MPI_Bsend_init(a, 10, MPI_INT, 0, 27 + j, MPI_COMM_WORLD, &request);)"}
{"predict": "std::cout << \"Execution time: \" << run_time << std::endl;\n114:   max_run_time = run_time;\n115:   a.close();\n116:   delete[] buff;\n117:   return 0;\n118: }", "label": "(31, MPI, auto err = MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided);); (119, MPI, MPI_Finalize();); (37, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (40, MPI, ascent_opt[\"mpi_comm\"] = MPI_Comm_c2f(MPI_COMM_WORLD);); (111, MPI, MPI_Reduce(&run_time, &max_run_time, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);); (33, MPI, MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);)"}
{"predict": "(19, CUDA, for (unsigned int i = 0; i < AGENT_COUNT; i++) { AgentVector::Agent instance = population[i]; instance.setVariable<float>(\"x\", i + 1.0f); instance.setVariable<unsigned int>(\"id\", i); } cudaSimulation.setPopulationData(population);)", "label": "(15, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(12, OpenMP, #pragma omp parallel for collapse(2))\n(26, OpenMP, #pragma omp parallel for collapse(2))\n(30, OpenMP, #pragma omp parallel for collapse(2))\n(34, OpenMP, #pragma omp parallel for collapse(2))\n(38, OpenMP, #pragma omp parallel for collapse(2))\n(42, OpenMP, #pragma omp parallel for collapse(2))", "label": "(47, TBB, double t2d_auto = pforTranspose2d<tbb::auto_partitioner>(N, a, b, gs);); (45, TBB, pforTranspose2d<tbb::auto_partitioner>(N, a, b, gs);); (4, TBB, c.set_core_type(tbb::info::core_types().back());); (1, TBB, tbb::task_arena::constraints c;); (48, TBB, double t2d_simple = pforTranspose2d<tbb::simple_partitioner>(N, a, b, gs);); (37, TBB, double t1d_simple = pforTranspose<tbb::simple_partitioner>(N, a, b, gs);); (-1, TBB, c.set_numa_id(tbb::info::numa_nodes()[0]);); (29, TBB, pforTranspose<tbb::auto_partitioner>(N, a, b, gs);); (29, TBB, double t1d_auto = pforTranspose<tbb::auto_partitioner>(N, a, b, gs);); (32, TBB, pforTranspose<tbb::simple_partitioner>(N, a, b, gs);); (-2, TBB, c.set_max_concurrency(std::min(8, tbb::info::default_concurrency(c)));); (-2, TBB, tbb::task_arena a(c);); (39, TBB, pforTranspose2d<tbb::simple_partitioner>(N, a, b, gs);)"}
{"predict": "sO[ty][i * Bc + tx] += sSafeE[k] * sV[tx][k * Br + ty];\n108:       }\n109:       // NOTE: rescale once: old_denom * rescale + local_denom\n1010:       sO[ty][i * Bc + tx] = sO[ty][i * Bc + tx] / newDenom;\n1011:     }\n112:   }\n113:   // wait until g2s done\n114:   __syncthreads();\n115: \n116:   // write back to global memory\n117:   // O[Br, dim]\n118:   for (int i = 0; i < groupTx; i++) {\n119:     O[row * dim + i * Bc + tx] = sO[ty][i * Bc + tx];\n120:   }\n121: }\n\nAssistant:\n(1, CUDA, __global__ void flash_attention_v2_kernel(FP *Q, FP* K, FP* V, FP* O, int seqlen, FP smScale) {);", "label": "(12, CUDA, __shared__ FP sV[Bc][dim];); (19, CUDA, __shared__ FP sMax[Br];); (9, CUDA, __shared__ FP sK[Bc][dim];); (12, CUDA, __shared__ FP sQK[Br][Bc];); (13, CUDA, __shared__ FP sSafeE[Br][Bc];); (9, CUDA, __shared__ FP sO[Br][dim];); (4, CUDA, __shared__ FP sQ[Br][dim];); (12, CUDA, __shared__ FP sDenom[Br];)"}
{"predict": "(12, CUDA, device_id_ = std::stoi(device.substr(5));) (15, CUDA, device_id_ = std::stoi(device.substr(4));)", "label": "(22, CUDA, CudaCheck(cudaSetDevice(device_id_));)"}
{"predict": "(11, OpenMP, #pragma omp parallel for)", "label": "(14, OpenCL, retVal = clEnqueueUnmapMemObject(cmdQ.get(), buffer.get(), mappedPtr, 0, nullptr, nullptr);); (6, OpenCL, void *mappedPtr = clEnqueueMapBuffer(cmdQ.get(), buffer.get(), CL_FALSE, CL_MAP_WRITE, offset, size, 0, nullptr, nullptr, nullptr);)"}
{"predict": "(3, OpenMP, #pragma omp parallel for)", "label": "(69, OpenMP, #pragma omp parallel private(i)); (42, OpenMP, #pragma omp master taskloop lastprivate(a, b)); (49, OpenMP, #pragma omp master taskloop lastprivate(z, e, g)); (16, OpenMP, #pragma omp master taskloop lastprivate(argc); (2, OpenMP, #pragma omp parallel); (18, OpenMP, #pragma omp master taskloop lastprivate(argc,); (25, OpenMP, #pragma omp master taskloop lastprivate(argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (0, OpenMP, #pragma omp master taskloop lastprivate); (47, OpenMP, #pragma omp master taskloop lastprivate(h)); (18, OpenMP, #pragma omp master taskloop lastprivate(argc > 0 ? argv[1] : argv[2])); (64, OpenMP, #pragma omp master taskloop lastprivate(i)); (51, OpenMP, #pragma omp master taskloop allocate(omp_thread_mem_alloc: i) lastprivate(i)); (27, OpenMP, #pragma omp master taskloop lastprivate(S1)); (57, OpenMP, #pragma omp master taskloop lastprivate(j)); (-3, OpenMP, #pragma omp master taskloop lastprivate(); (32, OpenMP, #pragma omp master taskloop lastprivate(argv[1])); (-1, OpenMP, #pragma omp master taskloop lastprivate()); (18, OpenMP, #pragma omp master taskloop lastprivate(conditional: argc) lastprivate(conditional:); (50, OpenMP, #pragma omp parallel shared(i))"}
{"predict": "(14, CUDA, furthest_point_sampling_kernel<256><<<b, n_threads, 0, stream>>>(b, n, m, dataset, temp, idxs);) (18, CUDA, furthest_point_sampling_kernel<128><<<b, n_threads, 0, stream>>>(b, n, m, dataset, temp, idxs);) (22, CUDA, furthest_point_sampling_kernel<64><<<b, n_threads, 0, stream>>>(b, n, m, dataset, temp, idxs);) (26, CUDA, furthest_point_sampling_kernel<32><<<b, n_threads, 0, stream>>>(b, n, m, dataset, temp, idxs);) (30, CUDA, furthest_point_sampling_kernel<16><<<b, n_threads, 0, stream>>>(b, n, m, dataset, temp, idxs);) (34, CUDA, furthest_point_sampling_kernel<8><<<b, n_threads, 0, stream>>>(b, n, m, dataset, temp, idxs);) (38, CUDA, furthest_point_sampling_kernel<4><<<b, n_threads, 0, stream>>>(b, n, m, dataset, temp, idxs);) (42, CUDA, furthest_point_sampling_kernel<2><<<b, n_threads, 0, stream>>>(b, n, m, dataset, temp, idxs);) (46, CUDA, furthest_point_sampling_kernel<1><<<b, n_threads, 0, stream>>>(b, n, m, dataset, temp, idxs);) (50, CUDA, furthest_point_sampling_kernel<512><<<b, n_threads, 0, stream>>>(b, n, m, dataset, temp, idxs);)", "label": "(5, CUDA, cudaStream_t stream = at::cuda::getCurrentCUDAStream();)"}
{"predict": "(19, MPI, MPI_Barrier(MPI_COMM_WORLD, MPI_STATUS_IGNORE);)", "label": "(13, MPI, MPI_Recv(NULL, 0, MPI_BYTE, to, 100, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (11, MPI, MPI_Send(NULL, 0, MPI_BYTE, from, 100, MPI_COMM_WORLD);); (4, MPI, MPI_Irecv(rbuf, size, MPI_BYTE, from, tagno, MPI_COMM_WORLD, &request);); (18, MPI, MPI_Send(sbuf, size, MPI_BYTE, to, tagno, MPI_COMM_WORLD);); (20, MPI, MPI_Wait(&request, MPI_STATUS_IGNORE);)"}
{"predict": "(10, MPI, if (MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided) != MPI_SUCCESS) { fprintf(stderr, \"Failed to initialize MPI_Tools sub-system.\\n\"); } else { MPIT_result = MPI_SUCCESS; })\n(14, MPI, if (MPI_T_pvar_session_create(MPI_COMM_WORLD, &session) != MPI_SUCCESS) { printf(\"Failed to create a session for PVARs.\\n\"); MPI_Abort(MPI_COMM_WORLD, MPI_SUCCESS); } else { MPIT_result = MPI_SUCCESS; })", "label": "(13, MPI, MPI_Abort(MPI_COMM_WORLD, MPIT_result);); (14, MPI, MPIT_result = MPI_T_pvar_session_create(session);); (8, MPI, MPIT_result = MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);)"}
{"predict": "(12, OpenMP, #pragma omp atomic)", "label": "(9, OpenMP, #pragma omp atomic); (3, OpenMP, #pragma omp parallel); (5, OpenMP, #pragma omp for schedule(MY_SCHEDULE))"}
{"predict": "(5, CUDA, if (__CUDACC_VER_MAJOR__ < 12) { return 0; }); (12, CUDA, cudaGetDeviceProperties(&props, current_device_id);); (14, CUDA, if (props.major < 9) { return 0; });", "label": "(14, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (11, CUDA, CUDA_CHECK(cudaGetDevice(&current_device_id));); (11, CUDA, CUDA_CHECK(cudaGetDeviceProperties(&props, current_device_id));)"}
{"predict": "(5, CUDA, if (err != cudaSuccess && err != cudaErrorCudartUnloading) { return HIEDNN_STATUS_RUNTIME_ERROR; }); (9, CUDA, if (err != cudaSuccess && err != cudaErrorCudartUnloading) { return HIEDNN_STATUS_RUNTIME_ERROR; });", "label": "(4, CUDA, auto err = cudaFree(handle->deviceWs);); (8, CUDA, err = cudaEventDestroy(handle->wsMutex);)"}
{"predict": "(19, MPI, int rank; MPI_Comm_rank(MPI_COMM_SELF, &rank);)", "label": "(18, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "Func=DynamicFunc__append_input%d_to_output2_FINAL\\n\",side);\n61: }\n\nAssistant:\n(1, CUDA, auto d_pExpr = cudaMallocManaged(&pExpr, strlen(pExpr) + 1);) (1, CUDA, auto d_p = cudaMallocManaged(&p, strlen(p) + 1);) (1, CUDA, auto d_side = cudaMallocManaged(&side, sizeof(int));) (1, CUDA, cudaMemcpy(d_pExpr, pExpr, strlen(pExpr) + 1, cudaMemcpyHostToDevice);) (1, CUDA, cudaMemcpy(d_p, p, strlen(p) + 1, cudaMemcpyHostToDevice);) (1, CUDA, cudaMemcpy(d_side, &side, sizeof(int), cudaMemcpyHostToDevice);) (1, CUDA, compile_keys_base16_in1_type(d_pExpr, _p, salt_hex_len, keys_hex_len);) (1, CUDA, cudaMemcpy(pExpr, d_pExpr, strlen(pExpr) + 1, cudaMemcpyDeviceToHost);) (1, CUDA, cudaMemcpy(p, d_p, strlen(p) + 1, cudaMemcpyDeviceToHost);) (1, CUDA, cudaMemcpy(&side, d_side, sizeof(int), cudaMemcpyDeviceToHost);) (1, CUDA, cudaFree(d_pExpr);) (1, CUDA, cudaFree(d_p);) (1, CUDA, cudaFree(d_side);)", "label": "(39, OpenMP, comp_add_script_line(\"Func=DynamicFunc__\" #C \"_crypt_input%d_to_output1_FINAL\\n\",side);); (18, OpenMP, else                 comp_add_script_line(\"Func=DynamicFunc__append_salt%s\\n\", side==2?\"2\":\"\");); (9, OpenMP, comp_add_script_line(\"Func=DynamicFunc__clean_input2_kwik\\n\");); (61, OpenMP, comp_add_script_line(\"MaxInputLen=110\\n\");); (5, OpenMP, comp_add_script_line(\"Func=DynamicFunc__set_input_len_%d\\n\", keys_hex_len);)"}
{"predict": "(27, OpenCL, int32_t partitionCount = 4; debugManager.flags.ExperimentalSetWalkerPartitionType.set(partitionType); debugManager.flags.ExperimentalSetWalkerPartitionCount.set(partitionCount); debugManager.flags.EnableWalkerPartition.set(1u);)", "label": "(50, OpenCL, clReleaseEvent(event);)"}
{"predict": "Z\n128:     myCovDev.MCD(acc, out, dirs[0]);\n129:     myCovDev.MCD(pr1, out, dirs[1]);\n130:     quda::blas::xpy(acc, pr1);\n131:     applySpinTaste(acc, pr1, gDirs[dirs[0]]);\n132:     applySpinTaste(out, acc, QUDA_SPIN_TASTE_G5);\n133:     quda::blas::ax(0.5, out);\n134:     break;\n135:   }\n136: \n137:   case 11: // three-link XYZ\n138:   case 13: // three-link XYT\n139:   case 14: // three-link YZT\n140:   case 15: // three-link ZXT\n141:   {\n142:     int dirs[3];\n143: \n144:     {\n145:       if (offset == 11) {\n146:         dirs[0] = 0;\n147:         dirs[1] = 1;\n148:         dirs[2] = 2;\n149:       }\n150:       if (offset == 13) {\n151:         dirs[0] = 0;\n152:         dirs[1] = 1;\n153:         dirs[2] = 3;\n154:       }\n155:       if (offset == 14) {\n156:         dirs[0] = 1;\n157:         dirs[1] = 2;\n158:         dirs[2] = 3;\n159:       }\n160:       if (offset == 15) {\n161:         dirs[0] = 2;\n162:         dirs[1] = 0;\n163:         dirs[2] = 3;\n164:       }\n165:     }\n166: \n167:     ColorSpinorField pr1(cudaParam); // cudaColorSpinorField = 0\n168:     ColorSpinorField pr2(cudaParam", "label": "(17, CUDA, ColorSpinorParam cudaParam(cpuParam, inv_param, QUDA_CUDA_FIELD_LOCATION);)"}
{"predict": "(46, MPI, MPI_Isend(buf_send, size_send, MPI_BYTE, rank_send, 0, comm, &request[0]); MPI_Irecv(buf_recv, size_recv, MPI_BYTE, rank_recv, 0, comm, &request[1]);)", "label": "(55, MPI, MPI_Irecv(buf_recv, size_recv, MPI_BYTE, rank_recv, 0, comm, &request[num_req]);); (25, MPI, MPI_Irecv(&size_recv, 1, MPI_INT, rank_recv, 0, comm, &request[num_req]);); (60, MPI, MPI_Isend(buf_send, size_send, MPI_BYTE, rank_send, 0, comm, &request[num_req]);); (38, MPI, MPI_Isend(&size_send, 1, MPI_INT, rank_send, 0, comm, &request[num_req]);); (41, MPI, MPI_Waitall(num_req, request, status);)"}
{"predict": "(8, CUDA, *timer = clock();)", "label": "(3, CUDA, extern __shared__ float shared[];)"}
{"predict": "(5, OpenMP, #pragma omp parallel for private(priv_t, len) reduction(+:LOOPS))", "label": "(4, OpenMP, #pragma omp parallel for)"}
{"predict": "(4, OpenMP, #pragma omp parallel for)\n(10, OpenMP, #pragma omp parallel for)\n(14, OpenMP, #pragma omp parallel for)\n(18, OpenMP, #pragma omp parallel for)\n(22, OpenMP, #pragma omp parallel for)\n(26, OpenMP, #pragma omp parallel for)\n(30, OpenMP, #pragma omp parallel for)\n(34, OpenMP, #pragma omp parallel for)\n(38, OpenMP, #pragma omp parallel for)\n(42, OpenMP, #pragma omp parallel for)\n(46, OpenMP, #pragma omp parallel for)\n(50, OpenMP, #pragma omp parallel for)\n(54, OpenMP, #pragma omp parallel for)\n(58, OpenMP, #pragma omp parallel for)\n(62, OpenMP, #pragma omp parallel for)\n(66, OpenMP, #pragma omp parallel for)", "label": "(11, OpenMP, #pragma omp distribute simd safelen (); (71, OpenMP, #pragma omp distribute simd safelen (4)); (3, OpenMP, #pragma omp distribute simd safelen); (19, OpenMP, #pragma omp distribute simd safelen (argc); (51, OpenMP, #pragma omp distribute simd safelen (S)); (44, OpenMP, #pragma omp distribute simd safelen (foobool(argc)), safelen (true), safelen (-5)); (72, OpenMP, #pragma omp distribute simd safelen (N)); (27, OpenMP, #pragma omp distribute simd safelen (1))); (58, OpenMP, #pragma omp distribute simd safelen (argv[1]=2)); (-6, OpenMP, #pragma omp target); (18, OpenMP, #pragma omp distribute simd safelen (ST); (29, OpenMP, #pragma omp distribute simd safelen ((ST > 0) ? 1 + ST : 2)); (5, OpenMP, #pragma omp distribute simd safelen ()); (-9, OpenMP, #pragma omp teams)"}
{"predict": "(1, OpenMP, #pragma omp parallel for)", "label": "(33, OpenMP, #pragma omp parallel for simd private(argv[1])); (29, OpenMP, #pragma omp parallel for simd private(z, a, b)); (47, OpenMP, #pragma omp parallel for simd private(i)); (15, OpenMP, #pragma omp parallel for simd private(argc,); (23, OpenMP, #pragma omp parallel for simd private(S1)); (50, OpenMP, #pragma omp parallel shared(i)); (33, OpenMP, #pragma omp parallel for simd private(h)); (49, OpenMP, #pragma omp parallel private(i)); (-2, OpenMP, #pragma omp parallel); (12, OpenMP, #pragma omp parallel for simd private(argc > 0 ? argv[1] : argv[2])); (26, OpenMP, #pragma omp parallel for simd private(e, g)); (-2, OpenMP, #pragma omp parallel for simd private); (30, OpenMP, #pragma omp parallel for simd nowait); (44, OpenMP, #pragma omp parallel for simd private(j)); (10, OpenMP, #pragma omp parallel for simd private(argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (0, OpenMP, #pragma omp parallel for simd private(argc); (-4, OpenMP, #pragma omp parallel for simd private()"}
{"predict": "(34, OpenMP, for (int i = 0; i < 10; ++i) { struct S { void g() { throw 0; } }; } )", "label": "(3, OpenMP, #pragma omp parallel); (3, OpenMP, #pragma omp master taskloop)"}
{"predict": "(14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)\n(15, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprow);)\n(16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &npcol);)\n(27, MPI, MPI_Comm_rank(comm_row, &myrow);)\n(28, MPI, MPI_Comm_rank(comm_col, &mycol);)\n(39, MPI, MPI_Bcast(&updt, 1, MPI_INT, 0, comm_row);)\n(40, MPI, MPI_Bcast(&updt, 1, MPI_INT, 0, comm_col);)\n(46, MPI, MPI_Bcast(&updt, 1, MPI_INT, 0, comm_row);)\n(47, MPI, MPI_Bcast(&updt, 1, MPI_INT, 0, comm_col);)\n(53, MPI, MPI_Bcast(&updt, 1, MPI_INT, 0, comm_row);)\n(54, MPI, MPI_Bcast(&updt, 1, MPI_INT, 0, comm_col);)\n(57, MPI, MPI_Bcast(work1, mm[myrow]*updt, MPI_DOUBLE, curcol, comm_row);)\n(58, MPI, MPI_Bcast(work2, nn[mycol]*updt, MPI_DOUBLE, currow, comm_col);)\n(60, MPI, MPI_Bcast(c, mm[myrow]*nn[mycol], MPI_DOUBLE, 0, comm_row);)\n(60, MPI, MPI_Bcast(c, mm[myrow]*nn[mycol], MPI_DOUBLE, 0, comm_col);)\n(62, MPI, MPI_Allreduce(c, c, mm[myrow]*nn[mycol], MPI_DOUBLE, MPI_SUM, comm_row);)\n(62, MPI, MPI_Allreduce(c, c, mm[myrow]*nn[mycol], MPI_DOUBLE, MPI_SUM, comm_col);)", "label": "(25, MPI, MPI_Comm_rank(comm_row, &mycol);); (25, MPI, MPI_Comm_rank(comm_col, &myrow);); (25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(29, OpenMP, #pragma omp parallel for reduction(+:ucol))", "label": "(56, OpenMP, #pragma omp simd)"}
{"predict": "(34, MPI, MPI_Barrier(comm);)", "label": "(52, MPI, MPI_Comm_size(comm, &size);); (62, MPI, MPI_Barrier(comm);); (52, MPI, MPI_Win_create(target.buf, target.maxbufsize, base_type_size, MPI_INFO_NULL, comm, &win);); (69, MPI, MPI_Win_free(&win);); (25, MPI, MPI_Type_get_extent(dtp.DTP_base_type, &lb, &extent);); (46, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(12, CUDA, __syncthreads();)", "label": "(1, CUDA, __shared__ float cache[threadsPerBlock];)"}
{"predict": "nrhs, B, ldb, X, ldx);\n111: \n112: \t//================================================================\n113: \t// Call the zhesv function.\n114: \t//================================================================\n115: \tretval = LAPACKE_zhesv(CblasColMajor, uplo, n, nrhs, A, lda, ipiv, B, ldb);\n116: \tassert(retval == 0);\n117: \n118: \t//================================================================\n119: \t// Check the results.\n120: \t//================================================================\n121: \tif (test) {\n122: \t\tretval = LAPACKE_zgetrf(CblasColMajor, n, n, Aref, lda, ipiv2);\n123: \t\tassert(retval == 0);\n124: \t\tplasma_zgetrs(PlasmaNoTrans, n, nrhs, Aref, lda, ipiv2, X, ldx);\n125: \t\tretval = LAPACKE_zlacpy_work(LAPACK_COL_MAJOR, 'F', n, nrhs, Aref,\n126: \t\t\t\t\t    lda, A, lda);\n127: \t\tretval = LAPACKE_zlacpy_work(LAPACK_COL_MAJOR, 'F', n, nrhs, X, ldx,\n128: \t\t\t\t\t    B, ldb);\n129: \t\tretval = LAPACKE_zlange_work(LAPACK_COL_MAJOR, 'F', 'I', n, nrhs, B,\n130: \t\t\t\t\t    ldb, NULL);\n131: \t\tretval = LAPACKE_zla_nrm2_work(LAPACK_COL_MAJOR, 'F', n, X, ldx);\n132: \t\tretval = LAPACKE_zlange_work(LAPACK_COL_MAJOR, 'F', 'I', n, nrhs, A,\n133: \t\t\t\t\t    lda, NULL);\n134: \t}\n135: \n136: \t//================================================================\n137: \t// Free arrays.\n138: \t//================================================================\n139: \tfree(A);\n140: \tfree(T);\n141: \tfree(ipiv);\n142: \tfree(ipiv2);\n143: \tif (test) {\n144: \t\tfree(Aref);\n14", "label": "(117, OpenMP, plasma_time_t stop = omp_get_wtime();); (113, OpenMP, plasma_time_t start = omp_get_wtime();)"}
{"predict": "(2, CUDA, __global__ void softmax_backward_kernel(float* dpreatt, const float* datt, const float* att, int B, int T, int C, float scale) {); (44, CUDA, )", "label": "(7, CUDA, __shared__ float block_acc[32];)"}
{"predict": "(2, OpenMP, #pragma omp parallel for reduction(+:x))", "label": "(3, OpenMP, #pragma omp for collapse(2) schedule(runtime))"}
{"predict": "(26, OpenMP, #pragma omp parallel for simd reduction(+:v))", "label": "(32, OpenMP, #pragma omp target parallel for simd linear(a, b : B::ib)); (23, OpenMP, #pragma omp target parallel for simd linear(argc : 5) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (4, OpenMP, #pragma omp target parallel for simd linear); (12, OpenMP, #pragma omp target parallel for simd linear(argc); (40, OpenMP, #pragma omp target parallel for simd linear(i)); (33, OpenMP, #pragma omp target parallel for simd linear(e, g, z)); (45, OpenMP, #pragma omp target parallel for simd linear(v : i)); (34, OpenMP, #pragma omp target parallel for simd linear(h)); (39, OpenMP, #pragma omp parallel); (12, OpenMP, #pragma omp target parallel for simd linear(argc > 0 ? argv[1] : argv[2])); (-1, OpenMP, #pragma omp target parallel for simd linear(); (7, OpenMP, #pragma omp target parallel for simd linear(argc,); (49, OpenMP, #pragma omp target parallel for simd linear(v : j)); (22, OpenMP, #pragma omp target parallel for simd linear(argv[1])); (13, OpenMP, #pragma omp target parallel for simd linear(S1)); (42, OpenMP, #pragma omp target parallel for simd linear(j)); (-4, OpenMP, #pragma omp target parallel for simd linear())"}
{"predict": "(27, OpenMP, #pragma omp parallel for collapse(2))", "label": "(25, OpenMP, #pragma omp parallel for)"}
{"predict": "(22, OpenMP, #pragma omp parallel for;)\n(35, OpenMP, #pragma omp parallel for;)\n(48, OpenMP, #pragma omp parallel for collapse(4);)", "label": "(41, OpenMP, #pragma omp target simd collapse(4, )); (12, OpenMP, #pragma omp target simd collapse()); (48, OpenMP, #pragma omp target simd collapse(4 4)); (63, OpenMP, #pragma omp target simd collapse(4, 8)); (82, OpenMP, #pragma omp target simd collapse(5 - 5)); (4, OpenMP, #pragma omp target simd collapse(); (15, OpenMP, #pragma omp target simd collapse(, )); (10, OpenMP, #pragma omp target simd collapse(,); (74, OpenMP, #pragma omp target simd collapse(0)); (36, OpenMP, #pragma omp target simd collapse(4)); (45, OpenMP, #pragma omp target simd collapse(4, , 4)); (63, OpenMP, #pragma omp target simd collapse(foo())); (66, OpenMP, #pragma omp target simd collapse(-5)); (23, OpenMP, #pragma omp target simd collapse(4,); (17, OpenMP, #pragma omp target simd collapse(4); (11, OpenMP, #pragma omp target simd collapse 4)); (-11, OpenMP, #pragma omp target simd collapse); (53, OpenMP, #pragma omp target simd collapse(2.5))"}
{"predict": "(86, OpenMP, #pragma omp parallel for)", "label": "(97, OpenMP, nthreads = omp_get_num_threads();); (82, OpenMP, #pragma omp parallel firstprivate(niter) private(step)); (62, OpenMP, #pragma omp parallel); (85, OpenMP, #pragma omp master)"}
{"predict": "(32, OpenMP, #pragma omp parallel for)", "label": "(35, OpenMP, iis3dhhc_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)"}
{"predict": "argc],x[:]) map(always,release: e)\n58: \n59: // CHECK:      #pragma omp target exit data nowait depend(in : argc,argv[i:argc],x[:]) depend(inout : c) map(from: a)\n60: \n61: // CHECK: #pragma omp target exit data nowait depend(in : argc,argv[i:argc],x[:]) depend(inout : c) map(from: a) if(target exit data: b)\n61: // CHECK: #pragma omp target exit data nowait depend(in : argc,argv[i:argc],x[:]) depend(inout : c) map(from: a) if(b > g) nowait\n62: \n63: // CHECK-NEXT: #pragma omp target exit data map(from: c) depend(in : argc,argv[i:argc],x[:]) nowait\n64: \n65: // CHECK-NEXT: #pragma omp target exit data depend(in : argc,argv[i:argc],x[:]) map(release: c) nowait if(b > g)\n65: // CHECK-NEXT: #pragma omp target exit data nowait map(from: x[0:10],c) depend(in : argc,argv[i:argc],x[:])\n66: \n67: // CHECK-NEXT: #pragma omp target exit data nowait map(from: c) depend(in : argc,argv[i:argc],x[:]) map(release: d)\n67: // CHECK-NEXT: #pragma omp target exit data nowait depend(in : argc,argv[i:argc],x[:]) map(always,release: e)\n68: \n69: // CHECK:      #pragma omp target exit data nowait depend(in : argc,argv[i:argc],x[:]) depend(inout : c) map(from: a) depend(inout : d)\n70: \n71: // CHECK: #pragma omp target exit data nowait depend(in : argc,argv[i:argc],x[:]) depend(inout : c) map(from: a) depend(inout : d) if(target exit data: b)\n71: // CHECK: #pragma omp target exit data nowait depend(in : argc,argv[i:argc],x[:]) depend(inout : c) map(from: a) depend(inout : d) if(b > g) nowait\n72", "label": "(72, OpenMP, #pragma omp target exit data depend(in : argc,argv[i:argc],x[:]) map(release: c) nowait if(b>g)); (17, OpenMP, #pragma omp target exit data map(release: c) if(b>g)); (52, OpenMP, #pragma omp target exit data nowait map(from: c) map(release: d)); (57, OpenMP, #pragma omp target exit data nowait depend(in : argc,argv[i:argc],x[:]) map(from: a)); (1, OpenMP, #pragma omp target exit data map(from: a)); (16, OpenMP, #pragma omp target exit data map(from: x[0:10], c)); (24, OpenMP, #pragma omp target exit data map(from: c) map(release: d)); (41, OpenMP, #pragma omp target exit data map(release: c) nowait if(b>g)); (58, OpenMP, #pragma omp target exit data map(from: a) if (b > g) nowait depend(in : argc,argv[i:argc],x[:])); (72, OpenMP, #pragma omp target exit data nowait depend(in : argc,argv[i:argc],x[:]) map(always,release: e)); (5, OpenMP, #pragma omp target exit data map(from: c)); (58, OpenMP, #pragma omp target exit data map(from: c) depend(in : argc,argv[i:argc],x[:]) nowait); (30, OpenMP, #pragma omp target exit data map(from: a) if (b > g) nowait); (38, OpenMP, #pragma omp target exit data nowait map(from: x[0:10], c)); (19, OpenMP, #pragma omp target exit data map(always,release: e)); (30, OpenMP, #pragma omp target exit data map(from: c) nowait); (62, OpenMP, #pragma omp target exit data nowait map(from: c) depend(in : argc,argv[i:argc],x[:]) map(release: d)); (58, OpenMP, #pragma omp target exit data nowait map(from: x[0:10], c) depend(in : argc,argv[i:argc],x[:])); (-6, OpenMP, #pragma omp target exit data map(from: a) if (b > g)); (5, OpenMP, #pragma omp target exit data map(delete: x[0:10])); (43, OpenMP, #pragma omp target exit data nowait map(from: a) depend(in : argc,argv[i:argc],x[:]) if (target exit data: b)); (36, OpenMP, #pragma omp target exit data nowait map(always,release: e)); (17, OpenMP, #pragma omp target exit data nowait map(from: a) if (target exit data: b)); (-14, OpenMP, #pragma omp target exit data map(from: a) if (target exit data: b)); (12, OpenMP, #pragma omp target exit data nowait map(from: a)); (2, OpenMP, #pragma omp target exit data map(always, delete: x[0:10]))"}
{"predict": "(14, OpenCL, cl_command_queue_properties properties = CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE;); (16, OpenCL, cl_command_queue ocl_queue = clCreateCommandQueue(ocl_ctx, ocl_dev, properties, &err););", "label": "(62, OpenCL, TEST_OCL_CHECK(clReleaseEvent(read_buffer_event));); (64, OpenCL, TEST_OCL_CHECK(clReleaseCommandQueue(ocl_queue));); (52, OpenCL, TEST_OCL_CHECK(clWaitForEvents(1, &read_buffer_event));); (61, OpenCL, TEST_OCL_CHECK(clReleaseEvent(eltwise_event));); (59, OpenCL, TEST_OCL_CHECK(clReleaseEvent(write_buffer_event));)"}
{"predict": "(12, OpenMP, #pragma omp parallel for)", "label": "(10, CUDA, extent = make_cudaExtent(4, 5, 0);); (3, CUDA, cudaExtent extent = make_cudaExtent(4, 5, 6);)"}
{"predict": "(17, CUDA, cudaMalloc(&device_array, num_bytes);)", "label": "(16, CUDA, cudaMalloc((void**)&device_array, num_bytes);); (49, CUDA, printf(\"CUDA Error: %s\\n\", cudaGetErrorString(error));); (35, CUDA, cudaThreadSynchronize();); (42, CUDA, cudaError_t error = cudaGetLastError();); (66, CUDA, cudaFree(device_array);); (54, CUDA, cudaMemcpy(host_array, device_array, num_bytes, cudaMemcpyDeviceToHost);)"}
{"predict": "(27, OpenMP, #pragma omp parallel for private(index, threadIndices, threadWeights, threadWeightsDu, threadWeightsDv, threadResult, threadResultDu, threadResultDv))", "label": "(19, OpenMP, int numThreads = omp_get_max_threads();); (25, OpenMP, #pragma omp parallel for); (35, OpenMP, int threadId = omp_get_thread_num();)"}
{"predict": "(29, MPI, MPI_Type_free(&params_type);)", "label": "(56, MPI, MPI_Finalize();); (22, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (29, MPI, MPI_Bcast(&params, 1, *params_type, root, MPI_COMM_WORLD);); (14, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (10, MPI, MPI_Init_thread(NULL, NULL, MPI_THREAD_FUNNELED, &thread_level);)"}
{"predict": "(3, MPI, MPI_Bcast(iparams, num_int_params, MPI_INT, 0, MPI_COMM_WORLD);)", "label": "(23, MPI, MPI_Bcast(&fparams[0], 1, MPI_FLOAT, 0, MPI_COMM_WORLD);); (6, MPI, MPI_Bcast(&iparams[0], num_int_params, MPI_INT, 0, MPI_COMM_WORLD);)"}
{"predict": "(14, OpenMP, #pragma omp parallel for)\n(27, OpenMP, #pragma omp parallel for)\n(30, OpenMP, #pragma omp parallel for)", "label": "(35, MPI, MPI_Finalize();); (2, MPI, MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &mpi_threads_provided);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);)"}
{"predict": "uint64_t data_padding = (data_size % 512 == 0) ? 0 : (512 - (data_size % 512));\n120: \n121:             /* advance pointer to start of data segment */\n122:             ptr += (header_offset - buf_offset);\n123: \n124:             /* compute file offset to start of data segment */\n125:             uint64_t file_offset = pos + buf_offset + header_offset;\n126: \n127:             /* record this data offset */\n128:             *out_offsets = (uint64_t*) realloc(*out_offsets, (out_count[0] + 1) * sizeof(uint64_t));\n129:             (*out_offsets)[out_count[0]] = file_offset;\n130:             out_count[0]++;\n131:         }\n132: \n133:         /* free archive object */\n134:         archive_read_free(a);\n135:     }\n136: \n137:     /* if we found any entries, record the max offset */\n138:     if (found_one) {\n139:         *out_starting_pos = pos;\n140:     }\n141:     else {\n142:         *out_starting_pos = pos + overlap_before;\n143:     }\n144: \n145:     *out_count = out_count[0];\n146: \n147:     return rc;\n148: }", "label": "(141, MPI, MPI_Exscan(&max_offset, &starting_pos, 1, MPI_UINT64_T, MPI_MAX, MPI_COMM_WORLD);); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (258, MPI, MPI_Allreduce(&pos, &max_pos, 1, MPI_UINT64_T, MPI_MAX, MPI_COMM_WORLD);); (13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(27, CUDA, cudaStream_t s; cudaStreamCreate(&s);) (62, CUDA, cudaStreamDestroy(s);)", "label": "(72, CUDA, cudaStreamDestroy(s);); (5, CUDA, cudaStreamCreate(&s);); (31, CUDA, cudaEventRecord(end, s);); (31, CUDA, cudaEventSynchronize(end);); (23, CUDA, cudaEventRecord(begin, s);); (66, CUDA, cudaEventDestroy(end);); (3, CUDA, cudaEventCreate(&end);); (63, CUDA, cudaEventDestroy(begin);); (0, CUDA, cudaEventCreate(&begin);); (27, CUDA, cudaEventElapsedTime(&time, begin, end);)"}
{"predict": "(7, MPI, MPI_Comm comm = MPI_COMM_WORLD;)\n(11, MPI, MPI_Comm comm = MPI_COMM_WORLD;)\n(17, MPI, MPI_Comm comm = MPI_COMM_WORLD;)\n(21, MPI, MPI_Comm comm = MPI_COMM_WORLD;)\n(33, MPI, if(::dray::dray::mpi_rank() == 0))", "label": "(11, MPI, ::dray::dray::mpi_comm(MPI_Comm_c2f(comm));)"}
{"predict": "(3, OpenMP, #pragma omp parallel for)", "label": "(12, OpenMP, #pragma omp target parallel defaultmap(tofrom,); (1, OpenMP, #pragma omp target parallel defaultmap(); (14, OpenMP, #pragma omp target parallel defaultmap(tofrom, scalar); (24, OpenMP, #pragma omp target parallel defaultmap(none:pointer) defaultmap(none:scalar) map(g)); (35, OpenMP, #pragma omp target defaultmap(none:aggregate)); (26, OpenMP, #pragma omp target parallel defaultmap(none:aggregate) defaultmap(none:scalar) map(argc)); (8, OpenMP, #pragma omp target parallel defaultmap (scalar:); (47, OpenMP, #pragma omp target defaultmap(none:pointer)); (32, OpenMP, #pragma omp parallel); (14, OpenMP, #pragma omp target parallel defaultmap(none:pointer)); (0, OpenMP, #pragma omp target parallel defaultmap(tofrom)); (-7, OpenMP, #pragma omp target parallel defaultmap()); (7, OpenMP, #pragma omp target parallel defaultmap(tofrom:scalar) defaultmap(to:scalar)); (8, OpenMP, #pragma omp target parallel defaultmap(alloc:pointer) defaultmap(to:scalar) defaultmap(firstprivate:pointer)); (15, OpenMP, #pragma omp target parallel defaultmap(none:pointer) defaultmap(none:scalar) defaultmap(none:aggregate) map(g) firstprivate(argc)); (-7, OpenMP, #pragma omp target parallel defaultmap (tofrom:); (9, OpenMP, #pragma omp target parallel defaultmap(none:pointer) defaultmap(none:scalar) map(argc)); (-11, OpenMP, #pragma omp target parallel defaultmap(tofrom)"}
{"predict": "close the chunk file */\n125:   redset_close(fd_chunk);\n126: \n127:   /* free resources */\n128:   redset_lofi_close(&rsf);\n129:   kvtree_delete(&current_hash);\n130:   kvtree_delete(&desc_hash);\n131:   kvtree_delete(&partner_hash);\n132: \n133:   return rc;\n134: }", "label": "(57, MPI, MPI_Allreduce(&my_bytes, &max_bytes, 1, MPI_UNSIGNED_LONG, MPI_MAX, comm);)"}
{"predict": "(10, CUDA, T local_sum = __syncthreads_block_reduce_add(local_sum);)", "label": "(15, CUDA, __shared__ typename BlockReduceT::TempStorage temp_storage;)"}
{"predict": "119:       }\n120:     }; // if\n121:   }; // if\n122:   return err;\n123: }", "label": "(7, OpenMP, int tid = omp_get_thread_num();)"}
{"predict": "(8, OpenMP, #pragma omp parallel for private(lv, rv))", "label": "(14, OpenMP, return comp_raw(lv, rv, by_number, ascendingly, casesensitively);)"}
{"predict": "context);\n90:   assert(cudaMemcpyAsync(out.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, context.device().stream()) == cudaSuccess);\n91:   assert(cudaStreamSynchronize(context.device().stream()) == cudaSuccess);\n92:   for (int i = 0; i < 40; ++i) {\n93:     for (int j = 0; j < 49; ++j) {\n94:       for (int k = 0; k < 69; ++k) {\n95:         const float result = out(i,j,k);\n96:         const float expected = (in1(i,j,k) * 3.14f + in1(i,j+1,k) * 2.7f +\n97:                                 in1(i,j,k+1) * 0.2f + in1(i,j+1,k+1) * 7.0f +\n98:                                 in1(i,j,k+2) * 0.5f + in1(i,j+1,k+2) * 0.1f);\n99:         VERIFY_IS_APPROX(expected, result);\n100:       }\n101:     }\n102:   }\n103: \n104:   test_reduce(&context);\n105:   assert(cudaMemcpyAsync(out.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, context.device().stream()) == cudaSuccess);\n106:   assert(cudaStreamSynchronize(context.device().stream()) == cudaSuccess);\n107:   for (int i = 0; i < 40; ++i) {\n108:     for (int j = 0; j < 40; ++j) {\n109:       const float result = out(i,j,0);\n110:       float expected = 0;\n111:       for (int k = 0; k < 50; ++k) {\n112:         for (int l = 0; l < 70; ++l) {\n113:           expected += in1(i, k, l) + in2(j, k, l);\n114:         }\n115:       }\n116:       VERIFY_IS_APPROX(expected, result);\n117:     }\n118", "label": "(27, CUDA, assert(cudaMemcpy(out.data(), d_out, out_bytes, cudaMemcpyDeviceToHost) == cudaSuccess);); (15, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (12, CUDA, cudaMalloc((void**)(&d_in1), in1_bytes);); (69, CUDA, assert(cudaMemcpyAsync(out.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, context.device().stream()) == cudaSuccess);); (14, CUDA, cudaMemcpy(d_in1, in1.data(), in1_bytes, cudaMemcpyHostToDevice);); (10, CUDA, cudaMalloc((void**)(&d_in2), in2_bytes);); (67, CUDA, assert(cudaStreamSynchronize(context.device().stream()) == cudaSuccess);); (12, CUDA, cudaMemcpy(d_in2, in2.data(), in2_bytes, cudaMemcpyHostToDevice);)"}
{"predict": "(18, MPI, if (rank == 0) { err = MTest_thread_barrier_init(); if (err) { fprintf(stderr, \"Could not create thread barrier\\n\"); MPI_Abort(MPI_COMM_WORLD, 1); } nthreads = nprocs - 1; for (i = 1; i < nprocs; i++) MTest_Start_thread(run_test_send, (void *) (long) i); } else if (rank < MAX_NTHREAD) { run_test_recv(); })", "label": "(11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (19, MPI, MPI_Barrier(MPI_COMM_WORLD);); (13, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (4, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(17, MPI, int send_size = make_message_distance(world_size, message_padding); send_buffer.resize(send_size); send_sizes.resize(world_size); send_offsets.resize(world_size); int counter = 0; for (int rank = 0; rank < world_size; rank++) { send_sizes[rank] = make_message_size(rank); send_offsets[rank] = make_message_distance(rank, message_padding); for (int i = 0; i < send_sizes[rank] + message_padding; i++, counter++) { send_buffer[counter] = rank; } } )", "label": "(6, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(39, CUDA, *(res + k * inc) = static_cast<Q>(tmp[0]) + beta * (*(res + k * inc));)", "label": "(15, CUDA, __shared__ thrust::cuda_cub::core::uninitialized_array<T, 1024> tmp;)"}
{"predict": "(24, CUDA, cuda(EventCreate( &evStart ) );) (25, CUDA, cuda(EventCreate( &evStop ) );) (26, CUDA, cuda(Malloc( &inGPU, N*sizeof(T) ) );) (27, CUDA, cuda(Malloc( &outGPU, N*sizeof(T) ) );) (29, CUDA, cuda(Memset( inGPU, 0, N*sizeof(T) ) );) (30, CUDA, cuda(Memset( outGPU, 0, N*sizeof(T) ) );) (32, CUDA, cuda(Memset( outGPU, 0, N*sizeof(T) ) );) (40, CUDA, cuda(Memcpy( inGPU, inCPU, N*sizeof(T), cudaMemcpyHostToDevice ) );) (42, CUDA, cuda(EventRecord( evStart, 0 ) );) (43, CUDA, cuda(EventRecord( evStop, 0 ) );) (45, CUDA, cuda(Memcpy( hostGPU, outGPU, N*sizeof(T), cudaMemcpyDeviceToHost ) );) (58, CUDA, cuda(EventElapsedTime( &ms, evStart, evStop ) );)", "label": "(65, CUDA, cudaFree( outGPU );); (63, CUDA, cudaEventDestroy( evStop );); (64, CUDA, cudaFree( inGPU );); (60, CUDA, cudaEventDestroy( evStart );)"}
{"predict": "(2, CUDA, __global__)", "label": "(3, CUDA, __shared__ float maxScore[FMC2H];); (1, CUDA, __shared__ float siftPoint[128];); (2, CUDA, __shared__ float maxScor2[FMC2H];); (2, CUDA, __shared__ int maxIndex[FMC2H];)"}
{"predict": "//     printf(\"ya4 %5d k %5d lk %5d\\n\",thread_id,k,lk);\n108: \t\t\t//     fflush(stdout);\n109: \t\t\t// }\n110: \t\t\tnbrow_ref = 4*maxsuper;\n111: \t\t}else{\n112: \t\t\tnbrow_ref = m;\n113: \t\t}\n114: \n115: \t\tnbrow = nbrow_ref;\n116: \n117: \t\tluptr_tmp1 = luptr_tmp;\n118: \n119: \t\t// printf(\"ya5 %5d k %5d lk %5d\\n\",thread_id,k,lk);\n120: \t\t// fflush(stdout);\n121: \n122: \t\tlptr1_tmp = luptr_tmp;\n123: \n124: \t\tnlb1 = nlb;\n11:", "label": "(174, OpenMP, #pragma omp simd); (112, OpenMP, #pragma\tomp\ttaskloop private (lptr1,luptr1,nlb1,thread_id1,lsub1,lusup1,nsupr1,Linv,nn,lbstart,lbend,luptr_tmp1,nbrow,lb,lptr1_tmp,rtemp_loc,nbrow_ref,lptr,nbrow1,ik,rel,lk,iknsupc,il,i,irow,fmod_tmp,ikcol,p,ii,jj,t1,t2,j) untied); (229, OpenMP, #pragma omp simd lastprivate(irow))"}
{"predict": "(12, MPI, MPI_Send(port_name, MPI_MAX_PORT_NAME, MPI_CHAR, dest, tag, MPI_COMM_WORLD);)", "label": "(12, MPI, rc = MPI_Unpublish_name(serv_name, MPI_INFO_NULL, port_name);); (12, MPI, MPI_Error_class(rc, &errclass);); (8, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(54, OpenMP, #pragma omp parallel for)", "label": "(11, OpenMP, #pragma omp target enter data map(alloc : a [0:array_size], b [0:array_size])); (18, OpenMP, #pragma omp target teams distribute parallel for); (3, OpenMP, #pragma omp target map(tofrom : warp_size)); (98, OpenMP, #pragma omp target exit data map(release : a [0:array_size], b [0:array_size]))"}
{"predict": "(27, CUDA, cudaEvent_t start, stop; cudaEventCreate(&start); cudaEventCreate(&stop); cudaEventRecord(start, 0);) (28, CUDA, gpumult0<<<blocks,threads>>>(dev_C.data().get(),dev_A.data().get(),dev_B.data().get(),Arow,Acol,Bcol);) (30, CUDA, cudaEventRecord(stop, 0); cudaEventSynchronize(stop); float milliseconds = 0; cudaEventElapsedTime(&milliseconds, start, stop); double t2 = milliseconds/(double)(nacc);)", "label": "(18, CUDA, thrust::device_vector<float> dev_B(Brow*Bcol);); (12, CUDA, thrust::host_vector<float>       A(Arow*Acol);); (14, CUDA, thrust::device_vector<float> dev_C(Crow*Ccol);); (32, CUDA, cudaDeviceSynchronize();); (10, CUDA, thrust::host_vector<float>       B(Brow*Bcol);); (10, CUDA, thrust::host_vector<float>       C(Crow*Ccol);); (11, CUDA, thrust::device_vector<float> dev_A(Arow*Acol);)"}
{"predict": "(24, MPI, auto const tree = makeDistributedTree<DeviceType, ArborX::Box<3>>(comm, ExecutionSpace{}, {{{{(float)comm_rank, 0., 0.}}, {{(float)comm_rank + 1, 1., 1.}}}});)", "label": "(8, MPI, MPI_Comm_rank(comm, &comm_rank);); (9, MPI, MPI_Comm_size(comm, &comm_size);)"}
{"predict": "\");\n82:     ERROR(\"MPI_Type_free\");\n83: \n84:     /* Create a contiguous datatype for the local buffer */\n84:     err = MPI_Type_contiguous(NVARS * LEN * LEN, MPI_BYTE, &buftype);\n85:     ERROR(\"MPI_Type_contiguous\");\n86:     err = MPI_Type_commit(&buftype);\n87:     ERROR(\"MPI_Type_commit\");\n88:     ERROR(\"MPI_Type_free\");\n89: \n90:     /* Create a fileview for collective I/O */\n91:     MPI_Type_create_resized(filetype, 0, fsize, &buftype);\n92:     ERROR(\"MPI_Type_create_resized\");\n93:     MPI_Type_commit(&buftype);\n94:     ERROR(\"MPI_Type_commit\");\n95:     ERROR(\"MPI_Type_free\");\n96: \n97:     /* Create file handle and open file */\n98:     MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_CREATE | MPI_MODE_WRONLY, MPI_INFO_NULL, &fh);\n99:     ERROR(\"MPI_File_open\");\n100: \n101:     /* Create the buffer */\n102:     buf = (char *) malloc(buf_len);\n103:     if (!buf) {\n104:         if (!rank)\n105:             printf(\"Allocation error\\n\");\n106:         MPI_Abort(MPI_COMM_WORLD, 1);\n107:     }\n108:     /* Initialize buffer */\n109:     for (i = 0; i < buf_len; i++)\n110:         buf[i] = 0;\n111: \n112:     /* Write data to file */\n113:     MPI_File_set_view(fh, 0, MPI_BYTE, buftype, \"native\", MPI_INFO_NULL);\n114:     MPI_File_write_all(fh, buf, buf_len, MPI_BYTE, &status);\n115:     ERROR(\"MPI_File_write_all\");\n116:     MPI_File_close(&fh);\n117:     ERROR(\"MPI_File_close\");\n118:     free(buf);\n119:     return 0;\n120: }", "label": "(14, MPI, MPI_Init(&argc, &argv);); (130, MPI, err = MPI_File_iwrite_all(fh, buf, 1, buftype, &req);); (100, MPI, err = MPI_Type_commit(&buftype);); (56, MPI, err = MPI_Type_create_subarray(2, gsize, count, start, MPI_ORDER_C, MPI_BYTE, &subType);); (57, MPI, err = MPI_Type_commit(&subType);); (68, MPI, err = MPI_Type_commit(&filetype);); (69, MPI, err = MPI_Type_free(&subType);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (115, MPI, err = MPI_File_write_all(fh, buf, 1, buftype, &status);); (125, MPI, err = MPI_File_read_all(fh, buf, 1, buftype, &status);); (129, MPI, err = MPI_File_iread_all(fh, buf, 1, buftype, &req);); (121, MPI, MPI_Wait(&req, &status);); (8, MPI, MPI_Finalize();); (106, MPI, err = MPI_File_set_view(fh, 0, MPI_BYTE, filetype, \"native\", MPI_INFO_NULL);); (134, MPI, err = MPI_Type_free(&buftype);); (131, MPI, err = MPI_Type_free(&filetype);); (127, MPI, MPI_File_close(&fh);); (10, MPI, err = MPI_Dims_create(nprocs, 2, psize);); (109, MPI, err = MPI_File_seek(fh, 0, MPI_SEEK_SET);)"}
{"predict": "(12, OpenMP, #pragma omp parallel sections;)\n(13, OpenMP, #pragma omp section;)\n(15, OpenMP, #pragma omp section;)", "label": "(5, OpenMP, #pragma omp sections); (3, OpenMP, #pragma omp parallel); (5, OpenMP, #pragma omp section)"}
{"predict": "", "label": "(42, OpenMP, #pragma omp target); (42, OpenMP, #pragma omp target firstprivate(val))"}
{"predict": "(16, MPI, MPI_Bcast(&count, 1, MPI_INT, 0, MPI_COMM_WORLD);)", "label": "(23, MPI, MPI_Error_class(err, &errclass);); (36, MPI, MPI_Free_mem(ap);); (16, MPI, MPI_Info_set(hints, \"mpich_buf_type\", hintstrs[i]);); (10, MPI, MPI_Info_set(hints, \"mpi_minimum_memory_alignment\", \"4096\");); (8, MPI, MPI_Info_create(&hints);); (20, MPI, err = MPI_Alloc_mem(count, MPI_INFO_NULL, &ap);); (3, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (35, MPI, MPI_Info_free(&hints);); (12, MPI, err = MPI_Alloc_mem(count, hints, &ap);)"}
{"predict": "(14, OpenMP, #pragma omp parallel for reduction(+:a))", "label": "(24, CUDA, cuda_safe_call(cudaStreamSynchronize(stream));); (10, CUDA, cuda_safe_call(cudaGetDeviceCount(&ndevs));)"}
{"predict": "(16, CUDA, __syncthreads();)", "label": "(2, CUDA, __shared__ float s[Ny][Nx];)"}
{"predict": ": #pragma omp teams\n69:   for (int i = 0; i < 10; ++i)\n70:     foo();\n71: #pragma omp target\n72: #pragma omp teams\n73:   for (int i = 0; i < 10; ++i)\n74:     foo();\n75: #pragma omp target\n76: #pragma omp teams\n77:   for (int i = 0; i < 10; ++i)\n78:     foo();\n79: #pragma omp target\n80: #pragma omp teams\n81:   for (int i = 0; i < 10; ++i)\n82:     foo();\n83: #pragma omp target\n84: #pragma omp teams\n85:   for (int i = 0; i < 10; ++i)\n86:     foo();\n87: #pragma omp target\n88: #pragma omp teams\n89:   for (int i = 0; i < 10; ++i)\n90:     foo();\n91: #pragma omp target\n92: #pragma omp teams\n93:   for (int i = 0; i < 10; ++i)\n94:     foo();\n95: #pragma omp target\n96: #pragma omp teams\n97:   for (int i = 0; i < 10; ++i)\n98:     foo();\n99: #pragma omp target\n100: #pragma omp teams\n101:   for (int i = 0; i < 10; ++i)\n102:     foo();\n103: #pragma omp target\n104: #pragma omp teams\n105:   for (int i = 0; i < 10; ++i)\n106:     foo();\n107: #pragma omp target\n108: #pragma omp teams\n109:   for (int i = 0; i < 10; ++i)\n110:     foo();\n111: #pragma omp target\n112: #pragma omp teams\n113:   for (int i = 0; i < 10; ++", "label": "(109, OpenMP, #pragma omp distribute parallel for simd reduction(^ : fl)); (118, OpenMP, #pragma omp distribute parallel for simd reduction(&& : S2::S2sc)); (132, OpenMP, #pragma omp distribute parallel for simd reduction(+ : z, o)); (21, OpenMP, #pragma omp distribute parallel for simd reduction(); (45, OpenMP, #pragma omp distribute parallel for simd reduction(foo : argc); (140, OpenMP, #pragma omp distribute parallel for simd reduction(+ : p), reduction(+ : p)); (6, OpenMP, #pragma omp target); (87, OpenMP, #pragma omp distribute parallel for simd reduction(+ : ba)); (131, OpenMP, #pragma omp distribute parallel for simd private(i), reduction(+ : j), reduction(+ : q)); (146, OpenMP, #pragma omp distribute parallel for simd reduction(+ : r)); (161, OpenMP, #pragma omp parallel reduction(* : fl)); (151, OpenMP, #pragma omp distribute parallel for simd reduction(max : j)); (112, OpenMP, #pragma omp distribute parallel for simd reduction(& : e, g)); (172, OpenMP, #pragma omp distribute parallel for simd reduction(task, + : m)); (166, OpenMP, #pragma omp distribute parallel for simd reduction(+ : m)); (127, OpenMP, #pragma omp parallel private(k)); (63, OpenMP, #pragma omp distribute parallel for simd reduction(+ : a, b, c, d, f)); (67, OpenMP, #pragma omp distribute parallel for simd reduction(min : a, b, c, d, f)); (71, OpenMP, #pragma omp distribute parallel for simd reduction(max : h.b)); (55, OpenMP, #pragma omp distribute parallel for simd reduction(^ : S1)); (109, OpenMP, #pragma omp distribute parallel for simd reduction(+ : h, k, B::x)); (48, OpenMP, #pragma omp distribute parallel for simd reduction(&& : argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (17, OpenMP, #pragma omp distribute parallel for simd reduction(*)); (136, OpenMP, #pragma omp parallel reduction(min : i)); (20, OpenMP, #pragma omp distribute parallel for simd reduction(\\)); (140, OpenMP, #pragma omp parallel private(fl)); (8, OpenMP, #pragma omp distribute parallel for simd reduction()); (72, OpenMP, #pragma omp distribute parallel for simd reduction(* : ca)); (140, OpenMP, #pragma omp distribute parallel for simd reduction(+ : fl)); (30, OpenMP, #pragma omp distribute parallel for simd reduction(|| : argc > 0 ? argv[1] : argv[2])); (-1, OpenMP, #pragma omp distribute parallel for simd reduction(-); (73, OpenMP, #pragma omp distribute parallel for simd reduction(- : da)); (-13, OpenMP, #pragma omp distribute parallel for simd reduction +); (-19, OpenMP, #pragma omp distribute parallel for simd reduction); (20, OpenMP, #pragma omp distribute parallel for simd reduction(| : argc,); (123, OpenMP, #pragma omp parallel shared(i)); (78, OpenMP, #pragma omp distribute parallel for simd reduction(&& : S2::S2s)); (27, OpenMP, #pragma omp distribute parallel for simd reduction(~ : argc)); (-25, OpenMP, #pragma omp teams)"}
{"predict": "(6, OpenCL, clReleaseMemObject(bufX);) (9, OpenCL, clReleaseMemObject(bufY);) (12, OpenCL, clReleaseMemObject(bufParam);)", "label": "(5, OpenCL, clReleaseMemObject(bufX);); (8, OpenCL, clReleaseMemObject(bufY);); (11, OpenCL, clReleaseMemObject(bufParam);)"}
{"predict": "(9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);) (14, MPI, MPI_Cart_create(MPI_COMM_WORLD, 1, dims, periods, 1, &cartcomm);) (16, MPI, MPI_Cart_shift(cartcomm, 0, 1, &source, &dest);) (21, MPI, MPI_Recv(&newrank, 1, MPI_INT, source, 0, cartcomm, MPI_STATUS_IGNORE);) (26, MPI, MPI_Recv(&newrank, 1, MPI_INT, source, 0, cartcomm, MPI_STATUS_IGNORE);)", "label": "(9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (15, MPI, MPI_Cart_map(MPI_COMM_WORLD, 1, dims, periods, &newrank);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (29, MPI, MPI_Cart_map(MPI_COMM_WORLD, 0, dims, periods, &newrank);)"}
{"predict": "(14, OpenCL, clFinish(pCmdQ->getCommandStreamReceiver()->getCsIface());)", "label": "(26, OpenCL, status = clReleaseEvent(event);); (13, OpenCL, auto status = clWaitForEvents(1, &event);)"}
{"predict": "(2, OpenMP, #pragma omp parallel for reduction(+:combine_sum, combine_ref_sum, combine_finit_sum, combine_each_sum, copy_construct_sum, copy_assign_sum, move_construct_sum, move_assign_sum))", "label": "(41, TBB, tbb::combinable<T> copy_constructed(sums);); (31, TBB, tbb::combinable<T> finit_combinable(my_finit_decl);); (31, TBB, tbb::parallel_for( tbb::blocked_range<int>( 0, N, 10000 ), ParallelScalarBodyNoInit<T>( finit_combinable ) );); (19, TBB, if (Verbose && t == 0) t0 = tbb::tick_count::now();); (22, TBB, tbb::parallel_for( tbb::blocked_range<int>( 0, N, 10000 ), ParallelScalarBody<T>( sums ) );); (40, TBB, tbb::combinable<T> assigned;); (-3, TBB, tbb::task_scheduler_init init(tbb::task_scheduler_init::deferred);); (44, TBB, tbb::combinable<T> moved1(std::move(sums));); (2, TBB, tbb::tick_count t0;); (16, TBB, tbb::combinable<T> sums;); (45, TBB, tbb::combinable<T> moved2;)"}
{"predict": "unsigned char *)expectedResults;\n99:     outPtr = (unsigned char *)outData;\n100:     for (i = 0; i < count; i++) {\n101:         if (memcmp(expectedPtr, outPtr, outStride) != 0) {\n102:             log_error(\"Mismatch at index %zu\\n\", i);\n103:             retCode = -1;\n104:         }\n105:         expectedPtr += outStride;\n106:         outPtr += outStride;\n107:     }\n108: \n109:     /* Clean up */\n110:     for (i = 0; i < 3; i++) {\n111:         test_error( error, \"Unable to free memory object\" );\n112:     }\n113:     free( outData );\n114:     return retCode;\n115: }", "label": "(119, OpenCL, clReleaseMemObject( streams[2] );); (80, OpenCL, error = clSetKernelArg(kernel, 0, sizeof( streams[0] ), &streams[0] );); (119, OpenCL, clReleaseProgram( program );); (82, OpenCL, error = clSetKernelArg(kernel, 2, sizeof( streams[2] ), &streams[2] );); (114, OpenCL, clReleaseMemObject( streams[1] );); (115, OpenCL, clReleaseKernel( kernel );); (77, OpenCL, error = clSetKernelArg(kernel, 1, sizeof( streams[1] ), &streams[1] );); (92, OpenCL, error = clEnqueueReadBuffer( queue, streams[2], CL_TRUE, 0, outStride * count, outData, 0, NULL, NULL );); (86, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 1, NULL, &threadSize, &groupSize, 0, NULL, NULL );); (108, OpenCL, clReleaseMemObject( streams[0] );)"}
{"predict": "(17, MPI, MPI_Wait(&req, &status);)", "label": "(25, MPI, MPI_Wait(&req, &status);); (12, MPI, MPI_Bcast_init(&a, 1, MPI_INT, 0, MPI_COMM_WORLD, MPI_INFO_NULL, &req);); (15, MPI, MPI_Test(&req, &flag, &status);); (25, MPI, MPI_Request_free(&req);)"}
{"predict": "(11, OpenMP, #pragma omp parallel for private(i, lens, pin, pout) reduction(+:any_cracked)) (33, OpenMP, #pragma omp atomic)", "label": "(10, OpenMP, #pragma omp parallel for); (32, OpenMP, #pragma omp atomic)"}
{"predict": "(10, OpenCL, cl_event retEvent;); (14, OpenCL, cl_event retEvent2;); (17, OpenCL, auto neoEvent = castToObject<Event>(retEvent);); (20, OpenCL, auto neoEvent2 = castToObject<Event>(retEvent2);)", "label": "(25, OpenCL, clReleaseEvent(retEvent2);); (25, OpenCL, clReleaseEvent(retEvent);)"}
{"predict": "(7, MPI, int rank; MPI_Comm_rank(comm, &rank);)", "label": "(3, MPI, MPI_Barrier(MPI_COMM_WORLD);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (16, MPI, MPI_Comm_free(&comm);)"}
{"predict": "[i * COUNT + j] = rank + i + j;\n75:             recvbuf_h[i * COUNT + j] = 0xdeadbeef;\n76:         }\n77:     }\n78:     MTestCopyContent(buf_h, buf, COUNT * size * sizeof(int), memtype);\n79:     MTestCopyContent(recvbuf_h, recvbuf, COUNT * size * sizeof(int), memtype);\n80:     if (rank == root) {\n81:         MTestCopyContent(recvbuf, recvbuf_h, COUNT * size * sizeof(int), memtype);\n82:         for (i = 0; i < size; ++i) {\n83:             for (j = 0; j < COUNT; ++j) {\n84:                 if (recvbuf_h[i * COUNT + j] != ((i + j) * size))\n85:                     printf(\"got recvbuf_h[%d]=%d, expected %d\\n\", i * COUNT + j,\n86:                            recvbuf_h[i * COUNT + j], ((i + j) * size));\n87:                 my_assert(recvbuf_h[i * COUNT + j] == ((i + j) * size));\n88:             }\n89:         }\n90:     }\n91: \n92:     /* MPI_Ireduce_scatter */\n93:     for (i = 0; i < size; ++i) {\n94:         recvcounts[i] = COUNT;\n95:         for (j = 0; j < COUNT; ++j) {\n96:             buf_h[i * COUNT + j] = rank + i + j;\n97:             recvbuf_h[i * COUNT + j] = 0xdeadbeef;\n98:         }\n99:     }\n100:     MTestCopyContent(buf_h, buf, COUNT * size * sizeof(int), memtype);\n101:     MTestCopyContent(recvbuf_h, recvbuf, COUNT * size * sizeof(int), memtype);\n102:     MPI_Wait(&ireq, MPI_STATUS_IGNORE);\n103:     MTestCopyContent(recvbuf, recvbuf_h, COUNT * size * sizeof(int), memtype);\n104:     for (i = 0; i < size; ++i) {\n105:         for (j =", "label": "(169, MPI, MPI_Scan(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (28, MPI, MPI_Ireduce(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD, &req);); (82, MPI, MPI_Reduce_scatter(buf, recvbuf, recvcounts, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (45, MPI, MPI_Allreduce(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (124, MPI, MPI_Reduce_scatter_block(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (59, MPI, MPI_Iallreduce(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD, &req);); (100, MPI, MPI_Ireduce_scatter(buf, recvbuf, recvcounts, MPI_INT, MPI_SUM, MPI_COMM_WORLD, &req);); (23, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (3, MPI, MPI_Reduce(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD);); (187, MPI, MPI_Exscan(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (139, MPI, MPI_Ireduce_scatter_block(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD, &req);); (171, MPI, MPI_Iscan(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD, &req);); (198, MPI, MPI_Iexscan(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD, &req);)"}
{"predict": "(6, MPI, t = MPI_Wtime();)", "label": "(9, MPI, t = MPI_Wtime();); (11, MPI, t = MPI_Wtime() - t;)"}
{"predict": "(3, OpenMP, #pragma omp parallel for)\n(16, OpenMP, #pragma omp parallel for)", "label": "(14, OpenMP, #pragma omp parallel for proc_bind(close), proc_bind(spread)); (10, OpenMP, #pragma omp parallel for proc_bind(master); (19, OpenMP, #pragma omp parallel for proc_bind(master)); (5, OpenMP, #pragma omp parallel for proc_bind()); (-2, OpenMP, #pragma omp parallel for proc_bind); (12, OpenMP, #pragma omp parallel for proc_bind(x)); (19, OpenMP, #pragma omp parallel proc_bind(close)); (19, OpenMP, #pragma omp parallel for proc_bind(spread)); (-3, OpenMP, #pragma omp parallel for proc_bind()"}
{"predict": "(4, MPI, vtkh::InitializeMPI(&comm_size, &rank);) (11, MPI, MPI_Bcast(&num_blocks, 1, MPI_INT, 0, MPI_COMM_WORLD);) (17, MPI, MPI_Bcast(&domain_id, 1, MPI_INT, 0, MPI_COMM_WORLD);) (18, MPI, data_set.AddDomain(CreateTestData(domain_id, num_blocks, base_size), domain_id);) (21, MPI, vtkm::Bounds bounds = data_set.GetGlobalBounds(); MPI_Bcast(bounds, 6, MPI_DOUBLE, 0, MPI_COMM_WORLD);) (25, MPI, vtkm::rendering::Camera camera; MPI_Bcast(camera.GetData(), 3, MPI_DOUBLE, 0, MPI_COMM_WORLD); camera.ResetToBounds(bounds);) (32, MPI, vtkm::cont::ColorTable color_map(\"cool to warm\"); MPI_Bcast(color_map.GetData(), color_map.GetSize(), MPI_DOUBLE, 0, MPI_COMM_WORLD);) (37, MPI, vtkh::VolumeRenderer tracer; MPI_Bcast(tracer.GetData(), tracer.GetSize(), MPI_DOUBLE, 0, MPI_COMM_WORLD);) (40, MPI, vtkh::Scene scene; MPI_Bcast(scene.GetData(), scene.GetSize(), MPI_DOUBLE, 0, MPI_COMM_WORLD); scene.Render();)", "label": "(6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &comm_size);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (7, MPI, vtkh::SetMPICommHandle(MPI_Comm_c2f(MPI_COMM_WORLD));)"}
{"predict": "(20, CUDA, instance.setVariable<float>(\"x\", i + 1.0f);)", "label": "(16, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "6: // expected-error@+1 {{expected expression}}\n107: #pragma omp distribute simd aligned(x)\n108:   for (i = 0; i < 16; ++i)\n109:     ;\n110: #pragma omp target\n111: #pragma omp teams\n112: // expected-error@+1 {{expected expression}}\n113: #pragma omp distribute simd aligned(x)\n114:   for (i = 0; i < 16; ++i)\n115:     ;\n116: #pragma omp target\n117: #pragma omp teams\n118: // expected-error@+1 {{expected expression}}\n119: #pragma omp distribute simd aligned(x)\n120:   for (i = 0; i < 16; ++i)\n121:     ;\n122: #pragma omp target\n123: #pragma omp teams\n124: // expected-error@+1 {{expected expression}}\n125: #pragma omp distribute simd aligned(x)\n126:   for (i = 0; i < 16; ++i)\n127:     ;\n128: #pragma omp target\n129: #pragma omp teams\n130: // expected-error@+1 {{expected expression}}\n131: #pragma omp distribute simd aligned(x)\n132:   for (i = 0; i < 16; ++i)\n133:     ;\n134: #pragma omp target\n135: #pragma omp teams\n136: // expected-error@+1 {{expected expression}}\n137: #pragma omp distribute simd aligned(x)\n138:   for (i = 0; i < 16; ++i)\n139:     ;\n140: #pragma omp target\n141: #pragma omp teams\n142: // expected-error@+1 {{expected expression}}\n143: #pragma omp distribute simd aligned(x)\n144:   for (i = 0; i < 16; ++i)\n145:     ;\n146: #pragma omp target\n147: #pragma omp teams\n148: // expected-error@+1 {{expected expression}}\n149", "label": "(19, OpenMP, #pragma omp distribute simd aligned(, )); (75, OpenMP, #pragma omp distribute simd aligned(x :)); (80, OpenMP, #pragma omp distribute simd aligned(x :, )); (9, OpenMP, #pragma omp distribute simd aligned(,); (-2, OpenMP, #pragma omp target); (82, OpenMP, #pragma omp distribute simd aligned(x : 1)); (52, OpenMP, #pragma omp distribute simd aligned(x, y, z)); (24, OpenMP, #pragma omp distribute simd aligned(int)); (62, OpenMP, #pragma omp distribute simd aligned(z)); (-4, OpenMP, #pragma omp distribute simd aligned(); (40, OpenMP, #pragma omp distribute simd aligned(x, y)); (32, OpenMP, #pragma omp distribute simd aligned(x)); (80, OpenMP, #pragma omp distribute simd aligned(x : 2 * 2)); (12, OpenMP, #pragma omp distribute simd aligned()); (23, OpenMP, #pragma omp distribute simd aligned(0)); (83, OpenMP, #pragma omp distribute simd aligned(x : 1, y)); (88, OpenMP, #pragma omp distribute simd aligned(x : 1, y, z : 1)); (108, OpenMP, #pragma omp distribute simd aligned(x) aligned(z, x)); (116, OpenMP, #pragma omp distribute simd aligned(x, y, z) aligned(y, z)); (-16, OpenMP, #pragma omp teams)"}
{"predict": "(11, OpenMP, #pragma omp parallel for collapse(2))", "label": "(14, OpenMP, #pragma omp parallel for collapse(2)); (12, OpenMP, #pragma omp target map(tofrom: data))"}
{"predict": "(27, CUDA, cudaMalloc(&d_A, sizeof(data_type) * A.size());) (28, CUDA, cudaMalloc(&d_B, sizeof(data_type) * B.size());) (29, CUDA, cudaMalloc(&d_C, sizeof(data_type) * C.size());) (52, CUDA, CUBLAS_CHECK(cublasZherkx(cublasH, uplo, transa, n, k, &alpha, d_A, lda, d_B, ldb, &beta, d_C, ldc));) (58, CUDA, CUDA_CHECK(cudaMemcpyAsync(C.data(), d_C, sizeof(data_type) * C.size(), cudaMemcpyDeviceToHost, stream));) (62, CUDA, cudaFree(d_A);) (63, CUDA, cudaFree(d_B);) (64, CUDA, cudaFree(d_C);)", "label": "(78, CUDA, CUDA_CHECK(cudaFree(d_C));); (63, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (46, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_B), sizeof(data_type) * B.size()));); (74, CUDA, CUDA_CHECK(cudaFree(d_B));); (45, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_C), sizeof(data_type) * C.size()));); (79, CUDA, CUDA_CHECK(cudaDeviceReset());); (76, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (40, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (68, CUDA, CUDA_CHECK(cudaFree(d_A));); (34, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(8, OpenCL, cl_int err = clGetDeviceInfo(device, CL_DEVICE_IL_VERSION, str_size, str.data(), NULL);); (17, OpenCL, if (err != CL_SUCCESS) { log_error(\"clGetDeviceInfo: cannot read CL_DEVICE_IL_VERSION value;\"); return false; }); (28, OpenCL, cl_int err = clGetDeviceInfo(device, CL_DEVICE_IL_VERSION_KHR, 0, NULL, &str_size);); (37, OpenCL, if (err != CL_SUCCESS) { log_error(\"clGetDeviceInfo: cannot read CL_DEVICE_IL_VERSION_KHR value;\"); return false; });", "label": "(7, OpenCL, err = clGetDeviceInfo(device, CL_DEVICE_IL_VERSION, 0, NULL, &str_size);)"}
{"predict": "(11, OpenMP, #pragma omp parallel for reduction(min:minval))", "label": "(10, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:minval) reduction(min:minval))"}
{"predict": "have rank %d\\n\", icrank, wrank + csize);\n105:                 }\n106:             }\n107:         }\n108: \n109:         /* Finalize the tests */\n110:         MTest_Finalize(errs);\n111:         MPI_Finalize();\n112:     }\n113:     return errs;\n114: }", "label": "(65, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (56, MPI, MPI_Intercomm_merge(intercomm, isChild, &intracomm);); (161, MPI, MPI_Comm_free(&intracomm3);); (95, MPI, MPI_Comm_rank(intracomm2, &icrank);); (12, MPI, MPI_Comm_get_parent(&parentcomm);); (34, MPI, MPI_Send(&i, 1, MPI_INT, i, 0, intercomm);); (141, MPI, MPI_Ssend(&errs, 1, MPI_INT, 0, 1, intercomm);); (56, MPI, MPI_Comm_size(intracomm, &icsize);); (146, MPI, MPI_Recv(&err, 1, MPI_INT, i, 1, intercomm, MPI_STATUS_IGNORE);); (117, MPI, MPI_Intercomm_merge(intercomm, 0, &intracomm3);); (65, MPI, MPI_Comm_remote_size(parentcomm, &psize);); (80, MPI, MPI_Intercomm_merge(intercomm, !isChild, &intracomm2);); (149, MPI, MPI_Comm_free(&intracomm);); (102, MPI, MPI_Comm_remote_size(intercomm, &csize);); (83, MPI, MPI_Comm_size(intracomm2, &icsize);); (49, MPI, MPI_Comm_rank(intracomm, &icrank);); (150, MPI, MPI_Comm_free(&intercomm);); (32, MPI, MPI_Recv(&i, 1, MPI_INT, 0, 0, intercomm, &status);); (114, MPI, MPI_Comm_rank(intracomm3, &icrank);); (10, MPI, MPI_Comm_rank(intercomm, &rank);); (7, MPI, MPI_Comm_remote_size(intercomm, &rsize);); (7, MPI, MPI_Comm_size(intercomm, &size);); (109, MPI, MPI_Comm_size(intracomm3, &icsize);); (139, MPI, MPI_Comm_free(&intracomm2);); (150, MPI, MPI_Finalize();)"}
{"predict": "Descr(&descrA);\n70:   status = cusparseCreateMatDescr(&descrA);\n71:   // CHECK: assert(HIPSPARSE_STATUS_SUCCESS == status);\n72:   assert(CUSPARSE_STATUS_SUCCESS == status);\n73:   // CHECK: status = hipsparseSetMatType(descrA, HIPSPARSE_MATRIX_TYPE_GENERAL);\n74:   status = cusparseSetMatType(descrA, CUSPARSE_MATRIX_TYPE_GENERAL);\n75:   // CHECK: assert(HIPSPARSE_STATUS_SUCCESS == status);\n76:   assert(CUSPARSE_STATUS_SUCCESS == status);\n77:   // CHECK: status = hipsparseSetMatIndexBase(descrA, HIPSPARSE_INDEX_BASE_ZERO);\n78:   status = cusparseSetMatIndexBase(descrA, CUSPARSE_INDEX_BASE_ZERO);\n79:   // CHECK: assert(HIPSPARSE_STATUS_SUCCESS == status);\n80:   assert(CUSPARSE_STATUS_SUCCESS == status);\n81: \n82:   // CHECK: status = hipsparseCreateMatDescr(&descrC);\n83:   status = cusparseCreateMatDescr(&descrC);\n84:   // CHECK: assert(HIPSPARSE_STATUS_SUCCESS == status);\n85:   assert(CUSPARSE_STATUS_SUCCESS == status);\n86:   // CHECK: status = hipsparseSetMatType(descrC, HIPSPARSE_MATRIX_TYPE_GENERAL);\n87:   status = cusparseSetMatType(descrC, CUSPARSE_MATRIX_TYPE_GENERAL);\n88:   // CHECK: assert(HIPSPARSE_STATUS_SUCCESS == status);\n89:   assert(CUSPARSE_STATUS_SUCCESS == status);\n90:   // CHECK: status = hipsparseSetMatIndexBase(descrC, HIPSPARSE_INDEX_BASE_ZERO);\n91:   status = cusparseSetMatIndexBase(descrC, CUSPARSE_INDEX_BASE_ZERO);\n92:   // CHECK: assert(HIPSPARSE_STATUS_SUCCESS == status);\n93:   assert(CUSPARSE_STATUS_SUCCESS == status);\n94: \n95:   /* step 3: allocate device memory */\n96:   // CHECK: cudaStat1 = hipMalloc((void**)&d_csrRowPtrA, (m + 1) * sizeof(int));\n97:   cudaStat1 = cudaMalloc((void**)&d_csrRowPtrA, (m + 1) * sizeof(int", "label": "(106, CUDA, cudaStat1 = cudaMemcpy(d_csrRowPtrA, csrRowPtrA, sizeof(int)*(m + 1), cudaMemcpyHostToDevice);); (234, CUDA, if (d_csrColIndC) cudaFree(d_csrColIndC);); (211, CUDA, cudaStat1 = cudaMemcpy(csrRowPtrC, d_csrRowPtrC, sizeof(int)*(m + 1), cudaMemcpyDeviceToHost);); (99, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrRowPtrC, sizeof(int)*(m + 1));); (94, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrValA, sizeof(float)*nnzA);); (109, CUDA, cudaStat1 = cudaMemcpy(d_csrValA, csrValA, sizeof(float)*nnzA, cudaMemcpyHostToDevice);); (221, CUDA, if (d_csrRowPtrA) cudaFree(d_csrRowPtrA);); (237, CUDA, if (stream) cudaStreamDestroy(stream);); (225, CUDA, if (d_csrRowPtrC) cudaFree(d_csrRowPtrC);); (170, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrValC, sizeof(float) * nnzC);); (227, CUDA, if (d_csrValC) cudaFree(d_csrValC);); (154, CUDA, cudaStat1 = cudaDeviceSynchronize();); (78, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrRowPtrA, sizeof(int)*(m + 1));); (204, CUDA, cudaStat1 = cudaMemcpy(csrColIndC, d_csrColIndC, sizeof(int)*nnzC, cudaMemcpyDeviceToHost);); (80, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrColIndA, sizeof(int)*nnzA);); (41, CUDA, cudaStat1 = cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking);); (94, CUDA, cudaStat1 = cudaMemcpy(d_csrColIndA, csrColIndA, sizeof(int)*nnzA, cudaMemcpyHostToDevice);); (125, CUDA, cudaStat1 = cudaMalloc((void**)&d_work, lworkInBytes);); (211, CUDA, if (d_csrColIndA) cudaFree(d_csrColIndA);); (202, CUDA, cudaStat1 = cudaMemcpy(csrValC, d_csrValC, sizeof(float)*nnzC, cudaMemcpyDeviceToHost);); (155, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrColIndC, sizeof(int) * nnzC);); (210, CUDA, if (d_csrValA) cudaFree(d_csrValA);); (228, CUDA, cudaDeviceReset();)"}
{"predict": "and attributes.\n108:      */\n109:     test_data_nocomparables(DATA_NO_COMPARE, 0);\n110:     test_objs_nocomparables(OBJ_NO_COMPARE, OBJ_NO_COMPARE);\n111: \n112:     /* -------------------------------------------------\n113:      * Create test files with dataset and attribute with container types \n114:      * (array, vlen) with multiple nested compound types.\n115:      */\n116:     /* file1 */\n117:     write_attr_in(ATTR_FILE1, \"dset1\", ATTR_FILE1, 0);\n118:     write_dset_in(ATTR_FILE1, \"dset1\", ATTR_FILE1, 0);\n119:     /* file2 */\n120:     write_attr_in(ATTR_FILE2, \"dset1\", ATTR_FILE2, 1);\n121:     write_dset_in(ATTR_FILE2, \"dset1\", ATTR_FILE2, 1);\n122: \n123:     /* -------------------------------------------------\n124:      * Generate data registry for all files created above.\n125:      */\n126:     gen_datareg(ATTR_FILE2, 1);\n127:     gen_datareg(ATTR_FILE1, 0);\n128: \n129:     return 0;\n130: }", "label": "(81, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (81, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)"}
{"predict": "(14, OpenMP, #pragma omp parallel for)", "label": "(26, OpenCL, clReleaseContext(knode->key.context);); (24, OpenCL, clReleaseProgram(kern->program);)"}
{"predict": "8: \n119:     MPI_Barrier(MPI_COMM_WORLD);\n120: \n121:     /* Test Sendrecv, head to head, using MPI_Sendrecv_replace */\n122:     if (wrank == 0 && verbose) {\n123:         printf(\"Sendrecv_replace\\n\");\n124:         printf(\"len\\ttime (usec)\\trate (MB/s)\\n\");\n125:     }\n126: \n127:     /* Send powers of 2 bytes */\n128:     len = 1;\n129:     for (k = 0; k < 20; k++) {\n130:         /* We use a simple linear form for the number of tests to\n131:          * reduce the impact of the granularity of the timer */\n132:         reps = 50 - k;\n133:         repsleft = reps;\n134:         /* Make sure that both processes are ready to start */\n135:         MPI_Sendrecv(MPI_BOTTOM, 0, MPI_BYTE, partner, 0,\n136:                      MPI_BOTTOM, 0, MPI_BYTE, partner, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n137:         t1 = MPI_Wtime();\n138:         while (repsleft--) {\n139:             MPI_Sendrecv_replace(sbuf, len, MPI_BYTE, partner, k,\n140:                                 rbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n141:         }\n142:         t1 = MPI_Wtime() - t1;\n143:         times[2][k] = t1 / reps;\n144:         if (wrank == 0) {\n145:             t1 = t1 / reps;\n146:             if (t1 > 0) {\n147:                 t1 = t1 * 1.e6;\n148:                 if (verbose)\n149:                     printf(\"%d\\t%g\\t%g\\n\", len, t1, len / t1);\n150:             } else {\n151:                 t1 = t1 * 1.e6;\n152:                 if (verbose)\n153:                     printf(\"%d\\t%", "label": "(13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (60, MPI, MPI_Irecv(rbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD, &rreq);); (57, MPI, t1 = MPI_Wtime();); (60, MPI, MPI_Wait(&rreq, MPI_STATUS_IGNORE);); (146, MPI, MPI_Recv(rbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (57, MPI, MPI_Send(sbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (34, MPI, MPI_Barrier(MPI_COMM_WORLD);); (57, MPI, t1 = MPI_Wtime() - t1;); (8, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (24, MPI, MPI_Abort(MPI_COMM_WORLD, 2);)"}
{"predict": "(21, OpenMP, #pragma omp parallel for)", "label": "(1, TBB, using namespace tbb::flow;)"}
{"predict": "(2, OpenMP, #pragma omp parallel for)", "label": "(7, OpenMP, #pragma omp teams distribute parallel for simd safelen (); (21, OpenMP, #pragma omp teams distribute parallel for simd safelen (2+2))); (10, OpenMP, #pragma omp teams distribute parallel for simd safelen ()); (54, OpenMP, #pragma omp teams distribute parallel for simd safelen(safelen(tmain<int, char, -1, -2>(argc, argv)); (37, OpenMP, #pragma omp teams distribute parallel for simd safelen (S1)); (12, OpenMP, #pragma omp teams distribute parallel for simd safelen (4); (45, OpenMP, #pragma omp teams distribute parallel for simd safelen (argv[1]=2)); (21, OpenMP, #pragma omp teams distribute parallel for simd safelen (foobool(1) > 0 ? 1 : 2)); (28, OpenMP, #pragma omp teams distribute parallel for simd safelen (foobool(argc)), safelen (true), safelen (-5)); (-8, OpenMP, #pragma omp target); (-8, OpenMP, #pragma omp teams distribute parallel for simd safelen)"}
{"predict": "(27, MPI, SimModel_start();) (28, MPI, Sim_readLicenseFile(0);) (29, MPI, SimPartitionedMesh_start(0, 0);) (30, MPI, gmi_sim_start();) (31, MPI, gmi_register_sim();) (33, MPI, gmi_register_mesh();) (34, MPI, gmi_register_null();) (51, MPI, gmi_sim_stop();) (52, MPI, SimPartitionedMesh_stop();) (53, MPI, SimModel_stop();) (54, MPI, Sim_unregisterAllKeys();)", "label": "(24, MPI, MPI_Finalize();); (12, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(4, OpenMP, #pragma omp parallel for private(v0, r))", "label": "(20, TBB, tbb::flow::make_edge( n, r[i] );); (54, TBB, tbb::flow::remove_edge( n, r[i] );); (1, TBB, tbb::flow::write_once_node<R> n(g);); (-1, TBB, tbb::flow::graph g;)"}
{"predict": "(4, OpenMP, #pragma omp parallel for) (6, OpenMP, #pragma omp parallel for) (8, OpenMP, #pragma omp parallel for) (10, OpenMP, #pragma omp parallel for) (12, OpenMP, #pragma omp parallel for) (14, OpenMP, #pragma omp parallel for) (16, OpenMP, #pragma omp parallel for) (18, OpenMP, #pragma omp parallel for) (20, OpenMP, #pragma omp parallel for) (22, OpenMP, #pragma omp parallel for) (24, OpenMP, #pragma omp parallel for) (26, OpenMP, #pragma omp parallel for) (28, OpenMP, #pragma omp parallel for) (30, OpenMP, #pragma omp parallel for) (35, OpenMP, #pragma omp parallel for) (37, OpenMP, #pragma omp parallel for) (42, OpenMP, #pragma omp parallel for)", "label": "(16, OpenMP, #pragma omp distribute parallel for schedule (auto); (45, OpenMP, #pragma omp distribute parallel for schedule (guided, foobool(argc)), schedule (static, true), schedule (dynamic, -5)); (48, OpenMP, #pragma omp distribute parallel for schedule (guided, S1)); (52, OpenMP, #pragma omp distribute parallel for schedule (static, argv[1]=2)); (0, OpenMP, #pragma omp distribute parallel for schedule); (31, OpenMP, #pragma omp distribute parallel for schedule (static, 2+2))); (2, OpenMP, #pragma omp distribute parallel for schedule (); (5, OpenMP, #pragma omp distribute parallel for schedule ()); (32, OpenMP, #pragma omp distribute parallel for schedule (dynamic, foobool(1) > 0 ? 1 : 2 - z)); (23, OpenMP, #pragma omp distribute parallel for schedule (guided, 4); (51, OpenMP, #pragma omp distribute parallel for schedule(dynamic, schedule(tmain<int, char, -1, -2>(argc, argv)); (17, OpenMP, #pragma omp distribute parallel for schedule (runtime, 3)); (-10, OpenMP, #pragma omp target); (11, OpenMP, #pragma omp distribute parallel for schedule (auto,); (6, OpenMP, #pragma omp distribute parallel for schedule (auto_dynamic); (-12, OpenMP, #pragma omp teams)"}
{"predict": "(12, CUDA, const int block_size = 256; const int grid_size = (N + block_size - 1) / block_size; for (int iter = 0; iter < niters; iter++) { for (int i = 0; i < N; i++) { ctx.task(var_handle.relaxed(redux_op))->*[=](cudaStream_t stream, auto d_var) { add_val<<<grid_size, block_size, 0, stream>>>(d_var.data_handle(), i); }; } } for (int iter = 0; iter < niters; iter++) { ctx.task(var_handle.rw())->*[=](cudaStream_t stream, auto d_var) { int expected = (N * (N - 1)) / 2; check_value_and_reset<<<grid_size, block_size, 0, stream>>>(d_var.data_handle(), expected, 0); }; } ctx.finalize();)", "label": "(18, CUDA, cuda_safe_call(cudaGetLastError());)"}
{"predict": "(26, OpenMP, #pragma omp parallel for private(i, j) reduction(+:hostMatrix[i * width + j]));", "label": "(21, CUDA, cudaMalloc((void**)&deviceDiagonalResult, sizeof(bool));); (55, CUDA, cudaMemcpy(hostDiagonalResult, deviceDiagonalResult, sizeof(bool), cudaMemcpyDeviceToHost);); (99, CUDA, cudaFree(deviceSymmetricResult);); (20, CUDA, cudaMalloc((void**)&deviceSkewSymmetricResult, sizeof(bool));); (54, CUDA, cudaMemcpy(hostSkewSymmetricResult, deviceSkewSymmetricResult, sizeof(bool), cudaMemcpyDeviceToHost);); (17, CUDA, cudaMalloc((void**)&deviceSymmetricResult, sizeof(bool));); (33, CUDA, cudaMemcpy(deviceMatrix, hostMatrix, width * height * sizeof(float), cudaMemcpyHostToDevice);); (37, CUDA, cudaMemcpy(deviceIdempotentResult, hostIdempotentResult, sizeof(bool), cudaMemcpyHostToDevice);); (11, CUDA, cudaMalloc((void**)&deviceMatrix, width * height * sizeof(float));); (31, CUDA, cudaMemcpy(deviceToeplitzResult, hostToeplitzResult, sizeof(bool), cudaMemcpyHostToDevice);); (49, CUDA, cudaMemcpy(hostIdempotentResult, deviceIdempotentResult, sizeof(bool), cudaMemcpyDeviceToHost);); (91, CUDA, cudaFree(deviceSkewSymmetricResult);); (31, CUDA, cudaMemcpy(deviceSkewSymmetricResult, hostSkewSymmetricResult, sizeof(bool), cudaMemcpyHostToDevice);); (44, CUDA, cudaMemcpy(hostSymmetricResult, deviceSymmetricResult, sizeof(bool), cudaMemcpyDeviceToHost);); (28, CUDA, cudaMemcpy(deviceSymmetricResult, hostSymmetricResult, sizeof(bool), cudaMemcpyHostToDevice);); (85, CUDA, cudaFree(deviceDiagonalResult);); (4, CUDA, cudaMalloc((void**)&deviceToeplitzResult, sizeof(bool));); (82, CUDA, cudaFree(deviceToeplitzResult);); (85, CUDA, cudaFree(deviceIdempotentResult);); (36, CUDA, cudaMemcpy(hostToeplitzResult, deviceToeplitzResult, sizeof(bool), cudaMemcpyDeviceToHost);); (78, CUDA, cudaFree(deviceMatrix);); (20, CUDA, cudaMemcpy(deviceDiagonalResult, hostDiagonalResult, sizeof(bool), cudaMemcpyHostToDevice);); (2, CUDA, cudaMalloc((void**)&deviceIdempotentResult, sizeof(bool));)"}
{"predict": "clReleaseEvent(writeEvent);\n101:             clReleaseMemObject( streams[ii] );\n102:             clReleaseMemObject( streams[ii+1] );\n103:             free( outptr[i] );\n104:             return -1;\n105:         }\n106: \n107:         err = clGetEventProfilingInfo( writeEvent, CL_PROFILING_COMMAND_END, sizeof( cl_ulong ), &writeEnd, NULL );\n108:         if( err != CL_SUCCESS ){\n109:             print_error( err, \"clGetEventProfilingInfo failed\" );\n110:             clReleaseEvent(writeEvent);\n111:             clReleaseMemObject( streams[ii] );\n112:             clReleaseMemObject( streams[ii+1] );\n113:             free( outptr[i] );\n114:             return -1;\n115:         }\n116: \n117:         // test kernel execution\n118:         program[ii] = clCreateProgramWithSource(context, 1, (const char **)&kernelCode[ii], NULL, &err);\n119:         if( err != CL_SUCCESS ){\n120:             print_error( err, \"clCreateProgramWithSource failed\" );\n121:             clReleaseMemObject( streams[ii] );\n122:             clReleaseMemObject( streams[ii+1] );\n123:             free( outptr[i] );\n124:             return -1;\n125:         }\n126: \n127:         err = clBuildProgram(program[ii], 1, &device, NULL, NULL, NULL);\n128:         if( err != CL_SUCCESS ){\n129:             print_error( err, \"clBuildProgram failed\" );\n130:             clReleaseMemObject( streams[ii] );\n131:             clReleaseMemObject( streams[ii+1] );\n132:             free( outptr[i] );\n133:             return -1;\n134:         }\n135: \n136:         kernel[ii] = clCreateKernel(program[ii], kernelName[ii], &err);\n137:         if( err != CL_SUCCESS ){\n138:             print_error( err, \"clCreateKernel failed\" );\n139", "label": "(57, OpenCL, err = clEnqueueWriteBuffer( queue, streams[ii], false, 0, ptrSizes[i]*num_elements, inptr[i], 0, NULL, &writeEvent );); (132, OpenCL, err = clSetKernelArg( kernel[i], 0, sizeof( cl_mem ), (void *)&streams[ii] );); (49, OpenCL, clReleaseMemObject(streams[ii]);); (135, OpenCL, clReleaseProgram( program[i] );); (123, OpenCL, clReleaseMemObject(streams[ii+1]);); (55, OpenCL, clReleaseMemObject( streams[ii+1] );); (140, OpenCL, err = clEnqueueNDRangeKernel( queue, kernel[i], 1, NULL, threads, NULL, 0, NULL, NULL );); (52, OpenCL, clReleaseMemObject( streams[ii] );); (64, OpenCL, clReleaseEvent(writeEvent);); (59, OpenCL, err = clWaitForEvents( 1, &writeEvent );); (127, OpenCL, clReleaseKernel( kernel[i] );); (123, OpenCL, err |= clSetKernelArg( kernel[i], 1, sizeof( cl_mem ), (void *)&streams[ii+1] );); (148, OpenCL, err = clEnqueueReadBuffer( queue, streams[ii+1], true, 0, outPtrSizes[i]*num_elements, outptr[i], 0, NULL, NULL );)"}
{"predict": "(14, OpenMP, #pragma omp parallel for) (21, OpenMP, #pragma omp parallel for) (28, OpenMP, #pragma omp parallel for) (35, OpenMP, #pragma omp parallel for)", "label": "(15, OpenMP, #pragma omp target teams distribute parallel for simd linear (); (11, OpenMP, #pragma omp target teams distribute parallel for simd linear); (34, OpenMP, #pragma omp target teams distribute parallel for simd linear (S1)); (37, OpenMP, #pragma omp target teams distribute parallel for simd linear (a, b)); (39, OpenMP, #pragma omp target teams distribute parallel for simd linear (argv[1])); (44, OpenMP, #pragma omp target teams distribute parallel for simd linear(h, C::x)); (23, OpenMP, #pragma omp target teams distribute parallel for simd linear (argc > 0 ? argv[1] : argv[2])); (15, OpenMP, #pragma omp target teams distribute parallel for simd linear (argc); (10, OpenMP, #pragma omp target teams distribute parallel for simd linear ()); (24, OpenMP, #pragma omp target teams distribute parallel for simd linear (argc)); (36, OpenMP, #pragma omp target teams distribute parallel for simd linear(e, g)); (15, OpenMP, #pragma omp target teams distribute parallel for simd linear (argc,)"}
{"predict": "(22, OpenCL, props[1] = (cl_context_properties)platform;)", "label": "(56, OpenCL, err = clEnqueueWriteBuffer(queue, bufY, CL_TRUE, 0, (lenY*sizeof(cl_float)), Y, 0, NULL, NULL);); (82, OpenCL, clReleaseMemObject(bufX);); (44, OpenCL, clReleaseCommandQueue(queue);); (28, OpenCL, printf( \"clCreateContext() failed with %d\\n\", err );); (75, OpenCL, clReleaseEvent(event);); (32, OpenCL, printf( \"clCreateCommandQueue() failed with %d\\n\", err );); (29, OpenCL, queue = clCreateCommandQueue(ctx, device, 0, &err);); (46, OpenCL, bufY = clCreateBuffer(ctx, CL_MEM_READ_WRITE, (lenY*sizeof(cl_float)), NULL, &err);); (30, OpenCL, clReleaseContext(ctx);); (73, OpenCL, clReleaseMemObject(bufY);); (42, OpenCL, bufX = clCreateBuffer(ctx, CL_MEM_READ_ONLY, (lenX*sizeof(cl_float)), NULL, &err);); (44, OpenCL, err = clEnqueueWriteBuffer(queue, bufX, CL_TRUE, 0, (lenX*sizeof(cl_float)), X, 0, NULL, NULL);); (17, OpenCL, ctx = clCreateContext(props, 1, &device, NULL, NULL, &err);); (53, OpenCL, err = clWaitForEvents(1, &event);)"}
{"predict": "(14, MPI, CUFFT_CHECK(cufftMpAttachComm(plan_r2c, CUFFT_COMM_MPI, &comm));) (15, MPI, CUFFT_CHECK(cufftMpAttachComm(plan_c2r, CUFFT_COMM_MPI, &comm));)", "label": "(62, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (6, CUDA, CUDA_CHECK(cudaStreamCreate(&stream));); (46, CUDA, CUDA_CHECK(cudaGetLastError());); (52, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));)"}
{"predict": "(2, CUDA, if (EasyCL::isOpenCLAvailable()) { cudaSetDevice(0); }); (14, CUDA, float *in_d = new float[bufferSize]; cudaMemcpy(in_d, in, bufferSize * sizeof(float), cudaMemcpyHostToDevice);); (15, CUDA, float *in2_d = new float[bufferSize]; cudaMemcpy(in2_d, in2, bufferSize * sizeof(float), cudaMemcpyHostToDevice);); (21, CUDA, cudaMemcpy(in2_d, in_d, bufferSize * sizeof(float), cudaMemcpyDeviceToDevice);); (24, CUDA, cudaMemcpy(in, in2_d, bufferSize * sizeof(float), cudaMemcpyDeviceToHost);); (27, CUDA, cudaMemcpy(in, in_d, bufferSize * sizeof(float), cudaMemcpyDeviceToHost);); (33, CUDA, delete[] in_d; delete[] in2_d;);", "label": "(7, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)"}
{"predict": "(12, CUDA, for (size_t devi = 0; devi < devs_size; devi++) {)\n(12, CUDA, auto streams = new cudaStream_t[devs_size];)\n(12, CUDA, for (size_t devi = 0; devi < devs_size; devi++) { streams[devi] = 0; })\n(12, CUDA, for (size_t devi = 0; devi < devs_size; devi++) { cudaStreamCreate(&streams[devi]); })\n(12, CUDA, for (size_t devi = 0; devi < devs_size; devi++) { auto &dev = devs[devi]; })\n(12, CUDA, for (size_t devi = 0; devi < devs_size; devi++) { auto &plan = plans->at(devi); })\n(12, CUDA, for (size_t devi = 0; devi < devs_size; devi++) { auto &grid_size = grid_sizes->at(devi); })\n(12, CUDA, for (size_t devi = 0; devi < devs_size; devi++) { auto &blocks = blocks; })\n(12, CUDA, for (size_t devi = 0; devi < devs_size; devi++) { auto &bins = bins; })\n(12, CUDA, for (size_t devi = 0; devi < devs_size; devi++) { auto &offset = offset; })\n(12, CUDA, for (size_t devi = 0; devi < devs_size; devi++) { auto &row = row; })\n(12, CUDA, for (size_t devi = 0; devi < devs_size; devi++) { auto &delta = delta; })\n(12, CUDA, for (size_t devi = 0; devi < devs_size; devi++) { auto &binavgcount = binavgcount; })\n(12, CUDA, for (size_t devi = 0; devi < devs_size; devi++) { auto &binsize = binsize; })\n(12, CUDA, for (size_t devi = 0; devi < devs_size; devi++) { auto &plan = plan; })\n(12, CUDA, for (size_t devi", "label": "(11, OpenMP, #pragma omp parallel for)"}
{"predict": "(4, OpenCL, clReleaseMemObject(objA);) (7, OpenCL, clReleaseMemObject(objB);) (10, OpenCL, clReleaseMemObject(objC);)", "label": "(5, OpenCL, clReleaseMemObject(objA);); (8, OpenCL, clReleaseMemObject(objB);); (11, OpenCL, clReleaseMemObject(objC);)"}
{"predict": "(13, OpenMP, #pragma omp parallel for shared(nthreads))", "label": "(7, OpenMP, #pragma omp parallel); (7, OpenMP, #pragma omp single); (3, OpenMP, my_dev = omp_get_initial_device();); (6, OpenMP, nthreads = omp_get_num_threads();); (10, OpenMP, #pragma omp parallel shared(nthreads))"}
{"predict": "(9, CUDA, float sum2 = 0.0;)\n(11, CUDA, sum2 += x * x;)\n(17, CUDA, for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {)\n(21, CUDA, sdata2[tid] += sdata2[tid + s];)\n(27, CUDA, scale = 1.0 / sqrt(sdata2[0] / channels + eps);)\n(33, CUDA, output[i] = (input[i] * scale * weight[i]);)", "label": "(7, CUDA, __shared__ float scale;); (5, CUDA, __shared__ float sdata2[THREAD_PER_BLOCK];)"}
{"predict": "(43, OpenMP, #pragma omp parallel for)", "label": "(8, OpenMP, #pragma omp teams shared); (61, OpenMP, #pragma omp teams shared(i)); (45, OpenMP, #pragma omp teams shared(e, g, z)); (4, OpenMP, #pragma omp target); (13, OpenMP, #pragma omp teams shared (argc); (51, OpenMP, #pragma omp teams firstprivate(i), shared(i)); (17, OpenMP, #pragma omp teams shared (argc > 0 ? argv[1] : argv[2])); (22, OpenMP, #pragma omp teams shared (S1)); (18, OpenMP, #pragma omp teams shared (argc)); (35, OpenMP, #pragma omp teams shared(da)); (31, OpenMP, #pragma omp teams shared(ca)); (54, OpenMP, #pragma omp teams shared(j)); (23, OpenMP, #pragma omp teams shared (argv[1])); (37, OpenMP, #pragma omp teams shared(h, B::x)); (39, OpenMP, #pragma omp teams private(i)); (-1, OpenMP, #pragma omp teams shared ()); (22, OpenMP, #pragma omp teams shared(ba)); (3, OpenMP, #pragma omp teams shared (argc,); (14, OpenMP, #pragma omp teams shared (a, b, c, d, f)); (-8, OpenMP, #pragma omp teams shared (); (48, OpenMP, #pragma omp teams firstprivate(i))"}
{"predict": "(10, OpenMP, #pragma omp parallel for private(l_val))", "label": "(15, OpenMP, #pragma omp target map(l_val)); (9, OpenMP, #pragma omp task); (3, OpenMP, #pragma omp parallel); (4, OpenMP, #pragma omp single)"}
{"predict": "(16, MPI, MPIX_Stream_create(MPI_INFO_NULL, &streams[i]);)", "label": "(8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (41, MPI, mpi_errno = MPI_Comm_free(&stream_comm);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (13, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(30, MPI, MPE_Log_event(5004, 0, NULL);)", "label": "(58, MPI, MPI_Type_commit(file_realm_types);); (56, MPI, MPI_Type_contiguous_c((max_end_offset - min_st_offset + 1), MPI_BYTE, file_realm_types);)"}
{"predict": "(37, OpenCL, OCL_NDRANGE(1);)", "label": "(25, OpenCL, OCL_ASSERT(clBuildProgram(program, 1, &device, NULL, NULL, NULL) == CL_SUCCESS);); (26, OpenCL, kernel = clCreateKernel(program, \"compiler_ceil\", &status);)"}
{"predict": "(34, CUDA, for (int point = 0; point < 100; point++) { auto s_cpu = state_trajectory.col(point); auto u_cpu = control_trajectory.col(point); auto state_der_cpu = BicycleSlipParametric::state_array::Zero(); dynamics.computeDynamics(s_cpu, u_cpu, state_der_cpu); for (int dim = 0; dim < 12; dim++) { s_der[point][dim] = state_der_cpu(dim); } });", "label": "(4, CUDA, CudaCheckError();); (49, CUDA, dynamics.freeCudaMem();)"}
{"predict": "(16, CUDA, cudaDeviceSynchronize();)", "label": "(18, CUDA, CUDA_CHECK(cudaGetLastError());)"}
{"predict": "(19, CUDA, auto inData = in.exportData<nvcv::TensorDataStridedCuda>();) (22, CUDA, auto inAccess = nvcv::TensorDataAccessStridedImagePlanar::Create(*inData);) (35, CUDA, std::vector<uint8_t> inVec(inBufSize, uint8_t{0});) (38, CUDA, for (int z = 0; z < inShape.z; ++z) for (int y = 0; y < inShape.y; ++y) for (int x = 0; x < inShape.x; ++x) test::ValueAt<InVT>(inVec, inStrides, int3{x, y, z}).x = rg(g_rng);) (43, CUDA, nvcv::Tensor minVal({{inShape.z}, \"N\"}, valDataType);) (46, CUDA, nvcv::Tensor minLoc({{inShape.z, capacity}, \"NM\"}, nvcv::TYPE_2S32);) (49, CUDA, nvcv::Tensor numMin({{inShape.z}, \"N\"}, nvcv::TYPE_S32);) (54, CUDA, cvcuda::MinMaxLoc op;) (60, CUDA, EXPECT_NO_THROW(op(stream, in, minVal, minLoc, numMin, nullptr, nullptr, nullptr));) (63, CUDA, EXPECT_NO_THROW(op(stream, in, nullptr, nullptr, nullptr, maxVal, maxLoc, numMax));) (66, CUDA, EXPECT_NO_THROW(op(stream, in, minVal, minLoc, numMin, maxVal, maxLoc, numMax));) (74, CUDA, GoldMinMaxLoc<InVT, OutVT>(minVal, minLoc, numMin, maxVal, maxLoc, numMax, inVec, inStrides, inShape, res);)", "label": "(39, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(inData->basePtr(), inVec.data(), inBufSize, cudaMemcpyHostToDevice));); (71, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (71, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamDestroy(stream));); (51, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));)"}
{"predict": "(14, CUDA, cudaMalloc((void**)&c, datasize);)", "label": "(5, CUDA, cudaSetDevice(0);); (8, CUDA, cudaMemGetInfo(&freeCUDAMem, &totalCUDAMem);)"}
{"predict": "std::cout << \"Rate (MB/s): \" << 1.0e-6 * bytes / avgtime << std::endl;\n133:     std::cout << \"Avg time (s): \" << avgtime << std::endl;\n134:   } else {\n135:     std::cout << \"ERROR: Aggregate squared error \" << abserr << \" exceeds threshold \" << epsilon << std::endl;\n136:   }\n137: \n138:   return 0;\n139: }", "label": "(48, OpenMP, std::cout << \"Number of threads    = \" << omp_get_max_threads() << std::endl;)"}
{"predict": "(13, MPI, MPI_Init(&argc, &argv);)", "label": "(35, MPI, merr = MPI_Unpublish_name(serv_name, MPI_INFO_NULL, port_name);); (24, MPI, merr = MPI_Publish_name(serv_name, MPI_INFO_NULL, port_name);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (25, MPI, MPI_Error_string(merr, errmsg, &msglen);); (41, MPI, merr = MPI_Lookup_name(serv_name, MPI_INFO_NULL, port_name_out);); (64, MPI, MPI_Error_class(merr, &mclass);); (16, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (25, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(3, OpenMP, #pragma omp parallel for private(gtid, data1, data2))", "label": "(13, OpenMP, __kmpc_omp_task(nullptr, gtid, task1);); (1, OpenMP, #pragma omp parallel for); (18, OpenMP, __kmpc_omp_task(nullptr, gtid, task2);)"}
{"predict": "(27, OpenMP, #pragma omp parallel for private(num_pushed, num_popped, failed_pushes, failed_pops) reduction(+:num_pushed, num_popped, failed_pushes, failed_pops))", "label": "(55, TBB, tbb::concurrent_bounded_queue<int> iq4;); (22, TBB, tbb::concurrent_bounded_queue<int> iq2;); (35, TBB, tbb::concurrent_bounded_queue<int> iq3;); (3, TBB, tbb::concurrent_bounded_queue<int> iq1;)"}
{"predict": "(16, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(bool), cudaMemcpyDeviceToHost);)", "label": "(22, CUDA, cudaFree(res_dev);); (15, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(bool), cudaMemcpyDeviceToHost);); (11, CUDA, cudaMalloc(&res_dev, sizeof(bool));)"}
{"predict": "(28, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(27, OpenMP, auto start_clock = omp_get_wtime();); (44, OpenMP, auto stop_clock = omp_get_wtime();)"}
{"predict": "(13, OpenMP, #pragma omp parallel for)", "label": "(4, OpenMP, #pragma omp target teams distribute simd firstprivate(g, g1, sivar))"}
{"predict": "(16, CUDA, destination.sync_device();) (19, CUDA, convert_bf16_f32<<< dim3(1,1), dim3(N, 1) >>>(destination.device_data(), source.device_data(), N);) (21, CUDA, destination.sync_host();) (38, CUDA, destination.sync_device();) (41, CUDA, convert_and_pack_bf16<<< dim3(1,1), dim3(N, 1) >>>(destination.device_data(), source.device_data(), N);) (42, CUDA, ASSERT_EQ(cudaGetLastError(), cudaSuccess) << \"Kernel launch error.\"); (44, CUDA, destination.sync_host();)", "label": "(19, CUDA, ASSERT_EQ(cudaGetLastError(), cudaSuccess) << \"Kernel launch error.\";)"}
{"predict": "(32, OpenCL, format.image_channel_order = CL_RGBA;)\n(33, OpenCL, format.image_channel_data_type = CL_UNORM_INT8;)\n(34, OpenCL, desc.image_type = CL_MEM_OBJECT_IMAGE2D;)\n(35, OpenCL, desc.image_width = w;)\n(36, OpenCL, desc.image_height = h;)\n(37, OpenCL, desc.image_row_pitch = 0;)\n(40, OpenCL, OCL_CREATE_IMAGE(buf[0], 0, &format, &desc, NULL);)\n(41, OpenCL, OCL_CREATE_IMAGE(buf[1], 0, &format, &desc, NULL);)\n(43, OpenCL, OCL_MAP_BUFFER_GTT(0);)\n(44, OpenCL, OCL_MAP_BUFFER_GTT(1);)\n(47, OpenCL, for (uint32_t j = 0; j < h; ++j) { for (uint32_t i = 0; i < w*4; i++) { src[j * w * 4 + i] = i; dst[j * w * 4 + i] = 0; } })\n(49, OpenCL, OCL_UNMAP_BUFFER_GTT(0);)\n(50, OpenCL, OCL_UNMAP_BUFFER_GTT(1);)\n(54, OpenCL, OCL_SET_ARG(0, sizeof(cl_mem), &buf[0]);)\n(55, OpenCL, OCL_SET_ARG(1, sizeof(cl_mem), &buf[1]);)\n(56, OpenCL, OCL_SET_ARG(2, sizeof(unsigned int), &d);)\n(57, OpenCL, globals[0] = w;)\n(58, OpenCL, globals[1] = h;)\n(60, OpenCL, OCL_CALL (clEnqueueNDRangeKernel, queue, kernel, 2, NULL, globals, NULL, 0, NULL, NULL);)\n(63, OpenCL, OCL_MAP_BUFFER_GTT(0);)\n(64, OpenCL, OCL_MAP_BUFFER_GTT(1);)\n(67, OpenCL, for (uint32_t j = 0; j < h; ++j) { for (uint32_t i = 0; i < w*", "label": "(23, OpenCL, kernel = clCreateKernel(program, \"simplemov\", &status);); (20, OpenCL, OCL_ASSERT(clBuildProgram(program, 1, &device, NULL, NULL, NULL) == CL_SUCCESS);)"}
{"predict": "(5, CUDA, kernelLauncher<<<gridSize, blockSize>>>(d_out, d_temp, W, H, bc);)", "label": "(2, CUDA, cudaGraphicsMapResources(1, &cuda_pbo_resource, 0);); (7, CUDA, cudaGraphicsUnmapResources(1, &cuda_pbo_resource, 0);)"}
{"predict": "(8, OpenMP, #pragma omp parallel \\\nprivate(threadCount))", "label": "(3, OpenMP, #pragma omp target map(tofrom: threadCount)); (4, OpenMP, #pragma omp metadirective)"}
{"predict": "(3, MPI, int rank; MPI_Comm_rank(MPI_COMM_WORLD, &rank);) (7, MPI, MFU_LOG(MFU_LOG_INFO, \"%s\", msg);) (13, MPI, MFU_LOG(MFU_LOG_INFO, \"Sync completed in %.3lf seconds.\", (end - start));)", "label": "(5, MPI, MPI_Barrier(MPI_COMM_WORLD);); (5, MPI, double start = MPI_Wtime();); (12, MPI, double end = MPI_Wtime();); (0, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(14, MPI, MPI_Send(&send_value, 1, MPI_INT, (rank + 1) % size, 4711, MPI_COMM_WORLD, &send_request);)", "label": "(9, MPI, MPI_Init(&argc, &argv);); (18, MPI, MPI_Finalize();); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (31, MPI, MPI_Wait(&recv_request, &status);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (17, MPI, MPI_Isend(&send_value, 1, MPI_INT, (rank + 1) % size, 4711, MPI_COMM_WORLD, &send_request);); (27, MPI, MPI_Wait(&send_request, &status);)"}
{"predict": "(13, OpenCL, cl_event retEvent;); (14, OpenCL, retVal = callOneWorkItemNDRKernel(nullptr, 0, &retEvent);); (17, OpenCL, cl_int submittedClbExecStatus = -1;); (18, OpenCL, cl_int runningClbExecStatus = -1;); (19, OpenCL, cl_int completeClbExecStatus = -1;); (20, OpenCL, retVal = clSetEventCallback(retEvent, CL_SUBMITTED, HelperClb::setClbStatus, &submittedClbExecStatus);); (21, OpenCL, retVal = clSetEventCallback(retEvent, CL_RUNNING, HelperClb::setClbStatus, &runningClbExecStatus);); (22, OpenCL, retVal = clSetEventCallback(retEvent, CL_COMPLETE, HelperClb::setClbStatus, &completeClbExecStatus);); (27, OpenCL, auto result = UserEvent::waitForEvents(1, &retEvent););", "label": "(33, OpenCL, clReleaseEvent(retEvent);)"}
{"predict": "(7, CUDA, ensemble = new flamegpu::CUDAEnsemble(model, 2, argv);)", "label": "(16, CUDA, EXPECT_NO_THROW(ensemble = new flamegpu::CUDAEnsemble(model, sizeof(argv) / sizeof(char*), argv));); (6, CUDA, EXPECT_NO_THROW(ensemble = new flamegpu::CUDAEnsemble(model, 0, nullptr));)"}
{"predict": "(6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wSize);) (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wRank);)", "label": "(2, MPI, MPI_Comm cComm = MPI_Comm_f2c(*comm);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wSize);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wRank);); (5, MPI, MPI_Comm_rank(cComm, &cRank);); (3, MPI, MPI_Comm_size(cComm, &cSize);)"}
{"predict": "ldb=%d ldc=%d alpha=%f beta=%f\\n\", params.lda,\n122:            params ldb, params ldc, params.alpha, params.beta);\n123: \n124:     for (int i = 0; i < opts.N; i++) {\n125:       if (opts.test_method == tmBatched) {\n126:         status1 = cublasDgemmBatched(handle, CUBLAS_OP_N, CUBLAS_OP_N,\n127:                                     matrixM, matrixN, matrixK, &params.alpha,\n128:                                     devPtrA_dev[i], matrixM, devPtrB_dev[i],\n129:                                     matrixK, &params.beta, devPtrC_dev[i],\n130:                                     matrixM, opts.N, streamArray[i]);\n131:       } else {\n132:         status1 = cublasDgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, matrixM,\n133:                               matrixN, matrixK, &params.alpha, devPtrA[i],\n134:                               matrixM, devPtrB[i], matrixK, &params.beta,\n135:                               devPtrC[i], matrixM, streamArray[i]);\n136:       }\n137:       status2 = cublasGetErrorString(status1, &err);\n138:       if (status1 != CUBLAS_STATUS_SUCCESS) {\n139:         printf(\"cublasDgemm returned %s\\n\", err);\n140:         errors++;\n141:       }\n142:     }\n143: \n144:     numTests *= 2;\n145:   }\n146: \n147:   printf(\"errors: %d\\n\", errors);\n148: \n149:   for (int i = 0; i < opts.N; i++) {\n150:     if (opts.test_method == tmBatched) {\n151:       cudaFree(devPtrA_dev[i]);\n152:       cudaFree(devPtrB_dev[i]);\n153:       cudaFree(devPtrC_dev[i]);\n154:     }\n155:     cudaFree(devPtrA[i]);\n156:     cudaFree(devPtrB[i]);\n157:     cuda", "label": "(193, CUDA, cudaError_t cudaStatus = cudaDeviceSynchronize();); (103, CUDA, cudaError_t cudaErr = cudaStreamCreate(&streamArray[i]);); (163, CUDA, cudaError_t cudaStatus = cudaGetLastError();)"}
{"predict": "(17, OpenMP, #pragma omp parallel for private(li))", "label": "(19, MPI, *ierr = MPI_Info_get_valuelen( (MPI_Info)(*v1), p2, v3, &l4 );)"}
{"predict": "9:         ADIOI_Type_create_hindexed_x(nprocs_recv, recv_size, offset_list, MPI_BYTE,\n120:                                      recv_types + j);\n121:     }\n122: \n123:     ADIOI_Free(recv_types);\n124: \n125:     if (nprocs_send) {\n126:         requests = (MPI_Request *) ADIOI_Malloc(nprocs_send * sizeof(MPI_Request));\n127:         send_req = requests;\n128:         ADIOI_Send(buf, send_size, MPI_BYTE, off, fd->comm, MPI_COMM_WORLD, send_req++);\n129:     }\n130: \n131:     if (nprocs_recv) {\n132:         statuses = (MPI_Status *) ADIOI_Malloc(nprocs_recv * sizeof(MPI_Status));\n133:         ADIOI_Recv(send_buf, recv_size, MPI_BYTE, off, fd->comm, MPI_COMM_WORLD, statuses);\n134:     }\n135: \n136:     ADIOI_Free(statuses);\n137: \n138:     if (nprocs_send) {\n139:         ADIOI_Waitall(nprocs_send, requests, MPI_STATUSES_IGNORED);\n140:     }\n141: \n142:     ADIOI_Free(requests);\n143: }", "label": "(65, MPI, MPI_Type_commit(recv_types + j);); (30, MPI, MPI_Alltoall(recv_size, 1, MPI_COUNT, send_size, 1, MPI_COUNT, fd->comm);)"}
{"predict": "(8, OpenMP, #pragma omp parallel for)", "label": "(3, CUDA, cudaFree( 0 );)"}
{"predict": "(19, CUDA, CHECK_CUDA(cudaMemcpyAsync(dx, hx, xSize * sizeof(float), cudaMemcpyHostToDevice, stream));)", "label": "(52, CUDA, CHECK_CUDA(cudaFreeHost(hIndices));); (15, CUDA, CHECK_CUDA(cudaMallocHost(&hIndicesRef, ySize * sizeof(uint32_t)));); (19, CUDA, CHECK_CUDA(cudaMalloc(&dx, xSize * sizeof(float)));); (50, CUDA, CHECK_CUDA(cudaFreeHost(hIndicesRef));); (8, CUDA, CHECK_CUDA(cudaMallocHost(&hx, xSize * sizeof(float)));); (-3, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (44, CUDA, CHECK_CUDA(cudaFreeHost(hx));); (42, CUDA, CHECK_CUDA(cudaFree(dIndices));); (40, CUDA, CHECK_CUDA(cudaFree(dx));); (4, CUDA, CHECK_CUDA(cudaMallocHost(&hyRef, ySize * sizeof(float)));); (5, CUDA, CHECK_CUDA(cudaMallocHost(&hIndices, ySize * sizeof(uint32_t)));); (40, CUDA, CHECK_CUDA(cudaFreeHost(hyRef));); (11, CUDA, CHECK_CUDA(cudaMalloc(&dIndices, ySize * sizeof(uint32_t)));)"}
{"predict": "(3, CUDA, cudaMallocManaged(&m_histoArray, size);)", "label": "(4, CUDA, cudaExtent extent = make_cudaExtent(4, 5, 6);)"}
{"predict": "7:                 << \" Avg time (s): \" << avgtime\n128:                 << \" Iterations: \" << iterations << std::endl;\n129:   }\n130: }", "label": "(39, SYCL, sycl::accessor B(d_B, h, sycl::read_only);); (37, SYCL, sycl::accessor A(d_A, h);); (38, SYCL, sycl::accessor C(d_C, h, sycl::read_only);); (81, SYCL, prk::SYCL::print_exception_details(e);)"}
{"predict": "(17, CUDA, self->stream = cudaStreamCreate();)", "label": "(2, CUDA, LogDebug(LOG_PY_UTILS \"PyCudaMemory_New()\\n\");)"}
{"predict": "(32, MPI, MPI_Barrier(comm);)", "label": "(3, MPI, MPI_Init(nullptr, nullptr);); (100, MPI, MPI_Finalize();); (3, MPI, dray::dray::mpi_comm(MPI_Comm_c2f(comm));)"}
{"predict": "(35, SYCL, sycl::queue q; syclBLAS::gemv(q, handle, trans, N_b, M_b, alpha, B.device_data(), N_b, A.device_data(), 1, beta, C.device_data(), 1).wait();)", "label": "(9, SYCL, sycl::queue handle = getSYCLDefaultDeviceDefaultQueue();)"}
{"predict": "(26, CUDA, cudaDeviceSynchronize();)", "label": "(24, OpenMP, double *dbl_min = (double*) omp_target_alloc(sizeof(double)*1, omp_get_default_device() );); (22, OpenMP, int *max = (int*) omp_target_alloc(sizeof(int)*1, omp_get_default_device() );); (23, OpenMP, double *dbl_max = (double*) omp_target_alloc(sizeof(double)*1, omp_get_default_device() );); (33, OpenMP, #pragma omp atomic compare); (6, OpenMP, #pragma omp target teams distribute parallel for); (38, OpenMP, #pragma omp atomic compare hint(ompx_fast_fp_atomics)); (50, OpenMP, #pragma omp target teams distribute parallel for  map(tofrom:min2,max2))"}
{"predict": "_d.host_view() -= tensor_ref_d.host_view();\n143:     result.reference_check = cutlass::reference::host::TensorNorm(tensor_d.host_view()) < 1e-5;\n144:   }\n145: \n146:   return result;\n147: }", "label": "(202, CUDA, result.error = cudaEventRecord(events[0]);); (223, CUDA, std::cerr << \"cudaEventSynchronize() failed: \" << cudaGetErrorString(result.error) << std::endl;); (230, CUDA, std::cerr << \"cudaEventElapsed() failed: \" << cudaGetErrorString(result.error) << std::endl;); (201, CUDA, std::cerr << \"cudaEventRecord() failed: \" << cudaGetErrorString(result.error) << std::endl;); (211, CUDA, result.error = cudaEventRecord(events[1]);); (237, CUDA, (void)cudaEventDestroy(event);); (190, CUDA, std::cerr << \"cudaEventCreate() failed: \" << cudaGetErrorString(result.error) << std::endl;); (223, CUDA, result.error = cudaEventElapsedTime(&runtime_ms, events[0], events[1]);); (214, CUDA, result.error = cudaEventSynchronize(events[1]);); (185, CUDA, result.error = cudaEventCreate(&event);)"}
{"predict": "(12, MPI, MPI_Isend(&send_value, 10, MPI_INT, (rank + 1) % size, 4711, MPI_COMM_WORLD, &send_request);)", "label": "(9, MPI, MPI_Init(&argc, &argv);); (25, MPI, MPI_Finalize();); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (30, MPI, MPI_Isend(&send_value, 9, MPI_INT, (rank + 1) % size, 4711, MPI_COMM_WORLD, &send_request);); (34, MPI, MPI_Wait(&recv_request, &status);); (32, MPI, MPI_Wait(&send_request, &status);)"}
{"predict": "is larger than the buffer size */\n115: #ifdef IO_DEBUG\n116:                     printf(\"[%d/%d] nc mem c file (1) writing loc = %Ld sz = %Ld\\n\",\n117:                            rank, nprocs, off, flat_buf->blocklens[i]);\n118: #endif\n119: #ifdef ADIOI_MPE_LOGGING\n120:                     MPE_Log_event(ADIOI_MPE_write_a, 0, NULL);\n121: #endif\n122:                     err = write(fd->fd_sys, (char *) buf + (off_t) flat_buf->indices[i] * etype_size,\n123:                                 flat_buf->blocklens[i]);\n124: #ifdef ADIOI_MPE_LOGGING\n125:                     MPE_Log_event(ADIOI_MPE_write_b, 0, NULL);\n126: #endif\n127:                     if (err == -1)\n128:                         err_flag = 1;\n129:                 } else {\n130:                     /* copy data from buf into combine buffer */\n131:                     memcpy(combine_buf_ptr, (char *) buf + (off_t) flat_buf->indices[i] * etype_size,\n132:                            flat_buf->blocklens[i]);\n133:                     combine_buf_ptr += flat_buf->blocklens[i];\n134:                     combine_buf_remain -= flat_buf->blocklens[i];\n135:                 }\n136:             }\n137:             /* update our file pointer */\n138:             if (file_ptr_type == ADIO_EXPLICIT_OFFSET) {\n139:                 off += flat_buf->blocklens[0];\n140:             } else {\n141:                 fd->fp_ind += flat_buf->blocklens[0];\n142:             }\n143:         }\n144:         /* if there is data left in the buffer, write it out */\n145:         if (combine_buf != combine_buf_ptr) {\n146: #ifdef IO_DEBUG\n147:             printf(\"[%d/%d] nc mem c file (2) writing loc = %Ld sz = %Ld\\n\",\n148:                    rank, nprocs, off, fd", "label": "(44, MPI, MPI_Type_size_x(fd->filetype, &filetype_size);); (52, MPI, MPI_Comm_rank(fd->comm, &rank);); (52, MPI, MPI_Comm_size(fd->comm, &nprocs);); (56, MPI, MPI_Type_get_extent(datatype, &lb, &buftype_extent);); (54, MPI, MPI_Type_size_x(datatype, &buftype_size);)"}
{"predict": "*radius) {\n109:       printf(\"ERROR: Grid size %d too small for stencil radius %d\\n\",\n110:              (int) size, radius);\n111:       error = 1;\n112:       goto ENDOFTESTS;\n113:     }\n114: \n115:     /* compute the stencil size */\n116:     stencil_size = (radius+1)*(radius+1);\n117: \n118:     /* allocate arrays */\n119:     matrix_space = size2 * sizeof(double);\n120:     vector_space = size * sizeof(double);\n121:     index_space = size2 * sizeof(s64Int);\n122:     colIndex = (s64Int *) malloc(index_space);\n123:     vector = (double *) malloc(vector_space);\n124:     matrix = (double *) malloc(matrix_space);\n125:     result = (double *) malloc(vector_space);\n126: \n127:     /* check that memory has been allocated */\n128:     if (!colIndex || !vector || !matrix || !result) {\n129:       printf(\"ERROR: Could not allocate enough memory.\\n\");\n130:       error = 1;\n131:       goto ENDOFTESTS;\n132:     }\n133: \n134:     /* initialize the matrix and vector */\n135:     init_matrix(matrix, matrix_space, colIndex, size2, stencil_size, radius, 0.0, 1.0);\n136:     init_vector(vector, vector_space, size, 0.0, 1.0);\n137: \n138:     /* print the stencil parameters */\n139:     printf(\"Number of iterations = %d\\n\", iterations);\n140:     printf(\"Grid size = %d\\n\", size);\n141:     printf(\"Log of grid size = %d\\n\", lsize);\n142:     printf(\"Stencil radius = %d\\n\", radius);\n143:     printf(\"Stencil size = %d\\n\", stencil_size);\n144:     printf(\"Sparsity = %f\\n\", sparsity);\n145:   }\n146: \n147:   MPI_Bcast(&", "label": "(146, MPI, MPI_Bcast(&size2,      1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (146, MPI, MPI_Bcast(&radius,     1, MPI_INT,           root, MPI_COMM_WORLD);); (303, MPI, MPI_Finalize();); (141, MPI, MPI_Bcast(&lsize2,     1, MPI_INT,           root, MPI_COMM_WORLD);); (139, MPI, MPI_Bcast(&lsize,      1, MPI_INT,           root, MPI_COMM_WORLD);); (140, MPI, MPI_Bcast(&size,       1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (277, MPI, MPI_Reduce(&vector_sum, &check_sum, 1, MPI_DOUBLE, MPI_SUM, root, MPI_COMM_WORLD);); (44, MPI, MPI_Init(&argc,&argv);); (230, MPI, MPI_Barrier(MPI_COMM_WORLD);); (44, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (138, MPI, MPI_Bcast(&iterations, 1, MPI_INT,           root, MPI_COMM_WORLD);); (41, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "105: \t\t\t\t\t\tCASE(HEX);\n106: \t\t\t\t\t\tdefault: break;\n107: \t\t\t\t\t}\n108: #ifdef _OPENMP\n109: #undef CASE\n110: #endif\n111: \t\t\t\t}\n112: \t\t\t}\n113: \t\t}\n114: \t}\n115: \n116: \t// Call the crypt function.\n117: \tif (curdat.dynamic_use_sse == 0) {\n118: \t\tif (curdat.store_keys_normal_but_precompute_hash_to_output2_base16_to_input1) {\n119: #ifdef _OPENMP\n120: #define CASE(H) case MGF__##H: DynamicFunc__##H##_crypt_input2_overwrite_input1(0,m_count,0); break\n121: #else\n122: #define CASE(H) case MGF__##H: DynamicFunc__##H##_crypt_input2_overwrite_input1(); break\n123: #endif\n124: \t\t\tswitch(curdat.store_keys_normal_but_precompute_hash_to_output2_base16_type)\n125: \t\t\t{\n126: \t\t\t\tCASE(MD5);\n127: \t\t\t\tCASE(MD4);\n128: \t\t\t\tCASE(SHA1);\n129: \t\t\t\tCASE(SHA224);\n130: \t\t\t\tCASE(SHA256);\n131: \t\t\t\tCASE(SHA384);\n132: \t\t\t\tCASE(SHA512);\n133: \t\t\t\tCASE(GOST);\n134: \t\t\t\tCASE(WHIRLPOOL);\n135: \t\t\t\tCASE(Tiger);\n136: \t\t\t\tCASE(RIPEMD128);\n137: \t\t\t\tCASE(RIPEMD160);\n138: \t\t\t\tCASE(RIPEMD256);\n139: \t\t\t\tCASE(RIPEMD320);\n140: \t\t\t\tCASE(HAVAL128_3);\n141: \t\t\t\tCASE(HAVAL128_4);\n142: \t\t\t\tCASE(HAVAL128_5);\n143: \t\t\t\tCASE(HAVAL160_3);\n144:", "label": "(251, OpenMP, #pragma omp parallel for shared(curdat, inc, m_count))"}
{"predict": "must be a strictly positive integer value}}\n70:   for (i = 0; i < 16; ++i)\n71:     ;\n72: // expected-error@+1 {{argument to 'collapse' clause must be a strictly positive integer value}}\n73:   for (i = 0; i < 16; ++i)\n74:     ;\n75: // expected-error@+1 {{argument to 'collapse' clause must be a strictly positive integer value}}\n76:   for (i = 0; i < 16; ++i)\n77:     ;\n78: // expected-error@+1 {{argument to 'collapse' clause must be a strictly positive integer value}}\n79:   for (i = 0; i < 16; ++i)\n80:     ;\n81: // expected-error@+1 {{argument to 'collapse' clause must be a strictly positive integer value}}\n82:   for (i = 0; i < 16; ++i)\n83:     ;\n84: // expected-error@+1 {{argument to 'collapse' clause must be a strictly positive integer value}}\n85:   for (i = 0; i < 16; ++i)\n86:     ;\n87: // expected-error@+1 {{argument to 'collapse' clause must be a strictly positive integer value}}\n88:   for (i = 0; i < 16; ++i)\n89:     ;\n90: // expected-error@+1 {{argument to 'collapse' clause must be a strictly positive integer value}}\n91:   for (i = 0; i < 16; ++i)\n92:     ;\n93: // expected-error@+1 {{argument to 'collapse' clause must be a strictly positive integer value}}\n94:   for (i = 0; i < 16; ++i)\n95:     ;\n96: // expected-error@+1 {{argument to 'collapse' clause must be a strictly positive integer value}}\n97:   for (i = 0; i < 16; ++i)\n98:     ;\n99: // expected-error@+1 {{argument to 'collapse' clause must be a strictly positive integer value}}\n100:  for (i = 0; i <", "label": "(21, OpenMP, #pragma omp target parallel for simd collapse(, )); (54, OpenMP, #pragma omp target parallel for simd collapse(4, , 4)); (34, OpenMP, #pragma omp target parallel for simd collapse(4,); (79, OpenMP, #pragma omp target parallel for simd collapse(0)); (46, OpenMP, #pragma omp target parallel for simd collapse(4 4)); (85, OpenMP, #pragma omp target parallel for simd collapse(2) firstprivate(i)); (72, OpenMP, #pragma omp target parallel for simd collapse(-5)); (6, OpenMP, #pragma omp target parallel for simd collapse()); (-3, OpenMP, #pragma omp target parallel for simd collapse); (57, OpenMP, #pragma omp target parallel for simd collapse(4, 8)); (7, OpenMP, #pragma omp target parallel for simd collapse(,); (63, OpenMP, #pragma omp target parallel for simd collapse(foo())); (74, OpenMP, #pragma omp target parallel for simd collapse(5 - 5)); (57, OpenMP, #pragma omp target parallel for simd collapse(2.5)); (17, OpenMP, #pragma omp target parallel for simd collapse(4); (11, OpenMP, #pragma omp target parallel for simd collapse 4)); (-7, OpenMP, #pragma omp target parallel for simd collapse(); (24, OpenMP, #pragma omp target parallel for simd collapse(4, )); (27, OpenMP, #pragma omp target parallel for simd collapse(4))"}
{"predict": "(4, OpenCL, if (!EasyCL::isOpenCLAvailable()) { return; }); (7, OpenCL, cl_int retVal = CL_SUCCESS;); (11, OpenCL, cl->storeKernel(\"kernela\", kernel);); (14, OpenCL, cl->storeKernel(\"kernelb\", kernel);); (17, OpenCL, delete kernel;); (18, OpenCL, delete cl;);", "label": "(7, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)"}
{"predict": "}\n130: \t\t\t}\n131: \t\t}\n132: \t}\n133: \telse\n134: \t{\n135: \t\tbTestResult = false;\n136: \t}\n137: \n138: \tcleanup<T, T>(h_in_signal, d_in_signal, h_output_signal, d_output_signal, outPlace);\n139: \treturn bTestResult;\n140: }", "label": "(83, CUDA, cudaDeviceSynchronize();); (20, CUDA, checkCudaErrors(cudaMalloc((void **)&d_output_signal, mem_size));); (73, CUDA, checkCudaErrors(cufftExecZ2Z(plan, (cufftDoubleComplex *)d_in_signal, (cufftDoubleComplex *)d_output_signal, direction));); (69, CUDA, checkCudaErrors(cufftExecC2C(plan, (cufftComplex *)d_in_signal, (cufftComplex *)d_output_signal, direction));); (155, CUDA, checkCudaErrors(cufftDestroy(plan));); (9, CUDA, checkCudaErrors(cudaMalloc((void **)&d_in_signal, mem_size));); (163, CUDA, cudaDeviceReset();)"}
{"predict": "(28, CUDA, const int block_size = 128; const int grid_size = (N + block_size - 1) / block_size; add<<<grid_size, block_size>>>(d_x, d_y, d_z, N);)", "label": "(48, CUDA, CHECK(cudaFree(d_y));); (26, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice));); (47, CUDA, CHECK(cudaFree(d_z));); (35, CUDA, CHECK(cudaMemcpy(h_z, d_z, M, cudaMemcpyDeviceToHost));); (18, CUDA, CHECK(cudaMalloc((void**)&d_x, M));); (18, CUDA, CHECK(cudaMalloc((void**)&d_y, M));); (41, CUDA, CHECK(cudaFree(d_x));); (17, CUDA, CHECK(cudaMalloc((void**)&d_z, M));); (26, CUDA, CHECK(cudaGetLastError());); (26, CUDA, CHECK(cudaDeviceSynchronize());); (18, CUDA, CHECK(cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice));)"}
{"predict": "(10, MPI, MPI_Init(&argc, &argv);) (44, MPI, MPI_Finalize();)", "label": "(5, MPI, MPI_Get_library_version(version, &versionlen);)"}
{"predict": "(6, OpenMP, #pragma omp parallel for private(i) reduction(+:count))", "label": "(6, OpenMP, #pragma omp for reduction(+:count) lastprivate(obj)); (9, OpenMP, #pragma omp cancel for); (2, OpenMP, #pragma omp parallel private(i) num_threads(1))"}
{"predict": "(33, CUDA, cudaMemcpy(device_buffer, input.data(), size_bytes, cudaMemcpyHostToDevice);)", "label": "(31, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&device_buffer, size_bytes));); (32, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(device_buffer, input.data(), size_bytes, cudaMemcpyHostToDevice));); (41, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(device_buffer));); (35, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (33, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (37, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(output.data(), device_buffer, size_bytes, cudaMemcpyDeviceToHost));)"}
{"predict": "= MIN(min_st_offset, all_st_end_offsets[i * 2]);\n1113:             max_end_offset = MAX(max_end_offset, all_st_end_offsets[i * 2 + 1]);\n114:         }\n115: \n116:         /* check if we need to use the collective buffering algorithm */\n117:         if (interleave_count == 0) {\n118:             cb_enable = ADIOI_HINT_DISABLE;\n119:         }\n120: \n121:         /* free the array of start/end pairs */\n122:         ADIOI_Free(all_st_end_offsets);\n123:     }\n124: \n125:     /* calculate the size of the buffer for the collective buffering\n126:      * algorithm */\n127:     if (cb_enable != ADIOI_HINT_DISABLE) {\n128:         bufsize = max_end_offset - min_st_offset;\n129:     } else {\n130:         bufsize = count * datatype->extent;\n131:     }\n132: \n133:     /* create the buffer for the collective buffering algorithm */\n134:     if (cb_enable != ADIOI_HINT_DISABLE) {\n135:         cb_buf = (char *)ADIOI_Malloc(bufsize);\n136:     }\n137: \n138:     /* calculate the displacement and datatype for the collective\n139:      * buffering algorithm */\n140:     if (cb_enable != ADIOI_HINT_DISABLE) {\n141:         agg_disp = min_st_offset;\n142:         agg_dtype = datatype;\n143:     }\n144: \n145:     /* calculate the number of aggregators */\n146:     if (cb_enable != ADIOI_HINT_DISABLE) {\n147:         if (nprocs_for_coll > 0) {\n148:             aggs_client_count = nprocs_for_coll;\n149:         } else {\n150:             aggs_client_count = nprocs;\n151:         }\n152:     }\n153: \n154:     /* allocate memory for the arrays used in the collective buffering\n155:      * algorithm */\n156:     if (cb_enable != ADIOI_HINT_DISABLE", "label": "(101, MPI, MPI_Allgather(st_end_offset, 2, ADIO_OFFSET, all_st_end_offsets, 2, ADIO_OFFSET, fd->comm);); (66, MPI, MPI_Comm_rank(fd->comm, &myrank);); (522, MPI, MPI_Type_free(&client_comm_dtype_arr[i]);); (582, MPI, MPI_Type_free(&fd->file_realm_types[0]);); (62, MPI, MPI_Comm_size(fd->comm, &nprocs);); (584, MPI, MPI_Type_free(&fd->file_realm_types[i]);); (150, MPI, MPI_Type_get_extent(datatype, &lb, &extent);); (302, MPI, MPI_Type_size_x(agg_comm_dtype_arr[i], &size);); (296, MPI, MPI_Type_free(&agg_dtype);)"}
{"predict": "(10, MPI, MTest_Init(&argc, &argv);)", "label": "(13, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(6, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, 0, 0, win);) (12, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, 0, 0, win);) (14, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, 0, 0, win);) (16, MPI, MPI_Win_unlock(0, win);) (17, MPI, MPI_Win_unlock(0, win);)", "label": "(12, MPI, MPI_Win_unlock(0, win);); (9, MPI, MPI_Accumulate(one, nlevels, MPI_INT, 0, 0, 1, acc_type, MPI_SUM, win);); (9, MPI, MPI_Get(get_array, nlevels, MPI_INT, 0, 0, 1, get_type, win);); (6, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, 0, 0, win);)"}
{"predict": "(39, OpenMP, nstream_time = omp_get_wtime();)", "label": "(31, OpenMP, std::cout << \"OpenMP default device = \" << omp_get_default_device() << std::endl;)"}
{"predict": "(11, CUDA, half_type_##expr##_kernel<T><<<1, 1, 0, 0>>>(a, b, res_dev);) (14, CUDA, type_half_##expr##_kernel<T><<<1, 1, 0, 0>>>(a, b, res_dev);)", "label": "(18, CUDA, cudaFree(res_dev);); (8, CUDA, cudaMalloc(&res_dev, sizeof(float));); (10, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(28, OpenMP, #pragma omp parallel for reduction(+:serial_time))\n(32, OpenMP, #pragma omp parallel for reduction(+:reduce_time))\n(36, OpenMP, #pragma omp parallel for reduction(+:graph_time))", "label": "(29, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (37, TBB, reduce_time = (tbb::tick_count::now() - t0).seconds();); (43, TBB, graph_time = (tbb::tick_count::now() - t0).seconds();); (28, TBB, serial_time = (tbb::tick_count::now() - t0).seconds();)"}
{"predict": "(13, CUDA, cudaFree(dev_ptr);)", "label": "(6, CUDA, cudaMalloc((void**) &raw_ptr, N * sizeof(int));); (11, CUDA, thrust::fill(dev_ptr, dev_ptr + N, (int) 0);); (7, CUDA, thrust::device_ptr<int> dev_ptr = thrust::device_pointer_cast(raw_ptr);); (15, CUDA, cudaFree(raw_ptr);)"}
{"predict": "(28, CUDA, pminh.x = min(pminh.x, px); pminh.y = min(pminh.y, py); pminh.z = min(pminh.z, pz); pmaxh.x = max(pmaxh.x, px); pmaxh.y = max(pmaxh.y, py); pmaxh.z = max(pmaxh.z, pz);)", "label": "(19, CUDA, cudaMemcpy(poscellh,poscell+pini,sizeof(float4)*np,cudaMemcpyDeviceToHost);); (19, CUDA, cudaMemcpy(checkh,check+pini,sizeof(byte)*np,cudaMemcpyDeviceToHost);)"}
{"predict": "(15, MPI, MPI_RESULT mpi_errno = reqFn( vc, sreq, complete );)", "label": "(29, MPI, Mpi.SignalProgress();)"}
{"predict": "(5, OpenMP, #pragma omp parallel for private(idx, ctx, i, len, cp, tmp, tmpBuf) schedule(static))", "label": "(3, OpenMP, #pragma omp parallel for default(none) private(idx) shared(count, sapH_cur_salt, saved_plain, crypt_key))"}
{"predict": "(19, MPI, MPI_Bcast(&flag, 1, MPI_INT, 0, MPI_COMM_WORLD);)", "label": "(23, MPI, rc = MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (32, MPI, MPI_Finalize();); (6, MPI, rc = MPI_Init(NULL, NULL);)"}
{"predict": "(5, CUDA, cudaStream_t stream = *(cudaStream_t *) stream_p;); (16, CUDA, cerr = cudaMemcpyAsync((char *) outbuf + type->true_lb, inbuf, count * type->size, cudaMemcpyDefault, stream);); (27, CUDA, for (uintptr_t i = 0; i < actual_iov_len; i++) { sbuf += iov[i].iov_len; }); (32, CUDA, offset += actual_iov_len;); (34, CUDA, unsigned int n_threads; unsigned int n_blocks_x, n_blocks_y, n_blocks_z; rc = get_thread_block_dims(count, type, &n_threads, &n_blocks_x, &n_blocks_y, &n_blocks_z);); (36, CUDA, int cur_device; YAKSURI_CUDAI_CUDA_ERR_CHKANDJUMP(cerr, rc, fn_fail);); (38, CUDA, YAKSURI_CUDAI_CUDA_ERR_CHKANDJUMP(cerr, rc, fn_fail);); (46, CUDA, cuda_type->unpack(inbuf, outbuf, count, op, cuda_type->md, n_threads, n_blocks_x, n_blocks_y, n_blocks_z, stream);); (48, CUDA, YAKSURI_CUDAI_CUDA_ERR_CHKANDJUMP(cerr, rc, fn_fail););", "label": "(9, CUDA, uintptr_t iov_unpack_threshold = yaksuri_cudai_get_iov_unpack_threshold(info);); (29, CUDA, cudaMemcpyAsync(iov[i].iov_base, sbuf, iov[i].iov_len, cudaMemcpyDefault, stream);); (16, CUDA, YAKSURI_CUDAI_CUDA_ERR_CHKANDJUMP(cerr, rc, fn_fail);); (52, CUDA, cerr = cudaSetDevice(cur_device);); (42, CUDA, cerr = cudaGetDevice(&cur_device);); (32, CUDA, rc = yaksuri_cudai_md_alloc(type);); (43, CUDA, cerr = cudaSetDevice(target);)"}
{"predict": "(18, OpenMP, #pragma omp parallel for reduction(+:a[:N]))", "label": "(15, OpenMP, #pragma omp target parallel for)"}
{"predict": "(17, CUDA, __syncthreads();)", "label": "(4, CUDA, extern __shared__ float shared[];)"}
{"predict": "(12, OpenMP, #pragma omp parallel for)", "label": "(13, OpenMP, #pragma omp parallel for)"}
{"predict": "(16, TBB, tbb::parallel_for(tbb::blocked_range<int>(0, 2), [](const tbb::blocked_range<int>& range) { for (int p = range.begin(); p != range.end(); ++p) { generate_test<parallel_test, tbb::flow::tuple<float, double> >::do_test(); } }));", "label": "(31, TBB, generate_test<parallel_test, tbb::flow::tuple<float, double, int, double, double, long, int, float, long> >::do_test();); (27, TBB, generate_test<parallel_test, tbb::flow::tuple<float, int, double, float, long, float, long> >::do_test();); (23, TBB, generate_test<parallel_test, tbb::flow::tuple<double, double, int, int, short> >::do_test();); (15, TBB, generate_test<serial_test, tbb::flow::tuple<float, double, int, double, double, float, long, int, float, long> >::do_test();); (3, TBB, generate_test<serial_test, tbb::flow::tuple<float, double> >::do_test();); (4, TBB, generate_test<serial_test, tbb::flow::tuple<float, double, int, long> >::do_test();); (16, TBB, generate_test<parallel_test, tbb::flow::tuple<float, int, long> >::do_test();); (5, TBB, generate_test<serial_test, tbb::flow::tuple<double, double, int, long, int, short> >::do_test();); (7, TBB, generate_test<serial_test, tbb::flow::tuple<float, double, double, double, float, int, float, long> >::do_test();)"}
{"predict": "(17, MPI, MPI_Bcast(in_buf, elems * nproc, MPI_FLOAT, 0, comm, &retVal);)", "label": "(25, MPI, MPI_Isend(&out_buf[elems * i], elems, MPI_FLOAT, i, 0, comm, &reqs[i + nproc]);); (20, MPI, MPI_Irecv(&in_buf[elems * i], elems, MPI_FLOAT, i, 0, comm, &reqs[i]);); (11, MPI, MPI_Comm_size(comm, &nproc);); (9, MPI, MPI_Comm_rank(comm, &rank);); (24, MPI, MPI_Waitall(nproc * 2, reqs, MPI_STATUSES_IGNORE);)"}
{"predict": "(12, CUDA, cudaStream_t stream = 0;);", "label": "(22, CUDA, PyErr_SetString(PyExc_Exception, LOG_PY_UTILS \"cudaOverlay() failed to get input/output image pointers (should be cudaImage)\");); (32, CUDA, PYCUDA_ASSERT_NOGIL(cudaOverlay(input->base.ptr, input->width, input->height, output->base.ptr, output->width, output->height, output->format, x, y, stream));); (26, CUDA, PyErr_SetString(PyExc_Exception, LOG_PY_UTILS \"cudaOverlay() input and output image formats are different\");)"}
{"predict": "(12, CUDA, for(i = 0; i < spatial; i += threads){); (20, CUDA, if(id == 0){);", "label": "(3, CUDA, __shared__ float local[threads];)"}
{"predict": "(13, OpenMP, #pragma omp parallel for collapse(2) private(dx, dy, dz, r, ig2))", "label": "(33, OpenMP, #pragma omp atomic); (15, OpenMP, #pragma omp target teams distribute parallel for collapse(2) private(dx, dy, dz, r, ig2))"}
{"predict": "(30, OpenMP, #pragma omp parallel for collapse(3))\n(56, OpenMP, #pragma omp parallel for collapse(3))", "label": "(19, OpenMP, #pragma omp for private(k)); (37, OpenMP, #pragma omp for private ( k)); (66, OpenMP, #pragma omp for private (k))"}
{"predict": "(16, MPI, t1 = MPI_Wtime();)", "label": "(8, MPI, t1 = MPI_Wtime();); (6, MPI, MPI_Init(&argc, &argv);); (38, MPI, MPI_Finalize();); (31, MPI, tick = MPI_Wtick();); (5, MPI, t2 = MPI_Wtime();)"}
{"predict": "(36, OpenCL, extensions = calloc(1, len);)", "label": "(29, OpenCL, err = clGetDeviceInfo(device, CL_DEVICE_EXTENSIONS, 0, NULL, &len);); (42, OpenCL, err = clGetDeviceInfo(device, CL_DEVICE_EXTENSIONS, len, extensions, NULL);)"}
{"predict": "return 0;\n138: }", "label": "(2, MPI, MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided);); (136, MPI, MPI_Send(x.data(), chunk, MPI_DOUBLE, 0, 42, comm);); (2, MPI, MPI_Finalize();); (65, MPI, MPI_Allgather(&chunk, 1, MPI_INT, &domain[1], 1, MPI_INT, comm);)"}
{"predict": "rank != MPI_UNDEFINED) {\n77:             errs++;\n78:             printf(\"incorrect lo group rank: %d\\n\", rank);\n79:             fflush(stdout);\n80:         } else {\n81:             /* printf(\"lo in\\n\");FFLUSH; */\n82:             /* printf(\"lo out\\n\");FFLUSH; */\n83:         }\n84:     }\n85: \n86:     MPI_Comm_group(dup_comm_world, &world_group);\n87:     if (rank != world_rank) {\n88:         errs++;\n89:         printf(\"incorrect rank in world comm: %d\\n\", rank);\n90:     }\n91: \n92:     ranges[0][0] = (world_size - n);\n93:     ranges[0][1] = world_size - 1;\n94:     ranges[0][2] = 1;\n95: \n96: #ifdef DEBUG\n97:     printf(\"world rank = %d before range excl\\n\", world_rank);\n98:     FFLUSH;\n99: #endif\n100:     MPI_Group_range_excl(world_group, 1, ranges, &rev_group);\n101: #ifdef DEBUG\n102:     printf(\"world rank = %d after range excl\\n\", world_rank);\n103:     FFLUSH;\n104: #endif\n105:     MPI_Comm_create(world_comm, rev_group, &rev_comm);\n106: #ifdef DEBUG\n107:     printf(\"world rank = %d before group free\\n\", world_rank);\n108:     FFLUSH;\n109: #endif\n110:     MPI_Group_free(&rev_group);\n111: \n112: #ifdef DEBUG\n113:     printf(\"world rank = %d after group free\\n\", world_rank);\n114:     FFLUSH;\n115: #endif\n116: \n117:     if (world_rank >= (world_size - n)) {\n118:         if (rank == MPI_UNDEFINED) {\n119:             errs++;\n120:             printf(\"incorrect rev group rank: %d\\n\", rank);\n121:             fflush(stdout);\n122:         } else {\n123:", "label": "(172, MPI, MPI_Comm_split(dup_comm_world, color, key, &split_comm);); (120, MPI, MPI_Attr_put(lo_comm, key_1, (void *) (MPI_Aint) world_rank);); (111, MPI, MPI_Keyval_create(copy_fn, delete_fn, &key_1, &value);); (142, MPI, MPI_Abort(MPI_COMM_WORLD, 3005);); (118, MPI, MPI_Attr_put(lo_comm, key_3, (void *) 0);); (238, MPI, MPI_Comm_free(&split_comm);); (210, MPI, MPI_Comm_compare(world_comm, rev_comm, &result);); (28, MPI, MPI_Comm_create(dup_comm_world, world_group, &world_comm);); (28, MPI, MPI_Comm_rank(world_comm, &rank);); (2, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (114, MPI, MPI_Comm_dup(lo_comm, &dup_comm);); (146, MPI, MPI_Keyval_free(&key_3);); (230, MPI, MPI_Comm_free(&rev_comm);); (135, MPI, MPI_Attr_get(dup_comm, key_3, (void **) &vvalue, &flag);); (210, MPI, MPI_Comm_compare(world_comm, lo_comm, &result);); (190, MPI, MPI_Abort(MPI_COMM_WORLD, 3011);); (71, MPI, MPI_Abort(MPI_COMM_WORLD, 3003);); (228, MPI, MPI_Group_free(&world_group);); (155, MPI, MPI_Comm_size(split_comm, &size);); (-7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &world_size);); (194, MPI, MPI_Comm_create(world_comm, rev_group, &rev_comm);); (135, MPI, MPI_Keyval_free(&key_1);); (157, MPI, MPI_Abort(MPI_COMM_WORLD, 3009);); (190, MPI, MPI_Group_range_incl(world_group, 1, ranges, &rev_group);); (92, MPI, MPI_Keyval_create(MPI_NULL_COPY_FN, MPI_NULL_DELETE_FN, &key_3, &value);); (129, MPI, MPI_Abort(MPI_COMM_WORLD, 3008);); (148, MPI, MPI_Comm_rank(split_comm, &rank);); (201, MPI, MPI_Abort(MPI_COMM_WORLD, 3013);); (173, MPI, MPI_Comm_compare(lo_comm, dup_comm, &result);); (47, MPI, MPI_Abort(MPI_COMM_WORLD, 3002);); (152, MPI, MPI_Barrier(split_comm);); (40, MPI, MPI_Comm_rank(lo_comm, &rank);); (106, MPI, MPI_Abort(MPI_COMM_WORLD, 3004);); (213, MPI, MPI_Group_free(&rev_group);); (163, MPI, MPI_Abort(MPI_COMM_WORLD, 3010);); (205, MPI, MPI_Comm_free(&dup_comm_world);); (4, MPI, MPI_Abort(MPI_COMM_WORLD, 3001);); (213, MPI, MPI_Comm_free(&dup_comm);); (41, MPI, MPI_Barrier(lo_comm);); (210, MPI, MPI_Comm_free(&lo_comm);); (91, MPI, MPI_Attr_get(dup_comm, key_1, (void **) &vvalue, &flag);); (179, MPI, MPI_Abort(MPI_COMM_WORLD, 3012);)"}
{"predict": "09:         int const smem_idx = half_idx * half_rotary_dim + intra_half_idx;\n110:         int const k_cache_idx = token_idx * rotary_embedding_dim + (tidx * vec_size);\n111: \n112:         if (do_rotary)\n113:         {\n114:             mmha::apply_rotary_embedding(\n115:                 k, tidx, rotary_embedding_dim, rotary_embedding_base, rotary_embedding_scale, token_pos_idx);\n116:         }\n117:         else\n118:         {\n119:             T* k_smem_ptr = k_smem + smem_idx;\n120:             *k_smem_ptr = k_cache[k_cache_idx];\n121:         }\n122:         break;\n123:     }\n124:     default:\n125:     {\n126:         break;\n127:     }\n128:     }\n129: \n130:     // Shift k cache\n131:     shiftKCacheBuffer[blockIdx.x + blockIdx.y * gridDim.x + blockIdx.z * gridDim.x * gridDim.y] = k;\n132: }", "label": "(16, CUDA, extern __shared__ __align__(sizeof(float2)) char smem_[];)"}
{"predict": "67:         // Copy data to GPU\n68:         cudaMemcpyAsync(a[dev_id], a_h, nx * iy_end[dev_id] * sizeof(float), cudaMemcpyHostToDevice,\n69:                        compute_stream[dev_id]);\n70:     }\n71: \n72:     // Launch kernel\n73:     for (int dev_id = 0; dev_id < num_devices; ++dev_id) {\n74:         dim3 grid_size((iy_end[dev_id] - iy_start[dev_id] + blockDim.x - 1) / blockDim.x,\n75:                        (nx + blockDim.y - 1) / blockDim.y, 1);\n76:         dim3 block_size(blockDim.x, blockDim.y, 1);\n77:         kernel<<<grid_size, block_size, 0, compute_stream[dev_id]>>>(a[dev_id], a_new[dev_id],\n78:                                                                 a_ref_h, nx, iy_start[dev_id],\n79:                                                                 iy_end[dev_id]);\n80:     }\n81: \n82:     // Copy result back to host\n83:     for (int dev_id = 0; dev_id < num_devices; ++dev_id) {\n84:         cudaMemcpyAsync(a_h, a[dev_id], nx * iy_end[dev_id] * sizeof(float), cudaMemcpyDeviceToHost,\n85:                        compute_stream[dev_id]);\n86:     }\n87: \n88:     // Wait for all CUDA streams to complete\n89:     for (int dev_id = 0; dev_id < num_devices; ++dev_id) {\n90:         cudaStreamSynchronize(compute_stream[dev_id]);\n91:     }\n92: \n93:     // Free memory\n94:     for (int dev_id = 0; dev_id < num_devices; ++dev_id) {\n95:         cudaFree(a[dev_id]);\n96:         cudaFree(a_new[dev_id]);\n97:     }\n98:     cudaFree(a_ref_h);\n99: \n100:     return 0;\n101: }", "label": "(207, OpenMP, double stop = omp_get_wtime();); (146, CUDA, CUDA_RT_CALL(cudaStreamSynchronize(push_top_stream[bottom]));); (77, CUDA, CUDA_RT_CALL(cudaDeviceSynchronize());); (28, CUDA, CUDA_RT_CALL(cudaFree(0));); (88, CUDA, CUDA_RT_CALL(cudaDeviceCanAccessPeer(&canAccessPeer, dev_id, top));); (250, CUDA, CUDA_RT_CALL(cudaFreeHost(a_h));); (29, CUDA, CUDA_RT_CALL(cudaMallocHost(&a_ref_h, nx * ny * sizeof(float)));); (71, CUDA, CUDA_RT_CALL(cudaGetLastError());); (91, CUDA, CUDA_RT_CALL(cudaDeviceCanAccessPeer(&canAccessPeer, dev_id, bottom));); (92, CUDA, CUDA_RT_CALL(cudaDeviceEnablePeerAccess(bottom, 0));); (235, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_bottom_stream[dev_id]));); (83, CUDA, CUDA_RT_CALL(cudaDeviceEnablePeerAccess(top, 0));); (74, CUDA, CUDA_RT_CALL(cudaMalloc(l2_norm_d + dev_id, sizeof(float)));); (44, CUDA, CUDA_RT_CALL(cudaMalloc(a + dev_id, nx * (chunk_size[dev_id] + 2) * sizeof(float)));); (157, CUDA, CUDA_RT_CALL(cudaStreamSynchronize(compute_stream[dev_id]));); (45, CUDA, CUDA_RT_CALL(cudaMemset(a[dev_id], 0, nx * (chunk_size[dev_id] + 2) * sizeof(float)));); (132, CUDA, CUDA_RT_CALL(cudaStreamSynchronize(push_bottom_stream[top]));); (9, CUDA, CUDA_RT_CALL(cudaGetDeviceCount(&num_devices));); (228, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_top_stream[dev_id]));); (11, CUDA, CUDA_RT_CALL(cudaSetDevice(dev_id));); (38, CUDA, CUDA_RT_CALL(cudaMalloc(a_new + dev_id, nx * (chunk_size[dev_id] + 2) * sizeof(float)));); (228, CUDA, CUDA_RT_CALL(cudaFreeHost(l2_norm_h[dev_id]));); (230, CUDA, CUDA_RT_CALL(cudaFree(a_new[dev_id]));); (59, CUDA, CUDA_RT_CALL(cudaStreamCreate(compute_stream + dev_id));); (60, CUDA, CUDA_RT_CALL(cudaStreamCreate(push_bottom_stream + dev_id));); (36, CUDA, CUDA_RT_CALL(cudaMemset(a_new[dev_id], 0, nx * (chunk_size[dev_id] + 2) * sizeof(float)));); (230, CUDA, CUDA_RT_CALL(cudaFreeHost(a_ref_h));); (9, CUDA, CUDA_RT_CALL(cudaMallocHost(&a_h, nx * ny * sizeof(float)));); (219, CUDA, CUDA_RT_CALL(cudaStreamDestroy(compute_stream[dev_id]));); (102, OpenMP, double start = omp_get_wtime();); (57, CUDA, CUDA_RT_CALL(cudaMallocHost(l2_norm_h + dev_id, sizeof(float)));); (222, CUDA, CUDA_RT_CALL(cudaFree(a[dev_id]));); (218, CUDA, CUDA_RT_CALL(cudaFree(l2_norm_d[dev_id]));); (50, CUDA, CUDA_RT_CALL(cudaStreamCreate(push_top_stream + dev_id));)"}
{"predict": ":         x_cu = static_cast<ST *>(malloc(sizeof(ST) * shape.X_len)); \\\n127:         y_cu = static_cast<ST *>(malloc(sizeof(ST) * shape.Y_len)); \\\n128:         z_cu = static_cast<DT *>(malloc(sizeof(DT) * shape.Z_len)); \\\n129:         \\\n130:         for (int i = 0; i < shape.X_len; ++i) { \\\n131:             x_cu[i] = x[i]; \\\n132:         } \\\n133:         \\\n134:         for (int i = 0; i < shape.Y_len; ++i) { \\\n135:             y_cu[i] = y[i]; \\\n136:         } \\\n137:         \\\n138:         for (int i = 0; i < shape.Z_len; ++i) { \\\n139:             z_cu[i] = z[i]; \\\n140:         } \\\n141:         \\\n142:         cl_mem x_gpu = clCreateBuffer(clContext.get(), CL_MEM_READ_ONLY, shape.X_len * sizeof(ST), nullptr, &retVal); \\\n143:         cl_mem y_gpu = clCreateBuffer(clContext.get(), CL_MEM_READ_ONLY, shape.Y_len * sizeof(ST), nullptr, &retVal); \\\n144:         cl_mem z_gpu = clCreateBuffer(clContext.get(), CL_MEM_WRITE_ONLY, shape.Z_len * sizeof(DT), nullptr, &retVal); \\\n145:         \\\n146:         clEnqueueWriteBuffer(queue.get(), x_gpu, CL_TRUE, 0, shape.X_len * sizeof(ST), x, 0, nullptr, nullptr); \\\n147:         clEnqueueWriteBuffer(queue.get(), y_gpu, CL_TRUE, 0, shape.Y_len * sizeof(ST), y, 0, nullptr, nullptr); \\\n148:         \\\n149:         clSetKernelArg(kernel.get(), 0, sizeof(cl_mem), &x_gpu); \\\n150:         clSetKernelArg(kernel.get(), 1, sizeof(cl_mem), &y_gpu); \\\n151:         clSetKernelArg(kernel.get(), 2, sizeof(cl_mem), &z_gpu); \\\n152:         \\\n153:         clFinish(queue.get", "label": "(129, CUDA, cudaMemcpy(z_cu, z, shape.Z_len * sizeof(DT), cudaMemcpyHostToDevice);); (221, CUDA, cudaFree(x_cu);); (122, CUDA, cudaMalloc(&x_cu, shape.X_len * sizeof(ST));); (125, CUDA, cudaMemcpy(y_cu, y, shape.Y_len * sizeof(ST), cudaMemcpyHostToDevice);); (123, CUDA, cudaMemcpy(x_cu, x, shape.X_len * sizeof(ST), cudaMemcpyHostToDevice);); (218, CUDA, cudaFree(y_cu);); (120, CUDA, cudaMalloc(&z_cu, shape.Z_len * sizeof(DT));); (118, CUDA, cudaMalloc(&y_cu, shape.Y_len * sizeof(ST));); (128, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (220, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (214, CUDA, cudaFree(z_cu);)"}
{"predict": "(1, CUDA, __global__ void MatrixMulCUDA1(", "label": "(22, CUDA, __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];); (20, CUDA, __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];)"}
{"predict": "01: \t                          ncols * sizeof(double), cudaMemcpyHostToDevice,\n102: \t                          FunCallStream)) ;\n103: \n104: \tcheckCuda(cudaMemcpyAsync(A_gpu->scubufs[streamId].Remain_info, Remain_info,\n105: \t                          sizeof(Remain_info_t), cudaMemcpyHostToDevice,\n106: \t                          FunCallStream)) ;\n107: \n108: \tcheckCuda(cudaMemcpyAsync(A_gpu->scubufs[streamId].xsup, xsup,\n109: \t                          sizeof(int_t), cudaMemcpyHostToDevice,\n110: \t                          FunCallStream)) ;\n111: \n112: \tcheckCuda(cudaMemcpyAsync(A_gpu->scubufs[streamId].usub, usub,\n113: \t                          sizeof(int_t), cudaMemcpyHostToDevice,\n114: \t                          FunCallStream)) ;\n115: \n116: \tcheckCuda(cudaMemcpyAsync(A_gpu->scubufs[streamId].lsub, lsub,\n117: \t                          sizeof(int_t), cudaMemcpyHostToDevice,\n118: \t                          FunCallStream)) ;\n119: \n120: \tcheckCuda(cudaMemcpyAsync(A_gpu->scubufs[streamId].Ublock_info, Ublock_info,\n121: \t                          sizeof(Ublock_info_t), cudaMemcpyHostToDevice,\n122: \t                          FunCallStream)) ;\n123: \n124: \tcheckCuda(cudaMemcpyAsync(A_gpu->scubufs[streamId].bigU, bigU,\n125: \t                          sizeof(double), cudaMemcpyHostToDevice,\n126: \t                          FunCallStream)) ;\n127: \n128: \tcheckCuda(cudaMemcpyAsync(A_gpu->scubufs[streamId].lsub, lsub,\n129: \t                          sizeof(int_t), cudaMemcpyHostToDevice,\n130: \t                          FunCallStream)) ;\n131: \n132: \tcheckCuda(cudaMemcpyAsync(A_gpu->scubufs[streamId].usub, usub,\n133: \t                          sizeof(int_t), cudaMemcpyHostToDevice,\n134: \t                          FunCallStream)) ;\n135: \n136: \tcheckCuda(cudaMemcpyAsync(A_gpu->scubufs[streamId", "label": "(93, CUDA, cudaEventRecord(A_gpu->ePCIeH2D[k0], FunCallStream);); (231, CUDA, cudaEventRecord(A_gpu->GemmStart[k0], FunCallStream);); (239, CUDA, cudaStreamSynchronize(FunCallStream);)"}
{"predict": "(17, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:sum)) (31, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:sum))", "label": "(6, OpenMP, #pragma omp atomic  hint(AMD_safe_fp_atomics)); (34, OpenMP, #pragma omp atomic  hint(AMD_unsafe_fp_atomics)); (2, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:sum)); (17, OpenMP, #pragma omp atomic  hint(AMD_fast_fp_atomics))"}
{"predict": "(16, CUDA, <<<gridSize, blockSize>>>(dynamics, state, control, state_der, next_state, output, t, dt, num);)", "label": "(6, CUDA, extern __shared__ float entire_buffer[];)"}
{"predict": "(13, CUDA, cudaMalloc((void**)&d_x, M * sizeof(real));) (14, CUDA, cudaMalloc((void**)&d_y, M * sizeof(real));) (15, CUDA, cudaMalloc((void**)&d_z, M * sizeof(real));) (29, CUDA, cudaFree(d_x);) (30, CUDA, cudaFree(d_y);) (31, CUDA, cudaFree(d_z);)", "label": "(37, CUDA, CHECK(cudaFree(d_y));); (14, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice));); (36, CUDA, CHECK(cudaFree(d_z));); (10, CUDA, CHECK(cudaMalloc(&d_y, M));); (8, CUDA, CHECK(cudaMalloc(&d_x, M));); (31, CUDA, CHECK(cudaFree(d_x));); (8, CUDA, CHECK(cudaMalloc(&d_z, M));); (9, CUDA, CHECK(cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice));)"}
{"predict": "(10, CUDA, naive_nrow_gemm<<<1, qk_block>>>(Q, K, sm_o, sm_scale, 0, m, m, n, mBlock);) (18, CUDA, row_softmax<<<1, sm_block>>>(sm_o, sm_o, m);) (26, CUDA, naive_pv<<<1, qkv_block>>>(sm_o, V, O, m, n, mBlock);)", "label": "(10, CUDA, cudaDeviceSynchronize();); (5, CUDA, cudaMalloc((void **)&sm_o, sizeof(float) * m * m);); (35, CUDA, cudaFree(sm_o);)"}
{"predict": "(12, MPI, MPI_Scatterv(orig_buf, counts, displs, MPI_LONG_DOUBLE_INT, tar_buf, counts, MPI_LONG_DOUBLE_INT, 0, win, MPI_STATUS_IGNORE);)", "label": "(49, MPI, MPI_Win_unlock(1, win);); (32, MPI, MPI_Win_unlock(rank, win);); (45, MPI, MPI_Win_lock(MPI_LOCK_SHARED, 1, 0, win);); (68, MPI, MPI_Type_free(&vector_dtp);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (64, MPI, MPI_Win_free(&win);); (41, MPI, MPI_Accumulate(orig_buf, 1, vector_dtp, 1, 0, 1, vector_dtp, MPI_MAXLOC, win);); (14, MPI, MPI_Type_commit(&vector_dtp);); (64, MPI, MPI_Free_mem(orig_buf);); (65, MPI, MPI_Free_mem(tar_buf);); (17, MPI, MPI_Win_lock(MPI_LOCK_SHARED, rank, 0, win);); (22, MPI, MPI_Barrier(MPI_COMM_WORLD);); (1, MPI, MPI_Alloc_mem(sizeof(long_double_int_t) * DATA_SIZE, MPI_INFO_NULL, &orig_buf);)"}
{"predict": "(1, CUDA, __global__ void hist_kernel(ImageBatchVarShapeWrapNHWC<uchar> img, int *histogram) {)", "label": "(2, CUDA, __shared__ int hist[256];)"}
{"predict": "(33, CUDA, __syncthreads();)", "label": "(74, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(test.data(), dstDev->basePtr(), test.size(), cudaMemcpyDeviceToHost));); (66, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (50, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(srcDev->basePtr(), srcVec.data(), srcVec.size(), cudaMemcpyHostToDevice));); (60, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (64, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamDestroy(stream));)"}
{"predict": "(14, OpenCL, cl_int retVal = clCreateCommandQueue(context, 0, nullptr, &retVal);); (31, OpenCL, clReleaseCommandQueue(queue);)", "label": "(18, OpenCL, clCreateCommandQueueWithProperties_StubWithCallback(clCreateCommandQueueWithProperties_testCommandQueueFromSpecifiedContext);)"}
{"predict": "(13, OpenCL, cl_int retVal = CL_INVALID_GL_OBJECT;); (14, OpenCL, auto glImage = clCreateFromGLTexture(clContext.get(), 0, GL_TEXTURE_1D, 0, textureId, &retVal);); (17, OpenCL, retVal = clReleaseMemObject(glImage);)", "label": "(25, OpenCL, clReleaseEvent(outEvent);)"}
{"predict": "NULL);\n131:             test_error(error, \"clEnqueueFillBuffer failed!\\n\");\n132:         }\n133:     }\n134: \n135:     if ((error = clFinish(tinfo->tQueue))) vlog(\"clFinish failed\\n\");\n136: \n137:     // Compute the result\n138:     cl_half *r = (cl_half *)gOut + thread_id * buffer_elements;\n139:     for (j = 0; j < buffer_elements; j++)\n140:     {\n141:         r[j] = func(p[j], p2[j]);\n142:     }\n143: \n144:     if ((error = clEnqueueWriteBuffer(tinfo->tQueue, tinfo->outBuf, CL_FALSE, 0,\n145:                                       buffer_size, r, 0, NULL, NULL)))\n146:     {\n147:         vlog_error(\"Error: clEnqueueWriteBuffer failed! err: %d\\n\", error);\n148:         return error;\n149:     }\n150: \n151:     if ((error = clFinish(tinfo->tQueue))) vlog(\"clFinish failed\\n\");\n152: \n153:     if (gHostFill)\n154:     {\n155:         for (j = gMinVectorSizeIndex; j < gMaxVectorSizeIndex; j++)\n156:         {\n157:             if ((error = clWaitForEvents(1, e + j)))\n158:             {\n159:                 vlog_error(\"Error: clWaitForEvents failed! err: %d\\n\", error);\n160:                 return error;\n161:             }\n162:             if ((error = clReleaseEvent(e[j])))\n163:             {\n164:                 vlog_error(\"Error: clReleaseEvent failed! err: %d\\n\", error);\n165:                 return error;\n166:             }\n167:         }\n168:     }\n169: \n170:     // Verify the result\n171:     for (j = 0; j < buffer_elements; j++)\n172:     {\n173:", "label": "(142, OpenCL, error = clSetKernelArg(kernel, 1, sizeof(tinfo->inBuf), &tinfo->inBuf);)"}
{"predict": "int rank = 0; rank < NumBlocksPerCluster; rank++)\n115:     {\n116:         int32_t const prevCount = blockExpertOffset;\n117:         blockExpertOffset += expertCounts[rank];\n118:         expertCounts[rank] = prevCount;\n119:     }\n120: \n121:     // Make the exclusive prefix sum available to all threads in the cluster.\n92:     cg::cluster_group::sync();\n93: \n94:     // Write out the histogram bins.\n95:     int32_t expertIdx = expertIndexes[threadIdx.x];\n96:     int32_t expertOffset = expertOffsets[threadIdx.x];\n97:     int32_t blockExpertOffset = expertCounts[blockIdx.x];\n98:     int32_t expertGlobalOffset = blockExpertOffset + expertOffset;\n99:     smemExpertCount[threadIdx.x] = expertGlobalOffset;\n100:     __syncthreads();\n101:     // Write out the histogram bins.\n102:     int32_t expertIdx = expertIndexes[threadIdx.x];\n103:     int32_t expertOffset = expertOffsets[threadIdx.x];\n104:     int32_t blockExpertOffset = expertCounts[blockIdx.x];\n105:     int32_t expertGlobalOffset = blockExpertOffset + expertOffset;\n106:     smemExpertCount[threadIdx.x] = expertGlobalOffset;\n107:     __syncthreads();\n108:     if (expertIdx >= 0 && expertIdx < params.mNumExperts)\n109:     {\n110:         smemExpertCount[threadIdx.x] = expertGlobalOffset;\n111:         __syncthreads();\n112:         atomicAdd(params.mPtrExpertCount[expertIdx], 1);\n113:     }\n114: }", "label": "(10, CUDA, __shared__ typename Scan::TempStorage tempStorage;); (33, CUDA, cudaGridDependencySynchronize();); (4, CUDA, __shared__ int32_t __attribute((aligned(128))) smemExpertCount[NumThreads];); (4, CUDA, __shared__ int32_t __attribute((aligned(128))) smemExpertOffset[NumThreads];)"}
{"predict": "hi = lo + block_order;\n113:         const int hi = (r==np-1) ? order : lo + block_order;\n114:         for (int i=lo; i<hi; i++) {\n115:             for (int j=0; j<block_order; j++) {\n116:                 B[i][j] += T[i][j];\n117:             }\n118:         }\n119:     }\n120:     MPI_Barrier(MPI_COMM_WORLD);\n121: \n122:     // Compute time\n123:     if (iter==0) {\n124:         t0 = MPI_Wtime();\n125:     }\n126:   }\n127:   t1 = MPI_Wtime();\n128:   MPI_Reduce(&t1, &t0, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);\n129:   if (me==0) {\n130:       printf(\"Time= %f\\n\", t0);\n131:   }\n132:   MPI_Finalize();\n133:   prk_free(A);\n134:   prk_free(B);\n135:   prk_free(T);\n136:   return 0;\n137: }", "label": "(9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &np);); (142, MPI, t1 = MPI_Wtime();); (20, MPI, MPI_Finalize();); (98, MPI, MPI_Type_commit(&trans_dt);); (144, MPI, MPI_Type_free(&stride_dt);); (159, MPI, MPI_Allreduce(MPI_IN_PLACE, &abserr, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);); (2, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (90, MPI, MPI_Type_size(MPI_DOUBLE,&dsize);); (141, MPI, MPI_Type_free(&trans_dt);); (70, MPI, MPI_Abort(MPI_COMM_WORLD,99);); (-5, MPI, if (provided < requested) MPI_Abort(MPI_COMM_WORLD,provided);); (89, MPI, MPI_Type_hvector(block_order, 1, dsize, stride_dt, &trans_dt);); (83, MPI, MPI_Type_vector(block_order, 1, block_order, MPI_DOUBLE, &stride_dt);); (54, MPI, MPI_Barrier(MPI_COMM_WORLD);); (96, MPI, t0 = MPI_Wtime();); (-11, MPI, MPI_Init_thread(&argc, &argv, requested, &provided);)"}
{"predict": "(26, OpenMP, #pragma omp parallel for reduction(+:axis_size))", "label": "(55, CUDA, cudaMalloc((void**)&result_data, axis_size * sizeof(float));); (52, CUDA, cudaMalloc((void**)&result_data, tensor->size * sizeof(float));)"}
{"predict": "(7, OpenCL, cl_status = clGetPlatformInfo(id, param_name, 0, NULL, &param_size);)", "label": "(8, OpenCL, cl_status = clGetPlatformInfo(id, param_name, param_size, some_info, NULL);)"}
{"predict": "(12, MPI, comm_hdls = (MPI_Comm *)malloc(sizeof(MPI_Comm) * MAX_NCOMM);) (13, MPI, ranks = (int *)malloc(sizeof(int) * nproc);) (28, MPI, if (mpi_errno == MPI_SUCCESS) { ncomm++; } else { if (verbose) printf(\"%d: Error creating comm %d\\n\", rank, i); errs = 0; break; } ) (37, MPI, MPI_Group_free(&comm_group);) (40, MPI, MPI_Comm_free(&comm_hdls[i]);)", "label": "(37, MPI, MPI_Group_free(&comm_group);); (29, MPI, mpi_errno = MPI_Comm_create(MPI_COMM_WORLD, comm_group, &comm_hdls[i]);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (8, MPI, MPI_Comm_group(MPI_COMM_WORLD, &world_group);); (45, MPI, MPI_Group_free(&world_group);); (8, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(10, MPI, MPI_Bcast(&max_offset, 1, MPI_INT, 0, comm);)", "label": "(6, MPI, MPI_Comm_size(comm, &size);); (4, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(16, CUDA, cudaMalloc((void**)&state, (b * H + h) * N * N * sizeof(F));)", "label": "(15, CUDA, __shared__ float rr[TILE_T][N], kk[TILE_T][N];)"}
{"predict": "(15, MPI, MPI_T_category_get_info(i, name, &nameLen, desc, &descLen, &numCvars, &numPvars, &numSubcat);) (29, MPI, MPI_T_cvar_get_info(cvarIndex[j], varname, &varnameLen, &verb, &datatype, NULL, NULL, NULL, &binding, &scope);) (50, MPI, MPI_T_pvar_get_info(pvarIndex[j], varname, &varnameLen, &verb, &varclass, &datatype, NULL, NULL, NULL, &binding, &isReadonly, &isContinuous, &isAtomic);) (72, MPI, MPI_T_category_get_info(subcatIndex[j], catname, &catnameLen, NULL, NULL, &ncvars, &npvars, &nsubcats);)", "label": "(51, MPI, MPI_T_category_get_pvars(i, numPvars, pvarIndex);); (28, MPI, MPI_T_category_get_cvars(i, numCvars, cvarIndex);); (3, MPI, MPI_T_category_get_num(&numCat);); (72, MPI, MPI_T_category_get_categories(i, numSubcat, subcatIndex);)"}
{"predict": "(12, CUDA, AgentVector::Agent instance = init_population[i];)", "label": "(13, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "= (int_t *) intMalloc_dist(nnz_loc);\n92: \n93:     /* Distribute the values of GA to each row */\n94:     for (i = 0; i < n; ++i)\n95:       for (j = colptr[i]; j < colptr[i+1]; ++j) {\n96:         row = rowind[j];\n97:         if (row >= fst_row && row < fst_row + m_loc) {\n98:           relpos = marker[row] - rowptr[row];\n99:           nzval_loc[relpos] = nzval[j];\n100:           colind[relpos] = i;\n101:           ++marker[row];\n102:         }\n103:     }\n104: \n105:     /* Create a local matrix A in NR_loc format. */\n106:     sCreate_CompCol_Matrix_dist(&A, m_loc, n, nnz_loc, nzval_loc, rowptr,\n107:                                colind, SLU_NC, SLU_S, SLU_GE);\n108: \n109:     /* Free up memory. */\n110:     free(marker);\n111:     free(rowptr);\n112:     free(nzval_loc);\n113:     free(colind);\n114: \n115:     /* Perturb the 1st and last diagonal of the matrix to lower\n116:        values. Intention is to change perm_r[].   */\n117:     nzval_loc[0] *= 0.01;\n118:     nzval_loc[nnz_loc-1] *= 0.0001; \n119: \n120:     /* Solve the linear system. */\n121:     dsysv_dist(trans, &A, nrhs, rhs, ldb, x, ldx, grid);\n122: \n123:     /* Free up memory. */\n124:     Destroy_SuperMatrix_Store(&GA);\n125:     Destroy_SuperMatrix_Store(&A);\n126:     Destroy_SuperMatrix_Store(&GA);\n127:     Destroy_SuperMatrix_Store(&A);\n128:     floatFree_dist(b_global);\n129:     floatFree", "label": "(33, MPI, MPI_Bcast( nzval,  nnz, MPI_FLOAT, 0, grid->comm );); (33, MPI, MPI_Bcast( rowind, nnz, mpi_int_t,  0, grid->comm );); (45, MPI, MPI_Bcast( colptr,  n+1, mpi_int_t,  0, grid->comm );); (43, MPI, MPI_Bcast( rowind,  nnz, mpi_int_t,  0, grid->comm );); (34, MPI, MPI_Bcast( &m,   1,   mpi_int_t,  0, grid->comm );); (26, MPI, MPI_Bcast( &n,     1,   mpi_int_t,  0, grid->comm );); (24, MPI, MPI_Bcast( &m,     1,   mpi_int_t,  0, grid->comm );); (33, MPI, MPI_Bcast( &nnz, 1,   mpi_int_t,  0, grid->comm );); (27, MPI, MPI_Bcast( colptr, n+1, mpi_int_t,  0, grid->comm );); (23, MPI, MPI_Bcast( &nnz,   1,   mpi_int_t,  0, grid->comm );); (29, MPI, MPI_Bcast( &n,   1,   mpi_int_t,  0, grid->comm );); (34, MPI, MPI_Bcast( nzval,   nnz, MPI_FLOAT, 0, grid->comm );)"}
{"predict": "(19, TBB, utils::NativeParallelFor(1, RunInNativeThread(/*blocking=*/true));)", "label": "(15, TBB, bool ok = tbb::finalize(schBlock, std::nothrow);); (11, TBB, bool ok = tbb::finalize(schBlock1, std::nothrow);); (4, TBB, tbb::task_scheduler_handle schBlock1;)"}
{"predict": "(4, OpenMP, #pragma omp critical; (11, OpenMP, #pragma omp critical; (13, OpenMP, #pragma omp critical;", "label": "(9, MPI, MPI_Type_free(&(c->key));)"}
{"predict": "(3, OpenMP, #pragma omp parallel for;)\n(6, OpenMP, #pragma omp parallel for;)\n(9, OpenMP, #pragma omp parallel for;)\n(12, OpenMP, #pragma omp parallel for;)\n(15, OpenMP, #pragma omp parallel for;)\n(18, OpenMP, #pragma omp parallel for;)\n(21, OpenMP, #pragma omp parallel for;)\n(24, OpenMP, #pragma omp parallel for;)\n(27, OpenMP, #pragma omp parallel for;)\n(30, OpenMP, #pragma omp parallel for;)\n(33, OpenMP, #pragma omp parallel for;)\n(36, OpenMP, #pragma omp parallel for;)\n(39, OpenMP, #pragma omp parallel for;)\n(42, OpenMP, #pragma omp parallel for;)\n(45, OpenMP, #pragma omp parallel for;)\n(48, OpenMP, #pragma omp parallel for;)\n(51, OpenMP, #pragma omp parallel for;)\n(54, OpenMP, #pragma omp parallel for;)\n(57, OpenMP, #pragma omp parallel for;)\n(60, OpenMP, #pragma omp parallel for;)", "label": "(2, OpenMP, #pragma omp dispatch)"}
{"predict": "(14, OpenCL, clSetKernelArg(global_check, 0, sizeof(buffer), &buffer);); (20, OpenCL, clSetKernelArg(global_check, 1, sizeof(work_dim), &work_dim);); (21, OpenCL, clSetKernelArg(global_check, 2, sizeof(global_work_offset), &global_work_offset);); (22, OpenCL, clSetKernelArg(global_check, 3, sizeof(global_work_size), &global_work_size););", "label": "(16, OpenCL, status = clSetKernelArg(global_check, 0, sizeof(cl_mem), &buffer);)"}
{"predict": "0; i < 10; ++i)\n62:     foo();\n63:   for (int i = 0; i < 10; ++i)\n64:     foo();\n65:   for (int i = 0; i < 10; ++i)\n66:     foo();\n67:   for (int i = 0; i < 10; ++i)\n68:     foo();\n69:   for (int i = 0; i < 10; ++i)\n70:     foo();\n71:   for (int i = 0; i < 10; ++i)\n72:     foo();\n73:   for (int i = 0; i < 10; ++i)\n74:     foo();\n75:   for (int i = 0; i < 10; ++i)\n76:     foo();\n77:   for (int i = 0; i < 10; ++i)\n78:     foo();\n79:   for (int i = 0; i < 10; ++i)\n80:     foo();\n81:   for (int i = 0; i < 10; ++i)\n82:     foo();\n83:   for (int i = 0; i < 10; ++i)\n84:     foo();\n85:   for (int i = 0; i < 10; ++i)\n86:     foo();\n87:   for (int i = 0; i < 10; ++i)\n88:     foo();\n89:   for (int i = 0; i < 10; ++i)\n90:     foo();\n91:   for (int i = 0; i < 10; ++i)\n92:     foo();\n93:   for (int i = 0; i < 10; ++i)\n94:     foo();\n95:   for (int i = 0; i < 10; ++i)\n96:     foo();\n97:   for (int i = 0; i < 10; ++i)\n98:     foo();\n99:   for (int i", "label": "(45, OpenMP, #pragma omp master taskloop simd reduction(&& : argc, z)); (101, OpenMP, #pragma omp master taskloop simd reduction(max : j)); (67, OpenMP, #pragma omp master taskloop simd reduction(^ : fl)); (111, OpenMP, #pragma omp master taskloop simd reduction(+ : m)); (20, OpenMP, #pragma omp master taskloop simd reduction()); (46, OpenMP, #pragma omp master taskloop simd reduction(+ : a, b, c, d, f)); (48, OpenMP, #pragma omp master taskloop simd reduction(min : a, b, c, d, f)); (53, OpenMP, #pragma omp master taskloop simd reduction(+ : ba)); (34, OpenMP, #pragma omp master taskloop simd reduction(~ : argc)); (100, OpenMP, #pragma omp parallel reduction(* : fl)); (17, OpenMP, #pragma omp master taskloop simd reduction(*)); (1, OpenMP, #pragma omp master taskloop simd reduction); (63, OpenMP, #pragma omp master taskloop simd reduction(&& : S2::S2sc)); (68, OpenMP, #pragma omp master taskloop simd reduction(+ : h, k, B::x)); (106, OpenMP, #pragma omp master taskloop simd reduction(task, + : m)); (75, OpenMP, #pragma omp parallel private(k)); (62, OpenMP, #pragma omp master taskloop simd reduction(& : e, g)); (67, OpenMP, #pragma omp master taskloop simd reduction(+ : o)); (-3, OpenMP, #pragma omp master taskloop simd reduction +); (72, OpenMP, #pragma omp master taskloop simd reduction(+ : p), reduction(+ : p)); (43, OpenMP, #pragma omp master taskloop simd reduction(* : ca)); (9, OpenMP, #pragma omp master taskloop simd reduction(\\)); (14, OpenMP, #pragma omp master taskloop simd reduction(| : argc,); (78, OpenMP, #pragma omp parallel reduction(min : i)); (81, OpenMP, #pragma omp parallel private(fl)); (72, OpenMP, #pragma omp master taskloop simd reduction(+ : r)); (46, OpenMP, #pragma omp master taskloop simd reduction(&& : S2::S2s)); (30, OpenMP, #pragma omp master taskloop simd reduction(max : h.b)); (20, OpenMP, #pragma omp master taskloop simd reduction(^ : S1)); (10, OpenMP, #pragma omp master taskloop simd reduction(|| : argc > 0 ? argv[1] : argv[2])); (-12, OpenMP, #pragma omp master taskloop simd reduction(); (56, OpenMP, #pragma omp master taskloop simd private(i), reduction(+ : j), reduction(+ : q)); (85, OpenMP, #pragma omp master taskloop simd reduction(+ : m) nogroup); (-12, OpenMP, #pragma omp master taskloop simd reduction(-); (66, OpenMP, #pragma omp parallel shared(i)); (-2, OpenMP, #pragma omp master taskloop simd reduction(foo : argc); (30, OpenMP, #pragma omp master taskloop simd reduction(- : da)); (69, OpenMP, #pragma omp master taskloop simd reduction(+ : fl))"}
{"predict": "(22, CUDA, gpu_in.device(gpu_device) = gpu_in.argmin().eval();)", "label": "(18, CUDA, cudaMemcpy(d_in, in.data(), in_bytes, cudaMemcpyHostToDevice);); (32, CUDA, cudaFree(d_in);); (13, CUDA, cudaMalloc((void**)(&d_in), in_bytes);); (13, CUDA, cudaMalloc((void**)(&d_out_min), out_bytes);); (25, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (23, CUDA, assert(cudaMemcpyAsync(out_min.data(), d_out_min, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);); (28, CUDA, cudaFree(d_out_min);)"}
{"predict": "(11, OpenMP, #pragma omp parallel for private(i) reduction(+:total_set))", "label": "(3, CUDA, cudaExtent extent = make_cudaExtent(10, 20, 0);)"}
{"predict": "(12, CUDA, err = cudaMemcpy(dst, src, sz, type);)", "label": "(7, CUDA, err = cudaMemcpy( dst, src, sz, type );); (10, CUDA, ss << cudaGetErrorString(err) << endl;)"}
{"predict": "(5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);) (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);) (10, MPI, MPI_Recv(&t, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)", "label": "(4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (9, MPI, MPI_Recv(&t, 1, MPI_DOUBLE, i, 11, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (1, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (11, MPI, MPI_Send(&t, 1, MPI_DOUBLE, 0, 11, MPI_COMM_WORLD);)"}
{"predict": "(11, MPI, MPI_Bcast(&array[0][0], 64, MPI_INT, 0, MPI_COMM_WORLD);)", "label": "(39, MPI, MPI_Type_commit(&columntype);); (7, MPI, MPI_Init(&argc, &argv);); (61, MPI, MPI_Finalize();); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (34, MPI, MPI_Type_vector(8, 1, 8, MPI_INT, &columntype);); (45, MPI, MPI_Type_free(&columntype);); (37, MPI, MPI_Send(&array[0][1], 1, columntype, 1, 1, MPI_COMM_WORLD);)"}
{"predict": "(28, OpenMP, #pragma omp parallel for reduction(+:err))", "label": "(41, SYCL, sycl::platform Plt = Dev.get_platform();)"}
{"predict": "(10, OpenMP, #pragma omp parallel for;)\n(11, OpenMP, #pragma omp parallel for;)\n(12, OpenMP, #pragma omp parallel for;)\n(13, OpenMP, #pragma omp parallel for;)\n(14, OpenMP, #pragma omp parallel for;)\n(15, OpenMP, #pragma omp parallel for;)\n(16, OpenMP, #pragma omp parallel for;)\n(17, OpenMP, #pragma omp parallel for;)\n(18, OpenMP, #pragma omp parallel for;)\n(19, OpenMP, #pragma omp parallel for;)\n(20, OpenMP, #pragma omp parallel for;)\n(21, OpenMP, #pragma omp parallel for;)\n(22, OpenMP, #pragma omp parallel for;)\n(23, OpenMP, #pragma omp parallel for;)\n(24, OpenMP, #pragma omp parallel for;)\n(25, OpenMP, #pragma omp parallel for;)\n(26, OpenMP, #pragma omp parallel for;)\n(27, OpenMP, #pragma omp parallel for;)\n(28, OpenMP, #pragma omp parallel for;)\n(29, OpenMP, #pragma omp parallel for;)\n(30, OpenMP, #pragma omp parallel for;)\n(31, OpenMP, #pragma omp parallel for;)\n(32, OpenMP, #pragma omp parallel for;)\n(33, OpenMP, #pragma omp parallel for;)\n(34, OpenMP, #pragma omp parallel for;)\n(35, OpenMP, #pragma omp parallel for;)\n(36, OpenMP, #pragma omp parallel for;)\n(37, OpenMP, #pragma omp parallel for;)\n(38, OpenMP, #pragma omp parallel for;)\n(39, OpenMP, #pragma omp parallel for;)\n(40, OpenMP, #pragma omp parallel for;)\n(47, OpenMP, #pragma omp parallel for;)\n(60, OpenMP, #pragma omp parallel for;)\n(62, OpenMP, #pragma omp parallel for simd;)\n(65, OpenMP, return 0; #pragma omp end target;)", "label": "(45, OpenMP, #pragma omp teams distribute parallel for simd aligned(e, g)); (7, OpenMP, #pragma omp teams distribute parallel for simd aligned); (14, OpenMP, #pragma omp teams distribute parallel for simd aligned ()); (9, OpenMP, #pragma omp teams distribute parallel for simd aligned (); (45, OpenMP, #pragma omp teams distribute parallel for simd aligned(h)); (51, OpenMP, #pragma omp parallel); (18, OpenMP, #pragma omp teams distribute parallel for simd aligned (argc,); (66, OpenMP, #pragma omp teams distribute parallel for simd aligned(f:j)); (33, OpenMP, #pragma omp teams distribute parallel for simd aligned (argv[1])); (24, OpenMP, #pragma omp teams distribute parallel for simd aligned (argc : 5)); (43, OpenMP, #pragma omp teams distribute parallel for simd aligned(i)); (56, OpenMP, #pragma omp teams distribute parallel for simd aligned(f)); (17, OpenMP, #pragma omp teams distribute parallel for simd aligned (*argc > 0 ? argv[1] : argv[2])); (-6, OpenMP, #pragma omp target); (47, OpenMP, #pragma omp teams distribute parallel for simd aligned(v:16)); (22, OpenMP, #pragma omp teams distribute parallel for simd aligned (S1)); (4, OpenMP, #pragma omp teams distribute parallel for simd aligned (argc)"}
{"predict": "(3, TBB, tbb::parallel_for(tbb::blocked_range<size_t>(0, size), [&](const tbb::blocked_range<size_t>& range) { TestOperations<ScopedColor1>(ScopedColor1::ScopedRed, ScopedColor1::ScopedGreen, ScopedColor1::ScopedBlue); })); (8, TBB, tbb::parallel_for(tbb::blocked_range<size_t>(0, size), [&](const tbb::blocked_range<size_t>& range) { TestOperations<ScopedColor2>(ScopedColor2::ScopedRed, ScopedColor2::ScopedGreen, ScopedColor2::ScopedBlue); }));", "label": "(10, TBB, __TBB_STATIC_ASSERT( !HasImplicitConversionToInt< tbb::atomic<ScopedColor1> >::value, \"The implicit conversion is not expected.\" );); (12, TBB, __TBB_STATIC_ASSERT( sizeof(tbb::atomic<ScopedColor2>) == sizeof(ScopedColor2), \"tbb::atomic instantiated with scoped enum should have the same size as scoped enum.\" );); (10, TBB, __TBB_STATIC_ASSERT( sizeof(tbb::atomic<ScopedColor1>) == sizeof(ScopedColor1), \"tbb::atomic instantiated with scoped enum should have the same size as scoped enum.\" );)"}
{"predict": "(28, OpenMP, #pragma omp parallel for private(z, d, xs, ys, zs))", "label": "(18, OpenMP, #pragma omp threadprivate (ident))"}
{"predict": "(11, OpenMP, #pragma omp parallel for;)", "label": "(17, OpenMP, #pragma omp taskwait); (0, OpenMP, #pragma omp task); (2, OpenMP, #pragma omp parallel for)"}
{"predict": "(12, MPI, MPI_Bcast(array, 48, MPI_INT, 0, MPI_COMM_WORLD);) (24, MPI, MPI_Gather(recvarray, 48, MPI_INT, array, 48, MPI_INT, 0, MPI_COMM_WORLD);)", "label": "(52, MPI, MPI_Type_free(&vector);); (6, MPI, MPI_Init(&argc, &argv);); (42, MPI, MPI_Type_create_resized(vector, 0, sizeof(int), &vector);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (64, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (37, MPI, MPI_Type_vector(8, 1, 6, MPI_INT, &vector);); (38, MPI, MPI_Type_commit(&vector);); (57, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(39, CUDA, CUDA_CHECK(cudaMemcpyAsync(d_A, A.data(), sizeof(data_type) * A.size(), cudaMemcpyHostToDevice, stream));) (42, CUDA, CUBLAS_CHECK(cublasCsyrk3mEx(cublasH, uplo, trans, n, k, &alpha, d_A, traits<data_type>::cuda_data_type, lda, &beta, d_C, traits<data_type>::cuda_data_type, ldc));) (48, CUDA, CUDA_CHECK(cudaMemcpyAsync(C.data(), d_C, sizeof(data_type) * C.size(), cudaMemcpyDeviceToHost, stream));)", "label": "(66, CUDA, CUDA_CHECK(cudaFree(d_C));); (52, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (37, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_C), sizeof(data_type) * C.size()));); (69, CUDA, CUDA_CHECK(cudaDeviceReset());); (66, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (33, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (59, CUDA, CUDA_CHECK(cudaFree(d_A));); (27, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(13, MPI, MPI_Bcast(&msgsize, 1, MPI_SIZE_T, 0, team.comm);)", "label": "(7, MPI, MPI_Comm_rank(team.comm, &rank);)"}
{"predict": "(10, MPI, if (size != 2) { fprintf(stderr, \"# error: only two can play ping-pong, run with two processes\\n\"); } MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);)", "label": "(59, MPI, double end_time = MPI_Wtime();); (60, MPI, MPI_Finalize();); (47, MPI, MPI_Recv(recv_buff, msg_size, MPI_INT, 1, TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (17, MPI, MPI_Bcast(&msg_size, 1, MPI_INT, 0, MPI_COMM_WORLD);); (2, MPI, MPI_Init(NULL, NULL);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (13, MPI, MPI_Bcast(&nr_msgs, 1, MPI_INT, 0, MPI_COMM_WORLD);); (48, MPI, MPI_Ssend(send_buff, msg_size, MPI_INT, 0, TAG, MPI_COMM_WORLD);); (24, MPI, double start_time = MPI_Wtime();); (31, MPI, MPI_Recv(recv_buff, msg_size, MPI_INT, 0, TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (28, MPI, MPI_Ssend(send_buff, msg_size, MPI_INT, 1, TAG, MPI_COMM_WORLD);); (0, MPI, MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);)"}
{"predict": "(22, CUDA, Bi = static_cast<thrust::complex<Q>>(*(A + ni));) (25, CUDA, *(B + ni) = Bi;) (36, CUDA, Bi = static_cast<thrust::complex<Q>>(*(A + ni));) (39, CUDA, *(B + ni) = Bi;)", "label": "(5, CUDA, thrust::complex<Q> Bi;)"}
{"predict": "(33, CUDA, cutlass::reference::host::TensorFillAsync(dst_tensor.host_view(), oob_value);) (36, CUDA, cutlass::reference::host::BlockFillSequentialAsync(src_tensor.host_data(), src_tensor.capacity());) (57, CUDA, dst_tensor.sync_host_async();) (80, CUDA, if(!equal) { std::cerr << \"Error - source tile differs from destination tile.\" << std::endl; return cudaErrorUnknown; })", "label": "(51, CUDA, cudaError_t result = cudaGetLastError();)"}
{"predict": "(2, OpenMP, #pragma omp parallel for private(n) reduction(+:FooConstructed,FooDestroyed))", "label": "(4, TBB, tbb::concurrent_queue<Foo> queue;)"}
{"predict": "ufs[streamId].usub = (int_t *) tmp_ptr;\n79: \t\tgpu_mem_used += A_host->bufmax[0] * sizeof(int_t);\n80: \t}\n81: \n82: \t/*allocate memory for LU buffers*/\n83: \tint_t sizeL = 0;\n84: \tint_t sizeU = 0;\n85: \tint_t sizeLU = 0;\n86: \tint_t sizeLUbuf = 0;\n87: \tint_t sizeLbuf = 0;\n88: \tint_t sizeUbuf = 0;\n89: \n90: \tsizeL = (nsupers + Pc - 1) / Pc;\n91: \tsizeU = (nsupers + Pr - 1) / Pr;\n92: \tsizeLU = sizeL * sizeU;\n93: \tsizeLUbuf = sizeLU * A_host->bufmax[0];\n94: \tsizeLbuf = sizeL * A_host->bufmax[0];\n95: \tsizeUbuf = sizeU * A_host->bufmax[0];\n96: \n97: \tA_gpu->sizeL = sizeL;\n98: \tA_gpu->sizeU = sizeU;\n99: \tA_gpu->sizeLU = sizeLU;\n100: \tA_gpu->sizeLUbuf = sizeLUbuf;\n101: \tA_gpu->sizeLbuf = sizeLbuf;\n102: \tA_gpu->sizeUbuf = sizeUbuf;\n103: \n104: \tA_gpu->LU_host = (double *) tmp_ptr;\n105: \tgpu_mem_used += sizeLU * sizeof(double);\n106: \tA_gpu->LU = (double *) tmp_ptr;\n107: \tgpu_mem_used += sizeLU * sizeof(double);\n108: \tA_gpu->Lbuf_host = (double *) tmp_ptr;\n109: \tgpu_mem_used += sizeLbuf * sizeof(double);\n110: \tA_gpu->Lbuf = (double *) tmp_ptr;\n111: \tgpu_mem_used += sizeLbuf * sizeof(double);\n112: \tA_gpu->Ubuf_host = (double *) tmp_ptr;\n113: \tgpu_mem_used += sizeUbuf * sizeof(double", "label": "(61, CUDA, checkCudaErrors(cudaMallocHost(  &tmp_ptr,  remain_l_max * sizeof(double) )) ;); (50, CUDA, checkCudaErrors(cudaMallocHost(  &tmp_ptr, (n) * sizeof(int_t) )) ;); (256, CUDA, checkCudaErrors(cudaMemcpy( (A_gpu->local_u_blk_infoVec), local_u_blk_infoVec, cum_num_u_blocks * sizeof(local_u_blk_info_t), cudaMemcpyHostToDevice)) ;); (476, CUDA, checkCudaErrors(cudaMemcpy( (A_gpu->UrowindVec), indtemp1, u_ind_len * sizeof(int_t), cudaMemcpyHostToDevice)) ;); (62, CUDA, checkCudaErrors(cudaMallocHost ( &tmp_ptr, sizeof(double) * (A_host->bufmax[1])));); (67, CUDA, checkCudaErrors(cudaMallocHost ( &tmp_ptr, sizeof(int_t) * (A_host->bufmax[2])));); (81, CUDA, checkCudaErrors(cudaMalloc(  &tmp_ptr,  buffer_size * sizeof(double))) ;); (179, CUDA, checkCudaErrors(cudaMemcpy( (A_gpu->local_l_blk_infoVec), local_l_blk_infoVec, cum_num_l_blocks * sizeof(local_l_blk_info_t), cudaMemcpyHostToDevice)) ;); (51, CUDA, checkCudaErrors(cudaMallocHost(  &tmp_ptr, mcb * sizeof(Ublock_info_t) )) ;); (174, CUDA, checkCudaErrors(cudaMalloc(  &tmp_ptr,  cum_num_l_blocks * sizeof(local_l_blk_info_t))) ;); (477, CUDA, checkCudaErrors(cudaMemcpy( (A_gpu->UrowindPtr), temp_UrowindPtr"}
{"predict": "(12, CUDA, for (size_t tid = blockIdx.x * blockDim.x + threadIdx.x; tid < (n * obase_size + unroll_number - 1) / unroll_number; tid += blockDim.x * gridDim.x) {)", "label": "(5, CUDA, extern __shared__ uint64_t s_qiHat_mod_pj[];)"}
{"predict": "(49, OpenMP, #pragma omp parallel for shared(i);)", "label": "(26, OpenMP, #pragma omp teams distribute shared (argc)); (22, OpenMP, #pragma omp teams distribute shared (argc > 0 ? argv[1] : argv[2])); (39, OpenMP, #pragma omp teams distribute shared(ca)); (41, OpenMP, #pragma omp teams distribute shared(da)); (16, OpenMP, #pragma omp teams distribute shared (argc,); (3, OpenMP, #pragma omp teams distribute shared); (5, OpenMP, #pragma omp teams distribute shared (); (49, OpenMP, #pragma omp teams distribute firstprivate(i), shared(i)); (27, OpenMP, #pragma omp teams distribute shared (argv[1])); (-2, OpenMP, #pragma omp target); (55, OpenMP, #pragma omp teams distribute shared(j)); (57, OpenMP, #pragma omp teams distribute firstprivate(i)); (17, OpenMP, #pragma omp teams distribute shared (S1)); (37, OpenMP, #pragma omp teams distribute shared(h, B::x)); (18, OpenMP, #pragma omp teams distribute shared (a, b, c, d, f)); (2, OpenMP, #pragma omp teams distribute shared (argc); (46, OpenMP, #pragma omp teams distribute shared(i)); (21, OpenMP, #pragma omp teams distribute shared(ba)); (35, OpenMP, #pragma omp teams distribute private(i), shared(i)); (28, OpenMP, #pragma omp teams distribute shared(e, g)); (39, OpenMP, #pragma omp teams distribute private(i)); (-7, OpenMP, #pragma omp teams distribute shared ())"}
{"predict": "(27, OpenMP, #pragma omp parallel for private(icnt) reduction(+:numOfStencilsComputed))", "label": "(33, CUDA, cudaMalloc( (void**) &fn_d,  XYZ_SIZE*sizeof(FLOAT));); (30, OpenMP, start_time_total = omp_get_wtime();); (55, OpenMP, elapsed_time_total = omp_get_wtime() - start_time_total;); (33, OpenMP, start_computation_time = omp_get_wtime();); (50, OpenMP, elapsed_computation_time = omp_get_wtime() - start_computation_time;); (29, CUDA, cudaMemcpy(f_d,f,XYZ_SIZE*sizeof(FLOAT), cudaMemcpyHostToDevice);); (50, CUDA, cudaMemcpy(f, f_d,XYZ_SIZE*sizeof(FLOAT), cudaMemcpyDeviceToHost);); (25, CUDA, cudaMalloc( (void**) &f_d,  XYZ_SIZE*sizeof(FLOAT));); (51, CUDA, cudaFree(fn_d);); (49, CUDA, cudaFree(f_d);)"}
{"predict": "114:     /* Timer to count the number of Schur updates */\n115:     SCT->SchurCompUdtTimer = 0.0;\n116:     SCT->SchurCompUdtFlOp = 0.0;\n117:     SCT->SchurCompUdtCount = 0;\n118: \n119:     SCT->ScatterTimer = 0.0;\n120:     SCT->ScatterFlOp = 0.0;\n121:     SCT->ScatterCount = 0;\n122: \n123:     SCT->GatherTimer = 0.0;\n124:     SCT->GatherFlOp = 0.0;\n125:     SCT->GatherCount = 0;\n126: \n127:     SCT->ScatterMOP = 0.0;\n128:     SCT->GatherMOP = 0.0;\n129:     SCT->ScatterMOPCount = 0;\n130:     SCT->GatherMOPCount = 0;\n131: \n132:     SCT->LookAheadRowSepTimer = 0.0;\n133:     SCT->LookAheadRowSepMOP = 0.0;\n134: \n135:     SCT->LookAheadGEMMTimer = 0.0;\n136:     SCT->LookAheadGEMMFlOp = 0.0;\n137:     SCT->LookAheadScatterTimer = 0.0;\n138:     SCT->LookAheadScatterMOP = 0.0;\n139:     SCT->AssemblyTimer = 0.0;\n140:     SCT->offloadable_flops = 0.0;\n141:     SCT->offloadable_mops = 0.0;\n142: \n143:     SCT->SchurCompUdtThreadTime = (double *) _mm_malloc(num_threads * CACHE_LINE_SIZE * sizeof(double), 64);\n144: \n145:     SCT->schur_flop_counter = 0.0;\n146:     SCT->schur_flop_timer = 0.0;\n147: \n148:", "label": "(12, OpenMP, #pragma omp master); (9, OpenMP, #pragma omp parallel default(shared))"}
{"predict": "(5, MPI, MPI_Init(&argc, &argv);)", "label": "(10, MPI, MPI_Allreduce(MPI_IN_PLACE, buf, N, MPIX_BFLOAT16, MPI_SUM, comm);); (20, MPI, MPI_Allreduce(MPI_IN_PLACE, buf_f16, N, MPIX_C_FLOAT16, MPI_SUM, comm);)"}
{"predict": "(11, CUDA, cudaMalloc(reinterpret_cast<void **>(&d_data), sizeof(data_type) * h_data.size());) (14, CUDA, CURAND_CHECK(curandCreateGenerator(&gen, CURAND_RNG_PSEUDO_MRG32K3A));) (16, CUDA, CURAND_CHECK(curandSetStream(gen, stream));) (19, CUDA, CURAND_CHECK(curandSetGeneratorOrdering(gen, order));) (22, CUDA, CURAND_CHECK(curandSetPseudoRandomGeneratorSeed(gen, seed));) (25, CUDA, CURAND_CHECK(curandGenerateUniform(gen, d_data, h_data.size()));) (28, CUDA, CUDA_CHECK(cudaMemcpyAsync(h_data.data(), d_data, sizeof(data_type) * h_data.size(), cudaMemcpyDeviceToHost, stream));)", "label": "(32, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (34, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "29:         );\n130:         k++;\n131:       }\n132:       /* wait for all sends and receives */\n133:       MPI_Waitall(k, request, status);\n134: \n135:       /* set up the new group */\n136:       new_group = lwgrp_chain_create(\n137:         comm, new_rank, new_left, new_right, ranks\n138:       );\n139:     }\n140:   }\n141: \n142:   /* do the reduce on the new chain */\n143:   lwgrp_chain_allreduce_recursive(\n144:     recvbuf, tempbuf, count, type, op, new_group\n145:   );\n146: \n147:   /* now reduce data from odd ranks out and remove them from the chain */\n148:   if (odd_rank_out) {\n149:     /* if we are an odd rank out, we need to send our data to our\n150:      * left neighbor */\n151:     MPI_Send(\n152:       recvbuf, count, type, left_rank, LWGRP_MSG_TAG_0, comm\n153:     );\n154:   }\n155: \n156:   /* free the temporary buffer */\n157:   lwgrp_type_dtbuf_free(tempbuf);\n158: \n159:   /* clean up the new group */\n160:   lwgrp_chain_destroy(new_group);\n161: \n162:   /* if we are the odd rank out, we need to recv the data from our\n163:    * left neighbor */\n164:   if (odd_rank_out) {\n165:     MPI_Recv(\n166:       recvbuf, count, type, left_rank,\n167:       LWGRP_MSG_TAG_0, comm, status\n168:     );\n169:   }\n170: \n171:   return MPI_SUCCESS;\n172: }", "label": "(133, MPI, MPI_Waitall(k, request, status);); (90, MPI, MPI_Reduce_local(recvbuf, tempbuf, count, type, op);)"}
{"predict": "(13, MPI, MPI_Bcast(bufin, count, dtype, 0, comm, MPI_STATUS_IGNORE);)", "label": "(10, MPI, MPI_Comm_size(comm, &size);); (15, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (22, MPI, MPI_Allreduce(bufin, bufout, count, dtype, MPI_SUM, comm);)"}
{"predict": "(34, CUDA, cudaMalloc((void **)&d_Data, DATA_SIZE);) (35, CUDA, cudaMalloc((void **)&d_Kernel, KERNEL_SIZE);) (48, CUDA, h_Kernel = (float *)malloc(KERNEL_SIZE);) (49, CUDA, h_Data = (float *)malloc(DATA_SIZE);) (50, CUDA, h_ResultCPU = (float *)malloc(DATA_SIZE);) (51, CUDA, h_ResultGPU = (float *)malloc(DATA_SIZE);) (53, CUDA, cudaFree(d_Data);) (54, CUDA, cudaFree(d_Kernel);) (86, CUDA, checkCudaErrors(cudaMemcpy(h_ResultGPU, d_Data, DATA_SIZE, cudaMemcpyDeviceToHost));)", "label": "(68, CUDA, checkCudaErrors(cudaMemset(d_Kernel, 0, DATA_SIZE));); (110, CUDA, checkCudaErrors(cudaFree(d_Kernel));); (40, CUDA, findCudaDevice(argc, (const char **)argv);); (72, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (49, CUDA, checkCudaErrors(cudaMalloc((void **)&d_Kernel, DATA_SIZE));); (105, CUDA, checkCudaErrors(cudaFree(d_Data));); (48, CUDA, checkCudaErrors(cudaMalloc((void **)&d_Data, DATA_SIZE));)"}
{"predict": "(9, MPI, MPI_Barrier(MPI_COMM_WORLD);) (12, MPI, rank = MPI_Comm_rank(MPI_COMM_WORLD);) (35, MPI, MPI_Barrier(MPI_COMM_WORLD);) (46, MPI, MPI_Bcast(&version, 1, MPI_UINT64_T, 0, MPI_COMM_WORLD);) (67, MPI, MPI_Barrier(MPI_COMM_WORLD);) (75, MPI, MPI_Barrier(MPI_COMM_WORLD);) (85, MPI, MPI_Reduce(&all_count, &all_count, 1, MPI_UINT64_T, MPI_SUM, 0, MPI_COMM_WORLD);) (86, MPI, if (mfu_rank == 0) { time_diff = MPI_Wtime() - start_read; } MPI_Bcast(&time_diff, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);)", "label": "(39, MPI, MPI_Error_string(mpirc, mpierrstr, &mpierrlen);); (98, MPI, MPI_Barrier(MPI_COMM_WORLD);); (22, MPI, int mpirc = MPI_File_open(MPI_COMM_WORLD, (char*)name, amode, MPI_INFO_NULL, &fh);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (79, MPI, double end_read = MPI_Wtime();); (3, MPI, double start_read = MPI_Wtime();); (67, MPI, mpirc = MPI_File_close(&fh);); (30, MPI, mpirc = MPI_File_set_view(fh, disp, MPI_BYTE, MPI_BYTE, datarep, MPI_INFO_NULL);); (47, MPI, MPI_Bcast(&version, 1, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (36, MPI, mpirc = MPI_File_read_at(fh, 0, &version_packed, 8, MPI_BYTE, &status);)"}
{"predict": "ADIO_Offset *) ADIOI_Malloc(nprocs_for_coll * sizeof(ADIO_Offset));\n91:         fd_start = (ADIO_Offset *) ADIOI_Malloc(nprocs_for_coll * sizeof(ADIO_Offset));\n92:         fd_end = (ADIO_Offset *) ADIOI_Malloc(nprocs_for_coll * sizeof(ADIO_Offset));\n93: \n94:         /* For each other process's request, calculate the list of offsets\n95:          * and lengths in the file and determine the start and end offsets. */\n96: \n97:         ADIOI_Calc_others_off_len(fd, others_req, count, datatype, file_ptr_type,\n98:                                   offset, &offset_list, &len_list, &start_offset,\n99:                                   &end_offset, &contig_access_count);\n100: \n101:         /* each process communicates its start and end offsets to other\n102:          * processes. The result is an array each of start and end offsets stored\n103:          * in order of process rank. */\n104: \n105:         ADIOI_Gather_all_off_len(fd, &offset_list, &len_list, st_offsets, fd_start,\n106:                                  fd_end, nprocs_for_coll);\n107: \n108:         /* Now that we have the start and end offsets for each process,\n109:          * calculate the min_st_offset and fd_size. */\n110: \n111:         min_st_offset = ADIOI_MIN(st_offsets, nprocs_for_coll);\n112:         fd_size = ADIOI_MAX(fd_start, nprocs_for_coll) - min_st_offset + 1;\n113: \n114:         /* Free the memory for the offset and length lists. */\n115:         ADIOI_Free(offset_list);\n116:         ADIOI_Free(len_list);\n117: \n118:         /* For each process, determine the number of bytes that will be sent\n119:          * to each other process. */\n120: \n121:         ADIOI_Determine_send_size(fd, st_offsets, fd_start, fd_end, &count_my_req_per_proc,\n122:                                   &count_my_req_procs, &count_others_req_per_proc,\n123:", "label": "(191, MPI, MPI_Allreduce(&tmp_error, error_code, 1, MPI_INT, MPI_MAX, fd->comm);); (65, MPI, MPI_Comm_size(fd->comm, &nprocs);); (93, MPI, MPI_Allgather(&end_offset, 1, ADIO_OFFSET, end_offsets, 1, ADIO_OFFSET, fd->comm);); (91, MPI, MPI_Allgather(&start_offset, 1, ADIO_OFFSET, st_offsets, 1, ADIO_OFFSET, fd->comm);); (63, MPI, MPI_Comm_rank(fd->comm, &myrank);); (210, MPI, MPI_Type_size_x(datatype, &size);)"}
{"predict": "(3, OpenMP, #pragma omp parallel for private(i) shared(atom) schedule(static) for (int i = 0; i < N; i++) { atom->m[i] = (real*) malloc(N * sizeof(real)); atom->x[i] = (real*) malloc(N * sizeof(real)); atom->y[i] = (real*) malloc(N * sizeof(real)); atom->z[i] = (real*) malloc(N * sizeof(real)); atom->vx[i] = (real*) malloc(N * sizeof(real)); atom->vy[i] = (real*) malloc(N * sizeof(real)); atom->vz[i] = (real*) malloc(N * sizeof(real)); atom->fx[i] = (real*) malloc(N * sizeof(real)); atom->fy[i] = (real*) malloc(N * sizeof(real)); atom->fz[i] = (real*) malloc(N * sizeof(real)); atom->pe[i] = (real*) malloc(N * sizeof(real)); atom->ke[i] = (real*) malloc(N * sizeof(real)); }); (15, OpenMP, #pragma omp parallel for private(i) shared(atom) schedule(static) for (int i = 0; i < 6; i++) { atom->box[i] = (real*) malloc(6 * sizeof(real)); });", "label": "(23, CUDA, CHECK(cudaMalloc((void**)&atom->g_fz, sizeof(real) * N));); (19, CUDA, CHECK(cudaMalloc((void**)&atom->g_z, sizeof(real) * N));); (17, CUDA, CHECK(cudaMalloc((void**)&atom->g_y, sizeof(real) * N));); (21, CUDA, CHECK(cudaMalloc((void**)&atom->g_pe, sizeof(real) * N));); (14, CUDA, CHECK(cudaMalloc((void**)&atom->g_x, sizeof(real) * N));); (12, CUDA, CHECK(cudaMalloc((void**)&atom->g_NL, sizeof(int) * N * MN));); (16, CUDA, CHECK(cudaMalloc((void**)&atom->g_fy, sizeof(real) * N));); (9, CUDA, CHECK(cudaMalloc((void**)&atom->g_NN, sizeof(int) * N));); (13, CUDA, CHECK(cudaMalloc((void**)&atom->g_fx, sizeof(real) * N));)"}
{"predict": "(33, OpenCL, OCL_NDRANGE(dim, dim_arg_global);)", "label": "(25, OpenCL, err = clEnqueueWriteBuffer( queue, buf[1], CL_TRUE, 0, sizeof(int), &dim_arg_global, 0, NULL, NULL);)"}
{"predict": "newc, delvc, ss4o3, rho0,\n112:                           numElemReg, regElemList);\n113: \n114:    // Free memory\n115:    Free(e_old);\n116:    Free(delvc);\n117:    Free(p_old);\n118:    Free(q_old);\n119:    Free(compression);\n120:    Free(compHalfStep);\n121:    Free(qq_old);\n122:    Free(ql_old);\n123:    Free(work);\n124:    Free(p_new);\n125:    Free(e_new);\n126:    Free(q_new);\n127:    Free(bvc);\n128:    Free(pbvc);\n129: }", "label": "(106, OpenMP, #pragma omp parallel for firstprivate(numElemReg)); (53, OpenMP, #pragma omp for firstprivate(numElemReg)); (65, OpenMP, #pragma omp for nowait firstprivate(numElemReg, eosvmin)); (34, OpenMP, #pragma omp parallel); (74, OpenMP, #pragma omp for nowait firstprivate(numElemReg, eosvmax)); (36, OpenMP, #pragma omp for nowait firstprivate(numElemReg))"}
{"predict": "(12, CUDA, cudaStreamCreateWithPriority(&stream, flags, priority);)", "label": "(13, CUDA, PYCUDA_ASSERT_NOGIL(cudaStreamCreateWithPriority(&stream, flags, priority));)"}
{"predict": "(31, CUDA, res_host = *res_dev;)", "label": "(38, CUDA, cudaFree(res_dev);); (31, CUDA, cudaMalloc(&res_dev, sizeof(float));); (33, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(40, MPI, MPI_Bcast(&access_mode, 1, MPI_INT, 0, comm, &rc);)", "label": "(40, MPI, MPI_Error_class(*error_code, &rc);); (12, MPI, MPI_Comm_size(comm, &mpi_size);); (42, MPI, MPI_Bcast(&rc, 1, MPI_INT, 0, comm);)"}
{"predict": "(17, CUDA, cudaMemcpyAsync(d_S, h_S, h_S.size() * sizeof(Element), cudaMemcpyHostToDevice, stream);) (20, CUDA, cudaMemcpyAsync(d_D, h_D, h_D.size() * sizeof(Element), cudaMemcpyHostToDevice, stream);) (29, CUDA, cudaMemcpyAsync(h_D, d_D, h_D.size() * sizeof(Element), cudaMemcpyDeviceToHost, stream);) (99, CUDA, cudaStreamSynchronize(stream);)", "label": "(81, CUDA, cudaError result = cudaDeviceSynchronize();); (30, CUDA, Tensor tensor_S = make_tensor(make_gmem_ptr(thrust::raw_pointer_cast(d_S.data())), make_layout(tensor_shape));); (14, CUDA, thrust::host_vector<Element> h_S(size(tensor_shape));); (22, CUDA, thrust::device_vector<Element> d_D = h_D;); (79, CUDA, std::cerr << \"CUDA Runtime error: \" << cudaGetErrorString(result) << std::endl;); (12, CUDA, thrust::host_vector<Element> h_D(size(tensor_shape));); (26, CUDA, Tensor tensor_D = make_tensor(make_gmem_ptr(thrust::raw_pointer_cast(d_D.data())), make_layout(tensor_shape));); (17, CUDA, thrust::device_vector<Element> d_S = h_S;)"}
{"predict": "(1, CUDA, __global__ void FindMaxCorr7(SiftPoint *sift1, SiftPoint *sift2, int numPts1, int numPts2) {)", "label": "(3, CUDA, __shared__ float siftParts2[16*64];); (1, CUDA, __shared__ float siftParts1[17*64];)"}
{"predict": "(13, OpenMP, #pragma omp parallel for)", "label": "(12, TBB, my_thread_array = tbb::cache_aligned_allocator<padded_private_worker>().allocate( my_n_thread );)"}
{"predict": "(2, CUDA, __global__ void acceptDraftTokensKernel(T const* draftProbs, T* targetProbs, SizeType32 const* numsDraftTokens, bool const* batchUseDraftLogits, TokenIdType const* draftIds, FinishedState const* finishedInput, FinishedState* finishedOutput, curandState_t* curandState, SizeType32 const* batchSlots, SizeType32 maxDraftTokens, SizeType32 beamWidth, SizeType32 vocabSize, bool randomThreshold, float constantThreshold, SizeType32 step, bool* batchIsAccepted, SizeType32* targetOutputIds))", "label": "(52, CUDA, __shared__ bool isAccepted;); (52, CUDA, __shared__ T sSumVal;)"}
{"predict": "(13, MPI, MPI_Win_create(dst_buf, sizeof(double) * nproc * MAXELEMS, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &dst_win);)", "label": "(54, MPI, MPI_Free_mem(dst_buf);); (9, MPI, MPI_Alloc_mem(sizeof(double) * nproc * MAXELEMS, MPI_INFO_NULL, &dst_buf);); (30, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, target, 0, dst_win);); (50, MPI, MPI_Win_free(&dst_win);); (13, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, me, 0, dst_win);); (17, MPI, MPI_Win_unlock(me, dst_win);); (18, MPI, MPI_Barrier(MPI_COMM_WORLD);); (28, MPI, MPI_Win_unlock(target, dst_win);)"}
{"predict": "(14, OpenMP, #pragma omp parallel for reduction(+:total) private(value, sleeps) <<< omp_get_num_threads(), 1 >>>); (20, OpenMP, #pragma omp critical);", "label": "(7, OpenMP, omp_init_lock(&l);); (27, OpenMP, omp_destroy_lock(&l);); (7, OpenMP, #pragma omp parallel); (14, OpenMP, omp_set_lock(&l);); (21, OpenMP, omp_unset_lock(&l);); (-1, OpenMP, auto numThreads = omp_get_max_threads();)"}
{"predict": "(12, OpenMP, #pragma omp parallel for simd private(i);)", "label": "(22, OpenMP, #pragma omp target parallel for simd private(argc > 0 ? argv[1] : argv[2])); (36, OpenMP, #pragma omp target parallel for simd private(e, g)); (44, OpenMP, #pragma omp target parallel for simd private(i)); (49, OpenMP, #pragma omp target parallel for simd private(j)); (47, OpenMP, #pragma omp parallel private(i)); (54, OpenMP, #pragma omp target parallel for simd private(m)); (4, OpenMP, #pragma omp target parallel for simd private(); (36, OpenMP, #pragma omp parallel); (32, OpenMP, #pragma omp target parallel for simd private(h, B::x)); (22, OpenMP, #pragma omp target parallel for simd private(a, b)); (24, OpenMP, #pragma omp target parallel for simd private(argv[1])); (17, OpenMP, #pragma omp target parallel for simd private(S1)); (38, OpenMP, #pragma omp parallel shared(i)); (12, OpenMP, #pragma omp target parallel for simd private(argc, z)); (2, OpenMP, #pragma omp target parallel for simd private(argc); (-2, OpenMP, #pragma omp target parallel for simd private()); (-9, OpenMP, #pragma omp target parallel for simd private); (2, OpenMP, #pragma omp target parallel for simd private(argc,)"}
{"predict": "_max, *o_y_min, *o_y_max,\n95:                *o_pixels_across, *o_pixels_down);\n96:     }\n97: #endif\n98: }\n\nOutput:", "label": "(81, MPI, MPI_Finalize();)"}
{"predict": "(8, MPI, MPI_Irecv(recvbuf, count * nprocs, datatype, rank + 1, tag, MPI_COMM_WORLD, &req);) (10, MPI, MPI_Isend(sendbuf, count, datatype, rank - 1, tag, MPI_COMM_WORLD, &req);)", "label": "(9, MPI, MPI_Scan_c(sendbuf, recvbuf, count, datatype, MPI_SUM, comm);); (12, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (10, MPI, MPI_Iscan_c(sendbuf, recvbuf, count, datatype, MPI_SUM, comm, &req);)"}
{"predict": "(26, OpenMP, #pragma omp parallel for shared(i, j))", "label": "(45, OpenMP, #pragma omp parallel firstprivate(i)); (40, OpenMP, #pragma omp parallel private(i)); (17, OpenMP, #pragma omp task shared(argc, z)); (39, OpenMP, #pragma omp task shared(i)); (7, OpenMP, #pragma omp task shared()); (20, OpenMP, #pragma omp task shared(argv[1])); (29, OpenMP, #pragma omp task shared(h, B::x)); (26, OpenMP, #pragma omp task shared(e, g)); (31, OpenMP, #pragma omp task firstprivate(i), shared(i)); (34, OpenMP, #pragma omp task shared(j)); (17, OpenMP, #pragma omp task shared(ba)); (6, OpenMP, #pragma omp task shared(argc > 0 ? argv[1] : argv[2])); (-5, OpenMP, #pragma omp task shared); (-4, OpenMP, #pragma omp task shared(); (23, OpenMP, #pragma omp task private(i), shared(i)); (0, OpenMP, #pragma omp task shared(argc,); (7, OpenMP, #pragma omp task shared(a, b, c, d, f)); (4, OpenMP, #pragma omp task shared(S1)); (13, OpenMP, #pragma omp task shared(da)); (-6, OpenMP, #pragma omp task shared(argc); (9, OpenMP, #pragma omp task shared(ca))"}
{"predict": "(14, OpenMP, #pragma omp parallel for)", "label": "(14, OpenMP, #pragma omp target map(p)); (51, OpenMP, #pragma omp target map(from: ps->ps->ps)); (38, OpenMP, #pragma omp target map(ps->i)); (31, OpenMP, #pragma omp target map(s.ps->ps->ps)); (32, OpenMP, #pragma omp target map(to: s.ps->ps->s.f[:22])); (27, OpenMP, #pragma omp target map(to: s.ps->ps)); (20, OpenMP, #pragma omp target map(to: s.p[:22])); (17, OpenMP, #pragma omp target map(s.p)); (2, OpenMP, #pragma omp target map(i)); (37, OpenMP, #pragma omp target map(to: ps->p[:22])); (18, OpenMP, #pragma omp target map(s.ps)); (27, OpenMP, #pragma omp target map(ps)); (30, OpenMP, #pragma omp target map(ps->s.f)); (41, OpenMP, #pragma omp target map(ps->ps->ps->ps)); (8, OpenMP, #pragma omp target map(s.s.f)); (-3, OpenMP, #pragma omp target map(i[1:23])); (-8, OpenMP, #pragma omp target map(d)); (3, OpenMP, #pragma omp target map(s.i)); (26, OpenMP, #pragma omp target map(from: ps->p)); (41, OpenMP, #pragma omp target map(from: s.f[:22]) map(to: ps->p[:33])); (38, OpenMP, #pragma omp target map(to: s.f[:22]) map(from: s.p[:33])); (41, OpenMP, #pragma omp target map(from: s.f[:22], s.s) map(to: ps->p[:33])); (-4, OpenMP, #pragma omp target map(s)); (25, OpenMP, #pragma omp target map(ps->ps)); (6, OpenMP, #pragma omp target map(from: s.ps->s.i)); (31, OpenMP, #pragma omp target map(to: ps->ps->ps->s.f[:22])); (-10, OpenMP, #pragma omp target map(p[1:24])); (23, OpenMP, #pragma omp target map(from: ps->ps->s.i))"}
{"predict": "(16, CUDA, <<<iDivUp(threadCount, 256), 256>>>(d_RanksA, d_RanksB, d_SrcKey, stride, N, threadCount);)\n(18, CUDA, <<<iDivUp(threadCount, 256), 256>>>(d_RanksA, d_RanksB, d_SrcKey, stride, N, threadCount);)", "label": "(15, CUDA, getLastCudaError(\"generateSampleRanksKernel<1U><<<>>> failed\\n\");); (19, CUDA, getLastCudaError(\"generateSampleRanksKernel<0U><<<>>> failed\\n\");)"}
{"predict": "(14, MPI, MPI_Comm_size(comm, &ranks);) (16, MPI, if (ranks < 2) { return DTCMP_Sort_local(inbuf, outbuf, count, key, keysat, cmp, hints); }) (27, MPI, MPI_Comm_rank(comm, &rank);) (31, MPI, MPI_Allreduce(&count, &total_count, 1, MPI_INT, MPI_SUM, comm, &status);) (32, MPI, MPI_Scatterv(databuf, displs, recvcounts, MPI_BYTE, state.buf, state.count, key, comm, &status);) (34, MPI, MPI_Gatherv(state.buf, state.count, key, outbuf, recvcounts, displs, key, comm, &status);)", "label": "(13, MPI, MPI_Comm_size(comm, &ranks);); (11, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(15, OpenCL, clFinish(queue);)", "label": "(18, OpenCL, rc = clEnqueueNDRangeKernel(queue, write_kernel, 1, NULL, global_work_size, local_work_size, 0, NULL, NULL); CLERR;); (23, OpenCL, rc = clSetKernelArg(read_kernel, 4, sizeof(TYPE), &p2); CLERR;); (35, OpenCL, clReleaseKernel(read_kernel);); (28, OpenCL, rc = clEnqueueNDRangeKernel(queue, read_kernel, 1, NULL, global_work_size, local_work_size, 0, NULL, NULL);CLERR;); (5, OpenCL, cl_kernel read_kernel = clCreateKernel(program, \"kernel_modtest_read\", &rc); CLERR;); (20, OpenCL, rc = clSetKernelArg(read_kernel, 5, sizeof(cl_mem), &mc->err_count); CLERR;); (14, OpenCL, rc = clSetKernelArg(read_kernel, 0, sizeof(cl_mem), &mc->device_mem); CLERR;); (22, OpenCL, rc = clSetKernelArg(read_kernel, 9, sizeof(cl_mem), &mc->err_second_read); CLERR;); (0, OpenCL, cl_kernel write_kernel = clCreateKernel(program, \"kernel_modtest_write\", &rc); CLERR;); (8, OpenCL, rc = clSetKernelArg(write_kernel, 4, sizeof(TYPE), &p2); CLERR;); (11, OpenCL, rc = clSetKernelArg(read_kernel, 1, sizeof(cl_ulong), &mc->mem_size); CLERR;); (3, OpenCL, rc = clSetKernelArg(write_kernel, 1, sizeof(cl_ulong), &mc->mem_size); CLERR;); (14, OpenCL, rc = clSetKernelArg(read_kernel, 6, sizeof(cl_mem), &mc->err_addr); CLERR;); (0, OpenCL, rc = clSetKernelArg(write_kernel, 0, sizeof(cl_mem), &mc->device_mem); CLERR;); (22, OpenCL, clReleaseKernel(write_kernel);); (7, OpenCL, rc = clSetKernelArg(read_kernel, 2, sizeof(unsigned int), &offset); CLERR;); (0, OpenCL, rc = clSetKernelArg(write_kernel, 3, sizeof(TYPE), &p1); CLERR;); (6, OpenCL, rc = clSetKernelArg(read_kernel, 3, sizeof(TYPE), &p1); CLERR;); (-3, OpenCL, rc = clSetKernelArg(write_kernel, 2, sizeof(unsigned int), &offset); CLERR;); (9, OpenCL, rc = clSetKernelArg(read_kernel, 8, sizeof(cl_mem), &mc->err_current); CLERR;); (7, OpenCL, rc = clSetKernelArg(read_kernel, 7, sizeof(cl_mem), &mc->err_expect); CLERR;)"}
{"predict": "iter<iterations; iter+=2) {\n121:         for (i=0; i<vector_length; i++) { \n122:           aux = -(3 - (i&7));\n123:           if (vector[index[i]]>0) vector[i] -= 2*vector[i];\n124:           else                    vector[i] -= 2*aux;\n125:         }\n126:         for (i=0; i<vector_length; i++) { \n127:           aux = (3 - (i&7));\n128:           if (vector[index[i]]>0) vector[i] -= 2*vector[i];\n129:           else                    vector[i] -= 2*aux;\n130:         }\n131:       }\n132:       break;\n133: \n134:     case NO_VECTOR:\n135:       for (iter=0; iter<iterations; iter+=2) {\n136:         for (i=0; i<vector_length; i++) { \n137:           aux = -(3 - (i&7));\n138:           if (vector[i]>0) vector[i] -= 2*vector[i];\n139:           else                    vector[i] -= 2*aux;\n140:         }\n141:         for (i=0; i<vector_length; i++) { \n142:           aux = (3 - (i&7));\n143:           if (vector[i]>0) vector[i] -= 2*vector[i];\n144:           else                    vector[i] -= 2*aux;\n145:         }\n146:       }\n147:       break;\n148: \n149:     case INS_HEAVY:\n150:       for (iter=0; iter<iterations; iter+=2) {\n151:         for (i=0; i<vector_length; i++) { \n152:           aux = -(3 - (i&7));\n153:           if (vector[index[i]]>0) vector[i] -= 2*vector[i];\n154:           else                    vector[i] -= 2*aux;\n155:         }\n156:         for (i=0;", "label": "(211, MPI, MPI_Reduce(&total, &total_sum, 1, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD);); (234, MPI, MPI_Finalize();); (83, MPI, MPI_Bcast(&iterations,    1, MPI_INT, root, MPI_COMM_WORLD);); (25, MPI, MPI_Init(&argc,&argv);); (80, MPI, MPI_Bcast(&vector_length, 1, MPI_INT, root, MPI_COMM_WORLD);); (81, MPI, MPI_Bcast(&btype,         1, MPI_INT, root, MPI_COMM_WORLD);); (97, MPI, MPI_Barrier(MPI_COMM_WORLD);); (23, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (21, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "Merge(srt_off, srt_len, count, offset_list, len_list, sum, others_req, nprocs, myrank, iter, &io_time);\n101: \n102:     io_time = MPI_Wtime();\n103:     /* read data */\n104:     ADIOI_ReadStrided(fd, all_recv_buf, srt_off, srt_len, sum, iter, &io_time);\n105: \n106:     ADIOI_Free(srt_off);\n107:     ADIOI_Free(srt_len);\n108: \n109:     io_time = MPI_Wtime();\n110:     /* write data */\n111:     ADIOI_WriteStrided(fd, all_recv_buf, srt_off, srt_len, sum, iter, &io_time);\n112: \n113:     ADIOI_Free(all_recv_buf);\n114:     ADIOI_Free(rdispls);\n115: \n116:     /* Free resources */\n117:     ADIOI_Free(tmp_len);\n118:     ADIOI_Free(send_buf);\n119:     ADIOI_Free(send_req);\n120:     ADIOI_Free(done_to_proc);\n121:     ADIOI_Free(curr_to_proc);\n122:     ADIOI_Free(partial_recv);\n123:     ADIOI_Free(start_pos);\n124:     ADIOI_Free(count);\n125:     ADIOI_Free(offset_list);\n126:     ADIOI_Free(len_list);\n127:     ADIOI_Free(send_size);\n128:     ADIOI_Free(recv_size);\n129:     ADIOI_Free(buf_idx);\n130:     ADIOI_Free(hole);\n131: \n132:     if (error_code)\n133:         *error_code = err;\n134:     return;\n135: }\n\nOutput:", "label": "(121, MPI, gpfsmpio_prof_cw[GPFSMPIO_CIO_T_DEXCH_SORT] += MPI_Wtime() - io_time;); (90, MPI, gpfsmpio_prof_cw[GPFSMPIO_CIO_T_DEXCH_NET] += MPI_Wtime() - io_time;); (23, MPI, int ret = MPI_Alltoall(recv_size, 1, MPI_COUNT, send_size, 1, MPI_COUNT, fd->comm);); (19, MPI, io_time = MPI_Wtime();); (24, MPI, gpfsmpio_prof_cw[GPFSMPIO_CIO_T_DEXCH_RECV_EXCH] += MPI_Wtime() - io_time;); (132, MPI, gpfsmpio_prof_cw[GPFSMPIO_CIO_T_DEXCH_SIEVE] += MPI_Wtime() - io_time;); (75, MPI, gpfsmpio_prof_cw[GPFSMPIO_CIO_T_DEXCH_SETUP] += MPI_Wtime() - io_time;)"}
{"predict": "(2, CUDA, __global__ void kernel_backward_222(const int B, const int T, const int C, const int H, float *__restrict__ const _buf, const F *__restrict__ const _r, const F *__restrict__ const _k, const F *__restrict__ const _v, const float *__restrict__ _w, const F *__restrict__ _u, const F *__restrict__ const _gy, F *__restrict__ const _gw))", "label": "(10, CUDA, __shared__ float v[_N_], gy[_N_];)"}
{"predict": "(12, CUDA, KerCountFreeSurface <<<sgridf,bsfluid,smem,stm>>> (fluidnum,fluidini,fstype,listp);)\n(16, CUDA, KerScanUmbrellaRegion <<<sgridf,bsfluid,0,stm>>> (count,fluidini,dvd.scelldiv,dvd.nc,dvd.cellzero,dvd.beginendcell,dvd.cellfluid,dcell,poscell,code,simulate2d,fsnormal,listp,fstype);)", "label": "(14, CUDA, cudaDeviceSynchronize();); (12, CUDA, cudaMemcpy(&count,listp+fluidnum,sizeof(unsigned),cudaMemcpyDeviceToHost);); (6, CUDA, cudaMemset(listp+fluidnum,0,sizeof(unsigned));)"}
{"predict": "(7, CUDA, cudaFree(ptr);)", "label": "(6, CUDA, st = cudaMalloc(&test_ptr, TEST_ALLOC_SIZE);); (25, CUDA, cudaFree(test_ptr);)"}
{"predict": "(10, MPI, MPI_Scatter(a, 10, MPI_INT, b, 10, MPI_INT, 0, MPI_COMM_WORLD, &status);)", "label": "(45, MPI, MPI_Buffer_detach(&bptr, &bl);); (25, MPI, MPI_Recv(b, 10, MPI_INT, i, 27 + j, MPI_COMM_WORLD, &status);); (8, MPI, MPI_Buffer_attach(buf, BUFSIZE);); (14, MPI, MPI_Wait(&request, &status);); (12, MPI, MPI_Ibsend(a, 10, MPI_INT, 0, 27 + j, MPI_COMM_WORLD, &request);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)"}
{"predict": "(33, OpenMP, #pragma omp parallel for private(i, v);)", "label": "(32, OpenMP, #pragma omp for lastprivate(argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (73, OpenMP, #pragma omp parallel private(i)); (22, OpenMP, #pragma omp for lastprivate(argc,); (45, OpenMP, #pragma omp for lastprivate(a, b)); (12, OpenMP, #pragma omp for lastprivate()); (2, OpenMP, #pragma omp parallel); (69, OpenMP, #pragma omp for lastprivate(j)); (60, OpenMP, #pragma omp for lastprivate(i)); (4, OpenMP, #pragma omp for lastprivate(); (11, OpenMP, #pragma omp for lastprivate(argc); (30, OpenMP, #pragma omp for lastprivate(foo:argc)); (-3, OpenMP, #pragma omp for lastprivate); (32, OpenMP, #pragma omp for lastprivate(conditional: argc,s) lastprivate(conditional:); (15, OpenMP, #pragma omp for lastprivate(argc > 0 ? argv[1] : argv[2])); (46, OpenMP, #pragma omp for lastprivate(h)); (37, OpenMP, #pragma omp for lastprivate(argv[1])); (40, OpenMP, #pragma omp for lastprivate(k, e, g)); (56, OpenMP, #pragma omp parallel shared(i)); (18, OpenMP, #pragma omp for lastprivate(S1))"}
{"predict": "(29, CUDA, checkCudnnFE(graph->execute(cudnn_handle, variant_pack, cudnn_workspace));)", "label": "(30, CUDA, cudaCheck(cudaGetLastError());)"}
{"predict": "(12, MPI, Sim_readLicenseFile(NULL);) (13, MPI, gmi_sim_start();) (14, MPI, gmi_register_sim();) (26, MPI, gmi_sim_stop();) (27, MPI, Sim_unregisterAllKeys();) (28, MPI, SimModel_stop();)", "label": "(8, MPI, MPI_Finalize();); (0, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(13, OpenMP, #pragma omp atomic write)", "label": "(25, CUDA, auto UserTarget = SemaRef.IdentifyCUDATarget(FD);); (23, CUDA, auto VarTarget = SemaRef.IdentifyCUDATarget(Var);)"}
{"predict": "(26, OpenMP, #pragma omp parallel for private(file))", "label": "(35, CUDA, CHECK_CUDA(cudaMalloc((void**)&imgs_dev, img_N * 3 * img_w * img_h * sizeof(uchar)));)"}
{"predict": "(14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);) (15, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);) (27, MPI, MPI_Win_create(data, 2 * sizeof(twoint_t), sizeof(twoint_t), MPI_INFO_NULL, MPI_COMM_WORLD, &win);) (73, MPI, MPI_Win_fence(0, win);) (95, MPI, MPI_Win_fence(0, win);) (103, MPI, MPI_Win_fence(0, win);) (110, MPI, MPI_Win_free(&win);)", "label": "(119, MPI, MPI_Free_mem(data);); (30, MPI, MPI_Win_fence(0, win);); (73, MPI, MPI_Win_fence(MPI_MODE_NOSUCCEED, win);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (12, MPI, MPI_Win_fence(MPI_MODE_NOPRECEDE, win);); (110, MPI, MPI_Win_free(&win);); (28, MPI, MPI_Accumulate(&mine, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_MAXLOC, win);); (5, MPI, MPI_Alloc_mem(sizeof(twoint_t), MPI_INFO_NULL, &data);); (71, MPI, MPI_Win_lock(MPI_LOCK_SHARED, 0, MPI_MODE_NOCHECK, win);); (72, MPI, MPI_Win_unlock(0, win);); (73, MPI, MPI_Barrier(MPI_COMM_WORLD);); (4, MPI, MPI_Win_create(data, me == 0 ? sizeof(twoint_t) : 0, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &win);); (41, MPI, MPI_Accumulate(&mine, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_MINLOC, win);)"}
{"predict": "(29, MPI, MPI_Recv(&message, 1, MPI_INT, prev, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)", "label": "(6, MPI, MPI_Init(&argc, &argv);); (25, MPI, MPI_Send(&message, 1, MPI_INT, next, tag, MPI_COMM_WORLD);); (60, MPI, MPI_Finalize();); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (34, MPI, MPI_Recv(&message, 1, MPI_INT, prev, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)"}
{"predict": "(15, CUDA, CUCH(cudaMemcpyAsync(gen->plans[devi].get(), plans[devi].data(), plan_size * sizeof(int32_t), cudaMemcpyHostToDevice), mhcudaMemoryCopyError);)", "label": "(12, CUDA, CUCH(cudaMalloc(&ptr, plan_size * sizeof(int32_t)), mhcudaMemoryAllocationFailure);); (6, CUDA, CUCH(cudaSetDevice(devs[devi]), mhcudaNoSuchDevice);)"}
{"predict": "(12, MPI, int comm_rank; MPI_Comm_rank(comm, &comm_rank);)", "label": "(9, MPI, MPI_Comm_rank(comm, &comm_rank);); (10, MPI, MPI_Comm_size(comm, &comm_size);)"}
{"predict": "(60, OpenMP, #pragma omp parallel for reduction(+:nstream_time))", "label": "(80, SYCL, sycl::free(d_A, q);); (32, SYCL, prk::SYCL::print_device_platform(q);); (51, SYCL, double *d_C = sycl::malloc_device<double>(length, q);); (76, SYCL, sycl::free(d_B, q);); (74, SYCL, sycl::free(d_C, q);); (47, SYCL, double *d_B = sycl::malloc_device<double>(length, q);); (26, SYCL, sycl::queue q(sycl::default_selector_v);); (44, SYCL, double *d_A = sycl::malloc_device<double>(length, q);)"}
{"predict": ":                 tensor->resize_single_dim(0, 1);\n125:                 tensor->resize_single_dim(1, input_width_);\n126:                 tensor->resize_single_dim(2, input_height_);\n127:                 tensor->resize_single_dim(3, 3);\n128:                 tensor->to_gpu();\n129:                 tensor->set_stream(stream_);\n130:             }\n131: \n132:             if(tensor->get_stream() != preprocess_stream){\n133:                 tensor->set_stream(preprocess_stream);\n134:             }\n135: \n136:             tensor->resize_single_dim(0, 1);\n137:             tensor->copy_from_cpu(tensor->offset(0), image.data, image.cols * image.rows * 3);\n138:             tensor->normalize(normalize_);\n139:             tensor->set_stream(stream_);\n140: \n141:             return true;\n142:         }\n143: \n144:         virtual bool postprocess(Job& job) override{\n145: \n146:             if(tensor_allocator_ == nullptr){\n147:                 INFOE(\"tensor_allocator_ is nullptr\");\n148:                 return false;\n149:             }\n150: \n151:             if(job.mono_tensor == nullptr){\n152:                 INFOE(\"mono_tensor is nullptr\");\n153:                 return false;\n154:             }\n155: \n156:             job.mono_tensor->release();\n157:             tensor_allocator_->recycle(job.mono_tensor);\n158:             return true;\n159:         }\n160:     };\n161: }", "label": "(151, CUDA, checkCudaRuntime(cudaMemcpyAsync(image_device, image_host, size_image, cudaMemcpyHostToDevice, preprocess_stream));); (125, CUDA, checkCudaRuntime(cudaStreamCreate(&preprocess_stream));); (67, CUDA, checkCudaRuntime(cudaStreamSynchronize(mono->get_stream()));)"}
{"predict": ",7,8,9,10,11,12,13,14,15,16] (17 tokens, three blocks)\n93:     auto inputTokens2 = std::make_shared<VecTokens>(VecTokens{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16});\n94:     auto const inputLength2 = static_cast<SizeType32>(inputTokens2->size());\n95:     LlmRequest::RequestIdType requestId2{1};\n96:     auto llmRequest1 = std::make_shared<LlmRequest>(requestId2, maxNewTokens, inputTokens2, samplingConfig, isStreaming);\n97:     GenerationRequest seq1{requestId2, inputLength2, beamWidth, blockManager.getWindowSizesMetadata()};\n98:     auto promptLen1 = llmRequest1->getNumTokens(beamIdx);\n99:     auto numContextBlocks1 = tc::ceilDiv(promptLen1, blockManager.getTokensPerBlock());\n100:     blockManager.addSequence(seq1, promptLen1, numContextBlocks1, *llmRequest1, maxAttentionWindow);\n101:     EXPECT_EQ(llmRequest1->getContextCurrentPosition(), 0);\n102:     auto cacheBlockIds1 = seq1.getCacheBlockIds(maxAttentionWindow).at(beamIdx);\n103:     EXPECT_THAT(cacheBlockIds1, ::testing::ElementsAreArray({0, 1, 2}));\n104: \n105:     // Offload all 3 blocks, fill with predictable pattern, onboard\n106:     for (auto cacheBlockId : cacheBlockIds1)\n107:     {\n108:         auto block = blockManager.getBlockById(cacheBlockId, maxAttentionWindow);\n109:         EXPECT_TRUE(block->isPrimary());\n110:         // offload so we can write to block in CPU code\n111:         blockManager.offloadBlock(block, maxAttentionWindow);\n112:         EXPECT_FALSE(block->isPrimary());\n113:         // need to sync so D2H transfer is done before accessing blocks\n114:", "label": "(114, CUDA, EXPECT_EQ(cudaMemset(block2Ptr->data(), 0, blockSize * sizeof(T)), cudaSuccess);); (74, CUDA, EXPECT_EQ(cudaDeviceSynchronize(), cudaSuccess);)"}
{"predict": "(19, CUDA, kernel_twostructs<<<dim3(1,1,1), dim3(32,1,1), 0, stream>>>(mystruct, mystruct2);)", "label": "(25, CUDA, cudaMemcpy(hostFloats2, gpuFloats2, 4 * sizeof(float), cudaMemcpyDeviceToHost);); (37, CUDA, cudaFree(gpuFloats1);); (8, CUDA, cudaMalloc((void**)(&gpuFloats2), N * sizeof(float));); (21, CUDA, cudaMemcpy(hostFloats1, gpuFloats1, 4 * sizeof(float), cudaMemcpyDeviceToHost);); (3, CUDA, cudaMalloc((void**)(&gpuFloats1), N * sizeof(float));); (35, CUDA, cudaFree(gpuFloats3);); (20, CUDA, cudaMemcpy(hostFloats3, gpuFloats3, 4 * sizeof(float), cudaMemcpyDeviceToHost);); (32, CUDA, cudaFree(gpuFloats2);); (5, CUDA, cudaMalloc((void**)(&gpuFloats3), N * sizeof(float));)"}
{"predict": "(27, CUDA, cudaSimulation.setPopulationData(pop);) (32, CUDA, cudaSimulation.step();) (35, CUDA, cudaSimulation.getPopulationData(pop);)", "label": "(28, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(10, CUDA, if (threadIdx.x == 0) { init(&barrier, blockDim.x); };)", "label": "(9, CUDA, __shared__ cuda::barrier<cuda::thread_scope_block> barrier;)"}
{"predict": "(17, CUDA, auto output_it = thrust::make_zip_iterator(thrust::make_tuple(nearest_point_id.begin(), nearest_linestring_parts_id.begin(), nearest_linestring_segment_id.begin(), neartest_point_coordinate.begin()));)", "label": "(6, CUDA, auto points_geometry_it = thrust::make_counting_iterator(0);); (8, CUDA, auto linestring_geometry_it  = thrust::make_counting_iterator(0);)"}
{"predict": "ifndef CRPL_COMP\n97: #elif CRPL_COMP == 0\n98:     #pragma omp target teams map (alloc: flux_G, u, qs) \\\n99:     num_teams(nz-2)\n100:     #pragma omp distribute parallel for collapse(3) \n101: #endif\n102:     for (k = 1; k < nz - 1; k++) {\n103:       for (j = jst; j <= jend; j++) {\n104:         for (i = ist; i <= iend; i++) {\n105:           u21 = u[1][k][j][i] * rho_i[k][j][i];\n106:           q = qs[k][j][i];\n107:           flux_G[0][k][j][i] = u[1][k][j][i] * u21 + C2 * ( u[4][k][j][i] - q );\n108:           flux_G[1][k][j][i] = u[1][k][j][i] * u21 + C2 * ( u[4][k][j][i] - q );\n109:           flux_G[2][k][j][i] = u[2][k][j][i] * u21;\n110:           flux_G[3][k][j][i] = u[3][k][j][i] * u21;\n111:           flux_G[4][k][j][i] = ( C1 * u[4][k][j][i] - C2 * q ) * u21;\n112:         }\n113:       }\n114:     }\n115: \n116:     if (timeron) timer_stop(t_rhsx);\n117:     if (timeron) timer_start(t_rhsy);\n118:     if(((iend-ist+1))<32)\n119:       num_workers2 = (iend-ist+1);\n120:     else\n121:       num_workers2 = 32;\n122:     //---------------------------------------------------------------------\n123:     // yi-direction flux differences\n124:", "label": "(569, OpenMP, #pragma omp teams distribute parallel for collapse(3) private(i,j,k) num_teams(jend - jst)); (26, OpenMP, #pragma omp distribute parallel for collapse (3) private(tmp)); (106, OpenMP, #pragma omp simd simdlen(32) private (tmp, u21i, u31i, u41i, u51i, u21im1, u31im1, u41im1, u51im1)); (21, OpenMP, #pragma omp target teams); (15, OpenMP, #pragma omp target data); (59, OpenMP, #pragma omp distribute)"}
{"predict": "(2, CUDA, extern \"C\" __global__ void computeAttentionScoresKernel(", "label": "(11, CUDA, extern __shared__ char shared_mem[];)"}
{"predict": "(12, OpenMP, #pragma omp parallel for reduction(+:parse_time))", "label": "(59, CUDA, CHECK_CUDA(cudaMalloc((void**)&ibuf[i].pixel_data[c], sz));); (56, CUDA, CHECK_CUDA(cudaFree(ibuf[i].pixel_data[c]));)"}
{"predict": "(17, MPI, int par_rank; int par_size; MPI_Comm comm = MPI_COMM_WORLD; MPI_Comm_rank(comm, &par_rank); MPI_Comm_size(comm, &par_size);)", "label": "(93, MPI, ascent_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (99, MPI, MPI_Barrier(comm);); (16, MPI, MPI_Comm_rank(comm, &par_rank);); (16, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(11, OpenMP, #pragma omp parallel for reduction(+:s))", "label": "(10, OpenMP, f = test_choleskyc_decomp_dim(&p3.matrix, 2 * 8.0 * GSL_DBL_EPSILON);)"}
{"predict": "sycl::device sycl_device = sycl::ext::oneapi::level_zero::make_device(\n99:           sycl_platform, (pi_native_handle)hDevice);\n100:      std::vector<sycl::device> sycl_devices(1);\n101:      sycl_devices[0] = sycl_device;\n102:      sycl::context sycl_context = sycl::ext::oneapi::level_zero::make_context(\n103:          sycl_devices, (pi_native_handle)hDriver, 1);\n104: \n105:      if (isImmCmdList) {\n106:        sycl_queue = sycl::ext::oneapi::level_zero::make_queue(\n107:            sycl_context, sycl_device, (pi_native_handle)hCommandList, true, 1,\n108:            sycl::property::queue::in_order());\n109:      } else {\n110:        sycl_queue = sycl::ext::oneapi::level_zero::make_queue(\n111:            sycl_context, sycl_device, (pi_native_handle)hQueue, false, 1,\n112:            sycl::property::queue::in_order());\n113:      }\n114: #endif\n115:   }\n116: \n117:   return 0;\n118: }\n\nOutput:", "label": "(54, SYCL, std::vector<sycl::device> sycl_devices(1);); (3, SYCL, sycl::queue sycl_queue;)"}
{"predict": "(8, OpenMP, #pragma omp parallel for private(hn_degrees[hn])); (17, OpenMP, #pragma omp parallel for reduction(+:num_high_degree_nodes.local()));", "label": "(15, TBB, tbb::enumerable_thread_specific<size_t> num_high_degree_nodes(0);)"}
{"predict": "do {\n121:       ism303dac_xl_status_reg_get(&dev_ctx_xl, &reg.status_a);\n122:     } while (!reg.status_a.drdy);\n123: \n124:     /* Read data and accumulate the mg value */\n125:     ism303dac_acceleration_raw_get(&dev_ctx_xl, data_raw);\n126: \n127:     for (j = 0; j < 3; j++) {\n128:       meas_st_on[j] += ism303dac_from_fs2g_to_mg(data_raw[j]);\n129:     }\n130:   }\n131: \n132:   /* Calculate the mg average values */\n133:   for (i = 0; i < 3; i++) {\n134:     meas_st_on[i] /= SAMPLES_XL;\n135:   }\n136: \n137:   /* Calculate test value */\n138:   for (i = 0; i < 3; i++) {\n139:     test_val[i] = (meas_st_on[i] - meas_st_off[i]) * 100 / meas_st_off[i];\n140:   }\n141: \n142:   /* Check if test is passed */\n143:   for (i = 0; i < 3; i++) {\n144:     if (test_val[i] < ST_PASS_THR)\n145:       st_result = ST_FAIL;\n146:   }\n147: \n148:   /* Disable Self Test */\n149:   ism303dac_xl_self_test_set(&dev_ctx_xl, ISM303DAC_XL_ST_DISABLE);\n150: \n151:   /* Check if test is passed */\n152:   if (st_result == ST_PASS)\n153:     printf(\"Self Test Passed\\n\");\n154:   else\n155:     printf(\"Self Test Failed\\n\");\n156: }", "label": "(162, OpenMP, ism303dac_mg_offset_temp_comp_set(&dev_ctx_mg, PROPERTY_ENABLE);)"}
{"predict": "(26, OpenMP, #pragma omp parallel for simd)", "label": "(4, OpenMP, #pragma omp master taskloop simd); (2, OpenMP, #pragma omp parallel)"}
{"predict": "(9, MPI, MPI_Info_get(info, key, MPI_MAX_INFO_VAL, buf, &flag);)", "label": "(7, MPI, MPI_Info_get(info, key, MPI_MAX_INFO_VAL, buf, &flag);)"}
{"predict": "(36, CUDA, __global__ void topk_bmm_tn(", "label": "(36, CUDA, __shared__ _VOLATILE_ float bSmem[16][128+4];); (34, CUDA, __shared__ _VOLATILE_ float aSmem[16][128+4];)"}
{"predict": "(29, OpenMP, #pragma omp parallel for)", "label": "(11, OpenMP, #pragma omp teams distribute simd shared (); (43, OpenMP, #pragma omp teams distribute simd shared(da)); (30, OpenMP, #pragma omp teams distribute simd shared (a, b, c, d, f)); (5, OpenMP, #pragma omp teams distribute simd shared); (34, OpenMP, #pragma omp teams distribute simd shared(ba)); (2, OpenMP, #pragma omp target); (35, OpenMP, #pragma omp teams distribute simd shared(ca)); (55, OpenMP, #pragma omp teams distribute simd shared(i)); (57, OpenMP, #pragma omp teams distribute simd shared(j)); (17, OpenMP, #pragma omp teams distribute simd shared (argc)); (43, OpenMP, #pragma omp teams distribute simd private(i), shared(i)); (24, OpenMP, #pragma omp teams distribute simd shared (argv[1])); (17, OpenMP, #pragma omp teams distribute simd shared (S1)); (43, OpenMP, #pragma omp teams distribute simd firstprivate(i), shared(i)); (9, OpenMP, #pragma omp teams distribute simd shared (argc > 0 ? argv[1] : argv[2])); (35, OpenMP, #pragma omp teams distribute simd shared(h, B::x)); (43, OpenMP, #pragma omp teams distribute simd private(i)); (51, OpenMP, #pragma omp teams distribute simd firstprivate(i)); (2, OpenMP, #pragma omp teams distribute simd shared (argc,); (28, OpenMP, #pragma omp teams distribute simd shared(e, g, z)); (-6, OpenMP, #pragma omp teams distribute simd shared ()); (-4, OpenMP, #pragma omp teams distribute simd shared (argc)"}
{"predict": "7:          /* The middle block: position i */\n148:          cols[nnz] = i;\n149:          values[nnz] = 4.0;\n150:          nnz++;\n151: \n152:          /* The right -1: position i+1 */\n153:          if ((i+1)%n)\n154:          {\n155:             cols[nnz] = i+1;\n156:             values[nnz] = -1.0;\n157:             nnz++;\n158:          }\n159: \n160:          /* The right identity block: position i+n */\n161:          if ((i+n)<=N-1)\n162:          {\n163:             cols[nnz] = i+n;\n164:             values[nnz] = -1.0;\n165:             nnz++;\n166:          }\n167: \n168:          /* Set the matrix entries */\n169:          HYPRE_IJMatrixSetValues(A, 1, &nnz, &i, cols, values);\n170:       }\n171:    }\n172: \n173:    /* Assemble the matrix */\n174:    HYPRE_IJMatrixAssemble(A);\n175: \n176:    /* Convert the matrix to a ParCSR matrix */\n177:    HYPRE_IJMatrixGetObject(A, (void **) &parcsr_A);\n178: \n179:    /* Create the right-hand side vector */\n180:    HYPRE_IJVectorCreate(MPI_COMM_WORLD, ilower, iupper, &b);\n181: \n182:    /* Choose a parallel storage format (see the User's Manual) */\n183:    HYPRE_IJVectorSetObjectType(b, HYPRE_PARCSR);\n184: \n185:    /* Initialize before setting coefficients */\n186:    HYPRE_IJVectorInitialize(b);\n187: \n188:    /* Now go through my local rows and set the vector entries */\n189:    {\n190:       double values[1];\n191: \n192:       for (i = il", "label": "(25, MPI, MPI_Init(&argc, &argv);); (79, MPI, MPI_Finalize();); (25, MPI, MPI_Comm_size(MPI_COMM_WORLD, &num_procs);); (23, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myid);)"}
{"predict": "(23, OpenMP, #pragma omp parallel for reduction(+:sum))", "label": "(1, TBB, using namespace oneapi::tbb::flow;)"}
{"predict": "(1, CUDA, __global__ void TestHomographies(float *d_coord, float *d_homo, int *d_counts, int numPts, float thresh2) {)", "label": "(3, CUDA, __shared__ float homo[8*TESTHOMO_LOOPS];); (3, CUDA, __shared__ int cnts[TESTHOMO_TESTS*TESTHOMO_LOOPS];)"}
{"predict": "(59, OpenMP, #pragma omp parallel for)", "label": "(34, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (85, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(j)); (7, OpenMP, #pragma omp target); (59, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(S2::S2s)); (10, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (); (21, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argc,); (36, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (a, b, c, d, f)); (47, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(ca)); (66, OpenMP, #pragma omp teams distribute parallel for simd private(i), firstprivate(i)); (13, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argc); (40, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(ba, z)); (59, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(h)); (26, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (S1)); (33, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argv[1])); (52, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(S2::S2sc)); (63, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(i)); (2, OpenMP, #pragma omp teams distribute parallel for simd firstprivate ()); (-7, OpenMP, #pragma omp teams distribute parallel for simd firstprivate); (40, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(da)); (72, OpenMP, #pragma omp teams distribute parallel for simd lastprivate(argc), firstprivate(argc)); (10, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argc > 0 ? argv[1] : argv[2]))"}
{"predict": "(19, MPI, s[i] = MPI_Win_create(base_ptr[i], (rank==0?99:0)*sizeof(char), 1, MPI_INFO_NULL, node, &win[i]);)", "label": "(25, MPI, s[i] = MPI_Win_allocate_shared((rank==0?99:0)*sizeof(char), 1, MPI_INFO_NULL, node, &base_ptr[i], &win[i]);); (36, MPI, MPI_Barrier(node);); (-1, MPI, MPI_Init(&argc, &argv);); (44, MPI, if(win[i] != MPI_WIN_NULL) MPI_Win_free(&win[i]);); (46, MPI, MPI_Finalize();); (2, MPI, MPI_Comm_rank(node, &rank);); (16, MPI, int sam = MPI_Alloc_mem((rank==0?99:0)*sizeof(char), MPI_INFO_NULL, &base_ptr[i]);); (25, MPI, MPI_Win_shared_query(win[i], 0, &size[i], &a[i], &ptr[i]);); (-5, MPI, int ss = MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, 0, MPI_INFO_NULL, &node);)"}
{"predict": "\"\n135:             \"can also be disabled by setting the environment variable \"\n136:             \"TIMEMORY_DISABLE_SIGNALS=1\")\n137:         .count(0)\n138:         .action([](parser_t&) { use_sample() = false; });\n139:     parser.add_argument({ \"-s\", \"--script\" }, \"Script to run\")\n140:         .count(1)\n141:         .action([](parser_t& p) {\n142:             script() = p.get<std::string>(\"script\");\n143:             use_script() = true;\n144:         });\n145:     parser.add_argument({ \"--no-cuda\" }, \"Disable CUDA\")\n146:         .count(0)\n147:         .action([](parser_t&) { use_cuda() = false; });\n148:     parser.add_argument({ \"--no-cuda-mem\" }, \"Disable CUDA memory tracking\")\n149:         .count(0)\n150:         .action([](parser_t&) { use_cuda_memory() = false; });\n151:     parser.add_argument({ \"--no-cuda-peak\" }, \"Disable CUDA peak memory tracking\")\n152:         .count(0)\n153:         .action([](parser_t&) { use_cuda_peak_memory() = false; });\n154:     parser.add_argument({ \"--no-cuda-peak-mem\" }, \"Disable CUDA peak memory tracking\")\n155:         .count(0)\n156:         .action([](parser_t&) { use_cuda_peak_memory() = false; });\n157:     parser.add_argument({ \"--no-opencl\" }, \"Disable OpenCL\")\n158:         .count(0)\n159:         .action([](parser_t&) { use_opencl() = false; });\n160:     parser.add_argument({ \"--no-opencl-mem\" }, \"Disable OpenCL memory tracking\")\n161:         .count(0)\n162:         .action([](parser_t&) { use_opencl_memory() = false; });\n163:     parser.add_argument({ \"--no-opencl-peak\" }, \"Disable OpenCL peak memory tracking\")\n164:         .count(0)\n165:         .action([](parser_t&)", "label": "(20, MPI, tim::settings::mpi_init()       = false;); (20, MPI, tim::settings::mpi_finalize()   = false;)"}
{"predict": "(25, sycl, double *cd = sycl::malloc_device<double>(size, queue);)", "label": "(9, SYCL, const bool doHalfs = dev.has(sycl::aspect::fp16);); (47, SYCL, sycl::free(cd, queue);); (12, SYCL, const bool doDoubles = dev.has(sycl::aspect::fp64);)"}
{"predict": "(67, CUDA, C[N * ( BLOCK_SIZE * by + ty ) + BLOCK_SIZE * bx + tx ] = Csub;);", "label": "(42, CUDA, __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];); (37, CUDA, __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];)"}
{"predict": "(5, MPI, tXferGpu2Host = SuperLU_timer_();)", "label": "(8, CUDA, cudaStreamSynchronize(LuH->A_gpu.cuStreams[0]);)"}
{"predict": "(3, OpenMP, #pragma omp parallel for)", "label": "(8, OpenMP, #pragma omp target teams device (argc); (5, OpenMP, #pragma omp target teams device ()); (10, OpenMP, #pragma omp target teams device (argc > 0 ? argv[1] : argv[2])); (15, OpenMP, #pragma omp target teams device (S1)); (12, OpenMP, #pragma omp target teams device (argc), device (argc+1)); (15, OpenMP, #pragma omp target teams device (-2)); (8, OpenMP, #pragma omp target teams device (argc + argc + z)); (15, OpenMP, #pragma omp target teams device (-10u)); (2, OpenMP, #pragma omp target teams device (argc))); (-7, OpenMP, #pragma omp target teams device); (14, OpenMP, #pragma omp target teams device (3.14)); (-7, OpenMP, #pragma omp target teams device ()"}
{"predict": "(49, OpenMP, #pragma omp parallel for reduction(+:sum))", "label": "(32, CUDA, checkCudaErrors(cudaMallocHost(&b, N_ROWS * sizeof(double)));); (27, CUDA, int dev = findCudaDevice(argc, (const char **)argv);); (67, CUDA, checkCudaErrors(cudaMalloc(&d_A, sizeof(float) * N_ROWS * N_ROWS));); (99, CUDA, checkCudaErrors(cudaFree(d_x_new));); (101, CUDA, checkCudaErrors(cudaFreeHost(b));); (95, CUDA, checkCudaErrors(cudaFree(d_A));); (64, CUDA, checkCudaErrors(cudaMalloc(&d_x, sizeof(double) * N_ROWS));); (92, CUDA, checkCudaErrors(cudaFree(d_b));); (93, CUDA, checkCudaErrors(cudaFree(d_x));); (59, CUDA, checkCudaErrors(cudaMalloc(&d_b, sizeof(double) * N_ROWS));); (57, CUDA, checkCudaErrors(cudaStreamCreateWithFlags(&stream1, cudaStreamNonBlocking));); (60, CUDA, checkCudaErrors(cudaMalloc(&d_x_new, sizeof(double) * N_ROWS));); (61, CUDA, checkCudaErrors(cudaMemsetAsync(d_x, 0, sizeof(double) * N_ROWS, stream1));); (21, CUDA, checkCudaErrors(cudaMallocHost(&A, N_ROWS * N_ROWS * sizeof(float)));); (90, CUDA, checkCudaErrors(cudaFreeHost(A));)"}
{"predict": "(27, OpenMP, #pragma omp parallel for)", "label": "(11, OpenMP, #pragma omp single private()); (39, OpenMP, #pragma omp parallel private(i)); (21, OpenMP, #pragma omp single private(a, b)); (16, OpenMP, #pragma omp single private(argc)); (23, OpenMP, #pragma omp single private(e, g)); (2, OpenMP, #pragma omp single private); (19, OpenMP, #pragma omp single private(argv[1])); (26, OpenMP, #pragma omp parallel); (9, OpenMP, #pragma omp single private(argc > 0 ? argv[1] : argv[2])); (20, OpenMP, #pragma omp single private(h, B::x)); (11, OpenMP, #pragma omp single private(S1)); (2, OpenMP, #pragma omp single private(argc); (34, OpenMP, #pragma omp single private(m)); (2, OpenMP, #pragma omp single private(argc,); (27, OpenMP, #pragma omp single private(j)); (16, OpenMP, #pragma omp single shared(i)); (-7, OpenMP, #pragma omp single private(); (22, OpenMP, #pragma omp parallel shared(i)); (18, OpenMP, #pragma omp single private(i))"}
{"predict": "127: }", "label": "(96, MPI, MPI_Finalize();); (88, MPI, MPI_Allreduce( &problem, &any_problem, 1, MPI_INT, MPI_LOR, smpi->world() );)"}
{"predict": "(20, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(k);) (25, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(z);) (30, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(aa);) (35, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(raa);) (40, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(h);) (45, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(rh);) (50, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(da);)", "label": "(37, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(raa)); (48, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(rh)); (41, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(h)); (28, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(aa)); (51, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(da)); (20, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(z)); (13, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(k))"}
{"predict": "(36, MPI, if (rank == 0) {)", "label": "(34, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(25, MPI, MPI_Allreduce(&lenv, &SCT->commVolRed, 1, MPI_DOUBLE, MPI_SUM, grid->comm);)", "label": "(24, MPI, MPI_Send(unzval, lenv, MPI_DOUBLE, receiver, k, grid3d->zscp.comm);)"}
{"predict": "(1, CUDA, __global__ void Match9(float *d_pts1, float *d_pts2, float *d_score, int *d_index) {)", "label": "(5, CUDA, __shared__ float4 buffer2[M7H*NUM];); (3, CUDA, __shared__ float4 buffer1[M7W*NDIM/4];)"}
{"predict": "86:     int *src_dev, *dst_dev;\n87:     cudaMalloc(&src_dev, src_host.size() * sizeof(int));\n88:     cudaMalloc(&dst_dev, dst_host_expected.size() * sizeof(int));\n89: \n90:     cudaMemcpyAsync(src_dev, src_host.data(), src_host.size() * sizeof(int), cudaMemcpyHostToDevice, stream);\n91: \n92:     CUFFT_CHECK(cufftMpReshape(handle, src_dev, dst_dev, stream));\n93: \n94:     std::vector<int> dst_host;\n95:     cudaMemcpyAsync(dst_host.data(), dst_dev, dst_host_expected.size() * sizeof(int), cudaMemcpyDeviceToHost, stream);\n96: \n97:     cudaFree(src_dev);\n98:     cudaFree(dst_dev);\n99: \n100:     cufftMpDestroyReshape(handle);\n101: }", "label": "(15, CUDA, CUDA_CHECK(cudaGetDeviceCount(&ndevices));); (77, CUDA, CUDA_CHECK(cudaStreamCreate(&stream));); (5, MPI, MPI_Comm_size(comm, &size);); (119, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (12, CUDA, CUDA_CHECK(cudaSetDevice(rank % ndevices));); (-3, MPI, MPI_Init(&argc, &argv);); (90, MPI, MPI_Allreduce(&my_io_max, &global_io_max, 1, MPI_UNSIGNED_LONG_LONG, MPI_MAX, comm);); (116, CUDA, CUDA_CHECK(cudaMemcpy(dst_host.data(), dst, dst_host_expected.size() * sizeof(int), cudaMemcpyDefault));); (139, MPI, MPI_Finalize();); (118, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (94, CUDA, CUDA_CHECK(cudaMemsetAsync(dst, 0, dst_host_expected.size(), stream));); (92, CUDA, CUDA_CHECK(cudaMemcpyAsync(src, src_host.data(), src_host.size() * sizeof(int), cudaMemcpyDefault, stream));); (-6, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(35, OpenMP, #pragma omp parallel for private(s, n) reduction(+:global_count, inc_count))", "label": "(66, TBB, g.reset(tbb::flow::rf_reset_bodies);); (20, TBB, tbb::flow::make_edge( exe_node, *receivers[r] );); (58, TBB, inc_functor body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (64, TBB, body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (1, TBB, tbb::flow::graph g;); (45, TBB, tbb::flow::remove_edge( exe_node, *receivers[r] );); (24, TBB, tbb::flow::make_edge( *senders.back(), exe_node );); (4, TBB, tbb::flow::function_node< InputType, OutputType > exe_node( g, lc, cf );)"}
{"predict": "(12, MPI, self = PCUObj->Self();) (14, MPI, expandedPCU = PCUObj;) (26, MPI, m->switchPCU(groupedPCU.get());) (33, MPI, m->switchPCU(expandedPCU);)", "label": "(23, MPI, MPI_Comm_split(expandedPCU->GetMPIComm(), group, groupRank, &groupComm);); (31, MPI, MPI_Comm_free(&groupComm);)"}
{"predict": "(10, CUDA, HANDLE_ERROR(cudaMalloc((void**)&u_intermediate_dev, sizeof(float) * u_intermediate_host.size()))); (11, CUDA, HANDLE_ERROR(cudaMalloc((void**)&du_new_dev, sizeof(float) * du_new_host.size()))); (13, CUDA, HANDLE_ERROR(cudaMemcpy(u_intermediate_dev, u_intermediate_host.data(), sizeof(float) * u_intermediate_host.size(), cudaMemcpyHostToDevice))); (21, CUDA, HANDLE_ERROR(cudaMemcpy(du_new_host.data(), du_new_dev, sizeof(float) * du_new_host.size(), cudaMemcpyDeviceToHost))); (23, CUDA, cudaFree(u_intermediate_dev));", "label": "(19, CUDA, CudaCheckError();); (23, CUDA, cudaFree(du_new_dev);)"}
{"predict": "(20, OpenCL, props[1] = (cl_context_properties)platform;); (27, OpenCL, err = clGetDeviceIDs(platform, CL_DEVICE_TYPE_GPU, 1, &device, NULL);); (35, OpenCL, bufA = clCreateBuffer(ctx, CL_MEM_READ_ONLY, N * lda * sizeof(cl_float), NULL, &err);); (44, OpenCL, err = clEnqueueWriteBuffer(queue, bufA, CL_TRUE, 0, N * lda * sizeof(cl_float), A, 0, NULL, NULL);); (60, OpenCL, err = clblasSsbmv(order, uplo, N, K, alpha, bufA, 0, lda, bufX, 0, incx, beta, bufY, 0, incy, 1, &queue, 0, NULL, &event);); (70, OpenCL, err = clEnqueueReadBuffer(queue, bufY, CL_TRUE, 0, N * sizeof(cl_float), Y, 0, NULL, NULL);); (79, OpenCL, clReleaseMemObject(bufA);); (84, OpenCL, clReleaseCommandQueue(queue);); (85, OpenCL, clReleaseContext(ctx););", "label": "(87, OpenCL, clReleaseMemObject(bufX);); (43, OpenCL, clReleaseCommandQueue(queue);); (27, OpenCL, printf( \"clCreateContext() failed with %d\\n\", err );); (85, OpenCL, clReleaseMemObject(bufA);); (79, OpenCL, clReleaseEvent(event);); (30, OpenCL, printf( \"clCreateCommandQueue() failed with %d\\n\", err );); (27, OpenCL, queue = clCreateCommandQueue(ctx, device, 0, &err);); (29, OpenCL, clReleaseContext(ctx);); (78, OpenCL, clReleaseMemObject(bufY);); (18, OpenCL, ctx = clCreateContext(props, 1, &device, NULL, NULL, &err);); (63, OpenCL, err = clWaitForEvents(1, &event);)"}
{"predict": "0, i64 [[GRAINSIZE]], i8* null)\n44: // CHECK: call void @__kmpc_end_taskgroup(%struct.ident_t* [[DEFLOC]], i32 [[GTID]])\n45: // CHECK-NEXT:  call {{.*}}void @__kmpc_end_master(%struct.ident_t* [[DEFLOC]], i32 [[GTID]])\n46: // CHECK-NEXT:  br label {{%?}}[[EXIT]]\n47: // CHECK:       [[EXIT]]\n48:   for (int i = 0; i < 10; ++i)\n49:     ;\n50: }\n\nOutput:", "label": "(4, OpenMP, #pragma omp task); (25, OpenMP, #pragma omp master taskloop priority(argc)); (46, OpenMP, #pragma omp master taskloop); (71, OpenMP, #pragma omp master taskloop if(argc) shared(argc, argv) collapse(2) num_tasks(4)); (87, OpenMP, #pragma omp cancel taskgroup); (87, OpenMP, #pragma omp cancellation point taskgroup)"}
{"predict": "(23, CUDA, cudaEventCreate(&start);) (24, CUDA, cudaEventCreate(&stop);) (47, CUDA, cudaEventRecord(start, 0);) (52, CUDA, cudaEventRecord(stop, 0);) (54, CUDA, cudaEventSynchronize(stop);) (55, CUDA, cudaEventElapsedTime(&elapsedTime, start, stop);)", "label": "(67, CUDA, cudaFree(B);); (54, CUDA, cudaEventRecord(stop);); (54, CUDA, cudaEventSynchronize(stop);); (65, CUDA, cudaFree(C);); (44, CUDA, cudaEventRecord(start);); (29, CUDA, cudaGetDeviceCount(&num_of_total_devices);); (18, CUDA, cudaEventCreate(&stop);); (53, CUDA, cudaEventElapsedTime(&elapsedTime, start, stop);); (15, CUDA, cudaEventCreate(&start);); (57, CUDA, cudaFree(A);)"}
{"predict": "(23, CUDA, cudaMemcpyAsync(out.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) = cudaSuccess;)", "label": "(11, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (9, CUDA, cudaMalloc((void**)(&d_in1), in1_bytes);); (11, CUDA, cudaMemcpy(d_in1, in1.data(), in1_bytes, cudaMemcpyHostToDevice);); (31, CUDA, cudaFree(d_out);); (23, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (28, CUDA, cudaFree(d_in1);)"}
{"predict": "(12, OpenMP, #pragma omp parallel for reduction(+:v) reduction(+:i))", "label": "(17, OpenMP, #pragma omp parallel for simd linear (argc : 5) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (39, OpenMP, #pragma omp parallel for simd linear(j)); (41, OpenMP, #pragma omp parallel for simd linear(v:j)); (34, OpenMP, #pragma omp parallel for simd linear(v:i)); (27, OpenMP, #pragma omp parallel for simd linear(i, z)); (4, OpenMP, #pragma omp parallel for simd linear ()); (13, OpenMP, #pragma omp parallel for simd linear (S1)); (6, OpenMP, #pragma omp parallel for simd linear (argc,); (-3, OpenMP, #pragma omp parallel); (36, OpenMP, #pragma omp parallel for simd linear(i)); (17, OpenMP, #pragma omp parallel for simd linear(e, g)); (12, OpenMP, #pragma omp parallel for simd linear (a, b:B::ib)); (-1, OpenMP, #pragma omp parallel for simd linear (argc); (12, OpenMP, #pragma omp parallel for simd linear (argv[1])); (15, OpenMP, #pragma omp parallel for simd linear(h)); (-8, OpenMP, #pragma omp parallel for simd linear (); (-1, OpenMP, #pragma omp parallel for simd linear (argc > 0 ? argv[1] : argv[2]))"}
{"predict": "(9, OpenMP, #pragma omp parallel for private(pfs1, numpairsp1, defgradp1, p1, vol0p1, posp1, pos0p1, kercorrp1, pair, pfs2, p2, posp2, pos0p2, drx, dry, drz, drx0, dry0, drz0, rr20, fac0, frx0, fry0, frz0) reduction(+:defgradp1))", "label": "(8, OpenMP, #pragma omp parallel for schedule (guided))"}
{"predict": ").size() << std::endl;\n109:   // CHECK: custom_annotation1\n110:   std::cout << kernel_info5->get_string_annotations(1)[0] << std::endl;\n111: \n112:   // CHECK: 0\n113:   // CHECK: 1\n114:   // CHECK: 0\n115:   // CHECK: 1\n116:   std::cout << kernel_info6->get_string_annotations(0).size() << std::endl;\n117:   std::cout << kernel_info6->get_string_annotations(1).size() << std::endl;\n118:   std::cout << kernel_info6->get_known_annotations(0).size() << std::endl;\n119:   std::cout << kernel_info6->get_known_annotations(1).size() << std::endl;\n120:   // CHECK: custom_annotation1\n121:   std::cout << kernel_info6->get_string_annotations(1)[0] << std::endl;\n122: \n123:   // CHECK: 0\n124:   // CHECK: 0\n125:   // CHECK: 0\n126:   // CHECK: 0\n127:   std::cout << kernel_info1->get_string_annotations(2).size() << std::endl;\n128:   std::cout << kernel_info1->get_string_annotations(3).size() << std::endl;\n129:   std::cout << kernel_info1->get_known_annotations(2).size() << std::endl;\n130:   std::cout << kernel_info1->get_known_annotations(3).size() << std::endl;\n131: \n132:   // CHECK: 0\n133:   // CHECK: 0\n134:   // CHECK: 0\n135:   // CHECK: 0\n136:   std::cout << kernel_info2->get_string_annotations(2).size() << std::endl;\n137:   std::cout << kernel_info2->get_string_annotations(3).size() << std::endl;\n138:   std::cout << kernel_info2->get_known_annotations(2).size()", "label": "(9, SYCL, sycl::queue q = get_queue();); (36, SYCL, sycl::free(data, q);); (8, SYCL, int* data = sycl::malloc_device<int>(1,q);)"}
{"predict": "(3, CUDA, cudaGetDeviceProperties(&prop, i);)", "label": "(6, CUDA, HANDLE_ERROR( cudaGetDeviceProperties( &prop, i ) );); (3, CUDA, HANDLE_ERROR( cudaGetDeviceCount( &count ) );)"}
{"predict": "(27, OpenMP, #pragma omp parallel for reduction(+:nnzL) private(c, k, r, nmax))", "label": "(49, MPI, MPI_Allreduce(MPI_IN_PLACE,&n,1,mpi_int_t,MPI_MAX,grid->comm);); (47, MPI, MPI_Allreduce(MPI_IN_PLACE,&nnzL,1,mpi_int_t,MPI_SUM,grid->comm);)"}
{"predict": "1] = file_size / t[2];             /* read rate */\n88:   rates_l[2] = (file_size * 2) / (t[1] + t[2]); /* avg rate */\n89:   rates_l[3] = (file_size * 2) / (t[1] + t[2]); /* avg rate */\n90: \n91:   MPI_Allreduce(rates_l, rates_g, 4, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n92: \n93:   if (mype == 0) {\n94:     printf(\"  IO write rate: %f MB/s\\n\", rates_g[0]);\n95:     printf(\"  IO read rate: %f MB/s\\n\", rates_g[1]);\n96:     printf(\"  IO avg rate: %f MB/s\\n\", rates_g[2]);\n97:   }\n98: \n99:   MPI_Finalize();\n100: }", "label": "(17, MPI, MPI_Comm_size(MPI_COMM_WORLD,&totpes);); (39, MPI, MPI_Cart_create(MPI_COMM_WORLD,3,numpes,isperiodic,reorder,&comm_cart);); (39, MPI, MPI_Comm_rank(comm_cart,&mype);); (19, MPI, MPI_Finalize();); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&rank);); (72, MPI, MPI_Barrier(comm_cart);); (36, MPI, MPI_Cart_coords(comm_cart,mype,3,pe_coords);); (94, MPI, MPI_Allreduce(t, t_g, 20, MPI_DOUBLE, MPI_MAX, comm_cart);); (31, MPI, MPI_Dims_create(totpes,3,numpes);); (18, MPI, MPI_Bcast(filename, 256, MPI_CHAR, 0, MPI_COMM_WORLD);); (115, MPI, MPI_Allreduce(MPI_IN_PLACE, &nerrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (5, MPI, MPI_Init(&argc,&argv);); (102, MPI, MPI_Comm_free(&comm_cart);); (87, MPI, MPI_Allreduce(rates_l, rates_g, 4, MPI_DOUBLE, MPI_MIN, comm_cart);); (105, MPI, MPI_Reduce(&malloc_size, &sum_size, 1, MPI_OFFSET, MPI_SUM, 0, MPI_COMM_WORLD);)"}
{"predict": "(5, OpenMP, #pragma omp parallel for private(r, accept_count) reduction(+:c))", "label": "(10, TBB, tbb::flow::make_edge( lim, r );); (22, TBB, tbb::flow::limiter_node< T > lim_copy( lim );); (3, TBB, tbb::flow::graph g;); (3, TBB, tbb::flow::limiter_node< T > lim(g, i);); (4, TBB, tbb::atomic<int> accept_count;)"}
{"predict": "(14, OpenMP, #pragma omp parallel for collapse(2))", "label": "(15, OpenMP, #pragma omp loop); (19, OpenMP, #pragma omp target teams distribute parallel for map(MTX) reduction(*:EMTX)); (11, OpenMP, #pragma omp target teams loop map(MTX) reduction(*:MTX))"}
{"predict": "(27, OpenMP, #pragma omp parallel for private(part_id) reduction(+:phg.hypergraph().connectivity(he)))", "label": "(67, TBB, tbb::task_group tg;)"}
{"predict": "(20, OpenMP, counts_team = 0;)", "label": "(26, OpenMP, #pragma omp for reduction(+:counts_team)); (8, OpenMP, #pragma omp for); (15, OpenMP, #pragma omp target teams map(from:counts2)); (4, OpenMP, #pragma omp parallel); (0, OpenMP, #pragma omp target teams map(from:counts1)); (17, OpenMP, #pragma omp allocate(counts_team) allocator(omp_pteam_mem_alloc))"}
{"predict": "defined(_OPENMP)\n121:     #pragma omp parallel for\n122:     for (i=0; i<proc_length; i++) {\n123:       catstring[my_ID+i*Num_procs] = iterstring[i];\n124:     }\n125: #else\n126:     for (i=0; i<proc_length; i++) {\n127:       catstring[my_ID+i*Num_procs] = iterstring[i];\n128:     }\n129: #endif\n130: \n131:     /* Everybody sends own string to everybody else and concatenates */\n132: \n133:     /* Everybody sends own string to everybody else and concatenates */\n134: \n135:     /* Everybody sends own string to everybody else and concatenates */\n136: \n137:     /* Everybody sends own string to everybody else and concatenates */\n138: \n139:     /* Everybody sends own string to everybody else and concatenates */\n140: \n141:     /* Everybody sends own string to everybody else and concatenates */\n142: \n143:     /* Everybody sends own string to everybody else and concatenates */\n144: \n145:     /* Everybody sends own string to everybody else and concatenates */\n146: \n147:     /* Everybody sends own string to everybody else and concatenates */\n148: \n149:     /* Everybody sends own string to everybody else and concatenates */\n150: \n151:     /* Everybody sends own string to everybody else and concatenates */\n152: \n153:     /* Everybody sends own string to everybody else and concatenates */\n154: \n155:     /* Everybody sends own string to everybody else and concatenates */\n156: \n157:     /* Everybody sends own string to everybody else and concatenates */\n158: \n159:     /* Everybody sends own string to everybody else and concatenates */\n160: \n161:     /* Everybody sends own string to everybody else and concatenates */\n162: \n163:     /* Everybody sends own string to everybody else and concatenates */\n164: \n165:     /* Everybody sends own string to everybody else", "label": "(115, MPI, MPI_Type_commit(&mpi_word);); (122, MPI, MPI_Allgather(iterstring,1,mpi_word, catstring,1,mpi_word, MPI_COMM_WORLD);); (154, MPI, MPI_Finalize();); (63, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (110, MPI, MPI_Type_contiguous(proc_length,MPI_CHAR, &mpi_word);); (17, MPI, MPI_Init(&argc,&argv);); (61, MPI, MPI_Bcast(&length,     1, MPI_LONG, root, MPI_COMM_WORLD);); (110, MPI, MPI_Barrier(MPI_COMM_WORLD);); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(12, MPI, tstart=GET_TIME();)", "label": "(10, MPI, MPI_Barrier(MPI_COMM_WORLD);); (12, MPI, MPI_Allreduce(data,gr_comm_buf,n,mpi_bool,MPI_LAND,MPI_COMM_WORLD);)"}
{"predict": "(29, OpenMP, #pragma omp parallel for private(err) reduction(+:errs);)", "label": "(16, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(12, OpenMP, #pragma omp parallel for)", "label": "(42, TBB, tbb::flow::make_edge(buffer2, tbb::flow::input_port<1>(join));); (43, TBB, tbb::flow::make_edge(join, function);); (19, TBB, tbb::flow::join_node<tuple_type, tbb::flow::reserving> join(g);); (38, TBB, tbb::flow::make_edge(buffer1, tbb::flow::input_port<0>(join));); (-3, TBB, tbb::task_arena arena(1);); (-1, TBB, tbb::flow::graph g;); (11, TBB, tbb::flow::queue_node<int> buffer1(g);); (12, TBB, tbb::flow::queue_node<int> buffer3(g);); (35, TBB, tbb::flow::make_edge(buffer3, tbb::flow::input_port<2>(join));); (9, TBB, tbb::flow::queue_node<int> buffer2(g);)"}
{"predict": "(11, MPI, MPI_Init(&argc, &argv);) (33, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);) (34, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);) (37, MPI, MPI_Reduce(&localpi, &pi, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);) (46, MPI, MPI_Finalize();)", "label": "(42, MPI, MPI_Recv(&localpi, 1, MPI_DOUBLE, MPI_ANY_SOURCE, 0, MPI_COMM_WORLD, &status);); (47, MPI, MPI_Send(&localpi, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);); (4, MPI, MPI_Init(&argc, &argv);); (48, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(5, CUDA, CP_host->GPUSetup();)", "label": "(13, CUDA, CP_host->freeCudaMem();)"}
{"predict": "(5, OpenMP, #pragma omp parallel for private(k, sse_mode))", "label": "(12, TBB, tbb::parallel_for( 0, N, 1, RoundingModeCheckBody(mode, sse_mode) );); (10, TBB, tbb::task_scheduler_init init;)"}
{"predict": "(16, OpenMP, #pragma omp parallel for) (19, OpenMP, #pragma omp parallel for) (27, OpenMP, #pragma omp parallel for) (31, OpenMP, #pragma omp parallel for) (35, OpenMP, #pragma omp parallel for) (42, OpenMP, #pragma omp parallel for) (44, OpenMP, #pragma omp parallel for)", "label": "(13, OpenMP, fprintf(stderr, \"#pragma omp target teams distribute thread_limit(thdLim) 27\\n\");); (43, OpenMP, #pragma omp target teams distribute thread_limit(1024)); (22, OpenMP, fprintf(stderr, \"#pragma omp target teams distribute thread_limit(thdLim) 1024\\n\");); (31, OpenMP, #pragma omp target teams distribute thread_limit(27)); (34, OpenMP, constLimit[i] = omp_get_num_threads();); (9, OpenMP, #pragma omp target teams distribute thread_limit(thdLim)); (42, OpenMP, constLimitHuge[i] = omp_get_num_threads();); (21, OpenMP, varLimitHuge[i] = omp_get_num_threads();); (9, OpenMP, varLimit[i] = omp_get_num_threads();)"}
{"predict": "(13, CUDA, if (prop.major < 2) { printf(\"This application requires SM 2.x (for surface load/store)\\n\"); goto Error; } else { cudaMalloc((void **)&array, NUM_VALUES * sizeof(float)); })", "label": "(50, CUDA, cudaFreeHost( foutHost );); (27, CUDA, CUDART_CHECK(surf1Dmemset( array, 3.141592654f, 0, NUM_VALUES ));)"}
{"predict": "RangePop(); //Pop for Entropy Calculation\n126:     cout << \"Entropy Calculation Completed\" << endl;\n127:     cout << \"s2 = \" << s2 << \" s2bond = \" << s2bond << endl;\n128:     cout << \"RDF Calculation Completed\" << endl;\n129:     cout << \"Pair Entropy Calculation Completed\" << endl;\n130:     pairfile.close();\n131:     stwo.close();\n132:     free(h_x);\n133:     free(h_y);\n134:     free(h_z);\n135:     free(h_g2);\n136:     return 0;\n137: }", "label": "(72, OpenMP, #pragma omp target data map(h_x [0:nconf * numatm], h_y [0:nconf * numatm], h_z [0:nconf * numatm], h_g2 [0:nbin]))"}
{"predict": "(6, OpenMP, #pragma omp parallel for private(index, af_decrypted, i, dklen, keycandidate, masterkeycandidate, lens, pin, x))", "label": "(6, OpenMP, #pragma omp parallel for)"}
{"predict": "(4, OpenMP, #pragma omp parallel for)", "label": "(4, SYCL, int* data = sycl::malloc_shared<int>(7, q);); (19, SYCL, hipsycl::rt::runtime_keep_alive_token rt;); (1, SYCL, sycl::queue q = get_queue();); (67, SYCL, sycl::free(data, q);)"}
{"predict": "(24, MPI, MPI_Bcast(vecin, n * stride, MPI_DOUBLE, root, comm, &err);)", "label": "(13, MPI, MPI_Comm_size( comm, &size );); (22, MPI, MPI_Type_commit( &vec );); (26, MPI, MPI_Gather( vecin, 1, vec, vecout, n, MPI_DOUBLE, root, comm );); (46, MPI, MPI_Finalize();); (5, MPI, MPI_Init( &argc, &argv );); (35, MPI, MPI_Type_free( &vec );); (6, MPI, MPI_Comm_rank( comm, &rank );); (15, MPI, MPI_Type_vector( n, 1, stride, MPI_DOUBLE, &vec );); (39, MPI, MPI_Gather( NULL, 0, MPI_BYTE, NULL, 0, MPI_BYTE, 0, MPI_COMM_WORLD );)"}
{"predict": "(14, MPI, MPI_Cart_create(MPI_COMM_WORLD, NR_DIMS, dims, periodic, reorder, &cart_comm);)", "label": "(34, OpenMP, #pragma omp for reduction(+:sum)); (32, OpenMP, printf(\"rank %d:%d has %d threads\\n\", rank, thread_num, omp_get_num_threads());); (11, MPI, MPI_Init(&argc, &argv);); (42, MPI, MPI_Finalize();); (26, OpenMP, #pragma omp parallel); (36, MPI, MPI_Reduce(&sum, &avg, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (25, OpenMP, int thread_num = omp_get_thread_num();); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (7, MPI, MPI_Dims_create(size, NR_DIMS, dims);); (13, MPI, MPI_Cart_coords(cart_comm, rank, NR_DIMS, cart_coords);); (10, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(13, OpenCL, size_t res1 = k.getSubGroupInfo<CL_KERNEL_MAX_SUB_GROUP_SIZE_FOR_NDRANGE_KHR>(d, ndrange, &err);) (17, OpenCL, err = k.getSubGroupInfo(d, CL_KERNEL_SUB_GROUP_COUNT_FOR_NDRANGE_KHR, ndrange, &res2);)", "label": "(8, OpenCL, clReleaseKernel_ExpectAndReturn(make_kernel(0), CL_SUCCESS);); (6, OpenCL, clReleaseDevice_ExpectAndReturn(make_device_id(0), CL_SUCCESS);)"}
{"predict": "(15, CUDA, auto p_A = thrust::raw_pointer_cast(A.data());) (16, CUDA, auto p_x = thrust::raw_pointer_cast(x.data());) (17, CUDA, auto p_y = thrust::raw_pointer_cast(y.data());) (24, CUDA, y_i += p_A[i + j * N] * p_x[j];) (27, CUDA, p_y[i] = y_i;)", "label": "(40, CUDA, CUDA_SAFE_CALL(cudaFree(x.data()));); (38, CUDA, CUDA_SAFE_CALL(cudaFree(A.data()));); (36, CUDA, CUDA_SAFE_CALL(cudaDeviceSynchronize());); (38, CUDA, CUDA_SAFE_CALL(cudaFree(y.data()));)"}
{"predict": "(22, MPI, *w = MPI_Wtime();)", "label": "(22, MPI, *w = MPI_Wtime();)"}
{"predict": "(1, CUDA, __global__ void reduceTT(r_Ptr<float> sums, cr_Ptr<half> data, int n) {)", "label": "(2, CUDA, extern __shared__ half  hs[][256];)"}
{"predict": "(6, OpenMP, #pragma omp parallel for)", "label": "(7, TBB, tbb::task_scheduler_init init( p );)"}
{"predict": "(17, CUDA, for (unsigned int i = 0; i < AGENT_COUNT; i++) { __syncthreads(); })\n(18, CUDA, AgentVector::Agent instance = population[i];)\n(19, CUDA, instance.setVariable<float>(\"x\", i + 1.0f);)\n(20, CUDA, instance.setVariable<unsigned int>(\"id\", i);)\n(22, CUDA, cudaSimulation.setPopulationData(population);)\n(24, CUDA, cudaSimulation.step();)\n(26, CUDA, cudaSimulation.getPopulationData(population);)\n(29, CUDA, for (unsigned int i = 0; i < population.size(); ++i) { __syncthreads(); })\n(36, CUDA, AgentVector::Agent ai = population[i];)\n(37, CUDA, float val = ai.getVariable<float>(\"x\") - ai.getVariable<unsigned int>(\"id\");)\n(38, CUDA, if (val == 1.0f) { __syncthreads(); })\n(40, CUDA, if (ai.getVariable<unsigned int>(\"id\") % 2 == 0) { __syncthreads(); })\n(42, CUDA, } else { __syncthreads(); })\n(44, CUDA, } else if (val == 12.0f) { __syncthreads(); })\n(46, CUDA, ASSERT_EQ(ai.getVariable<unsigned int>(\"id\") % 4, 1u);)\n(48, CUDA, } else { __syncthreads(); })\n(50, CUDA, } __syncthreads(); }", "label": "(13, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(17, MPI, MPI_Win_allocate_shared(sizeof(int) * ELEM_PER_PROC, sizeof(int), MPI_INFO_NULL, shm_comm, &my_base, &shm_win);) (24, MPI, if (disp_unit != sizeof(int)) { errors++; }) (26, MPI, if (size != ELEM_PER_PROC * sizeof(int)) { errors++; }) (28, MPI, if ((shm_rank == 0) && (base != my_base)) { errors++; }) (31, MPI, if (shm_rank && (base == my_base)) { errors++; }) (40, MPI, my_base[i] = i;) (48, MPI, if (base[i * ELEM_PER_PROC + j] != j) { errors++; printf(\"%d -- Got %d at rank %d index %d, expected %d\\n\", shm_rank, base[i * ELEM_PER_PROC + j], i, j, j); })", "label": "(18, MPI, MPI_Comm_rank(shm_comm, &shm_rank);); (41, MPI, MPI_Win_lock_all(MPI_MODE_NOCHECK, shm_win);); (24, MPI, MPI_Win_shared_query(shm_win, MPI_PROC_NULL, &size, &disp_unit, &base);); (13, MPI, MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, rank, MPI_INFO_NULL, &shm_comm);); (10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (58, MPI, MPI_Win_unlock_all(shm_win);); (58, MPI, MPI_Win_free(&shm_win);); (58, MPI, MPI_Comm_free(&shm_comm);); (10, MPI, MPI_Comm_size(shm_comm, &shm_nproc);); (40, MPI, MPI_Barrier(shm_comm);); (38, MPI, MPI_Win_sync(shm_win);)"}
{"predict": "(46, CUDA, auto host_a = example::get_random_data<TA>(0.1, 1.0, global_a_size);)\n(47, CUDA, auto host_b = example::get_random_data<TB>(0.1, 1.0, global_b_size);)\n(48, CUDA, auto host_c = example::get_random_data<TC>(0.1, 1.0, global_c_size);)\n(55, CUDA, gemm_kernel<BLAS><<<1, BLAS::block_dim, cublasdx::get_shared_storage_size<BLAS>()>>>(a, b, c, alpha, beta);)\n(63, CUDA, cudaFree(a);)\n(64, CUDA, cudaFree(b);)\n(65, CUDA, cudaFree(c);)", "label": "(41, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&b, global_b_size * sizeof(TB)));); (71, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(c));); (68, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(a));); (37, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&a, global_a_size * sizeof(TA)));); (47, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(a, host_a.data(), global_a_size * sizeof(TA), cudaMemcpyHostToDevice));); (49, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (47, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(c, host_c.data(), global_a_size * sizeof(TC), cudaMemcpyHostToDevice));); (45, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(b, host_b.data(), global_a_size * sizeof(TB), cudaMemcpyHostToDevice));); (58, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(host_output.data(), c, global_c_size * sizeof(TC), cudaMemcpyDeviceToHost));); (33, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&c, global_c_size * sizeof(TC)));); (61, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(b));)"}
{"predict": "(14, CUDA, unsigned char *idata = (unsigned char *) malloc( padWidth*padHeight );)\n(21, CUDA, cudaMallocPitch( (void **) &ddata, &dPitch, padWidth, padHeight );)\n(48, CUDA, if ( (size_t) w != fread( idata+row*padWidth, 1, w, fp ) ) goto Error;)\n(52, CUDA, cudaMallocPitch( (void **) &ddata, &dPitch, padWidth, padHeight );)\n(56, CUDA, *pDeviceData = ddata;)\n(60, CUDA, cudaFree(ddata);)", "label": "(62, CUDA, cudaFree( ddata );)"}
{"predict": "(8, OpenMP, #pragma omp parallel for private(tXferGpu2Host))\n(16, OpenMP, #pragma omp parallel for private(tXferGpu2Host))", "label": "(10, CUDA, cudaStreamSynchronize(LU_v1->A_gpu.cuStreams[0]);)"}
{"predict": "(5, OpenMP, #pragma omp parallel for)", "label": "(2, OpenMP, omp_autotune(self, OMP_SCALE);)"}
{"predict": "(16, CUBLAS, cublasSgemmBatched(handle, CUBLAS_OP_N, CUBLAS_OP_N, B_COL, A_ROW, A_COL, &alpha, dB_buff, B_COL, dA_buff, A_COL, &beta, dC_buff, B_COL, batch_num);)", "label": "(18, CUDA, cudaMemcpy(dB_buff, B_buff, sizeof(float*) * batch_num, cudaMemcpyHostToDevice);); (12, CUDA, cudaMalloc(&dA_buff, sizeof(float*) * batch_num);); (31, CUDA, cudaFree(dC_buff);); (29, CUDA, cudaFree(dB_buff);); (27, CUDA, cudaFree(dA_buff);); (12, CUDA, cudaMemcpy(dA_buff, A_buff, sizeof(float*) * batch_num, cudaMemcpyHostToDevice);); (8, CUDA, cudaMalloc(&dB_buff, sizeof(float*) * batch_num);); (12, CUDA, cudaMemcpy(dC_buff, C_buff, sizeof(float*) * batch_num, cudaMemcpyHostToDevice);); (7, CUDA, cudaMalloc(&dC_buff, sizeof(float*) * batch_num);)"}
{"predict": "(6, CUDA, cudaMallocManaged(&buffer, length * sizeof(T));) (11, CUDA, cudaFree(buffer);)", "label": "(7, CUDA, cudaMalloc((void **)&buffer_fp32, length * sizeof(float));); (9, CUDA, cudaMalloc((void **)&buffer, length * sizeof(T));); (10, CUDA, cudaFree(buffer_fp32);)"}
{"predict": "(10, OpenMP, #pragma omp teams distribute parallel for if(cond) firstprivate(fp) reduction(+: rd);)", "label": "(16, OpenMP, #pragma omp target); (22, OpenMP, #pragma omp teams distribute parallel for if(cond) firstprivate(fp) reduction(+:rd))"}
{"predict": "(12, CUDA, KerReduUintLimits<DIVBSIZE><<<sgrid, DIVBSIZE, smemSize>>>(n, dat, res);)", "label": "(19, CUDA, cudaMemcpy(resf,dat,sizeof(unsigned)*2,cudaMemcpyDeviceToHost);)"}
{"predict": "(16, CUDA, cudaGetDeviceProperties(&props, 0);) (17, CUDA, cudaMallocArray(&texArray, &channelDesc, Width, Height, cudaArraySurfaceLoadStore);) (31, CUDA, Bandwidth = Width*Height*sizeof(T) / (ms/1000.0) / 1e9;)", "label": "(29, CUDA, CUDART_CHECK(surf2DmemsetArray_time( &ms, texArray, 3.141592654f, threadWidth, threadHeight ));); (50, CUDA, cudaFreeArray( texArray );)"}
{"predict": "153:     if (checkCmdLineFlag(argc, argv, \"dtod\"))\n154:     {\n155:         dtod = true;\n156:     }\n157: \n158:     if (checkCmdLineFlag(argc, argv, \"wc\"))\n159:     {\n160:         wc = true;\n161:     }\n162: \n163:     //run the test\n164:     runTestForAllDevices(start, end, increment, mode, htod, dtoh, dtod, wc, printmode, memMode);\n165:     return 0;\n166: }", "label": "(267, CUDA, cudaSetDevice(nDevice);); (108, CUDA, printf(\"cudaGetDeviceProperties returned %d\\n-> %s\\n\", (int)error_id, cudaGetErrorString(error_id));); (54, CUDA, cudaError_t error_id = cudaGetDeviceCount(&deviceCount);); (98, CUDA, fprintf(stderr, \"Error: device is running in <Compute Mode Prohibited>, no threads can use ::cudaSetDevice().\\n\");); (56, CUDA, printf(\"cudaGetDeviceCount returned %d\\n-> %s\\n\", (int)error_id, cudaGetErrorString(error_id));); (88, CUDA, cudaError_t error_id = cudaGetDeviceProperties(&deviceProp, currentDevice);); (96, CUDA, checkCudaErrors(cudaSetDevice(currentDevice));)"}
{"predict": "(12, CUDA, cache[lane] = value;)\n(17, CUDA, if(lane < i) value += cache[lane + i];)", "label": "(5, CUDA, extern __shared__ float cache[];)"}
{"predict": "(15, OpenMP, #pragma omp parallel for)", "label": "(77, OpenMP, #pragma omp parallel num_threads(NumThreads1)); (14, OpenMP, #pragma omp parallel); (26, OpenMP, int nestedThreadNum = omp_get_thread_num();); (7, OpenMP, inParallel = omp_in_parallel();); (22, OpenMP, int nestedNumThreads = omp_get_num_threads();); (19, OpenMP, int nestedInParallel = omp_in_parallel();); (12, OpenMP, check1[id] += omp_get_num_threads() + omp_in_parallel();); (4, OpenMP, numThreads = omp_get_num_threads();); (21, OpenMP, #pragma omp atomic); (3, OpenMP, threadNum = omp_get_thread_num();); (-2, OpenMP, #pragma omp target map(inParallel, numThreads, threadNum, check1[:], check2[:])); (6, OpenMP, int id = omp_get_thread_num();)"}
{"predict": "(13, CUDA, dim3 grid(blocks, 1); dim3 block(threads, 1); gpu_sin<<<grid, block>>>(dptr, steps, terms, (float)step_size);)", "label": "(11, CUDA, float *dptr = thrust::raw_pointer_cast(&dsums[0]);); (14, CUDA, double gpu_sum = thrust::reduce(dsums.begin(),dsums.end());); (8, CUDA, thrust::device_vector<float> dsums(steps);)"}
{"predict": "(17, OpenMP, #pragma omp parallel for private(fat1) reduction(+:one_link)) (22, OpenMP, #pragma omp parallel for private(staple)) (27, OpenMP, #pragma omp parallel for private(staple)) (30, OpenMP, #pragma omp parallel for private(tempmat1)) (35, OpenMP, #pragma omp parallel for)", "label": "(12, OpenMP, #pragma omp parallel for)"}
{"predict": "(13, TBB, my_thread_array = static_cast<padded_private_worker*>(aligned_alloc(my_stack_size, my_n_thread * sizeof(padded_private_worker)));)", "label": "(12, TBB, my_thread_array = tbb::cache_aligned_allocator<padded_private_worker>().allocate( my_n_thread );)"}
{"predict": "(19, CUDA, cudaMalloc((void**)&d_inPtr, size * sizeof(float));) (20, CUDA, cudaMalloc((void**)&d_outPtr, size * sizeof(float));) (36, CUDA, cudaFree(d_inPtr);) (37, CUDA, cudaFree(d_outPtr);)", "label": "(40, CUDA, cudaFree(d_inPtr);); (24, CUDA, cudaMalloc((void**)&d_outPtr, size * sizeof(float));); (30, CUDA, cudaMemcpy(&result_gpu, &d_outPtr[0], sizeof(float), cudaMemcpyDeviceToHost);); (24, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);); (35, CUDA, cudaFree(d_outPtr);); (19, CUDA, cudaMalloc((void**)& d_inPtr, size * sizeof(float));)"}
{"predict": "cudaDeviceSynchronize: 同步主机线程和设备线程,等待所有设备上的任务\n66: \t// 完成,再继续执行主机线程中的后续代码\n67: \terr = cudaDeviceSynchronize();\n68: \tif (err != cudaSuccess) PRINT_ERROR_INFO(cudaDeviceSynchronize);\n69: \t// cudaEventDestroy: 销毁一个事件对象,释放与之关联的资源\n70: \tcudaEventDestroy(start);\n71: \tcudaEventDestroy(stop);\n72: \t// cudaFree: 在设备端释放内存\n73: \tcudaFree(d_A);\n74: \tcudaFree(d_B);\n75: \tcudaFree(d_C);\n76: \treturn 0;\n77: }\n\nAssistant\n(7, CUDA, err = cudaEventCreate(&start);) (8, CUDA, err = cudaEventCreate(&stop);) (11, CUDA, cudaEventRecord(start, 0);) (43, CUDA, vector_add<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, numElements);) (46, CUDA, cudaEventRecord(stop, 0);) (48, CUDA, cudaEventSynchronize(stop);) (49, CUDA, float milliseconds = 0; cudaEventElapsedTime(&milliseconds, start, stop);) (51, CUDA, *elapsed_time = milliseconds;)", "label": "(88, CUDA, cudaEventSynchronize(stop);); (70, CUDA, err = cudaGetLastError();); (19, CUDA, err = cudaMalloc(&d_A, length);); (12, CUDA, cudaEventRecord(start, 0);); (78, CUDA, err = cudaFree(d_C);); (87, CUDA, cudaEventDestroy(start);); (74, CUDA, err = cudaFree(d_B);); (21, CUDA, err = cudaMalloc(&d_B, length);); (78, CUDA, cudaEventRecord(stop, 0);); (32, CUDA, err = cudaMemcpy(d_A, A, length, cudaMemcpyHostToDevice);); (80, CUDA, cudaEventElapsedTime(elapsed_time, start, stop);); (1, CUDA, cudaEventCreate(&start);); (66, CUDA, err = cudaFree(d_A);); (30, CUDA, err = cudaMemcpy(d_B, B, length, cudaMemcpyHostToDevice);); (16, CUDA, err = cudaMalloc(&d_C, length);); (78, CUDA, cudaEventDestroy(stop);); (-3, CUDA, cudaEventCreate(&stop);); (57, CUDA, err = cudaMemcpy(C, d_C, length, cudaMemcpyDeviceToHost);)"}
{"predict": "(3, OpenMP, #pragma omp parallel for)", "label": "(16, OpenMP, #pragma omp target defaultmap (scalar:); (3, OpenMP, #pragma omp target defaultmap(); (10, OpenMP, #pragma omp target defaultmap(tofrom)); (11, OpenMP, #pragma omp target defaultmap(tofrom,); (16, OpenMP, #pragma omp target defaultmap(tofrom:scalar) defaultmap(tofrom:scalar)); (-3, OpenMP, #pragma omp target defaultmap); (19, OpenMP, #pragma omp target defaultmap(tofrom: something)); (11, OpenMP, #pragma omp target defaultmap(tofrom, scalar); (2, OpenMP, #pragma omp target defaultmap (tofrom:); (-3, OpenMP, #pragma omp target defaultmap()); (-2, OpenMP, #pragma omp target defaultmap(tofrom)"}
{"predict": "(29, CUDA, CUDA_CALL(hipDeviceSynchronize());)", "label": "(52, CUDA, CUDA_CALL(cudaFree(data));); (28, CUDA, CUDA_CALL(cudaDeviceSynchronize());); (6, CUDA, CUDA_CALL(cudaMalloc((void**)&data, size * sizeof(T)));)"}
{"predict": "(44, MPI, MPI_Alltoallv(sbuf, sendcounts, sdispls, MPI_INT, rbuf, recvcounts, rdispls, MPI_INT, comm);)", "label": "(29, MPI, MPI_Abort(comm, 1);); (15, MPI, MPI_Comm_size(comm, &size);); (15, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(26, OpenMP, #pragma omp for)", "label": "(25, OpenMP, #pragma omp parallel sections shared(argv[1])); (34, OpenMP, #pragma omp parallel sections shared(h, B::x)); (41, OpenMP, #pragma omp parallel sections shared(i)); (20, OpenMP, #pragma omp parallel sections shared(a, b, c, d, f, k)); (9, OpenMP, #pragma omp parallel sections shared(argc); (10, OpenMP, #pragma omp parallel sections shared(argc,); (33, OpenMP, #pragma omp parallel sections firstprivate(i)); (22, OpenMP, #pragma omp parallel sections shared(ca)); (19, OpenMP, #pragma omp parallel sections shared(ba)); (28, OpenMP, #pragma omp parallel sections private(i)); (-3, OpenMP, #pragma omp parallel sections shared); (20, OpenMP, #pragma omp parallel sections shared(da)); (5, OpenMP, #pragma omp parallel sections shared(argc > 0 ? argv[1] : argv[2])); (8, OpenMP, #pragma omp parallel sections shared(S1)); (5, OpenMP, #pragma omp parallel sections shared(argc)); (30, OpenMP, #pragma omp parallel sections shared(j)); (-5, OpenMP, #pragma omp parallel sections shared()); (-8, OpenMP, #pragma omp parallel sections shared(); (15, OpenMP, #pragma omp parallel sections shared(e, g))"}
{"predict": "(49, MPI, MPI_Comm_rank(comm, rank);)", "label": "(45, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(29, MPI, MPI_Scatter(send_buffer.data(), 2, MPI_DOUBLE, recv_buffer.data(), 2, MPI_DOUBLE, send_rank, MPI_COMM_WORLD);)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(26, OpenMP, #pragma omp parallel for private(i))", "label": "(25, OpenMP, #pragma omp parallel for)"}
{"predict": "(2, CUDA, __global__ void LayerNormFusedBackwardKernel_Data(const int nbatch, const int nchannel, const float* __restrict__ in_data, const float* __restrict__ out_grad, const float* __restrict__ mean_data, const float* __restrict__ std_data, const float* __restrict__ gamma, float* data_grad, const int LOAD_UNROLL) {)", "label": "(13, CUDA, extern __shared__ char buf[];)"}
{"predict": "", "label": "(66, MPI, MPI_Comm_size(fd->comm, &nprocs);); (66, MPI, MPI_Comm_rank(fd->comm, &myrank);)"}
{"predict": "(9, MPI, MPI_Bcast(&dtp_args, sizeof(dtp_args), MPI_BYTE, 0, MPI_COMM_WORLD);)", "label": "(6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &world_size);)"}
{"predict": "argc; ++i)\n104:     foo();\n105: #pragma omp target\n106: #pragma omp teams\n107:   for (i = 0; i < argc; ++i)\n108:     foo();\n109: #pragma omp target\n110: #pragma omp teams\n111:   for (i = 0; i < argc; ++i)\n112:     foo();\n113: #pragma omp target\n114: #pragma omp teams\n115:   for (i = 0; i < argc; ++i)\n116:     foo();\n117: #pragma omp target\n118: #pragma omp teams\n119:   for (i = 0; i < argc; ++i)\n120:     foo();\n121: #pragma omp target\n122: #pragma omp teams\n123:   for (i = 0; i < argc; ++i)\n124:     foo();\n125: #pragma omp target\n126: #pragma omp teams\n127:   for (i = 0; i < argc; ++i)\n128:     foo();\n129: #pragma omp target\n130: #pragma omp teams\n131:   for (i = 0; i < argc; ++i)\n132:     foo();\n133: #pragma omp target\n134: #pragma omp teams\n135:   for (i = 0; i < argc; ++i)\n136:     foo();\n137: #pragma omp target\n138: #pragma omp teams\n139:   for (i = 0; i < argc; ++i)\n140:     foo();\n141: #pragma omp target\n142: #pragma omp teams\n143:   for (i = 0; i < argc; ++i)\n144:     foo();\n145: #pragma omp target\n146: #pragma omp teams\n147:   for (i = 0; i < argc; ++i)\n148:     foo();\n149: #pragma omp target\n150: #pragma omp", "label": "(82, OpenMP, #pragma omp distribute parallel for lastprivate(xa)); (15, OpenMP, #pragma omp distribute parallel for lastprivate(); (59, OpenMP, #pragma omp distribute parallel for lastprivate(2 * 2)); (73, OpenMP, #pragma omp distribute parallel for lastprivate(da)); (93, OpenMP, #pragma omp distribute parallel for safelen(5)); (137, OpenMP, #pragma omp distribute parallel for lastprivate(j)); (25, OpenMP, #pragma omp distribute parallel for lastprivate(argc,); (2, OpenMP, #pragma omp target); (109, OpenMP, #pragma omp distribute parallel for lastprivate(B::x)); (62, OpenMP, #pragma omp distribute parallel for lastprivate(ca)); (138, OpenMP, #pragma omp distribute parallel for firstprivate(m) lastprivate(m)); (116, OpenMP, #pragma omp distribute parallel for lastprivate(i)); (142, OpenMP, #pragma omp distribute parallel for lastprivate(n) firstprivate(n)); (94, OpenMP, #pragma omp distribute parallel for lastprivate(m)); (32, OpenMP, #pragma omp distribute parallel for lastprivate(S1)); (97, OpenMP, #pragma omp distribute parallel for lastprivate(h)); (-5, OpenMP, #pragma omp distribute parallel for lastprivate); (39, OpenMP, #pragma omp distribute parallel for lastprivate(argv[1])); (18, OpenMP, #pragma omp distribute parallel for lastprivate(argc > 0 ? argv[1] : argv[2])); (32, OpenMP, #pragma omp distribute parallel for lastprivate(a, b, c, d, f)); (140, OpenMP, #pragma omp distribute parallel for lastprivate(si)); (20, OpenMP, #pragma omp distribute parallel for lastprivate(argc)); (80, OpenMP, #pragma omp distribute parallel for lastprivate(e, g)); (3, OpenMP, #pragma omp distribute parallel for lastprivate(argc); (42, OpenMP, #pragma omp distribute parallel for lastprivate(ba)); (62, OpenMP, #pragma omp distribute parallel for lastprivate(S2::S2s)); (66, OpenMP, #pragma omp distribute parallel for lastprivate(S2::S2sc)); (95, OpenMP, #pragma omp distribute parallel for private(xa), lastprivate(xa)); (-7, OpenMP, #pragma omp distribute parallel for lastprivate()); (-19, OpenMP, #pragma omp teams)"}
{"predict": "20:     \n121:     /* flush cache line after last iteration */\n122:     if (iter == iterations) { \n123:       local_nstream_time = wtime();\n124:     }\n125:   }\n126:  \n127:   avgtime = local_nstream_time/(double)iterations;\n128:   if (my_ID == root) {\n129:     printf(\"Function    Best Rate MB/s  Avg time     Min time     Max time\\n\");\n130:     printf(\"Triad:      %11.4f  %11.4f  %11.4f  %11.4f\\n\", 1.0E-06 * bytes/avgtime,\n131:            avgtime, avgtime, avgtime);\n132:   }\n133:   bail_out(error);\n134:  \n135:   /* --- Clean-up --- */\n136:   free(a);\n137:   if (my_ID == root) {\n138:     printf(\"Total time: %f seconds\\n\", wtime());\n139:   }\n140:   return 0;\n141: }", "label": "(101, OpenMP, printf(\"Number of threads    = %d\\n\", omp_get_max_threads());); (24, MPI, MPI_Comm_size(MPI_COMM_WORLD,&Num_procs);); (148, MPI, MPI_Finalize();); (104, OpenMP, #pragma omp parallel for); (74, MPI, MPI_Bcast(&length,        1, MPI_LONG, root, MPI_COMM_WORLD);); (21, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&my_ID);); (75, MPI, MPI_Bcast(&nthread_input, 1, MPI_INT,  root, MPI_COMM_WORLD);); (76, OpenMP, omp_set_num_threads(nthread_input);); (16, MPI, MPI_Init(&argc,&argv);); (71, MPI, MPI_Bcast(&iterations,    1, MPI_INT,  root, MPI_COMM_WORLD);); (113, MPI, MPI_Barrier(MPI_COMM_WORLD);); (68, MPI, MPI_Bcast(&offset,        1, MPI_LONG, root, MPI_COMM_WORLD);)"}
{"predict": "(13, MPI, MTest_Init(&argc, &argv);) (14, MPI, parse_args(argc, argv);) (21, MPI, err = builtin_float_test();) (26, MPI, err = vector_of_vectors_test();) (29, MPI, err = optimizable_vector_of_basics_test();) (34, MPI, err = indexed_of_basics_test();) (39, MPI, err = indexed_of_vectors_test();) (44, MPI, #ifdef HAVE_MPI_TYPE_CREATE_STRUCT) (45, MPI, err = struct_of_basics_test();) (46, MPI, #endif) (53, MPI, MTest_Finalize(errs);) (54, MPI, return MTestReturnValue(errs);)", "label": "(18, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(33, OpenMP, #pragma omp parallel for)", "label": "(9, OpenMP, #pragma omp target); (10, OpenMP, #pragma omp distribute simd firstprivate(g, g1, svar, sfvar)); (46, OpenMP, #pragma omp distribute simd firstprivate(t_var, vec, s_arr, s_arr, var, var, svar)); (7, OpenMP, #pragma omp teams)"}
{"predict": "(28, CUDA, cub::BlockRadixSort<custom_t, 2, 3, int> temp_storage;); (29, CUDA, temp_storage.SortBlockedToStriped(thread_keys[threadIdx.x], thread_values[threadIdx.x], decomposer_t{});)", "label": "(7, CUDA, __shared__ block_radix_sort_t::TempStorage temp_storage;)"}
{"predict": "(16, CUDA, __syncthreads();)", "label": "(12, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(15, CUDA, cudaMalloc((void**)&d_inPtr, size * sizeof(float));) (26, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);) (29, CUDA, cudaMemcpy(h_inPtr, d_outPtr, size * sizeof(float), cudaMemcpyDeviceToHost);) (30, CUDA, cudaFree(d_inPtr);) (30, CUDA, cudaFree(d_outPtr);)", "label": "(33, CUDA, cudaFree(d_inPtr);); (21, CUDA, cudaMalloc((void**)&d_outPtr, size * sizeof(float));); (26, CUDA, cudaMemcpy(&result_gpu, &d_outPtr[0], sizeof(float), cudaMemcpyDeviceToHost);); (21, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);); (28, CUDA, cudaFree(d_outPtr);); (16, CUDA, cudaMalloc((void**)& d_inPtr, size * sizeof(float));)"}
{"predict": "(10, TBB, my_broadcast_node.try_put(v);)", "label": "(3, TBB, tbb::flow::broadcast_node<my_type> my_broadcast_node(g);); (7, TBB, const tbb::tick_count t0 = tbb::tick_count::now();); (14, TBB, return (tbb::tick_count::now() - t0).seconds();); (-1, TBB, tbb::flow::queue_node<my_type> my_queue(g);)"}
{"predict": "(26, MPI, MPI_Allreduce(&err, &temperr, 1, MPI_DOUBLE, MPI_MAX, slucomm);) (27, MPI, MPI_Allreduce(&xnorm, &tempxnorm, 1, MPI_DOUBLE, MPI_MAX, slucomm);) (28, MPI, MPI_Allreduce(&errcomp, &temperr, 1, MPI_DOUBLE, MPI_MAX, slucomm);)", "label": "(26, MPI, MPI_Allreduce( &temperr, &errcomp, 1, MPI_FLOAT, MPI_MAX, slucomm);); (23, MPI, MPI_Allreduce( &tempxnorm, &xnorm, 1, MPI_DOUBLE, MPI_MAX, slucomm);); (20, MPI, MPI_Allreduce( &temperr, &err, 1, MPI_DOUBLE, MPI_MAX, slucomm);)"}
{"predict": "(19, OpenMP, #pragma omp target map(present, alloc: arr[0:0]));", "label": "(8, OpenMP, #pragma omp target map(present, alloc: arr[0:0])); (6, OpenMP, #pragma omp target data map(alloc: arr[0:5])); (19, OpenMP, #pragma omp target data map(alloc: arr[0:0]))"}
{"predict": "(14, MPI, MPI_Barrier(inter, &err);)", "label": "(14, MPI, err = MPI_Barrier(ab_c_inter);); (16, MPI, err = MPI_Intercomm_merge(ab_c_inter, 0, &abc_intra);); (25, MPI, MPI_Comm_disconnect(&ab_inter);); (21, MPI, MPI_Comm_free(&ab_intra);); (18, MPI, MPI_Comm_free(&abc_intra);); (5, MPI, err = MPI_Intercomm_create(ab_intra, 0, ac_intra, count, tag, &ab_c_inter);); (17, MPI, MPI_Comm_free(&ab_c_inter);); (18, MPI, MPI_Comm_free(&ac_intra);); (20, MPI, MPI_Comm_disconnect(&ac_inter);); (10, MPI, err = MPI_Barrier(abc_intra);)"}
{"predict": "(30, MPI, double t0 = MPI_Wtime();)", "label": "(2, MPI, MPI_Init(&argc, &argv);); (55, MPI, MPI_Finalize();)"}
{"predict": "(6, OpenMP, #pragma omp parallel for)", "label": "(5, OpenMP, #pragma omp parallel num_threads(2)); (3, OpenMP, omp_set_nested(1);)"}
{"predict": "(10, OpenCL, cl_int e = clEnqueueBarrierWithWaitList(commandQueues[i], 0, nullptr, nullptr);)", "label": "(23, OpenCL, clReleaseEvent(events[i]);)"}
{"predict": "(8, OpenMP, #pragma omp parallel for private(index, ctx))", "label": "(6, OpenMP, #pragma omp parallel for)"}
{"predict": "_all();\n137:             if (!silent)\n138:                 output.display();\n139:             assert(output.get_value() == high);\n140:             input0.flip(); // 1 0 1\n141:             g.wait_for_all();\n142:             if (!silent)\n143:                 output.display();\n144:             assert(output.get_value() == high);\n145:             input1.flip(); // 1 1 0\n146:             g.wait_for_all();\n147:             if (!silent)\n148:                 output.display();\n149:             assert(output.get_value() == high);\n150:             input2.flip();\n151:             input1.flip(); // 1 1 1\n152:             g.wait_for_all();\n153:             if (!silent)\n154:                 output.display();\n155:             assert(output.get_value() == high);\n156:         }\n157:     }\n158: }\n\nOutput:", "label": "(720, TBB, utility::report_elapsed_time((oneapi::tbb::tick_count::now() - start).seconds());); (15, TBB, oneapi::tbb::global_control c(oneapi::tbb::global_control::max_allowed_parallelism, p);); (19, TBB, oneapi::tbb::flow::graph g;); (11, TBB, oneapi::tbb::tick_count start = oneapi::tbb::tick_count::now();)"}
{"predict": "(12, CUDA, cudaMallocManaged(&h_x, M * sizeof(real));) (13, CUDA, cudaMallocManaged(&h_y, M * sizeof(real));) (14, CUDA, cudaMallocManaged(&h_z, M * sizeof(real));) (21, CUDA, cudaMalloc(&d_x, M * sizeof(real));) (22, CUDA, cudaMalloc(&d_y, M * sizeof(real));) (23, CUDA, cudaMalloc(&d_z, M * sizeof(real));) (26, CUDA, cudaMemcpyAsync(d_x, h_x, M * sizeof(real), cudaMemcpyHostToDevice, stream);) (27, CUDA, cudaMemcpyAsync(d_y, h_y, M * sizeof(real), cudaMemcpyHostToDevice, stream);) (28, CUDA, cudaMemcpyAsync(d_z, h_z, M * sizeof(real), cudaMemcpyHostToDevice, stream);) (42, CUDA, cudaStreamCreate(&stream);) (47, CUDA, cudaMemcpyAsync(d_x, h_x, M * sizeof(real), cudaMemcpyHostToDevice, stream);) (48, CUDA, cudaMemcpyAsync(d_y, h_y, M * sizeof(real), cudaMemcpyHostToDevice, stream);) (49, CUDA, cudaMemcpyAsync(d_z, h_z, M * sizeof(real), cudaMemcpyHostToDevice, stream);) (51, CUDA, cudaStreamDestroy(stream);) (60, CUDA, cudaMallocManaged(&h_x2, M * sizeof(real));) (61, CUDA, cudaMallocManaged(&h_y2, M * sizeof(real));) (62, CUDA, cudaMallocManaged(&h_z2, M * sizeof(real));) (66, CUDA, cudaMemcpyAsync(d_x, h_x2, M * sizeof(real), cudaMemcpyHostToDevice, stream);) (67, CUDA, cudaMemcpyAsync(d_y, h_y2, M * sizeof(real), cudaMemcpyHostToDevice, stream);) (68, CUDA, cudaMemcpyAsync(d_z, h_z2, M * sizeof(real), cudaMemcpyHostToDevice, stream);) (74, CUDA, cudaStreamDestroy(stream);)", "label": "(47, CUDA, CHECK(cudaStreamCreate(&(streams[n])));); (84, CUDA, CHECK(cudaStreamDestroy(streams[i]));); (94, CUDA, CHECK(cudaFree(d_y));); (19, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice));); (93, CUDA, CHECK(cudaFree(d_z));); (53, CUDA, CHECK(cudaStreamDestroy(streams[n]));); (58, CUDA, CHECK(cudaMallocHost(&h_x2, M));); (82, CUDA, CHECK(cudaFreeHost(h_y2));); (82, CUDA, CHECK(cudaFreeHost(h_z2));); (66, CUDA, CHECK(cudaStreamCreate(&(streams[i])));); (10, CUDA, CHECK(cudaMalloc(&d_y, M));); (8, CUDA, CHECK(cudaMalloc(&d_x, M));); (83, CUDA, CHECK(cudaFree(d_x));); (53, CUDA, CHECK(cudaMallocHost(&h_z2, M));); (7, CUDA, CHECK(cudaMalloc(&d_z, M));); (73, CUDA, CHECK(cudaFreeHost(h_x2));); (7, CUDA, CHECK(cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice));); (48, CUDA, CHECK(cudaMallocHost(&h_y2, M));)"}
{"predict": "(21, MPI, CHECK(MPI_File_open(MPI_COMM_WORLD, TEST_FILENAME, MPI_MODE_RDWR | MPI_MODE_CREATE, MPI_INFO_NULL, &fileh));) (26, MPI, CHECK(MPI_File_write_at(fileh, (MPI_Offset) rank, (void *) &sample_i, 1, MPI_INT, MPI_STATUS_IGNORE));) (30, MPI, MPI_File_close(&fileh);) (39, MPI, MPI_File_delete(TEST_FILENAME, MPI_INFO_NULL);)", "label": "(14, MPI, MPI_Init(&argc, &argv);); (47, MPI, MPI_Finalize();); (33, MPI, MPI_Barrier(MPI_COMM_WORLD);); (12, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (40, MPI, CHECK(MPI_File_delete(TEST_FILENAME, MPI_INFO_NULL));); (27, MPI, CHECK(MPI_File_close(&fileh));); (19, MPI, CHECK(MPI_File_set_view(fileh, 0, MPI_INT, MPI_INT, datarep[i], MPI_INFO_NULL));)"}
{"predict": "(17, MPI, t0 = MPI_Wtime();)", "label": "(28, MPI, MPI_Recv(NULL, 0, MPI_BYTE, p->nbor2, 1, MPI_COMM_WORLD, &status);); (22, MPI, MPI_Recv(NULL, 0, MPI_BYTE, p->nbor, 1, MPI_COMM_WORLD, &status);); (25, MPI, MPI_Send(NULL, 0, MPI_BYTE, p->nbor2, 1, MPI_COMM_WORLD);); (34, MPI, latency = (MPI_Wtime() - t0) / (2 * g_latency012_reps);); (18, MPI, MPI_Send(NULL, 0, MPI_BYTE, p->nbor, 1, MPI_COMM_WORLD);); (10, MPI, t0 = MPI_Wtime();)"}
{"predict": "(26, CUDA, CUDA_PROFILE_START(\"Setup\", \"MyGroup\");)\n(27, CUDA, CUDA_PROFILE_END(\"Setup\", \"MyGroup\");)\n(44, CUDA, cudaMalloc((void**)&gpuMatrix, NUM * sizeof(float));)\n(45, CUDA, cudaMalloc((void**)&gpuTransposeMatrix, NUM * sizeof(float));)\n(79, CUDA, cudaFree(gpuMatrix);)\n(80, CUDA, cudaFree(gpuTransposeMatrix);)", "label": "(20, CUDA, cudaGetDeviceProperties(&devProp, 0);); (81, CUDA, cudaFree(gpuMatrix);); (43, CUDA, cudaMalloc((void**)&gpuMatrix, NUM * sizeof(float));); (81, CUDA, cudaFree(gpuTransposeMatrix);); (43, CUDA, cudaMalloc((void**)&gpuTransposeMatrix, NUM * sizeof(float));)"}
{"predict": "(27, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);)", "label": "(24, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (19, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (50, MPI, MPI_Finalize();); (47, MPI, MPI_T_finalize();); (18, MPI, MPI_T_init_thread(required, &provided);); (31, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(14, MPI, if (rank == root) { for (int i = 0; i < nprocs; i++) { counts[i] = count; displs[i] = i * (count * stride * sizeof(CTYPE)); } }); (22, MPI, if (!is_nonblock) { } else { MPI_Request req; MPI_Igatherv_c(sendbuf, count, datatype, recvbuf, counts, displs, datatype, root, comm, &req); }); (29, MPI, if (rank == root) { for (int i = 0; i < nprocs; i++) { void *buf = (char *) recvbuf + i * (count * stride * sizeof(CTYPE)); check_buf(buf, count, stride, i); } free(recvbuf); });", "label": "(22, MPI, MPI_Gatherv_c(sendbuf, count, datatype, recvbuf, counts, displs, datatype, root, comm);); (26, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);)"}
{"predict": "3: \n104: #pragma omp parallel\n105: // omp4-error@+3 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', or '>=') of loop variable 'i'}} omp5-error@+3 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', '>=', or '!=') of loop variable 'i'}}\n106: // expected-error@+2 {{increment clause of OpenMP for loop must perform simple addition or subtraction on loop variable 'i'}}\n107: #pragma omp taskloop\n108:   for (int i = 0; i < kk; jj++)\n109:     c[i] = a[i];\n110: \n111: #pragma omp parallel\n112: // omp4-error@+3 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', or '>=') of loop variable 'i'}} omp5-error@+3 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', '>=', or '!=') of loop variable 'i'}}\n113: // expected-error@+2 {{increment clause of OpenMP for loop must perform simple addition or subtraction on loop variable 'i'}}\n114: #pragma omp taskloop\n115:   for (int i = 0; i < kk; i += fii)\n116:     c[i] = a[i];\n117: \n118: #pragma omp parallel\n119: // omp4-error@+3 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', or '>=') of loop variable 'i'}} omp5-error@+3 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', '>=', or '!=') of loop variable 'i'}}\n120: // expected-error@+2 {{increment clause of OpenMP for loop must perform simple addition or subtraction on loop variable 'i'}}\n121: #pragma omp taskloop\n122:   for (int i = 0; i < kk; i += dii)\n123:     c[i] = a[i];\n124: \n125: #pragma omp parallel\n126: //", "label": "(286, OpenMP, #pragma omp taskloop lastprivate(ii)); (280, OpenMP, #pragma omp taskloop private(ii)); (5, OpenMP, #pragma omp parallel); (5, OpenMP, #pragma omp taskloop); (317, OpenMP, #pragma omp taskloop collapse(2)); (265, OpenMP, #pragma omp taskloop firstprivate(ii)); (270, OpenMP, #pragma omp taskloop linear(ii))"}
{"predict": "(18, MPI, MTest_Init_thread(&argc, &argv, required, &provided);)", "label": "(40, MPI, MPI_T_finalize();); (15, MPI, MPI_T_init_thread(required, &provided);)"}
{"predict": "(3, OpenMP, #pragma omp parallel for private(gtpinHelper, compilerProductHelper))", "label": "(104, OpenCL, retVal = clReleaseKernel(kernel);); (34, OpenCL, context = clCreateContext(nullptr, 1, &device, nullptr, nullptr, &retVal);); (108, OpenCL, retVal = clReleaseCommandQueue(cmdQ);); (110, OpenCL, retVal = clReleaseContext(context);); (57, OpenCL, kernel = clCreateKernel(pProgram, \"CopyBuffer\", &retVal);); (102, OpenCL, retVal = clReleaseProgram(pProgram);); (33, OpenCL, cmdQ = clCreateCommandQueue(context, device, properties, &retVal);)"}
{"predict": "(12, CUDA, const dim3 gridDim(iDivUp(width,blockDim.x), iDivUp(height,blockDim.y), 1);)", "label": "(14, CUDA, return CUDA(cudaGetLastError());)"}
{"predict": "(9, OpenMP, #pragma omp parallel)", "label": "(1, OpenMP, #pragma omp target teams); (5, OpenMP, #pragma omp parallel)"}
{"predict": "root) */\n101:   Num_procsx = sqrt(Num_procs);\n102:   while (Num_procsx * Num_procsx > Num_procs) {\n103:     Num_procsx--;\n104:   }\n105:   Num_procsy = Num_procs / Num_procsx;\n106: \n107:   /* compute my_IDx, my_IDy */\n108:   my_IDx = my_ID % Num_procsx;\n109:   my_IDy = my_ID / Num_procsx;\n110: \n111:   /* compute bounds of my grid tile */\n112:   istart = my_IDx * n / Num_procsx;\n113:   iend = istart + n / Num_procsx - 1;\n114:   jstart = my_IDy * n / Num_procsy;\n115:   jend = jstart + n / Num_procsy - 1;\n116: \n117:   /* allocate buffers for ghost points */\n118:   top_buf_out  = (DTYPE *)malloc(sizeof(DTYPE)*(n+2*RADIUS));\n119:   top_buf_in   = (DTYPE *)malloc(sizeof(DTYPE)*(n+2*RADIUS));\n120:   bottom_buf_out = (DTYPE *)malloc(sizeof(DTYPE)*(n+2*RADIUS));\n121:   bottom_buf_in  = (DTYPE *)malloc(sizeof(DTYPE)*(n+2*RADIUS));\n122:   right_buf_out  = (DTYPE *)malloc(sizeof(DTYPE)*(n+2*RADIUS));\n123:   right_buf_in   = (DTYPE *)malloc(sizeof(DTYPE)*(n+2*RADIUS));\n124:   left_buf_out   = (DTYPE *)malloc(sizeof(DTYPE)*(n+2*RADIUS));\n125:   left_buf_in    = (DTYPE *)malloc(sizeof(DTYPE)*(n+2*RADIUS));\n126: \n127:   /* allocate input and output grids */\n128:   total_length_in  = nsquare + 8*RADIUS*(n+2*RADIUS);\n129:   total_length_out = nsquare + 8*RADIUS*(n+2*RADIUS);\n130:   in = (DTYPE *)malloc(sizeof(DTYPE)*total_length_in", "label": "(210, MPI, MPI_Info_create(&rma_winfo);); (212, MPI, MPI_Info_set(rma_winfo, \"no_locks\", \"true\");); (370, MPI, MPI_Info_free(&rma_winfo);); (371, MPI, MPI_Finalize();); (128, MPI, MPI_Bcast(&n,          1, MPI_INT, root, MPI_COMM_WORLD);); (128, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (267, MPI, MPI_Win_fence(MPI_MODE_NOSTORE, rma_winx);); (39, MPI, MPI_Init(&argc,&argv);); (232, MPI, MPI_Barrier(MPI_COMM_WORLD);); (321, MPI, MPI_Reduce(&local_norm, &norm, 1, MPI_DTYPE, MPI_SUM, root, MPI_COMM_WORLD);); (38, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (234, MPI, MPI_Win_fence(MPI_MODE_NOSTORE, rma_winy);); (35, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(16, OpenCL, auto image = clCreateImageWithPropertiesINTEL(&mockContext, nullptr, 0, &imageFormat, &imageDesc, nullptr, &retVal);); (25, OpenCL, EXPECT_EQ(CL_SUCCESS, retVal);)", "label": "(28, OpenCL, retVal = clReleaseMemObject(image);); (6, OpenCL, auto status = clGetDeviceInfo(&mockClDevice, CL_DEVICE_IMAGE_SUPPORT, sizeof(imageSupportInfo), &imageSupportInfo, nullptr);)"}
{"predict": "(24, MPI, MPI_Scatter(data, local_rows*local_cols, MPI_DOUBLE, recvbuf, local_rows*local_cols, MPI_DOUBLE, 0, cart_comm, MPI_STATUS_IGNORE);)", "label": "(12, MPI, MPI_Init(&argc, &argv);); (34, MPI, MPI_Finalize();); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (11, MPI, MPI_Dims_create(size, NR_DIMS, dims);); (17, MPI, MPI_Cart_coords(cart_comm, rank, NR_DIMS, cart_coords);); (27, MPI, MPI_Barrier(MPI_COMM_WORLD);); (13, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(33, MPI, MPI_Initialized(&flag); if (flag) { MPI_Finalize(); } )", "label": "(27, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(10, CUDA, cudaMalloc((void**)&df_A, M * K * sizeof(float));) (11, CUDA, cudaMalloc((void**)&df_B, K * N * sizeof(float));) (12, CUDA, cudaMalloc((void**)&df_C, M * N * sizeof(float));) (43, CUDA, cublasSetStream(handle, stream);) (46, CUDA, cublasSgemmAsync(handle, CUBLAS_OP_N, CUBLAS_OP_N, M, N, K, &alpha, df_A, M, df_B, K, &beta, df_C, M, stream);) (52, CUDA, cublasGetMatrixAsync(M, N, sizeof(*df_C), df_C, M, pf_C, M, stream);) (59, CUDA, cudaFree(df_A);) (60, CUDA, cudaFree(df_B);) (61, CUDA, cudaFree(df_C);)", "label": "(39, CUDA, cudaMalloc((void **)&df_C, M * N * sizeof(float));); (62, CUDA, cudaFree(df_A);); (36, CUDA, cudaMalloc((void **)&df_B, K * N * sizeof(float));); (38, CUDA, cudaStat = cudaStreamCreate(&stream);); (61, CUDA, cudaFree(df_C);); (32, CUDA, cudaMalloc((void **)&df_A, M * K * sizeof(float));); (55, CUDA, cudaStreamDestroy(stream);); (48, CUDA, cudaStreamSynchronize(stream);); (56, CUDA, cudaFree(df_B);)"}
{"predict": "80:   checkCudaErrors(cudaMalloc((void **)&g0, buf_size));\n81:   checkCudaErrors(cudaMemset(g0, 0, buf_size));\n82:   checkCudaErrors(cudaSetDevice(gpuid[1]));\n83:   float *g1;\n84:   checkCudaErrors(cudaMalloc((void **)&g1, buf_size));\n85:   checkCudaErrors(cudaMemset(g1, 0, buf_size));\n86:   checkCudaErrors(cudaSetDevice(0));\n87:   float *h;\n88:   checkCudaErrors(cudaMallocHost((void **)&h, buf_size));\n89:   checkCudaErrors(cudaMemset(h, 0, buf_size));\n90: \n91:   // Copy data to GPU0\n92:   checkCudaErrors(cudaMemcpy(g0, h, buf_size, cudaMemcpyHostToDevice));\n93:   checkCudaErrors(cudaSetDevice(gpuid[1]));\n94:   // Copy data to GPU1\n95:   checkCudaErrors(cudaMemcpy(g1, g0, buf_size, cudaMemcpyDeviceToDevice));\n96:   checkCudaErrors(cudaSetDevice(0));\n97: \n98:   // Free buffers\n99:   checkCudaErrors(cudaFree(g0));\n100:  checkCudaErrors(cudaFree(g1));\n101:  checkCudaErrors(cudaFreeHost(h));\n102:  }", "label": "(46, CUDA, checkCudaErrors(cudaDeviceCanAccessPeer(&can_access_peer, i, j));); (99, CUDA, checkCudaErrors(cudaEventCreateWithFlags(&stop_event, eventflags));); (101, CUDA, checkCudaErrors(cudaEventRecord(start_event, 0));); (107, CUDA, checkCudaErrors(cudaMemcpy(g1, g0, buf_size, cudaMemcpyDefault));); (190, CUDA, checkCudaErrors(cudaFreeHost(h0));); (142, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (156, CUDA, checkCudaErrors(cudaMemcpy(h0, g0, buf_size, cudaMemcpyDefault));); (109, CUDA, checkCudaErrors(cudaEventRecord(stop_event, 0));); (6, CUDA, checkCudaErrors(cudaGetDeviceCount(&gpu_n));); (66, CUDA, checkCudaErrors(cudaSetDevice(gpuid[0]));); (181, CUDA, checkCudaErrors(cudaFree(g0));); (20, CUDA, checkCudaErrors(cudaGetDeviceProperties(&prop[i], i));); (74, CUDA, checkCudaErrors(cudaMalloc(&g0, buf_size));); (176, CUDA, checkCudaErrors(cudaEventDestroy(stop_event));); (62, CUDA, checkCudaErrors(cudaDeviceEnablePeerAccess(gpuid[1], 0));); (97, CUDA, checkCudaErrors(cudaMemcpy(g0, g1, buf_size, cudaMemcpyDefault));); (102, CUDA, checkCudaErrors(cudaEventElapsedTime(&time_memcpy, start_event, stop_event));); (171, CUDA, checkCudaErrors(cudaEventDestroy(start_event));); (59, CUDA, checkCudaErrors(cudaSetDevice(gpuid[1]));); (70, CUDA, checkCudaErrors(cudaMalloc(&g1, buf_size));); (58, CUDA, checkCudaErrors(cudaDeviceEnablePeerAccess(gpuid[0], 0));); (163, CUDA, checkCudaErrors(cudaDeviceDisablePeerAccess(gpuid[0]));); (160, CUDA, checkCudaErrors(cudaDeviceDisablePeerAccess(gpuid[1]));); (76, CUDA, checkCudaErrors(cudaEventCreateWithFlags(&start_event, eventflags));); (173, CUDA, checkCudaErrors(cudaSetDevice(i));); (107, CUDA, checkCudaErrors(cudaMemcpy(g0, h0, buf_size, cudaMemcpyDefault));); (91, CUDA, checkCudaErrors(cudaEventSynchronize(stop_event));); (166, CUDA, checkCudaErrors(cudaFree(g1));)"}
{"predict": "(8, OpenMP, #pragma omp parallel for)", "label": "(3, CUDA, Check_CudaErroorFun(\"At the beginning.\");)"}
{"predict": "(17, MPI, int peer_lead = (result.group == 0) ? 0 : 1;)\n(20, MPI, MPI_Intercomm_create(result.local, 0, result.global, peer_lead, 42, &result.inter);)", "label": "(4, MPI, MPI_Comm_size(result.global, &result.size);); (4, MPI, MPI_Comm_rank(result.global, &result.rank);); (1, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &result.global);); (15, MPI, MPI_Comm_rank(result.local, &result.local_rank);); (12, MPI, MPI_Comm_split(result.global, result.group, result.rank, &result.local);); (-3, MPI, MPI_Init(nullptr, nullptr);); (11, MPI, MPI_Comm_size(result.local, &result.local_size);)"}
{"predict": ": \n128:             if (strcmp(src_name, dst_name) != 0) {\n129:                 dcmp_strmap_item_update(src_map, key, DCMPF_CONTENT, DCMPS_DIFFER);\n130:                 dcmp_strmap_item_update(dst_map, key, DCMPF_CONTENT, DCMPS_DIFFER);\n131:             }\n132:             continue;\n133:         }\n134: \n135:         /* now, we can only compare regular files */\n136:         uint64_t src_size = mfu_flist_file_get_size(src_list, src_index);\n137:         uint64_t dst_size = mfu_flist_file_get_size(dst_list, dst_index);\n138: \n139:         /* compare sizes */\n140:         if (src_size != dst_size) {\n141:             dcmp_strmap_item_update(src_map, key, DCMPF_CONTENT, DCMPS_DIFFER);\n142:             dcmp_strmap_item_update(dst_map, key, DCMPF_CONTENT, DCMPS_DIFFER);\n143:         } else {\n144:             dcmp_strmap_item_update(src_map, key, DCMPF_CONTENT, DCMPS_COMMON);\n145:             dcmp_strmap_item_update(dst_map, key, DCMPF_CONTENT, DCMPS_COMMON);\n146:         }\n147: \n148:         /* compare contents */\n149:         if (src_size != 0) {\n150:             rc = dcmp_compare_content(src_list, src_map, src_index,\n151:                  dst_list, dst_map, dst_index,\n152:                  key);\n153:             if (rc != 0) {\n154:                 dcmp_strmap_item_update(src_map, key, DCMPF_CONTENT, DCMPS_DIFFER);\n155:                 dcmp_strmap_item_update(dst_map, key, DCMPF_CONTENT, DCMPS_DIFFER);\n156:             }\n157:         }\n158:     }\n159: \n160:     /* end timer */\n161:     time(&time_ended);\n162: \n163:     /* calculate elapsed time */\n164:     double", "label": "(17, MPI, MPI_Barrier(MPI_COMM_WORLD);); (26, MPI, double start_compare = MPI_Wtime();); (199, MPI, double end_compare = MPI_Wtime();)"}
{"predict": "(17, CUDA, AgentVector pop(agent, AGENT_COUNT);) (22, CUDA, instance.setVariable<int>(\"initial_order\", i);) (23, CUDA, instance.setVariable<float, 2>(\"xy\", { static_cast<float>(-i), static_cast<float>(-i)});) (28, CUDA, cudaSimulation.step();) (31, CUDA, cudaSimulation.getPopulationData(pop);)", "label": "(24, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(14, MPI, if (provided != MPI_THREAD_MULTIPLE) { MPI_Abort(MPI_COMM_WORLD, 1); } return 0; )", "label": "(54, MPI, MPI_Comm_free(&comm2);); (46, MPI, MPI_Ssend(buffer, 0, MPI_INT, rank, 1, MPI_COMM_WORLD);); (31, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &comm2);); (43, MPI, MPI_Barrier(comm4);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (38, MPI, MPI_Comm_dup(comm1, &comm4);); (46, MPI, MPI_Comm_free(&comm4);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (45, MPI, MPI_Comm_free(&comm1);); (39, MPI, MPI_Recv(buffer, 0, MPI_INT, rank, 2, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (30, MPI, MPI_Ssend(buffer, 0, MPI_INT, rank, 0, MPI_COMM_WORLD);); (21, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &comm1);)"}
{"predict": "_NEIGHBORS; i++) {\n91:                 touch_recv_buff(size, recv_buffers[i]);\n92:             }\n93:             number_of_messages++;\n94:         }\n95:         imb_p2p_resume();\n96:         imb_p2p_barrier(MPI_COMM_WORLD);\n97:         time = MPI_Wtime();\n98:         imb_p2p_barrier(MPI_COMM_WORLD);\n99:         for (i = 0; i < NUMBER_OF_NEIGHBORS; i++) {\n100:             imb_p2p_free_mem(send_buffers[i]);\n101:         }\n102:         for (i = 0; i < NUMBER_OF_NEIGHBORS; i++) {\n103:             imb_p2p_free_mem(recv_buffers[i]);\n104:         }\n105:     }\n106:     if (rank == 0) {\n107:         fprintf(unit, \"\\n\");\n108:         fprintf(unit, \"#----------------------------------------------------------------\\n\");\n109:         fprintf(unit, \"# End of Benchmarking\\n\");\n110:         fflush(unit);\n111:     }\n112:     imb_p2p_barrier(MPI_COMM_WORLD);\n113:     imb_p2p_finalize();\n114: }", "label": "(75, MPI, MPI_Waitall((NUMBER_OF_NEIGHBORS * 2), requests, MPI_STATUSES_IGNORE);); (97, MPI, time = MPI_Wtime() - time;); (71, MPI, MPI_Isend(send_buffers[i], size, MPI_BYTE, neighbours[i], 0, MPI_COMM_WORLD, &requests[NUMBER_OF_NEIGHBORS + i]);); (78, MPI, time = MPI_Wtime();); (101, MPI, MPI_Send(&report, sizeof(imb_p2p_report_t), MPI_BYTE, 0, 1, MPI_COMM_WORLD);); (65, MPI, MPI_Irecv(recv_buffers[i], size, MPI_BYTE, neighbours[i], 0, MPI_COMM_WORLD, &requests[i]);); (108, MPI, MPI_Recv(&report, sizeof(imb_p2p_report_t), MPI_BYTE, source_rank, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)"}
{"predict": "(33, CUDA, sumCPU = plus( sumCPU, nextrand );)", "label": "(59, CUDA, cudaEventRecord( evStop );); (75, CUDA, cudaFree( in );); (72, CUDA, cudaEventDestroy( evStart );); (52, CUDA, cudaEventRecord( evStart );); (73, CUDA, cudaFree( out );); (70, CUDA, cudaEventDestroy( evStop );)"}
{"predict": "(37, OpenMP, #pragma omp parallel for)", "label": "(35, OpenMP, #pragma omp target teams distribute parallel for map(a, b))"}
{"predict": "4:   free(h_OutputGPU);\n125:   cudaFree(d_Output);\n126:   return 0;\n127: }", "label": "(40, CUDA, checkCudaErrors(cudaMemset(d_Output, 0, QRNG_DIMENSIONS * N * sizeof(float)));); (44, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (124, CUDA, checkCudaErrors(cudaFree(d_Output));)"}
{"predict": "(15, CUDA, kernel_vKKwij_to_vwKiKj<<<grid_dim, block_dim>>>(nwalk, nkpts, nmo_max, nmo_tot, kk, nmo, nmo0, A, B);)", "label": "(15, CUDA, qmc_cuda::cuda_check(cudaGetLastError(), \"vKKwij_to_vwKiKj\");); (15, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize(), \"vKKwij_to_vwKiKj\");)"}
{"predict": "(10, OpenCL, retVal = clSetUserEventStatus(userEvent, CL_COMPLETE);)", "label": "(9, OpenCL, auto userEvent = clCreateUserEvent(pContext, &retVal);); (30, OpenCL, retVal = clReleaseEvent(userEvent);)"}
{"predict": "(10, OpenMP, #pragma omp parallel for)", "label": "(10, OpenMP, #pragma omp teams distribute parallel for); (8, OpenMP, #pragma omp target); (12, OpenMP, #pragma omp teams distribute parallel for if (parallel: false))"}
{"predict": "(24, MPI, MPI_Scatter(array, 10, MPI_INT, MPI_IN_PLACE, 10, MPI_INT, 0, MPI_COMM_WORLD);)", "label": "(12, MPI, MPI_Pack_external_size((char *) \"external32\", 1, MPI_INT, &sizeofint);); (66, MPI, MPI_Type_free(&parent_type);); (36, MPI, MPI_Pack_external((char *) \"external32\", array, 1, parent_type, buf, sizeoftype, &position);); (46, MPI, MPI_Unpack_external((char *) \"external32\", buf, sizeoftype, &position, array, 1, parent_type);); (21, MPI, MPI_Pack_external_size((char *) \"external32\", 1, parent_type, &sizeoftype);); (18, MPI, MPI_Type_commit(&parent_type);); (15, MPI, MPI_Type_vector(10, 2, 2, MPI_INT, &parent_type);)"}
{"predict": "C_bf16 ? \"BF16\" : \"Unknown FP type\"))));\n93: \n94:     std::cout << \"Problem size: \" << order << \" x \" << order << \" x \" << iterations << \"\\n\"\n95:               << \"Type: \" << pnameA << \", \" << pnameB << \", \" << pnameC << \"\\n\"\n96:               << \"GEMM Time: \" << avgtime << \" s\\n\"\n97:               << \"GFLOPS: \" << nflops/avgtime/1e9 << \"\\n\";\n98:   } else {\n99:     std::cout << \"Reference checksum = \" << reference << \"\\n\"\n100:               << \"Actual checksum = \" << checksum << std::endl;\n101:     std::cout << \"Error: residual is too large\" << std::endl;\n102:   }\n103: }", "label": "(82, SYCL, auto isA_fp16 = (typeid(TA) == typeid(sycl::half));); (28, SYCL, sycl::free(h_a, q);); (6, SYCL, auto h_c = sycl::malloc_host<TC>( nelems, q);); (4, SYCL, auto h_b = sycl::malloc_host<TB>( nelems, q);); (111, SYCL, sycl::free(h_c, q);); (14, SYCL, auto  A = sycl::malloc_device<TA>( nelems, q);); (94, SYCL, auto isC_fp16 = (typeid(TC) == typeid(sycl::half));); (23, SYCL, sycl::free(h_b, q);); (49, SYCL, sycl::free(A, q);); (47, SYCL, sycl::free(B, q);); (-4, SYCL, auto h_a = sycl::malloc_host<TA>( nelems, q);); (80, SYCL, auto isB_fp16 = (typeid(TB) == typeid(sycl::half));); (9, SYCL, auto  C = sycl::malloc_device<TC>( nelems, q);); (7, SYCL, auto  B = sycl::malloc_device<TB>( nelems, q);); (41, SYCL, sycl::free(C, q);)"}
{"predict": "(10, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &dup_comm);)", "label": "(16, MPI, mpi_errno = MPI_Comm_create_group(dup_comm, group, 10, NULL);); (20, MPI, MPI_Comm_free(&dup_comm);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (18, MPI, MPI_Group_free(&group);); (12, MPI, MPI_Error_class(mpi_errno, &errclass);); (7, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (5, MPI, MPI_Comm_group(dup_comm, &group);); (3, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &dup_comm);)"}
{"predict": "(17, MPI, MPI_Allreduce(t, ttmp, LOOPS, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);)", "label": "(24, MPI, t[i] = MPI_Wtime() - ttmp;); (19, MPI, MPI_Type_commit(&xpose[j]);); (34, MPI, MPI_Type_free(&column[j]);); (32, MPI, MPI_Type_free(&xpose[j]);); (11, MPI, ttmp = MPI_Wtime();); (13, MPI, MPI_Type_vector(nrows, 1, ncols, MPI_INT, &column[j]);); (13, MPI, MPI_Type_create_hvector(ncols, 1, sizeof(int), column[j], &xpose[j]);)"}
{"predict": "(15, CUDA, input.sync_device();) (17, CUDA, uint128_add_operator<<< dim3(1,1), dim3(N, 1) >>>(sum.device_data(), input.device_data(), b, N);) (21, CUDA, sum.sync_host();)", "label": "(18, CUDA, ASSERT_EQ(cudaGetLastError(), cudaSuccess) << \"Kernel launch error.\";)"}
{"predict": ":   }\n98: }\n99: \n100: extern \"C\" void __attribute__((visibility(\"default\"))) ExtractSiftDescriptorsKernel(cudaTextureObject_t texObj, SiftPoint *d_sift, int fstPts, float subsampling) {\n101:   ExtractSiftDescriptors<<<(numPts+15)/16, 16>>>(texObj, d_sift, fstPts, subsampling);\n102: }\n\nOutput:", "label": "(3, CUDA, __shared__ float buffer[128];); (3, CUDA, __shared__ float sums[4];); (0, CUDA, __shared__ float gauss[16];)"}
{"predict": "(26, MPI, MPI_Abort(MPI_COMM_WORLD, 1);) (39, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(26, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(7, OpenMP, #pragma omp parallel for)", "label": "(12, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(17, CUDA, flip_kernel<T><<<gridSize, blockSize, 0, stream>>>(src, dst, flip_code);)", "label": "(15, CUDA, checkCudaErrors(cudaGetLastError());); (13, CUDA, checkCudaErrors(cudaStreamSynchronize(stream));)"}
{"predict": "4:             if (min_displ_idx == -1)\n105:                 min_displ_idx = i;\n106:             else if (attr->u.blkindx.array_of_displs[i] <\n107:                      attr->u.blkindx.array_of_displs[min_displ_idx])\n108:                 min_displ_idx = i;\n109:         }\n110: \n111:         extent = attr->u.blkindx.array_of_displs[max_displ_idx] -\n112:                  attr->u.blkindx.array_of_displs[min_displ_idx] +\n113:                  attr->u.blkindx.blklen;\n114: \n115:         /* check if this extent is good enough */\n116:         if (extent >= DTPI_ATTR_BLKINDX_MIN_EXTENT) {\n117:             break;\n118:         }\n119:     }\n120: \n121:     /* setup the new datatype */\n122:     MPI_Type_vector(attr->u.blkindx.numblks, attr->u.blkindx.blklen, extent, type, newtype);\n123:     MPI_Type_commit(newtype);\n124:     MPI_Type_get_extent(*newtype, &tmp_lb, &extent);\n125:     *new_count = count;\n126:     return DTP_SUCCESS;\n127: }", "label": "(14, MPI, rc = MPI_Type_get_extent(type, &tmp_lb, &attr->child_type_extent);); (127, MPI, rc = MPI_Type_free(&type);)"}
{"predict": "(13, OpenMP, #pragma omp parallel for)", "label": "(20, OpenMP, #pragma omp target update to(mapper(id): fd)); (10, OpenMP, #pragma omp declare mapper(id: N1::vec v) map(v.len)); (14, OpenMP, #pragma omp target map(mapper(id) alloc: fd)); (15, OpenMP, #pragma omp target map(mapper(idd) alloc: fd.b)); (19, OpenMP, #pragma omp target update from(mapper(idd): fd.b)); (4, OpenMP, #pragma omp declare mapper(id: struct foodat v) map(v.a)); (15, OpenMP, #pragma omp target update to(mapper(idd): fd.b)); (3, OpenMP, #pragma omp declare mapper(idd: struct foodatchild v) map(v.k)); (14, OpenMP, #pragma omp target update from(mapper(id): fd))"}
{"predict": "(4, CUDA, LBMCollideAndPropagate<<<grid, BLOCK_SIZE, BLOCK_SIZE*6*sizeof(float)>>>(state.dmap, state.d1, state.d2, state.drho, state.dvx, state.dvy);) (10, CUDA, LBMCollideAndPropagate<<<grid, BLOCK_SIZE, BLOCK_SIZE*6*sizeof(float)>>>(state.dmap, state.d2, state.d1, state.drho, state.dvx, state.dvy);)", "label": "(4, CUDA, cudaMemcpy(state.vx, state.dvx, size_f, cudaMemcpyDeviceToHost);); (4, CUDA, cudaMemcpy(state.vy, state.dvy, size_f, cudaMemcpyDeviceToHost);); (4, CUDA, cudaMemcpy(state.rho, state.drho, size_f, cudaMemcpyDeviceToHost);)"}
{"predict": "(15, CUDA, tails[ty * SMALL_KERNEL_BLOCK + tx] = *src.ptr(batchIdx, y, x, channel);) (29, CUDA, arr[i] = fetchAs1d(i, SMALL_KERNEL_BLOCK);)", "label": "(12, CUDA, __shared__ T tails[SMALL_KERNEL_BLOCK * SMALL_KERNEL_BLOCK];); (18, CUDA, extern __shared__ char _arrays[];)"}
{"predict": ":         // own kernel\n131:         error = clSetKernelArg(kernel, 0, sizeof(cl_mem), &tinfo->inBuf);\n132:         test_error(error, \"clSetKernelArg failed!\\n\");\n133:         error = clSetKernelArg(kernel, 1, sizeof(cl_mem), &tinfo->inBuf2);\n134:         test_error(error, \"clSetKernelArg failed!\\n\");\n135:         error = clSetKernelArg(kernel, 2, sizeof(cl_mem), &tinfo->outBuf[j]);\n136:         test_error(error, \"clSetKernelArg failed!\\n\");\n137:         error = clSetKernelArg(kernel, 3, sizeof(cl_mem), &tinfo->params);\n138:         test_error(error, \"clSetKernelArg failed!\\n\");\n139:         error = clSetKernelArg(kernel, 4, sizeof(cl_uint), &base);\n140:         test_error(error, \"clSetKernelArg failed!\\n\");\n141:         error = clSetKernelArg(kernel, 5, sizeof(cl_uint), &buffer_elements);\n142:         test_error(error, \"clSetKernelArg failed!\\n\");\n143:         error = clSetKernelArg(kernel, 6, sizeof(cl_uint), &vectorCount);\n144:         test_error(error, \"clSetKernelArg failed!\\n\");\n145:         error = clSetKernelArg(kernel, 7, sizeof(cl_uint), &idx);\n146:         test_error(error, \"clSetKernelArg failed!\\n\");\n147:         error = clSetKernelArg(kernel, 8, sizeof(cl_uint), &ftz);\n148:         test_error(error, \"clSetKernelArg failed!\\n\");\n149:         error = clSetKernelArg(kernel, 9, sizeof(cl_uint), &job_id);\n150:         test_error(error, \"clSetKernelArg failed!\\n\");\n151:         error = clSetKernelArg(kernel, 10, sizeof(cl_uint), &thread_id);\n152:         test_error(error, \"clSetKernelArg failed!\\n\");\n153:         error = clSetKernelArg(kernel, 11, sizeof(cl_uint), &ulps);\n154:         test_error", "label": "(134, OpenCL, error = clSetKernelArg(kernel, 1, sizeof(tinfo->inBuf), &tinfo->inBuf);)"}
{"predict": "(17, MPI, ierr = MPI_Init(&argc, &argv);)", "label": "(45, MPI, ierr = MPI_Finalize();); (15, MPI, ierr = MPI_Comm_rank(MPI_COMM_WORLD, &myid);)"}
{"predict": "(12, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);)", "label": "(9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (14, MPI, MPI_Dims_create(tests[k].size, tests[k].dim, dims);)"}
{"predict": "(2, MPI, MPI_Init(&argc, &argv);) (110, MPI, MPI_Finalize();)", "label": "(8, MPI, MPI_Finalize();); (0, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(21, CUDA, if (NClass > 1) { status = argMaxWOBackground<32>(stream, N, dtype, samples, NClass, inScores, inDelta, inCountValid, argMaxScorePtr, argMaxLabelPtr, argMaxBBoxPtr); } else { argMaxScorePtr = const_cast<void*>(inScores); argMaxBBoxPtr = const_cast<void*>(inDelta); int threads = 512; int blocks = (N * samples + threads - 1) / threads; blocks = dMIN(blocks, 8); switch (dtype) { case nvinfer1::DataType::kFLOAT: resetMemValue_kernel<float><<<blocks, threads, 0, stream>>>(argMaxLabelPtr, N * samples, 0); break; case nvinfer1::DataType::kHALF: { break; } default: assert(false); } } cudaError_t status = DecodeBBoxes(stream, N, samples, regWeight, inputHeight, inputWidth, inROI, argMaxBBoxPtr, argMaxBBoxPtr, dtype); assert(status == cudaSuccess); if (samples <= 1024) { status = sortPerClass<256, 4>(stream, N, dtype, samples, NClass, param.backgroundLabelId, param.scoreThreshold, inCountValid, argMaxScorePtr, argMaxLabelPtr, argMaxBBoxPtr, sortClassPosPtr, sortClassScorePtr, sortClassLabelPtr, sortClassSampleIdxPtr, sortClassValidCountPtr); } else if (samples <= 2048) { status = sortPerClass<256, 8>(stream, N, dtype, samples, NClass, param.backgroundLabelId, param.scoreThreshold, inCountValid, argMaxScorePtr, argMaxLabelPtr, argMaxBBoxPtr, sortClassPosPtr, sortClassScorePtr, sortClassLabelPtr, sortClassSampleIdxPtr, sortClassValidCountPtr); } else if (samples <= 4096) { status = sortPerClass<256, 16>(stream, N, dtype, samples, NClass, param.backgroundLabelId, param.scoreThreshold, inCountValid, argMaxScorePtr, argMaxLabelPtr, argMaxBBoxPtr, sortClassPosPtr, sortClassScorePtr, sortClassLabelPtr, sortClassSampleIdxPtr, sortClass", "label": "(21, CUDA, CUASSERT(cudaMemsetAsync(sortClassValidCountPtr, 0, N * sizeof(int), stream));); (19, CUDA, CUASSERT(cudaMemsetAsync(argMaxBBoxPtr, 0, N * samples * 4 * sizeof(float), stream));); (20, CUDA, CUASSERT(cudaMemsetAsync(sortClassPosPtr, 0, N * (NClass + 1) * sizeof(int), stream));); (20, CUDA, CUASSERT(cudaMemsetAsync(sortClassSampleIdxPtr, 0, N * (samples + 1) * sizeof(int), stream));); (15, CUDA, CUASSERT(cudaMemsetAsync(argMaxScorePtr, 0, N * samples * sizeof(float), stream));)"}
{"predict": "(4, MPI, sendbuf.push_back(self_rank * self_rank);)\n(7, MPI, srand(time(NULL) * self_rank);)\n(8, MPI, int random_milliseconds = rand() % 600 + 100;)\n(9, MPI, int send_times = rand() % 5 + 1;)\n(11, MPI, int tag = rand() % 1000000;)\n(13, MPI, std::this_thread::sleep_for(std::chrono::milliseconds(random_milliseconds));)\n(15, MPI, MPI_Send(sendbuf.data(), kSendLength, MPI_INT, /*dest=*/0, /*tag=*/tag, MPI_COMM_WORLD);)\n(19, MPI, MPI_Send(pre_info, 2, MPI_INT, 0, kTag, MPI_COMM_WORLD);)", "label": "(11, MPI, MPI_Send(pre_info, 2, MPI_INT, 0, kTag, MPI_COMM_WORLD);)"}
{"predict": "(28, OpenCL, cl_mem in_buffer = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR, bytes, h_in.data(), &err); SPIRV_CHECK_ERROR(err, \"Failed to create in buffer\");) (31, OpenCL, cl_mem out_buffer = clCreateBuffer(context, CL_MEM_WRITE_ONLY, bytes, nullptr, &err); SPIRV_CHECK_ERROR(err, \"Failed to create out buffer\");) (37, OpenCL, prog.create_kernel(spvName);) (41, OpenCL, err = prog.set_arg(1, in_buffer); SPIRV_CHECK_ERROR(err, \"Failed to set arg 1\");) (42, OpenCL, err = prog.set_arg(2, out_buffer); SPIRV_CHECK_ERROR(err, \"Failed to set arg 2\");) (45, OpenCL, err = clEnqueueNDRangeKernel(queue, prog, 1, nullptr, &global, nullptr, 0, nullptr, nullptr); SPIRV_CHECK_ERROR(err, \"Failed to enqueue cl kernel\");) (50, OpenCL, err = clEnqueueReadBuffer(queue, out_buffer, CL_TRUE, 0, bytes, h_out.data(), 0, nullptr, nullptr); SPIRV_CHECK_ERROR(err, \"Failed to read from ref\");) (51, OpenCL, clReleaseMemObject(in_buffer); clReleaseMemObject(out_buffer);)", "label": "(32, OpenCL, err = clEnqueueWriteBuffer(queue, in, CL_TRUE, 0, bytes, &h_in[0], 0, NULL, NULL);); (44, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(cl_mem), &in);); (51, OpenCL, err = clEnqueueReadBuffer(queue, in, CL_TRUE, 0, bytes, &h_out[0], 0, NULL, NULL);); (26, OpenCL, clMemWrapper in = clCreateBuffer(context, CL_MEM_READ_WRITE, bytes, NULL, &err);); (38, OpenCL, clKernelWrapper kernel = clCreateKernel(prog, spvName, &err);); (44, OpenCL, err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &global, NULL, 0, NULL, NULL);)"}
{"predict": "(7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (6, MPI, MPI_Error_string(err, errorString, &errorStringLen);); (10, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &num_procs);)", "label": "(7, MPI, MPI_Get_library_version(version, &resultlen);)"}
{"predict": "(13, OpenMP, #pragma omp parallel for private(ii))", "label": "(61, TBB, tbb::internal::segmented_iterator<nested_vec, const T> my_csi(my_vec);); (18, TBB, tbb::internal::segmented_iterator<nested_vec, T> my_si(my_vec);)"}
{"predict": "(17, OpenCL, auto glImage = clCreateFromGLTexture(clContext.get(), 0, GL_TEXTURE_1D, 0, textureId, &retVal);); (5, OpenCL, retVal = clReleaseMemObject(glImage);)", "label": "(61, OpenCL, clReleaseMemObject(mem);); (33, OpenCL, cl_mem buf = clCreateBuffer(ctx, CL_MEM_USE_HOST_PTR, sz, data, &err);); (23, OpenCL, ctx->image_queue = clCreateCommandQueueWithProperties(ctx, ctx->devices[0], 0, &err);); (55, OpenCL, err = clEnqueueCopyBufferToImage(ctx->image_queue, buf, mem, 0, origin, region, 0, NULL, NULL);); (38, OpenCL, clReleaseMemObject(buf);)"}
{"predict": "firstprivate by the compiler}}\n100:     foo();\n101:   int xb;\n102:   for (i = 0; i < argc; ++i)\n103:     foo();\n104:   int xc;\n105:   for (i = 0; i < argc; ++i)\n106:     foo();\n107:   int xd;\n108:   for (i = 0; i < argc; ++i)\n109:     foo();\n110:   int xe;\n111:   for (i = 0; i < argc; ++i)\n112:     foo();\n113:   int xf;\n114:   for (i = 0; i < argc; ++i)\n115:     foo();\n116:   int xg;\n117:   for (i = 0; i < argc; ++i)\n118:     foo();\n119:   int xh;\n120:   for (i = 0; i < argc; ++i)\n121:     foo();\n122:   int xi;\n123:   for (i = 0; i < argc; ++i)\n124:     foo();\n125:   int xj;\n126:   for (i = 0; i < argc; ++i)\n127:     foo();\n128:   int xk;\n129:   for (i = 0; i < argc; ++i)\n130:     foo();\n131:   int xl;\n132:   for (i = 0; i < argc; ++i)\n133:     foo();\n134:   int xm;\n135:   for (i = 0; i < argc; ++i)\n136:     foo();\n137:   int xn;\n138:   for (i = 0; i < argc; ++i)\n139:     foo();\n140:   int xo;\n141:   for (i = 0; i < argc; ++i)\n142:     foo();\n143:   int xp;\n144:   for (i = 0", "label": "(51, OpenMP, #pragma omp distribute parallel for firstprivate(a, b, c, d, f)); (86, OpenMP, #pragma omp distribute parallel for firstprivate(S2::S2s)); (158, OpenMP, #pragma omp parallel private(i)); (94, OpenMP, #pragma omp distribute parallel for safelen(5)); (98, OpenMP, #pragma omp distribute parallel for firstprivate(e, g)); (170, OpenMP, #pragma omp distribute parallel for firstprivate(si)); (70, OpenMP, #pragma omp distribute parallel for firstprivate(da)); (75, OpenMP, #pragma omp distribute parallel for firstprivate(xa)); (28, OpenMP, #pragma omp distribute parallel for firstprivate(argc > 0 ? argv[1] : argv[2])); (113, OpenMP, #pragma omp distribute parallel for firstprivate(i)); (-1, OpenMP, #pragma omp target); (30, OpenMP, #pragma omp distribute parallel for firstprivate(argc)); (133, OpenMP, #pragma omp distribute parallel for lastprivate(n) firstprivate(n)); (99, OpenMP, #pragma omp distribute parallel for firstprivate(k, h, B::x)); (111, OpenMP, #pragma omp parallel); (151, OpenMP, #pragma omp parallel reduction(+ : i)); (0, OpenMP, #pragma omp distribute parallel for firstprivate(); (44, OpenMP, #pragma omp distribute parallel for firstprivate(2 * 2)); (53, OpenMP, #pragma omp distribute parallel for firstprivate(ca)); (114, OpenMP, #pragma omp distribute parallel for firstprivate(j)); (36, OpenMP, #pragma omp distribute parallel for firstprivate(argv[1])); (96, OpenMP, #pragma omp distribute parallel for private(xa), firstprivate(xa)); (70, OpenMP, #pragma omp distribute parallel for firstprivate(S2::S2sc)); (-2, OpenMP, #pragma omp distribute parallel for firstprivate()); (2, OpenMP, #pragma omp distribute parallel for firstprivate(argc); (114, OpenMP, #pragma omp distribute parallel for lastprivate(g) firstprivate(g)); (20, OpenMP, #pragma omp distribute parallel for firstprivate(S1)); (4, OpenMP, #pragma omp distribute parallel for firstprivate(argc,); (79, OpenMP, #pragma omp distribute parallel for firstprivate(m)); (37, OpenMP, #pragma omp distribute parallel for firstprivate(ba)); (-19, OpenMP, #pragma omp distribute parallel for firstprivate); (-21, OpenMP, #pragma omp teams)"}
{"predict": "(12, OpenMP, #pragma omp parallel for)", "label": "(11, OpenCL, auto commandQueue = clCreateCommandQueueWithProperties(pContext, testedClDevice, properties.data(), &retVal);); (21, OpenCL, clReleaseCommandQueue(commandQueue);)"}
{"predict": "(7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);) (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);) (10, MPI, MPI_Bcast(sbuf, length, MPI_CHAR, 0, MPI_COMM_WORLD);) (13, MPI, MPI_Bcast(rbuf, length, MPI_CHAR, 0, MPI_COMM_WORLD);) (16, MPI, MPI_Bcast(sbuf, length, MPI_CHAR, 0, MPI_COMM_WORLD);) (19, MPI, MPI_Bcast(rbuf, length, MPI_CHAR, 0, MPI_COMM_WORLD);) (22, MPI, MPI_Bcast(sbuf, length, MPI_CHAR, 0, MPI_COMM_WORLD);) (25, MPI, MPI_Bcast(rbuf, length, MPI_CHAR, 0, MPI_COMM_WORLD);) (28, MPI, MPI_Bcast(sbuf, length, MPI_CHAR, 0, MPI_COMM_WORLD);) (31, MPI, MPI_Bcast(rbuf, length, MPI_CHAR, 0, MPI_COMM_WORLD);) (34, MPI, MPI_Bcast(sbuf, length, MPI_CHAR, 0, MPI_COMM_WORLD);) (37, MPI, MPI_Bcast(rbuf, length, MPI_CHAR, 0, MPI_COMM_WORLD);) (40, MPI, MPI_Bcast(sbuf, length, MPI_CHAR, 0, MPI_COMM_WORLD);) (43, MPI, MPI_Bcast(rbuf, length, MPI_CHAR, 0, MPI_COMM_WORLD);) (46, MPI, MPI_Bcast(sbuf, length, MPI_CHAR, 0, MPI_COMM_WORLD);) (49, MPI, MPI_Bcast(rbuf, length, MPI_CHAR, 0, MPI_COMM_WORLD);)", "label": "(6, MPI, MPI_Type_get_extent(sddt, &lb, &extent);)"}
{"predict": "(12, OpenMP, #pragma omp parallel for private(p1, visc, arp1, posp1, rsymp1, velrhop1, ngs, z, y, pif, rsym, drx, dry, drz, rr2, fac, frx, fry, frz, massp2, compute, ftp2, dvx, dvy, dvz, dot, dot_rr2, dot_rr2))", "label": "(71, OpenMP, const int th=omp_get_thread_num();); (10, OpenMP, #pragma omp parallel for schedule (guided))"}
{"predict": "(57, CUDA, auto cudaStream = cudaStream_t();); (58, CUDA, cudaStreamCreate(&cudaStream);); (60, CUDA, for (auto N : batch_sizes) {); (61, CUDA, cudaStream_t innerStream;); (62, CUDA, cudaStreamCreate(&innerStream);); (63, CUDA, options.update({N, layer.h, layer.w, layer.c}, {layer.k, layer.r, layer.s, layer.c});); (64, CUDA, Result result = profile_convolution(options);); (65, CUDA, result.print(std::cout, idx, options) << std::endl;); (66, CUDA, cudaStreamDestroy(innerStream);); (67, CUDA, }); (68, CUDA, ++idx;); (69, CUDA, }); (70, CUDA, cudaStreamDestroy(cudaStream););", "label": "(11, CUDA, CUDA_CHECK(cudaGetDeviceProperties(&props, 0));)"}
{"predict": "(4, MPI, MPI_Init(&argc, &argv);) (11, MPI, MPI_Finalize();)", "label": "(3, MPI, MPI_Init(&argc, &argv);); (9, MPI, MPI_Finalize();)"}
{"predict": "(12, OpenCL, result = clGetPlatformIDs(1, &platform, NULL);) (15, OpenCL, std::vector<char> extensions(set_size);) (27, OpenCL, for (cl_uint i = 0; i < devicesCount; i++) {)", "label": "(12, OpenCL, result = clGetPlatformInfo(platform, CL_PLATFORM_EXTENSIONS, 0, NULL, &set_size);); (14, OpenCL, result = clGetPlatformInfo(platform, CL_PLATFORM_EXTENSIONS, extensions.size(), extensions.data(), NULL);)"}
{"predict": "(14, OpenMP, #pragma omp parallel for)", "label": "(47, OpenMP, #pragma omp teams num_teams (3.14)); (42, OpenMP, #pragma omp teams num_teams (-10u)); (17, OpenMP, #pragma omp teams num_teams (argc))); (4, OpenMP, #pragma omp teams num_teams (); (7, OpenMP, #pragma omp teams num_teams ()); (10, OpenMP, #pragma omp teams num_teams (argc); (25, OpenMP, #pragma omp teams num_teams (argc), num_teams (argc+1)); (32, OpenMP, #pragma omp teams num_teams (-2)); (15, OpenMP, #pragma omp teams num_teams (argc > 0 ? argv[1] : argv[2])); (-7, OpenMP, #pragma omp target); (25, OpenMP, #pragma omp teams num_teams (S1)); (16, OpenMP, #pragma omp teams num_teams (argc + argc-z)); (-9, OpenMP, #pragma omp teams num_teams)"}
{"predict": "}\n112:       if (nwrite < 0) {\n113:         nwrite = 0;\n114:       }\n115:       write_pos += (off_t) nwrite; /* update write pointer */\n116: \n117:       /* check whether we've written the whole file */\n118:       if (filesize_send == write_pos) {\n119:         receiving = 0;\n120:       }\n121:     }\n122:   }\n123: \n124:   /* close our file */\n125:   if (have_outgoing) {\n126:     shuffile_close(fd);\n127:   } else if (have_incoming) {\n128:     shuffile_close(fd);\n129:   }\n130: \n131:   /* free our send and recv buffers */\n132:   if (have_outgoing) {\n133:     shuffile_align_free(buf_send);\n134:   }\n135:   if (have_incoming) {\n136:     shuffile_align_free(buf_recv);\n137:   }\n138: \n139:   return rc;\n140: }", "label": "(101, MPI, MPI_Wait(&request[1], &status[1]);); (110, MPI, MPI_Wait(&request[0], &status[0]);); (110, MPI, MPI_Get_count(&status[0], MPI_BYTE, &nwrite);); (74, MPI, MPI_Irecv(buf_recv, shuffile_mpi_buf_size, MPI_BYTE, rank_recv, 0, comm, &request[0]);); (96, MPI, MPI_Isend(buf_send, nread, MPI_BYTE, rank_send, 0, comm, &request[1]);)"}
{"predict": "(10, MPI, *group = MPI_COMM_NULL; return;) (13, MPI, *group = MPI_COMM_SELF; return;) (42, MPI, MPI_Intercomm_merge(pgroup_new, 0 /* LOW */ , &pgroup);) (67, MPI, MPI_Intercomm_merge(pgroup_new, 0 /* HIGH */ , &pgroup);)", "label": "(48, MPI, MPI_Comm_accept(port, MPI_INFO_NULL, 0, pgroup, &pgroup_new);); (52, MPI, MPI_Comm_free(&pgroup_old);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (64, MPI, MPI_Comm_connect(port, MPI_INFO_NULL, 0, MPI_COMM_SELF, &pgroup_new);); (42, MPI, MPI_Send(port, PORT_SIZE, MPI_CHAR, members[i], 0, MPI_COMM_WORLD);); (35, MPI, MPI_Open_port(port_info, port);); (44, MPI, MPI_Comm_free(&pgroup_new);); (48, MPI, MPI_Close_port(port);); (66, MPI, MPI_Comm_connect(port, MPI_INFO_NULL, 0, pgroup, &pgroup_new);)"}
{"predict": "(12, CUDA, __syncthreads();)", "label": "(5, CUDA, __shared__ typename BlockMergeSort::TempStorage temp_storage_shuffle;)"}
{"predict": "(15, MPI, MPI_Scatter(buf, count, MPI_INT, MPI_IN_PLACE, count, MPI_INT, 0, comm, &op);)", "label": "(22, MPI, MPI_Allreduce(MPI_IN_PLACE, buf, count, MPI_INT, op, comm);); (13, MPI, MPI_Comm_size(comm, &size);); (35, MPI, MPI_Op_free(&op);); (6, MPI, MPI_Op_create(mysum, 0, &op);); (11, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(17, MPI, MPI_Comm_rank(comm, &rank); MPI_Comm_size(comm, &ranks);) (34, MPI, MPI_Comm_rank(comm, &rank); MPI_Comm_size(comm, &ranks);) (43, MPI, MPI_Sendrecv_replace(value, count, keysat, rank, rank + 1, rank + 1, comm, MPI_STATUS_IGNORE);) (44, MPI, MPI_Sendrecv_replace(value, count, keysat, rank, rank + 1, rank + 1, comm, MPI_STATUS_IGNORE);) (45, MPI, MPI_Sendrecv_replace(value, count, keysat, rank, rank + 1, rank + 1, comm, MPI_STATUS_IGNORE);) (46, MPI, MPI_Sendrecv_replace(value, count, keysat, rank, rank + 1, rank + 1, comm, MPI_STATUS_IGNORE);)", "label": "(34, MPI, MPI_Comm_size(comm, &ranks);); (32, MPI, MPI_Comm_rank(comm, &rank);); (12, MPI, MPI_Type_get_true_extent(keysat, &true_lb, &true_extent);)"}
{"predict": "(8, OpenMP, #pragma omp parallel for private(off, len))", "label": "(10, OpenMP, #pragma omp parallel for)"}
{"predict": "(7, OpenMP, #pragma omp parallel for private(index))", "label": "(6, OpenMP, #pragma omp parallel for)"}
{"predict": "(14, CUDA, cudaStreamDestroy(stream);)", "label": "(11, CUDA, cudaStreamCreate(&stream);); (14, CUDA, cudaStreamDestroy(stream);)"}
{"predict": "(1, CUDA, __global__ void sum(clock_t *d_clocks, int N) {)", "label": "(3, CUDA, __shared__ clock_t s_clocks[32];)"}
{"predict": "(11, MPI, MPI_Get_info_val(info, key, buff, sizeof(buff), &flag);)", "label": "(11, MPI, MPI_Info_get(info, key, sizeof(buff) - 1, buff, &flag);); (8, MPI, MPI_Info_create(&info);); (19, MPI, MPI_Info_free(&info);); (7, MPI, MPI_Info_set(info, key, val);)"}
{"predict": "=0; i<proc_length; i++) checksum+=iterstring[i];\n122:       printf(\"Rank %d: sum of selected string %d\\n\", my_ID, checksum);\n123:     }\n124: #endif\n125: \n126:     /* everybody sends own string to everybody else and concatenates */\n127:     for (i=0; i<Num_procs; i++) {\n128:       if (i!=my_ID) {\n129:         MPI_Sendrecv_replace(iterstring, proc_length, MPI_CHAR, i, 1, i, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n130:       }\n131:     }\n132: \n133:     /* everybody now has the concatenated string */\n134:     for (i=0; i<proc_length; i++) catstring[my_ID+i*Num_procs]=iterstring[i];\n135:   }\n136: \n137:   /* check for errors */\n138:   if (my_ID == root){\n139:     checksum=0;\n140:     for (i=0; i<length; i++) checksum+=catstring[i];\n141:     printf(\"Checksum of final aggregate string %d\\n\", checksum);\n142:   }\n143: \n144:   /* clean up */\n145:   free(basestring);\n146:   free(catstring);\n147:   free(iterstring);\n148: \n149:   MPI_Finalize();\n150: \n151:   return 0;\n152: }", "label": "(113, MPI, MPI_Type_commit(&mpi_word);); (120, MPI, MPI_Allgather(iterstring,1,mpi_word, catstring,1,mpi_word, MPI_COMM_WORLD);); (152, MPI, MPI_Finalize();); (61, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (108, MPI, MPI_Type_contiguous(proc_length,MPI_CHAR, &mpi_word);); (17, MPI, MPI_Init(&argc,&argv);); (59, MPI, MPI_Bcast(&length,     1, MPI_LONG, root, MPI_COMM_WORLD);); (108, MPI, MPI_Barrier(MPI_COMM_WORLD);); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(13, CUDA, extern \"C\" __global__ void MatrixMulAsyncCopyMultiStageSharedState(...);)", "label": "(13, CUDA, __shared__ float Bs[maxPipelineStages][BLOCK_SIZE_X][BLOCK_SIZE_X];); (8, CUDA, __shared__ float As[maxPipelineStages][BLOCK_SIZE_X][BLOCK_SIZE_X];)"}
{"predict": "(19, CUDA, sdata[tid] = g_idata[tid];)", "label": "(11, CUDA, extern __shared__ float sdata[];)"}
{"predict": "(7, OpenMP, #pragma omp parallel for reduction(+:errors))", "label": "(7, OpenMP, #pragma omp target map(tofrom : device)); (9, OpenMP, printf(\"omp_is_initial_device() returned false on host\\n\");); (12, OpenMP, printf(\"omp_is_initial_device() returned true on device\\n\");)"}
{"predict": "(3, TBB, tbb::parallel_for(tbb::blocked_range<size_t>(0, max_counter), [&](const tbb::blocked_range<size_t>& r) {", "label": "(32, TBB, tbb::tbb_allocator<t1>().destroy(my_storage);); (35, TBB, tbb::tbb_allocator<t2>().destroy(my_storage);); (0, TBB, tbb::atomic<int> counter;)"}
{"predict": "(35, CUDA, cudaMalloc((void**)&hvfield, sizeof(cData) * DS);) (40, CUDA, cudaMalloc((void**)&particles, sizeof(cData) * DS);) (52, CUDA, cufftPlan3d(&plan, DIM, DIM, DIM, CUFFT_Z2Z);) (53, CUDA, cufftExecZ2Z(plan, (cufftDoubleComplex*)particles, (cufftDoubleComplex*)hvfield, CUFFT_FORWARD);) (55, CUDA, cufftDestroy(plan);) (61, CUDA, cudaFree(hvfield);) (62, CUDA, cudaFree(particles);)", "label": "(56, CUDA, checkCudaErrors(cufftPlan2d(&planc2r, DIM, DIM, CUFFT_C2R));); (54, CUDA, checkCudaErrors(cufftPlan2d(&planr2c, DIM, DIM, CUFFT_R2C));); (37, CUDA, checkCudaErrors(cudaMallocPitch((void **)&dvfield, &tPitch, sizeof(cData)*DIM, DIM));); (38, CUDA, checkCudaErrors(cudaMemcpy(dvfield, hvfield, sizeof(cData) * DS, cudaMemcpyHostToDevice));); (40, CUDA, checkCudaErrors(cudaMalloc((void **)&vyfield, sizeof(cData) * PDS));); (38, CUDA, checkCudaErrors(cudaMalloc((void **)&vxfield, sizeof(cData) * PDS));)"}
{"predict": "(34, OpenMP, #pragma omp parallel for private(j) reduction(min:sum2)) (47, OpenMP, #pragma omp parallel for private(i) reduction(max:sum))", "label": "(46, OpenMP, #pragma omp teams reduction(+:sum)); (43, OpenMP, #pragma omp target map(tofrom: b, sum, t) map(alloc: a)); (46, OpenMP, #pragma omp distribute parallel for reduction(+:sum)); (48, OpenMP, t[i] = omp_get_team_num();)"}
{"predict": "// h_silu2\n76:     state_sizes[10] = B * C_out * H_out * W_out; // h_2\n77:     state_sizes[11] = B * C_out * H_out * W_out; // x_2\n78:     state_sizes[12] = B * C_out; // emb_2\n79:     state_sizes[13] = B * C_out * H_out * W_out; // h_plus_emb\n80:     if (up) {\n81:         state_sizes[14] = B * C_out * H_out * W_out; // h_1_up\n82:         state_sizes[15] = B * C_out * H_out * W_out; // x_1_up\n83:         state_sizes[16] = B * C_out; // emb_1_up\n84:         state_sizes[17] = B * C_out * H_out * W_out; // h_plus_emb_up\n85:     } else if (down) {\n86:         state_sizes[18] = B * C_out * H_out * W_out; // h_1_down\n87:         state_sizes[19] = B * C_out * H_out * W_out; // x_1_down\n88:         state_sizes[20] = B * C_out; // emb_1_down\n89:         state_sizes[21] = B * C_out * H_out * W_out; // h_plus_emb_down\n90:     }\n91: \n92:     size_t total_states_size = 0;\n93:     for (int i = 0; i < num_debug_states; i++) {\n94:         total_states_size += state_sizes[i];\n95:     }\n96: \n97:     // allocate host memory for states\n98:     float* states_memory_cpu = (float*)mallocCheck(total_states_size * sizeof(float));\n99:     freadCheck(states_memory_cpu, sizeof(float), total_states_size, states_file);\n100:     fcloseCheck(states_file);\n101: \n102:     // allocate device memory for states\n103:     float* states_memory_gpu;\n104:     printf(\"States: total state size: %.3f", "label": "(125, CUDA, cudaCheck(cudaMalloc(&acts_memory_gpu, num_acts_params * sizeof(float)));); (257, CUDA, cudaCheck(cudaFree(acts_memory_gpu));); (173, CUDA, cudaCheck(cudaMalloc(&back_acts_memory_gpu, num_back_acts * sizeof(float)));); (254, CUDA, cudaCheck(cudaFree(params_memory_gpu));); (259, CUDA, cudaCheck(cudaFree(acts.emb));); (53, CUDA, cudaCheck(cudaMemset(grads_memory_gpu, 0, num_parameters * sizeof(float)));); (170, CUDA, cudaCheck(cudaMemset(back_acts_memory_gpu, 0, num_back_acts * sizeof(float)));); (182, CUDA, cudaCheck(cudaMemcpy(back_acts.dout, debug_states.dout, B * C_out * H_out * W_out * sizeof(float), cudaMemcpyHostToDevice));); (254, CUDA, cudaCheck(cudaFree(acts.input));); (117, CUDA, cudaCheck(cudaMemset(acts_memory_gpu, 0, num_acts_params * sizeof(float)));); (43, CUDA, cudaCheck(cudaMemcpy(params_memory_gpu, params_memory_cpu, num_parameters * sizeof(float), cudaMemcpyHostToDevice));); (249, CUDA, cudaCheck(cudaFree(back_acts_memory_gpu));); (122, CUDA, cudaCheck(cudaMemcpy(acts.input, debug_states.input, state_sizes[0] * sizeof(float), cudaMemcpyHostToDevice));); (122, CUDA, cudaCheck(cudaMemcpy(acts.emb, debug_states.emb, state_sizes[1] * sizeof(float), cudaMemcpyHostToDevice));); (43, CUDA, cudaCheck(cudaMalloc(&grads_memory_gpu, num_parameters * sizeof(float)));); (116, CUDA, cudaCheck(cudaMalloc(&acts.input, B * C * H * W * sizeof(float)));); (36, CUDA, cudaCheck(cudaMalloc(&params_memory_gpu, num_parameters * sizeof(float)));); (115, CUDA, cudaCheck(cudaMalloc(&acts.emb, B * C_emb * sizeof(float)));); (241, CUDA, cudaCheck(cudaFree(grads_memory_gpu));)"}
{"predict": "(7, OpenMP, #pragma omp parallel for)\n(10, OpenMP, #pragma omp parallel for)", "label": "(6, OpenMP, omp_unset_nest_lock(&nest_lock);); (2, OpenMP, omp_init_nest_lock(&nest_lock);); (3, OpenMP, omp_test_nest_lock(&nest_lock);); (5, OpenMP, omp_set_nest_lock(&nest_lock);); (9, OpenMP, omp_destroy_nest_lock(&nest_lock);)"}
{"predict": "%lu, tgtcount: %i, tgttype: %p)\\n\",\n113:                   (unsigned long)copyargs.src, copyargs.srccount, copyargs.srctype,\n114:                   (unsigned long)copyargs.tgt, copyargs.tgtcount, copyargs.tgttype);\n115:         /* get buffers */\n116:         if(copyargs.tmpbufsrc) {\n117:           buf1=(char*)handle->tmpbuf+(long)copyargs.src;\n118:         } else {\n119:           buf1=(void *)copyargs.src;\n120:         }\n121:         if(copyargs.tmpbuftgt) {\n122:           buf2=(char*)handle->tmpbuf+(long)copyargs.tgt;\n123:         } else {\n124:           buf2=copyargs.tgt;\n125:         }\n126: \n127:         /* copy the data */\n128:         memcpy(buf2, buf1, copyargs.srccount * opal_dataloop_get_size(copyargs.srctype));\n129:         break;\n130:       case UNPACK:\n131:         NBC_DEBUG(5, \"  UNPACK (offset %li) \", offset);\n132:         NBC_GET_BYTES(ptr,unpackargs);\n133:         NBC_DEBUG(5, \"*buf: %p, count: %i, type: %p)\\n\", unpackargs.buf, unpackargs.count,\n134:                   unpackargs.datatype);\n135:         /* get buffer */\n136:         if(unpackargs.tmpbuf) {\n137:           buf1=(char*)handle->tmpbuf+(long)unpackargs.buf;\n138:         } else {\n139:           buf1=unpackargs.buf;\n140:         }\n141: \n142:         /* unpack the data */\n143:         opal_dataloop_unpack(buf1, unpackargs.count, unpackargs.datatype);\n144:         break;\n145:     }\n146:     ptr += NBC_Get_size(type);\n147:   }\n148:   return OMPI_SUCCESS;\n149: }", "label": "(73, MPI, Irecv_time -= MPI_Wtime();); (55, MPI, Isend_time += MPI_Wtime();); (88, MPI, Irecv_time += MPI_Wtime();); (35, MPI, Isend_time -= MPI_Wtime();)"}
{"predict": "(26, CUDA, cudaMemcpy(d_A, h_A, M, cudaMemcpyHostToDevice);)\n(27, CUDA, cudaMemcpy(d_B, h_B, M, cudaMemcpyHostToDevice);)\n(28, CUDA, cudaMemcpy(d_A, h_A, M, cudaMemcpyHostToDevice);)\n(29, CUDA, cudaMemcpy(d_B, h_B, M, cudaMemcpyHostToDevice);)\n(30, CUDA, cudaMemcpy(d_A, h_A, M, cudaMemcpyHostToDevice);)\n(31, CUDA, cudaMemcpy(d_B, h_B, M, cudaMemcpyHostToDevice);)\n(36, CUDA, cudaMemcpy(h_A, d_A, M, cudaMemcpyDeviceToHost);)\n(37, CUDA, cudaMemcpy(h_B, d_B, M, cudaMemcpyDeviceToHost);)", "label": "(24, CUDA, CHECK(cudaMalloc(&d_B, M));); (24, CUDA, CHECK(cudaMemcpy(d_A, h_A, M, cudaMemcpyHostToDevice));); (46, CUDA, CHECK(cudaFree(d_B));); (20, CUDA, CHECK(cudaMalloc(&d_A, M));); (32, CUDA, CHECK(cudaMemcpy(h_B, d_B, M, cudaMemcpyDeviceToHost));); (42, CUDA, CHECK(cudaFree(d_A));)"}
{"predict": "110:             t1 = getticks();\n111:             d2[n] = (t1-t0)/REPEAT;\n112: \n113:             /* basic w/ stride-1 loads */\n114:             t0 = getticks();\n115:             for ( int t=0 ; t<REPEAT ; t++ )\n116:             {\n117: #ifdef OMP\n118: #pragma omp parallel for if(n>OMP_MIN_SIZE)\n119: #endif\n120:                 for ( int i=0 ; i<n ; i++ )\n121:                     for ( int j=0 ; j<n ; j++ )\n122:                         B[j*n+i] = A[i*n+j];\n123:             }\n124:             t1 = getticks();\n125:             d3[n] = (t1-t0)/REPEAT;\n126: \n127:             /* pur 4x4 + stride-1 loads */\n128:             t0 = getticks();\n129:             for ( int t=0 ; t<REPEAT ; t++ )\n130:             {\n131: #ifdef OMP\n132: #pragma omp parallel for if(n>OMP_MIN_SIZE)\n133: #endif\n134:                 for ( int i=0 ; i<n ; i+=4 )\n135:                     for ( int j=0 ; j<n ; j+=4 )\n136:                     {\n137:                         B[i*n+j] = A[i*n+j];\n138:                         B[i*n+j+1] = A[i*n+j+1];\n139:                         B[i*n+j+2] = A[i*n+j+2];\n140:                         B[i*n+j+3] = A[i*n+j+3];\n141:                     }\n142:             }\n143:             t1 = getticks();\n144:             d4[n] = (t1-t0)/REPEAT;\n145: \n146:             /* mur 4x4 + stride-1 loads */\n147:             t0 = getticks();\n148:             for ( int t=0 ; t<REPEAT ; t++ )", "label": "(8, MPI, MPI_Comm_rank( MPI_COMM_WORLD , &rank );); (364, MPI, MPI_Finalize();); (34, OpenMP, fprintf( stderr , \"starting test: OpenMP ON with %d threads... \\n\", omp_get_max_threads() );); (164, OpenMP, #pragma omp parallel if(n>OMP_MIN_SIZE)); (168, OpenMP, #pragma omp for private(i,j,n4)); (63, OpenMP, #pragma omp parallel for if(n>OMP_MIN_SIZE)); (1, MPI, MPI_Init_thread( &argc , &argv , MPI_THREAD_FUNNELED , &provided );); (28, OpenMP, printf( \"# starting test: OpenMP ON with %d threads... \\n\", omp_get_max_threads() );)"}
{"predict": "(3, CUDA, extern \"C\" __global__ static void pipeline_device(uint32_t const NumIterations) {)", "label": "(5, CUDA, extern __shared__ char shared_memory[];)"}
{"predict": "(12, MPI, MPI_Type_struct(blocks, disps, types, 3, &eviltype);)", "label": "(23, MPI, err = MPI_Type_size(eviltype, &val);); (11, MPI, err = MPI_Type_struct(3, blocks, disps, types, &eviltype);); (66, MPI, err = MPI_Type_get_extent(eviltype, &lb, &extent);); (86, MPI, err = MPI_Type_ub(eviltype, &aval);); (121, MPI, MPI_Type_free(&eviltype);); (99, MPI, err = MPI_Type_get_true_extent(eviltype, &true_lb, &aval);); (32, MPI, err = MPI_Type_extent(eviltype, &aval);); (46, MPI, err = MPI_Type_lb(eviltype, &aval);)"}
{"predict": "(3, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &CommDeuce);) (6, MPI, MPI_Group world_group = MPI_COMM_WORLD.Get_group();) (7, MPI, MPI_Group_incl(world_group, 2, &rank, &comm_group);) (17, MPI, MPI_Sendrecv_replace(A, SIZE, MPI_INT, destrank, 0, CommDeuce, 0, MPI_INT, destrank, 0, CommDeuce, MPI_STATUS_IGNORE);) (36, MPI, MPI_Win_create(A, SIZE * sizeof(int), sizeof(int), MPI_INFO_NULL, CommDeuce, &win);) (48, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, rank, 0, win);) (57, MPI, MPI_Win_unlock(rank, win);) (60, MPI, MPI_Group_free(&group);) (61, MPI, MPI_Group_free(&world_group);) (62, MPI, MPI_Comm_free(&CommDeuce);)", "label": "(67, MPI, MPI_Win_unlock(rank, win);); (28, MPI, i = MPI_Alloc_mem(SIZE * sizeof(int), MPI_INFO_NULL, &B);); (45, MPI, MPI_Win_start(group, 0, win);); (92, MPI, MPI_Win_free(&win);); (13, MPI, MPI_Comm_split(MPI_COMM_WORLD, (rank < 2), rank, &CommDeuce);); (66, MPI, MPI_Win_post(group, 0, win);); (44, MPI, MPI_Send(B, SIZE, MPI_INT, 1, 100, MPI_COMM_WORLD);); (34, MPI, MPI_Win_allocate(0, 1, MPI_INFO_NULL, CommDeuce, &base_ptr, &win);); (47, MPI, MPI_Win_allocate(SIZE * sizeof(int), sizeof(int), MPI_INFO_NULL, CommDeuce, &B, &win);); (63, MPI, MPI_Recv(A, SIZE, MPI_INT, 0, 100, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (63, MPI, MPI_Win_wait(win);); (41, MPI, MPI_Free_mem(B);); (37, MPI, MPI_Win_complete(win);); (35, MPI, MPI_Put(A, SIZE, MPI_INT, 1, 0, SIZE, MPI_INT, win);); (0, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (5, MPI, i = MPI_Alloc_mem(SIZE * sizeof(int), MPI_INFO_NULL, &A);); (82, MPI, MPI_Comm_free(&CommDeuce);); (77, MPI, MPI_Group_free(&comm_group);); (-9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (7, MPI, MPI_Comm_group(CommDeuce, &comm_group);); (-10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (75, MPI, MPI_Free_mem(A);); (71, MPI, MPI_Group_free(&group);); (23, MPI, MPI_Group_incl(comm_group, 1, &destrank, &group);); (38, MPI, MPI_Win_create(B, SIZE * sizeof(int), sizeof(int), MPI_INFO_NULL, CommDeuce, &win);)"}
{"predict": "(29, CUDA, int* d_in = nullptr; int* d_reference = nullptr; unsigned char* d_flags = nullptr; cudaMalloc((void**)&d_in, sizeof(int) * num_items); cudaMalloc((void**)&d_reference, sizeof(int) * num_items); cudaMalloc((void**)&d_flags, sizeof(unsigned char) * num_items);) (35, CUDA, int num_selected = Solve(d_in, d_flags, d_reference, num_items);) (42, CUDA, cudaFree(d_in); cudaFree(d_reference); cudaFree(d_flags);)", "label": "(52, CUDA, CubDebugExit(cudaMemcpy(d_flags, h_flags, sizeof(unsigned char) * num_items, cudaMemcpyHostToDevice));); (50, CUDA, CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));)"}
{"predict": "(5, MPI, int myrank, p; MPI_Comm_rank(comm, &myrank); MPI_Comm_size(comm, &p);) (50, MPI, MPI_Alltoallw(localA, sendcounts, sdispls, sendtypes, localB, recvcounts, rdispls, recvtypes, comm, MPI_INFO_NULL, &req);) (52, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);)", "label": "(58, MPI, MPI_Start(&req);); (13, MPI, MPI_Comm_rank(comm, &myrank);); (76, MPI, MPI_Type_free(&stype[i][j]);); (50, MPI, MPI_Info_create(&info);); (55, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (72, MPI, MPI_Type_free(&xtype[i][j]);); (56, MPI, MPI_Request_free(&req);); (6, MPI, MPI_Comm_size(comm, &p);); (55, MPI, MPI_Info_free(&info);)"}
{"predict": "configuration.fence = &vkGPU->fence;\n59: \t\t\tconfiguration.commandPool = &vkGPU->commandPool;\n60: \t\t\tconfiguration.physicalDevice = &vkGPU->physicalDevice;\n61: #elif(VKFFT_BACKEND==1)\n62: \t\t\tconfiguration.queue = vkGPU->queue;\n63: \t\t\tconfiguration.stream = vkGPU->stream;\n64: #elif(VKFFT_BACKEND==2)\n65: \t\t\tconfiguration.queue = vkGPU->queue;\n66: \t\t\tconfiguration.stream = vkGPU->stream;\n67: #elif(VKFFT_BACKEND==3)\n68: \t\t\tconfiguration.queue = vkGPU->queue;\n69: \t\t\tconfiguration.stream = vkGPU->stream;\n70: #elif(VKFFT_BACKEND==4)\n71: \t\t\tconfiguration.queue = vkGPU->queue;\n72: \t\t\tconfiguration.stream = vkGPU->stream;\n73: #endif\n74: \n75: \t\t\t//app->runFFT(&configuration);\n76: \t\t\tapp.runFFT(&configuration);\n77: \t\t\trun_time[r] = app.getRunTime();\n78: \t\t}\n79: \t\tbenchmark_result += (double)configuration.size[0] / run_time[0];\n80: \t}\n81: \tdouble result = benchmark_result / (double)num_benchmark_samples;\n82: \tif (file_output)\n83: \t\tfprintf(output, \"6 - VkFFT benchmark result: %.4f\\n\", result);\n84: \tprintf(\"6 - VkFFT benchmark result: %.4f\\n\", result);\n85: \tfree(buffer_input);\n86: \treturn resFFT;\n87: }", "label": "(161, OpenCL, clGetDeviceInfo(vkGPU->device, CL_DEVICE_VENDOR_ID, sizeof(cl_int), &vendorID, 0);); (81, CUDA, res = cudaMalloc((void**)&buffer, bufferSize);); (90, OpenCL, buffer = clCreateBuffer(vkGPU->context, CL_MEM_READ_WRITE, bufferSize, 0, &res);)"}
{"predict": "(14, MPI, MPI_Comm_size(intercomm, &rsize);) (17, MPI, MPI_Comm_rank(intercomm, &rank);) (33, MPI, MPI_Recv(&err, 1, MPI_INT, i, 0, intercomm, &status);) (34, MPI, errs += err;) (68, MPI, MPI_Send(&errs, 1, MPI_INT, 0, 0, intercomm);)", "label": "(26, MPI, MPI_Comm_size(intercomm, &size);); (84, MPI, MPI_Comm_disconnect(&intercomm);); (91, MPI, MPI_Finalize();); (11, MPI, MPI_Comm_get_parent(&parentcomm);); (66, MPI, MPI_Recv(&i, 1, MPI_INT, 0, 0, intercomm, &status);); (32, MPI, MPI_Send(&i, 1, MPI_INT, i, 0, intercomm);); (52, MPI, MPI_Comm_get_name(intercomm, cname, &rlen);); (69, MPI, MPI_Ssend(&errs, 1, MPI_INT, 0, 1, intercomm);); (35, MPI, MPI_Recv(&err, 1, MPI_INT, i, 1, intercomm, MPI_STATUS_IGNORE);); (18, MPI, MPI_Comm_rank(intercomm, &rank);); (15, MPI, MPI_Comm_remote_size(intercomm, &rsize);)"}
{"predict": "(4, OpenMP, #pragma omp parallel for)\n(7, OpenMP, #pragma omp parallel for)\n(10, OpenMP, #pragma omp parallel for)\n(17, OpenMP, #pragma omp parallel for)\n(22, OpenMP, #pragma omp parallel for)\n(27, OpenMP, #pragma omp parallel for)\n(30, OpenMP, #pragma omp parallel for)\n(32, OpenMP, #pragma omp parallel for)\n(37, OpenMP, #pragma omp parallel for)\n(40, OpenMP, #pragma omp parallel for)\n(48, OpenMP, #pragma omp parallel for)\n(55, OpenMP, #pragma omp parallel for)\n(62, OpenMP, #pragma omp parallel for)", "label": "(31, OpenMP, #pragma omp teams distribute parallel for collapse (1)); (35, OpenMP, #pragma omp teams distribute parallel for collapse ((ST > 0) ? 1 + ST : 2)); (1, OpenMP, #pragma omp teams distribute parallel for collapse); (10, OpenMP, #pragma omp teams distribute parallel for collapse ()); (44, OpenMP, #pragma omp distribute collapse (S)); (21, OpenMP, #pragma omp teams distribute parallel for collapse (ST); (62, OpenMP, #pragma omp teams distribute parallel for collapse (N)); (13, OpenMP, #pragma omp teams distribute parallel for collapse (argc); (65, OpenMP, #pragma omp teams distribute parallel for collapse (2)); (49, OpenMP, #pragma omp teams distribute parallel for collapse (argv[1]=2)); (-8, OpenMP, #pragma omp target); (-3, OpenMP, #pragma omp teams distribute parallel for collapse (); (32, OpenMP, #pragma omp teams distribute parallel for collapse (foobool(argc)), collapse (true), collapse (-5))"}
{"predict": "size_t)offset_start;\n126:             }\n127:     \n128:             /* compute size of buffer we need to allocate */\n129:             bufsize = region_size + overlap_before;\n130:             buf = (char*)malloc(bufsize);\n131:             if (buf == NULL) {\n132:                 MFU_LOG(MFU_LOG_ERR, \"Failed to allocate buffer of size %zu\", bufsize);\n133:                 rc = MFU_FAILURE;\n134:             }\n135:         }\n136:     \n137:         /* read data from archive file */\n138:         if (rc == MFU_SUCCESS) {\n139:             ssize_t read_size = mfu_read(fd, buf, bufsize);\n140:             if (read_size < 0) {\n141:                 MFU_LOG(MFU_LOG_ERR, \"Failed to read archive file (errno=%d %s)\",\n142:                     errno, strerror(errno)\n143:                 );\n144:                 rc = MFU_FAILURE;\n145:             }\n146:         }\n147:     \n148:         /* read our portion of the archive file */\n149:         if (rc == MFU_SUCCESS) {\n150:             uint64_t read_offset = 0;\n151:             uint64_t read_count = 0;\n152:             while (read_offset < region_size && rc == MFU_SUCCESS) {\n153:                 uint64_t read_len = region_size - read_offset;\n154:                 if (read_len > opts->buf_size) {\n155:                     read_len = opts->buf_size;\n156:                 }\n157:     \n158:                 /* read next chunk of data */\n159:                 ssize_t read_bytes = mfu_read(fd, buf + read_offset, read_len);\n160:                 if (read_bytes < 0) {\n161:                     MFU_LOG(MFU_LOG_ERR, \"Failed to read archive file (errno=%d %s)\",\n162:                         errno, strerror(errno)\n163:                     );\n164:                     rc = MFU_FAILURE;\n165:                 } else if (read_bytes > 0) {\n166:", "label": "(45, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (47, MPI, double start = MPI_Wtime();); (237, MPI, double now = MPI_Wtime();); (41, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(19, MPI, MPI_Comm_rank(comm, &par_rank); MPI_Comm_size(comm, &par_size);)", "label": "(33, MPI, flow::Workspace::set_default_mpi_comm(MPI_Comm_c2f(comm));); (16, MPI, MPI_Comm_rank(comm, &par_rank);); (16, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(4, OpenMP, #pragma omp parallel for private(retVal))", "label": "(13, OpenCL, retVal = clReleaseMemObject(smallBuffer);); (17, OpenCL, EXPECT_EQ(clReleaseContext(context), CL_SUCCESS);); (1, OpenCL, cl_mem smallBuffer = clCreateBufferWithProperties(clContext, &memProperties, flags, size, hostPtr, &retVal);)"}
{"predict": "(4, OpenMP, #pragma omp parallel for private(i, j, ctid, s))", "label": "(8, OpenMP, int tid = omp_get_thread_num();); (4, OpenMP, #pragma omp parallel num_threads(8)); (2, OpenMP, omp_set_affinity_format(formats[i]);)"}
{"predict": "(29, CUDA, __syncthreads();)", "label": "(16, CUDA, __shared__ uninitialized_array<thrust::complex<T>, 1024> cache;); (29, CUDA, cache[id] += static_cast<thrust::complex<T>>(A_[(a * nchol + i) * nocc + b] * A_[(b * nchol + i) * nocc + a]);); (21, CUDA, thrust::complex<T2> const* A_(Tab + blockIdx.x * nocc * nocc * nchol);); (18, CUDA, cache[id]              = thrust::complex<T>(0.0);); (18, CUDA, thrust::complex<T> alp = static_cast<thrust::complex<T>>(alpha);)"}
{"predict": "_cast<void **>(&d_C[i]),\n94:                               sizeof(data_type) * C_array[i].size()));\n95:         CUDA_CHECK(cudaMemcpyAsync(d_A[i], A_array[i].data(),\n96:                                    sizeof(data_type) * A_array[i].size(),\n97:                                    cudaMemcpyHostToDevice, stream));\n98:         CUDA_CHECK(cudaMemcpyAsync(d_B[i], B_array[i].data(),\n99:                                    sizeof(data_type) * B_array[i].size(),\n100:                                   cudaMemcpyHostToDevice, stream));\n101:         CUDA_CHECK(cudaMemcpyAsync(d_C[i], C_array[i].data(),\n102:                                    sizeof(data_type) * C_array[i].size(),\n103:                                    cudaMemcpyHostToDevice, stream));\n104:     }\n105: \n106:     /* step 3: run GEMM */\n107:     for (int i = 0; i < gemm_count; i++) {\n108:         CUBLAS_CHECK(cublasGemmEx(cublasH, transa_array[i], transb_array[i],\n109:                                   m_array[i], n_array[i], k_array[i],\n110:                                   &alpha_array[i], d_A[i], CUDA_R_32F,\n111:                                   d_B[i], CUDA_R_32F, &beta_array[i], d_C[i],\n112:                                   CUDA_R_32F, compute_type));\n113:     }\n114: \n115:     /* step 4: copy data back to host */\n116:     for (int i = 0; i < gemm_count; i++) {\n117:         CUDA_CHECK(cudaMemcpyAsync(C_array[i].data(), d_C[i],\n118:                                    sizeof(data_type) * C_array[i].size(),\n119:                                    cudaMemcpyDeviceToHost, stream));\n120:     }\n121: \n122:     /* step 5: destroy cublas handle */\n123:     CUBLAS_CHECK(cublasDestroy(cublasH));\n124: \n125:     /* step 6: free device memory */\n126:     for (int i = 0; i < gemm_count; i++) {\n127:         CUDA_CHECK(cudaFree(d_A[i]));\n12", "label": "(169, CUDA, CUDA_CHECK(cudaFree(d_B_array));); (169, CUDA, CUDA_CHECK(cudaFree(d_C_array));); (177, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (165, CUDA, CUDA_CHECK(cudaFree(d_A_array));); (168, CUDA, CUDA_CHECK(cudaFree(d_A[i]));); (169, CUDA, CUDA_CHECK(cudaFree(d_C[i]));); (133, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (174, CUDA, CUDA_CHECK(cudaDeviceReset());); (165, CUDA, CUDA_CHECK(cudaFree(d_B[i]));); (75, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(17, CUDA, __syncthreads();)", "label": "(48, CUDA, HANDLE_ERROR( cudaFree( dev_a ) );); (49, CUDA, HANDLE_ERROR( cudaFree( dev_partial_c ) );); (47, CUDA, HANDLE_ERROR( cudaFree( dev_b ) );)"}
{"predict": "(9, OpenMP, #pragma omp parallel for reduction(+:nr_iterations) reduction(+:result))", "label": "(12, OpenMP, omp_set_nest_lock(&lck);); (18, OpenMP, omp_unset_nest_lock(&lck);); (8, OpenMP, #pragma omp for); (10, OpenMP, #pragma omp flush); (4, OpenMP, #pragma omp parallel shared(lck)); (2, OpenMP, omp_init_nest_lock(&lck);); (16, OpenMP, omp_destroy_nest_lock(&lck);)"}
{"predict": "*outkeysat = values->keysat;\n117:   *outcmp    = values->cmp;\n118: \n119:   return DTCMP_SUCCESS;\n120: }", "label": "(19, MPI, MPI_Type_get_true_extent(inkeysat, &keysat_true_lb, &keysat_true_extent);); (76, MPI, MPI_Type_dup(inkey,    &values->key);); (13, MPI, MPI_Type_get_true_extent(inkey, &key_true_lb, &key_true_extent);); (75, MPI, MPI_Type_dup(inkeysat, &values->keysat);)"}
{"predict": "(7, OpenMP, #pragma omp task shared(x) firstprivate(n) x = fib(n);) (8, OpenMP, #pragma omp task shared(y) firstprivate(n) y = fib(n);) (9, OpenMP, #pragma omp taskwait;)", "label": "(6, TBB, tbb::task_group g;); (6, TBB, tbb::task_handle h1 = make_task(g, x, n-1);); (6, TBB, tbb::task_handle h2 = make_task(g, y, n-2);)"}
{"predict": "(16, CUDA, for (i = 0; i < ucc_ec_cuda_config->exec_num_streams; i++) { if (resources->exec_streams[i] != NULL) { cudaStreamDestroy(resources->exec_streams[i]); } });", "label": "(17, CUDA, CUDA_FUNC(cudaStreamDestroy(resources->exec_streams[i]));)"}
{"predict": "_code);\n105:     if (error_code != MPI_SUCCESS) {\n106:         goto fn_fail;\n107:     }\n108: \n109:     return MPI_SUCCESS;\n110: fn_fail:\n111:     if (dupcomm != MPI_COMM_NULL) {\n112:         MPI_Comm_free(&dupcomm);\n113:     }\n114:     return error_code;\n115: }", "label": "(31, MPI, error_code = MPI_Comm_test_inter(comm, &flag);); (56, MPI, MPI_Comm_dup(comm, &dupcomm);); (126, MPI, MPI_Comm_rank(dupcomm, &rank);); (136, MPI, MPI_Barrier(dupcomm);); (67, MPI, MPI_Allreduce(&amode, &tmp_amode, 1, MPI_INT, ADIO_same_amode, dupcomm);)"}
{"predict": "(7, OpenMP, #pragma omp parallel for private(index, akey, mask_key, unmasked_keys, hashed_mask_key, i, j))", "label": "(5, OpenMP, #pragma omp parallel for)"}
{"predict": "std::cout << \"zoneId = \" << zoneId << \" index_dim = \" << index_dim << \"\\n\";\n57:             }\n58:         }\n59:     }\n60: \n61:     if ( Parallel::pid == Parallel::serverid )\n62:     {\n63:         cg_close( fileId );\n64:     }\n65: }", "label": "(224, MPI, MPI_Bcast( &bocotype, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (190, MPI, MPI_Bcast( &nbocos, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (202, MPI, MPI_Bcast( &location, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (38, MPI, MPI_Bcast( &nzones, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (84, MPI, MPI_Bcast( zonename, 33, MPI_CHAR, Parallel::serverid, MPI_COMM_WORLD );); (295, MPI, MPI_Bcast( range.data(), npnts * index_dim, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (219, MPI, MPI_Bcast( &ptset_type, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (251, MPI, MPI_Bcast( pnts.data(), npnts * index_dim, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (81, MPI, MPI_Bcast( isize.data(), index_dim * 3, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (113, MPI, MPI_Bcast( &zoneType, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (220, MPI, MPI_Bcast( normalIndex.data(), index_dim, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (216, MPI, MPI_Bcast( &npnts, 1, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (289, MPI, MPI_Bcast( donor_range.data(), npnts * index_dim, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (164, MPI, MPI_Bcast( pcoor, nNodes, MPI_DOUBLE, Parallel::serverid, MPI_COMM_WORLD );); (145, MPI, MPI_Bcast( &dataType, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (283, MPI, MPI_Bcast( connectname, 33, MPI_CHAR, Parallel::serverid, MPI_COMM_WORLD );); (243, MPI, MPI_Bcast( normalList.data(), nNodes * iphysdim * sizeof( double ), MPI_CHAR, Parallel::serverid, MPI_COMM_WORLD );); (-3, MPI, MPI_Bcast( &nbases, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (205, MPI, MPI_Bcast( boconame, 33, MPI_CHAR, Parallel::serverid, MPI_COMM_WORLD );); (139, MPI, MPI_Bcast( coordname, 33, MPI_CHAR, Parallel::serverid, MPI_COMM_WORLD );); (208, MPI, MPI_Bcast( &normalListSize, 1, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (256, MPI, MPI_Bcast( &n1to1, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (6, MPI, MPI_Bcast( &icelldim, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (6, MPI, MPI_Bcast( &iphysdim, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (205, MPI, MPI_Bcast( &ndataset, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (274, MPI, MPI_Bcast( donorname, 33, MPI_CHAR, Parallel::serverid, MPI_COMM_WORLD );); (104, MPI, MPI_Bcast( &ncoords, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (199, MPI, MPI_Bcast( &normalDataType, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );)"}
{"predict": "(13, OpenMP, #pragma omp parallel for)", "label": "(58, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (58, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)"}
{"predict": "(11, CUDA, axpy_gpu_legacy<<<numBlocks, threadsPerBlock>>>(2.0, x, y);)", "label": "(3, CUDA, using alloc = thrust::cuda::allocator<double>;); (11, CUDA, cudaDeviceSynchronize(); benchmark::DoNotOptimize(y.base()); benchmark::ClobberMemory();)"}
{"predict": "(36, OpenMP, #pragma omp parallel for private(i))", "label": "(32, OpenMP, #pragma omp target); (35, OpenMP, int device_id = omp_get_default_device();); (54, OpenMP, #pragma omp target data use_device_ptr(a))"}
{"predict": "(16, CUDA, vadd<<<blocks, threads>>>(d_A, d_B, d_C, DSIZE);)", "label": "(28, CUDA, cudaCheckErrors(\"kernel execution failure or cudaMemcpy H2D failure\");); (14, CUDA, cudaMemcpy(d_A, h_A, DSIZE*sizeof(float), cudaMemcpyHostToDevice);); (16, CUDA, cudaCheckErrors(\"cudaMemcpy H2D failure\");); (23, CUDA, cudaMemcpy(h_C, d_C, DSIZE*sizeof(float), cudaMemcpyDeviceToHost);); (13, CUDA, cudaMemcpy(d_B, h_B, DSIZE*sizeof(float), cudaMemcpyHostToDevice);); (7, CUDA, cudaMalloc(&d_C, DSIZE*sizeof(float));); (5, CUDA, cudaMalloc(&d_B, DSIZE*sizeof(float));); (6, CUDA, cudaCheckErrors(\"cudaMalloc failure\");); (15, CUDA, cudaCheckErrors(\"kernel launch failure\");); (1, CUDA, cudaMalloc(&d_A, DSIZE*sizeof(float));)"}
{"predict": "(18, OpenMP, #pragma omp parallel for reduction(+:C[:N2]))", "label": "(17, OpenMP, #pragma omp target teams distribute parallel for collapse(2)); (23, OpenMP, #pragma omp target exit data map(delete:A[0:N1],B[0:N1],C[0:N2])); (14, OpenMP, #pragma omp target enter data map(to: A[0:N1],B[0:N1],C[0:N2])); (20, OpenMP, #pragma omp target update from(C[0:N2]))"}
{"predict": "(16, OpenMP, #pragma omp parallel for private(b))", "label": "(16, OpenMP, #pragma omp parallel for)"}
{"predict": "(32, OpenMP, #pragma omp parallel for)", "label": "(54, OpenCL, clReleaseEvent(event);)"}
{"predict": "(13, MPI, for (li=0; li<*v2; li++) { l4[li] = MPIR_FROM_FLOG(v4[li]); } -> #pragma omp parallel for)", "label": "(17, MPI, *ierr = MPI_Cart_create( (MPI_Comm)(*v1), *v2, v3, l4, l5, (MPI_Comm *)(v6) );)"}
{"predict": "Malloc fails for dtemp[].\");\n108:       }\n109:     }\n110: \n111:     /* ------------------------------------------------------------\n112:        RECEIVE THE A TRIPLETS.\n113:        ------------------------------------------------------------*/\n114:     ia[0] = 0;\n115:     for (p = 0; p < procs; ++p) {\n116:         if ( p != iam ) {\n117:             MPI_Recv( ja+ia[p], nnzToRecv[p], MPI_INT, p, 0, grid->comm,\n118:                      &status );\n119:             MPI_Recv( aij+ia[p], nnzToRecv[p], MPI_FLOAT, p, 1, grid->comm,\n120:                      &status );\n121:         } else {\n122:             ia[p+1] = ia[p] + nnzToRecv[p];\n123:         }\n124:     }\n125: \n126:     /* ------------------------------------------------------------\n127:        SEND THE A TRIPLETS.\n128:        ------------------------------------------------------------*/\n129:     ia_send[iam] = ia;\n130:     aij_send[iam] = aij;\n131:     ptr_to_send[iam] = 0;\n132:     if ( procs > 1 ) {\n133:         for (p = 0; p < procs; ++p) {\n134:             if ( p != iam ) {\n135:                 MPI_Isend( ja+ia[p], nnzToSend[p], MPI_INT, p, 0, grid->comm,\n136:                           &send_req[2*p]);\n137:                 MPI_Isend( aij+ia[p], nnzToSend[p], MPI_FLOAT, p, 1, grid->comm,\n138:                           &send_req[2*p+1]);\n139:             }\n140:         }\n141:         MPI_Waitall(2*procs, send_req, MPI_STATUSES_IGNORED);\n142:     }\n143: \n144:     /* ------------------------------------------------------------\n145:        REDISTRIBUTE THE A TRIPLETS.\n146:        ------------------------------------------------------------*/\n147:     if (", "label": "(191, MPI, MPI_Wait( &send_req[p], &status);); (191, MPI, MPI_Wait( &send_req[procs+p], &status);); (170, MPI, MPI_Recv( itemp, it, mpi_int_t, p, p, grid->comm, &status );)"}
{"predict": "(15, CUDA, RGBToRGB_Norm<T_in, T_out, isBGR><<<gridDim, blockDim, 0, stream>>>( srcDev, dstDev, width, height, inputRange, multiplier);)", "label": "(16, CUDA, return CUDA(cudaGetLastError());)"}
{"predict": "(49, CUDA, CHECK_CUDA(cudaMalloc((void **)&dx, dataSize * sizeof(int32_t)))); (51, CUDA, CHECK_CUDA(cudaMalloc((void **)&dy, dataSize * sizeof(int32_t)))); (69, CUDA, CHECK_CUDA(cudaFree(dy))); (70, CUDA, CHECK_CUDA(cudaFree(dx)));", "label": "(50, CUDA, CHECK_CUDA(cudaMalloc(&dy, dataSize * sizeof(int32_t)));); (75, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (32, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (76, CUDA, CHECK_CUDA(cudaFree(dy));); (45, CUDA, CHECK_CUDA(cudaMalloc(&dx, dataSize * sizeof(int32_t)));); (32, CUDA, CHECK_HIEDNN(hiednnSetCudaStream(handle, 0));); (72, CUDA, CHECK_CUDA(cudaFree(dx));)"}
{"predict": "(17, MPI, MPI_Init(&argc, &argv);)", "label": "(23, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(7, OpenMP, #pragma omp parallel for reduction(+:Red)) (11, OpenMP, #pragma omp parallel for reduction(+:Red)) (15, OpenMP, #pragma omp parallel for reduction(+:Red)) (19, OpenMP, #pragma omp parallel for reduction(+:Red)) (23, OpenMP, #pragma omp parallel for reduction(+:Red)) (27, OpenMP, #pragma omp parallel for reduction(+:Red))", "label": "(28, OpenMP, #pragma omp parallel for reduction(&&: Red)); (7, OpenMP, #pragma omp parallel for reduction(-: Red)); (14, OpenMP, #pragma omp parallel for reduction(&: Red)); (17, OpenMP, #pragma omp parallel for reduction(|: Red)); (0, OpenMP, #pragma omp parallel for reduction(+: Red)); (19, OpenMP, #pragma omp parallel for reduction(^: Red)); (6, OpenMP, #pragma omp parallel for reduction(*: Red)); (25, OpenMP, #pragma omp parallel for reduction(||: Red))"}
{"predict": "(7, OpenMP, #pragma omp parallel for)", "label": "(25, OpenMP, #pragma omp simd aligned(e, g)); (18, OpenMP, #pragma omp simd aligned (argc : 5)); (29, OpenMP, #pragma omp simd aligned(i)); (5, OpenMP, #pragma omp simd aligned (); (17, OpenMP, #pragma omp simd aligned (S1)); (28, OpenMP, #pragma omp parallel); (40, OpenMP, #pragma omp simd aligned(f:j)); (10, OpenMP, #pragma omp simd aligned (*argc > 0 ? argv[1] : argv[2])); (6, OpenMP, #pragma omp simd aligned (argc,); (14, OpenMP, #pragma omp simd aligned (argv[1])); (-4, OpenMP, #pragma omp simd aligned); (-1, OpenMP, #pragma omp simd aligned ()); (25, OpenMP, #pragma omp simd aligned(v:16)); (28, OpenMP, #pragma omp simd aligned(f)); (14, OpenMP, #pragma omp simd aligned(h)); (-3, OpenMP, #pragma omp simd aligned (argc)"}
{"predict": "(10, MPI, MPI_Type_ub(type, &errclass);)", "label": "(13, MPI, MPI_Type_size(type, &size);); (14, MPI, mpi_errno = MPI_Type_ub(type, NULL);); (18, MPI, MPI_Type_free(&type);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (11, MPI, MPI_Error_class(mpi_errno, &errclass);); (4, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(13, MPI, MPI_Comm_size(comm, &size);) (14, MPI, MPI_Comm_rank(comm, &rank);) (24, MPI, MPI_Reduce_scatter(sendbuf, recvbuf, recvcounts, MPI_INT, MPI_SUM, comm, &req);)", "label": "(26, MPI, MPI_Start(&req);); (11, MPI, MPI_Comm_size(comm, &size);); (13, MPI, MPI_Info_create(&info);); (24, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (36, MPI, MPI_Info_free(&info);); (34, MPI, MPI_Request_free(&req);); (18, MPI, MPI_Reduce_scatter_init(sendbuf, &recvbuf, recvcounts, MPI_INT, MPI_SUM, comm, info, &req);); (6, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(7, MPI, if(rank==0) printf(\"Support for MPI is enabled\\n\");)", "label": "(4, MPI, MPI_Init(&argc, &argv);); (30, MPI, MPI_Finalize();); (3, MPI, int ierr = MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(29, CUDA, block_radix_sort_t(temp_storage).SortDescending(thread_keys[threadIdx.x], thread_values[threadIdx.x], decomposer_t{});)", "label": "(7, CUDA, __shared__ block_radix_sort_t::TempStorage temp_storage;)"}
{"predict": ":     half2 stats;\n98:     stats.x = __hadd(stats_local.x, stats_local.y);\n99:     stats.y = __hadd(stats_local.z, stats_local.w);\n100: \n101:     half2 stats_reduced = __shfl_xor_sync(uint32_t(-1), stats, 1);\n102:     __syncwarp();\n103: \n104:     stats.x = __hadd(stats.x, stats_reduced.x);\n105:     stats.y = __hadd(stats.y, stats_reduced.y);\n106: \n107:     stats = __hadd(stats, __shfl_xor_sync(uint32_t(-1), stats, 2));\n108:     stats = __hadd(stats, __shfl_xor_sync(uint32_t(-1), stats, 4));\n109:     stats = __hadd(stats, __shfl_xor_sync(uint32_t(-1), stats, 8));\n110:     stats = __hadd(stats, __shfl_xor_sync(uint32_t(-1), stats, 16));\n111: \n112:     if (is_cta_lead)\n113:     {\n114:         *output = stats.x;\n115:         *preln = stats.y;\n116:     }\n117: }", "label": "(24, CUDA, extern __shared__ char smem_[];); (26, CUDA, __shared__ half2 smem_red[VECS_PER_CTA][WARPS];)"}
{"predict": ": \terr = clCreateProgramWithSource(context, 1, (const char **)&source_str, NULL, &err);\n131: \tERR_MACRO(err);\n132: \t\n133: \terr = clBuildProgram(plan->program, num_devices, devices, NULL, NULL, NULL);\n134: \tERR_MACRO(err);\n135: \t\n136: \terr = clCreateKernelsInProgram(plan->program, plan->num_kernels, NULL, &plan->num_kernels);\n137: \tERR_MACRO(err);\n138: \t\n139: \treturn plan;\n140: }", "label": "(76, OpenCL, err = clBuildProgram(plan->program, 1, &devices[i], coptions, NULL, NULL);); (59, OpenCL, plan->program = clCreateProgramWithSource(context, 1, (const char**) &source_str, NULL, &err);); (68, OpenCL, err = clGetDeviceInfo(devices[i], CL_DEVICE_TYPE, sizeof(device_type), &device_type, NULL);); (88, OpenCL, err = clGetDeviceInfo(devices[i], CL_DEVICE_NAME, sizeof(devicename), devicename, NULL);)"}
{"predict": "(16, MPI, MPI_Sendrecv(in, count, type, right, tag, out, count, type, left, tag, comm, MPI_STATUS_IGNORE);)", "label": "(24, MPI, MPI_Recv(&ack, 1, MPI_CHAR, right, tag, comm, MPI_STATUS_IGNORE);); (26, MPI, MPI_Irecv(out, count, type, set_src ? left : MPI_ANY_SOURCE, tag, comm, &requests[w]);); (28, MPI, MPI_Isend(in, count, type, left, (tag == MPI_ANY_TAG ? 0 : tag), comm, &requests[w + window_size]);); (20, MPI, MPI_Waitall(2 * window_size, requests, MPI_STATUSES_IGNORE);); (17, MPI, MPI_Isend(in, count, type, right, (tag == MPI_ANY_TAG ? 0 : tag), comm, &requests[w + window_size]);); (13, MPI, MPI_Irecv(out, count, type, set_src ? right : MPI_ANY_SOURCE, tag, comm, &requests[w]);); (27, MPI, MPI_Send(&ack, 1, MPI_CHAR, left, (tag == MPI_ANY_TAG ? 0 : tag), comm);)"}
{"predict": "(14, OpenMP, #pragma omp parallel for collapse(1))", "label": "(13, OpenMP, #pragma omp parallel master taskloop simd collapse (ST); (5, OpenMP, #pragma omp parallel master taskloop simd collapse ()); (15, OpenMP, #pragma omp parallel master taskloop simd collapse ((ST > 0) ? 1 + ST : 2)); (1, OpenMP, #pragma omp parallel master taskloop simd collapse (); (20, OpenMP, #pragma omp parallel master taskloop simd collapse (S)); (10, OpenMP, #pragma omp parallel master taskloop simd collapse (1))); (27, OpenMP, #pragma omp parallel master taskloop simd collapse (1)); (30, OpenMP, #pragma omp parallel master taskloop simd collapse (2)); (14, OpenMP, #pragma omp parallel master taskloop simd collapse (foobool(argc)), collapse (true), collapse (-5)); (-7, OpenMP, #pragma omp parallel master taskloop simd collapse); (0, OpenMP, #pragma omp parallel master taskloop simd collapse (argc); (24, OpenMP, #pragma omp parallel master taskloop simd collapse (N))"}
{"predict": "(10, TBB, tbb::parallel_for(0, 10, [](int i) { std::printf(\"N\"); std::this_thread::sleep_for(std::chrono::milliseconds(10)); }, tgc);) (15, TBB, tbb::parallel_for(0, 10, [](int i) { std::printf(\"H\"); std::this_thread::sleep_for(std::chrono::milliseconds(10)); }, tgc);)", "label": "(5, TBB, tbb::task_group_context tgc;); (15, TBB, tgc.set_priority(tbb::priority_high);); (4, TBB, tgc.set_priority(tbb::priority_normal);)"}
{"predict": "(9, OpenMP, #pragma omp parallel for private(r_item) shared(rData) reduction(+:rData))", "label": "(8, OpenMP, #pragma omp parallel for)"}
{"predict": "(14, OpenMP, #pragma omp parallel);", "label": "(13, OpenMP, #pragma omp parallel); (13, OpenMP, #pragma omp task if (task: false))"}
{"predict": "(10, OpenMP, #pragma omp parallel for schedule(static);)\n(16, OpenMP, #pragma omp parallel for schedule(static);)\n(22, OpenMP, #pragma omp parallel for schedule(static);)\n(28, OpenMP, #pragma omp parallel for schedule(static);)\n(34, OpenMP, #pragma omp parallel for schedule(static);)\n(40, OpenMP, #pragma omp parallel for schedule(static);)\n(46, OpenMP, #pragma omp parallel for schedule(static);)", "label": "(42, OpenMP, #pragma omp distribute parallel for schedule(static, ch)); (49, OpenMP, #pragma omp distribute parallel for schedule(dynamic)); (7, OpenMP, #pragma omp distribute parallel for); (15, OpenMP, #pragma omp distribute parallel for dist_schedule(static)); (30, OpenMP, #pragma omp distribute parallel for schedule(static)); (53, OpenMP, #pragma omp distribute parallel for schedule(dynamic, ch)); (5, OpenMP, #pragma omp cancel for); (0, OpenMP, #pragma omp target); (18, OpenMP, #pragma omp distribute parallel for dist_schedule(static, ch)); (-1, OpenMP, #pragma omp teams)"}
{"predict": "(8, OpenMP, #pragma omp parallel for private(k))", "label": "(35, OpenMP, #pragma omp distribute private(e, g)); (51, OpenMP, #pragma omp parallel private(i)); (5, OpenMP, #pragma omp distribute private); (12, OpenMP, #pragma omp distribute private (argc,); (9, OpenMP, #pragma omp distribute private (argc); (36, OpenMP, #pragma omp target); (19, OpenMP, #pragma omp distribute private (argv[1])); (12, OpenMP, #pragma omp distribute private (argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (9, OpenMP, #pragma omp distribute private (argc > 0 ? argv[1] : argv[2])); (14, OpenMP, #pragma omp distribute private (a, b, c, d, f)); (17, OpenMP, #pragma omp distribute private(ba)); (39, OpenMP, #pragma omp distribute private(j)); (-1, OpenMP, #pragma omp distribute private ()); (24, OpenMP, #pragma omp distribute private(h)); (44, OpenMP, #pragma omp teams reduction(+:i)); (18, OpenMP, #pragma omp distribute private(S2::S2s)); (43, OpenMP, #pragma omp distribute private(i)); (14, OpenMP, #pragma omp distribute private(da)); (-9, OpenMP, #pragma omp distribute private (); (2, OpenMP, #pragma omp distribute private (S1)); (29, OpenMP, #pragma omp teams private(i)); (8, OpenMP, #pragma omp distribute private(ca)); (17, OpenMP, #pragma omp distribute); (22, OpenMP, #pragma omp distribute firstprivate(i)); (30, OpenMP, #pragma omp teams firstprivate(i)); (17, OpenMP, #pragma omp teams)"}
{"predict": "(15, CUDA, if (lid < n_labels){ label = labels[lid]; write_at_count = ioa[lid]; } else { label = -1; write_at_count = -1; } smem[tid] = label; smem[tid + _TPB_] = write_at_count; __syncthreads(); if (tid < n_labels){ label = smem[tid]; write_at_count = smem[tid + _TPB_]; } if (lid < n_labels){ write_adr[lid] = label; } __syncthreads(); if (tid < n_labels){ write_adr[lid] = write_at_count; } __syncthreads(); if (tid < n_labels){ write_adr[lid] = empty_adr[lid]; } __syncthreads(); if (tid < n_labels){ write_adr[lid] = div_of_empty_adr[lid]; } __syncthreads(); if (tid < n_labels){ write_adr[lid] = ioa[lid]; } __syncthreads(); if (tid < n_labels){ write_adr[lid] = labels[lid]; } __syncthreads(); if (tid < n_labels){ write_adr[lid] = empty_adr[lid]; } __syncthreads(); if (tid < n_labels){ write_adr[lid] = div_of_empty_adr[lid]; } __syncthreads(); if (tid < n_labels){ write_adr[lid] = ioa[lid]; } __syncthreads(); if (tid < n_labels){ write_adr[lid] = labels[lid]; } __syncthreads(); if (tid < n_labels){ write_adr[lid] = empty_adr[lid]; } __syncthreads(); if (tid < n_labels){ write_adr[lid] = div_of_empty_adr[lid]; } __syncthreads(); if (tid < n_labels){ write_adr[lid] = ioa[lid]; } __syncthreads(); if (tid < n_labels){ write_adr[lid] = labels[lid]; } __syncthreads(); if (tid < n_labels){ write_adr[lid] = empty_adr[lid]; } __syncthreads(); if (tid < n_labels){ write_adr[lid] = div_of_empty_adr[lid]; } __syncthreads(); if (tid < n_labels){", "label": "(22, CUDA, __shared__ volatile ll_t smem[_TPB_];)"}
{"predict": "MG time: \" << times[8] << \" seconds.\" << endl;\n123: #endif\n124: \n125:   // Compute optimization ratio\n126:   double optimization_ratio = times[8] / times[9];\n127:   if (rank==0) {\n128:     HPCG_fout << \"SpMV+MG optimization ratio: \" << optimization_ratio << \"\\n\" << endl;\n129:   }\n130: \n131:   // Free memory\n132:   DeleteVector(&x_overlap);\n133:   DeleteVector(&b_computed);\n134: \n135:   ///////////////////////////////\n136:   // Reference SpMV+MG Timing Phase //\n137:   ///////////////////////////////\n138: \n139:   // Call Reference SpMV and MG. Compute Optimization time as ratio of times in these routines\n140: \n141:   // First load vector with random values\n142:   FillRandomVector(x);\n143: \n144:   numberOfCalls = 10;\n145:   if (quickPath) numberOfCalls = 1; //QuickPath means we do on one call of each block of repetitive code\n146:   t_begin = mytimer();\n147:   for (int i=0; i< numberOfCalls; ++i) {\n148:     ierr = ComputeSPMV_ref(A, x, b); // b = A*x\n149:     if (ierr) HPCG_fout << \"Error in call to SpMV: \" << ierr << \".\\n\" << endl;\n150:     ierr = ComputeMG_ref(A, b, x); // b = Minv*y\n151:     if (ierr) HPCG_fout << \"Error in call to MG: \" << ierr << \".\\n\" << endl;\n152:   }\n153:   times[7] = (mytimer() - t_begin)/((double) numberOfCalls);  // Total time divided by number of calls.\n154: #ifdef HPCG_DEBUG\n155:   if (rank==0) HPCG_fout << \"Total SpMV+MG time: \" << times[7] <<", "label": "(3, MPI, MPI_Init(&argc, &argv);); (303, MPI, MPI_Finalize();); (23, MPI, MPI_Barrier(MPI_COMM_WORLD);); (222, MPI, MPI_Allreduce(&local_opt_worst_time, &opt_worst_time, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);)"}
{"predict": "(8, OpenMP, #pragma omp parallel for reduction(+:res.x, res.y, res.z))", "label": "(7, CUDA, fcuda::Check_CudaErroorFun(\"DgReduSumFloat3\");); (5, CUDA, cudaMemcpy(data,datag+inidata,sizeof(float3)*ndata,cudaMemcpyDeviceToHost);)"}
{"predict": "_fixed4_div_cpt_rev_fp\n72: ATOMIC_CMPXCHG_CPT_REV_MIX(fixed4u, kmp_uint32, div_cpt_rev, 32, /, fp, _Quad,\n73:                            4i, 3, 0) // __kmpc_atomic_fixed4u_div_cpt_rev_fp\n74: \n75: ATOMIC_CMPXCHG_CPT_REV_MIX(fixed8, kmp_int64, sub_cpt_rev, 64, -, fp, _Quad, 8i,\n76:                            5, 0) // __kmpc_atomic_fixed8_sub_cpt_rev_fp\n77: ATOMIC_CMPXCHG_CPT_REV_MIX(fixed8u, kmp_uint64, sub_cpt_rev, 64, -, fp, _Quad,\n78:                            8i, 5, 0) // __kmpc_atomic_fixed8u_sub_cpt_rev_fp\n79: ATOMIC_CMPXCHG_CPT_REV_MIX(fixed8, kmp_int64, div_cpt_rev, 64, /, fp, _Quad, 8i,\n80:                            5, 0) // __kmpc_atomic_fixed8_div_cpt_rev_fp\n81: ATOMIC_CMPXCHG_CPT_REV_MIX(fixed8u, kmp_uint64, div_cpt_rev, 64, /, fp, _Quad,\n82:                            8i, 5, 0) // __kmpc_atomic_fixed8u_div_cpt_rev_fp\n83: \n84: ATOMIC_CMPXCHG_CPT_REV_MIX(fixed10, int128, sub_cpt_rev, 128, -, fp, _Quad, 10i,\n85:                            6, 0) // __kmpc_atomic_fixed10_sub_cpt_rev_fp\n86: ATOMIC_CMPXCHG_CPT_REV_MIX(fixed10u, uint128, sub_cpt_rev, 128, -, fp, _Quad,\n87:                            10i, 6, 0) // __kmpc_atomic_fixed10u_sub_cpt_rev_fp\n88: ATOMIC_CMPXCHG_CPT_REV_MIX(fixed10", "label": "(127, OpenMP, #define GOMP_CRITICAL_SWP(FLAG))"}
{"predict": "(14, CUDA, cudaEvent_t startEvent, stopEvent; cudaEventCreate(&startEvent); cudaEventCreate(&stopEvent);)\n(26, CUDA, cudaEventRecord(startEvent, 0);)\n(28, CUDA, time = timer.toc();)\n(30, CUDA, cudaEventRecord(stopEvent, 0);)\n(31, CUDA, cudaEventSynchronize(stopEvent);)\n(33, CUDA, cudaEventElapsedTime(&time, startEvent, stopEvent);)\n(40, CUDA, cudaEventDestroy(startEvent); cudaEventDestroy(stopEvent);)", "label": "(27, CUDA, cudaMemcpy(dst, src, bytes, kind);)"}
{"predict": "(11, OpenMP, #pragma omp parallel for)", "label": "(9, TBB, tbb::atomic<int> b;); (6, TBB, std::list< std::pair< const int, tbb::atomic<int> > > arrIntTbb;)"}
{"predict": ":        ------------------------------------------------------------ */\n143:     /* Initialize A, b, xtrue, and other parameters. */\n144:     init_problem (&grid, &A, &b, &xtrue, &b1, &xtrue1, nrhs, &options, &stat);\n145: \n146:     /* ------------------------------------------------------------\n147:        SET UP SCALE/PERM STRUCTS.\n148:        ------------------------------------------------------------ */\n149:     ScalePermstructInit (&ScalePermstruct);\n150:     LUstructInit (&LUstruct);\n151:     SOLVEstructInit (&SOLVEstruct);\n152: \n153:     /* ------------------------------------------------------------\n154:        SOLVE THE LINEAR SYSTEM.\n155:        ------------------------------------------------------------ */\n156:     if (iam == 0) {\n157:         printf (\"Starting LU factorization and solve...\\n\");\n158:         fflush(stdout);\n159:     }\n160:     dgesv_dist (&grid, &A, &b, nrhs, &ScalePermstruct, &LUstruct, &SOLVEstruct, &options, &stat, &info);\n161: \n162:     /* ------------------------------------------------------------\n163:        VERIFY THE SOLUTION.\n164:        ------------------------------------------------------------ */\n165:     if (iam == 0) {\n166:         printf (\"Verifying the solution...\\n\");\n167:         fflush(stdout);\n168:     }\n169:     check_result (&grid, &A, &b, &xtrue, &xtrue1, nrhs, &SOLVEstruct, &options, &stat, &berr);\n170: \n171:     /* ------------------------------------------------------------\n172:        RELEASE THE DATA STRUCTURES.\n173:        ------------------------------------------------------------ */\n174:     Destroy_SuperMatrix_Store (&A);\n175:     Destroy_SuperNode_Matrix (&LUstruct.L);\n176:     Destroy_CompCol_Matrix (&LUstruct.U);\n177:     Destroy_CompCol_Matrix (&LUstruct.LT);\n178:     Destroy_CompCol_Matrix (&LUstruct.UT);\n179:     Destroy_ScalePermstruct (&ScalePermstruct);\n180:     Destroy_LUstruct (&LUstruct);\n181:     Destroy_SOLVEstruct (&S", "label": "(97, MPI, MPI_Query_thread(&omp_mpi_level);); (38, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (33, MPI, MPI_Init_thread(&argc, &argv, required, &provided);)"}
{"predict": "IOV array for the process\n114:      *********************************************************************/\n115:     ret = mca_fcoll_vulcan_get_iov_array ( fh, decoded_iov, iov_count, fh->f_procs_in_group,\n116:                                           fh->f_rank, fh->f_comm, fh->f_aggr_list,\n117:                                           fh->f_num_aggrs, fh->f_procs_per_group, fh->f_iov_buffer_size,\n118:                                           fh->f_iov_count_per_group, fh->f_iov_count_per_aggregator,\n119:                                           fh->f_iov_offset_per_aggregator, fh->f_iov_offset_per_group,\n120:                                           fh->f_iov_size_per_aggregator, fh->f_iov_size_per_group,\n121:                                           fh->f_iov_type_per_aggregator, fh->f_iov_type_per_group,\n122:                                           fh->f_iov_buffer_type, fh->f_iov_buffer_size_per_aggregator,\n123:                                           fh->f_iov_buffer_size_per_group, fh->f_iov_buffer_offset_per_aggregator,\n124:                                           fh->f_iov_buffer_offset_per_group, fh->f_iov_buffer_size_per_aggregator,\n125:                                           fh->f_iov_buffer_size_per_group, fh->f_iov_buffer_offset_per_aggregator,\n126:                                           fh->f_iov_buffer_offset_per_group, fh->f_iov_buffer_size_per_aggregator,\n127:                                           fh->f_iov_buffer_size_per_group, fh->f_iov_buffer_offset_per_aggregator,\n128:                                           fh->f_iov_buffer_offset_per_group, fh->f_iov_buffer_size_per_aggregator,\n129:                                           fh->f_iov_buffer_size_per_group, fh->f_iov_buffer_offset_per_aggregator,\n130:                                           fh->f_iov_buffer_offset_per_group, fh->f_iov_buffer_size_per_aggregator,\n131:                                           fh->f_iov_buffer_size_per_group, fh->f_iov_buffer_offset_per_aggregator,\n132:                                           fh->f_iov_buffer_offset_per_group, fh->f_iov_buffer_size_per_aggregator,\n133:                                           fh->f_iov_buffer_size_per_group, fh->f_iov_buffer_offset_per_aggregator,\n134:                                           fh->", "label": "(391, MPI, start_exch = MPI_Wtime();); (143, MPI, start_comm_time = MPI_Wtime();); (156, MPI, end_comm_time = MPI_Wtime();); (435, MPI, start_write_time = MPI_Wtime();); (442, MPI, end_write_time = MPI_Wtime();); (495, MPI, end_exch = MPI_Wtime();)"}
{"predict": "(12, CUDA, cudaMallocManaged(reinterpret_cast<void **>(&d_data), sizeof(data_type) * h_data.size());)", "label": "(33, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (35, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(17, MPI, MPI_Scatter(a, 100, MPI_INT, MPI_IN_PLACE, 100, MPI_INT, 0, MPI_COMM_WORLD);)", "label": "(43, MPI, MPI_Pack(a, 1, xpose, buffer, bufsize, &position, MPI_COMM_WORLD);); (59, MPI, MPI_Type_free(&row);); (31, MPI, MPI_Type_commit(&xpose);); (56, MPI, MPI_Type_free(&xpose);); (28, MPI, MPI_Type_hvector(100, 1, sizeofint, row, &xpose);); (23, MPI, MPI_Type_extent(MPI_INT, &sizeofint);); (30, MPI, MPI_Pack_size(1, xpose, MPI_COMM_WORLD, &bufsize);); (34, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (39, MPI, MPI_Unpack(buffer, bufsize, &position, b, 100 * 100, MPI_INT, MPI_COMM_WORLD);); (22, MPI, MPI_Type_vector(100, 1, 100, MPI_INT, &row);)"}
{"predict": "(24, MPI, MPI_Isend(sendbuf, BUF_COUNT, MPI_INT, i, x, MPI_COMM_WORLD, &sendreqs[i]);)\n(27, MPI, MPI_Irecv(recvbuf, BUF_COUNT, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &recvreqs[i]);)\n(39, MPI, MPI_Waitall(nprocs, sendreqs, sendstats);)", "label": "(10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (42, MPI, MPI_Waitall(nprocs, sendreqs, sendstats);); (13, MPI, MPI_Barrier(MPI_COMM_WORLD);); (21, MPI, MPI_Isend(sendbuf, BUF_COUNT, MPI_INT, i, 0, MPI_COMM_WORLD, &sendreqs[i]);)"}
{"predict": "6: \n97:     /* Initialize distributed matrices */\n98:     calStat = cal_init_dist_matrix(cal_comm, d_A, &d_A, M, N, MA, NA, IA, JA, RSRCA, CSRCA, cal_dev_type_t::CAL_DEV_TYPE_GPU, &gridA);\n99:     assert(calStat == CAL_OK);\n100: \n101:     /* Initialize distributed matrix descriptors */\n102:     calStat = cal_init_dist_matrix_descriptor(cal_comm, descrA, d_A, M, N, MA, NA, IA, JA, RSRCA, CSRCA, cal_dev_type_t::CAL_DEV_TYPE_GPU);\n103:     assert(calStat == CAL_OK);\n104: \n105:     /* Allocate memory for distributed matrices */\n106:     cudaStat = cudaMalloc(&d_A, MA * NA * sizeof(double));\n107:     cudaStat = cudaMalloc(&d_tau, M * sizeof(double));\n108:     cudaStat = cudaMalloc(&d_work_geqrf, workspaceInBytesOnDevice_geqrf);\n109:     cudaStat = cudaMalloc(&h_work_geqrf, workspaceInBytesOnHost_geqrf);\n110:     assert(cudaStat == cudaSuccess);\n111: \n112:     /* Initialize workspace */\n113:     calStat = cal_init_workspace(cal_comm, h_work_geqrf, workspaceInBytesOnHost_geqrf);\n114:     assert(calStat == CAL_OK);\n115: \n116:     /* Geqrf on distributed matrix */\n117:     cusolverStat = cusolverMpGeqrf(cusolverMpHandle, M, N, d_A, MA, d_tau, d_work_geqrf, workspaceInBytesOnDevice_geqrf);\n118:     assert(cusolverStat == CUSOLVER_STATUS_SUCCESS);\n119: \n120:     /* Free resources */\n121:     cudaFree(d_A);\n122:     cudaFree(d_tau);\n123:     cudaFree(d_work_geqrf);\n124:     cudaFree(h_work_geqrf);\n125:     calDestroy(cal_comm);\n126:     cusolverMpDestroy(cusolverMpHandle);\n127:     MPI_Finalize();\n12", "label": "(358, CUDA, cudaStat = cudaMemcpy(h_A, d_global_Q, sizeof(double) * lda * colsA, cudaMemcpyDeviceToHost);); (196, CUDA, cudaStat = cudaMalloc((void**)&d_info_geqrf, sizeof(int));); (27, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rankId);); (429, CUDA, cudaStat = cudaFree(d_tau);); (-1, MPI, MPI_Init(NULL, NULL);); (274, CUDA, cudaStat = cudaMemcpyAsync(&h_info_geqrf, d_info_geqrf, sizeof(int), cudaMemcpyDeviceToHost, localStream);); (471, CUDA, cudaStat = cudaStreamDestroy(localStream);); (474, MPI, MPI_Barrier(MPI_COMM_WORLD);); (42, CUDA, cudaStat = cudaSetDevice(localDeviceId);); (43, CUDA, cudaStat = cudaFree(0);); (194, CUDA, cudaStat = cudaMemset(d_info_geqrf, 0, sizeof(int));); (428, CUDA, cudaStat = cudaFree(d_work_geqrf);); (178, CUDA, cudaStat = cudaMalloc((void**)&d_tau, localColsA * sizeof(double));); (301, CUDA, cudaStat = cudaMalloc((void**)&d_global_Q, lda * colsA * sizeof(double));); (213, CUDA, cudaStat = cudaMalloc((void**)&d_work_geqrf, workspaceInBytesOnDevice_geqrf);); (431, CUDA, cudaStat = cudaFree(d_info_geqrf);); (12, MPI, MPI_Comm_size(MPI_COMM_WORLD, &rankSize);); (154, CUDA, cudaStat = cudaMalloc((void**)&d_A, localColsA * LLDA * sizeof(double));); (305, CUDA, cudaStat = cudaMemcpy(d_global_R, d_global_Q, sizeof(double) * lda * colsA, cudaMemcpyDeviceToDevice);); (302, CUDA, cudaStat = cudaMemcpy(d_global_Q, h_A, sizeof(double) * lda * colsA, cudaMemcpyHostToDevice);); (53, CUDA, cudaStat = cudaStreamCreate(&localStream);); (463, MPI, MPI_Finalize();); (403, CUDA, cudaStat = cudaFree(d_A);); (293, CUDA, cudaStat = cudaMalloc((void**)&d_global_R, lda * colsA * sizeof(double));); (294, CUDA, cudaStat = cudaMalloc((void**)&d_global_tau, colsA * sizeof(double));)"}
{"predict": "(4, CUDA, extern __shared__ T sdata[];) (11, CUDA, int tid = threadIdx.x; int i = blockIdx.x * BlockSize + tid;) (23, CUDA, threadVal += g_idata[i + blockDim.x * 2 * tid];) (26, CUDA, if ((i + 2 * blockDim.x * tid) < n) { threadVal += g_idata[i + 2 * blockDim.x * tid]; }) (29, CUDA, i += gridSize * 2 * tid;) (32, CUDA, i = blockIdx.x * BlockSize + tid;) (35, CUDA, i += gridSize * 2 * tid;) (38, CUDA, threadVal = cg_reduce_n(threadVal, multiWarpTile);) (41, CUDA, sdata[multiWarpTile.meta_group_rank()] = threadVal;) (44, CUDA, if (multiWarpTile.thread_rank() == 0) { sdata[multiWarpTile.meta_group_rank()] = threadVal; }) (47, CUDA, threadVal += sdata[i];)", "label": "(4, CUDA, __shared__ cg::block_tile_memory<BlockSize> scratch;)"}
{"predict": "(47, CUDA, cudaLaunchKernel<<<dim3(1), dim3(1)>>>(kernelName, deviceInt, hostInt, testVector, numInts, cIterations);)", "label": "(59, CUDA, cudaFree( deviceInt );); (59, CUDA, cudaFreeHost( hostInt );)"}
{"predict": "(18, CUDA, cudaEvent_t startEvent, endEvent; cudaEventCreate(&startEvent); cudaEventCreate(&endEvent); cudaEventRecord(startEvent, 0);) (84, CUDA, cudaEventRecord(endEvent, 0); cudaEventSynchronize(endEvent); float milliseconds = 0; cudaEventElapsedTime(&milliseconds, startEvent, endEvent); cudaEventDestroy(startEvent); cudaEventDestroy(endEvent); ret = milliseconds * 1000.0f;)", "label": "(98, CUDA, cudaFree( dptrPosMass[i] );); (100, CUDA, cudaSetDevice( oldDevice );); (97, CUDA, cudaFree( dptrForce[i] );)"}
{"predict": "(4, OpenCL, clReleaseMemObject(objX);) (7, OpenCL, clReleaseMemObject(objiAmax);) (10, OpenCL, clReleaseMemObject(objScratch);)", "label": "(5, OpenCL, clReleaseMemObject(objX);); (12, OpenCL, clReleaseMemObject(objScratch);); (7, OpenCL, clReleaseMemObject(objiAmax);)"}
{"predict": "(15, OpenMP, #pragma omp parallel for)", "label": "(19, TBB, REMARK(\"Wraparound on tbb::concurrent_bounded_queue...\");); (2, TBB, tbb::concurrent_queue<T>* q = new tbb::concurrent_queue<T>;); (16, TBB, tbb::concurrent_bounded_queue<T>* q = new tbb::concurrent_bounded_queue<T>;)"}
{"predict": "(13, OpenMP, #pragma omp parallel;)", "label": "(25, OpenMP, printf(\"Thread %d sleeps now for %d seconds\\n\", omp_get_thread_num(), SLEEPTIME);); (8, OpenMP, #pragma omp parallel); (31, OpenMP, printf(\"Thread %d wakes up again\\n\", omp_get_thread_num());)"}
{"predict": "(4, LLVM, auto MDAlloca = llvm::MDNode::get(EntryBlock->getContext(), {llvm::MDString::get(EntryBlock->getContext(), MDKind::LoopState)});)", "label": "(45, SYCL, GEP->setMetadata(hipsycl::compiler::MDKind::Arrayified, MDAlloca);); (36, SYCL, Alloca->setMetadata(hipsycl::compiler::MDKind::Arrayified, MDAlloca);)"}
{"predict": "; i < argc; ++i)\n84:     foo();\n85:   xa = 0;\n86:   int ya = 0;\n87:   #pragma omp parallel for\n88:   for (i = 0; i < argc; ++i)\n89:     ya = i;\n90:   int za = 0;\n91:   #pragma omp parallel for simd\n92:   for (i = 0; i < argc; ++i)\n93:     za = i;\n94:   int va = 0;\n95:   #pragma omp parallel for simd\n96:   for (i = 0; i < argc; ++i)\n97:     va = i;\n98:   int wa = 0;\n99:   #pragma omp parallel for simd\n100:   for (i = 0; i < argc; ++i)\n101:     wa = i;\n102:   int xa = 0;\n103:   #pragma omp parallel for simd\n104:   for (i = 0; i < argc; ++i)\n105:     xa = i;\n106:   int ya = 0;\n107:   #pragma omp parallel for simd\n108:   for (i = 0; i < argc; ++i)\n109:     ya = i;\n110:   int za = 0;\n111:   #pragma omp parallel for simd\n112:   for (i = 0; i < argc; ++i)\n113:     za = i;\n114:   int va = 0;\n115:   #pragma omp parallel for simd\n116:   for (i = 0; i < argc; ++i)\n117:     va = i;\n118:   int wa = 0;\n119:   #pragma omp parallel for simd\n120:   for (i = 0; i < argc; ++i)\n121:     wa = i;\n122:   int xa = 0;\n123:   #pragma omp parallel for simd\n124:   for (i = 0; i < argc; ++i)\n125", "label": "(22, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argc); (78, OpenMP, #pragma omp parallel master taskloop simd safelen(5)); (81, OpenMP, #pragma omp parallel master taskloop simd firstprivate(e, g)); (125, OpenMP, #pragma omp parallel private(i)); (87, OpenMP, #pragma omp parallel master taskloop simd firstprivate(h)); (53, OpenMP, #pragma omp parallel master taskloop simd firstprivate(ca)); (48, OpenMP, #pragma omp parallel master taskloop simd firstprivate(ba)); (27, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (38, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argv[1])); (90, OpenMP, #pragma omp parallel master taskloop simd firstprivate(i)); (-1, OpenMP, #pragma omp parallel); (64, OpenMP, #pragma omp parallel master taskloop simd firstprivate(S2::S2sc)); (128, OpenMP, #pragma omp parallel master taskloop simd firstprivate(B::x)); (119, OpenMP, #pragma omp parallel reduction(+ : i)); (88, OpenMP, #pragma omp parallel shared(xa)); (23, OpenMP, #pragma omp parallel master taskloop simd firstprivate(S1)); (14, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argc > 0 ? argv[1] : argv[2])); (78, OpenMP, #pragma omp parallel master taskloop simd private(xa), firstprivate(xa)); (53, OpenMP, #pragma omp parallel master taskloop simd firstprivate(S2::S2s)); (125, OpenMP, #pragma omp parallel master taskloop simd firstprivate(si)); (-6, OpenMP, #pragma omp parallel master taskloop simd firstprivate(); (-3, OpenMP, #pragma omp parallel master taskloop simd firstprivate()); (65, OpenMP, #pragma omp parallel master taskloop simd firstprivate(m)); (84, OpenMP, #pragma omp parallel master taskloop simd firstprivate(j)); (18, OpenMP, #pragma omp parallel master taskloop simd firstprivate(a, b, c, d, f)); (86, OpenMP, #pragma omp parallel master taskloop simd lastprivate(g) firstprivate(g)); (89, OpenMP, #pragma omp parallel master taskloop simd lastprivate(n) firstprivate(n)); (-1, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argc,); (39, OpenMP, #pragma omp parallel master taskloop simd firstprivate(xa)); (21, OpenMP, #pragma omp parallel master taskloop simd firstprivate(2 * 2)); (32, OpenMP, #pragma omp parallel master taskloop simd firstprivate(da)); (-21, OpenMP, #pragma omp parallel master taskloop simd firstprivate)"}
{"predict": "(11, OpenMP, #pragma omp parallel for private(sum, i2) reduction(+:sum))", "label": "(14, OpenMP, #pragma omp flush); (9, OpenMP, #pragma omp parallel for reduction(+:sum) schedule(static,1) private(i) private(i2))"}
{"predict": "(16, MPI, MPI_Isend(in, count, type, right, tag, comm, &requests[w])); (17, MPI, MPI_Irecv(out, count, type, left, tag, comm, &requests[w])); (21, MPI, MPI_Isend(out, count, type, left, tag, comm, &requests[w])); (22, MPI, MPI_Irecv(in, count, type, right, tag, comm, &requests[w]));", "label": "(21, MPI, MPI_Recv(&ack, 1, MPI_CHAR, right, tag, comm, MPI_STATUS_IGNORE);); (23, MPI, MPI_Irecv(out, count, type, set_src ? left : MPI_ANY_SOURCE, tag, comm, &requests[w]);); (18, MPI, MPI_Waitall(window_size, requests, MPI_STATUSES_IGNORE);); (15, MPI, MPI_Isend(in, count, type, right, (tag == MPI_ANY_TAG ? 0 : tag), comm, &requests[w]);); (23, MPI, MPI_Send(&ack, 1, MPI_CHAR, left, (tag == MPI_ANY_TAG ? 0 : tag), comm);)"}
{"predict": "(11, OpenMP, #pragma omp parallel for private(curTok, fpcurTok, curTokLen))", "label": "(34, OpenMP, comp_do_parse(cur, tail);)"}
{"predict": "(15, OpenMP, #pragma omp parallel for)", "label": "(3, OpenMP, #pragma omp target device(0)); (13, OpenMP, #pragma omp target device(1))"}
{"predict": "(7, OpenMP, #pragma omp atomic var++;) (15, OpenMP, #pragma omp atomic var++;)", "label": "(4, OpenMP, #pragma omp master); (5, OpenMP, #pragma omp taskgroup); (6, OpenMP, #pragma omp task shared(var, a)); (0, OpenMP, #pragma omp parallel num_threads(2) shared(var, a))"}
{"predict": "(3, MPI, MPI_Comm_rank(comm, &rank); MPI_Comm_size(comm, &size);) (16, MPI, MPI_Alltoallv(data_out, counts, displs, MPI_INT, data_in, counts, displs, MPI_INT, comm);)", "label": "(3, MPI, MPI_Comm_size(comm, &size);); (3, MPI, MPI_Comm_rank(comm, &rank);); (13, MPI, MPI_Alltoallv(data_out, counts, displs, MPI_INT, data_in, counts, displs, MPI_INT, comm);)"}
{"predict": "(29, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(11, MPI, MPI_Error_string(error, err_string, &length);); (20, MPI, error = MPI_Init(&argc, &argv);); (27, MPI, error = MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (60, MPI, error = MPI_Finalize();); (50, MPI, error = MPI_Barrier(MPI_COMM_WORLD);); (29, MPI, MPI_Abort(MPI_COMM_WORLD, error);); (3, MPI, error = MPI_Finalized(&flag);)"}
{"predict": "(14, CUDA, dim3 block(32, 32); dim3 grid(N / 32, M / 32);) (17, CUDA, mat_trans_smem_naive_kernel<<<grid, block>>>(dev_A, M, N, dev_B);) (21, CUDA, mat_trans_smem_padding_kernel<<<grid, block>>>(dev_A, M, N, dev_B);) (25, CUDA, mat_trans_smem_swizzle_kernel<<<grid, block>>>(dev_A, M, N, dev_B);)", "label": "(13, CUDA, cudaMalloc(&dev_B, size_b);); (13, CUDA, cudaDeviceSynchronize();); (10, CUDA, cudaMalloc(&dev_A, size_a);); (27, CUDA, cudaFree(dev_B);); (25, CUDA, cudaFree(dev_A);)"}
{"predict": "57:   {}\n58:   {}\n59:   {}\n60:   {}\n61:   {}\n62:   {}\n63:   {}\n64:   {}\n65:   {}\n66:   {}\n67:   {}\n68:   {}\n69:   {}\n70:   {}\n71:   {}\n72:   {}\n73:   {}\n74:   {}\n75:   {}\n76:   {}\n77:   {}\n78:   {}\n79:   {}\n80:   {}\n81:   {}\n82:   {}\n83:   {}\n84:   {}\n85:   {}\n86:   {}\n87:   {}\n88:   {}\n89:   {}\n90:   {}\n91:   {}\n92:   {}\n93:   {}\n94:   {}\n95:   {}\n96:   {}\n97:   {}\n98:   {}\n99:   {}\n100:  {}\n101:  {}\n102:  {}\n103:  {}\n104:  {}\n105:  {}\n106:  {}\n107:  {}\n108:  {}\n109:  {}\n110:  {}\n111:  {}\n112:  {}\n113:  {}\n114:  {}\n115:  {}\n116:  {}\n117:  {}\n118:  {}\n119:  {}\n120:  {}\n121:  {}\n122:  {}\n123:  {}\n124:  {}\n125:  {}\n126:  {}\n127:  {}\n128:  {}\n129:  {}\n130:  {}\n131:  {}\n132:  {}\n133:  {}\n134:  {}\n135:  {}\n136:  {}\n137:  {}\n138:  {}\n139:  {}\n140:  {}\n141:  {}\n142:  {}\n143:  {}\n144:  {}\n145:  {}\n146:  {}\n147:  {}\n148:  {}\n149", "label": "(35, OpenMP, #pragma omp target teams map(marr[:arg][n:])); (166, OpenMP, #pragma omp target teams map(r.S.Arr[4:5])); (53, OpenMP, #pragma omp target teams map(marr[:arg][:2][:])); (120, OpenMP, #pragma omp target teams map(r.PtrS[0], r.PtrS->B)); (117, OpenMP, #pragma omp target teams map(r.ArrS[0].A, t.ArrS[1].A)); (136, OpenMP, #pragma omp target teams map(r.C, r.D)); (9, OpenMP, #pragma omp target teams map(r)); (80, OpenMP, #pragma omp target teams map(mvla2[:1][:2][:10])); (123, OpenMP, #pragma omp target teams map(r.RPtrS, r.RPtrS->B)); (136, OpenMP, #pragma omp target teams map(r.C) map(r.C)); (21, OpenMP, #pragma omp target teams map(marr[arg:][:arg][:])); (189, OpenMP, #pragma omp target data map(to: t)); (157, OpenMP, #pragma omp target teams map(r.S.Ptr[4:5])); (54, OpenMP, #pragma omp target teams map(marr[:2][2+2-4:][0:5+5])); (35, OpenMP, #pragma omp target teams map(marr[:2][:][1:])); (12, OpenMP, #pragma omp target teams map(marr[arg:][:][:])); (93, OpenMP, #pragma omp target teams map(r.ArrS[:arg].B)); (0, OpenMP, #pragma omp target teams map(marr[2][0:2][0:2])); (80, OpenMP, #pragma omp target teams map(mptr[:1][:2][0:2])); (53, OpenMP, #pragma omp target teams map(marr2[:1][:2][0])); (65, OpenMP, #pragma omp target teams map(mvla2[:1][:2][:11])); (86, OpenMP, #pragma omp target teams map(r.ArrS[:1].B)); (35, OpenMP, #pragma omp target teams map(marr[:1][3:1][:2])); (6, OpenMP, #pragma omp target teams map(marr[arg:])); (29, OpenMP, #pragma omp target teams map(marr[:1][0][:])); (134, OpenMP, #pragma omp target teams map(r.Ptr[3:5])); (74, OpenMP, #pragma omp target teams map(mptr[:1][:][0:2])); (108, OpenMP, #pragma omp target teams map(r.S.Arr[:12])); (125, OpenMP, #pragma omp target teams map(r.A)); (90, OpenMP, #pragma omp target teams map(r.ArrS[0].A, r.ArrS[1].A)); (95, OpenMP, #pragma omp target teams map(r.PtrS, r.PtrS->B)); (164, OpenMP, #pragma omp target teams map(t.D)); (149, OpenMP, #pragma omp target teams map(r.D)); (96, OpenMP, #pragma omp target teams map(r.RPtrS[0], r.RPtrS->B)); (77, OpenMP, #pragma omp target teams map(r.ArrS[0].Arr[1:23])); (98, OpenMP, #pragma omp target teams map(r.RPtrS->A, r.RPtrS->B)); (45, OpenMP, #pragma omp target teams map(mvla[1][2:arg][:])); (142, OpenMP, #pragma omp target data map(to: r.C)); (21, OpenMP, #pragma omp target teams map(marr[:1][3:arg][:2])); (8, OpenMP, #pragma omp target teams map(marr[:2][:][:1])); (37, OpenMP, #pragma omp target teams map(mvla[:2][:arg][:])); (61, OpenMP, #pragma omp target teams map(mptr[:2][:1][0:2])); (71, OpenMP, #pragma omp target teams map(r.ArrS[0].Arr[1:arg])); (142, OpenMP, #pragma omp target data map(to: t.Ptr)); (31, OpenMP, #pragma omp target teams map(mvla[:1][:][0])); (112, OpenMP, #pragma omp target teams map(r.Arr[3:5])); (46, OpenMP, #pragma omp target teams map(mptr[:1][:2-1][2:4-3])); (108, OpenMP, #pragma omp target teams map(r.Arr)); (123, OpenMP, #pragma omp target teams map(r.S.Ptr[:])); (21, OpenMP, #pragma omp target teams map(marr[:1][:2][0])); (-7, OpenMP, #pragma omp target teams map(marr[:2][:1][:])); (125, OpenMP, #pragma omp target teams map(u.B)); (111, OpenMP, #pragma omp target teams map(r.ArrS[3:5].Arr[6:7])); (-16, OpenMP, #pragma omp target teams map(marr[:][:arg][n:])); (97, OpenMP, #pragma omp target teams map(r.C, t.C)); (62, OpenMP, #pragma omp target teams map"}
{"predict": "(6, OpenMP, #pragma omp parallel for) (12, OpenMP, #pragma omp parallel for) (15, OpenMP, #pragma omp parallel for)", "label": "(12, OpenMP, #pragma omp target map(tofrom: C) map(to: tt, tt.p1[:10]))"}
{"predict": "(3, CUDA, cusolverDnDestroySyevjInfo(cusolverData.syevj_params);) (5, CUDA, cusolverDnDestroy(cusolverData.cusolverH);)", "label": "(7, CUDA, cudaFree(bufferOffset.W);); (3, CUDA, cudaFree(bufferOffset.srcShiftSum);); (1, CUDA, cudaFree(bufferOffset.dstMean);); (10, CUDA, cudaFree(cusolverData.cusolverBuffer);); (5, CUDA, cudaFree(bufferOffset.J);); (3, CUDA, cudaFree(bufferOffset.r);); (4, CUDA, cudaFree(bufferOffset.calc_buffer);); (-1, CUDA, cudaFree(bufferOffset.LtL);); (-6, CUDA, cudaFree(bufferOffset.srcMean);); (-4, CUDA, cudaFree(bufferOffset.dstShiftSum);); (4, CUDA, cudaFree(cusolverData.cusolverInfo);)"}
{"predict": "\")\";\n97:         std::cout << \"\\n\";\n98:     }\n99: #endif\n100: }\n\nOutput:", "label": "(77, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamCreate(&stream));); (71, CUDA, CUDA_CHECK_AND_EXIT(cudaMemset(output, 0b11111111, flat_fft_size_bytes));); (71, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(input, input_host.data(), flat_fft_size_bytes, cudaMemcpyHostToDevice));); (91, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(output));); (70, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (88, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(input));); (62, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&input, flat_fft_size_bytes));); (83, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamDestroy(stream));); (61, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&output, flat_fft_size_bytes));)"}
{"predict": ":         test_error( error, \"Unable to enqueue kernel\" );\n99:     }\n100:     /* Wait for all the buffers to be ready */\n101:     clFinish(queue);\n102:     clWaitForEvents(1, &error);\n103:     /* Read the output data back to host */\n104:     test_error( error, \"Unable to enqueue read buffer\" );\n105:     test_error( error, \"Unable to read buffer\" );\n106:     /* Verify the results */\n107:     for( i = 0; i < TEST_SIZE; i++ )\n108:     {\n109:         if( verifyFn( outData[i], inDataA[i], inDataB[i], ulpLimit ) )\n110:         {\n111:             log_error(\"ERROR: test failed for element %zu\\n\", i);\n112:             return -1;\n113:         }\n114:     }\n115:     return 0;\n116: }", "label": "(108, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 1, NULL, threads, localThreads, 0, NULL, NULL );); (22, OpenCL, error = clGetDeviceInfo( device, CL_DEVICE_SINGLE_FP_CONFIG, sizeof( config ), &config, NULL );); (16, OpenCL, error = clGetDeviceInfo( device, CL_DEVICE_PROFILE, sizeof( kernelSource ), kernelSource, NULL );); (95, OpenCL, error = clSetKernelArg(kernel, (int)i, sizeof( streams[i] ), &streams[i]);); (108, OpenCL, error = clEnqueueReadBuffer( queue, streams[2], true, 0, sizeof( cl_float ) * TEST_SIZE, outData, 0, NULL, NULL );)"}
{"predict": "(14, CUDA, hipDeviceSynchronize();)", "label": "(11, OpenMP, #pragma omp parallel for schedule(static,1)); (12, OpenMP, #pragma omp task)"}
{"predict": "(12, OpenMP, #pragma omp parallel for private(i))", "label": "(8, CUDA, safeCall(cudaMemcpy(h_data, data.d_data, sizeof(SiftPoint)*data.numPts, cudaMemcpyDeviceToHost));)"}
{"predict": "(14, Cilk, #include <cilk/cilk.h>) (14, Cilk, #include <cilk/cilk_api.h>) (14, Cilk, #include <cilk/cilk_undisturbed.h>) (14, Cilk, cilk_for (int i = 0; i < n; ++i)) (14, Cilk, cilk_for (int i = 0; i < n; ++i)) (14, Cilk, cilk_for (int i = 0; i < n; ++i)) (14, Cilk, cilk_for (int i = 0; i < n; ++i)) (14, Cilk, cilk_for (int i = 0; i < n; ++i)) (14, Cilk, cilk_for (int i = 0; i < n; ++i)) (14, Cilk, cilk_for (int i = 0; i < n; ++i)) (14, Cilk, cilk_for (int i = 0; i < n; ++i)) (14, Cilk, cilk_for (int i = 0; i < n; ++i)) (14, Cilk, cilk_for (int i = 0; i < n; ++i)) (14, Cilk, cilk_for (int i = 0; i < n; ++i)) (14, Cilk, cilk_for (int i = 0; i < n; ++i)) (14, Cilk, cilk_for (int i = 0; i < n; ++i)) (14, Cilk, cilk_for (int i = 0; i < n; ++i)) (14, Cilk, cilk_for (int i = 0; i < n; ++i)) (14, Cilk, cilk_for (int i = 0; i < n; ++i)) (14, Cilk, cilk_for (int i = 0; i < n; ++i)) (14, Cilk, cilk_for (int i = 0; i < n; ++i)) (14, Cilk, cilk_for (int i = 0; i < n; ++i)) (14", "label": "(12, TBB, tbb::mutex *m = new tbb::mutex;)"}
{"predict": "(7, OpenMP, #pragma omp parallel for schedule(dynamic) default(none) copyin(g) linear(a) allocate(a) lastprivate(conditional: d,e,f) order(concurrent))", "label": "(7, OpenMP, #pragma omp parallel for schedule(dynamic) default(none) copyin(g) linear(a) allocate(a) lastprivate(conditional: d, e,f) order(concurrent)); (5, OpenMP, #pragma omp threadprivate(g)); (11, OpenMP, #pragma omp parallel for allocate(argc) private(argc, b), firstprivate(c, d), lastprivate(d, f) collapse(N) schedule(static, N) ordered(N) if (parallel :argc) num_threads(N) default(shared) shared(e) reduction(+ : h))"}
{"predict": "(26, MPI, *val = 0; if (envval) { printf(\" envval = %s\\n\", envval); if (strcmp(envval, \"yes\") == 0 || strcmp(envval, \"YES\") == 0 || strcmp(envval, \"true\") == 0 || strcmp(envval, \"TRUE\") == 0) *val = 1; } else { *val = d; } )", "label": "(19, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(11, OpenMP, #pragma omp parallel for private(excessHC))", "label": "(31, TBB, ASSERT(default_ss == tbb::global_control::active_value(tbb::global_control::thread_stack_size), NULL);); (3, TBB, const unsigned h_c = tbb::tbb_thread::hardware_concurrency();); (5, TBB, tbb::task_scheduler_init t(h_c+1);); (20, TBB, size_t default_ss = tbb::global_control::active_value(tbb::global_control::thread_stack_size);)"}
{"predict": "1024)\n135:     {\n136:         max_memory_size = 512 * 1024 * 1024;\n137:     }\n138: \n139:     memory_size = (cl_uint)(max_memory_size / size_increase_per_iteration);\n140:     memory_size = memory_size < 1024 * 1024 ? 1024 * 1024 : memory_size;\n141: \n142:     if (memory_size < min_dim)\n143:     {\n144:         memory_size = min_dim;\n145:     }\n146:     else if (memory_size > max_dim)\n147:     {\n148:         memory_size = max_dim;\n149:     }\n150: \n151:     log_info(\"Allocating %d bytes.\\n\", memory_size);\n152: \n153:     array = clCreateBuffer(context, CL_MEM_READ_WRITE | CL_MEM_COPY_HOST_PTR,\n154:                            memory_size, hostPtr, &err);\n155:     test_error(err, \"Failed to create buffer\");\n156: \n157:     cl_event event;\n158:     cl_ulong execTime = 0;\n159:     err = clEnqueueNDRangeKernel(queue, kernel, dimensions, NULL, &memory_size,\n160:                                  NULL, 0, NULL, &event);\n161:     test_error(err, \"Failed to enqueue kernel\");\n162: \n163:     err = clWaitForEvents(1, &event);\n164:     test_error(err, \"Failed to wait for events\");\n165: \n166:     err = clGetEventProfilingInfo(event, CL_PROFILING_COMMAND_START,\n167:                                   sizeof(cl_ulong), &execTime, NULL);\n168:     test_error(err, \"Failed to get profiling info\");\n169: \n170:     cl_ulong execTime2 = 0;\n171:     err = clGetEventProfilingInfo(event, CL_PROFILING_COMMAND_END,\n172:                                   sizeof(cl_ulong), &execTime2, NULL);\n173:     test_error(err, \"Failed to", "label": "(558, OpenCL, clReleaseKernel(kernel);); (558, OpenCL, clReleaseProgram(program);); (555, OpenCL, clReleaseMemObject(array);)"}
{"predict": "(35, CUDA, model.computeDynamics(s_cpu, u, state_deriv_cpu);) (41, CUDA, UpdateStateKernel<DYN><<<dimGrid, dimBlock, 0, s1>>>(model.model_d_, s_d, u_d, state_deriv_GPU, dt);) (47, CUDA, eigen_assert_float_eq<DYN::state_array>(s_cpu, s_gpu);)", "label": "(34, CUDA, HANDLE_ERROR(cudaMalloc((void**)&state_deriv_GPU, sizeof(float) * DYN::STATE_DIM));); (37, CUDA, HANDLE_ERROR(cudaMemcpyAsync(s_d, s_gpu.data(), sizeof(float) * DYN::STATE_DIM, cudaMemcpyHostToDevice, s1));); (46, CUDA, CudaCheckError();); (34, CUDA, HANDLE_ERROR(cudaMemcpyAsync(u_d, u.data(), sizeof(float) * DYN::CONTROL_DIM, cudaMemcpyHostToDevice, s1));); (29, CUDA, HANDLE_ERROR(cudaMalloc((void**)&s_d, sizeof(float) * DYN::STATE_DIM));); (27, CUDA, HANDLE_ERROR(cudaMalloc((void**)&u_d, sizeof(float) * DYN::CONTROL_DIM));); (45, CUDA, HANDLE_ERROR(cudaMemcpyAsync(s_gpu.data(), s_d, sizeof(float) * DYN::STATE_DIM, cudaMemcpyDeviceToHost, s1));); (32, CUDA, HANDLE_ERROR(cudaStreamSynchronize(s1));); (19, CUDA, cudaStreamCreate(&s1);)"}
{"predict": ":         grad_spheres.scalar_type(), \"kin_fused_backward_16t\", ([&] {\n126:         kin_fused_backward_kernel3<scalar_t, double, true, true, 256, parallel_write>\n127:           << < blocksPerGrid, threadsPerBlock, sharedMemSize, stream >> > (\n128:           grad_out.data_ptr<float>(),\n129:           grad_nlinks_pos.data_ptr<float>(),\n130:           grad_nlinks_quat.data_ptr<float>(),\n131:           grad_spheres.data_ptr<scalar_t>(),\n132:           global_cumul_mat.data_ptr<float>(),\n133:           joint_vec.data_ptr<float>(),\n134:           fixed_transform.data_ptr<float>(),\n135:           robot_spheres.data_ptr<float>(),\n136:           joint_map_type.data_ptr<int8_t>(),\n137:           joint_map.data_ptr<int16_t>(), link_map.data_ptr<int16_t>(),\n138:           store_link_map.data_ptr<int16_t>(),\n139:           link_sphere_map.data_ptr<int16_t>(),\n140:           link_chain_map.data_ptr<int16_t>(),\n141:           joint_offset_map.data_ptr<float>(),\n142:           batch_size, n_spheres,\n143:           n_links, n_joints, store_n_links);\n144:       }));\n145:     }\n146:   }\n147:   else\n148:   {\n149:     AT_DISPATCH_FLOATING_TYPES(\n150:       grad_spheres.scalar_type(), \"kin_fused_backward_16t\", ([&] {\n151:         kin_fused_backward_kernel3<scalar_t, double, true, false, 16, parallel_write>\n152:           << < blocksPerGrid, threadsPerBlock, sharedMemSize, stream >> > (\n153:           grad_out.data_ptr<float>(),\n154:           grad_nlinks_pos.data_ptr<float>(),\n155:           grad_nlinks_quat.data_ptr<float>(),\n156:           grad_spheres.data_ptr<scalar_t>(),\n157:           global_cumul_mat.data_ptr<float>(),\n158:           joint_vec.data_ptr<float>(),\n159:           fixed_transform.data_ptr<float>(),\n160:           robot_spheres.data_ptr<float>(),", "label": "(69, CUDA, cudaStream_t stream = at::cuda::getCurrentCUDAStream();)"}
{"predict": "(12, OpenMP, #pragma omp parallel for reduction(+:s))", "label": "(24, MPI, MPI_Allreduce( &s, &berr, 1, MPI_FLOAT, MPI_MAX, grid->comm );)"}
{"predict": "(13, OpenMP, #pragma omp parallel for private(k, div) reduction(+:cpmap[k].b[:voxBox][:voxBox]))", "label": "(34, CUDA, checkCudaErrors(cudaDeviceSynchronize());)"}
{"predict": "(29, CUDA, auto image = cv::imread(\"street.jpg\");)", "label": "(18, CUDA, checkRuntime(cudaStreamCreate(&stream));); (79, CUDA, checkRuntime(cudaMemcpyAsync(output_data_host, output_data_device, sizeof(float) * output_numel, cudaMemcpyDeviceToHost, stream));); (79, CUDA, checkRuntime(cudaStreamSynchronize(stream));); (57, CUDA, checkRuntime(cudaMemcpyAsync(input_data_device, input_data_host, input_numel * sizeof(float), cudaMemcpyHostToDevice, stream));); (24, CUDA, checkRuntime(cudaMallocHost(&input_data_host, input_numel * sizeof(float)));); (24, CUDA, checkRuntime(cudaMalloc(&input_data_device, input_numel * sizeof(float)));); (89, CUDA, checkRuntime(cudaFree(input_data_device));); (86, CUDA, checkRuntime(cudaFreeHost(input_data_host));); (88, CUDA, checkRuntime(cudaFree(output_data_device));); (62, CUDA, checkRuntime(cudaMalloc(&output_data_device, sizeof(float) * output_numel));); (-5, CUDA, auto engine = make_nvshared(runtime->deserializeCudaEngine(engine_data.data(), engine_data.size()));); (59, CUDA, checkRuntime(cudaMallocHost(&output_data_host, sizeof(float) * output_numel));); (82, CUDA, checkRuntime(cudaFreeHost(output_data_host));); (79, CUDA, checkRuntime(cudaStreamDestroy(stream));)"}
{"predict": "task\n107:   ++r;\n108: // expected-note@+1 {{defined as reduction}}\n109: #pragma omp parallel reduction(+ : r)\n110: #pragma omp task\n111:   ++r;\n112: #pragma omp task\n113:   ++r;\n114: // expected-note@+1 {{defined as reduction}}\n115: #pragma omp parallel reduction(+ : r)\n116: #pragma omp task\n117:   ++r;\n118: // expected-note@+1 {{defined as reduction}}\n119: #pragma omp parallel reduction(+ : r)\n120: #pragma omp task\n121:   ++r;\n122: // expected-note@+1 {{defined as reduction}}\n123: #pragma omp parallel reduction(+ : r)\n124: #pragma omp task\n125:   ++r;\n126: #pragma omp parallel reduction(+ : r)\n127: #pragma omp task\n128:   ++r;\n129: // expected-note@+1 {{defined as reduction}}\n130: #pragma omp parallel reduction(+ : r)\n131: #pragma omp task\n132:   ++r;\n133: #pragma omp parallel reduction(+ : r)\n134: #pragma omp task\n135:   ++r;\n136: #pragma omp parallel reduction(+ : r)\n137: #pragma omp task\n138:   ++r;\n139: #pragma omp parallel reduction(+ : r)\n140: #pragma omp task\n141:   ++r;\n142: #pragma omp parallel reduction(+ : r)\n143: #pragma omp task\n144:   ++r;\n145: #pragma omp parallel reduction(+ : r)\n146: #pragma omp task\n147:   ++r;\n148: #pragma omp parallel reduction(+ : r)\n149: #pragma omp task\n150:   ++r;\n151: #pragma omp parallel reduction(+ : r)\n152: #pragma omp task\n153:   ++r;\n154: #pragma omp parallel reduction(+ : r)\n155: #pragma omp", "label": "(174, OpenMP, #pragma omp task mergeable detach(evt)); (19, OpenMP, #pragma omp task unknown()); (14, OpenMP, #pragma omp task }); (154, OpenMP, #pragma omp task untied untied); (106, OpenMP, #pragma omp parallel shared(sa, sb)); (125, OpenMP, #pragma omp for reduction(+ : r)); (109, OpenMP, #pragma omp task firstprivate(r)); (160, OpenMP, #pragma omp task detach(); (-2, OpenMP, #pragma omp task); (104, OpenMP, #pragma omp parallel reduction(+ : r)); (160, OpenMP, #pragma omp task detach(evt) detach(evt)); (161, OpenMP, #pragma omp task detach(evt) mergeable); (60, OpenMP, #pragma omp parallel); (162, OpenMP, #pragma omp task detach(-evt)); (-6, OpenMP, #pragma omp task(); (48, OpenMP, #pragma omp task default(shared)); (-6, OpenMP, #pragma omp task[); (152, OpenMP, #pragma omp task detach(a)); (-6, OpenMP, #pragma omp task]); (58, OpenMP, #pragma omp parallel shared(a, b)); (157, OpenMP, #pragma omp task detach(evt) shared(evt)); (-7, OpenMP, #pragma omp task)); (144, OpenMP, #pragma omp task detach); (148, OpenMP, #pragma omp task detach(cevt) detach(revt)); (136, OpenMP, #pragma omp task mergeable mergeable); (153, OpenMP, #pragma omp task detach(evt) firstprivate(evt)); (19, OpenMP, #pragma omp task default(none)); (141, OpenMP, #pragma omp task detach()); (66, OpenMP, #pragma omp parallel shared(sa))"}
{"predict": "(9, MPI, MPI_Recv(buf, 10, MPI_INT, MPI_ANY_SOURCE, tag, MPI_COMM_WORLD, &s);)", "label": "(8, MPI, MPI_Start(&r);); (11, MPI, MPI_Send_init(buf, 10, MPI_INT, MPI_PROC_NULL, tag, MPI_COMM_WORLD, &r);); (7, MPI, MPI_Wait(&r, &s);); (4, MPI, MPI_Recv_init(buf, 10, MPI_INT, MPI_PROC_NULL, tag, MPI_COMM_WORLD, &r);); (6, MPI, MPI_Request_free(&r);)"}
{"predict": "(15, CUDA, <<<grid, block, 0, stream>>>(x.data_ptr<float>(), y.data_ptr<float>(), M, N, tA, tB, sA_layout, sB_layout);)", "label": "(16, CUDA, CUDA_CHECK(cudaGetLastError());)"}
{"predict": "15:         }\n116: #endif\n117:     }\n118: }", "label": "(35, MPI, MPI_Get_library_version(mpiVersion, &resultLen);); (132, MPI, ret = MPI_Init_thread(argc, argv, MPI_THREAD_MULTIPLE, &provided);); (146, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (126, MPI, MPI_Initialized(&isMpiInited);); (0, MPI, MPI_Finalized(&isMpiFinalized);)"}
{"predict": "(1, OpenMP, #pragma omp parallel for)", "label": "(14, OpenMP, #pragma omp parallel reduction(+ : incomplete, ([10])p)); (1, OpenMP, #pragma omp parallel reduction(); (2, OpenMP, #pragma omp parallel reduction(unknown); (9, OpenMP, #pragma omp parallel reduction(inscan, + : a)); (6, OpenMP, #pragma omp parallel reduction(default, + : a)); (1, OpenMP, #pragma omp parallel reduction(default,); (2, OpenMP, #pragma omp parallel reduction(unknown, +: a))"}
{"predict": "(27, OpenMP, #pragma omp parallel for reduction(+:_dev_u[:n]))", "label": "(13, OpenMP, int _dev_thread_id = omp_get_thread_num();); (11, OpenMP, int _dev_thread_num = omp_get_num_threads();)"}
{"predict": "(16, CUDA, if (err != cudaSuccess) { LOG_ERROR(\"CUDA memory allocation error of size: \" << sizeof(int) * 256 * maxBatchSize); throw std::runtime_error(\"CUDA memory allocation error!\"); })", "label": "(14, CUDA, cudaError_t err = cudaMalloc(&m_histogram, sizeof(int) * 256 * maxBatchSize);)"}
{"predict": ".group\n56: // CHECK: @__kmpc_for_static_fini\n57:   for(int i = 0; i < 10; ++i);\n58: // CHECK: @__kmpc_for_static_init\n59: // CHECK-NOT: !llvm.access.group\n60: // CHECK: @__kmpc_for_static_fini\n61:   for(int i = 0; i < 10; ++i);\n62: // CHECK: @__kmpc_for_static_init\n63: // CHECK: !llvm.access.group\n64: // CHECK: @__kmpc_for_static_fini\n65:   for(int i = 0; i < 10; ++i);\n66: // CHECK: @__kmpc_dispatch_init\n67: // CHECK: !llvm.access.group\n68:   for(int i = 0; i < 10; ++i);\n69: // CHECK: @__kmpc_dispatch_init\n70: // CHECK: !llvm.access.group\n71:   for(int i = 0; i < 10; ++i);\n72: // CHECK: @__kmpc_dispatch_init\n73: // CHECK: !llvm.access.group\n74:   for(int i = 0; i < 10; ++i);\n75: // CHECK: @__kmpc_dispatch_init\n76: // CHECK: !llvm.access.group\n77:   for(int i = 0; i < 10; ++i);\n78: // CHECK: @__kmpc_dispatch_init\n79: // CHECK: !llvm.access.group\n80:   for(int i = 0; i < 10; ++i);\n81: // CHECK: @__kmpc_dispatch_init\n82: // CHECK: !llvm.access.group\n83:   for(int i = 0; i < 10; ++i);\n84: // CHECK: @__kmpc_dispatch_init\n85: // CHECK: !llvm.access.group\n86:   for(int i = 0; i < 10; ++i);\n87: // CHECK: @__kmpc_dispatch_init\n88: // CHECK: !llvm.access.group\n89:   for(int i =", "label": "(137, OpenMP, #pragma omp for schedule(nonmonotonic: auto)); (3, OpenMP, #pragma omp for); (47, OpenMP, #pragma omp for schedule(guided)); (106, OpenMP, #pragma omp for schedule(monotonic: dynamic)); (41, OpenMP, #pragma omp for simd schedule(runtime)); (56, OpenMP, #pragma omp for simd schedule(dynamic)); (107, OpenMP, #pragma omp for simd schedule(monotonic: dynamic)); (46, OpenMP, #pragma omp for simd schedule(guided)); (89, OpenMP, #pragma omp for simd schedule(monotonic: runtime)); (10, OpenMP, #pragma omp for simd schedule(static)); (83, OpenMP, #pragma omp for schedule(monotonic: runtime)); (90, OpenMP, #pragma omp for schedule(monotonic: guided)); (144, OpenMP, #pragma omp for schedule(static, 2) ordered(1)); (28, OpenMP, #pragma omp for schedule(runtime)); (62, OpenMP, #pragma omp for schedule(monotonic: static, 2)); (-1, OpenMP, #pragma omp for schedule(static)); (21, OpenMP, #pragma omp for simd schedule(auto)); (144, OpenMP, #pragma omp for simd schedule(static, 2) ordered); (152, OpenMP, #pragma omp for simd schedule(auto) ordered); (66, OpenMP, #pragma omp for schedule(monotonic: auto)); (105, OpenMP, #pragma omp for schedule(nonmonotonic: dynamic)); (145, OpenMP, #pragma omp for schedule(auto) ordered(1)); (67, OpenMP, #pragma omp for simd schedule(monotonic: auto)); (110, OpenMP, #pragma omp for simd schedule(nonmonotonic: static)); (57, OpenMP, #pragma omp for simd schedule(monotonic: static, 2)); (46, OpenMP, #pragma omp for simd schedule(monotonic: static)); (-2, OpenMP, #pragma omp for schedule(static, 2)); (39, OpenMP, #pragma omp for schedule(monotonic: static)); (113, OpenMP, #pragma omp for simd schedule(nonmonotonic: runtime)); (28, OpenMP, #pragma omp for schedule(dynamic)); (87, OpenMP, #pragma omp for schedule(nonmonotonic: guided)); (149, OpenMP, #pragma omp for simd schedule(runtime) ordered); (97, OpenMP, #pragma omp for simd schedule(nonmonotonic: dynamic)); (157, OpenMP, #pragma omp for simd schedule(guided) ordered); (-25, OpenMP, #pragma omp for simd); (111, OpenMP, #pragma omp for schedule(static) ordered); (69, OpenMP, #pragma omp for simd schedule(monotonic: guided)); (-4, OpenMP, #pragma omp for schedule(auto)); (83, OpenMP, #pragma omp for simd schedule(nonmonotonic: guided)); (156, OpenMP, #pragma omp for schedule(dynamic) ordered(1)); (111, OpenMP, #pragma omp for simd schedule(static) ordered); (144, OpenMP, #pragma omp for schedule(guided) ordered(1)); (-13, OpenMP, #pragma omp for simd schedule(static, 2)); (132, OpenMP, #pragma omp for schedule(runtime) ordered)"}
{"predict": "(13, OpenCL, auto glImage = clCreateFromGLTexture(clContext.get(), 0, GL_TEXTURE_1D, 0, textureId, &retVal);); (26, OpenCL, retVal = clReleaseMemObject(glImage);)", "label": "(51, OpenCL, clReleaseEvent(event);)"}
{"predict": "[i-1][j][k][3]*um1);\n82: \n83: \trhs[i][j][k][4] = rhs[i][j][k][4] + dx5tx1 * \n84: \t  (u[i+1][j][k][4] - 2.0*u[i][j][k][4] +\n85: \t   u[i-1][j][k][4]) +\n86: \t  xxcon2 * (qs[i+1][j][k] - 2.0*qs[i][j][k] +\n87: \t\t    qs[i-1][j][k]) -\n88: \t  tx2 * (u[i+1][j][k][4]*up1 - \n89: \t\t u[i-1][j][k][4]*um1);\n90:       }\n91:     }\n92:   }\n93: \n94: /*--------------------------------------------------------------------\n95: c     compute y-direction fluxes \n96: c-------------------------------------------------------------------*/\n97:   for (i = 1; i < grid_points[0]-1; i++) {\n98:     for (j = 1; j < grid_points[1]-1; j++) {\n99:       for (k = 1; k < grid_points[2]-1; k++) {\n100: \tvijk = vs[i][j][k];\n101: \tvp1  = vs[i][j+1][k];\n102: \tvm1  = vs[i][j-1][k];\n103: \n104: \trhs[i][j][k][0] = rhs[i][j][k][0] + dy1ty1 * \n105: \t  (v[i][j+1][k][0] - 2.0*v[i][j][k][0] + \n106: \t   v[i][j-1][k][0]) -\n107: \t  ty2 * (v[i][j+1][k][1] - v[i][j-1][k][1]);\n108: \n109: \trhs[i][j][k][1] = rhs[i][j][k][1] + dy2ty1 * \n110: \t  (v[i", "label": "(171, OpenMP, #pragma omp for private(j,k,vijk,vp1,vm1)); (397, OpenMP, #pragma omp for private(j,m)); (7, OpenMP, #pragma omp for nowait private(j,k,rho_inv)); (127, OpenMP, #pragma omp for nowait private(j,k,m)); (103, OpenMP, #pragma omp for nowait private(k,m)); (343, OpenMP, #pragma omp for nowait private(j,m)); (151, OpenMP, #pragma omp for private(k,m)); (25, OpenMP, #pragma omp for private(j,k,m)); (38, OpenMP, #pragma omp for private(j,k,uijk,up1,um1)); (282, OpenMP, #pragma omp for private(j,k,wijk,wp1,wm1)); (399, OpenMP, #pragma omp for private(k,m,i))"}
{"predict": "(4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)\n(5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)", "label": "(28, MPI, MPI_Type_free(&ddt);); (7, MPI, MPI_Init(&argc, &argv);); (12, MPI, MPI_Finalize();); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (21, MPI, MPI_DDT_DUMP(ddt);)"}
{"predict": "(15, MPI, MPI_Init(&argc, &argv);)", "label": "(57, MPI, MPI_Recv(outbufs[j], counts[j], types[j], partner, tag, comm, &status);); (45, MPI, MPI_Type_get_name(types[j], myname, &mynamelen);); (49, MPI, MPI_Send(inbufs[j], counts[j], types[j], partner, tag, comm);); (35, MPI, MPI_Comm_size(comm, &np);); (25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (32, MPI, MPI_Comm_rank(comm, &rank);); (53, MPI, MPI_Get_count(&status, types[j], &count);)"}
{"predict": "(12, MPI, MPI_Info_create(&infos[i]);) (25, MPI, MPI_Info_free(&infos[i]);) (35, MPI, MPI_Info_create(&infos[i]);) (37, MPI, MPI_Info_free(&infos[i]);)", "label": "(16, MPI, MPI_Init(&argc, &argv);); (20, MPI, MPI_Finalize();)"}
{"predict": "9:             status = dmapp_get(target, source, seg, rmpe, i, DMAPP_DW);\n110:             t1 = MPI_Wtime();\n111:             assert(status==DMAPP_RC_SUCCESS);\n112:             dt = t1-t0;\n113:             bw = 4 * 1e-6 * (double)i / dt;\n114:             fprintf(stderr,\"%d: %12d bytes %12lf seconds = %lf MB/s \\n\", mype, 4*i, dt, bw);\n115:         }\n116:     }\n117:     fflush(stderr);\n118:     PMI_Barrier();\n119: \n119:     /* Free memory allocated in the sheap. */\n120:     dmapp_sheap_free( source );\n121:     dmapp_sheap_free( target );\n122: \n123:     /* Finalize DMAPP resources. */\n124:     status = dmapp_fini();\n125:     assert(status==DMAPP_RC_SUCCESS);\n126: \n127:     return 0;\n128: }", "label": "(14, MPI, MPI_Init(&argc, &argv);); (148, MPI, MPI_Finalize();); (72, MPI, t0 = MPI_Wtime();); (73, MPI, t1 = MPI_Wtime();)"}
{"predict": "(11, MPI, fprintf(stderr, \"Failed to initialize MPI_Tools sub-system.\\n\");) (13, MPI, printf(\"Failed to create a session for PVARs.\\n\");) (14, MPI, MPI_Abort(MPI_COMM_WORLD, MPIT_result);)", "label": "(11, MPI, MPI_Abort(MPI_COMM_WORLD, MPIT_result);); (12, MPI, MPIT_result = MPI_T_pvar_session_create(session);); (6, MPI, MPIT_result = MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);)"}
{"predict": "(10, CUDA, if (warp.meta_group_rank() == 0) { dpreatt += t * T; datt += t * T; att += t * T; } block.sync(); local_sum = 0; for (int t2 = block.thread_rank(); t2 <= t; t2 += BlockSize) { local_sum += att_bth[t2] * datt_bth[t2]; } block_acc[warp.meta_group_rank()] = cg::reduce(warp, local_sum, cg::plus<float>{}); block.sync(); local_sum = cg::reduce(warp, block_acc[warp.thread_rank()], cg::plus<float>{}); for (int t3 = block.thread_rank(); t3 <= t; t3 += BlockSize) { float acc = __ldcs(att_bth + t3) * (__ldcs(datt_bth + t3) - local_sum); __stcs(dpreatt_bth + t3, scale * acc); } )", "label": "(6, CUDA, __shared__ float block_acc[32];)"}
{"predict": "(8, OpenMP, #pragma omp parallel for private(i) reduction(+:Stats[0], Stats[1]))", "label": "(3, OpenMP, auto maxThreads = omp_get_max_threads();)"}
{"predict": "(10, MPI, if (rank == 3) { MPI_Recv(str, 10, MPI_CHAR, 0, 0, intercomm, MPI_STATUS_IGNORE); })", "label": "(6, MPI, MPI_Init(&argc, &argv);); (19, MPI, MPI_Finalize();); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (7, MPI, MPI_Comm_get_parent(&intercomm);); (11, MPI, err = MPI_Recv(str, 4, MPI_CHAR, 3, 0, intercomm, MPI_STATUS_IGNORE);); (8, MPI, err = MPI_Send(\"hi\", 3, MPI_CHAR, 3, 0, intercomm);)"}
{"predict": "(30, CUDA, reference_gemm_naive_kernel<<<grid_dim, block_dim>>>(alpha, tensor_a, tensor_b, beta, tensor_c);)", "label": "(31, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (29, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());)"}
{"predict": "(3, OpenMP, #pragma omp parallel for)", "label": "(5, OpenMP, #pragma omp task final (argc); (3, OpenMP, #pragma omp task final ()); (8, OpenMP, #pragma omp task final (argv[1]=2)); (4, OpenMP, #pragma omp task final (argc > 0 ? argv[1] : argv[2])); (8, OpenMP, #pragma omp task final (1 0)); (1, OpenMP, #pragma omp task final (z+argc))); (2, OpenMP, #pragma omp task final (foobool(argc)), final (true)); (6, OpenMP, #pragma omp task final(if(tmain(argc, argv)); (-5, OpenMP, #pragma omp task final (); (2, OpenMP, #pragma omp task final (argc argc)); (-8, OpenMP, #pragma omp task final); (-2, OpenMP, #pragma omp task final (S1))"}
{"predict": "(17, OpenMP, #pragma omp parallel for reduction(+:nnzL) reduction(max:n))", "label": "(49, MPI, MPI_Allreduce(MPI_IN_PLACE,&n,1,mpi_int_t,MPI_MAX,grid->comm);); (47, MPI, MPI_Allreduce(MPI_IN_PLACE,&nnzL,1,mpi_int_t,MPI_SUM,grid->comm);)"}
{"predict": "= 0; i < BUFFER_SIZE; ++i)\n97:     {\n98:         cmp_host[i] = src1_host[i] == src2_host[i];\n99:     }\n100:     initDestBuffer(dest_host.data(), stype, d);\n101:     initRefBuffer(ref.data(), stype, d);\n102:     initRefBuffer(sref.data(), stype, d);\n103: \n104:     src1 = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR,\n105:                           BUFFER_SIZE, src1_host.data(), &err);\n106:     test_error_count(err, \"Error: could not create src1 buffer\\n\");\n107:     src2 = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR,\n108:                           BUFFER_SIZE, src2_host.data(), &err);\n109:     test_error_count(err, \"Error: could not create src2 buffer\\n\");\n110:     cmp = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR,\n111:                          BUFFER_SIZE, cmp_host.data(), &err);\n112:     test_error_count(err, \"Error: could not create cmp buffer\\n\");\n113:     dest = clCreateBuffer(context, CL_MEM_WRITE_ONLY, BUFFER_SIZE, nullptr, &err);\n114:     test_error_count(err, \"Error: could not create dest buffer\\n\");\n115: \n116:     for (size_t vecsize = 0; vecsize < VECTOR_SIZE_COUNT; ++vecsize)\n117:     {\n118:         test_error_count(err, \"Error: cannot enqueue kernel!\\n\");\n119:         test_error_count(err, \"Error: cannot enqueue kernel!\\n\");\n120:         test_error_count(err, \"Error: cannot enqueue kernel!\\n\");\n121:         test_error_count(err, \"Error: cannot enqueue kernel!\\n\");\n122:     }\n123: \n124:     // We block the test as we are running over the range of compare values\n125:     // \"block the test\" means \"break the test into blocks\"\n126:     if( type_size[stype] == 4 )\n127:         cmp_stride = block_elements", "label": "(51, OpenCL, dest = clCreateBuffer( context, CL_MEM_WRITE_ONLY, BUFFER_SIZE, NULL, &err );); (44, OpenCL, src1 = clCreateBuffer( context, CL_MEM_READ_ONLY, BUFFER_SIZE, NULL, &err );); (72, OpenCL, err = clSetKernelArg(kernels[vecsize], 0, sizeof dest, &dest);); (44, OpenCL, src2 = clCreateBuffer( context, CL_MEM_READ_ONLY, BUFFER_SIZE, NULL, &err );); (76, OpenCL, err = clSetKernelArg(kernels[vecsize], 3, sizeof cmp, &cmp);); (71, OpenCL, err = clSetKernelArg(kernels[vecsize], 1, sizeof src1, &src1);); (134, OpenCL, err = clEnqueueNDRangeKernel(queue, kernels[vecsize], 1, NULL, &vector_count, NULL, 0, NULL, NULL);); (42, OpenCL, cmp = clCreateBuffer( context, CL_MEM_READ_ONLY, BUFFER_SIZE, NULL, &err );); (70, OpenCL, err = clSetKernelArg(kernels[vecsize], 2, sizeof src2, &src2);)"}
{"predict": "(9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);)", "label": "(17, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (21, MPI, MPI_Reduce(&data, &result, 1, eq_type, op, 0, MPI_COMM_WORLD);); (25, MPI, MPI_Op_free(&op);); (10, MPI, MPI_Type_create_struct(2, blklens, displs, types, &eq_type);); (10, MPI, MPI_Type_commit(&eq_type);); (3, MPI, MPI_Op_create(myop, 0, &op);); (10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (21, MPI, MPI_Type_free(&eq_type);)"}
{"predict": "(7, OpenMP, #pragma omp parallel) (8, OpenMP, #pragma omp atomic) (9, OpenMP, #pragma omp atomic)", "label": "(7, OpenMP, #pragma omp critical(add_first)); (3, OpenMP, #pragma omp parallel shared(var1, var2)); (7, OpenMP, #pragma omp atomic); (8, OpenMP, #pragma omp single)"}
{"predict": "(7, TBB, parallel_for(blocked_range<int>(MinThread, MaxThread + 1), [&](int p) { test_concurrency(p); }));", "label": "(9, TBB, test_ports_return_references<tbb::flow::rejecting>();); (9, TBB, lightweight_testing::test<tbb::flow::multifunction_node>(10);); (13, TBB, test_extract<tbb::flow::rejecting>();); (5, TBB, test_ports_return_references<tbb::flow::queueing>();); (12, TBB, test_extract<tbb::flow::queueing>();)"}
{"predict": "MPI_Error_string(mpirc, mpierrstr, &mpierrlen);\n119:             MFU_LOG(MFU_LOG_ERR, \"Failed to read file: `%s' rc=%d %s\", name, mpirc, mpierrstr);\n120:         }\n121: \n122:         /* scan for newline, split into records */\n123:         char* ptr = bufstart;\n124:         char* newline = (char*) memchr(ptr, '\\n', read_count);\n125:         while (newline != NULL) {\n126:             /* create a new record and insert it into list */\n127:             flist_t* rec = flist_new();\n128:             rec->data = MFU_MALLOC(newline - ptr + 1);\n129:             memcpy(rec->data, ptr, newline - ptr + 1);\n130:             rec->data[newline - ptr + 1] = '\\0';\n131:             flist_insert(flist, rec);\n132: \n133:             /* advance ptr to start of next record */\n134:             ptr = newline + 1;\n135: \n136:             /* find next newline */\n137:             newline = (char*) memchr(ptr, '\\n', read_count - (ptr - bufstart));\n138:         }\n139: \n140:         /* if we have remaining data that's not a newline, then\n141:          * create a new record for it */\n142:         if (ptr != bufstart + read_count) {\n143:             flist_t* rec = flist_new();\n144:             rec->data = MFU_MALLOC(ptr - bufstart + 1);\n145:             memcpy(rec->data, bufstart, ptr - bufstart + 1);\n146:             rec->data[ptr - bufstart + 1] = '\\0';\n147:             flist_insert(flist, rec);\n148:         }\n149: \n150:         /* update buffer offset and read offset */\n151:         bufoffset = ptr - (char*) buf;\n152:         read_offset += read_count;\n153: \n154:         /* if we", "label": "(64, MPI, MPI_Error_string(mpirc, mpierrstr, &mpierrlen);); (45, MPI, MPI_Exscan(&chunk_count, &chunk_offset, 1, MPI_UINT64_T, MPI_SUM, MPI_COMM_WORLD);); (88, MPI, mpirc = MPI_File_read_at(fh, pos, buf, 1, MPI_CHAR, &status);); (120, MPI, mpirc = MPI_File_read_at(fh, read_offset, bufstart, read_count, MPI_CHAR, &status);); (58, MPI, int mpirc = MPI_File_set_view(fh, disp, MPI_CHAR, MPI_CHAR, datarep, MPI_INFO_NULL);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (19, MPI, MPI_Bcast(&filesize, 1, MPI_UINT64_T, 0, MPI_COMM_WORLD);)"}
{"predict": "(17, CUDA, atomicAdd(sum, val);)", "label": "(1, CUDA, __shared__ float s_mem[32];)"}
{"predict": "(12, MPI, MPI_Get_address(&tmp_lb, &tmp_lb));", "label": "(15, MPI, rc = MPI_Type_dup(type, newtype);); (11, MPI, rc = MPI_Type_get_extent(type, &tmp_lb, &attr->child_type_extent);); (19, MPI, rc = MPI_Type_free(&type);)"}
{"predict": "(27, OpenMP, #pragma omp parallel for reduction(+:ret_sum))", "label": "(60, OpenMP, #pragma omp parallel for reduction(+ : ret_sum))"}
{"predict": "(19, TBB, tbb::task::spawn(t);)", "label": "(3, TBB, tbb::task* dummy_root = new( tbb::task::allocate_root() ) tbb::empty_task;); (22, TBB, tbb::task::destroy( *dummy_root );); (13, TBB, tbb::task::spawn_root_and_wait(t);); (16, TBB, tbb::task::enqueue(t);)"}
{"predict": "(23, OpenMP, #pragma omp parallel for)", "label": "(28, OpenMP, #pragma omp target update from(z[:10])); (17, OpenMP, #pragma omp target update to(z[:10])); (18, OpenMP, #pragma omp target map(to:k[:5]) map(always, tofrom:x) map(tofrom:y))"}
{"predict": "(22, OpenMP, #pragma omp parallel for private(t))\n(25, OpenMP, #pragma omp parallel for private(t))", "label": "(28, OpenMP, #pragma omp target update from(z[:10])); (17, OpenMP, #pragma omp target update to(z[:10])); (18, OpenMP, #pragma omp target map(to:k[:5]) map(always, tofrom:x) map(tofrom:y))"}
{"predict": "(20, OpenMP, return NULL; #pragma omp parallel)", "label": "(18, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(12, OpenMP, #pragma omp atomic update b[i]++;)", "label": "(4, OpenMP, #pragma omp teams distribute); (2, OpenMP, #pragma omp target)"}
{"predict": "(9, CUDA, __syncthreads();)", "label": "(41, CUDA, return cudaGetLastError();)"}
{"predict": ":       scalar_t alpha = gamma * epsilon;\n123:       scalar_t beta = gamma * epsilon;\n124:       // step_vec = alpha * step_vec + beta * q\n125:       step_vec[batch * v_dim + threadIdx.x] =\n126:         alpha * step_vec[batch * v_dim + threadIdx.x] +\n127:         beta * q[batch * v_dim + threadIdx.x];\n128:     }", "label": "(14, CUDA, extern __shared__ float alpha_buffer_sh[];)"}
{"predict": "(26, MPI, MPI_Bcast(&rc, 1, MPI_INT, 0, MPI_COMM_WORLD);)\n(27, MPI, MPI_Bcast(attr, 1, MPI_DTPI_ATTR_S, 0, MPI_COMM_WORLD);)\n(28, MPI, MPI_Bcast(newtype, 1, MPI_DATATYPE, 0, MPI_COMM_WORLD);)\n(29, MPI, MPI_Bcast(new_count, 1, MPI_AINT, 0, MPI_COMM_WORLD);)\n(30, MPI, MPI_Bcast(desc_list, 1, MPI_CHAR, 0, MPI_COMM_WORLD);)\n(31, MPI, MPI_Bcast(&depth, 1, MPI_INT, 0, MPI_COMM_WORLD);)", "label": "(65, MPI, rc = MPI_Type_free(&type);)"}
{"predict": "(12, MPI, MPI_Dims_create(ntasks, 2, dims);) (29, MPI, MPI_Cart_create(MPI_COMM_WORLD, 2, dims, period, 1, &comm2d);) (31, MPI, MPI_Cart_shift(comm2d, 0, 1, &neighbors[0], &neighbors[1]);) (32, MPI, MPI_Cart_shift(comm2d, 1, 1, &neighbors[2], &neighbors[3]);) (34, MPI, MPI_Cart_coords(comm2d, crank, 2, coords);)", "label": "(37, MPI, MPI_Cart_shift(comm2d, 0, 1, &neighbors[0], &neighbors[1]);); (8, MPI, MPI_Init(&argc, &argv);); (26, MPI, MPI_Finalize();); (38, MPI, MPI_Cart_coords(comm2d, crank, 2, coords);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (28, MPI, MPI_Cart_create(MPI_COMM_WORLD, 2, dims, period, 1, &comm2d);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (42, MPI, MPI_Barrier(comm2d);); (26, MPI, MPI_Comm_rank(comm2d, &crank);); (29, MPI, MPI_Cart_shift(comm2d, 1, 1, &neighbors[2], &neighbors[3]);)"}
{"predict": "(18, CUDA, kernel_KaKjw_to_KKwaj<<<grid_dim, block_dim>>>(nwalk, nkpts, npol, nmo_max, nmo_tot, nocc_max, nmo, nmo0, nocc, nocc0, reinterpret_cast<thrust::complex<double> const*>(A), reinterpret_cast<thrust::complex<double>*>(B));)", "label": "(21, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize(), \"KaKjw_to_KKwaj\");); (19, CUDA, qmc_cuda::cuda_check(cudaGetLastError(), \"KaKjw_to_KKwaj\");)"}
{"predict": "(3, OpenMP, #pragma omp parallel for reduction(+:a) private(a))", "label": "(12, OpenMP, #pragma omp atomic hint(1) hint(1)); (7, OpenMP, #pragma omp atomic hint(a); (4, OpenMP, #pragma omp atomic hint(+); (1, OpenMP, #pragma omp atomic hint(); (6, OpenMP, #pragma omp atomic hint(a)); (-3, OpenMP, #pragma omp atomic hint)"}
{"predict": "**************************/\n136: \n137:     cutensorAlgo_t algo;\n138:     HANDLE_ERROR(cutensorFindBestHeuristic(handle, desc, nullptr, nullptr, nullptr, &algo));\n139: \n140:     /**************************\n141:     * Allocate memory on device\n142:     **************************/\n143: \n144:     HANDLE_ERROR(cutensorMalloc(handle, &A_d, sizeA));\n145:     HANDLE_ERROR(cutensorMalloc(handle, &C_d, sizeC));\n146: \n147:     /**************************\n148:     * Copy data from host to device\n149:     **************************/\n150: \n151:     HANDLE_ERROR(cutensorMemcpyAsync(handle, C_d, A, sizeA, CUDA_R2D));\n152: \n153:     /**************************\n154:     * Execute the operation\n155:     **************************/\n156: \n157:     HANDLE_ERROR(cutensorExecute(handle, desc, alpha, A_d, nullptr, C_d, nullptr, nullptr, algo));\n158: \n159:     /**************************\n160:     * Copy data back to host\n161:     **************************/\n162: \n163:     HANDLE_ERROR(cutensorMemcpyAsync(handle, C, C_d, sizeC, CUDA_D2R));\n164: \n165:     /**************************\n166:     * Free resources\n167:     **************************/\n168: \n169:     HANDLE_ERROR(cutensorDestroyTensorDescriptor(descA));\n170:     HANDLE_ERROR(cutensorDestroyTensorDescriptor(descC));\n171:     HANDLE_ERROR(cutensorDestroyPermutationDescriptor(desc));\n172:     HANDLE_ERROR(cutensorDestroy(handle));\n173: \n174:     /**************************\n175:     * Free device memory\n176:     **************************/\n177: \n178:     HANDLE_ERROR(cutensorFree(handle, A_d));\n179:     HANDLE_ERROR(cutensorFree(handle, C_d));\n180: \n181:     return 0;\n182: }", "label": "(63, CUDA, HANDLE_CUDA_ERROR(cudaMallocHost((void**) &C, sizeof(floatTypeC) * elementsC));); (78, CUDA, HANDLE_CUDA_ERROR(cudaMemcpy2DAsync(A_d, sizeA, A, sizeA, sizeA, 1, cudaMemcpyDefault, nullptr));); (192, CUDA, if (C_d) cudaFree(C_d);); (188, CUDA, if (A) cudaFreeHost(A);); (51, CUDA, HANDLE_CUDA_ERROR(cudaMalloc((void**) &C_d, sizeC));); (49, CUDA, HANDLE_CUDA_ERROR(cudaMalloc((void**) &A_d, sizeA));); (187, CUDA, if (A_d) cudaFree(A_d);); (185, CUDA, if (C) cudaFreeHost(C);); (54, CUDA, HANDLE_CUDA_ERROR(cudaMallocHost((void**) &A, sizeof(floatTypeA) * elementsA));)"}
{"predict": "(17, OpenMP, #pragma omp atomic add myr = r1 + r2; #pragma omp atomic add myg = g1 + g2; #pragma omp atomic add myb = b1 + b2;)", "label": "(18, TBB, tbb::spin_mutex::scoped_lock lock(*rMutex);)"}
{"predict": "(20, CUDA, cudaMalloc(&d_RanksA, MAX_SAMPLE_COUNT * sizeof(uint));) (20, CUDA, cudaMalloc(&d_RanksB, MAX_SAMPLE_COUNT * sizeof(uint));) (20, CUDA, cudaMalloc(&d_LimitsA, MAX_SAMPLE_COUNT * sizeof(uint));) (20, CUDA, cudaMalloc(&d_LimitsB, MAX_SAMPLE_COUNT * sizeof(uint));) (19, CUDA, cudaFree(d_RanksA);) (19, CUDA, cudaFree(d_RanksB);) (19, CUDA, cudaFree(d_LimitsA);) (19, CUDA, cudaFree(d_LimitsB);)", "label": "(21, CUDA, checkCudaErrors(cudaMalloc((void **)&d_LimitsA, MAX_SAMPLE_COUNT * sizeof(uint)));); (19, CUDA, checkCudaErrors(cudaMalloc((void **)&d_RanksB,  MAX_SAMPLE_COUNT * sizeof(uint)));); (17, CUDA, checkCudaErrors(cudaMalloc((void **)&d_RanksA,  MAX_SAMPLE_COUNT * sizeof(uint)));); (19, CUDA, checkCudaErrors(cudaMalloc((void **)&d_LimitsB, MAX_SAMPLE_COUNT * sizeof(uint)));)"}
{"predict": "123: \n124:   std::cout << \"Rate (MFlops/s): \" << 2.*n*n*iterations/(1000000.*pipeline_time) << std::endl;\n125:   std::cout << \"Time (s): \" << pipeline_time << std::endl;\n126: \n127:   delete [] grid;\n128:   return 0;\n129: }", "label": "(79, OpenACC, #pragma acc parallel loop independent); (70, OpenACC, #pragma acc data pcopy(grid[0:n*n])); (86, OpenACC, #pragma acc parallel loop gang); (99, OpenACC, #pragma acc kernels); (89, OpenACC, #pragma acc loop vector)"}
{"predict": "(14, MPI, MPI_Allreduce(&errs, &errs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);)", "label": "(6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &world_size);)"}
{"predict": "(10, MPI, OMPI_Affinity_str(OMPI_AFFINITY_RSRC_STRING_FMT, ompi_bound, current_binding, exists);) (17, MPI, OMPI_Affinity_str(OMPI_AFFINITY_LAYOUT_FMT, ompi_bound, current_binding, exists);)", "label": "(25, MPI, MPI_Finalize();); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Init(NULL, NULL);)"}
{"predict": "(\"Matrix order             = %d\\n\", order);\n96:     printf(\"Outer block size         = %d\\n\", nb);\n97:     printf(\"Inner block flag         = %d\\n\", inner_block_flag);\n98:     printf(\"Local block size         = %d\\n\", nb);\n99:   }\n100: \n101:   /* generate a random A and B */\n102:   srand(my_ID);\n103:   for (i = 0; i < order; i++) {\n104:     for (j = 0; j < order; j++) {\n105:       a[i*order + j] = (double) rand() / (double) (RAND_MAX);\n106:       b[i*order + j] = (double) rand() / (double) (RAND_MAX);\n107:     }\n108:   }\n109: \n110:   /* generate the matrix C */\n111:   for (i = 0; i < order; i++) {\n112:     for (j = 0; j < order; j++) {\n113:       c[i*order + j] = 0.0;\n114:     }\n115:   }\n116: \n117:   /* compute local row and column indices */\n118:   mynrows = order / nprow;\n119:   myfrow = mynrows * myrow;\n120:   mylrow = myfrow + mynrows - 1;\n121:   myncols = order / npcol;\n122:   myfcol = mynrows * mycol;\n123:   mylcol = myfcol + mynrows - 1;\n124: \n125:   /* compute local dimensions of a, b, and c */\n126:   lda = order;\n127:   ldb = order;\n128:   ldc = order;\n129: \n130:   /* create the row and column communicators */\n131:   MPI_Comm_split(MPI_COMM_WORLD, myrow, my_ID, &comm_row);\n132:   MPI_Comm_split(MPI_COMM_WORLD, mycol, my_ID, &comm_col);", "label": "(124, MPI, MPI_Comm_group( MPI_COMM_WORLD, &world_group );); (143, MPI, MPI_Comm_rank( comm_col, &myrow );); (205, MPI, MPI_Barrier(MPI_COMM_WORLD);); (83, MPI, MPI_Bcast(&inner_block_flag, 1, MPI_INT, root, MPI_COMM_WORLD);); (135, MPI, MPI_Group_incl( world_group, nprow, ranks, &temp_group );); (39, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &my_ID );); (78, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (124, MPI, MPI_Group_incl( world_group, npcol, ranks, &temp_group );); (35, MPI, MPI_Init(&argc,&argv);); (131, MPI, MPI_Comm_create( MPI_COMM_WORLD, temp_group, &comm_col );); (35, MPI, MPI_Comm_size( MPI_COMM_WORLD, &Num_procs );); (121, MPI, MPI_Comm_create( MPI_COMM_WORLD, temp_group, &comm_row );); (245, MPI, MPI_Finalize();); (70, MPI, MPI_Bcast(&order,  1, MPI_INT, root, MPI_COMM_WORLD);); (167, MPI, MPI_Allgather( &mynrows, 1, MPI_INT, mm, 1, MPI_INT, comm_col );); (70, MPI, MPI_Bcast(&nb, 1, MPI_INT, root, MPI_COMM_WORLD);); (127, MPI, MPI_Comm_rank( comm_row, &mycol );); (172, MPI, MPI_Allgather( &myncols, 1, MPI_INT, nn, 1, MPI_INT, comm_row );)"}
{"predict": "(3, OpenMP, #pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp target teams distribute parallel for if (false)); (9, OpenMP, #pragma omp target teams distribute parallel for if (parallel: Arg)); (0, OpenMP, #pragma omp target teams distribute parallel for if (true))"}
{"predict": "(7, MPI, T result = mpi_world_communicator.Min(local, root);)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(9, MPI, MPI_Reduce(inbuf, inoutbuf, count, datatype, MPI_SUM, 0, MPI_COMM_WORLD);)", "label": "(8, MPI, MPI_Reduce_local_c(inbuf, inoutbuf, count, datatype, MPI_SUM);)"}
{"predict": "(10, MPI, MPI_Allreduce(local.data(), output.data(), local.size(), MPI_DOUBLE, MPI_SUM, mpi_world_communicator.get());) (21, MPI, MPI_Reduce(local.data(), returned_result.data(), local.size(), MPI_DOUBLE, MPI_SUM, root, mpi_world_communicator.get());)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "32:                     DIAGNOSTICS(\"Read shared \", c_info, c_info->r_buffer, Locsize, Totalsize, j, pos);\n133: \n134:                     CHK_DIFF(\"Read_shar\", c_info, c_info->r_buffer, 0,\n135:                              Locsize, Totalsize, asize,\n136:                              get, pos, i_sample*j_sample, j,\n137:                              -2, &defect);\n138:                 } /*for( j=0; j<j_sample; j++ )*/\n139:             } else {\n140:                 printf(\"Error: unknown position type\\n\");\n141:                 exit(1);\n142:             }\n143:             if (time_inner) {\n144:                 *time += MPI_Wtime();\n145:             }\n146:         } /*for( i=0; i<i_sample; i++ )*/\n147: \n148:         if (time_inner)\n149:             *time = *time / i_sample;\n150: \n151:     } else {\n152:         printf(\"Error: File_rank is not initialized\\n\");\n153:         exit(1);\n154:     }\n155: }", "label": "(102, MPI, MPI_Barrier(c_info->File_comm);); (108, MPI, MPI_ERRHAND(GEN_File_read(c_info->fh, c_info->r_buffer, Locsize, c_info->etype, &stat));); (121, MPI, MPI_ERRHAND(GEN_File_read_at(c_info->fh, Offset, c_info->r_buffer, Locsize, c_info->etype, &stat));); (133, MPI, MPI_ERRHAND(GEN_File_read_shared(c_info->fh, c_info->r_buffer, Locsize, c_info->etype, &stat));); (92, MPI, *time = MPI_Wtime();)"}
{"predict": "(10, CUDA, qmc_cuda::cuda_check(cudaMemcpyAsync(std::addressof(res), thrust::raw_pointer_cast(d_ptr), sizeof(std::complex<double>), cudaMemcpyDeviceToHost, stream)));", "label": "(6, CUDA, thrust::device_ptr<thrust::complex<double>> d_ptr = thrust::device_malloc<thrust::complex<double>>(1);); (14, CUDA, thrust::device_free(d_ptr);); (9, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize());); (7, CUDA, qmc_cuda::cuda_check(cudaGetLastError());)"}
{"predict": "(1, CUDA, __global__ void foomain(I argc, C **argv) {", "label": "(63, OpenMP, #pragma omp distribute parallel for simd private(h)); (86, OpenMP, #pragma omp distribute parallel for simd private(j)); (82, OpenMP, #pragma omp parallel private(i)); (65, OpenMP, #pragma omp distribute parallel for simd nowait); (9, OpenMP, #pragma omp distribute parallel for simd private(); (1, OpenMP, #pragma omp target); (27, OpenMP, #pragma omp distribute parallel for simd private(argc > 0 ? argv[1] : argv[2])); (1, OpenMP, #pragma omp distribute parallel for simd private); (35, OpenMP, #pragma omp distribute parallel for simd private(S1)); (19, OpenMP, #pragma omp distribute parallel for simd private(argc,); (61, OpenMP, #pragma omp parallel); (12, OpenMP, #pragma omp distribute parallel for simd private(argc); (36, OpenMP, #pragma omp distribute parallel for simd private(a, b)); (25, OpenMP, #pragma omp distribute parallel for simd private(argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (39, OpenMP, #pragma omp distribute parallel for simd private(argv[1])); (43, OpenMP, #pragma omp distribute parallel for simd private(e, g)); (2, OpenMP, #pragma omp distribute parallel for simd private()); (60, OpenMP, #pragma omp distribute parallel for simd private(i)); (65, OpenMP, #pragma omp parallel shared(i)); (-12, OpenMP, #pragma omp teams)"}
{"predict": "(4, OpenCL, clReleaseMemObject(objX);) (7, OpenCL, clReleaseMemObject(objNrm2);) (10, OpenCL, clReleaseMemObject(objScratch);)", "label": "(5, OpenCL, clReleaseMemObject(objX);); (12, OpenCL, clReleaseMemObject(objScratch);); (7, OpenCL, clReleaseMemObject(objNrm2);)"}
{"predict": "float dxy = val - data1[-pitch-1] - data1[-pitch+1] - data1[pitch-1] - data1[pitch+1];\n96:     float det = dxx*dyy - dxy*dxy;\n97:     float trace = dxx + dyy;\n98:     float scale2 = subsampling * subsampling * scale * scale;\n99:     float scale4 = scale2 * scale2;\n100:    float det2 = det * det;\n101:    float det4 = det2 * det2;\n102:    float k = 0.04f * scale4 * scale4 * scale2;\n103:    float r = det2 / (k + det4);\n104:    if (r < factor) {\n105:      float edge = sqrtf(trace * trace - 4.0f * det);\n106:      if (edge < edgeLimit) {\n107:        float s = sqrtf(scale2);\n108:        d_Sift[pos] = SiftPoint(xpos, ypos, scale, s, trace, det, r, edge);\n109:        atomicAdd(&d_PointCounter[2*octave+0], 1);\n110:      }\n111:    }\n112:  }\n113:}\n\nOutput:\n(5, CUDA, atomicMax(&d_PointCounter[2*octave+0], d_PointCounter[2*octave-1]);) (5, CUDA, atomicMax(&d_PointCounter[2*octave+1], d_PointCounter[2*octave-1]);)", "label": "(3, CUDA, __shared__ unsigned short points[2*MEMWID];)"}
{"predict": "(16, CUDA, ASSERT_NO_THROW(cudaSimulation.simulate());)", "label": "(13, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(11, MPI, int rank = group->group_rank;)\n(29, MPI, MPI_Waitall(2, request, status);)\n(77, MPI, MPI_Waitall(2, request, status);)", "label": "(67, MPI, MPI_Waitall(2, request, status);)"}
{"predict": "(12, MPI, MTest_Init(&argc, &argv);)", "label": "(28, MPI, MPI_Group_free(&wingroup);); (19, MPI, MPI_Win_create(buf, sizeof(int) * 10, sizeof(int), MPI_INFO_NULL, comm, &win);); (28, MPI, MPI_Win_free(&win);); (20, MPI, MPI_Group_compare(group, wingroup, &result);); (25, MPI, MPI_Group_free(&group);); (17, MPI, MPI_Comm_group(comm, &group);); (15, MPI, MPI_Win_get_group(win, &wingroup);)"}
{"predict": "(34, OpenMP, #pragma omp parallel for reduction(+:blitz::Array<double,4>))", "label": "(14, CUDA, ColorSpinorParam cudaParam(cpuParam, *inv_param, QUDA_CUDA_FIELD_LOCATION);)"}
{"predict": "(17, OpenMP, #pragma omp parallel for private(j) shared(a, b, N) for (int j = 0; j< N; j++))", "label": "(15, OpenMP, #pragma omp target parallel for)"}
{"predict": "(35, CUDA, dim3 grid(numBlkPerSM*numSM); dim3 block(nTPB); cudaLaunchCooperativeKernel((void *)my_remove_if<mytype>, grid, block, args, 0, str);)", "label": "(58, CUDA, thrust::host_vector<mytype> th_data = t_data;); (40, CUDA, thrust::device_vector<mytype> t_data(h_data, h_data+test_dsize);); (49, CUDA, cudaEventElapsedTime(&et, start, stop);); (22, CUDA, cudaStreamCreate(&str);); (27, CUDA, cudaLaunchCooperativeKernel((void *)my_remove_if<mytype>, FIXME);); (8, CUDA, cudaError_t err = cudaGetDeviceProperties(&prop, 0);); (26, CUDA, cudaMemcpy(h_data, d_odata, tsize, cudaMemcpyDeviceToHost);); (49, CUDA, thrust::remove(t_data.begin(), t_data.end(), -1);); (15, CUDA, cudaMemcpy(d_idata, h_data, tsize, cudaMemcpyHostToDevice);); (39, CUDA, cudaEventRecord(stop);); (-1, CUDA, cudaMalloc(&d_idxs, test_dsize*sizeof(unsigned));); (33, CUDA, cudaEventCreate(&start);); (-5, CUDA, cudaMalloc(&d_odata, tsize);); (33, CUDA, cudaEventRecord(start);); (-6, CUDA, cudaMemset(d_odata, 0, tsize);); (-9, CUDA, cudaMalloc(&d_idata, tsize);); (29, CUDA, cudaEventCreate(&stop);); (2, CUDA, cudaOccupancyMaxActiveBlocksPerMultiprocessor(&numBlkPerSM, my_remove_if<mytype>, nTPB, 0);)"}
{"predict": "(24, OpenMP, #pragma omp parallel for reduction(+:nprocs_recv))\n(44, OpenMP, #pragma omp parallel for)\n(57, OpenMP, #pragma omp parallel for reduction(+:sum))", "label": "(48, MPI, MPI_Type_commit(recv_types + j);)"}
{"predict": "(27, CUDA, trajectory_costs_gpu = Eigen::MatrixXf::Zero(2 * num_rollouts, 1);)", "label": "(25, CUDA, HANDLE_ERROR(cudaStreamSynchronize(stream));); (9, CUDA, HANDLE_ERROR(cudaMalloc((void**)&initial_x_d, sizeof(float) * 2 * DYN_T::STATE_DIM));); (9, CUDA, HANDLE_ERROR(cudaMalloc((void**)&cost_trajectories_d, sizeof(float) * 2 * num_rollouts));)"}
{"predict": "(2, CUDA, extern \"C\" __global__ void scan_v2_kernel(float *d_output, float *d_input, int length) {)", "label": "(6, CUDA, extern __shared__ float s_buffer[];)"}
{"predict": "(10, MPI, nr_complete = 0;)\n(12, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(19, MPI, MPI_Grequest_complete(aio_reqlist[i]->req);)"}
{"predict": "(29, CUDA, cudaEvent_t start, stop;)\n(32, CUDA, cudaEventRecord(start, 0);)\n(41, CUDA, cudaEventRecord(stop, 0);)\n(42, CUDA, cudaEventSynchronize(stop);)", "label": "(67, CUDA, cudaEventSynchronize(stop);); (39, CUDA, cudaMalloc((void **) &d_a, sizeof(int)*m*n);); (114, CUDA, cudaFreeHost(h_cc);); (34, CUDA, cudaEventRecord(start, 0);); (60, CUDA, cudaThreadSynchronize();); (36, CUDA, cudaMalloc((void **) &d_b, sizeof(int)*n*k);); (39, CUDA, cudaMemcpy(d_a, h_a, sizeof(int)*m*n, cudaMemcpyHostToDevice);); (103, CUDA, cudaFree(d_a);); (62, CUDA, cudaEventElapsedTime(&gpu_elapsed_time_ms, start, stop);); (3, CUDA, cudaMallocHost((void **) &h_c, sizeof(int)*m*k);); (3, CUDA, cudaMallocHost((void **) &h_cc, sizeof(int)*m*k);); (35, CUDA, cudaMemcpy(d_b, h_b, sizeof(int)*n*k, cudaMemcpyHostToDevice);); (-1, CUDA, cudaMallocHost((void **) &h_b, sizeof(int)*n*k);); (99, CUDA, cudaFree(d_c);); (97, CUDA, cudaFree(d_b);); (66, CUDA, cudaEventElapsedTime(&cpu_elapsed_time_ms, start, stop);); (26, CUDA, cudaMalloc((void **) &d_c, sizeof(int)*m*k);); (49, CUDA, cudaEventRecord(stop, 0);); (45, CUDA, cudaMemcpy(h_c, d_c, sizeof(int)*m*k, cudaMemcpyDeviceToHost);); (95, CUDA, cudaFreeHost(h_b);); (93, CUDA, cudaFreeHost(h_a);); (12, CUDA, cudaEventCreate(&start);); (55, OpenMP, omp_mm(h_a, m, n, h_b, n, k, h_cc);); (-13, CUDA, cudaMallocHost((void **) &h_a, sizeof(int)*m*n);); (10, CUDA, cudaEventCreate(&stop);); (90, CUDA, cudaFreeHost(h_c);)"}
{"predict": "(14, CUDA, int* shape = (int*)malloc(ndim * sizeof(int));) (28, CUDA, float* result_data = (float*)malloc(tensor1->size * sizeof(float));)", "label": "(37, CUDA, cudaMalloc((void **)&result_data, tensor1->size * sizeof(float));)"}
{"predict": "(12, OpenMP, ++(this)->a, --b, this->c /= 1;)", "label": "(1, OpenMP, #pragma omp parallel firstprivate(a, b, c, e)); (4, OpenMP, #pragma omp parallel firstprivate(a, b, c))"}
{"predict": "(7, CUDA, cudaMalloc((void**)&M_device, size);) (9, CUDA, cudaMemcpy(M_device, M_host, size, cudaMemcpyHostToDevice);) (13, CUDA, cudaMalloc((void**)&N_device, size);) (15, CUDA, cudaMemcpy(N_device, N_host, size, cudaMemcpyHostToDevice);) (17, CUDA, cudaMalloc((void**)&P_device, size);) (25, CUDA, cudaMemcpy(P_host, P_device, size, cudaMemcpyDeviceToHost);) (27, CUDA, cudaDeviceSynchronize();) (29, CUDA, cudaFree(M_device);) (30, CUDA, cudaFree(N_device);) (31, CUDA, cudaFree(P_device);)", "label": "(29, CUDA, CUDA_CHECK(cudaMemcpy(P_host, P_device, size, cudaMemcpyDeviceToHost));); (35, CUDA, cudaFree(P_device);); (10, CUDA, CUDA_CHECK(cudaMemcpy(M_device, M_host, size, cudaMemcpyHostToDevice));); (34, CUDA, cudaFree(N_device);); (34, CUDA, cudaFree(M_device);); (25, CUDA, CUDA_CHECK(cudaDeviceSynchronize());); (7, CUDA, CUDA_CHECK(cudaMemcpy(N_device, N_host, size, cudaMemcpyHostToDevice));); (2, CUDA, CUDA_CHECK(cudaMalloc((void**)&N_device, size));); (9, CUDA, CUDA_CHECK(cudaMalloc((void**)&P_device, size));;); (-1, CUDA, CUDA_CHECK(cudaMalloc((void**)&M_device, size));)"}
{"predict": "(26, OpenCL, clFinish(cl->context());)", "label": "(11, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)"}
{"predict": "(7, OpenMP, #pragma omp parallel for)", "label": "(17, OpenMP, #pragma omp task default(firstprivate)); (4, OpenMP, #pragma omp task default(shared), default(shared)); (0, OpenMP, #pragma omp task default(); (0, OpenMP, #pragma omp task default()); (5, OpenMP, #pragma omp task default(none)); (1, OpenMP, #pragma omp task default(x)); (7, OpenMP, #pragma omp task default(shared)); (-6, OpenMP, #pragma omp task default); (-4, OpenMP, #pragma omp task default(none)"}
{"predict": "(8, OpenMP, #pragma omp parallel for private(i, j, k, l, m, n) reduction(+:Array[l][m][n]));", "label": "(4, TBB, const tbb::blocked_range3d<int> r( 0, i, 5, 0, j, 3, 0, k, 1 );); (4, TBB, tbb::parallel_for( r, Striker() );)"}
{"predict": "{\n104:                   icol = __sync_fetch_and_add(&bmod[lib*aln_i],1);\n105:               }while(icol >= iknsupc);\n106:           }\n107: \n108:           temp1 = Uinv_bc_dat[Uinv_bc_offset[lk]+bb];\n109:           for (i = tid; i < ncol; i+=block_size){\n110:               x[icol + idx_s + i] += temp1 * lusup[idx_s + i];\n111:           }\n112:           __syncthreads();\n113:         }\n114:       }\n115:     }\n116: }", "label": "(31, CUDA, volatile __shared__ int s_bmod;); (28, CUDA, __shared__ double s_lsum[MAXSUPER];)"}
{"predict": "(4, OpenMP, #pragma omp parallel for reduction(+:nnzL) private(i, j, k, c, r, len, gb, nsupr, nsupc, nmax, mycol, extra, ncb, index, nzval) collapse(2))", "label": "(52, MPI, MPI_Allreduce(MPI_IN_PLACE,&n,1,mpi_int_t,MPI_MAX,grid->comm);); (50, MPI, MPI_Allreduce(MPI_IN_PLACE,&nnzL,1,mpi_int_t,MPI_SUM,grid->comm);)"}
{"predict": "(21, CUBLAS, cublasSetStream(cublasH, stream);) (26, CUDA, CUDA_CHECK(cudaMemcpyAsync(d_A, A.data(), sizeof(data_type) * A.size(), cudaMemcpyHostToDevice, stream));) (30, CUBLAS, CUBLAS_CHECK(cublasIdamax(cublasH, A.size(), d_A, incx, &result));) (43, CUBLAS, CUBLAS_CHECK(cublasDestroy(cublasH));)", "label": "(34, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (50, CUDA, CUDA_CHECK(cudaDeviceReset());); (47, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (23, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (41, CUDA, CUDA_CHECK(cudaFree(d_A));); (17, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(14, MPI, MPI_Type_struct(blocks, disps, types, 3, &eviltype);) (17, MPI, MPI_Type_vector(1, 4, 1, MPI_BYTE, &inttype);) (20, MPI, MPI_Type_size(inttype, &val);) (23, MPI, MPI_Type_extent(inttype, &extent);) (26, MPI, MPI_Type_lb(inttype, &lb);) (29, MPI, MPI_Type_get_extent(inttype, &aval, &true_lb);) (32, MPI, MPI_Type_ub(inttype, &aval);) (35, MPI, MPI_Type_get_true_extent(inttype, &true_lb, &aval);)", "label": "(11, MPI, err = MPI_Type_struct(3, blocks, disps, types, &inttype);); (32, MPI, err = MPI_Type_size(eviltype, &val);); (134, MPI, MPI_Type_free(&inttype);); (75, MPI, err = MPI_Type_get_extent(eviltype, &lb, &extent);); (96, MPI, err = MPI_Type_ub(eviltype, &aval);); (17, MPI, err = MPI_Type_vector(3, 4, -5, inttype, &eviltype);); (131, MPI, MPI_Type_free(&eviltype);); (108, MPI, err = MPI_Type_get_true_extent(eviltype, &true_lb, &aval);); (40, MPI, err = MPI_Type_extent(eviltype, &aval);); (54, MPI, err = MPI_Type_lb(eviltype, &aval);)"}
{"predict": "/* Test result */\n136:   if (st_result == ST_PASS)\n137:     printf(\"Test passed\\n\");\n138:   else\n139:     printf(\"Test failed\\n\");\n140: }", "label": "(48, OpenMP, lis2mdl_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)"}
{"predict": "(2, OpenMP, #pragma omp parallel for collapse(4))", "label": "(4, TBB, const tbb::blocked_range3d<int> r( 0, i, 5, 0, j, 3, 0, k, 1 );); (4, TBB, tbb::parallel_for( r, Striker() );)"}
{"predict": "(24, CUDA, for (unsigned int w = 0; w < TEST_DIMS[3]; ++w) {)\n(26, CUDA, for (unsigned int w = 0; w < TEST_DIMS[3]; ++w) {)", "label": "(36, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(27, OpenMP, #pragma omp parallel for)", "label": "(9, OpenMP, #pragma omp target); (65, OpenMP, #pragma omp distribute parallel for lastprivate(t_var, vec, s_arr, s_arr, var, var, svar)); (9, OpenMP, #pragma omp distribute parallel for lastprivate(g, g1, svar, sfvar)); (7, OpenMP, #pragma omp teams)"}
{"predict": "(21, CUDA, unpermute_kernel_backward<<<num_blocks, block_size, 0, main_stream>>>(scratch, dout, B, T, NH, HS);) (27, CUDA, cublasCheck(cublasGemmStridedBatchedEx(cublas_handle, CUBLAS_OP_T, CUBLAS_OP_N, T, T, HS, &alpha, v, CUBLAS_LOWP, HS, T * HS, scratch, CUBLAS_LOWP, HS, T * HS, &beta, datt, CUBLAS_LOWP, T, T * T, B * NH, cublas_compute, CUBLAS_GEMM_DEFAULT));) (30, CUDA, cublasCheck(cublasGemmStridedBatchedEx(cublas_handle, CUBLAS_OP_N, CUBLAS_OP_T, HS, T, T, &alpha, scratch, CUBLAS_LOWP, HS, T * HS, att, CUBLAS_LOWP, T, T * T, &beta, dv, CUBLAS_LOWP, HS, T * HS, B * NH, cublas_compute, CUBLAS_GEMM_DEFAULT));) (36, CUDA, cublasCheck(cublasGemmStridedBatchedEx(cublas_handle, CUBLAS_OP_N, CUBLAS_OP_N, HS, T, T, &alpha, k, CUBLAS_LOWP, HS, T * HS, dpreatt, CUBLAS_LOWP, T, T * T, &beta, dq, CUBLAS_LOWP, HS, T * HS, B * NH, cublas_compute, CUBLAS_GEMM_DEFAULT));) (39, CUDA, cublasCheck(cublasGemmStridedBatchedEx(cublas_handle, CUBLAS_OP_N, CUBLAS_OP_T, HS, T, T, &alpha, q, CUBLAS_LOWP, HS, T * HS, dpreatt, CUBLAS_LOWP, T, T * T, &beta, dk, CUBLAS_LOWP, HS, T * HS, B * NH, cublas_compute, CUBLAS_GEMM_DEFAULT));) (44, CUDA, permute_kernel_backward<<<num_blocks, block_size, 0, main_stream>>>(dinp, dq, dk, dv, B, T, NH, HS);)", "label": "(45, CUDA, cudaCheck(cudaGetLastError());)"}
{"predict": "(data_raw.i16bit[0]);\n109:       acceleration_mg[1] =\n110:         ism330dhcx_from_fs2g_to_mg(data_raw.i16bit[1]);\n111:       acceleration_mg[2] =\n112:         ism330dhcx_from_fs2g_to_mg(data_raw.i16bit[2]);\n113: \n114:       /* Read angular rate data */\n115:       ism330dhcx_angular_rate_raw_get(&ag_ctx, data_raw.i16bit);\n116:       angular_rate_mdps[0] =\n117:         ism330dhcx_from_fs2000dps_to_mdps(data_raw.i16bit[0]);\n118:       angular_rate_mdps[1] =\n119:         ism330dhcx_from_fs2000dps_to_mdps(data_raw.i16bit[1]);\n120:       angular_rate_mdps[2] =\n121:         ism330dhcx_from_fs2000dps_to_mdps(data_raw.i16bit[2]);\n122:     }\n123:   }\n124: }", "label": "(66, OpenMP, iis2mdc_offset_temp_comp_set(&mag_ctx, PROPERTY_ENABLE);)"}
{"predict": "(11, CUDA, mass = calculatePoleMassOnGPU(*CP_host);)", "label": "(13, CUDA, CP_host->freeCudaMem();)"}
{"predict": "32)); //32 (warp) * 8 =256\n92:     dim3 dimBlock(nthread_x, nthread_y, 1);\n93:     dim3 dimGrid(nblock_ex, 1, 1);\n94:     slsum_bmod_inv_gpu_mrhs_nvshmem<<< dimGrid, dimBlock, 0, 0 >>>(nbcol_loc,lsum,x,nrhs,nsupers,bmod, UBtree_ptr,URtree_ptr,ilsum,Ucolind_bc_dat,Ucolind_bc_offset,Unzval_bc_dat,Unzval_bc_offset,Uinv_bc_dat,Uinv_bc_offset,Uindval_loc_bc_dat,Uindval_loc_bc_offset,xsup,grid,flag_bc_q,flag_rd_q,sready_x,sready_lsum,my_flag_bc,my_flag_rd,d_nfrecv_u,h_nfrecv_u,d_status,d_colnum_u,d_mynum_u,d_mymaskstart_u,d_mymasklength_u,d_nfrecvmod_u,d_statusmod,d_colnummod_u,d_mynummod_u,d_mymaskstartmod_u,d_mymasklengthmod_u,d_recv_cnt_u,d_msgnum,d_flag_mod_u);\n95:     gpuDeviceSynchronize();\n96:     #endif\n97: }\n\nAssistant:\n(80, CUDA, slsum_bmod_inv_gpu_1rhs_new_rowdata<<< nbrow_loc, dimBlock >>>(nbrow_loc,lsum,x,nrhs,nsupers,bmod, UBtree_ptr,URtree_ptr,ilsum,Ucolind_br_dat,Ucolind_br_offset,Unzval_br_new_dat,Unzval_br_new_offset,Uinv_bc_dat,Uinv_bc_offset,xsup,grid);)", "label": "(117, CUDA, cudaDeviceSetLimit(cudaLimitStackSize, cuattr.localSizeBytes);); (93, CUDA, cudaStreamCreateWithFlags(&stream[i], cudaStreamNonBlocking);); (121, CUDA, cudaOccupancyMaxPotentialBlockSize(&minGridSize, &myblockSize, (const void *) swait_bcrd_u, 0, 0);); (173, CUDA, CUDA_CHECK(cudaGetLastError());); (112, CUDA, cudaFuncGetAttributes(&cuattr, slsum_bmod_inv_gpu_mrhs_nvshmem);); (181, CUDA, CUDA_CHECK(cudaStreamDestroy(stream[i]));)"}
{"predict": "(72, CUDA, checkHIPErrors( ::hipDeviceSynchronize() );)", "label": "(101, CUDA, checkHIPErrors( ::cudaDeviceSynchronize() );)"}
{"predict": "(11, OpenMP, #pragma omp parallel for private(i))", "label": "(27, OpenMP, #pragma omp atomic update); (21, OpenMP, #pragma omp loop order(concurrent)); (29, OpenMP, num_threads = omp_get_num_threads();); (17, OpenMP, #pragma omp target parallel num_threads(THREADS) map(tofrom: x[0:N], num_threads, total_wait_errors) map(to: y[0:N], z[0:N]))"}
{"predict": "(15, OpenMP, #pragma omp parallel for)", "label": "(9, OpenMP, #pragma omp target defaultmap(tofrom:); (4, OpenMP, #pragma omp target defaultmap()); (25, OpenMP, #pragma omp target defaultmap(none:pointer)); (10, OpenMP, #pragma omp target defaultmap(tofrom,); (40, OpenMP, #pragma omp parallel); (-4, OpenMP, #pragma omp target defaultmap); (27, OpenMP, #pragma omp target defaultmap(none:pointer) defaultmap(none:scalar) defaultmap(none:aggregate) map(g) firstprivate(argc)); (8, OpenMP, #pragma omp target defaultmap(scalar:); (11, OpenMP, #pragma omp target defaultmap(tofrom: scalar) defaultmap(tofrom: scalar)); (20, OpenMP, #pragma omp target defaultmap(none:pointer) defaultmap(none:scalar) map(argc)); (1, OpenMP, #pragma omp target defaultmap(tofrom)); (6, OpenMP, #pragma omp target defaultmap(tofrom, scalar); (23, OpenMP, #pragma omp target defaultmap(none:aggregate)); (-10, OpenMP, #pragma omp target defaultmap(); (9, OpenMP, #pragma omp target defaultmap(tofrom : scalar) defaultmap(to: scalar)); (16, OpenMP, #pragma omp target defaultmap(none:pointer) defaultmap(none:scalar) map(g)); (9, OpenMP, #pragma omp target defaultmap(alloc: pointer) defaultmap(to: scalar) defaultmap(firstprivate: pointer)); (20, OpenMP, #pragma omp target defaultmap(none:aggregate) defaultmap(none:scalar) map(argc)); (-11, OpenMP, #pragma omp target defaultmap(tofrom)"}
{"predict": "vector_length/avgtime,\n132:            avgtime);\n133:   }\n134: \n135:   /* deallocate data structures */\n136:   free(vector);\n137:   MPI_Finalize();\n138:   return 0;\n139: }", "label": "(139, MPI, MPI_Finalize();); (63, MPI, MPI_Bcast(&iterations,    1, MPI_INT, root, MPI_COMM_WORLD);); (63, MPI, MPI_Bcast(&vector_length, 1, MPI_LONG, root, MPI_COMM_WORLD);); (17, MPI, MPI_Init(&argc,&argv);); (81, MPI, MPI_Barrier(MPI_COMM_WORLD);); (17, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (15, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(3, OpenMP, #pragma omp parallel for) (7, OpenMP, #pragma omp parallel for) (11, OpenMP, #pragma omp parallel for) (15, OpenMP, #pragma omp parallel for) (19, OpenMP, #pragma omp parallel for) (23, OpenMP, #pragma omp parallel for) (27, OpenMP, #pragma omp parallel for) (31, OpenMP, #pragma omp parallel for) (35, OpenMP, #pragma omp parallel for) (39, OpenMP, #pragma omp parallel for) (43, OpenMP, #pragma omp parallel for) (47, OpenMP, #pragma omp parallel for) (51, OpenMP, #pragma omp parallel for)", "label": "(25, OpenMP, #pragma omp target teams distribute parallel for simd collapse (1))); (60, OpenMP, #pragma omp target teams distribute parallel for simd collapse (2)); (4, OpenMP, #pragma omp target teams distribute parallel for simd collapse (); (54, OpenMP, #pragma omp target teams distribute parallel for simd collapse (N)); (36, OpenMP, #pragma omp target teams distribute parallel for simd collapse (S)); (24, OpenMP, #pragma omp target teams distribute parallel for simd collapse ((ST > 0) ? 1 + ST : 2)); (-4, OpenMP, #pragma omp target teams distribute parallel for simd collapse); (9, OpenMP, #pragma omp target teams distribute parallel for simd collapse (argc); (28, OpenMP, #pragma omp target teams distribute parallel for simd collapse (foobool(argc)), collapse (true), collapse (-5)); (12, OpenMP, #pragma omp target teams distribute parallel for simd collapse (ST); (43, OpenMP, #pragma omp target teams distribute parallel for simd collapse (1)); (-1, OpenMP, #pragma omp target teams distribute parallel for simd collapse ())"}
{"predict": "3:         clReleaseMemObject( memobjs[2] );\n104:         free( dst );\n105:         free( inptr );\n106:         return -1;\n107:     }\n108: \n109:     clFinish( queue );\n110: \n111:     err = clEnqueueReadBuffer(queue, memobjs[1], CL_TRUE, 0, num_bytes, dst, 0, NULL, &copyEvent);\n112:     if( err != CL_SUCCESS ){\n113:         log_error(\"clEnqueueReadBuffer failed\\n\");\n114:         clReleaseKernel( kernel[0] );\n115:         clReleaseProgram( program[0] );\n116:         clReleaseMemObject( memobjs[0] );\n117:         clReleaseMemObject( memobjs[1] );\n118:         clReleaseMemObject( memobjs[2] );\n119:         free( dst );\n120:         free( inptr );\n121:         return -1;\n122:     }\n123: \n124:     clFinish( queue );\n125: \n126:     free( inptr );\n127:     free( dst );\n128:     return 0;\n129: }", "label": "(85, OpenCL, clReleaseMemObject( memobjs[2] );); (140, OpenCL, clReleaseEvent(copyEvent);); (103, OpenCL, err = clEnqueueNDRangeKernel( queue, kernel[0], 2, NULL, threads, NULL, 0, NULL, NULL );); (72, OpenCL, err = clEnqueueWriteBuffer( queue, memobjs[1], true, 0, num_bytes, inptr, 0, NULL, NULL );); (87, OpenCL, err = clSetKernelArg( kernel[0], 0, sizeof( cl_mem ), (void *)&memobjs[1] );); (91, OpenCL, clReleaseProgram( program[0] );); (77, OpenCL, clReleaseMemObject( memobjs[0] );); (88, OpenCL, clReleaseKernel( kernel[0] );); (84, OpenCL, err |= clSetKernelArg( kernel[0], 1, sizeof( cl_mem ), (void *)&memobjs[0] );); (48, OpenCL, clReleaseMemObject(memobjs[0]);); (113, OpenCL, err = clEnqueueCopyImage( queue, memobjs[0], memobjs[2], srcPt, destPt, region, 0, NULL, &copyEvent );); (57, OpenCL, clReleaseMemObject(memobjs[1]);); (201, OpenCL, err = clEnqueueReadImage( queue, memobjs[2], true, origin, region2, 0, 0, dst, 0, NULL, NULL );); (125, OpenCL, err = clWaitForEvents( 1, &copyEvent );); (70, OpenCL, clReleaseMemObject( memobjs[1] );)"}
{"predict": "(9, CUDA, float milliseconds = 0;)\n(15, CUDA, cudaEventElapsedTime(&milliseconds, start, stop);)", "label": "(12, CUDA, cudaEventRecord(stop);); (12, CUDA, cudaEventSynchronize(stop);); (9, CUDA, cudaMemcpy(d_data, h_data, SIZE * sizeof(float), cudaMemcpyHostToDevice);); (7, CUDA, cudaEventRecord(start);); (-1, CUDA, cudaMalloc((void **)&d_data, SIZE * sizeof(float));); (10, CUDA, cudaEventElapsedTime(&milliseconds, start, stop);); (1, CUDA, cudaEventCreate(&stop);); (13, CUDA, cudaMemcpy(h_data, d_data, SIZE * sizeof(float), cudaMemcpyDeviceToHost);); (19, CUDA, cudaFree(d_data);); (-3, CUDA, cudaEventCreate(&start);)"}
{"predict": "(11, OpenMP, #pragma omp parallel for;)", "label": "(6, OpenMP, #pragma omp tile sizes(5)); (0, OpenMP, #pragma omp for)"}
{"predict": "time = reduce_time/iterations;\n134:     printf(\"Rate (MB/s): %lf Avg time (s): %lf\\n\", 1.0E-6*vector_length*Num_procs/avgtime, avgtime);\n135:   }\n136: \n137:   /* Free memory */\n138:   prk_free(vector);\n139: \n140:   MPI_Finalize();\n141:   return 0;\n142: }", "label": "(143, MPI, MPI_Finalize();); (67, MPI, MPI_Bcast(&iterations,    1, MPI_INT, root, MPI_COMM_WORLD);); (67, MPI, MPI_Bcast(&vector_length, 1, MPI_LONG, root, MPI_COMM_WORLD);); (19, MPI, MPI_Init(&argc,&argv);); (85, MPI, MPI_Barrier(MPI_COMM_WORLD);); (19, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (17, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(12, OpenMP, #pragma omp parallel for private(index, seed, i, lens, pin, pout))", "label": "(10, OpenMP, #pragma omp parallel for); (45, OpenMP, #pragma omp atomic)"}
{"predict": "(18, CUDA, int const* piv_ = piv + batch * pstride;)\n(20, CUDA, for (int ip = threadIdx.x; ip < N; ip += blockDim.x))", "label": "(20, CUDA, thrust::complex<T> const* m_ = m + batch * mstride;); (16, CUDA, tmp[t] = thrust::complex<T>(0.0);); (9, CUDA, __shared__ thrust::cuda_cub::core::uninitialized_array<thrust::complex<T>, 64> tmp;)"}
{"predict": "(27, CUDA, load_data_from_global_memory_to_shared_memory_vectorized<T, BLOCK_TILE_SIZE_X, BLOCK_TILE_SIZE_Y, BLOCK_TILE_SIZE_K, NUM_THREADS>(A, lda, B, ldb, A_thread_block_tile, B_thread_block_tile, thread_block_tile_idx, thread_linear_idx, m, n, k); __syncthreads(););", "label": "(19, CUDA, __shared__ T B_thread_block_tile[BLOCK_TILE_SIZE_K][BLOCK_TILE_SIZE_X];); (17, CUDA, __shared__ T A_thread_block_tile[BLOCK_TILE_SIZE_Y][BLOCK_TILE_SIZE_K];)"}
{"predict": "(39, MPI, MPI_Bcast(A_mat, nnz, MPI_DOUBLE, 0, MPI_COMM_WORLD);)\n(42, MPI, MPI_Bcast(Minv_mat, nnz, MPI_DOUBLE, 0, MPI_COMM_WORLD);)\n(48, MPI, MPI_Bcast(rhs, N_local, MPI_DOUBLE, 0, MPI_COMM_WORLD);)\n(50, MPI, MPI_Allreduce(pcg_solver.get_convergence_info(), convergence_info, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);)\n(53, MPI, MPI_Reduce(pcg_solver.get_convergence_info(), convergence_info, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);)", "label": "(109, MPI, MPI_Finalize();); (5, MPI, err = MPI_Init(&argc, &argv);); (8, MPI, err = MPI_Comm_size(MPI_COMM_WORLD, &numRanks);); (5, MPI, err = MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(44, CUDA, CHECK_CUDA(cudaMalloc((void **)&dX, size * sizeof(int32_t)));) (46, CUDA, CHECK_CUDA(cudaMemcpy(dX, x, size * sizeof(int32_t), cudaMemcpyHostToDevice));) (52, CUDA, CHECK_CUDA(cudaMemcpy(y, dY, size * sizeof(int32_t), cudaMemcpyDeviceToHost));) (64, CUDA, printf(\"check output tenosr... \");) (65, CUDA, for (int i = 0; i < size; ++i) {) (66, CUDA, if (y[i] != yref[i]) {) (67, CUDA, printf(\"FAILED\\n\");) (68, CUDA, exit(1);) (69, CUDA, } else {) (70, CUDA, printf(\"OK\\n\");) (71, CUDA, }) (72, CUDA, CHECK_HIEDNN(hiednnDestroyTensorDesc(xDesc));) (73, CUDA, CHECK_HIEDNN(hiednnDestroyTensorDesc(yDesc));) (74, CUDA, free(y);)", "label": "(29, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (72, CUDA, CHECK_CUDA(cudaFree(dX));); (43, CUDA, CHECK_CUDA(cudaMalloc(&dX, size * sizeof(int32_t)));); (29, CUDA, CHECK_HIEDNN(hiednnSetCudaStream(handle, 0));); (67, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (69, CUDA, CHECK_CUDA(cudaFree(dY));); (40, CUDA, CHECK_CUDA(cudaMalloc(&dY, size * sizeof(int32_t)));)"}
{"predict": "7:                 if (h_target_abs >= 0 && h_target_abs < H && w_target_abs >= 0 && w_target_abs < W) {\n108:                     int out_abs = out_batch_offset + o_abs * H * W + h_target_abs * W + w_target_abs;\n109:                     atomicAdd(&out[out_abs], thread_results[k * TO * TH + i * TH + j]);\n110:                 }\n111:             }\n112:         }\n113:     }\n114: }\n\nOutput:", "label": "(29, CUDA, __shared__ float x_s[x_s_sz];); (27, CUDA, __shared__ float w_s[w_s_sz];)"}
{"predict": "(16, OpenCL, auto acc = buf.get_access<mode::read_write>(cgh);); (18, OpenCL, auto scratch = cl::sycl::local_accessor<int, 1>{local_size, cgh};); (20, OpenCL, cl::sycl::nd_range<1>{global_size, local_size},); (21, OpenCL, [=](cl::sycl::nd_item<1> item) noexcept {); (22, OpenCL, const auto lid = item.get_local_id(0);); (23, OpenCL, const auto group_size = item.get_local_range(0);); (25, OpenCL, scratch[lid] = acc[item.get_global_id()];); (27, OpenCL, const auto load = scratch[(lid + 1) % group_size];); (29, OpenCL, scratch[lid] += load;); (31, OpenCL, if(lid == 0) acc[item.get_global_id()] = scratch[lid];);", "label": "(16, SYCL, using namespace cl::sycl::access;); (4, SYCL, cl::sycl::queue queue;)"}
{"predict": "(3, OpenMP, #pragma omp parallel for)\n(5, OpenMP, #pragma omp parallel for)\n(12, OpenMP, #pragma omp parallel for collapse(1))\n(24, OpenMP, #pragma omp parallel for collapse(1))", "label": "(36, OpenMP, #pragma omp distribute collapse (N)); (9, OpenMP, #pragma omp distribute collapse (argc); (36, OpenMP, #pragma omp distribute collapse (2)); (22, OpenMP, #pragma omp distribute collapse (S)); (2, OpenMP, #pragma omp distribute collapse ()); (-1, OpenMP, #pragma omp distribute collapse (); (9, OpenMP, #pragma omp distribute collapse (1))); (27, OpenMP, #pragma omp distribute collapse (1)); (9, OpenMP, #pragma omp distribute collapse ((ST > 0) ? 1 + ST : 2)); (-7, OpenMP, #pragma omp distribute collapse); (3, OpenMP, #pragma omp distribute collapse (ST); (12, OpenMP, #pragma omp distribute collapse (foobool(argc)), collapse (true), collapse (-5))"}
{"predict": "(14, MPI, MPI_Type_get_extent(type, &tmp_lb, NULL);)", "label": "(8, MPI, rc = MPI_Type_get_extent(type, &tmp_lb, &attr->child_type_extent);)"}
{"predict": "(12, OpenMP, #pragma omp parallel for reduction(+: sivar);)", "label": "(5, OpenMP, #pragma omp target teams distribute simd reduction(+: sivar))"}
{"predict": "ptr[m_loc];\n93: \n94:     /* Get the local portion of GA */\n95:     nzval_loc = (doublecomplex *) doublecomplexMalloc_dist(nnz_loc);\n96:     colind = (int_t *) intMalloc_dist(nnz_loc);\n97:     for (i = 0; i < n; ++i)\n98:       for (j = colptr[i]; j < colptr[i+1]; ++j) {\n99:         relpos = marker[rowind[j]];\n100:         nzval_loc[relpos].r = nzval[j].r;\n101:         nzval_loc[relpos].i = nzval[j].i;\n102:         colind[relpos] = rowind[j];\n103:         ++marker[rowind[j]];\n104:       }\n105: \n106:     /* Create local A in NR_loc format. */\n107:     zCreate_CompCol_Matrix_loc(&A, m_loc, n, nnz_loc, nzval_loc, colind, rowptr,\n108:                               SLU_NC, SLU_Z, SLU_GE);\n109: \n110:     /* Free memory. */\n111:     free(marker);\n112:     free(rowptr);\n113:     free(nzval_loc);\n114:     free(colind);\n115:     free(nzval);\n116:     free(rowind);\n117:     free(colptr);\n118: \n119:     /* Solve the linear system. */\n120:     zSolve(A, nrhs, rhs, ldb, x, ldx, grid);\n121: \n122:     /* Free memory. */\n123:     Destroy_SuperMatrix_Store(&GA);\n124:     Destroy_SuperMatrix_Store(&A);\n125: \n126:     return 0;\n127: }", "label": "(50, MPI, MPI_Bcast( nzval,   nnz, SuperLU_MPI_DOUBLE_COMPLEX, 0, grid->comm );); (38, MPI, MPI_Bcast( rowind, nnz, mpi_int_t,  0, grid->comm );); (50, MPI, MPI_Bcast( colptr,  n+1, mpi_int_t,  0, grid->comm );); (48, MPI, MPI_Bcast( rowind,  nnz, mpi_int_t,  0, grid->comm );); (39, MPI, MPI_Bcast( &m,   1,   mpi_int_t,  0, grid->comm );); (31, MPI, MPI_Bcast( &n,     1,   mpi_int_t,  0, grid->comm );); (29, MPI, MPI_Bcast( &m,     1,   mpi_int_t,  0, grid->comm );); (38, MPI, MPI_Bcast( &nnz, 1,   mpi_int_t,  0, grid->comm );); (32, MPI, MPI_Bcast( colptr, n+1, mpi_int_t,  0, grid->comm );); (28, MPI, MPI_Bcast( &nnz,   1,   mpi_int_t,  0, grid->comm );); (28, MPI, MPI_Bcast( nzval,  nnz, SuperLU_MPI_DOUBLE_COMPLEX, 0, grid->comm );); (33, MPI, MPI_Bcast( &n,   1,   mpi_int_t,  0, grid->comm );)"}
{"predict": "(11, MPI, MPI_Recv(buff, count, type, 0, tag, comm, &s);)", "label": "(22, MPI, MPI_Request_free(&r);); (14, MPI, MPI_Recv_init(buff, count, type, 0, tag, comm, &r);); (2, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(4, OpenMP, #pragma omp parallel for private(v0, r) reduction(+:v0))", "label": "(20, TBB, tbb::flow::make_edge( n, r[i] );); (54, TBB, tbb::flow::remove_edge( n, r[i] );); (1, TBB, tbb::flow::write_once_node<R> n(g);); (-1, TBB, tbb::flow::graph g;)"}
{"predict": "(28, OpenCL, auto compressedImage = castToObject<Image>(clCreateFromGLTexture(clContext.get(), 0, GL_TEXTURE_2D, 0, textureId, &retVal));); (55, OpenCL, retVal = clReleaseMemObject(compressedImage);)", "label": "(64, OpenCL, clReleaseMemObject(clCompressedImage);); (25, OpenCL, auto clCompressedImage = clCreateImage(context, CL_MEM_READ_WRITE, &imageFormat, &imageDescriptor, nullptr, &retVal);)"}
{"predict": "(3, CUDA, for (int row = blockIdx.x; row < row_dim; row += gridDim.x) {)\n(16, CUDA, } else {)", "label": "(7, CUDA, extern __shared__ float share_mem[];)"}
{"predict": "(18, OpenMP, #pragma omp parallel for private(t_y, t_yy, i_y, i_yy, dst_ptr, span))", "label": "(7, OpenMP, #pragma omp parallel for)"}
{"predict": "{\n109:                         std::make_shared<TensorProdObsT>(X0, Y1, Z2),\n110:                         std::make_shared<TensorProdObsT>(Z0, X1, Y2),\n111:                         std::make_shared<TensorProdObsT>(Y0, Z1, X2),\n112:                     });\n113: \n114:                 REQUIRE(ham1 == ham2);\n115:                 REQUIRE(ham1 != ham3);\n116:                 REQUIRE(ham1 != ham4);\n117:             }\n118:         }\n119:     }\n120: }\n\nOutput:", "label": "(24, CUDA, cudaSetDevice(deviceId);); (20, CUDA, cudaGetDeviceCount(&nDevices);); (6, MPI, MPIManager mpi_manager(MPI_COMM_WORLD);)"}
{"predict": "();\n87: \n88:     // Log\n89:     printf(\"BACKWARD:\\n\");\n90:     printMatrix(result_backward, c, m);\n91: \n92:     // Free\n93:     // CHECK: hipFree(d_fcLayer);\n94:     // CHECK: hipFree(d_softmaxData);\n95:     // CHECK: hipFree(d_gradData);\n96:     // CHECK: hipFree(d_diffData);\n97:     // CHECK: hipDestroyTensorDescriptor(srcTensorDesc);\n98:     // CHECK: hipDestroyTensorDescriptor(sftTensorDesc);\n99:     // CHECK: hipDestroyTensorDescriptor(diffTensorDesc);\n100:     // CHECK: hipDestroyHandle(handle);\n101:     cudaFree(d_fcLayer);\n102:     cudaFree(d_softmaxData);\n103:     cudaFree(d_gradData);\n104:     cudaFree(d_diffData);\n105:     cudnnDestroyTensorDescriptor(srcTensorDesc);\n106:     cudnnDestroyTensorDescriptor(sftTensorDesc);\n107:     cudnnDestroyTensorDescriptor(diffTensorDesc);\n108:     cudnnDestroy(handle);\n109: \n110:     free(fcLayer);\n111:     free(result);\n112:     free(result_backward);\n113:     free(diffData);\n114: \n115:     return 0;\n116: }\n\nOutput:", "label": "(73, CUDA, cudaMalloc((void**) &d_gradData, m * c * sizeof(double));); (116, CUDA, cudaFree(d_fcLayer);); (118, CUDA, cudaFree(d_diffData);); (116, CUDA, cudaFree(d_gradData);); (12, CUDA, cudaMalloc((void**) &d_fcLayer, m * c * sizeof(double));); (17, CUDA, cudaMalloc((void**) &d_softmaxData, m * c * sizeof(double));); (43, CUDA, cudaDeviceSynchronize();); (11, CUDA, cudaMemcpy(d_fcLayer, fcLayer, m * c * sizeof(double), cudaMemcpyHostToDevice);); (72, CUDA, cudaMalloc((void**) &d_diffData, m * c * sizeof(double));); (109, CUDA, cudaFree(d_softmaxData);); (83, CUDA, cudaMemcpy(result_backward, d_gradData, m * c * sizeof(double), cudaMemcpyDeviceToHost);); (44, CUDA, cudaMemcpy(result, d_softmaxData, m * c * sizeof(double), cudaMemcpyDeviceToHost);); (69, CUDA, cudaMemcpy(d_diffData, diffData, m * c * sizeof(double), cudaMemcpyHostToDevice);)"}
{"predict": "(13, OpenMP, #pragma omp parallel for private(index, K3, ddata, checksum, rckey))", "label": "(11, OpenMP, #pragma omp parallel for); (77, OpenMP, #pragma omp atomic)"}
{"predict": "0:             } else {\n101:                 off = fd->disp + (ADIO_Offset) (fd->etype_size) * fd->fp_ind;\n102:             }\n103:             ADIOI_WriteStridedCollInd(fd, (char *) buf, count, datatype, file_ptr_type,\n104:                                     off, status, error_code);\n105:         } else {\n106:             ADIOI_WriteStridedCollIndContigFiletype(fd, (char *) buf, count, datatype,\n107:                                                     file_ptr_type, offset, status,\n108:                                                     error_code);\n109:         }\n110:     } else {\n111:         /* use collective accesses */\n112:         ADIOI_Datatype_iscontig(datatype, &buftype_is_contig);\n113:         ADIOI_Datatype_iscontig(fd->filetype, &filetype_is_contig);\n114: \n115:         if (buftype_is_contig && filetype_is_contig) {\n116:             ADIOI_WriteStridedCollCollContig(fd, (char *) buf, count, datatype,\n117:                                             file_ptr_type, offset, status, error_code);\n118:         } else {\n119:             ADIOI_WriteStridedCollCollContigFiletype(fd, (char *) buf, count, datatype,\n119:                                                     file_ptr_type, offset, status,\n120:                                                     error_code);\n121:         }\n122:     }\n123: }", "label": "(173, MPI, MPI_Allreduce(&tmp_error, error_code, 1, MPI_INT, MPI_MAX, fd->comm);); (47, MPI, MPI_Comm_size(fd->comm, &nprocs);); (75, MPI, MPI_Allgather(&end_offset, 1, ADIO_OFFSET, end_offsets, 1, ADIO_OFFSET, fd->comm);); (73, MPI, MPI_Allgather(&start_offset, 1, ADIO_OFFSET, st_offsets, 1, ADIO_OFFSET, fd->comm);); (45, MPI, MPI_Comm_rank(fd->comm, &myrank);); (192, MPI, MPI_Type_size_x(datatype, &size);)"}
{"predict": "(11, MPI, MPI_Allreduce(&cvolRed, &cvolRed, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);)\n(12, MPI, MPI_Allreduce(&cvolFactor, &cvolFactor, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);)", "label": "(4, MPI, MPI_Reduce( &SCT->commVolFactor, &cvolFactor,  1, MPI_DOUBLE, MPI_SUM, 0, grid3d->comm );); (5, MPI, MPI_Reduce( &SCT->commVolRed, &cvolRed,  1, MPI_DOUBLE, MPI_SUM, 0, grid3d->comm );)"}
{"predict": ", output_cpu_data.data());\n88:     for (auto it = out_begin_h; it != out_end_h; ++it) { \n89:         std::cout << \"output data, global 3D index [\" << it.x() << \",\" << it.y() << \",\" << it.z() << \"], local index \" << it.i() << \", rank \" << rank << \" is (\" << it->real() << \",\" << it->imag() << \")\\n\";\n90:     }\n91:     \n92:     // Free resources\n93:     CUFFT_CHECK(cufftDestroy(plan));\n94:     CUFFT_CHECK(cufftXtFree(desc));\n95:     CUFFT_CHECK(cudaStreamDestroy(stream));\n96: }", "label": "(10, CUDA, CUDA_CHECK(cudaGetDeviceCount(&ndevices));); (54, CUDA, CUDA_CHECK(cudaStreamCreate(&stream));); (88, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (8, CUDA, CUDA_CHECK(cudaSetDevice(rank % ndevices));); (-2, MPI, MPI_Init(&argc, &argv);); (113, MPI, MPI_Finalize();); (1, MPI, MPI_Comm_size(mpi_comm, &size);); (103, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (78, CUDA, CUDA_CHECK(cudaGetLastError());); (-3, MPI, MPI_Comm_rank(mpi_comm, &rank);)"}
{"predict": "(11, TBB, g.run([&]{ParallelSearch(begin, mid);});) (12, TBB, g.run([&]{ParallelSearch(mid, end);});)", "label": "(5, TBB, if(tbb::is_current_task_group_canceling()) return;)"}
{"predict": "(10, OpenMP, #pragma omp parallel for)", "label": "(14, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(4, OpenMP, #pragma omp parallel for reduction(+:total_worktime) private(t1, t0))", "label": "(4, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (5, TBB, tbb::tick_count t1 = tbb::tick_count::now();)"}
{"predict": "(15, MPI, MPI_Scatter(sendbuf.data(), block_size, MPI_INT, recvbuf.data(), block_size, MPI_INT, 0, MPI_COMM_WORLD, &status);)", "label": "(11, MPI, MPI_Init(&argc, &argv);); (44, MPI, MPI_Finalize();); (10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (35, MPI, MPI_Recv(recvbuf.data(), block_size, MPI_INT, 0, rank, MPI_COMM_WORLD, &status);); (26, MPI, MPI_Send(&sendbuf[i*block_size], block_size, MPI_INT, i, i, MPI_COMM_WORLD);); (20, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(7, OpenMP, #pragma omp parallel for reduction(+:gFrstCost[:numForests], gFrstWt[:numForests]) private(i))", "label": "(23, MPI, MPI_Reduce(gFrstCost, gFrstCostAcc, numForests, MPI_DOUBLE, MPI_SUM, 0, grid3d->zscp.comm);)"}
{"predict": "(3, MPI, PCU_ALWAYS_ASSERT(argc == 4);)", "label": "(17, MPI, MPI_Finalize();); (2, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(3, CUDA, const int N_ = blockDim.x; const int H_ = blockDim.y; const int C_ = blockDim.z; const int T_ = gridDim.x; const int B_ = gridDim.y; const int _N_ = N_ * H_ * C_ * T_ * B_; const int _N_ = N_ * H_ * C_ * T_ * B_; const int _N_ = N_ * H_ * C_ * T_ * B_; const int _N_ = N_ * H_ * C_ * T_ * B_; const int _N_ = N_ * H_ * C_ * T_ * B_; const int _N_ = N_ * H_ * C_ * T_ * B_; const int _N_ = N_ * H_ * C_ * T_ * B_; const int _N_ = N_ * H_ * C_ * T_ * B_; const int _N_ = N_ * H_ * C_ * T_ * B_; const int _N_ = N_ * H_ * C_ * T_ * B_; const int _N_ = N_ * H_ * C_ * T_ * B_; const int _N_ = N_ * H_ * C_ * T_ * B_; const int _N_ = N_ * H_ * C_ * T_ * B_; const int _N_ = N_ * H_ * C_ * T_ * B_; const int _N_ = N_ * H_ * C_ * T_ * B_; const int _N_ = N_ * H_ * C_ * T_ * B_; const int _N_ = N_ * H_ * C_ * T_ * B_; const int _N_ = N_ * H_ * C_ * T_ * B_; const int _N_ = N_ * H_ * C_ * T_ * B_; const int _N_ = N_ * H_ * C_ * T_ * B_; const int _N_ = N_ * H_ * C_ * T_ * B_; const int _N_ = N_ * H_ * C_ * T_ * B_; const int _N_ = N_ * H_ * C_ * T_ * B_; const int _N_", "label": "(11, CUDA, __shared__ float u_[_N_];); (11, CUDA, __shared__ float r[_N_], k[_N_], v[_N_], w_[_N_], gy[_N_];)"}
{"predict": "(4, MPI, MPI_Init(&argc, &argv);) (48, MPI, MPI_Finalize();)", "label": "(12, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);)", "label": "(7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);)"}
{"predict": "(10, OpenMP, #pragma omp parallel for reduction(+:myCount))\n(17, OpenMP, #pragma omp parallel for reduction(+:etsSum, etsMax, etsMin, etsSigmaSq))\n(30, OpenMP, #pragma omp parallel for reduction(+:etsSigmaSq))", "label": "(3, TBB, tbb::enumerable_thread_specific<double> test_locals;); (13, TBB, REMARK(\"%d threads, work ratio %d per cent, time %g\", nThreads, csWorkRatio, (tbb::tick_count::now() - t0).seconds());); (0, TBB, tbb::task_scheduler_init init(tbb::task_scheduler_init::deferred);); (5, TBB, tbb::tick_count t0;); (6, TBB, t0 = tbb::tick_count::now();)"}
{"predict": "(33, CUDA, dim3 threadsPerBlock(BS); dim3 numBlocks(GS); kernel[t_size - 1]<<<numBlocks, threadsPerBlock, BS * t_size * sizeof(bool), stream>>>(num, num_classes, num_preds_per_class, top_k, nms_threshold, share_location, isNormalized, (T_BBOX*) bbox_data, (T_SCORE*) beforeNMS_scores, (int*) beforeNMS_index_array, (T_SCORE*) afterNMS_scores, (int*) afterNMS_index_array, flipXY, score_shift, caffeSemantics);)", "label": "(43, CUDA, CSC(cudaGetLastError(), STATUS_FAILURE);)"}
{"predict": "{\n149:             print_usage();\n150:         }\n151:         mfu_finalize();\n152:         return 0;\n153:     }\n154: \n155:     /* initialize mfu_walk_opts_t structure */\n156:     mfu_walk_opts_set_inputname(walk_opts, inputname);\n157:     mfu_walk_opts_set_preserve_times(preserve_times, walk_opts);\n158:     mfu_walk_opts_set_relative_targets(relative_targets, walk_opts);\n159:     mfu_walk_opts_set_progress_timeout(mfu_progress_timeout, walk_opts);\n160:     mfu_walk_opts_set_walk(walk, walk_opts);\n161:     mfu_walk_opts_set_paths(numpaths, paths, walk_opts);\n162: \n163:     /* free memory allocated for inputname and paths */\n164:     if (inputname) {\n165:         MFU_FREE(&inputname);\n166:     }\n167:     if (paths) {\n168:         MFU_FREE(&paths);\n169:     }\n170: \n171:     /* initialize MPI if we're walking */\n172:     if (walk) {\n173:         MPI_Init(&argc, &argv);\n174:         MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n175:         MPI_Comm_size(MPI_COMM_WORLD, &ranks);\n176:     }\n177: \n178:     /* if we're not walking, we need to read from a file */\n179:     if (!walk) {\n180:         /* read file contents */\n181:         mfu_file_read(mfu_file, oldpath);\n182:         /* write file contents */\n183:         mfu_file_write(mfu_file, newpath);\n184:     } else {\n185:         /* if we're walking, then walk the filesystem */\n186:         mfu_walk(walk_opts, oldpath, mfu_file);\n187:     }\n188: \n189:     /* free mfu_file object */\n190:     mfu_file_free(&mfu_file);\n191: \n192:     /* finalize MPI if we're walking */\n1", "label": "(2, MPI, MPI_Init(&argc, &argv);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (105, MPI, MPI_Finalize();); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(24, TBB, g.wait_for_all();)", "label": "(38, TBB, CHECK_MESSAGE( (tbb::flow::output_port<0>(outer_node).try_get(out) == output.try_get(out)), \"output from outer_node does not match output from graph\");); (13, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<1>(outer_node) == &tbb::flow::input_port<1>(inner_node1)), \"input port 1 of inner_node1 not bound to input port 1 in outer_node\");); (0, TBB, tbb::flow::composite_node<std::tuple<int, int>, std::tuple<int> > outer_node(g);); (8, TBB, outer_node.set_external_ports(base_type::input_ports_type(tbb::flow::input_port<0>(inner_node1), tbb::flow::input_port<1>(inner_node1)), base_type::output_ports_type(tbb::flow::output_port<0>(inner_node3)));); (-1, TBB, typedef tbb::flow::composite_node<std::tuple<int, int>, std::tuple<int> > base_type;); (18, TBB, tbb::flow::make_edge(inner_node2, tbb::flow::input_port<0>(inner_node3));); (18, TBB, tbb::flow::make_edge(inner_node2, tbb::flow::input_port<1>(inner_node3));); (13, TBB, tbb::flow::make_edge(inner_node1, tbb::flow::input_port<0>(inner_node2));); (-7, TBB, tbb::flow::graph g;); (6, TBB, CHECK_MESSAGE( (&tbb::flow::output_port<0>(outer_node) == &tbb::flow::output_port<0>(inner_node3)), \"output port 0 of inner_node3 not bound to output port 0 in outer_node\");); (11, TBB, tbb::flow::make_edge(inner_node1, tbb::flow::input_port<1>(inner_node2));); (2, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<0>(outer_node) == &tbb::flow::input_port<0>(inner_node1)), \"input port 0 of inner_node1 not bound to input port 0 in outer_node\");); (-8, TBB, tbb::flow::broadcast_node<int> input(g);); (-8, TBB, tbb::flow::queue_node<int> output(g);)"}
{"predict": "(15, OpenMP, #pragma omp parallel for)", "label": "(16, CUDA, CudaFile aCudaFile( aReader, GPUSPHOptions(), aTemplDir );)"}
{"predict": "(4, OpenMP, #pragma omp parallel for)\n(11, OpenMP, #pragma omp parallel for)\n(18, OpenMP, #pragma omp parallel for)\n(25, OpenMP, #pragma omp parallel for)", "label": "(29, OpenMP, #pragma omp target teams distribute simd safelen (foobool(argc)), safelen (true), safelen (-5)); (46, OpenMP, #pragma omp target teams distribute simd safelen(safelen(tmain<int, char, -1, -2>(argc, argv)); (-1, OpenMP, #pragma omp target teams distribute simd safelen); (19, OpenMP, #pragma omp target teams distribute simd safelen (foobool(1) > 0 ? 1 : 2)); (9, OpenMP, #pragma omp target teams distribute simd safelen (4); (0, OpenMP, #pragma omp target teams distribute simd safelen (); (27, OpenMP, #pragma omp target teams distribute simd safelen (S1)); (10, OpenMP, #pragma omp target teams distribute simd safelen (2+2))); (1, OpenMP, #pragma omp target teams distribute simd safelen ())"}
{"predict": "(12, OpenMP, #pragma omp parallel for reduction(+:entropy))", "label": "(11, OpenMP, #pragma omp parallel for reduction(+ : entropy))"}
{"predict": "(17, OpenMP, #pragma omp parallel for private(k, lk, i, ii, knsupc) reduction(+:lwork))", "label": "(28, MPI, MPI_Bcast( work, lwork, MPI_FLOAT, pkk, grid->comm );); (29, MPI, MPI_Bcast( work, diag_len[p], MPI_FLOAT, pkk, grid->comm );)"}
{"predict": "(12, MPI, MPI_Sendrecv_replace((void*)src, count, type, 0, LWGRP_MSG_TAG_0, 0, LWGRP_MSG_TAG_0, lwgrp_comm_self, status);)", "label": "(6, MPI, MPI_Comm_dup(MPI_COMM_SELF, &lwgrp_comm_self);)"}
{"predict": "(14, OpenMP, #pragma omp parallel for private(g, g1, sivar))", "label": "(4, OpenMP, #pragma omp target teams distribute parallel for private(g, g1, sivar))"}
{"predict": "cuBufferSizeBytes = cuBufferSize * sizeof(double);\n47: \n48: \t\t\tdataC = (cufftDoubleComplex*)malloc(cuBufferSizeBytes);\n49: \t\t\tif (cudaGetLastError() != cudaSuccess) {\n50: \t\t\t\tfprintf(stderr, \"Cuda error: Failed to allocate\\n\");\n51: \t\t\t\treturn;\n52: \t\t\t}\n53: \t\t\tmemcpy(dataC, inputC, cuBufferSizeBytes);\n54: \n55: \t\t\tdouble start = MPI_Wtime();\n56: \t\t\tcufftExecZ2Z(planZ2Z, dataC, dataC, CUFFT_FORWARD);\n57: \t\t\tdouble end = MPI_Wtime();\n58: \t\t\trun_time[r][0] = end - start;\n59: \n60: \t\t\tstart = MPI_Wtime();\n61: \t\t\tcufftExecZ2Z(planZ2Z, dataC, dataC, CUFFT_INVERSE);\n62: \t\t\tend = MPI_Wtime();\n63: \t\t\trun_time[r][1] = end - start;\n64: \n65: \t\t\tcufftDestroy(planZ2Z);\n66: \t\t\tfree(dataC);\n67: \t\t}\n68: \t\tbenchmark_result[0] += run_time[0][0] + run_time[0][1];\n69: \t\tbenchmark_result[1] += run_time[0][0] + run_time[0][1];\n70: \t}\n71: \tbenchmark_result[0] /= (double)num_benchmark_samples;\n72: \tbenchmark_result[1] /= (double)num_benchmark_samples;\n73: \tif (file_output) {\n74: \t\tfprintf(output, \"benchmark_result = %lf %lf\\n\", benchmark_result[0], benchmark_result[1]);\n75: \t}\n76: \tprintf(\"benchmark_result = %lf %lf\\n\", benchmark_result[0], benchmark_result[1]);\n77: \tfree(inputC);\n78: }\n\nOutput:", "label": "(27, CUDA, cudaMalloc((void**)&dataC, sizeof(cufftDoubleComplex) * dims[0] * dims[1] * dims[2]);); (55, CUDA, cudaDeviceSynchronize();); (27, CUDA, cudaMemcpy(dataC, inputC, sizeof(cufftDoubleComplex) * dims[0] * dims[1] * dims[2], cudaMemcpyHostToDevice);); (78, CUDA, cudaFree(dataC);); (1, CUDA, cudaSetDevice(device_id);)"}
{"predict": "(7, OpenMP, #pragma omp parallel for reduction(+:free))", "label": "(6, CUDA, cudaCheck(cudaMemGetInfo(&free, &total));)"}
{"predict": "(25, CUDA, curandStates = static_cast<curandState_t*>(cudaMalloc(batchSize * sizeof(curandState_t)));) (30, CUDA, generateRandomNumber<<<1, batchSize, 0, this->mStream->get()>>>(bufferCast<SizeType32>(*randValsDevice), batchSlotsPtr, curandStates, batchSize);)", "label": "(48, CUDA, cudaFree(curandStates);); (4, CUDA, cudaMalloc(&curandStates, sizeof(curandState_t) * 2 * batchSize);)"}
{"predict": "(16, MPI, MTest_Init(&argc, &argv);)", "label": "(29, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_ARE_FATAL);); (23, MPI, MPI_Error_string(ierr, str, &slen);); (14, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(4, TBB, tbb::parallel_for_each(image_vector.begin(), image_vector.end(), [&](ImagePtr& img) {", "label": "(40, TBB, tbb::flow::make_edge(src, gamma);); (40, TBB, tbb::flow::make_edge(gamma, tint);); (40, TBB, tbb::flow::make_edge(tint, write);); (0, TBB, tbb::flow::graph g;)"}
{"predict": "(11, CUDA, if (temp) { odata[my_idx-1] = temp; }})", "label": "(3, CUDA, __shared__ unsigned sidxs[nTPB];)"}
{"predict": "(7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);) (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &np);) (15, MPI, MPI_Allreduce(&mysize, &totalsize, 1, MPI_SIZE_T, MPI_SUM, MPI_COMM_WORLD);) (20, MPI, out.resize(totalsize);) (22, MPI, MPI_Scatterv(in.data(), counts.data(), displs.data(), MPI_CHAR, out.data(), mysizeint, MPI_CHAR, 0, MPI_COMM_WORLD);) (31, MPI, MPI_Gatherv(out.data(), mysizeint, MPI_CHAR, in.data(), counts.data(), displs.data(), MPI_CHAR, 0, MPI_COMM_WORLD);)", "label": "(19, MPI, MPI_Reduce(&mysize, &totalsize, 1, MPI_INT64_T, MPI_SUM, 0, MPI_COMM_WORLD);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&me);); (0, MPI, MPI_Init(&argc, &argv);); (23, MPI, MPI_Gather(&mysizeint, 1, MPI_INT, &(counts[0]), 1, MPI_INT, 0, MPI_COMM_WORLD);); (37, MPI, MPI_Finalize();); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD,&np);); (29, MPI, MPI_Gatherv(in.c_str(), mysizeint, MPI_CHAR, &(out[0]), &(counts[0]), &(displs[0]), MPI_CHAR, 0, MPI_COMM_WORLD);)"}
{"predict": "(5, OpenMP, #pragma omp order);", "label": "(18, OpenMP, #pragma omp teams distribute parallel for order(concurrent); (1, OpenMP, #pragma omp teams distribute parallel for order); (20, OpenMP, #pragma omp teams distribute parallel for order(concurrent)); (-2, OpenMP, #pragma omp target); (10, OpenMP, #pragma omp teams distribute parallel for order(none); (5, OpenMP, #pragma omp teams distribute parallel for order()"}
{"predict": "(11, CUDA, pillarScatterHalfkernel<<<blocks, threads, 0, stream>>>(pillar_features_data, coords_data, params_data, featureX, featureY, spatial_feature_data);)", "label": "(13, CUDA, fprintf(stderr, \"CUDA kernel failed : %s\\n\", cudaGetErrorString(err));); (10, CUDA, auto err = cudaGetLastError();)"}
{"predict": ": %s\",\n146: \t\t          options.target_enc == ENC_UTF8 ? \"UTF-8\" : \"Raw\",\n147: \t\t          enc_name(options.target_enc));\n148: \t}\n149: \n150: \t/* Process the input file */\n151: \tjohn_process_input_file();\n152: \n153: \tif (john_main_process && options.target_enc != ENC_RAW) {\n154: \t\tlog_event(\"- %s output encoding: %s\",\n155: \t\t          options.target_enc == ENC_UTF8 ? \"UTF-8\" : \"Raw\",\n156: \t\t          enc_name(options.target_enc));\n157: \t}\n158: \n159: \tjohn_cleanup();\n160: \tjohn_exit(0);\n161: }", "label": "(14, OpenMP, john_omp_fallback(argv);); (18, MPI, mpi_setup(argc, argv);); (78, OpenMP, john_omp_maybe_adjust_or_fallback(argv);); (6, OpenMP, john_omp_init();)"}
{"predict": "(24, MPI, startTime = MPI_Wtime();)", "label": "(44, MPI, MPI_Comm_split(MPI_COMM_WORLD, MPI_UNDEFINED, rank, &newcomm);); (21, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (40, MPI, MPI_Comm_free(&newcomm);); (21, MPI, startTime = MPI_Wtime();); (24, MPI, double rate = MPI_Wtime() - startTime;); (36, MPI, MPI_Comm_split(MPI_COMM_WORLD, 1, rank, &newcomm);)"}
{"predict": "(83, CUDA, dim3 threadsPerBlock(16, 16, 1); dim3 numBlocks((AGENT_COUNT + threadsPerBlock.x - 1) / threadsPerBlock.x, (AGENT_COUNT + threadsPerBlock.y - 1) / threadsPerBlock.y, 1);) (85, CUDA, auto instance = population[i].getThreadInstance(threadsPerBlock, numBlocks);)", "label": "(56, CUDA, flamegpu::CUDASimulation  cudaSimulation(model, argc, argv);)"}
{"predict": "(4, OpenMP, #pragma omp parallel for)", "label": "(36, TBB, typedef tbb::concurrent_hash_map<MyKey,MyData2,YourHashCompare> YourTable2;); (34, TBB, typedef tbb::concurrent_hash_map<MyKey,MyData2,YourHashCompare,MyAllocator> YourTable1;)"}
{"predict": "ft_pack(user_buf, users);\n95:             mpirc = MPI_File_read_at_all(fh, disp, user_buf, user_buf_size, MPI_CHAR, &status);\n96:             if (mpirc != MPI_SUCCESS) {\n97:                 MPI_Error_string(mpirc, mpierrstr, &mpierrlen);\n98:                 MFU_ABORT(1, \"Failed to read file: `%s' rc=%d %s\", name, mpirc, mpierrstr);\n99:             }\n100:             mfu_free(&user_buf);\n101:         }\n102:         /* broadcast data to other processes */\n103:         MPI_Bcast(users->buf, users->count, users->dt, 0, MPI_COMM_WORLD);\n104:         mfu_free(&user_buf);\n105:     }\n106: \n107:     /* read groups, if any */\n108:     if (groups->count > 0 && groups->chars > 0) {\n109:         /* create type */\n110:         mfu_flist_usrgrp_create_stridtype((int)groups->chars,  &(groups->dt));\n111: \n112:         /* get extent */\n113:         MPI_Aint lb_group, extent_group;\n114: \n115:         /* allocate memory to hold data */\n116:         size_t bufsize_group = groups->count * (size_t)extent_group;\n117:         groups->buf = (void*) MFU_MALLOC(bufsize_group);\n118:         groups->bufsize = bufsize_group;\n119: \n120:         /* set view to read data */\n121:         if (mpirc != MPI_SUCCESS) {\n122:             MPI_Error_string(mpirc, mpierrstr, &mpierrlen);\n123:             MFU_ABORT(1, \"Failed to set view on file: `%s' rc=%d %s\", name, mpirc, mpierrstr);\n124:         }\n125: \n126:         /* read data */\n127:         int group_buf_size = (int) buft_pack_size(groups);\n128:         if (rank == 0) {\n129:             char* group_buf = (char*) MFU_MALLOC", "label": "(28, MPI, MPI_Error_string(mpirc, mpierrstr, &mpierrlen);); (142, MPI, MPI_Bcast(groups->buf, (int)groups->count, groups->dt, 0, MPI_COMM_WORLD);); (96, MPI, mpirc = MPI_File_read_at(fh, 0, user_buf, user_buf_size, MPI_BYTE, &status);); (132, MPI, mpirc = MPI_File_read_at(fh, 0, group_buf, group_buf_size, MPI_BYTE, &status);); (102, MPI, MPI_Bcast(users->buf, (int)users->count, users->dt, 0, MPI_COMM_WORLD);); (15, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (28, MPI, mpirc = MPI_File_read_at(fh, 0, header_packed, header_size, MPI_BYTE, &status);); (41, MPI, MPI_Bcast(header, 6, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (60, MPI, MPI_Exscan(&count, &offset, 1, MPI_UINT64_T, MPI_SUM, MPI_COMM_WORLD);); (12, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (16, MPI, int mpirc = MPI_File_set_view(fh, disp, MPI_BYTE, MPI_BYTE, datarep, MPI_INFO_NULL);); (69, MPI, MPI_Type_get_extent(users->dt, &lb_user, &extent_user);); (105, MPI, MPI_Type_get_extent(groups->dt, &lb_group, &extent_group);); (190, MPI, mpirc = MPI_File_read_at(fh, read_offset, buf, read_size, MPI_BYTE, &status);); (74, MPI, mpirc = MPI_File_set_view(fh, disp, MPI_BYTE, MPI_BYTE, datarep, MPI_INFO_NULL);); (159, MPI, MPI_Allreduce(&iters, &all_iters, 1, MPI_UINT64_T, MPI_MAX, MPI_COMM_WORLD);)"}
{"predict": "(19, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank); MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);)", "label": "(22, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (22, MPI, MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);)"}
{"predict": "(5, OpenMP, #pragma omp parallel for)", "label": "(8, CUDA, CHECK(cudaFree(m_output_boxes_device));); (9, CUDA, CHECK(cudaFree(m_output_classes_device));); (2, CUDA, CHECK(cudaFree(m_input_resize_device));); (0, CUDA, CHECK(cudaFree(m_input_src_device));); (3, CUDA, CHECK(cudaFree(m_output_num_device));); (0, CUDA, CHECK(cudaFree(m_input_rgb_device));); (3, CUDA, CHECK(cudaFree(m_output_scores_device));)"}
{"predict": "_item_idx]) : T_SCORE(0);\n117:                 afterNMS_index_array[write_item_idx] = kept_bboxinfo_flag[cur_idx] ? loc_bboxIndex[t] : -1;\n118:             }\n119:         }\n120:     }\n121: }\n\nOutput:", "label": "(19, CUDA, extern __shared__ bool kept_bboxinfo_flag[];)"}
{"predict": "(11, MPI, MPI_Comm_rank(comm_world, &rank_world);)", "label": "(9, MPI, MPI_Comm_rank(comm_world, &rank_world);)"}
{"predict": "(14, MPI, MPI_Waitall(3, reqs, stats);) (26, MPI, MPI_Waitall(3, reqs, stats);)", "label": "(39, MPI, MPI_Request_free(&reqs[i]);)"}
{"predict": "(16, MPI, lsendcounts[i] = sendcounts[i];) (17, MPI, lsdispls[i] = sdispls[i];) (25, MPI, sendcounts[i] = lsendcounts[i];) (26, MPI, sdispls[i] = lsdispls[i];)", "label": "(11, MPI, MPI_Comm_size(comm, &npes);)"}
{"predict": "(11, CUDA, d_B = new float[DSIZE];)\n(12, CUDA, d_C = new float[DSIZE];)\n(16, CUDA, cudaMemcpy(d_A, h_A, DSIZE * sizeof(float), cudaMemcpyHostToDevice);)\n(22, CUDA, cudaMemcpy(h_C, d_C, DSIZE * sizeof(float), cudaMemcpyDeviceToHost);)", "label": "(15, CUDA, cudaMemcpy(d_A, h_A, DSIZE*sizeof(float), cudaMemcpyHostToDevice);); (20, CUDA, cudaCheckErrors(\"kernel launch failure\");); (8, CUDA, cudaMalloc(&d_A, DSIZE*sizeof(float));)"}
{"predict": "(7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &comm_size);) (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);) (12, Kokkos, Kokkos::initialize();) (25, MPI, MPI_Bcast(&base_size, 1, MPI_INT, 0, MPI_COMM_WORLD);) (26, MPI, MPI_Bcast(&blocks_per_rank, 1, MPI_INT, 0, MPI_COMM_WORLD);) (27, MPI, MPI_Bcast(&num_blocks, 1, MPI_INT, 0, MPI_COMM_WORLD);) (29, MPI, MPI_Gather(data_set.GetNumberOfDomains(), 1, MPI_INT, num_domains, 1, MPI_INT, 0, MPI_COMM_WORLD);) (32, MPI, if(rank == 0) { res->PrintSummary(std::cout); })", "label": "(35, MPI, MPI_Finalize();); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &comm_size);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (8, MPI, vtkh::SetMPICommHandle(MPI_Comm_c2f(MPI_COMM_WORLD));)"}
{"predict": "(18, CUDA, #pragma omp parallel for)", "label": "(19, CUDA, result = cudaMalloc((void**)&devPtrA, M*N*sizeof(*a));); (30, CUDA, cudaFree(devPtrA);)"}
{"predict": "(8, CUDA, for (int i = 0; i < 16; i++) {) (12, CUDA, if (pidx >= 992) {) (20, CUDA, if (pidx < 160) {) (22, CUDA, s_permutations[pidx] = permutation;) (25, CUDA, float error = evalPermutation4(colors, permutation, &start, &end, color_sum);) (27, CUDA, if (error < bestError) {) (30, CUDA, bestPermutation = permutation;) (31, CUDA, bestStart = start;) (32, CUDA, bestEnd = end;) (35, CUDA, __syncthreads();) (44, CUDA, for (int i = 0; i < 3; i++) {) (48, CUDA, if (pidx >= 160) {) (52, CUDA, ushort start, end;,) (54, CUDA, uint permutation = s_permutations[pidx];) (55, CUDA, float error = evalPermutation3(colors, permutation, &start, &end, color_sum);) (57, CUDA, if (error < bestError) {) (60, CUDA, bestPermutation = permutation;) (61, CUDA, bestStart = start;) (62, CUDA, bestEnd = end;) (64, CUDA, if (bestStart > bestEnd) {) (67, CUDA, bestPermutation ^= (~bestPermutation >> 1) & 0x55555555;) (70, CUDA, errors[idx] = bestError;)", "label": "(6, CUDA, __shared__ uint s_permutations[160];)"}
{"predict": "112: \n113:         /* Check stopping criterion. */\n114:         if (s <= eps * anorm || s == lstres) {\n115:             break;\n116:         }\n117:         lstres = s;\n118:         count++;\n119:         if (count > ITMAX) {\n120:             pxerr_dist(\"PSGSRFS3D\", grid, -10);\n121:             *info = -10;\n122:             return;\n123:         }\n124:     }\n125:     }\n126: \n127:     /* Release memory. */\n128:     SUPERLU_FREE(work);\n129: \n130: #if ( DEBUGlevel>=1 )\n131:     CHECK_MALLOC(iam, \"Exit psgsrfs3D()\");\n132: #endif\n133: \n134:     return;\n135: }", "label": "(110, MPI, MPI_Allreduce( &s, &berr[j], 1, MPI_FLOAT, MPI_MAX, grid->comm );); (111, MPI, MPI_Bcast( &berr[j], 1, MPI_FLOAT, 0,  grid3d->zscp.comm);)"}
{"predict": "(13, OpenMP, #pragma omp parallel for private(ii))", "label": "(62, TBB, tbb::internal::segmented_iterator<nested_vec, const T> my_csi(my_vec);); (19, TBB, tbb::internal::segmented_iterator<nested_vec, T> my_si(my_vec);)"}
{"predict": "(2, CUDA, __global__ void centroidKernel(const uchar4 *d_img, int *d_centroidCol, int *d_centroidRow, int *d_pixelCount, int width, int height))", "label": "(4, CUDA, __shared__ uint4 s_img[TPB];)"}
{"predict": "(11, MPI, *group = MPI_COMM_NULL; return;) (26, MPI, *group = MPI_COMM_NULL; return;) (46, MPI, MPI_Intercomm_merge(pgroup_new, 0 /* LOW */ , &pgroup);) (60, MPI, MPI_Recv(port, PORT_SIZE, MPI_CHAR, members[0], 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE); MPI_Intercomm_merge(pgroup_new, 1 /* HIGH */ , &pgroup); MPI_Comm_free(&pgroup_new); merged = 1;) (68, MPI, MPI_Intercomm_merge(pgroup_new, 0 /* HIGH */ , &pgroup); MPI_Comm_free(&pgroup_new); MPI_Comm_free(&pgroup_old);)", "label": "(49, MPI, MPI_Comm_accept(port, MPI_INFO_NULL, 0, pgroup, &pgroup_new);); (53, MPI, MPI_Comm_free(&pgroup_old);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (65, MPI, MPI_Comm_connect(port, MPI_INFO_NULL, 0, MPI_COMM_SELF, &pgroup_new);); (43, MPI, MPI_Send(port, PORT_SIZE, MPI_CHAR, members[i], 0, MPI_COMM_WORLD);); (36, MPI, MPI_Open_port(port_info, port);); (45, MPI, MPI_Comm_free(&pgroup_new);); (49, MPI, MPI_Close_port(port);); (67, MPI, MPI_Comm_connect(port, MPI_INFO_NULL, 0, pgroup, &pgroup_new);)"}
{"predict": "(11, MPI, MPI_Group_incl(worldGroup, groupSize, ranks.data(), &group);)", "label": "(10, MPI, MPICHECK(MPI_Group_size(group, &groupSize));); (6, MPI, MPICHECK(MPI_Comm_group(comm, &group));); (13, MPI, MPICHECK(MPI_Group_translate_ranks(group, groupSize, ranks.data(), worldGroup, worldRanks.data()));); (13, MPI, MPICHECK(MPI_Group_free(&group));); (2, MPI, MPICHECK(MPI_Comm_group(MPI_COMM_WORLD, &worldGroup));); (12, MPI, MPICHECK(MPI_Group_free(&worldGroup));)"}
{"predict": "(34, OpenMP, #pragma omp parallel for private(i))", "label": "(33, OpenMP, #pragma omp taskloop simd private(argv[1])); (38, OpenMP, #pragma omp taskloop simd private(h)); (13, OpenMP, #pragma omp taskloop simd private(argc); (52, OpenMP, #pragma omp parallel shared(i)); (52, OpenMP, #pragma omp parallel private(i)); (1, OpenMP, #pragma omp taskloop simd private); (15, OpenMP, #pragma omp taskloop simd private(argc > 0 ? argv[1] : argv[2])); (38, OpenMP, #pragma omp parallel); (41, OpenMP, #pragma omp taskloop simd private(i)); (21, OpenMP, #pragma omp taskloop simd private(a, b)); (32, OpenMP, #pragma omp taskloop simd shared(i)); (13, OpenMP, #pragma omp taskloop simd private(argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (15, OpenMP, #pragma omp taskloop simd private(S1)); (5, OpenMP, #pragma omp taskloop simd private(argc,); (-5, OpenMP, #pragma omp taskloop simd private(); (-3, OpenMP, #pragma omp taskloop simd private()); (20, OpenMP, #pragma omp taskloop simd private(e, g, z)); (40, OpenMP, #pragma omp taskloop simd private(j))"}
{"predict": "(14, OpenMP, #pragma omp parallel for)", "label": "(6, OpenMP, std::vector<BabelFlow::Payload> comp_outputs( 1 );)"}
{"predict": "(24, MPI, MPI_Init(NULL, NULL);)", "label": "(43, MPI, printf(\" MPI library version      : %s\\n\", mpi_version());)"}
{"predict": "(11, OpenMP, #pragma omp parallel for reduction(&:workload->success))", "label": "(15, CUDA, checkCudaErrors(cudaFree(workload->d_data));); (15, CUDA, checkCudaErrors(cudaFreeHost(workload->h_data));); (3, CUDA, checkCudaErrors(cudaSetDevice(workload->cudaDeviceID));); (14, CUDA, checkCudaErrors(cudaStreamDestroy(workload->stream));)"}
{"predict": "(13, MPI, MPI_Init(&argc, &argv);)", "label": "(9, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(12, CUDA, cudaMalloc((void**)&d_tmp_storage, tmp_storage_size);) (14, CUDA, cudaMemcpy(d_tmp_storage, h_tmp_storage, tmp_storage_size, cudaMemcpyHostToDevice);) (30, CUDA, cudaMemcpy(d_tmp_storage, h_tmp_storage, tmp_storage_size, cudaMemcpyHostToDevice);) (32, CUDA, cudaMemcpy(h_num_out, d_num_out, sizeof(int), cudaMemcpyDeviceToHost);)", "label": "(10, CUDA, int *d_num_out = thrust::raw_pointer_cast(num_out.data());); (10, CUDA, int *d_data = thrust::raw_pointer_cast(data.data());); (24, CUDA, d_tmp_storage = thrust::raw_pointer_cast(tmp_storage.data());); (32, CUDA, thrust::fill(flags.begin(), flags.begin() + num_selected, true);); (3, CUDA, thrust::device_vector<bool> flags(num_items);); (20, CUDA, thrust::device_vector<char> tmp_storage(tmp_storage_size);); (0, CUDA, thrust::device_vector<int> data(num_items);); (29, CUDA, thrust::fill(flags.begin() + num_selected, flags.end(), false);); (26, CUDA, thrust::sequence(data.begin(), data.end());); (28, CUDA, thrust::shuffle(flags.begin(), flags.end(), g);); (-5, CUDA, thrust::device_vector<int> num_out(1);); (36, CUDA, cudaMemcpy(&h_num_out, d_num_out, sizeof(int), cudaMemcpyDeviceToHost);); (0, CUDA, bool *d_flags = thrust::raw_pointer_cast(flags.data());)"}
{"predict": "(27, OpenCL, status = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &global, NULL, 0, NULL, NULL);)", "label": "(12, OpenCL, status = clSetKernelArg(kernel, 0, sizeof(cl_mem), &test_buf);); (20, OpenCL, status = clSetKernelArg(kernel, 0, sizeof(cl_mem), NULL);)"}
{"predict": "(60, CUDA, shfl_scan_test<<<gridSize,blockSize, shmem_sz>>>(d_data, 32, d_partial_sums);) (61, CUDA, shfl_scan_test<<<p_gridSize,p_blockSize, shmem_sz>>>(d_partial_sums,32);) (62, CUDA, uniform_add<<<gridSize-1, blockSize>>>(d_data+blockSize, d_partial_sums, n_elements);) (65, CUDA, checkCudaErrors(cudaMemcpy(h_partial_sums, d_partial_sums, partial_sz, cudaMemcpyDeviceToHost));)", "label": "(76, CUDA, checkCudaErrors(cudaEventRecord(stop, 0));); (15, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, cuda_device));); (94, CUDA, checkCudaErrors(cudaFree(d_partial_sums));); (57, CUDA, checkCudaErrors(cudaEventCreate(&start));); (57, CUDA, checkCudaErrors(cudaEventCreate(&stop));); (73, CUDA, checkCudaErrors(cudaEventElapsedTime(&inc, start, stop));); (66, CUDA, checkCudaErrors(cudaEventRecord(start, 0));); (70, CUDA, checkCudaErrors(cudaEventSynchronize(stop));); (86, CUDA, checkCudaErrors(cudaFreeHost(h_partial_sums));); (84, CUDA, checkCudaErrors(cudaFreeHost(h_result));); (55, CUDA, checkCudaErrors(cudaMalloc((void **)&d_data, sz));); (84, CUDA, checkCudaErrors(cudaFree(d_data));); (18, CUDA, checkCudaErrors(cudaMallocHost((void **)&h_result, sizeof(int)*n_elements));); (79, CUDA, checkCudaErrors(cudaFreeHost(h_data));); (56, CUDA, checkCudaErrors(cudaMemcpy(d_data, h_data, sz, cudaMemcpyHostToDevice));); (-4, CUDA, cuda_device = findCudaDevice(argc, (const char **)argv);); (65, CUDA, checkCudaErrors(cudaMemcpy(h_result, d_data, sz, cudaMemcpyDeviceToHost));); (50, CUDA, checkCudaErrors(cudaMemset(d_partial_sums, 0, partial_sz));); (11, CUDA, checkCudaErrors(cudaMallocHost((void **)&h_data, sizeof(int)*n_elements));); (-5, CUDA, checkCudaErrors(cudaGetDevice(&cuda_device));); (46, CUDA, checkCudaErrors(cudaMalloc((void **)&d_partial_sums, partial_sz));); (48, CUDA, checkCudaErrors(cudaMallocHost((void **)&h_partial_sums, partial_sz));)"}
{"predict": "(8, CUDA, thrust::sort(act_h.begin(), act_h.end(), stream);)", "label": "(6, CUDA, RAFT_CUDA_TRY(cudaStreamSynchronize(stream));)"}
{"predict": "(2, OpenMP, #pragma omp parallel for)", "label": "(1, OpenMP, #pragma omp target teams defaultmap); (2, OpenMP, #pragma omp target teams defaultmap(); (9, OpenMP, #pragma omp target teams defaultmap(tofrom)); (12, OpenMP, #pragma omp target teams defaultmap(tofrom,); (3, OpenMP, #pragma omp target teams defaultmap(tofrom); (4, OpenMP, #pragma omp target teams defaultmap (tofrom:); (13, OpenMP, #pragma omp target teams defaultmap(tofrom, scalar); (-2, OpenMP, #pragma omp target teams defaultmap()); (5, OpenMP, #pragma omp target teams defaultmap(tofrom scalar)); (8, OpenMP, #pragma omp target teams defaultmap (scalar:)"}
{"predict": "(11, MPI, result = mpi_world_communicator.MinAll(local);)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(28, MPI, MPI_Allgather(ranklist, ranks_comm, MPI_INT, ranklist, ranks_comm, MPI_INT, parent_comm);)", "label": "(19, MPI, MPI_Comm_size(d->comm, &ranks_comm);); (28, MPI, MPI_Comm_rank(parent_comm, &parent_rank);); (28, MPI, MPI_Allgather(&parent_rank, 1, MPI_INT, ranklist, 1, MPI_INT, d->comm);)"}
{"predict": "(34, CUDA, cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice); cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice);) (37, CUDA, cudaMemcpy(h_z, d_z, M, cudaMemcpyDeviceToHost);) (44, CUDA, cudaFree(d_x); cudaFree(d_y); cudaFree(d_z);)", "label": "(52, CUDA, CHECK(cudaFree(d_y));); (30, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice));); (51, CUDA, CHECK(cudaFree(d_z));); (39, CUDA, CHECK(cudaMemcpy(h_z, d_z, M, cudaMemcpyDeviceToHost));); (22, CUDA, CHECK(cudaMalloc((void**)&d_x, M));); (22, CUDA, CHECK(cudaMalloc((void**)&d_y, M));); (45, CUDA, CHECK(cudaFree(d_x));); (21, CUDA, CHECK(cudaMalloc((void**)&d_z, M));); (30, CUDA, CHECK(cudaGetLastError());); (30, CUDA, CHECK(cudaDeviceSynchronize());); (22, CUDA, CHECK(cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice));)"}
{"predict": "(37, MPI, MPI_Recv(lnzval, len2, MPI_DOUBLE, pc, jb, MPI_COMM_WORLD, &status);)", "label": "(32, MPI, MPI_Send(lnzval, len2, MPI_DOUBLE, 0, jb, grid3d->zscp.comm);); (36, MPI, MPI_Recv(lnzval, len2, MPI_DOUBLE, layer, jb, grid3d->zscp.comm, &status);)"}
{"predict": "\\\n105:     }                                                                          \\\n106:     return new_value;                                                          \\\n107:   }\n108: \n109: // ------------------------------------------------------------------------\n110: // Operation on *lhs, rhs using \"compare_and_store\" routine\n111: //     TYPE    - operands' type\n112: //     BITS    - size in bits, used to distinguish low level calls\n113: //     OP      - operator\n114: // Note: temp_val introduced in order to force the compiler to read\n115: //       *lhs only once (w/o it the compiler reads *lhs twice)\n116: #define OP_UPDATE_CMPXCHG_CPT(TYPE, BITS, OP)                                  \\\n117:   {                                                                            \\\n118:     TYPE KMP_ATOMIC_VOLATILE temp_val;                                         \\\n119:     TYPE old_value, new_value;                                                 \\\n120:     temp_val = *lhs;                                                           \\\n121:     old_value = temp_val;                                                      \\\n122:     new_value = (TYPE)(old_value OP rhs);                                      \\\n123:     while (!KMP_COMPARE_AND_STORE_ACQ##BITS(                                   \\\n124:         (kmp_int##BITS *)lhs, *VOLATILE_CAST(kmp_int##BITS *) & old_value,     \\\n125:         *VOLATILE_CAST(kmp_int##BITS *) & new_value)) {                        \\\n126:       temp_val = *lhs;                                                         \\\n127:       old_value = temp_val;                                                    \\\n128:       new_value = (TYPE)(old_value OP rhs);                                    \\\n129:     }                                                                          \\\n130:     *lhs = new_value;                                                          \\\n131:     return new_value;                                                          \\\n132:   }\n133: \n134: // ------------------------------------------------------------------------\n135: // Operation on *lhs, rhs using \"compare_and_store\" routine\n136: //     TYPE    - operands' type\n137: //     BITS    - size in bits, used to distinguish low level calls\n138: //     OP      - operator\n139: // Note: temp_val introduced in order to force the compiler to read\n140: //       *lhs only once (w/o it the compiler reads", "label": "(76, OpenMP, #define OP_GOMP_CRITICAL_CPT(TYPE, OP, FLAG))"}
{"predict": ") {\n97:             comp_mask |= i_oflag.second;\n98:         }\n99:     }\n100:     const int comp_mask_all = FLAG_S8S8_COMP | FLAG_ZP_COMP;\n101:     const int comp_mask_any = comp_mask & comp_mask_all;\n102:     assert(comp_mask_any != 0);\n103: \n104:     const auto comp_idx = prb->get_comp_idx(0);\n105:     const int64_t comp_nelems = prb->comp_nelems(0);\n106:     const int64_t comp_nelems_s8 = s8_comp.nelems();\n107:     const int64_t comp_nelems_zp = zp_comp.nelems();\n108:     const int64_t comp_nelems_max = MAX2(comp_nelems_s8, comp_nelems_zp);\n109:     const int64_t comp_nelems_min = MIN2(comp_nelems_s8, comp_nelems_zp);\n110:     const int64_t comp_nelems_sum = comp_nelems_s8 + comp_nelems_zp;\n111:     const int64_t comp_nelems_diff = comp_nelems_s8 - comp_nelems_zp;\n112:     const int64_t comp_nelems_avg = comp_nelems_sum / 2;\n113:     const int64_t comp_nelems_avg_rounded = ROUND_TO_NEAREST(comp_nelems_avg);\n114:     const int64_t comp_nelems_avg_floor = FLOOR(comp_nelems_avg);\n115:     const int64_t comp_nelems_avg_ceil = CEIL(comp_nelems_avg);\n116:     const int64_t comp_nelems_avg_trunc = TRUNC(comp_nelems_avg);\n117:     const int64_t comp_nelems_avg_mod = comp_nelems_avg % 2;\n118:     const int64_t comp_nelems_avg_div = comp_nelems_avg / 2;\n119:     const int64_t comp_nelems_avg_mul = comp_nelems_avg * 2;\n120:     const int64_t comp_nelems_avg_add = comp_nelems_avg + 2;\n121:     const int64_t comp_nelems_avg_sub =", "label": "(101, OpenMP, dims_t comp_dims(ndims, 1);)"}
{"predict": "(3, OpenMP, #pragma omp parallel for)\n(4, OpenMP, #pragma omp parallel for)\n(5, OpenMP, #pragma omp parallel for)\n(6, OpenMP, #pragma omp parallel for)\n(7, OpenMP, #pragma omp parallel for)\n(8, OpenMP, #pragma omp parallel for)\n(9, OpenMP, #pragma omp parallel for)\n(10, OpenMP, #pragma omp parallel for)\n(11, OpenMP, #pragma omp parallel for)\n(12, OpenMP, #pragma omp parallel for)\n(13, OpenMP, #pragma omp parallel for)\n(14, OpenMP, #pragma omp parallel for)", "label": "(24, OpenMP, #pragma omp target parallel device (3.14)); (9, OpenMP, #pragma omp target parallel device (argc))); (20, OpenMP, #pragma omp target parallel device (-10u)); (5, OpenMP, #pragma omp target parallel device (argc); (12, OpenMP, #pragma omp target parallel device (k), device (argc+1)); (-3, OpenMP, #pragma omp target parallel device); (6, OpenMP, #pragma omp target parallel device (argc > 0 ? argv[1] : argv[2])); (11, OpenMP, #pragma omp target parallel device (S1)); (12, OpenMP, #pragma omp target parallel device (-2)); (-3, OpenMP, #pragma omp target parallel device ()); (4, OpenMP, #pragma omp target parallel device (argc + argc)); (-7, OpenMP, #pragma omp target parallel device ()"}
{"predict": "(17, CUDA, __syncthreads();)", "label": "(1, CUDA, __shared__ float cache[threadsPerBlock];)"}
{"predict": "(12, CUDA, cudaMalloc(&dptrPosMass, 4 * p->N * sizeof(float))); (13, CUDA, cudaMalloc(&dptrForce, 3 * p->n * sizeof(float))); (14, CUDA, cudaMemcpyAsync(dptrPosMass, p->hostPosMass, 4 * p->N * sizeof(float), cudaMemcpyHostToDevice)); (25, CUDA, ComputeNBodyGravitation_multiGPU<<<300, 256, 256 * sizeof(float4)>>>(dptrForce, dptrPosMass, p->softeningSquared, p->i, p->n, p->N)); (26, CUDA, cudaMemset(dptrForce, 0, 3 * p->n * sizeof(float))); (27, CUDA, cudaMemcpy(p->hostForce + 3 * p->i, dptrForce, 3 * p->n * sizeof(float), cudaMemcpyDeviceToHost));", "label": "(33, CUDA, cudaFree( dptrPosMass );); (33, CUDA, cudaFree( dptrForce );)"}
{"predict": "(13, CUDA, nvtxRangePush(\"Init boundaries\");) (14, CUDA, initialize_boundaries<<<ny / 128 + 1, 128>>>(a, a_new, PI, 0, nx, ny, ny);) (15, CUDA, nvtxRangePop();) (29, CUDA, nvtxRangePush(\"Jacobi Solve\");) (31, CUDA, jacobi_kernel<<<dim_grid, dim_block>>>(a_new, a, l2_norm_d, iy_start, iy_end, nx);) (32, CUDA, CUDA_RT_CALL(cudaGetLastError());) (36, CUDA, CUDA_RT_CALL(cudaMemcpy(a_new, a_new + (iy_end - 1) * nx, nx * sizeof(float), cudaMemcpyDeviceToDevice));) (37, CUDA, CUDA_RT_CALL(cudaMemcpy(a_new + iy_end * nx, a_new + iy_start * nx, nx * sizeof(float), cudaMemcpyDeviceToDevice));) (40, CUDA, CUDA_RT_CALL(cudaDeviceSynchronize());) (43, CUDA, CUDA_RT_CALL(cudaDeviceSynchronize());) (44, CUDA, l2_norm = *l2_norm_h;) (45, CUDA, l2_norm = std::sqrt(l2_norm);) (47, CUDA, iter++;) (48, CUDA, if ((iter % 100) == 0) printf(\"%5d, %0.6f\\n\", iter, l2_norm);) (50, CUDA, std::swap(a_new, a);) (52, CUDA, nvtxRangePop();)", "label": "(63, OpenMP, double stop = omp_get_wtime();); (70, CUDA, CUDA_RT_CALL(cudaFree(a));); (18, CUDA, CUDA_RT_CALL(cudaDeviceSynchronize());); (10, CUDA, CUDA_RT_CALL(cudaMemset(a, 0, nx * ny * sizeof(float)));); (6, CUDA, CUDA_RT_CALL(cudaMalloc(&a, nx * ny * sizeof(float)));); (63, CUDA, CUDA_RT_CALL(cudaFree(l2_norm_d));); (13, CUDA, CUDA_RT_CALL(cudaGetLastError());); (32, CUDA, CUDA_RT_CALL(cudaMemset(l2_norm_d, 0, sizeof(float)));); (36, CUDA, CUDA_RT_CALL(cudaMemcpy(l2_norm_h, l2_norm_d, sizeof(float), cudaMemcpyDeviceToHost));); (5, CUDA, CUDA_RT_CALL(cudaMemset(a_new, 0, nx * ny * sizeof(float)));); (14, CUDA, CUDA_RT_CALL(cudaMallocHost(&l2_norm_h, sizeof(float)));); (54, CUDA, CUDA_RT_CALL(cudaMemcpy(a_ref_h, a, nx * ny * sizeof(float), cudaMemcpyDeviceToHost));); (58, CUDA, CUDA_RT_CALL(cudaFree(a_new));); (-2, CUDA, CUDA_RT_CALL(cudaMalloc(&a_new, nx * ny * sizeof(float)));); (22, OpenMP, double start = omp_get_wtime();); (8, CUDA, CUDA_RT_CALL(cudaMalloc(&l2_norm_d, sizeof(float)));); (51, CUDA, CUDA_RT_CALL(cudaFreeHost(l2_norm_h));)"}
{"predict": "(16, MPI, MPI_Comm_rank(comm, &rank);)", "label": "(20, MPI, MPI_Comm_size(comm, &size);); (64, MPI, MPI_Error_class(errval, &errclass);); (95, MPI, MPI_Ssend(NULL, 0, MPI_INT, dest, 10, comm);); (39, MPI, errval = MPI_Recv(NULL, 0, MPI_INT, src, 100, comm, MPI_STATUS_IGNORE);); (85, MPI, MPI_Ssend(NULL, 0, MPI_INT, dest, 100, comm);); (59, MPI, errval = MPI_Waitsome(2, r, &outcount, indices, s);); (87, MPI, MPI_Send(b1, 10, MPI_INT, dest, 0, comm);); (25, MPI, errval = MPI_Irecv(b2, 10, MPI_INT, src, 10, comm, &r[1]);); (44, MPI, errval = MPI_Recv(NULL, 0, MPI_INT, src, 10, comm, MPI_STATUS_IGNORE);); (16, MPI, MPI_Comm_set_errhandler(comm, MPI_ERRORS_RETURN);); (16, MPI, errval = MPI_Irecv(b1, 10, MPI_INT, src, 0, comm, &r[0]);); (8, MPI, MPI_Comm_rank(comm, &rank);); (82, MPI, MPI_Send(b2, 11, MPI_INT, dest, 10, comm);)"}
{"predict": "(4, OpenMP, #pragma omp parallel for reduction(+:sum)) (9, OpenMP, #pragma omp parallel for reduction(+:gsum))", "label": "(3, OpenMP, #pragma omp parallel for reduction(+:sum)); (8, OpenMP, #pragma omp target teams distribute parallel for reduction(+:gsum))"}
{"predict": "(12, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank); MPI_Comm_size(MPI_COMM_WORLD, &size);)", "label": "(35, MPI, MPI_Start(&reqs[root]);); (20, MPI, MPI_Type_vector(n, 1, stride, MPI_DOUBLE, &vec);); (11, MPI, MPI_Info_create(&info);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (32, MPI, MPI_Wait(&reqs[root], MPI_STATUS_IGNORE);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (48, MPI, MPI_Type_free(&vec);); (15, MPI, MPI_Type_commit(&vec);); (39, MPI, MPI_Request_free(&reqs[root]);); (44, MPI, MPI_Info_free(&info);)"}
{"predict": "Call a function when library is loaded, stub should return a success.\" );\n84:                     CHECK_TBB_IS_LOADED();\n85:                 }\n86:             }\n87:         }\n88: \n89:         {\n90:             SAY( \"Call a function with wrong arguments.\" );\n91:             CHECK_TBB_IS_LOADED();\n92:         }\n93: \n94:         {\n95:             SAY( \"Call a function with wrong arguments.\" );\n96:             CHECK_TBB_IS_LOADED();\n97:         }\n98:     } __TBB_CATCH ( tbb::runtime_loader::error_code c ) {\n99:         code = c;\n100:     }; // __TBB_TRY\n101: }\n\nOutput:\n(8, TBB, tbb::task_scheduler_init init;); (12, TBB, tbb::task_scheduler_init init;); (16, TBB, tbb::task_scheduler_init init;); (20, TBB, tbb::task_scheduler_init init;); (24, TBB, tbb::task_scheduler_init init;); (28, TBB, tbb::task_scheduler_init init;); (32, TBB, tbb::task_scheduler_init init;); (36, TBB, tbb::task_scheduler_init init;); (40, TBB, tbb::task_scheduler_init init;); (44, TBB, tbb::task_scheduler_init init;); (48, TBB, tbb::task_scheduler_init init;); (52, TBB, tbb::task_scheduler_init init;); (56, TBB, tbb::task_scheduler_init init;); (60, TBB, tbb::task_scheduler_init init;); (64, TBB, tbb::task_scheduler_init init;); (68, TBB, tbb::task_scheduler_init init;); (72, TBB, tbb::task_scheduler_init init;); (76, TBB, tbb::task_scheduler_init init;); (80, TBB, tbb::task_scheduler_init init;); (84, TBB, tbb::task_scheduler_init init;); (88, TBB, tbb::task_scheduler_init init;); (92, TBB, tbb::", "label": "(172, TBB, CHECK( rtl1.status() == tbb::runtime_loader::ec_bad_ver );); (97, TBB, tbb::task_scheduler_init init( 1 );); (136, TBB, CHECK_EXCEPTION( eid_invalid_multiple_scheduling, tbb::invalid_multiple_scheduling );); (40, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_no_lib );); (8, TBB, tbb::runtime_loader rtl( tbb::runtime_loader::em_status );); (158, TBB, tbb::runtime_loader rtl1( tbb::runtime_loader::em_status );); (158, TBB, CHECK( rtl0.status() == tbb::runtime_loader::ec_ok );); (126, TBB, CHECK_EXCEPTION( eid_bad_last_alloc,              tbb::bad_last_alloc              );); (154, TBB, tbb::runtime_loader rtl0( tbb::runtime_loader::em_status );); (130, TBB, CHECK_EXCEPTION( eid_improper_lock,               tbb::improper_lock               );); (113, TBB, tbb::internal::throw_exception( tbb::internal::id );); (84, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_bad_call );); (96, TBB, tbb::internal::throw_exception( tbb::internal::exception_id( id ) );); (39, TBB, tbb::runtime_loader rtl( tbb::runtime_loader::em_throw );); (133, TBB, tbb::internal::handle_perror( EAGAIN, \"apple\" );); (-1, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_ok );); (37, TBB, tbb::runtime_loader::error_code code = tbb::runtime_loader::ec_ok;); (8, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_bad_arg );); (147, TBB, CHECK( rtl1.status() == tbb::runtime_loader::ec_ok );); (46, TBB, CHECK( code == tbb::runtime_loader::ec_no_lib );); (117, TBB, CHECK_EXCEPTION( eid_missing_wait,                tbb::missing_wait                );); (121, TBB, CHECK_EXCEPTION( eid_user_abort,                  tbb::user_abort                  );); (37, TBB, CHECK( code == tbb::runtime_loader::ec_bad_arg );)"}
{"predict": "1+k1][i];\n99: \n100:                 x22_real = gty2_real[k][i12+k1][i];\n101:                 x22_imag = gty2_imag[k][i12+k1][i];\n102: \n103:                 gty1_real[k][i21+k1][i] = x12_real + x22_real;\n104:                 gty1_imag[k][i21+k1][i] = x12_imag + x22_imag;\n105: \n106:                 temp2_real = x12_real - x22_real;\n107:                 temp2_imag = x12_imag - x22_imag;\n108:                 gty1_real[k][i22+k1][i] = (uu2_real)*(temp2_real) - (uu2_imag)*(temp2_imag);\n109:                 gty1_imag[k][i22+k1][i] = (uu2_real)*(temp2_imag) + (uu2_imag)*(temp2_real);\n110:               }\n111:             }\n112:           }\n113:         }\n114:       }\n115:     }\n116:   }\n117: }\n\nOutput:\n(114, OpenMP, #pragma omp parallel for collapse(2))", "label": "(31, OpenMP, #pragma omp teams distribute collapse(2)); (47, OpenMP, #pragma omp parallel for collapse(2)); (34, OpenMP, #pragma omp parallel for)"}
{"predict": "(4, OpenMP, #pragma omp parallel for;)", "label": "(20, OpenMP, #pragma omp distribute parallel for proc_bind(master); (4, OpenMP, #pragma omp distribute parallel for proc_bind); (13, OpenMP, #pragma omp distribute parallel for proc_bind()); (7, OpenMP, #pragma omp distribute parallel for proc_bind(); (26, OpenMP, #pragma omp distribute parallel for proc_bind(x)); (38, OpenMP, #pragma omp distribute parallel for proc_bind(spread)); (19, OpenMP, #pragma omp distribute parallel for proc_bind(close), proc_bind(spread)); (-4, OpenMP, #pragma omp target); (32, OpenMP, #pragma omp parallel proc_bind(close)); (27, OpenMP, #pragma omp distribute parallel for proc_bind(master)); (-6, OpenMP, #pragma omp teams)"}
{"predict": "(17, MPI, MPI_Bcast(cindex, sizeof(scr_cache_index), MPI_BYTE, 0, scr_storedesc_cntl->comm);)", "label": "(33, MPI, MPI_Bcast(&rc, 1, MPI_INT, 0, scr_storedesc_cntl->comm);)"}
{"predict": "(26, OpenMP, #pragma omp parallel for)", "label": "(69, OpenMP, #pragma omp cancellation point parallel); (66, OpenMP, #pragma omp cancellation point taskgroup); (60, OpenMP, #pragma omp cancel for); (57, OpenMP, #pragma omp cancel); (64, OpenMP, #pragma omp cancel parallel); (50, OpenMP, #pragma omp target parallel copyin(pvt)); (-5, OpenMP, #pragma omp target parallel); (-4, OpenMP, #pragma omp target parallel (); (53, OpenMP, #pragma omp cancellation point); (57, OpenMP, #pragma omp cancel taskgroup); (-1, OpenMP, #pragma omp target parallel )); (0, OpenMP, #pragma omp target parallel }); (4, OpenMP, #pragma omp target parallel unknown()); (50, OpenMP, #pragma omp cancellation point for); (-9, OpenMP, #pragma omp target parallel [); (49, OpenMP, #pragma omp cancel sections); (49, OpenMP, #pragma omp cancellation point sections); (-10, OpenMP, #pragma omp target parallel ])"}
{"predict": "(11, CUDA, dim3 threadsPerBlock(16, 16, 1); dim3 numBlocks((numThreads + threadsPerBlock.x - 1) / threadsPerBlock.x, 1, 1); TestStreamCompact<<<numBlocks, threadsPerBlock>>>( \"streamCompact_odd\", streamCompact_odd<int, false>, 32, numThreads, fRatio );) (12, CUDA, dim3 threadsPerBlock(16, 16, 1); dim3 numBlocks((numThreads + threadsPerBlock.x - 1) / threadsPerBlock.x, 1, 1); TestStreamCompact<<<numBlocks, threadsPerBlock>>>( \"streamCompact_odd\", streamCompact_odd<int, false>, 1024, numThreads, fRatio );) (13, CUDA, dim3 threadsPerBlock(16, 16, 1); dim3 numBlocks((numThreads + threadsPerBlock.x - 1) / threadsPerBlock.x, 1, 1); TestStreamCompact<<<numBlocks, threadsPerBlock>>>( \"streamCompact_odd\", streamCompact_odd<int, false>, 1020, numThreads, fRatio );) (14, CUDA, dim3 threadsPerBlock(16, 16, 1); dim3 numBlocks((numThreads + threadsPerBlock.x - 1) / threadsPerBlock.x, 1, 1); TestStreamCompact<<<numBlocks, threadsPerBlock>>>( \"streamCompact_odd\", streamCompact_odd<int, false>, 16*1024*1024, numThreads, fRatio );) (15, CUDA, dim3 threadsPerBlock(16, 16, 1); dim3 numBlocks((numThreads + threadsPerBlock.x - 1) / threadsPerBlock.x, 1, 1); TestStreamCompact<<<numBlocks, threadsPerBlock>>>( \"streamCompact_odd\", streamCompact_odd<int, false>, 16*1024*1024 -10, numThreads, fRatio );) (22, CUDA, dim3 threadsPerBlock(16, 16, 1); dim3 numBlocks((numThreads + threadsPerBlock.x - 1) / threadsPerBlock.x, 1, 1); TestStreamCompact<<<numBlocks, threadsPerBlock>>>( \"streamCompact_odd", "label": "(9, CUDA, cudaGetDeviceProperties( &prop, 0 );); (5, CUDA, cudaSetDeviceFlags( cudaDeviceMapHost );); (3, CUDA, cudaSetDevice( 0 );)"}
{"predict": "(12, MPI, MPI_Scatter(in.data(), block_size/2, MPI_DOUBLE, result.data(), block_size/2, MPI_DOUBLE, 0, MPI_COMM_WORLD);)", "label": "(1, MPI, MPI_Init(&argc, &argv);); (61, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &P);)"}
{"predict": "(30, OpenMP, #pragma omp parallel for)", "label": "(43, CUDA, setCommonICUDataPointer(&U_ICUDATA_ENTRY_POINT, FALSE, pErrorCode);)"}
{"predict": "(25, TBB, tbb::parallel_for( tbb::blocked_range<std::size_t>(0, per_thread_iters * num_threads), [&](const tbb::blocked_range<std::size_t>& r) { trace[thread_id].emplace_back(r.begin(), r.end()); const bool is_uniform_split = r.size() == per_thread_iters; CHECK_MESSAGE(is_uniform_split, \"static partitioner split the range incorrectly.\"); std::this_thread::yield(); std::forward<PerBodyFunc>(body)(); }, tbb::static_partitioner() );)", "label": "(3, TBB, tbb::task_arena big_arena(static_cast<int>(num_threads));); (28, TBB, int thread_id = tbb::this_task_arena::current_thread_index();); (0, TBB, tbb::global_control concurrency(tbb::global_control::max_allowed_parallelism, num_threads);)"}
{"predict": "(3, OpenMP, #pragma omp parallel for)", "label": "(13, OpenMP, #pragma omp parallel for simd collapse (ST); (34, OpenMP, #pragma omp parallel for simd collapse (N)); (20, OpenMP, #pragma omp parallel for simd collapse (foobool(argc)), collapse (true), collapse (-5)); (-1, OpenMP, #pragma omp parallel for simd collapse); (13, OpenMP, #pragma omp parallel for simd collapse ((ST > 0) ? 1 + ST : 2)); (19, OpenMP, #pragma omp parallel for simd collapse (S)); (9, OpenMP, #pragma omp parallel for simd collapse (1)); (-3, OpenMP, #pragma omp parallel for simd collapse (); (-2, OpenMP, #pragma omp parallel for simd collapse ()); (1, OpenMP, #pragma omp parallel for simd collapse (argc); (27, OpenMP, #pragma omp parallel for simd collapse (2))"}
{"predict": "(25, OpenMP, #pragma omp parallel for reduction(+:sum_total) private(i)) (44, OpenMP, #pragma omp parallel for reduction(+:sum_total) private(i))", "label": "(61, TBB, tbb::flow::make_edge( a1, tbb::flow::input_port<1>(a3) );); (58, TBB, tbb::flow::remove_edge( a2, s2 );); (60, TBB, tbb::flow::make_edge( a3, s2 );); (0, TBB, tbb::flow::function_node<int,int> c(g, tbb::flow::unlimited, cube_body());); (19, TBB, CHECK_MESSAGE( (&std::get<1>(a3.input_ports()) == &tbb::flow::input_port<1>(a3.j)), \"input_port1 of j not bound to input port 1 in composite_node a3\");); (25, TBB, tbb::flow::make_edge( c, tbb::flow::input_port<1>(a0) );); (21, TBB, tbb::flow::queue_node<int> q(g);); (25, TBB, tbb::flow::make_edge( c, tbb::flow::input_port<0>(a1) );); (27, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(a0), tbb::flow::input_port<0>(a2) );); (20, TBB, tbb::flow::make_edge( s, tbb::flow::input_port<0>(a0) );); (16, TBB, tbb::flow::function_node<int,int> s2(g, tbb::flow::unlimited, square_body());); (22, TBB, tbb::flow::make_edge( c, tbb::flow::input_port<1>(a1) );); (24, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(a1), tbb::flow::input_port<1>(a2) );); (9, TBB, CHECK_MESSAGE( (&std::get<0>(a3.input_ports()) == &tbb::flow::input_port<0>(a3.j)), \"input_port 0 of j not bound to input port 0 in composite_node a3\");); (3, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<0>(a2) == &tbb::flow::input_port<0>(a2.j)), \"input_port 0 of j not bound to input port 0 in composite_node a2\");); (24, TBB, tbb::flow::make_edge( s2, q );); (-15, TBB, tbb::flow::graph g;); (-15, TBB, tbb::flow::function_node<int,int> s(g, tbb::flow::unlimited, square_body());); (0, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<1>(a2) == &tbb::flow::input_port<1>(a2.j)), \"input_port 1 of j not bound to input port 1 in composite_node a2\");); (-12, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<0>(a0) == &tbb::flow::input_port<0>(a0.j)), \"input_port 0 of j not bound to input port 0 in composite_node a0\");); (18, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(a2), s2 );); (-12, TBB, CHECK_MESSAGE( (&tbb::flow::output_port<0>(a0) == &a0.f), \"f not bound to output port 0 in composite_node a0\");); (-18, TBB, tbb::flow::function_node<int,int> p(g, tbb::flow::unlimited, passthru_body());); (35, TBB, tbb::flow::remove_edge(s2, q);); (-11, TBB, CHECK_MESSAGE( (&std::get<1>(a0.input_ports()) == &tbb::flow::input_port<1>(a0.j)), \"input_port1 of j not bound to input port 1 in composite_node a1\");); (-17, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<1>(a0) == &tbb::flow::input_port<1>(a0.j)), \"input_port 1 of j not bound to input port 1 in composite_node a0\");); (-14, TBB, CHECK_MESSAGE( (&std::get<0>(a0.input_ports()) == &tbb::flow::input_port<0>(a0.j)), \"input_port 0 of j not bound to input"}
{"predict": "(34, CUDA, for (unsigned int i = 0; i < AGENT_COUNT; i++) { __syncthreads(); } )", "label": "(29, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(6, TBB, parallel_for(blocked_range<int>(MinThread, MaxThread + 1), [&](const blocked_range<int>& r) { for( int p=r.begin(); p<r.end(); ++p ) { test_concurrency(p); } }));", "label": "(10, TBB, test_extract<tbb::flow::rejecting>();); (10, TBB, test_extract<tbb::flow::queueing>();)"}
{"predict": "(11, CUDA, cudaEventCreate(&start);) (12, CUDA, cudaEventCreate(&end);) (37, CUDA, square<<<gridSize, blockSize, dynamicSMemUsage>>>(array, arrayCount);) (39, CUDA, cudaEventRecord(start, 0);) (40, CUDA, cudaEventRecord(end, 0);) (41, CUDA, cudaEventSynchronize(end);) (42, CUDA, cudaEventElapsedTime(&elapsedTime, start, end);) (43, CUDA, cudaEventDestroy(start);) (44, CUDA, cudaEventDestroy(end);)", "label": "(13, CUDA, checkCudaErrors(cudaEventCreate(&start));); (13, CUDA, checkCudaErrors(cudaEventCreate(&end));); (41, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (38, CUDA, checkCudaErrors(cudaEventRecord(end));); (35, CUDA, checkCudaErrors(cudaEventRecord(start));); (50, CUDA, checkCudaErrors(cudaEventElapsedTime(&elapsedTime, start, end));)"}
{"predict": "=[[NESTED_IMPLICIT_TASK_ID]]\n58:   // THREADS: {{^}}[[THREAD_ID]]: ompt_event_parallel_end: parallel_id=[[NESTED_PARALLEL_ID]], task_id=[[IMPLICIT_TASK_ID]], invoker=[[PARALLEL_INVOKER]]\n59:   // THREADS: {{^}}[[THREAD_ID]]: fuzzy_address={{.*}}[[NESTED_RETURN_ADDRESS]]\n60:   // THREADS: {{^}}[[THREAD_ID]]: ompt_event_implicit_task_end: parallel_id={{[0-9]+}}, task_id=[[IMPLICIT_TASK_ID]]\n61:   // THREADS: {{^}}[[THREAD_ID]]: ompt_event_parallel_end: parallel_id=[[PARALLEL_ID]], task_id=[[PARENT_TASK_ID]], invoker=[[PARALLEL_INVOKER]]\n62:   // THREADS: {{^}}[[THREAD_ID]]: fuzzy_address={{.*}}[[RETURN_ADDRESS]]\n63: }\n\nAssistant:\n(4, OpenMP, #pragma omp parallel for)\n(10, OpenMP, #pragma omp for)\n(12, OpenMP, #pragma omp for)", "label": "(5, OpenMP, #pragma omp parallel num_threads(2)); (1, OpenMP, omp_set_nested(1);); (1, OpenMP, omp_set_max_active_levels(1);)"}
{"predict": "(54, CUDA, for (int i = 0; i < batch; i++) {)", "label": "(3, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (73, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (40, CUDA, EXPECT_EQ(cudaSuccess, cudaMemset(outAccess->sampleData(0), 0xFA, outBufferSize));); (104, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamDestroy(stream));)"}
{"predict": "(28, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate(i);)", "label": "(25, OpenMP, #pragma omp target teams distribute parallel for simd shared (argv[1])); (28, OpenMP, #pragma omp target teams distribute parallel for simd shared(ca)); (19, OpenMP, #pragma omp target teams distribute parallel for simd shared (S1)); (42, OpenMP, #pragma omp target teams distribute parallel for simd shared(j)); (29, OpenMP, #pragma omp target teams distribute parallel for simd shared(e, g)); (34, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate(i)); (11, OpenMP, #pragma omp target teams distribute parallel for simd shared (argc > 0 ? argv[1] : argv[2])); (30, OpenMP, #pragma omp target teams distribute parallel for simd private(i), shared(i)); (7, OpenMP, #pragma omp target teams distribute parallel for simd shared (argc,); (22, OpenMP, #pragma omp target teams distribute parallel for simd shared(da)); (3, OpenMP, #pragma omp target teams distribute parallel for simd shared (argc); (24, OpenMP, #pragma omp target teams distribute parallel for simd shared(h, B::x)); (31, OpenMP, #pragma omp target teams distribute parallel for simd shared(i)); (28, OpenMP, #pragma omp target teams distribute parallel for simd private(i)); (-5, OpenMP, #pragma omp target teams distribute parallel for simd shared (); (12, OpenMP, #pragma omp target teams distribute parallel for simd shared(ba)); (-9, OpenMP, #pragma omp target teams distribute parallel for simd shared); (-6, OpenMP, #pragma omp target teams distribute parallel for simd shared ()); (1, OpenMP, #pragma omp target teams distribute parallel for simd shared (argc)); (4, OpenMP, #pragma omp target teams distribute parallel for simd shared (a, b, c, d, f, z))"}
{"predict": "(27, CUDA, #pragma omp parallel for collapse(2))", "label": "(65, OpenMP, #pragma omp target update to(u1_real, u1_imag))"}
{"predict": "(3, CUDA, extern __shared__ float siftPoint[];) (15, CUDA, float sum = 0.0f;) (27, CUDA, maxScor2[ty] = maxScore[ty];)", "label": "(3, CUDA, __shared__ float maxScore[FMC2H];); (4, CUDA, __shared__ int maxIndex[FMC2H];); (2, CUDA, __shared__ float maxScor2[FMC2H];); (-1, CUDA, __shared__ float siftPoint[128*FMC2H];)"}
{"predict": "(7, MPI, MPI_Bcast(value, MPI_MAX_INFO_VAL + 1, MPI_CHAR, 0, MPI_COMM_WORLD);)", "label": "(25, MPI, MPI_Bcast(&tmp_val, 1, MPI_INT, 0, fd->comm);)"}
{"predict": "(8, OpenMP, #pragma omp parallel for)", "label": "(7, TBB, tbb::concurrent_unordered_set<int> static_init_time_set;)"}
{"predict": "(27, OpenMP, #pragma omp parallel for private(i, rel, ucol))", "label": "(56, OpenMP, #pragma omp simd)"}
{"predict": "123:       diff = fabs(gpu_result - cpu_result);\n124:     }\n125: \n126:     // check the result\n127:     ASSERT_LE(diff, threshold);\n128: \n129:     sdkDeleteTimer(&timer);\n130: \n131:     // free resources\n132:     free(h_idata);\n133:     free(h_odata);\n134:     cudaFree(d_idata);\n135:     cudaFree(d_odata);\n136:     return true;\n137: }", "label": "(132, CUDA, checkCudaErrors(cudaFree(d_idata));); (71, CUDA, checkCudaErrors(cudaMalloc((void **)&d_idata, bytes));); (131, CUDA, checkCudaErrors(cudaFree(d_odata));); (70, CUDA, checkCudaErrors(cudaMalloc((void **)&d_odata, numBlocks * sizeof(T)));)"}
{"predict": "reinterpret_cast<ptrdiff_t*>(&dia[0]);\n114:     auto f_col = reinterpret_cast<ptrdiff_t*>(&d_col[0]);\n115:     auto f_val = reinterpret_cast<double*>(&dia[0]);\n116:     d_ptr = std::vector<ptrdiff_t>(f_ptr, f_ptr + chunk + 1);\n117:     d_col = std::vector<ptrdiff_t>(f_col, f_col + chunk);\n118:     d_val = std::vector<double>(f_val, f_val + chunk * chunk);\n119:     d_ptr.back() = chunk * chunk;\n120: \n121:     // Create the distributed matrix for the RHS:\n122:     amgcl::mpi::distributed_matrix<DBackend> F(world,\n123:             amgcl::adapter::block_matrix<dmat_type>(\n124:                 std::tie(chunk * chunk, d_ptr, d_col, d_val)));\n125: \n126:     // Solve the system:\n127:     prof.tic(\"solve\");\n128:     auto x = Solver(prm, A, F);\n129:     prof.toc(\"solve\");\n130: \n131:     // Output the solution:\n132:     if (world.rank == 0) {\n133:         std::cout << \"Solver: \" << x << std::endl;\n134:         std::cout << \"Profiling: \" << prof << std::endl;\n135:     }\n136: \n137:     // Clean up:\n138:     amgcl::mpi::finish();\n139:     amgcl::mpi::finalize();\n140:     return 0;\n141: }", "label": "(17, MPI, MPI_Barrier(world);)"}
{"predict": "(26, OpenMP, #pragma omp parallel for private(j) shared(data, i, thread_sinks, ELS_PER_THREAD))", "label": "(11, OpenMP, #pragma omp parallel for num_threads(N_THREADS))"}
{"predict": "0:         bh.outputIds[bid * nBM + tid] = bh.outputIdsCBA[bid * nBM + smemRank[tid]];\n101:         bh.sequenceLengths[bid * nBM + tid] = bh.sequenceLengthsCBA[bid * nBM + smemRank[tid]];\n102:         bh.cumLogProbs[bid * nBM + tid] = bh.cumLogProbsCBA[bid * nBM + smemRank[tid]];\n103:         bh.logProbs[bid * nBM + tid] = bh.logProbsCBA[bid * nBM + smemRank[tid]];\n104:     }\n105: }\n\nOutput:\n(11, CUDA, int* smemRank = (int*) (smem);); (17, CUDA, float* smemScore = (float*) (smemRank + nBM);); (23, CUDA, smemScore[i] = bh.normedScoresCBA[bid * nBM * 2 + i];); (30, CUDA, int const warpid = tid / 32;); (31, CUDA, int const laneid = tid % 32;); (42, CUDA, rankNorm = swap(rankNorm, 0x02, bfe(laneid, 2) ^ bfe(laneid, 1));); (43, CUDA, rankNorm = swap(rankNorm, 0x01, bfe(laneid, 2) ^ bfe(laneid, 0));); (54, CUDA, rankNorm = swap(rankNorm, 0x04, bfe(laneid, 3) ^ bfe(laneid, 2));); (55, CUDA, rankNorm = swap(rankNorm, 0x02, bfe(laneid, 3) ^ bfe(laneid, 1));); (56, CUDA, rankNorm = swap(rankNorm, 0x01, bfe(laneid, 3) ^ bfe(laneid, 0));); (67, CUDA, if (tid < nBM) { smemRank[tid] = rankNorm.rank; }); (70, CUDA, for (int i = 0; i < nBM; ++i) { float maxScore = -FLT_MAX;", "label": "(15, CUDA, extern __shared__ char smem[];)"}
{"predict": "(27, OpenCL, retVal = clSetKernelExecInfo(multiDeviceKernel.get(), CL_KERNEL_EXEC_INFO_USM_PTRS_INTEL, sizeof(compressedDeviceMemAlloc2), &compressedDeviceMemAlloc2);)", "label": "(31, OpenCL, retVal = clEnqueueNDRangeKernel(pCmdQ, multiDeviceKernel.get(), 1, nullptr, globalWorkSize, nullptr, 0, nullptr, nullptr);); (20, OpenCL, retVal = clEnqueueMemcpyINTEL(pCmdQ, true, compressedDeviceMemAlloc1, bufferData, bufferSize, 0, nullptr, nullptr);); (14, OpenCL, retVal = clEnqueueMemcpyINTEL(pCmdQ, true, compressedDeviceMemAlloc2, bufferData, bufferSize, 0, nullptr, nullptr);); (37, OpenCL, retVal = clEnqueueMemcpyINTEL(pCmdQ, true, uncompressibleHostMemAlloc, compressedDeviceMemAlloc2, bufferSize, 0, nullptr, nullptr);)"}
{"predict": "Id64, agg_files);\n116:         MFU_LOG(MFU_LOG_INFO, \"  Links: %\" PRId64, agg_links);\n117:     }\n118: \n119:     return rc;\n120: }", "label": "(77, MPI, mfu_copy_stats.wtime_ended = MPI_Wtime();); (12, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (24, MPI, mfu_copy_stats.wtime_started = MPI_Wtime();); (89, MPI, MPI_Allreduce(values, sums, 3, MPI_INT64_T, MPI_SUM, MPI_COMM_WORLD);); (120, MPI, MPI_Allreduce(&rc, &all_rc, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);)"}
{"predict": ", IN_DATATYPE, OUT_DATATYPE, \\\n137:         BINARY_OP, IN1, IN2, OUT) \\\n138: UTEST_BINARY_ELEMENTWISE(TEST_NAME, HIE_BINARY_OP, BIAS, IN1, EXT_TYPE, IN_DATATYPE, OUT_DATATYPE, BINARY_OP, IN2, OUT)\n139: \n140: UTEST_BINARY_ELEMENTWISE_BROADCAST(Mod_S32_BROADCAST,\n141:                                   0,\n142:                                   HIEDNN_BINARY_MATH_MOD,\n143:                                   0,\n144:                                   EXT_MOD,\n145:                                   int32_t,\n146:                                   int32_t,\n147:                                   Mod<int32_t>(),\n148:                                   x,\n149:                                   &y);\n150: UTEST_BINARY_ELEMENTWISE_BROADCAST(Mod_F32_BROADCAST,\n151:                                   0,\n152:                                   HIEDNN_BINARY_MATH_MOD,\n153:                                   0,\n154:                                   EXT_FMOD,\n155:                                   float,\n156:                                   float,\n157:                                   fmod,\n158:                                   x,\n159:                                   &y);\n160: UTEST_BINARY_ELEMENTWISE_BROADCAST(Prelu_F32_BROADCAST,\n161:                                   0,\n162:                                   HIEDNN_BINARY_MATH_PRELU,\n163:                                   0,\n164:                                   EXT_PARAM_F,\n165:                                   float,\n166:                                   float,\n167:                                   [](float x, float y) { return x >= 0 ? x : x * y; },\n168:                                   x,\n169:                                   &y);\n170: UTEST_BINARY_ELEMENTWISE_BROADCAST(EQUAL_S32_BROADCAST,\n171:                                   0,\n172:                                   HIEDNN_BINARY_COMPARE_EQ,\n173:                                   0,\n174:                                   EXT_PARAM_F,\n175:                                   int32_t,\n176:                                   char,\n177:                                   [](int32_t x, int32_t y) { return x == y; },\n178:                                   x,\n179:                                   &y);\n180: UTEST_BINARY_ELEMENTWISE_BROADCAST(GREATER_S32_BROADCAST", "label": "(169, CUDA, cudaMemcpy(z_cu, z, shape.Z_len * sizeof(DT), cudaMemcpyHostToDevice);); (261, CUDA, cudaFree(x_cu);); (162, CUDA, cudaMalloc(&x_cu, shape.X_len * sizeof(ST));); (165, CUDA, cudaMemcpy(y_cu, y, shape.Y_len * sizeof(ST), cudaMemcpyHostToDevice);); (163, CUDA, cudaMemcpy(x_cu, x, shape.X_len * sizeof(ST), cudaMemcpyHostToDevice);); (258, CUDA, cudaFree(y_cu);); (160, CUDA, cudaMalloc(&z_cu, shape.Z_len * sizeof(DT));); (158, CUDA, cudaMalloc(&y_cu, shape.Y_len * sizeof(ST));); (168, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (260, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (254, CUDA, cudaFree(z_cu);)"}
{"predict": "(14, MPI, MPI_Comm_rank(comm, &rank);) (14, MPI, MPI_Comm_size(comm, &size);) (21, MPI, MPI_Scatter(sbuf, count, MPI_INT, rbuf, count, MPI_INT, 0, comm, MPI_STATUS_IGNORE);) (32, MPI, MPI_Isend(rbuf, count, MPI_INT, (rank + 1) % size, 0, comm, &req);) (33, MPI, MPI_Recv(rbuf, count, MPI_INT, (rank - 1 + size) % size, 0, comm, MPI_STATUS_IGNORE);)", "label": "(36, MPI, MPI_Start(&req);); (13, MPI, MPI_Comm_size(comm, &size);); (28, MPI, MPI_Info_create(&info);); (34, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (48, MPI, MPI_Info_free(&info);); (45, MPI, MPI_Request_free(&req);); (27, MPI, MPI_Alltoall_init(sbuf, count, MPI_INT, rbuf, count, MPI_INT, comm, info, &req);); (8, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(18, OpenMP, #pragma omp parallel for private(next_value, initialization_method))", "label": "(30, TBB, result_arena = new tbb::task_arena(tmp);); (18, TBB, result_arena = new tbb::task_arena( arena_args... );); (12, TBB, tbb::task_arena* result_arena = nullptr;); (26, TBB, tbb::task_arena tmp(dummy_max_concurrency, dummy_reserved_for_masters);); (19, TBB, result_arena = new tbb::task_arena;)"}
{"predict": "(9, OpenMP, #pragma omp parallel for private(i) reduction(+:a_ch_map))", "label": "(4, TBB, using ch_map = tbb::concurrent_hash_map<int,int>;)"}
{"predict": "(11, MPI, MPI_Aint true_lb; MPI_Aint true_extent; MPI_Aint lb; MPI_Aint extent; MPI_Get_address(obj_priv, &true_lb); MPI_Get_address(&obj_priv->attr_tree, &lb); MPI_Get_address(&obj_priv->attr_tree, &extent); true_extent = extent - true_lb; extent = MPI_UNDEFINED; MPI_Get_address(obj_priv, &extent); MPI_Get_address(obj_priv, &true_extent); obj->DTP_bufsize = (extent * obj->DTP_type_count) + true_extent - extent; if (true_lb > 0) { obj->DTP_bufsize += true_lb; obj->DTP_buf_offset = 0; } else { obj->DTP_buf_offset = -true_lb; } )", "label": "(32, MPI, rc = MPI_Type_get_true_extent(obj->DTP_datatype, &true_lb, &true_extent);); (22, MPI, rc = MPI_Type_commit(&obj->DTP_datatype);); (32, MPI, rc = MPI_Type_get_extent(obj->DTP_datatype, &lb, &extent);)"}
{"predict": "MTest_dtp_check(&recv, 0, 1, sendcnt, &send, errs < 10);\n124:     }\n125: \n126:     /* Allreduce to find the maximum error */\n127:     MPI_Allreduce(&errs, &errs, 1, MPI_INT, MPI_MAX, comm);\n128: \n129:     MTest_dtp_obj_end(&send);\n130:     MTest_dtp_obj_end(&recv);\n131: \n132:     DTP_pool_destroy(dtp);\n133: \n134:     return errs;\n135: }", "label": "(73, MPI, err = MPI_Wait(&req, &status);); (13, MPI, MPI_Comm_size(comm, &size);); (34, MPI, MPI_Comm_set_errhandler(comm, MPI_ERRORS_RETURN);); (10, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(26, OpenMP, #pragma omp parallel for)", "label": "(2, OpenMP, #pragma omp parallel); (4, OpenMP, #pragma omp parallel master taskloop)"}
{"predict": "6:     spath_delete(&base);\n127:   }\n128: \n129:   /* write out rank2file map */\n130:   if (scr_my_rank_world == 0) {\n131:     if (scr_write_rankfile(rankfile, filelist) != SCR_SUCCESS) {\n132:       scr_abort(-1, \"Failed to write rank2file map @ %s:%d\",\n133:         __FILE__, __LINE__\n134:       );\n135:     }\n136:   }\n137:   kvtree_delete(&filelist);\n138:   scr_free(&rankfile);\n139: \n140:   /* start transfer of files */\n141:   scr_flush_transfer_files(src_filelist, dst_filelist, numfiles, dataset_path);\n142: \n143:   /* free file lists */\n144:   scr_free(&src_filelist);\n145:   scr_free(&dst_filelist);\n146: \n147:   /* free dataset */\n148:   scr_dataset_delete(&dataset);\n149: \n150:   /* update the status of the transfer */\n151:   if (scr_my_rank_world == 0) {\n152:     /* end timer */\n153:     timestamp_end = scr_log_seconds();\n154:     kvtree_util_set_unsigned_long(dset_hash, ASYNC_KEY_OUT_TIME, (unsigned long)timestamp_end);\n155:     kvtree_util_set_double(dset_hash, ASYNC_KEY_OUT_WTIME, timestamp_end - timestamp_start);\n156: \n157:     /* log the end of the flush */\n158:     if (scr_log_enable) {\n159:       scr_log_event(\"ASYNC_FLUSH_END\", NULL, &id, dset_name,\n160:                     &timestamp_end, NULL);\n161:     }\n162:   }\n163: \n164:   /* make sure all processes make it this far before progressing */\n165:   MPI_Barrier(scr_comm_world);\n166: \n167:   /* check if flush has been canceled */\n168:   if (scr_flush_file_is_canceled(id)) {\n169:     if (", "label": "(24, MPI, MPI_Barrier(scr_comm_world);); (60, MPI, double time_end = MPI_Wtime();); (35, MPI, time_start = MPI_Wtime();)"}
{"predict": "(14, OpenMP, #pragma omp target data map(tofrom: i) use_device_ptr(z) use_device_addr(i,j,k[:i]));", "label": "(9, OpenMP, #pragma omp target data map(tofrom: i) use_device_ptr(k) use_device_addr(i, j)); (13, OpenMP, #pragma omp target data map(tofrom: i) use_device_ptr(z) use_device_addr(i, j, k[:i]))"}
{"predict": "(30, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(2, MPI, MPI_Init(&argc, &argv);); (86, MPI, MPI_Finalize();); (23, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&rank);); (61, MPI, MPI_Barrier(MPI_COMM_WORLD);); (59, MPI, MPI_Reduce(&counter, &totalNum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);)"}
{"predict": "(14, OpenCL, cl_int result = CL_SUCCESS; cl_mem mem[2] = {NULL, NULL};) (49, OpenCL, mem[0] = clCreateFromD3D11BufferKHR(context, 0, pBuffer, &result); TestRequire(result == CL_SUCCESS, \"clCreateFromD3D11BufferKHR\");) (55, OpenCL, mem[1] = clCreateFromD3D11Texture2DKHR(context, 0, pTexture, 1, &result); TestRequire(result == CL_SUCCESS, \"clCreateFromD3D11Texture2DKHR failed\");) (64, OpenCL, for (UINT i = 0; i < 2; ++i) { if (mem[i]) { clReleaseMemObject(mem[i]); } })", "label": "(67, OpenCL, clReleaseMemObject(mem[i]);)"}
{"predict": "a proper detection mechanism\n112:     if (Version == CudaVersion::UNKNOWN)\n113:       llvm::errs() << \"WARNING: Could not determine CUDA version for \"\n114:                   << InstallPath << \".\\n\";\n115: \n116:     if (Version >= CudaVersion::CUDA_75) {\n117:       return true;\n118:     }\n119:   }\n120:   return false;\n121: }", "label": "(123, CUDA, std::string GpuArchName(CudaArchToString(GpuArch));)"}
{"predict": "(27, CUDA, SleepSingleStream<<<taskCnt, blockSize>>>(src_host, tar_host, width, blockSize, taskCnt);) (34, CUDA, SleepSingleStream<<<taskCnt, blockSize>>>(src_host, tar_host, width, blockSize, taskCnt);) (43, CUDA, SleepMultiStream<<<taskCnt, blockSize>>>(src_host, tar_host, width, blockSize, taskCnt);)", "label": "(19, CUDA, cudaMallocHost(&tar_host, size * sizeof(float));); (17, CUDA, cudaMallocHost(&src_host, size * sizeof(float));)"}
{"predict": "(13, OpenMP, #pragma omp parallel for)", "label": "(63, OpenMP, #define OP_GOMP_CRITICAL_READ(OP, FLAG))"}
{"predict": "(3, OpenMP, #pragma omp parallel for) (6, OpenMP, #pragma omp parallel for) (9, OpenMP, #pragma omp parallel for) (12, OpenMP, #pragma omp parallel for) (15, OpenMP, #pragma omp parallel for) (18, OpenMP, #pragma omp parallel for) (21, OpenMP, #pragma omp parallel for) (24, OpenMP, #pragma omp parallel for) (27, OpenMP, #pragma omp parallel for) (30, OpenMP, #pragma omp parallel for)", "label": "(2, OpenMP, omp_autotune(self, OMP_SCALE);)"}
{"predict": "(6, OpenMP, #pragma omp parallel for private(j))", "label": "(3, OpenMP, #pragma omp parallel num_threads(2)); (12, OpenMP, #pragma omp ordered depend(source)); (7, OpenMP, #pragma omp ordered depend(sink : i - 1, j) depend(sink : i, j - 1)); (1, OpenMP, #pragma omp for ordered(2))"}
{"predict": "(26, OpenMP, #pragma omp for)", "label": "(3, OpenMP, #pragma omp parallel); (3, OpenMP, #pragma omp taskloop)"}
{"predict": "(5, MPI, MPI_Sendrecv(p->buff, p->bufflen, MPI_BYTE, p->prot.nbor, 1, p->buff1, p->bufflen, MPI_BYTE, p->prot.nbor, 1, MPI_COMM_WORLD, &status);)", "label": "(9, MPI, MPI_Wait(&request, &status);); (6, MPI, MPI_Irecv(p->buff1, p->bufflen, MPI_BYTE, p->prot.nbor, 1, MPI_COMM_WORLD, &request);); (6, MPI, MPI_Send(p->buff, p->bufflen, MPI_BYTE, p->prot.nbor, 1, MPI_COMM_WORLD);)"}
{"predict": "(8, CUDA, cudaMalloc(&d_a, DataSize);) (10, CUDA, cudaMemcpy(d_a, p_a, DataSize, cudaMemcpyHostToDevice);) (19, CUDA, cudaMemcpy(p_a, d_a, DataSize, cudaMemcpyDeviceToHost);) (20, CUDA, cudaFree(d_a);)", "label": "(16, OpenMP, omp_target_associate_ptr(p_a, d_a, DataSize, 0, omp_get_default_device());); (21, OpenMP, omp_target_free(d_a, omp_get_default_device());); (12, OpenMP, d_a = (int *)omp_target_alloc(DataSize, omp_get_default_device());); (17, OpenMP, omp_target_disassociate_ptr(p_a, omp_get_default_device());)"}
{"predict": "(3, MPI, int par_rank; MPI_Comm_rank(comm, &par_rank);) (5, MPI, int par_size; MPI_Comm_size(comm, &par_size);) (11, MPI, if(par_size > 4) { EXPECT_TRUE(false); }) (31, MPI, MPI_Barrier(comm);) (44, MPI, MPI_Gather(output.data(), output.size(), MPI_FLOAT, partials.data(), output.size(), MPI_FLOAT, 0, comm);) (47, MPI, if(par_rank == 0) { partials_to_png(partials, width, height, output_file); })", "label": "(50, OpenMP, EXPECT_TRUE(check_test_image(output_file, t_apcomp_baseline_dir()));); (9, MPI, MPI_Comm_rank(comm, &par_rank);); (10, MPI, apcomp::mpi_comm(MPI_Comm_c2f(comm));); (8, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(21, MPI, IF_VERBOSE((\"Post Init ...\\n\"));) (24, MPI, grequest = request; /* copy the handle */) (27, MPI, IF_VERBOSE((\"Waiting ...\\n\"));) (30, MPI, grequest = request; /* copy the handle */) (33, MPI, IF_VERBOSE((\"Waiting ...\\n\"));) (36, MPI, grequest = request; /* copy the handle */) (39, MPI, IF_VERBOSE((\"Waiting ...\\n\"));)", "label": "(27, MPI, MPI_Wait(&request, &status);); (40, MPI, MPI_Waitall(1, &request, &status);); (16, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (31, MPI, MPI_Waitsome(1, &request, &outcount, indices, &status);); (19, MPI, MPI_Grequest_start(query_fn, free_fn, cancel_fn, NULL, &request);)"}
{"predict": "(10, OpenMP, #pragma omp parallel for private(i) reduction(+:num_failed))", "label": "(6, OpenMP, omp_set_dynamic(0);)"}
{"predict": "(12, CUDA, dim3 grid_dim((nrow + xblock_dim - 1) / xblock_dim, (ncol + yblock_dim - 1) / yblock_dim, 1); kernel_tbt_mv_div<<<grid_dim, block_dim>>>(dim, nrow, ncol, reinterpret_cast<thrust::complex<double>*>(A), lda, reinterpret_cast<thrust::complex<double> const*>(x), incx);)", "label": "(14, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize());); (12, CUDA, qmc_cuda::cuda_check(cudaGetLastError());)"}
{"predict": "(11, TBB, for (int i = 0; i < Count; ++i ) {)", "label": "(6, TBB, tbb::flow::queue_node< q3_input_type >  q3(g);); (7, TBB, tbb::flow::make_edge( q3, my_split );)"}
{"predict": "f\\n\", ave_diff_subc);\n81:     EXPECT_EQ(ave_diff_perc <= 5e-1, true);\n82:     EXPECT_EQ(ave_diff_subc <= 5e-1, true);\n83:   }\n84: }\n\nOutput:\n(18, CUDA, __syncthreads();)\n(22, CUDA, __syncthreads();)\n(26, CUDA, __syncthreads();)\n(30, CUDA, __syncthreads();)\n(34, CUDA, __syncthreads();)\n(38, CUDA, __syncthreads();)\n(42, CUDA, __syncthreads();)\n(46, CUDA, __syncthreads();)\n(50, CUDA, __syncthreads();)\n(54, CUDA, __syncthreads();)\n(58, CUDA, __syncthreads();)\n(62, CUDA, __syncthreads();)\n(66, CUDA, __syncthreads();)\n(70, CUDA, __syncthreads();)\n(74, CUDA, __syncthreads();)\n(78, CUDA, __syncthreads();)\n(82, CUDA, __syncthreads();)", "label": "(3, CUDA, cudaGetDevice(&device_id);); (3, CUDA, cudaGetDeviceProperties(&device_prop, device_id);)"}
{"predict": "(11, OpenMP, #pragma omp atomic;)", "label": "(3, OpenMP, #pragma omp atomic)"}
{"predict": "(16, MPI, timers[t] = MPI_Wtime();)", "label": "(18, MPI, timers[t] = MPI_Wtime();); (22, MPI, timers[t] = (MPI_Wtime() - timers[t]) / cycles;); (11, MPI, MPI_Pack(sbuf, scount, sdt, packed_buf, outsize, &position, MPI_COMM_WORLD);); (2, MPI, MPI_Type_size(sdt, &outsize);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myself);)"}
{"predict": "CHECK:       [[TASK:%.+]] = call i8* @__kmpc_omp_task_alloc(%struct.ident_t* @1, i32 [[GTID:%.+]], i32 1, i[[SZ]] {{20|40}}, i[[SZ]] 4, i32 (i32, i8*)* bitcast (i32 (i32, %{{.+}}*)* [[TASK_ENTRY1:@.+]] to i32 (i32, i8*)*))\n57:   // CHECK:       [[BC_TASK:%.+]] = bitcast i8* [[TASK]] to [[TASK_TY1:%.+]]*\n58:   // CHECK:       getelementptr %struct.kmp_depend_info, %struct.kmp_depend_info* %{{.+}}, i[[SZ]] 0\n59:   // CHECK:       getelementptr %struct.kmp_depend_info, %struct.kmp_depend_info* %{{.+}}, i[[SZ]] 1\n60:   // CHECK:       getelementptr %struct.kmp_depend_info, %struct.kmp_depend_info* %{{.+}}, i[[SZ]] 2\n61:   // CHECK:       getelementptr %struct.kmp_depend_info, %struct.kmp_depend_info* %{{.+}}, i[[SZ]] 3\n62:   // CHECK:       [[DEP:%.+]] = bitcast %struct.kmp_depend_info* %{{.+}} to i8*\n63:   // CHECK:       call void @__kmpc_omp_wait_deps(%struct.ident_t* @1, i32 [[GTID]], i32 4, i8* [[DEP]], i32 0, i8* null)\n64:   // CHECK:       call void @__kmpc_omp_task_begin_if0(%struct.ident_t* @1, i32 [[GTID]], i8* [[TASK]])\n65:   // CHECK:       call i32 [[TASK_ENTRY1]](i32 [[GTID]], [[TASK_TY1]]* [[BC_TASK]])\n66:   // CHECK:       call void @__kmpc_omp_task_complete_if0(%struct.ident_t* @1, i32 [[GTID]], i8* [[TASK]])\n67:   // CHECK:       br label %[[", "label": "(86, OpenMP, #pragma omp target teams device(global + a) nowait depend(inout); (25, OpenMP, #pragma omp target teams device(global + a) depend(in); (100, OpenMP, #pragma omp target teams if (0) firstprivate(global) depend(out)"}
{"predict": "(12, CUDA, auto kernel_batched_dot_wabn_wban = launch_kernel(kernel_batched_dot_wabn_wban, grid_dim, DOT_BLOCK_SIZE, nbatch, nwalk, nocc, nchol, reinterpret_cast<thrust::complex<float> const*>(alpha), reinterpret_cast<thrust::complex<float> const*>(Tab), reinterpret_cast<thrust::complex<double>*>(y), incy);)", "label": "(16, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize(), \"batched_dot_wabn_wban\");); (14, CUDA, qmc_cuda::cuda_check(cudaGetLastError(), \"batched_dot_wabn_wban\");)"}
{"predict": "CPY_K\n77: \n78:   //\n79:   // Perform the MMA operation\n80:   //\n81: \n82:   // TUTORIAL: Example of MMA operation\n83: \n84:   for (int k = 0; k < size<2>(gA); ++k) {\n85:     Tensor gA_k = make_tensor(make_gmem_ptr(A), select<0,2>(shape_MNK), dA, k); // (M,K)\n86:     Tensor gB_k = make_tensor(make_gmem_ptr(B), select<1,2>(shape_MNK), dB, k); // (N,K)\n87:     Tensor gC_k = make_tensor(make_gmem_ptr(C), select<0,1>(shape_MNK), dC, k); // (M,N)\n88:     Tensor sA_k = make_tensor(make_smem_ptr(smemA), sA_layout, k);            // (BLK_M,BLK_K)\n89:     Tensor sB_k = make_tensor(make_smem_ptr(smemB), sB_layout, k);            // (BLK_N,BLK_K)\n90:     Tensor sC_k = make_tensor(make_smem_ptr(smemC), CSmemLayout{}, k);         // (BLK_M,BLK_N)\n91: \n92:     // Load A, B, C into shared memory\n93:     load(tAsA, gA_k);                                                        // Load A into shared memory\n94:     load(tBsB, gB_k);                                                        // Load B into shared memory\n95:     __syncthreads();\n96:     // Perform MMA operation\n97:     mma(sA_k, sB_k, sC_k);                                                   // MMA operation\n98:     __syncthreads();\n99:     // Store C from shared memory to global memory\n100:    store(tCsC, gC_k);                                                        // Store C from shared memory to global memory\n101:  }\n102:}\n\nOutput:", "label": "(48, CUDA, __shared__ TB smemB[cosize_v<BSmemLayout>];); (46, CUDA, __shared__ TA smemA[cosize_v<ASmemLayout>];)"}
{"predict": "(23, CUDA, sdkCreateTimer(&timerCUDA);) (46, CUDA, sdkStopTimer(&timerCUDA);) (75, CUDA, FreePlane(ImgF1);)", "label": "(13, CUDA, checkCudaErrors(cudaMallocPitch((void **)&src, &DeviceStride, Size.width * sizeof(float), Size.height));); (79, CUDA, checkCudaErrors(cudaFree(src));); (12, CUDA, checkCudaErrors(cudaMallocPitch((void **)&dst, &DeviceStride, Size.width * sizeof(float), Size.height));); (35, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (40, CUDA, getLastCudaError(\"Kernel execution failed\");); (74, CUDA, checkCudaErrors(cudaFree(dst));)"}
{"predict": "(15, OpenMP, #pragma omp parallel for)", "label": "(14, OpenMP, #pragma omp master taskloop shared(c) num_tasks(a))"}
{"predict": "(1, CUDA, __global__ void min_bmm_nn(\n2, CUDA, const float* __restrict__ A,\n3, CUDA, const float* __restrict__ B,\n4, CUDA, float* __restrict__ values,\n5, CUDA, ll_t* __restrict__ indices,\n6, CUDA, int M, int N, int K, int DIM\n7, CUDA, ){\n8, CUDA, int tid = threadIdx.x;     // thread idx\n9, CUDA, int bid = blockIdx.z;      // batch idx\n10, CUDA, \n11, CUDA, // Neighboring blocks are grouped into PN x PM block groups in order to increase\n12, CUDA, // L1 cache hit rate\n13, CUDA, // There are ceil(M/PM) x ceil(N/PN) block groups in total.\n14, CUDA, // Blocks within block groups are indexed with blockIdx.x % PN and blockIdx.x / PN\n15, CUDA, int px = blockIdx.x % _PN_;\n16, CUDA, int py = blockIdx.x / _PN_;\n17, CUDA, int bDimX = (N + (128*_PN_) - 1) / (128*_PN_); \n18, CUDA, int bDimY = (M + (128*_PM_) - 1) / (128*_PM_); \n19, CUDA, int bIdxX = (blockIdx.y % bDimX) * _PN_ + px;\n20, CUDA, int bIdxY = (blockIdx.y / bDimX) * _PM_ + py;\n21, CUDA, int gStartx = bIdxX * 128;   // starting index of block on N axis\n22, CUDA, int gStarty = bIdxY * 128;   // starting index of block on M axis\n23, CUDA, if (gStartx > N || gStarty > M){\n24, CUDA,   return;\n25, CUDA, }\n26, CUDA, // These are used to re-arrange threads into different shapes\n27, CUDA, // for example: (256) -> (16, 16) -> (8, 32) -> (32, 8)\n28, CUDA, int vx = tid % 16;\n2", "label": "(35, CUDA, __shared__ _VOLATILE_ float bSmem[16][128+4];); (33, CUDA, __shared__ _VOLATILE_ float aSmem[16][128+4];)"}
{"predict": "(4, OpenMP, for (int i = 0; i < 100; i++) { c[i] = a[i] + b[i]; })\n(10, OpenMP, for (int i = 0; i < 100; i++) { c[i] = a[i] + b[i]; })\n(16, OpenMP, for (int i = 0; i < 10; i++) { c[i] = a[i] + b[i]; })", "label": "(4, OpenMP, #pragma omp master taskloop simd); (1, OpenMP, #pragma omp parallel)"}
{"predict": "(28, CUDA, deviceA = cudaMalloc((void**)&deviceA, size); deviceB = cudaMalloc((void**)&deviceB, size); deviceC = cudaMalloc((void**)&deviceC, size);) (30, CUDA, cudaMemcpy(deviceA, hostA, size, cudaMemcpyHostToDevice); cudaMemcpy(deviceB, hostB, size, cudaMemcpyHostToDevice);) (33, CUDA, cudaMemcpy(deviceC, hostC, size, cudaMemcpyDeviceToHost);) (44, CUDA, cudaFree(deviceA); cudaFree(deviceB); cudaFree(deviceC);)", "label": "(28, CUDA, cudaMalloc((void**)&deviceC, size);); (55, CUDA, cudaFree(deviceB);); (40, CUDA, cudaMemcpy(hostC, deviceC, size, cudaMemcpyDeviceToHost);); (54, CUDA, cudaFree(deviceC);); (51, CUDA, cudaFree(deviceA);); (22, CUDA, cudaMalloc((void**)&deviceB, size);); (25, CUDA, cudaMemcpy(deviceA, hostA, size, cudaMemcpyHostToDevice);); (25, CUDA, cudaMemcpy(deviceB, hostB, size, cudaMemcpyHostToDevice);); (18, CUDA, cudaMalloc((void**)&deviceA, size);)"}
{"predict": "* sizeof(ADIO_Offset));\n104:         vars->ed_offsets = (ADIO_Offset *) ADIOI_Malloc(nprocs * sizeof(ADIO_Offset));\n105: \n106:         ADIOI_Allgather(&vars->start_offset, 1, MPIU_ADIO_OFFSET, vars->st_offsets,\n107:                        1, MPIU_ADIO_OFFSET, MPI_COMM_WORLD);\n108:         ADIOI_Allgather(&vars->end_offset, 1, MPIU_ADIO_OFFSET, vars->ed_offsets,\n109:                        1, MPIU_ADIO_OFFSET, MPI_COMM_WORLD);\n110: \n111:         /* Check if the read/write requests overlap with each other. If they do,\n112:          * the request is interleaved. If they don't, the request is contiguous.\n113:          * If the request is interleaved, the write request is handled differently\n114:          * to prevent race conditions.\n115:          */\n116:         if (vars->contig_access_count == 0) {\n117:             /* the request is interleaved */\n118:             ADIOI_Interleave_vars *interleave_vars = NULL;\n119:             interleave_vars =\n120:                 (ADIOI_Interleave_vars *) ADIOI_Calloc(1, sizeof(ADIOI_Interleave_vars));\n121:             vars->interleave_vars = interleave_vars;\n122: \n123:             /* calculate the start and end offsets of the file that each process\n124:              * will write to */\n125:             ADIOI_Calc_my_write_offsets(fd, vars->st_offsets, vars->ed_offsets,\n126:                                        &interleave_vars->start_offsets,\n127:                                        &interleave_vars->end_offsets);\n128: \n129:             /* calculate the start and end offsets of the buffer that each process\n130:              * will write to */\n131:             ADIOI_Calc_my_write_buf_offsets(vars->offset_list, vars->len_list,\n132:                                            interleave_vars->start_offsets,\n133:                                            interleave_vars->end_offsets,\n134:                                            &interleave_vars->buf_start_offsets,\n135:                                            &interleave_vars->buf_end", "label": "(77, MPI, MPI_Comm_size(fd->comm, &nprocs);); (77, MPI, MPI_Comm_rank(fd->comm, &myrank);)"}
{"predict": "(11, OpenMP, #pragma omp parallel for reduction(+:err))", "label": "(11, OpenMP, ref_sched = str2omp_sched(argv[2]);); (13, OpenMP, omp_get_schedule(&sched, &chunk);)"}
{"predict": "(12, CUDA, cudaMemcpyAsync(imageData, d_vbo_buffer, mesh_width * mesh_height * sizeof(float), cudaMemcpyDeviceToHost, stream);)", "label": "(7, CUDA, cudaDeviceSynchronize();); (7, CUDA, getLastCudaError(\"launch_kernel failed\");)"}
{"predict": "(12, OpenMP, #pragma omp parallel for private(bbp, t1, t2, timed) reduction(+:values))", "label": "(6, TBB, tbb::global_control global_limit(tbb::global_control::max_allowed_parallelism, idx);)"}
{"predict": "();\n77: \n78:         if (gc.x < shape.x && gc.y < shape.y && gc.z < shape.z)\n79:         {\n80:             labels[tc.z * BW * BH + tc.y * BW + tc.x] = FindRoot(labels, label);\n81:         }\n82: \n83:         __syncthreads();\n84:     }\n85: }\n\nOutput:\n(13, CUDA, __shared__ DT sharedData[BW][BH][BD];) (22, CUDA, sharedData[tc.z][tc.y][tc.x] = pzyx;) (23, CUDA, sharedData[tc.z][tc.y][tc.x] = pzym1x;) (24, CUDA, sharedData[tc.z][tc.y][tc.x] = pzm1yx;) (25, CUDA, sharedData[tc.z][tc.y][tc.x] = pzm1ym1x;) (26, CUDA, sharedData[tc.z][tc.y][tc.x] = pzyxm1;) (27, CUDA, sharedData[tc.z][tc.y][tc.x] = pzym1xm1;) (28, CUDA, sharedData[tc.z][tc.y][tc.x] = pzm1yxm1;) (29, CUDA, sharedData[tc.z][tc.y][tc.x] = nzm1yx;) (30, CUDA, sharedData[tc.z][tc.y][tc.x] = nzym1x;) (31, CUDA, sharedData[tc.z][tc.y][tc.x] = nzyxm1;) (32, CUDA, sharedData[tc.z][tc.y][tc.x] = nzym1xm1;) (33, CUDA, sharedData[tc.z][tc.y][tc.x] = nzm1yxm1;) (34, CUDA, sharedData[tc.z][tc.y][tc.x] = nzm1ym1x;) (35, CUDA, sharedData[tc.z][tc.y][tc.x] = label;) (65, CUDA, labels[tc.z * BW * BH + tc.y * BW + tc.x] = sharedData[tc.z][tc.y][tc.x];) (72, CUDA, labels[tc.z * BW * BH + tc.y * BW + tc.x", "label": "(5, CUDA, __shared__ DT labels[BW * BH * BD];)"}
{"predict": "(4, CUDA, ctx->rx_batch_size = batch_size;)\n(5, CUDA, ctx->d_scratchpads_size = batch_size * (ctx->algorithm.l3() + 64);)", "label": "(9, CUDA, CUDA_CHECK(ctx->device_id, cudaMalloc(&ctx->d_rx_dataset, dataset_size));); (15, CUDA, CUDA_CHECK(ctx->device_id, cudaMalloc(&ctx->d_rx_vm_states, batch_size * 2560));); (4, CUDA, CUDA_CHECK(ctx->device_id, cudaHostGetDevicePointer(&ctx->d_rx_dataset, const_cast<void *>(dataset), 0));); (7, CUDA, CUDA_CHECK(ctx->device_id, cudaMemcpy(ctx->d_rx_dataset, dataset, dataset_size, cudaMemcpyHostToDevice));); (11, CUDA, CUDA_CHECK(ctx->device_id, cudaMalloc(&ctx->d_rx_entropy, batch_size * (128 + 2560)));); (12, CUDA, CUDA_CHECK(ctx->device_id, cudaMalloc(&ctx->d_rx_rounding, batch_size * sizeof(uint32_t)));); (8, CUDA, CUDA_CHECK(ctx->device_id, cudaMalloc(&ctx->d_rx_hashes, batch_size * 64));); (6, CUDA, CUDA_CHECK(ctx->device_id, cudaMalloc(&ctx->d_long_state, ctx->d_scratchpads_size));)"}
{"predict": "(31, CUDA, cudaMalloc((void**)&res_dev, sizeof(float));) (32, CUDA, cudaMemcpy(res_dev, &res, sizeof(float), cudaMemcpyHostToDevice);) (33, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);) (34, CUDA, cudaFree(res_dev);)", "label": "(39, CUDA, cudaFree(res_dev);); (32, CUDA, cudaMalloc(&res_dev, sizeof(float));); (34, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(14, MPI, MPI_Probe(MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &status);)", "label": "(16, MPI, MPI_Error_string(err, msg, &len);); (8, MPI, err = MPI_Probe(-80, 1, MPI_COMM_WORLD, &status);); (5, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(4, OpenMP, #pragma omp parallel for reduction(+:conv) private(i, j, tmp))\n(52, OpenMP, #pragma omp parallel for private(j, tmp))\n(76, OpenMP, #pragma omp parallel for reduction(+:conv) private(i, tmp))\n(104, OpenMP, #pragma omp parallel for reduction(+:err, chksum) private(i, tmp))", "label": "(69, OpenMP, #pragma omp target map(tofrom : conv)); (87, OpenMP, elapsed_time = omp_get_wtime() - start_time;); (53, OpenMP, #pragma omp teams distribute parallel for simd); (83, OpenMP, #pragma omp target exit data map(from : xold[0 : Ndim], xnew[0 : Ndim])); (37, OpenMP, start_time = omp_get_wtime();); (43, OpenMP, #pragma omp target enter data map(to : xold[0 : Ndim], xnew[0 : Ndim],); (48, OpenMP, #pragma omp target); (63, OpenMP, #pragma omp teams distribute parallel for simd reduction(+ : conv))"}
{"predict": "(13, CUDA, dim3 grid((M + BM - 1) / BM, (N + BN - 1) / BN); dim3 block(size(tA));)", "label": "(16, CUDA, CUDA_CHECK(cudaGetLastError());)"}
{"predict": "ensors);\n132: \n133:   // Create input/output tensors.\n134:   err = cudlaCreateTensors(devHandle, numInputTensors, inputTensorDesc,\n135:                            &inputTensors);\n136:   if (err != cudlaSuccess) {\n137:     DPRINTF(\"Error in cudlaCreateTensors = %d\\n\", err);\n138:     cleanUp(&resourceList);\n139:     return 1;\n140:   }\n141:   err = cudlaCreateTensors(devHandle, numOutputTensors, outputTensorDesc,\n142:                            &outputTensors);\n143:   if (err != cudlaSuccess) {\n144:     DPRINTF(\"Error in cudlaCreateTensors = %d\\n\", err);\n145:     cleanUp(&resourceList);\n146:     return 1;\n147:   }\n148: \n149:   resourceList.inputTensors = inputTensors;\n150:   resourceList.outputTensors = outputTensors;\n151: \n152:   // Set tensor attributes.\n153:   for (int i = 0; i < numInputTensors; i++) {\n154:     cudlaTensorAttribute inputAttribute;\n155:     err = cudlaTensorGetAttributes(inputTensors[i], &inputAttribute);\n156:     if (err != cudlaSuccess) {\n157:       DPRINTF(\"Error in getting inputTensorAttribute = %d\\n\", err);\n158:       cleanUp(&resourceList);\n159:       return 1;\n160:     }\n161:     err = cudlaSetTensorAttributes(inputTensors[i], &inputAttribute);\n162:     if (err != cudlaSuccess) {\n163:       DPRINTF(\"Error in setting inputTensorAttribute = %d\\n\", err);\n164:       cleanUp(&resourceList);\n165:       return 1;\n166:     }\n167:   }\n168: \n169:   for (int i = 0; i < numOutputTensors; i++) {\n170:     cudlaTensorAttribute outputAttribute;\n171:     err =", "label": "(55, CUDA, result = cudaFree(0);); (56, CUDA, errPtr = cudaGetErrorName(result);); (320, CUDA, cudaFree(outputBufferGPU);); (318, CUDA, cudaFree(inputBufferGPU);); (205, CUDA, result = cudaMalloc(&inputBufferGPU, inputTensorDesc[0].size);); (88, CUDA, result = cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking);); (56, CUDA, result = cudaSetDevice(0);); (211, CUDA, result = cudaMalloc(&outputBufferGPU, outputTensorDesc[0].size);); (324, CUDA, result = cudaStreamDestroy(stream);); (282, CUDA, result = cudaStreamSynchronize(stream);)"}
{"predict": "(48, CUDA, cudaMalloc((void**)&d_A, m*k*sizeof(TA));)\n(50, CUDA, cudaMalloc((void**)&d_B, n*k*sizeof(TB));)\n(52, CUDA, cudaMalloc((void**)&d_C, m*n*sizeof(TC));)\n(89, CUDA, cudaMemcpy(d_A, h_A, m*k*sizeof(TA), cudaMemcpyHostToDevice);)\n(91, CUDA, cudaMemcpy(d_B, h_B, n*k*sizeof(TB), cudaMemcpyHostToDevice);)\n(95, CUDA, cudaMemcpy(d_C, h_C, m*n*sizeof(TC), cudaMemcpyHostToDevice);)\n(100, CUDA, cudaMemcpy(h_C, d_C, m*n*sizeof(TC), cudaMemcpyDeviceToHost);)", "label": "(49, CUDA, thrust::host_vector<TB> h_B(n*k);); (4, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;); (1, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (55, CUDA, thrust::device_vector<TC> d_C = h_C;); (46, CUDA, thrust::host_vector<TC> h_C(m*n);); (52, CUDA, thrust::device_vector<TB> d_B = h_B;); (86, CUDA, thrust::host_vector<TC> cute_result = d_C;); (49, CUDA, thrust::device_vector<TA> d_A = h_A;); (40, CUDA, thrust::host_vector<TA> h_A(m*k);)"}
{"predict": "(12, OpenMP, #pragma omp parallel for reduction(+:counter_N0))", "label": "(8, OpenMP, #pragma omp taskloop shared(counter_N0)); (5, OpenMP, #pragma omp target data map(tofrom : counter_N0)); (8, OpenMP, #pragma omp target teams distribute parallel for map(tofrom : counter_N0) nowait); (9, OpenMP, #pragma omp atomic update)"}
{"predict": "07:     printf(\"Could not allocate vector: %d\\n\", my_ID);\n108:     error = 1;\n109:     goto ENDOFTESTS;\n110:   }\n111: \n112:   /* Initialize the input vector                                               */\n113:   for (jj = 0; jj < total_length; jj++) {\n114:     vector[jj] = 0.0;\n115:   }\n116: \n117:   /* Initialize input buffers and outbuf                                       */\n118:   inbuf  = (double *) malloc(2*n*sizeof(double));\n119:   outbuf = (double *) malloc(2*n*sizeof(double));\n120:   if (inbuf == NULL || outbuf == NULL) {\n121:     printf(\"Could not allocate buffers: %d\\n\", my_ID);\n122:     error = 1;\n123:     goto ENDOFTESTS;\n124:   }\n125: \n126:   /* Broadcast vector to all ranks                                             */\n127: \n128:   /* Broadcast segment_size to all ranks                                       */\n129:   MPI_Bcast(&segment_size, 1, MPI_INT, root, MPI_COMM_WORLD);\n130:   MPI_Bcast(&total_length, 1, MPI_LONG, root, MPI_COMM_WORLD);\n131:   MPI_Bcast(vector, total_length, MPI_DOUBLE, root, MPI_COMM_WORLD);\n132: \n133:   /* Initialize local_pipeline_time to 0.0                                   */\n134:   local_pipeline_time = 0.0;\n135: \n136:   /* Perform pipeline iterations                                             */\n137:   for (iter = 0; iter < iterations; iter++) {\n138:     corner_val = 0.0;\n139:     /* Compute values for this iteration                                     */\n140:     for (jj = 0; jj < segment_size; jj++) {\n141:       jjsize = jj+1;\n142:       for (i = 0; i < n; i++) {\n143:         corner_val += vector[(jjsize*n) + i];\n144:       }\n145:     }\n146:", "label": "(177, MPI, MPI_Recv(inbuf, jjsize, MPI_DOUBLE, my_ID-1, j, MPI_COMM_WORLD, &status);); (88, MPI, MPI_Bcast(&m,          1, MPI_INT, root, MPI_COMM_WORLD);); (88, MPI, MPI_Bcast(&n,          1, MPI_INT, root, MPI_COMM_WORLD);); (89, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (241, MPI, MPI_Finalize();); (163, MPI, MPI_Send(&(ARRAY(end,j)), 1, MPI_DOUBLE, my_ID+1, j, MPI_COMM_WORLD);); (195, MPI, MPI_Send(&corner_val,1,MPI_DOUBLE,0,888,MPI_COMM_WORLD);); (197, MPI, MPI_Recv(&(ARRAY(0,0)),1,MPI_DOUBLE,root,888,MPI_COMM_WORLD,&status);); (18, MPI, MPI_Init(&argc,&argv);); (183, MPI, MPI_Send(outbuf, jjsize, MPI_DOUBLE, my_ID+1, j, MPI_COMM_WORLD);); (138, MPI, MPI_Barrier(MPI_COMM_WORLD);); (17, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (79, MPI, MPI_Bcast(&grp,        1, MPI_INT, root, MPI_COMM_WORLD);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(12, TBB, utils_t::increment_data(range, data);)", "label": "(10, TBB, range_t r = utils_t::make_range((1 << (20 / DimAmount - 1)) - DimAmount, false, &MakeInt, oneapi::tbb::detail::make_index_sequence<DimAmount>());); (1, TBB, using range_t = oneapi::tbb::blocked_nd_range<int, DimAmount>;)"}
{"predict": "(35, MPI, int_t ii = LSUM_BLK (lk); float* lsum_k = &lsum[ii]; superlu_scope_t *scp = &grid->rscp; MPI_Reduce( lsum_k, recvbuf, knsupc * nrhs, MPI_FLOAT, MPI_SUM, kcol, scp->comm); xtrsTimer->trsDataSendXY += knsupc * nrhs; xtrsTimer->trsDataRecvXY += knsupc * nrhs; if (mycol == kcol) { xtrsTimer->trsDataSendZ += knsupc * nrhs; }) (62, MPI, MPI_Status status; xtrsTimer->trsDataRecvZ += knsupc * nrhs; int_t ii = LSUM_BLK (lk); float* dest = &lsum[ii]; float* tempv = recvbuf; for (int_t j = 0; j < nrhs; ++j) { for (int_t i = 0; i < knsupc; ++i) { dest[i + j * knsupc] += tempv[i + j * knsupc]; } })", "label": "(52, MPI, MPI_Recv( recvbuf, knsupc * nrhs, MPI_FLOAT, sender, k, grid3d->zscp.comm, &status );); (42, MPI, MPI_Send( recvbuf, knsupc * nrhs, MPI_FLOAT, receiver, k,  grid3d->zscp.comm);)"}
{"predict": "(31, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(33, MPI, MPI_Barrier(scr_comm_world);)"}
{"predict": "(29, CUDA, for (int token_id = grid.cluster_rank(); token_id < num_token; token_id += grid.num_clusters()) {)", "label": "(27, CUDA, cudaGridDependencySynchronize();); (85, CUDA, cudaTriggerProgrammaticLaunchCompletion();)"}
{"predict": "(17, OpenCL, context = clCreateContext(nullptr, 1, &device, nullptr, nullptr, &error);)", "label": "(16, OpenCL, context = clCreateContext(nullptr, 1, &device, nullptr, nullptr, &error);)"}
{"predict": "(11, OpenMP, #pragma omp parallel for)", "label": "(24, OpenCL, clReleaseEvent(event);)"}
{"predict": "auto const tokenKVPosition = tokenStartIdx + tokenPos;\n90:             auto* vPtr = reinterpret_cast<MoveEltType*>(kvCacheBuffer.getVBlockPtr(seqSlot, tokenKVPosition));\n91:             for (SizeType32 loadChannelIdx = laneIdx; loadChannelIdx < eltCountCurrentMove; loadChannelIdx += 32)\n92:             {\n93:                 auto const channelIdx = loadChannelIdx + startChannelOffset;\n94:                 auto const kvLocationIdx\n95:                     = kvCacheBuffer.getKVLocalIdx(tokenKVPosition, headIdx, eltCountPerHead, channelIdx);\n96:                 vPtr[kvLocationIdx] = tokenSmemBuffer[loadChannelIdx];\n97:             }\n98:         }\n99:         __syncthreads();\n100:     }\n101: }\n\nOutput:", "label": "(31, CUDA, __shared__ char loadSmemBuffer[kUpdateKVCacheKernelShmSize];)"}
{"predict": "(3, OpenMP, #pragma omp parallel for collapse(3))", "label": "(6, OpenMP, #pragma omp teams distribute); (7, OpenMP, #pragma omp parallel for); (8, OpenMP, #pragma omp simd); (1, OpenMP, #pragma omp target map (alloc: u0_real,u0_imag,u1_real,u1_imag,twiddle))"}
{"predict": "(13, MPI, if (MPIT_result != MPI_SUCCESS) { fprintf(stderr, \"Failed to initialize MPI_Tools sub-system.\\n\"); } else { MPI_T_init_thread(MPI_THREAD_MULTIPLE, &provided); })\n(15, MPI, if (MPIT_result != MPI_SUCCESS) { printf(\"Failed to create a session for PVARs.\\n\"); MPI_Abort(MPI_COMM_WORLD, MPIT_result); } else { MPI_T_pvar_session_open(MPI_THREAD_MULTIPLE, &session); })\n(21, MPI, old_cvalues = (size_t *)malloc(2 * world_size * sizeof(size_t)); if (old_cvalues == NULL) { fprintf(stderr, \"Memory allocation failed.\\n\"); MPI_Abort(MPI_COMM_WORLD, MPI_ERR_NO_MEM); } old_svalues = old_cvalues + world_size;)\n(22, MPI, MPI_T_pvar_session_open(MPI_THREAD_MULTIPLE, &session);)\n(23, MPI, pvar_pml_init(*session);)\n(24, MPI, pvar_osc_s_init(*session);)\n(25, MPI, pvar_osc_r_init(*session);)\n(26, MPI, pvar_coll_init(*session);)\n(27, MPI, pvar_o2a_init(*session);)\n(28, MPI, pvar_a2o_init(*session);)\n(29, MPI, pvar_a2a_init(*session);)", "label": "(16, MPI, MPI_Abort(MPI_COMM_WORLD, MPIT_result);); (17, MPI, MPIT_result = MPI_T_pvar_session_create(session);); (11, MPI, MPIT_result = MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);)"}
{"predict": "(2, CUDA, auto buffer = createMockBuffer<FamilyType>(false);)", "label": "(19, OpenCL, retVal = clEnqueueUnmapMemObject(cmdQ.get(), buffer.get(), mappedPtr, 1, &clUnmapEvent, nullptr);); (10, OpenCL, void *mappedPtr = clEnqueueMapBuffer(cmdQ.get(), buffer.get(), CL_FALSE, CL_MAP_WRITE, offset, size, 1, &clMapEvent, nullptr, &retVal);)"}
{"predict": "benchmark_fixed_n(n,\"reduce_lockstep\",[] (size_t N) -> double { return reduce.run_locksteptaskscheduler(N); });\t\n134:       }\t     \n135:       // Always disable observation before observers destruction\n136:       tracker.observe( false );\n137:       pinner.observe( false );\n138:       \n139:       fs.close();\n140:       TaskScheduler::destroy();\n141:     }\n142: \n143:     if (test == 7)\n144:     {\n145:       fs.open (\"benchmark_reduce_sequential.csv\", std::fstream::out);\n146:       benchmark(N_start,N_seq,\"reduce_sequential\",[] (size_t N) -> double { return reduce.run_sequential(N); });\n147:       fs.close();\n148:     }\n149:   }\n150: }", "label": "(62, TBB, PRINT( tbb::TBB_runtime_interface_version() );); (68, TBB, while (tracker.get_concurrency() < tbb::task_scheduler_init::default_num_threads()) tbb_pi<double> (N);); (11, TBB, tbb::task_scheduler_init init(tbb::task_scheduler_init::default_num_threads());); (82, TBB, TaskSchedulerTBB::create(0);); (90, TBB, TaskSchedulerTBB::destroy();)"}
{"predict": "(8, OpenMP, #pragma omp parallel for)\n(16, OpenMP, #pragma omp parallel for)\n(20, OpenMP, #pragma omp parallel for)", "label": "(7, OpenMP, #pragma omp target data map(tofrom: W[:M])); (12, OpenMP, #pragma omp target teams); (12, OpenMP, #pragma omp distribute parallel for)"}
{"predict": "Llu->Ucb_indptr = (Ucb_indptr_t**) SUPERLU_MALLOC (nub * sizeof(Ucb_indptr_t*));\n89:         Llu->Ucb_valptr = (int_t**) SUPERLU_MALLOC (nub * sizeof(int_t*));\n90:     }\n91:     else\n92:     {\n93:         Llu->Ucb_indptr = (Ucb_indptr_t**) mpiMalloc(nub * sizeof(Ucb_indptr_t*));\n94:         Llu->Ucb_valptr = (int_t**) mpiMalloc(nub * sizeof(int_t*));\n95:     }\n96:     if (Llu->Ucb_indptr == NULL || Llu->Ucb_valptr == NULL) {\n97:         SUPERLU_ABORT(\"SUPERLU_MALLOC fails\");\n98:     }\n99:     zAllocBcast(nub * sizeof(int_t), (void**)&(Llu->nsuper), grid3d);\n100:     zAllocBcast(nub * sizeof(int_t), (void**)&(Llu->u_nzval), grid3d);\n101:     zAllocBcast(nub * sizeof(int_t), (void**)&(Llu->u_nzval2), grid3d);\n102:     zAllocBcast(nub * sizeof(int_t), (void**)&(Llu->u_nzval3), grid3d);\n103:     zAllocBcast(nub * sizeof(int_t), (void**)&(Llu->u_nzval4), grid3d);\n104:     zAllocBcast(nbr * sizeof(int_t), (void**)&(Llu->u_nzval5), grid3d);\n105:     zAllocBcast(nub * sizeof(int_t), (void**)&(Llu->u_nzval6), grid3d);\n106:     zAllocBcast(nub * sizeof(int_t), (void**)&(Llu->u_nzval7), grid3d);\n107:     zAllocBcast(nub * sizeof(int_t), (void**)&(Llu->u_nzval8), grid3d);\n108:     zAllocBcast(nub * sizeof(int_t), (void**)&(Llu->u_nzval9), grid3d);\n109:", "label": "(66, MPI, MPI_Bcast( ToRecv, nsupers, MPI_INT, 0,  grid3d->zscp.comm);); (39, MPI, MPI_Bcast( supno, n, mpi_int_t, 0,  grid3d->zscp.comm);); (27, MPI, MPI_Bcast( &nsupers, 1, mpi_int_t, 0,  grid3d->zscp.comm);); (69, MPI, MPI_Bcast( ToSendR[i], Pc, MPI_INT, 0,  grid3d->zscp.comm);); (11, MPI, MPI_Bcast( etree, n, mpi_int_t, 0,  grid3d->zscp.comm);); (233, MPI, MPI_Bcast(*((void**)&(Llu->fsendx_plist[lk])), Pr * sizeof (int), MPI_BYTE, 0, grid3d->zscp.comm);); (84, MPI, MPI_Bcast( &(Llu->ldalsum), 1, mpi_int_t, 0,  grid3d->zscp.comm);); (61, MPI, MPI_Bcast( ToSendD, nbr, MPI_INT, 0,  grid3d->zscp.comm);); (29, MPI, MPI_Bcast( xsup, nsupers + 1, mpi_int_t, 0,  grid3d->zscp.comm);); (80, MPI, MPI_Bcast( &(Llu->nbsendx), 1, MPI_INT, 0,  grid3d->zscp.comm);); (78, MPI, MPI_Bcast( &(Llu->nfsendx), 1, MPI_INT, 0,  grid3d->zscp.comm);); (45, MPI, MPI_Bcast( bufmax, NBUFFERS, mpi_int_t, 0,  grid3d->zscp.comm);)"}
{"predict": "(2, OpenMP, #pragma omp parallel for private(image, cmdQ))", "label": "(16, OpenCL, retVal = clEnqueueUnmapMemObject(cmdQ.get(), image.get(), mappedPtr, 0, nullptr, nullptr);); (8, OpenCL, void *mappedPtr = clEnqueueMapImage(cmdQ.get(), image.get(), CL_TRUE, CL_MAP_READ, &origin[0], &region[0], nullptr, nullptr, 0, nullptr, nullptr, &retVal);)"}
{"predict": "(13, OpenMP, #pragma omp parallel sections;)\n(14, OpenMP, #pragma omp section;)\n(19, OpenMP, #pragma omp atomic;)\n(27, OpenMP, #pragma omp section;)\n(32, OpenMP, #pragma omp atomic;)\n(38, OpenMP, #pragma omp section;)\n(43, OpenMP, #pragma omp atomic;)", "label": "(13, OpenMP, #pragma omp section); (10, OpenMP, #pragma omp parallel); (21, OpenMP, #pragma omp critical)"}
{"predict": "(31, OpenMP, #pragma omp parallel for)\n(44, OpenMP, #pragma omp parallel for)", "label": "(57, MPI, MPI_Finalize();); (10, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(28, CUDA, for (int i = 0; i < 72; ++i) { for (int j = 0; j < 53; ++j) { for (int k = 0; k < 97; ++k) { assert(out(Eigen::array<Eigen::DenseIndex, 3>(i,j,k)) == (in1(Eigen::array<Eigen::DenseIndex, 3>(i,j,k)) == in2(Eigen::array<Eigen::DenseIndex, 3>(i,j,k)))); } } } }", "label": "(17, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (14, CUDA, cudaMalloc((void**)(&d_in1), in1_bytes);); (29, CUDA, assert(cudaMemcpyAsync(out.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);); (16, CUDA, cudaMemcpy(d_in1, in1.data(), in1_bytes, cudaMemcpyHostToDevice);); (40, CUDA, cudaFree(d_out);); (38, CUDA, cudaFree(d_in2);); (26, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (35, CUDA, cudaFree(d_in1);); (8, CUDA, cudaMalloc((void**)(&d_in2), in2_bytes);); (11, CUDA, cudaMemcpy(d_in2, in2.data(), in2_bytes, cudaMemcpyHostToDevice);)"}
{"predict": "(25, CUDA, cudaEventCreate(&start);) (26, CUDA, cudaEventCreate(&end);) (28, CUDA, cudaEventRecord(start, 0);) (29, CUDA, cudaEventRecord(end, 0);) (30, CUDA, cudaEventSynchronize(end);) (31, CUDA, cudaEventElapsedTime(&msec, start, end);) (32, CUDA, cudaEventDestroy(start);) (33, CUDA, cudaEventDestroy(end);)", "label": "(27, CUDA, cudaEventRecord(start);); (31, CUDA, cudaEventRecord(end);); (31, CUDA, cudaEventSynchronize(end);); (23, CUDA, cudaEventCreate(&end);); (21, CUDA, cudaEventCreate(&start);); (31, CUDA, cudaEventElapsedTime(&msec, start, end);)"}
{"predict": "(8, CUDA, for (int i = 0; i < len; i++) { arr[i] = 1; }); (16, CUDA, cudaMemcpy(d_arr, arr, len * sizeof(int), cudaMemcpyHostToDevice);); (22, CUDA, reduce_naive_kernel<blocksize><<<gridsize, blocksize>>>(d_arr, d_out, len);); (24, CUDA, cudaMemcpy(out, d_out, gridsize * sizeof(int), cudaMemcpyDeviceToHost);); (27, CUDA, long long sum = 0; for (int i = 0; i < gridsize; i++) { sum += out[i]; }); (33, CUDA, printf(\"sum = %d\\n\", sum);); (35, CUDA, long long sum2 = 0; for (int i = 0; i < len; i++) { sum2 += arr[i]; }); (42, CUDA, if (sum == sum2) { printf(\"success\\n\"); } else { printf(\"failed, the result is %d\\n\", sum2); });", "label": "(13, CUDA, cudaMalloc((void **)&d_arr, sizeof(int) * len);); (16, CUDA, cudaMemcpy(d_arr, arr, sizeof(int) * len, cudaMemcpyHostToDevice);); (53, CUDA, cudaFree(d_out);); (11, CUDA, cudaMalloc((void **)&d_out, sizeof(int) * len);); (23, CUDA, cudaMemcpy(out, d_out, sizeof(int) * len, cudaMemcpyDeviceToHost);); (49, CUDA, cudaFree(d_arr);)"}
{"predict": "(14, OpenMP, #pragma omp parallel for private(i))", "label": "(11, OpenMP, #pragma omp distribute); (16, OpenMP, #pragma omp parallel for); (13, OpenMP, threads[j] = omp_get_num_threads();); (7, OpenMP, #pragma omp target teams num_teams(numTeams) thread_limit(1024) map (tofrom: CUs, lims, threads, Res)); (16, OpenMP, CUs[i] = omp_ext_get_smid();); (9, OpenMP, lims[j] = omp_get_num_teams();)"}
{"predict": "(11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)", "label": "(19, MPI, rc = MPI_Comm_size(MPI_COMM_WORLD, &size);); (8, MPI, rc = MPI_Comm_get_attr(MPI_COMM_WORLD, MPI_UNIVERSE_SIZE, &v, &flag);)"}
{"predict": "(9, CUDA, if (result != cudaSuccess) { cudaSetDevice(i); })", "label": "(4, CUDA, cudaError_t result = cudaSetDevice(i);); (6, CUDA, nvDebug(\"*** CUDA Error: %s\\n\", cudaGetErrorString(result));)"}
{"predict": "n;\n108: \n109:   /* allocate memory for vectors */\n110:   vector = (double *)malloc(total_length * sizeof(double));\n111:   inbuf  = (double *)malloc(segment_size * n * sizeof(double));\n112:   outbuf = (double *)malloc(segment_size * n * sizeof(double));\n113: \n114:   /* Initialize vector with grid values */\n115:   for (i = 0; i < total_length; i++)\n116:     vector[i] = i;\n117: \n118:   /* Initialize input and output buffers */\n119:   for (i = 0; i < segment_size; i++)\n120:     for (j = 0; j < n; j++) {\n121:       inbuf[i*n+j] = vector[start+i*n+j];\n122:       outbuf[i*n+j] = 0.0;\n123:     }\n124: \n125:   /* Start timing */\n126:   local_pipeline_time = MPI_Wtime();\n127: \n128:   /* Call the pipeline function */\n129:   for (iter = 0; iter < iterations; iter++){\n130:     for (jj = 0; jj < segment_size; jj++) {\n131:       jjsize = n;\n132:       if (jj == 0) jjsize = n-1;\n133:       for (i = 0; i < jjsize; i++) {\n134:         corner_val = inbuf[jj*n+i];\n135:         inbuf[jj*n+i] = inbuf[jj*n+i+1];\n136:         inbuf[jj*n+i+1] = corner_val;\n137:       }\n138:     }\n139:   }\n140: \n141:   /* End timing */\n142:   local_pipeline_time = MPI_Wtime() - local_pipeline_time;\n143: \n144:   /* Reduce local timing to get overall pipeline time */\n145:   MPI_Reduce(&local_pipeline_time, &pipeline_time, 1, MPI_DOUBLE, MPI_MAX, root, MPI_COMM_WORLD);\n146: \n147:", "label": "(180, MPI, MPI_Recv(inbuf, jjsize, MPI_DOUBLE, my_ID-1, j, MPI_COMM_WORLD, &status);); (91, MPI, MPI_Bcast(&m,          1, MPI_INT, root, MPI_COMM_WORLD);); (91, MPI, MPI_Bcast(&n,          1, MPI_INT, root, MPI_COMM_WORLD);); (92, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (244, MPI, MPI_Finalize();); (166, MPI, MPI_Send(&(ARRAY(end,j)), 1, MPI_DOUBLE, my_ID+1, j, MPI_COMM_WORLD);); (198, MPI, MPI_Send(&corner_val,1,MPI_DOUBLE,0,888,MPI_COMM_WORLD);); (200, MPI, MPI_Recv(&(ARRAY(0,0)),1,MPI_DOUBLE,root,888,MPI_COMM_WORLD,&status);); (19, MPI, MPI_Init(&argc,&argv);); (186, MPI, MPI_Send(outbuf, jjsize, MPI_DOUBLE, my_ID+1, j, MPI_COMM_WORLD);); (141, MPI, MPI_Barrier(MPI_COMM_WORLD);); (18, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (82, MPI, MPI_Bcast(&grp,        1, MPI_INT, root, MPI_COMM_WORLD);); (15, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(15, CUDA, cudaMalloc(&d_work, workspaceInBytesOnDevice);) (23, CUDA, cudaFree(h_work);)", "label": "(33, CUDA, CUDA_CHECK(cudaFree(d_work));); (14, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_work), workspaceInBytesOnDevice));)"}
{"predict": "::setw(20) << \"Kernel\" << std::setw(20) << \"Time\"\n110:             << std::setw(20) << \"Speedup\" << std::setw(20) << \"ETOL\"\n111:             << std::setw(20) << \"Pass/Fail\" << std::endl;\n112:   for (int i = 0; i < 6; i++) {\n113:     double avg_time = 0.0;\n114:     for (unsigned int j = ignore_times; j < repeat_num_times; j++) {\n115:       avg_time += timings[i][j];\n116:     }\n117:     avg_time /= (repeat_num_times - ignore_times);\n118:     std::cout << std::left << std::setw(20) << \"Kernel \" << i + 1 << std::setw(20)\n119:               << avg_time << std::setw(20) << \"N/A\" << std::setw(20) << ETOL\n120:               << std::setw(20) << (avg_time < ETOL ? \"Pass\" : \"Fail\") << std::endl;\n121:   }\n122: \n123:   // Free arrays\n124:   free(a);\n125:   free(b);\n126:   free(c);\n127: }\n\nAssistant:\n(20, OpenMP, #pragma omp parallel for)", "label": "(12, OpenMP, #pragma omp target enter data map(alloc); (4, OpenMP, #pragma omp target map(tofrom : warp_size)); (135, OpenMP, #pragma omp target exit data map(release)"}
{"predict": "(14, OpenCL, err = clGetPlatformIDs(1, &platform, NULL);) (16, OpenCL, if (err != CL_SUCCESS) { printf( \"clGetPlatformIDs() failed with %d\\n\", err ); return 1; }) (18, OpenCL, err = clGetDeviceIDs(platform, CL_DEVICE_TYPE_GPU, 1, &device, NULL);) (20, OpenCL, if (err != CL_SUCCESS) { printf( \"clGetDeviceIDs() failed with %d\\n\", err ); return 1; }) (26, OpenCL, props[1] = (cl_context_properties)platform;) (32, OpenCL, if (err != CL_SUCCESS) { return 1; }) (34, OpenCL, if (err != CL_SUCCESS) { return 1; }) (44, OpenCL, err = clblasSetup();) (46, OpenCL, if (err != CL_SUCCESS) { printf(\"clblasSetup() failed with %d\\n\", err); clReleaseContext(ctx); return 1; }) (49, OpenCL, err = clblasSrotg(bufSA, 0, bufSB, 0, bufC, 0, bufS, 0, 1, &queue, 0, NULL, &event);) (51, OpenCL, if (err != CL_SUCCESS) { printf(\"clblasSrotg() failed with %d\\n\", err); ret = 1; }) (55, OpenCL, err = clWaitForEvents(1, &event);) (64, OpenCL, clReleaseEvent(event);) (66, OpenCL, clReleaseMemObject(bufSA);) (67, OpenCL, clReleaseMemObject(bufSB);) (68, OpenCL, clReleaseMemObject(bufC);) (69, OpenCL, clReleaseMemObject(bufS);) (71, OpenCL, clblasTeardown();) (73, OpenCL, clReleaseCommandQueue(queue);) (74, OpenCL, clReleaseContext(ctx);)", "label": "(50, OpenCL, bufSA = clCreateBuffer(ctx, CL_MEM_READ_WRITE, sizeof(cl_float), NULL, &err);); (52, OpenCL, bufS  = clCreateBuffer(ctx, CL_MEM_READ_WRITE, sizeof(cl_float), NULL, &err);); (67, OpenCL, err = clWaitForEvents(1, &event);); (55, OpenCL, err = clEnqueueWriteBuffer(queue, bufS, CL_TRUE, 0, sizeof(cl_float), &S, 0, NULL, NULL);); (83, OpenCL, clReleaseMemObject(bufC);); (77, OpenCL, clReleaseEvent(event);); (79, OpenCL, clReleaseMemObject(bufSA);); (68, OpenCL, err = clEnqueueReadBuffer(queue, bufS, CL_TRUE, 0, sizeof(cl_float), &S, 0, NULL, NULL);); (28, OpenCL, clReleaseContext(ctx);); (63, OpenCL, err = clEnqueueReadBuffer(queue, bufSA, CL_TRUE, 0, sizeof(cl_float), &SA, 0, NULL, NULL);); (47, OpenCL, err = clEnqueueWriteBuffer(queue, bufC, CL_TRUE, 0, sizeof(cl_float), &C, 0, NULL, NULL);); (62, OpenCL, err = clEnqueueReadBuffer(queue, bufSB, CL_TRUE, 0, sizeof(cl_float), &SB, 0, NULL, NULL);); (32, OpenCL, clReleaseCommandQueue(queue);); (38, OpenCL, bufSB = clCreateBuffer(ctx, CL_MEM_READ_WRITE, sizeof(cl_float), NULL, &err);); (74, OpenCL, clReleaseMemObject(bufS);); (71, OpenCL, clReleaseMemObject(bufSB);); (19, OpenCL, printf( \"clCreateCommandQueue() failed with %d\\n\", err );); (16, OpenCL, queue = clCreateCommandQueue(ctx, device, 0, &err);); (34, OpenCL, bufC  = clCreateBuffer(ctx, CL_MEM_READ_WRITE, sizeof(cl_float), NULL, &err);); (8, OpenCL, ctx = clCreateContext(props, 1, &device, NULL, NULL, &err);); (9, OpenCL, printf( \"clCreateContext() failed with %d\\n\", err );); (34, OpenCL, err = clEnqueueWriteBuffer(queue, bufSA, CL_TRUE, 0, sizeof(cl_float), &SA, 0, NULL, NULL);); (34, OpenCL, err = clEnqueueWriteBuffer(queue, bufSB, CL_TRUE, 0, sizeof(cl_float), &SB, 0, NULL, NULL);); (51, OpenCL, err = clEnqueueReadBuffer(queue, bufC, CL_TRUE, 0, sizeof(cl_float), &C, 0, NULL, NULL);)"}
{"predict": "31:               << std::endl;\n132:   } else {\n133:     std::cout << \"Gsamples/s: \" << 1e-9 * n_vectors * n_dimensions / time\n134:               << std::endl;\n135:   }\n136: \n137:   // Copy the output from the device to the host\n138:   std::cout << \"Copying output to host...\" << std::endl;\n139:   checkCudaErrors(cudaMemcpy(h_outputGPU, d_output,\n140:                              n_vectors * n_dimensions * sizeof(float),\n141:                              cudaMemcpyDeviceToHost));\n142: \n143:   // Verify the results\n144:   std::cout << \"Verifying results...\" << std::endl;\n145:   checkResult(h_outputCPU, h_outputGPU, n_vectors, n_dimensions);\n146: \n147:   // Free memory\n148:   std::cout << \"Freeing memory...\" << std::endl;\n149:   delete[] h_directions;\n150:   delete[] h_outputCPU;\n151:   delete[] h_outputGPU;\n152:   cudaFree(d_directions);\n153:   cudaFree(d_output);\n154: \n155:   // Destroy the timer\n156:   sdkDestroyTimer(&hTimer);\n157: \n158:   return 0;\n159: }", "label": "(93, CUDA, throw std::runtime_error(cudaGetErrorString(cudaResult));); (57, CUDA, findCudaDevice(argc, (const char **)argv);); (117, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (211, CUDA, checkCudaErrors(cudaFree(d_output));); (209, CUDA, checkCudaErrors(cudaFree(d_directions));)"}
{"predict": "(4, OpenMP, #pragma omp parallel for private(i);)", "label": "(82, OpenMP, #pragma omp parallel private(i)); (95, OpenMP, #pragma omp distribute parallel for private(m)); (5, OpenMP, #pragma omp target); (26, OpenMP, #pragma omp distribute parallel for private(argc,); (60, OpenMP, #pragma omp distribute parallel for private(h, B::x)); (64, OpenMP, #pragma omp distribute parallel for nowait); (8, OpenMP, #pragma omp distribute parallel for private(); (65, OpenMP, #pragma omp parallel); (16, OpenMP, #pragma omp distribute parallel for private(argc); (40, OpenMP, #pragma omp distribute parallel for private(a, b)); (49, OpenMP, #pragma omp distribute parallel for private(e, g)); (33, OpenMP, #pragma omp distribute parallel for private(S1)); (42, OpenMP, #pragma omp distribute parallel for private(argv[1])); (6, OpenMP, #pragma omp distribute parallel for private()); (-5, OpenMP, #pragma omp distribute parallel for private); (62, OpenMP, #pragma omp distribute parallel for private(i)); (69, OpenMP, #pragma omp distribute parallel for private(j)); (22, OpenMP, #pragma omp distribute parallel for private(argc)); (16, OpenMP, #pragma omp distribute parallel for private(argc > 0 ? argv[1] : argv[2])); (62, OpenMP, #pragma omp parallel shared(i)); (-12, OpenMP, #pragma omp teams)"}
{"predict": "(3, OpenMP, #pragma omp parallel for)\n(5, OpenMP, #pragma omp parallel for)\n(7, OpenMP, #pragma omp parallel for)\n(9, OpenMP, #pragma omp parallel for)\n(11, OpenMP, #pragma omp parallel for)\n(13, OpenMP, #pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp target parallel for device ()); (23, OpenMP, #pragma omp target parallel for device (3.14)); (18, OpenMP, #pragma omp target parallel for device (-2)); (13, OpenMP, #pragma omp target parallel for device (argc), device (argc+1)); (14, OpenMP, #pragma omp target parallel for device (S1)); (5, OpenMP, #pragma omp target parallel for device (argc))); (-4, OpenMP, #pragma omp target parallel for device); (1, OpenMP, #pragma omp target parallel for device (argc); (14, OpenMP, #pragma omp target parallel for device (-10u)); (5, OpenMP, #pragma omp target parallel for device (z + argc)); (-6, OpenMP, #pragma omp target parallel for device (); (1, OpenMP, #pragma omp target parallel for device (argc > 0 ? argv[1] : argv[2]))"}
{"predict": "(7, OpenMP, #pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp target map(from:c))"}
{"predict": "08:               idx_i = __atomic_load_n(&ilsum[lib], __ATOMIC_SEQ_CST);\n109:               if(idx_i<lib){\n110:                 __atomic_compare_exchange_n(&ilsum[lib], &idx_i, lib+1, 0, __ATOMIC_SEQ_CST, __ATOMIC_SEQ_CST);\n111:               }\n112:             }\n113:             __syncthreads();\n114: \n115:             // if(lne==0)printf(\"  afa kernel:   %i %i %i %i %i %i %i %i %i %i\\n\", threadIdx_x, bid, grid->npcol, nsupers,myrow,krow,wrp,k,ilsum[lib]);\n116: \n117:             // if(lne==0)printf(\"  afa kernel:   %i %i %i %i %i %i %i %i %i %i\\n\", threadIdx_x, bid, grid->npcol, nsupers,myrow,krow,wrp,k,lib,ilsum[lib]);\n118: \n119:             if(lne==0){  /*only the first thread in a warp handles the lock */\n120:               idx_v = __atomic_load_n(&Uindval_loc_bc_dat[lib*aln_i], __ATOMIC_SEQ_CST);\n121:               if(idx_v<lib){\n122:                 __atomic_compare_exchange_n(&Uindval_loc_bc_dat[lib*aln_i], &idx_v, lib+1, 0, __ATOMIC_SEQ_CST, __ATOMIC_SEQ_CST);\n123:               }\n124:             }\n125:             __syncthreads();\n126: \n127:             // if(lne==0)printf(\"  afa kernel:   %i %i %i %i %i %i %i %i %i %i\\n\", threadIdx_x, bid, grid->npcol, nsupers,myrow,krow,wrp,k,lib,ilsum[lib]);\n128: \n129:             // if(lne==0)printf(\"  afa kernel:   %i %i %i %i %i %i %i %i", "label": "(35, CUDA, volatile __shared__ int s_bmod;); (32, CUDA, __shared__ double s_lsum[MAXSUPER];)"}
{"predict": "(43, MPI, int rank; MPI_Comm_rank(comm, &rank);)", "label": "(47, MPI, MPI_Ibcast(&(prg->keep_going), 1, MPI_INT, 0, prg->comm, &(prg->bcast_req));); (15, MPI, MPI_Comm_dup(comm, &prg->comm);); (37, MPI, prg->time_start = MPI_Wtime();); (42, MPI, MPI_Comm_rank(prg->comm, &rank);)"}
{"predict": "(11, MPI, int_t k = CEILING(nsupers, grid->nprow); MPI_Allreduce(&k, &k, 1, MPI_INT, MPI_MAX, MPI_COMM_WORLD);) (34, MPI, MPI_Bcast(Ufstnz_br_ptr, k, MPI_INT, PCOL(jb, grid), MPI_COMM_WORLD);) (47, MPI, MPI_Bcast(Lrowind_bc_ptr, k, MPI_INT, PCOL(jb, grid), MPI_COMM_WORLD);)", "label": "(28, MPI, MPI_DATATYPE_ALLOC(uval_new, lenv);); (25, MPI, MPI_INT_ALLOC(usub_new, lens);); (59, MPI, MPI_INT_ALLOC(lsub_new, len1);); (60, MPI, MPI_DATATYPE_ALLOC(lnzval_new, len2);)"}
{"predict": "(13, CUDA, dim3 blockSize(256); dim3 gridSize(iDivUp(threadCount, 256)); mergeRanksAndIndicesKernel<<<gridSize, blockSize>>>(d_LimitsA, d_RanksA, stride, N, threadCount); mergeRanksAndIndicesKernel<<<gridSize, blockSize>>>(d_LimitsB, d_RanksB, stride, N, threadCount);)", "label": "(28, CUDA, getLastCudaError(\"mergeRanksAndIndicesKernel(B)<<<>>> failed\\n\");); (18, CUDA, getLastCudaError(\"mergeRanksAndIndicesKernel(A)<<<>>> failed\\n\");)"}
{"predict": "(8, CUDA, cudaGetFunctionAttributes(&attr, ptr);)", "label": "(9, CUDA, cudaFuncGetAttributes(&attr,(void*)ptr);); (14, CUDA, fcuda::Check_CudaErroorFun(\"Error collecting kernel information.\");); (5, CUDA, cudaOccupancyMaxPotentialBlockSize(&mingridsize,&qblocksize,(void*)ptr,0,0);)"}
{"predict": "(42, OpenMP, #pragma omp parallel for reduction(+:failure_counter) reduction(+:counter))", "label": "(20, TBB, tbb::global_control control(tbb::global_control::max_allowed_parallelism, n_threads);); (1, TBB, n_threads = tbb::this_task_arena::max_concurrency();); (29, TBB, tbb::enumerable_thread_specific<tbb::task_arena*> tls;); (25, TBB, std::vector<tbb::task_arena> arenas(n_arenas);)"}
{"predict": "(17, OpenCL, auto clContext = clCreateContext(nullptr, 0, nullptr, nullptr, nullptr, &retVal);)", "label": "(15, OpenCL, int error = clGetDeviceInfo( device, CL_DEVICE_IMAGE2D_MAX_WIDTH, sizeof( maxWidth ), &maxWidth, NULL );); (16, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_GLOBAL_MEM_SIZE, sizeof( memSize ), &memSize, NULL );); (14, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_MAX_MEM_ALLOC_SIZE, sizeof( maxAllocSize ), &maxAllocSize, NULL );)"}
{"predict": "(26, MPI, MPI_Comm_spawn((char *) \"./disconnect3\", MPI_ARGV_NULL, np, MPI_INFO_NULL, 0, MPI_COMM_WORLD, &intercomm, MPI_ERRCODES_IGNORE);)", "label": "(36, MPI, MPI_Comm_size(intercomm, &size);); (56, MPI, MPI_Comm_disconnect(&intercomm);); (102, MPI, MPI_Finalize();); (19, MPI, MPI_Comm_get_parent(&parentcomm);); (47, MPI, MPI_Recv(&data, 1, MPI_INT, RECEIVER_RANK, DATA_TAG, intercomm, &status);); (79, MPI, MPI_Abort(intercomm, 1);); (73, MPI, MPI_Recv(&i, 1, MPI_INT, SENDER_RANK, DATA_TAG, intercomm, &status);); (43, MPI, MPI_Send(&i, 1, MPI_INT, RECEIVER_RANK, DATA_TAG, intercomm);); (29, MPI, MPI_Comm_rank(intercomm, &rank);); (26, MPI, MPI_Comm_remote_size(intercomm, &rsize);); (76, MPI, MPI_Send(&i, 1, MPI_INT, SENDER_RANK, DATA_TAG, intercomm);)"}
{"predict": "99:   /* Broadcast input data to all ranks */\n100:   MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);\n101:   MPI_Bcast(&n, 1, MPI_INT, root, MPI_COMM_WORLD);\n102:   MPI_Bcast(&nsquare, 1, MPI_LONG, root, MPI_COMM_WORLD);\n103:   MPI_Bcast(&RADIUS, 1, MPI_INT, root, MPI_COMM_WORLD);\n104:   MPI_Bcast(&weight, (2*RADIUS+1)*(2*RADIUS+1), MPI_DOUBLE, root, MPI_COMM_WORLD);\n105: \n106:   /* Allocate input and output arrays */\n107:   total_length_in  = nsquare;\n108:   total_length_out = nsquare;\n109:   in    = (DTYPE *) malloc(total_length_in * sizeof(DTYPE));\n110:   out   = (DTYPE *) malloc(total_length_out * sizeof(DTYPE));\n111: \n112:   /* Initialize input and output arrays */\n113:   if (my_ID == root) {\n114:     for (int i = 0; i < nsquare; i++) {\n115:       in[i] = 1.0;\n116:       out[i] = 0.0;\n117:     }\n118:   }\n119: \n120:   /* Broadcast input data to all ranks */\n121:   MPI_Bcast(in, total_length_in, MPI_DOUBLE, root, MPI_COMM_WORLD);\n122:   MPI_Bcast(out, total_length_out, MPI_DOUBLE, root, MPI_COMM_WORLD);\n123: \n124:   /* Compute the stencil size */\n125:   stencil_size = (2*RADIUS+1)*(2*RADIUS+1);\n126: \n127:   /* Compute the local grid size */\n128:   width  = n / Num_procs;\n129:   height = n / Num_procs;\n130:   if (n % Num_procs != 0) {\n131:     width += 1;\n132:   }\n133:   if (n % Num_procs != 0) {\n134:     height +=", "label": "(295, MPI, MPI_Wait(&(request[1+4]), MPI_STATUS_IGNORE);); (301, MPI, MPI_Wait(&(request[3+4]), MPI_STATUS_IGNORE);); (234, MPI, MPI_Barrier(MPI_COMM_WORLD);); (128, MPI, MPI_Bcast(&n,          1, MPI_INT, root, MPI_COMM_WORLD);); (43, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (289, MPI, MPI_Wait(&(request[0+4]), MPI_STATUS_IGNORE);); (254, MPI, MPI_Wait(&(request[0]), MPI_STATUS_IGNORE);); (294, MPI, MPI_Wait(&(request[2+4]), MPI_STATUS_IGNORE);); (124, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (36, MPI, MPI_Init(&argc,&argv);); (251, MPI, MPI_Wait(&(request[1]), MPI_STATUS_IGNORE);); (256, MPI, MPI_Wait(&(request[2]), MPI_STATUS_IGNORE);); (365, MPI, MPI_Finalize();); (255, MPI, MPI_Wait(&(request[3]), MPI_STATUS_IGNORE);); (324, MPI, MPI_Reduce(&local_norm, &norm, 1, MPI_DTYPE, MPI_SUM, root, MPI_COMM_WORLD);); (31, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(14, OpenMP, #pragma omp parallel for)", "label": "(6, CUDA, extent = make_cudaExtent(30, 40, 0);); (3, CUDA, cudaExtent extent = make_cudaExtent(10, 20, 0);)"}
{"predict": "(14, MPI, MPI_Comm cart;)\n(16, MPI, MPI_Cart_create(MPI_COMM_WORLD, 1, &wsize, periods, /*reorder= */ 0, &cart);)\n(22, MPI, MPI_Neighbor_alltoallv(sendbuf, sendcounts, sdispls, MPI_INT, recvbuf, recvcounts, rdispls, MPI_INT, cart);)", "label": "(13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (37, MPI, MPI_Comm_free(&cart);); (12, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);)"}
{"predict": "(9, MPI, MPI_Barrier(comm, MPI_COMM_WORLD);)", "label": "(20, MPI, MPI_Recv(buff, count, type, 0, tag, comm, MPI_STATUS_IGNORE);); (4, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(54, OpenMP, #pragma omp parallel for private(it_const, transformation_matrix, constant_vector, slave_solution_vector, master_solution_vector, p_master_node, p_slave_node))", "label": "(55, OpenMP, #pragma omp for schedule(guided, 512)); (52, OpenMP, #pragma omp parallel firstprivate(transformation_matrix, constant_vector, slave_solution_vector, master_solution_vector, p_master_node, p_slave_node))"}
{"predict": "(9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);) (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);) (17, MPI, MPI_Isend(&a, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, &request);) (20, MPI, MPI_Recv(&a, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)", "label": "(24, MPI, MPI_Request_free(&request);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (15, MPI, MPI_Psend_init(&a, 1, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, MPI_INFO_NULL, &request);); (16, MPI, MPI_Precv_init(&a, 1, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_INFO_NULL, &request);); (21, MPI, MPI_Barrier(MPI_COMM_WORLD);); (6, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": ", buf, &buftype_extent, &lb);\n106:     }\n107: \n108:     buftype_extent = ADIOI_Datatype_get_extent(datatype);\n109:     MPI_Type_get_extent(datatype, &lb, &buftype_extent);\n110: \n111:     for (i = 0; i < nprocs; i++) {\n112:         if (others_req[i].count) {\n113:             /* compute the actual number of bytes to be read */\n114:             for_curr_iter = (others_req[i].lens[j] + coll_bufsize - 1) / coll_bufsize;\n115:             /* and the total number of bytes that will be read */\n116:             for_next_iter = for_curr_iter * coll_bufsize;\n117:         }\n118:     }\n119: \n120:     /* The following loop is where the actual I/O is performed. */\n121:     for (m = 0; m < ntimes; m++) {\n122:         /* calculate the offset and length of the next read */\n123:         done = 0;\n124:         for (i = 0; i < nprocs; i++) {\n125:             if (others_req[i].count) {\n126:                 if (done) {\n127:                     curr_offlen_ptr[i] = curr_offlen_ptr[i - 1];\n128:                 } else {\n129:                     curr_offlen_ptr[i] = others_req[i].offsets[0];\n130:                     done = 1;\n131:                 }\n132:                 /* adjust the offset and length of the next read */\n133:                 if (curr_offlen_ptr[i] < end_loc) {\n134:                     curr_offlen_ptr[i] += coll_bufsize;\n135:                     curr_offlen_ptr[i] = MPL_MIN(curr_offlen_ptr[i], end_loc);\n136:                 } else {\n137:                     curr_offlen_ptr[i] = -1;\n138:                 }\n139:             }\n140:         }\n141: \n142:         /* now, determine the actual data to be sent to each proc */\n1", "label": "(113, MPI, MPI_Comm_rank(fd->comm, &rank);); (180, MPI, MPI_Get_count(&read_status, MPI_BYTE, &actual_size);); (105, MPI, MPI_Type_get_extent(datatype, &lb, &buftype_extent);); (69, MPI, MPI_Allreduce(&ntimes, &max_ntimes, 1, MPI_INT, MPI_MAX, fd->comm);); (219, MPI, MPI_Get_address(read_buf + req_off - real_off, &addr);)"}
{"predict": "(12, OpenMP, #pragma omp parallel for private(sum1, sum2, x, k) reduction(+:m_gate))", "label": "(16, OpenMP, #pragma omp target teams distribute parallel for thread_limit(128)); (8, OpenMP, #pragma omp target enter data map(to: Vm[:nCells])); (6, OpenMP, #pragma omp target enter data map(to: m_gate[:nCells]))"}
{"predict": "(7, MPI, MPI_Get_attr(win, key[i], &val_p, &flag);)", "label": "(6, MPI, MPI_Win_get_attr(win, key[i], &val_p, &flag);)"}
{"predict": "(22, CUDA, cudaMalloc((void**)&deviceTransformCoeff, sizeof(int)*nOutputDims);) (24, CUDA, cudaFree(deviceTransformCoeff);)", "label": "(23, CUDA, cudaMemcpy(output, data, copySize, cudaMemcpyDeviceToDevice);); (20, CUDA, cudaMemcpy(workspace, transformCoeff, sizeof(int)*nOutputDims,cudaMemcpyHostToDevice );)"}
{"predict": "(10, MPI, MPI_Type_commit(&vecs[i]);)", "label": "(19, MPI, MPI_Type_commit(&mystruct);); (27, MPI, MPI_Type_create_hvector(0, 1, stride, MPI_INT, &vecs[i]);); (19, MPI, MPI_Type_free(&mystruct);); (8, MPI, MPI_Type_commit(&vecs[i]);); (14, MPI, MPI_Type_create_struct(3, blockcount, displs, vecs, &mystruct);); (5, MPI, MPI_Type_create_hvector(i, 1, stride, MPI_INT, &vecs[i]);); (17, MPI, MPI_Type_free(&vecs[i]);)"}
{"predict": "(20, CUDA, start = second();) (37, CUDA, stop = second();)", "label": "(14, CUDA, checkCudaErrors(cudaMalloc(&info, sizeof(int)));); (14, CUDA, checkCudaErrors(cudaMalloc(&buffer, sizeof(double) * bufferSize));); (14, CUDA, checkCudaErrors(cudaMalloc(&A, sizeof(double) * lda * n));); (36, CUDA, checkCudaErrors(cusolverDnDpotrs(handle, uplo, n, 1, A, lda, x, n, info));); (37, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (43, CUDA, checkCudaErrors(cudaFree(info));); (45, CUDA, checkCudaErrors(cudaFree(buffer));); (14, CUDA, checkCudaErrors(cudaMemset(info, 0, sizeof(int)));); (46, CUDA, checkCudaErrors(cudaFree(A));)"}
{"predict": "(5, OpenMP, #pragma omp parallel for reduction(+:tmp) collapse(1))\n(13, OpenMP, #pragma omp parallel for reduction(+:tmp) collapse(1))", "label": "(12, OpenMP, #pragma omp target map(tofrom: N) map(from:tmp)); (2, OpenMP, #pragma omp target map(to: N) map(tofrom: tmp))"}
{"predict": "(3, MPI, int rank = 0; MPI_Comm_rank(MPI_COMM_WORLD, &rank;))", "label": "(4, MPI, MPI_Init(&argc, &argv);); (52, MPI, MPI_Finalize();); (6, MPI, ierr = MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, int ierr = MPI_Comm_size(MPI_COMM_WORLD, &comm_size);)"}
{"predict": "<cublasGemmAlgo_t>(CUBLAS_GEMM_DEFAULT));\n77:         #endif\n78: \n79:         if (bias.dims.size() > 0) {\n80:             FastllmCudaAddBiasKernel <<< (len - 1) / threadPerBlock + 1, threadPerBlock>>>((float *) cudaOutput,\n81:                                                                                     cudaBiasData, len);\n82:         }\n83: \n84:         FastllmCudaHalf2FloatKernel <<< (len - 1) / threadPerBlock + 1, threadPerBlock>>>(cudaOutput, cudaFp16Output,\n85:                                                                                       len);\n86: \n87:         FastllmCudaFree(cudaFp16Input);\n87:         FastllmCudaFree(cudaFp16Weight);\n88:         FastllmCudaFree(cudaFp16Output);\n89:     }\n90: }\n\nOutput:\n(33, CUDA, cudaMalloc((void**)&cudaFp16Input, n * m * sizeof(half));) (34, CUDA, cudaMalloc((void**)&cudaFp16Weight, k * m * sizeof(half));) (38, CUDA, cudaMalloc((void**)&cudaFp16Input, n * m * sizeof(half));) (39, CUDA, cudaMalloc((void**)&cudaFp16Weight, k * m * sizeof(half));) (60, CUDA, cudaFree(cudaFp16Input);) (61, CUDA, cudaFree(cudaFp16Weight);) (62, CUDA, cudaFree(cudaFp16Output);) (67, CUDA, cudaFree(cudaFp16Input);) (68, CUDA, cudaFree(cudaFp16Weight);) (69, CUDA, cudaFree(cudaFp16Output);) (76, CUDA, cudaFree(cudaFp16Output);)", "label": "(119, CUDA, FastllmCudaFinishOutput(output, cudaOutput);); (39, CUDA, cudaFp16Input = (half *) FastllmCudaMalloc(n * m * sizeof(half));); (21, CUDA, state = cudaMemset(cudaBiasData, 0, k * sizeof(float));); (115, CUDA, FastllmCudaFinishInput(input, cudaInput);); (43, CUDA, cudaFp16Output = (half *) FastllmCudaMalloc(n * k * sizeof(half));); (9, CUDA, state = cudaMemcpy(cudaMins, mins, k * sizeof(float), cudaMemcpyHostToDevice);); (-2, CUDA, state = cudaMalloc(&cudaScales, k * sizeof(float));); (94, CUDA, FastllmCudaFree(cudaFp16Input);); (103, CUDA, FastllmCudaFree(cudaFp16Output);); (32, CUDA, cudaFp16Weight = (half *) FastllmCudaMalloc(k * m * sizeof(half));); (-1, CUDA, state = cudaMalloc(&cudaMins, k * sizeof(float));); (-6, CUDA, state = cudaMemcpy(cudaScales, weight.scales.data(), k * sizeof(float), cudaMemcpyHostToDevice);); (7, CUDA, state = cudaMalloc(&cudaBiasData, k * sizeof(float));); (12, CUDA, checkCudaErrors(\"Error: CUDA error when moving bias to device!\", state);); (7, CUDA, state = cudaMemcpy(cudaBiasData, (uint8_t*)bias.cudaData, k * sizeof(float), cudaMemcpyDeviceToDevice);); (87, CUDA, FastllmCudaFree(cudaFp16Weight);); (18, CUDA, float *cudaOutput = (float*)FastllmCudaPrepareOutput(output);); (16, CUDA, float *cudaInput = (float*)FastllmCudaPrepareInput(input);)"}
{"predict": "(6, OpenMP, #pragma omp parallel for)", "label": "(12, TBB, g.reset(tbb::flow::rf_clear_edges);); (4, TBB, tbb::flow::queue_node<int> qout(g);); (5, TBB, tbb::flow::make_edge(on,qout);); (0, TBB, tbb::flow::graph g;)"}
{"predict": "(28, CUDA, cudaDeviceSynchronize();)", "label": "(34, CUDA, CSC(cudaGetLastError(), STATUS_FAILURE);)"}
{"predict": "(5, OpenMP, #pragma omp parallel for reduction(+:tmp2)) (10, OpenMP, #pragma omp parallel for reduction(+:tmp))", "label": "(8, OpenMP, #pragma omp target teams distribute parallel for reduction(+:tmp) map(A, N, M) map(tofrom:M))"}
{"predict": "(2, OpenMP, #pragma omp parallel for simd)", "label": "(12, OpenMP, #pragma omp for simd collapse (foobool(1) > 0 ? 1 : 2)); (8, OpenMP, #pragma omp for simd collapse (2+2))); (-1, OpenMP, #pragma omp for simd collapse); (16, OpenMP, #pragma omp for simd collapse (S1)); (-1, OpenMP, #pragma omp for simd collapse (); (0, OpenMP, #pragma omp for simd collapse ()); (26, OpenMP, #pragma omp for simd collapse (2)); (10, OpenMP, #pragma omp for simd collapse (foobool(argc)), collapse (true), collapse (-5)); (-1, OpenMP, #pragma omp for simd collapse (4); (21, OpenMP, #pragma omp for simd collapse(collapse(tmain<int, char, -1, -2>(argc, argv))"}
{"predict": "(7, MPI, MPI_Bcast(value, MPI_MAX_INFO_VAL + 1, MPI_CHAR, 0, MPI_COMM_WORLD);)", "label": "(35, MPI, MPI_Bcast(&tmp_val, 1, MPI_INT, 0, fd->comm);)"}
{"predict": "(7, MPI, const int send_size = make_message_size(world_rank);)\n(14, MPI, send_buffer[i] = Vector(4);)\n(21, MPI, const int recv_size = make_message_distance(world_size, message_padding);)\n(28, MPI, recv_buffer.resize(recv_size, Vector(4, -1));)\n(33, MPI, recv_sizes[rank] = make_message_size(rank);)\n(34, MPI, recv_offsets[rank] = make_message_distance(rank, message_padding);)\n(37, MPI, mpi_world_communicator.AllGatherv(send_buffer, recv_buffer, recv_sizes, recv_offsets);)\n(61, MPI, const auto& return_buffer = mpi_world_communicator.AllGatherv(send_buffer);)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(3, OpenMP, #pragma omp parallel for collapse(2))", "label": "(1, TBB, tbb::task_arena sequential_arena(1);)"}
{"predict": "ElementsPerLine + srcMargin);\n117:     const size_t outBufferSize = elementSize\n118:         * (totalLines * numElementsPerLine + dstMargin);\n119:     const size_t totalBufferSize = inBufferSize + outBufferSize;\n120: \n121:     cl_mem inBuffer = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR,\n122:                                     inBufferSize, (void *)srcData, &error);\n123:     test_error(error, \"Unable to create input buffer\");\n124: \n125:     cl_mem outBuffer = clCreateBuffer(context, CL_MEM_WRITE_ONLY,\n126:                                      outBufferSize, NULL, &error);\n127:     test_error(error, \"Unable to create output buffer\");\n128: \n129:     cl_mem localBuffer = clCreateBuffer(context, CL_MEM_READ_WRITE,\n130:                                         localBufferSize, NULL, &error);\n131:     test_error(error, \"Unable to create local buffer\");\n132: \n133:     size_t globalWorkSize[3] = { totalLines, 1, 1 };\n134:     size_t localWorkSize[3] = { localWorkgroupSize, 1, 1 };\n135: \n136:     cl_event kernelEvent;\n137:     error = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, globalWorkSize,\n138:                                    localWorkSize, 0, NULL, &kernelEvent);\n139:     test_error(error, \"Unable to enqueue kernel\");\n140: \n141:     clWaitForEvents(1, &kernelEvent);\n142:     clReleaseEvent(kernelEvent);\n143: \n144:     error = clEnqueueReadBuffer(queue, outBuffer, CL_TRUE, 0, outBufferSize,\n145:                                 outData, 0, NULL, NULL);\n146:     test_error(error, \"Unable to read output buffer\");\n147: \n148:     clReleaseMemObject(inBuffer);\n149:     clReleaseMemObject(outBuffer);\n150:     clReleaseMemObject(localBuffer);\n151: \n152:     return 0;\n153: }", "label": "(169, OpenCL, error = clSetKernelArg(kernel, 0, sizeof(streams[0]), &streams[0]);); (170, OpenCL, error = clSetKernelArg(kernel, 1, sizeof(streams[1]), &streams[1]);); (184, OpenCL, error = clSetKernelArg(kernel, 7, sizeof(dstStride), &dstStride);); (170, OpenCL, error = clSetKernelArg(kernel, 2, localBufferSize, NULL);); (180, OpenCL, error = clSetKernelArg(kernel, 6, sizeof(srcStride), &srcStride);)"}
{"predict": "(12, OpenMP, #pragma omp parallel for private(err))", "label": "(11, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(9, CUDA, warp_val = warp_reduction(val);)", "label": "(4, CUDA, __shared__ float shared_val[32];)"}
{"predict": "(38, CUDA, cublasSetStream(cublasH, stream);)", "label": "(60, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (78, CUDA, CUDA_CHECK(cudaDeviceReset());); (75, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (40, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (41, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_y), sizeof(data_type) * y.size()));); (39, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_x), sizeof(data_type) * x.size()));); (67, CUDA, CUDA_CHECK(cudaFree(d_y));); (65, CUDA, CUDA_CHECK(cudaFree(d_x));); (63, CUDA, CUDA_CHECK(cudaFree(d_A));); (30, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(11, OpenMP, #pragma omp parallel for private(p)) (22, OpenMP, #pragma omp parallel for private(p))", "label": "(37, TBB, tbb::flow::make_edge(source2, tbb::flow::input_port<1>(merge));); (35, TBB, tbb::flow::make_edge(source1, tbb::flow::input_port<0>(merge));); (36, TBB, tbb::flow::make_edge(merge, output);); (-2, TBB, tbb::flow::graph g;)"}
{"predict": "(40, CUDA, CHECK(cudaEventCreate(&start));) (41, CUDA, CHECK(cudaEventCreate(&stop));) (54, CUDA, CHECK(cudaEventRecord(start));) (72, CUDA, CHECK(cudaEventRecord(start));) (90, CUDA, CHECK(cudaEventRecord(start));)", "label": "(37, CUDA, CHECK(cudaMalloc(&d_matrix_org, M));); (37, CUDA, CHECK(cudaMalloc(&d_matrix_res, M));); (37, CUDA, CHECK(cudaMemcpy(d_matrix_org, h_matrix_org, M, cudaMemcpyDefault));); (42, CUDA, CHECK(cudaEventCreate(&stop));); (50, CUDA, CHECK(cudaEventRecord(stop));); (46, CUDA, CHECK(cudaMemcpy(h_matrix_res, d_matrix_res, M, cudaMemcpyDefault));); (41, CUDA, cudaEventQuery(start);); (48, CUDA, CHECK(cudaEventSynchronize(stop));); (48, CUDA, CHECK(cudaEventElapsedTime(&curr_time, start, stop));); (86, CUDA, CHECK(cudaFree(d_matrix_org));); (86, CUDA, CHECK(cudaFree(d_matrix_res));); (35, CUDA, CHECK(cudaEventRecord(start));); (32, CUDA, CHECK(cudaEventCreate(&start));); (3, CUDA, CHECK(cudaMemcpyFromSymbol(&SIZE, c_TILE_DIM, sizeof(int)));)"}
{"predict": "(34, OpenCL, OCL_ENQUEUE_KERNEL(k0, 1, NULL, globals, locals, 0, NULL, NULL);) (35, OpenCL, OCL_ENQUEUE_KERNEL(k1, 1, NULL, globals, locals, 0, NULL, NULL);)", "label": "(55, OpenCL, clReleaseProgram(program);); (52, OpenCL, clReleaseKernel(k0);); (52, OpenCL, clReleaseKernel(k1);); (11, OpenCL, cl_kernel k1 = clCreateKernel(program, \"compiler_program_global1\", &err);); (8, OpenCL, cl_kernel k0 = clCreateKernel(program, \"compiler_program_global0\", &err);)"}
{"predict": "(13, OpenMP, #pragma omp parallel for private(k))", "label": "(10, OpenMP, #pragma omp target map(tofrom: a[0:100])); (12, OpenMP, #pragma omp simd)"}
{"predict": "\" << elements_per_thread << std::endl;\n97:         std::cout << \"FFTs per block: \" << ffts_per_block << std::endl;\n98:         report_time_and_performance(\"cuFFT\", time_cufft);\n99:     #ifdef CUFFTDX_EXAMPLES_CUFFT_CALLBACK\n100:         report_time_and_performance(\"cuFFT Callback\", time_cufft_cb);\n101:     #endif\n102:     }\n103: \n104:     // Free memory\n105:     CUDA_CHECK_AND_EXIT(cudaFree(device_buffer));\n106:     CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());\n107: }\n\nOutput:", "label": "(66, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&device_buffer, input_size_bytes));); (56, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamCreate(&stream));); (80, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(device_buffer));); (67, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (65, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(device_buffer, input.data(), input_size_bytes, cudaMemcpyHostToDevice));)"}
{"predict": "(5, OpenMP, #pragma omp parallel for simd;)\n(10, OpenMP, #pragma omp parallel for simd;)\n(15, OpenMP, #pragma omp parallel for simd;)\n(20, OpenMP, #pragma omp parallel for simd;)", "label": "(12, OpenMP, #pragma omp distribute parallel for simd firstprivate(x);); (5, OpenMP, #pragma omp distribute parallel for simd;); (24, OpenMP, #pragma omp distribute parallel for simd, private(x);); (0, OpenMP, #pragma omp target); (15, OpenMP, #pragma omp distribute parallel for simd private(x);); (-1, OpenMP, #pragma omp teams)"}
{"predict": "_MAX_WORK_GROUP_SIZE\", __func__);\n87: \n88: \t//Enqueue the kernel\n89: \terr = clEnqueueNDRangeKernel(commQueues[0], kernel, 1, NULL, &global_size, &local_size, numWaitEvents, waitEvents, outEvents);\n90: \tif (err != CLRNG_SUCCESS)\n91: \t\treturn clrngSetErrorString(err, \"%s(): cannot enqueue kernel\", __func__);\n92: \n93: \t// Wait for the kernel to finish execution\n94: \terr = clFinish(commQueues[0]);\n95: \tif (err != CLRNG_SUCCESS)\n96: \t\treturn clrngSetErrorString(err, \"%s(): cannot finish kernel execution\", __func__);\n97: \n98: \treturn CLRNG_SUCCESS;\n99: }", "label": "(95, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(streams), &streams);); (105, OpenCL, clReleaseKernel(kernel);); (85, OpenCL, err = clGetDeviceInfo(dev, CL_DEVICE_MAX_WORK_GROUP_SIZE, sizeof(local_size), &local_size, NULL);); (59, OpenCL, err = clBuildProgram(program, 0, NULL, includes, NULL, NULL);); (103, OpenCL, clReleaseProgram(program);); (48, OpenCL, cl_program program = clCreateProgramWithSource(ctx, 4, sources, NULL, &err);); (96, OpenCL, err = clEnqueueNDRangeKernel(commQueues[0], kernel, 1, NULL, &streamCount, &local_size, numWaitEvents, waitEvents, outEvents);); (89, OpenCL, err |= clSetKernelArg(kernel, 1, sizeof(number_count_per_stream), &number_count_per_stream);); (89, OpenCL, err |= clSetKernelArg(kernel, 2, sizeof(outBuffer), &outBuffer);); (68, OpenCL, cl_kernel kernel = clCreateKernel(program, \"fillBufferU01\", &err);)"}
{"predict": "(7, OpenMP, #pragma omp parallel for)", "label": "(3, OpenMP, #pragma omp target)"}
{"predict": "(27, OpenMP, #pragma omp parallel for private(i, j) reduction(min: sum2))", "label": "(28, OpenMP, #pragma omp teams reduction(+:sum)); (25, OpenMP, #pragma omp target map(tofrom: b, sum, t) map(alloc: a)); (28, OpenMP, #pragma omp distribute parallel for reduction(+:sum)); (30, OpenMP, t[i] = omp_get_team_num();)"}
{"predict": "}\n92: \n93:   //\n94:   // BEGIN TILING\n95:   //\n96:   CUTE_UNROLL\n97:   for (int pipe = 0; pipe < K_PIPE_MAX; ++pipe) {\n98:     int pipe_idx = cute::elect_one_sync();\n99:     int tile = k_tile + pipe_idx;\n100:     if (tile < k_tile_count) {\n101:       //\n102:       // Prefetch A\n103:       //\n104:       prefetch<fetch::rw, fetch::tma>(tAgA, tAsA, tile, warp_idx, lane_predicate);\n105:       //\n106:       // Prefetch B\n107:       //\n108:       prefetch<fetch::rw, fetch::tma>(tBgB, tBsB, tile, warp_idx, lane_predicate);\n109:       //\n110:       // Barrier: Wait for the prefetch to complete\n111:       //\n112:       ProducerBarType::wait(&producer_mbar[pipe], 1);\n113:       //\n114:       // Perform MMA\n115:       //\n116:       mma(gC, sA, sB, alpha, beta);\n117:       //\n118:       // Barrier: Wait for the MMA to complete\n119:       //\n120:       ConsumerBarType::wait(&consumer_mbar[pipe], 128);\n121:     }\n122:   }\n123: \n124:   //\n125:   // BEGIN TILING\n126:   //\n127:   CUTE_UNROLL\n128:   for (int pipe = 0; pipe < K_PIPE_MAX; ++pipe) {\n129:     int pipe_idx = cute::elect_one_sync();\n130:     int tile = k_tile + pipe_idx;\n131:     if (tile < k_tile_count) {\n132:       //\n133:       // Prefetch A\n134:       //\n135:       prefetch<fetch::rw, fetch::tma>(tAgA, tAsA, tile, warp_idx, lane_predicate);\n13", "label": "(38, CUDA, extern __shared__ char shared_memory[];)"}
{"predict": "(7, MPI, auto const& worldComm = tensorrt_llm::mpi::MpiComm::world();)\n(9, MPI, worldComm.send(&mpiId, 1, mpi::MpiType::kUINT64, fastLogitsInfo.draftParticipantId, kMPI_SPEC_DEC_ID_TAG);)\n(10, MPI, worldComm.send(&fastLogitsInfo.draftRequestId, 1, mpi::MpiType::kUINT64, fastLogitsInfo.draftParticipantId, kMPI_SPEC_DEC_DATA_TAG);)\n(14, MPI, worldComm.mprobe(fastLogitsInfo.draftParticipantId, kMPI_SPEC_DEC_ID_TAG, &msg, &status);)\n(18, MPI, TLLM_CHECK(mpiId == FastLogitsMpiId::SEND_TENSOR);)\n(22, MPI, worldComm.mprobe(fastLogitsInfo.draftParticipantId, kMPI_SPEC_DEC_DATA_TAG, &msg, &status);)\n(27, MPI, auto const logitsDtype = modelConfig.getLogitsDtype();)\n(30, MPI, auto tensor = tensorrt_llm::runtime::BufferManager::pinnedPool(runtime::ITensor::makeShape({dims[0], dims[1]}), logitsDtype);)\n(33, MPI, worldComm.mprobe(fastLogitsInfo.draftParticipantId, kMPI_SPEC_DEC_DATA_TAG, &msg, &status);)", "label": "(28, MPI, MPICHECK(MPI_Mrecv(&dims, count, MPI_INT64_T, &msg, &status));); (36, MPI, MPICHECK(MPI_Get_count(&status, MPI_UINT8_T, &count));); (40, MPI, MPICHECK(MPI_Mrecv(tensor->data(), count, MPI_UINT8_T, &msg, &status));); (16, MPI, MPICHECK(MPI_Mrecv(&mpiId, count, MPI_UINT64_T, &msg, &status));); (20, MPI, MPICHECK(MPI_Get_count(&status, MPI_INT64_T, &count));); (11, MPI, MPICHECK(MPI_Get_count(&status, MPI_UINT64_T, &count));)"}
{"predict": "(28, CUDA, __syncthreads();)", "label": "(15, CUDA, extern __shared__ float sdata[];)"}
{"predict": "(8, MPI, MPI_Init(&argc, &argv);)", "label": "(11, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(17, OpenCL, clKernelWrapper kernel = clCreateKernel(prog, \"kernel_name\", &error);); (29, OpenCL, clFinish(queue);)", "label": "(15, OpenCL, clKernelWrapper kernel = clCreateKernel(prog, \"usersemantic_test\", &error);); (22, OpenCL, error |= clSetKernelArg(kernel, 0, sizeof(dst), &dst);)"}
{"predict": "(14, CUDA, bitonicMergeElementaryIntervalsKernel<1U><<<mergePairs, SAMPLE_STRIDE>>>(d_DstKey, d_DstVal, d_SrcKey, d_SrcVal, d_LimitsA, d_LimitsB, stride, N);)\n(17, CUDA, bitonicMergeElementaryIntervalsKernel<0U><<<mergePairs, SAMPLE_STRIDE>>>(d_DstKey, d_DstVal, d_SrcKey, d_SrcVal, d_LimitsA, d_LimitsB, stride, N);)", "label": "(15, CUDA, getLastCudaError(\"mergeElementaryIntervalsKernel<1> failed\\n\");); (19, CUDA, getLastCudaError(\"mergeElementaryIntervalsKernel<0> failed\\n\");)"}
{"predict": "A, b, ldb);\n109: \n110:     /* Initialize the solution vector. */\n111:     if (!(berr=doubleMalloc_dist(nrhs))) ABORT(\"Malloc fails for berr[]\");\n112:     for (i=0; i<nrhs; ++i) berr[i] = 0.;\n113: \n114:     /* Set up scaling and permutation structures. */\n115:     dScalePermstructInit(m, n, &ScalePermstruct);\n116:     dGetScalePermstruct(&ScalePermstruct, &A, NULL, NULL);\n117: \n118:     /* Set up LU structures. */\n119:     dLUstructInit(&LUstruct);\n120:     \n121:     /* ------------------------------------------------------------\n122:        PERFORM THE LU DECOMPOSITION.\n123:        ------------------------------------------------------------*/\n124:     superlu_dist_options_t options;\n125:     SuperLUStat_t stat;\n126:     dgssv_dist(&options, &A, &ScalePermstruct, &LUstruct, grid, b, ldb, xtrue, ldx, berr, &stat);\n127:     \n128:     /* ------------------------------------------------------------\n129:        GET THE LU DECOMPOSITION STATISTICS.\n130:        ------------------------------------------------------------*/\n131:     superlu_gridexit(&grid);\n132:     \n133:     /* Print the solution. */\n134:     if ( !iam ) {\n135: \tdPrintSol_dist(n, nrhs, xtrue, ldx);\n136: \tdPrintBerr_dist(n, nrhs, berr, ldb);\n137:     }\n138:     \n139:     /* ------------------------------------------------------------\n140:        DEALLOCATE WORKSPACE.\n141:        ------------------------------------------------------------*/\n142:     SUPERLU_FREE(a);\n143:     SUPERLU_FREE(asub);\n144:     SUPERLU_FREE(xa);\n145:     SUPERLU_FREE(b);\n146:     SUPERLU_FREE(xtrue);\n147:     SUPERLU_FREE(berr);\n148:     SUPERLU_FREE(ScalePermstruct.perm_r);\n149:     SUPERLU_FREE(ScalePermstruct.perm_c);\n150:", "label": "(87, MPI, MPI_Bcast( asub, nnz, mpi_int_t,  0, grid.comm );); (224, MPI, MPI_Finalize();); (83, MPI, MPI_Bcast( &nnz, 1,   mpi_int_t,  0, grid.comm );); (83, MPI, MPI_Bcast( a,    nnz, MPI_DOUBLE, 0, grid.comm );); (22, MPI, MPI_Init( &argc, &argv );); (83, MPI, MPI_Bcast( xa,   n+1, mpi_int_t,  0, grid.comm );); (78, MPI, MPI_Bcast( &n,   1,   mpi_int_t,  0, grid.comm );); (76, MPI, MPI_Bcast( &m,   1,   mpi_int_t,  0, grid.comm );)"}
{"predict": "(10, MPI, MPI_Bcast(buf, MAX_CNT * sizeof(int), MPI_INT, 0, MPI_COMM_WORLD);)", "label": "(26, MPI, MPI_Isend(buf, cnt, MPI_INT, thread_num, tag, MPI_COMM_WORLD, &r[thread_num - 1]);); (36, MPI, t = MPI_Wtime() - t;); (21, MPI, t = MPI_Wtime();); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (29, MPI, MPI_Waitall(wsize - 1, r, MPI_STATUSES_IGNORE);); (23, MPI, MPI_Wait(&r[thread_num - 1], MPI_STATUS_IGNORE);)"}
{"predict": "(2, OpenMP, #pragma omp parallel for private(i) reduction(+:s))", "label": "(8, OpenMP, #pragma omp target parallel for simd order(); (13, OpenMP, #pragma omp target parallel for simd order(concurrent); (15, OpenMP, #pragma omp target parallel for simd order(concurrent)); (2, OpenMP, #pragma omp target parallel for simd order); (-2, OpenMP, #pragma omp target parallel for simd lastprivate(s) firstprivate(s)); (6, OpenMP, #pragma omp target parallel for simd order(none)"}
{"predict": "(40, CUDA, block_radix_sort_t<<<gridSize, blockSize>>>(temp_storage, thread_keys[threadIdx.x], decomposer_t{}, begin_bit, end_bit);)", "label": "(7, CUDA, __shared__ block_radix_sort_t::TempStorage temp_storage;)"}
{"predict": "(27, CUDA, nvcv::Tensor inTensor = nvcv::util::CreateTensor(batches, width, height, format, stream);) (32, CUDA, nvcv::Tensor outTensor = nvcv::util::CreateTensor(batches, width, height, format, stream);) (61, CUDA, cudaMemcpyAsync(inData->ptr(), inVec.data(), inBufSize, cudaMemcpyHostToDevice, stream);) (65, CUDA, averageBlurOp(stream, inTensor, outTensor, kernelSize, kernelAnchor, borderMode);) (71, CUDA, cudaMemcpyAsync(outVec.data(), outData->ptr(), outBufSize, cudaMemcpyDeviceToHost, stream);)", "label": "(68, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (2, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (59, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(inData->basePtr(), inVec.data(), inBufSize, cudaMemcpyHostToDevice));); (72, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(testVec.data(), outData->basePtr(), outBufSize, cudaMemcpyDeviceToHost));); (65, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamDestroy(stream));)"}
{"predict": "(30, CUDA, cudaMalloc(&dev_max, SEQLEN * sizeof(FP));) (31, CUDA, cudaMalloc(&dev_denom, SEQLEN * sizeof(FP));) (40, CUDA, cudaMemcpy(host_max, dev_max, SEQLEN * sizeof(FP), cudaMemcpyDeviceToHost);) (41, CUDA, cudaMemcpy(host_denom, dev_denom, SEQLEN * sizeof(FP), cudaMemcpyDeviceToHost);) (43, CUDA, cudaFree(dev_max);) (44, CUDA, cudaFree(dev_denom);)", "label": "(26, CUDA, CUDA_CHECK(cudaMalloc((void **)&dev_max, sizeof(FP) * SEQLEN * DIM));); (45, CUDA, cudaFree(dev_max);); (28, CUDA, CUDA_CHECK(cudaMemcpy(dev_denom, host_denom, sizeof(FP) * SEQLEN * DIM, cudaMemcpyHostToDevice));); (24, CUDA, CUDA_CHECK(cudaMalloc((void **)&dev_denom, sizeof(FP) * SEQLEN * DIM));); (24, CUDA, CUDA_CHECK(cudaMalloc((void **)&QK, sizeof(FP) * SEQLEN * SEQLEN));); (24, CUDA, CUDA_CHECK(cudaMemcpy(dev_max, host_max, sizeof(FP) * SEQLEN * DIM, cudaMemcpyHostToDevice));); (39, CUDA, cudaFree(QK);); (40, CUDA, cudaFree(dev_denom);)"}
{"predict": "(14, OpenMP, #pragma omp parallel for schedule(runtime) reduction(+:total) linear(idx:1))", "label": "(19, OpenMP, #pragma omp for schedule(runtime) reduction(+:total) linear(idx:1)); (13, OpenMP, thread_num = omp_get_thread_num();); (15, OpenMP, #pragma omp master); (7, OpenMP, #pragma omp parallel shared(data, std::cerr, total, idx) default(none))"}
{"predict": "(74, MPI, PMPI_Type_free(&types[j]);)", "label": "(62, MPI, MPI_Type_commit(newtype);)"}
{"predict": "(13, CUDA, cudaMalloc((void **)&cpu_inputs, sizeof(typename types<params>::input_t) * count);) (14, CUDA, cudaMalloc((void **)&gpu_inputs, sizeof(typename types<params>::input_t) * count);) (16, CUDA, cudaMalloc((void **)&compare, sizeof(typename types<params>::output_t) * count);) (17, CUDA, cudaMalloc((void **)&cpu_outputs, sizeof(typename types<params>::output_t) * count);) (19, CUDA, cudaMemcpy(gpu_inputs, cpu_inputs, sizeof(typename types<params>::input_t) * count, cudaMemcpyHostToDevice);) (20, CUDA, cudaMemcpy(gpu_outputs, cpu_outputs, sizeof(typename types<params>::output_t) * count, cudaMemcpyHostToDevice);) (22, CUDA, cudaMemcpy(cpu_outputs, gpu_outputs, sizeof(typename types<params>::output_t) * count, cudaMemcpyDeviceToHost);) (57, CUDA, cudaFree(cpu_inputs);) (58, CUDA, cudaFree(gpu_inputs);) (59, CUDA, cudaFree(compare);) (60, CUDA, cudaFree(cpu_outputs);)", "label": "(21, CUDA, $GPU(cudaMemcpy(compare, gpu_outputs, sizeof(typename types<params>::output_t)*count, cudaMemcpyDeviceToHost));); (15, CUDA, $GPU(cudaMalloc((void **)&gpu_outputs, sizeof(typename types<params>::output_t)*count));); (15, CUDA, $GPU(cudaMemset(gpu_outputs, 0, sizeof(typename types<params>::output_t)*count));); (58, CUDA, $GPU(cudaFree(gpu_outputs));)"}
{"predict": "(11, CUDA, cudaFree(d_data);)", "label": "(38, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (40, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": ");\n116: \n117:          if(tid==0){\n118:              // printf(\"before trsm %i %i %i %i %i %i %i %i\\n\", threadIdx_x, blockIdx_x, grid->npcol, nsupers,myrow,krow,bid,tid);\n119:              trsm(&alpha, &Uinv_bc_dat[Uinv_bc_offset[lk]], &x[rel*nrhs], &nrhs, &knsupc, &knsupc, &Uinv);\n120:              // printf(\"after trsm %i %i %i %i %i %i %i %i\\n\", threadIdx_x, blockIdx_x, grid->npcol, nsupers,myrow,krow,bid,tid);\n121:          }\n122:          __syncthreads();\n123:          //   if(tid==0)\n124:          //   printf(\"after trsm %d %d \\n\",threadIdx_x, blockIdx_x);\n125: \n126:          for(i=0; i<nrhs; i++){\n127:              if(myrow==krow){\n128:                  for(l=0; l<knsupc; l++){\n129:                      if(mycol+l==rel){\n130:                          for(j=0; j<nrhs; j++){\n131:                              x[(rel+j)*nrhs+i] = zero;\n132:                          }\n133:                      }\n134:                  }\n135:              }\n136:          }\n137:          __syncthreads();\n138:          //   if(tid==0)\n139:          //   printf(\"before broadcast %d %d \\n\",threadIdx_x, blockIdx_x);\n140: \n141:          if(tid==0){\n142:              for(i=0; i<knsupc; i++){\n143:                  rC[i][0] = x[rel*nrhs+i];\n144:              }\n145:          }\n146:          __syncthreads();\n147:          //   if(tid==0)\n148:          //   printf(\"after broadcast %d %d \\n\",threadIdx_x, blockIdx_x);\n1", "label": "(45, CUDA, __shared__ doublecomplex temp2[MAXSUPER];)"}
{"predict": "(6, OpenCL, auto buffer = clCreateBuffer(clContext.get(), CL_MEM_ALLOC_HOST_PTR, 123, nullptr, &retVal);) (10, OpenCL, retVal = clReleaseMemObject(buffer);)", "label": "(10, OpenCL, auto mappedPtr = clEnqueueMapBuffer(&cmdQ, buffer.get(), CL_TRUE, CL_MAP_READ, 0, 1, 0, nullptr, nullptr, &retVal);); (12, OpenCL, retVal = clEnqueueUnmapMemObject(&cmdQ, buffer.get(), mappedPtr, 0, nullptr, nullptr);)"}
{"predict": "(11, OpenMP, #pragma omp parallel for)", "label": "(24, OpenMP, #pragma omp target parallel firstprivate (a, b, c, d, f)); (43, OpenMP, #pragma omp target parallel shared(i)); (46, OpenMP, #pragma omp target parallel firstprivate(j)); (19, OpenMP, #pragma omp target parallel firstprivate (S1)); (6, OpenMP, #pragma omp target parallel firstprivate (); (11, OpenMP, #pragma omp target parallel firstprivate (argc,); (28, OpenMP, #pragma omp target parallel firstprivate(S2::S2s)); (35, OpenMP, #pragma omp target parallel private(i), firstprivate(i)); (12, OpenMP, #pragma omp target parallel firstprivate (argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (29, OpenMP, #pragma omp target parallel firstprivate(e, g)); (30, OpenMP, #pragma omp target parallel firstprivate(h, B::x)); (19, OpenMP, #pragma omp target parallel firstprivate(ca, z)); (16, OpenMP, #pragma omp target parallel firstprivate(ba) allocate(omp_thread_mem_alloc: ba)); (-1, OpenMP, #pragma omp target parallel firstprivate ()); (36, OpenMP, #pragma omp target parallel firstprivate(m)); (-1, OpenMP, #pragma omp target parallel firstprivate (argc); (-8, OpenMP, #pragma omp target parallel firstprivate); (29, OpenMP, #pragma omp target parallel firstprivate(i)); (18, OpenMP, #pragma omp target parallel firstprivate(S2::S2sc)); (7, OpenMP, #pragma omp target parallel firstprivate (argv[1])); (-2, OpenMP, #pragma omp target parallel firstprivate (argc > 0 ? argv[1] : argv[2])); (11, OpenMP, #pragma omp target parallel firstprivate(da))"}
{"predict": "(19, TBB, start = tbb::tick_count::now();)", "label": "(8, TBB, start = tbb::tick_count::now();); (0, TBB, tbb::tick_count now;); (8, TBB, now = tbb::tick_count::now();)"}
{"predict": "(7, OpenMP, #pragma omp parallel for)", "label": "(17, OpenMP, #pragma omp barrier); (2, OpenMP, #pragma omp parallel); (3, OpenMP, int me = omp_get_thread_num();)"}
{"predict": "seed);\n116:             if( retCode )\n117:                 return retCode;\n118:         }\n119:     }\n120:     else\n121:     {\n122:         // Test a single image size\n123:         imageInfo.width = 256;\n124:         imageInfo.arraySize = 1;\n125:         imageInfo.rowPitch = imageInfo.slicePitch = imageInfo.width * pixelSize;\n126:         log_info(\"Testing 256 x 1\\n\");\n127:         if(gTestMipmaps)\n128:             imageInfo.num_mip_levels = (size_t)random_in_range(2, (compute_max_mip_levels(imageInfo.width, 0, 0)-1), seed);\n129:         if( gDebugTrace )\n130:             log_info( \"   at size 256,1\\n\" );\n131:         int retCode = test_read_image(context, queue, kernel, &imageInfo,\n132:                                       imageSampler, floatCoords, outputType, seed);\n133:         if( retCode )\n134:             return retCode;\n135:     }\n136:     return 0;\n137: }", "label": "(27, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_IMAGE_MAX_ARRAY_SIZE, sizeof( maxArraySize ), &maxArraySize, NULL );); (28, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_GLOBAL_MEM_SIZE, sizeof( memSize ), &memSize, NULL );); (26, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_MAX_MEM_ALLOC_SIZE, sizeof( maxAllocSize ), &maxAllocSize, NULL );); (23, OpenCL, error = clGetDeviceInfo( device, CL_DEVICE_IMAGE2D_MAX_WIDTH, sizeof( maxWidth ), &maxWidth, NULL );)"}
{"predict": "(33, CUDA, for (int idim = 0; idim < DIM; idim++) { grads_now[iw * DIM + idim] = sum[idim * COLBS]; });", "label": "(11, CUDA, __shared__ uninitialized_array<T, SUM_SIZE> sum;)"}
{"predict": "(14, OpenMP, #pragma omp parallel for private(i))", "label": "(10, TBB, static const size_t number_of_threads = 8 * tbb::task_scheduler_init::default_num_threads();)"}
{"predict": "(11, CUDA, __syncthreads();)", "label": "(7, CUDA, __shared__ typename BlockReduce::TempStorage temp_storage;)"}
{"predict": "(14, OpenMP, #pragma omp parallel for)", "label": "(2, TBB, using namespace tbb::internal;)"}
{"predict": "(4, OpenMP, #pragma omp parallel if(argc))\n(10, OpenMP, #pragma omp parallel if(argc))\n(16, OpenMP, #pragma omp parallel if(argc))\n(20, OpenMP, #pragma omp parallel if(argc))\n(26, OpenMP, #pragma omp parallel if(argc))\n(30, OpenMP, #pragma omp parallel if(argc))\n(34, OpenMP, #pragma omp parallel if(argc))\n(36, OpenMP, #pragma omp parallel if(argc))\n(40, OpenMP, #pragma omp parallel if(argc))", "label": "(2, OpenMP, #pragma omp taskyield allocate(argc)); (3, OpenMP, #pragma omp taskyield untied); (3, OpenMP, #pragma omp taskyield)"}
{"predict": "(16, CUDA, return hiop::cuda::get_val_from_device(ret_host, 1);)", "label": "(14, CUDA, cudaError_t cuerr = cudaMemcpy(ret_host, ret_ptr, (1)*sizeof(double), cudaMemcpyDeviceToHost);); (3, CUDA, double* d_ptr = thrust::raw_pointer_cast(dv_ptr);); (7, CUDA, thrust::device_ptr<double> ret_dev_ptr = thrust::min_element(thrust::device, dv_ptr, dv_ptr+n);); (10, CUDA, double *ret_ptr = thrust::raw_pointer_cast(ret_dev_ptr);); (-1, CUDA, thrust::device_ptr<double> dv_ptr = thrust::device_malloc(n*sizeof(double));); (14, CUDA, thrust::device_free(dv_ptr);)"}
{"predict": "(11, MPI, MPI_Init(&argc, &argv);)", "label": "(14, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(15, MPI, MPI_Scatter(a, 100, MPI_INT, MPI_IN_PLACE, 100, MPI_INT, 0, MPI_COMM_WORLD);) (40, MPI, MPI_Gather(b, 100, MPI_INT, MPI_IN_PLACE, 100, MPI_INT, 0, MPI_COMM_WORLD);)", "label": "(25, MPI, MPI_Type_get_extent(MPI_INT, &tmp_lb, &sizeofint);); (39, MPI, MPI_Pack(a, 1, xpose, buffer, bufsize, &position, MPI_COMM_WORLD);); (55, MPI, MPI_Type_free(&row);); (27, MPI, MPI_Type_commit(&xpose);); (52, MPI, MPI_Type_free(&xpose);); (28, MPI, MPI_Pack_size(1, xpose, MPI_COMM_WORLD, &bufsize);); (23, MPI, MPI_Type_create_hvector(100, 1, sizeofint, row, &xpose);); (31, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (36, MPI, MPI_Unpack(buffer, bufsize, &position, b, 100 * 100, MPI_INT, MPI_COMM_WORLD);); (19, MPI, MPI_Type_vector(100, 1, 100, MPI_INT, &row);)"}
{"predict": "(12, CUDA, cudaFree(d_data);)", "label": "(34, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (36, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "etc.\n122:     ions.update();\n123:     els.update();\n124:     ions.updateCell();\n125:     els.updateCell();\n126:     ions.updateBoundary();\n127:     els.updateBoundary();\n128:     ions.updateCell();\n129:     els.updateCell();\n130:     ions.updateBoundary();\n131:     els.updateBoundary();\n132:     ions.updateCell();\n133:     els.updateCell();\n134:     ions.updateBoundary();\n135:     els.updateBoundary();\n136:     ions.updateCell();\n137:     els.updateCell();\n138:     ions.updateBoundary();\n139:     els.updateBoundary();\n140:     ions.updateCell();\n131:     els.updateCell();\n132:     ions.updateBoundary();\n133:     els.updateBoundary();\n134:     ions.updateCell();\n135:     els.updateCell();\n136:     ions.updateBoundary();\n137:     els.updateBoundary();\n138:     ions.updateCell();\n139:     els.updateCell();\n140:     ions.updateBoundary();\n141:     els.updateBoundary();\n142:     ions.updateCell();\n143:     els.updateCell();\n144:     ions.updateBoundary();\n145:     els.updateBoundary();\n146:     ions.updateCell();\n147:     els.updateCell();\n148:     ions.updateBoundary();\n149:     els.updateBoundary();\n150:     ions.updateCell();\n151:     els.updateCell();\n152:     ions.updateBoundary();\n153:     els.updateBoundary();\n154:     ions.updateCell();\n155:     els.updateCell();\n156:     ions.updateBoundary();\n157:     els.updateBoundary();\n158:     ions.updateCell();\n159:     els.updateCell();\n160:     ions.updateBoundary();\n161:     els.updateBoundary();\n162:     ions.updateCell();\n163:     els.updateCell();\n164:     ions.updateBoundary();\n165:     els.updateBoundary();\n166:     ions.updateCell();\n167:     els.updateCell();\n168:     ions.updateBoundary", "label": "(115, OpenMP, ncrews = omp_get_max_threads();); (98, OpenMP, const int np = omp_get_num_threads();); (189, OpenMP, #pragma omp barrier); (233, OpenMP, double dTotalThreads = omp_get_max_threads();); (268, OpenMP, double nMajorThreads  = omp_get_max_threads();); (137, OpenMP, #pragma omp parallel reduction(+ : vgh_t_loc, v_t_loc, my_vals)); (106, OpenMP, #pragma omp master); (278, OpenMP, cout << \"\\nMPI: \" << nmpi << \" and Threads: \" << omp_get_max_threads();); (92, OpenMP, const int ip = omp_get_thread_num();); (86, OpenMP, #pragma omp parallel reduction(+ : t0, tInit, ratio, vgh_t, val_t, nspheremoves, dNumVGHCalls))"}
{"predict": "(8)); // 4096/8=512\n112:     dim3 grid(S);\n113: \n114:     // warmup\n115:     for (int i = 0; i < 5; ++i)\n116:       elementwise_add_f16x8_kernel<<<grid, block>>>(a_device, b_device,\n117:                                                     c_device, N);\n118:     cudaDeviceSynchronize(); // synchronzie\n119: \n120:     cudaEvent_t start, stop;\n121:     float time;\n122:     cudaEventCreate(&start);\n123:     cudaEventCreate(&stop);\n124:     cudaEventRecord(start, 0);\n125: \n126:     for (int i = 0; i < R; ++i)\n127:       elementwise_add_f16x8_kernel<<<grid, block>>>(a_device, b_device,\n128:                                                     c_device, N);\n129:     cudaDeviceSynchronize(); // synchronzie\n130: \n131:     cudaEventRecord(stop, 0);\n132:     cudaEventSynchronize(stop);\n133:     cudaEventElapsedTime(&time, start, stop);\n134:     cudaEventDestroy(start);\n135:     cudaEventDestroy(stop);\n136:     printf(\"pack elementwise: %f ms\\n\", time / (float)R);\n137: \n138:     cudaMemcpy(c_host, c_device, N * sizeof(half), cudaMemcpyDeviceToHost);\n139:   }\n140: \n141:   free(a_host);\n142:   free(b_host);\n143:   free(c_host);\n144:   cudaFree(a_device);\n145:   cudaFree(b_device);\n146:   cudaFree(c_device);\n147: \n148:   return 0;\n149: }\n\nOutput:", "label": "(25, CUDA, cudaMalloc((void **)&c_device, N * sizeof(half));); (49, CUDA, cudaEventSynchronize(stop);); (50, CUDA, cudaEventDestroy(start);); (155, CUDA, cudaFree(a_device);); (32, CUDA, cudaDeviceSynchronize();); (155, CUDA, cudaFree(c_device);); (43, CUDA, cudaEventRecord(stop, 0);); (46, CUDA, cudaEventDestroy(stop);); (151, CUDA, cudaFree(b_device);); (33, CUDA, cudaEventRecord(start, 0);); (11, CUDA, cudaMemcpy(b_device, b_host, N * sizeof(half), cudaMemcpyHostToDevice);); (30, CUDA, cudaEventCreate(&stop);); (6, CUDA, cudaMalloc((void **)&b_device, N * sizeof(half));); (-2, CUDA, cudaMalloc((void **)&a_device, N * sizeof(half));); (37, CUDA, cudaEventElapsedTime(&time, start, stop);); (-1, CUDA, cudaMemcpy(a_device, a_host, N * sizeof(half), cudaMemcpyHostToDevice);); (24, CUDA, cudaEventCreate(&start);); (39, CUDA, cudaMemcpy(c_host, c_device, N * sizeof(half), cudaMemcpyDeviceToHost);)"}
{"predict": "(12, CUDA, x[n] = a; y[n] = b;)", "label": "(27, CUDA, CHECK(cudaMemPrefetchAsync(z, M, cudaCpuDeviceId, NULL));); (20, CUDA, CHECK(cudaMemPrefetchAsync(x, M, device_id, NULL));); (20, CUDA, CHECK(cudaMemPrefetchAsync(y, M, device_id, NULL));); (7, CUDA, CHECK(cudaMallocManaged((void **)&z, M));); (28, CUDA, CHECK(cudaFree(x));); (-2, CUDA, CHECK(cudaGetDevice(&device_id));); (2, CUDA, CHECK(cudaMallocManaged((void **)&x, M));); (26, CUDA, CHECK(cudaFree(y));); (26, CUDA, CHECK(cudaFree(z));); (14, CUDA, CHECK(cudaMemPrefetchAsync(z, M, device_id, NULL));); (19, CUDA, CHECK(cudaDeviceSynchronize());); (-2, CUDA, CHECK(cudaMallocManaged((void **)&y, M));)"}
{"predict": "(28, CUDA, glGenBuffers( 1, &bufferObj );)", "label": "(14, CUDA, HANDLE_ERROR( cudaGLSetGLDevice( dev ) );); (51, CUDA, HANDLE_ERROR( cudaGraphicsUnmapResources( 1, &resource, NULL ) );); (39, CUDA, HANDLE_ERROR( cudaGraphicsMapResources( 1, &resource, NULL ) );); (4, CUDA, HANDLE_ERROR( cudaChooseDevice( &dev, &prop ) );)"}
{"predict": "43:                indexing in both parts, the same relative indices are used. */\n144:             HYPRE_SStructGridSetNeighborPart(grid, part, nbor_part, b_ilower, b_iupper);\n145:          }\n146:       }\n147: \n148:       /* Processor 1 does not need to call this function since it does not\n149:          own any boxes that border a different part. */\n150: \n151:       /* Now we can build the graph */\n152:       HYPRE_SStructGraphCreateFromGrid(MPI_COMM_WORLD, grid, &graph);\n153: \n154:       /* Now we can define the stencils */\n155:       {\n156:          int ns = 2;\n157:          int stencil_indices[2][3] = {{0, -1, 0}, {0, 1, 0}};\n158: \n159:          /* Define a 5-point stencil */\n160:          HYPRE_SStructStencilCreate(ndim, 3, &stencil_5pt);\n161:          for (i = 0; i < ns; i++)\n162:             HYPRE_SStructStencilSetEntry(stencil_5pt, i, stencil_indices[i], 0, 0);\n163: \n164:          /* Define a 9-point stencil */\n165:          HYPRE_SStructStencilCreate(ndim, 3, &stencil_9pt);\n166:          for (i = 0; i < ns; i++)\n167:             HYPRE_SStructStencilSetEntry(stencil_9pt, i, stencil_indices[i], 0, 0);\n168:       }\n169: \n170:       /* Set the stencil on the graph */\n171:       {\n172:          int part = 0;\n173:          HYPRE_SStructGraphSetStencil(graph, part, 0, stencil_5pt);\n174:       }\n175: \n176:       /* Set the stencil on the graph */\n177:       {\n178:          int part = 1;\n179:          HYPRE_SStructGraphSetStencil(graph, part, 0", "label": "(19, MPI, MPI_Init(&argc, &argv);); (25, MPI, MPI_Finalize();); (19, MPI, MPI_Comm_size(MPI_COMM_WORLD, &num_procs);); (17, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myid);)"}
{"predict": "(5, OpenMP, #pragma omp parallel for)\n(10, OpenMP, #pragma omp parallel for)\n(13, OpenMP, #pragma omp parallel for)\n(14, OpenMP, #pragma omp parallel for)\n(18, OpenMP, #pragma omp parallel for)\n(21, OpenMP, #pragma omp parallel for)\n(26, OpenMP, #pragma omp parallel for)\n(31, OpenMP, #pragma omp parallel for)\n(33, OpenMP, #pragma omp parallel for)\n(37, OpenMP, #pragma omp parallel for)", "label": "(26, OpenMP, #pragma omp for private(int)); (10, OpenMP, #pragma omp for private(,); (35, OpenMP, #pragma omp for private(x)); (2, OpenMP, #pragma omp for private(); (-2, OpenMP, #pragma omp parallel); (26, OpenMP, #pragma omp for private(0)); (10, OpenMP, #pragma omp for private(, )); (34, OpenMP, #pragma omp for private(x, y)); (13, OpenMP, #pragma omp for private()); (36, OpenMP, #pragma omp for private(x, y, z))"}
{"predict": "(4, TBB, parallel_for(range(MinThread, MaxThread + 1), [](int p) { parallel_read_write_tests<int>(); })); (5, TBB, parallel_for(range(MinThread, MaxThread + 1), [](int p) { parallel_read_write_tests<float>(); }));", "label": "(7, TBB, test_reserving_nodes<tbb::flow::write_once_node, size_t>();); (3, TBB, tbb::task_scheduler_init init(p);); (8, TBB, test_extract_on_node<tbb::flow::write_once_node, int>();)"}
{"predict": "(9, MPI, MPI_Isend(buffer, buf_size, MPI_CHAR, 1, tag1, MPI_COMM_WORLD, &request1);) (11, MPI, MPI_Isend(buffer, buf_size, MPI_CHAR, 0, tag2, MPI_COMM_WORLD, &request2);)", "label": "(13, MPI, MPI_Isend(buffer, buf_size, MPI_BYTE, 1, tag1, MPI_COMM_WORLD, &request1);); (15, MPI, MPI_Wait(&request2, &status);); (12, MPI, MPI_Isend(buffer, buf_size, MPI_BYTE, 1, tag2, MPI_COMM_WORLD, &request2);); (16, MPI, MPI_Irecv(buffer, buf_size, MPI_BYTE, 0, tag1, MPI_COMM_WORLD, &request2);); (11, MPI, MPI_Wait(&request1, &status);); (13, MPI, MPI_Irecv(buffer, buf_size, MPI_BYTE, 0, tag2, MPI_COMM_WORLD, &request1);)"}
{"predict": "(13, CUDA, StopBenchmarkTiming();)\n(14, CUDA, Eigen::CudaStreamDevice stream;)\n(15, CUDA, Eigen::GpuDevice device(&stream);)\n(16, CUDA, BenchmarkSuite<Eigen::GpuDevice, float> suite(device, D1, D2, D3);)", "label": "(15, CUDA, cudaDeviceSynchronize();)"}
{"predict": "(13, OpenMP, #pragma omp parallel for private(i, j, k, row_i, row_c))", "label": "(12, OpenMP, #pragma omp parallel for private(i,j,k) num_threads(dtn(border, 1)))"}
{"predict": "(38, OpenMP, #pragma omp parallel for)", "label": "(36, OpenMP, #pragma omp target parallel for map(a, b))"}
{"predict": "(10, CUDA, int *loc = (int *)shared + threadIdx.x * input1_dims;); (12, CUDA, index2loc(index, temp_shape, input1_dims - 1, loc);); (15, CUDA, loc[dim] = 0;); (17, CUDA, int base = loc2index(loc, input1_shape, input1_dims);); (20, CUDA, for (int i = 0; i < length; i++) { total += input1[base + i * dim_stride]; });)", "label": "(4, CUDA, extern __shared__ int shared[];)"}
{"predict": "(10, CUDA, cudaMalloc(&d_a, DataSize);)", "label": "(32, OpenMP, omp_target_free(d_a, DefaultDeviceNum);); (23, OpenMP, #pragma omp target update to(p_a)); (12, OpenMP, d_a = (int *)omp_target_alloc(DataSize, DefaultDeviceNum);); (6, OpenMP, int DefaultDeviceNum = omp_get_default_device();); (14, OpenMP, omp_target_associate_ptr(p_a, d_a, DataSize, 0, DefaultDeviceNum);); (23, OpenMP, omp_target_disassociate_ptr(p_a, DefaultDeviceNum);)"}
{"predict": "(14, TBB, tbb::parallel_pipeline(n_tokens, ...));", "label": "(9, TBB, tbb::parallel_pipeline( n_tokens, one_filter, context... );); (5, TBB, tbb::filter<void, void> one_filter( filter_table[i], i_filter );)"}
{"predict": "(4, MPI, int comm_rank; MPI_Comm_rank(comm, &comm_rank);)", "label": "(13, MPI, MPI_Bcast(dims, 2, MPI_INT, 0, comm);); (2, MPI, MPI_Comm_rank(comm, &comm_rank);)"}
{"predict": "(7, printfQuda, printfQuda(\"CUDA Driver version = %d\\n\", driver_version);) (10, printfQuda, printfQuda(\"CUDA Runtime version = %d\\n\", runtime_version);) (22, CUDA, for (int i = 0; i < get_device_count(); i++) { logQuda(QUDA_SUMMARIZE, \"Found device %d: %s\\n\", i, deviceProp.name); }) (66, printfQuda, printfQuda(\"Using device %d: %s\\n\", dev, deviceProp.name);)", "label": "(10, CUDA, CHECK_CUDA_ERROR(cudaRuntimeGetVersion(&runtime_version));); (72, CUDA, CHECK_CUDA_ERROR(cudaSetDevice(dev));); (26, CUDA, CHECK_CUDA_ERROR(cudaGetDeviceProperties(&deviceProp, dev));); (3, CUDA, CHECK_CUDA_ERROR(cudaDriverGetVersion(&driver_version));); (20, CUDA, CHECK_CUDA_ERROR(cudaGetDeviceProperties(&deviceProp, i));)"}
{"predict": "{0};\n96:     SpanAttnV2<T>::GetRefWorkspaceBytes(outObj, ref_ws_bytes);\n97: #endif\n98: \n99:     std::vector<allspark::dim_t> shape_ws = {ws_bytes};\n100:     std::string ws_name = \"workspace\";\n101:     common::AddTensor(tensors, ws_name, asINT8);\n102: \n103:     std::vector<allspark::dim_t> shape_ref_ws = {ref_ws_bytes};\n104:     std::string ref_ws_name = \"ref_workspace\";\n105:     common::AddTensor(tensors, ref_ws_name, asINT8);\n106: \n107:     std::vector<void*> ws_ptrs;\n108:     ws_ptrs.push_back(ws_name.c_str());\n109:     ws_ptrs.push_back(ref_ws_name.c_str());\n110: \n111:     // host workspace\n112:     std::vector<void*> host_ws_ptrs;\n113:     host_ws_ptrs.push_back(host_ws);\n114:     host_ws_ptrs.push_back(nullptr);\n115: \n116:     // tensor buffers\n117:     std::vector<void*> tensor_ptrs;\n118:     tensor_ptrs.push_back(span_pool_name.c_str());\n119:     tensor_ptrs.push_back(out_name.c_str());\n120:     tensor_ptrs.push_back(q_name.c_str());\n121:     tensor_ptrs.push_back(k_span_name.c_str());\n122:     tensor_ptrs.push_back(v_span_name.c_str());\n123:     tensor_ptrs.push_back(ws_name.c_str());\n124:     tensor_ptrs.push_back(ref_ws_name.c_str());\n125: \n126:     // tensor buffers host\n127:     std::vector<void*> tensor_ptrs_host;\n128:     tensor_ptrs_host.push_back(span_pool_name.c_str());\n129:     tensor_ptrs_host.push_back(out_name.c_str());\n130:     tensor_ptrs_host.push_back(q_name.c_str());\n131:     tensor_ptrs_host.push_back(k_span_name.c_str());\n132:     tensor_ptrs_host.push_back(v_span_name.c_str());\n133:     tensor_ptrs_host.push_back(nullptr);\n134:     tensor_ptrs_host.push_back", "label": "(90, CUDA, ASSERT_EQ(cudaSuccess, cudaMallocHost(&host_ws, host_ws_bytes));); (297, CUDA, ASSERT_EQ(cudaSuccess, cudaEventDestroy(ref_start));); (230, CUDA, ASSERT_EQ(cudaSuccess, cudaEventElapsedTime(&ref_time, ref_start, ref_end));); (297, CUDA, ASSERT_EQ(cudaSuccess, cudaEventDestroy(out_start));); (297, CUDA, ASSERT_EQ(cudaSuccess, cudaEventDestroy(out_end));); (249, CUDA, ASSERT_EQ(cudaSuccess, cudaEventElapsedTime(&out_time, out_start, out_end));); (201, CUDA, ASSERT_EQ(cudaSuccess, cudaEventCreate(&ref_end));); (292, CUDA, ASSERT_EQ(cudaSuccess, cudaEventDestroy(ref_end));); (244, CUDA, ASSERT_EQ(cudaSuccess, cudaEventRecord(out_end, cuda_stream));); (199, CUDA, ASSERT_EQ(cudaSuccess, cudaEventCreate(&out_start));); (199, CUDA, ASSERT_EQ(cudaSuccess, cudaEventCreate(&out_end));); (220, CUDA, ASSERT_EQ(cudaSuccess, cudaEventSynchronize(ref_end));); (241, CUDA, ASSERT_EQ(cudaSuccess, cudaEventSynchronize(out_end));); (217, CUDA, ASSERT_EQ(cudaSuccess, cudaEventRecord(ref_end, cuda_stream));); (192, CUDA, ASSERT_EQ(cudaSuccess, cudaEventCreate(&ref_start));)"}
{"predict": "F) | 0x40000000) * 1333333333u);\n110:                 }\n111: \n112:                 d[x] = a ^\n113:                     t_fn0(x_0 & 0xff) ^\n114:                     t_fn1((x_0 >> 8) & 0xff) ^\n115:                     t_fn2((x_0 >> 16) & 0xff) ^\n116:                     t_fn3((x_0 >> 24));\n117:             }\n118: \n119:             if (ALGO == Algorithm::CN_HEAVY_TUBE) {\n120:                 /* the first shuffle is not needed */\n121:                 d[x] = shuffle<4>(sPtr,sub, d[x], sub + 1);\n122:             }\n123: \n124:             res = d[x] ^ d[1];\n125: \n126:             uint32_t y = (res >> 24) | (res << 8);\n127: \n128:             if (ALGO == Algorithm::CN_HEAVY_TUBE) {\n129:                 if (sub2 == 0) {\n130:                     storeGlobal32<uint32_t>(long_state + j, y);\n131:                 }\n132:             } else {\n133:                 storeGlobal32<uint32_t>(long_state + j, y);\n134:             }\n135:         }\n136:     }\n137: \n138:     if (ALGO == Algorithm::CN_HEAVY_TUBE) {\n139:         // state is stored after all ctx_b states\n140:         *(d_ctx_b + threads * 4 + thread) = idx0;\n141:     }\n142: }", "label": "(18, CUDA, __shared__ uint32_t sharedMemory[1024];); (29, CUDA, extern __shared__ uint32_t shuffleMem[];)"}
{"predict": "(6, MPI, int lerrs = 0;)\n(8, MPI, lerrs++;)\n(10, MPI, lerrs++;)\n(12, MPI, lerrs++;)", "label": "(6, MPI, MPI_Comm_get_attr(comm, lkey[i], &val_p, &flag);)"}
{"predict": "str, 0, 0, make_float4(0.0f, 190.0f, 255.0f, 255.0f));\n143: \n144: \t\t\t\t// unmap the texture\n145: \t\t\t\ttexture->Unmap();\n146: \n147: \t\t\t\t// display the texture\n148: \t\t\t\tdisplay->RenderTexture(texture, 0, 0, texture->GetWidth(), texture->GetHeight());\n149: \t\t\t}\n150: \t\t}\n151: \n152: \t\tdisplay->EndRender();\n153: \t\tdisplay->SwapBuffers();\n154: \t}\n155: \n156: \t// clean up\n157: \tdelete display;\n158: \tdelete texture;\n159: \tdelete buffer;\n160: \tdelete camera;\n161: \tdelete font;\n162: \n163: \treturn 0;\n164: }", "label": "(89, CUDA, CUDA(cudaGeneratePointGrid(points, GRID_N, GRID_WORLD_SIZE, apptime()));); (89, CUDA, CUDA(cudaDeviceSynchronize());)"}
{"predict": "(4, OpenMP, #pragma omp parallel for)\n(10, OpenMP, #pragma omp parallel for)\n(16, OpenMP, #pragma omp parallel for)\n(22, OpenMP, #pragma omp parallel for)", "label": "(29, OpenMP, #pragma omp target teams distribute num_teams (-2)); (4, OpenMP, #pragma omp target teams distribute num_teams (); (30, OpenMP, #pragma omp target teams distribute num_teams (-10u)); (5, OpenMP, #pragma omp target teams distribute num_teams ()); (7, OpenMP, #pragma omp target teams distribute num_teams (argc); (15, OpenMP, #pragma omp target teams distribute num_teams (argc + argc+z)); (17, OpenMP, #pragma omp target teams distribute num_teams (argc), num_teams (argc+1)); (28, OpenMP, #pragma omp target teams distribute num_teams (3.14)); (-6, OpenMP, #pragma omp target teams distribute num_teams); (5, OpenMP, #pragma omp target teams distribute num_teams (argc))); (7, OpenMP, #pragma omp target teams distribute num_teams (argc > 0 ? argv[1] : argv[2])); (15, OpenMP, #pragma omp target teams distribute num_teams (S1))"}
{"predict": "(11, CUDA, hiop::cuda::set_linear_damping_term_kernel<<<numBlocks, threadsPerBlock>>>(n, dv_ptr, vd, ld, rd);)", "label": "(14, CUDA, double term = thrust::reduce(thrust::device, v_temp.begin(), v_temp.end(), 0.0, thrust::plus<double>());); (7, CUDA, thrust::device_vector<double> v_temp(n);); (7, CUDA, double* dv_ptr = thrust::raw_pointer_cast(v_temp.data());)"}
{"predict": "(1, CUDA, __global__ void shfl_scan_test(int *data, int width, int *partial_sums=NULL) {)", "label": "(2, CUDA, extern __shared__ int sums[];)"}
{"predict": "(24, MPI, no_timeout = timeout > 0 ? 0 : 1;); (35, MPI, if (events[i] != NULL && events[i]->errorcode != EINPROGRESS) { errcode = MPI_Grequest_complete(aio_reqlist[i]->req); events[i] = NULL; num_in_progress--; } else { i++; if (i >= count) { i = 0; } else { i++; } })", "label": "(24, MPI, double start_time = MPI_Wtime();); (16, MPI, errcode = MPI_Grequest_complete(aio_reqlist[i]->req);)"}
{"predict": "(12, OpenMP, #pragma omp parallel for)", "label": "(13, OpenMP, printf(\"main parallel 2 from %d of %d\\n\", omp_get_thread_num(), omp_get_num_threads() );); (1, OpenMP, #pragma omp parallel); (2, OpenMP, printf(\"main parallel 1 from %d of %d\\n\", omp_get_thread_num(), omp_get_num_threads() );)"}
{"predict": "(10, OpenMP, #pragma omp parallel for private(index, failed, t) reduction(+:failed))", "label": "(8, OpenMP, #pragma omp parallel for default(none) private(index) shared(count, failed, params, max_threads, local, saved_key, cur_salt, crypt_out)); (15, OpenMP, int t = omp_get_thread_num();)"}
{"predict": ": clEnqueueFillBuffer failed! err: %d\\n\",\n133:                            error);\n134:                 return error;\n135:             }\n136:         }\n137:     }\n138: \n139:     if ((error = clFlush(tinfo->tQueue))) vlog(\"clFlush failed\\n\");\n140: \n141:     // Do the actual test\n142:     if ((error = clEnqueueNDRangeKernel(tinfo->tQueue, tinfo->kernel, 1, NULL,\n143:                                        &job->globalWorkSize, &job->localWorkSize,\n144:                                        0, NULL, NULL)))\n145:     {\n146:         vlog_error(\"Error: clEnqueueNDRangeKernel failed! err: %d\\n\", error);\n147:         return error;\n148:     }\n149: \n150:     if ((error = clFlush(tinfo->tQueue))) vlog(\"clFlush failed\\n\");\n151: \n152:     if ((error = clFinish(tinfo->tQueue))) vlog(\"clFinish failed\\n\");\n153: \n154:     // Read the result back\n155:     if ((error = clEnqueueReadBuffer(tinfo->tQueue, tinfo->outBuf[0], CL_TRUE,\n156:                                      0, buffer_size, out[0], 0, NULL, NULL)))\n157:     {\n158:         vlog_error(\"Error: clEnqueueReadBuffer failed! err: %d\\n\", error);\n159:         return error;\n160:     }\n161: \n162:     // Check results\n163:     dptr f = func;\n164:     for (size_t i = 0; i < buffer_elements; i++)\n165:     {\n166:         t[i] = f(r[i]);\n167:     }\n168: \n169:     if (relaxedMode)\n170:     {\n171:         cl_ulong *p = out[0];\n172:         for (size_t i = 0; i < buffer_elements; i++)\n173:         {\n1", "label": "(146, OpenCL, error = clSetKernelArg(kernel, 1, sizeof(tinfo->inBuf), &tinfo->inBuf);)"}
{"predict": "(11, MPI, MPI_Info_create(&i1);) (12, MPI, MPI_Info_create(&i2);) (22, MPI, MPI_Info_free(&i1);) (23, MPI, MPI_Info_free(&i2);)", "label": "(20, MPI, MPI_Info_get(i1, (char *) \"key1\", 64, value, &flag);); (28, MPI, MPI_Info_free(&i1);); (11, MPI, MPI_Info_set(i2, (char *) \"key2\", (char *) \"value2\");); (9, MPI, MPI_Info_set(i1, (char *) \"key1\", (char *) \"value1\");); (11, MPI, MPI_Info_get(i1, (char *) \"key2\", 64, value, &flag);); (25, MPI, MPI_Info_free(&i2);); (3, MPI, MPI_Info_create(&i1);); (3, MPI, MPI_Info_create(&i2);)"}
{"predict": "the contents of H\n86:     for(int i = 0; i < H.size(); i++) {\n87:         std::cout << \"H[\" << i << \"] = \" << H[i] << std::endl;\n88:     }\n89: \n90:     // H2.size() returns the size of vector H2\n91:     std::cout << \"H2 has size \" << H2.size() << std::endl;\n92: \n93:     // print the contents of H2\n94:     for(int i = 0; i < H2.size(); i++) {\n95:         std::cout << \"H2[\" << i << \"] = \" << H2[i] << std::endl;\n96:     }\n97: \n98:     // H2 has storage for 4 integers\n99:     H2[0] = 14;\n100:     H2[1] = 20;\n101:     H2[2] = 38;\n102:     H2[3] = 46;\n103:     assert( H2[2] == 99 );\n104: \n105:     // print the contents of H2\n106:     for(int i = 0; i < H2.size(); i++) {\n107:         std::cout << \"H2[\" << i << \"] = \" << H2[i] << std::endl;\n108:     }\n109: \n110:     std::vector<int> H3 = {1, 2, 3, 4};\n111:     assert( H3[2] == 3 );\n112:     assert( H3[3] == 4 );\n113:     std::vector<int> H4 = {1, 2, 3, 4};\n114:     assert( H4[2] == 3 );\n115:     assert( H4[3] == 4 );\n116:     assert( H3 == H4 );\n117:     std::cout << \"H3 == H4 is \" << (H3 == H4) << std::endl;\n118:     std::cout << \"H3[", "label": "(15, CUDA, multi::array<double, 1, thrust::device_allocator<double>> BB(10, 99.);); (34, CUDA, thrust::copy(begin(rotated(A2)[0]), end(rotated(A2)[0]), begin(rotated(B2)[0]));); (16, CUDA, multi::array<std::complex<double>, 1, thrust::device_allocator<std::complex<double> >> BBB_cpy = BBB;); (14, CUDA, multi::array<std::complex<double>, 1, thrust::device_allocator<std::complex<double> >> BBB(10, 99.);); (97, CUDA, thrust::device_vector<int> D = H;); (78, CUDA, multi::array<int, 1, thrust::device_allocator<int>> H2(4, 99); assert(size(H2) == 4);); (69, CUDA, thrust::host_vector<int> H(4);); (21, CUDA, thrust::copy(begin(B), end(B), begin(C));)"}
{"predict": "(4, OpenMP, #pragma omp parallel for private(x, slot_map, slot))", "label": "(8, OpenMP, const std::string comp_name(x.first);)"}
{"predict": "(11, OpenCL, clBuildProgram(program, numDevicesInContext, devices, buildOptions, NULL, &status);)", "label": "(27, OpenCL, status = clBuildProgram(program, 1, &device, buildOptions, NULL, NULL);); (9, OpenCL, cl_program program = clCreateProgramWithSource(context,1, &source, NULL, &status);)"}
{"predict": "(34, MPI, MPI_Scatter(full_data[0], nx_local * ny, MPI_DOUBLE, inner_data[0], nx_local * ny, MPI_DOUBLE, 0, MPI_COMM_WORLD);)", "label": "(15, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(18, OpenMP, #pragma omp parallel for simd;)\n(23, OpenMP, #pragma omp parallel for simd;)", "label": "(2, OpenMP, #pragma omp target teams distribute parallel for simd)"}
{"predict": "(13, MPI, MPI_Win_lock(MPI_LOCK_SHARED, rank, 0, win);) (28, MPI, MPI_Get(winbuf + i, 1, MPI_INT, target_rank, i, 1, MPI_INT, locbuf + i, MPI_COMM_WORLD);) (32, MPI, MPI_Flush_local(win);)", "label": "(25, MPI, MPI_Win_unlock(rank, win);); (48, MPI, MPI_Win_unlock_all(win);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (31, MPI, MPI_Win_flush_local(target_rank, win);); (45, MPI, MPI_Win_free(&win);); (23, MPI, MPI_Win_lock_all(0, win);); (11, MPI, MPI_Win_create(winbuf, BUFCNT * sizeof(int), sizeof(int), MPI_INFO_NULL, MPI_COMM_WORLD, &win);); (26, MPI, MPI_Get(locbuf, BUFCNT, MPI_INT, target_rank, 0, BUFCNT, MPI_INT, win);); (18, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(26, MPI, MPI_Waitall(num_ep, block_coll_request, MPI_STATUSES_IGNORE);)", "label": "(27, MPI, return MPI_Waitall(num_ep, block_coll_request, MPI_STATUS_IGNORE);); (8, MPI, return MPI_Wait(&tmprequest, MPI_STATUS_IGNORE);)"}
{"predict": "00][2];\n118:         generate_max_image_sizes(&numberOfSizes2DArray, sizes2DArray,\n119:                                  &numberOfSizes2D, sizes2D);\n120: \n121:         for( size_t i = 0; i < numberOfSizes2DArray; i++ )\n122:         {\n123:             imageInfo2Darray.width = sizes2DArray[i][0];\n124:             imageInfo2Darray.height = sizes2DArray[i][1];\n125:             imageInfo2Darray.arraySize = sizes2DArray[i][2];\n126:             imageInfo2Darray.rowPitch = gEnablePitch ? 256 : 0;\n127:             imageInfo2Darray.slicePitch = gEnablePitch ? 3 : 0;\n128:             set_image_dimensions(&imageInfo2Darray, imageInfo2Darray.width,\n129:                                  imageInfo2Darray.height, imageInfo2Darray.arraySize,\n130:                                  imageInfo2Darray.rowPitch,\n131:                                  imageInfo2Darray.slicePitch);\n132:             set_image_dimensions(&imageInfo2D, imageInfo2Darray.width,\n133:                                  imageInfo2Darray.height, 0, imageInfo2Darray.rowPitch,\n134:                                  imageInfo2Darray.slicePitch);\n135: \n136:             if( gTestMipmaps )\n137:             {\n138:                 imageInfo2D.num_mip_levels =\n139:                     (cl_uint)random_log_in_range(\n140:                         2,\n141:                         (int)compute_max_mip_levels(\n142:                             imageInfo2D.width, imageInfo2D.height, 0),\n143:                         seed);\n144:                 imageInfo2Darray.num_mip_levels =\n145:                     (cl_uint)random_log_in_range(\n146:                         2,\n147:                         (int)compute_max_mip_levels(\n148:                             imageInfo2Darray.width,\n149:                             imageInfo2Darray.height, 0),\n150:                         seed);\n151:                 imageInfo2D.rowPitch = imageInfo2", "label": "(28, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_IMAGE_MAX_ARRAY_SIZE, sizeof( maxArraySize ), &maxArraySize, NULL );); (25, OpenCL, int error = clGetDeviceInfo( device, CL_DEVICE_IMAGE2D_MAX_WIDTH, sizeof( maxWidth ), &maxWidth, NULL );); (28, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_GLOBAL_MEM_SIZE, sizeof( memSize ), &memSize, NULL );); (26, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_MAX_MEM_ALLOC_SIZE, sizeof( maxAllocSize ), &maxAllocSize, NULL );); (23, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_IMAGE2D_MAX_HEIGHT, sizeof( maxHeight ), &maxHeight, NULL );)"}
{"predict": "(14, CUDA, shist[sub_tid] = 0; //initialize the histogram for this bin in this block)", "label": "(3, CUDA, extern __shared__ int shist[];)"}
{"predict": "(27, OpenMP, #pragma omp parallel for)", "label": "(33, OpenMP, #pragma omp barrier); (28, OpenMP, int me    = omp_get_thread_num();); (25, OpenMP, #pragma omp parallel reduction(min:dtmin) reduction(max:dtmax) reduction(+:dtavg)); (-1, OpenMP, int nt = omp_get_max_threads();); (35, OpenMP, #pragma omp critical)"}
{"predict": "(16, MPI, MPI_Init(&argc, &argv);)", "label": "(20, CUDA, checkCudaErrors(cudaSetDevice(0));)"}
{"predict": "(15, OpenMP, #pragma omp parallel for reduction(+:sum))", "label": "(3, TBB, typedef std::vector<T, oneapi::tbb::tbb_allocator<T> > container_type;); (33, TBB, oneapi::tbb::flattened2d<ets_nokey_type> fvs = flatten2d(vs3);); (11, TBB, typedef typename oneapi::tbb::enumerable_thread_specific< container_type, Allocator<container_type>, oneapi::tbb::ets_key_per_instance > ets_tlskey_type;); (9, TBB, typedef typename oneapi::tbb::enumerable_thread_specific< container_type, Allocator<container_type>, oneapi::tbb::ets_no_key > ets_nokey_type;); (44, TBB, oneapi::tbb::flattened2d<ets_nokey_type> fvs2 = flatten2d(vs3, vs3.begin(), std::next(vs3.begin()));); (23, TBB, oneapi::tbb::parallel_reduce ( vs3.range(1), pvrb );); (0, TBB, oneapi::tbb::global_control gc(oneapi::tbb::global_control::max_allowed_parallelism, p);); (10, TBB, oneapi::tbb::parallel_for ( oneapi::tbb::blocked_range<int> (0, N*p, RANGE_MIN), parallel_vector_for_body<T, Allocator>( vs, allocator_name ) );); (42, TBB, const oneapi::tbb::flattened2d<ets_nokey_type>& cfvs2(fvs2);)"}
{"predict": "(2, OpenMP, #pragma omp parallel for)", "label": "(65, OpenMP, #define OP_GOMP_CRITICAL_READ(OP, FLAG))"}
{"predict": "(20, OpenMP, #pragma omp parallel for collapse(2) private(i, j) reduction(+:s))\n(37, OpenMP, #pragma omp parallel for collapse(2) private(i, j))\n(49, OpenMP, #pragma omp parallel for private(i))", "label": "(17, OpenMP, s += gsl_linalg_cholesky_decomp_unit(v, dv);)"}
{"predict": "(17, OpenMP, #pragma omp atomic update Arg = 0;)", "label": "(19, OpenMP, #pragma omp target teams distribute parallel for simd if (parallel: false)); (10, OpenMP, #pragma omp target teams distribute parallel for simd if(simd: true) nontemporal(Arg))"}
{"predict": "(9, OpenMP, #pragma omp parallel for reduction(+:bandwidths[:count]))", "label": "(19, CUDA, cudaSetDevice(currentDevice);)"}
{"predict": "; i < 10; ++i)\n74:     foo();\n75: #pragma omp target\n76: #pragma omp teams\n77:   for (int i = 0; i < 10; ++i)\n78:     foo();\n79: #pragma omp target\n80: #pragma omp teams\n81:   for (int i = 0; i < 10; ++i)\n82:     foo();\n83: #pragma omp target\n84: #pragma omp teams\n85:   for (int i = 0; i < 10; ++i)\n86:     foo();\n87: #pragma omp target\n88: #pragma omp teams\n89:   for (int i = 0; i < 10; ++i)\n90:     foo();\n91: #pragma omp target\n92: #pragma omp teams\n93:   for (int i = 0; i < 10; ++i)\n94:     foo();\n95: #pragma omp target\n96: #pragma omp teams\n97:   for (int i = 0; i < 10; ++i)\n98:     foo();\n99: #pragma omp target\n100: #pragma omp teams\n101:   for (int i = 0; i < 10; ++i)\n102:     foo();\n103: #pragma omp target\n104: #pragma omp teams\n105:   for (int i = 0; i < 10; ++i)\n106:     foo();\n107: #pragma omp target\n108: #pragma omp teams\n109:   for (int i = 0; i < 10; ++i)\n110:     foo();\n111: #pragma omp target\n112: #pragma omp teams\n113:   for (int i = 0; i < 10; ++i)\n114:     foo();\n115: #pragma omp target\n116: #pragma omp teams\n117:   for (int i = 0; i < 10; ++i)\n118:     foo();\n119:", "label": "(109, OpenMP, #pragma omp distribute parallel for reduction(^ : fl)); (154, OpenMP, #pragma omp distribute parallel for reduction(+ : r)); (117, OpenMP, #pragma omp distribute parallel for reduction(&& : S2::S2sc)); (126, OpenMP, #pragma omp distribute parallel for reduction(+ : h, k, B::x)); (164, OpenMP, #pragma omp distribute parallel for reduction(+ : fl, z)); (39, OpenMP, #pragma omp distribute parallel for reduction(\\)); (6, OpenMP, #pragma omp target); (87, OpenMP, #pragma omp distribute parallel for reduction(+ : ba)); (81, OpenMP, #pragma omp distribute parallel for reduction(max : h.b)); (60, OpenMP, #pragma omp distribute parallel for reduction(&& : argc)); (161, OpenMP, #pragma omp parallel reduction(* : fl)); (28, OpenMP, #pragma omp distribute parallel for reduction(*)); (102, OpenMP, #pragma omp distribute parallel for reduction(&& : S2::S2s)); (6, OpenMP, #pragma omp distribute parallel for reduction +); (85, OpenMP, #pragma omp distribute parallel for reduction(* : ca)); (39, OpenMP, #pragma omp distribute parallel for reduction(| : argc,); (118, OpenMP, #pragma omp distribute parallel for reduction(+ : o)); (125, OpenMP, #pragma omp parallel private(k)); (6, OpenMP, #pragma omp distribute parallel for reduction(); (143, OpenMP, #pragma omp distribute parallel for reduction(max : j)); (44, OpenMP, #pragma omp distribute parallel for reduction(~ : argc)); (138, OpenMP, #pragma omp parallel reduction(min : i)); (143, OpenMP, #pragma omp parallel private(fl)); (26, OpenMP, #pragma omp distribute parallel for reduction(foo : argc); (50, OpenMP, #pragma omp distribute parallel for reduction(^ : S1)); (34, OpenMP, #pragma omp distribute parallel for reduction(|| : argc > 0 ? argv[1] : argv[2])); (-12, OpenMP, #pragma omp distribute parallel for reduction); (153, OpenMP, #pragma omp distribute parallel for reduction(+ : m)); (76, OpenMP, #pragma omp distribute parallel for reduction(- : da)); (95, OpenMP, #pragma omp distribute parallel for reduction(& : e, g)); (144, OpenMP, #pragma omp distribute parallel for reduction(+ : fl)); (-2, OpenMP, #pragma omp distribute parallel for reduction(-); (47, OpenMP, #pragma omp distribute parallel for reduction(+ : a, b, c, d, f)); (112, OpenMP, #pragma omp distribute parallel for reduction(+ : p), reduction(+ : p)); (151, OpenMP, #pragma omp distribute parallel for reduction(task, + : m)); (49, OpenMP, #pragma omp distribute parallel for reduction(min : a, b, c, d, f)); (122, OpenMP, #pragma omp parallel shared(i)); (102, OpenMP, #pragma omp distribute parallel for private(i), reduction(+ : j), reduction(+ : q)); (-4, OpenMP, #pragma omp distribute parallel for reduction()); (-26, OpenMP, #pragma omp teams)"}
{"predict": "(34, CUDA, dim3 block(kThreads, 1); dim3 grid((alloc_extent[0] + kThreads - 1) / kThreads, (alloc_extent[1] + kThreads - 1) / kThreads);)", "label": "(41, CUDA, cudaError_t result = cudaGetLastError();); (41, CUDA, EXPECT_EQ(result, cudaSuccess) << \" - CUDA error: \" << cudaGetErrorString(result);)"}
{"predict": ",\n97: \t\t\t\t\tMBEDTLS_PKCS12_DERIVE_KEY,\n98: \t\t\t\t\t(unsigned char*)saved_key[index],\n99: \t\t\t\t\tsaved_len[index], cur_salt->salt,\n100: \t\t\t\t\tcur_salt->salt_size, key, keylen);\n101: \t\t\tpbe64 = (pbe_format_v5_64*)cur_salt->key;\n102: \t\t\tmemcpy(iv, pbe64->iv, 8);\n103: \t\t\tif (cur_salt->mode_id == kBCMode_XTS) {\n104: \t\t\t\tAES_XTS_decrypt_custom_tweak(key, iv, out, pbe64->keyblock, 256, 256);\n105: \t\t\t} else if (cur_salt->mode_id == kBCMode_CBC) {\n106: \t\t\t\tmemcpy(iv + 8, pbe64->iv, 8);\n107: \t\t\t\tAES_set_decrypt_key(key, 256, &aes_key);\n108: \t\t\t\tAES_cbc_encrypt(pbe64->keyblock, out, 160, &aes_key, iv, AES_DECRYPT);\n109: \t\t\t}\n110: \t\t\tSHA512_Init(&ctx);\n111: \t\t\tSHA512_Update(&ctx, out, 90);\n112: \t\t\tSHA512_Final(hash, &ctx);\n113: \t\t\tp = (struct KGEncryptedBlock64 *)out;\n114: \t\t\tcracked[index] = (0 == memcmp(hash, p->digest, kDigestSize32));\n115: \t\t}\n116: \t}\n117: }\n\nOutput:", "label": "(8, OpenMP, #pragma omp parallel for)"}
{"predict": "(37, OpenMP, #pragma omp parallel for)", "label": "(54, OpenMP, #pragma omp target parallel for map(alloc : c)); (64, OpenMP, #pragma omp barrier); (44, OpenMP, #pragma omp target exit data map(from : a)); (61, OpenMP, #pragma omp target update from(c) nowait); (21, OpenMP, #pragma omp target enter data map(to : a)); (30, OpenMP, #pragma omp target parallel for)"}
{"predict": "nstarts = poweroftwo(nstarts);\n90:     if (log2nstarts <0) {\n91:       printf(\"ERROR: Invalid vector length: %d, must be a power of 2\\n\",\n92:              nstarts);\n93:       error = 1;\n94:       goto ENDOFTESTS;\n95:     }\n96: \n97:     /* initialize random number generator                                        */\n98:     srand(123456789 + my_ID);\n99: \n100:    /* initialize global data                                                      */\n101:    tablespace = (s64Int) sizeof(u64Int) * (s64Int) tablesize;\n102:    loctablesize = (s64Int) sizeof(u64Int) * (s64Int) loctablesize;\n103:    ran = (u64Int *) malloc(loctablesize);\n104:    ranSendBucket = (u64Int **) malloc(loctablesize);\n105:    ranRecvBucket = (u64Int **) malloc(loctablesize);\n106:    sizeSendBucket = (int *) malloc(loctablesize);\n107:    sizeRecvBucket = (int *) malloc(loctablesize);\n108:    senddispls = (int *) malloc(loctablesize);\n109:    recvdispls = (int *) malloc(loctablesize);\n110:    Table = (u64Int *) malloc(loctablesize);\n111: \n112:    /* generate random numbers                                                     */\n113:    ran = genrand(my_ID, update_ratio, loctablesize);\n114: \n115:    /* broadcast random numbers to all ranks                                       */\n116:    MPI_Bcast(ran, loctablesize, MPI_UNSIGNED_LONG_LONG, root, MPI_COMM_WORLD);\n117: \n118:    /* perform random access updates                                               */\n119:    for (round = 0; round < update_ratio; round++) {\n120:      PRK_starts(round);\n121:      for (j = 0; j < loctablesize; j++) {\n122:        index = ran[j];\n123:        Table[index", "label": "(162, MPI, MPI_Bcast(&log2nstarts,      1, MPI_INT,           root, MPI_COMM_WORLD);); (46, MPI, MPI_Comm_size(MPI_COMM_WORLD,&Num_procs);); (155, MPI, MPI_Bcast(&log2nproc,        1, MPI_INT,           root, MPI_COMM_WORLD);); (303, MPI, MPI_Finalize();); (154, MPI, MPI_Bcast(&log2tablesize,    1, MPI_INT,           root, MPI_COMM_WORLD);); (155, MPI, MPI_Bcast(&log2update_ratio, 1, MPI_INT,           root, MPI_COMM_WORLD);); (42, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&my_ID);); (159, MPI, MPI_Bcast(&nupdate,          1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (153, MPI, MPI_Bcast(&nstarts,          1, MPI_INT,           root, MPI_COMM_WORLD);); (154, MPI, MPI_Bcast(&tablesize,        1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (155, MPI, MPI_Bcast(&tablespace,       1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (153, MPI, MPI_Bcast(&loctablesize,     1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (213, MPI, MPI_Barrier(MPI_COMM_WORLD);); (146, MPI, MPI_Bcast(&update_ratio,     1, MPI_INT,           root, MPI_COMM_WORLD);); (280, MPI, MPI_Reduce(&error, &tot_error, 1, MPI_LONG_LONG_INT, MPI_SUM, root, MPI_COMM_WORLD);)"}
{"predict": "(44, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(45, MPI, MPI_Op_free(&dtcmp_reduceop_randroot);); (49, MPI, MPI_Op_free(&dtcmp_reduceop_mms_3uint64t);); (64, MPI, MPI_Comm_free(&dtcmp_comm_self);); (52, MPI, MPI_Type_free(&dtcmp_type_3int);); (56, MPI, MPI_Type_free(&dtcmp_type_3uint64t);)"}
{"predict": "OMPT_RESTORE_RETURN_ADDRESS(gtid);\n117: #endif\n118: \n119:   __kmp_task_free(task);\n120: }\n\nAssistant:\n(3, OpenMP, #pragma omp parallel for)\n(6, OpenMP, #pragma omp parallel for)\n(9, OpenMP, #pragma omp parallel for)", "label": "(70, OpenMP, kmp_gomp_depends_info_t gomp_depends(depend);); (103, OpenMP, __kmpc_omp_wait_deps(&loc, gtid, ndeps, dep_list, 0, NULL);); (77, OpenMP, __kmpc_omp_task(&loc, gtid, task);); (74, OpenMP, __kmpc_omp_task_with_deps(&loc, gtid, task, ndeps_cnv, dep_list, 0, NULL);)"}
{"predict": "(13, MPI, MPI_Comm_group(comm, &basegroup);)", "label": "(14, MPI, MPI_Comm_size(comm, &size);); (19, MPI, MPI_Comm_group(dupcomm, &g2);); (28, MPI, MPI_Group_free(&g2);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &worldrank);); (28, MPI, MPI_Comm_free(&dupcomm);); (23, MPI, MPI_Group_free(&basegroup);); (12, MPI, MPI_Comm_group(newcomm, &g1);); (12, MPI, MPI_Comm_dup(comm, &dupcomm);); (15, MPI, mpi_errno = MPI_Group_difference(g1, g2, NULL);); (22, MPI, MPI_Comm_free(&newcomm);); (14, MPI, MPI_Error_class(mpi_errno, &errclass);); (4, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (5, MPI, MPI_Comm_split(comm, 0, size - rank, &newcomm);); (16, MPI, MPI_Group_free(&g1);); (-1, MPI, MPI_Comm_rank(comm, &rank);); (-3, MPI, MPI_Comm_group(comm, &basegroup);)"}
{"predict": "(16, CUDA, powerlaw_psd_gaussian<<<num_blocks, threads_per_block>>>(exponents, NUM_TIMESTEPS, NUM_ROLLOUTS, colored_noise_d, 0, gen, 0.0f, stream);)", "label": "(9, CUDA, HANDLE_ERROR(cudaMalloc((void**)&colored_noise_d, sizeof(float) * full_buffer_size));); (11, CUDA, cudaStreamCreate(&stream);); (18, CUDA, HANDLE_ERROR(cudaStreamSynchronize(stream));)"}
{"predict": "71:     ;\n72: // expected-error@+1 {{argument to 'collapse' clause must be a strictly positive integer value}}\n73:   for (i = 0; i < 16; ++i)\n74:     ;\n75: // expected-error@+1 {{argument to 'collapse' clause must be a strictly positive integer value}}\n76:   for (i = 0; i < 16; ++i)\n77:     ;\n78: // expected-error@+1 {{argument to 'collapse' clause must be a strictly positive integer value}}\n79:   for (i = 0; i < 16; ++i)\n80:     ;\n81: // expected-error@+1 {{argument to 'collapse' clause must be a strictly positive integer value}}\n82:   for (i = 0; i < 16; ++i)\n83:     ;\n84: // expected-error@+1 {{argument to 'collapse' clause must be a strictly positive integer value}}\n85:   for (i = 0; i < 16; ++i)\n86:     ;\n87: // expected-error@+1 {{argument to 'collapse' clause must be a strictly positive integer value}}\n88:   for (i = 0; i < 16; ++i)\n89:     ;\n90: // expected-error@+1 {{argument to 'collapse' clause must be a strictly positive integer value}}\n91:   for (i = 0; i < 16; ++i)\n92:     ;\n93: // expected-error@+1 {{argument to 'collapse' clause must be a strictly positive integer value}}\n94:   for (i = 0; i < 16; ++i)\n95:     ;\n96: // expected-error@+1 {{argument to 'collapse' clause must be a strictly positive integer value}}\n97:   for (i = 0; i < 16; ++i)\n98:     ;\n99: // expected-error@+1 {{argument to 'collapse' clause must be a strictly positive integer value}}\n100: // expected-error@+1 {{argument to 'collapse' clause must be a strictly positive integer value}}\n101: // expected-error@+1 {{argument to 'collapse' clause", "label": "(41, OpenMP, #pragma omp target teams distribute collapse(4, )); (90, OpenMP, #pragma omp target teams distribute collapse(2) firstprivate(i)); (15, OpenMP, #pragma omp target teams distribute collapse(,); (33, OpenMP, #pragma omp target teams distribute collapse(4,); (74, OpenMP, #pragma omp target teams distribute collapse(-5)); (8, OpenMP, #pragma omp target teams distribute collapse()); (15, OpenMP, #pragma omp target teams distribute collapse(, )); (79, OpenMP, #pragma omp target teams distribute collapse(5 - 5)); (66, OpenMP, #pragma omp target teams distribute collapse(foo())); (73, OpenMP, #pragma omp target teams distribute collapse(0)); (-1, OpenMP, #pragma omp target teams distribute collapse(); (55, OpenMP, #pragma omp target teams distribute collapse(4, 8)); (38, OpenMP, #pragma omp target teams distribute collapse(4 4)); (32, OpenMP, #pragma omp target teams distribute collapse(4)); (-9, OpenMP, #pragma omp target teams distribute collapse); (40, OpenMP, #pragma omp target teams distribute collapse(4, , 4)); (15, OpenMP, #pragma omp target teams distribute collapse(4); (9, OpenMP, #pragma omp target teams distribute collapse 4)); (52, OpenMP, #pragma omp target teams distribute collapse(2.5))"}
{"predict": "(14, CUDA, cudaMemcpy(cd, d_cd, size * sizeof(double), cudaMemcpyDeviceToHost);) (61, CUDA, cudaMemcpy(d_cd, cd, size * sizeof(double), cudaMemcpyHostToDevice);)", "label": "(64, CUDA, CUDA_SAFE_CALL( cudaMemcpy(c, cd, size*sizeof(double), cudaMemcpyDeviceToHost) );); (67, CUDA, CUDA_SAFE_CALL( cudaDeviceReset() );); (12, CUDA, CUDA_SAFE_CALL( cudaMemset(cd, 0, size*sizeof(double)) );); (8, CUDA, CUDA_SAFE_CALL( cudaMalloc((void**)&cd, size*sizeof(double)) );); (62, CUDA, CUDA_SAFE_CALL( cudaFree(cd) );); (12, CUDA, CUDA_SAFE_CALL( cudaThreadSynchronize() );)"}
{"predict": "(46, CUDA, ptrs[i] = (int *)cudaMalloc(size * Mtypes(i));)", "label": "(71, CUDA, cudaMemcpy(&((int *)cuda_mem)[j*size], ptrs[i], size * Mtypes(i), cudaMemcpyHostToDevice);); (68, CUDA, cudaMalloc(&cuda_mem, size * Mtypes(i) * maxGPT);); (65, CUDA, cudaMemcpy(cuda_mem, ptrs[i], size * Mtypes(i), cudaMemcpyHostToDevice);); (63, CUDA, cudaMalloc(&cuda_mem, size * Mtypes(i));)"}
{"predict": "(13, CUDA, accum = cutlass::epilogue::EpilogueOutputOp::operator()(accum);)", "label": "(3, CUDA, __shared__ typename Epilogue::SharedStorage shared_storage;)"}
{"predict": "(14, CUDA, thrust::device_vector<T, thrust::cuda::allocator<T>> A(1000, T{1.*WORLD.rank()});)", "label": "(3, CUDA, cudaSetDevice(WORLD.rank());); (11, CUDA, using T = thrust::complex<double>;); (40, CUDA, cudaStreamSynchronize(NULL);); (14, CUDA, thrust::host_vector<T> H = A;)"}
{"predict": "(5, OpenMP, T *arr_t = new T[N]; #pragma omp parallel for reduction(min:min_t) reduction(max:max_t) for (int i = 0; i < N; i++) arr_t[i] = std::min<T>(i, std::numeric_limits<T>::max());)", "label": "(9, OpenMP, #pragma omp target teams distribute parallel for reduction(min : min_t)); (6, OpenMP, #pragma omp target data map(to : arr_t[0 : N])); (11, OpenMP, #pragma omp target teams distribute parallel for reduction(max : max_t))"}
{"predict": "j, mainSize );\n119:                     }\n120:                     else\n121:                     {\n122:                         log_error(\"ERROR: Validation failure in sub-buffer %zu\\n\",\n123:                                   sbThatFailed );\n124:                     }\n125:                     numErrors++;\n126:                     break;\n127:                 }\n128:             }\n129:         }\n130:     }\n131: \n132:     free( mainBufferContents );\n133:     free( actualResults );\n134:     free_mtdata( m );\n135: \n136:     return numErrors;\n137: }", "label": "(23, OpenCL, mainBuffer = clCreateBuffer( context, CL_MEM_READ_WRITE | CL_MEM_COPY_HOST_PTR, mainSize, mainBufferContents, &error );); (93, OpenCL, error = clEnqueueReadBuffer( queueA, mainBuffer, CL_TRUE, 0, mainSize, actualResults, 0, NULL, NULL );)"}
{"predict": "(24, MPI, MPI_Waitany(10, requests, &index, MPI_STATUS_IGNORE, &err);)", "label": "(20, MPI, err = MPI_Waitany(10, requests, &index, statuses);); (33, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_ARE_FATAL);); (33, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (15, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(73, MPI, int mpi_rank, mpi_initialized, mpi_finalized; MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank); MPI_Initialized(&mpi_initialized); MPI_Finalized(&mpi_finalized);)", "label": "(69, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (65, MPI, MPI_Finalized(&mpi_finalized);); (63, MPI, MPI_Initialized(&mpi_initialized);)"}
{"predict": "( spa[jcol] == EMPTY ) {\n110: \t\tspa[jcol] = i;\n111: \t\tind_tosend[ptr_ind_tosend[row_to_proc[jcol]]++] = jcol;\n112: \t    }\n113: \t}\n114:     }\n115:     /* ------------------------------------------------------------\n116:        LOAD THE X-VALUES TO BE SENT TO THE OTHER PROCESSES.\n117:        ------------------------------------------------------------*/\n118:     if ( TotalIndSend ) {\n119:         if ( !(val_tosend = doublecomplexMalloc_dist(TotalIndSend)) )\n120:             ABORT(\"Malloc fails for val_tosend[]\");\n121:         for (i = 0; i < TotalIndSend; ++i) {\n122:             val_tosend[i] = nzval[ind_tosend[i]];\n123:         }\n124:     }\n125: \n126:     /* ------------------------------------------------------------\n127:        LOAD THE X-INDICES TO BE RECEIVED FROM OTHER PROCESSES.\n128:        ------------------------------------------------------------*/\n129:     for (p = 0; p < procs; ++p) RecvCounts[p] = 0;\n130:     for (i = 0; i < m_loc; ++i) { /* Loop through each row of A */\n131:         for (j = rowptr[i]; j < rowptr[i+1]; ++j) {\n132: \t    jcol = colind[j];\n133:             p = row_to_proc[jcol];\n134:             if ( p != iam ) {\n135: \t        ++RecvCounts[p];\n136:             }\n137:         }\n138:     }\n139: \n140:     /* ------------------------------------------------------------\n141:        LOAD THE X-VALUES TO BE RECEIVED FROM OTHER PROCESSES.\n142:        ------------------------------------------------------------*/\n143:     /* Build pointers to ind_torecv[]. */\n144:     ptr_ind_torecv[0] = 0;\n145:     for (p = 0, TotalValSend = 0; p < procs; ++p) {\n146:         TotalValSend += RecvCounts[p", "label": "(166, MPI, if ( SendCounts[p] ) MPI_Wait(&send_req[p], &status);); (166, MPI, if ( RecvCounts[p] ) MPI_Wait(&recv_req[p], &status);)"}
{"predict": "(22, OpenMP, #pragma omp parallel for)", "label": "(3, OpenMP, mz_uint comp_flags = TDEFL_COMPUTE_ADLER32 | tdefl_create_comp_flags_from_zip_params(level, window_bits, strategy);)"}
{"predict": "(4, OpenMP, #pragma omp simd simdlen(4))", "label": "(3, OpenMP, #pragma omp target simd simdlen); (18, OpenMP, #pragma omp target simd simdlen(, )); (70, OpenMP, #pragma omp target simd simdlen(-5)); (31, OpenMP, #pragma omp target simd simdlen(4,); (72, OpenMP, #pragma omp target simd simdlen(0)); (2, OpenMP, #pragma omp target simd simdlen(); (46, OpenMP, #pragma omp target simd simdlen(4, , 4)); (61, OpenMP, #pragma omp target simd simdlen(foo())); (3, OpenMP, #pragma omp target simd simdlen()); (55, OpenMP, #pragma omp target simd simdlen(2.5)); (5, OpenMP, #pragma omp target simd simdlen(,); (31, OpenMP, #pragma omp target simd simdlen(4)); (68, OpenMP, #pragma omp target simd simdlen(5 - 5)); (26, OpenMP, #pragma omp target simd simdlen(4, )); (33, OpenMP, #pragma omp target simd simdlen(4 4)); (45, OpenMP, #pragma omp target simd simdlen(4, 8)); (13, OpenMP, #pragma omp target simd simdlen(4); (7, OpenMP, #pragma omp target simd simdlen 4))"}
{"predict": "95: \n96:     printf(\"Executing Black-Scholes CPU kernel (%i iterations)...\\n\", NUM_ITERATIONS);\n97:     sdkResetTimer(&hTimer);\n98:     sdkStartTimer(&hTimer);\n99: \n100:     for (i = 0; i < NUM_ITERATIONS; i++)\n101:     {\n102:         BlackScholesCPU(\n103:             h_CallResultCPU,\n104:             h_PutResultCPU,\n105:             h_StockPrice,\n106:             h_OptionStrike,\n107:             h_OptionYears,\n108:             RISKFREE,\n109:             VOLATILITY,\n110:             OPT_N\n111:         );\n112:     }\n113: \n114:     sdkStopTimer(&hTimer);\n115:     double cpuTime = sdkGetTimerValue(&hTimer) / NUM_ITERATIONS;\n116: \n117:     printf(\"BlackScholesCPU() time    : %f msec\\n\", cpuTime);\n118:     printf(\"Effective memory bandwidth: %f GB/s\\n\", ((double)(5 * OPT_N * sizeof(float)) * 1E-9) / (cpuTime * 1E-3));\n119:     printf(\"Gigaoptions per second    : %f     \\n\\n\", ((double)(2 * OPT_N) * 1E-9) / (cpuTime * 1E-3));\n120: \n121:     printf(\"Black-Scholes speedup     : %f     \\n\", cpuTime / gpuTime);\n122:     printf(\"Black-Scholes efficiency  : %f     \\n\\n\", (2 * OPT_N * sizeof(float)) / (cpuTime * 1E-3));\n123: \n124:     printf(\"Cleaning up...\\n\");\n125:     printf(\"...releasing CPU memory for options.\\n\");\n126:     free(h_CallResultCPU);\n127:     free(h_PutResultCPU);\n128:     free(h_CallResultGPU);\n129:     free(h_PutResultGPU);\n130:     free(h_StockPrice);\n131:     free(h", "label": "(49, CUDA, checkCudaErrors(cudaMalloc((void **)&d_CallResult,   OPT_SZ));); (155, CUDA, checkCudaErrors(cudaFree(d_OptionYears));); (155, CUDA, checkCudaErrors(cudaFree(d_OptionStrike));); (155, CUDA, checkCudaErrors(cudaFree(d_StockPrice));); (155, CUDA, checkCudaErrors(cudaFree(d_PutResult));); (66, CUDA, checkCudaErrors(cudaMemcpy(d_OptionStrike, h_OptionStrike,  OPT_SZ, cudaMemcpyHostToDevice));); (45, CUDA, checkCudaErrors(cudaMalloc((void **)&d_StockPrice,   OPT_SZ));); (43, CUDA, checkCudaErrors(cudaMalloc((void **)&d_PutResult,    OPT_SZ));); (26, CUDA, findCudaDevice(argc, (const char **)argv);); (68, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (42, CUDA, checkCudaErrors(cudaMalloc((void **)&d_OptionStrike, OPT_SZ));); (149, CUDA, checkCudaErrors(cudaFree(d_CallResult));); (81, CUDA, getLastCudaError(\"BlackScholesGPU() execution failed\\n\");); (98, CUDA, checkCudaErrors(cudaMemcpy(h_CallResultGPU, d_CallResult, OPT_SZ, cudaMemcpyDeviceToHost));); (58, CUDA, checkCudaErrors(cudaMemcpy(d_OptionYears,  h_OptionYears,   OPT_SZ, cudaMemcpyHostToDevice));); (97, CUDA, checkCudaErrors(cudaMemcpy(h_PutResultGPU,  d_PutResult,  OPT_SZ, cudaMemcpyDeviceToHost));); (37, CUDA, checkCudaErrors(cudaMalloc((void **)&d_OptionYears,  OPT_SZ));); (53, CUDA, checkCudaErrors(cudaMemcpy(d_StockPrice,  h_StockPrice,   OPT_SZ, cudaMemcpyHostToDevice));)"}
{"predict": "(12, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(20, MPI, t[k].endSync = MPI_Wtime();); (9, MPI, t[k].startOp = MPI_Wtime();); (10, MPI, MPI_Put(&one, sz, MPI_INT, destRank, j, sz, MPI_INT, win);); (12, MPI, t[k].endOp = MPI_Wtime();); (15, MPI, MPI_Win_wait(win);); (2, MPI, MPI_Win_post(exposureGroup, 0, win);); (12, MPI, MPI_Win_complete(win);); (-1, MPI, MPI_Barrier(MPI_COMM_WORLD);); (0, MPI, MPI_Win_start(accessGroup, 0, win);)"}
{"predict": "(7, CUDA, auto id = cudf::column(rmm::device_uvector<cudf::size_type>(size, stream), rmm::device_buffer{}, 0);) (10, CUDA, auto xs = cudf::column(rmm::device_uvector<float>(size, stream), rmm::device_buffer{}, 0);) (12, CUDA, auto ys = cudf::column(rmm::device_uvector<float>(size, stream), rmm::device_buffer{}, 0);) (15, CUDA, auto ts = cudf::column(rmm::device_uvector<cudf::timestamp_ms>(size, stream), rmm::device_buffer{}, 0);) (18, CUDA, auto nulls = rmm::device_uvector<int>(1000, stream);)", "label": "(17, CUDA, cudaMemsetAsync(nulls.data(), 0xcccc, nulls.size(), rmm::cuda_stream_default.value());)"}
{"predict": "(14, MPI, MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);)\n(26, MPI, MPI_Bcast(filename, len + 10, MPI_CHAR, 0, MPI_COMM_WORLD);)\n(43, MPI, MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_CREATE | MPI_MODE_RDWR, MPI_INFO_NULL, &fh);)\n(55, MPI, MPI_Wait(&request, &status);)\n(76, MPI, MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_CREATE | MPI_MODE_RDWR, MPI_INFO_NULL, &fh);)\n(88, MPI, MPI_Wait(&request, &status);)", "label": "(29, MPI, MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);); (51, MPI, errcode = MPI_File_set_view(fh, 0, MPI_INT, MPI_INT, \"native\", MPI_INFO_NULL);); (8, MPI, MPI_Init(&argc, &argv);); (59, MPI, MPI_Wait(&request, &status);); (112, MPI, MPI_Finalize();); (25, MPI, MPI_Bcast(filename, len + 10, MPI_CHAR, 0, MPI_COMM_WORLD);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (96, MPI, MPI_Allreduce(&errs, &toterrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (74, MPI, errcode = MPI_File_iread(fh, buf, nints, MPI_INT, &request);); (14, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (47, MPI, errcode = MPI_File_iwrite(fh, buf, nints, MPI_INT, &request);)"}
{"predict": "(9, MPI, local_b = (int *)malloc(COUNT * sizeof(int));) (26, MPI, MPI_Win_fence(0, win[id]);) (33, MPI, MPI_Win_fence(0, win[id]);)", "label": "(30, MPI, MPI_Get(local_b, COUNT, MPI_INT, 0, 0, COUNT, MPI_INT, win[id]);); (11, MPI, MPI_Win_fence(0, win[id]);); (43, MPI, MPI_Free_mem(local_b);); (11, MPI, MPI_Put(local_b, COUNT, MPI_INT, 0, 0, COUNT, MPI_INT, win[id]);); (4, MPI, MPI_Alloc_mem(COUNT * sizeof(int), MPI_INFO_NULL, &local_b);)"}
{"predict": "9:     MPI_CHECK(MPI_File_set_view(fh, disp, MPI_INT, typevec, \"native\", MPI_INFO_NULL));\n100:     MPI_CHECK(MPI_File_write_all(fh, buf, len, MPI_INT, &status));\n101: \n102:     MPI_CHECK(MPI_File_set_view(fh, disp, MPI_INT, typevec, \"native\", MPI_INFO_NULL));\n103:     MPI_CHECK(MPI_File_read_all(fh, buf, len, MPI_INT, &status));\n104: \n105:     MPI_CHECK(MPI_File_close(&fh));\n106: \n107:     MPI_Reduce(&errs, &toterrs, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n108: \n109:     if (!mynod) {\n110:         if (toterrs) {\n111:             fprintf(stderr, \"Total Errors: %d\\n\", toterrs);\n112:         } else {\n113:             fprintf(stderr, \"No Errors\\n\");\n114:         }\n115:     }\n116: \n117:     MPI_Finalize();\n118:     free(buf);\n119:     free(filename);\n120:     return 0;\n121: }", "label": "(197, MPI, MPI_CHECK(MPI_File_write_all(fh, buf, SIZE, MPI_INT, &status));); (9, MPI, MPI_Init(&argc, &argv);); (80, MPI, MPI_CHECK(MPI_File_write_all(fh, buf, 1, newtype, &status));); (42, MPI, MPI_Type_vector(SIZE / 2, 1, 2, MPI_INT, &typevec);); (49, MPI, MPI_Type_create_struct(1, &len, &disp, &typevec, &tmptype);); (73, MPI, MPI_CHECK(MPI_File_set_view(fh, 0, MPI_INT, newtype, \"native\", MPI_INFO_NULL));); (67, MPI, MPI_Barrier(MPI_COMM_WORLD);); (107, MPI, MPI_CHECK(MPI_File_close(&fh));); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mynod);); (48, MPI, MPI_Type_free(&tmptype);); (136, MPI, MPI_CHECK(MPI_File_read_at_all(fh, mynod * (SIZE / 2) * sizeof(int), buf, 1, newtype, &status));); (5, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (192, MPI, MPI_CHECK(MPI_File_read_at_all(fh, 0, buf, SIZE, MPI_INT, &status));); (22, MPI, MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);); (54, MPI, MPI_Error_class(err, &errorclass);); (-4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (42, MPI, MPI_Type_commit(&newtype);); (72, MPI, MPI_CHECK(MPI_File_read_at_all(fh, 0, buf, 1, newtype, &status));); (36, MPI, MPI_Type_free(&typevec);); (37, MPI, MPI_Type_create_resized(tmptype, 0, extent, &newtype);); (220, MPI, MPI_Finalize();); (15, MPI, MPI_Bcast(filename, len + 1, MPI_CHAR, 0, MPI_COMM_WORLD);); (206, MPI, MPI_Allreduce(&errs, &toterrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (214, MPI, MPI_Type_free(&newtype);)"}
{"predict": "(18, TBB, tbb::parallel_pipeline(n_tokens, tbb::make_filter<void, void>(filter_table[i], [&counter](tbb::flow_control& control) { if (counter-- == 0) control.stop(); })));", "label": "(10, TBB, tbb::filter_t<void, void> one_filter( filter_table[i], i_filter );); (15, TBB, tbb::atomic<int> counter;); (11, TBB, tbb::parallel_pipeline( n_tokens, one_filter );)"}
{"predict": "(29, CUDA, int* h_in = new int[num_items];) (30, CUDA, int* h_reference = new int[num_items];) (48, CUDA, CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));) (59, CUDA, CubDebugExit(cudaMemcpy(d_out, h_out, sizeof(int) * num_items, cudaMemcpyHostToDevice));) (60, CUDA, CubDebugExit(cudaMemcpy(d_num_selected_out, &num_selected, sizeof(int), cudaMemcpyHostToDevice));) (72, CUDA, delete[] h_in;) (73, CUDA, delete[] h_reference;) (82, CUDA, CubDebugExit(cudaFree(d_in));) (83, CUDA, CubDebugExit(cudaFree(d_out));) (84, CUDA, CubDebugExit(cudaFree(d_num_selected_out));)", "label": "(47, CUDA, CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));)"}
{"predict": "(13, CUDA, cudaMalloc((void**)&d_inPtr, size * sizeof(float));) (15, CUDA, cudaMalloc((void**)&d_outPtr, size * sizeof(float));) (22, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);) (25, CUDA, cudaMemcpy(h_inPtr, d_outPtr, size * sizeof(float), cudaMemcpyDeviceToHost);) (30, CUDA, cudaFree(d_inPtr);) (30, CUDA, cudaFree(d_outPtr);)", "label": "(34, CUDA, cudaFree(d_inPtr);); (18, CUDA, cudaMalloc((void**)&d_outPtr, size * sizeof(float));); (24, CUDA, cudaMemcpy(&result_gpu, &d_outPtr[0], sizeof(float), cudaMemcpyDeviceToHost);); (18, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);); (29, CUDA, cudaFree(d_outPtr);); (13, CUDA, cudaMalloc((void**)& d_inPtr, size * sizeof(float));)"}
{"predict": "25:   Table = (u64Int *) prk_malloc(tablespace);\n126:   if (!Table) {\n127:     printf(\"ERROR: Could not allocate space for table\\n\");\n128:     exit(EXIT_FAILURE);\n129:   }\n130: \n131:   /* initialize Table array with random numbers                    */\n132:   ran = (u64Int *) prk_malloc(tablespace);\n133:   if (!ran) {\n134:     printf(\"ERROR: Could not allocate space for random numbers\\n\");\n135:     exit(EXIT_FAILURE);\n136:   }\n137:   random_time = omp_get_wtime();\n138:   PRK_starts(0);\n139:   for (i=0; i<tablesize; i++) {\n140:     ran[i] = PRK_starts(i);\n141:   }\n142:   random_time = omp_get_wtime() - random_time;\n143: \n144:   /* print random number generation time                             */\n145:   printf(\"Random number generation time: %f seconds\\n\", random_time);\n146: \n147:   /* print input values                                              */\n148:   printf(\"Number of threads = %d\\n\", nthread_input);\n149:   printf(\"Log2 of table size = %d\\n\", log2tablesize);\n150:   printf(\"Update ratio = %d\\n\", update_ratio);\n151:   printf(\"Vector length = %d\\n\", nstarts);\n152: \n153:   /* create a thread team                                          */\n154:   #pragma omp parallel private(my_ID, nupdate, tablespace, oldsize, round, ran)\n155:   {\n156:     #pragma omp single\n157:     {\n158:       nthread = omp_get_num_threads();\n159:       printf(\"Number of threads requested = %d\\n\", nthread_input);\n160:       printf(\"Number of threads obtained  = %d\\n\", nthread);\n161:       if (nthread_input != nthread) {\n162:         num_error = 1;\n163:       }\n164:     }\n165:", "label": "(202, OpenMP, #pragma omp barrier); (150, OpenMP, nthread = omp_get_num_threads();); (197, OpenMP, #pragma omp for); (146, OpenMP, #pragma omp master); (54, OpenMP, omp_set_num_threads(nthread_input);); (137, OpenMP, #pragma omp parallel private(i, j, ran, round, index, my_ID) reduction(+:error)); (223, OpenMP, #pragma omp atomic); (140, OpenMP, my_ID   = omp_get_thread_num();)"}
{"predict": "118:         memcpy(sortptr, &flist2->mfu_flist_elem[idx].mtime, sizeof(time_t));\n119:         sortptr += sizeof(time_t);\n120: \n121:         /* copy in file path */\n122:         memcpy(sortptr, flist2->mfu_flist_elem[idx].name, chars);\n123:         sortptr += chars;\n124: \n125:         /* copy in stat data */\n126:         memcpy(sortptr, &flist2->mfu_flist_elem[idx].sat, bytes);\n127:         sortptr += bytes;\n128:         idx++;\n129:     }\n130: \n131:     /* create sort request */\n132:     DTCMP_Request request;\n133:     if (DTCMP_Request_create(dt_keysat, incount, sortbuf, &request) != DTCMP_SUCCESS) {\n134:         MFU_ABORT(1, \"Failed to create sort request\");\n135:     }\n136: \n137:     /* sort the elements */\n138:     if (DTCMP_Request_sort(request) != DTCMP_SUCCESS) {\n139:         MFU_ABORT(1, \"Failed to sort elements\");\n140:     }\n141: \n142:     /* get sorted indices */\n143:     uint64_t* sorted_indices = (uint64_t*) MFU_MALLOC(incount * sizeof(uint64_t));\n144:     DTCMP_Request_get_sorted_indices(request, sorted_indices);\n145: \n146:     /* copy sorted indices back to caller */\n147:     mfu_flist_set_sorted_indices(flist, sorted_indices);\n148: \n149:     /* free sort request and buffer */\n150:     DTCMP_Request_free(&request);\n151:     MFU_FREE(&sortbuf);\n152:     MFU_FREE(&sorted_indices);\n153: \n154:     /* free list */\n155:     mfu_flist_free(&flist2);\n156: }", "label": "(106, MPI, MPI_Type_get_extent(dt_key, &key_lb, &key_extent);); (174, MPI, MPI_Type_free(&dt_key);); (16, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (20, MPI, MPI_Type_commit(&dt_filepath);); (169, MPI, MPI_Type_free(&dt_keysat);); (170, MPI, MPI_Type_free(&dt_filepath);); (107, MPI, MPI_Type_get_extent(dt_sat, &sat_lb, &sat_extent);); (102, MPI, MPI_Type_get_extent(dt_keysat, &keysat_lb, &keysat_extent);); (5, MPI, MPI_Type_contiguous((int)bytes, MPI_BYTE, &dt_sat);); (13, MPI, MPI_Type_contiguous((int)chars, MPI_CHAR, &dt_filepath);); (171, MPI, MPI_Type_free(&dt_sat);)"}
{"predict": "(3, OpenMP, #pragma omp parallel for private(iset, iget))", "label": "(12, OpenMP, iget = omp_get_teams_thread_limit();); (3, OpenMP, omp_set_num_teams(iset);); (3, OpenMP, iget = omp_get_max_teams();); (8, OpenMP, omp_set_teams_thread_limit(iset);)"}
{"predict": "(8, TBB, REMARK(\"Queue_Node Time=%6.6f\\n\", (stop-start).seconds());)", "label": "(12, TBB, test_resets<int, tbb::flow::queue_node<int> >();); (2, TBB, tbb::task_scheduler_init init(p);); (7, TBB, stop = tbb::tick_count::now();); (18, TBB, test_buffer_extract<tbb::flow::queue_node<int> >().run_tests();); (-3, TBB, tbb::tick_count start = tbb::tick_count::now(), stop;); (8, TBB, test_resets<float, tbb::flow::queue_node<float> >();)"}
{"predict": "(15, CUDA, cudaDeviceSynchronize();)", "label": "(16, CUDA, cudaDeviceSynchronize();); (6, CUDA, cudaMallocManaged(&y, N*sizeof(float));); (4, CUDA, cudaMallocManaged(&x, N*sizeof(float));); (23, CUDA, cudaFree(y);); (21, CUDA, cudaFree(x);)"}
{"predict": "(14, CUDA, if (err != cudaSuccess) { errorQuda(\"Failed to free device memory (%s:%d in %s())\\n\", file, line, func); } cudaFree(ptr);)", "label": "(13, CUDA, cudaError_t err = cudaFree(ptr);)"}
{"predict": "(17, OpenMP, #pragma omp parallel for)", "label": "(17, OpenMP, #pragma omp parallel for num_threads(num_devices)); (10, OpenMP, #pragma omp target teams distribute parallel for); (3, OpenMP, size_t num_devices = omp_get_num_devices();); (17, OpenMP, #pragma omp target teams distribute parallel for device(omp_get_thread_num()))"}
{"predict": "(10, MPI, if (rank == 0) { std::cout << region << \" MIN TIME [ms]: \" << 1.0 * min_time / n_rep_ << std::endl; std::cout << region << \" MAX TIME [ms]: \" << 1.0 * max_time / n_rep_ << std::endl; std::cout << region << \" AVG TIME [ms]: \" << 1.0 * sum_time / (n_rep_ * size) << std::endl; std::cout << \"\\n\"; } else { MPI_Send(&min_time, 1, MPI_LONG_LONG, 0, 0, MPI_COMM_WORLD); MPI_Send(&max_time, 1, MPI_LONG_LONG, 0, 1, MPI_COMM_WORLD); MPI_Send(&sum_time, 1, MPI_LONG_LONG, 0, 2, MPI_COMM_WORLD); } ) (11, MPI, if (rank == 0) { } else { MPI_Recv(&min_time, 1, MPI_LONG_LONG, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE); MPI_Recv(&max_time, 1, MPI_LONG_LONG, 0, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE); MPI_Recv(&sum_time, 1, MPI_LONG_LONG, 0, 2, MPI_COMM_WORLD, MPI_STATUS_IGNORE); } )", "label": "(9, MPI, MPI_Reduce(&time, &sum_time, 1, MPI_LONG_LONG, MPI_SUM, 0, comm_);); (6, MPI, MPI_Reduce(&time, &max_time, 1, MPI_LONG_LONG, MPI_MAX, 0, comm_);); (10, MPI, MPI_Comm_size(comm_, &size);); (8, MPI, MPI_Comm_rank(comm_, &rank);); (4, MPI, MPI_Reduce(&time, &min_time, 1, MPI_LONG_LONG, MPI_MIN, 0, comm_);)"}
{"predict": "(10, MPI, MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided);)", "label": "(9, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &MSG_COMM_WORLD);); (4, MPI, MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided);); (63, MPI, MPI_Type_size(type, &type_size);); (16, MPI, MPI_Barrier(MSG_COMM_WORLD);); (130, MPI, MPI_Comm_free(&MSG_COMM_WORLD);); (131, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_rank(MSG_COMM_WORLD, &rank);); (5, MPI, MPI_Comm_size(MSG_COMM_WORLD, &size);)"}
{"predict": "(10, CUDA, cudaMalloc((void**)&dptr, 4*sizeof(int))); (11, CUDA, cuda(HostGetDevicePointer( &dptr, hptr, 0 ) )); (18, CUDA, TestFunnelShift<SHIFT_FLAGS_RIGHT|SHIFT_FLAGS_CLAMP><<<1,1>>>( dptr );); (20, CUDA, TestFunnelShift<SHIFT_FLAGS_RIGHT><<<1,1>>>( dptr );); (27, CUDA, TestFunnelShift<SHIFT_FLAGS_LEFT|SHIFT_FLAGS_CLAMP><<<1,1>>>( dptr );); (29, CUDA, TestFunnelShift<SHIFT_FLAGS_LEFT><<<1,1>>>( dptr ););", "label": "(37, CUDA, cudaFreeHost( hptr );)"}
{"predict": "(10, OpenMP, #pragma omp parallel for private(i))", "label": "(17, OpenMP, #pragma omp target simd aligned(X::x : ::z)); (2, OpenMP, #pragma omp target simd aligned(B:bfoo())); (23, OpenMP, #pragma omp target simd aligned(B::bfoo())); (17, OpenMP, #pragma omp target simd aligned(B,rp,::z: X::x)); (24, OpenMP, #pragma omp target simd aligned(B::ib,B:C1+C2)); (6, OpenMP, #pragma omp target simd aligned(z:B:bfoo())); (0, OpenMP, #pragma omp target simd aligned(B::ib:B:bfoo())); (1, OpenMP, #pragma omp target simd aligned(B:B::bfoo())); (14, OpenMP, #pragma omp target simd aligned(::z))"}
{"predict": "(3, OpenMP, #pragma omp parallel for)", "label": "(10, OpenMP, __kmpc_omp_task(NULL, 0, task);); (2, OpenMP, printf(\"%d: creating task\\n\", omp_get_thread_num());)"}
{"predict": "(27, OpenMP, #pragma omp parallel for)", "label": "(25, TBB, tbb::task_scheduler_init init;); (25, TBB, tbb::parallel_for( tbb::blocked_range<int>(0,n_cams), vtask);)"}
{"predict": "(7, MPI, const int world_rank = mpi_world_communicator.Rank();) (7, MPI, const int world_size = mpi_world_communicator.Size();) (8, MPI, double local = 2.0;)\n(8, MPI, double result = mpi_world_communicator.Sum(local, root);)\n(12, MPI, KRATOS_EXPECT_EQ(result, 2.0*world_size);)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(65, CUDA, gpu_timer.Start();) (71, CUDA, gpu_timer.Stop();)", "label": "(34, CUDA, CubDebugExit(cudaGetDevice(&device_ordinal));); (42, CUDA, CubDebugExit(cudaDeviceGetAttribute(&max_block_threads, cudaDevAttrMaxThreadsPerBlock, device_ordinal));); (40, CUDA, CubDebugExit(cudaDeviceGetAttribute(&sm_count, cudaDevAttrMultiProcessorCount, device_ordinal));); (69, CUDA, retval = CubDebug(cudaDeviceSynchronize());)"}
{"predict": "(89, OpenMP, #pragma omp parallel for collapse(2))", "label": "(92, OpenMP, double tic = omp_get_wtime();); (108, OpenMP, #pragma omp target exit data map(from: u[0:n*n])); (116, OpenMP, double stop = omp_get_wtime();); (103, OpenMP, double toc = omp_get_wtime();); (6, OpenMP, double start = omp_get_wtime();); (80, OpenMP, #pragma omp target enter data map(to: u[0:n*n], u_tmp[0:n*n]))"}
{"predict": ":     // Clean up\n99:     test_error( error, \"Unable to clean up\" );\n100:     clReleaseMemObject( read_only_image );\n101:     if(gTestReadWrite)\n102:     {\n103:         clReleaseMemObject( read_write_image );\n104:     }\n105:     clReleaseMemObject( actualSampler );\n106:     clReleaseMemObject( resultValues );\n107:     clFinish( queue );\n108:     return error;\n109: }", "label": "(46, OpenCL, actualSampler = clCreateSampler( context, CL_FALSE, CL_ADDRESS_NONE, CL_FILTER_NEAREST, &error );); (55, OpenCL, clEnqueueWriteBuffer( queue, results, CL_TRUE, 0, resultValuesSize, resultValues, 0, NULL, NULL );); (80, OpenCL, error = clEnqueueReadBuffer( queue, results, CL_TRUE, 0, resultValuesSize, resultValues, 0, NULL, NULL );); (75, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 3, NULL, threads, NULL, 0, NULL, NULL );); (60, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_mem ), &read_write_image );); (98, OpenCL, clReleaseMemObject(results);); (96, OpenCL, clReleaseSampler(actualSampler);); (100, OpenCL, clReleaseMemObject(read_write_image);); (96, OpenCL, clReleaseMemObject(read_only_image);); (51, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_mem ), &read_only_image );); (59, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_mem ), &results );); (56, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_sampler ), &actualSampler );); (38, OpenCL, cl_mem results = clCreateBuffer( context, 0, imageInfo->width * imageInfo->height * imageInfo->arraySize * sizeof(cl_int), NULL, &error);)"}
{"predict": "(14, OpenMP, #pragma omp parallel for private(i, offset))\n(41, OpenMP, #pragma omp parallel for private(i, offset))\n(44, OpenMP, #pragma omp parallel for private(i, offset))\n(47, OpenMP, #pragma omp parallel for private(i, offset))\n(50, OpenMP, #pragma omp parallel for private(i, offset))\n(53, OpenMP, #pragma omp parallel for private(i, offset))\n(56, OpenMP, #pragma omp parallel for private(i, offset))\n(59, OpenMP, #pragma omp parallel for private(i, offset))\n(62, OpenMP, #pragma omp parallel for private(i, offset))\n(65, OpenMP, #pragma omp parallel for private(i, offset))\n(68, OpenMP, #pragma omp parallel for private(i, offset))\n(71, OpenMP, #pragma omp parallel for private(i, offset))\n(74, OpenMP, #pragma omp parallel for private(i, offset))", "label": "(51, OpenMP, omp_set_num_threads(num_operator);); (16, MPI, MPI_Init(&argc, &argv);); (37, CUDA, cudaMalloc((void**)&d_a, bufsize);); (80, CUDA, cudaFree(d_a);); (24, CUDA, cudaMallocHost((void**)&h_a, bufsize);); (25, CUDA, cudaMallocHost((void**)&h_c, bufsize);); (58, CUDA, cudaDeviceSynchronize();); (78, CUDA, cudaFree(d_c);); (44, OpenMP, #pragma omp parallel); (75, CUDA, cudaFree(d_b);); (30, CUDA, cudaMalloc((void**)&d_b, bufsize);); (43, OpenMP, int i = omp_get_thread_num();); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (16, CUDA, cudaMallocHost((void**)&h_b, bufsize);); (75, CUDA, cudaFreeHost(h_b);); (73, CUDA, cudaFreeHost(h_a);); (25, CUDA, cudaMalloc((void**)&d_c, bufsize);); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &np);); (74, MPI, MPI_Finalize();); (71, CUDA, cudaFreeHost(h_c);)"}
{"predict": "(9, OpenMP, #pragma omp simd private(myind,g_ind) linear(ind) aligned(arr) linear(uval(ref)))", "label": "(7, OpenMP, #pragma omp simd private(myind, g_ind), linear(ind), aligned(arr), linear(uval(ref)))"}
{"predict": "(12, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);) (13, MPI, MPI_Comm_size(MPI_COMM_WORLD, &count);)", "label": "(29, MPI, MPI_Type_contiguous(1, struct_type, &contig);); (51, MPI, MPI_Type_free(&struct_type);); (26, MPI, MPI_Type_create_struct(3, blens, displs, types, &struct_type);); (27, MPI, MPI_Type_commit(&struct_type);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (26, MPI, MPI_Type_commit(&contig);); (45, MPI, MPI_Get_elements(&status, contig, &count);); (34, MPI, MPI_Sendrecv(&sendbuf, 1, contig, 0, 0, &recvbuf, 1, contig, 0, 0, MPI_COMM_SELF, &status);); (45, MPI, MPI_Type_free(&contig);)"}
{"predict": "(8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)", "label": "(4, MPI, MPI_Init(&argc, &argv);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (11, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(14, OpenMP, #pragma omp parallel for reduction(+:serial_list_time)) (26, OpenMP, #pragma omp parallel for reduction(+:tbb_list_time)) (39, OpenMP, #pragma omp parallel for reduction(+:serial_tree_time)) (49, OpenMP, #pragma omp parallel for reduction(+:tbb_tree_time))", "label": "(28, TBB, tbb_list_time = (tbb::tick_count::now() - t0).seconds();); (42, TBB, serial_tree_time = (tbb::tick_count::now() - t0).seconds();); (15, TBB, serial_list_time = (tbb::tick_count::now() - t0).seconds();); (12, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (50, TBB, tbb_tree_time = (tbb::tick_count::now() - t0).seconds();)"}
{"predict": "1:     ;\n72: // expected-error@+1 {{argument to 'collapse' clause must be a strictly positive integer value}}\n73:   for (i = 0; i < 16; ++i)\n74:     ;\n75: // expected-error@+1 {{argument to 'collapse' clause must be a strictly positive integer value}}\n76:   for (i = 0; i < 16; ++i)\n77:     ;\n78: // expected-error@+1 {{argument to 'collapse' clause must be a strictly positive integer value}}\n79:   for (i = 0; i < 16; ++i)\n80:     ;\n81: // expected-error@+1 {{argument to 'collapse' clause must be a strictly positive integer value}}\n82:   for (i = 0; i < 16; ++i)\n83:     ;\n84: // expected-error@+1 {{argument to 'collapse' clause must be a strictly positive integer value}}\n85:   for (i = 0; i < 16; ++i)\n86:     ;\n87: // expected-error@+1 {{argument to 'collapse' clause must be a strictly positive integer value}}\n88:   for (i = 0; i < 16; ++i)\n89:     ;\n90: // expected-error@+1 {{argument to 'collapse' clause must be a strictly positive integer value}}\n91:   for (i = 0; i < 16; ++i)\n92:     ;\n93: // expected-error@+1 {{argument to 'collapse' clause must be a strictly positive integer value}}\n94:   for (i = 0; i < 16; ++i)\n95:     ;\n96: // expected-error@+1 {{argument to 'collapse' clause must be a strictly positive integer value}}\n97:   for (i = 0; i < 16; ++i)\n98:     ;\n99: // expected-error@+1 {{argument to 'collapse' clause must be a strictly positive integer value}}\n100: // expected-error@+1 {{argument to 'collapse' clause must be a strictly positive integer value}}\n101: // expected-error@+1 {{argument to 'collapse' clause must", "label": "(31, OpenMP, #pragma omp target parallel for collapse(4); (54, OpenMP, #pragma omp target parallel for collapse(4, , 4)); (43, OpenMP, #pragma omp target parallel for collapse(4)); (2, OpenMP, #pragma omp target parallel for collapse); (22, OpenMP, #pragma omp target parallel for collapse 4)); (73, OpenMP, #pragma omp target parallel for collapse(-5)); (3, OpenMP, #pragma omp target parallel for collapse(); (79, OpenMP, #pragma omp target parallel for collapse(5 - 5)); (82, OpenMP, #pragma omp target parallel for collapse(2) firstprivate(i)); (41, OpenMP, #pragma omp target parallel for collapse(4 4)); (60, OpenMP, #pragma omp target parallel for collapse(2.5)); (63, OpenMP, #pragma omp target parallel for collapse(foo())); (29, OpenMP, #pragma omp target parallel for collapse(4, )); (0, OpenMP, #pragma omp target parallel for collapse()); (68, OpenMP, #pragma omp target parallel for collapse(0)); (2, OpenMP, #pragma omp target parallel for collapse(,); (20, OpenMP, #pragma omp target parallel for collapse(4,); (4, OpenMP, #pragma omp target parallel for collapse(, )); (48, OpenMP, #pragma omp target parallel for collapse(4, 8))"}
{"predict": "(11, CUDA, d_data = nullptr;)\n(13, CUDA, CURAND_CHECK(curandCreateGenerator(&gen, CURAND_RNG_PSEUDO_MT19937));)\n(16, CUDA, CURAND_CHECK(curandSetStream(gen, stream));)\n(19, CUDA, CURAND_CHECK(curandSetGeneratorOrdering(gen, order));)\n(22, CUDA, CURAND_CHECK(curandSetPseudoRandomGeneratorSeed(gen, seed));)\n(25, CUDA, CURAND_CHECK(curandGeneratePoisson(gen, d_data, h_data.size(), lambda));)\n(28, CUDA, CUDA_CHECK(cudaMemcpyAsync(h_data.data(), d_data, sizeof(data_type) * h_data.size(), cudaMemcpyDeviceToHost, stream));)\n(33, CUDA, cudaStreamSynchronize(stream);)\n(35, CUDA, CURAND_CHECK(curandDestroyGenerator(gen));)", "label": "(32, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (34, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(11, CUDA, err = cudaDeviceSynchronize();)", "label": "(5, CUDA, err = cudaMalloc(&numErr_d, sizeof(int));); (25, CUDA, printf(\"Cuda Main: cudaFree failed with %s\\n\", cudaGetErrorString(err));); (11, CUDA, err = cudaDeviceSynchronize();); (4, CUDA, printf(\"Cuda Main: cudaMemcpy failed with %s\\n\", cudaGetErrorString(err));); (20, CUDA, err = cudaFree(numErr_d);); (13, CUDA, err = cudaMemcpy(&numErr_h, numErr_d, sizeof(int), cudaMemcpyDeviceToHost);); (2, CUDA, cudaFree(numErr_d);)"}
{"predict": "(1, CUDA, __global__ void histogram256Kernel(uint *d_PartialHistograms, uint *d_Data, uint dataCount) {)", "label": "(5, CUDA, __shared__ uint s_Hist[HISTOGRAM256_THREADBLOCK_MEMORY];)"}
{"predict": "(7, MPI, MPI_Comm_rank(comm, &par_rank);) (8, MPI, MPI_Comm_size(comm, &par_size);) (14, MPI, ASCENT_INFO(\"Rank \" << par_rank << \" of \" << par_size << \" reporting\");)", "label": "(55, MPI, ascent_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (7, MPI, MPI_Comm_rank(comm, &par_rank);); (7, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(18, OpenMP, #pragma omp parallel for reduction(+:sum) private(i))", "label": "(17, OpenMP, #pragma omp parallel for default(shared) private(i))"}
{"predict": "(28, CUDA, float res_host = res;)", "label": "(35, CUDA, cudaFree(res_dev);); (28, CUDA, cudaMalloc(&res_dev, sizeof(float));); (30, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(17, MPI, MPI_Comm_rank(comm, &par_rank);) (18, MPI, MPI_Comm_size(comm, &par_size);)", "label": "(43, MPI, ascent_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (15, MPI, MPI_Comm_rank(comm, &par_rank);); (15, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "h_C.data(), mbC, nbC, ic, jc, lldc, nprow, npcol, myprow, mypcol);\n101: \n102:     CUDA_CHECK(cudaMalloc(&d_A, h_A.size() * sizeof(double)));\n103:     CUDA_CHECK(cudaMalloc(&d_C, h_C.size() * sizeof(double)));\n104: \n105:     CUDA_CHECK(cudaMemcpy(d_A, h_A.data(), h_A.size() * sizeof(double), cudaMemcpyHostToDevice));\n106:     CUDA_CHECK(cudaMemcpy(d_C, h_C.data(), h_C.size() * sizeof(double), cudaMemcpyHostToDevice));\n107: \n108:     CUDA_CHECK(cudaMalloc(&d_work, workspaceInBytesOnDevice));\n109: \n110:     CUBLASMP_CHECK(cublasMpSetMatrixDescriptor(descA, CUBLASMP_MATRIX_LAYOUT_ROW_MAJOR, CUBLASMP_DATA_TYPE_DOUBLE, global_m_a, global_n_a, llda, ia, ja));\n111:     CUBLASMP_CHECK(cublasMpSetMatrixDescriptor(descC, CUBLASMP_MATRIX_LAYOUT_ROW_MAJOR, CUBLASMP_DATA_TYPE_DOUBLE, global_m_c, global_n_c, lldc, ic, jc));\n112: \n113:     CUBLASMP_CHECK(cublasMpGemm(handle, CUBLASMP_OP_N, CUBLASMP_OP_N, global_m_c, global_n_c, global_n_a, &alpha, descA, d_A, descC, d_C, &beta, descC, d_C, d_work, workspaceInBytesOnHost));\n114: \n115:     CUDA_CHECK(cudaMemcpy(h_C.data(), d_C, h_C.size() * sizeof(double), cudaMemcpyDeviceToHost));\n116: \n117:     cal_comm_destroy(cal_comm);\n118: \n119:     cublasMpDestroy(handle);\n120: \n121:     cudaFree(d_A);\n122:     cudaFree(d_C);\n123:     cudaFree(d_work);\n124: \n125:     return 0;\n126: }", "label": "(197, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (109, CUDA, CUDA_CHECK(cudaMemcpyAsync(d_A, h_A.data(), llda * loc_n_a * sizeof(double), cudaMemcpyHostToDevice, stream));); (197, MPI, MPI_Barrier(MPI_COMM_WORLD);); (48, CUDA, CUDA_CHECK(cudaFree(nullptr));); (186, CUDA, CUDA_CHECK(cudaFreeAsync(d_C, stream));); (45, CUDA, CUDA_CHECK(cudaSetDevice(local_device));); (20, MPI, MPI_Init(nullptr, nullptr);); (184, CUDA, CUDA_CHECK(cudaFreeAsync(d_work, stream));); (99, CUDA, CUDA_CHECK(cudaMallocAsync(&d_A, llda * loc_n_a * sizeof(double), stream));); (141, MPI, const double begin = MPI_Wtime();); (179, CUDA, CUDA_CHECK(cudaFreeAsync(d_A, stream));); (33, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (96, CUDA, CUDA_CHECK(cudaMallocAsync(&d_C, lldc * loc_n_c * sizeof(double), stream));); (98, CUDA, CUDA_CHECK(cudaMemcpyAsync(d_C, h_C.data(), lldc * loc_n_c * sizeof(double), cudaMemcpyHostToDevice, stream));); (55, CUDA, CUDA_CHECK(cudaStreamCreate(&stream));); (186, MPI, MPI_Finalize();); (27, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nranks);); (126, CUDA, CUDA_CHECK(cudaMallocAsync(&d_work, workspaceInBytesOnDevice, stream));); (157, MPI, const double end = MPI_Wtime();)"}
{"predict": "(15, CUDA, auto gpuTensor = CreateConstOnGPU(variant_proto);); (22, CUDA, TF_CHECK_OK(session.Run({gpuTensor}, &outputs));)", "label": "(1, CUDA, if (!IsGoogleCudaEnabled()) return;)"}
{"predict": "(1, CUDA, __global__ void KerLimitsCell(unsigned n, unsigned pini, unsigned cellcode, const unsigned *dcell, const typecode *code, unsigned *results) {)", "label": "(3, CUDA, extern __shared__ unsigned scx1[];)"}
{"predict": "(1, CUDA, __global__ void layernorm_backward_kernel(float* dinp, float* dweight, float* dbias, float* scratch, const float* dout, const float* inp, const float* weight, const float* mean, const float* rstd, int B, int T, int C) {); (89, CUDA, } __syncthreads(); if (*tmp_flag == gridDim.x-1) { for(int i = threadIdx.x; i < C; i+= blockDim.x) { dbias[i] = (float)scratch_dbias[i]; dweight[i] = (float)scratch_dweight[i]; } }));", "label": "(3, CUDA, extern __shared__ float shared[];)"}
{"predict": "(35, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(48, MPI, MPI_Startall(2, requests);); (50, MPI, MPI_Waitall(2, requests, statuses);); (57, MPI, t1 = MPI_Wtime();); (12, MPI, MPI_Init(&argc, &argv);); (61, MPI, MPI_Finalize();); (11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (31, MPI, MPI_Barrier(MPI_COMM_WORLD);); (31, MPI, t0 = MPI_Wtime();)"}
{"predict": "(32, OpenCL, cl_int retVal = test_buffer_kernel( context, queue, vecType[ typeIndex ], vecSizes[ index ], numElements, 1, seed );); (34, OpenCL, if (retVal != 0) { log_error(\"   Buffer test %s%s FAILED\\n\", get_explicit_type_name(vecType[typeIndex]), sizeNames[vecSizes[index]]); retVal++; });", "label": "(12, OpenCL, retVal = clGetDeviceInfo(device, CL_DEVICE_PROFILE, sizeof(profile), profile, NULL);)"}
{"predict": "(3, CUDA, if (cudaSuccess != cudaStreamGetAttribute(cudaStream, attr, &nativeObject))) (5, CUDA, if (cudaSuccess != cudaStreamGetAttribute(cudaStream, attr, &nativeObject))) (10, CUDA, if (cudaSuccess != cudaStreamGetAttribute(cudaStream, attr, &nativeObject))) (24, CUDA, if (cudaSuccess != cudaStreamGetAttribute(cudaStream, attr, &nativeObject)))", "label": "(21, CUDA, setCudaAccessPolicyWindow(env, javaObject, nativeObject.accessPolicyWindow);)"}
{"predict": "(14, CUDA, for(int i = 0; i < numAmps; ++i) {)", "label": "(22, CUDA, fptype ret = thrust::norm(totalAmp);)"}
{"predict": "(8, OpenMP, #pragma omp parallel for private(index, ctx))", "label": "(6, OpenMP, #pragma omp parallel for)"}
{"predict": "(28, MPI, MPI_Get_address(oldtypes[i], &true_lb); MPI_Get_extent(oldtypes[i], &true_lb, &true_extent);)", "label": "(38, MPI, MPI_Type_free(&aligned_type);); (36, MPI, MPI_Type_commit(newtype);); (33, MPI, MPI_Type_create_struct(num, blocklens, displs, types, &aligned_type);); (33, MPI, MPI_Type_create_resized(aligned_type, 0, disp, newtype);); (23, MPI, MPI_Type_get_true_extent(oldtypes[i], &true_lb, &true_extent);)"}
{"predict": "(14, OpenMP, #pragma omp parallel for)", "label": "(8, OpenCL, void *mappedPtr = clEnqueueMapImage(cmdQ.get(), image.get(), CL_TRUE, CL_MAP_WRITE, &origin[0], &region[0], nullptr, nullptr, 0, nullptr, nullptr, &retVal);); (14, OpenCL, retVal = clEnqueueUnmapMemObject(cmdQ.get(), image.get(), mappedPtr, 0, nullptr, nullptr);)"}
{"predict": "(3, OpenMP, #pragma omp parallel for)", "label": "(29, OpenMP, #pragma omp target simd defaultmap(tofrom, scalar); (1, OpenMP, #pragma omp target simd defaultmap); (9, OpenMP, #pragma omp target simd defaultmap(tofrom); (20, OpenMP, #pragma omp target simd defaultmap(tofrom,); (10, OpenMP, #pragma omp target simd defaultmap(tofrom:); (15, OpenMP, #pragma omp target simd defaultmap(tofrom scalar)); (20, OpenMP, #pragma omp target simd defaultmap(scalar:); (-2, OpenMP, #pragma omp target simd defaultmap(); (9, OpenMP, #pragma omp target simd defaultmap(tofrom)); (-1, OpenMP, #pragma omp target simd defaultmap())"}
{"predict": "(2, MPI, int par_rank; int par_size; MPI_Comm comm = MPI_COMM_WORLD; MPI_Comm_rank(comm, &par_rank); MPI_Comm_size(comm, &par_size);)", "label": "(85, MPI, ascent_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (91, MPI, MPI_Barrier(comm);); (16, MPI, MPI_Comm_rank(comm, &par_rank);); (16, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(4, OpenMP, #pragma omp parallel for)", "label": "(2, OpenMP, #pragma omp target map(alloc:colstat[0:NZ])); (5, OpenMP, #pragma omp target map(alloc:colstat[:0])); (9, OpenMP, #pragma omp target update from(colstat))"}
{"predict": "(37, OpenMP, #pragma omp parallel for)", "label": "(41, TBB, ASSERT( (int)tbb::tbb_thread::hardware_concurrency() == availableProcs, NULL );); (39, TBB, ASSERT( tbb::task_scheduler_init::default_num_threads() == availableProcs, NULL );)"}
{"predict": "(9, CUDA, if (err.first != d_reference_end) { return CompareDeviceResults(host_reference + index, d_data + index, 1, true, g_verbose); } else { return 0; } )", "label": "(7, CUDA, auto err = thrust::mismatch(thrust::device, d_reference_begin, d_reference_end, d_data);); (2, CUDA, CubDebugExit(cudaMemcpy(d_tmp_buffer, host_reference, sizeof(T) * num_items, cudaMemcpyHostToDevice));); (9, CUDA, const auto index = thrust::distance(d_reference_begin, err.first);)"}
{"predict": "(13, CUDA, CUDA_SAFE_CALL(cudaMalloc((void**)&device_buffer, buffer_size * sizeof(value_type)));)", "label": "(12, CUDA, CUDA_SAFE_CALL(cudaMalloc(&device_buffer, buffer_size * sizeof(value_type)));)"}
{"predict": "(24, CUDA, transpose<true><<<grid, block>>>(ptr, rows, cols, (*samples)[devi].get());); (27, CUDA, transpose<false><<<grid, block>>>(ptr, rows, cols, (*samples)[devi].get());)", "label": "(8, CUDA, cudaSetDevice(devs[0]);); (5, CUDA, CUCH(cudaMallocManaged(&ptr, size), kmcudaMemoryAllocationFailure);); (7, CUDA, CUCH(cudaMemcpy(ptr, (*samples)[0].get(), size, cudaMemcpyDefault), kmcudaMemoryCopyError);)"}
{"predict": "(34, CUDA, CUDA_CHECK(cudaMemcpyAsync(d_A, A.data(), sizeof(data_type) * A.size(), cudaMemcpyHostToDevice, stream));) (36, CUDA, CUDA_CHECK(cudaMemcpyAsync(d_B, B.data(), sizeof(data_type) * B.size(), cudaMemcpyHostToDevice, stream));) (40, CUDA, CUBLAS_CHECK(cublasAxpyEx(cublasH, A.size(), &alpha, traits<data_type>::cuda_data_type, d_A, traits<data_type>::cuda_data_type, incx, d_B, traits<data_type>::cuda_data_type, incy, traits<data_type>::cuda_data_type));) (46, CUDA, CUDA_CHECK(cudaMemcpyAsync(B.data(), d_B, sizeof(data_type) * B.size(), cudaMemcpyDeviceToHost, stream));)", "label": "(51, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (33, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_B), sizeof(data_type) * B.size()));); (61, CUDA, CUDA_CHECK(cudaFree(d_B));); (66, CUDA, CUDA_CHECK(cudaDeviceReset());); (63, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (28, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (56, CUDA, CUDA_CHECK(cudaFree(d_A));); (22, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(10, CUDA, int index = j*spatial*filters + filter*spatial + i + id;); (15, CUDA, local[id] += (i+id < spatial) ? powf((x[index] - mean[filter]), 2) : 0;); (21, CUDA, variance[filter] = 0;); (25, CUDA, variance[filter] += local[i];);", "label": "(3, CUDA, __shared__ float local[threads];)"}
{"predict": "(12, MPI, MPI_Comm_rank(comm, &par_rank);)\n(13, MPI, MPI_Comm_size(comm, &par_size);)", "label": "(47, OpenMP, EXPECT_TRUE(check_test_image(output_file, t_apcomp_baseline_dir()));); (9, MPI, MPI_Comm_rank(comm, &par_rank);); (10, MPI, apcomp::mpi_comm(MPI_Comm_c2f(comm));); (8, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(28, MPI, MPI_Comm_rank(shm_comm, &shm_rank);); (15, MPI, MPI_Win_shared_query(shm_win, MPI_PROC_NULL, &query_size, &query_disp_unit, &query_base);); (13, MPI, MPI_Win_allocate_shared(size, sizeof(int), MPI_INFO_NULL, MPI_COMM_SELF, &my_base, &shm_win);); (23, MPI, MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, rank, MPI_INFO_NULL, &shm_comm);); (34, MPI, MPI_Win_allocate_shared(0, sizeof(int), MPI_INFO_NULL, shm_comm, &my_base, &shm_win);); (20, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (67, MPI, MPI_Comm_free(&shm_comm);); (15, MPI, MPI_Win_free(&shm_win);); (21, MPI, MPI_Comm_size(shm_comm, &shm_nproc);); (31, MPI, MPI_Win_allocate_shared(size, sizeof(int), MPI_INFO_NULL, shm_comm, &my_base, &shm_win);)"}
{"predict": "(14, CUDA, cudaMalloc((void**)&d_arr, len * sizeof(int));) (15, CUDA, cudaMalloc((void**)&d_out, len * sizeof(int));) (24, CUDA, cudaMemcpy(d_arr, arr, len * sizeof(int), cudaMemcpyHostToDevice);) (27, CUDA, cudaMemcpy(out, d_out, gridsize * sizeof(int), cudaMemcpyDeviceToHost);) (51, CUDA, cudaFree(d_arr);) (52, CUDA, cudaFree(d_out);)", "label": "(13, CUDA, cudaMalloc((void **)&d_arr, sizeof(int) * len);); (16, CUDA, cudaMemcpy(d_arr, arr, sizeof(int) * len, cudaMemcpyHostToDevice);); (53, CUDA, cudaFree(d_out);); (11, CUDA, cudaMalloc((void **)&d_out, sizeof(int) * len);); (23, CUDA, cudaMemcpy(out, d_out, sizeof(int) * len, cudaMemcpyDeviceToHost);); (49, CUDA, cudaFree(d_arr);)"}
{"predict": "(7, OpenMP, #pragma omp parallel for) (12, OpenMP, #pragma omp parallel for reduction(+:sum))", "label": "(4, OpenMP, int *shared_ptr = llvm_omp_target_alloc_shared(N * sizeof(int), device);); (5, OpenMP, #pragma omp target teams distribute parallel for device(device)); (0, OpenMP, const int device = omp_get_default_device();); (13, OpenMP, omp_target_free(shared_ptr, device);)"}
{"predict": "(13, OpenMP, #pragma omp parallel for reduction(+:j))", "label": "(52, TBB, tbb::flow::make_edge( b2, b3 );); (50, TBB, tbb::flow::make_edge( b, b2 );); (73, TBB, tbb::flow::buffer_node<T> b_copy(b);); (2, TBB, tbb::flow::buffer_node<T> b3(g);); (0, TBB, tbb::flow::buffer_node<T> b2(g);); (-3, TBB, tbb::flow::graph g;); (-3, TBB, tbb::flow::buffer_node<T> b(g);)"}
{"predict": "(10, OpenCL, cl_event outEvent1;); (18, OpenCL, cl_event outEvent2;); (22, OpenCL, clWaitForEvents(1, &outEvent2);)", "label": "(22, OpenCL, clReleaseEvent(outEvent1);)"}
{"predict": "(28, CUDA, cuda(EventSynchronize(start));) (30, CUDA, cuda(EventSynchronize(stop));)", "label": "(59, CUDA, cudaFree( deviceClockValues );); (60, CUDA, cudaFreeHost( hostOut );); (61, CUDA, cudaEventDestroy( stop );); (59, CUDA, cudaEventDestroy( start );); (56, CUDA, cudaFree( deviceThreadIDs );)"}
{"predict": "(11, MPI, MPI_Init(NULL, NULL);) (75, MPI, MPI_Finalize();)", "label": "(63, MPI, t2 = MPI_Wtime();); (58, MPI, t1 = MPI_Wtime();)"}
{"predict": "4Body::DtoAP1_AtoVP2_VtoP3P4, _mD0, 3, 2, 0, 1));\n58:     SFA1RD.push_back(new SpinFactor(\"SF\", SF_4Body::DtoAP1_AtoVP2_VtoP3P4, _mD0, 0, 2, 3, 1));\n59: \n60:     std::vector<SpinFactor *> SFK2R;\n61:     SFK2R.push_back(new SpinFactor(\"SF\", SF_4Body::DtoAP1_AtoVP2_VtoP3P4, _mD0, 3, 2, 0, 1));\n62:     SFK2R.push_back(new SpinFactor(\"SF\", SF_4Body::DtoAP1_AtoVP2_VtoP3P4, _mD0, 0, 2, 3, 1));\n63: \n64:     std::vector<SpinFactor *> SFK3R;\n65:     SFK3R.push_back(new SpinFactor(\"SF\", SF_4Body::DtoAP1_AtoVP2_VtoP3P4, _mD0, 3, 2, 0, 1));\n66:     SFK3R.push_back(new SpinFactor(\"SF\", SF_4Body::DtoAP1_AtoVP2_VtoP3P4, _mD0, 0, 2, 3, 1));\n67: \n68:     std::vector<SpinFactor *> SFK1D;\n69:     SFK1D.push_back(new SpinFactor(\"SF\", SF_4Body::DtoAP1_AtoVP2_VtoP3P4, _mD0, 3, 2, 0, 1));\n70:     SFK1D.push_back(new SpinFactor(\"SF\", SF_4Body::DtoAP1_AtoVP2_VtoP3P4, _mD0, 0, 2, 3, 1));\n71: \n72:     std::vector<SpinFactor *> SFK2D;\n73:     SFK2D.push_back(new SpinFactor(\"SF\", SF_4Body::DtoAP1_AtoVP2_VtoP3P4", "label": "(184, CUDA, long accepted  = thrust::count_if(flags.begin(), flags.end(), thrust::identity<bool>());)"}
{"predict": "_array1 created\\n\");\n118: \n119:     /* create a memory dataspace with size of the slab */\n120:     mem_dataspace = H5Screate_simple (RANK, block, NULL);\n121:     VRFY((mem_dataspace >= 0), \"H5Screate_simple succeeded\");\n122: \n123:     /* create a file dataspace */\n124:     file_dataspace = H5Dget_space(dataset1);\n125:     VRFY((file_dataspace >= 0), \"H5Dget_space succeeded\");\n126: \n127:     /* set up transfer property list */\n128:     xfer_plist = H5Pcreate(H5P_DATASET_XFER);\n129:     VRFY((xfer_plist >= 0), \"H5Pcreate succeeded\");\n130:     ret = H5Pset_dxpl_mpio(xfer_plist, H5FD_MPIO_COLLECTIVE);\n131:     VRFY((ret >= 0), \"H5Pset_dxpl_mpio succeeded\");\n132: \n133:     /* write data to dataset collectively */\n134:     ret = H5Dwrite(dataset1, H5T_NATIVE_INT, mem_dataspace, file_dataspace, xfer_plist, data_array1);\n135:     VRFY((ret >= 0), \"H5Dwrite succeeded\");\n136: \n137:     /* release resource */\n138:     H5Sclose(file_dataspace);\n139:     H5Sclose(mem_dataspace);\n140:     H5Pclose(xfer_plist);\n141: \n142:     /* close dataset collectively */\n143:     ret = H5Dclose(dataset1);\n144:     VRFY((ret >= 0), \"H5Dclose succeeded\");\n145: \n146:     /* close file collectively */\n147:     ret = H5Fclose(fid);\n148:     VRFY((ret >= 0), \"H5Fclose succeeded\");\n149: \n150:     /* free data_array1 */\n151:     HDfree(data_array1);\n152: \n153:", "label": "(34, MPI, MPI_Comm_size(MPI_COMM_WORLD,&mpi_size);); (34, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&mpi_rank);)"}
{"predict": "(7, CUDA, __shared__ uint64_t s_QHatModp[256 * unroll_number];)", "label": "(6, CUDA, extern __shared__ uint64_t s_QHatModp[];)"}
{"predict": "\");\n88:     }\n89: \n90:     if (me == 0) {\n91:         data[0].loc = 0;\n92:         data[0].val = 0;\n93:     }\n94:     MPI_Win_fence(0, win);\n95:     /* 2.b. Large array test */\n96:     if (me == nproc - 1) {\n97:         MPI_Accumulate(mine, ARRAY_LEN, MPI_2INT, 0, 0, ARRAY_LEN, MPI_2INT, MPI_REPLACE, win);\n98:         MPI_Accumulate(mine_plus, ARRAY_LEN, MPI_2INT, 0, 0, ARRAY_LEN, MPI_2INT, MPI_REPLACE, win);\n99:     }\n100: \n101:     MPI_Win_fence(0, win);\n102:     if (me == 0) {\n103:         verify_nondeterministic_result(data, ARRAY_LEN, expected,\n104:                                        \"Large array test case for none ordering\");\n105:     }\n106:     MPI_Win_free(&win);\n107:     MPI_Info_free(&info_in);\n108:     free(data);\n109:     free(expected);\n110:     free(mine);\n111:     free(mine_plus);\n112:     MTest_Finalize();\n113:     return 0;\n114: }", "label": "(81, MPI, MPI_Win_create(data, sizeof(twoint_t) * ARRAY_LEN, 1, info_in, MPI_COMM_WORLD, &win);); (81, MPI, MPI_Info_free(&info_in);); (134, MPI, MPI_Accumulate(mine_plus, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_MAXLOC, win);); (151, MPI, MPI_Accumulate(mine_plus, ARRAY_LEN, MPI_2INT, 0, 0, ARRAY_LEN, MPI_2INT, MPI_MAXLOC, win);); (34, MPI, MPI_Win_fence(0, win);); (56, MPI, MPI_Accumulate(mine, ARRAY_LEN, MPI_2INT, 0, 0, ARRAY_LEN, MPI_2INT, MPI_REPLACE, win);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (63, MPI, MPI_Win_free(&win);); (70, MPI, MPI_Info_create(&info_in);); (70, MPI, MPI_Info_set(info_in, \"accumulate_ordering\", \"none\");); (32, MPI, MPI_Accumulate(mine, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_REPLACE, win);); (181, MPI, MPI_Accumulate(mine, ARRAY_LEN, MPI_2INT, 0, 0, ARRAY_LEN, MPI_2INT, MPI_MINLOC, win);); (31, MPI, MPI_Accumulate(mine_plus, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_REPLACE, win);); (48, MPI, MPI_Accumulate(mine_plus, ARRAY_LEN, MPI_2INT, 0, 0, ARRAY_LEN, MPI_2INT, MPI_REPLACE, win);); (160, MPI, MPI_Accumulate(mine, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_MINLOC, win);); (1, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (20, MPI, MPI_Win_create(data, sizeof(twoint_t) * ARRAY_LEN, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &win);)"}
{"predict": "(26, CUDA, auto size_value = cudaGetDeviceCount(); for (int i = 0; i < size_value; ++i) { CUDA_CHECK(cudaSetDevice(i)); CuEVM::kernel_evm_multiple_instances<<<num_blocks, CGBN_TPI * CGBN_IBP>>>(report, instances_data, num_instances); }});", "label": "(56, CUDA, cudaEventRecord(stop);); (22, CUDA, CUDA_CHECK(cudaDeviceSetLimit(cudaLimitStackSize, 2 * 1024));); (55, CUDA, cudaEventSynchronize(stop);); (58, CUDA, cudaEventDestroy(start);); (5, CUDA, CUDA_CHECK(cudaDeviceReset());); (21, CUDA, CUDA_CHECK(cudaDeviceSynchronize());); (46, CUDA, cudaEventRecord(start);); (55, CUDA, cudaEventDestroy(stop);); (57, CUDA, CUDA_CHECK(cudaGetLastError());); (8, CUDA, cudaDeviceGetLimit(&size_value, cudaLimitStackSize);); (12, CUDA, CUDA_CHECK(cudaDeviceSetLimit(cudaLimitMallocHeapSize, heap_size));); (40, CUDA, cudaEventCreate(&stop);); (46, CUDA, cudaEventElapsedTime(&milliseconds, start, stop);); (37, CUDA, cudaEventCreate(&start);)"}
{"predict": "(16, OpenMP, #pragma omp parallel for reduction(+:s))", "label": "(19, OpenMP, #pragma omp parallel for reduction(+:s)); (10, OpenMP, #pragma omp parallel for)"}
{"predict": "(10, MPI, MPI_Allreduce(input, output, 3, MPI_UINT64_T, MPI_MINMAXSUM, comm);)", "label": "(10, MPI, MPI_Allreduce(input, output, 1, dtcmp_type_3uint64t, dtcmp_reduceop_mms_3uint64t, comm);)"}
{"predict": "(14, MPI, MPI_Bcast(&array_size, 1, MPI_INT, 0, MPI_COMM_WORLD);) (15, MPI, MPI_Bcast(&nr_iters, 1, MPI_INT, 0, MPI_COMM_WORLD);) (33, MPI, MPI_Wait(&request, &status);) (46, MPI, MPI_Wait(&request, &status);)", "label": "(19, MPI, MPI_Bcast(&array_size, 1, MPI_INT, 0, MPI_COMM_WORLD);); (26, MPI, MPI_Wait(&request, &status);); (41, MPI, end_time = MPI_Wtime();); (66, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (2, MPI, MPI_Init(NULL, NULL);); (2, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (25, MPI, start_time = MPI_Wtime();); (12, MPI, MPI_Bcast(&nr_iters, 1, MPI_INT, 0, MPI_COMM_WORLD);); (6, MPI, MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);)"}
{"predict": "(3, OpenMP, #pragma omp parallel for private(saved_plain, saved_len, output, saved_ctx))", "label": "(2, OpenMP, omp_autotune(self, OMP_SCALE);)"}
{"predict": ":             for (j=0; j<(pin-p)+1; j++) { pdata[j] = p[j]; }\n118:             pdata[j] = 0;\n119:             /* Advance to the next entry in the array */\n120:             p += (*v1) * d3;\n121:             /* Advance to the next location in the data area */\n122:             pdata += (pin-p) + 2;   /* add 1 for the null */\n123:         }\n124:       }\n125:     }\n126: \n127:     /* Call the MPI function */\n128:     MPI_Comm_spawn_multiple(*v1, p2, v3, v4, v5, *v6, *v7, v8, v9, ierr);\n129: \n130: fn_cleanup3_2:\n131:     if (p3) {\n132:         for (k3=0; k3<*v1; k3++) {\n133:             MPIU_Free(p3[k3]);\n134:         }\n135:         MPIU_Free(p3);\n136:     }\n137: fn_cleanup3_1:\n138:     if (p2) {\n139:         MPIU_Free(p2[0]);\n140:         MPIU_Free(p2);\n141:     }\n142: fn_cleanup2_2:\n143:     if (ptmp2) {\n144:         MPIU_Free(ptmp2);\n145:     }\n146: fn_cleanup2_1:\n147:     return;\n148: }", "label": "(127, MPI, *ierr = MPI_Comm_spawn_multiple( *v1, p2, p3, v4, v5, *v6, (MPI_Comm)(*v7), (MPI_Comm *)(v8), (int *)v9 );)"}
{"predict": "(3, OpenMP, #pragma omp parallel for reduction(max:my_largest, localBestScore))", "label": "(10, OpenMP, #pragma omp parallel); (14, OpenMP, #pragma omp for); (95, OpenMP, #pragma omp critical)"}
{"predict": "(26, MPI, MPI_Waitall(2, requests, MPI_STATUSES_IGNORE);)", "label": "(22, MPI, MPI_Start(&requests[0]);); (17, MPI, MPI_Barrier_init(MPI_COMM_WORLD, MPI_INFO_NULL, &requests[1]);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (14, MPI, MPI_Bcast_init(&a, 1, MPI_INT, 0, MPI_COMM_WORLD, MPI_INFO_NULL, &requests[0]);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (18, MPI, MPI_Start(&requests[1]);); (23, MPI, MPI_Waitall(2, requests, MPI_STATUSES_IGNORE);); (5, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (27, MPI, MPI_Request_free(&requests[0]);); (27, MPI, MPI_Request_free(&requests[1]);)"}
{"predict": "(17, OpenMP, #pragma omp parallel for reduction(+:sum1))", "label": "(11, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:sum1) reduction(+:sum1) collapse(2))"}
{"predict": "(17, OpenMP, #pragma omp parallel for reduction(+:serial_time))\n(22, OpenMP, #pragma omp parallel for reduction(+:serial_tiled_time))\n(26, OpenMP, #pragma omp parallel for reduction(+:tbb_time))", "label": "(16, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (22, TBB, serial_tiled_time = (tbb::tick_count::now() - t0).seconds();); (27, TBB, tbb_time = (tbb::tick_count::now() - t0).seconds();); (15, TBB, serial_time = (tbb::tick_count::now() - t0).seconds();)"}
{"predict": "7:     // 4. row/col\n98:     bwprofile[iter++] = profileTransfer(d_b, h_aCached, nElements, cudaMemcpyHostToDevice, \"cached to local\", verbose);\n99:     bwprofile[iter++] = profileTransfer(h_bPageable, h_aCached, nElements, cudaMemcpyHostToHost, \"cached to pageable\", verbose);\n100:     bwprofile[iter++] = profileTransfer(h_bPinned, h_aCached, nElements, cudaMemcpyHostToHost, \"cached to pinned\", verbose);\n101:     bwprofile[iter++] = profileTransfer(h_bCached, h_aCached, nElements, cudaMemcpyHostToHost, \"cached to cached\", verbose);\n102: \n103:     //\n104:     // cleanup\n105:     free(h_aPageable);\n106:     free(h_bPageable);\n107:     free(h_aPinned);\n108:     free(h_bPinned);\n109:     free(h_aCached);\n110:     free(h_bCached);\n111:     cudaFree(d_a);\n112:     cudaFree(d_b);\n113: }", "label": "(50, CUDA, cudaGetDeviceProperties(&prop, 0);); (125, CUDA, cudaFree(h_aCached);); (29, CUDA, cudaMallocHost((void**)&h_aCached, bytes);); (119, CUDA, cudaFreeHost(h_aPinned);); (119, CUDA, cudaFreeHost(h_bPinned);); (29, CUDA, cudaHostAlloc((void**)&h_bPinned, bytes, cudaHostAllocWriteCombined);); (24, CUDA, cudaMalloc((void**)&d_b, bytes);); (26, CUDA, cudaHostAlloc((void**)&h_aPinned, bytes, cudaHostAllocWriteCombined);); (24, CUDA, cudaMallocHost((void**)&h_bCached, bytes);); (116, CUDA, cudaFree(d_b);); (-1, CUDA, cudaSetDevice(0);); (113, CUDA, cudaFree(d_a);); (115, CUDA, cudaFree(h_bCached);); (-10, CUDA, cudaGetDeviceCount(&count);); (-2, CUDA, cudaGetDevice(&deviceID);); (14, CUDA, cudaMalloc((void**)&d_a, bytes);)"}
{"predict": "_input_ptr );\n112:         return -1;\n113:     }\n114: \n115:     // Release all resources\n116:     clReleaseEvent(copyEvent);\n117:     clReleaseMemObject( streams[0] );\n118:     clReleaseMemObject( streams[1] );\n119:     free( (void *)int_output_ptr );\n120:     free( (void *)int_input_ptr );\n121:     return 0;\n122: }", "label": "(30, OpenCL, err = clEnqueueWriteBuffer( queue, streams[0], true, 0, sizeof(cl_int)*num_elements, (void *)int_input_ptr, 0, NULL, NULL );); (39, OpenCL, err = clEnqueueCopyBuffer( queue, streams[0], streams[1], 0, 0, sizeof(cl_int)*num_elements, 0, NULL, &copyEvent );); (32, OpenCL, clReleaseMemObject( streams[1] );); (107, OpenCL, err = clEnqueueReadBuffer( queue, streams[1], true, 0, sizeof(cl_int)*num_elements, (void *)int_output_ptr, 0, NULL, NULL );); (48, OpenCL, err = clWaitForEvents( 1, &copyEvent );); (28, OpenCL, clReleaseMemObject( streams[0] );); (49, OpenCL, clReleaseEvent(copyEvent);)"}
{"predict": "(9, MPI, MPI_Sendrecv_replace(buffer, 10, MPI_INT, right, 0, left, 0, MPI_COMM_WORLD, &status);)", "label": "(9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myid);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &numprocs);); (16, MPI, MPI_Finalize();); (4, MPI, MPI_Init(&argc,&argv);); (12, MPI, MPI_Sendrecv_replace(buffer, 10, MPI_INT, left, 123, right, 123, MPI_COMM_WORLD, &status);)"}
{"predict": "(12, OpenMP, #pragma omp parallel for)", "label": "(3, TBB, REMARK(\"is_large_object<int>::value=%d\\n\", tbb::interface6::internal::is_large_object<int>::value);); (4, TBB, REMARK(\"is_large_object<int *>::value=%d\\n\", tbb::interface6::internal::is_large_object<int *>::value);); (2, TBB, REMARK(\"is_large_object<double>::value=%d\\n\", tbb::interface6::internal::is_large_object<double>::value);); (4, TBB, REMARK(\"is_large_object<check_type<int>* >::value=%d\\n\", tbb::interface6::internal::is_large_object<check_type<int>* >::value);); (2, TBB, REMARK(\"is_large_object<check_type<int> >::value=%d\\n\", tbb::interface6::internal::is_large_object<check_type<int> >::value);); (3, TBB, REMARK(\"is_large_object<check_type<short> >::value=%d\\n\\n\", tbb::interface6::internal::is_large_object<check_type<short> >::value);); (8, TBB, tbb::task_scheduler_init init(nthread);)"}
{"predict": "(12, MPI, KRATOS_EXPECT_EQ(output[0], mpi_world_communicator.MinAll(local)[0]);) (15, MPI, KRATOS_EXPECT_EQ(returned_result[0], mpi_world_communicator.MinAll(local)[0]);)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(12, MPI, sendbuf = (int *) (malloc(sizeof(int) * 10 * num_ranks));)", "label": "(47, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_ARE_FATAL);); (21, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (21, MPI, MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);); (18, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (38, MPI, MPI_Error_string(ierr, str, &slen);)"}
{"predict": "6:   }\n107: }\n\nOutput:\n(2, CUDA, extern \"C\" __global__ void cuComputeGradInput2(const float *__restrict__ dout, const float *__restrict__ input, const int n1, const int n2, const float *__restrict__ mean, const float *__restrict__ invvar, float epsilon, const float *gamma, float *grad_input) {); (107, CUDA, } extern \"C\" __global__ void cuComputeGradInput2(const float *__restrict__ dout, const float *__restrict__ input, const int n1, const int n2, const float *__restrict__ mean, const float *__restrict__ invvar, float epsilon, const float *gamma, float *grad_input) {);", "label": "(53, CUDA, extern __shared__ float s_float[];)"}
{"predict": "(17, CUDA, rmm::cuda_stream stream; gpu_wait<<<1, 1, 0, stream.value()>>>(1); interruptible::synchronize(stream);) (22, CUDA, thread_tokens[i] = interruptible::get_token();) (24, CUDA, gpu_wait<<<1, 1, 0, stream.value()>>>((1 + i) * thread_delay_millis); interruptible::synchronize(stream);) (33, CUDA, for (auto token : thread_tokens) token->cancel();) (38, CUDA, interruptible::yield_no_throw();)", "label": "(26, OpenMP, #pragma omp barrier); (19, OpenMP, common::nvtx::range omp_scope(\"interruptible::thread-%d\", i);); (13, OpenMP, omp_set_dynamic(0);); (14, OpenMP, #pragma omp parallel reduction(+ : n_finished) reduction(+ : n_cancelled) num_threads(n_threads)); (12, OpenMP, omp_set_num_threads(n_threads);); (14, OpenMP, auto i = omp_get_thread_num();)"}
{"predict": "(26, MPI, SCT->commVolRed += len2 * sizeof(double);)", "label": "(25, MPI, MPI_Send(lnzval, len2, MPI_DOUBLE, receiver, k, grid3d->zscp.comm);)"}
{"predict": "sizeof(tx_buffer), \"PASS\");\n136:   } else {\n137:     snprintf((char *)tx_buffer, sizeof(tx_buffer), \"FAIL\");\n138:   }\n139:   printf(\"lis2mdl_self_test %s\\n\", tx_buffer);\n140: }", "label": "(47, OpenMP, lis2mdl_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)"}
{"predict": "(1, OpenMP, #pragma omp parallel for)", "label": "(2, OpenMP, #pragma omp for schedule(nonmonotonic: dynamic))"}
{"predict": "(26, MPI, envval = getenv(envname);)", "label": "(21, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(19, CUDA, for (int i = 0; i < WARMUP; ++i) { l2_latency_kernel<ROUND><<<1, 32>>>(d_stride, d_ret, d_clk); } (24, CUDA, l2_latency_kernel<ROUND><<<1, 32>>>(d_stride, d_ret, d_clk);))", "label": "(35, CUDA, cudaFree(d_ret);); (7, CUDA, cudaMallocHost(&h_stride, STRIDE_MEM_SIZE);); (14, CUDA, cudaMalloc(&d_ret, sizeof(uint32_t));); (31, CUDA, cudaFree(d_stride);); (32, CUDA, cudaFree(d_clk);); (32, CUDA, cudaFreeHost(h_stride);); (25, CUDA, cudaMemcpy(h_clk, d_clk, 32 * sizeof(uint32_t), cudaMemcpyDeviceToHost);); (10, CUDA, cudaMemcpy(d_stride, h_stride, STRIDE_MEM_SIZE, cudaMemcpyHostToDevice);); (7, CUDA, cudaMalloc(&d_stride, STRIDE_MEM_SIZE);); (11, CUDA, cudaMalloc(&d_clk, 32 * sizeof(uint32_t));)"}
{"predict": "(3, MPI, const int world_rank = mpi_world_communicator.Rank();) (4, MPI, const int world_size = mpi_world_communicator.Size();) (16, MPI, flags = mpi_world_communicator.OrReduceAll(flags, ACTIVE | RIGID | STRUCTURE | MPI_BOUNDARY | PERIODIC | INLET | OUTLET | ISOLATED);) (22, MPI, if (world_size > 1) {)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(2, OpenMP, #pragma omp parallel for)\n(3, OpenMP, #pragma omp parallel for)", "label": "(5, OpenMP, #pragma omp target enter data map(to:a[:N]))"}
{"predict": "(10, OpenMP, #pragma omp parallel for)", "label": "(15, TBB, test_basic<MyCheckedStateSet>(\"concurrent unordered set (checked element state)\", tbb::internal::true_type());)"}
{"predict": "(1, CUDA, __global__ void ivfpq_top1_residual)", "label": "(17, CUDA, extern __shared__ _VOLATILE_ float sMem[];)"}
{"predict": "(14, CUDA, factory = *internal::MakeCLExecutorImplementation();) (16, CUDA, factory = internal::MakeHostExecutorImplementation;)\n(19, CUDA, factory = nullptr;)", "label": "(9, CUDA, factory = *internal::MakeCUDAExecutorImplementation();)"}
{"predict": "length; j++) {\n120:       a[j] = b[j] + scalar*c[j];\n121:     }\n122:   }\n123:  \n124:   /* end timer before any output operations */\n125:   nstream_time = wtime() - local_nstream_time;\n126:  \n127:   /* *** Scale times by number of iterations *** */\n128:   avgtime = nstream_time/iterations;\n129:  \n130:   /* *** Report timing results *** */\n131:  \n132:   if (my_ID == root) {\n133:     printf(\"Function    Best Rate MB/s  Avg time     Min time     Max time\\n\");\n134:     printf(\"Triad       %11.4f  %11.4f  %11.4f  %11.4f\\n\", 1.0E-6 * bytes/avgtime, avgtime, avgtime, avgtime);\n135:   }\n136:  \n137:   /* --- CLEAN-UP --- */\n138:  \n139:   #if !STATIC_ALLOCATION\n140:   free(a);\n141:   #endif\n142:   bail_out(error);\n143:   return 0;\n144: }", "label": "(103, OpenMP, printf(\"Number of threads    = %d\\n\", omp_get_max_threads());); (25, MPI, MPI_Comm_size(MPI_COMM_WORLD,&Num_procs);); (107, OpenMP, #pragma omp parallel for simd); (147, MPI, MPI_Finalize();); (76, MPI, MPI_Bcast(&length,        1, MPI_LONG, root, MPI_COMM_WORLD);); (22, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&my_ID);); (77, MPI, MPI_Bcast(&nthread_input, 1, MPI_INT,  root, MPI_COMM_WORLD);); (78, OpenMP, omp_set_num_threads(nthread_input);); (17, MPI, MPI_Init(&argc,&argv);); (73, MPI, MPI_Bcast(&iterations,    1, MPI_INT,  root, MPI_COMM_WORLD);); (114, MPI, MPI_Barrier(MPI_COMM_WORLD);); (70, MPI, MPI_Bcast(&offset,        1, MPI_LONG, root, MPI_COMM_WORLD);)"}
{"predict": "(18, CUDA, auto t1 = cudaEventCreate();) (23, CUDA, auto t2 = cudaEventCreate();) (46, CUDA, CHECK(cudaEventCreate(&start));) (51, CUDA, CHECK(cudaEventCreate(&stop));)", "label": "(6, CUDA, CHECK(cudaMemcpyFromSymbol(&block_size, BLOCK_DIM, sizeof(real)));); (34, CUDA, CHECK(cudaEventCreate(&stop));); (43, CUDA, CHECK(cudaEventRecord(stop));); (78, CUDA, CHECK(cudaFree(d_y));); (33, CUDA, cudaEventQuery(start);); (41, CUDA, CHECK(cudaEventSynchronize(stop));); (41, CUDA, CHECK(cudaEventElapsedTime(&curr_time, start, stop));); (34, CUDA, CHECK(cudaMemcpy(h_y, d_y, size, cudaMemcpyDefault));); (28, CUDA, CHECK(cudaEventRecord(start));); (20, CUDA, CHECK(cudaMalloc(&d_x, M));); (70, CUDA, CHECK(cudaFree(d_x));); (20, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyDefault));); (30, CUDA, CHECK(cudaGetLastError());); (21, CUDA, CHECK(cudaEventCreate(&start));); (16, CUDA, CHECK(cudaMalloc(&d_y, size));)"}
{"predict": "(34, MPI, *error_code = MPI_SUCCESS; return 0;)", "label": "(28, MPI, MPI_Type_get_extent(fd->filetype, &lb, &filetype_extent);); (28, MPI, MPI_Type_size_x(fd->filetype, &filetype_size);)"}
{"predict": "(14, CUDA, cudaMalloc((void **)&device_signals, complex_size_bytes);) (17, CUDA, cudaMalloc((void **)&device_params, sizeof(cb_params));) (41, CUDA, cufftExecR2C(forward_plan, (cufftReal *)device_signals, (cufftComplex *)device_signals);) (45, CUDA, cufftExecC2R(inverse_plan_cb, (cufftComplex *)device_signals, (cufftReal *)device_signals);) (48, CUDA, cudaMemcpy((void **)&output_signals[0][0], device_signals, complex_size_bytes, cudaMemcpyDeviceToHost);) (54, CUDA, cudaFree(device_signals);) (55, CUDA, cudaFree(device_params);)", "label": "(60, CUDA, CHECK_ERROR(cudaFree(device_params));); (12, CUDA, CHECK_ERROR(cudaMalloc((void **)&device_signals, complex_size_bytes));); (50, CUDA, CHECK_ERROR(cudaMemcpy(output_signals, device_signals, complex_size_bytes, cudaMemcpyDeviceToHost));); (56, CUDA, CHECK_ERROR(cudaFree(device_signals));); (34, CUDA, CHECK_ERROR(cudaMemcpyFromSymbol(&host_callback_ptr, device_callback_ptr, sizeof(host_callback_ptr)));); (9, CUDA, CHECK_ERROR(cudaMemcpy(device_signals, input_signals, complex_size_bytes, cudaMemcpyHostToDevice));); (17, CUDA, CHECK_ERROR(cudaMalloc((void **)&device_params, sizeof(cb_params)));); (17, CUDA, CHECK_ERROR(cudaMemcpy(device_params, &host_params, sizeof(cb_params), cudaMemcpyHostToDevice));)"}
{"predict": "(22, CUDA, Eigen::CudaStreamDevice stream; Eigen::GpuDevice device(&stream); BenchmarkSuite<Eigen::GpuDevice, Eigen::half> suite(device, D1, D2, D3);)", "label": "(25, CUDA, cudaDeviceSynchronize();)"}
{"predict": "(16, CUDA, cudaMemcpyAsync(image_device, image.data, size_image, cudaMemcpyHostToDevice, stream);)", "label": "(15, CUDA, checkCudaRuntime(cudaMemcpyAsync(image_device, image_host, size_image, cudaMemcpyHostToDevice, stream));)"}
{"predict": "(30, CUDA, accumulators[k] = __ldg(dout + global_oc + idx*OC);)", "label": "(35, CUDA, __shared__ float sub_results[x128::size][32][bdy];)"}
{"predict": "f MB/s\\n\", (width * width * num_faces * sizeof(float) * 2) /\n106:                          sdkGetTimerValue(&timer) / 1000 / 1024);\n107: \n108:   // copy result back to host\n109:   cudaMemcpy3DParms myparms2 = {0};\n110:   myparms2.dstPtr = make_cudaPitchedPtr(h_data, width * sizeof(float), width,\n111:                                        width);\n112:   myparms2.srcArray = cu_3darray;\n113:   myparms2.kind = cudaMemcpyDeviceToHost;\n114: \n115:   checkCudaErrors(cudaMemcpy3D(&myparms2));\n116: \n117:   // validate result\n118:   for (unsigned int layer = 0; layer < num_layers; layer++) {\n119:     for (int i = 0; i < (int)(cubemap_size); i++) {\n120:       if (fabs(h_data[layer * cubemap_size + i] - h_data_ref[layer * cubemap_size + i]) >\n121:           1e-6) {\n122:         bResult = false;\n123:         break;\n124:       }\n125:     }\n126:     if (!bResult) {\n127:       break;\n128:     }\n129:   }\n130: \n131:   sdkDeleteTimer(&timer);\n132:   sdkDeleteTimer(&timer);\n133: \n134:   // free memory\n135:   free(h_data);\n136:   free(h_data_ref);\n137:   checkCudaErrors(cudaFree(d_data));\n138:   checkCudaErrors(cudaFreeArray(cu_3darray));\n139:   checkCudaErrors(cudaDestroyTextureObject(tex));\n140: \n141:   return 0;\n142: }", "label": "(59, CUDA, myparms.dstPos = make_cudaPos(0, 0, 0);); (9, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProps, devID));); (56, CUDA, myparms.srcPos = make_cudaPos(0, 0, 0);); (119, CUDA, checkCudaErrors(cudaMemcpy(h_odata, d_data, size, cudaMemcpyDeviceToHost));); (94, CUDA, getLastCudaError(\"warmup Kernel execution failed\");); (60, CUDA, checkCudaErrors(cudaMemcpy3D(&myparms));); (57, CUDA, myparms.extent = make_cudaExtent(width, width, num_faces);); (137, CUDA, checkCudaErrors(cudaFreeArray(cu_3darray));); (76, CUDA, checkCudaErrors(cudaCreateTextureObject(&tex, &texRes, &texDescr, NULL));); (91, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (36, CUDA, checkCudaErrors(cudaMalloc((void **)&d_data, size));); (99, CUDA, getLastCudaError(\"Kernel execution failed\");); (131, CUDA, checkCudaErrors(cudaFree(d_data));); (-10, CUDA, int devID = findCudaDevice(argc, (const char **)argv);); (128, CUDA, checkCudaErrors(cudaDestroyTextureObject(tex));)"}
{"predict": "8:      */\n109:     test_data_nocomparables(DATA_NO_COMPARE1, 0);\n110:     test_data_nocomparables(DATA_NO_COMPARE2, 5);\n111: \n112:     /*-------------------------------------------------\n113:      * Create test files with non-comparable datasets and attributes with \n114:      * comparable datasets and attributes.  All the comparables should display \n115:      * differences.\n116:      */\n117:     test_objs_nocomparables(OBJ_NO_COMPARE1, OBJ_NO_COMPARE2);\n118: \n119:     /* -------------------------------------------------\n120:      * Create test files with dataset and attribute with container types \n121:      * (array, vlen) with multiple nested compound types.\n122:      */\n123:     /* file1 */\n124:     write_attr_in(ATTR_FILE1,\"dset1\",H5F1,0);\n125:     write_dset_in(DSET_FILE1,\"dset1\",H5F1,0);\n126:     /* file2 */\n127:     write_attr_in(ATTR_FILE2,\"dset1\",H5F2,1);\n128:     write_dset_in(DSET_FILE2,\"dset1\",H5F2,1);\n129: \n130:     /* -------------------------------------------------\n131:      * Generate a test file with dataset and attribute with container types \n132:      * (array, vlen) with multiple nested compound types.\n133:      */\n134:     gen_datareg(DATAREG_FILE1,0);\n135:     gen_datareg(DATAREG_FILE2,1);\n136: \n137:     /* -------------------------------------------------\n138:      * Test for vlen and fixlen string types in a compound dataset\n139:      */\n140:     test_comp_vlen_strings(COMPS_VLEN_STR,\"grp1\",0);\n141:     test_comp_vlen_strings(COMPS_VLEN_STR,\"grp2\",1);\n142: \n143:     return 0;\n144: }", "label": "(80, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (80, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)"}
{"predict": "(10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(48, MPI, MPI_Waitall(WINDOW, req, status);); (23, MPI, start = MPI_Wtime();); (50, MPI, end = MPI_Wtime();)"}
{"predict": "(8, HIPSYCL, *shrd_x = static_cast<__acpp_##input_type *>(__acpp_sscp_host_get_internal_local_memory());)", "label": "(8, SYCL, return hipsycl::libkernel::sscp::wg_broadcast(sender, x, shrd_x);)"}
{"predict": "{\n156: \ta[i] = b[i] + c[i];\n157:       }();\n158:     }\n159: \n160:     // schedule: static chunk\n161:     #pragma omp target\n162:     #pragma omp teams\n163: \n164:     for (int i = 0; i < n; ++i) {\n165:       a[i] = b[i] + c[i];\n166: \n167:       // skip rest of implementation of 'distribute' as it is tested above for default dist_schedule case\n168: \n169:       // 'parallel for' implementation is the same as the case without schedule clase (static no chunk is the default)\n170: \n171: \n172:       // initialize lb and ub to PrevLB and PrevUB\n173: \n174:       // PrevEUB is only used when 'for' has a chunked schedule, otherwise EUB is used\n175:       // In this case we use EUB\n176: \n177:       // initialize omp.iv\n178: \n179:       // check exit condition\n180: \n181:       // check that PrevLB and PrevUB are passed to the 'for'\n182: \n183:       // check stride 1 for 'for' in 'distribute parallel for'\n184: \n185:       [&]() {\n186: \ta[i] = b[i] + c[i];\n187:       }();\n188:     }\n189:   }();\n190: #endif\n191: }\n\nOutput:", "label": "(169, OpenMP, #pragma omp distribute parallel for schedule(static, ch)); (201, OpenMP, #pragma omp distribute parallel for schedule(dynamic)); (17, OpenMP, #pragma omp distribute parallel for); (59, OpenMP, #pragma omp distribute parallel for dist_schedule(static)); (227, OpenMP, #pragma omp distribute parallel for schedule(dynamic, ch)); (132, OpenMP, #pragma omp distribute parallel for schedule(static)); (10, OpenMP, #pragma omp target); (97, OpenMP, #pragma omp distribute parallel for dist_schedule(static, ch)); (9, OpenMP, #pragma omp teams)"}
{"predict": "starting\\n\");\n113:   for (int t = 0; t < timesteps; t++) {\n114:     checkCudaErrors(cudaMemcpy(bufferSrc, bufferDst,\n115:                                volumeSize * sizeof(float),\n116:                                cudaMemcpyDeviceToDevice));\n117:     checkCudaErrors(cudaMemcpy(bufferDst, bufferSrc,\n118:                                volumeSize * sizeof(float),\n119:                                cudaMemcpyDeviceToDevice));\n120:   }\n121:   printf(\" GPU FDTD loop finished\\n\");\n122: \n123:   // Copy the output to the host\n124:   checkCudaErrors(cudaMemcpy(output, bufferOut + padding,\n125:                              volumeSize * sizeof(float),\n126:                              cudaMemcpyDeviceToHost));\n127: \n128:   // Free the device buffers\n129:   checkCudaErrors(cudaFree(bufferOut));\n130:   checkCudaErrors(cudaFree(bufferIn));\n131: \n132:   // Free the events\n133:   #ifdef GPU_PROFILING\n134:   checkCudaErrors(cudaEventDestroy(profileStart));\n135:   checkCudaErrors(cudaEventDestroy(profileEnd));\n136:   #endif\n137: \n138:   return true;\n139: }", "label": "(109, CUDA, checkCudaErrors(cudaEventCreate(&profileStart));); (43, CUDA, checkCudaErrors(cudaSetDevice(targetDevice));); (40, CUDA, targetDevice = findCudaDevice(argc, (const char **)argv);); (193, CUDA, checkCudaErrors(cudaEventDestroy(profileEnd));); (140, CUDA, checkCudaErrors(cudaEventRecord(profileEnd, 0));); (116, CUDA, checkCudaErrors(cudaEventRecord(profileStart, 0));); (186, CUDA, checkCudaErrors(cudaEventDestroy(profileStart));); (61, CUDA, checkCudaErrors(cudaFuncGetAttributes(&funcAttrib, FiniteDifferencesKernel));); (31, CUDA, checkCudaErrors(cudaGetDeviceCount(&deviceCount));); (101, CUDA, checkCudaErrors(cudaEventCreate(&profileEnd));); (176, CUDA, checkCudaErrors(cudaFree(bufferOut));); (171, CUDA, checkCudaErrors(cudaFree(bufferIn));)"}
{"predict": "(31, CUDA, float res_host = expr(a);)", "label": "(38, CUDA, cudaFree(res_dev);); (31, CUDA, cudaMalloc(&res_dev, sizeof(float));); (33, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(4, CUDA, int batch_size = inData.numImages();)", "label": "(18, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (18, CUDA, checkCudaErrors(cudaGetLastError());)"}
{"predict": "(7, CUDA, cudaGetDeviceProperties(&props, 0);)", "label": "(8, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (9, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;)"}
{"predict": "(30, OpenMP, #pragma omp taskwait;)", "label": "(29, OpenMP, #pragma omp taskwait); (22, OpenMP, #pragma omp task); (9, OpenMP, #pragma omp parallel); (10, OpenMP, #pragma omp single)"}
{"predict": "(10, OpenMP, #pragma omp parallel for private(idx) reduction(+:result[:1024]))", "label": "(2, SYCL, sycl::queue q = get_queue();); (3, SYCL, int* data = sycl::malloc_device<int>(1024, q);)"}
{"predict": "(11, MPI, int rank = MPI_Comm_rank(comm);)", "label": "(7, MPI, MPI_Comm_rank(comm, &rank);); (49, MPI, MPI_Bcast(&num_groups, 1, MPI_INT, 0, comm);)"}
{"predict": "(4, OpenMP, #pragma omp parallel)\n(5, OpenMP, #pragma omp for)", "label": "(4, OpenMP, #pragma omp sections); (49, OpenMP, #pragma omp sections nowait); (28, OpenMP, #pragma omp section)"}
{"predict": "(12, CUDA, cudaMallocManaged(reinterpret_cast<void **>(&d_data), sizeof(data_type) * h_data.size());) (29, CUDA, CURAND_CHECK(curandGeneratePoisson(gen, d_data, h_data.size(), lambda));)", "label": "(36, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (38, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(14, CUDA, fprintf(stderr, \"CUDA Driver API error occurred during cuCtxGetCurrent at %s(%d): %s.\\n\", __FILE__, __LINE__, error_str);)", "label": "(5, CUDA, c.CUDAConfig().device_id = 0;)"}
{"predict": "(4, MPI, int rank, ranks; MPI_Comm_rank(MPI_COMM_WORLD, &rank); MPI_Comm_size(MPI_COMM_WORLD, &ranks);) (12, MPI, MPI_Bcast(&range, 1, MPI_UINT64_T, 0, MPI_COMM_WORLD);) (17, MPI, MPI_Bcast(&count, 1, MPI_UINT64_T, 0, MPI_COMM_WORLD);) (18, MPI, MPI_Bcast(&total, 1, MPI_UINT64_T, 0, MPI_COMM_WORLD);) (19, MPI, MPI_Bcast(&offset, 1, MPI_UINT64_T, 0, MPI_COMM_WORLD);) (24, MPI, MPI_Scatter(sendbuf, pack_size, MPI_BYTE, recvbuf, pack_size, MPI_BYTE, 0, MPI_COMM_WORLD);) (50, MPI, MPI_Gather(recvbuf, recvbytes, MPI_BYTE, recvbuf, recvbytes, MPI_BYTE, 0, MPI_COMM_WORLD);) (71, MPI, MPI_Bcast(&recvbytes, 1, MPI_INT, 0, MPI_COMM_WORLD);) (73, MPI, MPI_Scatter(recvbuf, pack_size, MPI_BYTE, recvbuf, pack_size, MPI_BYTE, 0, MPI_COMM_WORLD);) (83, MPI, MPI_Bcast(&tmpsize, 1, MPI_UINT64_T, 0, MPI_COMM_WORLD);)", "label": "(65, MPI, MPI_Gatherv(sendbuf, bytes, MPI_BYTE, recvbuf, counts, disps, MPI_BYTE, 0, MPI_COMM_WORLD);); (13, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (36, MPI, MPI_Gather(&bytes, 1, MPI_INT, counts, 1, MPI_INT, 0, MPI_COMM_WORLD);)"}
{"predict": "(12, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)", "label": "(32, MPI, MPI_Comm_test_inter(intercomm, &flag);); (23, MPI, MPI_Comm_rank(intercomm, &key);); (90, MPI, MPI_Comm_free(&intercomm);); (38, MPI, MPI_Comm_size(intercomm, &orig_size);); (38, MPI, MPI_Comm_size(newcomm, &new_size);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (33, MPI, MPI_Comm_remote_size(intercomm, &orig_rsize);); (19, MPI, MPI_Comm_split(intercomm, color, key, &newcomm);); (3, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (31, MPI, MPI_Comm_remote_size(newcomm, &new_rsize);)"}
{"predict": "(9, CUDA, dout_perm += blockIdx.x * B * H * W;); (12, CUDA, float th_sum = 0.0f;); (14, CUDA, th_sum += dout_perm[i];); (16, CUDA, shared[threadIdx.x] = th_sum;); (21, CUDA, if (threadIdx.x < stride) { shared[threadIdx.x] += shared[threadIdx.x + stride]; } ); (27, CUDA, dbias[blockIdx.x] = shared[0];)", "label": "(6, CUDA, extern __shared__ float shared[];)"}
{"predict": "(10, CUDA, HANDLE_ERROR(cudaMallocManaged((void**)&state_d, sizeof(float) * S_DIM * state.size() * BLOCKDIM_Z));) (11, CUDA, HANDLE_ERROR(cudaMallocManaged((void**)&state_der_d, sizeof(float) * S_DIM * state_der.size() * BLOCKDIM_Z));) (12, CUDA, HANDLE_ERROR(cudaMallocManaged((void**)&control_d, sizeof(float) * C_DIM * control.size() * BLOCKDIM_Z));) (23, CUDA, fullARNNTestKernel<NETWORK_T, S_DIM, C_DIM, BLOCKDIM_X, BLOCKDIM_Z><<<numBlocks, threadsPerBlock, 0, stream>>>(model.model_d_, state_d, control_d, state_der_d, dt);)", "label": "(34, CUDA, cudaFree(control_d);); (32, CUDA, cudaFree(state_der_d);); (22, CUDA, CudaCheckError();); (29, CUDA, cudaFree(state_d);)"}
{"predict": "(5, OpenMP, #pragma omp parallel for reduction(+:res) private(p) for (unsigned p=0;p<ndata;p++)res=res+data[p];)", "label": "(7, CUDA, fcuda::Check_CudaErroorFun(\"DgReduSumUint\");); (5, CUDA, cudaMemcpy(data,datag+inidata,sizeof(unsigned)*ndata,cudaMemcpyDeviceToHost);)"}
{"predict": "(20, OpenMP, #pragma omp parallel for reduction(+:col.r, col.g, col.b))", "label": "(37, TBB, tbb::spin_mutex::scoped_lock lock (MyMutex);)"}
{"predict": "();  // such that the outer loop can use the correct m and l\n111:     }\n112: }\n\nOutput:\n(31, CUDA, for (int j = 0; j < Tc; j++) {) (32, CUDA, for (int j = 0; j < Tc; j++) {) (33, CUDA, for (int j = 0; j < Tc; j++) {) (34, CUDA, for (int j = 0; j < Tc; j++) {) (35, CUDA, for (int j = 0; j < Tc; j++) {) (36, CUDA, for (int j = 0; j < Tc; j++) {) (37, CUDA, for (int j = 0; j < Tc; j++) {) (38, CUDA, for (int j = 0; j < Tc; j++) {) (39, CUDA, for (int j = 0; j < Tc; j++) {) (40, CUDA, for (int j = 0; j < Tc; j++) {) (41, CUDA, for (int j = 0; j < Tc; j++) {) (42, CUDA, for (int j = 0; j < Tc; j++) {) (43, CUDA, for (int j = 0; j < Tc; j++) {) (44, CUDA, for (int j = 0; j < Tc; j++) {) (45, CUDA, for (int j = 0; j < Tc; j++) {) (46, CUDA, for (int j = 0; j < Tc; j++) {) (47, CUDA, for (int j = 0; j < Tc; j++) {) (48, CUDA, for (int j = 0; j < Tc; j++) {) (49, CUDA, for (int j = 0; j < Tc; j++) {) (50, CUDA, for (int j = 0; j < Tc; j++) {) (51, CUDA, for (int j = 0; j < Tc; j++) {)", "label": "(24, CUDA, extern __shared__ float sram[];)"}
{"predict": "(15, OpenMP, #pragma omp parallel for)", "label": "(14, OpenMP, #pragma omp parallel); (14, OpenMP, #pragma omp task if (task: false))"}
{"predict": "(12, OpenMP, #pragma omp parallel for)", "label": "(47, OpenMP, #pragma omp target teams distribute parallel for collapse(collapse(tmain<int, char, -1, -2>(argc, argv)); (4, OpenMP, #pragma omp target teams distribute parallel for collapse (); (7, OpenMP, #pragma omp target teams distribute parallel for collapse ()); (18, OpenMP, #pragma omp target teams distribute parallel for collapse (foobool(1) > 0 ? 1 : 2)); (13, OpenMP, #pragma omp target teams distribute parallel for collapse (2+2))); (23, OpenMP, #pragma omp target teams distribute parallel for collapse (foobool(argc)), collapse (true), collapse (-5)); (7, OpenMP, #pragma omp target teams distribute parallel for collapse (4); (25, OpenMP, #pragma omp target teams distribute parallel for collapse (S1)); (-7, OpenMP, #pragma omp target teams distribute parallel for collapse); (41, OpenMP, #pragma omp target teams distribute parallel for collapse (2))"}
{"predict": "(3, OpenMP, #pragma omp parallel for)", "label": "(32, CUDA, cudaFree(resourceList->outputBufferGPU);); (27, CUDA, cudaFree(resourceList->inputBufferGPU);); (44, CUDA, cudaStreamDestroy(resourceList->stream);)"}
{"predict": "(2, CUDA, cudaStream_t stream;)\n(4, CUDA, int inN = 1;)\n(5, CUDA, int inW = 224;)\n(6, CUDA, int inH = 224;)\n(7, CUDA, int sed = 22;)\n(8, CUDA, nvcv::ImageFormat format = nvcv::FMT_RGBA8;)\n(9, CUDA, cvcuda::OSD op;)\n(10, CUDA, NVCVOSDType type = NVCVOSDType::NVCV_OSD_TEXT;)\n(12, CUDA, srand(sed);)\n(44, CUDA, std::shared_ptr<NVCVElementsImpl> ctx = std::make_shared<NVCVElementsImpl>(elementVec);)\n(67, CUDA, EXPECT_NO_THROW(op(stream, imgIn, imgOut, (NVCVElements)ctx.get()));)", "label": "(77, CUDA, EXPECT_EQ(cudaSuccess, cudaMemcpy(test.data(), output->basePtr(), outBufSize, cudaMemcpyDeviceToHost));); (2, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (64, CUDA, EXPECT_EQ(cudaSuccess, cudaMemset(input->basePtr(), 0xFF, inSampleStride * inAccess->numSamples()));); (72, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (72, CUDA, EXPECT_EQ(cudaSuccess, cudaMemcpy(testIn.data(), input->basePtr(), inBufSize, cudaMemcpyDeviceToHost));); (62, CUDA, EXPECT_EQ(cudaSuccess, cudaMemset(output->basePtr(), 0xFF, outSampleStride * outAccess->numSamples()));); (78, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamDestroy(stream));)"}
{"predict": "(12, OpenMP, #pragma omp parallel for)", "label": "(28, OpenMP, #pragma omp parallel num_threads(NumThreads3)); (29, OpenMP, check3[omp_get_thread_num()] += omp_get_num_threads();); (15, OpenMP, #pragma omp parallel); (10, OpenMP, threadLimit = omp_get_thread_limit();); (6, OpenMP, #pragma omp target teams num_teams(1) thread_limit(ThreadLimit)); (20, OpenMP, check2[omp_get_thread_num()] += omp_get_num_threads();); (16, OpenMP, omp_set_num_threads(NumThreads2);); (12, OpenMP, check1[omp_get_thread_num()] += omp_get_num_threads();)"}
{"predict": "(10, CUDA, auto stream = cudaStream_t();); (11, CUDA, mergeElementaryIntervalsKernel<1U><<<mergePairs, SAMPLE_STRIDE, 0, stream>>>(d_DstKey, d_DstVal, d_SrcKey, d_SrcVal, d_LimitsA, d_LimitsB, stride, N);); (13, CUDA, mergeElementaryIntervalsKernel<0U><<<mergePairs, SAMPLE_STRIDE, 0, stream>>>(d_DstKey, d_DstVal, d_SrcKey, d_SrcVal, d_LimitsA, d_LimitsB, stride, N);); (15, CUDA, cudaStreamDestroy(stream);)", "label": "(13, CUDA, getLastCudaError(\"mergeElementaryIntervalsKernel<1> failed\\n\");); (17, CUDA, getLastCudaError(\"mergeElementaryIntervalsKernel<0> failed\\n\");)"}
{"predict": "(13, CUDA, cudaMallocManaged(&A, sizeof(double) * lda * M);) (14, CUDA, cudaMallocManaged(&B, sizeof(double) * ldb * N);) (15, CUDA, cudaMallocManaged(&C_cutlass, sizeof(double) * ldc * N);) (16, CUDA, cudaMallocManaged(&C_reference, sizeof(double) * ldc * N);) (44, CUDA, cudaFree(A);) (45, CUDA, cudaFree(B);) (46, CUDA, cudaFree(C_cutlass);) (47, CUDA, cudaFree(C_reference);)", "label": "(42, CUDA, cudaFree(B);); (109, CUDA, result = cudaMemcpy(host_cutlass.data(), C_cutlass, sizeof_C, cudaMemcpyDeviceToHost);); (59, CUDA, cudaFree(C_reference);); (52, CUDA, result = cudaMemcpy(C_reference, C_cutlass, sizeof_C, cudaMemcpyDeviceToDevice);); (47, CUDA, cudaFree(C_cutlass);); (119, CUDA, result = cudaMemcpy(host_reference.data(), C_reference, sizeof_C, cudaMemcpyDeviceToHost);); (28, CUDA, cudaFree(A);)"}
{"predict": "(15, OpenMP, #pragma omp parallel for private(i, j))\n(28, OpenMP, #pragma omp parallel for private(i, j))\n(41, OpenMP, #pragma omp parallel for private(i))", "label": "(40, OpenMP, #pragma omp teams reduction(+:sum)); (37, OpenMP, #pragma omp target map(tofrom: b, sum, t) map(alloc: a)); (40, OpenMP, #pragma omp distribute parallel for reduction(+:sum)); (42, OpenMP, t[i] = omp_get_team_num();)"}
{"predict": "(11, CUDA, three_interpolate_grad_kernel_fast<<<blocks, threads, 0, stream>>>(b, c, n, m, grad_out, idx, weight, grad_points);)", "label": "(14, CUDA, fprintf(stderr, \"CUDA kernel failed : %s\\n\", cudaGetErrorString(err));); (11, CUDA, err = cudaGetLastError();)"}
{"predict": "108: \n109: #endif\n110: \n111:   return 0;\n112: }\n\nOutput:", "label": "(4, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (5, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;)"}
{"predict": "(12, CUDA, auto helper = new TwoDTextureHelper<float>();)", "label": "(4, CUDA, CudaCheckError();)"}
{"predict": "dScalePermstructInit(n, nrhs, &ScalePermstruct);\n132: \tdLUstructInit(n, nrhs, &LUstruct);\n133: \tdSOLVEstructInit(n, nrhs, &SOLVEstruct);\n134: \n135:         /* Perform numerical factorization. */\n136:         dgetrf_dist(&A, &ScalePermstruct, &LUstruct, &stat);\n137: \n138:         /* Solve the linear system. */\n139:         dgetrs_dist(&A, &ScalePermstruct, &LUstruct, &SOLVEstruct, nrhs, b, ldb, berr, &stat);\n140: \n141:         /* ------------------------------------------------------------\n142:            CHECK THE SOLUTION.\n143:            ------------------------------------------------------------*/\n144: \n145:         if ( !iam ) {\n146:             print_sol_dist(m, n, nrhs, b, ldb, xtrue, ldx);\n147:             print_berr_dist(nrhs, berr);\n148:         }\n149: \n148:         /* ------------------------------------------------------------\n150:            RELEASE ALL SUPERLU WORKSPACE.\n151:            ------------------------------------------------------------*/\n152: \n153:         dScalePermstructFree(&ScalePermstruct);\n154:         dLUstructFree(&LUstruct);\n155:         dSOLVEstructFree(&SOLVEstruct);\n156:         dFree(berr);\n157:         dFree(b);\n158:         dFree(xtrue);\n159:         Destroy_CompCol_Matrix(&A);\n160:         Destroy_SuperNode_Matrix(&A);\n161:         Destroy_CompCol_Matrix(&A);\n162:         Destroy_SuperNode_Matrix(&A);\n163:         Destroy_CompCol_Matrix(&A);\n164:         Destroy_SuperNode_Matrix(&A);\n165:         Destroy_CompCol_Matrix(&A);\n166:         Destroy_SuperNode_Matrix(&A);\n167:         Destroy_CompCol_Matrix(&A);\n168:         Destroy_SuperNode_Matrix(&A);\n169:         Destroy_CompCol_Matrix(&A);\n160:         Destroy_SuperNode_Matrix(&A);\n161:         Destroy", "label": "(82, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &iam );); (257, MPI, MPI_Finalize();); (24, MPI, MPI_Init_thread( &argc, &argv, MPI_THREAD_MULTIPLE, &omp_mpi_level);); (25, MPI, MPI_Comm_size( MPI_COMM_WORLD, &nprocs );)"}
{"predict": "d, recv_cnt=%d\\n\", mype, bid, tid, i, d_recv_cnt[d_colnummod[tid]], d_recv_cnt[d_colnummod[tid]]);\n100:                    nvshmem_signal_wait_until((uint64_t *) (flag_rd_q + d_colnummod[tid]), NVSHMEM_CMP_EQ, 1);\n101:                    d_statusmod[d_colnummod[tid]] = 1;\n102:                    //printf(\"RD WAIT1 (%d,%d) msg arrived in col %d\\n\", mype, tid, d_colnummod[tid]);\n103:                }\n104:            }\n105:        } else { // multiple threads wait for multiple cols\n106:            int delta = d_nfrecvmod[1] % WAIT_NUM_THREADS;\n107:            if (tid < delta) {\n108:                d_mynum[tid] = d_nfrecvmod[1] / WAIT_NUM_THREADS + 1;\n109:            } else {\n110:                d_mynum[tid] = d_nfrecvmod[1] / WAIT_NUM_THREADS;\n111:            }\n112:            __syncthreads();\n113:            d_mymaskstart[tid] = 0;\n114:            for (int i = 0; i < tid; i++) {\n115:                d_mymaskstart[tid] += d_mynum[i];\n116:            }\n117:            d_mymasklength[tid] = d_colnummod[d_mymaskstart[tid] + d_mynum[tid] - 1] - d_colnummod[d_mymaskstart[tid]] + 1;\n118:            __syncthreads();\n119:            //printf(\"RD WAIT2 (%d,%d) mynum=%d, start=%d,%d length=%d\\n\",mype,tid,d_mynum[tid],d_mymaskstart[tid],d_colnummod[d_mymaskstart[tid]],d_mymasklength[tid]);\n120: \n121:            for (int i = 0; i < d_mynum[tid]; i++) {\n122:                int wm_val = nvshmem_uint64_wait_until_any(flag_rd_q + d_colnummod[tid], d_mymasklength[tid],", "label": "(343, CUDA, __shared__ int cur_send_num;); (341, CUDA, __shared__ int recv_num, finish_num;)"}
{"predict": "(2, CUDA, extern \"C\" __global__ void FastllmGemvInt4Kernel2(float *A, uint8_t *B, float *C, float *bias, float *scales, uint8_t *zeros, int m, int k) {)", "label": "(4, CUDA, __shared__ float sdata[THREAD_PER_BLOCK];)"}
{"predict": "\"Failed to open file for writing: %s errno=%d (%s)\",\n117:             dst_name, errno, strerror(errno));\n118:         mfu_close(src_name, fd);\n119:         return MFU_FAILURE;\n120:     }\n121: \n122:     /* check that all processes were able to open the file */\n123:     if (! mfu_alltrue(fd_out >= 0, MPI_COMM_WORLD)) {\n124:         /* some process failed to open so bail with error,\n125:          * if we opened ok, close file */\n126:         if (fd_out >= 0) {\n127:             mfu_close(dst_name, fd_out);\n128:         }\n129:         mfu_close(src_name, fd);\n130:         return MFU_FAILURE;\n131:     }\n132: \n133:     /* compute number of blocks */\n134:     int64_t num_blocks = block_total;\n135: \n136:     /* compute number of bytes to read from each block */\n137:     int64_t bytes_per_block = (int64_t)mfu_min(block_size, (int64_t)st.st_size - block_meta);\n138: \n139:     /* allocate memory for buffer */\n140:     uint8_t* buffer = (uint8_t*)malloc(bytes_per_block);\n141:     if (buffer == NULL) {\n142:         MFU_LOG(MFU_LOG_ERR, \"Failed to allocate buffer for reading blocks\");\n143:         mfu_close(src_name, fd);\n144:         mfu_close(dst_name, fd_out);\n145:         return MFU_FAILURE;\n146:     }\n147: \n148:     /* loop over blocks and read data */\n149:     for (int64_t i = 0; i < num_blocks; i++) {\n150:         /* read data from block */\n151:         ssize_t read_rc = mfu_read(src_name, fd, buffer, bytes_per_block);\n152:         if (read_rc < 0) {\n153:             MFU_LOG(MFU_LOG_ERR, \"Failed to read block %lld: %", "label": "(10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (76, MPI, MPI_Bcast(&footer_flag, 1, MPI_INT, 0, MPI_COMM_WORLD);); (76, MPI, MPI_Bcast(&footer, 8, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(1, CUDA, __global__ void softmax_kernel_2(float* __restrict__ xd, float* __restrict__ resd, int M, int N) {)", "label": "(2, CUDA, __shared__ float smem[1024];)"}
{"predict": "(44, OpenMP, #pragma omp parallel for private(i, pos, do_output, bin_pos, bin_index))", "label": "(38, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(30, OpenMP, #pragma omp parallel for reduction(+:sum))", "label": "(29, CUDA, checkCudaErrors(cudaSetDevice(gpuID));)"}
{"predict": "117:         err = clEnqueueNDRangeKernel(queue, kernel, dimension, offset, gws, lws, 0, NULL, NULL);\n118:         if (err != CL_SUCCESS) {\n119:             cout << \"Error launching kernel\" << endl;\n120:             abort();\n121:         }\n122:         err = clFinish(queue);\n123:         if (err != CL_SUCCESS) {\n124:             cout << \"Error clFinish\" << endl;\n125:             abort();\n126:         }\n127:         clEnqueueReadBuffer(queue, buffer, CL_TRUE, 0, bufferSize, ptr, 0, NULL, NULL);\n128:         if (err != CL_SUCCESS) {\n129:             cout << \"Error reading from buffer\" << endl;\n130:             abort();\n131:         }\n132:     }\n133:     return retVal;\n134: }", "label": "(69, OpenCL, program = clCreateProgramWithSource(context, 1, &strings, 0, &err);); (129, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(cl_mem), &buffer);); (190, OpenCL, clReleaseKernel(kernel);); (191, OpenCL, clReleaseCommandQueue(queue);); (189, OpenCL, clReleaseProgram(program);); (139, OpenCL, err = clEnqueueNDRangeKernel(queue, kernel, dimension, offset, gws, lws, 0, 0, nullptr);); (189, OpenCL, clReleaseContext(context);); (101, OpenCL, buffer = clCreateBuffer(context, flags, bufferSize, nullptr, &err);); (115, OpenCL, err = clEnqueueUnmapMemObject(queue, buffer, ptr, 0, nullptr, nullptr);); (106, OpenCL, void *ptr = clEnqueueMapBuffer(queue, buffer, CL_TRUE, CL_MAP_WRITE, 0, bufferSize, 0, nullptr, nullptr, &err);); (38, OpenCL, queue = clCreateCommandQueue(context, device_id, 0, &err);); (140, OpenCL, void *ptr = clEnqueueMapBuffer(queue, buffer, CL_TRUE, CL_MAP_READ, 0, bufferSize, 0, nullptr, nullptr, &err);); (64, OpenCL, err = clBuildProgram(program, 1, &device_id, nullptr, nullptr, nullptr);); (178, OpenCL, clReleaseMemObject(buffer);); (28, OpenCL, context = clCreateContext(0, 1, &device_id, NULL, NULL, &err);); (84, OpenCL, kernel = clCreateKernel(program, \"hello\", &err);)"}
{"predict": "(10, MPI, MPI_Isend(buf + offset * elems, elems, MPI_FLOAT, (offset + 1) % nproc, 0, comm, &reqs[offset])); (11, MPI, MPI_Irecv(buf + (offset + 1) % nproc * elems, elems, MPI_FLOAT, offset, 0, comm, &reqs[offset]));", "label": "(6, MPI, MPI_Comm_size(comm, &nproc);); (4, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(18, OpenMP, #pragma omp parallel for private(v0, v1, v2, v3, v4))", "label": "(27, TBB, tbb::tick_count t1 = tbb::tick_count::now();); (23, TBB, reducePiExample<tbb::auto_partitioner>(num_intervals, gs);); (6, TBB, c.set_max_concurrency(std::min(8, tbb::info::default_concurrency(c)));); (29, TBB, double v3 = deterministicReducePiExample<tbb::simple_partitioner>(num_intervals, gs);); (27, TBB, tbb::tick_count t3 = tbb::tick_count::now();); (-1, TBB, tbb::task_arena::constraints c;); (11, TBB, tbb::tick_count ts_1 = tbb::tick_count::now();); (18, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (27, TBB, tbb::tick_count t5 = tbb::tick_count::now();); (25, TBB, double v4 = deterministicReducePiExample<tbb::static_partitioner>(num_intervals, gs);); (5, TBB, tbb::tick_count ts_0 = tbb::tick_count::now();); (-2, TBB, tbb::task_arena a(c);); (-6, TBB, c.set_core_type(tbb::info::core_types().back());); (20, TBB, tbb::tick_count t4 = tbb::tick_count::now();); (14, TBB, double v1 = reducePiExample<tbb::simple_partitioner>(num_intervals, gs);); (15, TBB, double v2 = reducePiExample<tbb::static_partitioner>(num_intervals, gs);); (-11, TBB, c.set_numa_id(tbb::info::numa_nodes()[0]);); (12, TBB, tbb::tick_count t2 = tbb::tick_count::now();); (8, TBB, double v0 = reducePiExample<tbb::auto_partitioner>(num_intervals, gs);)"}
{"predict": "(13, CUDA, cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal);) (20, CUDA, cudaStreamEndCapture(stream, &graph);) (23, CUDA, cudaGraphInstantiate(&instance, graph, nullptr, nullptr, 0);) (25, CUDA, cudaGraphLaunch(instance, stream);) (27, CUDA, cudaGraphDestroy(instance);) (29, CUDA, cudaGraphExecDestroy(exec);) (31, CUDA, cudaGraphDestroy(graph);) (33, CUDA, cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal);) (40, CUDA, cudaStreamEndCapture(stream, &graph);) (42, CUDA, cudaGraphInstantiate(&instance, graph, nullptr, nullptr, 0);) (44, CUDA, cudaGraphLaunch(instance, stream);) (46, CUDA, cudaGraphDestroy(instance);) (48, CUDA, cudaGraphExecDestroy(exec);) (50, CUDA, cudaGraphDestroy(graph);) (52, CUDA, cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal);) (59, CUDA, cudaStreamEndCapture(stream, &graph);) (61, CUDA, cudaGraphInstantiate(&instance, graph, nullptr, nullptr, 0);) (63, CUDA, cudaGraphLaunch(instance, stream);) (65, CUDA, cudaGraphDestroy(instance);) (67, CUDA, cudaGraphExecDestroy(exec);) (69, CUDA, cudaGraphDestroy(graph);)", "label": "(18, CUDA, cuda_safe_call(cudaStreamSynchronize(stream));); (7, CUDA, cuda_safe_call(cudaMalloc(&d_ptrB, N * sizeof(double)));); (9, CUDA, cuda_safe_call(cudaStreamCreate(&stream));); (4, CUDA, cuda_safe_call(cudaMalloc(&d_ptrA, N * sizeof(double)));)"}
{"predict": "(30, CUDA, auto Csub = 0.0;); (62, CUDA, #pragma unroll); (80, CUDA, C[c + wB * threadIdx.y + threadIdx.x] = Csub;);", "label": "(13, CUDA, __shared__ alignas(alignof(float4)) float Bs[BLOCK_SIZE][BLOCK_SIZE];); (8, CUDA, __shared__ alignas(alignof(float4)) float As[BLOCK_SIZE][BLOCK_SIZE];)"}
{"predict": "uint8_t) (SLAVE_ADDRESS << 1);\n111:   sh_cfg_read.slv_add_bit = LSM6DSO_SLV_ADD_BIT;\n112:   sh_cfg_read.slv_subadd = 0x00;\n113:   sh_cfg_read.slv_reg = 0x00;\n114:   sh_cfg_read.slv_data_len = 0x06;\n115:   sh_cfg_read.slv_data = dummy;\n116:   sh_cfg_read.slv_mode = LSM6DSO_SLV_ACT;\n117:   sh_cfg_read.slv_batch = LSM6DSO_SLV_ACT_BYPASS;\n118:   sh_cfg_read.slv_odr = LSM6DSO_SH_ODR_13Hz;\n119:   lsm6dso_sh_cfg_read(&ag_ctx, &sh_cfg_read);\n120:   /* Poll for FIFO watermark interrupt and read samples */\n121:   while (1) {\n122:     lsm6dso_fifo_wtm_flag_get(&ag_ctx, &wtm_flag);\n123:     if (wtm_flag) {\n124:       /* Read FIFO samples */\n125:       lsm6dso_fifo_tag_get(&ag_ctx, &tag);\n126:       lsm6dso_fifo_data_get(&ag_ctx, tag, data_raw_acceleration,\n127:                             data_raw_angular_rate, data_raw_magnetic);\n128:     }\n129:   }\n130: }", "label": "(78, OpenMP, lis2mdl_offset_temp_comp_set(&mag_ctx, PROPERTY_ENABLE);)"}
{"predict": "(4, OpenMP, #pragma omp parallel for)", "label": "(2, OpenMP, const int num_devices = omp_get_num_devices();); (6, OpenMP, #pragma omp target map(from : num) device(i)); (6, OpenMP, num = omp_get_device_num();)"}
{"predict": "(38, OpenMP, #pragma omp parallel for)", "label": "(55, OpenCL, clReleaseEvent(clEvent);)"}
{"predict": "(35, CUDA, if (err != cudaSuccess) { devs.pop_back(); } else { cudaSetDevice(dev); } )", "label": "(47, CUDA, cudaDeviceCanAccessPeer(&access, dev1, dev2);); (53, CUDA, cudaSetDevice(dev);); (57, CUDA, auto err = cudaDeviceEnablePeerAccess(odev, 0);); (14, CUDA, auto err = cudaGetDeviceProperties(&props, dev);); (9, CUDA, INFO(\"failed to cudaSetDevice(%d)\\n\", dev);); (-2, CUDA, cudaGetDeviceCount(reinterpret_cast<int *>(&device));)"}
{"predict": "(32, OpenMP, #pragma omp parallel for)", "label": "(9, TBB, oneapi::tbb::spin_mutex::scoped_lock l(locks[u]);); (20, TBB, oneapi::tbb::spin_mutex::scoped_lock l(locks[v]);)"}
{"predict": "(11, MPI, MPI_Bcast(&nccl_id, sizeof(ncclUniqueId), MPI_BYTE, 0, MPI_COMM_WORLD);) (19, NCCL, NCCL_CHECK(ncclCommInitRank(&nccl_comm, world_size, nccl_id, rank));)", "label": "(6, MPI, MPI_CHECK(MPI_Init(NULL, NULL));); (15, MPI, MPI_CHECK(MPI_Bcast((void *)&nccl_id, sizeof(nccl_id), MPI_BYTE, 0, MPI_COMM_WORLD));); (16, CUDA, cudaSetDevice(rank);)"}
{"predict": ":     }\n113: }\n\nOutput:\n(1, CUDA, extern \"C\" __global__ void kernel_backward(const int B, const int T, const int C, const int H, const F *__restrict__ const _r, const F *__restrict__ const _k, const F *__restrict__ const _v, const float *__restrict__ _w1, const float *__restrict__ __w1, const F *__restrict__ _u1, const float *__restrict__ _w2, const float *__restrict__ __w2, const F *__restrict__ _u2, const F *__restrict__ const _gy, F *__restrict__ const _gr, F *__restrict__ const _gk, F *__restrict__ const _gv, F *__restrict__ const _gw1, F *__restrict__ const _gu1, F *__restrict__ const _gw2, F *__restrict__ const _gu2) {)\n(107, CUDA, float state[_N_] = {0}, saaaa[_N_] = {0}, sbbbb[_N_] = {0}, scccc[_N_] = {0}, sdddd[_N_] = {0};)\n(107, CUDA, float gu1 = 0, gu2 = 0, gw1 = 0, gw2 = 0;)\n(107, CUDA, extern \"C\" __global__ void kernel_backward(const int B, const int T, const int C, const int H, const F *__restrict__ const _r, const F *__restrict__ const _k, const F *__restrict__ const _v, const float *__restrict__ _w1, const float *__restrict__ __w1, const F *__restrict__ _u1, const float *__restrict__ _w2, const float *__restrict__ __w2, const F *__restrict__ _u2, const F *__restrict__ const _gy, F *__restrict__ const _gr, F *__restrict__ const _gk, F *__restrict__ const _gv, F *__restrict__ const _gw1, F *__restrict__ const _gu1, F *__restrict__ const _gw2, F *__restrict__ const _gu2) {)\n(107, CUDA, float state[_N_] = {0}, saaaa[_N_] = {0}, sbbbb[_N_] = {0}, scccc[_N_] = {0}, sdddd[_", "label": "(22, CUDA, __shared__ float v[_N_], r[_N_], k[_N_], gy[_N_];)"}
{"predict": "(81, OpenMP, #pragma omp parallel for)\n(93, OpenMP, #pragma omp parallel for)", "label": "(93, OpenMP, #pragma omp target teams distribute parallel for); (86, OpenMP, #pragma omp target parallel for)"}
{"predict": "= 0; j < NBLOCK; j++) {\n97:                 rmabuf[NELM * wsize + NBLOCK * (rank + i * wsize) + j] = j;\n98:             }\n99:         }\n100:         MPI_Win_lock(MPI_LOCK_EXCLUSIVE, trank, 0, win);\n101:         toterrs += testValues(1, NELM, wsize, rmabuf, \"Longer puts (1)\");\n102:         toterrs += testValues(1, NELM, wsize, rmabuf + wsize * NELM, \"Longer puts (2)\");\n103:         MPI_Win_unlock(trank, win);\n104:     }\n105: \n106:     /* Free resources */\n107:     MPI_Win_free(&win);\n108:     free(rmabuf);\n109:     free(localbuf);\n109:     free(localbuf2);\n110:     free(vals);\n111: \n112:     MTest_Finalize();\n113:     return 0;\n114: }", "label": "(12, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (208, MPI, MPI_Win_free(&win);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (46, MPI, MPI_Put(&localbuf[i], 1, MPI_INT, trank, rank + i * wsize, 1, MPI_INT, win);); (44, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, trank, 0, win);); (45, MPI, MPI_Put(&localbuf[i], 1, MPI_INT, trank, rank + (i + NELM) * wsize, 1, MPI_INT, win);); (45, MPI, MPI_Win_unlock(trank, win);); (127, MPI, MPI_Get(&vals[i], 1, MPI_INT, trank, i, 1, MPI_INT, win);); (183, MPI, MPI_Accumulate(&localbuf[i], 1, MPI_INT, trank, rank + i * wsize, 1, MPI_INT, MPI_SUM, win);); (37, MPI, MPI_Barrier(MPI_COMM_WORLD);); (6, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(10, CUDA, for (int i = 0; i < kNumel; ++i) { __syncthreads(); h_src[i] = (Element)i; __syncthreads(); });", "label": "(14, CUDA, thrust::device_vector<Element> d_src = h_src;); (8, CUDA, thrust::host_vector<Element> h_src(kNumel);)"}
{"predict": "(44, OpenMP, #pragma omp parallel for reduction(+:res_host))", "label": "(51, CUDA, cudaFree(res_dev);); (44, CUDA, cudaMalloc(&res_dev, sizeof(float));); (46, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "*nub_loc;\n99: \t\t\t\tlbend = (nn+1)*nub_loc;\n100: \t\t\t}\n101: \n102: \t\t\tfor(ii=lbstart;ii<lbend;++ii){\n103: \n104: \t\t\t\tik = Urbs[ii]; /* Block number */\n105: \t\t\t\tirow = ik * knsupc; /* Row block number */\n106: \n107: \t\t\t\tikfrow = ik * knsupc; /* First row of block */\n108: \t\t\t\tirow = ik * knsupc; /* Row block number */\n109: \n110: \t\t\t\tiword = sizeof(int_t);\n111: \t\t\t\tdword = sizeof(double);\n112: \n113: \t\t\t\tfor(i=ikfrow;i<irow+knsupc;++i){\n114: \n115: \t\t\t\t\tfor(j=0;j<nrhs;++j){\n116: \n117: \t\t\t\t\t\ty = &x[i*nrhs+j];\n118: \n119: \t\t\t\t\t\tlsub = Llu->lsub[i];\n120: \t\t\t\t\t\tlusup = Llu->lusup[i];\n121: \n122: \t\t\t\t\t\tiknsupc = SuperSize( ik );\n123: \n124: \t\t\t\t\t\tfor (ii=0;ii<iknsupc;++ii){\n125: \n126: \t\t\t\t\t\t\tikfrow = ik * knsupc; /* First row of block */\n127: \t\t\t\t\t\t\tirow = ik * knsupc; /* Row block number */\n128: \n129: \t\t\t\t\t\t\tuptr = Llu->Ucb_indptr[ik][ii];\n130: \t\t\t\t\t\t\tluptr = Llu->Lcb_indptr[ik][ii];\n131: \n132: \t\t\t\t\t\t\tgik = Llu->Ucb_valptr[ik][uptr];\n133: \t\t\t\t\t\t\tgikcol = Llu->Ucb_valptr[ik][uptr+1];\n134: \n135: \t\t\t\t\t\t\ttemp = conj(Llu->Ucb_valptr[ik][uptr+2]);\n136: \n137: \t\t\t\t\t\t\tlsub = Llu->lsub[i];\n138: \t\t\t\t\t\t\tlusup =", "label": "(82, OpenMP, #pragma\tomp\ttaskloop firstprivate (stat) private (thread_id1,Uinv,nn,lbstart,lbend,ub,temp,rtemp_loc,ik,lk1,gik,gikcol,usub,uval,lsub,lusup,iknsupc,il,i,irow,bmod_tmp,p,ii,jj,t1,t2,j,ikfrow,iklrow,dest,y,uptr,fnz,nsupr,nroot_send_tmp) untied nogroup); (126, OpenMP, #pragma omp simd); (146, OpenMP, #pragma omp atomic capture); (75, OpenMP, #pragma\tomp\tparallel for private (thread_id1,Uinv,nn,lbstart,lbend,ub,temp,rtemp_loc,ik,lk1,gik,gikcol,usub,uval,lsub,lusup,iknsupc,il,i,irow,bmod_tmp,p,ii,jj,t1,t2,j,ikfrow,iklrow,dest,y,uptr,fnz,nsupr))"}
{"predict": "(12, OpenMP, #pragma omp parallel for)", "label": "(48, CUDA, CubDebugExit(cudaMemcpy(d_flags, h_flags, sizeof(unsigned char) * num_items, cudaMemcpyHostToDevice));); (46, CUDA, CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));)"}
{"predict": "(1, CUDA, __global__ void bitonicMergeShared(uint *d_DstKey, uint *d_DstVal, uint *d_SrcKey, uint *d_SrcVal, uint arrayLength, uint size, uint dir) {); (38, CUDA, )", "label": "(6, CUDA, __shared__ uint s_key[SHARED_SIZE_LIMIT];); (6, CUDA, __shared__ uint s_val[SHARED_SIZE_LIMIT];)"}
{"predict": "8: \t\tlen = 120;\n59: \telse\n60: \t\tlen = max_inp_len;\n61: \tpScr = mem_alloc(len);\n62: \tif (!pScr)\n63: \t\treturn 1;\n64: \tif (bNeedPuc)\n65: \t\tpScr[0] = '1';\n66: \telse\n67: \t\tpScr[0] = '0';\n68: \ti = 0;\n69: \tif (bNeedPuc) {\n70: \t\ti++;\n71: \t\tpScr[i] = '\\0';\n72: \t}\n73: \t// now, start processing the input\n74: \tfor (i = 0; i < len; i++) {\n75: \t\tif (pExpr[i] == 'x' || pExpr[i] == 'X') {\n76: \t\t\ti++;\n77: \t\t\tif (pExpr[i] == 'p' || pExpr[i] == 'P') {\n78: \t\t\t\ti++;\n79: \t\t\t\tif (pExpr[i] == 'u' || pExpr[i] == 'U') {\n80: \t\t\t\t\ti++;\n81: \t\t\t\t\tif (pExpr[i] == 'c' || pExpr[i] == 'C') {\n82: \t\t\t\t\t\ti++;\n83: \t\t\t\t\t\tif (pExpr[i] == 'k' || pExpr[i] == 'K') {\n84: \t\t\t\t\t\t\ti++;\n85: \t\t\t\t\t\t\tif (pExpr[i] == 'e' || pExpr[i] == 'E') {\n86: \t\t\t\t\t\t\t\ti++;\n87: \t\t\t\t\t\t\t\tif (pExpr[i] == 'd' || pExpr[i] == 'D') {\n88: \t\t\t\t\t\t\t\t\ti++;\n89: \t\t\t\t\t\t\t\t\tif (pExpr[i] == 'c' || pExpr[i] == 'C') {\n90: \t\t\t\t\t\t\t\t\t\ti++;\n91: \t\t\t\t\t\t\t\t\t\tif (pExpr[i] == 'k' || pExpr[i] == 'K') {\n92: \t\t\t\t\t\t\t\t\t\t\ti++;\n93: \t\t\t\t\t\t\t\t\t\t\tif (pExpr[i] == 's' || pExpr[i] == 'S') {\n94: \t\t\t\t\t\t\t\t\t\t\t\ti++;\n95: \t\t\t\t\t\t\t\t\t\t\t\tif (pExpr[i] == 'i' || pExpr[i] == 'I') {\n96:", "label": "(31, OpenMP, comp_add_script_line(\"Flag=MGF_SALT_AS_HEX_%s\\n\", tmp);); (19, OpenMP, comp_do_parse(0, nSyms-1);); (222, OpenMP, comp_add_script_line(\"Func=DynamicFunc__append_keys_pad20\\n\"); if (use_inp1) len_comp += 20; else len_comp2 += 20; }); (58, OpenMP, if (bNeedS) comp_add_script_line(\"Flag=MGF_SALTED\\n\");); (153, OpenMP, comp_add_script_line(\"Func=DynamicFunc__LargeHash_OUTMode_raw\\n\");); (18, OpenMP, comp_add_script_line(\"Expression=dynamic=%s\\n\", p->pExpr);); (306, OpenMP, comp_add_script_line(\"Flag=MGF_INPBASE64b\\n\");); (307, OpenMP, comp_add_script_line(\"Flag=MGF_INPBASE64m\\n\");); (18"}
{"predict": "(13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);) (14, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)", "label": "(38, MPI, MPI_Op_free(&op);); (16, MPI, MPI_Type_free(&dt_vector);); (35, MPI, MPI_Type_free(&dt_noncontig);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (13, MPI, MPI_Type_commit(&dt_noncontig);); (9, MPI, MPI_Type_vector(1, 1, noncontig_stride, MPI_INT, &dt_vector);); (9, MPI, MPI_Type_create_resized(dt_vector, 0, sizeof(int) * noncontig_stride, &dt_noncontig);)"}
{"predict": "(2, CUDA, extern \"C\" __global__ void mat_trans_smem_padding_kernel(int *dev_A, int M, int N, int *dev_B) {)", "label": "(6, CUDA, __shared__ int s_data[32][33];)"}
{"predict": "(66, CUDA, for (auto const &layer : layers) {) (70, CUDA, options.update({N, layer.h, layer.w, layer.c}, {layer.k, layer.r, layer.s, layer.c});) (72, CUDA, Result result = profile_convolution(options);) (73, CUDA, result.print(std::cout, idx, options) << std::endl;) (75, CUDA, ++idx;)}", "label": "(13, CUDA, CUDA_CHECK(cudaGetDeviceProperties(&props, 0));)"}
{"predict": "(7, CUDA, if (st != cudaSuccess) { return; } else { st = cudaSuccess; } );", "label": "(6, CUDA, st = cudaMalloc(&test_ptr, TEST_ALLOC_SIZE);); (11, CUDA, cudaFree(test_ptr);)"}
{"predict": "(20, OpenMP, #pragma omp parallel for)\n(23, OpenMP, #pragma omp parallel for)\n(28, OpenMP, #pragma omp parallel for)", "label": "(19, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (25, TBB, serial_tiled_time = (tbb::tick_count::now() - t0).seconds();); (30, TBB, tbb_time = (tbb::tick_count::now() - t0).seconds();); (18, TBB, serial_time = (tbb::tick_count::now() - t0).seconds();)"}
{"predict": "(43, CUDA, for (unsigned int i = 0; i < AGENT_COUNT/2; i++) {) (44, CUDA, flamegpu::AgentVector::Agent instance = population[i];) (45, CUDA, instance.setVariable<float>(\"x\", static_cast<float>(i));) (46, CUDA, instance.setVariable<int>(\"a\", i % 2 == 0 ? 1 : 0);) (53, CUDA, cudaSimulation.SimulationConfig().steps = 0;) (56, CUDA, cudaSimulation.initialise(argc, argv);) (57, CUDA, cudaSimulation.simulate();) (59, CUDA, cudaSimulation.getPopulationData(population);)", "label": "(52, CUDA, flamegpu::CUDASimulation cudaSimulation(model);)"}
{"predict": "(12, OpenMP, #pragma omp for)", "label": "(8, OpenMP, #pragma omp section); (6, OpenMP, #pragma omp parallel)"}
{"predict": "(28, MPI, int index = 0; int num = n; int sorted = (hints & (DTCMP_FLAG_SORTED | DTCMP_FLAG_SORTED_LOCAL)); if (!sorted) { printf(\"ERROR: data must be locally sorted @ %s:%d\\n\",__FILE__, __LINE__); } MPI_Aint keysat_lb, keysat_extent; MPI_Aint key_true_lb, key_true_extent; size_t size_key = key_true_extent; char* my_val = dtcmp_malloc(size_key, 0, __FILE__, __LINE__); char* out_val = dtcmp_malloc(size_key, 0, __FILE__, __LINE__); int found_exact = 0; while (1) { if (num > 0) { int median_index = (num / 2) + index; if (median_index < n) { const void* median = (char*)data + median_index * keysat_extent; DTCMP_Memcpy(my_val, 1, key, median, 1, key); } } int flag; dtcmp_randbcast(my_val, num, out_val, &flag, 1, key, comm); if (flag == 0) { printf(\"ERROR: random bcast failed @ %s:%d\\n\",__FILE__, __LINE__); MPI_Abort(MPI_COMM_WORLD, 1); } uint64_t counts[2]; if (num == 0) { counts[LT] = 0; counts[EQ] = 0; } else { void* target = out_val; int start_index = index; int end_index = index + num - 1; int flag, lowest, highest; DTCMP_Search_low_local(target, data, start_index, end_index, key, keysat, cmp, hints, &flag, &lowest); DTCMP_Search_high_local(target, data, lowest, end_index, key, keysat, cmp, hints, &flag, &highest); counts[LT] = (uint64_t) (lowest - start_index); counts[EQ] = (uint64_t) ((highest + 1) - lowest); } uint64_t all_counts[2]; if (k <= all_counts[LT]) { num = (int) counts[LT]; } else if (k > (all_counts[LT] + all_counts[EQ])) { int num_lte = (int) (counts[LT] + counts[", "label": "(25, MPI, MPI_Type_get_true_extent(key, &key_true_lb, &key_true_extent);); (15, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (83, MPI, MPI_Allreduce(counts, all_counts, 2, MPI_UINT64_T, MPI_SUM, comm);); (18, MPI, MPI_Type_get_extent(keysat, &keysat_lb, &keysat_extent);)"}
{"predict": "(6, OpenMP, #pragma omp parallel for private(last) reduction(+:last))", "label": "(24, CUDA, checkCudaErrors(cudaMalloc((void **)&bLines_d, N_LINES * sizeof(BezierLine)));); (35, CUDA, checkCudaErrors(cudaFree(bLines_d));)"}
{"predict": "(1, CUDA, __global__ void layernorm_forward_kernel5(float* __restrict__ out, float* __restrict__ mean, float* __restrict__ rstd, const float*  __restrict__ inp, const float*  __restrict__ weight, const float* __restrict__ bias, int N, int C) {)", "label": "(7, CUDA, __shared__ float shared_sum2[32];); (5, CUDA, __shared__ float shared_sum[32];)"}
{"predict": "(4, OpenMP, #pragma omp parallel for)", "label": "(2, OpenMP, #pragma omp parallel for schedule(static))"}
{"predict": "relational comparison ('<', '<=', '>', or '>=') of loop variable 'i'}} omp5-error@+2 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', '>=', or '!=') of loop variable 'i'}}\n93: #pragma omp parallel for simd\n94:   for (int i = 0; i == 0; i++)\n95:     c[i] = a[i];\n96: \n97: // omp4-error@+2 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', or '>=') of loop variable 'i'}} omp5-error@+2 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', '>=', or '!=') of loop variable 'i'}}\n98: #pragma omp parallel for simd\n99:   for (int i = 0; i != 0; i++)\n100:     c[i] = a[i];\n101: \n102: // omp4-error@+2 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', or '>=') of loop variable 'i'}} omp5-error@+2 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', '>=', or '!=') of loop variable 'i'}}\n103: #pragma omp parallel for simd\n104:   for (int i = 0; i == 1; i++)\n105:     c[i] = a[i];\n106: \n107: // omp4-error@+2 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', or '>=') of loop variable 'i'}} omp5-error@+2 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', '>=', or '!=') of loop variable 'i'}}\n108: #pragma omp parallel for simd\n109:   for (int i = 0; i != 1; i++)\n110:     c[i] = a[i];\n111: \n112: // omp4-error@+2 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', or '>=') of loop", "label": "(235, OpenMP, #pragma omp parallel for simd private(ii)); (5, OpenMP, #pragma omp parallel for simd); (257, OpenMP, #pragma omp parallel for simd collapse(2)); (226, OpenMP, #pragma omp parallel for simd linear(ii)); (237, OpenMP, #pragma omp parallel for simd lastprivate(ii)); (220, OpenMP, #pragma omp parallel for simd firstprivate(ii))"}
{"predict": "(13, TBB, tbb::parallel_for(tbb::blocked_range<size_t>(0, n), [&](const tbb::blocked_range<size_t>& r) { for (size_t i = r.begin(); i != r.end(); ++i) { if (!Validate(iter, sorted_list, i)) passed = false; } }));", "label": "(12, TBB, tbb::parallel_sort(iter, iter + n, *compare );)"}
{"predict": "(10, OpenMP, #pragma omp parallel for)", "label": "(10, TBB, double serial_time = (tbb::tick_count::now() - t0).seconds();); (7, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (16, TBB, double tbb_time = (tbb::tick_count::now() - t1).seconds();); (13, TBB, tbb::tick_count t1 = tbb::tick_count::now();)"}
{"predict": "(14, CUDA, cudaMalloc((void **)&d_A, M); cudaMalloc((void **)&d_B, M);)\n(15, CUDA, cudaMemcpy(d_A, h_A, M, cudaMemcpyHostToDevice);)\n(27, CUDA, cudaMemcpy(d_B, h_B, M, cudaMemcpyHostToDevice);)\n(30, CUDA, cudaMemcpy(h_B, d_B, M, cudaMemcpyDeviceToHost);)\n(40, CUDA, cudaFree(d_A); cudaFree(d_B);)", "label": "(24, CUDA, CHECK(cudaMalloc(&d_B, M));); (24, CUDA, CHECK(cudaMemcpy(d_A, h_A, M, cudaMemcpyHostToDevice));); (42, CUDA, CHECK(cudaFree(d_B));); (20, CUDA, CHECK(cudaMalloc(&d_A, M));); (28, CUDA, CHECK(cudaMemcpy(h_B, d_B, M, cudaMemcpyDeviceToHost));); (38, CUDA, CHECK(cudaFree(d_A));)"}
{"predict": "(3, CUDA, cudaStream_t stream = nullptr;)\n(13, CUDA, model(u_constraint, stream);)", "label": "(4, CUDA, HANDLE_ERROR(cudaStreamCreate(&stream));); (15, CUDA, HANDLE_ERROR(cudaStreamDestroy(stream));)"}
{"predict": "(14, CUDA, __syncthreads();)", "label": "(12, CUDA, __shared__ uint data[MERGE_THREADBLOCK_SIZE];)"}
{"predict": "(19, OpenMP, #pragma omp parallel for)", "label": "(2, OpenMP, #pragma omp critical)"}
{"predict": "(25, OpenCL, auto clBuffer = clCreateBuffer(context.get(), flags, testBufferSizeInBytes, pHostPtr, &retVal);); (36, OpenCL, auto subBuffer = clCreateSubBuffer(clBuffer, invalidFlags, CL_BUFFER_CREATE_TYPE_REGION, &region, &retVal);)", "label": "(38, OpenCL, retVal = clReleaseMemObject(clBuffer);)"}
{"predict": "(27, CUDA, data = (complex_type*)cudaMallocManaged(&size_bytes);)", "label": "(26, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&data, size_bytes));); (19, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamCreate(&stream));); (48, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (46, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (53, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(data));); (53, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamDestroy(stream));)"}
{"predict": "(29, OpenMP, #pragma omp parallel for)", "label": "(15, OpenMP, #pragma omp target map(from:px)); (7, OpenMP, #pragma omp target data map(to:x[0:count])); (21, OpenMP, #pragma omp target data use_device_ptr(x)); (26, OpenMP, #pragma omp target data use_device_addr(x[0:0]))"}
{"predict": "(32, CUDA, cudaDeviceProp deviceProp; cudaGetDeviceProperties(&deviceProp, 0);) (15, CUDA, printf(\"> GPU Device has Compute Capabilities SM %d.%d\\n\\n\", deviceProp.major, deviceProp.minor);) (46, CUDA, CHECKED_CALL(cudaFuncSetCacheConfig(test_interval_newton<T>, cudaFuncCachePreferL1));) (53, CUDA, interval_gpu<T> i(0.01f, 4.0f);) (55, CUDA, test_interval_newton<T><<<GRID_SIZE, BLOCK_SIZE>>>(d_result, d_nresults, i, implementation_choice);) (66, CUDA, CHECKED_CALL(cudaMemcpy(h_result, d_result, THREADS * DEPTH_RESULT * sizeof(*d_result), cudaMemcpyDeviceToHost));) (68, CUDA, CHECKED_CALL(cudaMemcpy(h_nresults, d_nresults, THREADS * sizeof(*d_nresults), cudaMemcpyDeviceToHost));) (80, CUDA, float time; time = cudaGetEventElapsedTime(start, stop);)", "label": "(59, CUDA, CHECKED_CALL(cudaEventRecord(start, 0));); (52, CUDA, CHECKED_CALL(cudaDeviceSetLimit(cudaLimitStackSize, 8192));); (62, CUDA, CHECKED_CALL(cudaGetLastError());); (41, CUDA, CHECKED_CALL(cudaEventCreate(&start));); (36, CUDA, CHECKED_CALL(cudaSetDevice(devID));); (88, CUDA, CHECKED_CALL(cudaEventDestroy(start));); (89, CUDA, CHECKED_CALL(cudaFree(d_result));); (61, CUDA, CHECKED_CALL(cudaDeviceSynchronize());); (6, CUDA, cudaGetDeviceProperties(&deviceProp, devID);); (3, CUDA, int devID = findCudaDevice(argc, (const char **)argv);); (78, CUDA, CHECKED_CALL(cudaEventElapsedTime(&time, start, stop));); (56, CUDA, CHECKED_CALL(cudaEventRecord(stop, 0));); (84, CUDA, CHECKED_CALL(cudaFree(d_nresults));); (30, CUDA, CHECKED_CALL(cudaMalloc((void **)&d_nresults, THREADS * sizeof(*d_nresults)));); (80, CUDA, CHECKED_CALL(cudaEventDestroy(stop));); (30, CUDA, CHECKED_CALL(cudaEventCreate(&stop));)"}
{"predict": "(11, OpenMP, #pragma omp parallel for collapse(2))", "label": "(4, OpenACC, #pragma acc enter data create(in1[:w*h], in2[:h*w], out[:w*h])); (12, OpenACC, #pragma acc loop); (8, OpenACC, #pragma acc update device(in1[lower*w:(upper-lower)*w],in2[lower*w:(upper-lower)*w]) async(block%2)); (17, OpenACC, #pragma acc wait); (7, OpenACC, #pragma acc parallel loop present(in1,in2, out) async(block%2)); (13, OpenACC, #pragma acc update self(out[lower*w:(upper-lower)*w]) async(block%2)); (15, OpenACC, #pragma acc exit data delete(in1, in2, out))"}
{"predict": "(15, CUDA, diffImg[i].Allocate(w, h, p, false, memoryTmp + i*p*h);)\n(34, CUDA, cudaTextureObject_t texObj = 0;)\n(37, CUDA, TimerGPU timer1;)\n(41, CUDA, LaplaceMulti(texObj, img, diffImg, octave);)\n(42, CUDA, FindPointsMulti(diffImg, siftData, thresh, 10.0f, 1.0f/NUM_SCALES, lowestScale/subsampling, subsampling, octave);)\n(44, CUDA, TimerGPU timer4;)\n(47, CUDA, ComputeOrientations(texObj, img, siftData, octave);)\n(48, CUDA, ExtractSiftDescriptors(texObj, siftData, subsampling, octave);)", "label": "(5, CUDA, safeCall(cudaGetSymbolAddress((void**)&d_PointCounterAddr, d_PointCounter));); (57, CUDA, safeCall(cudaMemcpy(&totPts, &d_PointCounterAddr[2*octave+1], sizeof(int), cudaMemcpyDeviceToHost));); (5, CUDA, safeCall(cudaMemcpy(&fstPts, &d_PointCounterAddr[2*octave-1], sizeof(int), cudaMemcpyDeviceToHost));); (50, CUDA, safeCall(cudaDestroyTextureObject(texObj));); (32, CUDA, cudaCreateTextureObject(&texObj, &resDesc, &texDesc, NULL);)"}
{"predict": "(7, OpenMP, #pragma omp parallel for)\n(12, OpenMP, #pragma omp parallel for reduction(+:j))\n(17, OpenMP, #pragma omp parallel for)\n(28, OpenMP, #pragma omp parallel for)\n(30, OpenMP, #pragma omp parallel for)\n(32, OpenMP, #pragma omp parallel for)\n(40, OpenMP, #pragma omp parallel for)\n(42, OpenMP, #pragma omp parallel for)\n(52, OpenMP, #pragma omp parallel for)\n(56, OpenMP, #pragma omp parallel for)\n(74, OpenMP, #pragma omp parallel for)", "label": "(4, TBB, tbb::flow::queue_node<T> q2(g);); (51, TBB, tbb::flow::make_edge( q2, q3 );); (3, TBB, tbb::flow::queue_node<T> q3(g);); (48, TBB, tbb::flow::make_edge( q, q2 );); (-2, TBB, tbb::flow::graph g;); (68, TBB, tbb::flow::queue_node<T> q_copy(q);); (-3, TBB, tbb::flow::queue_node<T> q(g);)"}
{"predict": "(29, MPI, MPI_Win_set_attr(win, key[1], &attrval[1]);) (30, MPI, MPI_Win_set_attr(win, key[2], &attrval[2]);) (31, MPI, MPI_Win_set_attr(win, key[0], &attrval[0]);) (33, MPI, MPI_Win_delete_attr(win, key[2]);) (34, MPI, MPI_Win_delete_attr(win, key[1]);) (35, MPI, MPI_Win_delete_attr(win, key[0]);) (37, MPI, MPI_Win_set_attr(win, key[0], &attrval[0]);) (38, MPI, MPI_Win_set_attr(win, key[1], &attrval[1]);) (39, MPI, MPI_Win_set_attr(win, key[2], &attrval[2]);) (41, MPI, MPI_Win_delete_attr(win, key[1]);) (42, MPI, MPI_Win_delete_attr(win, key[2]);) (43, MPI, MPI_Win_delete_attr(win, key[0]);)", "label": "(27, MPI, MPI_Win_set_attr(win, key[0], &attrval[0]);); (12, MPI, MPI_Win_create(buf, sizeof(int), sizeof(int), MPI_INFO_NULL, comm, &win);); (29, MPI, MPI_Win_delete_attr(win, key[0]);); (30, MPI, MPI_Win_delete_attr(win, key[2]);); (58, MPI, MPI_Win_free_keyval(&key[i]);); (20, MPI, MPI_Win_set_attr(win, key[2], &attrval[2]);); (58, MPI, MPI_Win_free(&win);); (19, MPI, MPI_Win_set_attr(win, key[1], &attrval[1]);); (24, MPI, MPI_Win_delete_attr(win, key[1]);)"}
{"predict": "unsigned char *compute_checkum[SSE_GROUP_SZ_SHA1];\n120: \t\t\tunsigned char *iv[SSE_GROUP_SZ_SHA1];\n121: \t\t\tunsigned char *key[SSE_GROUP_SZ_SHA1];\n122: \t\t\tTwofish_key *tkey[SSE_GROUP_SZ_SHA1];\n123: \t\t\tint datalen[SSE_GROUP_SZ_SHA1];\n124: \t\t\tunsigned char *store_data_decrypted[SSE_GROUP_SZ_SHA1];\n125: \t\t\tSHA_CTX *ctx[SSE_GROUP_SZ_SHA1];\n126: \n127: \t\t\tfor (j = 0; j < SSE_GROUP_SZ_SHA1; ++j) {\n128: \t\t\t\tiv[j] = (unsigned char*)calloc(16, 1);\n129: \t\t\t\tkey[j] = (unsigned char*)calloc(32, 1);\n130: \t\t\t\ttkey[j] = (Twofish_key*)calloc(1, sizeof(Twofish_key));\n131: \t\t\t\tstore_data_decrypted[j] = (unsigned char*)calloc(MAX_STORE_DATA_LENGTH, 1);\n132: \t\t\t\tctx[j] = (SHA_CTX*)calloc(1, sizeof(SHA_CTX));\n133: \t\t\t}\n134: \n135: \t\t\tfor (j = 0; j < SSE_GROUP_SZ_SHA1; ++j) {\n136: \t\t\t\tdatalen[j] = Twofish_Decrypt(tkey[j], keys[j], store_data_decrypted[j], lens[j], iv[j]);\n137: \t\t\t\tif (datalen[j] < 0)\n138: \t\t\t\t\tcontinue;\n139: \t\t\t\tSHA1_Init(ctx[j]);\n140: \t\t\t\tSHA1_Update(ctx[j], store_data_decrypted[j], datalen[j] - 20);\n141: \t\t\t\tSHA1_Final(compute_checkum[j], ctx[j]);\n142: \n143: \t\t\t\tif (!memcmp(compute_checkum[j], store_data_decrypted[j] + datalen[j] - 20, 20))\n144: \t\t\t\t{\n145: \t\t\t\t\tcracked[index+j] = 1;\n146: #ifdef _OPENMP\n147: #pragma omp atomic\n148: #endif\n149: \t\t\t\t\tany_cracked |= 1;\n1", "label": "(11, OpenMP, #pragma omp parallel for); (38, OpenMP, #pragma omp atomic)"}
{"predict": "(17, MPI, MPI_Comm_rank(comm, &rank);)", "label": "(19, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": ":     } else {\n104:         printf(\"  MPI-4 COMPLIANT:\\tpost-finalize MPI_ERROR_STRING returned %s\\n\", estr);\n105:     }\n106: \n107:     return 0;\n108: }", "label": "(28, MPI, MPI_Init(&argc, &argv);); (82, MPI, MPI_Comm_disconnect(&icomm);); (76, MPI, MPI_Comm_spawn(argv[0], &spawn_argv[1], 1, spawn_info, 0, MPI_COMM_WORLD, &icomm, spawn_err);); (26, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (83, MPI, rc = MPI_Comm_set_errhandler(MPI_COMM_SELF, MPI_ERRORS_ARE_FATAL);); (43, MPI, MPI_Info_get(MPI_INFO_ENV, \"mpi_initial_errhandler\", MPI_MAX_INFO_VAL, init_errh_info, &flag);); (68, MPI, MPI_Info_set(spawn_info, \"mpi_initial_errhandler\", \"mpi_errors_return\");); (90, MPI, rc = MPI_Finalize();); (74, MPI, MPI_Barrier(icomm);); (80, MPI, MPI_Error_string(rc, estr, &slen);); (49, MPI, MPI_Comm_get_errhandler(MPI_COMM_SELF, &errh);); (62, MPI, MPI_Info_create(&spawn_info);); (93, MPI, rc = MPI_Error_string(MPI_ERR_WIN, estr, &slen);)"}
{"predict": "(8, OpenMP, #pragma omp parallel)", "label": "(3, OpenMP, #pragma omp target map(tofrom: threadCount)); (4, OpenMP, #pragma omp metadirective)"}
{"predict": "(3, CUDA, int major, minor;); (4, CUDA, assert (devID>=0 && devID<xomp_max_num_devices);); (5, CUDA, if (major <= 2) //1.x and 2.x: 8 blocks per multiprocessor); (6, CUDA, return 8;); (7, CUDA, else if (major == 3)); (8, CUDA, return 16;); (9, CUDA, else if (major == 5)); (10, CUDA, return 32;); (11, CUDA, else); (12, CUDA, {); (13, CUDA, assert (false);); (14, CUDA, }); (15, CUDA, assert (false);); (16, CUDA, return 0;);", "label": "(14, OpenMP, printf(\"Error: xomp_get_maxThreadBlocksPerMultiprocessor(): unhandled Compute Capability numbers%d.%d \\n\", major, minor);); (3, CUDA, major = xomp_getCudaDeviceProp(devID)-> major;); (3, CUDA, minor = xomp_getCudaDeviceProp(devID)-> minor;)"}
{"predict": "(7, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &comm);) (14, MPI, MPI_Comm_dup(comm, &self_dup);) (22, MPI, MPI_Comm_dup(comm, &self_dup);)", "label": "(26, MPI, MPI_Comm_dup(comms[*(int *) arg], &comm);); (4, MPI, MPI_Comm_rank(comms[*(int *) arg], &rank);); (20, MPI, MPI_Comm_free(&self_dup);); (24, MPI, MPI_Comm_free(&comm);); (13, MPI, MPI_Comm_dup(MPI_COMM_SELF, &self_dup);)"}
{"predict": "(29, CUDA, cudaStream_t stream;); (30, CUDA, cudaStreamCreate(&stream);); (32, CUDA, HANDLE_ERROR(cudaMemcpyAsync(costs_dev_only_d, cost_vec.data(), sizeof(float) * num_rollouts, cudaMemcpyHostToDevice, stream));); (33, CUDA, HANDLE_ERROR(cudaMemcpyAsync(costs_host_only_d, cost_vec.data(), sizeof(float) * num_rollouts, cudaMemcpyHostToDevice, stream));); (35, CUDA, HANDLE_ERROR(cudaMemcpyAsync(host_dev_costs.data(), costs_host_only_d, num_rollouts * sizeof(float), cudaMemcpyDeviceToHost, stream));); (36, CUDA, HANDLE_ERROR(cudaStreamSynchronize(stream));); (38, CUDA, HANDLE_ERROR(cudaMemcpyAsync(host_dev_costs.data(), costs_host_only_d, num_rollouts * sizeof(float), cudaMemcpyDeviceToHost, stream));); (39, CUDA, HANDLE_ERROR(cudaStreamSynchronize(stream));); (41, CUDA, HANDLE_ERROR(cudaMemcpyAsync(dev_only_costs.data(), costs_dev_only_d, num_rollouts * sizeof(float), cudaMemcpyDeviceToHost, stream));); (42, CUDA, HANDLE_ERROR(cudaMemcpyAsync(&device_components, baseline_and_normalizer_d, sizeof(float2), cudaMemcpyDeviceToHost, stream));); (43, CUDA, HANDLE_ERROR(cudaStreamSynchronize(stream));); (70, CUDA, cudaStreamDestroy(stream);)", "label": "(12, CUDA, cudaStreamCreate(&stream);); (37, CUDA, HANDLE_ERROR(cudaStreamSynchronize(stream));); (17, CUDA, HANDLE_ERROR(cudaMalloc((void**)&costs_dev_only_d, sizeof(float) * num_rollouts));); (15, CUDA, HANDLE_ERROR(cudaMalloc((void**)&baseline_and_normalizer_d, sizeof(float2)));); (16, CUDA, HANDLE_ERROR(cudaMalloc((void**)&costs_host_only_d, sizeof(float) * num_rollouts));)"}
{"predict": "(6, CUDA, for (int c = threadIdx.x; c < cols; c += blockDim.x) {); (15, CUDA, m_val = BlockReduce<float>(m_storage).Sum(m_val);); (17, CUDA, v_val = BlockReduce<float>(v_storage).Sum(v_val);); (22, CUDA, if (threadIdx.x == 0) {);", "label": "(2, CUDA, __shared__ typename BlockReduce<float>::TempStorage v_storage;); (0, CUDA, __shared__ typename BlockReduce<float>::TempStorage m_storage;)"}
{"predict": "(31, TBB, tbb::parallel_for(tbb::blocked_range<int>(0, N), [&](const tbb::blocked_range<int>& r) { for (int i = r.begin(); i != r.end(); ++i) { serialFwdSub(serial_x[i], serial_a[i], serial_b[i]); } }));", "label": "(33, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (41, TBB, reduce_time = (tbb::tick_count::now() - t0).seconds();); (47, TBB, graph_time = (tbb::tick_count::now() - t0).seconds();); (32, TBB, serial_time = (tbb::tick_count::now() - t0).seconds();)"}
{"predict": "(9, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &world_size);)"}
{"predict": "(12, OpenMP, #pragma omp parallel for shared(a, b))", "label": "(8, OpenMP, rec = get_decomp_record(code);)"}
{"predict": "(13, MPI, if (rank % 2 == 0) { MTest_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &pmode); } else { MPI_Finalize(); return 0; }); (14, MPI, if (pmode != MPI_THREAD_MULTIPLE) { fprintf(stderr, \"Thread Multiple not supported by the MPI implementation\\n\"); MPI_Finalize(); return 0; }); (17, MPI, if (size % 2) { fprintf(stderr, \"This test needs an even number of processes\\n\"); MPI_Finalize(); return 0; }); (21, MPI, for (i = 0; i < MAX_THREADS; i++) { tp[i].thread_id = i; tp[i].use_proc_null = 1; tp[i].use_blocking_comm = 1; tp[i].msg_size = 0; }); (27, MPI, if (!rank && verbose) { printf(\"\\nUsing MPI_PROC_NULL\\n\"); printf(\"-------------------\\n\"); }); (31, MPI, loops()); (34, MPI, for (j = 0; j < MAX_MSG_SIZE; j = (!j ? 1 : j * HOP)) { for (i = 0; i < MAX_THREADS; i++) { tp[i].thread_id = i; tp[i].use_proc_null = 0; tp[i].use_blocking_comm = 1; tp[i].msg_size = j; } if (!rank && verbose) { printf(\"\\nBlocking communication with message size %6d bytes\\n\", j); printf(\"------------------------------------------------------\\n\"); } loops(); }); (39, MPI, for (j = 0; j < MAX_MSG_SIZE; j = (!j ? 1 : j * HOP)) { for (i = 0; i < MAX_THREADS; i++) { tp[i].thread_id = i; tp[i].use_proc_null = 0; tp[i].use_blocking_comm = 0; tp[i].msg_size = j; } if (!rank && verbose) { printf(\"\\nNon-blocking communication with message size %6d bytes\\n\", j); printf(\"----------------------------------------------------------\\n\"); } loops(); }); (63, MPI, MTest_Finalize(0));", "label": "(10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(29, CUDA, float mean = 0.0, var = 0.0;); (30, CUDA, mean = sdata[0] / channels; var = sdata2[0] + mean * mean * channels - 2 * mean * channels * mean; var = sqrt(var / channels + 1e-10););", "label": "(6, CUDA, __shared__ float sdata[THREAD_PER_BLOCK];); (8, CUDA, __shared__ float var;); (5, CUDA, __shared__ float sdata2[THREAD_PER_BLOCK];); (5, CUDA, __shared__ float mean;)"}
{"predict": "*/\n42:     TestTypeDefinitionPresence( mutex );\n43:     TestTypeDefinitionPresence( recursive_mutex );\n44:     TestTypeDefinitionPresence( spin_mutex );\n45:     TestTypeDefinitionPresence( tbb::spin_mutex );\n46:     TestTypeDefinitionPresence( spin_rw_mutex );\n47:     TestTypeDefinitionPresence( tbb::spin_rw_mutex );\n48:     TestTypeDefinitionPresence( null_mutex );\n49:     TestTypeDefinitionPresence( recursive_spin_mutex );\n50:     TestTypeDefinitionPresence( recursive_spin_rw_mutex );\n51:     TestTypeDefinitionPresence( recursive_null_mutex );\n52:     TestTypeDefinitionPresence( simple_mutex );\n53:     TestTypeDefinitionPresence( simple_spin_mutex );\n54:     TestTypeDefinitionPresence( simple_rw_mutex );\n55:     TestTypeDefinitionPresence( simple_spin_rw_mutex );\n56:     TestTypeDefinitionPresence( tbb::simple_spin_rw_mutex );\n57:     TestTypeDefinitionPresence( recursive_simple_rw_mutex );\n58:     TestTypeDefinitionPresence( recursive_simple_spin_rw_mutex );\n59:     TestTypeDefinitionPresence( null_rw_mutex );\n59:     TestTypeDefinitionPresence( recursive_null_rw_mutex );\n60:     TestTypeDefinitionPresence( recursive_null_spin_rw_mutex );\n61:     TestTypeDefinitionPresence( recursive_null_simple_rw_mutex );\n62:     TestTypeDefinitionPresence( recursive_null_simple_spin_rw_mutex );\n63:     TestTypeDefinitionPresence( simple_null_mutex );\n64:     TestTypeDefinitionPresence( simple_null_rw_mutex );\n65:     TestTypeDefinitionPresence( simple_null_spin_rw_mutex );\n66:     TestTypeDefinitionPresence( simple_null_simple_rw_mutex );\n67:     TestTypeDefinitionPresence( simple_null_simple_spin_rw_mutex );\n68:     TestTypeDefinitionPresence( recursive_simple_null_mutex );\n69:     TestTypeDefinitionPresence( recursive_simple_null_rw_mutex );\n70:     TestTypeDefinitionPresence( recursive_simple_null_spin_rw_mutex );\n71:     TestTypeDefinitionPresence( recursive_simple_null_simple_rw_mutex );\n72:     TestTypeDefinitionPresence( recursive_simple_null_simple_spin_rw_mutex );\n73:     TestTypeDefinitionPresence( recursive_simple_null_null_rw_mutex );\n74:     TestTypeDefinitionPresence( recursive_simple_null_null_spin_rw_mutex );\n75:     TestTypeDefinitionPresence( recursive_simple_null_null_simple_rw_mutex );\n76:     TestTypeDefinitionPresence", "label": "(103, TBB, TestFuncDefinitionPresence( parallel_deterministic_reduce, (const tbb::blocked_range<int>&, Body2&, const tbb::simple_partitioner&, tbb::task_group_context&), void );); (32, TBB, TestTypeDefinitionPresence3(flow::async_node<int, int, tbb::flow::queueing_lightweight> );); (96, TBB, TestFuncDefinitionPresence( parallel_for_each, (const intarray&, const Body1&, tbb::task_group_context&), void );); (93, TBB, TestFuncDefinitionPresence( parallel_invoke, (const Body&, const Body&, tbb::task_group_context&), void );); (93, TBB, TestFuncDefinitionPresence( parallel_do, (const intarray&, const Body1a&, tbb::task_group_context&), void );); (27, TBB, TestTypeDefinitionPresence3(flow::multifunction_node<int, intpair, tbb::flow::queueing> );); (30, TBB, TestTypeDefinitionPresence2(flow::join_node<intpair, tbb::flow::key_matching<int> > );); (82, TBB, TestFuncDefinitionPresence( parallel_scan, (const tbb::blocked_range<int>&, const int&, const Body3a&, const Body1b&), int );); (75, TBB, TestFuncDefinitionPresence( parallel_for, (const tbb::blocked_range<int>&, const Body2&, const tbb::simple_partitioner&), void );); (79, TBB, TestFuncDefinitionPresence( parallel_scan, (const tbb::blocked_range2d<int>&, Body3&, const tbb::auto_partitioner&), void );); (74, TBB, TestFuncDefinitionPresence( parallel_reduce, (const tbb::blocked_range<int>&, const int&, const Body2a&, const Body1b&), int );); (23, TBB, TestTypeDefinitionPresence2(flow::continue_node<int, tbb::flow::lightweight> );); (73, TBB, TestFuncDefinitionPresence( parallel_reduce, (const tbb::blocked_range<int>&, Body2&, tbb::affinity_partitioner&), void );); (74, TBB, TestFuncDefinitionPresence( parallel_deterministic_reduce, (const tbb::blocked_range<int>&, Body2&, const tbb::static_partitioner&), void );); (14, TBB, typedef tbb::flow::tuple<int, int> intpair;); (33, TBB, TestTypeDefinitionPresence2(flow::composite_node<tbb::flow::tuple<int>, tbb::flow::tuple<int> > );); (15, TBB, TestTypeDefinitionPresence3(flow::function_node<int, int, tbb::flow::rejecting> );); (82, TBB, TestFuncDefinitionPresence( parallel_for, (int, int, const Body1&, const tbb::auto_partitioner&, tbb::task_group_context&), void );); (68, TBB, TestFuncDefinitionPresence( parallel_deterministic_reduce, (const tbb::blocked_range<int>&, const int&, const Body2a&, const Body1b&), int );); (75, TBB, TestFuncDefinitionPresence( parallel_pipeline, (size_t, const tbb::filter_t<void,void>&), void );); (81, TBB, TestFuncDefinitionPresence( parallel_reduce, (const tbb::blocked_range<int>&, Body2&, const tbb::auto_partitioner&, tbb::task_group_context&), void );); (5, TBB, TestFuncDefinitionPresence( flow::make_edge, (tbb::flow::sender<Msg>&, tbb::flow::receiver<Msg>&), void );); (82, TBB, TestFuncDefinitionPresence( parallel_deterministic_reduce, (const tbb::blocked_range<int>&, Body2&, tbb::task_group_context&), void );); (4, TBB, TestFuncDefinitionPresence( flow::remove_edge, (tbb::flow::sender<Msg>&, tbb::flow::receiver<Msg>&), void );); (76, TBB, TestFuncDefinitionPresence( parallel_for, (int, int, const Body1&, tbb::task_group_context&), void );); (77, TBB, TestFuncDefinitionPresence( parallel_reduce, (const tbb::blocked_range<int>&, Body2&, tbb::task_group_context&), void );); (9, TBB, TestTypeDefinitionPresence2(flow::join_node<intpair, tbb::flow::reserving> );)"}
{"predict": "allow for chunk size == 0)\n111:         // __kmp_chunk = KMP_MIN_CHUNK;\n112:       }\n113:     }\n114:   }\n115:   // Restore default schedule if any error occurred\n116:   if (sched == kmp_sch_default) {\n117:     __kmp_omp_schedule_restore();\n118:   }\n119:   return (const char *)__kmp_omp_schedule_set(sched, sched_modifier, chunk, layer);\n120: }\n\nOutput:", "label": "(30, OpenMP, __kmp_omp_schedule_restore();)"}
{"predict": "(14, MPI, MPI_Comm_rank(team.comm, &rank); MPI_Comm_size(team.comm, &nprocs);)", "label": "(12, MPI, MPI_Comm_rank(team.comm, &rank);); (12, MPI, MPI_Comm_size(team.comm, &nprocs);)"}
{"predict": "(13, OpenCL, kernel_info = kernel_info->next;)", "label": "(9, OpenCL, kernel_info->kernel = clCreateKernel(program, kernel_info->kernel_name, &err);)"}
{"predict": "(8, OpenCL, *pNumCorrect = clEnqueueMapBuffer(cmdq, num_correct, CL_MAP_READ, 0, sizeof(cl_int), 0, NULL, NULL, &error);); (11, OpenCL, test_error(error, \"clEnqueueUnmapMemObject failed.\");); (13, OpenCL, test_error(error,\"clEnqueueNDRangeKernel failed\");); (16, OpenCL, *pNumCorrect = clEnqueueMapBuffer(cmdq, num_correct, CL_MAP_READ, 0, sizeof(cl_int), 0, NULL, NULL, &error);); (18, OpenCL, error = clEnqueueUnmapMemObject(cmdq, num_correct, pNumCorrect, 0,NULL,NULL);); (20, OpenCL, test_error(error,\"clFinish failed\"););", "label": "(6, OpenCL, cl_int *pNumCorrect = (cl_int*) clEnqueueMapBuffer(cmdq, num_correct, CL_TRUE, CL_MAP_READ | CL_MAP_WRITE, 0, sizeof(cl_int), 0, NULL,NULL, &error);); (10, OpenCL, error = clEnqueueUnmapMemObject(cmdq, num_correct, pNumCorrect, 0,NULL,NULL);); (15, OpenCL, pNumCorrect = (cl_int*) clEnqueueMapBuffer(cmdq, num_correct, CL_TRUE, CL_MAP_READ | CL_MAP_WRITE, 0, sizeof(cl_int), 0, NULL,NULL, &error);); (11, OpenCL, error = clEnqueueNDRangeKernel(cmdq, kernel_verify_lists, 1, NULL, &numLists, NULL, 0, NULL, NULL);)"}
{"predict": "(4, TBB, task_scheduler_init init;)\n(9, TBB, parallel_for(blocked_range<size_t>(0, MaxThread), [&](const blocked_range<size_t>& r) { TestConcurrentFunctionality(); }));", "label": "(8, TBB, tbb::task_scheduler_init init_market_p_plus_one(MaxThread + 1);)"}
{"predict": "(13, MPI, MPI_Comm_size(comm, &comm_size);) (14, MPI, MPI_Comm_rank(comm, &comm_rank);)", "label": "(7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &comm_size);); (69, MPI, MPI_Comm_split(MPI_COMM_WORLD, 0, rand(), &comm);); (57, MPI, MPI_Comm_split(MPI_COMM_WORLD, (comm_rank == comm_size - 1) ? 0 : 1, 0, &comm);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &comm_rank);); (58, MPI, MPI_Comm_free(&comm);); (7, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (30, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(11, TBB, auto bufferMsg = BufferMsg::createBufferMsg(io.chunksRead(), io.chunkSize());) (23, TBB, io.writeChunk(bufferMsg.outputBuffer); BufferMsg::destroyBufferMsg(bufferMsg);)", "label": "(1, TBB, oneapi::tbb::flow::graph g;)"}
{"predict": "(7, OpenMP, #pragma omp parallel for) (14, OpenMP, #pragma omp parallel for)", "label": "(12, CUDA, checkCudaErrors(cudaMemcpy(dArray, array, size, cudaMemcpyHostToDevice));); (10, CUDA, checkCudaErrors(cudaMalloc(&dArray, size));); (19, CUDA, checkCudaErrors(cudaFree(dArray));); (17, CUDA, checkCudaErrors(cudaMemcpy(array, dArray, size, cudaMemcpyDeviceToHost));)"}
{"predict": "(19, TBB, auto &a_ref = a[i];)", "label": "(9, TBB, c.set_core_type(tbb::info::core_types().back());); (6, TBB, tbb::task_arena::constraints c;); (36, TBB, tbb::affinity_partitioner aff_p;); (5, TBB, c.set_numa_id(tbb::info::numa_nodes()[0]);); (26, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (46, TBB, double static_time = (tbb::tick_count::now() - t0).seconds();); (5, TBB, c.set_max_concurrency(std::min(8, tbb::info::default_concurrency(c)));); (36, TBB, double affinity_time = (tbb::tick_count::now() - t0).seconds();); (31, TBB, t0 = tbb::tick_count::now();); (25, TBB, double auto_time = (tbb::tick_count::now() - t0).seconds();); (2, TBB, tbb::task_arena arena(c);)"}
{"predict": "89:         float* pconf = ptheta + 1;\n90:         float* pw = pconf + 1;\n91:         float* ph = pw + 1;\n92:         float* pcx = ph + 1;\n93:         float* pcy = pcx + 1;\n94: \n95:         float cx = *pcx;\n96:         float cy = *pcy;\n97:         float w = *pw;\n98:         float h = *ph;\n99:         float theta = *ptheta;\n100:        float conf = *pconf;\n101:        float area = w * h;\n102:        float sin_t = sin(theta);\n103:        float cos_t = cos(theta);\n104:        float x1 = cx - w * 0.5 * cos_t - h * 0.5 * sin_t;\n105:        float y1 = cy - w * 0.5 * sin_t + h * 0.5 * cos_t;\n106:        float x2 = cx - w * 0.5 * cos_t + h * 0.5 * sin_t;\n107:        float y2 = cy - w * 0.5 * sin_t - h * 0.5 * sin_t;\n108:        float x3 = cx + w * 0.5 * cos_t + h * 0.5 * sin_t;\n109:        float y3 = cy + w * 0.5 * cos_t - h * 0.5 * sin_t;\n110:        float x4 = cx + w * 0.5 * cos_t - h * 0.5 * sin_t;\n111:        float y4 = cy + w * 0.5 * cos_t + h * 0.5 * sin_t;\n112:        float bbox_area = area * conf;\n113:        vector<float> bbox = {x1, y1, x2, y2, x3, y3, x4, y4, conf};\n114:        bboxes.push_back(bbox);\n115:     }\n116: \n117:     // nms\n118:     vector<vector", "label": "(18, CUDA, checkRuntime(cudaStreamCreate(&stream));); (80, CUDA, checkRuntime(cudaMemcpyAsync(output_data_host, output_data_device, sizeof(float) * output_numel, cudaMemcpyDeviceToHost, stream));); (80, CUDA, checkRuntime(cudaStreamSynchronize(stream));); (58, CUDA, checkRuntime(cudaMemcpyAsync(input_data_device, input_data_host, input_numel * sizeof(float), cudaMemcpyHostToDevice, stream));); (24, CUDA, checkRuntime(cudaMallocHost(&input_data_host, input_numel * sizeof(float)));); (24, CUDA, checkRuntime(cudaMalloc(&input_data_device, input_numel * sizeof(float)));); (191, CUDA, checkRuntime(cudaFree(input_data_device));); (188, CUDA, checkRuntime(cudaFreeHost(input_data_host));); (190, CUDA, checkRuntime(cudaFree(output_data_device));); (63, CUDA, checkRuntime(cudaMalloc(&output_data_device, sizeof(float) * output_numel));); (-5, CUDA, auto engine = make_nvshared(runtime->deserializeCudaEngine(engine_data.data(), engine_data.size()));); (60, CUDA, checkRuntime(cudaMallocHost(&output_data_host, sizeof(float) * output_numel));); (184, CUDA, checkRuntime(cudaFreeHost(output_data_host));); (181, CUDA, checkRuntime(cudaStreamDestroy(stream));)"}
{"predict": "(12, MPI, MPI_Type_commit(&newtype);)", "label": "(17, MPI, MPI_Sendrecv(b, 1, newtype, 0, 0, d, 0, newtype, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (10, MPI, MPI_Type_commit(&newtype);); (8, MPI, MPI_Type_create_hvector(0, 1, 10, MPI_DOUBLE, &newtype);); (9, MPI, MPI_Type_free(&newtype);); (10, MPI, MPI_Type_indexed(0, b, d, MPI_DOUBLE, &newtype);)"}
{"predict": "(10, OpenMP, #pragma omp parallel for)\n(27, OpenMP, #pragma omp parallel for)", "label": "(9, CUDA, thrust::device_vector<std::uint8_t> data_source(elements);); (12, CUDA, thrust::sequence(data_source.begin(), data_source.end());); (12, CUDA, thrust::shuffle(data_source.begin(), data_source.end(), re);); (5, CUDA, thrust::default_random_engine re;); (15, CUDA, thrust::device_vector<ValueT> in_values(data_source);); (17, CUDA, thrust::host_vector<KeyT> host_keys(in_keys);); (17, CUDA, thrust::host_vector<ValueT> host_values(in_values);); (13, CUDA, thrust::device_vector<ValueT> out_values(elements);); (8, CUDA, thrust::device_vector<KeyT> out_keys(elements);); (6, CUDA, thrust::device_vector<KeyT> in_keys(data_source);)"}
{"predict": "(14, _OPENMP, #pragma omp parallel for reduction(+:cracked[:count]))", "label": "(13, OpenMP, #pragma omp parallel for); (72, OpenMP, #pragma omp atomic); (43, OpenMP, #pragma omp critical)"}
{"predict": "(115, OpenMP, #pragma omp parallel for)", "label": "(84, OpenCL, sub_buf = clCreateSubBuffer(buf[1], 0, CL_BUFFER_CREATE_TYPE_REGION, &region, &error );); (87, OpenCL, void * map_ptr = clEnqueueMapBuffer(queue, buf[0], 1, CL_MAP_READ, 0, 64, 0, NULL, NULL, NULL);)"}
{"predict": "(12, OpenMP, #pragma omp parallel if (omp_get_thread_num() == 0)) (13, OpenMP, #pragma omp critical) (19, OpenMP, #pragma omp critical)", "label": "(1, OpenMP, #pragma omp parallel num_threads(2)); (10, OpenMP, #pragma omp cancellation point parallel); (3, OpenMP, #pragma omp cancel parallel)"}
{"predict": "(20, OpenMP, t1 = omp_get_wtime();) (25, OpenMP, t2 = omp_get_wtime();)", "label": "(4, OpenMP, omp_set_default_device(0);); (33, OpenMP, #pragma omp target teams distribute parallel for is_device_ptr(DATA) nowait); (15, OpenMP, #pragma omp target teams distribute parallel for is_device_ptr(DATA)); (35, OpenMP, #pragma omp taskwait); (19, OpenMP, t1=omp_get_wtime();); (4, OpenMP, #pragma omp parallel for); (-1, OpenMP, DATA = (double*)   omp_target_alloc(N*sizeof(double), omp_get_default_device()) ;); (22, OpenMP, t2=omp_get_wtime();)"}
{"predict": "(29, CUDA, atomicAdd(pi, sum[tid]);)", "label": "(8, CUDA, __shared__ double sum[block_size];)"}
{"predict": "(11, CUDA, maskedScaledSoftmaxKernelSmall<T, blockSize><<<grid, blockSize, 0, stream>>>(ld, rsqrtHeadSize, maskIdx, input, output);) (16, CUDA, maskedScaledSoftmaxKernelSmall<T, blockSize><<<grid, blockSize, 0, stream>>>(ld, rsqrtHeadSize, maskIdx, input, output);) (23, CUDA, maskedSoftmax<T, blockSize, VPT><<<grid, blockSize, 0, stream>>>(rsqrtHeadSize, input, output, maskIdx);) (29, CUDA, maskedSoftmax<T, blockSize, VPT><<<grid, blockSize, 0, stream>>>(rsqrtHeadSize, input, output, maskIdx);) (33, CUDA, maskedSoftmax<T, blockSize, VPT><<<grid, blockSize, 0, stream>>>(rsqrtHeadSize, input, output, maskIdx);) (38, CUDA, maskedSoftmax<T, blockSize, VPT><<<grid, blockSize, 0, stream>>>(rsqrtHeadSize, input, output, maskIdx);) (43, CUDA, maskedSoftmax<T, blockSize, VPT><<<grid, blockSize, 0, stream>>>(rsqrtHeadSize, input, output, maskIdx);) (48, CUDA, maskedSoftmax<T, blockSize, VPT><<<grid, blockSize, 0, stream>>>(rsqrtHeadSize, input, output, maskIdx);) (52, CUDA, maskedScaledSoftmaxKernel<T, blockSize><<<grid, blockSize, 0, stream>>>(ld, rsqrtHeadSize, maskIdx, input, output);)", "label": "(62, CUDA, CHECK(cudaPeekAtLastError());)"}
{"predict": "(23, CUDA, cudaMalloc(&d_inPtr, size * sizeof(float));) (24, CUDA, cudaMalloc(&d_outPtr, size * sizeof(float));) (32, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);) (33, CUDA, run_benchmark(atomic_reduction, d_outPtr, d_inPtr, size);) (34, CUDA, cudaMemcpy(h_inPtr, d_outPtr, size * sizeof(float), cudaMemcpyDeviceToHost);) (37, CUDA, cudaFree(d_inPtr);) (38, CUDA, cudaFree(d_outPtr);)", "label": "(42, CUDA, cudaFree(d_inPtr);); (27, CUDA, cudaMalloc((void**)&d_outPtr, size * sizeof(float));); (32, CUDA, cudaMemcpy(&result_gpu, &d_outPtr[0], sizeof(float), cudaMemcpyDeviceToHost);); (27, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);); (37, CUDA, cudaFree(d_outPtr);); (22, CUDA, cudaMalloc((void**)& d_inPtr, size * sizeof(float));)"}
{"predict": "(12, CUDA, cudaStream_t stream; cudaStreamCreate(&stream);) (15, CUDA, cudaStreamDestroy(stream);) (30, CUDA, checkCudaErrors(cudaStreamSynchronize(stream));) (52, CUDA, checkCudaErrors(cudaStreamSynchronize(stream));) (57, CUDA, checkCudaErrors(cudaStreamSynchronize(stream));) (60, CUDA, checkCudaErrors(cudaStreamSynchronize(stream));) (70, CUDA, checkCudaErrors(cudaStreamSynchronize(stream));) (83, CUDA, for (i = 0; i < procCount; i++) { checkCudaErrors(cudaStreamDestroy((cudaStream_t)ptrs[i])); })", "label": "(21, CUDA, checkCudaErrors(cudaSetDevice(shm->devices[id]));); (54, CUDA, checkCudaErrors(cudaStreamWaitEvent(stream, events[bufferId], 0));); (71, CUDA, checkCudaErrors(cudaStreamWaitEvent(stream, events[id], 0));); (56, CUDA, checkCudaErrors(cudaGetLastError());); (92, CUDA, checkCudaErrors(cudaStreamDestroy(stream));); (18, CUDA, checkCudaErrors(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));); (86, CUDA, checkCudaErrors(cudaIpcCloseMemHandle(ptrs[i]));); (86, CUDA, checkCudaErrors(cudaEventDestroy(events[i]));); (53, CUDA, checkCudaErrors(cudaEventRecord(events[bufferId], stream));); (68, CUDA, checkCudaErrors(cudaStreamSynchronize(stream));); (12, CUDA, checkCudaErrors(cudaGetDeviceProperties(&prop, shm->devices[id]));)"}
{"predict": "(2, OpenCL, auto buffer = clCreateBuffer(context.get(), flags, poolAllocator->params.smallBufferThreshold, hostPtr, &retVal);); (24, OpenCL, retVal = clReleaseMemObject(buffer);)", "label": "(21, OpenCL, retVal = clReleaseMemObject(smallBuffer);); (5, OpenCL, cl_mem smallBuffer = clCreateBuffer(clContext, flags, size, hostPtr, &retVal);); (24, OpenCL, EXPECT_EQ(clReleaseContext(context), CL_SUCCESS);)"}
{"predict": "(17, CUDA, cudaDeviceSynchronize();)", "label": "(13, CUDA, cudaCheck(cudaGetLastError());)"}
{"predict": "7: #endif\n118: \n119:   /* free the buffers */\n120:   if (buf_send) {\n121:     shuffile_align_free(buf_send);\n122:   }\n123:   if (buf_recv) {\n124:     shuffile_align_free(buf_recv);\n125:   }\n126: \n127:   return rc;\n128: }", "label": "(82, MPI, MPI_Wait(&request[1], &status[1]);); (90, MPI, MPI_Wait(&request[0], &status[0]);); (90, MPI, MPI_Get_count(&status[0], MPI_BYTE, &nwrite);); (66, MPI, MPI_Irecv(buf_recv, shuffile_mpi_buf_size, MPI_BYTE, rank_recv, 0, comm, &request[0]);); (77, MPI, MPI_Isend(buf_send, nread, MPI_BYTE, rank_send, 0, comm, &request[1]);)"}
{"predict": "(18, CUDA, cudaSimulation.SimulationConfig().steps = 1; cudaSimulation.setPopulationData(population); cudaSimulation.simulate();)", "label": "(16, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(22, CUDA, dim3 grid(16); dim3 block(256);)", "label": "(65, CUDA, CHECK_CUDART(cudaFree(indata));); (19, CUDA, CHECK_CUDART(cudaMemcpy(indata, &inval, sizeof(T), cudaMemcpyHostToDevice));); (61, CUDA, CHECK_CUDART(cudaMemcpy(&outval, outdata, sizeof(T), cudaMemcpyDeviceToHost));); (14, CUDA, CHECK_CUDART(cudaMalloc((void**)&indata, sizeof(T)));); (14, CUDA, CHECK_CUDART(cudaMalloc((void**)&outdata, sizeof(T)));); (59, CUDA, CHECK_CUDART(cudaFree(outdata));)"}
{"predict": "void**)&buffer, bufferSize);\n84: \t\t\tif (res != hipSuccess) return VKFFT_ERROR_FAILED_TO_ALLOCATE;\n85: \t\t\tconfiguration.buffer = (void**)&buffer;\n86: #elif(VKFFT_BACKEND==3)\n87: \t\t\tcl_mem buffer = 0;\n88: \t\t\tres = clCreateBuffer(vkGPU->context, CL_MEM_READ_WRITE | CL_MEM_COPY_HOST_PTR, bufferSize, buffer_input, &res);\n89: \t\t\tif (res != CL_SUCCESS) return VKFFT_ERROR_FAILED_TO_ALLOCATE;\n90: \t\t\tconfiguration.buffer = &buffer;\n91: #elif(VKFFT_BACKEND==4)\n92: \t\t\tze_device_mem_alloc_desc_t desc = {ZE_STRUCTURE_TYPE_DEVICE_MEM_ALLOC_DESC};\n93: \t\t\tvoid* buffer = nullptr;\n94: \t\t\tres = zeMemAllocDevice(vkGPU->context, &desc, bufferSize, 0, vkGPU->device, &buffer);\n95: \t\t\tif (res != ZE_RESULT_SUCCESS) return VKFFT_ERROR_FAILED_TO_ALLOCATE;\n96: \t\t\tconfiguration.buffer = buffer;\n97: #elif(VKFFT_BACKEND==5)\n98: \t\t\thipMalloc((void**)&buffer, bufferSize);\n99: \t\t\tconfiguration.buffer = buffer;\n100: #endif\n101: \t\t\t//Allocate buffer for the output data.\n102: \t\t\tuint64_t bufferSize = (uint64_t)sizeof(float) * 2 * configuration.size[0] * configuration.numberBatches;\n103: #if(VKFFT_BACKEND==0)\n104: \t\t\tVkBuffer buffer = {};\n105: \t\t\tVkDeviceMemory bufferDeviceMemory = {};\n106: \t\t\tresFFT = allocateBuffer(vkGPU, &buffer, &bufferDeviceMemory, VK_BUFFER_USAGE_STORAGE_BUFFER_BIT | VK_BUFFER_USAGE_TRANSFER_SRC_BIT | VK_BUFFER_USAGE_TRANSFER_DST_BIT, VK_MEMORY_HEAP_DEVICE_LOCAL_BIT, bufferSize);\n107: \t\t\tif (resFFT != VKFFT_SUCCESS) return resFFT;\n108: \t\t\tconfiguration.outputBuffer = &buffer;\n109: #elif(VKFFT_BACKEND==1)\n110: \t\t\tcuFloatComplex* buffer = 0;\n111: \t\t\tif (res != cudaSuccess) return VKFFT_ERROR_FAILED_TO_ALLOCATE;\n112: \t\t\tconfiguration.outputBuffer = (void**)&buffer;\n11", "label": "(162, OpenCL, clGetDeviceInfo(vkGPU->device, CL_DEVICE_VENDOR_ID, sizeof(cl_int), &vendorID, 0);); (77, CUDA, res = cudaMalloc((void**)&buffer, bufferSize);); (86, OpenCL, buffer = clCreateBuffer(vkGPU->context, CL_MEM_READ_WRITE, bufferSize, 0, &res);)"}
{"predict": "0; j<100; j++) foo();\n66: #pragma omp target\n67:   for (int j=0; j<100; j++) foo();\n68: #pragma omp target\n69:   for (int j=0; j<100; j++) foo();\n70: #pragma omp target\n71:   for (int j=0; j<100; j++) foo();\n72: #pragma omp target\n73:   for (int j=0; j<100; j++) foo();\n74: #pragma omp target\n75:   for (int j=0; j<100; j++) foo();\n76: #pragma omp target\n77:   for (int j=0; j<100; j++) foo();\n78: #pragma omp target\n79:   for (int j=0; j<100; j++) foo();\n80: #pragma omp target\n81:   for (int j=0; j<100; j++) foo();\n82: #pragma omp target\n83:   for (int j=0; j<100; j++) foo();\n84: #pragma omp target\n85:   for (int j=0; j<100; j++) foo();\n86: #pragma omp target\n87:   for (int j=0; j<100; j++) foo();\n88: #pragma omp target\n89:   for (int j=0; j<100; j++) foo();\n90: #pragma omp target\n91:   for (int j=0; j<100; j++) foo();\n92: #pragma omp target\n93:   for (int j=0; j<100; j++) foo();\n94: #pragma omp target\n95:   for (int j=0; j<100; j++) foo();\n96: #pragma omp target\n97:   for (int j=0; j<100; j++) foo();\n98: #pragma omp target\n99:   for (int j=0; j<100; j++) foo();\n100: #pragma omp", "label": "(113, OpenMP, #pragma omp teams distribute reduction(task, + : m)); (15, OpenMP, #pragma omp teams distribute reduction +); (29, OpenMP, #pragma omp teams distribute reduction(\\)); (37, OpenMP, #pragma omp teams distribute reduction(|| : argc > 0 ? argv[1] : argv[2])); (106, OpenMP, #pragma omp teams distribute reduction(+ : m)); (20, OpenMP, #pragma omp teams distribute reduction()); (6, OpenMP, #pragma omp target); (42, OpenMP, #pragma omp teams distribute reduction(^ : S1)); (44, OpenMP, #pragma omp teams distribute reduction(+ : a, b, c, d, f)); (67, OpenMP, #pragma omp teams distribute reduction(&& : S2::S2sc)); (57, OpenMP, #pragma omp teams distribute reduction(- : da)); (32, OpenMP, #pragma omp teams distribute reduction(~ : argc)); (10, OpenMP, #pragma omp teams distribute reduction(-); (45, OpenMP, #pragma omp teams distribute reduction(max : h.b)); (5, OpenMP, #pragma omp teams distribute reduction(); (75, OpenMP, #pragma omp parallel private(k)); (18, OpenMP, #pragma omp teams distribute reduction(foo : argc); (38, OpenMP, #pragma omp teams distribute reduction(min : a, b, c, d, f)); (70, OpenMP, #pragma omp teams distribute private(i), reduction(+ : j), reduction(+ : q)); (79, OpenMP, #pragma omp teams distribute reduction(+ : r)); (41, OpenMP, #pragma omp teams distribute reduction(+ : ba)); (25, OpenMP, #pragma omp teams distribute reduction(&& : argc)); (81, OpenMP, #pragma omp teams distribute reduction(max : j)); (78, OpenMP, #pragma omp parallel reduction(min : i)); (4, OpenMP, #pragma omp teams distribute reduction(*)); (67, OpenMP, #pragma omp teams distribute reduction(+ : p), reduction(+ : p)); (44, OpenMP, #pragma omp teams distribute reduction(^ : fl)); (58, OpenMP, #pragma omp teams distribute reduction(+ : o)); (54, OpenMP, #pragma omp teams distribute reduction(+ : h, k, B::x)); (35, OpenMP, #pragma omp teams distribute reduction(* : ca)); (76, OpenMP, #pragma omp teams distribute reduction(+ : fl, z)); (48, OpenMP, #pragma omp teams distribute reduction(& : e, g)); (68, OpenMP, #pragma omp parallel shared(i)); (4, OpenMP, #pragma omp teams distribute reduction(| : argc,); (-21, OpenMP, #pragma omp teams distribute reduction); (38, OpenMP, #pragma omp teams distribute reduction(&& : S2::S2s))"}
{"predict": "(4, CUDA, cudaGetDeviceProperties(&prop, whichDevice);)", "label": "(4, CUDA, checkCudaErrors(cudaGetDeviceProperties(&prop, whichDevice));); (2, CUDA, checkCudaErrors(cudaGetDevice(&whichDevice));)"}
{"predict": "(25, OpenMP, #pragma omp parallel for private(i, len) shared(cnt, index, dirty, saved_key, saved_nt, crypt_out, AzureAD_cur_salt))", "label": "(4, OpenMP, #pragma omp parallel for)"}
{"predict": ", fb_control);\n100:       if (crash_status[0] == 0)\n101:       {\n102:         curr_cost += fb_control[0];\n103:       }\n104:     }\n105:     __syncthreads();\n106: \n107:     // Update cost\n108:     if (t < num_timesteps)\n109:     {\n110:       costs->updateCosts(y, u, theta_c, curr_cost, dt);\n111:     }\n112:     __syncthreads();\n113:   }\n114:   /*<----End of simulation loop-----> */\n115: \n116:   // Write out the cost\n117:   if (cost_index < num_timesteps * num_rollouts)\n118:   {\n119:     trajectory_costs_d[cost_index] = curr_cost;\n120:   }\n121: }\n\nOutput:", "label": "(21, CUDA, extern __shared__ float entire_buffer[];)"}
{"predict": "(40, MPI, if (wrank == 0) { fprintf(stderr, \"Unrecognized command line argument %s\\n\", argv[i]); fprintf(stderr, \"Usage: mpivars [ -nodesc ]\\n\"); return 1; })", "label": "(29, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (24, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (55, MPI, MPI_Finalize();); (52, MPI, MPI_T_finalize();); (23, MPI, MPI_T_init_thread(required, &provided);); (36, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(13, TBB, ASSERT( vs.empty(), NULL );)", "label": "(30, TBB, tbb::parallel_reduce ( vs.range(1), pvrb );); (60, TBB, tbb::parallel_for ( tbb::blocked_range<int> (0, N, RANGE_MIN), parallel_vector_for_body<T,Allocator>( vvs, allocator_name ) );); (1, TBB, typedef std::vector<T, tbb::tbb_allocator<T> > container_type;); (5, TBB, tbb::task_scheduler_init init(p);); (10, TBB, if (Verbose && t == 0) t0 = tbb::tick_count::now();); (64, TBB, tbb::flattened2d<ets_type> fvvs = flatten2d(vvs);); (59, TBB, tbb::parallel_reduce ( vvs.range(1), pvrb2 );); (-5, TBB, tbb::tick_count t0;); (30, TBB, tbb::flattened2d<ets_type> fvs = flatten2d(vs);); (50, TBB, ets_type vvs( typename container_type::size_type(25), minus_one, tbb::tbb_allocator<T>() );); (-6, TBB, typedef tbb::enumerable_thread_specific< container_type, Allocator<container_type> > ets_type;); (66, TBB, REMARK(\"done\\nparallel %s, %d, %g, %g\\n\", test_name, p, result_value, ( tbb::tick_count::now() - t0).seconds());)"}
{"predict": "(14, OpenCL, clFinish(clContext.get());) (17, OpenCL, clFinish(clContext.get());) (30, OpenCL, clFinish(clContext.get());) (33, OpenCL, clFinish(clContext.get());) (36, OpenCL, clFinish(clContext.get());)", "label": "(1, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)"}
{"predict": "(14, OpenMP, #pragma omp parallel for reduction(+:gradients)) (26, OpenMP, #pragma omp parallel for reduction(+:gradients))", "label": "(14, OpenMP, #pragma omp parallel for)"}
{"predict": "(3, OpenCL, cl_int retVal = CL_INVALID_GL_OBJECT;); (5, OpenCL, retVal = clCreateFromGLTexture(clContext.get(), 0, GL_TEXTURE_1D, 0, textureId, &retVal);); (8, OpenCL, retVal = clReleaseMemObject(glImage);)", "label": "(12, OpenCL, status = clEnqueueNDRangeKernel(pCmdQ, mockKernel.mockMultiDeviceKernel, testedWorkDim, nullptr, globalWorkSize, localWorkSize, 0, nullptr, nullptr);)"}
{"predict": "(17, CUDA, const int BS = 512;)\n(18, CUDA, const int GS = (nthreads + BS - 1) / BS;)\n(19, CUDA, decodeBBoxes_kernel<T_BBOX, BS><<<GS, BS, 0, stream>>>(nthreads, code_type, variance_encoded_in_target, num_priors, share_location, num_loc_classes, background_label_id, clip_bbox, (const T_BBOX*) loc_data, (const T_BBOX*) prior_data, (T_BBOX*) bbox_data, batch_agnostic);)", "label": "(23, CUDA, CSC(cudaGetLastError(), STATUS_FAILURE);)"}
{"predict": "(4, OpenMP, #pragma omp parallel for private(p, cf, global_execute_count, fake_sender, receivers, c, inc_count))", "label": "(46, TBB, g.reset(tbb::flow::rf_reset_bodies);); (21, TBB, tbb::flow::make_edge( exe_node, *receivers[r] );); (7, TBB, tbb::flow::continue_node< OutputType > exe_node( g, cf );); (22, TBB, utils::NativeParallelFor( p, parallel_puts<tbb::flow::continue_msg>(exe_node) );); (36, TBB, body_copy = tbb::flow::copy_body< inc_functor<OutputType> >( exe_node );); (-1, TBB, tbb::flow::graph g;); (29, TBB, tbb::flow::remove_edge( exe_node, *receivers[r] );); (5, TBB, tbb::detail::d2::register_predecessor(exe_node, fake_sender);)"}
{"predict": "(23, MPI, MPI_Init(&argc, &argv);) (37, MPI, MPI_Finalize();) (25, MPI, MPI_Send(message.data(), numElements, MPI_INT, destination, rank + 1, MPI_COMM_WORLD);) (28, MPI, MPI_Recv(receiveBuffer.data(), numElements, MPI_INT, source, rank, MPI_COMM_WORLD, MPI_STATUS_IGNORE);) (39, MPI, MPI_Barrier(MPI_COMM_WORLD);) (42, MPI, MPI_Wtime() - t0;)", "label": "(6, MPI, MPI_Init(&argc, &argv);); (47, MPI, MPI_Finalize();); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (38, MPI, double t1 = MPI_Wtime();); (20, MPI, MPI_Barrier(MPI_COMM_WORLD);); (20, MPI, double t0 = MPI_Wtime();)"}
{"predict": "(11, OpenMP, #pragma omp parallel for reduction(+:num_tasks) reduction(+:curr_tasks))", "label": "(1, TBB, tbb::task_arena a(1);); (13, TBB, CHECK(tbb::this_task_arena::max_concurrency() == 2);)"}
{"predict": "(6, OpenMP, #pragma omp parallel for)", "label": "(31, OpenMP, #pragma omp teams distribute parallel for proc_bind(master)); (25, OpenMP, #pragma omp teams distribute parallel for proc_bind(x)); (4, OpenMP, #pragma omp teams distribute parallel for proc_bind); (34, OpenMP, #pragma omp teams distribute parallel for proc_bind(spread)); (6, OpenMP, #pragma omp teams distribute parallel for proc_bind(); (9, OpenMP, #pragma omp teams distribute parallel for proc_bind()); (16, OpenMP, #pragma omp teams distribute parallel for proc_bind(close), proc_bind(spread)); (-2, OpenMP, #pragma omp target); (27, OpenMP, #pragma omp parallel proc_bind(close)); (9, OpenMP, #pragma omp teams distribute parallel for proc_bind(master)"}
{"predict": "(35, OpenMP, #pragma omp parallel for private(res_dev, res_host))", "label": "(42, CUDA, cudaFree(res_dev);); (35, CUDA, cudaMalloc(&res_dev, sizeof(float));); (37, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(29, CUDA, __syncthreads();)", "label": "(11, CUDA, extern __shared__ __align__(sizeof(double)) unsigned char buf_[];)"}
{"predict": "_coll = 0, num_skipped = 0, num_failed = 0;\n134:         for (auto s : test->results) {\n135:             int coll_num = ucc_ilog2(std::get<0>(s));\n136:             num_all += total_done_skipped_failed[coll_num][0];\n137:             num_coll += total_done_skipped_failed[coll_num][1];\n138:             num_skipped += total_done_skipped_failed[coll_num][2];\n139:             num_failed += total_done_skipped_failed[coll_num][3];\n140:         }\n141:         std::cout << \"Total: \" << num_all << \" Coll: \" << num_coll\n142:                   << \" Skipped: \" << num_skipped << \" Failed: \" << num_failed\n143:                   << std::endl;\n144:         std::cout << \"Time: \" << std::chrono::duration_cast<std::chrono::seconds>(end - begin).count()\n145:                   << std::endl;\n146:     }\n147: \n148:     MPI_Wait(&req, MPI_STATUS_IGNORE);\n149:     MPI_Finalize();\n150:     delete test;\n151:     return failed;\n152: }", "label": "(21, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (193, MPI, MPI_Finalize();); (25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (23, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (124, MPI, MPI_Test(&req, &completed, MPI_STATUS_IGNORE);)"}
{"predict": "(4, MPI, MpiaIsInitializedOrExit();)", "label": "(40, MPI, mpi_errno = Mpi.Finalize();); (86, MPI, Mpi.CompleteFinalize();)"}
{"predict": "(28, MPI, if (verbose) { fprintf(stdout, \"\\n\"); }) (32, MPI, if (verbose) { fprintf(stdout, \"\\n\"); })", "label": "(44, MPI, MPI_T_finalize();); (19, MPI, MPI_T_init_thread(required, &provided);)"}
{"predict": "rank, but 1 for other ranks. So, we need to add the\n94:          * difference in the count sizes to my_count_size to get the total\n95:          * number of bytes that need to be written.\n96:          */\n97:         ADIOI_Fill_st_offsets(st_offsets, end_offsets, offset_list, len_list,\n98:                               &my_count_size, &contig_access_count);\n99: \n100:         /* the size of the data to be written is the sum of the sizes of\n101:          * the data on each process. */\n102:         fd_size = my_count_size;\n103:         min_st_offset = st_offsets[0];\n104: \n105:         /* Now, calculate the count sizes for each process. */\n106:         count_sizes = (ADIO_Offset *) ADIOI_Malloc(nprocs * sizeof(ADIO_Offset));\n107:         ADIOI_Calc_count_sizes(st_offsets, end_offsets, count_sizes, nprocs);\n108: \n109:         /* Now, allocate space for the global offsets and gather them. */\n110:         gpfs_offsets0 = (ADIO_Offset *) ADIOI_Malloc(nprocs * sizeof(ADIO_Offset));\n111:         ADIOI_Gather_offsets(st_offsets, gpfs_offsets0, nprocs);\n112: \n113:         /* calculate the global offsets */\n114:         ADIOI_Calc_global_offs(gpfs_offsets0, st_offsets, end_offsets, count_sizes,\n115:                                nprocs, &min_st_offset);\n116: \n117:         /* calculate the number of processes that will actually participate\n118:          * in the I/O, nprocs_for_coll. This is stored in the hints off the\n119:          * ADIO_File structure. */\n120:         ADIOI_Check_cb_nodes(fd, nprocs, nprocs_for_coll);\n121: \n122:         /* calculate the number of processes that will actually participate\n123:          * in the I/O, nprocs_for_coll. This is stored in the hints off the\n124:          * ADIO_File structure. */\n125:         ADIOI_Check_cb_nodes(fd, nprocs, n", "label": "(141, MPI, MPI_Allgather(&end_offset, 1, ADIO_OFFSET, end_offsets, 1, ADIO_OFFSET, fd->comm);); (59, MPI, MPI_Comm_rank(fd->comm, &myrank);); (138, MPI, MPI_Allgather(&start_offset, 1, ADIO_OFFSET, st_offsets, 1, ADIO_OFFSET, fd->comm);); (395, MPI, MPI_Type_size_x(datatype, &size);); (94, MPI, MPI_Type_size_x(datatype, &buftype_size);)"}
{"predict": "(13, OpenMP, #pragma omp parallel for)", "label": "(41, OpenMP, lis2mdl_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)"}
{"predict": "(20, (\"GOMP_task: T#%d, task %p\\n\", gtid, task));\n119: \n120:   __kmp_task_free(task);\n121: }\n\nAssistant:\n(13, OpenMP, #pragma omp task);", "label": "(67, OpenMP, kmp_gomp_depends_info_t gomp_depends(depend);); (100, OpenMP, __kmpc_omp_wait_deps(&loc, gtid, ndeps, dep_list, 0, NULL);); (74, OpenMP, __kmpc_omp_task(&loc, gtid, task);); (71, OpenMP, __kmpc_omp_task_with_deps(&loc, gtid, task, ndeps_cnv, dep_list, 0, NULL);)"}
{"predict": "(12, MPI, c_err = OMPI_Progress(OMPI_FINT_2_INT(*count), c_comm);)", "label": "(10, MPI, MPI_Comm c_comm = MPI_Comm_f2c(*comm);)"}
{"predict": "3532453074774e-06,  1.1507473269884770e-08 };\n112:             const double Mhu_b[] = { 0.0000000000000000e+00,  0.0000000000000000e+00,  0.0000000000000000e+00,  0.0000000000000000e+00,  0.0000000000000000e+00,  0.0000000000000000e+00,  0.0000000000000000e+00,  0.0000000000000000e+00,  0.0000000000000000e+00,  0.0000000000000000e+00 };\n113: \n114:             sum1 = 0.0;\n115:             sum2 = 0.0;\n116: \n117:             for (int mm=0; mm<Mhu_l; mm++)\n118:             {\n119:                 sum1 += Mhu_a[mm] * exp(-Mhu_b[mm] * (x - 0.0));\n120:             }\n121: \n122:             for (int mm=0; mm<Mhu_m; mm++)\n123:             {\n124:                 sum2 += Mhu_a[mm] * exp(-Mhu_b[mm] * (x - 0.0));\n125:             }\n126: \n127:             m_gate[ii] = sum1 - sum2;\n128:         }\n129:         \n130:         /*\n13", "label": "(162, OpenMP, #pragma omp target exit data map(delete:Vm[:nCells])); (66, OpenMP, #pragma omp target enter data map(to: m_gate[:nCells])); (149, OpenMP, #pragma omp target update from (m_gate[0:nCells])); (65, OpenMP, #pragma omp target enter data map(to: Vm[:nCells])); (102, OpenMP, #pragma omp target teams distribute parallel for); (156, OpenMP, #pragma omp target exit data map(delete:m_gate[:nCells]))"}
{"predict": "(7, MPI, MPI_Attr_get(type, key[i], &val_p, &flag);)", "label": "(6, MPI, MPI_Type_get_attr(type, key[i], &val_p, &flag);)"}
{"predict": "(8, OpenMP, #pragma omp parallel for private(local_mbox, serial))", "label": "(7, OpenMP, #pragma omp for nowait schedule(runtime))"}
{"predict": "(34, MPI, MPI_Reduce(&err, &errs, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);)", "label": "(9, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(12, CUDA, if (block.thread_index().x < stride) { s_data[block.thread_index().x] += s_data[block.thread_index().x + stride]; }); (27, CUDA, if (block.thread_index().x < stride) { s_data[block.thread_index().x] += s_data[block.thread_index().x + stride]; });", "label": "(6, CUDA, extern __shared__ float s_data[];)"}
{"predict": "(10, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(10, MPI, MPI_Info_create(&infos[i]);); (57, MPI, MPI_Info_get(infos[i], key, sizeof(value), value, &flag);); (47, MPI, MPI_Info_get_nthkey(infos[i], j, key);); (22, MPI, MPI_Info_get_nkeys(infos[0], &nkeys);); (33, MPI, MPI_Info_get_nkeys(infos[i], &nkeys);); (71, MPI, MPI_Info_free(&infos[i]);)"}
{"predict": "(7, OpenMP, #pragma omp parallel for private(index, ctx))", "label": "(5, OpenMP, #pragma omp parallel for)"}
{"predict": "(12, MPI, MPI_Isend(sendbuf, count, datatype, dest, tag, MPI_COMM_WORLD, &req);)", "label": "(14, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (12, MPI, MPI_Ireduce_scatter_block_c(sendbuf, recvbuf, count, datatype, MPI_SUM, comm, &req);); (8, MPI, MPI_Reduce_scatter_block_c(sendbuf, recvbuf, count, datatype, MPI_SUM, comm);)"}
{"predict": "i < nGPUs; i++) {\n119:     cudaDeviceProp deviceProp;\n120:     cudaGetDeviceProperties(&deviceProp, whichGPUs[i]);\n121:     printf(\"GPU Device %d: \\\"%s\\\" with compute capability %d.%d\\n\", i,\n122:            deviceProp.name, deviceProp.major, deviceProp.minor);\n123:   }\n124: \n125:   // Create plan for the input signal\n126:   result = cufftPlan1d(&plan_input, new_size, CUFFT_Z2Z, 1);\n127:   if (result != CUFFT_SUCCESS) {\n128:     printf(\"cufftPlan1d failed\\n\");\n129:     exit(EXIT_FAILURE);\n130:   }\n131: \n132:   // Allocate device memory for the input signal\n133:   cufftComplex *d_signal;\n134:   cudaMalloc((void **)&d_signal, sizeof(cufftComplex) * new_size);\n135: \n136:   // Copy the input signal to the device\n137:   cudaMemcpy(d_signal, h_signal, sizeof(Complex) * SIGNAL_SIZE,\n138:              cudaMemcpyHostToDevice);\n139: \n140:   // Allocate device memory for the output signal\n141:   cufftComplex *d_signal_out;\n142:   cudaMalloc((void **)&d_signal_out, sizeof(cufftComplex) * new_size);\n143: \n144:   // Perform the FFT on the input signal\n145:   cufftExecZ2Z(plan_input, d_signal, d_signal_out, CUFFT_FORWARD);\n146: \n147:   // Allocate device memory for the filter kernel\n148:   cufftComplex *d_filter_kernel;\n149:   cudaMalloc((void **)&d_filter_kernel, sizeof(cufftComplex) * FILTER_KERNEL_SIZE);\n150: \n151:   // Copy the filter kernel to the device\n152:   cudaMemcpy(d_filter_kernel, h_filter_kernel,\n153:              sizeof(Complex) * FILTER_KERNEL_SIZE, cudaMemcpyHostToDevice);\n154: \n155:   // Perform the FFT on the filter kernel\n1", "label": "(29, CUDA, checkCudaErrors(cudaGetDeviceCount(&GPU_N));); (219, CUDA, checkCudaErrors(cufftXtFree(d_out_filter_kernel));); (120, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, whichGPUs[i]));); (216, CUDA, checkCudaErrors(cufftXtFree(d_out_signal));); (100, CUDA, checkCudaErrors(cufftCreate(&plan_input));); (40, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, i));); (211, CUDA, checkCudaErrors(cufftXtFree(d_signal));); (216, CUDA, checkCudaErrors(cufftDestroy(plan_input));); (210, CUDA, checkCudaErrors(cufftXtFree(d_filter_kernel));)"}
{"predict": "(14, MPI, MTest_Init(&argc, &argv);)\n(17, MPI, comm = MPI_COMM_WORLD;)\n(20, MPI, blksize = 2048;)\n(21, MPI, counts[0] = blksize;)\n(22, MPI, counts[1] = blksize;)\n(23, MPI, displs[0] = 0;)\n(24, MPI, displs[1] = blksize + 1;)\n(25, MPI, totsize = 2 * blksize;)\n(26, MPI, source = 0;)\n(27, MPI, dest = size - 1;)\n(29, MPI, recvBuf = (int *) malloc(totsize * sizeof(int));)\n(30, MPI, srcBuf = (int *) malloc((totsize + 1) * sizeof(int));)\n(32, MPI, for (i = 0; i < totsize; i++) {)\n(33, MPI, recvBuf[i] = -1;)\n(34, MPI, for (i = 0; i < blksize; i++) {)\n(35, MPI, srcBuf[i] = i;)\n(36, MPI, srcBuf[blksize + 1 + i] = blksize + i;)\n(37, MPI, srcBuf[blksize] = -1;)\n(39, MPI, if (rank == source) {)\n(40, MPI, errs += CheckMPIErr(err);)\n(41, MPI, errs += CheckMPIErr(err);)\n(42, MPI, } else if (rank == dest) {)\n(43, MPI, MPI_Win_fence(0, win);)\n(44, MPI, for (i = 0; i < totsize; i++) {)\n(45, MPI, if (recvBuf[i] != i) {)\n(46, MPI, errs++;)\n(47, MPI, if (errs < 10) {)\n(48, MPI, printf(\"recvBuf[%d] = %d should = %d\\n\", i, recvBuf[i], i);)\n(49, MPI, } else {)\n(50, MPI, } else {)\n(51, MPI, MPI_Win_fence(", "label": "(56, MPI, MPI_Type_get_extent(MPI_INT, &tmp_lb, &extent);); (33, MPI, MPI_Comm_size(comm, &size);); (55, MPI, MPI_Win_create(recvBuf, totsize * extent, extent, MPI_INFO_NULL, comm, &win);); (24, MPI, MPI_Type_indexed(2, counts, displs, MPI_INT, &originType);); (54, MPI, MPI_Win_fence(0, win);); (23, MPI, MPI_Type_commit(&originType);); (77, MPI, MPI_Win_free(&win);); (75, MPI, MPI_Type_free(&originType);); (58, MPI, err = MPI_Win_fence(0, win);); (53, MPI, MPI_Win_set_errhandler(win, MPI_ERRORS_RETURN);); (33, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (22, MPI, MPI_Comm_rank(comm, &rank);); (52, MPI, err = MPI_Put(srcBuf, 1, originType, dest, 0, totsize, MPI_INT, win);)"}
{"predict": "(9, OpenMP, #pragma omp parallel for)\n(13, OpenMP, #pragma omp parallel for)", "label": "(8, OpenMP, #pragma omp task private(); (17, OpenMP, #pragma omp task private(ca)); (20, OpenMP, #pragma omp task private(threadvar, B::x)); (10, OpenMP, #pragma omp task private(argc argv)); (17, OpenMP, #pragma omp task private(e, g)); (31, OpenMP, #pragma omp task private(m)); (1, OpenMP, #pragma omp task private); (9, OpenMP, #pragma omp task private(argv[1])); (1, OpenMP, #pragma omp task private()); (19, OpenMP, #pragma omp task private(j)); (4, OpenMP, #pragma omp task private(S1)); (0, OpenMP, #pragma omp task private(argc,); (5, OpenMP, #pragma omp task private(ba) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (7, OpenMP, #pragma omp task private(S2::S2s)); (5, OpenMP, #pragma omp task private(da)); (-3, OpenMP, #pragma omp task private(argc > 0 ? argv[1] : argv[2])); (11, OpenMP, #pragma omp task private(i)); (6, OpenMP, #pragma omp task shared(i), private(i)); (14, OpenMP, #pragma omp task private(i) allocate(omp_thread_mem_alloc: i)); (6, OpenMP, #pragma omp task firstprivate(i) private(i)); (10, OpenMP, #pragma omp task firstprivate(i)); (-11, OpenMP, #pragma omp task private(argc); (-7, OpenMP, #pragma omp task private(z, a, b, c, d, f))"}
{"predict": "(14, OpenMP, #pragma omp parallel for private(d_a))", "label": "(25, OpenMP, omp_target_free(d_a, omp_get_default_device());); (12, OpenMP, d_a = (int *)omp_target_alloc(DataSize, omp_get_default_device());); (18, OpenMP, #pragma omp target update to(p_a)); (20, OpenMP, omp_target_disassociate_ptr(p_a, omp_get_default_device());); (11, OpenMP, omp_target_associate_ptr(p_a, d_a, DataSize, 0, omp_get_default_device());)"}
{"predict": "(32, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(35, MPI, err = MPI_Type_create_indexed_block(count, 1, disp, vectype, &newtype);); (42, MPI, MPI_Type_size(MPI_INT, &int_size);); (58, MPI, MPI_Type_commit(&newtype);); (24, MPI, err = MPI_Type_vector(2, 1, 2, MPI_INT, &vectype);); (41, MPI, err = MPI_Type_size(newtype, &size);); (74, MPI, MPI_Type_free(&newtype);); (72, MPI, MPI_Type_free(&vectype);)"}
{"predict": "(12, OpenCL, cl_event eventWaitList[] = {event1};) (13, OpenCL, int sizeOfWaitList = sizeof(eventWaitList) / sizeof(cl_event);) (14, OpenCL, cl_event retEvent;) (28, OpenCL, retVal = callOneWorkItemNDRKernel(eventWaitList, sizeOfWaitList, &retEvent);) (32, OpenCL, retVal = clSetEventCallback(event1, CL_COMPLETE, E2Clb::signalEv2, &callbackCalled);) (37, OpenCL, event1->setStatus(CL_COMPLETE);) (42, OpenCL, Event::waitForEvents(1, &retEvent);)", "label": "(44, OpenCL, clReleaseEvent(retEvent);)"}
{"predict": "(13, OpenMP, #pragma omp parallel for)", "label": "(24, CUDA, ASSERT_EQ(cudaEventRecord(wm.ready, 0), cudaSuccess);); (25, CUDA, cvcuda::WorkspaceMemAllocator wa(wm, cudaStream_t(0));); (51, CUDA, EXPECT_EQ(cudaEventQuery(wm.ready), cudaSuccess);); (1, CUDA, ASSERT_EQ(cudaMalloc(&_junk, junk_size), cudaSuccess);); (14, CUDA, ASSERT_EQ(cudaMemset(junk.get(), i, junk_size), cudaSuccess);); (6, CUDA, ASSERT_EQ(cudaEventCreateWithFlags(&wm.ready, cudaEventDisableTiming), cudaSuccess);); (26, CUDA, ASSERT_EQ(cudaDeviceSynchronize(), cudaSuccess);); (50, CUDA, ASSERT_EQ(cudaEventDestroy(wm.ready), cudaSuccess);); (19, CUDA, EXPECT_EQ(cudaEventQuery(wm.ready), cudaErrorNotReady);)"}
{"predict": "(60, CUDA, CHECK_CUDA(cudaMalloc(&dx, dataSize * sizeof(int32_t)));) (61, CUDA, CHECK_CUDA(cudaMalloc(&dIndices, indicesSize * sizeof(int32_t)));) (62, CUDA, CHECK_CUDA(cudaMalloc(&dUpdates, updatesSize * sizeof(int32_t)));) (63, CUDA, CHECK_CUDA(cudaMalloc(&dy, dataSize * sizeof(int32_t)));) (76, CUDA, CHECK_CUDA(cudaFree(dy));) (77, CUDA, CHECK_CUDA(cudaFree(dUpdates));) (78, CUDA, CHECK_CUDA(cudaFree(dIndices));) (79, CUDA, CHECK_CUDA(cudaFree(dx));)", "label": "(65, CUDA, CHECK_CUDA(cudaMalloc(&dy, dataSize * sizeof(int32_t)));); (100, CUDA, CHECK_CUDA(cudaFree(dUpdates));); (62, CUDA, CHECK_CUDA(cudaMalloc(&dUpdates, updatesSize * sizeof(int32_t)));); (94, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (37, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (97, CUDA, CHECK_CUDA(cudaFree(dy));); (56, CUDA, CHECK_CUDA(cudaMalloc(&dx, dataSize * sizeof(int32_t)));); (93, CUDA, CHECK_CUDA(cudaFree(dIndices));); (36, CUDA, CHECK_HIEDNN(hiednnSetCudaStream(handle, 0));); (90, CUDA, CHECK_CUDA(cudaFree(dx));); (53, CUDA, CHECK_CUDA(cudaMalloc(&dIndices, indicesSize * sizeof(int32_t)));)"}
{"predict": "(17, OpenMP, #pragma omp parallel for private(i))", "label": "(28, TBB, tbb::flow::input_node<T> src2(g, my_input_body<T>() );); (18, TBB, tbb::flow::function_node<T,bool> dest3(g, tbb::flow::unlimited, b3 );); (31, TBB, tbb::flow::function_node<T,bool,tbb::flow::rejecting> dest2(g, tbb::flow::serial, b2 );); (3, TBB, tbb::flow::make_edge( src, dest );); (0, TBB, tbb::flow::input_node<T> src(g, my_input_body<T>() );); (29, TBB, tbb::flow::make_edge( src2, dest2 );); (36, TBB, tbb::flow::input_node<T> src_copy(src);); (13, TBB, tbb::flow::make_edge( src3, dest3 );); (-5, TBB, tbb::flow::graph g;); (6, TBB, tbb::flow::input_node<T> src3(g, my_input_body<T>() );)"}
{"predict": "(22, CUDA, cudaMallocManaged(&input_data, input_size_bytes);) (26, CUDA, cudaMallocManaged(&output_data, output_size_bytes);) (39, CUDA, block_fft_kernel_r2c<FFT><<<1, FFT::block_dim, FFT::shared_memory_size, 0>>>(input_data, output_data);)", "label": "(26, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&output_data, output_size_bytes));); (18, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&input_data, input_size_bytes));); (48, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(output_data));); (39, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (37, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (44, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(input_data));)"}
{"predict": "_bc_offset,Lnzval_bc_dat,Lnzval_bc_offset,Linv_bc_dat,Linv_bc_offset,Lindval_loc_bc_dat,Lindval_loc_bc_offset, xsup,bcols_masked, grid);\n103:     //}else{\n104:         if (mype == 0) {\n105:             slsum_fmod_inv_gpu_mrhs<<< dimGrid_bc, dimBlock_bc >>>(nbcol_loc, nblock_ex, lsum, x, nrhs, maxsup, nsupers, fmod, LBtree_ptr, LRtree_ptr, ilsum, Lrowind_bc_dat, Lrowind_bc_offset, Lnzval_bc_dat, Lnzval_bc_offset, Linv_bc_dat, Linv_bc_offset, Lindval_loc_bc_dat, Lindval_loc_bc_offset, xsup, bcols_masked, grid);\n106:         } else {\n107:             slsum_fmod_inv_gpu_1rhs_warp<<< dimGrid, dimBlock >>>(nbcol_loc, nblock_ex, lsum, x, nrhs, maxsup, nsupers, fmod, LBtree_ptr, LRtree_ptr, ilsum, Lrowind_bc_dat, Lrowind_bc_offset, Lnzval_bc_dat, Lnzval_bc_offset, Linv_bc_dat, Linv_bc_offset, Lindval_loc_bc_dat, Lindval_loc_bc_offset, xsup, bcols_masked, grid);\n108:         }\n109:         checkGPU(gpuGetLastError());\n110:     }\n111: \n112:     checkGPU(gpuGetLastError());\n113: }", "label": "(85, CUDA, cudaDeviceSetLimit(cudaLimitStackSize, cuattr.localSizeBytes);); (140, CUDA, CUDA_CHECK(cudaDeviceSynchronize());); (78, CUDA, cudaStreamCreateWithFlags(&stream[i], cudaStreamNonBlocking);); (135, CUDA, CUDA_CHECK(cudaGetLastError());); (80, CUDA, cudaFuncGetAttributes(&cuattr, slsum_fmod_inv_gpu_mrhs_nvshmem);); (138, CUDA, CUDA_CHECK(cudaStreamDestroy(stream[i]));); (82, CUDA, cudaOccupancyMaxPotentialBlockSize(&minGridSize,&myblockSize,(const void *) swait_bcrd ,0,0 );)"}
{"predict": "(1, CUDA, __global__ void radix_warp_sort()){)", "label": "(2, CUDA, __shared__ volatile unsigned int s_data[WARP_SIZE*2];)"}
{"predict": "(32, OpenCL, error = clEnqueueNDRangeKernel(queue, kernel, 1, nullptr, &global_size, nullptr, 0, nullptr, nullptr);)", "label": "(29, OpenCL, error |= clSetKernelArg(kernel, 1, sizeof(value), &value);); (27, OpenCL, error |= clSetKernelArg(kernel, 0, sizeof(dst), &dst);)"}
{"predict": "0, this sets the maximum buffer size in bytes for \"\n131:             \"the output file. If <= 0, no buffer is used.\")\n132:         .count(1)\n133:         .action([](parser_t& p) { buffer_size() = p.get<int>(\"buffer-size\"); });\n134:     parser\n135:         .add_argument({ \"--output-file\" }, \"Set the output file for timemory\")\n136:         .count(1)\n137:         .action([](parser_t& p) { output_file() = p.get<std::string>(\"output-file\"); });\n138:     parser\n139:         .add_argument({ \"--profile\", \"--perf\" }, \"Enable performance profiling\")\n140:         .count(0)\n141:         .action([](parser_t&) { tim::settings::profile() = true; });\n142:     parser\n143:         .add_argument({ \"--timemory\", \"--tm\" }, \"Enable timemory\")\n144:         .count(0)\n145:         .action([](parser_t&) { tim::settings::enabled() = true; });\n146:     parser\n147:         .add_argument({ \"--no-timemory\", \"--notm\" }, \"Disable timemory\")\n148:         .count(0)\n149:         .action([](parser_t&) { tim::settings::enabled() = false; });\n150:     parser\n151:         .add_argument({ \"--no-sampling\", \"--nosample\" }, \"Disable sampling\")\n152:         .count(0)\n153:         .action([](parser_t&) { use_sample() = false; });\n154:     parser\n155:         .add_argument({ \"--no-signal\", \"--nosig\" }, \"Disable signal handling\")\n156:         .count(0)\n157:         .action([](parser_t&) { tim::disable_signal_detection(); });\n158:     parser\n159:         .add_argument({ \"--signal\" }, \"Enable signal handling\")\n160:         .count(0)\n161:         .action([](parser_t&) { tim::enable_signal_detection(); });\n162:     parser\n163", "label": "(8, MPI, tim::settings::mpi_init()       = false;); (8, MPI, tim::settings::mpi_finalize()   = false;)"}
{"predict": "func = measureExclusiveN;\n130:       testName = \"Lock Exclusivity\";\n131:       unit = '%';\n132:     }\n133:     runExclusive(locks, func, stats, nThreads, nLocks, testName, unit);\n134:     printf(\"%s\\n\"\n135:            \"%s, %s, %d locks\\n\"\n136:            \"# %s\"\n137:            \"%s\"\n138:            \"%s\\n\",\n139:            testName, targetName.c_str(), locks[0]->name(), nLocks,\n140:            getDateTime().c_str(),\n141:            hintsIgnored ? \"# BEWARE lock hints ignored\\n\" : \"\",\n142:            argv[1][0] == 'C' ? \"# Clumping and exclusive locks are not \"\n143:                             \"exactly the same\\n\" : \"\");\n144:     for (int i = 0; i < nThreads; i++)\n145:       printf(\"%6d, %s\\n\", i + 1, stats[i].format('s').c_str());\n146:     break;\n147:   }\n148:   default:\n149:     printHelp();\n150:     return 1;\n151:   }\n152:   return 0;\n153: }", "label": "(19, OpenMP, #pragma omp parallel); (0, OpenMP, int nThreads = omp_get_max_threads();)"}
{"predict": "(9, OpenMP, #pragma omp parallel for)", "label": "(4, TBB, using MyVector = oneapi::tbb::concurrent_vector<move_support_tests::Foo, MyAllocator>;); (8, TBB, oneapi::tbb::parallel_for(oneapi::tbb::blocked_range<std::size_t>(0, 10000*s, s), GrowToAtLeast<MyVector>(use_two_arg_form, v, copy_from), oneapi::tbb::simple_partitioner());)"}
{"predict": "1;\n99:     range[0][2] = 2;\n100:    g4 = MPI_Group_range_incl(g1, 1, range);\n101:    range[0][0] = 1;\n102:    range[0][1] = size - 1;\n103:    range[0][2] = -2;\n104:    g5 = MPI_Group_range_excl(g1, 1, range);\n105:    g45 = MPI_Group_union(g4, g5);\n106:    if (g45 != MPI_COMM_WORLD) {\n107:        fprintf(stderr, \"Union of incl and excl should be world\\n\");\n108:        errs++;\n109:    }\n110:    MPI_Group_free(&g45);\n111:    MPI_Group_free(&g4);\n112:    MPI_Group_free(&g5);\n113:    MPI_Group_free(&g1);\n114: \n115:    /* Allreduce operation */\n116:    MPI_Allreduce(&errs, &result, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n117: \n118:    if (result != 0) {\n119:        fprintf(stderr, \"Some tests failed\\n\");\n120:    } else {\n121:        printf(\"All tests passed\\n\");\n122:    }\n123: \n124:    MTest_Finalize();\n125:    return errs;\n126: }", "label": "(84, MPI, MPI_Group_excl(g1, g1size, lranks, &g6);); (27, MPI, MPI_Group_size(g2, &size);); (41, MPI, MPI_Group_translate_ranks(g2, 3, rin, g1, rout);); (70, MPI, MPI_Group_free(&selfgroup);); (105, MPI, MPI_Group_free(&g2);); (24, MPI, MPI_Group_rank(g2, &rank);); (117, MPI, MPI_Group_compare(MPI_GROUP_EMPTY, g4, &result);); (123, MPI, MPI_Group_free(&g45);); (113, MPI, MPI_Group_union(g4, g5, &g45);); (105, MPI, MPI_Group_range_excl(g1, 1, range, &g5);); (109, MPI, MPI_Group_range_incl(g1, 1, range, &g4);); (118, MPI, MPI_Group_free(&g5);); (-1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (12, MPI, MPI_Group_incl(g1, 3, ranks, &g2);); (114, MPI, MPI_Group_free(&g4);); (140, MPI, MPI_Group_free(&g1);); (72, MPI, MPI_Group_free(&g6);); (62, MPI, MPI_Group_size(g1, &g1size);); (34, MPI, MPI_Comm_group(MPI_COMM_SELF, &selfgroup);); (-10, MPI, MPI_Comm_group(MPI_COMM_WORLD, &g1);); (-10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myrank);); (41, MPI, MPI_Group_translate_ranks(g1, size, rin, selfgroup, rout);); (32, MPI, MPI_Group_translate_ranks(selfgroup, 1, rin, g1, rout);)"}
{"predict": "(13, MPI, MPI_Allgatherv(in.data(), local_size, MPI_DOUBLE, result.data(), sizes.data(), dspls.data(), MPI_DOUBLE, MPI_COMM_WORLD);) (16, MPI, MPI_Allgather(in_padded.data(), max_size, MPI_DOUBLE, result_padded.data(), max_size, MPI_DOUBLE, MPI_COMM_WORLD);)", "label": "(1, MPI, MPI_Init(&argc, &argv);); (58, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &P);)"}
{"predict": "(1, CUDA, __global__ void adaptive_threshold(SrcWrapper src, DstWrapper dst, Size2D dstSize, const uchar maxValue, KernelWrapper kernel, const int blockSize, const int idelta) {); (71, CUDA, )", "label": "(7, CUDA, extern __shared__ __align__(sizeof(float)) uchar s[];)"}
{"predict": "(14, OpenMP, #pragma omp parallel for reduction(+:r))", "label": "(5, OpenMP, #pragma omp barrier); (23, OpenMP, #pragma omp task); (18, OpenMP, #pragma omp for); (0, OpenMP, #pragma omp cancel parallel if(flag)); (4, OpenMP, #pragma omp sections); (7, OpenMP, #pragma omp section); (22, OpenMP, #pragma omp parallel sections); (-6, OpenMP, #pragma omp parallel); (18, OpenMP, #pragma omp cancel taskgroup); (32, OpenMP, #pragma omp parallel for reduction(+: r)); (0, OpenMP, #pragma omp cancel sections); (11, OpenMP, #pragma omp cancel for)"}
{"predict": "(12, OpenCL, auto clDeviceHandleList = clCreateFromGLDeviceListKHR(context, 2, &deviceId, &retVal);)", "label": "(31, OpenCL, clReleaseMemObject(buffer);)"}
{"predict": "(11, OpenMP, #pragma omp parallel for reduction(+:counter.value))", "label": "(5, TBB, tbb::profiling::set_name(counter.mutex, name);); (11, TBB, tbb::parallel_for(tbb::blocked_range<size_t>(0,n,n/10),AddOne<Counter<M> >(counter));); (11, TBB, tbb::tick_count t1 = tbb::tick_count::now();)"}
{"predict": "dLUstructInit(m, n, &LUstruct);\n136:     dSOLVEstructInit(m, n, nrhs, &SOLVEstruct);\n137: \n138:     /* Initialize SuperLU data structures. */\n139:     if (!iam) {\n140: \tprintf(\"Initializing SuperLU data structures.\\n\");\n141: \tfflush(stdout);\n142:     }\n143:     superlu_dist_init(&options, &grid, &A, &ScalePermstruct, &LUstruct, &SOLVEstruct, &stat);\n144: \n145:     /* ------------------------------------------------------------\n146:        2. WE PERFORM A FORWARD BACKWARD SOLVE.\n147:        ------------------------------------------------------------*/\n148:     if (!iam) {\n149: \tprintf(\"Starting forward-backward solve.\\n\");\n150: \tfflush(stdout);\n151:     }\n152:     superlu_dist_solve(&options, &A, &ScalePermstruct, &LUstruct, &SOLVEstruct, b, ldb, xtrue, ldx, berr, &stat);\n153: \n154:     /* ------------------------------------------------------------\n155:        3. WE REFINE THE SOLUTION.\n156:        ------------------------------------------------------------*/\n157:     if (!iam) {\n158: \tprintf(\"Starting iterative refinement.\\n\");\n159: \tfflush(stdout);\n160:     }\n161:     superlu_dist_gstrf(&options, &A, &ScalePermstruct, &LUstruct, &SOLVEstruct, &stat);\n162:     superlu_dist_refine(&options, &A, &ScalePermstruct, &LUstruct, &SOLVEstruct, b, ldb, xtrue, ldx, berr, &stat);\n163: \n164:     /* ------------------------------------------------------------\n165:        4. WE COMPUTE THE RESIDUAL.\n166:        ------------------------------------------------------------*/\n167:     if (!iam) {\n168: \tprintf(\"Computing residual.\\n\");\n169: \tfflush(stdout);\n170:     }\n171:     superlu_dist_gthr(&options, &A, &ScalePermstruct, &LUstruct, &SOLVEstruct, b, ldb, xtrue, ldx", "label": "(232, MPI, MPI_Finalize();); (35, MPI, MPI_Init_thread( &argc, &argv, MPI_THREAD_MULTIPLE, &omp_mpi_level);)"}
{"predict": "(7, OpenMP, #pragma omp parallel for;)\n(10, OpenMP, #pragma omp parallel for;)\n(13, OpenMP, #pragma omp parallel for;)\n(17, OpenMP, #pragma omp parallel for;)\n(20, OpenMP, #pragma omp parallel for;)\n(23, OpenMP, #pragma omp parallel for;)\n(27, OpenMP, #pragma omp parallel for;)\n(30, OpenMP, #pragma omp parallel for;)\n(33, OpenMP, #pragma omp parallel for;)\n(36, OpenMP, #pragma omp parallel for;)\n(39, OpenMP, #pragma omp parallel for;)\n(42, OpenMP, #pragma omp parallel for;)\n(45, OpenMP, #pragma omp parallel for;)", "label": "(34, OpenMP, #pragma omp teams distribute parallel for simd linear (argc : 5) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (19, OpenMP, #pragma omp teams distribute parallel for simd linear (argc); (27, OpenMP, #pragma omp teams distribute parallel for simd linear (argc > 0 ? argv[1] : argv[2])); (43, OpenMP, #pragma omp teams distribute parallel for simd linear (argv[1])); (38, OpenMP, #pragma omp teams distribute parallel for simd linear (a, b:B::ib)); (55, OpenMP, #pragma omp teams distribute parallel for simd linear(i)); (19, OpenMP, #pragma omp teams distribute parallel for simd linear (argc,); (8, OpenMP, #pragma omp teams distribute parallel for simd linear ()); (30, OpenMP, #pragma omp teams distribute parallel for simd linear (S1)); (-2, OpenMP, #pragma omp teams distribute parallel for simd linear); (41, OpenMP, #pragma omp teams distribute parallel for simd linear(e, g)); (-5, OpenMP, #pragma omp target); (-1, OpenMP, #pragma omp teams distribute parallel for simd linear (); (42, OpenMP, #pragma omp teams distribute parallel for simd linear(h))"}
{"predict": "DOFTESTS;\n108:     }\n109: \n110:     stencil_size = 2*radius+1;\n111:     /* compute the number of non-zero elements in the sparse matrix                 */\n112:     nent = size2*stencil_size;\n113: \n114:     /* allocate memory for sparse matrix                                           */\n115:     matrix_space = nent * sizeof(double);\n116:     matrix = (double *) malloc(matrix_space);\n117:     if (!matrix) {\n118:       printf(\"ERROR: Insufficient memory to allocate matrix\\n\");\n119:       error = 1;\n120:       goto ENDOFTESTS;\n121:     }\n122: \n123:     /* allocate memory for column indices                                          */\n124:     index_space = nent * sizeof(s64Int);\n125:     colIndex = (s64Int *) malloc(index_space);\n126:     if (!colIndex) {\n127:       printf(\"ERROR: Insufficient memory to allocate column indices\\n\");\n128:       error = 1;\n129:       goto ENDOFTESTS;\n130:     }\n131: \n132:     /* allocate memory for vector                                                  */\n133:     vector_space = size * sizeof(double);\n134:     vector = (double *) malloc(vector_space);\n135:     if (!vector) {\n136:       printf(\"ERROR: Insufficient memory to allocate vector\\n\");\n137:       error = 1;\n138:       goto ENDOFTESTS;\n139:     }\n140: \n141:     /* allocate memory for result                                                  */\n142:     result = (double *) malloc(vector_space);\n143:     if (!result) {\n144:       printf(\"ERROR: Insufficient memory to allocate result\\n\");\n145:       error = 1;\n146:       goto ENDOFTESTS;\n147:     }\n148: \n149:     /* initialize matrix, vector and result                                        */\n150:     initializeSparseMatrix(matrix, colIndex, nent, size, radius);\n151:     initializeVector(vector, size);\n1", "label": "(134, MPI, MPI_Bcast(&size2,      1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (134, MPI, MPI_Bcast(&radius,     1, MPI_INT,           root, MPI_COMM_WORLD);); (316, MPI, MPI_Finalize();); (129, MPI, MPI_Bcast(&lsize2,     1, MPI_INT,           root, MPI_COMM_WORLD);); (127, MPI, MPI_Bcast(&lsize,      1, MPI_INT,           root, MPI_COMM_WORLD);); (128, MPI, MPI_Bcast(&size,       1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (290, MPI, MPI_Reduce(&vector_sum, &check_sum, 1, MPI_DOUBLE, MPI_SUM, root, MPI_COMM_WORLD);); (41, MPI, MPI_Init(&argc,&argv);); (243, MPI, MPI_Barrier(MPI_COMM_WORLD);); (41, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (126, MPI, MPI_Bcast(&iterations, 1, MPI_INT,           root, MPI_COMM_WORLD);); (38, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(16, OpenMP, #pragma omp parallel for private(i) reduction(+:A[:blockSize]))", "label": "(18, OpenMP, #pragma omp parallel for); (14, OpenMP, #pragma omp target map(tofrom: Inner, Outer, wayout)); (14, OpenMP, #pragma omp teams num_teams(1  ) thread_limit(tl))"}
{"predict": "(7, OpenMP, #pragma omp parallel for private(tmpV))", "label": "(6, OpenMP, #pragma omp parallel for firstprivate(length, v_cut))"}
{"predict": "directory exists\n136:     conduit::Node io_opts;\n137:     io_opts[\"io/output_dir\"] = \"_output\";\n138: \n139:     ascent::Runtime runtime;\n140:     runtime.open(actions, io_opts);\n141:     runtime.execute(actions);\n142:     runtime.close();\n143: }", "label": "(166, MPI, opts[\"mpi_comm\"] = MPI_Comm_c2f(MPI_COMM_WORLD);); (171, MPI, MPI_Barrier(comm);); (21, MPI, MPI_Comm_rank(comm, &par_rank);); (21, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(27, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);)", "label": "(25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (20, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (51, MPI, MPI_Finalize();); (48, MPI, MPI_T_finalize();); (19, MPI, MPI_T_init_thread(required, &provided);); (32, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(11, CUDA, cudaLaunchKernelGGL(KernelFunction, kGridDimension, kBlockDimension, 0, 0, ...);)", "label": "(101, CUDA, checkHIPErrors( ::cudaDeviceSynchronize() );)"}
{"predict": "(33, OpenMP, #pragma omp parallel sections) (34, OpenMP, #pragma omp section) (35, OpenMP, d += 1;) (36, OpenMP, Gd += 1.0;) (37, OpenMP, Sd += 1.0;) (38, OpenMP, #pragma omp section) (39, OpenMP, b += 1;) (40, OpenMP, Gb += 1.0;) (41, OpenMP, Sb += 1.0;) (42, OpenMP, c += 1;) (43, OpenMP, Sc += 1.0;) (44, OpenMP, a += 1;) (45, OpenMP, Sa += 1.0;) (46, OpenMP, return a + b + c + d + (int)Sa + (int)Sb + (int)Sc + (int)Sd;)", "label": "(32, OpenMP, #pragma omp parallel if(Gc>0.0 && c>0 && Sc>0.0)); (22, OpenMP, #pragma omp target if(Ga>0.0 && a>0 && Sa>0.0))"}
{"predict": "(44, CUDA, block_radix_sort_t(temp_storage).SortBlockedToStriped(thread_keys[threadIdx.x], thread_values[threadIdx.x], decomposer_t{}, begin_bit, end_bit);)", "label": "(7, CUDA, __shared__ block_radix_sort_t::TempStorage temp_storage;)"}
{"predict": "(17, OpenMP, #pragma omp target teams distribute if(cond) firstprivate(fp) reduction(+: rd) map(alloc: map))", "label": "(18, OpenMP, #pragma omp target teams distribute if(cond) firstprivate(fp) reduction(+:rd) map(alloc:map))"}
{"predict": "(29, OpenMP, #pragma omp parallel for)", "label": "(14, CUDA, thrust::host_vector<uint32_t> h_increments(num_increments);); (43, CUDA, thrust::device_vector<uint32_t> increments_in(num_increments);); (53, CUDA, thrust::fill(counts_out.begin(), counts_out.end(), 814920U);); (40, CUDA, thrust::device_vector<uint32_t> bins_in(num_increments);); (41, CUDA, thrust::device_vector<uint32_t> counts_out(NUM_ITEMS);); (47, CUDA, thrust::host_vector<uint32_t> host_counts(num_increments);); (7, CUDA, thrust::host_vector<uint32_t> h_bins(num_increments);)"}
{"predict": "(25, OpenMP, #pragma omp parallel for private(i) reduction(+:err))", "label": "(21, OpenMP, #pragma omp target update from(a[:n])); (12, OpenMP, #pragma omp target teams distribute parallel for map(to:b[:n])); (15, OpenMP, #pragma omp target teams distribute parallel for)"}
{"predict": "(4, CUDA, cudaMalloc((void**)&d_int, size * sizeof(int));) (26, CUDA, cudaFree(d_int);)", "label": "(6, CUDA, gpuErrchk(cudaPointerGetAttributes(&attributes, d_int));); (28, CUDA, gpuErrchk(cudaFree(d_int));); (1, CUDA, gpuErrchk(cudaMalloc(&d_int, sizeof(int)));)"}
{"predict": "(13, CUDA, auto cellImagePos = mapWorldPosToImagePos(rectUpperLeft, cell->pos, universeImageSize, zoom);) (15, CUDA, if (threadIdx.x == 0) { color = calcColor(cell, cell->selected, coloring, true); })", "label": "(4, CUDA, __shared__ float3 color;); (2, CUDA, __shared__ bool isContained;); (0, CUDA, __shared__ float2 cellImagePos;)"}
{"predict": "(14, CUDA, auto result = naiveReduceRowsByKeyKernel<<<dim3((ncols + 31) / 32, nkeys), dim3(32, 1), 0, stream>>>(d_A, lda, d_keys, d_weight, d_char_keys, nrows, ncols, nkeys, d_sums);)", "label": "(12, CUDA, cudaMemset(d_sums, 0, sizeof(Type) * nkeys * ncols);)"}
{"predict": "(16, OpenMP, #pragma omp parallel for reduction(+:sum))", "label": "(11, OpenMP, omp_set_num_threads(nr_threads);); (12, OpenMP, #pragma omp parallel default(none) shared(a)); (13, OpenMP, #pragma omp sections); (7, OpenMP, omp_set_dynamic(0);); (13, OpenMP, #pragma omp section); (15, OpenMP, int thread_nr = omp_get_thread_num();)"}
{"predict": "(8, CUDA, uint64_t samples[8] __shared__); (22, CUDA, if (tid < n / 8 * coeff_mod_size) {) (25, CUDA, twr_idx = tid / (n / 8) + start_mod_idx;) (27, CUDA, n_idx = tid % (n / 8);) (29, CUDA, data_ptr = inout + twr_idx * n;) (31, CUDA, psi = twiddles + twr_idx * n;) (32, CUDA, psi_shoup = twiddles_shoup + twr_idx * n;) (33, CUDA, modulus_table = modulus;) (34, CUDA, modulus = modulus_table[twr_idx].value();) (36, CUDA, n_init = t / 4 / group * pad_idx + pad_tid + pad * (n_idx / (group * pad));) (42, CUDA, fntt8<<<1, 8>>>(samples, psi, psi_shoup, tw_idx, modulus);) (47, CUDA, for (size_t j = 8, k = group / 2; j < group + 1; j *= 8, k >>= 3) {) (50, CUDA, samples[l] = buffer[(n1 + pad) * pad_tid + 2 * m_idx2 * k + t_idx2 + (k / 4) * l];) (54, CUDA, fntt8<<<1, 8>>>(samples, psi, psi_shoup, tw_idx2, modulus);) (58, CUDA, buffer[(n1 + pad) * pad_tid + 2 * m_idx2 * k + t_idx2 + (k / 4) * l] = samples[l];) (64, CUDA, if (group < 8) {) (66, CUDA, ct_butterfly<<<1, 4>>>(samples[0], samples[1], psi[tw_idx2], psi_shoup[tw_idx2], modulus);) (67, CUDA, ct_butterfly<<<1, 4>>>(samples[2], samples[3], psi[tw_idx2 + 1], psi_shoup[tw_idx2 + 1], modulus);) (68, CUDA, ct_butterfly<<<1, 4>>>(samples[4], samples[5], psi", "label": "(10, CUDA, extern __shared__ uint64_t buffer[];)"}
{"predict": "(1, CUDA, __global__ void CdistP(float *x1, float *x2, float *result, double p, const int64_t r2, const int64_t m, const int64_t r_size, const int64_t l1_size, const int64_t l2_size) {)", "label": "(22, CUDA, __shared__ float shared[forward_threads];)"}
{"predict": "(11, CUDA, sm_o = (float *)cudaMalloc(m * m * sizeof(float));) (17, CUDA, sm_o = (float *)cudaMalloc(m * m * sizeof(float));) (27, CUDA, O = (float *)cudaMalloc(m * n * sizeof(float));)", "label": "(12, CUDA, cudaDeviceSynchronize();); (7, CUDA, cudaMalloc((void **)&sm_o, sizeof(float) * m * m);); (34, CUDA, cudaFree(sm_o);)"}
{"predict": "(28, CUBLAS, cublasSetStream(cublasH, stream);) (34, CUDA, CUDA_CHECK(cudaMemcpyAsync(d_AP, AP.data(), sizeof(data_type) * AP.size(), cudaMemcpyHostToDevice, stream));) (39, CUBLAS, CUBLAS_CHECK(cublasDtpttr(cublasH, uplo, n, d_AP, d_A, lda));) (41, CUDA, CUDA_CHECK(cudaMemcpyAsync(A.data(), d_A, sizeof(data_type) * A.size(), cudaMemcpyDeviceToHost, stream));)", "label": "(46, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (32, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_AP), sizeof(data_type) * AP.size()));); (63, CUDA, CUDA_CHECK(cudaDeviceReset());); (60, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (30, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (53, CUDA, CUDA_CHECK(cudaFree(d_AP));); (53, CUDA, CUDA_CHECK(cudaFree(d_A));); (22, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(13, OpenCL, OCL_CHECK(status = clEnqueueNDRangeKernel(profiling_queue, kernel, 1, NULL, globals, locals, 0, NULL, &exec_event));)", "label": "(40, OpenCL, clReleaseEvent(exec_event);); (11, OpenCL, profiling_queue = clCreateCommandQueue(ctx, device, CL_QUEUE_PROFILING_ENABLE, &status);); (47, OpenCL, clReleaseCommandQueue(profiling_queue);)"}
{"predict": "tp_init(&orig, 0, 0, count);\n119:         }\n120: \n121:         if (flush_type == FLUSH_TYPE__FLUSH) {\n122:             for (t = target_start_idx; t <= target_end_idx; t++)\n123:                 MPI_Win_flush(t, win);\n124:         } else if (flush_type == FLUSH_TYPE__FLUSH_ALL) {\n125:             MPI_Win_flush_all(win);\n126:         }\n127: \n128:         MPI_Win_lock(lock_type, target_start_idx, target_end_idx, win);\n129:         MPI_Win_sync(win);\n130:         MPI_Win_unlock(target_start_idx, target_end_idx, win);\n131: \n132:         MTest_dtp_validate(&orig, &result);\n133:     }\n134: \n135:     /* second barrier */\n136:     MPI_Barrier(comm);\n137: \n138:     /* check if the results are correct */\n139:     if (rank == target_rank) {\n140:         MTest_dtp_validate(&orig, &result);\n141:     }\n142: \n143:     /* clean up */\n144:     MTest_Finalize();\n145:     MPI_Finalize();\n146:     return errs;\n147: }", "label": "(163, MPI, MPI_Win_unlock(rank, win);); (22, MPI, MPI_Comm_size(comm, &size);); (125, MPI, MPI_Barrier(comm);); (134, MPI, MPI_Win_unlock(target_rank, win);); (135, MPI, MPI_Win_unlock_all(win);); (76, MPI, MPI_Win_lock(MPI_LOCK_SHARED, target_rank, 0, win);); (120, MPI, MPI_Win_flush_all(win);); (76, MPI, MPI_Win_lock_all(0, win);); (150, MPI, MPI_Win_lock(MPI_LOCK_SHARED, rank, 0, win);); (111, MPI, MPI_Win_flush_local_all(win);); (12, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(27, CUDA, <<<blocks, threads, 0, stream>>>(samples, anchors, delta, regWeight, inputHeight, inputWidth, outputBbox, bboxClipThresh);)", "label": "(39, CUDA, return cudaGetLastError();)"}
{"predict": "(11, MPI, PMPI_Comm_size(MPI_COMM_WORLD, &comm_world_size);) (12, MPI, PMPI_Comm_rank(MPI_COMM_WORLD, &comm_world_rank);)", "label": "(19, MPI, MPIT_result = MPI_T_pvar_session_create(&session);); (12, MPI, MPIT_result = MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);)"}
{"predict": "(16, CUDA, checkCudaErrors(cudaMalloc(&dData, size));)", "label": "(127, CUDA, checkCudaErrors(cudaFreeArray(cuArray));); (33, CUDA, checkCudaErrors(cudaMalloc((void **) &dData, size));); (56, CUDA, checkCudaErrors(cudaBindTextureToArray(tex, cuArray, channelDesc));); (63, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (71, CUDA, getLastCudaError(\"Kernel execution failed\");); (-3, CUDA, int devID = findCudaDevice(argc, (const char **) argv);); (120, CUDA, checkCudaErrors(cudaFree(dData));)"}
{"predict": "(7, MPI, MPI_Comm_group(MPI_COMM_WORLD, &group);)", "label": "(9, MPI, MPI_Group_compare(group, wgroup, &result);); (5, MPI, MPI_File_get_group(cFile, &group);); (0, MPI, MPI_File cFile = MPI_File_f2c(*file);); (4, MPI, MPI_Comm_group(MPI_COMM_WORLD, &wgroup);); (12, MPI, MPI_Group_free(&wgroup);); (10, MPI, MPI_Group_free(&group);)"}
{"predict": "(28, CUDA, cudaStreamCreate(&streams[i]);) (47, CUDA, for (int i = 0; i < num_streams; ++i){ kernel_a<<<blocks, threads, 0, streams[i]>>>(d_x, d_y); kernel_b<<<blocks, threads, 0, streams[i]>>>(d_x, d_y); kernel_c<<<blocks, threads, 0, streams[i]>>>(d_x, d_y); kernel_d<<<blocks, threads, 0, streams[i]>>>(d_x, d_y); }) (86, CUDA, high_resolution_clock::time_point t1 = high_resolution_clock::now();) (96, CUDA, duration<double> total_time = duration_cast<duration<double>>(t2 - t1);)", "label": "(75, CUDA, cudaCheckErrors(\"Kernel c failed\");); (99, CUDA, cudaCheckErrors(\"Launching graph failed\");); (13, CUDA, cudaStreamCreateWithFlags(&streams[i], cudaStreamNonBlocking);); (63, CUDA, cudaCheckErrors(\"Event record failed\");); (56, CUDA, cudaCheckErrors(\"Stream begin capture failed\");); (31, CUDA, cudaMalloc((void**) &d_x, N * sizeof(float));); (44, CUDA, cudaGraphCreate(FIXME, 0);); (62, CUDA, cudaCheckErrors(\"Kernel b failed\");); (104, CUDA, cudaDeviceSynchronize();); (98, CUDA, cudaGraphGetNodes(graph, nodes, &numNodes);); (121, CUDA, cudaMemcpy(h_y, d_y, N, cudaMemcpyDeviceToHost);); (73, CUDA, cudaCheckErrors(\"Kernel d failed\");); (89, CUDA, cudaStreamSynchronize(streams[0]);); (58, CUDA, cudaStreamWaitEvent(streams[1], event1);); (72, CUDA, cudaStreamEndCapture(streams[0], &graph);); (48, CUDA, cudaCheckErrors(\"Kernel a failed\");); (64, CUDA, cudaStreamWaitEvent(streams[0], event2);); (-11, CUDA, cudaEventCreateWithFlags(&event1, cudaEventDisableTiming);); (-11, CUDA, cudaEventCreateWithFlags(&event2, cudaEventDisableTiming);); (53, CUDA, cudaCheckErrors(\"Event wait failed\");); (88, CUDA, cudaCheckErrors(\"Graph get nodes failed\");); (44, CUDA, cudaEventRecord(event1, streams[0]);); (19, CUDA, cudaMemcpy(d_y, h_y, N, cudaMemcpyHostToDevice);); (54, CUDA, cudaEventRecord(event2, streams[1]);); (16, CUDA, cudaMemcpy(d_x, h_x, N, cudaMemcpyHostToDevice);); (107, CUDA, cudaCheckErrors(\"Finishing memcpy failed\");); (11, CUDA, cudaMalloc((void**) &d_y, N * sizeof(float));); (32, CUDA, cudaStreamBeginCapture(streams[0], cudaStreamCaptureModeGlobal);); (91, CUDA, cudaGraphLaunch(instance, streams[0]);); (63, CUDA, cudaCheckErrors(\"instantiating graph failed\");); (8, CUDA, cudaCheckErrors(\"cudaMalloc failed\");); (56, CUDA, cudaCheckErrors(\"Stream end capture failed\");); (59, CUDA, cudaGraphInstantiate(FIXME, graph, NULL, NULL, 0);); (66, CUDA, cudaGraphLaunch(FIXME, streams[0]);)"}
{"predict": "(13, MPI, rc = lwgrp_logring_exscan_recursive(inbuf, outbuf, count, type, op, group, list);)", "label": "(19, MPI, MPI_Reduce_local((void*)inbuf, outbuf, count, type, op);)"}
{"predict": "(34, CUDA, __shared__ float aBuffer1[4]; __shared__ float bBuffer1[4]; __shared__ float aBuffer2[4]; __shared__ float bBuffer2[4];)", "label": "(35, CUDA, __shared__ _VOLATILE_ float aSmem2[8][128+4];); (33, CUDA, __shared__ _VOLATILE_ float bSmem1[8][128+4];); (34, CUDA, __shared__ _VOLATILE_ float bSmem2[8][128+4];); (30, CUDA, __shared__ _VOLATILE_ float aSmem1[8][128+4];)"}
{"predict": "(1, CUDA, __global__ void KerReduSumUint4(unsigned n, unsigned ini, const uint4* dat, uint4* res) {)", "label": "(3, CUDA, extern __shared__ unsigned sfdatx[];)"}
{"predict": "(11, OpenMP, #pragma omp parallel if (mpi.vm()[\"order\"].as<int>() == 1)) (14, OpenMP, #pragma omp parallel if (mpi.vm()[\"order\"].as<int>() == 2)) (22, OpenMP, #pragma omp parallel if (mpi.vm()[\"order\"].as<int>() == 3)) (30, OpenMP, #pragma omp parallel if (mpi.vm()[\"order\"].as<int>() == 4)) (37, OpenMP, #pragma omp parallel if (mpi.vm()[\"order\"].as<int>() == 5))", "label": "(9, MPI, test_integration_sin<2,1,double> t1( mpi.vm()[\"hsize\"].as<double>() );); (31, MPI, test_integration_sin<2,4,double> t2( mpi.vm()[\"hsize\"].as<double>() );); (14, MPI, test_integration_sin<2,2,double> t2( mpi.vm()[\"hsize\"].as<double>() );); (2, MPI, std::cout << \"Order = \" << mpi.vm()[\"order\"].as<int>() << \" / \" << FEELPP_MESH_MAX_ORDER << \"\\n\";); (20, MPI, test_integration_sin<2,3,double> t2( mpi.vm()[\"hsize\"].as<double>() );); (37, MPI, test_integration_sin<2,5,double> t2( mpi.vm()[\"hsize\"].as<double>() );)"}
{"predict": "(14, CUDA, FastllmSplitBatchKernel <256> <<< part * outer, 256 >>> ((uint8_t*)input.cudaData, cpuPointers, outer, part, inner * unitSize);)", "label": "(16, CUDA, FastllmCudaFree(pointers);); (12, CUDA, cudaMemcpy(pointers, cpuPointers, sizeof(uint8_t*) * part, cudaMemcpyHostToDevice);); (6, CUDA, uint8_t ** pointers = (uint8_t**)FastllmCudaMalloc(sizeof(uint8_t*) * part);)"}
{"predict": "(13, MPI, MPI_Finalize();) (24, MPI, MPI_Finalize();) (35, MPI, MPI_Finalize();) (44, MPI, MPI_Finalize();)", "label": "(30, MPI, errs = MPI_Irecv(buf, 10, MPI_CHAR, 0, 0, MPI_COMM_WORLD, &request);); (5, MPI, MPI_Init(&argc, &argv);); (65, MPI, MPI_Finalize();); (18, MPI, errs = MPI_Issend(\"No Errors\", 10, MPI_CHAR, 3, 0, MPI_COMM_WORLD, &request);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (16, MPI, errs += MPI_Wait(&request, MPI_STATUS_IGNORE);); (2, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (5, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (34, MPI, errs = MPI_Issend(\"No Errors\", 10, MPI_CHAR, 2, 0, MPI_COMM_WORLD, &request);)"}
{"predict": "(2, OpenMP, #pragma omp parallel for)", "label": "(7, OpenMP, #pragma omp target teams distribute parallel for simd order(); (15, OpenMP, #pragma omp target teams distribute parallel for simd order(concurrent)); (-1, OpenMP, #pragma omp target teams distribute parallel for simd nowait nowait); (7, OpenMP, #pragma omp target teams distribute parallel for simd order(none); (9, OpenMP, #pragma omp target teams distribute parallel for simd order(concurrent); (-1, OpenMP, #pragma omp target teams distribute parallel for simd order)"}
{"predict": "(21, CUDA, <<<grid, BLOCK, 0, stream>>>(x, y, condition, z, zSize, zDivMod, xStrides, yStrides, condStrides);)", "label": "(24, CUDA, CHECK_CUDA_RETURN(cudaGetLastError());)"}
{"predict": "(4, OpenMP, #pragma omp parallel for reduction(*:tolerance_multiplier))", "label": "(7, TBB, tbb::tick_count t1 = tbb::tick_count::now();); (14, TBB, ASSERT( num_threads > 0, \"tbb::thread::hardware_concurrency() has returned an incorrect value\" );); (3, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (11, TBB, int num_threads = tbb::tbb_thread::hardware_concurrency();); (5, TBB, tbb::tick_count t2 = tbb::tick_count::now();)"}
{"predict": "];\n110: \n111: #pragma omp target\n112: #pragma omp teams\n113: // omp4-error@+2 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', or '>=') of loop variable 'i'}} omp5-error@+2 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', '>=', or '!=') of loop variable 'i'}}\n114: #pragma omp distribute parallel for simd\n115:   for (int i = 0; i < 10; i + 1)\n116:     c[i] = a[i];\n117: \n118: #pragma omp target\n119: #pragma omp teams\n120: // omp4-error@+2 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', or '>=') of loop variable 'i'}} omp5-error@+2 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', '>=', or '!=') of loop variable 'i'}}\n121: #pragma omp distribute parallel for simd\n122:   for (int i = 0; i < 10; ++i + 1)\n123:     c[i] = a[i];\n124: \n125: #pragma omp target\n126: #pragma omp teams\n127: // omp4-error@+2 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', or '>=') of loop variable 'i'}} omp5-error@+2 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', '>=', or '!=') of loop variable 'i'}}\n128: #pragma omp distribute parallel for simd\n129:   for (int i = 0; i < 10; i++)\n130:     c[i] = a[i];\n131: \n132: #pragma omp target\n133: #pragma omp teams\n134: // omp4-error@+2 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', or '>=') of loop variable 'i'}} omp5-error", "label": "(311, OpenMP, #pragma omp distribute parallel for simd firstprivate(ii)); (332, OpenMP, #pragma omp distribute parallel for simd lastprivate(ii)); (6, OpenMP, #pragma omp distribute parallel for simd); (322, OpenMP, #pragma omp distribute parallel for simd private(ii)); (313, OpenMP, #pragma omp distribute parallel for simd linear(ii)); (1, OpenMP, #pragma omp target); (347, OpenMP, #pragma omp distribute parallel for simd collapse(2)); (0, OpenMP, #pragma omp teams)"}
{"predict": "(7, CUDA, cusparseDestroy(handle_);) (9, CUDA, cusolverSpDestroy(handle_cusolver_);) (11, CUDA, cublasDestroy(handle_cublas_);) (13, CUDA, delete[] hostx_;) (16, CUDA, delete[] residual_;) (17, CUDA, delete[] solution_;) (21, CUDA, cusparseDestroyMatDescr(descr_M_);) (23, CUDA, cusolverSpDestroyGluInfo(info_M_);) (25, CUDA, cusolverSpDestroyGluInfo(info_M_);) (27, CUDA, klu_free_symbolic(&Symbolic_, &Common_);) (28, CUDA, klu_free_numeric(&Numeric_, &Common_);) (30, CUDA, delete[] mia_;) (31, CUDA, delete[] mja_;)", "label": "(17, CUDA, cudaFree(devx_);); (27, CUDA, cudaFree(d_T_);); (24, CUDA, cudaFree(d_P_);); (24, CUDA, cudaFree(d_Q_);); (12, CUDA, cudaFree(devr_);); (1, CUDA, cudaFree(d_work_);)"}
{"predict": "(16, OpenMP, num_threads[j] = omp_get_num_threads();) (17, OpenMP, num_teams[j] = omp_get_num_teams();)", "label": "(7, OpenMP, printf(\"#pragma omp target teams distribute parallel for thread_limit(4)\\n\");); (10, OpenMP, num_teams[j] = omp_get_num_teams();); (13, OpenMP, printf(\"#pragma omp target teams distribute parallel for\\n\");); (7, OpenMP, num_threads[j] = omp_get_num_threads();); (4, OpenMP, #pragma omp target teams distribute parallel for)"}
{"predict": "(18, OpenCL, clLinkProgram_StubWithCallback(clLinkProgram_testLinkProgram);)", "label": "(19, OpenCL, clReleaseContext_ExpectAndReturn(make_context(0), CL_SUCCESS);)"}
{"predict": "(72, MPI, MPI_Sendrecv((char*)c_info->s_buffer + i%ITERATIONS->s_cache_iter*ITERATIONS->s_offs, s_num, c_info->s_data_type, dest, s_tag, (char*)c_info->r_buffer + i%ITERATIONS->r_cache_iter*ITERATIONS->r_offs, r_num, c_info->r_data_type, source, r_tag, c_info->communicator, &stat);)", "label": "(63, MPI, *time -= MPI_Wtime();); (43, MPI, MPI_Type_size(c_info->r_data_type, &r_size);); (41, MPI, MPI_Type_size(c_info->s_data_type, &s_size);); (73, MPI, *time += MPI_Wtime();)"}
{"predict": "(14, OpenMP, #pragma omp parallel for)", "label": "(14, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(exe_node), receivers[r] );); (1, TBB, typedef typename tbb::flow::tuple_element<0,OutputTuple>::type OutputType;); (27, TBB, tbb::flow::remove_edge( tbb::flow::output_port<0>(exe_node), receivers[r] );); (3, TBB, tbb::flow::multifunction_node< InputType, OutputTuple, tbb::flow::rejecting > exe_node( g, tbb::flow::unlimited, body );); (1, TBB, tbb::flow::graph g;)"}
{"predict": "118: \n119:     /* Advance to the destination node that is to contain this x coordinate */\n120:     while ((mesh_x >= slice_offset[dest_node_check] + slice_width[dest_node_check]) ||\n121:            (slice_width[dest_node_check] == 0)) {\n122:       dest_node_check++;\n123:     }\n124: \n125:     /* Add this mesh cell */\n126:     nr_send_check[dest_node_check]++;\n127:   }\n128: \n129:   /* Check that both versions agree */\n130:   for (int i = 0; i < nr_nodes; i++) {\n131:     if (nr_send[i] != nr_send_check[i])\n132:       error(\"Number of elements to be sent to node %d disagree: %zu vs %zu\",\n133:             i, nr_send[i], nr_send_check[i]);\n134:   }\n135: \n136:   free(nr_send_check);\n137: #endif\n138: \n139:   /* Allocate space for the final send buffer */\n140:   struct mesh_key_value_rho *mesh_sendbuf_final = (struct mesh_key_value_rho *)malloc(\n141:       sizeof(struct mesh_key_value_rho) * nr_send[dest_node]);\n142: \n143:   /* Loop over the sorted array and put the elements in the correct order */\n144:   for (int i = 0; i < N; ++i) {\n145:     if (i < N - 1) {\n146:       size_t j = sorted_offsets[i];\n147:       while (j < sorted_offsets[i + 1]) {\n148:         mesh_sendbuf_final[i] = mesh_sendbuf[j];\n149:         j++;\n150:       }\n151:     } else {\n152:       size_t j = sorted_offsets[i];\n153:       while (j < count) {\n154:         mesh_sendbuf_final[i] = mesh_sendbuf[j];\n155:         j++;\n156:       }\n157:     }\n158:   }\n159: \n160:   if", "label": "(11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &nodeID);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nr_nodes);); (73, MPI, MPI_Allgather(&local_n0, 1, MPI_INT, slice_width, 1, MPI_INT, MPI_COMM_WORLD);)"}
{"predict": "pragma omp teams distribute simd\n105: // omp4-error@+1 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', or '>=') of loop variable 'i'}} omp5-error@+1 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', '>=', or '!=') of loop variable 'i'}}\n106:   for (int i = 0; i != 10; i++)\n107:     c[i] = a[i];\n108: \n109: #pragma omp target\n110: #pragma omp teams distribute simd\n111: // omp4-error@+1 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', or '>=') of loop variable 'i'}} omp5-error@+1 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', '>=', or '!=') of loop variable 'i'}}\n112:   for (int i = 0; i != 10; ++i)\n113:     c[i] = a[i];\n114: \n115: #pragma omp target\n116: #pragma omp teams distribute simd\n117: // omp4-error@+1 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', or '>=') of loop variable 'i'}} omp5-error@+1 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', '>=', or '!=') of loop variable 'i'}}\n118:   for (int i = 0; i <= 10; i++)\n119:     c[i] = a[i];\n120: \n121: #pragma omp target\n122: #pragma omp teams distribute simd\n123: // omp4-error@+1 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', or '>=') of loop variable 'i'}} omp5-error@+1 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', '>=', or '!=') of loop variable 'i'}}\n124:   for (int i = 0; i < 10;", "label": "(266, OpenMP, #pragma omp teams distribute simd firstprivate(ii)); (6, OpenMP, #pragma omp teams distribute simd); (270, OpenMP, #pragma omp teams distribute simd private(ii)); (275, OpenMP, #pragma omp teams distribute simd lastprivate(ii)); (287, OpenMP, #pragma omp teams distribute simd collapse(2)); (1, OpenMP, #pragma omp target)"}
{"predict": "\", 5, 0);\n107: \n108:     /* -------------------------------------------------\n109:      * Create test files with dataset and attribute with container types \n110:      * (array, vlen) with multiple nested compound types.\n111:      * Also test with the datasets having no comparables\n112:      */\n113:     /* file1 */\n114:     test_data_nocomparables(DATA_NO_COMPARES1, 0);\n115:     test_objs_nocomparables(OBJ_NO_COMPARES1, OBJ_NO_COMPARES2);\n116:     /* file2 */\n117:     test_data_nocomparables(DATA_NO_COMPARES2, 5);\n118:     test_objs_nocomparables(OBJ_NO_COMPARES3, OBJ_NO_COMPARES4);\n119: \n120:     return 0;\n121: }\n\nOutput:", "label": "(84, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (84, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)"}
{"predict": "(7, OpenMP, #pragma omp parallel for private(move_min))", "label": "(6, OpenMP, #pragma omp parallel for)"}
{"predict": "(6, MPI, MPI_Comm_rank(comm, &rank); if (rank == 0) { std::cout << \"mfem disabled: skipping test that requires high order input \" << std::endl; return; })", "label": "(20, MPI, int rank = dray::dray::mpi_rank();); (9, MPI, dray::dray::mpi_comm(MPI_Comm_c2f(comm));); (19, MPI, int size = dray::dray::mpi_size();)"}
{"predict": "(33, MPI, MPI_Scatter(keys, count / 2, MPI_INT, keys, count / 2, MPI_INT, 0, MPI_COMM_WORLD);)\n(34, MPI, MPI_Scatter(vals, count / 2, MPI_INT, vals, count / 2, MPI_INT, 0, MPI_COMM_WORLD);)\n(35, MPI, MPI_Scatter(ltr, count / 2, MPI_INT, ltr, count / 2, MPI_INT, 0, MPI_COMM_WORLD);)\n(36, MPI, MPI_Scatter(rtl, count / 2, MPI_INT, rtl, count / 2, MPI_INT, 0, MPI_COMM_WORLD);)\n(38, MPI, MPI_Gather(ltr, count / 2, MPI_INT, ltr, count / 2, MPI_INT, 0, MPI_COMM_WORLD);)\n(39, MPI, MPI_Gather(rtl, count / 2, MPI_INT, rtl, count / 2, MPI_INT, 0, MPI_COMM_WORLD);)", "label": "(6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (104, MPI, MPI_Barrier(MPI_COMM_WORLD);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "of A in the 3D grid. */\n104: \t    A3d->num_procs_to_send = 0;\n105: \t    A3d->procs_to_send_list = (int*) SUPERLU_MALLOC(npdep * sizeof(int));\n106: \t    A3d->send_count_list = (int*) SUPERLU_MALLOC(npdep * sizeof(int));\n107: \n108: \t    /* grid-0 processes send to all processes in 3D grid */\n109: \t    for (i = 0; i < npdep; ++i) {\n110: \t        A3d->procs_to_send_list[i] = i;\n111: \t        A3d->send_count_list[i] = b_counts_int[i];\n112: \t        A3d->num_procs_to_send++;\n113: \t    }\n114: \t}\n115: \n116: \tnum_procs_to_send = A3d->num_procs_to_send;\n117: \tprocs_to_send_list = A3d->procs_to_send_list;\n118: \tsend_count_list = A3d->send_count_list;\n119: \n120: \t// Set up the source processes to each destination process,\n121: \t// and the recv counts.\n122: \tnum_procs_to_recv = pxy;\n123: \tprocs_recv_from_list = (int*) SUPERLU_MALLOC(pxy * sizeof(int));\n124: \trecv_count_list = (int*) SUPERLU_MALLOC(pxy * sizeof(int));\n125: \n126: \tfor (i = 0; i < pxy; ++i) {\n127: \t\tprocs_recv_from_list[i] = i;\n128: \t\trecv_count_list[i] = nrhs * A3d->m_loc;\n129: \t}\n130: \n131: \t// Scatterv(B1, b_counts_int, b_disp, MPI_DOUBLE, Btmp, nrhs * A3d->m_loc, MPI_DOUBLE, 0, grid3d->zscp.comm);\n132: \tfor (i = 0; i < num_procs_to_send; ++i) {\n133: \t\tdest = procs_to_send_list[i];\n134: \t\tsrc", "label": "(270, MPI, MPI_Wait(&recv_req, &recv_status);)"}
{"predict": "12:           memcpy(ptr + sizeof(int), median, key_true_extent);\n113:         }\n114:       }\n115:       ptr += size_int_with_key;\n116:     }\n117: \n118:     /* sort my_num_with_median based on the new comparison operation */\n119:     DTCMP_Sort(my_num_with_median, ranks, type_int_with_key, cmp_int_with_key);\n120: \n121:     /* gather my_num_with_median to root rank */\n122:     lwgrp_comm_gather(my_num_with_median, ranks, type_int_with_key, 0, out_num_with_median, lwgcomm);\n123: \n124:     /* if we are the root rank, process the sorted list to compute\n125:      * median-of-medians */\n126:     if (lwgcomm->rank == 0) {\n127:       int i;\n128:       char* ptr = out_num_with_median;\n129:       for (i = 0; i < ranks; i++) {\n130:         int count = *(int*)ptr;\n131:         ptr += sizeof(int);\n132:         if (count > 0) {\n133:           /* if there is at least one element, copy in median element */\n134:           int median_index = (count / 2) + index[i];\n135:           if (median_index < n) {\n136:             /* TODO: for general key types, we need to use DTCMP_Memcpy here\n137:              * and adjust for lb */\n138:             const void* median = (char*)data + median_index * keysat_extent;\n139:             memcpy(weighted_median_scratch + i * size_int_with_key, median, key_true_extent);\n140:           }\n141:         }\n142:       }\n143:       DTCMP_Sort(weighted_median_scratch, ranks, type_int_with_key, cmp_int_with_key);\n144:       /* compute the median-of-medians */\n145:       int median_index = (ranks / 2) + index[0];\n146:       if (median_index < n) {\n147:", "label": "(24, MPI, MPI_Type_get_true_extent(key, &key_true_lb, &key_true_extent);); (204, MPI, MPI_Type_free(&type_int_with_key);); (280, MPI, MPI_Comm_rank(comm, &comm_rank);); (17, MPI, MPI_Type_get_extent(keysat, &keysat_lb, &keysat_extent);)"}
{"predict": "(7, MPI, MPI_Comm_dup(comm, &mycomm);)", "label": "(10, MPI, MPI_Comm_call_errhandler( comm, MPI_ERR_OTHER );); (8, MPI, MPI_Comm_set_errhandler( comm, newerr );); (14, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_create_errhandler( eh, &newerr );); (1, MPI, MPI_Init( &argc, &argv );); (6, MPI, MPI_Errhandler_free( &newerr );)"}
{"predict": "for elements to send\n89:     }\n90:   }\n91: \n92:   // Call the MPI exchange function to send and receive halo data\n93:   MPI_Request * requests = new MPI_Request[2 * sendList.size()];\n94:   MPI_Status * statuses = new MPI_Status[2 * sendList.size()];\n95:   MPI_Isend(sendBuffer, totalToBeSent, MPI_DOUBLE, neighbors, 0, MPI_COMM_WORLD, &requests[0]);\n96:   MPI_Irecv(receiveBuffer, totalToBeReceived, MPI_DOUBLE, neighbors, 0, MPI_COMM_WORLD, &requests[1]);\n97:   MPI_Waitall(2 * sendList.size(), requests, statuses);\n98:   MPI_Free_request(requests);\n99:   MPI_Free_request(statuses);\n100: \n101:   // Free memory used for building the communication lists\n102:   delete [] sendBuffer;\n103:   delete [] elementsToSend;\n104:   delete [] neighbors;\n105:   delete [] receiveLength;\n106:   delete [] sendLength;\n107: \n108:   // Exchange halo data\n109:   ExchangeHalo(A, receiveBuffer, sendBuffer, receiveLength, sendLength, neighbors, elementsToSend, externalToLocalMap);\n110: }", "label": "(11, OpenMP, #pragma omp parallel for)"}
{"predict": "(12, MPI, MPI_Comm_set_errhandler(parent, MPI_ERRORS_RETURN);)", "label": "(38, MPI, MPI_Comm_disconnect(&parent);); (27, MPI, err = MPI_Intercomm_merge(inter, 0, &merge1);); (34, MPI, MPI_Comm_free(&intra);); (31, MPI, MPI_Comm_free(&merge1);); (6, MPI, err = MPI_Intercomm_merge(parent, 1, &intra);); (30, MPI, MPI_Comm_free(&inter);); (25, MPI, err = MPI_Barrier(merge1);); (18, MPI, err = MPI_Barrier(inter);); (21, MPI, MPI_Comm_rank(merge1, &rank);); (7, MPI, err = MPI_Intercomm_create(intra, 0, MPI_COMM_NULL, 0, tag, &inter);); (1, MPI, MPI_Comm_rank(intra, &rank);); (10, MPI, err = MPI_Intercomm_create(MPI_COMM_WORLD, 0, intra, 0, tag, &inter);)"}
{"predict": "i < nprocs; i++) {\n117:             if (recv_size[i]) {\n118:                 *actual_recved_bytes += recv_size[i];\n119:             }\n120:         }\n121:     }\n122: \n123:     /* wait on the sends */\n124:     if (nprocs_send) {\n125: #ifdef NEEDS_MPI_TEST\n126:         int flag = 0;\n127:         while (!flag)\n128:             MPI_Testall(nprocs_send, requests + nprocs_recv, &flag, statuses);\n129: #else\n130:         MPI_Waitall(nprocs_send, requests + nprocs_recv, statuses);\n131: #endif\n132:     }\n133: \n134:     /* free recv_buf if it was allocated */\n135:     if (recv_buf) {\n136:         ADIOI_Free(recv_buf[0]);\n137:         ADIOI_Free(recv_buf);\n138:     }\n139: \n140:     /* free the requests */\n141:     ADIOI_Free(requests);\n142:     ADIOI_Free(statuses);\n143: }", "label": "(94, MPI, MPI_Type_commit(&send_type);); (137, MPI, MPI_Waitall(nprocs_send, requests + nprocs_recv, MPI_STATUSES_IGNORE);); (19, MPI, MPI_Alltoall(send_size, 1, MPI_COUNT, recv_size, 1, MPI_COUNT, fd->comm);); (118, MPI, MPI_Get_count_c(&statuses[j], MPI_BYTE, &count_recved);); (93, MPI, MPI_Type_free(&send_type);)"}
{"predict": "_info* [[VLA0]], i{{.+}} 0, i{{.+}} 2\n56:   // CHECK: store i32 0, i32* [[FLAGS_ADDR]],\n57:   // CHECK: [[BASE_ADDR:%.+]] = getelementptr inbounds %struct.kmp_depend_info, %struct.kmp_depend_info* [[VLA0]], i{{.+}} 1\n58:   // CHECK: [[A_ADDR_CAST:%.+]] = ptrtoint i8* [[D_ADDR]] to i64\n59:   // CHECK: store i64 [[A_ADDR_CAST]], i64* [[BASE_ADDR]],\n60:   // CHECK: [[SIZE_ADDR:%.+]] = getelementptr inbounds %struct.kmp_depend_info, %struct.kmp_depend_info* [[VLA0]], i{{.+}} 1, i{{.+}} 1\n61:   // CHECK: store i64 4, i64* [[SIZE_ADDR]],\n62:   // CHECK: [[FLAGS_ADDR:%.+]] = getelementptr inbounds %struct.kmp_depend_info, %struct.kmp_depend_info* [[VLA0]], i{{.+}} 1, i{{.+}} 2\n63:   // CHECK: store i32 0, i32* [[FLAGS_ADDR]],\n64:   // CHECK: [[BASE_ADDR:%.+]] = getelementptr inbounds %struct.kmp_depend_info, %struct.kmp_depend_info* [[VLA0]], i{{.+}} 2\n65:   // CHECK: [[A_ADDR_CAST:%.+]] = ptrtoint i8* [[X_ADDR]] to i64\n66:   // CHECK: store i64 [[A_ADDR_CAST]], i64* [[BASE_ADDR]],\n67:   // CHECK: [[SIZE_ADDR:%.+]] = getelementptr inbounds %struct.kmp_depend_info, %struct.kmp_depend_info* [[VLA0]], i{{.+}} 2, i{{.+}} 1\n68:   // CHECK: store i64 4, i64* [[SIZE_ADDR]],\n69:   // CHECK: [[FLAGS_ADDR:%.+]] = getelementptr inbounds %struct.kmp_depend_info, %struct.kmp_depend_info* [[VLA0]], i{{.+}} 2, i{{.+}}", "label": "(101, OpenMP, #pragma omp task); (102, OpenMP, #pragma omp taskgroup)"}
{"predict": "(12, CUDA, kernel_dot<<<m, DOT_BLOCK_SIZE>>>(n, static_cast<thrust::complex<double> const>(alpha), reinterpret_cast<thrust::complex<double> const*>(A), lda, reinterpret_cast<thrust::complex<double> const*>(B), ldb, static_cast<thrust::complex<double> const>(beta), reinterpret_cast<thrust::complex<double>*>(y), incy);)", "label": "(17, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize());); (15, CUDA, qmc_cuda::cuda_check(cudaGetLastError());)"}
{"predict": "(25, MPI, MPI_Allreduce(&local, &global, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);)", "label": "(28, MPI, MPI_Finalize();); (8, MPI, MPI_Get_processor_name(mach_name,&mach_len);); (22, MPI, MPI_Allreduce(&local,&global,1,MPI_INT,MPI_SUM,MPI_COMM_WORLD);)"}
{"predict": "(2, OpenMP, #pragma omp parallel for)", "label": "(3, OpenMP, #pragma omp distribute parallel for if (true)); (11, OpenMP, #pragma omp distribute parallel for if (false)); (19, OpenMP, #pragma omp distribute parallel for if (Arg)); (-2, OpenMP, #pragma omp target); (-2, OpenMP, #pragma omp teams)"}
{"predict": "end1; ++I)\n92:     ++I;\n93: }\n\nAssistant:\n(9, OpenMP, #pragma omp for;)\n(11, OpenMP, #pragma omp for;)\n(15, OpenMP, #pragma omp for;)\n(16, OpenMP, #pragma omp for;)\n(17, OpenMP, #pragma omp for;)\n(18, OpenMP, #pragma omp for;)\n(19, OpenMP, #pragma omp for;)\n(20, OpenMP, #pragma omp for;)\n(21, OpenMP, #pragma omp for;)\n(22, OpenMP, #pragma omp for;)\n(23, OpenMP, #pragma omp for;)\n(24, OpenMP, #pragma omp for;)\n(25, OpenMP, #pragma omp for;)\n(26, OpenMP, #pragma omp for;)\n(30, OpenMP, #pragma omp for;)\n(36, OpenMP, #pragma omp for;)\n(39, OpenMP, #pragma omp for;)\n(42, OpenMP, #pragma omp for;)\n(45, OpenMP, #pragma omp for;)\n(49, OpenMP, #pragma omp for;)\n(52, OpenMP, #pragma omp for;)\n(54, OpenMP, #pragma omp for;)\n(60, OpenMP, #pragma omp for;)\n(62, OpenMP, #pragma omp for;)\n(66, OpenMP, #pragma omp for;)\n(70, OpenMP, #pragma omp for;)\n(72, OpenMP, #pragma omp for;)\n(74, OpenMP, #pragma omp for;)\n(76, OpenMP, #pragma omp for;)\n(80, OpenMP, #pragma omp for;)\n(85, OpenMP, #pragma omp for;)", "label": "(3, OpenMP, #pragma omp parallel for)"}
{"predict": "(14, OpenCL, retVal = this->pCmdQ->enqueueSVMMemcpy(false, pDstSVM, pSrcSVM, 256, 0, nullptr, &event, nullptr);)", "label": "(19, OpenCL, clReleaseEvent(event);)"}
{"predict": "/* set up the error handler */\n145:         if ( H5Eset_auto2(max_nerrors, &err_func, &err_client_data) < 0 ) {\n146:             nerrors++;\n147:             if ( verbose ) {\n148:                 HDfprintf(stdout, \"%d:%s: H5Eset_auto2() failed.\\n\",\n149:                           world_mpi_rank, fcn_name);\n150:             }\n151:         }\n152:     }\n153: \n154:     if ( MAINPROCESS ) {\n155:         printf(\"===================================\\n\");\n156:         printf(\"Metadata cache tests\\n\");\n157:         printf(\"===================================\\n\");\n158:     }\n159: \n160:     /* set up the cache parameters */\n161:     cache_params.type = H5C__L2_CACHE;\n162:     cache_params.set_initial_size = TRUE;\n163:     cache_params.initial_size = 1000000;\n164:     cache_params.set_max_size = TRUE;\n165:     cache_params.max_size = 1000000;\n166:     cache_params.set_min_clean_size = TRUE;\n167:     cache_params.min_clean_size = 1000000;\n168:     cache_params.set_rd_miss_inc = TRUE;\n169:     cache_params.rd_miss_inc = 100;\n170:     cache_params.set_wr_miss_inc = TRUE;\n171:     cache_params.wr_miss_inc = 100;\n172:     cache_params.set_rd_hit_inc = TRUE;\n173:     cache_params.rd_hit_inc = 100;\n174:     cache_params.set_wr_hit_inc = TRUE;\n175:     cache_params.wr_hit_inc = 100;\n176:     cache_params.set_clean_percent = TRUE;\n177:     cache_params.clean_percent = 50;\n178:     cache_params.set_eviction_percent = TRUE;\n179:     cache_params.eviction_percent = 50;\n180:     cache_params.set_xfer_mode = TRUE;\n181:     cache_params.xfer_mode = H5C__XFER_PARALLEL;", "label": "(10, MPI, MPI_Init(&argc, &argv);); (206, MPI, MPI_Finalize();); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (185, MPI, MPI_Barrier(MPI_COMM_WORLD);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);)"}
{"predict": "(9, OpenMP, #pragma omp parallel for private(vec, s_arr))", "label": "(8, OpenMP, #pragma omp master taskloop lastprivate(t_var, vec, s_arr, s_arr, var, var))"}
{"predict": "(7, OpenMP, #pragma omp parallel for private(i))", "label": "(11, OpenMP, #pragma omp target teams distribute parallel for map(tofrom : a[ : n]) map(to : b[ : n]))"}
{"predict": "(14, OpenMP, #pragma omp target update to(i);)", "label": "(8, OpenMP, #pragma omp target update CLAUSE(present: i)); (6, OpenMP, #pragma omp target enter data map(alloc: i)); (7, OpenMP, #pragma omp target exit data map(delete: i))"}
{"predict": "[[UB:%.+]],\n65:   // CHECK: store i64 0, i64* [[I:%.+]],\n66:   // CHECK: [[NUM_ITERS_VAL:%.+]] = load i64, i64* [[NUM_ITERS]],\n67:   // CHECK: store i64 [[NUM_ITERS_VAL]], i64* [[STEP:%.+]],\n68:   // CHECK: br label %[[LOOP_START:[^,]+]]\n69:   // CHECK: [[LOOP_START]]:\n70:   // CHECK: [[I:%.+]] = load i64, i64* [[I]],\n71:   // CHECK: [[CMP:%.+]] = icmp ult i64 [[I]], [[UB]]\n72:   // CHECK: br i1 [[CMP]], label %[[LOOP_BODY:[^,]+]], label %[[LOOP_END:[^,]+]]\n73:   // CHECK: [[LOOP_BODY]]:\n74:   // CHECK: [[I:%.+]] = load i64, i64* [[I]],\n75:   // CHECK: [[STEP:%.+]] = load i64, i64* [[STEP]],\n76:   // CHECK: [[ADD:%.+]] = add i64 [[I]], [[STEP]]\n77:   // CHECK: store i64 [[ADD]], i64* [[I]],\n78:   // CHECK: br label %[[LOOP_START]]\n79:   // CHECK: [[LOOP_END]]:\n80:   // CHECK: ret void\n81: }\n82: \n83: void loop_with_counter_collapse_no_loop() {\n84:   // CHECK: store i32 0, i32* [[I_TMP:%.+]],\n85:   // CHECK: [[VAL:%.+]] = load i32, i32* [[I_TMP]],\n86:   // CHECK: store i32 [[VAL]], i32* [[J_LB_MIN:%.+]],\n87:   // CHECK: store i32 3, i32* [[I_TMP]],\n88:   // CHECK: [[VAL:%.+]] = load i32, i32* [[I_TMP]],\n89:   // CHECK: store i32 [[VAL]], i32* [[J_LB_MAX:%.+]],\n90:   // CHECK", "label": "(163, OpenMP, #pragma omp for collapse(2))"}
{"predict": "(35, OpenMP, #pragma omp parallel for private(k);)", "label": "(38, OpenMP, #pragma omp distribute parallel for firstprivate(argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (78, OpenMP, #pragma omp parallel private(i)); (56, OpenMP, #pragma omp distribute parallel for firstprivate(e, g)); (30, OpenMP, #pragma omp distribute parallel for firstprivate(argc > 0 ? argv[1] : argv[2])); (68, OpenMP, #pragma omp distribute parallel for firstprivate(i)); (1, OpenMP, #pragma omp target); (57, OpenMP, #pragma omp distribute parallel for firstprivate(k, h)); (59, OpenMP, #pragma omp parallel); (40, OpenMP, #pragma omp distribute parallel for firstprivate(a, b)); (93, OpenMP, #pragma omp parallel reduction(+ : i)); (3, OpenMP, #pragma omp distribute parallel for firstprivate(); (71, OpenMP, #pragma omp distribute parallel for firstprivate(j)); (41, OpenMP, #pragma omp distribute parallel for firstprivate(argv[1])); (5, OpenMP, #pragma omp distribute parallel for firstprivate()); (9, OpenMP, #pragma omp distribute parallel for firstprivate(argc); (78, OpenMP, #pragma omp distribute parallel for lastprivate(g) firstprivate(g)); (27, OpenMP, #pragma omp distribute parallel for firstprivate(S1)); (11, OpenMP, #pragma omp distribute parallel for firstprivate(argc,); (60, OpenMP, #pragma omp parallel shared(i)); (-11, OpenMP, #pragma omp distribute parallel for firstprivate); (-13, OpenMP, #pragma omp teams)"}
{"predict": "(27, MPI, MPI_Comm_free(&comms[x]);)", "label": "(25, MPI, MPI_Comm_free(&comms[x]);); (18, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &comms[x]);)"}
{"predict": "(11, OpenCL, cl::Context context({device0, device1}, props);)", "label": "(5, OpenCL, clReleaseContext_ExpectAndReturn(make_context(0), CL_SUCCESS);); (3, OpenCL, clCreateContext_StubWithCallback(clCreateContext_testContextNonNullProperties);)"}
{"predict": "(10, OpenMP, #pragma omp parallel for private(i))", "label": "(5, OpenMP, #pragma omp sections firstprivate(); (24, OpenMP, #pragma omp sections firstprivate()); (35, OpenMP, #pragma omp sections firstprivate(0)); (51, OpenMP, #pragma omp sections lastprivate(x, y, z) firstprivate(x, y, z)); (45, OpenMP, #pragma omp sections lastprivate(x, y) firstprivate(x, y)); (39, OpenMP, #pragma omp sections lastprivate(x) firstprivate(x)); (-4, OpenMP, #pragma omp parallel); (6, OpenMP, #pragma omp sections firstprivate(,); (11, OpenMP, #pragma omp sections firstprivate(, )); (22, OpenMP, #pragma omp sections firstprivate(int))"}
{"predict": "(4, OpenMP, #pragma omp parallel for private(i))", "label": "(10, OpenMP, #pragma omp target update to(i) nowait (3.14) device (-10u)); (3, OpenMP, #pragma omp target nowait update to(i)); (7, OpenMP, #pragma omp target update to(i) nowait device (-10u)); (5, OpenMP, #pragma omp target update to(i) nowait (argc))); (7, OpenMP, #pragma omp target update to(i) nowait nowait); (7, OpenMP, #pragma omp target update nowait to(i) nowait); (-3, OpenMP, #pragma omp nowait target update to(i)); (-1, OpenMP, #pragma omp target update nowait() to(i)); (-1, OpenMP, #pragma omp target update to(i) nowait()"}
{"predict": "(13, OpenMP, #pragma omp parallel for)", "label": "(7, OpenACC, #pragma acc loop gang worker vector); (5, OpenACC, #pragma acc parallel deviceptr(position,cell_keys))"}
{"predict": "(14, TBB, parallel_for(blocked_range<int>(0, Count), [&](int i) { serial_queue_helper<SIZE,IType>::put_one_queue_val(i,my_indexer); }));", "label": "(6, TBB, tbb::flow::queue_node< q3_input_type >  q3(g);); (11, TBB, ASSERT(tbb::flow::input_port<0>(my_indexer).predecessor_count() == 0, NULL);); (7, TBB, tbb::flow::make_edge(my_indexer, q3);)"}
{"predict": "116:             __TBB_Yield();\n117:     }\n118:     {\n119:         tbb::task_scheduler_init tsi(1, 0, /*blocking=*/true);\n120:         tbb::atomic<int> flag;\n121:         flag = 0;\n122:         {\n123:             tbb::task_scheduler_init tsi(1, 0, /*blocking=*/true);\n124: \n125:             for (int i=0; i<10; i++) {\n126:                 FFTask* t = new( tbb::task::allocate_root() ) FFTask(&flag);\n127:                 tbb::task::enqueue(*t);\n128:             }\n129:         }\n130:         ASSERT(flag==10, \"The tasks must be terminated when task_scheduler_init destroyed.\");\n131:     }\n132:     {\n133:         tbb::task_scheduler_init tsi(1, 0, /*blocking=*/true);\n134:         Harness::SpinBarrier barr(threads);\n135:         RunWorkersLimited(1, 1, false);\n136: \n137:         NativeParallelFor( threads, FFTasksRun(&barr, &counter) );\n138:     }\n139:     ASSERT(counter == threads*FFTasksRun::ITERS, \"All tasks must be done when task_scheduler_init destroyed.\");\n140:     counter = 0;\n141:     {\n142:         tbb::task_scheduler_init tsi(1, 0, /*blocking=*/true);\n143:         Harness::SpinBarrier barr(threads);\n144:         RunWorkersLimited(1, 1, false);\n145: \n146:         NativeParallelFor( threads, FFTasksRun(&barr, &counter) );\n147:     }\n148:     ASSERT(counter == threads*FFTasksRun::ITERS, \"All tasks must be done when task_scheduler_init destroyed.\");\n149:     counter = 0;\n150:     {\n151:         tbb::task_scheduler_init tsi(1, 0, /*blocking=*/true);\n152:         Harness::SpinBarrier barr(threads);\n1", "label": "(5, TBB, tbb::atomic<bool> taskDoneFlag;); (85, TBB, tbb::task::enqueue(*t, p);); (9, TBB, tbb::task::enqueue(*w);); (13, TBB, FFTask* t = new( tbb::task::allocate_root() ) FFTask(&flag);); (81, TBB, const tbb::priority_t p = getPriorityByInt(i);); (98, TBB, tbb::atomic<int> counter;); (11, TBB, tbb::task::enqueue(*t);); (108, TBB, tbb::atomic<bool> signalToLeave;); (-4, TBB, tbb::atomic<int> flag;); (1, TBB, WaiterTask* w = new( tbb::task::allocate_root() ) WaiterTask(&taskDoneFlag);); (3, TBB, tbb::global_control c(tbb::global_control::max_allowed_parallelism, 1);)"}
{"predict": "(7, MPI, MPI_Comm_rank(comm, &par_rank);) (8, MPI, MPI_Comm_size(comm, &par_size);)", "label": "(8, MPI, MPI_Comm_rank(comm, &par_rank);); (24, MPI, open_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (7, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(10, OpenMP, #pragma omp parallel for)", "label": "(5, OpenMP, #pragma omp workshare)"}
{"predict": "(7, OpenCL, retVal = clEnqueueWriteBuffer(queue, buffer, CL_TRUE, 0, dataSize, input.data(), 0, nullptr, nullptr);)", "label": "(6, OpenCL, retVal = clEnqueueMemcpyINTEL(this->pCmdQ, true, unifiedMemoryPtr, input.data(), dataSize, 0, nullptr, nullptr);)"}
{"predict": "64:     foo();\n65:   for (int i = 0; i < 10; ++i)\n66:     foo();\n67:   for (int i = 0; i < 10; ++i)\n68:     foo();\n69:   for (int i = 0; i < 10; ++i)\n70:     foo();\n71:   for (int i = 0; i < 10; ++i)\n72:     foo();\n73:   for (int i = 0; i < 10; ++i)\n74:     foo();\n75:   for (int i = 0; i < 10; ++i)\n76:     foo();\n77:   for (int i = 0; i < 10; ++i)\n78:     foo();\n79:   for (int i = 0; i < 10; ++i)\n80:     foo();\n81:   for (int i = 0; i < 10; ++i)\n82:     foo();\n83:   for (int i = 0; i < 10; ++i)\n84:     foo();\n85:   for (int i = 0; i < 10; ++i)\n86:     foo();\n87:   for (int i = 0; i < 10; ++i)\n88:     foo();\n89:   for (int i = 0; i < 10; ++i)\n90:     foo();\n91:   for (int i = 0; i < 10; ++i)\n92:     foo();\n93:   for (int i = 0; i < 10; ++i)\n94:     foo();\n95:   for (int i = 0; i < 10; ++i)\n96:     foo();\n97:   for (int i = 0; i < 10; ++i)\n98:     foo();\n99:   for (int i = 0; i < 10; ++i)\n100:    foo();\n101:   for (int i = 0; i < 10;", "label": "(15, OpenMP, #pragma omp master taskloop reduction +); (65, OpenMP, #pragma omp master taskloop reduction(- : da)); (61, OpenMP, #pragma omp master taskloop reduction(* : ca)); (54, OpenMP, #pragma omp master taskloop reduction(max : h.b)); (47, OpenMP, #pragma omp master taskloop reduction(+ : a, b, c, d, f)); (55, OpenMP, #pragma omp master taskloop reduction(+ : ba)); (30, OpenMP, #pragma omp master taskloop reduction(| : argc,); (84, OpenMP, #pragma omp master taskloop reduction(+ : p), reduction(+ : p)); (46, OpenMP, #pragma omp master taskloop reduction(min : a, b, c, d, f)); (33, OpenMP, #pragma omp master taskloop reduction(~ : argc)); (38, OpenMP, #pragma omp master taskloop reduction(^ : S1)); (73, OpenMP, #pragma omp master taskloop reduction(+ : o)); (90, OpenMP, #pragma omp master taskloop reduction(max : j)); (11, OpenMP, #pragma omp master taskloop reduction()); (95, OpenMP, #pragma omp parallel reduction(* : fl)); (63, OpenMP, #pragma omp master taskloop reduction(& : e, g)); (65, OpenMP, #pragma omp master taskloop reduction(+ : h, k, B::x)); (73, OpenMP, #pragma omp parallel private(k)); (96, OpenMP, #pragma omp master taskloop reduction(+ : m)); (20, OpenMP, #pragma omp master taskloop reduction(|| : argc > 0 ? argv[1] : argv[2])); (-2, OpenMP, #pragma omp master taskloop reduction(); (24, OpenMP, #pragma omp master taskloop reduction(&& : argc, z)); (53, OpenMP, #pragma omp master taskloop reduction(&& : S2::S2sc)); (46, OpenMP, #pragma omp master taskloop reduction(^ : fl)); (6, OpenMP, #pragma omp master taskloop reduction(\\)); (-13, OpenMP, #pragma omp master taskloop reduction); (7, OpenMP, #pragma omp master taskloop reduction(foo : argc); (74, OpenMP, #pragma omp parallel reduction(min : i)); (77, OpenMP, #pragma omp parallel private(fl)); (91, OpenMP, #pragma omp master taskloop reduction(task, + : m)); (-9, OpenMP, #pragma omp master taskloop reduction(-); (86, OpenMP, #pragma omp master taskloop nogroup reduction(+ : m)); (65, OpenMP, #pragma omp master taskloop reduction(+ : r)); (-6, OpenMP, #pragma omp master taskloop reduction(*)); (53, OpenMP, #pragma omp master taskloop private(i), reduction(+ : j), reduction(+ : q)); (65, OpenMP, #pragma omp parallel shared(i)); (36, OpenMP, #pragma omp master taskloop reduction(&& : S2::S2s)); (69, OpenMP, #pragma omp master taskloop reduction(+ : fl))"}
{"predict": "0;\n83:     pillarOutSM[pillar_idx_inBlock][point_idx][4] = 0;\n84:     pillarOutSM[pillar_idx_inBlock][point_idx][5] = 0;\n85:     pillarOutSM[pillar_idx_inBlock][point_idx][6] = 0;\n86:     pillarOutSM[pillar_idx_inBlock][point_idx][7] = 0;\n87:     pillarOutSM[pillar_idx_inBlock][point_idx][8] = 0;\n88:     pillarOutSM[pillar_idx_inBlock][point_idx][9] = 0;\n89:   }\n90: }\n\nOutput:\n(1, CUDA, __global__ void generateFeatures_kernel_4x(\n91:   int batch_size,\n92:   int dense_pillar_num,\n93:   float* voxel_features,\n94:   unsigned int* voxel_num_points, unsigned int* coords,\n95:   unsigned int *params,\n96:   float voxel_x, float voxel_y, float voxel_z,\n97:   float range_min_x, float range_min_y, float range_min_z,\n98:   unsigned int voxel_features_size, unsigned int max_points,\n99:   unsigned int max_voxels,\n100:  float* features)\n101: {\n102:   int warp_size = max_points;\n103:   int pillar_idx = blockIdx.x * 4 + threadIdx.x / warp_size;\n104:   int point_idx = threadIdx.x % warp_size;\n105:   // In case the actual number of points is less than warp_size\n106:   // E.g., warp_size=32, max_points=20\n107:   // E.g., warp_size=32, max_points=20\n108:   if (point_idx >= max_points) return;\n109:   int batch_idx = pillar_idx / max_voxels;\n110:   if (batch_idx >= batch_size) return;\n111:   int pillar_idx_in_frame = pillar_idx % max_voxels;\n112:   int dense_pillar_idx = pillar_idx_in_frame + dense_pillar_num * batch_idx;\n113:   int pillar_idx_inBlock = threadIdx.x / warp_size", "label": "(34, CUDA, __shared__ int pointsNumSM[4];); (30, CUDA, __shared__ float4 pillarSM[4][64];); (30, CUDA, __shared__ float4 pillarSumSM[4];); (30, CUDA, __shared__ int4 cordsSM[4];); (31, CUDA, __shared__ float pillarOutSM[4][64][11];)"}
{"predict": "(11, OpenMP, #pragma omp parallel for private(i))", "label": "(1, OpenMP, omp_allocator_handle_t default_alloc = omp_get_default_allocator();); (4, OpenMP, int *p = (int *)omp_alloc(123456*sizeof(int));)"}
{"predict": "Q:%.+]] = icmp eq float* [[A_BUF_IDX_SUB_K2POW]], [[A_BUF_END]]\n68:     // CHECK: br i1 [[ISEQ]], label %[[INNER_BODY_2:[^,]+]], label %[[INNER_BODY_3:[^,]+]]\n69:     // CHECK: [[INNER_BODY_2]]:\n70:     // CHECK: call void @__kmpc_for_static_init_4(\n71:     // CHECK: call i8* @llvm.stacksave()\n72:     // CHECK: [[BASE_IDX_I:%.+]] = load i32, i32* [[IV_ADDR:%.+]],\n73:     // CHECK: [[BASE_IDX:%.+]] = zext i32 [[BASE_IDX_I]] to i64\n74:     // CHECK: [[IDX:%.+]] = mul nsw i64 [[BASE_IDX]], [[NUM_ELEMS]]\n75:     // CHECK: [[A_BUF_IDX:%.+]] = getelementptr inbounds float, float* [[A_BUF]], i64 [[IDX]]\n76:     // CHECK: [[A_PRIV:%.+]] = getelementptr inbounds [10 x float], [10 x float]* [[A_PRIV_ADDR:%.+]], i64 0, i64 0\n77:     // CHECK: [[BYTES:%.+]] = mul nuw i64 [[NUM_ELEMS:%.+]], 4\n78:     // CHECK: [[DEST:%.+]] = bitcast float* [[A_BUF_IDX]] to i8*\n79:     // CHECK: [[SRC:%.+]] = bitcast float* [[A_PRIV]] to i8*\n80:     // CHECK: call void @llvm.memcpy.p0i8.p0i8.i64(i8* {{.*}}[[DEST]], i8* {{.*}}[[SRC]], i64 [[BYTES]], i1 false)\n81:     // CHECK: br label %[[DISPATCH:[^,]+]]\n82:     // CHECK: [[INPUT_PHASE:.+]]:\n83:     // CHECK: call void @{{.+}}foo{{.+}}()\n84:     // CHECK: [[DISPATCH]]:\n85:     // CHECK: br label %[[INPUT_PHASE]]\n86:     // CHECK: [[LOOP_CONTINUE]]:\n87:     // CHECK: call void @llvm", "label": "(194, OpenMP, #pragma omp scan exclusive(a[:n], b)); (44, OpenMP, #pragma omp scan inclusive(a[:n], b)); (9, OpenMP, #pragma omp for reduction(inscan, +:a[:n], b))"}
{"predict": "(22, MPI, MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_CREATE | MPI_MODE_WRONLY, MPI_INFO_NULL, &fh);) (28, MPI, MPI_File_write(fh, buf, buflen, MPI_INT, &status);) (31, MPI, MPI_File_close(&fh);) (62, MPI, MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_RDONLY, MPI_INFO_NULL, &fh);) (66, MPI, MPI_File_read(fh, buf, buflen, MPI_INT, &status);) (69, MPI, MPI_File_close(&fh);)", "label": "(11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &np);); (68, MPI, err = MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_WRONLY, MPI_INFO_NULL, &fh);); (73, MPI, err = MPI_File_write_all(fh, buf, buflen, MPI_INT, &status);); (29, MPI, err = MPI_File_close(&fh);); (60, MPI, MPI_Type_commit(&filetype);); (24, MPI, err = MPI_File_write(fh, buf, MAXLEN, MPI_INT, &status);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (65, MPI, err = MPI_File_set_view(fh, 0, MPI_INT, filetype, \"native\", MPI_INFO_NULL);); (45, MPI, MPI_Type_create_hindexed(num, blocklen, addrs, MPI_INT, &filetype);); (76, MPI, err = MPI_File_open(MPI_COMM_SELF, filename, MPI_MODE_RDONLY, MPI_INFO_NULL, &fh);); (40, MPI, MPI_Type_indexed(num, blocklen, indices, MPI_INT, &filetype);); (67, MPI, MPI_Type_free(&filetype);); (76, MPI, err = MPI_File_read(fh, buf, MAXLEN, MPI_INT, &status);); (23, MPI, MPI_Barrier(MPI_COMM_WORLD);); (42, MPI, MPI_Type_create_struct(num, blocklen, addrs, types, &filetype);)"}
{"predict": "(17, OpenMP, #pragma omp parallel for)", "label": "(5, OpenMP, #pragma omp parallel sections default()); (2, OpenMP, #pragma omp parallel sections default(); (7, OpenMP, #pragma omp parallel sections default(shared), default(shared)); (4, OpenMP, #pragma omp parallel sections default(none); (7, OpenMP, #pragma omp parallel sections default(x)); (16, OpenMP, #pragma omp parallel sections default(none)); (-5, OpenMP, #pragma omp parallel sections default); (21, OpenMP, #pragma omp parallel sections default(shared))"}
{"predict": "r_c[tm][tn] += r_comp_a[tm] * r_comp_b[tn];\n96:             }\n97:         }\n98:     }\n99: \n100:     __syncthreads();\n101: \n102:     int c_gmem_addr = OFFSET(by * BM + ty * TM, bx * BN + tx * TN, N);\n103:     int c_smem_addr = ty * TM * TN + tx * TN;\n104: \n105:     c[c_gmem_addr] = r_c[ty][tx];\n106: }\n\nOutput:", "label": "(16, CUDA, __shared__ float s_a[2][BK][BM];); (16, CUDA, __shared__ float s_b[2][BK][BN];)"}
{"predict": "(23, OpenMP, #pragma omp parallel for private(thread_id))", "label": "(17, OpenMP, volatile  int_t thread_id = omp_get_thread_num();)"}
{"predict": "(5, OpenMP, #pragma omp parallel for reduction(+:expected_value))", "label": "(8, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (9, TBB, tbb::tick_count t1 = tbb::tick_count::now();); (7, TBB, tbb::parallel_for(tbb::blocked_range<size_t>(0,n,n/100),TwiddleInvariant<Invariant<M,8> >(invariant));)"}
{"predict": "(17, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(19, MPI, MPI_Info_set(info, \"cb_nodes\", \"1\");); (25, MPI, errcode = MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_WRONLY | MPI_MODE_CREATE, info, &fh);); (27, MPI, errcode = MPI_File_iwrite_at_all(fh, off, buffer, BUFSIZE, MPI_INT, &request);); (30, MPI, errcode = MPI_File_close(&fh);); (28, MPI, MPI_Wait(&request, &status);); (12, MPI, MPI_Info_create(&info);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (33, MPI, errcode = MPI_File_iread_at_all(fh, off, buf2, BUFSIZE, MPI_INT, &request);); (10, MPI, MPI_Info_set(info, \"romio_cb_write\", \"enable\");); (43, MPI, MPI_Info_free(&info);); (27, MPI, errcode = MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_RDONLY, info, &fh);)"}
{"predict": "(111, OpenMP, #pragma omp critical)", "label": "(109, OpenMP, #pragma omp barrier); (75, OpenMP, nthread = omp_get_num_threads();); (92, OpenMP, #pragma omp for); (71, OpenMP, #pragma omp master); (68, OpenMP, #pragma omp parallel private(j,iter))"}
{"predict": "(19, OpenMP, #pragma omp parallel for)", "label": "(14, OpenMP, #pragma omp target parallel for)"}
{"predict": "(17, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(16, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &scri_rank);); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &scri_ranks);)"}
{"predict": "inputValues[ i ] = random_in_range( formatMin, (int)formatMax, d );\n104:                     }\n105:                 }\n106:             }\n107:         }\n108: \n109:         for( size_t mem_flag_index = 0; mem_flag_index < num_flags; mem_flag_index++ )\n110:         {\n111:             cl_mem_flags mem_flags = mem_flag_types[ mem_flag_index ];\n112:             const char *mem_flag_name = mem_flag_names[ mem_flag_index ];\n113:             cl_mem mem_obj = NULL;\n114:             cl_int error;\n115:             cl_image_format image_format = *imageInfo->format;\n116:             image_format.image_channel_order = CL_RGBA;\n117:             image_format.image_channel_data_type = CL_UNORM_INT8;\n118: \n119:             mem_obj = clCreateImage( context, mem_flags, &image_format, NULL, imageValues, &error );\n120:             if( error != CL_SUCCESS )\n121:             {\n122:                 if( mem_flags == CL_MEM_WRITE_ONLY )\n123:                 {\n124:                     error = clCreateImage( context, CL_MEM_READ_WRITE, &image_format, NULL, imageValues, &error );\n125:                     if( error != CL_SUCCESS )\n126:                         return 1;\n127:                 }\n128:                 else\n129:                     return 1;\n130:             }\n131: \n132:             if( !verifyRounding )\n133:             {\n134:                 error = clSetKernelArg( kernel, 0, sizeof( cl_mem ), &mem_obj );\n135:                 if( error != CL_SUCCESS )\n136:                     return 1;\n137:             }\n138: \n139:             error = clEnqueueWriteImage( queue, mem_obj, CL_TRUE, origin, region, 0, 0, imageValues, 0, NULL, NULL );\n140:             if( error != CL_SUCCESS )\n141:                 return 1;\n142: \n143:             error = clEnqueueReadImage( queue, mem_obj, CL", "label": "(242, OpenCL, error = clSetKernelArg( kernel, 0, sizeof( cl_mem ), &inputStream );); (228, OpenCL, error = clSetKernelArg( kernel, 2, sizeof( int ), &lod );); (215, OpenCL, error = clSetKernelArg( kernel, 1, sizeof( cl_mem ), &image );); (244, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 1, NULL, threads, NULL, 0, NULL, NULL );); (260, OpenCL, error = clEnqueueReadImage( queue, image, CL_TRUE, origin, region, gEnablePitch ? imageInfo->rowPitch : 0, 0, resultValues, 0, NULL, NULL );)"}
{"predict": ":     checkGPU(gpuFree(Llu->Lnzval_bc_ptr));\n116:     checkGPU(gpuFree(Llu->Lnzval_br_ptr));\n117:     checkGPU(gpuFree(Llu->Unzval_bc_ptr));\n118:     checkGPU(gpuFree(Llu->Unzval_br_ptr));\n119:     checkGPU(gpuFree(Llu->Ucb_indptr));\n120:     checkGPU(gpuFree(Llu->Ucb_valptr));\n121:     checkGPU(gpuFree(Llu->Urbs));\n122: }\n123: #endif\n124: }\n\nOutput:", "label": "(9, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &iam );)"}
{"predict": "(30, MPI, if (c_info->File_rank == 0) { MPI_Barrier(c_info->Comm, MPI_COMM_WORLD); }); (33, MPI, MPI_Request request; MPI_Irecv(shared, size, MPI_BYTE, c_info->File_rank, 0, c_info->Comm, &request); MPI_Wait(&request, MPI_STATUS_IGNORE); ); (40, MPI, MPI_Request request; MPI_Irecv(shared, size, MPI_BYTE, c_info->File_rank, 0, c_info->Comm, &request); MPI_Wait(&request, MPI_STATUS_IGNORE); )", "label": "(36, MPI, MPI_File_close(&c_info->fh);)"}
{"predict": "(10, OpenMP, #pragma omp parallel for)", "label": "(8, OpenMP, #pragma omp parallel); (10, OpenMP, #pragma omp for); (14, OpenMP, #pragma omp critical)"}
{"predict": "(28, OpenMP, gs = (argc > 3) ? std::atoi(argv[3]) : 1;)\n(30, OpenMP, if (gs < 1 || gs > length) { throw \"ERROR: grainsize\"; })", "label": "(38, OpenMP, std::cout << \"Number of threads    = \" << omp_get_max_threads() << std::endl;)"}
{"predict": "(10, CUDA, cudaMallocManaged(reinterpret_cast<void **>(&d_data), sizeof(data_type) * h_data.size());)", "label": "(32, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (34, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(9, SYCL, a[idx] = Aval * static_cast<double>(idx[1] + 1);)\n(18, SYCL, b[idx] = static_cast<double>(idx[1] + 1) * Bval * static_cast<double>(idx[0] + 1);)", "label": "(16, SYCL, auto b = B.get_access<sycl::access_mode::write>(cgh);); (5, SYCL, auto a = A.get_access<sycl::access_mode::write>(cgh);)"}
{"predict": "(6, OpenMP, #pragma omp parallel for) (9, OpenMP, #pragma omp parallel for) (12, OpenMP, #pragma omp parallel for)", "label": "(20, OpenCL, clReleaseMemObject(mobjA_);); (16, OpenCL, clReleaseMemObject(mobjX_);)"}
{"predict": "(24, OpenMP, #pragma omp parallel for)\n(46, OpenMP, #pragma omp parallel for)\n(68, OpenMP, #pragma omp parallel for)", "label": "(4, TBB, tbb::flow::queue_node<T> q2(g);); (70, TBB, tbb::flow::make_edge( q2, q3 );); (67, TBB, tbb::flow::queue_node<T> q3(g);); (37, TBB, tbb::flow::make_edge( q, q2 );); (56, TBB, tbb::flow::remove_edge( q, q2 );); (87, TBB, tbb::flow::remove_edge( q,  q2 );); (-3, TBB, tbb::flow::queue_node<T> q(g);); (-5, TBB, tbb::flow::graph g;)"}
{"predict": "(14, OpenMP, #pragma omp parallel for)", "label": "(14, OpenCL, cl_command_queue cmdQ = clCreateCommandQueueWithProperties(context.get(), clDevice, properties, &retVal);); (38, OpenCL, retVal = clReleaseCommandQueue(cmdQ);); (1, OpenCL, clGetDeviceInfo(clDevice, CL_DEVICE_SLICE_COUNT_INTEL, sizeof(size_t), &maxSliceCount, nullptr);)"}
{"predict": "&& tdensity==DDT_DDT && massp2<=(MassFluid*1.2f))deltap1=FLT_MAX;\n83: #endif\n84:             if(ftp2 && shift)shiftposfsp1.x=FLT_MAX;  //-For floating objects do not calculate shifting. | Para floatings no se calcula shifting.\n85:           }\n86: \n87:           //===== Compute fluid interaction =====\n88:           if(pp1==PP_Fluid && pp2==PP_Fluid) {\n89:             //Interaction Fluid-Fluid.\n90:             const float fac1=fsph::GetKernel_Fac<tker>(CSP,rr2);\n91:             const float fac2=fsph::GetKernel_Fac<tker>(CSP,rr2);\n92:             const float f21=massp2*fac1; \n93:             const float f12=massp2*fac2; \n94:             arp1+=f21;  //sum of forces acting on particle p1\n95:             delta[p2]+=f12; //sum of forces acting on particle p2\n96:             if(ftp1 || ftp2) {\n97:               const float fr21=fac1*frx,fry21=fac1*fry,frz21=fac1*frz; \n98:               const float fr12=fac2*frx,fry12=fac2*fry,frz12=fac2*frz; \n99:               acep1.x+=fr21;\n100:               acep1.y+=fry21;\n101:               acep1.z+=frz21;\n102:               ace[p2].x+=fr12;\n103:               ace[p2].y+=fry12;\n104:               ace[p2].z+=frz12;\n105:             }\n106:           }\n107:           else if(pp1==PP_Fluid && pp2==PP_Bound) {\n108:             //Interaction Fluid-Bound.\n109:             //... (omitted for brevity)\n110:           }\n111:           else if", "label": "(205, OpenMP, const int th=omp_get_thread_num();); (19, OpenMP, #pragma omp parallel for schedule (guided))"}
{"predict": "(5, MPI, MPI_Init(&argc, &argv);) (10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);) (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);) (22, MPI, MPI_Finalize();) (50, MPI, MPI_Reduce(times.data(), &times[0], 4, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);) (67, MPI, MPI_Reduce(times.data(), &times[0], 4, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);) (78, MPI, MPI_Reduce(times.data(), &times[0], 4, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);)", "label": "(7, MPI, MPI_Init(&argc, &argv);); (85, MPI, MPI_Finalize();); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (57, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(17, CUDA, __global__ void kernelFunction() {", "label": "(14, CUDA, os << \"__device__ \";); (8, CUDA, os << \"__device__ __constant__ \";)"}
{"predict": "(8, OpenMP, #pragma omp parallel for private(i))", "label": "(12, OpenMP, #pragma omp target teams); (9, OpenMP, #pragma omp target update to(vec.v[0:N])); (16, OpenMP, #pragma omp target update from(vec.v[0:N])); (10, OpenMP, #pragma omp distribute parallel for)"}
{"predict": "(14, OpenMP, #pragma omp parallel for reduction(+:sum))", "label": "(12, OpenMP, #pragma omp target teams distribute parallel for)"}
{"predict": "(2, OpenCL, retVal = clSetKernelArg(kernel, 0, sizeof(cl_mem), &buffer);) (6, OpenCL, retVal = clSetKernelArg(kernel, 1, sizeof(cl_mem), &subBuffer);) (10, OpenCL, retVal = clSetKernelArg(kernel, 2, sizeof(size_t), &size);) (14, OpenCL, retVal = clSetKernelArg(kernel, 3, sizeof(cl_mem), &poolAllocator->bufferPools[0].mainStorage);)", "label": "(21, OpenCL, retVal = clReleaseMemObject(subBuffer);); (26, OpenCL, retVal = clReleaseMemObject(notUsedBuffer);); (28, OpenCL, EXPECT_EQ(clReleaseContext(context), CL_SUCCESS);); (-2, OpenCL, cl_mem notUsedBuffer = clCreateBuffer(clContext, flags, size, hostPtr, &retVal);); (20, OpenCL, retVal = clReleaseMemObject(buffer);); (0, OpenCL, cl_mem buffer = clCreateBuffer(clContext, flags, size, hostPtr, &retVal);); (8, OpenCL, cl_mem subBuffer = clCreateSubBuffer(buffer, flags, CL_BUFFER_CREATE_TYPE_REGION, &region, &retVal);)"}
{"predict": ";\n94:     if (rc) {\n95:         MTestPrintErrorMsg(\"MPI_BXOR and MPI_UNSIGNED_CHAR\", rc);\n96:         errs++;\n97:     } else {\n98:         if (rank == 0) {\n99:             if (ucoutbuf[0] != ((size % 2) ? (unsigned char) 0xff : (unsigned char) 0)) {\n100:                 errs++;\n101:                 fprintf(stderr, \"unsigned char BXOR(1) test failed\\n\");\n102:             }\n103:             if (ucoutbuf[1]) {\n104:                 errs++;\n105:                 fprintf(stderr, \"unsigned char BXOR(0) test failed\\n\");\n106:             }\n107:             if (ucoutbuf[2] != ((size % 2) ? (unsigned char) 0xc3 : (unsigned char) 0xff)) {\n108:                 errs++;\n109:                 fprintf(stderr, \"unsigned char BXOR(>) test failed\\n\");\n110:             }\n111:         }\n112:     }\n113: \n114:     /* short */\n115:     MTestPrintfMsg(10, \"Reduce of MPI_SHORT\\n\");\n116:     sinbuf[0] = 0x1234;\n117:     sinbuf[1] = 0;\n118:     sinbuf[2] = (rank > 0) ? 0x5678 : 0x8765;\n119: \n120:     soutbuf[0] = 0x12;\n121:     soutbuf[1] = 1;\n122:     soutbuf[2] = 1;\n123:     if (rc) {\n124:         MTestPrintErrorMsg(\"MPI_BXOR and MPI_SHORT\", rc);\n125:         errs++;\n126:     } else {\n127:         if (rank == 0) {\n128:             if (soutbuf[0] != ((size % 2) ? (short) 0x1234 : (short) 0)) {\n129:                 errs++;\n130:", "label": "(25, MPI, MPI_Comm_size(comm, &size);); (97, MPI, rc = MPI_Reduce(ucinbuf, ucoutbuf, 3, MPI_UNSIGNED_CHAR, MPI_BXOR, 0, comm);); (156, MPI, rc = MPI_Reduce(sinbuf, soutbuf, 3, MPI_SHORT, MPI_BXOR, 0, comm);); (125, MPI, rc = MPI_Reduce(cinbuf, coutbuf, 3, MPI_BYTE, MPI_BXOR, 0, comm);); (274, MPI, rc = MPI_Reduce(linbuf, loutbuf, 3, MPI_LONG, MPI_BXOR, 0, comm);); (63, MPI, rc = MPI_Reduce(scinbuf, scoutbuf, 3, MPI_SIGNED_CHAR, MPI_BXOR, 0, comm);); (31, MPI, rc = MPI_Reduce(cinbuf, coutbuf, 3, MPI_CHAR, MPI_BXOR, 0, comm);); (335, MPI, rc = MPI_Reduce(llinbuf, lloutbuf, 3, MPI_LONG_LONG, MPI_BXOR, 0, comm);); (14, MPI, MPI_Comm_set_errhandler(comm, MPI_ERRORS_RETURN);); (299, MPI, rc = MPI_Reduce(ulinbuf, uloutbuf, 3, MPI_UNSIGNED_LONG, MPI_BXOR, 0, comm);); (238, MPI, rc = MPI_Reduce(iinbuf, ioutbuf, 3, MPI_INT, MPI_BXOR, 0, comm);); (13, MPI, MPI_Comm_rank(comm, &rank);); (206, MPI, rc = MPI_Reduce(uinbuf, uoutbuf, 3, MPI_UNSIGNED, MPI_BXOR, 0, comm);); (175, MPI, rc = MPI_Reduce(usinbuf, usoutbuf, 3, MPI_UNSIGNED_SHORT, MPI_BXOR, 0, comm);)"}
{"predict": "(14, OpenMP, #pragma omp parallel for private(diracParam) if(gaugePrecise && gaugeFatPrecise && gaugeLongPrecise)) (35, OpenMP, #pragma omp parallel for if(gaugePrecise && gaugeFatPrecise && gaugeLongPrecise)) (41, OpenMP, #pragma omp parallel for if(gaugePrecise && gaugeFatPrecise && gaugeLongPrecise))", "label": "(19, CUDA, ColorSpinorParam cudaParam(cpuParam, *inv_param, QUDA_CUDA_FIELD_LOCATION);)"}
{"predict": "21:   /* Calculate the mg average values */\n122:   for (i = 0; i < 3; i++) {\n123:     meas_st_on[i] /= SAMPLES_XL;\n124:   }\n125: \n126:   /* Check if Self Test is correct */\n127:   for (i = 0; i < 3; i++) {\n128:     test_val[i] = meas_st_on[i] - meas_st_off[i];\n129:     if (fabs(test_val[i]) < ST_THRESHOLD) {\n130:       st_result = ST_FAIL;\n131:     }\n132:   }\n133: \n134:   /* Disable Self Test */\n135:   lsm303ah_xl_self_test_set(&dev_ctx_xl, LSM303AH_XL_ST_DISABLE);\n136: \n137:   /* Print results */\n138:   printf(\"Self Test Result: %s\\n\", st_result == ST_PASS ? \"PASS\" : \"FAIL\");\n139:   /* Print results */\n140:   printf(\"Self Test Result: %s\\n\", st_result == ST_PASS ? \"PASS\" : \"FAIL\");\n141: }", "label": "(145, OpenMP, lsm303ah_mg_offset_temp_comp_set(&dev_ctx_mg, PROPERTY_ENABLE);)"}
{"predict": ");\n108:     test_comps_array_vlen(COMPS_COMPLEX2,\"dset3\", \"attr3\", 5, 0);\n109:     test_comps_vlen_arry(COMPS_COMPLEX2,\"dset4\", \"attr4\", 5, 0);\n110:     /* file3 */\n111:     test_comps_array(COMPS_COMPLEX3,\"dset1\", \"attr1\", 10, 1);\n112:     test_comps_vlen(COMPS_COMPLEX3,\"dset2\", \"attr2\", 10, 0);\n113:     test_comps_array_vlen(COMPS_COMPLEX3,\"dset3\", \"attr3\", 10, 0);\n114:     test_comps_vlen_arry(COMPS_COMPLEX3,\"dset4\", \"attr4\", 10, 0);\n115: \n116:     /* -------------------------------------------------\n117:      * Create test files with dataset and attribute with container types \n118:      * (array, vlen) with multiple nested compound types.\n119:      * The test is performed with no comparables\n120:      */\n121:     /* file1 */\n122:     test_data_nocomparables(DATA_NO_COMPARE1, 0);\n123:     /* file2 */\n124:     test_data_nocomparables(DATA_NO_COMPARE2, 1);\n125:     /* -------------------------------------------------\n126:      * Create test files with dataset and attribute with container types \n127:      * (array, vlen) with multiple nested compound types.\n128:      * The test is performed with no comparables\n129:      * but with different number of comparables\n130:      */\n131:     /* file1 */\n132:     test_objs_nocomparables(OBJ_NO_COMPARE1_1, OBJ_NO_COMPARE1_2);\n133:     /* file2 */\n134:     test_objs_nocomparables(OBJ_NO_COMPARE2_1, OBJ_NO_COMPARE2_2);\n135: \n136:     return 0;\n137: }", "label": "(87, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (87, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)"}
{"predict": "(30, CUDA, cudaMalloc(&gieModelStream, gieModelStream->size());)", "label": "(22, CUDA, nvinfer1::ICudaEngine* engine = builder->buildCudaEngine(*network);)"}
{"predict": "(30, OpenCL, generate_random_inputs(input);)", "label": "(58, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(streams[0]), &streams[0]);); (58, OpenCL, err |= clSetKernelArg(kernel, 1, sizeof(streams[1]), &streams[1]);); (58, OpenCL, err |= clSetKernelArg(kernel, 2, sizeof(sampler), &sampler);); (33, OpenCL, streams[1] = clCreateBuffer(context, CL_MEM_READ_WRITE, length, NULL, &err);)"}
{"predict": "(39, CUDA, <<<dim_grid, dim_block, shm_size>>>(convert, loader, storer);)", "label": "(41, CUDA, cudaDeviceSynchronize();)"}
{"predict": "(9, OpenMP, #pragma omp atomic)\n(12, OpenMP, #pragma omp atomic)\n(18, OpenMP, #pragma omp atomic)\n(20, OpenMP, #pragma omp atomic)\n(22, OpenMP, #pragma omp atomic)\n(27, OpenMP, #pragma omp atomic)\n(30, OpenMP, #pragma omp atomic)\n(36, OpenMP, #pragma omp atomic)\n(39, OpenMP, #pragma omp atomic)\n(41, OpenMP, #pragma omp atomic)\n(46, OpenMP, #pragma omp atomic)\n(49, OpenMP, #pragma omp atomic)", "label": "(7, OpenMP, #pragma omp atomic)"}
{"predict": "(12, OpenMP, #pragma omp parallel for reduction(max:max))", "label": "(21, CUDA, CUDA_SAFE_CALL(cudaMemcpy(&max, tempCfl, sizeof(float), cudaMemcpyDeviceToHost));)"}
{"predict": "NamedObsMPI<StateVectorT>(\"PauliX\", {0});\n125:             auto Y1 = NamedObsMPI<StateVectorT>(\"PauliY\", {1});\n126:             auto Z2 = NamedObsMPI<StateVectorT>(\"PauliZ\", {2});\n127:             auto tensorProd = NamedObsMPI<StateVectorT>(\"TensorProd XYZ\", {0, 1, 2});\n128:             MeasurementsMPI<StateVectorT> Measurer_obs(statevector);\n129: \n130:             sv.applyOperations({\"PauliZ\", \"S\", \"Hadamard\"}, {{2}, {2}, {2}},\n131:                                {false, false, false});\n132: \n133:             MeasurementsMPI<StateVectorT> Measurer(sv);\n134: \n135:             auto prob_obs = Measurer_obs.probs(tensorProd);\n136:             auto prob = Measurer.probs({0, 1, 2});\n137: \n138:             REQUIRE_THAT(prob_obs, Catch::Approx(prob).margin(1e-6));\n139:         }\n140:     }\n141: }\n\nOutput:", "label": "(24, CUDA, cudaSetDevice(deviceId);); (20, CUDA, cudaGetDeviceCount(&nDevices);); (9, MPI, MPIManager mpi_manager(MPI_COMM_WORLD);)"}
{"predict": "(13, OpenMP, #pragma omp atomic);", "label": "(5, OpenMP, #pragma omp threadprivate(g)); (21, OpenMP, #pragma omp distribute parallel for private(argc, b), firstprivate(argv, c), lastprivate(d, f) collapse(2) schedule(auto) if (argc) num_threads(a) default(shared) shared(e) reduction(+ : h) dist_schedule(static, b)); (7, OpenMP, #pragma omp distribute parallel for schedule(guided, argc) default(none) copyin(g) dist_schedule(static, a) private(a) shared(argc)); (4, OpenMP, #pragma omp target); (4, OpenMP, #pragma omp teams)"}
{"predict": "(13, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &comm_hdls[i]);)", "label": "(11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (22, MPI, mpi_errno = MPI_Waitall(block, req, error_status);); (10, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (47, MPI, mpi_errno = MPI_Waitall(block, req, MPI_STATUSES_IGNORE);); (17, MPI, MPI_Comm_idup(MPI_COMM_WORLD, &comm_hdls[i], &req[block++]);); (28, MPI, mpi_errno = MPI_Wait(&req[j], MPI_STATUSES_IGNORE);)"}
{"predict": "(16, TBB, parallelQuickSort(tv.begin(), tv.end());)", "label": "(13, TBB, double serial_time = (tbb::tick_count::now() - t0).seconds();); (10, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (19, TBB, double tbb_time = (tbb::tick_count::now() - t1).seconds();); (16, TBB, tbb::tick_count t1 = tbb::tick_count::now();)"}
{"predict": "(17, CUDA, KerReduSumDoubleWarp<blockSize>(sdat, tid);)", "label": "(3, CUDA, extern __shared__ double sdat[];)"}
{"predict": "_loc;\n100: \t\t\t}\n101: \n102: \t\t\tiknsupc = SuperSize( k + lbstart );\n103: \t\t\tlk1 = LBj( k + lbstart, grid );\n104: \t\t\tNchunk = 1;\n105: \t\t\tnub_loc = 1;\n106: \t\t\tremainder = 0;\n107: \n108: \t\t\tfor(ii=lbstart;ii<lbend;++ii){\n109: \t\t\t\tirow = iword + (iknsupc-1)*dword;\n110: \t\t\t\tlsub = (int_t*) (Llu->Lsub + irow);\n111: \t\t\t\tlusup = (float*) (Llu->Lusup + irow);\n112: \n113: \t\t\t\tfor(i=0;i<iknsupc;++i){\n114: \t\t\t\t\tikfrow = iword + (iknsupc-1)*dword;\n115: \t\t\t\t\tiword = iword + (iknsupc-1)*dword;\n116: \t\t\t\t\tlsub = (int_t*) (Llu->Lsub + ikfrow);\n117: \t\t\t\t\tlusup = (float*) (Llu->Lusup + ikfrow);\n118: \n119: \t\t\t\t\tfor(j=0;j<iknsupc;++j){\n120: \t\t\t\t\t\ttemp = lusup[j];\n121: \n122: \t\t\t\t\t\tfor(jj=0;jj<iknsupc;++jj){\n123: \t\t\t\t\t\t\trtemp_loc[jj] -= temp * xk[jj];\n124: \t\t\t\t\t\t}\n125: \t\t\t\t\t}\n126: \t\t\t\t}\n127: \t\t\t}\n128: \t\t}\n129: \t}\n130: \t}\n131: \treturn;\n132: }", "label": "(82, OpenMP, #pragma\tomp\ttaskloop firstprivate (stat) private (thread_id1,Uinv,nn,lbstart,lbend,ub,temp,rtemp_loc,ik,lk1,gik,gikcol,usub,uval,lsub,lusup,iknsupc,il,i,irow,bmod_tmp,p,ii,jj,t1,t2,j,ikfrow,iklrow,dest,y,uptr,fnz,nsupr,nroot_send_tmp) untied nogroup); (126, OpenMP, #pragma omp simd); (141, OpenMP, #pragma omp atomic capture); (75, OpenMP, #pragma\tomp\tparallel for private (thread_id1,Uinv,nn,lbstart,lbend,ub,temp,rtemp_loc,ik,lk1,gik,gikcol,usub,uval,lsub,lusup,iknsupc,il,i,irow,bmod_tmp,p,ii,jj,t1,t2,j,ikfrow,iklrow,dest,y,uptr,fnz,nsupr))"}
{"predict": "(16, CUDA, state.PauseTiming(); auto d_input = make_input<T>(grid_side); auto d_output = make_output<T>(grid_side); cuproj_test::convert_coordinates<<<num_blocks, threads_per_block>>>(input, d_input, d_output); cudaMemcpy(pj_input.data(), d_output, sizeof(T) * pj_input.size(), cudaMemcpyDeviceToHost); state.ResumeTiming(););", "label": "(8, CUDA, auto input   = thrust::host_vector<coordinate<T>>(d_input);)"}
{"predict": "(14, TBB, my_n_thread = tbb::internal::rml::get_num_threads(IPC_MAX_THREADS_VAR_NAME);)", "label": "(20, TBB, my_thread_array = tbb::cache_aligned_allocator<padded_ipc_worker>().allocate( my_n_thread );); (14, TBB, my_n_thread = tbb::detail::r1::AvailableHwConcurrency();); (25, TBB, my_waker = tbb::cache_aligned_allocator<ipc_waker>().allocate(1);); (27, TBB, my_stopper = tbb::cache_aligned_allocator<ipc_stopper>().allocate(1);)"}
{"predict": "(11, OpenMP, #pragma omp parallel for)", "label": "(19, OpenMP, #pragma omp distribute parallel for); (17, OpenMP, #pragma omp target); (45, OpenMP, #pragma omp distribute parallel for num_threads(128)); (72, OpenMP, #pragma omp teams distribute parallel for thread_limit(512) num_threads(128)); (22, OpenMP, #pragma omp target teams thread_limit(64)); (76, OpenMP, #pragma omp teams distribute parallel for); (29, OpenMP, #pragma omp teams)"}
{"predict": "(75, CUDA, cudaEvent_t start, stop; cudaEventCreate(&start); cudaEventCreate(&stop); cudaEventRecord(start, 0);)\n(80, CUDA, cudaEventRecord(stop, 0); cudaEventSynchronize(stop); cudaEventElapsedTime(&reduceTime, start, stop);)\n(102, CUDA, cudaEventDestroy(start); cudaEventDestroy(stop);)", "label": "(68, CUDA, checkCudaErrors(cudaMalloc((void **)&d_odata, numBlocks * sizeof(float)));); (105, CUDA, cudaFree(d_odata);); (65, CUDA, checkCudaErrors(cudaMalloc((void **)&d_idata, bytes));); (10, CUDA, checkCudaErrors(cudaGetDeviceProperties(&prop, device));); (8, CUDA, checkCudaErrors(cudaSetDevice(device));); (100, CUDA, cudaFree(d_idata);); (65, CUDA, checkCudaErrors(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));)"}
{"predict": "(12, CUDA, half_type_##expr##_kernel<T><<<1, 1, 0, 0>>>(a, b, res_dev);); (16, CUDA, type_half_##expr##_kernel<T><<<1, 1, 0, 0>>>(a, b, res_dev);)", "label": "(20, CUDA, cudaFree(res_dev);); (10, CUDA, cudaMalloc(&res_dev, sizeof(float));); (12, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(10, MPI, if (nup < 0) { nup = MPI_PROC_NULL; } if (ndown > size - 1) { ndown = MPI_PROC_NULL; } )", "label": "(2, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (2, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(9, TBB, my_gain += performMoveWithAttributedGain(phg, my_moves[i], true);)", "label": "(14, TBB, return tbb::parallel_reduce(range, 0, accum, std::plus<>());); (3, TBB, auto range = tbb::blocked_range<size_t>(UL(0), end);)"}
{"predict": "(10, CUDA, Eigen::CudaStreamDevice stream; Eigen::GpuDevice device(&stream); BenchmarkSuite<Eigen::GpuDevice, float> suite(device, N);)", "label": "(12, CUDA, cudaDeviceSynchronize();)"}
{"predict": "(34, MPI, err = quobyte_aio_submit_with_callback(global_quobyte_io_context, aiocbp, quobyte_io_event_finished, quobyte_aio);)", "label": "(11, MPI, MPI_Type_size_x(type, &typesize);)"}
{"predict": "(3, MPI, pcu::PCU PCUObj = pcu::PCU(MPI_COMM_WORLD);)", "label": "(15, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "6: \n117:     // Create a model runner\n118:     flamegpu::ModelRunner modelRunner = model.newRunner();\n119:     // Configure the model runner\n120:     modelRunner.setPopulationSize(4000u);\n121:     modelRunner.setCudaEnabled(true);\n122:     modelRunner.setGridSize({10, 10, 10});\n123:     modelRunner.setBlockSize({256, 1, 1});\n124:     modelRunner.setCudaStreamBatchSize(1000);\n125: \n126:     // Initialise the model\n127:     modelRunner.initialise();\n128: \n129:     // Run the model\n130:     modelRunner.run(1000);\n131: \n132:     // Finalise the model\n133:     modelRunner.finish();\n134: \n135:     // Clean up\n136:     modelRunner.cleanup();\n137:     return 0;\n138: }", "label": "(80, CUDA, flamegpu::CUDASimulation cudaSimulation(model);)"}
{"predict": "(16, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);) (17, MPI, MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);)", "label": "(16, MPI, MPI_Comm_size(MPI_COMM_WORLD,&mpi_size);); (16, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&mpi_rank);)"}
{"predict": "(9, MPI, MPI_Allreduce(&actual_bytes, &actual_bytes, 1, MPI_COUNT, MPI_SUM, MPI_COMM_WORLD);)", "label": "(63, MPI, MPI_Unpack(check_pack_buf, pack_size, &pos, unpack_buf, 1, datatype, MPI_COMM_WORLD);); (6, MPI, MPI_Type_size_c(datatype, &type_size);); (34, MPI, MPI_Pack_size(1, datatype, MPI_COMM_WORLD, &pack_size);); (54, MPI, MPI_Pack(check_unpack_buf, 1, datatype, pack_buf, pack_size, &pos, MPI_COMM_WORLD);)"}
{"predict": "(12, OpenMP, ++I;)\n(18, OpenMP, ++I;)\n(25, OpenMP, ++I;)", "label": "(4, OpenMP, #pragma omp parallel for)"}
{"predict": "\"failed to init input data\\n\");\n113: \n114:             // run the test\n115:             int errCode = run_test(i, indx, num_elements, vectorSize, pThreadData);\n116:             test_error(errCode, \"run_test failed\\n\");\n117: \n118:             // check the results\n119:             if (errCode != CL_SUCCESS)\n120:             {\n121:                 error_count++;\n122:             }\n123:         }\n124:     }\n125: \n126:     if (error_count > 0)\n127:     {\n128:         return error_count;\n129:     }\n130:     else\n131:     {\n132:         return 0;\n133:     }\n134: }", "label": "(113, OpenCL, err = clEnqueueWriteBuffer( queue, pThreadData->m_streams[1], CL_FALSE, 0, pThreadData->m_type_size*num_elements * inputBVecSize, (void *)pThreadData->m_input_ptr[1], 0, NULL, NULL );); (110, OpenCL, err = clEnqueueWriteBuffer(queue, pThreadData->m_streams[0], CL_FALSE, 0, pThreadData->m_type_size*num_elements * inputAVecSize, (void *)pThreadData->m_input_ptr[0], 0, NULL, NULL);); (117, OpenCL, err = clEnqueueReadBuffer( queue, pThreadData->m_streams[2], CL_TRUE, 0, pThreadData->m_type_size*num_elements * vectorSize, (void *)pThreadData->m_output_ptr, 0, NULL, NULL );); (113, OpenCL, err = clEnqueueNDRangeKernel( queue, pThreadData->m_kernel[i], 1, NULL, threads, NULL, 0, NULL, NULL );)"}
{"predict": "37:   for (int i=radius; i<n-radius; i++) {\n138:     OMP_SIMD\n139:     for (int j=radius; j<n-radius; j++) {\n140:       norm += std::abs(out[i*n+j]-in[i*n+j]);\n141:     }\n142:   }\n143: \n144:   norm /= active_points;\n145:   std::cout << \"L1 norm = \" << norm << std::endl;\n146: \n147:   // compute L2 norm in parallel\n148:   double norm2 = 0.0;\n149:   OMP_PARALLEL_FOR_REDUCE( +:norm2 )\n150:   for (int i=radius; i<n-radius; i++) {\n151:     OMP_SIMD\n152:     for (int j=radius; j<n-radius; j++) {\n153:       norm2 += std::pow(out[i*n+j]-in[i*n+j], 2.0);\n154:     }\n155:   }\n156: \n157:   norm2 /= active_points;\n158:   norm2 = std::sqrt(norm2);\n159:   std::cout << \"L2 norm = \" << norm2 << std::endl;\n160: \n161:   delete[] in;\n162:   delete[] out;\n163: \n164:   return 0;\n165: }", "label": "(114, OpenMP, if (iter==1) stencil_time = omp_get_wtime();); (59, OpenMP, std::cout << \"Number of threads (max)   = \" << omp_get_max_threads() << std::endl;); (125, OpenMP, stencil_time = omp_get_wtime() - stencil_time;)"}
{"predict": "(8, OpenMP, #pragma omp parallel for)", "label": "(36, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (83, CUDA, cudaFree(d_in);); (83, CUDA, cudaFree(d_out);); (32, CUDA, cudaMalloc((void**)(&d_in), in_bytes);); (45, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (33, CUDA, cudaMemcpy(d_in, tensor.data(), in_bytes, cudaMemcpyHostToDevice);); (42, CUDA, assert(cudaMemcpyAsync(tensor_arg.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);)"}
{"predict": "(15, MPI, MPI_Type_create_hindexed(count, lens, displs, types, &newtype);)", "label": "(30, MPI, MPI_Send(MPI_BOTTOM, 1, newtype, 1, 0, MPI_COMM_WORLD);); (23, MPI, MPI_Type_commit(&newtype);); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (20, MPI, MPI_Type_create_struct(2, lens, displs, types, &newtype);); (15, MPI, MPI_Get_address(&count, &displs[0]);); (27, MPI, MPI_Recv(buf, 6, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (35, MPI, MPI_Type_free(&newtype);); (15, MPI, types[1] = MPI_Type_f2c(*ftype);)"}
{"predict": "(23, OpenMP, #pragma omp parallel for)", "label": "(11, OpenMP, #pragma omp taskloop simd linear(f) linear(f)); (54, OpenMP, #pragma omp taskloop simd linear(ref(j))); (35, OpenMP, #pragma omp taskloop simd linear(a, b)); (26, OpenMP, #pragma omp taskloop simd linear (argc > 0 ? argv[1] : argv[2])); (27, OpenMP, #pragma omp taskloop simd linear (argc, z)); (10, OpenMP, #pragma omp taskloop simd linear (); (39, OpenMP, #pragma omp taskloop simd linear(h, C::x)); (16, OpenMP, #pragma omp taskloop simd linear (foo())); (49, OpenMP, #pragma omp taskloop simd linear(i)); (38, OpenMP, #pragma omp parallel); (17, OpenMP, #pragma omp taskloop simd linear (argc,); (2, OpenMP, #pragma omp taskloop simd linear); (13, OpenMP, #pragma omp taskloop simd linear (argc); (37, OpenMP, #pragma omp taskloop simd linear(val(i))); (19, OpenMP, #pragma omp taskloop simd linear (S1)); (24, OpenMP, #pragma omp taskloop simd linear (argv[1])); (3, OpenMP, #pragma omp taskloop simd linear (val); (4, OpenMP, #pragma omp taskloop simd linear (ref())); (34, OpenMP, #pragma omp taskloop simd linear(uval(i) : 4)); (24, OpenMP, #pragma omp taskloop simd linear(val(e, g))); (-3, OpenMP, #pragma omp taskloop simd linear ())"}
{"predict": "(1, OpenMP, #pragma omp parallel for;)\n(1, CUDA, <<< grid, block >>>;)\n(1, MPI, MPI_Init(&argc, &argv);)\n(1, OpenCL, clFinish(commandQueue);)\n(1, CUDA, cudaStreamSynchronize(stream);)\n(1, MPI, MPI_Finalize();)\n(1, OpenMP, #pragma omp barrier;)\n(1, CUDA, cudaDeviceSynchronize();)\n(1, MPI, MPI_Barrier(MPI_COMM_WORLD);)\n(1, OpenMP, #pragma omp critical;)\n(1, CUDA, __syncthreads();)\n(1, MPI, MPI_Allreduce(&value, &result, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);)\n(1, OpenMP, #pragma omp sections;)\n(1, CUDA, cudaMalloc(&devPtr, size);)\n(1, MPI, MPI_Bcast(&data, count, MPI_INT, root, MPI_COMM_WORLD);)\n(1, OpenMP, #pragma omp for reduction(+:sum);)\n(1, CUDA, cudaMemcpy(devPtr, hostPtr, size, cudaMemcpyHostToDevice);)\n(1, MPI, MPI_Reduce(&value, &result, 1, MPI_INT, MPI_MAX, root, MPI_COMM_WORLD);)\n(1, OpenMP, #pragma omp single;)\n(1, CUDA, cudaFree(devPtr);)\n(1, MPI, MPI_Gather(&data, count, MPI_INT, recvbuf, count, MPI_INT, root, MPI_COMM_WORLD);)\n(1, OpenMP, #pragma omp ordered;)\n(1, CUDA, cudaMemcpy(hostPtr, devPtr, size, cudaMemcpyDeviceToHost);)\n(1, MPI, MPI_Scatter(&sendbuf, count, MPI_INT, &data, count, MPI_INT, root, MPI_COMM_WORLD);)\n(1, OpenMP, #pragma omp parallel sections;)\n(1, CUDA, cudaMalloc(&devPtr, size);)\n(1, MPI, MPI_Bcast(&data, count, MPI_INT, root, MPI_COMM_WORLD);)\n(1, OpenMP, #pragma omp for reduction(+:sum);)\n(1, CUDA, cudaMemcpy(devPtr, hostPtr, size, cudaMemcpyHostToDevice);)\n(1, MPI, MPI_Reduce(&value, &result, 1, MPI_INT, MPI_MAX, root, MPI_COMM_WORLD);)\n(1, Open", "label": "(3, OpenMP, #pragma omp taskyield untied); (0, OpenMP, #pragma omp taskyield); (2, OpenMP, #pragma omp taskyield unknown)"}
{"predict": "(11, CUDA, kerinfo->forcesdem_bsmax = attr.maxThreadsPerBlock;", "label": "(9, CUDA, cudaFuncGetAttributes(&attr,(void*)ptr);); (14, CUDA, fcuda::Check_CudaErroorFun(\"Error collecting kernel information.\");); (5, CUDA, cudaOccupancyMaxPotentialBlockSize(&mingridsize,&qblocksize,(void*)ptr,0,0);)"}
{"predict": "(11, OpenMP, #pragma omp parallel for private(j) reduction(+:next_value[tid]))", "label": "(52, TBB, tbb::flow::make_edge( b2, b3 );); (50, TBB, tbb::flow::make_edge( b, b2 );); (73, TBB, tbb::flow::buffer_node<T> b_copy(b);); (2, TBB, tbb::flow::buffer_node<T> b3(g);); (0, TBB, tbb::flow::buffer_node<T> b2(g);); (-3, TBB, tbb::flow::graph g;); (-3, TBB, tbb::flow::buffer_node<T> b(g);)"}
{"predict": ":         size_t gws[1] = { num_elements };\n136:         err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, gws, NULL, 0,\n137:                                      NULL, NULL);\n138:         test_error(err, \"clEnqueueNDRangeKernel failed\\n\");\n139: \n140:         // Copy result back to host\n141:         err = clEnqueueReadBuffer(queue, streams[3], CL_TRUE, 0, datasize,\n142:                                    output.data(), 0, NULL, NULL);\n143:         test_error(err, \"clEnqueueReadBuffer failed\\n\");\n144: \n145:         // Verify result\n146:         for (int i = 0; i < num_elements; i++)\n147:         {\n148:             T res = test.function(inputA[i], inputB[i], inputC[i]);\n149:             EXPECT_EQ(res, output[i]);\n150:         }\n151:     }\n152: \n153:     // Clean up\n154:     for (size_t i = 0; i < ARRAY_SIZE(streams); i++)\n155:     {\n156:         err = clReleaseMemObject(streams[i]);\n157:         test_error(err, \"clReleaseMemObject failed\\n\");\n158:     }\n159: \n160:     return TEST_PASSED;\n161: }", "label": "(132, OpenCL, err |= clSetKernelArg(kernel, 1, sizeof(cl_mem), &streams[1]);); (132, OpenCL, err |= clSetKernelArg(kernel, 2, sizeof(cl_mem), &streams[2]);); (132, OpenCL, err |= clSetKernelArg(kernel, 3, sizeof(cl_mem), &streams[3]);); (128, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(cl_mem), &streams[0]);)"}
{"predict": "(1, OpenMP, #pragma omp parallel for)", "label": "(19, OpenMP, #pragma omp parallel default(shared)); (11, OpenMP, #pragma omp parallel); (38, OpenMP, #pragma omp parallel num_threads(x)); (23, OpenMP, #pragma omp parallel default(none)); (29, OpenMP, #pragma omp parallel default(firstprivate))"}
{"predict": "(4, OpenMP, #pragma omp parallel for collapse(2))", "label": "(24, TBB, range_type r2(r,tbb::split());); (2, TBB, typedef tbb::blocked_range2d<row_type,col_type> range_type;); (18, TBB, AssertSameType( r.cols(), tbb::blocked_range<col_type>( coli, colj, 1 ));); (16, TBB, AssertSameType( r.rows(), tbb::blocked_range<row_type>( rowi, rowj, 1 ));)"}
{"predict": "(10, OpenMP, #pragma omp parallel for reduction(+:sum1))", "label": "(13, OpenMP, #pragma omp for private(sum0) schedule(static,1)); (7, OpenMP, #pragma omp parallel); (22, OpenMP, #pragma omp critical); (13, OpenMP, #pragma omp flush)"}
{"predict": "malloc\n54:     a_h = (real*) malloc(chunk_size * nx * sizeof(real));\n55:     a_ref_h = (real*) malloc(chunk_size * nx * sizeof(real));\n56:     if (!a_h || !a_ref_h) {\n57:         std::cerr << \"Allocation failed\\n\";\n58:         return 1;\n59:     }\n60: \n61:     //TODO: Initialize a and a_new from the NVSHMEM symmetric heap instead of using malloc\n62:     memset(a_h, 0, chunk_size * nx * sizeof(real));\n63:     memset(a_ref_h, 0, chunk_size * nx * sizeof(real));\n64: \n65:     //TODO: Copy data from the NVSHMEM symmetric heap to the local heap\n66:     memcpy(a_h, a_ref_h, chunk_size * nx * sizeof(real));\n67: \n68:     double runtime_gpu = single_gpu(nx, ny, iter_max, a_h, nccheck, !csv && (0 == rank));\n69: \n70:     free(a_h);\n71:     free(a_ref_h);\n72: \n73:     //TODO: Free the NVSHMEM symmetric heap\n74: \n75:     return 0;\n76: }", "label": "(118, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&reset_l2norm_done, cudaEventDisableTiming));); (143, MPI, MPI_CALL(MPI_Barrier(MPI_COMM_WORLD));); (21, MPI, MPI_CALL(MPI_Comm_free(&local_comm));); (249, CUDA, CUDA_RT_CALL(cudaStreamDestroy(compute_stream));); (201, MPI, double stop = MPI_Wtime();); (244, CUDA, CUDA_RT_CALL(cudaEventDestroy(push_done));); (257, CUDA, CUDA_RT_CALL(cudaFree(a));); (-4, MPI, MPI_CALL(MPI_Comm_rank(MPI_COMM_WORLD, &rank));); (94, CUDA, CUDA_RT_CALL(cudaDeviceSynchronize());); (18, CUDA, CUDA_RT_CALL(cudaFree(0));); (16, CUDA, CUDA_RT_CALL(cudaSetDevice(local_rank%num_devices));); (58, CUDA, CUDA_RT_CALL(cudaMalloc(&a, nx * (chunk_size + 2) * sizeof(real)));); (243, CUDA, CUDA_RT_CALL(cudaFree(l2_norm_d));); (138, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(push_stream, reset_l2norm_done, 0));); (100, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&push_prep_done, cudaEventDisableTiming));); (28, CUDA, CUDA_RT_CALL(cudaMallocHost(&a_h, nx * ny * sizeof(real)));); (25, CUDA, CUDA_RT_CALL(cudaMallocHost(&a_ref_h, nx * ny * sizeof(real)));); (84, CUDA, CUDA_RT_CALL(cudaGetLastError());); (145, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(compute_stream, push_prep_done, 0));); (89, CUDA, CUDA_RT_CALL(cudaStreamCreateWithPriority(&compute_stream, cudaStreamDefault, leastPriority));); (51, CUDA, CUDA_RT_CALL(cudaMalloc(&a_new, nx * (chunk_size + 2) * sizeof(real)));); (230, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_stream));); (-17, MPI, MPI_CALL(MPI_Comm_size(MPI_COMM_WORLD, &size));); (170, CUDA, CUDA_RT_CALL(cudaStreamSynchronize(compute_stream));); (125, CUDA, CUDA_RT_CALL(cudaEventRecord(reset_l2norm_done, compute_stream));); (-18, CUDA, CUDA_RT_CALL(cudaGetDeviceCount(&num_devices));); (102, CUDA, CUDA_RT_CALL(cudaMallocHost(&l2_norm_h, sizeof(real)));); (121, CUDA, CUDA_RT_CALL(cudaMemsetAsync(l2_norm_d, 0, sizeof(real), compute_stream));); (-27, MPI, MPI_CALL(MPI_Init(&argc, &argv));); (46, CUDA, CUDA_RT_CALL(cudaMemset(a_new, 0, nx * (chunk_size + 2) * sizeof(real)));); (76, CUDA, CUDA_RT_CALL(cudaDeviceGetStreamPriorityRange(&leastPriority, &greatestPriority));); (219, CUDA, CUDA_RT_CALL(cudaEventDestroy(push_prep_done));); (94, CUDA, CUDA_RT_CALL(cudaMalloc(&l2_norm_d, sizeof(real)));); (-12, MPI, MPI_CALL(MPI_Comm_rank(local_comm, &local_rank));); (233, CUDA, CUDA_RT_CALL(cudaFreeHost(a_ref_h));); (159, MPI, MPI_CALL(MPI_Allreduce(l2_norm_h, &l2_norm, 1, MPI_REAL_TYPE, MPI_SUM, MPI_COMM_WORLD));); (109, MPI, double start = MPI_Wtime();); (79, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&push_done, cudaEventDisableTiming));); (210, CUDA, CUDA_RT_CALL(cudaEventDestroy(reset_l2norm_done));); (215, CUDA, CUDA_RT_CALL(cudaFreeHost(l2_norm_h));); (120, CUDA, CUDA_RT_CALL(cudaEventRecord(push_prep_done, push_stream));)"}
{"predict": "(10, OpenMP, #pragma omp parallel for reduction(+:combine_sum, combine_ref_sum, combine_each_sum, combine_finit_sum))", "label": "(30, TBB, tbb::combinable<T> finit_combinable(my_finit_decl);); (32, TBB, tbb::parallel_for( tbb::blocked_range<int>( 0, N, 10000 ), ParallelScalarBodyNoInit<T>( finit_combinable ) );); (24, TBB, if (Verbose && t == 0) t0 = tbb::tick_count::now();); (31, TBB, tbb::parallel_for( tbb::blocked_range<int>( 0, N, 10000 ), ParallelScalarBody<T>( sums ) );); (40, TBB, tbb::combinable<T> assigned;); (-2, TBB, tbb::task_scheduler_init init(tbb::task_scheduler_init::deferred);); (7, TBB, tbb::tick_count t0;); (21, TBB, tbb::combinable<T> sums;)"}
{"predict": "(17, OpenMP, #pragma omp parallel for private(i) reduction(+:sum) ) (46, OpenMP, #pragma omp parallel for private(i) reduction(+:sum) )", "label": "(12, OpenMP, omp_set_num_threads(nr_threads);); (13, OpenMP, #pragma omp parallel default(none) shared(a)); (14, OpenMP, #pragma omp sections); (8, OpenMP, omp_set_dynamic(0);); (14, OpenMP, #pragma omp section); (16, OpenMP, int thread_nr = omp_get_thread_num();)"}
{"predict": "(3, CUDA, __global__ void kmeans_assign_lloyd)", "label": "(12, CUDA, extern __shared__ float _shared_centroids[];)"}
{"predict": "(49, CUDA, dynamics->initializeDynamics(x, u, y, theta_s_shared, 0.0f, dt);)\n(50, CUDA, sampling->initializeDistributions(y, 0.0f, dt, theta_d_shared);)\n(60, CUDA, sampling->readControlSample(global_idx, t, distribution_idx, u, theta_d_shared, blockDim.y, thread_idy, y);)\n(62, CUDA, bar->arrive_and_wait();)\n(67, CUDA, dynamics->enforceConstraints(x, u);)\n(69, CUDA, bar->arrive_and_wait();)\n(72, CUDA, sampling->writeControlSample(global_idx, t, distribution_idx, u, theta_d_shared, blockDim.y, thread_idy, y);)\n(75, CUDA, x_temp = x;)\n(76, CUDA, x = x_next;)\n(77, CUDA, x_next = x_temp;)\n(89, CUDA, mp1::loadArrayParallel<DYN_T::OUTPUT_DIM>(y_d, sample_time_offset * DYN_T::OUTPUT_DIM, y, 0);)", "label": "(21, CUDA, extern __shared__ float entire_buffer[];)"}
{"predict": "(9, OpenMP, #pragma omp atomic)", "label": "(5, OpenMP, #pragma omp flush acquire); (5, OpenMP, #pragma omp flush release); (1, OpenMP, #pragma omp flush); (4, OpenMP, #pragma omp flush(a)); (0, OpenMP, #pragma omp flush acq_rel)"}
{"predict": "(36, CUDA, CUSOLVER_CHECK(cusolverDnSetStream(handle, stream));)", "label": "(43, CUDA, CUDA_CHECK(cudaMemcpy(d_A_inv, d_A, sizeof(data_type) * lda * n, cudaMemcpyDeviceToDevice));); (47, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (69, CUDA, CUDA_CHECK(cudaFree(d_info));); (34, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * lda * n));); (34, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A_inv), sizeof(data_type) * lda * n));); (65, CUDA, CUDA_CHECK(cudaFree(d_A_inv));); (58, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (32, CUDA, CUDA_CHECK(cudaMalloc(&d_info, sizeof(int)));); (34, CUDA, CUDA_CHECK(cudaMemcpy(d_A, h_A, sizeof(data_type) * lda * n, cudaMemcpyHostToDevice));); (42, CUDA, CUDA_CHECK(cudaMemcpy(&h_info, d_info, sizeof(int), cudaMemcpyDeviceToHost));); (34, CUDA, CUDA_CHECK(cudaMemset(d_info, 0, sizeof(int)));); (58, CUDA, CUDA_CHECK(cudaFree(d_A));); (20, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(13, CUDA, extern \"C\" __global__ void kernel_vbias_from_v1(int nwalk, int nkpts, int nchol_max, int* Qsym, int* kminus, int* ncholpQ, int* ncholpQ0, thrust::complex<T2> const alpha, thrust::complex<T> const* v1, thrust::complex<T2>* vb);)", "label": "(31, CUDA, thrust::complex<T> const* v1_(v1 + Q * nchol_max * nwalk);); (31, CUDA, thrust::complex<T> const* v2_(v1 + Qm * nchol_max * nwalk);); (52, CUDA, thrust::complex<T2> ialpha(alpha * thrust::complex<T2>(0.0, 1.0));); (27, CUDA, thrust::complex<T2>* vb_(vb + nc0 * nwalk);); (46, CUDA, thrust::complex<T2>* vb_(vb + (nc0 + nc) * nwalk);)"}
{"predict": "(67, OpenMP, #pragma omp parallel for private(x, z, r, u, w, t, hr, ht))", "label": "(83, OpenACC, #pragma acc update host(state[(nz + 2 * hs) * (nx + 2 * hs) * NUM_VARS])); (45, OpenACC, #pragma acc data copyin(state_tmp[(nz + 2 * hs) * (nx + 2 * hs) * NUM_VARS], hy_dens_cell[nz + 2 * hs], hy_dens_theta_cell[nz + 2 * hs], hy_dens_int[nz + 1], hy_dens_theta_int[nz + 1], hy_pressure_int[nz + 1]))"}
{"predict": "(4, MPI, *w = MPI_Wtime();)", "label": "(32, MPI, CALL(MPI_Waitall(static_cast<int>(reqs.size()), reqs.data(), MPI_STATUSES_IGNORE));); (20, MPI, CALL(MPI_Waitall(reqs.size(), reqs.data(), MPI_STATUSES_IGNORE));)"}
{"predict": "(16, MPI, int* data = (int*)malloc(sizeof(int) * num_elements);)", "label": "(28, MPI, total_my_bcast_time += MPI_Wtime();); (31, MPI, total_mpi_bcast_time -= MPI_Wtime();); (45, MPI, MPI_Finalize();); (32, MPI, total_mpi_bcast_time += MPI_Wtime();); (5, MPI, MPI_Init(NULL, NULL);); (28, MPI, MPI_Bcast(data, num_elements, MPI_INT, 0, MPI_COMM_WORLD);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (17, MPI, total_my_bcast_time -= MPI_Wtime();); (15, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(13, CUDA, As[threadIdx.y][threadIdx.x] = A[idx * ds + threadIdx.x];) (14, CUDA, Bs[threadIdx.y][threadIdx.x] = B[idy * ds + threadIdx.x];) (21, CUDA, temp += As[threadIdx.y][threadIdx.x] * Bs[threadIdx.y][threadIdx.x];) (27, CUDA, C[idy * ds + idx] = temp;)", "label": "(4, CUDA, __shared__ float Bs[block_size][block_size];); (2, CUDA, __shared__ float As[block_size][block_size];)"}
{"predict": "(14, MPI, MPI_Allreduce(left_data, right_data, num, item, cmp, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)", "label": "(11, MPI, MPI_Type_get_extent(item, &lb, &extent);)"}
{"predict": "(14, CUDA, auto loadedCache = loadTimingCacheFile(build.timingCacheFile);) (15, CUDA, timingCache.reset(config->createTimingCache(static_cast<const void*>(loadedCache.data()), loadedCache.size()));) (24, CUDA, env.serializedEngine.reset(builder.buildSerializedNetwork(*env.network, *config));) (46, CUDA, env.engine.reset(runtime->deserializeCudaEngine(env.serializedEngine->data(), env.serializedEngine->size()));)", "label": "(46, CUDA, env.engine.reset(runtime->deserializeCudaEngine(env.serializedEngine->data(), env.serializedEngine->size()));); (19, CUDA, auto profileStream = samplesCommon::makeCudaStream();)"}
{"predict": "(28, CUDA, ret = microseconds / (float) cIterations;);", "label": "(28, CUDA, cudaFree( p );)"}
{"predict": "(21, OpenMP, #pragma omp target teams is_device_ptr(k);)\n(24, OpenMP, #pragma omp target teams is_device_ptr(z);)\n(27, OpenMP, #pragma omp target teams is_device_ptr(aa);)\n(30, OpenMP, #pragma omp target teams is_device_ptr(raa);)\n(33, OpenMP, #pragma omp target teams is_device_ptr(h);)\n(36, OpenMP, #pragma omp target teams is_device_ptr(rh);)\n(39, OpenMP, #pragma omp target teams is_device_ptr(da);)", "label": "(49, OpenMP, #pragma omp target teams is_device_ptr(da)); (23, OpenMP, #pragma omp target teams is_device_ptr(z)); (27, OpenMP, #pragma omp target teams is_device_ptr(aa)); (16, OpenMP, #pragma omp target teams is_device_ptr(k)); (30, OpenMP, #pragma omp target teams is_device_ptr(raa)); (39, OpenMP, #pragma omp target teams is_device_ptr(rh)); (33, OpenMP, #pragma omp target teams is_device_ptr(h))"}
{"predict": "(33, MPI, tim::mpi::barrier(tim::mpi::comm_world_v);)", "label": "(35, MPI, _measurements = get_measure()->mpi_get();)"}
{"predict": "(26, MPI, if (verbose) { fprintf(stdout, \"\\n\"); } (29, MPI, if (verbose) { fprintf(stdout, \"\\n\"); }", "label": "(42, MPI, MPI_T_finalize();); (17, MPI, MPI_T_init_thread(required, &provided);)"}
{"predict": "(2, CUDA, __global__ void dot(float *a, float *b, float *y, int N) {)", "label": "(5, CUDA, __shared__ float reduce_smem[NUM_WARPS];)"}
{"predict": "(7, OpenMP, #pragma omp parallel for reduction(+:inversions))", "label": "(3, TBB, tbb::parallel_for( tbb::blocked_range<int>(0,m,1000), GrowBy(v) );)"}
{"predict": "(45, OpenMP, #pragma omp parallel for reduction(+ : fl))", "label": "(54, OpenMP, #pragma omp parallel reduction(&& : S2::S2sc)); (25, OpenMP, #pragma omp parallel reduction(foo : argc); (44, OpenMP, #pragma omp parallel reduction(* : ca)); (78, OpenMP, #pragma omp for reduction(- : fl)); (28, OpenMP, #pragma omp parallel reduction(~ : argc)); (53, OpenMP, #pragma omp parallel reduction(+ : h, k, B::x)); (82, OpenMP, #pragma omp parallel reduction(task, + : m)); (79, OpenMP, #pragma omp parallel reduction(+ : m)); (36, OpenMP, #pragma omp parallel reduction(+ : ba)); (39, OpenMP, #pragma omp parallel reduction(- : da)); (59, OpenMP, #pragma omp parallel reduction(+ : r)); (5, OpenMP, #pragma omp parallel reduction(); (50, OpenMP, #pragma omp parallel private(i), reduction(+ : j), reduction(+ : q)); (47, OpenMP, #pragma omp parallel reduction(+ : o)); (-2, OpenMP, #pragma omp parallel); (37, OpenMP, #pragma omp parallel reduction(&& : S2::S2s)); (2, OpenMP, #pragma omp parallel reduction(-); (17, OpenMP, #pragma omp parallel reduction(&& : argc, z)); (-4, OpenMP, #pragma omp parallel reduction); (52, OpenMP, #pragma omp parallel shared(i)); (44, OpenMP, #pragma omp parallel private(k)); (21, OpenMP, #pragma omp parallel reduction(max : h.b)); (6, OpenMP, #pragma omp parallel reduction(| : argc,); (15, OpenMP, #pragma omp parallel reduction(+ : a, b, c, d, f)); (49, OpenMP, #pragma omp parallel reduction(max : j)); (5, OpenMP, #pragma omp parallel reduction(|| : argc > 0 ? argv[1] : argv[2])); (14, OpenMP, #pragma omp parallel reduction(min : a, b, c, d, f)); (45, OpenMP, #pragma omp parallel reduction(min : i)); (-4, OpenMP, #pragma omp parallel reduction(\\)); (-9, OpenMP, #pragma omp parallel reduction()); (60, OpenMP, #pragma omp parallel reduction(+:gptr[:argc])); (45, OpenMP, #pragma omp for private(fl)); (-10, OpenMP, #pragma omp parallel reduction(*)); (17, OpenMP, #pragma omp parallel reduction(^ : fl)); (2, OpenMP, #pragma omp parallel reduction(^ : S1)); (30, OpenMP, #pragma omp parallel reduction(+ : p), reduction(+ : p)); (20, OpenMP, #pragma omp parallel reduction(& : e, g))"}
{"predict": "(14, OpenMP, #pragma omp parallel for)", "label": "(10, TBB, static const size_t number_of_threads = 4 * tbb::task_scheduler_init::default_num_threads();)"}
{"predict": "(12, CUDA, auto dX = cudaMallocManaged(&dX, N * sizeof(double));) (13, CUDA, auto dY = cudaMallocManaged(&dY, N * sizeof(double));) (14, CUDA, auto dZ = cudaMallocManaged(&dZ, N * sizeof(double));) (27, CUDA, cudaFree(dX);) (28, CUDA, cudaFree(dY);) (29, CUDA, cudaFree(dZ);)", "label": "(8, CUDA, cuda_safe_call(cudaMalloc((void**) &dX, N * sizeof(double)));); (8, CUDA, cuda_safe_call(cudaMalloc((void**) &dY, N * sizeof(double)));); (8, CUDA, cuda_safe_call(cudaMalloc((void**) &dZ, N * sizeof(double)));); (11, CUDA, cuda_safe_call(cudaDeviceSynchronize());)"}
{"predict": "(10, OpenMP, #pragma omp parallel for reduction(+:errs) private(k) if(nmat > 100))", "label": "(8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);)"}
{"predict": "{\n89:         cudaSetDevice(i);\n90:       }\n91:     }\n92:   }\n93: \n94:   if (flag) {\n95:     delete flag;\n96:   }\n97: }", "label": "(147, CUDA, cudaStreamDestroy(stream0[d]);); (16, CUDA, cudaMalloc(&buffersD2D[d], numElems * sizeof(int));); (141, CUDA, cudaEventDestroy(start[d]);); (41, CUDA, cudaDeviceEnablePeerAccess(i, 0);); (7, CUDA, cudaCheckError();); (136, CUDA, cudaFree(buffersD2D[d]);); (35, CUDA, cudaDeviceEnablePeerAccess(j, 0);); (138, CUDA, cudaEventDestroy(stop[d]);); (25, CUDA, cudaSetDevice(i);); (106, CUDA, cudaDeviceDisablePeerAccess(i);); (139, CUDA, cudaStreamDestroy(stream1[d]);); (83, CUDA, cudaEventRecord(stop[i], stream0[i]);); (10, CUDA, cudaEventCreate(&stop[d]);); (140, CUDA, cudaFreeHost((void *)flag);); (79, CUDA, cudaStreamWaitEvent(stream0[i], stop[j], 0);); (3, CUDA, cudaMemset(buffersD2D[d], 0, numElems * sizeof(int));); (35, CUDA, cudaStreamSynchronize(stream1[j]);); (52, CUDA, cudaStreamWaitEvent(stream1[j], start[i], 0);); (-2, CUDA, cudaMemset(buffers[d], 0, numElems * sizeof(int));); (-4, CUDA, cudaMalloc(&buffers[d], numElems * sizeof(int));); (-10, CUDA, cudaHostAlloc((void **)&flag, sizeof(*flag), cudaHostAllocPortable);); (119, CUDA, cudaFree(buffers[d]);); (28, CUDA, cudaStreamSynchronize(stream0[i]);); (-9, CUDA, cudaSetDevice(d);); (0, CUDA, cudaStreamCreateWithFlags(&stream0[d], cudaStreamNonBlocking);); (13, CUDA, cudaDeviceCanAccessPeer(&access, i, j);); (42, CUDA, cudaEventRecord(start[i], stream0[i]);); (16, CUDA, cudaSetDevice(j);); (64, CUDA, cudaEventRecord(stop[j], stream1[j]);); (74, CUDA, cudaEventElapsedTime(&time_ms, start[i], stop[i]);); (-10, CUDA, cudaEventCreate(&start[d]);); (-5, CUDA, cudaStreamCreateWithFlags(&stream1[d], cudaStreamNonBlocking);); (81, CUDA, cudaDeviceDisablePeerAccess(j);)"}
{"predict": "(15, OpenMP, #pragma omp parallel for collapse(2))", "label": "(5, OpenACC, #pragma acc parallel loop)"}
{"predict": "(12, OpenMP, #pragma omp parallel for private(i);)", "label": "(10, OpenMP, #pragma omp master taskloop collapse (argc); (1, OpenMP, #pragma omp master taskloop collapse); (13, OpenMP, #pragma omp master taskloop collapse (1)); (14, OpenMP, #pragma omp master taskloop collapse ((ST > 0) ? 1 + ST : 2)); (31, OpenMP, #pragma omp master taskloop collapse (N)); (32, OpenMP, #pragma omp master taskloop collapse (2)); (16, OpenMP, #pragma omp master taskloop collapse (foobool(argc)), collapse (true), collapse (-5)); (-3, OpenMP, #pragma omp master taskloop collapse (); (-2, OpenMP, #pragma omp master taskloop collapse ()); (15, OpenMP, #pragma omp master taskloop collapse (S)); (3, OpenMP, #pragma omp master taskloop collapse (ST)"}
{"predict": "(1, CUDA, extern \"C\" __global__ void block_all_reduce_sum_f16_f16_kernel(half *a, float *y, int N);)", "label": "(5, CUDA, __shared__ float reduce_smem[NUM_WARPS];)"}
{"predict": "auto& ldAl = parsed_args.ldAl;\n84:   auto& ldBl = parsed_args.ldBl;\n85:   int rank_l = int(l.size());\n86: \n87:   // Parse out strides for each tensor\n88:   StrideA StrideA_(ldAm, ldAk, ldAl);\n89:   StrideB StrideB_(ldBn, ldBk, ldBl);\n90:   StrideC StrideC_(ldCm, ldCn, ldCl);\n91:   StrideD StrideD_(ldCm, ldCn, ldCl);\n92: \n93:   // Define the tensor views\n94:   Tensor<ElementA, Layout<StrideA>> A{parsed_args.A.data(), StrideA_};\n95:   Tensor<ElementB, Layout<StrideB>> B{parsed_args.B.data(), StrideB_};\n96:   Tensor<ElementC, Layout<StrideC>> C{parsed_args.C.data(), StrideC_};\n97:   Tensor<ElementD, Layout<StrideD>> D{parsed_args.D.data(), StrideD_};\n98: \n99:   // Perform the GEMM operation\n100:   cutlass::gemm::device::Gemm<ElementA, ElementB, ElementC, ElementD, ElementAccumulator, ElementEpilogue, Layout<StrideA>, Layout<StrideB>, Layout<StrideC>, Layout<StrideD>, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm90, cutlass::gemm::GemmShape<32, 32, 16>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::gemm::GemmShape<16, 8, 16>, cutlass::epilogue::thread::LinearCombination<ElementC, 1, ElementAccumulator, ElementEpilogue>, cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 32, 1, 128, cutlass::arch::OpMultiplyAdd, cutlass::gemm::KernelArguments<ElementA, ElementB, ElementC, ElementD, ElementAccumulator, ElementEpilogue, Layout<StrideA>, Layout<StrideB>, Layout<Stride", "label": "(161, CUDA, thrust::device_vector<ElementA> d_A = h_A;); (183, CUDA, cuda_err = cudaDeviceSynchronize();); (148, CUDA, thrust::host_vector<ElementB> h_B(N_size * K_size * L_size);); (184, CUDA, std::cerr << cudaGetErrorString(cuda_err) << \"\\n\";); (148, CUDA, thrust::host_vector<ElementD> h_D(M_size * N_size * L_size);); (157, CUDA, thrust::device_vector<ElementB> d_B = h_B;); (158, CUDA, thrust::device_vector<ElementD> cutlass_result = h_D;); (144, CUDA, thrust::host_vector<ElementC> h_C(M_size * N_size * L_size);); (213, CUDA, thrust::host_vector<ElementD> h_cutlass_result = cutlass_result;); (156, CUDA, thrust::device_vector<ElementD> reference_result = h_D;); (153, CUDA, thrust::device_vector<ElementC> d_C = h_C;); (138, CUDA, thrust::host_vector<ElementA> h_A(M_size * K_size * L_size);)"}
{"predict": "(6, CUDA, auto isActive = detail::cuda::cuDevicePrimaryContextIsActive(0);); (9, CUDA, isActive = detail::cuda::cuDevicePrimaryContextIsActive(0);); (12, CUDA, EXPECT_THROW(detail::cuda::cuDevicePrimaryContextIsActive(-1), exception::InvalidCUDAdevice);); (17, CUDA, EXPECT_THROW(detail::cuda::cuDevicePrimaryContextIsActive(deviceCount), exception::InvalidCUDAdevice);)", "label": "(18, CUDA, gpuErrchk(cudaGetDeviceCount(&deviceCount));); (1, CUDA, gpuErrchk(cudaSetDevice(0));); (2, CUDA, gpuErrchk(cudaFree(0));); (7, CUDA, gpuErrchk(cudaDeviceReset());)"}
{"predict": "(33, MPI, MPI_Error_string(ERR_CODE, aux_string, &L);)", "label": "(36, MPI, MPI_Error_string(ERR_CODE, aux_string, &L);)"}
{"predict": "i <= iend; i++) {\n106:           for (m = 0; m < 5; m++) {\n107:             u21i = frct[m][k][j][i];\n108:             u21j = frct[m][k][j][i+1];\n109:             u21k = frct[m][k][j+1][i];\n110:             u21im1 = frct[m][k][j][i-1];\n111:             u21jm1 = frct[m][k][j-1][i];\n112:             u21km1 = frct[m][k-1][j][i];\n113:             frct[m][k][j][i] = (u21i + u21j + u21k + u21im1 + u21jm1 + u21km1) / 6.0;\n114:           }\n115:         }\n116:       }\n117:     }\n118: \n119:     //---------------------------------------------------------------------\n120:     // second time step\n121:     //---------------------------------------------------------------------\n122:     for (k = 1; k < nz - 1; k++) {\n123:       for (j = jst; j <= jend; j++) {\n124:         for (i = ist; i <= iend; i++) {\n125:           frct[0][k][j][i] = 0.0;\n126:           frct[1][k][j][i] = 0.0;\n127:           frct[2][k][j][i] = 0.0;\n128:           frct[3][k][j][i] = 0.0;\n129:           frct[4][k][j][i] = 0.0;\n130:         }\n131:       }\n132:     }\n133:   }\n134: }", "label": "(22, OpenMP, #pragma omp distribute); (17, OpenMP, #pragma omp target data map(alloc: frct, rsd, ce, flux_G)); (19, OpenMP, #pragma omp target teams)"}
{"predict": "(71, MPI, double stop = MPI_Wtime();)", "label": "(22, CUDA, CUDA_RT_CALL(cudaMemset(a_new, 0, nx * ny * sizeof(real)));); (100, CUDA, CUDA_RT_CALL(cudaStreamDestroy(compute_stream));); (105, CUDA, CUDA_RT_CALL(cudaFree(a));); (23, CUDA, CUDA_RT_CALL(cudaDeviceSynchronize());); (100, CUDA, CUDA_RT_CALL(cudaFree(l2_norm_d));); (27, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&push_top_done, cudaEventDisableTiming));); (24, CUDA, CUDA_RT_CALL(cudaStreamCreate(&push_bottom_stream));); (26, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&push_bottom_done, cudaEventDisableTiming));); (49, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(compute_stream, push_bottom_done, 0));); (67, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(push_bottom_stream, compute_done, 0));); (69, CUDA, CUDA_RT_CALL(cudaEventRecord(push_bottom_done, push_bottom_stream));); (89, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_top_stream));); (86, CUDA, CUDA_RT_CALL(cudaEventDestroy(compute_done));); (81, CUDA, CUDA_RT_CALL(cudaMemcpy(a_ref_h, a, nx * ny * sizeof(real), cudaMemcpyDeviceToHost));); (82, CUDA, CUDA_RT_CALL(cudaEventDestroy(push_bottom_done));); (14, CUDA, CUDA_RT_CALL(cudaStreamCreate(&push_top_stream));); (81, CUDA, CUDA_RT_CALL(cudaEventDestroy(push_top_done));); (65, CUDA, CUDA_RT_CALL(cudaStreamSynchronize(compute_stream));); (44, CUDA, CUDA_RT_CALL(cudaEventRecord(compute_done, compute_stream));); (17, CUDA, CUDA_RT_CALL(cudaMallocHost(&l2_norm_h, sizeof(real)));); (54, CUDA, CUDA_RT_CALL(cudaEventRecord(push_top_done, push_top_stream));); (33, CUDA, CUDA_RT_CALL(cudaMemsetAsync(l2_norm_d, 0, sizeof(real), compute_stream));); (6, CUDA, CUDA_RT_CALL(cudaStreamCreate(&compute_stream));); (-2, CUDA, CUDA_RT_CALL(cudaMemset(a, 0, nx * ny * sizeof(real)));); (7, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&compute_done, cudaEventDisableTiming));); (81, CUDA, CUDA_RT_CALL(cudaFree(a_new));); (45, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(push_top_stream, compute_done, 0));); (72, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_bottom_stream));); (28, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(compute_stream, push_top_done, 0));); (6, CUDA, CUDA_RT_CALL(cudaMalloc(&l2_norm_d, sizeof(real)));); (21, MPI, double start = MPI_Wtime();); (-13, CUDA, CUDA_RT_CALL(cudaMalloc(&a, nx * ny * sizeof(real)));); (71, CUDA, CUDA_RT_CALL(cudaFreeHost(l2_norm_h));); (-14, CUDA, CUDA_RT_CALL(cudaMalloc(&a_new, nx * ny * sizeof(real)));)"}
{"predict": "(17, OpenMP, #pragma omp parallel for private(k, lk, ii, i, lwork, knsupc))", "label": "(30, MPI, MPI_Bcast( work, diag_len[p], MPI_DOUBLE, pkk, grid->comm );); (27, MPI, MPI_Bcast( work, lwork, MPI_DOUBLE, pkk, grid->comm );)"}
{"predict": "(4, CUDA, IntType gidx = blockIdx.x * blockDim.x + threadIdx.x;);", "label": "(4, CUDA, const T** input_ptrs = GetCudaDeviceArrayOnDevice(&input_ptr_data);)"}
{"predict": "(4, MPI, double *bins_ptr = bins.get_host_ptr();) (8, MPI, double *global_ptr = global_bins.get_host_ptr();) (13, MPI, if (op == \"sum\" || op == \"pdf\" || op == \"avg\" || op == \"count\" || op == \"std\" || op == \"var\" || op == \"rms\") { MPI_Allreduce(bins_ptr, global_ptr, bins_size, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD); }) (15, MPI, else if (op == \"min\") { MPI_Allreduce(bins_ptr, global_ptr, bins_size, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD); }) (17, MPI, else if (op == \"max\") { MPI_Allreduce(bins_ptr, global_ptr, bins_size, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD); }) (20, MPI, bins = global_bins;)", "label": "(17, MPI, MPI_Allreduce(bins_ptr, global_ptr, bins_size, MPI_DOUBLE, MPI_MIN, mpi_comm);); (12, MPI, MPI_Allreduce(bins_ptr, global_ptr, bins_size, MPI_DOUBLE, MPI_SUM, mpi_comm);); (19, MPI, MPI_Allreduce(bins_ptr, global_ptr, bins_size, MPI_DOUBLE, MPI_MAX, mpi_comm);); (2, MPI, MPI_Comm mpi_comm = MPI_Comm_f2c(flow::Workspace::default_mpi_comm());)"}
{"predict": "(12, OpenMP, #pragma omp parallel for private(p1, visc, arp1, posp1, rsymp1, velrhop1, ngs, z, y, pif, drx, dry, drz, rr2, fac, frx, fry, frz, massp2, compute, ftp2, velrhop2, dvx, dvy, dvz, dot, dot_rr2, rsym))", "label": "(71, OpenMP, const int th=omp_get_thread_num();); (10, OpenMP, #pragma omp parallel for schedule (guided))"}
{"predict": "(12, MPI, if (wrank == 0) { MPI_Bcast(dims, MAX_DIMS, MPI_INT, 0, MPI_COMM_WORLD); } else { MPI_Bcast(dims, MAX_DIMS, MPI_INT, 0, MPI_COMM_WORLD); })", "label": "(10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (15, MPI, MPI_Dims_create(tests[k].size, tests[k].dim, dims);)"}
{"predict": "(1, CUDA, __global__ void tiled_xgemm_1d_coarse_kernel(float* __restrict__ Ad, float* __restrict__ Bd, float* __restrict__ Cd, int M, int N, int K) {)", "label": "(17, CUDA, __shared__ float a_smem[BM * BK];); (17, CUDA, __shared__ float b_smem[BK * BN];)"}
{"predict": "(17, OpenCL, cl_platform_id platform = getPlatformFromDevice(device);) (20, OpenCL, GET_EXTENSION_FUNC(platform, clGetImageRequirementsInfoEXT);) (62, OpenCL, cl_mem buffer = clCreateBuffer(context, flag, max_size, nullptr, &err);) (69, OpenCL, cl_mem image_buffer = clCreateImage(context, flag, &format, &image_desc, nullptr, &err);) (83, OpenCL, test_error(err, \"Error clReleaseMemObject\");) (86, OpenCL, test_error(err, \"Error clReleaseMemObject\");)", "label": "(86, OpenCL, err = clReleaseMemObject(buffer);); (82, OpenCL, err = clReleaseMemObject(image_buffer);)"}
{"predict": "(27, CUDA, cudaEvent_t start, stop; cudaEventCreate(&start); cudaEventCreate(&stop); cudaEventRecord(start, 0);) (38, CUDA, cudaEventRecord(stop, 0); cudaEventSynchronize(stop); float milliseconds = 0; cudaEventElapsedTime(&milliseconds, start, stop); sdkStopTimer(&timer); sdkAddTimerValue(&timer, milliseconds); sdkDeleteTimer(&timer); cudaEventDestroy(start); cudaEventDestroy(stop);)", "label": "(12, CUDA, cudaMallocHost((void**)&h_a, bufsize);); (13, CUDA, cudaMallocHost((void**)&h_c, bufsize);); (43, CUDA, cudaDeviceSynchronize();); (20, CUDA, cudaMalloc((void**)&d_a, bufsize);); (63, CUDA, cudaFree(d_c);); (61, CUDA, cudaFree(d_b);); (7, CUDA, cudaMallocHost((void**)&h_b, bufsize);); (64, CUDA, cudaFreeHost(h_b);); (64, CUDA, cudaFreeHost(h_c);); (56, CUDA, cudaFree(d_a);); (60, CUDA, cudaFreeHost(h_a);); (14, CUDA, cudaMalloc((void**)&d_c, bufsize);); (12, CUDA, cudaMalloc((void**)&d_b, bufsize);)"}
{"predict": "(14, OpenMP, #pragma omp parallel for private(reg))", "label": "(43, OpenMP, lis2mdl_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)"}
{"predict": "(6, TBB, ValueType r2 = tbb::parallel_reduce(m_range, I, [](const tbb::blocked_range<ValueType*>& r, ValueType value) -> ValueType { for ( const ValueType* pv = r.begin(); pv != r.end(); ++pv ) value += *pv; return value; }, Sum(), partitioner);)", "label": "(3, TBB, ValueType r1 = tbb::parallel_reduce( m_range, I, Accumulator(), Sum(), partitioner );)"}
{"predict": "(1, CUDA, __global__ void kernel_forward(const int B, const int T, const int C, const int H, const F *__restrict__ const _r, const F *__restrict__ const _k, const F *__restrict__ const _v, const F *__restrict__ _w, const F *__restrict__ _u, const F *__restrict__ _s, F *__restrict__ const _y) {)", "label": "(11, CUDA, __shared__ float r[_N_], k[_N_], u[_N_], w[_N_];)"}
{"predict": "(103, OpenMP, #pragma omp parallel for private(instance, dist, rng))", "label": "(50, CUDA, flamegpu::CUDASimulation cudaSimulation(model, argc, argv);)"}
{"predict": "(27, CUDA, dim3 grid(1,1); dim3 block(32, 1, 1);)\n(28, CUDA, kernel<<< grid, block >>>(D.device_data(), alpha, A.device_data(), B.device_data(), beta, C.device_data());)", "label": "(12, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (13, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;); (80, CUDA, cudaError_t result = cudaDeviceSynchronize();)"}
{"predict": "(10, MPI, pcu::PCU PCUObj = pcu::PCU(MPI_COMM_WORLD);)", "label": "(22, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(13, MPI, MPI_Get_address(inbuf, &lb); MPI_Get_extent(int_with_key, &extent);) (15, MPI, int N = 0;) (16, MPI, int cnt = *(int*) ((char*)inbuf + i * extent); N += cnt;) (26, MPI, const char* ptr = (const char*) inbuf;) (27, MPI, void* target = (void*) (ptr + sizeof(int));) (31, MPI, int current_weight = *(int*) ptr;) (32, MPI, i++; ptr += extent;) (36, MPI, int next_weight = *(int*) ptr;) (38, MPI, void* next_target = (void*) (ptr + sizeof(int));) (41, MPI, result = dtcmp_op_eval(target, next_target, cmp);) (46, MPI, current_weight += next_weight;) (48, MPI, ptr += extent;) (51, MPI, next_weight = *(int*) ptr;) (53, MPI, next_target = (void*) (ptr + sizeof(int));) (56, MPI, result = dtcmp_op_eval(target, next_target, cmp);) (66, MPI, int after_weight = N - before_weight - current_weight;) (68, MPI, break;) (70, MPI, before_weight += current_weight;) (78, MPI, int* num = (int*) outbuf;) (79, MPI, *num = N;) (80, MPI, memcpy((char*)outbuf + sizeof(int), target, extent - sizeof(int));)", "label": "(11, MPI, MPI_Type_get_extent(int_with_key, &lb, &extent);)"}
{"predict": "(12, OpenCL, cl_event eventBlocking = userEvent;); (16, OpenCL, cl_event eventOut = nullptr;); (27, OpenCL, clSetEventCallback(eventOut, CL_COMPLETE, callback, &data);); (30, OpenCL, clSetUserEventStatus(eventBlocking, CL_COMPLETE);); (32, OpenCL, userEvent->release();); (34, OpenCL, data.signalCallbackDoneEvent->release();); (38, OpenCL, context->getAsyncEventsHandler().closeThread();)", "label": "(15, OpenCL, retVal = clEnqueueNDRangeKernel(clCommandQ, clKernel, 1, offset, gws, nullptr, 1, &eventBlocking, &eventOut);); (13, OpenCL, clCommandQ = clCreateCommandQueue(context, devices[0], properties, &retVal);); (38, OpenCL, clReleaseEvent(eventOut);); (31, OpenCL, clWaitForEvents(1, &callbackEvent);); (37, OpenCL, clReleaseCommandQueue(clCommandQ);); (28, OpenCL, clWaitForEvents(1, &eventOut);)"}
{"predict": "(11, CUDA, CUDA_CHECK(cudaMallocManaged(reinterpret_cast<void **>(&d_data), sizeof(data_type) * h_data.size()));) (25, CUDA, CURAND_CHECK(curandSetGeneratorOffset(gen, offset + blockIdx.x * blockDim.x));) (31, CUDA, CURAND_CHECK(curandGenerateLogNormal(gen, d_data + blockIdx.x * blockDim.x, blockDim.x, mean, stddev));) (33, CUDA, CUDA_CHECK(cudaMemcpyAsync(h_data.data() + blockIdx.x * blockDim.x, d_data + blockIdx.x * blockDim.x, sizeof(data_type) * blockDim.x, cudaMemcpyDeviceToHost, stream));)", "label": "(38, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (40, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(14, OpenMP, #pragma omp parallel for)", "label": "(12, OpenMP, #pragma omp target)"}
{"predict": "(7, OpenMP, #pragma omp parallel for)", "label": "(10, MPI, MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);)"}
{"predict": "(10, MPI, MPI_Type_indexed(3, len, disp, MPI_INT, &type);)", "label": "(33, MPI, MPI_Type_free(&type);); (16, MPI, MPI_Type_get_extent(type, &lb, &extent);); (7, MPI, err = MPI_Type_indexed(3, len, disp, MPI_INT, &type);)"}
{"predict": "(23, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);)", "label": "(23, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (17, MPI, MPI_Alltoallw_c(sendbuf, counts, displs, types, recvbuf, counts, displs, types, comm);)"}
{"predict": "(8, OpenMP, #pragma omp parallel for private(i))", "label": "(3, OpenMP, #pragma omp target enter data map(alloc:i)); (6, OpenMP, #pragma omp target data map(present, alloc: i)); (7, OpenMP, #pragma omp target exit data map(delete:i))"}
{"predict": "(5, MPI, aio_req = (ADIOI_AIO_Request *) extra_state;)\n(17, MPI, aio_req->nbytes = aio_req->qaiocbp->result;)\n(21, MPI, aio_req->nbytes = aio_req->qaiocbp->result;)\n(25, MPI, return errcode;)\n\nPlease note that the provided code does not have any obvious candidates for parallelization without additional context or knowledge about the application's requirements. The given code appears to be a function that handles asynchronous I/O operations and does not contain any loops or operations that could be parallelized using standard parallelization techniques like OpenMP or MPI.", "label": "(10, MPI, errcode = MPI_Grequest_complete(aio_req->req);)"}
{"predict": "(12, OpenMP, #pragma omp parallel for private(sum))", "label": "(20, OpenMP, #pragma omp flush); (13, OpenMP, #pragma omp task private(sum) shared(result, known_sum)); (7, OpenMP, #pragma omp parallel); (8, OpenMP, #pragma omp single); (21, OpenMP, #pragma omp critical)"}
{"predict": "(4, CUDA, cudaFree(AP_);) (7, CUDA, cudaFree(backAP_);) (10, CUDA, cudaFree(X_);)", "label": "(20, OpenCL, clReleaseMemObject(mobjAP_);); (16, OpenCL, clReleaseMemObject(mobjX_);)"}
{"predict": "(15, OpenCL, cl_int err; cl_queue_properties properties[] = {CL_QUEUE_PROPERTIES, CL_QUEUE_PROFILING_ENABLE, 0}; cl_command_queue ocl_queue = clCreateCommandQueueWithProperties(ocl_ctx, ocl_dev, properties, &err); TEST_OCL_CHECK(err);)", "label": "(27, OpenCL, TEST_OCL_CHECK(clReleaseCommandQueue(ocl_queue));)"}
{"predict": "(10, MPI, MPI_Comm_rank(comm, &comm_rank);) (11, MPI, MPI_Comm_size(comm, &comm_size);)", "label": "(9, MPI, MPI_Comm_rank(comm, &comm_rank);); (10, MPI, MPI_Comm_size(comm, &comm_size);)"}
{"predict": "(27, CUDA, sign_kernel<<<batch, MLDSA44Sign::BlockDim>>>(d_signatures, d_messages, message_size, d_secret_keys, randombytes, workspace);)", "label": "(36, CUDA, cudaMemcpy(signatures.data(), d_signatures, length_signature_aligned * batch, cudaMemcpyDeviceToHost);); (40, CUDA, cudaFree(d_secret_keys);); (26, CUDA, cudaMemcpy(d_secret_keys, secret_keys.data(), length_secret_key * batch, cudaMemcpyHostToDevice);); (26, CUDA, cudaMemcpy(d_messages, messages.data(), message_size * batch, cudaMemcpyHostToDevice);); (39, CUDA, cudaFree(d_messages);); (17, CUDA, cudaMalloc(reinterpret_cast<void**>(&d_secret_keys), length_secret_key * batch);); (15, CUDA, cudaMalloc(reinterpret_cast<void**>(&d_signatures), length_signature_aligned * batch);); (35, CUDA, cudaFree(d_signatures);); (15, CUDA, cudaMalloc(reinterpret_cast<void**>(&d_messages), message_size * batch);)"}
{"predict": "(27, OpenMP, #pragma omp parallel for reduction(+:o_ptr))", "label": "(19, OpenMP, #pragma omp parallel for)"}
{"predict": "(16, MPI, MPI_Isend(&value, 1, MPI_INT, right, tag, MPI_COMM_WORLD, &request);) (18, MPI, MPI_Recv(&buffer, 1, MPI_INT, left, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)", "label": "(27, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Init(NULL, NULL);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (20, MPI, MPI_Wait(&request, MPI_STATUS_IGNORE);); (6, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (9, MPI, MPI_Bcast(&factor, 1, MPI_INT, root, MPI_COMM_WORLD);)"}
{"predict": "(17, OpenMP, #pragma omp parallel for reduction(+:c[:10]))", "label": "(3, OpenMP, #pragma omp target); (4, OpenMP, #pragma omp distribute parallel for simd); (2, OpenMP, #pragma omp teams)"}
{"predict": "(12, OpenMP, #pragma omp parallel for)", "label": "(5, OpenMP, #pragma omp target teams distribute parallel for)"}
{"predict": "(4, MPI, MPI_Init(NULL, NULL);)", "label": "(43, MPI, err = MPI_Type_size(eviltype, &val);); (100, MPI, MPI_Type_free(&inttype);); (19, MPI, err = MPI_Type_create_resized(tmptype, 6, -9, &inttype);); (55, MPI, err = MPI_Type_get_extent(eviltype, &lb, &extent);); (28, MPI, err = MPI_Type_vector(3, 4, 5, inttype, &eviltype);); (95, MPI, MPI_Type_free(&tmptype);); (3, MPI, err = MPI_Type_contiguous(4, MPI_BYTE, &tmptype);); (95, MPI, MPI_Type_free(&eviltype);); (71, MPI, err = MPI_Type_get_true_extent(eviltype, &true_lb, &aval);)"}
{"predict": "(37, CUDA, cudaMalloc((void**)&d_x, N*sizeof(real));)\n(38, CUDA, cudaMalloc((void**)&d_y, N*sizeof(real));)\n(39, CUDA, cudaMalloc((void**)&d_NN, N*sizeof(int));)\n(40, CUDA, cudaMalloc((void**)&d_NL, N*M*sizeof(int));)\n(41, CUDA, cudaFree(d_x);)\n(42, CUDA, cudaFree(d_y);)\n(43, CUDA, cudaFree(d_NN);)\n(44, CUDA, cudaFree(d_NL);)", "label": "(78, CUDA, CHECK(cudaFree(d_y));); (74, CUDA, CHECK(cudaFree(d_NN));); (56, CUDA, CHECK(cudaMemcpy(NL, d_NL, N*M*sizeof(int), cudaMemcpyDefault));); (34, CUDA, CHECK(cudaMalloc(&d_NL, N*M*sizeof(int)));); (32, CUDA, CHECK(cudaMalloc(&d_NN, N*sizeof(int)));); (46, CUDA, CHECK(cudaMemcpy(d_y, y.data(), N*sizeof(real), cudaMemcpyDefault));); (51, CUDA, CHECK(cudaMemcpy(NN, d_NN, N*sizeof(int), cudaMemcpyDefault));); (33, CUDA, CHECK(cudaMalloc(&d_y, N*sizeof(real)));); (69, CUDA, CHECK(cudaFree(d_x));); (30, CUDA, CHECK(cudaMalloc(&d_x, N*sizeof(real)));); (66, CUDA, CHECK(cudaFree(d_NL));)"}
{"predict": "(20, MPI, buf = (char *) malloc(bufsize);)", "label": "(19, MPI, MPI_Pack_size(2, MPI_DOUBLE, comm, &s2);); (38, MPI, MPI_Recv(rmsg2, 10, MPI_DOUBLE, src, tag, comm, MPI_STATUS_IGNORE);); (21, MPI, MPI_Buffer_attach(buf, bufsize);); (56, MPI, MPI_Buffer_detach(&bbuf, &bsize);); (34, MPI, MPI_Recv(rmsg1, 7, MPI_CHAR, src, tag, comm, MPI_STATUS_IGNORE);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (27, MPI, MPI_Bsend(msg2, 2, MPI_DOUBLE, dest, tag, comm);); (13, MPI, MPI_Pack_size(17, MPI_CHAR, comm, &s3);); (24, MPI, MPI_Bsend(msg1, 7, MPI_CHAR, dest, tag, comm);); (25, MPI, MPI_Bsend(msg3, 17, MPI_CHAR, dest, tag, comm);); (8, MPI, MPI_Pack_size(7, MPI_CHAR, comm, &s1);); (29, MPI, MPI_Recv(rmsg3, 17, MPI_CHAR, src, tag, comm, MPI_STATUS_IGNORE);)"}
{"predict": "(3, OpenCL, SKIP_IF(engine::get_count(eng_kind) == 0, \"Engine not found.\");)", "label": "(9, SYCL, sycl::device device;)"}
{"predict": "I)\n79:   //DUMP: OMPInteropDirective\n80:   //DUMP: OMPInitClause\n81:   //DUMP: DeclRefExpr{{.*}}'omp_interop_t'{{.*}}Var{{.*}}'I'\n82:   //DUMP: IntegerLiteral{{.*}}2\n83:   //DUMP: IntegerLiteral{{.*}}4\n84:   //DUMP: IntegerLiteral{{.*}}6\n85:   //DUMP: IntegerLiteral{{.*}}1\n86: \n87:   //PRINT: #pragma omp interop use(target : I)\n88:   //DUMP: OMPInteropDirective\n89:   //DUMP: OMPUseClause\n90:   //DUMP: DeclRefExpr{{.*}}'omp_interop_t'{{.*}}Var{{.*}}'I'\n91: \n92:   //PRINT: #pragma omp interop destroy(target : I)\n93:   //DUMP: OMPInteropDirective\n94:   //DUMP: OMPDestroyClause\n95:   //DUMP: DeclRefExpr{{.*}}'omp_interop_t'{{.*}}Var{{.*}}'I'\n96: \n97:   //PRINT: #pragma omp interop init(target : IRef)\n98:   //DUMP: OMPInteropDirective\n99:   //DUMP: OMPInitClause\n100:  //DUMP: DeclRefExpr{{.*}}'omp_interop_t'{{.*}}Var{{.*}}'IRef'\n101:  //DUMP: IntegerLiteral{{.*}}1\n102:  //DUMP: IntegerLiteral{{.*}}2\n103:  //DUMP: IntegerLiteral{{.*}}3\n104:  //DUMP: IntegerLiteral{{.*}}4\n105:  //DUMP: IntegerLiteral{{.*}}5\n106:  //DUMP: IntegerLiteral{{.*}}6\n107: \n108:  //PRINT: #pragma omp interop init(prefer_type(1,2,3,4,5,6), targetsync : IRef)\n109:  //DUMP: OMPInteropDirective\n110:  //DUMP:", "label": "(140, OpenMP, #pragma omp interop use(I) use(J)); (129, OpenMP, #pragma omp interop init(target:I) init(targetsync:J)); (24, OpenMP, #pragma omp interop init(target:IRef)); (31, OpenMP, #pragma omp interop destroy(IRef)); (71, OpenMP, #pragma omp interop device(dev) depend(inout:ap) destroy(I)); (159, OpenMP, #pragma omp interop destroy(I) use(J)); (4, OpenMP, #pragma omp interop use(I)); (141, OpenMP, #pragma omp interop destroy(I) destroy(J)); (47, OpenMP, #pragma omp interop device(dev) depend(inout:ap) init(targetsync:I)); (7, OpenMP, #pragma omp interop destroy(I)); (-2, OpenMP, #pragma omp interop init(target:I)); (127, OpenMP, #pragma omp interop init(target:I) use(J)); (29, OpenMP, #pragma omp interop use(CI)); (84, OpenMP, #pragma omp interop init(prefer_type(2,4,6,1),targetsync:I)); (14, OpenMP, #pragma omp interop use(IRef)); (50, OpenMP, #pragma omp interop device(dev) depend(inout:ap) use(I)); (71, OpenMP, #pragma omp interop init(prefer_type(1,2,3,4,5,6),targetsync:I)); (145, OpenMP, #pragma omp interop init(target:I) destroy(J)); (-14, OpenMP, #pragma omp interop init()"}
{"predict": "(10, CUDA, CUDA_CHECK(cudaMallocManaged(reinterpret_cast<void **>(&d_data), sizeof(data_type) * h_data.size()));)", "label": "(36, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (38, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(33, CUDA, if (col % blockDim.x == 0) {); (35, CUDA, value.x = (float) (col + blockIdx.x * blockDim.x);); (36, CUDA, value.y = (float) row;); (41, CUDA, } else {); (43, CUDA, value.x = (float) col;); (44, CUDA, value.y = (float) (row + blockIdx.y * blockDim.y);); (51, CUDA, } else {); (53, CUDA, value.x = (float) (col + blockIdx.x * blockDim.x);); (54, CUDA, value.y = (float) row;); (60, CUDA, } else {); (62, CUDA, tex.filterMode = filterMode;); (63, CUDA, tex.addressMode[0] = addressModeX;); (64, CUDA, tex.addressMode[1] = addressModeY;); (65, CUDA, } else {); (67, CUDA, tex.filterMode = filterMode;); (68, CUDA, tex.addressMode[0] = addressModeX;); (69, CUDA, tex.addressMode[1] = addressModeY;); (70, CUDA, } else {); (72, CUDA, tex.filterMode = filterMode;); (73, CUDA, tex.addressMode[0] = addressModeX;); (74, CUDA, tex.addressMode[1] = addressModeY;); (75, CUDA, } else {); (77, CUDA, tex.filterMode = filterMode;); (78, CUDA, tex.addressMode[0] = addressModeX;); (79, CUDA, tex.addressMode[1] = addressModeY;); (80, CUDA, } else {); (82, CUDA, tex.filterMode = filterMode;); (83, CUDA, tex.addressMode[0] = addressModeX;); (84, CUDA, tex.addressMode[1] = addressModeY;);", "label": "(86, CUDA, cudaFreeHost( outHost );); (84, CUDA, cudaFree( texDevice );)"}
{"predict": "(19, OpenMP, #pragma omp parallel for reduction(min:sum2) collapse(2))", "label": "(31, OpenMP, #pragma omp teams reduction(+:sum)); (28, OpenMP, #pragma omp target map(tofrom: b, sum, t) map(alloc: a)); (31, OpenMP, #pragma omp distribute parallel for reduction(+:sum)); (33, OpenMP, t[i] = omp_get_team_num();)"}
{"predict": "(115, OpenMP, #pragma omp parallel for)", "label": "(91, OpenMP, const int NumThreads = omp_get_max_threads();)"}
{"predict": "17: \n118:     sdkDeleteTimer(&hTimer);\n119: \n120:     free(h_OutputGPU);\n121:     printf(\"Test passed.\\n\");\n122:     return 0;\n123: }", "label": "(44, CUDA, checkCudaErrors(cudaMemset(d_Output, 0, QRNG_DIMENSIONS * N * sizeof(float)));); (20, CUDA, int dev = findCudaDevice(argc, (const char **)argv);); (49, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (123, CUDA, checkCudaErrors(cudaFree(d_Output));); (18, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, dev));); (28, CUDA, checkCudaErrors(cudaMalloc((void **)&d_Output, QRNG_DIMENSIONS * N * sizeof(float)));); (60, CUDA, checkCudaErrors(cudaMemcpy(h_OutputGPU, d_Output, QRNG_DIMENSIONS * N * sizeof(float), cudaMemcpyDeviceToHost));)"}
{"predict": "2: static double u[IMAX/2*2+1][JMAX/2*2+1][KMAX/2*2+1][5+1];\n103: static double v[IMAX/2*2+1][JMAX/2*2+1][KMAX/2*2+1][5+1];\n104: static double w[IMAX/2*2+1][JMAX/2*2+1][KMAX/2*2+1][5+1];\n105: static double dt_cfl[IMAX/2*2+1][JMAX/2*2+1][KMAX/2*2+1];\n106: static double u_g[IMAX/2*2+1][JMAX/2*2+1][KMAX/2*2+1];\n107: static double v_g[IMAX/2*2+1][JMAX/2*2+1][KMAX/2*2+1];\n108: static double w_g[IMAX/2*2+1][JMAX/2*2+1][KMAX/2*2+1];\n109: static double dx, dy, dz;\n110: static double dxty1, dxty2, dxty3, dxtz1, dxtz2, dxtz3;\n111: static double dytz1, dytz2, dytz3, dytx1, dytx2, dytx3;\n112: static double dxtx1, dxtx2, dxtx3, dyty1, dyty2, dyty3;\n113: static double dtyz1, dtyz2, dtyz3, dtyx1, dtyx2, dtyx3;\n114: static double dtxy1, dtxy2, dtxy3, dtyz1, dtyz2, dtyz3;\n115: static double dtxy2, dtxy3, dtxy1, dtyz2, dtyz3, dtyz1;\n116: static double dtxy3, dtxy1, dtxy2, dtyz3, dtyz1, dtyz2;\n11", "label": "(249, OpenMP, nthreads = omp_get_num_threads();); (234, OpenMP, #pragma omp parallel firstprivate(niter) private(step)); (214, OpenMP, #pragma omp parallel); (237, OpenMP, #pragma omp master)"}
{"predict": "(13, MPI, if (rank == 0) { p = getenv(\"TMP_ENV_VAR\"); if (!p) { errs++; printf(\"Did not find TMP_ENV_VAR\\n\"); } else if (strcmp(p, \"1\") != 0) { errs++; printf(\"Value of TMP_ENV_VAR was %s, expected 1\\n\", p); } } else { p = nullptr; } )", "label": "(6, MPI, MPI_Init(&argc, &argv);); (32, MPI, MPI_Finalize();); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (20, MPI, MPI_Reduce(&errs, &toterrs, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);)"}
{"predict": "i=0 0c0  i=1 0c0&1&4  i=2 0c0&1&4&3  i=3 0c0&1&4&3&2\n114:             ompi_request_t *req;\n115:             int tag = i;\n116:             MPI_Request mpi_req;\n117:             MPI_Status mpi_status;\n118:             MPI_Isend(&myleaderrank, 1, MPI_INT, rpeer, tag, leader_comm, &mpi_req);\n119:             MPI_Wait(&mpi_req, &mpi_status);\n120:             MPI_Irecv(&speer, 1, MPI_INT, speer, tag, leader_comm, &req);\n121:             MPI_Wait(req, &mpi_status);\n122:             speer = rpeer;\n123:             rpeer = (rpeer - 1 + nleaderranks) % nleaderranks;\n124:         }\n125:     }\n126: \n127:     // Free the memory used to store the communication method for each rank.\n128:     free(method);\n129:     // Free the local_comm and leader_comm communicators.\n130:     ompi_comm_free(&local_comm);\n131:     ompi_comm_free(&leader_comm);\n132: }", "label": "(150, MPI, MPI_Type_commit(&mydt);); (154, MPI, MPI_Op_free(&myop);); (154, MPI, MPI_Type_free(&mydt);); (146, MPI, MPI_Type_contiguous(sizeof(comm_method_string_conversion_t), MPI_BYTE, &mydt);); (147, MPI, MPI_Op_create(myfn, 1, &myop);)"}
{"predict": "(10, MPI, keyval_copy = keyval;)", "label": "(8, MPI, MPI_Alloc_mem(DATA_SZ, MPI_INFO_NULL, &base_ptr[0]);); (20, MPI, MPI_Win_free_keyval(&keyval);); (21, MPI, MPI_Win_free(&windows[1]);); (6, MPI, MPI_Win_create(base_ptr[0], DATA_SZ, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &windows[0]);); (12, MPI, MPI_Win_set_attr(windows[0], keyval, NULL);); (5, MPI, MPI_Alloc_mem(DATA_SZ, MPI_INFO_NULL, &base_ptr[1]);); (14, MPI, MPI_Free_mem(base_ptr[0]);); (15, MPI, MPI_Win_free_keyval(&keyval_copy);); (16, MPI, MPI_Free_mem(base_ptr[1]);); (4, MPI, MPI_Win_create_keyval(MPI_WIN_NULL_COPY_FN, delete_fn, &keyval, NULL);); (9, MPI, MPI_Win_free(&windows[0]);); (0, MPI, MPI_Win_create(base_ptr[1], DATA_SZ, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &windows[1]);); (5, MPI, MPI_Win_set_attr(windows[1], keyval, NULL);)"}
{"predict": "(49, OpenMP, #pragma omp parallel for reduction(+:trans_time))", "label": "(46, CUDA, thrust::fill(thrust::device, B.begin(), B.end(), 0.0);); (44, CUDA, thrust::sequence(thrust::device, A.begin(), A.end() );); (40, CUDA, thrust::device_vector<double> A(order*order);); (40, CUDA, thrust::device_vector<double> B(order*order);)"}
{"predict": "(27, CUDA, tiled_mma.launch());", "label": "(2, CUDA, extern __shared__ char shared_memory[];)"}
{"predict": "(1, CUDA, __global__ void softmax_loss_kernel(float *reduced_loss, float *predict, float *target, float *workspace, int batch_size, int num_outputs) {)", "label": "(5, CUDA, extern __shared__ float s_data[];)"}
{"predict": "28: #pragma omp parallel\n129:   {\n130:     foo();\n131:   }\n132: #pragma omp parallel\n133:   {\n134:     foo();\n135:   }\n136: #pragma omp parallel\n137:   {\n138:     foo();\n139:   }\n140: #pragma omp parallel\n141:   {\n142:     foo();\n143:   }\n144: #pragma omp parallel\n145:   {\n146:     foo();\n147:   }\n148: #pragma omp parallel\n149:   {\n150:     foo();\n151:   }\n152: #pragma omp parallel\n153:   {\n154:     foo();\n155:   }\n156: #pragma omp parallel\n157:   {\n158:     foo();\n159:   }\n160: #pragma omp parallel\n161:   {\n162:     foo();\n163:   }\n164: #pragma omp parallel\n165:   {\n166:     foo();\n167:   }\n168: #pragma omp parallel\n169:   {\n170:     foo();\n171:   }\n172: #pragma omp parallel\n173:   {\n174:     foo();\n175:   }\n176: #pragma omp parallel\n177:   {\n178:     foo();\n179:   }\n180: #pragma omp parallel\n181:   {\n182:     foo();\n183:   }\n184: #pragma omp parallel\n185:   {\n186:     foo();\n187:   }\n188: #pragma omp parallel\n189:   {\n190:     foo();\n191:   }\n192: #pragma omp parallel\n193:   {\n194:     foo();\n195:   }\n196: #pragma omp parallel\n197:   {\n198:     foo();\n199:   }\n200: #pragma omp parallel", "label": "(58, OpenMP, #pragma omp sections reduction(|| : argc > 0 ? argv[1] : argv[2])); (102, OpenMP, #pragma omp sections reduction(- : da)); (131, OpenMP, #pragma omp sections reduction(+ : o)); (75, OpenMP, #pragma omp sections reduction(+ : a, b, c, d, f)); (160, OpenMP, #pragma omp sections reduction(+ : fl)); (83, OpenMP, #pragma omp sections reduction(max : h.b)); (117, OpenMP, #pragma omp sections reduction(& : e, g)); (146, OpenMP, #pragma omp sections reduction(+ : r)); (20, OpenMP, #pragma omp sections reduction(-); (129, OpenMP, #pragma omp sections private(i), reduction(+ : j), reduction(+ : q)); (164, OpenMP, #pragma omp sections reduction(+ : m)); (102, OpenMP, #pragma omp sections reduction(&& : S2::S2s)); (61, OpenMP, #pragma omp sections reduction(^ : S1)); (80, OpenMP, #pragma omp sections reduction(+ : ba)); (154, OpenMP, #pragma omp parallel reduction(* : fl)); (-3, OpenMP, #pragma omp parallel); (67, OpenMP, #pragma omp sections reduction(min : a, b, c, d, f)); (111, OpenMP, #pragma omp sections reduction(+ : h, k, B::x)); (0, OpenMP, #pragma omp sections reduction +); (-6, OpenMP, #pragma omp sections reduction); (122, OpenMP, #pragma omp parallel private(k)); (87, OpenMP, #pragma omp sections reduction(^ : fl)); (31, OpenMP, #pragma omp sections reduction(| : argc,); (120, OpenMP, #pragma omp sections reduction(+ : p), reduction(+ : p)); (154, OpenMP, #pragma omp sections reduction(task, + : m)); (13, OpenMP, #pragma omp sections reduction(*)); (132, OpenMP, #pragma omp parallel reduction(min : i)); (136, OpenMP, #pragma omp parallel private(fl)); (40, OpenMP, #pragma omp sections reduction(&& : argc, z)); (69, OpenMP, #pragma omp sections reduction(* : ca)); (3, OpenMP, #pragma omp sections reduction()); (87, OpenMP, #pragma omp sections reduction(&& : S2::S2sc)); (127, OpenMP, #pragma omp sections reduction(max : j)); (-10, OpenMP, #pragma omp sections reduction(); (9, OpenMP, #pragma omp sections reduction(\\)); (13, OpenMP, #pragma omp sections reduction(foo : argc); (121, OpenMP, #pragma omp parallel shared(i)); (26, OpenMP, #pragma omp sections reduction(~ : argc))"}
{"predict": "(14, CUDA, if (!blas->Init()) { cudaDeviceSynchronize(); return nullptr; })", "label": "(14, CUDA, cuda::CUDABlas *blas = new cuda::CUDABlas(cuda_executor);)"}
{"predict": "(11, CUDA, kernel_strided_determinant_from_getrf<<<nbatch, 64>>>(N, m, lda, mstride, piv, pstride, LogOverlapFactor, res, nbatch);)", "label": "(13, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize());); (11, CUDA, qmc_cuda::cuda_check(cudaGetLastError());)"}
{"predict": "(11, MPI, fprintf(stderr, \"%s: errcode = %d, class = %d, msg = %s\\n\", str, code, class, msg);)", "label": "(11, MPI, MPI_Error_string(code, msg, &resultLen);); (12, MPI, MPI_Abort(MPI_COMM_WORLD, code);); (8, MPI, MPI_Error_class(code, &class);)"}
{"predict": "(2, CUDA, __global__ void sobel6PT(cr_Ptr<uchar> a, r_Ptr<uchar> b, int nx, int ny) {)", "label": "(2, CUDA, __shared__ uchar as[18][66];)"}
{"predict": "(21, OpenMP, #pragma omp target parallel for is_device_ptr(k);)\n(26, OpenMP, #pragma omp target parallel for is_device_ptr(z);)\n(31, OpenMP, #pragma omp target parallel for is_device_ptr(aa);)\n(36, OpenMP, #pragma omp target parallel for is_device_ptr(raa);)\n(41, OpenMP, #pragma omp target parallel for is_device_ptr(h);)\n(46, OpenMP, #pragma omp target parallel for is_device_ptr(rh);)\n(51, OpenMP, #pragma omp target parallel for is_device_ptr(da);)", "label": "(25, OpenMP, #pragma omp target parallel for is_device_ptr(z)); (36, OpenMP, #pragma omp target parallel for is_device_ptr(raa)); (29, OpenMP, #pragma omp target parallel for is_device_ptr(aa)); (46, OpenMP, #pragma omp target parallel for is_device_ptr(rh)); (39, OpenMP, #pragma omp target parallel for is_device_ptr(h)); (14, OpenMP, #pragma omp target parallel for is_device_ptr(k)); (49, OpenMP, #pragma omp target parallel for is_device_ptr(da))"}
{"predict": "(8, OpenMP, #pragma omp parallel for collapse(2) reduction(&& : my_islarger))", "label": "(11, OpenMP, #pragma omp ordered); (3, OpenMP, #pragma omp parallel); (13, OpenMP, #pragma omp critical); (5, OpenMP, #pragma omp for private(i,j) schedule(static,1) collapse(2) ordered)"}
{"predict": "(12, OpenMP, #pragma omp barrier;)\n(23, OpenMP, #pragma omp barrier;)\n(36, OpenMP, #pragma omp barrier;)\n(47, OpenMP, #pragma omp barrier;)\n(60, OpenMP, #pragma omp barrier;)\n(68, OpenMP, #pragma omp barrier;)", "label": "(60, OpenMP, #pragma omp sections lastprivate(t_var, vec, s_arr, var, sivar)); (4, OpenMP, #pragma omp section); (2, OpenMP, #pragma omp parallel); (28, OpenMP, #pragma omp sections lastprivate(g, sivar)); (65, OpenMP, #pragma omp sections lastprivate(CONDITIONAL A::x, B::x))"}
{"predict": "(16, CUDA, for (int i = 0; i < WARMUP; ++i) { l1_latency_kernel<ROUND><<<1, THREAD>>>(d_ptr, d_ret, d_clk); } cudaDeviceSynchronize();)", "label": "(10, CUDA, cudaMallocHost(&h_ptr, THREAD * sizeof(void *));); (29, CUDA, cudaFree(d_ptr);); (25, CUDA, cudaMemcpy(h_clk, d_clk, THREAD * sizeof(uint32_t), cudaMemcpyDeviceToHost);); (28, CUDA, cudaFree(d_ret);); (12, CUDA, cudaMemcpy(d_ptr, h_ptr, THREAD * sizeof(void *), cudaMemcpyHostToDevice);); (27, CUDA, cudaFree(d_clk);); (-1, CUDA, cudaMalloc(&d_ptr, THREAD * sizeof(void *));); (26, CUDA, cudaFreeHost(h_ptr);); (-1, CUDA, cudaMalloc(&d_clk, THREAD * sizeof(uint32_t));); (-3, CUDA, cudaMalloc(&d_ret, sizeof(void *));)"}
{"predict": "(3, MPI, pcu::PCU PCUObj = pcu::PCU(MPI_COMM_WORLD);)", "label": "(14, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(31, OpenMP, #pragma omp parallel for reduction(+:global_execute_count))", "label": "(69, TBB, g.reset(tbb::flow::rf_reset_bodies);); (28, TBB, tbb::flow::make_edge( senders[s], exe_node );); (19, TBB, tbb::flow::make_edge( exe_node, *receivers[r] );); (61, TBB, inc_functor body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (66, TBB, body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (0, TBB, tbb::flow::graph g;); (44, TBB, tbb::flow::remove_edge( exe_node, *receivers[r] );); (4, TBB, tbb::flow::function_node< InputType, OutputType > exe_node( g, lc, cf );)"}
{"predict": "(4, OpenMP, #pragma omp parallel for private(vs, MyCombineEach, vs2, MyCombineEach2, vs3, MyCombineEach3, vs4, MyCombineEach4, vs5, MyCombineEach5))\n(20, OpenMP, #pragma omp for reduction(+:defaultConstructed_sum))\n(26, OpenMP, #pragma omp for reduction(+:copyConstructed_sum))\n(32, OpenMP, #pragma omp for reduction(+:copyAssigned_sum))\n(37, OpenMP, #pragma omp for reduction(+:moveConstructed_sum))\n(42, OpenMP, #pragma omp for reduction(+:moveAssigned_sum))", "label": "(8, TBB, oneapi::tbb::global_control gc(oneapi::tbb::global_control::max_allowed_parallelism, p);); (23, TBB, oneapi::tbb::parallel_for( oneapi::tbb::blocked_range<int> (0, N, 10000), ParallelVectorForBody<T>( vs ) );); (0, TBB, typedef std::vector<T, oneapi::tbb::tbb_allocator<T> > ContainerType;); (17, TBB, typedef typename oneapi::tbb::combinable< ContainerType > CombinableType;)"}
{"predict": "(29, OpenMP, #pragma omp parallel for private(s))", "label": "(16, TBB, tbb::flow::make_edge( exe_node, receivers[r] );); (23, TBB, tbb::flow::make_edge( senders[s], exe_node );); (43, TBB, tbb::flow::remove_edge( exe_node, receivers[r] );); (53, TBB, inc_functor body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (1, TBB, tbb::flow::graph g;); (6, TBB, tbb::flow::function_node< InputType, OutputType > exe_node( g, lc, cf );)"}
{"predict": "(51, CUDA, cudaEventCreate(&start);) (52, CUDA, cudaEventCreate(&stop);) (53, CUDA, cudaEventRecord(start, 0);) (55, CUDA, cudaEventRecord(stop, 0);) (56, CUDA, cudaEventSynchronize(stop);) (57, CUDA, cudaEventElapsedTime(&elapsed_time, start, stop);)", "label": "(29, CUDA, cudaMalloc((void **)&d_C_ref, m * n * sizeof(float));); (68, CUDA, cudaEventElapsedTime(&elapsed_time, start, stop);); (30, CUDA, cudaMemcpy(d_A, A, m * k * sizeof(float), cudaMemcpyHostToDevice);); (32, CUDA, cudaMemcpy(d_C_ref, C_ref, m * n * sizeof(float), cudaMemcpyHostToDevice);); (36, CUDA, cudaMemcpy(C, d_C, m * n * sizeof(float), cudaMemcpyDeviceToHost);); (61, CUDA, cudaEventRecord(stop);); (61, CUDA, cudaEventSynchronize(stop);); (15, CUDA, cudaMalloc((void **)&d_B, k * n * sizeof(float));); (13, CUDA, cudaMalloc((void **)&d_A, m * k * sizeof(float));); (52, CUDA, cudaEventRecord(start);); (13, CUDA, cudaMalloc((void **)&d_C, m * n * sizeof(float));); (23, CUDA, cudaMemcpy(d_C, C, m * n * sizeof(float), cudaMemcpyHostToDevice);); (21, CUDA, cudaMemcpy(d_B, B, k * n * sizeof(float), cudaMemcpyHostToDevice);); (46, CUDA, cudaEventCreate(&stop);); (44, CUDA, cudaEventCreate(&start);)"}
{"predict": "(14, OpenMP, #pragma omp parallel for private(index, i, lens, pin, x))", "label": "(6, OpenMP, #pragma omp parallel for)"}
{"predict": "(10, CUDA, A[blockIdx.z * lda + i * stride + j] = thrust::complex<T>(0, 0);)", "label": "(8, CUDA, A[blockIdx.z * stride + i * lda + i] = thrust::complex<T>(1.0, 0.0);); (11, CUDA, A[blockIdx.z * stride + j * lda + i] = thrust::complex<T>(0.0, 0.0);)"}
{"predict": "(27, CUDA, d_t_left = (float*)cudaMallocManaged(&t_left_bytes);) (28, CUDA, d_t_right = (float*)cudaMallocManaged(&t_right_bytes);) (29, CUDA, d_t_result = (float*)cudaMallocManaged(&t_result_bytes);) (40, CUDA, assert(cudaMemcpy(d_t_left, t_left.data(), t_left_bytes, cudaMemcpyHostToDevice) == cudaSuccess);) (41, CUDA, assert(cudaMemcpy(d_t_right, t_right.data(), t_right_bytes, cudaMemcpyHostToDevice) == cudaSuccess);) (42, CUDA, gpu_t_result.device(gpu_device) = gpu_t_left.contract(gpu_t_right, dims);) (43, CUDA, assert(cudaMemcpy(t_result_gpu.data(), d_t_result, t_result_bytes, cudaMemcpyDeviceToHost) == cudaSuccess);) (45, CUDA, cudaFree(d_t_left);) (46, CUDA, cudaFree(d_t_right);) (47, CUDA, cudaFree(d_t_result);)", "label": "(25, CUDA, cudaMalloc((void**)(&d_t_right), t_right_bytes);); (25, CUDA, cudaMalloc((void**)(&d_t_result), t_result_bytes);); (50, CUDA, cudaFree((void*)d_t_left);); (26, CUDA, cudaMemcpy(d_t_right, t_right.data(), t_right_bytes, cudaMemcpyHostToDevice);); (20, CUDA, cudaMalloc((void**)(&d_t_left), t_left_bytes);); (23, CUDA, cudaMemcpy(d_t_left, t_left.data(), t_left_bytes, cudaMemcpyHostToDevice);); (48, CUDA, cudaFree((void*)d_t_result);); (46, CUDA, cudaFree((void*)d_t_right);); (36, CUDA, cudaMemcpy(t_result_gpu.data(), d_t_result, t_result_bytes, cudaMemcpyDeviceToHost);)"}
{"predict": "(112, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(37, MPI, MPI_Comm_rank(adio_fh->comm, &myrank);); (24, MPI, MPI_Bcast(&tmp_whence, 1, MPI_INT, 0, adio_fh->comm);); (12, MPI, MPI_Bcast(&tmp_offset, 1, ADIO_OFFSET, 0, adio_fh->comm);); (110, MPI, MPI_Barrier(adio_fh->comm);)"}
{"predict": "(2, OpenMP, #pragma omp parallel for)", "label": "(22, OpenMP, #pragma omp flush(out)); (17, OpenMP, #pragma omp atomic read); (10, OpenMP, #pragma omp atomic write); (-1, OpenMP, #pragma omp target parallel num_threads(64) map(tofrom); (6, OpenMP, #pragma omp flush(data))"}
{"predict": "(17, TBB, parallel_for(blocked_range<unsigned>(1, MaxFilters + 1, MaxFilters), [&](const blocked_range<unsigned>& range) { for (unsigned n = range.begin(); n < range.end(); n *= MaxFilters) { PipelineTest::TestTrivialPipeline(nthread, n); } }));", "label": "(11, TBB, tbb::task_scheduler_init init(nthread);)"}
{"predict": "(14, hip, hipGetDeviceCount(&count);) (15, hip, hipGetDevice(&device);) (17, MPI, printf(\"I'm MPI rank %2d/%-2d (world) %2d/%-2d (node) on %s with GPU %2d/%-2d\\n\", rank, size, rank_node, size_node, procname, device, count);) (22, hip, hipSetDevice(rank_node % count);) (24, MPI, printf(\"Now MPI rank %2d/%-2d (world) %2d/%-2d (node) on %s with GPU %2d/%-2d\\n\", rank, size, rank_node, size_node, procname, device, count);)", "label": "(12, MPI, MPI_Comm_size(comm_node, &size_node);); (1, MPI, MPI_Init(&argc, &argv);); (32, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (13, MPI, MPI_Get_processor_name(procname, &namelen);); (5, MPI, MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, 0, MPI_INFO_NULL, &comm_node);); (-1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (6, MPI, MPI_Comm_rank(comm_node, &rank_node);); (19, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(12, OpenMP, #pragma omp parallel for private(index))", "label": "(12, OpenMP, #pragma omp parallel for)"}
{"predict": "(12, CUDA, cudaMallocManaged((void**)&a, size * sizeof(float))); (15, CUDA, cudaMallocManaged((void**)&b, size * sizeof(float))); (18, CUDA, cudaMallocManaged((void**)&partial_c, blocksPerGrid * sizeof(float))); (22, CUDA, cudaDeviceSynchronize()); (29, CUDA, __syncthreads()); (37, CUDA, cudaDeviceSynchronize()); (41, CUDA, __syncthreads());", "label": "(56, CUDA, HANDLE_ERROR( cudaEventDestroy( start ) );); (6, CUDA, HANDLE_ERROR( cudaEventCreate( &stop ) );); (55, CUDA, HANDLE_ERROR( cudaEventDestroy( stop ) );); (36, CUDA, HANDLE_ERROR( cudaThreadSynchronize() );); (20, CUDA, HANDLE_ERROR( cudaHostGetDevicePointer( &dev_b, b, 0 ) );); (18, CUDA, HANDLE_ERROR( cudaHostGetDevicePointer( &dev_a, a, 0 ) );); (28, CUDA, HANDLE_ERROR( cudaEventRecord( start, 0 ) );); (33, CUDA, HANDLE_ERROR( cudaEventRecord( stop, 0 ) );); (33, CUDA, HANDLE_ERROR( cudaEventSynchronize( stop ) );); (43, CUDA, HANDLE_ERROR( cudaFreeHost( b ) );); (41, CUDA, HANDLE_ERROR( cudaFreeHost( a ) );); (42, CUDA, HANDLE_ERROR( cudaFreeHost( partial_c ) );); (-6, CUDA, HANDLE_ERROR( cudaEventCreate( &start ) );)"}
{"predict": "(12, MPI, type = MPI_INT; MPI_Type_commit(&type);)", "label": "(13, MPI, mpi_err = MPI_Type_commit(&type);); (9, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(4, OpenMP, #pragma omp parallel for reduction(+:sum.first, sum.second)", "label": "(7, TBB, RangeType range2( range, tbb::split() );)"}
{"predict": "(10, MPI, MPI_Allreduce(&result, &tmp, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD); result = tmp;)", "label": "(20, MPI, MPI_Allreduce( & tmp , & result , 1 , MPI_DOUBLE , MPI_SUM , MPI_COMM_WORLD );)"}
{"predict": "(11, CUDA, for(j = 0; j < batch; ++j){ for(i = 0; i < spatial; i += threads){ int index = j*spatial*filters + filter*spatial + i + id; local[id] += (i+id < spatial) ? dxhat[index] : 0; } } __syncthreads(); if(id == 0){ mean_delta[filter] = 0; for(i = 0; i < threads; ++i){ mean_delta[filter] += local[i]; } mean_delta[filter] *= (-1.f/sqrtf(variance[filter] + .00001f)); } else { mean_delta[filter] = 0; } }", "label": "(3, CUDA, __shared__ float local[threads];)"}
{"predict": "(1, CUDA, __global__ void layer_norm_vec4(float *x, float *y, float g, float b, int N, int K) {); (1, CUDA, ) (34, CUDA, } return;)", "label": "(9, CUDA, __shared__ float s_variance;); (7, CUDA, __shared__ float s_mean;)"}
{"predict": "out the data size to file.\n102:     MPI_File_open(SAMRAI_MPI::getComm(), file_name.c_str(), MPI_MODE_CREATE | MPI_MODE_WRONLY, MPI_INFO_NULL, &file);\n103:     MPI_File_write_at_all(file, 0, &size_array, 1, MPI_INT, &status);\n104:     MPI_File_close(&file);\n105: \n106:     // Write out the actual data.\n107:     MPI_File_open(SAMRAI_MPI::getComm(), file_name.c_str(), MPI_MODE_WRONLY, MPI_INFO_NULL, &file);\n108:     MPI_File_write_at_all(file, offset, &pos_values[0], data_size[rank], MPI_DOUBLE, &status);\n109:     MPI_File_close(&file);\n1000: }", "label": "(99, MPI, MPI_File_open(MPI_COMM_WORLD, file_name.c_str(), MPI_MODE_CREATE | MPI_MODE_WRONLY, MPI_INFO_NULL, &file);); (105, MPI, MPI_File_write(file, &size_array, 1, MPI_INT, &status);); (110, MPI, MPI_File_close(&file);); (102, MPI, MPI_File_seek(file, mpi_offset, MPI_SEEK_SET);); (107, MPI, MPI_File_write(file, &pos_values[0], data_size[rank], MPI_DOUBLE, &status);)"}
{"predict": "(1, CUDA, __global__ void compute_product(", "label": "(16, CUDA, extern __shared__ volatile float Bsh[];)"}
{"predict": "(27, MPI, MPI_Finalize();)", "label": "(8, MPI, MPI_Init(&argc, &argv);); (28, MPI, MPI_Finalize();)"}
{"predict": "(32, CUDA, stream.synchronize();)", "label": "(20, CUDA, cudaStream_t stream = at::cuda::getCurrentCUDAStream();)"}
{"predict": ":     writeTensor(\"output\", stOutput);\n103:     writeInt64(\"o_strideB\", p.o_strideB);\n104:     writeInt64(\"o_strideM\", p.o_strideM());\n105:     writeInt64(\"o_strideH\", p.o_strideH);\n106:     writeTensor(\"logsumexp\", lse);\n107:     writeInt64(\"lse_strideB\", p.lse_strideB);\n108:     writeInt64(\"lse_strideH\", p.lse_strideH);\n109:     return 0;\n110: }\n\nOutput:", "label": "(85, CUDA, cudaDeviceSynchronize();); (59, CUDA, cudaMalloc(&p.workspace, p.workspace_size());); (114, CUDA, CUDA_CHECK(cudaEventSynchronize(events[1]));); (112, CUDA, CUDA_CHECK(cudaEventRecord(events[1]));); (84, CUDA, CUDA_CHECK(cudaFuncSetAttribute(kernel_fn, cudaFuncAttributeMaxDynamicSharedMemorySize, int(smem_bytes)));); (114, CUDA, CUDA_CHECK(cudaEventElapsedTime(&runtime_ms, events[0], events[1]));); (9, CUDA, std::cerr << \"FATAL: \" #FN \" failed: \" << cudaGetErrorString(cudaError) << std::endl;); (102, CUDA, CUDA_CHECK(cudaEventCreate(&event));); (103, CUDA, CUDA_CHECK(cudaEventRecord(events[0]));)"}
{"predict": "(5, CUDA, for (int i = 0; i < batch; i++) { total += heights[i]; } total = total * 3; uint8_t ** cpuPointers = new uint8_t*[total]; int cur = 0; for (int i = 0; i < batch; i++) { for (int h = 0; h < heights[i]; h++) { cpuPointers[cur] = (uint8_t*)dsts[i] + h * dpitchs[i]; cur++; } } <<<total, 256>>> (cpuPointers); delete[] cpuPointers; DeviceSync(); )", "label": "(19, CUDA, cudaMemcpy(pointers, cpuPointers, sizeof(uint8_t*) * total * 3, cudaMemcpyHostToDevice);); (6, CUDA, uint8_t ** pointers = (uint8_t**)FastllmCudaMalloc(sizeof(uint8_t*) * total * 3);); (20, CUDA, FastllmCudaFree(pointers);)"}
{"predict": "][ index2 ];\n87:                 A[ 3 ][ index2 ] -= alpha * A[ 3 ][ index1 ];\n88:                 A[ 4 ][ index2 ] -= gamma * A[ 4 ][ index1 ];\n89:             }\n90:         }\n91:         MPI_Bcast( A[ 3 ], N + 1, MPI_DOUBLE, 0, MPI_COMM_WORLD );\n92:         MPI_Bcast( A[ 4 ], N + 1, MPI_DOUBLE, 0, MPI_COMM_WORLD );\n93:     }\n94:     Print( A, \"Matrix A 2\" );\n95:     MPI_Finalize();\n96:     return 0;\n97: }", "label": "(106, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ j + 1 ], 0, MPI_COMM_WORLD );); (104, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ j - 1 ], 1, MPI_COMM_WORLD );); (108, MPI, MPI_Recv( A[ 3 ].data(), N + 1, MPI_DOUBLE, activep[ j - 1 ], 0, MPI_COMM_WORLD, &status );); (7, MPI, MPI_Comm_size( MPI_COMM_WORLD, &Parallel::nprocs );); (97, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ numactivep - 2 ], 1, MPI_COMM_WORLD );); (135, MPI, MPI_Allgather( &tmp, 1, MPI_DOUBLE, x.data(), 1, MPI_DOUBLE, MPI_COMM_WORLD);); (187, MPI, MPI_Finalize();); (157, MPI, MPI_Allgather( &tmp, 1, MPI_DOUBLE, x.data(), 1, MPI_DOUBLE, MPI_COMM_WORLD );); (1, MPI, MPI_Init( &argc, &argv );); (2, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &Parallel::pid );); (101, MPI, MPI_Recv( A[ 4 ].data(), N + 1, MPI_DOUBLE, activep[ j + 1 ], 1, MPI_COMM_WORLD, &status );); (86, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ 1 ], 0, MPI_COMM_WORLD );)"}
{"predict": "(10, OpenMP, #pragma omp parallel for)", "label": "(25, OpenMP, #pragma omp parallel default(shared)); (10, OpenMP, #pragma omp parallel); (37, OpenMP, #pragma omp parallel num_threads(x)); (15, OpenMP, #pragma omp parallel default(none)); (28, OpenMP, #pragma omp parallel default(firstprivate))"}
{"predict": "(1, CUDA, __global__ void MatrixMulCUDA(float *C, float *A, float *B, int wA, int wB) {)", "label": "(41, CUDA, __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];); (36, CUDA, __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];)"}
{"predict": "(7, CUDA, int index = (in[i] & mask) >> shift; atomicAdd(dptrHistogram + index, 1);)", "label": "(12, CUDA, __shared__ unsigned char sharedHistogram[NUM_THREADS][cBuckets];)"}
{"predict": "(9, OpenMP, #pragma omp parallel for)\n(21, OpenMP, #pragma omp parallel for)", "label": "(25, TBB, tbb::flow::make_edge(bnode, tbb::flow::input_port<0>(jnode));); (7, TBB, tbb::flow::make_edge(bnode, fnode);); (22, TBB, tbb::flow::join_node<std::tuple<int,int>,tbb::flow::reserving> jnode(g);); (-1, TBB, tbb::flow::sequencer_node<int> bnode(g, seq_body());); (-3, TBB, tbb::flow::graph g;); (32, TBB, g.reset(tbb::flow::rf_clear_edges);); (-2, TBB, tbb::flow::function_node<int> fnode(g, tbb::flow::serial, serial_fn_body<int>(serial_fn_state0));); (15, TBB, tbb::flow::remove_edge(bnode, fnode);)"}
{"predict": "(12, OpenMP, #pragma omp parallel for reduction(+:item_sum))", "label": "(34, TBB, oneapi::tbb::tick_count start = oneapi::tbb::tick_count::now();); (69, TBB, utility::report_elapsed_time((oneapi::tbb::tick_count::now() - start).seconds());); (49, TBB, oneapi::tbb::flow::make_edge(the_source, the_value_pool);); (53, TBB, oneapi::tbb::flow::make_edge(the_value_pool, *(bins[i]));); (44, TBB, oneapi::tbb::flow::graph g;); (31, TBB, oneapi::tbb::global_control c(oneapi::tbb::global_control::max_allowed_parallelism, p);)"}
{"predict": "(27, OpenMP, #pragma omp parallel for)", "label": "(37, OpenMP, #pragma omp target teams distribute parallel for simd safelen (S)); (17, OpenMP, #pragma omp target teams distribute parallel for simd safelen (ST); (20, OpenMP, #pragma omp target teams distribute parallel for simd safelen (1))); (51, OpenMP, #pragma omp target teams distribute parallel for simd safelen (N)); (46, OpenMP, #pragma omp target teams distribute parallel for simd safelen (4)); (2, OpenMP, #pragma omp target teams distribute parallel for simd safelen (); (20, OpenMP, #pragma omp target teams distribute parallel for simd safelen ((ST > 0) ? 1 + ST : 2)); (26, OpenMP, #pragma omp target teams distribute parallel for simd safelen (foobool(argc)), safelen (true), safelen (-5)); (3, OpenMP, #pragma omp target teams distribute parallel for simd safelen ()); (5, OpenMP, #pragma omp target teams distribute parallel for simd safelen (argc); (-7, OpenMP, #pragma omp target teams distribute parallel for simd safelen)"}
{"predict": "(14, CUDA, extern \"C\" __global__ void fusedFFNKernel(", "label": "(13, CUDA, extern __shared__ char shared_mem[];)"}
{"predict": "(17, MPI, MPI_Finalize();)", "label": "(14, MPI, MPI_Abort(MPI_COMM_WORLD, 127);)"}
{"predict": "(38, CUDA, dim3 threadsPerBlock(128); dim3 numBlocks((TOTAL_SIZE + threadsPerBlock.x - 1) / threadsPerBlock.x); for (int iter = 0; iter < NITER; iter++) { ctx.task(lUn.rw(cdp), lUn1.read(cdp))->*[&](auto stream, auto sUn, auto sUn1) { stencil_kernel<double><<<numBlocks, threadsPerBlock, 0, stream>>>(sUn, sUn1); }; } ctx.task(exec_place::host(), lUn.read())->*[&](auto stream, auto sUn) { for (size_t offset = 0; offset < TOTAL_SIZE; offset++) { sum += sUn(offset); } });", "label": "(51, CUDA, cuda_safe_call(cudaStreamSynchronize(stream));)"}
{"predict": "(14, OpenCL, dnnl_stream_t stream; DNNL_CHECK(dnnl_ocl_interop_stream_create(&stream, eng, interop_ocl_queue));)", "label": "(34, OpenCL, TEST_OCL_CHECK(clReleaseCommandQueue(interop_ocl_queue));)"}
{"predict": "(23, OpenMP, #pragma omp parallel for;);", "label": "(15, OpenMP, #pragma omp taskloop simd private(argc, b), firstprivate(c, d), lastprivate(d, f) collapse(N) shared(g) if (c) final(d) mergeable priority(f) simdlen(N) nogroup num_tasks(N)); (13, OpenMP, #pragma omp parallel); (5, OpenMP, #pragma omp taskloop simd allocate(d) if(taskloop: argc > N) default(shared) untied priority(N) safelen(N) linear(c) aligned(ptr) grainsize(N) reduction(+:g) in_reduction(+: d)); (3, OpenMP, #pragma omp taskgroup task_reduction(+: d) allocate(d))"}
{"predict": "(33, MPI, errs += test_stream_send(stream_comm, 0, i);) (34, MPI, errs += test_stream_send(stream_comm, i, 0);)", "label": "(10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (43, MPI, mpi_errno = MPI_Comm_free(&stream_comm);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (15, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(10, MPI, p2 = (char *)MPIU_Malloc( p-v2 + 1 );)", "label": "(18, MPI, *ierr = MPI_Info_delete( (MPI_Info)(*v1), p2 );)"}
{"predict": "(10, OpenMP, #pragma omp parallel for collapse(2))", "label": "(2, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (11, TBB, tbb::tick_count t1 = tbb::tick_count::now();)"}
{"predict": "(15, CUDA, __syncthreads();)", "label": "(12, CUDA, A_[j] = static_cast<thrust::complex<T1>>(B_[j]);)"}
{"predict": "(11, OpenMP, #pragma omp parallel for private(i))", "label": "(7, OpenMP, #pragma omp for linear (); (24, OpenMP, #pragma omp for linear (argv[1])); (17, OpenMP, #pragma omp for linear (S1)); (12, OpenMP, #pragma omp for linear (argc > 0 ? argv[1] : argv[2])); (1, OpenMP, #pragma omp for linear); (22, OpenMP, #pragma omp for linear(e, g, k)); (27, OpenMP, #pragma omp parallel); (33, OpenMP, #pragma omp for linear(j)); (3, OpenMP, #pragma omp for linear (argc); (34, OpenMP, #pragma omp for linear(v:j)); (-1, OpenMP, #pragma omp for linear ()); (2, OpenMP, #pragma omp for linear (argc,); (11, OpenMP, #pragma omp for linear (a, b:B::ib)); (34, OpenMP, #pragma omp for linear(i) ordered(1)); (15, OpenMP, #pragma omp for linear(h)); (2, OpenMP, #pragma omp for linear (argc : 5) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (21, OpenMP, #pragma omp for linear(v:i)); (14, OpenMP, #pragma omp for linear(i))"}
{"predict": "(18, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)", "label": "(16, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(5, MPI, tXferGpu2Host = SuperLU_timer_();) (14, MPI, tXferGpu2Host = SuperLU_timer_() - tXferGpu2Host;)", "label": "(8, CUDA, cudaStreamSynchronize(LU_v1->A_gpu.cuStreams[0]);)"}
{"predict": "(25, MPI, MPI_Init_thread(&argc, &argv, required, &provided);)", "label": "(31, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(1, CUDA, __global__ void dot_prod_f16_f32_kernel(half *a, half *b, float *y, int N) {)", "label": "(5, CUDA, __shared__ float reduce_smem[NUM_WARPS];)"}
{"predict": "(1, CUDA, __global__ void knn_assign_shmem)", "label": "(16, CUDA, extern __shared__ float buffer[];)"}
{"predict": ":         }\n133:       }\n134:       spath_delete(&file_path);\n135:     }\n136: \n137:     /* sort directories by depth */\n138:     qsort(dirs, num_dirs, sizeof(char*), scr_compare_dir_depth);\n139: \n140:     /* get global min and max values across all procs */\n141:     MPI_Allreduce(&min_depth, &min_global, 1, MPI_INT, MPI_MIN, scr_comm_world);\n142:     MPI_Allreduce(&max_depth, &max_global, 1, MPI_INT, MPI_MAX, scr_comm_world);\n143: \n144:     /* free memory for our list of directories */\n145:     for (i = 0; i < num_dirs; i++) {\n146:       scr_free(&dirs[i]);\n147:     }\n148:     SCR_FREE(&dirs);\n149:     SCR_FREE(&depths);\n150:   }\n151: \n152:   /* delete directories for user dataset files if any rank found them */\n153:   if (source_rank < scr_ranks_world) {\n154:     /* some rank has defined min/max values,\n155:      * get min_depth from that rank */\n156:     int min_source = min_depth;\n157: \n158:     /* initialize our own min/max if needed */\n159:     if (min_depth == -1) {\n160:       min_depth = min_source;\n161:     }\n162:     if (max_depth == -1) {\n163:       max_depth = min_source;\n164:     }\n165: \n166:     /* get global min and max values across all procs */\n167:     int min_global, max_global;\n168: \n169:     /* allocate memory to hold list of each of our directories */\n170:     char** dirs = (char**) SCR_MALLOC(num_dirs * sizeof(char*));\n171:     int* depths = (int*)   SCR_MALLOC(num_dirs * sizeof(int));\n172: \n173:     /* get list of directories */\n174:     int i = 0;", "label": "(85, MPI, MPI_Bcast(&min_source, 1, MPI_INT, source, scr_comm_world);); (96, MPI, MPI_Allreduce(&min_depth, &min_global, 1, MPI_INT, MPI_MIN, scr_comm_world);); (76, MPI, MPI_Allreduce(&source_rank, &source, 1, MPI_INT, MPI_MIN, scr_comm_world);); (95, MPI, MPI_Allreduce(&max_depth, &max_global, 1, MPI_INT, MPI_MAX, scr_comm_world);); (168, MPI, MPI_Barrier(scr_comm_world);)"}
{"predict": "(23, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc((void**)&states, grid_size * block_dim * sizeof(RNG)));) (28, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(d_out, h_out.data(), size * sizeof(DataType), cudaMemcpyHostToDevice));) (31, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc((void**)&d_ref, size * sizeof(DataType)));) (36, CUDA, CURAND_CHECK_AND_EXIT(curandSetStream(gen_curand, 0)));", "label": "(8, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc((void**)&d_out, size * sizeof(DataType)));); (32, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(h_out.data(), d_out, size * sizeof(DataType), cudaMemcpyDeviceToHost));); (28, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (35, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc((void**)&d_ref, size * sizeof(DataType)));); (48, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(d_out));); (16, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc((void**)&states, block_dim * grid_size * sizeof(RNG)));); (19, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (44, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(states));); (45, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(d_ref));); (39, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(h_ref.data(), d_ref, size * sizeof(DataType), cudaMemcpyDeviceToHost));)"}
{"predict": "(12, OpenCL, cl_int err = clGetDeviceInfo(device, CL_DEVICE_PLATFORM, sizeof(platform), &platform, NULL);)", "label": "(36, OpenCL, err = clSetKernelArg(scan_kernel, 1, dyn_mem_size, NULL);); (31, OpenCL, err = clSetKernelArg(scan_kernel, 0, sizeof(buffer), &buffer);)"}
{"predict": "(11, MPI, displ = astart * sizeof(int);)", "label": "(22, MPI, err = MPI_Type_create_resized(tmptype, 0, aext * sizeof(int), datatype);); (28, MPI, MPI_Type_free(&tmptype);); (12, MPI, err = MPI_Type_create_struct(1, &blen, &displ, &type, &tmptype);)"}
{"predict": "(8, OpenMP, #pragma omp atomic seq_cst;)\n(13, OpenMP, #pragma omp atomic seq_cst;)", "label": "(4, OpenMP, #pragma omp atomic seq_cst); (8, OpenMP, #pragma omp atomic seq_cst seq_cst); (10, OpenMP, #pragma omp atomic update seq_cst)"}
{"predict": "(10, CUDA, if (x >= imgWidth || y >= imgHeight || x < 0 || y < 0) return; const int idx = y * imgWidth + x; img[idx] = color; )", "label": "(12, CUDA, img[idx] = cudaAlphaBlend(img[idx], color);)"}
{"predict": ".out\", filename);\n124:     if (verbose && rank == 0)\n125:         fprintf(stdout, \"Testing ROMIO_FSTYPE_FORCE prefix (known but disabled): %s\",\n126:                 disabled_prefix);\n127:     err = MPI_File_open(MPI_COMM_WORLD, out_fname, MPI_MODE_CREATE | MPI_MODE_RDWR, MPI_INFO_NULL,\n128:                         &fh);\n129:     err_expected(err, MPI_ERR_IO);\n130:     if (verbose && rank == 0)\n131:         fprintf(stdout, \" ---- pass\\n\");\n132: \n133:     MPI_Barrier(MPI_COMM_WORLD);\n134: \n135:     /* test a file system type prefix unknown to ROMIO but enabled at configure */\n136:     sprintf(out_fname, \"%s%s.out\", unknown_prefix, filename);\n137:     if (verbose && rank == 0)\n138:         fprintf(stdout, \"Testing file name prefix (unknown but enabled): %s\", unknown_prefix);\n139:     err = MPI_File_open(MPI_COMM_WORLD, out_fname, MPI_MODE_CREATE | MPI_MODE_RDWR,\n140:                         MPI_INFO_NULL, &fh);\n141:     err_expected(err, MPI_ERR_IO);\n142:     if (verbose && rank == 0)\n143:         fprintf(stdout, \" ---- pass\\n\");\n144: \n145:     MPI_Barrier(MPI_COMM_WORLD);\n146: \n147:     /* set a known file system type prefix to ROMIO in environment variable ROMIO_FSTYPE_FORCE */\n148:     setenv(\"ROMIO_FSTYPE_FORCE\", unknown_prefix, 1);\n149:     sprintf(out_fname, \"%s.out\", filename);\n150:     if (verbose && rank == 0)\n151:         fprintf(stdout, \"Testing ROMIO_FSTYPE_FORCE prefix (unknown but enabled): %s\",\n152:                 unknown_prefix);\n153:     err = MPI_File_open(MPI_COMM_WORLD, out_fname, MPI_MODE_CREATE | MPI_MODE_RDWR, MPI_INFO_NULL,\n154:                         &fh);\n155:     err_expected(err, MPI_ERR_IO);\n156:     if (verbose && rank == 0)\n157:         fprintf(stdout, \" ---- pass\\n\");\n158", "label": "(25, MPI, MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);); (40, MPI, err = MPI_File_close(&fh);); (4, MPI, MPI_Init(&argc, &argv);); (143, MPI, MPI_Finalize();); (22, MPI, MPI_Bcast(filename, len + 10, MPI_CHAR, 0, MPI_COMM_WORLD);); (2, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (39, MPI, MPI_Barrier(MPI_COMM_WORLD);); (12, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": ";\n125:         llinbuf[1] = 0;\n126:         llinbuf[2] = rank;\n127: \n128:         lloutbuf[0] = 0;\n129:         lloutbuf[1] = 1;\n130:         lloutbuf[2] = 1;\n131:         if (MPI_LONG_LONG != MPI_DATATYPE_NULL) {\n132:             MTestPrintfMsg(10, \"Reduce of MPI_LONG_LONG\\n\");\n133:             if (rank == 0) {\n134:                 if (lloutbuf[0] != 1) {\n135:                     errs++;\n136:                     fprintf(stderr, \"long long MAX(1) test failed\\n\");\n137:                 }\n138:                 if (lloutbuf[1] != 0) {\n139:                     errs++;\n140:                     fprintf(stderr, \"long long MAX(0) test failed\\n\");\n141:                 }\n142:                 if (lloutbuf[2] != size - 1) {\n143:                     errs++;\n144:                     fprintf(stderr, \"long long MAX(>) test failed\\n\");\n145:                 }\n146:             }\n147:         }\n148:     }\n149: #endif /* HAVE_LONG_LONG */\n150: \n151:     /* int */\n152:     MTestPrintfMsg(10, \"Reduce of MPI_INT\\n\");\n153:     int inbuf[3], outbuf[3];\n154:     inbuf[0] = 1;\n155:     inbuf[1] = 0;\n156:     inbuf[2] = rank;\n157: \n158:     outbuf[0] = 0;\n159:     outbuf[1] = 1;\n160:     outbuf[2] = 1;\n161:     if (rank == 0) {\n162:         if (outbuf[0] != 1) {\n163:             errs++;\n164:             fprintf(stderr, \"int MAX(1) test failed\\n\");\n165:         }\n166", "label": "(106, MPI, MPI_Reduce(ldinbuf, ldoutbuf, 3, MPI_LONG_DOUBLE, MPI_MAX, 0, comm);); (13, MPI, MPI_Comm_size(comm, &size);); (24, MPI, MPI_Reduce(cinbuf, coutbuf, 3, MPI_CHAR, MPI_MAX, 0, comm);); (49, MPI, MPI_Reduce(scinbuf, scoutbuf, 3, MPI_SIGNED_CHAR, MPI_MAX, 0, comm);); (73, MPI, MPI_Reduce(ucinbuf, ucoutbuf, 3, MPI_UNSIGNED_CHAR, MPI_MAX, 0, comm);); (8, MPI, MPI_Comm_rank(comm, &rank);); (132, MPI, MPI_Reduce(llinbuf, lloutbuf, 3, MPI_LONG_LONG, MPI_MAX, 0, comm);)"}
{"predict": "(26, OpenMP, #pragma omp parallel for;)", "label": "(19, OpenMP, #pragma omp distribute parallel for proc_bind(master); (3, OpenMP, #pragma omp distribute parallel for proc_bind); (12, OpenMP, #pragma omp distribute parallel for proc_bind()); (6, OpenMP, #pragma omp distribute parallel for proc_bind(); (25, OpenMP, #pragma omp distribute parallel for proc_bind(x)); (37, OpenMP, #pragma omp distribute parallel for proc_bind(spread)); (18, OpenMP, #pragma omp distribute parallel for proc_bind(close), proc_bind(spread)); (-5, OpenMP, #pragma omp target); (31, OpenMP, #pragma omp parallel proc_bind(close)); (26, OpenMP, #pragma omp distribute parallel for proc_bind(master)); (-7, OpenMP, #pragma omp teams)"}
{"predict": "(2, OpenCL, cl_event retEvent;); (4, OpenCL, auto retVal = clGetEventInfo(retEvent, CL_EVENT_COMMAND_TYPE, sizeof(cmdType), &cmdType, &sizeReturned);); (9, OpenCL, retVal = clReleaseEvent(retEvent);); (14, OpenCL, retVal = clGetEventInfo(retEvent, CL_EVENT_COMMAND_TYPE, sizeof(cmdType), &cmdType, &sizeReturned);); (19, OpenCL, retVal = clReleaseEvent(retEvent);); (22, OpenCL, retVal = clReleaseEvent(retEvent);)", "label": "(19, OpenCL, retVal = clEnqueueReleaseGLObjects(commandQueue, 1, &glBuffer, 0, nullptr, &retEvent);); (30, OpenCL, retVal = clReleaseMemObject(glBuffer);); (3, OpenCL, auto glBuffer = clCreateFromGLBuffer(&context, 0, bufferId, &retVal);); (1, OpenCL, auto commandQueue = clCreateCommandQueue(&context, context.getDevice(0), 0, &retVal);); (12, OpenCL, retVal = clReleaseEvent(retEvent);); (1, OpenCL, retVal = clEnqueueAcquireGLObjects(commandQueue, 1, &glBuffer, 0, nullptr, &retEvent);); (23, OpenCL, retVal = clReleaseCommandQueue(commandQueue);)"}
{"predict": "(13, MPI, MPI_Type_create_keyval(MPI_TYPE_NULL_COPY_FN, MPI_TYPE_NULL_DELETE_FN, &key[i], (void *) 0);)", "label": "(21, MPI, MPI_Type_set_attr(type, key[2], &attrval[2]);); (22, MPI, MPI_Type_set_attr(type, key[0], &attrval[0]);); (56, MPI, MPI_Type_free_keyval(&key[i]);); (19, MPI, MPI_Type_set_attr(type, key[1], &attrval[1]);); (23, MPI, MPI_Type_delete_attr(type, key[0]);); (23, MPI, MPI_Type_delete_attr(type, key[1]);); (23, MPI, MPI_Type_delete_attr(type, key[2]);)"}
{"predict": "(16, CUDA, if (lane == 0) { reduce_smem[warp] = prod; } __syncthreads(); // make sure the data is in shared memory. if (warp == 0) { prod = warp_reduce_sum<NUM_WARPS>(prod); } if (tid == 0) { atomicAdd(y, prod); } )", "label": "(5, CUDA, __shared__ float reduce_smem[NUM_WARPS];)"}
{"predict": "(18, OpenMP, #pragma omp parallel for reduction(+:metric_before) private(cpu_id, from, to) collapse(2);) (26, OpenMP, #pragma omp atomic update) (28, OpenMP, #pragma omp atomic update) (30, OpenMP, #pragma omp parallel for reduction(+:delta) private(hn, from, to) collapse(2);)", "label": "(8, TBB, tbb::enumerable_thread_specific<HyperedgeWeight> deltas(0);)"}
{"predict": "(6, OpenCL, auto deviceMemAlloc = clDeviceMemAllocINTEL(context.get(), tileDevices[0], nullptr, bufferSize, 0, &retVal);); (13, OpenCL, auto hostMemAlloc = clHostMemAllocINTEL(context.get(), nullptr, bufferSize, 0, &retVal);); (21, OpenCL, retVal = clMemFreeINTEL(context.get(), deviceMemAlloc);); (24, OpenCL, retVal = clMemFreeINTEL(context.get(), hostMemAlloc);)", "label": "(8, OpenCL, retVal = clEnqueueMemcpyINTEL(commandQueues[0][0].get(), true, deviceMemAlloc, buffer.data(), bufferSize, 0, nullptr, nullptr);); (16, OpenCL, retVal = clEnqueueMemcpyINTEL(commandQueues[0][0].get(), true, hostMemAlloc, deviceMemAlloc, bufferSize, 0, nullptr, nullptr);)"}
{"predict": "(31, MPI, MPI_Send(&message[0], msgsize, MPI_INT, 1, tag, MPI_COMM_WORLD);) (38, MPI, MPI_Recv(&receiveBuffer[0], msgsize, MPI_INT, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status);) (43, MPI, MPI_Recv(&receiveBuffer[0], msgsize, MPI_INT, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status);) (46, MPI, MPI_Send(&message[0], msgsize, MPI_INT, 0, tag, MPI_COMM_WORLD);)", "label": "(42, MPI, MPI_Recv(receiveBuffer.data(), msgsize, MPI_INT, 1, MPI_ANY_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (7, MPI, MPI_Init(&argc, &argv);); (48, MPI, MPI_Send(message.data(), msgsize, MPI_INT, 0, 1, MPI_COMM_WORLD);); (54, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (33, MPI, MPI_Send(message.data(), msgsize, MPI_INT, 1, tag, MPI_COMM_WORLD);); (8, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (41, MPI, MPI_Recv(receiveBuffer.data(), msgsize, MPI_INT, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status);)"}
{"predict": "(15, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);) (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);) (23, MPI, MPI_Scatter(data.data(), my_nx * my_ny * my_nz, MPI_COMPLEX_FLOAT, data.data(), my_nx * my_ny * my_nz, MPI_COMPLEX_FLOAT, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);) (29, MPI, MPI_Allreduce(MPI_IN_PLACE, data.data(), my_nx * my_ny * my_nz, MPI_COMPLEX_FLOAT, MPI_SUM, MPI_COMM_WORLD);) (31, MPI, MPI_Reduce(&error, &error, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);)", "label": "(9, CUDA, CUDA_CHECK(cudaGetDeviceCount(&ndevices));); (9, CUDA, CUDA_CHECK(cudaSetDevice(rank % ndevices));); (0, MPI, MPI_Init(&argc, &argv);); (37, MPI, MPI_Finalize();); (1, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)"}
{"predict": "(2, CUDA, __global__ static void kernel_output(uint64_t, float* xout, float* x, T* w, float* rms_weight, int n, int d, float norm_eps, bool norm_ln) {)", "label": "(2, CUDA, extern __shared__ char smem[];)"}
{"predict": "i++) {\n104:         sNewO[ty][i * Bc + tx] += sSafeE[ty][k] * sV[k][i * Bc + tx];\n105:       }\n106:     }\n107: \n108:     // update global max of each row\n109:     sMax[ty] = newMax;\n110:     sDenom[ty] = newDenom;\n111:     __syncthreads();\n112:   }\n113: \n114:   // write back to global memory(HBM)\n115:   if (row < seqlen) {\n116:     for (int i = 0; i < groupTx; i++) {\n117:       O[row * dim + i * Bc + tx] = sNewO[ty][i * Bc + tx];\n118:     }\n119:   }\n120: }\n\nAssistant:\n(2, CUDA, __global__ void flash_attention_v1_kernel(FP *Q, FP* K, FP* V, FP* O, FP* gMAX, FP* gDenom, int seqlen, FP smScale) {)", "label": "(12, CUDA, __shared__ FP sV[Bc][dim];); (20, CUDA, __shared__ FP sMax[Br];); (9, CUDA, __shared__ FP sK[Bc][dim];); (11, CUDA, __shared__ FP sQK[Br][Bc];); (14, CUDA, __shared__ FP sSafeE[Br][Bc];); (8, CUDA, __shared__ FP sO[Br][dim];); (10, CUDA, __shared__ FP sNewO[Br][dim];); (3, CUDA, __shared__ FP sQ[Br][dim];); (12, CUDA, __shared__ FP sDenom[Br];)"}
{"predict": "(9, OpenMP, #pragma omp parallel for private(i, error_code, context))", "label": "(7, OpenMP, #pragma omp parallel for)"}
{"predict": "(13, CUDA, cudaGetDeviceProperties(&props, 0);)", "label": "(12, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (13, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;)"}
{"predict": "(27, OpenCL, clCreateCommandQueue_StubWithCallback(clCreateCommandQueue_testCommandQueueFromSpecifiedContext);)", "label": "(16, OpenCL, clCreateBuffer_StubWithCallback(clCreateBuffer_testBufferConstructorContextIterator);); (35, OpenCL, clReleaseCommandQueue_ExpectAndReturn(make_command_queue(0), CL_SUCCESS);); (43, OpenCL, clReleaseMemObject_ExpectAndReturn(expected, CL_SUCCESS);); (24, OpenCL, clCreateCommandQueueWithProperties_StubWithCallback(clCreateCommandQueueWithProperties_testCommandQueueFromSpecifiedContext);); (31, OpenCL, clReleaseEvent_StubWithCallback(clReleaseEvent_testCopyHostToBuffer);); (41, OpenCL, clReleaseContext_ExpectAndReturn(make_context(0), CL_SUCCESS);); (26, OpenCL, clEnqueueMapBuffer_StubWithCallback(clEnqueueMapBuffer_testCopyHostToBuffer);); (26, OpenCL, clEnqueueUnmapMemObject_StubWithCallback(clEnqueueUnmapMemObject_testCopyHostToBuffer);)"}
{"predict": "(12, MPI, t0 = MPI_Wtime();) (47, MPI, t1 = MPI_Wtime();)", "label": "(33, MPI, MPI_Cart_shift(cart_comm, 0, 1, &source, &destination);); (52, MPI, t1 = MPI_Wtime();); (12, MPI, MPI_Init(&argc, &argv);); (57, MPI, MPI_Finalize();); (11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (27, MPI, MPI_Cart_create(MPI_COMM_WORLD, ndims, dims, periods, 1, &cart_comm);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (52, MPI, MPI_Comm_free(&cart_comm);); (27, MPI, MPI_Comm_rank(cart_comm, &cart_id);); (29, MPI, MPI_Barrier(MPI_COMM_WORLD);); (29, MPI, t0 = MPI_Wtime();)"}
{"predict": "(8, OpenMP, #pragma omp parallel for reduction(+:entropy) private(rank))\n(57, OpenMP, #pragma omp parallel for reduction(+:entropy) private(rank))", "label": "(8, MPI, MPI_Comm_rank(mpi_comm, &rank);); (6, MPI, MPI_Comm mpi_comm = MPI_Comm_f2c(vtkh::GetMPICommHandle());); (56, MPI, MPI_Bcast(&entropy, 1, MPI_DOUBLE, 0, mpi_comm);)"}
{"predict": "53:     // copy over the input\n54:     // copy over the gradients\n55: \n56:     // call backward() on the GPU\n57:     backward(d_dinp, d_dpreatt, d_datt, d_dout, d_vaccum, d_qkvr, d_preatt, d_att, dout, dinp, B, T, C, NH, block_size);\n58: \n59:     // check that the gradients match between the CPU and GPU versions\n60:     printf(\"Checking the backward pass CPU <-> GPU...\\n\");\n61:     printf(\"[dinp]\\n\"); validate_result(d_dinp, dinp, \"dinp\", B * T * 3 * C, 5e-3f);\n62:     printf(\"[dpreatt]\\n\"); validate_result(d_dpreatt, dpreatt, \"dpreatt\", B * NH * T * T, 5e-3f);\n63:     printf(\"[datt]\\n\"); validate_result(d_datt, datt, \"datt\", B * NH * T * T, 5e-3f);\n64:     printf(\"[dout]\\n\"); validate_result(d_dout, dout, \"dout\", B * T * C, 5e-3f);\n65: \n66:     // free all memory\n67:     free(inp);\n68:     free(qkvr);\n69:     free(preatt);\n70:     free(att);\n71:     free(vaccum);\n72:     free(out);\n73:     free(dout);\n74:     free(dinp);\n75:     free(dpreatt);\n76:     free(datt);\n77:     free(dvaccum);\n78:     free(d_out);\n79: \n80:     return 0;\n81: }", "label": "(68, CUDA, cudaCheck(cudaMemset(d_dinp, 0, B * T * 3 * C * sizeof(float)));); (60, CUDA, cudaCheck(cudaMalloc(&d_dpreatt, B * NH * T * T * sizeof(float)));); (157, CUDA, cudaCheck(cudaFree(d_inp));); (69, CUDA, cudaCheck(cudaMemset(d_dvaccum, 0, B * T * C * sizeof(float)));); (86, CUDA, cudaCheck(cudaMemcpy(h_dinp, d_dinp, B * T * 3 * C * sizeof(float), cudaMemcpyDeviceToHost));); (162, CUDA, cudaCheck(cudaFree(d_dpreatt));); (159, CUDA, cudaCheck(cudaFree(d_dinp));); (159, CUDA, cudaCheck(cudaFree(d_dqkvr));); (152, CUDA, cudaCheck(cudaFree(d_qkvr));); (23, CUDA, cudaCheck(cudaMalloc(&d_att, B * NH * T * T * sizeof(float)));); (158, CUDA, cudaCheck(cudaFree(d_datt));); (58, CUDA, cudaCheck(cudaMemset(d_dqkvr, 0, B * T * 3 * C * sizeof(float)));); (149, CUDA, cudaCheck(cudaFree(d_preatt));); (50, CUDA, cudaCheck(cudaMalloc(&d_dvaccum, B * T * C * sizeof(float)));); (50, CUDA, cudaCheck(cudaMalloc(&d_dout, B * T * C * sizeof(float)));); (15, CUDA, cudaCheck(cudaMalloc(&d_qkvr, B * T * 3 * C * sizeof(float)));); (17, CUDA, cudaCheck(cudaMalloc(&d_vaccum, B * T * C * sizeof(float)));); (14, CUDA, cudaCheck(cudaMalloc(&d_preatt, B * NH * T * T * sizeof(float)));); (48, CUDA, cudaCheck(cudaMemcpy(d_dout, dout, B * T * C * sizeof(float), cudaMemcpyHostToDevice));); (144, CUDA, cudaCheck(cudaFree(d_vaccum));); (40, CUDA, cudaCheck(cudaMalloc(&d_dqkvr, B * T * 3 * C * sizeof(float)));); (38, CUDA, cudaCheck(cudaMalloc(&d_dinp, B * T * 3 * C * sizeof(float)));); (49, CUDA, cudaCheck(cudaMemset(d_datt, 0, B * NH * T * T * sizeof(float)));); (141, CUDA, cudaCheck(cudaFree(d_out));); (145, CUDA, cudaCheck(cudaFree(d_dvaccum));); (9, CUDA, cudaCheck(cudaMalloc(&d_out, B * T * C * sizeof(float)));); (36, CUDA, cudaCheck(cudaMalloc(&d_datt, B * NH * T * T * sizeof(float)));); (2, CUDA, cudaCheck(cudaMalloc(&d_inp, B * T * 3 * C * sizeof(float)));); (134, CUDA, cudaCheck(cudaFree(d_att));); (41, CUDA, cudaCheck(cudaMemset(d_dpreatt, 0, B * NH * T * T * sizeof(float)));); (6, CUDA, cudaCheck(cudaMemcpy(d_inp, inp, B * T * 3 * C * sizeof(float), cudaMemcpyHostToDevice));); (139, CUDA, cudaCheck(cudaFree(d_dout));)"}
{"predict": "(9, CUDA, x[n] = a; y[n] = b;)", "label": "(9, CUDA, CHECK(cudaMallocManaged((void **)&z, M));); (23, CUDA, CHECK(cudaFree(x));); (5, CUDA, CHECK(cudaMallocManaged((void **)&x, M));); (22, CUDA, CHECK(cudaFree(y));); (22, CUDA, CHECK(cudaFree(z));); (16, CUDA, CHECK(cudaDeviceSynchronize());); (2, CUDA, CHECK(cudaMallocManaged((void **)&y, M));)"}
{"predict": "(40, MPI, MPI_Allreduce(&value, &tempvalue, 1, MPI_DOUBLE, MPI_MAX, grid->comm);)", "label": "(26, MPI, MPI_Allreduce(&value, &tempvalue, 1, MPI_DOUBLE, MPI_MAX, grid->comm);); (50, MPI, MPI_Allreduce(rwork, temprwork, A->ncol, MPI_DOUBLE, MPI_SUM, grid->comm);)"}
{"predict": "(13, MPI, MTest_Gather(is_blocking, sendbuf, send_count, MPI_INT, &table[0][0], recv_count, MPI_INT, i, MPI_COMM_WORLD);)", "label": "(17, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (26, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(6, OpenMP, #pragma omp parallel for)\n(9, OpenMP, #pragma omp parallel for)\n(12, OpenMP, #pragma omp parallel for)\n(15, OpenMP, #pragma omp parallel for)\n(18, OpenMP, #pragma omp parallel for)\n(21, OpenMP, #pragma omp parallel for)", "label": "(25, OpenCL, clReleaseMemObject(scratchBuff);); (20, OpenCL, clReleaseMemObject(mobjDP_);); (15, OpenCL, clReleaseMemObject(mobjY_);); (10, OpenCL, clReleaseMemObject(mobjX_);)"}
{"predict": "(24, TBB, oneapi::tbb::parallel_for(oneapi::tbb::blocked_range<int>(starty, stopy, grain_size), parallel_task(), g_ap);) (26, TBB, oneapi::tbb::parallel_for(oneapi::tbb::blocked_range<int>(starty, stopy, grain_size), parallel_task(), oneapi::tbb::simple_partitioner());) (28, TBB, oneapi::tbb::parallel_for(oneapi::tbb::blocked_range<int>(starty, stopy, grain_size), parallel_task(), oneapi::tbb::auto_partitioner());)", "label": "(5, TBB, oneapi::tbb::global_control c(oneapi::tbb::global_control::max_allowed_parallelism, nthreads);); (21, TBB, static oneapi::tbb::affinity_partitioner g_ap;)"}
{"predict": "(14, CUDA, cudaMallocManaged(&host_buf, global_size * sizeof(int));) (18, CUDA, auto scratch = cudaMallocManaged<int[]>(local_size);) (33, CUDA, item.barrier(cl::sycl::memory_order::acquire_release);) (36, CUDA, cudaFree(host_buf);) (37, CUDA, cudaFree(scratch);)", "label": "(16, SYCL, using namespace cl::sycl::access;); (4, SYCL, cl::sycl::queue queue;)"}
{"predict": "(11, OpenMP, #pragma omp parallel for)", "label": "(33, OpenMP, omp_target_free(d_a, DefaultDeviceNum);); (15, OpenMP, d_a = (int *)omp_target_alloc(DataSize, DefaultDeviceNum);); (23, OpenMP, #pragma omp target update to(d_a)); (5, OpenMP, int DefaultDeviceNum = omp_get_default_device();); (16, OpenMP, omp_target_associate_ptr(p_a, d_a, DataSize, 0, DefaultDeviceNum);); (24, OpenMP, omp_target_disassociate_ptr(p_a, DefaultDeviceNum);)"}
{"predict": "(11, OpenCL, OCL_CREATE_BUFFER(buf[1], 0, BUFFERSIZE*sizeof(int), NULL);) (45, OpenCL, OCL_SET_ARG(2, sizeof(cl_mem), &buf[1]);) (49, OpenCL, clEnqueueNDRangeKernel(commandQueue, kernel, 1, NULL, &globals, &locals, 1, &ev[0], &ev[2]));)", "label": "(51, OpenCL, clReleaseEvent(ev[i]);); (24, OpenCL, clEnqueueNDRangeKernel(queue, kernel, 1, NULL, globals, locals, 2, &ev[0], &ev[2]);); (30, OpenCL, buf_data[0] = clEnqueueMapBuffer(queue, buf[0], CL_FALSE, 0, 0, BUFFERSIZE*sizeof(int), 1, &ev[2], NULL, NULL);); (14, OpenCL, clEnqueueWriteBuffer(queue, buf[0], CL_FALSE, 0, BUFFERSIZE*sizeof(int), (void *)cpu_src, 1, &ev[0], &ev[1]);)"}
{"predict": "(28, OpenMP, #pragma omp parallel for)", "label": "(50, MPI, MPI_Allreduce(MPI_IN_PLACE, &(flag), 1, mpi_int_t, MPI_MAX, grid3d->zscp.comm);); (105, MPI, MPI_Allreduce(  MPI_IN_PLACE, myusize, nsupers, mpi_int_t, MPI_MAX, grid->comm );); (102, MPI, MPI_Allreduce( MPI_IN_PLACE, mylsize, nsupers, mpi_int_t, MPI_MAX, grid->comm );); (50, MPI, MPI_Bcast( &(size), 1, mpi_int_t, gridid,  grid3d->zscp.comm);); (48, MPI, MPI_Allreduce(MPI_IN_PLACE, &(gridid), 1, mpi_int_t, MPI_MAX, grid3d->zscp.comm);)"}
{"predict": "(27, CUDA, cudaEventCreate(&ev1); cudaEventCreate(&ev2);)", "label": "(15, CUDA, checkCudaErrors(cudaMalloc((void **)&ringbuf, sizeof(qsortRingbuf)));); (28, CUDA, checkCudaErrors(cudaEventRecord(ev1));); (40, CUDA, checkCudaErrors(cudaEventRecord(ev2));); (38, CUDA, checkCudaErrors(cudaGetLastError());); (61, CUDA, checkCudaErrors(cudaFree(ringbuf));); (38, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (21, CUDA, checkCudaErrors(cudaEventCreate(&ev1));); (21, CUDA, checkCudaErrors(cudaEventCreate(&ev2));); (58, CUDA, checkCudaErrors(cudaFree(gpustack));)"}
{"predict": "(10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size); MPI_Comm_rank(MPI_COMM_WORLD, &rank);) (22, MPI, sendbuf = (char *) malloc(10 * size * sizeof(char));) (40, MPI, free(sendbuf); memset(rbuf, 0, sizeof(rbuf)); sendbuf = (char *) malloc(100000 * size * sizeof(char));) (58, MPI, MPI_Reduce(&errs, &toterrs, 1, MPI_INT, MPI_SUM, 0, newcomm);)", "label": "(21, MPI, MPI_Comm_create_group(MPI_COMM_WORLD, newgroup, 0, &newcomm);); (19, MPI, MPI_Group_excl(world, 1, deadprocs, &newgroup);); (7, MPI, MPI_Init(&argc, &argv);); (82, MPI, MPI_Group_free(&newgroup);); (84, MPI, MPI_Finalize();); (53, MPI, rc = MPI_Scatter(sendbuf, 100000, MPI_CHAR, rbuf, 100000, MPI_CHAR, 0, MPI_COMM_WORLD);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (76, MPI, MPI_Comm_free(&newcomm);); (3, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (76, MPI, MPI_Group_free(&world);); (25, MPI, rc = MPI_Scatter(sendbuf, 10, MPI_CHAR, rbuf, 10, MPI_CHAR, 0, MPI_COMM_WORLD);); (4, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (26, MPI, MPI_Error_class(rc, &errclass);); (5, MPI, MPI_Comm_group(MPI_COMM_WORLD, &world);)"}
{"predict": "(11, OpenMP, #pragma omp atomic update var++;)", "label": "(6, OpenMP, #pragma omp parallel num_threads(8) shared(var)); (7, OpenMP, omp_set_nest_lock(&lock);); (9, OpenMP, omp_unset_nest_lock(&lock);); (1, OpenMP, omp_init_nest_lock(&lock);); (13, OpenMP, omp_destroy_nest_lock(&lock);)"}
{"predict": "(3, CUDA, try {) (12, CUDA, } catch(std::exception &e) {)", "label": "(8, CUDA, sim->CUDAConfig().device_id = device;)"}
{"predict": ": \t\t\t\tgrid, options, thresh, LUstruct, stat, info, SCT, tag_ub);\n109: \n110:                 factored_D[k] = 1;\n111:             }\n112:         }\n113:     }\n114: \n115:     for (int_t topoLvl = 0; topoLvl < maxTopoLevel; ++topoLvl)\n116:     {\n117:         int_t k_st = eTreeTopLims[topoLvl];\n118:         int_t k_end = eTreeTopLims[topoLvl + 1];\n119:         for (int_t k0 = k_st; k0 < k_end; ++k0)\n120:         {\n121:             int_t k = perm_c_supno[k0];   // direct computation no perm_c_supno\n122:             int_t offset = k0 - k_st;\n123:             /* diagonal factorization */\n124:             if (!factored_L[k] )\n125:             {\n126:                 /*If LU panels from GPU are not reduced then reduce\n127:                 them before diagonal factorization*/\n128: \n129: \t\tsDiagFactIBCast(k, k, dFBufs[offset]->BlockUFactor,\n130: \t\t\t\tdFBufs[offset]->BlockLFactor, factStat->IrecvPlcd_D,\n131: \t\t\t\tcomReqss[offset]->U_diag_blk_recv_req,\n132: \t\t\t\tcomReqss[offset]->L_diag_blk_recv_req,\n133: \t\t\t\tcomReqss[offset]->U_diag_blk_send_req,\n134: \t\t\t\tcomReqss[offset]->L_diag_blk_send_req,\n135: \t\t\t\tgrid, options, thresh, LUstruct, stat, info, SCT, tag_ub);\n136: \n137:                 factored_L[k] = 1;\n138:             }\n139:         }\n140:     }\n141: \n142:     for (int_t topoLvl = 0; topoLvl < maxTopoLevel; ++topoLvl)\n143:     {\n144:         int_t k_st = eTreeTopLims[topoLvl];\n1", "label": "(203, OpenMP, #pragma omp parallel); (206, OpenMP, #pragma omp for schedule(dynamic,2) nowait)"}
{"predict": "(20, OpenMP, #pragma omp parallel for collapse(2))", "label": "(19, OpenMP, #pragma omp parallel for)"}
{"predict": "(4, OpenCL, clEnqueueReadBuffer(clCommandQueue, clBuffer, CL_TRUE, 0, itemSize, &bufferData[i], 0, nullptr, nullptr);) (7, OpenCL, clEnqueueWriteBuffer(clCommandQueue, clBuffer, CL_TRUE, 0, itemSize, &bufferData[i], 0, nullptr, nullptr);) (10, OpenCL, clEnqueueReadBufferRect(clCommandQueue, clBuffer, CL_TRUE, origin, region, rowPitch, slicePitch, 0, &bufferData[i], 0, nullptr, nullptr);) (13, OpenCL, clEnqueueWriteBufferRect(clCommandQueue, clBuffer, CL_TRUE, origin, region, rowPitch, slicePitch, 0, &bufferData[i], 0, nullptr, nullptr);) (16, OpenCL, clEnqueueFillBuffer(clCommandQueue, clBuffer, fillPattern, patternSize, 0, itemSize, 0, nullptr, nullptr);) (19, OpenCL, clEnqueueCopyBuffer(clCommandQueue, clBuffer, clBuffer2, 0, 0, itemSize, 0, nullptr, nullptr);) (22, OpenCL, clEnqueueCopyBufferRect(clCommandQueue, clBuffer, clBuffer2, origin, origin2, region, rowPitch, slicePitch, rowPitch2, slicePitch2, 0, nullptr, nullptr);) (25, OpenCL, clEnqueueMapBuffer(clCommandQueue, clBuffer, CL_TRUE, CL_MAP_READ, 0, itemSize, 0, nullptr, nullptr, &mapPtr);) (28, OpenCL, clRetainMemObject(clBuffer);) (31, OpenCL, clSetMemObjectDestructorCallback(clBuffer, destructorCallback, nullptr);) (34, OpenCL, clEnqueueUnmapMemObject(clCommandQueue, clBuffer, mapPtr, 0, nullptr, nullptr);) (37, OpenCL, clGetMemObjectInfo(clBuffer, CL_MEM_SIZE, sizeof(size_t), &bufferSize, nullptr);)", "label": "(24, OpenCL, test_clEnqueueCopyBuffer(&clEnqueueCopyBufferData[i]);); (27, OpenCL, test_clEnqueueCopyBufferRect(&clEnqueueCopyBufferRectData[i]);); (6, OpenCL, test_clEnqueueWriteBuffer(&clEnqueueWriteBufferData[i]);); (41, OpenCL, test_clEnqueueUnmapMemObject(&clEnqueueUnmapMemObjectData[i]);); (8, OpenCL, test_clEnqueueReadBufferRect(&clEnqueueReadBufferRectData[i]);); (-1, OpenCL, test_clEnqueueReadBuffer(&clEnqueueReadBufferData[i]);); (14, OpenCL, test_clEnqueueFillBuffer(&clEnqueueFillBufferData[i]);); (25, OpenCL, test_clEnqueueMapBuffer(&clEnqueueMapBufferData[i]);); (8, OpenCL, test_clEnqueueWriteBufferRect(&clEnqueueWriteBufferRectData[i]);)"}
{"predict": "(10, OpenMP, #pragma omp parallel sections reduction(task, *: f);) (15, OpenMP, #pragma omp parallel sections default(none) private(argc,b) firstprivate(argv) if(argc > 0) num_threads(ee) copyin(a) proc_bind(spread) reduction(|: c,d) reduction(*: e) lastprivate(argv);)", "label": "(7, OpenMP, #pragma omp parallel sections reduction(task,*:f)); (14, OpenMP, #pragma omp parallel sections default(none), private(argc, b) firstprivate(argv) if (argc > 0) num_threads(ee) copyin(a) proc_bind(spread) reduction(| : c, d) reduction(* : e) lastprivate(argv)); (2, OpenMP, #pragma omp threadprivate(a)); (16, OpenMP, #pragma omp section)"}
{"predict": "(12, OpenMP, #pragma omp parallel for collapse(2))", "label": "(15, OpenMP, #pragma omp target teams distribute parallel for collapse(2) depend(out:image[y_start]) nowait); (6, OpenMP, double st = omp_get_wtime();); (28, OpenMP, double et = omp_get_wtime();); (25, OpenMP, #pragma omp taskwait); (5, OpenMP, #pragma omp target data map(alloc:image[0:width*height])); (18, OpenMP, #pragma omp target update from(image[block*block_size:block_size]))"}
{"predict": "(47, MPI, MPI_Send(&sender, 1, MPI_INT, dest, 0, MPI_COMM_WORLD);)", "label": "(36, MPI, MPI_Type_size(c_info->s_data_type, &s_size);); (36, MPI, MPI_Type_size(c_info->r_data_type, &r_size);)"}
{"predict": "(12, CUDA, PyObject* dict = PyDict_New();)", "label": "(13, CUDA, PyObject* shape = PyCudaImage_GetShape(self, closure);)"}
{"predict": "(33, MPI, MPI_Scatter(buf, i, MPI_CHAR, MPI_IN_PLACE, i, MPI_CHAR, 0, MPI_COMM_WORLD, &retVal);) (43, MPI, MPI_Recv(buf, i, MPI_CHAR, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)", "label": "(31, MPI, MPI_Type_commit(&newtype);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (42, MPI, MPI_Send(buf, 1, newtype, 1, i, MPI_COMM_WORLD);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (66, MPI, MPI_Type_free(&newtype);); (24, MPI, MPI_Type_vector(i, 128, 129, MPI_CHAR, &newtype);); (70, MPI, MPI_Recv(&recv_errs, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (76, MPI, MPI_Send(&errs, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);); (39, MPI, MPI_Recv(buf, 1, newtype, 0, i, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)"}
{"predict": "]) ||\n112:                 (all_st_end_offsets[i * 2 - 1] > all_st_end_offsets[i * 2 + 1])) {\n113:                 interleave_count++;\n114:             }\n115:         }\n116: \n117:         /* free the array of start/end pairs */\n118:         ADIOI_Free(all_st_end_offsets);\n119:     }\n120: \n121:     /* check if I am the aggregator */\n122:     agg_rank = fd->hints->cb_agg_rank;\n123: \n124:     /* create a collective buffer if needed */\n125:     if (cb_enable != ADIOI_HINT_DISABLE) {\n126:         if (agg_rank == MPI_UNDEFINED) {\n127:             /* this process will be the aggregator */\n128:             cb_buf = (char *) ADIOI_Malloc(bufsize);\n129:             agg_rank = 0;\n130:         }\n131:         /* allocate a datatype for the collective buffer */\n132:         MPI_Type_create_resized(datatype, lb, extent, &agg_dtype);\n133:         MPI_Type_commit(&agg_dtype);\n134:     }\n135: \n136:     /* allocate a datatype for the client perspective */\n137:     MPI_Type_create_resized(datatype, lb, extent, &client_comm_dtype);\n138:     MPI_Type_commit(&client_comm_dtype);\n139: \n140:     /* allocate a datatype for the aggregator perspective */\n141:     MPI_Type_create_resized(datatype, lb, extent, &agg_comm_dtype);\n142:     MPI_Type_commit(&agg_comm_dtype);\n143: \n144:     /* get the client perspective of the file view */\n145:     ADIOI_Get_file_view(fd, &client_file_view_state_arr);\n146: \n147:     /* get the aggregator perspective of the file view */\n148:     ADIOI_Get_file_view(fd, &agg_file_view_state_arr);\n149: \n150:     /* get the local process perspective of the file view */\n151:     ADIOI_Get_file_view(fd, &my_mem_view_state_arr);\n152:", "label": "(106, MPI, MPI_Allgather(st_end_offset, 2, ADIO_OFFSET, all_st_end_offsets, 2, ADIO_OFFSET, fd->comm);); (71, MPI, MPI_Comm_rank(fd->comm, &myrank);); (527, MPI, MPI_Type_free(&client_comm_dtype_arr[i]);); (587, MPI, MPI_Type_free(&fd->file_realm_types[0]);); (67, MPI, MPI_Comm_size(fd->comm, &nprocs);); (589, MPI, MPI_Type_free(&fd->file_realm_types[i]);); (155, MPI, MPI_Type_get_extent(datatype, &lb, &extent);); (307, MPI, MPI_Type_size_x(agg_comm_dtype_arr[i], &size);); (301, MPI, MPI_Type_free(&agg_dtype);)"}
{"predict": "3:         recvbuf, recvcounts, recvdisps, MPI_UINT64_T,\n114:         MPI_COMM_WORLD\n115:     );\n116:     \n117:     /* deallocate send buffer */\n118:     MFU_FREE(sendbuf);\n119: \n120:     /* iterate over the recv buffer and update results */\n121:     disp = 0;\n122:     p = head;\n123:     for (i = 0; i < list_count; i++) {\n124:         /* if we have the last byte of the file, we need to update the result */\n125:         if (p->offset + p->length >= p->file_size) {\n126:             /* copy result back to the results array */\n127:             results[i] = (int) recvbuf[disp++];\n128:         }\n129: \n130:         /* advance to next chunk */\n131:         p = p->next;\n132:     }\n133:     \n134:     /* deallocate all memory */\n135:     MFU_FREE(sendcounts);\n136:     MFU_FREE(recvcounts);\n137:     MFU_FREE(recvdisps);\n138:     MFU_FREE(senddisps);\n139:     MFU_FREE(recvbuf);\n140:     MFU_FREE(keys);\n141:     MFU_FREE(ltr);\n142: }", "label": "(99, MPI, MPI_Alltoall(sendcounts, 1, MPI_INT, recvcounts, 1, MPI_INT, MPI_COMM_WORLD);); (46, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (40, MPI, MPI_Type_free(&keytype);)"}
{"predict": "(12, OpenMP, #pragma omp parallel for private(sum))", "label": "(13, OpenMP, #pragma omp parallel default(none) shared(data,x,y,z,repeats))"}
{"predict": "(11, MPI, mpi_world_communicator.MaxAll(local, output);) (17, MPI, returned_result = mpi_world_communicator.MaxAll(local);)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "] -= 2*vector[i];\n120:           else      vector[i] -= 2*aux;\n121:         }\n122:         for (i=0; i<vector_length; i++) {\n123:           aux = (3 - (i&7));\n124:           if (aux>0) vector[i] -= 2*vector[i];\n125:           else      vector[i] -= 2*aux;\n126:         }\n127:       }\n128:       break;\n129: \n130:     case NO_VECTOR:\n131:       for (iter=0; iter<iterations; iter+=2) {\n132:         for (i=0; i<vector_length; i++) {\n133:           aux = -(3 - (i&7));\n134:           if (vector[i]>0) vector[i] -= 2*vector[i];\n135:           else                    vector[i] -= 2*aux;\n136:         }\n137:         for (i=0; i<vector_length; i++) {\n138:           aux = (3 - (i&7));\n139:           if (vector[i]>0) vector[i] -= 2*vector[i];\n140:           else                    vector[i] -= 2*aux;\n141:         }\n142:       }\n143:       break;\n144: \n145:     case INS_HEAVY:\n146:       for (iter=0; iter<iterations; iter+=2) {\n147:         for (i=0; i<vector_length; i++) {\n148:           aux = -(3 - (i&7));\n149:           if (vector[i]>0) vector[i] -= 2*vector[i];\n150:           else                    vector[i] -= 2*aux;\n151:         }\n152:         for (i=0; i<vector_length; i++) {\n153:           aux = (3 - (i&7));\n154:           if (vector[i]>0) vector[i] -= 2*vector[i];\n155:           else                    vector[i] -= 2*aux;\n156:         }\n15", "label": "(207, MPI, MPI_Reduce(&total, &total_sum, 1, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD);); (230, MPI, MPI_Finalize();); (79, MPI, MPI_Bcast(&iterations,    1, MPI_INT, root, MPI_COMM_WORLD);); (21, MPI, MPI_Init(&argc,&argv);); (76, MPI, MPI_Bcast(&vector_length, 1, MPI_INT, root, MPI_COMM_WORLD);); (77, MPI, MPI_Bcast(&btype,         1, MPI_INT, root, MPI_COMM_WORLD);); (93, MPI, MPI_Barrier(MPI_COMM_WORLD);); (19, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (17, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(10, CUDA, CUDA_CHECK(cudaMallocManaged(reinterpret_cast<void **>(&d_data), sizeof(data_type) * h_data.size()));) (11, CUDA, ) (27, CUDA, ) (30, CUDA, ) (32, CUDA, )", "label": "(36, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (38, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(10, MPI, MPI_Scatter(a, 10, MPI_INT, b, 10, MPI_INT, 0, MPI_COMM_WORLD, &status);) (13, MPI, MPI_Gather(b, 10, MPI_INT, a, 10, MPI_INT, 0, MPI_COMM_WORLD, &status);)", "label": "(44, MPI, MPI_Buffer_detach(&bptr, &bl);); (24, MPI, MPI_Recv(b, 10, MPI_INT, i, 27 + j, MPI_COMM_WORLD, &status);); (14, MPI, MPI_Bsend(a, 10, MPI_INT, 0, 27 + j, MPI_COMM_WORLD);); (7, MPI, MPI_Buffer_attach(buf, BUFSIZE);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)"}
{"predict": "(17, CUDA, sharedMemory[localIndexX][localIndexY] = input[index];)", "label": "(2, CUDA, __shared__ int sharedMemory [BLOCK_SIZE] [BLOCK_SIZE + 1];)"}
{"predict": "(35, MPI, for (int i = 0; i < num_neighbors; i++) { MPI_Irecv(x_external, receiveLength[i], MPI_DOUBLE, neighbors[i], MPI_MY_TAG, MPI_COMM_WORLD, request + i); x_external += receiveLength[i]; })\n(46, MPI, for (local_int_t i=0; i<totalToBeSent; i++) { sendBuffer[i] = xv[elementsToSend[i]]; })\n(53, MPI, for (int i = 0; i < num_neighbors; i++) { MPI_Isend(sendBuffer, sendLength[i], MPI_DOUBLE, neighbors[i], MPI_MY_TAG, MPI_COMM_WORLD, request + i); sendBuffer += sendLength[i]; })\n(64, MPI, for (int i = 0; i < num_neighbors; i++) { MPI_Wait(request+i, &status); })", "label": "(57, MPI, MPI_Send(sendBuffer, n_send, MPI_DOUBLE, neighbors[i], MPI_MY_TAG, MPI_COMM_WORLD);); (15, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (36, MPI, MPI_Irecv(x_external, n_recv, MPI_DOUBLE, neighbors[i], MPI_MY_TAG, MPI_COMM_WORLD, request+i);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(11, OpenMP, #pragma omp parallel for nowait reduction(+:count) private(i) for (i = 0; i < 4; i++))", "label": "(5, OpenMP, #pragma omp parallel num_threads(4)); (11, OpenMP, #pragma omp for schedule(static) nowait); (8, OpenMP, rank = omp_get_thread_num();); (16, OpenMP, #pragma omp atomic); (20, OpenMP, #pragma omp for schedule(static))"}
{"predict": "(8, OpenMP, #pragma omp parallel for reduction(+:ai,af,ad,ri,rf,rd))", "label": "(4, OpenMP, #pragma omp parallel for)"}
{"predict": "(2, OpenMP, #pragma omp parallel for)", "label": "(1, OpenMP, #pragma omp parallel for ordered ordered); (15, OpenMP, #pragma omp parallel for order(concurrent)); (8, OpenMP, #pragma omp parallel for order(none); (4, OpenMP, #pragma omp parallel for order(); (0, OpenMP, #pragma omp parallel for order); (8, OpenMP, #pragma omp parallel for order(concurrent)"}
{"predict": "(24, OpenMP, #pragma omp parallel for)", "label": "(14, OpenMP, printf(\"Thread %d of %d\\n\", omp_get_thread_num(), omp_get_num_threads());)"}
{"predict": "(18, CUDA, cu_st = cuDeviceGetCount(&num_devices);)", "label": "(43, CUDA, CUDA_CHECK_GOTO(cudaGetDevice(&self->device), free_mpool, status);); (15, CUDA, cuda_st = cudaGetDeviceCount(&num_devices);)"}
{"predict": "(17, CUDA, d_in1 = (float*)cudaMalloc(in1_bytes);) (18, CUDA, d_out = (float*)cudaMalloc(out_bytes);) (20, CUDA, gpu_out.device(gpu_device) = gpu_in1.sum();) (30, CUDA, sum += in1(i, k, j, l);) (32, CUDA, cudaFree(d_in1);) (33, CUDA, cudaFree(d_out);)", "label": "(25, CUDA, assert(cudaMemcpyAsync(out.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);); (11, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (9, CUDA, cudaMalloc((void**)(&d_in1), in1_bytes);); (11, CUDA, cudaMemcpy(d_in1, in1.data(), in1_bytes, cudaMemcpyHostToDevice);); (39, CUDA, cudaFree(d_out);); (21, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (36, CUDA, cudaFree(d_in1);)"}
{"predict": "(12, OpenMP, #pragma omp teams default(none) private(argc,b) num_teams(f) firstprivate(argv) reduction(|: c,d) reduction(*: e) thread_limit(f + g);)", "label": "(14, OpenMP, #pragma omp teams default(none), private(argc,b) num_teams(f) firstprivate(argv) reduction(| : c, d) reduction(* : e) thread_limit(f+g)); (6, OpenMP, #pragma omp target); (2, OpenMP, #pragma omp threadprivate(a)); (5, OpenMP, #pragma omp teams)"}
{"predict": "_wait_until_any(flag_rd_q + d_colnummod[tid], d_statusmod + d_colnummod[tid] * 2, d_statusmod + d_colnummod[tid] * 2 + 1, NVSHMEM_CMP_EQ, 1);\n98:                     //printf(\"(%d,%d,%d) d_colnummod=%d,recv_cnt=%d,i=%d,wait_off=%d,%d,status=%d,%d\\n\", mype, bid, tid, d_colnummod[tid], d_recv_cnt[d_colnummod[tid]],i,d_colnummod[tid]*2, d_colnummod[tid]*2+1,d_statusmod[d_colnummod[tid]*2], d_statusmod[d_colnummod[tid]*2+1]);\n99:                     d_statusmod[d_colnummod[tid] * 2 + wm_val] = 1;\n100:                     //printf(\"(%d,%d,%d) d_colnummod=%d,recv_cnt=%d,i=%d,wait_off=%d,%d,status=%d,%d\\n\", mype, bid, tid, d_colnummod[tid], d_recv_cnt[d_colnummod[tid]],i,d_colnummod[tid]*2, d_colnummod[tid]*2+1,d_statusmod[d_colnummod[tid]*2], d_statusmod[d_colnummod[tid]*2+1]);\n101:                 }\n102:             }\n103:         } else {\n104:             //printf(\"(%d,%d,%d) d_colnummod=%d,recv_cnt=%d\\n\", mype, bid, tid, d_colnummod[tid], d_recv_cnt[d_colnummod[tid]]);\n105:             int delta = d_nfrecvmod[1] % WAIT_NUM_THREADS;\n106:             if (tid < delta) {\n107:                 d_mynum[tid] = d_nfrecvmod[1] / WAIT_NUM_THREADS + 1;\n108:             } else {\n109:                 d_mynum[tid] = d_nfrecvmod[1] / WAIT_NUM_THREADS;\n110:             }\n111:             __syncthreads();\n112:             d_mymaskstart[tid] = 0;\n113:             for (int i = 0; i < tid; i++) {\n114:                 d_mymaskstart[tid]", "label": "(341, CUDA, __shared__ int cur_send_num;); (339, CUDA, __shared__ int recv_num, finish_num;)"}
{"predict": "(22, OpenMP, #pragma omp parallel for) (46, OpenMP, #pragma omp parallel for) (70, OpenMP, #pragma omp parallel for)", "label": "(4, TBB, tbb::flow::queue_node<T> q2(g);); (70, TBB, tbb::flow::make_edge( q2, q3 );); (67, TBB, tbb::flow::queue_node<T> q3(g);); (37, TBB, tbb::flow::make_edge( q, q2 );); (56, TBB, tbb::flow::remove_edge( q, q2 );); (87, TBB, tbb::flow::remove_edge( q,  q2 );); (-3, TBB, tbb::flow::queue_node<T> q(g);); (-5, TBB, tbb::flow::graph g;)"}
{"predict": "(6, OpenCL, retVal = myCmdQ.enqueueStagingBufferMemcpy(false, dstPtr, srcPtr, copySize, nullptr);)", "label": "(23, OpenCL, clReleaseEvent(event);)"}
{"predict": ": \n118: #ifdef USE_MPE\n119: \tMPE_Log_event(s_close, 0, \"start close file\");\n120: #endif /* USE_MPE */\n121: \n122: \t/* Close the file. */\n123: \tif (nc_close(ncid))\n124: \t\tERR;\n125: \n126: \t/* Finalize MPI. */\n127: \tif (mpi_rank == 1) {\n128: \t\tprintf(\"\\n*** tst_parallel testing whether we can create file for parallel access and write to it... done.\\n\");\n129: \t}\n130: \tMPI_Finalize();\n131: \treturn 0;\n132: }", "label": "(25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (25, MPI, MPI_Get_processor_name(mpi_name, &mpi_namelen);); (22, MPI, MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);)"}
{"predict": "(34, OpenCL, auto surfaceState = FamilyType::cmdInitRenderSurfaceState; imageHw->setImageArg(&surfaceState, false, 0, context.getDevice(0)->getRootDeviceIndex());)", "label": "(43, OpenCL, clReleaseMemObject(imageDesc.mem_object);); (14, OpenCL, imageDesc.mem_object = clCreateBuffer(&context, CL_MEM_READ_WRITE, imageDesc.image_height * imageDesc.image_width, nullptr, &retVal);)"}
{"predict": ", i2+1, i1+1) + I3D(or, m1k, m2k, i3+2, i2+1, i1+1);\n73: \n74:         I3D(y1, m1k, m2k, i3, i2, i1+1) = y2;\n75:         I3D(x1, m1k, m2k, i3, i2, i1+1) = x2;\n76:       }\n77:     }\n78:   }\n79:   if (timeron) timer_stop(T_rprj3);\n80: \n81:   if (m1k == 3) {\n82:     d1 = 2;\n83:   } else {\n84:     d1 = 1;\n85:   }\n86: \n87:   if (m2k == 3) {\n88:     d2 = 2;\n89:   } else {\n90:     d2 = 1;\n91:   }\n92: \n93:   if (m3k == 3) {\n94:     d3 = 2;\n95:   } else {\n96:     d3 = 1;\n97:   }\n98: \n99:   /* #pragma omp parallel for default(shared) */\n100:   for (j3 = 1; j3 < m3j-1; j3++) {\n101:     for (j2 = 1; j2 < m2j-1; j2++) {\n102:       for (j1 = 1; j1 < m1j; j1++) {\n103:         i3 = 2*j3-d3;\n104:         i2 = 2*j2-d2;\n105:         i1 = 2*j1-d1;\n106: \n107:         I3D(x1, m1k, m2k, i3, i2, i1) = I3D(or, m1k, m2k, i3+1, i2, i1) + I3D(or, m1k, m2k, i3+1, i2+2", "label": "(114, OpenMP, omp_target_free(y1, omp_get_default_device());); (41, OpenMP, #pragma omp teams distribute parallel for default(shared) collapse(3)); (39, OpenMP, #pragma omp target is_device_ptr(x1, y1) map(tofrom: or[0:m3k*m2k*m1k]) map(tofrom: os[0:m3j*m2j*m1j])); (13, OpenMP, y1 = (double*)omp_target_alloc(m3k*m2k*m1k*sizeof(double), omp_get_default_device());)"}
{"predict": "(25, OpenMP, #pragma omp parallel for)", "label": "(9, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)"}
{"predict": ", finalIndex, value, index,\n131:             sMem, tid);\n132:             break;\n133:         case 256:\n134:           bitonic_sort_global_256(\n135:             finalValue, finalIndex, value, index,\n136:             sMem, tid);\n137:             break;\n138:         case 512:\n139:           bitonic_sort_global_512(\n140:             finalValue, finalIndex, value, index,\n141:             sMem, tid);\n142:             break;\n143:         case 1024:\n144:           bitonic_sort_global_1024(\n145:             finalValue, finalIndex, value, index,\n146:             sMem, tid);\n147:             break;\n148:       }\n149:       \n150:       __syncthreads();\n151:     }\n152:     finalValue = finalIndex = -INFINITY;\n153:   }\n154:   gValue[qid] = finalValue;\n155:   gIndex[qid] = finalIndex;\n156: }\n\nOutput:", "label": "(17, CUDA, extern __shared__ _VOLATILE_ float sMem[];)"}
{"predict": "(6, OpenCL, cl_event event; cl_int retVal = clEnqueueNDRangeKernel(cl->getCommandQueue(), kernel->getKernel(), 1, NULL, &workgroupSize, NULL, 0, NULL, &event); clWaitForEvents(1, &event); clReleaseEvent(event);)", "label": "(1, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)"}
