{"predict": "(16,OpenMP,#pragma omp parallel for), (16,OpenMP,for (int i = 0; i < SEQLEN; i++) {)", "label": "(20, CUDA, CUDA_CHECK(cudaMalloc((void **)&dev_max, sizeof(FP) * SEQLEN * DIM));); (39, CUDA, cudaFree(dev_max);); (22, CUDA, CUDA_CHECK(cudaMemcpy(dev_denom, host_denom, sizeof(FP) * SEQLEN * DIM, cudaMemcpyHostToDevice));); (18, CUDA, CUDA_CHECK(cudaMalloc((void **)&dev_denom, sizeof(FP) * SEQLEN * DIM));); (18, CUDA, CUDA_CHECK(cudaMalloc((void **)&QK, sizeof(FP) * SEQLEN * SEQLEN));); (18, CUDA, CUDA_CHECK(cudaMemcpy(dev_max, host_max, sizeof(FP) * SEQLEN * DIM, cudaMemcpyHostToDevice));); (33, CUDA, cudaFree(QK);); (34, CUDA, cudaFree(dev_denom);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for reduction(+:est.numPairs))", "label": "(32, OpenMP, #pragma omp critical(cerr)); (15, OpenMP, #pragma omp critical(out))"}
{"predict": "(20,OpenMP,#pragma omp parallel for)", "label": "(31, MPI, MPI_T_finalize();); (4, MPI, MPI_T_init_thread(required, &provided);); (6, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)\n(19,OpenMP,#pragma omp parallel for reduction(+:totalSum))", "label": "(19, OpenMP, #pragma omp parallel for simd reduction(+:totalSum) schedule(guided) aligned(data: 64) num_threads(N_THREADS)); (10, OpenMP, #pragma omp parallel for num_threads(N_THREADS))"}
{"predict": "(9,OpenMP,#pragma omp parallel for private(printfbuf_local, hdr) reduction(+:printf_count))", "label": "(15, CUDA, cudaMemcpy(printfbuf_local, bufptr, CUPRINTF_MAX_LEN, cudaMemcpyDeviceToHost);)"}
{"predict": "(31,OpenMP,#pragma omp parallel private(x,y,z)), (32,OpenMP,#pragma omp sections), (33,OpenMP,#pragma omp section), (36,OpenMP,#pragma omp section), (39,OpenMP,#pragma omp section)", "label": "(26, OpenMP, #pragma omp parallel sections firstprivate(int)); (10, OpenMP, #pragma omp parallel sections firstprivate(,); (14, OpenMP, #pragma omp parallel sections firstprivate(, )); (34, OpenMP, #pragma omp parallel sections lastprivate(x) firstprivate(x)); (17, OpenMP, #pragma omp parallel sections firstprivate()); (36, OpenMP, #pragma omp parallel sections lastprivate(x, y) firstprivate(x, y)); (25, OpenMP, #pragma omp parallel sections firstprivate(0)); (-3, OpenMP, #pragma omp parallel sections firstprivate(); (37, OpenMP, #pragma omp parallel sections lastprivate(x, y, z) firstprivate(x, y, z))"}
{"predict": "(13,OpenMP,#pragma omp parallel for)", "label": "(29, MPI, MPI_Recv(NULL, 0, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &status);); (16, MPI, MPI_Isend(a, 0, MPI_INT, rank, 0, MPI_COMM_WORLD, &request);); (24, MPI, MPI_Wait(&request, &status);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (24, MPI, MPI_Isend(NULL, 0, MPI_INT, rank, 0, MPI_COMM_WORLD, &request);); (14, MPI, MPI_Get_count(&status, MPI_INT, &count);); (12, MPI, MPI_Recv(b, 1, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &status);)"}
{"predict": "(15,OpenMP,#pragma omp parallel for)", "label": "(14, OpenCL, retVal = clReleaseMemObject(image);)"}
{"predict": "(56,OpenMP,#pragma omp parallel for private(offset, samples_double, dims, fsize, ssize))", "label": "(60, OpenMP, #pragma omp parallel for)"}
{"predict": "(18,OpenMP,#pragma omp parallel sections)", "label": "(17, CUDA, CHECK(cudaMalloc((void**) &anchorsDev, 4 * params.anchorsRatioCount * params.anchorsScaleCount * sizeof(float)));)"}
{"predict": "(13,OpenMP,#pragma omp parallel for)", "label": "(3, CUDA, extern __shared__ float entire_buffer[];)"}
{"predict": "(15,OpenMP,#pragma omp declare target)", "label": "(24, CUDA, cudaFree(res_dev);); (17, CUDA, cudaMalloc(&res_dev, sizeof(float));); (19, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "The provided code snippet is a unit test for a command queue in a graphics or compute context, and it does not contain any loops or computationally intensive tasks that would benefit from parallelization. Therefore, there are no lines in this code where parallelization instructions should be inserted.\n\nIf you have a different code snippet that contains loops or other parallelizable sections, feel free to share it, and I can provide appropriate parallelization instructions.", "label": "(16, OpenCL, auto b1 = clCreateBuffer(context, CL_MEM_READ_WRITE, 20, nullptr, nullptr);); (24, OpenCL, void *ptr2 = clEnqueueMapBuffer(&cmdQ, b2, CL_TRUE, CL_MAP_READ, 0, 8, 0, nullptr, nullptr, nullptr);); (32, OpenCL, clReleaseMemObject(b2);); (17, OpenCL, void *ptr1 = clEnqueueMapBuffer(&cmdQ, b1, CL_FALSE, CL_MAP_READ, 0, 8, 1, &gatingEvent, nullptr, nullptr);); (29, OpenCL, clReleaseMemObject(b1);); (31, OpenCL, clReleaseEvent(gatingEvent);); (25, OpenCL, clEnqueueUnmapMemObject(pCmdQ, b2, ptr2, 0, nullptr, nullptr);); (12, OpenCL, auto gatingEvent = clCreateUserEvent(context, nullptr);); (13, OpenCL, clEnqueueUnmapMemObject(&cmdQ, b1, ptr1, 0, nullptr, nullptr);); (8, OpenCL, auto b2 = clCreateBuffer(context, CL_MEM_READ_WRITE, 20, nullptr, nullptr);)"}
{"predict": "(29,OpenMP,#pragma omp parallel for), (95,OpenMP,#pragma omp parallel for private(errors))", "label": "(53, CUDA, cudaEventSynchronize(stop);); (86, CUDA, cudaMemcpy(TransposeMatrix, gpuTransposeMatrix, NUM * sizeof(float), cudaMemcpyDeviceToHost);); (9, CUDA, cudaGetDeviceProperties(&devProp, 0);); (44, CUDA, cudaMemcpy(gpuMatrix, Matrix, NUM * sizeof(float), cudaMemcpyHostToDevice);); (114, CUDA, cudaFree(gpuMatrix);); (32, CUDA, cudaMalloc((void**)&gpuMatrix, NUM * sizeof(float));); (49, CUDA, cudaEventElapsedTime(&eventMs, start, stop);); (36, CUDA, cudaEventRecord(start, NULL);); (112, CUDA, cudaFree(gpuTransposeMatrix);); (42, CUDA, cudaEventRecord(stop, NULL);); (10, CUDA, cudaEventCreate(&stop);); (28, CUDA, cudaMalloc((void**)&gpuTransposeMatrix, NUM * sizeof(float));); (6, CUDA, cudaEventCreate(&start);)"}
{"predict": "(22,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for), (46,OpenMP,#pragma omp parallel for)", "label": "(56, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (4, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (35, CUDA, ASSERT_EQ(cudaSuccess, cudaMemsetAsync(dstData->basePtr(), 0, dstBufSize * sizeof(T), stream));); (54, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamDestroy(stream));)"}
{"predict": "(46,OpenMP,#pragma omp parallel for), (54,OpenMP,#pragma omp parallel for), (87,OpenMP,#pragma omp parallel for)", "label": "(38, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (44, MPI, MPI_Allgather(&item_offset, 1, MPI_INT, rank_disps, 1, MPI_INT, MPI_COMM_WORLD);); (80, MPI, reduce_start = MPI_Wtime();)"}
{"predict": "(26,OpenMP,#pragma omp parallel for), (63,OpenMP,#pragma omp parallel for)", "label": "(58, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 1, NULL, threads, localThreads, 0, NULL, NULL );); (46, OpenCL, error = clSetKernelArg( kernel, 0, sizeof( streams[ 0 ] ), &streams[0] );); (60, OpenCL, error = clEnqueueReadBuffer( queue, streams[1], true, 0, sizeof( cl_double ) * TEST_SIZE, outData, 0, NULL, NULL );); (46, OpenCL, error = clSetKernelArg( kernel, 1, sizeof( streams[ 1 ] ), &streams[1] );)"}
{"predict": "(18,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for)", "label": "(26, CUDA, cudaMemcpy(d_A, A, m * k * sizeof(float), cudaMemcpyHostToDevice);); (56, CUDA, cudaFree(d_A);); (31, CUDA, cudaMemcpy(C, d_C, m * n * sizeof(float), cudaMemcpyDeviceToHost);); (55, CUDA, cudaFree(d_B);); (55, CUDA, cudaFree(d_C);); (17, CUDA, cudaMalloc((void **)&d_B, k * n * sizeof(float));); (15, CUDA, cudaMalloc((void **)&d_A, m * k * sizeof(float));); (16, CUDA, cudaMalloc((void **)&d_C, m * n * sizeof(float));); (20, CUDA, cudaMemcpy(d_C, C, m * n * sizeof(float), cudaMemcpyHostToDevice);); (18, CUDA, cudaMemcpy(d_B, B, k * n * sizeof(float), cudaMemcpyHostToDevice);)"}
{"predict": ", attention_backward,\n70:             cublas_handle, d_dinp, d_dqkvr, d_dpreatt, d_datt, d_scratch, d_dout, d_qkvr, d_att, B, T, C, NH\n71:         );\n72:         printf(\"block_size %4d | time %.4f ms\\n\", block_size, elapsed_time);\n73:     }\n74: \n75:     // cleanup\n76:     cublasCheck(cublasDestroy(cublas_handle));\n77:     cudaFree(d_inp);\n78:     cudaFree(d_qkvr);\n79:     cudaFree(d_out);\n80:     cudaFree(d_preatt);\n81:     cudaFree(d_att);\n82:     cudaFree(d_dout);\n83:     cudaFree(d_dinp);\n84:     cudaFree(d_dqkvr);\n85:     cudaFree(d_dpreatt);\n86:     cudaFree(d_datt);\n87:     cudaFree(d_scratch);\n88:     free(qkv);\n89:     free(out);\n90:     free(dout);\n91:     free(dqkv);\n92:     return 0;\n93: }", "label": "(45, CUDA, cudaCheck(cudaMemset(d_dinp, 0, B * T * 3 * C * sizeof(float)));); (47, CUDA, cudaCheck(cudaMalloc(&d_dpreatt, B * NH * T * T * sizeof(float)));); (99, CUDA, cudaCheck(cudaFree(d_inp));); (38, CUDA, cudaCheck(cudaMemset(d_att, 0, B * NH * T * T * sizeof(float)));); (105, CUDA, cudaCheck(cudaFree(d_dpreatt));); (102, CUDA, cudaCheck(cudaFree(d_dinp));); (102, CUDA, cudaCheck(cudaFree(d_dqkvr));); (28, CUDA, cudaCheck(cudaMemset(d_qkvr, 0, B * T * 3 * C * sizeof(float)));); (31, CUDA, cudaCheck(cudaMemset(d_preatt, 0, B * NH * T * T * sizeof(float)));); (93, CUDA, cudaCheck(cudaFree(d_qkvr));); (30, CUDA, cudaCheck(cudaMalloc(&d_att, B * NH * T * T * sizeof(float)));); (99, CUDA, cudaCheck(cudaFree(d_datt));); (35, CUDA, cudaCheck(cudaMemset(d_dqkvr, 0, B * T * 3 * C * sizeof(float)));); (24, CUDA, cudaCheck(cudaMemset(d_out, 0, B * T * C * sizeof(float)));); (90, CUDA, cudaCheck(cudaFree(d_preatt));); (27, CUDA, cudaCheck(cudaMalloc(&d_dout, B * T * C * sizeof(float)));); (18, CUDA, cudaCheck(cudaMalloc(&d_qkvr, B * T * 3 * C * sizeof(float)));); (16, CUDA, cudaCheck(cudaMemcpy(d_inp, qkv, B * T * 3 * C * sizeof(float), cudaMemcpyHostToDevice));); (35, CUDA, cudaCheck(cudaMemset(d_scratch, 0, B * T * C * sizeof(float)));); (19, CUDA, cudaCheck(cudaMalloc(&d_preatt, B * NH * T * T * sizeof(float)));); (91, CUDA, cudaCheck(cudaFree(d_scratch));); (22, CUDA, cudaCheck(cudaMemcpy(d_dout, dout, B * T * C * sizeof(float), cudaMemcpyHostToDevice));); (81, CUDA, cudaCheck(cudaFree(d_out));); (23, CUDA, cudaCheck(cudaMalloc(&d_dqkvr, B * T * 3 * C * sizeof(float)));); (20, CUDA, cudaCheck(cudaMalloc(&d_dinp, B * T * 3 * C * sizeof(float)));); (26, CUDA, cudaCheck(cudaMemset(d_datt, 0, B * NH * T * T * sizeof(float)));); (26, CUDA, cudaCheck(cudaMalloc(&d_scratch, B * T * C * sizeof(float)));); (9, CUDA, cudaCheck(cudaMalloc(&d_out, B * T * C * sizeof(float)));); (22, CUDA, cudaCheck(cudaMalloc(&d_datt, B * NH * T * T * sizeof(float)));); (3, CUDA, cudaCheck(cudaMalloc(&d_inp, B * T * 3 * C * sizeof(float)));); (75, CUDA, cudaCheck(cudaFree(d_att));); (18, CUDA, cudaCheck(cudaMemset(d_dpreatt, 0, B * NH * T * T * sizeof(float)));); (74, CUDA, cudaCheck(cudaFree(d_dout));)"}
{"predict": "(11,OpenMP,#pragma omp parallel for collapse(2)), (18,OpenMP,#pragma omp parallel for collapse(2))", "label": "(48, MPI, MPI_Buffer_detach(&bptr, &bl);); (17, MPI, MPI_Start(&request);); (27, MPI, MPI_Recv(b, 10, MPI_INT, i, 27 + j, MPI_COMM_WORLD, &status);); (8, MPI, MPI_Buffer_attach(buf, BUFSIZE);); (15, MPI, MPI_Wait(&request, &status);); (15, MPI, MPI_Request_free(&request);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (6, MPI, MPI_Bsend_init(a, 10, MPI_INT, 0, 27 + j, MPI_COMM_WORLD, &request);)"}
{"predict": "(57,OpenMP,#pragma omp parallel for), (108,MPI,MPI_Barrier(MPI_COMM_WORLD);), (109,OpenMP,#pragma omp parallel for)", "label": "(31, MPI, auto err = MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided);); (119, MPI, MPI_Finalize();); (37, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (40, MPI, ascent_opt[\"mpi_comm\"] = MPI_Comm_c2f(MPI_COMM_WORLD);); (111, MPI, MPI_Reduce(&run_time, &max_run_time, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);); (33, MPI, MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);)"}
{"predict": "(19,OpenMP,#pragma omp parallel for)", "label": "(15, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(16,OpenMP,#pragma omp parallel for collapse(2)), (26,OpenMP,#pragma omp parallel for collapse(2)), (32,OpenMP,#pragma omp parallel for collapse(2)), (36,OpenMP,#pragma omp parallel for collapse(2)), (40,OpenMP,#pragma omp parallel for collapse(2))", "label": "(47, TBB, double t2d_auto = pforTranspose2d<tbb::auto_partitioner>(N, a, b, gs);); (45, TBB, pforTranspose2d<tbb::auto_partitioner>(N, a, b, gs);); (4, TBB, c.set_core_type(tbb::info::core_types().back());); (1, TBB, tbb::task_arena::constraints c;); (48, TBB, double t2d_simple = pforTranspose2d<tbb::simple_partitioner>(N, a, b, gs);); (37, TBB, double t1d_simple = pforTranspose<tbb::simple_partitioner>(N, a, b, gs);); (-1, TBB, c.set_numa_id(tbb::info::numa_nodes()[0]);); (29, TBB, pforTranspose<tbb::auto_partitioner>(N, a, b, gs);); (29, TBB, double t1d_auto = pforTranspose<tbb::auto_partitioner>(N, a, b, gs);); (32, TBB, pforTranspose<tbb::simple_partitioner>(N, a, b, gs);); (-2, TBB, c.set_max_concurrency(std::min(8, tbb::info::default_concurrency(c)));); (-2, TBB, tbb::task_arena a(c);); (39, TBB, pforTranspose2d<tbb::simple_partitioner>(N, a, b, gs);)"}
{"predict": "(28,OpenMP,#pragma omp parallel for), (44,OpenMP,#pragma omp parallel for), (62,OpenMP,#pragma omp parallel for), (86,OpenMP,#pragma omp parallel for), (103,OpenMP,#pragma omp parallel for), (118,OpenMP,#pragma omp parallel for)", "label": "(12, CUDA, __shared__ FP sV[Bc][dim];); (19, CUDA, __shared__ FP sMax[Br];); (9, CUDA, __shared__ FP sK[Bc][dim];); (12, CUDA, __shared__ FP sQK[Br][Bc];); (13, CUDA, __shared__ FP sSafeE[Br][Bc];); (9, CUDA, __shared__ FP sO[Br][dim];); (4, CUDA, __shared__ FP sQ[Br][dim];); (12, CUDA, __shared__ FP sDenom[Br];)"}
{"predict": "(43,OpenMP,#pragma omp parallel for),", "label": "(22, CUDA, CudaCheck(cudaSetDevice(device_id_));)"}
{"predict": "This code snippet appears to be a unit test and does not contain any loops or computationally intensive tasks that would benefit from parallelization. The code is focused on setting up a test scenario and making assertions about the behavior of the `MultipleMapBufferTest` class. Therefore, there are no appropriate places to insert parallelization instructions in this specific code.\n\nIf there were loops or other parallelizable sections in the code, I would suggest using OpenMP for shared memory parallelism on multi-core CPUs. However, based on the provided code, no parallelization is necessary or applicable.", "label": "(14, OpenCL, retVal = clEnqueueUnmapMemObject(cmdQ.get(), buffer.get(), mappedPtr, 0, nullptr, nullptr);); (6, OpenCL, void *mappedPtr = clEnqueueMapBuffer(cmdQ.get(), buffer.get(), CL_FALSE, CL_MAP_WRITE, offset, size, 0, nullptr, nullptr, nullptr);)"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (49,OpenMP,#pragma omp parallel for reduction(+:v))", "label": "(69, OpenMP, #pragma omp parallel private(i)); (42, OpenMP, #pragma omp master taskloop lastprivate(a, b)); (49, OpenMP, #pragma omp master taskloop lastprivate(z, e, g)); (16, OpenMP, #pragma omp master taskloop lastprivate(argc); (2, OpenMP, #pragma omp parallel); (18, OpenMP, #pragma omp master taskloop lastprivate(argc,); (25, OpenMP, #pragma omp master taskloop lastprivate(argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (0, OpenMP, #pragma omp master taskloop lastprivate); (47, OpenMP, #pragma omp master taskloop lastprivate(h)); (18, OpenMP, #pragma omp master taskloop lastprivate(argc > 0 ? argv[1] : argv[2])); (64, OpenMP, #pragma omp master taskloop lastprivate(i)); (51, OpenMP, #pragma omp master taskloop allocate(omp_thread_mem_alloc: i) lastprivate(i)); (27, OpenMP, #pragma omp master taskloop lastprivate(S1)); (57, OpenMP, #pragma omp master taskloop lastprivate(j)); (-3, OpenMP, #pragma omp master taskloop lastprivate(); (32, OpenMP, #pragma omp master taskloop lastprivate(argv[1])); (-1, OpenMP, #pragma omp master taskloop lastprivate()); (18, OpenMP, #pragma omp master taskloop lastprivate(conditional: argc) lastprivate(conditional:); (50, OpenMP, #pragma omp parallel shared(i))"}
{"predict": "(7,OpenMP,#pragma omp parallel for)", "label": "(5, CUDA, cudaStream_t stream = at::cuda::getCurrentCUDAStream();)"}
{"predict": "(13,MPI,MPI_Irecv(NULL, 0, MPI_BYTE, to, 100, MPI_COMM_WORLD, &request);); (14,MPI,MPI_Send(NULL, 0, MPI_BYTE, from, 100, MPI_COMM_WORLD);); (15,MPI,MPI_Wait(&request, MPI_STATUS_IGNORE);)", "label": "(13, MPI, MPI_Recv(NULL, 0, MPI_BYTE, to, 100, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (11, MPI, MPI_Send(NULL, 0, MPI_BYTE, from, 100, MPI_COMM_WORLD);); (4, MPI, MPI_Irecv(rbuf, size, MPI_BYTE, from, tagno, MPI_COMM_WORLD, &request);); (18, MPI, MPI_Send(sbuf, size, MPI_BYTE, to, tagno, MPI_COMM_WORLD);); (20, MPI, MPI_Wait(&request, MPI_STATUS_IGNORE);)"}
{"predict": "(18,OpenMP,#pragma omp parallel for)", "label": "(13, MPI, MPI_Abort(MPI_COMM_WORLD, MPIT_result);); (14, MPI, MPIT_result = MPI_T_pvar_session_create(session);); (8, MPI, MPIT_result = MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);)"}
{"predict": "(7,OpenMP,#pragma omp parallel for schedule(MY_SCHEDULE)), (12,OpenMP,#pragma omp atomic)", "label": "(9, OpenMP, #pragma omp atomic); (3, OpenMP, #pragma omp parallel); (5, OpenMP, #pragma omp for schedule(MY_SCHEDULE))"}
{"predict": "(36,OpenMP,#pragma omp parallel sections)", "label": "(14, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (11, CUDA, CUDA_CHECK(cudaGetDevice(&current_device_id));); (11, CUDA, CUDA_CHECK(cudaGetDeviceProperties(&props, current_device_id));)"}
{"predict": "The provided code is a function for destroying a CUDA handle and does not contain any loops or sections that are inherently parallelizable. However, if we assume that there might be some loops or sections that could be parallelized in a larger context, I will provide a hypothetical example based on a common pattern.\n\nLet's assume there is a loop in the function that could be parallelized using OpenMP. Here is an example:\n\n### **Modified Hypothetical Code**\n\n1: hiednnStatus_t\n2: hiednnDestroyCudaHandle(HiednnCudaHandle *handle) {\n3:     if (handle != nullptr) {\n4:         // release cuda memory workspace\n5:         if (err != cudaSuccess && err != cudaErrorCudartUnloading) {\n6:             return HIEDNN_STATUS_RUNTIME_ERROR;\n7:         }\n8: \n9:         // Hypothetical loop that could be parallelized\n10:        for (int i = 0; i < numElements; i++) {\n11:            // some operations on handle->workspace[i]\n12:        }\n13: \n14:        if (err != cudaSuccess && err != cudaErrorCudartUnloading) {\n15:            return HIEDNN_STATUS_RUNTIME_ERROR;\n16:        }\n17: \n18:        delete handle;\n19:        return HIEDNN_STATUS_SUCCESS;\n20:     } else {\n21:        return HIEDNN_STATUS_INVALID_PARAMETER;\n22:     }\n23: }\n\n### **Parallelization Instructions**\n\n(10,OpenMP,#pragma omp parallel for)\n\nThis assumes that the loop starting at line 10 could be parallelized with OpenMP. If there are no such loops or parallelizable sections in the actual code, no parallelization instructions are needed.", "label": "(4, CUDA, auto err = cudaFree(handle->deviceWs);); (8, CUDA, err = cudaEventDestroy(handle->wsMutex);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for)", "label": "(18, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "78: \t\tlen += strlen(pScriptLines[i]);\n79: \tlen += strlen(pExpr);\n80: \tlen += strlen(keys_base16_in1_type);\n81: \tlen += strlen(keys_base16_in1_type);\n82: \tlen += 100;\n83: \t_p->pScript = mem_alloc(len);\n84: \t_p->pScript[0] = 0;\n85: \tfor (i = 0; i < nScriptLines; ++i)\n86: \t\tstrcat(_p->pScript, pScriptLines[i]);\n87: \t_p->pScript[len-1] = 0;\n88: \treturn 0;\n89: }", "label": "(39, OpenMP, comp_add_script_line(\"Func=DynamicFunc__\" #C \"_crypt_input%d_to_output1_FINAL\\n\",side);); (18, OpenMP, else                 comp_add_script_line(\"Func=DynamicFunc__append_salt%s\\n\", side==2?\"2\":\"\");); (9, OpenMP, comp_add_script_line(\"Func=DynamicFunc__clean_input2_kwik\\n\");); (61, OpenMP, comp_add_script_line(\"MaxInputLen=110\\n\");); (5, OpenMP, comp_add_script_line(\"Func=DynamicFunc__set_input_len_%d\\n\", keys_hex_len);)"}
{"predict": "(33,OpenMP,#pragma omp parallel for)", "label": "(50, OpenCL, clReleaseEvent(event);)"}
{"predict": "(tmp, pr1, dirs[1]);\n142:     myCovDev.MCD(pr1, pr1, dirs[1] + 4);\n143:     quda::blas::xpy(pr1, tmp);\n144:     applySpinTaste(pr1, tmp, gDirs[dirs[1]]);\n145:     quda::blas::axpy(acc, 1.0, pr1);\n146:     quda::blas::ax(0.5, acc);\n147:     applySpinTaste(acc, tmp, QUDA_SPIN_TASTE_G5);\n148:     out = acc;\n149:     break;\n150:   }\n151: \n152:   case 7:  // three-link XYZ\n153:   case 11: // three-link XYT\n154:   case 13: // three-link XZT\n155:   case 14: // three-link YZT\n156:   {\n157:     int dirs[3];\n158: \n159:     {\n160:       if (offset == 7) {\n161:         dirs[0] = 0;\n162:         dirs[1] = 1;\n163:         dirs[2] = 2;\n164:       }\n165:       if (offset == 11) {\n166:         dirs[0] = 0;\n167:         dirs[1] = 1;\n168:         dirs[2] = 3;\n169:       }\n170:       if (offset == 13) {\n171:         dirs[0] = 0;\n172:         dirs[1] = 2;\n173:         dirs[2] = 3;\n174:       }\n175:       if (offset == 14) {\n176:         dirs[0] = 1;\n177:         dirs[1] = 2;\n178:         dirs[2] = 3;\n179:       }\n180:     }\n181: \n182:     ColorSpinorField pr1(cudaParam", "label": "(17, CUDA, ColorSpinorParam cudaParam(cpuParam, inv_param, QUDA_CUDA_FIELD_LOCATION);)"}
{"predict": "(26,OpenMP,#pragma omp parallel sections), (30,OpenMP,#pragma omp section), (38,OpenMP,#pragma omp section), (50,OpenMP,#pragma omp parallel sections), (52,OpenMP,#pragma omp section), (56,OpenMP,#pragma omp section)", "label": "(55, MPI, MPI_Irecv(buf_recv, size_recv, MPI_BYTE, rank_recv, 0, comm, &request[num_req]);); (25, MPI, MPI_Irecv(&size_recv, 1, MPI_INT, rank_recv, 0, comm, &request[num_req]);); (60, MPI, MPI_Isend(buf_send, size_send, MPI_BYTE, rank_send, 0, comm, &request[num_req]);); (38, MPI, MPI_Isend(&size_send, 1, MPI_INT, rank_send, 0, comm, &request[num_req]);); (41, MPI, MPI_Waitall(num_req, request, status);)"}
{"predict": "(3,OpenMP,#pragma omp declare target), (11,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for)", "label": "(3, CUDA, extern __shared__ float shared[];)"}
{"predict": "(5,OpenMP,#pragma omp parallel for private(priv_t, len))", "label": "(4, OpenMP, #pragma omp parallel for)"}
{"predict": "(4,OpenMP,#pragma omp parallel for)", "label": "(11, OpenMP, #pragma omp distribute simd safelen (); (71, OpenMP, #pragma omp distribute simd safelen (4)); (3, OpenMP, #pragma omp distribute simd safelen); (19, OpenMP, #pragma omp distribute simd safelen (argc); (51, OpenMP, #pragma omp distribute simd safelen (S)); (44, OpenMP, #pragma omp distribute simd safelen (foobool(argc)), safelen (true), safelen (-5)); (72, OpenMP, #pragma omp distribute simd safelen (N)); (27, OpenMP, #pragma omp distribute simd safelen (1))); (58, OpenMP, #pragma omp distribute simd safelen (argv[1]=2)); (-6, OpenMP, #pragma omp target); (18, OpenMP, #pragma omp distribute simd safelen (ST); (29, OpenMP, #pragma omp distribute simd safelen ((ST > 0) ? 1 + ST : 2)); (5, OpenMP, #pragma omp distribute simd safelen ()); (-9, OpenMP, #pragma omp teams)"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (9,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (29,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for), (42,OpenMP,#pragma omp parallel for)", "label": "(33, OpenMP, #pragma omp parallel for simd private(argv[1])); (29, OpenMP, #pragma omp parallel for simd private(z, a, b)); (47, OpenMP, #pragma omp parallel for simd private(i)); (15, OpenMP, #pragma omp parallel for simd private(argc,); (23, OpenMP, #pragma omp parallel for simd private(S1)); (50, OpenMP, #pragma omp parallel shared(i)); (33, OpenMP, #pragma omp parallel for simd private(h)); (49, OpenMP, #pragma omp parallel private(i)); (-2, OpenMP, #pragma omp parallel); (12, OpenMP, #pragma omp parallel for simd private(argc > 0 ? argv[1] : argv[2])); (26, OpenMP, #pragma omp parallel for simd private(e, g)); (-2, OpenMP, #pragma omp parallel for simd private); (30, OpenMP, #pragma omp parallel for simd nowait); (44, OpenMP, #pragma omp parallel for simd private(j)); (10, OpenMP, #pragma omp parallel for simd private(argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (0, OpenMP, #pragma omp parallel for simd private(argc); (-4, OpenMP, #pragma omp parallel for simd private()"}
{"predict": "(4,OpenMP,#pragma omp parallel for), (7,OpenMP,#pragma omp parallel for private(j) firstprivate(a,b)), (24,OpenMP,#pragma omp parallel for private(j) firstprivate(c))", "label": "(3, OpenMP, #pragma omp parallel); (3, OpenMP, #pragma omp master taskloop)"}
{"predict": "(43,OpenMP,#pragma omp parallel for private(updt, currow, curcol, ii, jj) reduction(min:updt)), (47,OpenMP,#pragma omp single), (51,OpenMP,#pragma omp single), (55,OpenMP,#pragma omp single), (60,OpenMP,#pragma omp single)", "label": "(25, MPI, MPI_Comm_rank(comm_row, &mycol);); (25, MPI, MPI_Comm_rank(comm_col, &myrow);); (25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(49,OpenMP,#pragma omp parallel for private(segsize, fnz, ucol, rel) collapse(2)), (58,OpenMP,#pragma omp for)", "label": "(56, OpenMP, #pragma omp simd)"}
{"predict": "(53,OpenMP,#pragma omp parallel for private(errs, comm, win, orig, target, result))", "label": "(52, MPI, MPI_Comm_size(comm, &size);); (62, MPI, MPI_Barrier(comm);); (52, MPI, MPI_Win_create(target.buf, target.maxbufsize, base_type_size, MPI_INFO_NULL, comm, &win);); (69, MPI, MPI_Win_free(&win);); (25, MPI, MPI_Type_get_extent(dtp.DTP_base_type, &lb, &extent);); (46, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(6,OpenMP,#pragma omp parallel for reduction(+:temp))", "label": "(1, CUDA, __shared__ float cache[threadsPerBlock];)"}
{"predict": "hs, n, -1.0,\n130: \t\t\t\t     Aref, lda, X, ldx, 1.0, B, ldb);\n131: \n132: \t\t\tdouble anorm = LAPACKE_zlange_work(LAPACK_COL_MAJOR, 'I', n, n,\n133: \t\t\t\t\t\t\t  Aref, lda, NULL);\n134: \t\t\tdouble bnorm = LAPACKE_zlange_work(LAPACK_COL_MAJOR, 'I', n,\n135: \t\t\t\t\t\t\t  nrhs, B, ldb, NULL);\n136: \t\t\tdouble xnorm = LAPACKE_zlange_work(LAPACK_COL_MAJOR, 'I', n,\n137: \t\t\t\t\t\t\t  nrhs, X, ldx, NULL);\n138: \n139: \t\t\tdouble eps = LAPACKE_dlamch('E');\n140: \t\t\tdouble result = bnorm / (anorm * xnorm * eps * n);\n141: \n142: \t\t\tif (isinf(result) || isnan(result)) {\n143: \t\t\t\tresult = 1. / eps;\n144: \t\t\t}\n145: \n146: \t\t\tparam[PARAM_ERROR].d = result;\n147: \t\t\tparam[PARAM_SUCCESS].i = result < tol;\n148: \t\t} else {\n149: \t\t\tparam[PARAM_ERROR].d = 1. / LAPACKE_dlamch('E');\n150: \t\t\tparam[PARAM_SUCCESS].i = 0;\n151: \t\t}\n152: \t}\n153: \n154: \t//================================================================\n155: \t// Free arrays.\n156: \t//================================================================\n157: \tfree(A);\n158: \tfree(T);\n159: \tfree(ipiv);\n160: \tfree(ipiv2);\n161: \tfree(B);\n162: \tfree(X);\n163: \tif (test) {\n164: \t\tfree(Aref);\n165: \t}\n166: }", "label": "(117, OpenMP, plasma_time_t stop = omp_get_wtime();); (113, OpenMP, plasma_time_t start = omp_get_wtime();)"}
{"predict": "(21,OpenMP,#pragma omp parallel for private(local_sum, att_bth, datt_bth, dpreatt_bth))", "label": "(7, CUDA, __shared__ float block_acc[32];)"}
{"predict": "(20,OpenMP,#pragma omp parallel for collapse(2))", "label": "(3, OpenMP, #pragma omp for collapse(2) schedule(runtime))"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (9,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (29,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for), (36,OpenMP,#pragma omp parallel for reduction(+:v)), (45,OpenMP,#pragma omp parallel for reduction(+:v))", "label": "(32, OpenMP, #pragma omp target parallel for simd linear(a, b : B::ib)); (23, OpenMP, #pragma omp target parallel for simd linear(argc : 5) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (4, OpenMP, #pragma omp target parallel for simd linear); (12, OpenMP, #pragma omp target parallel for simd linear(argc); (40, OpenMP, #pragma omp target parallel for simd linear(i)); (33, OpenMP, #pragma omp target parallel for simd linear(e, g, z)); (45, OpenMP, #pragma omp target parallel for simd linear(v : i)); (34, OpenMP, #pragma omp target parallel for simd linear(h)); (39, OpenMP, #pragma omp parallel); (12, OpenMP, #pragma omp target parallel for simd linear(argc > 0 ? argv[1] : argv[2])); (-1, OpenMP, #pragma omp target parallel for simd linear(); (7, OpenMP, #pragma omp target parallel for simd linear(argc,); (49, OpenMP, #pragma omp target parallel for simd linear(v : j)); (22, OpenMP, #pragma omp target parallel for simd linear(argv[1])); (13, OpenMP, #pragma omp target parallel for simd linear(S1)); (42, OpenMP, #pragma omp target parallel for simd linear(j)); (-4, OpenMP, #pragma omp target parallel for simd linear())"}
{"predict": "(26,OpenMP,#pragma omp parallel for collapse(2)), (36,OpenMP,#pragma omp parallel for collapse(2)), (47,OpenMP,#pragma omp parallel for collapse(2))", "label": "(25, OpenMP, #pragma omp parallel for)"}
{"predict": "(47,OpenMP,#pragma omp parallel for collapse(4))", "label": "(41, OpenMP, #pragma omp target simd collapse(4, )); (12, OpenMP, #pragma omp target simd collapse()); (48, OpenMP, #pragma omp target simd collapse(4 4)); (63, OpenMP, #pragma omp target simd collapse(4, 8)); (82, OpenMP, #pragma omp target simd collapse(5 - 5)); (4, OpenMP, #pragma omp target simd collapse(); (15, OpenMP, #pragma omp target simd collapse(, )); (10, OpenMP, #pragma omp target simd collapse(,); (74, OpenMP, #pragma omp target simd collapse(0)); (36, OpenMP, #pragma omp target simd collapse(4)); (45, OpenMP, #pragma omp target simd collapse(4, , 4)); (63, OpenMP, #pragma omp target simd collapse(foo())); (66, OpenMP, #pragma omp target simd collapse(-5)); (23, OpenMP, #pragma omp target simd collapse(4,); (17, OpenMP, #pragma omp target simd collapse(4); (11, OpenMP, #pragma omp target simd collapse 4)); (-11, OpenMP, #pragma omp target simd collapse); (53, OpenMP, #pragma omp target simd collapse(2.5))"}
{"predict": "(84,OpenMP,#pragma omp parallel for)", "label": "(97, OpenMP, nthreads = omp_get_num_threads();); (82, OpenMP, #pragma omp parallel firstprivate(niter) private(step)); (62, OpenMP, #pragma omp parallel); (85, OpenMP, #pragma omp master)"}
{"predict": "(38,OpenMP,#pragma omp parallel for)", "label": "(35, OpenMP, iis3dhhc_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)"}
{"predict": "(1,MPI,MPI_Init(&argc, &argv);); (8,OpenMP,#pragma omp parallel for)", "label": "(72, OpenMP, #pragma omp target exit data depend(in : argc,argv[i:argc],x[:]) map(release: c) nowait if(b>g)); (17, OpenMP, #pragma omp target exit data map(release: c) if(b>g)); (52, OpenMP, #pragma omp target exit data nowait map(from: c) map(release: d)); (57, OpenMP, #pragma omp target exit data nowait depend(in : argc,argv[i:argc],x[:]) map(from: a)); (1, OpenMP, #pragma omp target exit data map(from: a)); (16, OpenMP, #pragma omp target exit data map(from: x[0:10], c)); (24, OpenMP, #pragma omp target exit data map(from: c) map(release: d)); (41, OpenMP, #pragma omp target exit data map(release: c) nowait if(b>g)); (58, OpenMP, #pragma omp target exit data map(from: a) if (b > g) nowait depend(in : argc,argv[i:argc],x[:])); (72, OpenMP, #pragma omp target exit data nowait depend(in : argc,argv[i:argc],x[:]) map(always,release: e)); (5, OpenMP, #pragma omp target exit data map(from: c)); (58, OpenMP, #pragma omp target exit data map(from: c) depend(in : argc,argv[i:argc],x[:]) nowait); (30, OpenMP, #pragma omp target exit data map(from: a) if (b > g) nowait); (38, OpenMP, #pragma omp target exit data nowait map(from: x[0:10], c)); (19, OpenMP, #pragma omp target exit data map(always,release: e)); (30, OpenMP, #pragma omp target exit data map(from: c) nowait); (62, OpenMP, #pragma omp target exit data nowait map(from: c) depend(in : argc,argv[i:argc],x[:]) map(release: d)); (58, OpenMP, #pragma omp target exit data nowait map(from: x[0:10], c) depend(in : argc,argv[i:argc],x[:])); (-6, OpenMP, #pragma omp target exit data map(from: a) if (b > g)); (5, OpenMP, #pragma omp target exit data map(delete: x[0:10])); (43, OpenMP, #pragma omp target exit data nowait map(from: a) depend(in : argc,argv[i:argc],x[:]) if (target exit data: b)); (36, OpenMP, #pragma omp target exit data nowait map(always,release: e)); (17, OpenMP, #pragma omp target exit data nowait map(from: a) if (target exit data: b)); (-14, OpenMP, #pragma omp target exit data map(from: a) if (target exit data: b)); (12, OpenMP, #pragma omp target exit data nowait map(from: a)); (2, OpenMP, #pragma omp target exit data map(always, delete: x[0:10]))"}
{"predict": "(36,OpenMP,#pragma omp parallel for), (56,OpenMP,#pragma omp parallel for)", "label": "(62, OpenCL, TEST_OCL_CHECK(clReleaseEvent(read_buffer_event));); (64, OpenCL, TEST_OCL_CHECK(clReleaseCommandQueue(ocl_queue));); (52, OpenCL, TEST_OCL_CHECK(clWaitForEvents(1, &read_buffer_event));); (61, OpenCL, TEST_OCL_CHECK(clReleaseEvent(eltwise_event));); (59, OpenCL, TEST_OCL_CHECK(clReleaseEvent(write_buffer_event));)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for)", "label": "(10, CUDA, extent = make_cudaExtent(4, 5, 0);); (3, CUDA, cudaExtent extent = make_cudaExtent(4, 5, 6);)"}
{"predict": "(57,OpenMP,#pragma omp parallel for)", "label": "(16, CUDA, cudaMalloc((void**)&device_array, num_bytes);); (49, CUDA, printf(\"CUDA Error: %s\\n\", cudaGetErrorString(error));); (35, CUDA, cudaThreadSynchronize();); (42, CUDA, cudaError_t error = cudaGetLastError();); (66, CUDA, cudaFree(device_array);); (54, CUDA, cudaMemcpy(host_array, device_array, num_bytes, cudaMemcpyDeviceToHost);)"}
{"predict": "(26,OpenMP,#pragma omp parallel for private(threadIndices, threadWeights, threadWeightsDu, threadWeightsDv, threadResult, threadResultDu, threadResultDv) shared(src, dst, dstDu, dstDv, weights, duWeights, dvWeights, indices, offsets, sizes, srcDesc, dstDesc, dstDuDesc, dstDvDesc, result, resultDu, resultDv))", "label": "(19, OpenMP, int numThreads = omp_get_max_threads();); (25, OpenMP, #pragma omp parallel for); (35, OpenMP, int threadId = omp_get_thread_num();)"}
{"predict": "(27,OpenMP,#pragma omp parallel for), (42,OpenMP,#pragma omp parallel for reduction(+:partial_result))", "label": "(56, MPI, MPI_Finalize();); (22, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (29, MPI, MPI_Bcast(&params, 1, *params_type, root, MPI_COMM_WORLD);); (14, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (10, MPI, MPI_Init_thread(NULL, NULL, MPI_THREAD_FUNNELED, &thread_level);)"}
{"predict": "(22,OpenMP,#pragma omp parallel for)", "label": "(23, MPI, MPI_Bcast(&fparams[0], 1, MPI_FLOAT, 0, MPI_COMM_WORLD);); (6, MPI, MPI_Bcast(&iparams[0], num_int_params, MPI_INT, 0, MPI_COMM_WORLD);)"}
{"predict": "(21,OpenMP,#pragma omp parallel sections)", "label": "(35, MPI, MPI_Finalize();); (2, MPI, MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &mpi_threads_provided);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);)"}
{"predict": "0;\n140:     if (rank == 0) {\n141:         MPI_Reduce(MPI_IN_PLACE, &starting_pos, 1, MPI_UINT64_T, MPI_MAX, 0, MPI_COMM_WORLD);\n142:     } else {\n143:         MPI_Reduce(&max_offset, &starting_pos, 1, MPI_UINT64_T, MPI_MAX, 0, MPI_COMM_WORLD);\n144:     }\n145: \n146:     /* broadcast the starting position to all ranks */\n147:     MPI_Bcast(&starting_pos, 1, MPI_UINT64_T, 0, MPI_COMM_WORLD);\n148: \n149:     /* get number of entries in our region */\n150:     uint64_t count = 0;\n151:     ptr = buf + (starting_pos - pos);\n152:     while (ptr < ptr_end) {\n153:         /* scan for the next \"ustar\" string we can find,\n154:          * note that this might not really be a valid entry\n155:          * even if we find the string */\n156:         size_t bytes_remaining = ptr_end - ptr;\n157:         char* ptr_found = (char*) memmem(ptr, bytes_remaining, \"ustar\", strlen(\"ustar\"));\n158:         if (ptr_found == NULL) {\n159:             /* no \"ustar\" found in the rest of the buffer */\n160:             break;\n161:         }\n162: \n163:         /* found a potential entry, check that we have enough bytes\n164:          * to read a full header, the \"ustar\" string starts at offset 257\n165:          * in the header */\n166:         uint64_t offset = (uint64_t)(ptr_found - ptr);\n167:         if (offset < 257) {\n168:             /* we don't have enough bytes before this string to even try,\n169:              * skip past this instance and try the next one */\n170:             ptr = ptr_found + strlen(\"ustar\");\n171:             continue;\n172:         }\n173: \n17", "label": "(141, MPI, MPI_Exscan(&max_offset, &starting_pos, 1, MPI_UINT64_T, MPI_MAX, MPI_COMM_WORLD);); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (258, MPI, MPI_Allreduce(&pos, &max_pos, 1, MPI_UINT64_T, MPI_MAX, MPI_COMM_WORLD);); (13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(16,OpenMP,#pragma omp parallel for private(config, time, found) reduction(min:fast_time) reduction(=:best_config))", "label": "(72, CUDA, cudaStreamDestroy(s);); (5, CUDA, cudaStreamCreate(&s);); (31, CUDA, cudaEventRecord(end, s);); (31, CUDA, cudaEventSynchronize(end);); (23, CUDA, cudaEventRecord(begin, s);); (66, CUDA, cudaEventDestroy(end);); (3, CUDA, cudaEventCreate(&end);); (63, CUDA, cudaEventDestroy(begin);); (0, CUDA, cudaEventCreate(&begin);); (27, CUDA, cudaEventElapsedTime(&time, begin, end);)"}
{"predict": "(34,OpenMP,#pragma omp parallel for)", "label": "(11, MPI, ::dray::dray::mpi_comm(MPI_Comm_c2f(comm));)"}
{"predict": "(27,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for)", "label": "(12, OpenMP, #pragma omp target parallel defaultmap(tofrom,); (1, OpenMP, #pragma omp target parallel defaultmap(); (14, OpenMP, #pragma omp target parallel defaultmap(tofrom, scalar); (24, OpenMP, #pragma omp target parallel defaultmap(none:pointer) defaultmap(none:scalar) map(g)); (35, OpenMP, #pragma omp target defaultmap(none:aggregate)); (26, OpenMP, #pragma omp target parallel defaultmap(none:aggregate) defaultmap(none:scalar) map(argc)); (8, OpenMP, #pragma omp target parallel defaultmap (scalar:); (47, OpenMP, #pragma omp target defaultmap(none:pointer)); (32, OpenMP, #pragma omp parallel); (14, OpenMP, #pragma omp target parallel defaultmap(none:pointer)); (0, OpenMP, #pragma omp target parallel defaultmap(tofrom)); (-7, OpenMP, #pragma omp target parallel defaultmap()); (7, OpenMP, #pragma omp target parallel defaultmap(tofrom:scalar) defaultmap(to:scalar)); (8, OpenMP, #pragma omp target parallel defaultmap(alloc:pointer) defaultmap(to:scalar) defaultmap(firstprivate:pointer)); (15, OpenMP, #pragma omp target parallel defaultmap(none:pointer) defaultmap(none:scalar) defaultmap(none:aggregate) map(g) firstprivate(argc)); (-7, OpenMP, #pragma omp target parallel defaultmap (tofrom:); (9, OpenMP, #pragma omp target parallel defaultmap(none:pointer) defaultmap(none:scalar) map(argc)); (-11, OpenMP, #pragma omp target parallel defaultmap(tofrom)"}
{"predict": "(57,MPI,MPI_Allreduce(&my_bytes, &max_bytes, 1, MPI_UNSIGNED_LONG, MPI_MAX, comm);)", "label": "(57, MPI, MPI_Allreduce(&my_bytes, &max_bytes, 1, MPI_UNSIGNED_LONG, MPI_MAX, comm);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for reduction(+:local_sum))", "label": "(15, CUDA, __shared__ typename BlockReduceT::TempStorage temp_storage;)"}
{"predict": "140:     }\n141:   }; // if\n142:   return err;\n143: }; // run_loop_64", "label": "(7, OpenMP, int tid = omp_get_thread_num();)"}
{"predict": "The provided code snippet is a function `comp_by_key` that compares two variants based on a specified key. The function does not contain any loops or large computations that would benefit from parallelization. However, if this function is called within a loop or a larger context where parallelization is possible, we can suggest potential places for parallelization based on that context.\n\nFor the sake of this exercise, let's assume that `comp_by_key` is called within a loop that iterates over a large number of elements, and this loop is a good candidate for parallelization using OpenMP. Here is an example of how you might parallelize a loop that calls `comp_by_key`:\n\n### **Assumed Loop Context**\n\n```c\n1: static int\n2: comp_by_key(purc_variant_t l, purc_variant_t r, const char *key, bool by_number,\n3:         bool ascendingly, bool casesensitively)\n4: {\n5:     purc_variant_t lv = PURC_VARIANT_INVALID;\n6:     purc_variant_t rv = PURC_VARIANT_INVALID;\n7:     if (purc_variant_is_object(l)) {\n8:         lv = purc_variant_object_get_by_ckey_ex(l, key, true);\n9:     }\n10: \n11:     if (purc_variant_is_object(r)) {\n12:         rv = purc_variant_object_get_by_ckey_ex(r, key, true);\n13:     }\n14: \n15:     // Assume some comparison logic here\n16:     return 0;\n17: }\n18: \n19: void sort_variants(purc_variant_t *variants, size_t count, const char *key) {\n20:     for (size_t i = 0; i < count; i++) {\n21:         for (size_t j = i + 1; j < count; j++) {\n22:             comp_by_key(variants[i], variants[j], key, true, true, true);\n23:         }\n24:     }\n25: }\n```\n\n### **Parallelization Instructions**\n\n(20,OpenMP,#pragma omp parallel for collapse(2))\n\nThis assumes that the nested loops in `sort_variants` can be parallelized. However, parallelizing nested loops with dependencies (like sorting) can be complex and may require additional considerations such as reducing race conditions or using parallel sorting algorithms. For", "label": "(14, OpenMP, return comp_raw(lv, rv, by_number, ascendingly, casesensitively);)"}
{"predict": ",k+1) * -1.1f);\n100:         VERIFY_IS_APPROX(expected, result);\n101:       }\n102:     }\n103:   }\n104: }\n105:", "label": "(27, CUDA, assert(cudaMemcpy(out.data(), d_out, out_bytes, cudaMemcpyDeviceToHost) == cudaSuccess);); (15, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (12, CUDA, cudaMalloc((void**)(&d_in1), in1_bytes);); (69, CUDA, assert(cudaMemcpyAsync(out.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, context.device().stream()) == cudaSuccess);); (14, CUDA, cudaMemcpy(d_in1, in1.data(), in1_bytes, cudaMemcpyHostToDevice);); (10, CUDA, cudaMalloc((void**)(&d_in2), in2_bytes);); (67, CUDA, assert(cudaStreamSynchronize(context.device().stream()) == cudaSuccess);); (12, CUDA, cudaMemcpy(d_in2, in2.data(), in2_bytes, cudaMemcpyHostToDevice);)"}
{"predict": "(25,OpenMP,#pragma omp parallel for private(i))", "label": "(11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (19, MPI, MPI_Barrier(MPI_COMM_WORLD);); (13, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (4, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(32,OpenMP,#pragma omp parallel for private(counter))", "label": "(6, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for private(t, tmp, x_, y_))", "label": "(15, CUDA, __shared__ thrust::cuda_cub::core::uninitialized_array<T, 1024> tmp;)"}
{"predict": "(34,OpenMP,#pragma omp parallel for),", "label": "(65, CUDA, cudaFree( outGPU );); (63, CUDA, cudaEventDestroy( evStop );); (64, CUDA, cudaFree( inGPU );); (60, CUDA, cudaEventDestroy( evStart );)"}
{"predict": "(19,OpenMP,#pragma omp parallel for private(pt2,sum) reduction(max:maxScore[:FMC2H],maxScor2[:FMC2H],maxIndex[:FMC2H]))", "label": "(3, CUDA, __shared__ float maxScore[FMC2H];); (1, CUDA, __shared__ float siftPoint[128];); (2, CUDA, __shared__ float maxScor2[FMC2H];); (2, CUDA, __shared__ int maxIndex[FMC2H];)"}
{"predict": "30: \t\t\t\t\tlbend = remainder+(nn+1)*nlb_loc;\n131: \t\t\t\t}\n132: \n133: \t\t\t\tfor(lb=lbstart;lb<lbend;++lb){\n134: \n135: \t\t\t\t\tlptr = lsub[idx_i+lb];\n136: \t\t\t\t\tluptr = lusup[lptr];\n137: \n138: \t\t\t\t\tlsub1 = Llu->Lrowind_bc_ptr[lb];\n139: \t\t\t\t\tlusup1 = Llu->Lnzval_bc_ptr[lb];\n140: \n141: \t\t\t\t\tiknsupc = lsub1[1];\n142: \n143: \t\t\t\t\tluptr1 = lusup1[lsub1[idx_v]];\n144: \n145: \t\t\t\t\tnbrow = lsub1[idx_n];\n146: \n147: \t\t\t\t\tnbrow1 = nbrow-iknsupc;\n148: \n149: \t\t\t\t\tif(nbrow1>0){\n150: \n151: \t\t\t\t\t\tidx_lsum = Llu->Lrowind_bc_2_lsum[lb];\n152: \n153: \t\t\t\t\t\t/* Compute index of the first row in the full block. */\n154: \t\t\t\t\t\ti = idx_lsum[0];\n155: \n156: \t\t\t\t\t\t/* Compute index of the first row in the diagonal block. */\n157: \t\t\t\t\t\til = idx_lsum[iknsupc];\n158: \n159: \t\t\t\t\t\t/* Compute index of the first row in the off-diagonal block. */\n160: \t\t\t\t\t\tii = idx_lsum[iknsupc+1];\n161: \n162: \t\t\t\t\t\t/* Compute index of the first row in the last block. */\n163: \t\t\t\t\t\tjj = idx_lsum[nbrow];\n164: \n165: \t\t\t\t\t\t/* Compute index of the first row in the last block of the diagonal block. */\n165: \t\t\t\t\t\tjj = idx_lsum[iknsupc+nbrow1];\n166: \n167: \t\t\t\t\t\t/* Compute index of the first row in the last block of the off-diagonal block. */\n168: \t\t\t\t\t\tjj", "label": "(174, OpenMP, #pragma omp simd); (112, OpenMP, #pragma\tomp\ttaskloop private (lptr1,luptr1,nlb1,thread_id1,lsub1,lusup1,nsupr1,Linv,nn,lbstart,lbend,luptr_tmp1,nbrow,lb,lptr1_tmp,rtemp_loc,nbrow_ref,lptr,nbrow1,ik,rel,lk,iknsupc,il,i,irow,fmod_tmp,ikcol,p,ii,jj,t1,t2,j) untied); (229, OpenMP, #pragma omp simd lastprivate(irow))"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(12, MPI, rc = MPI_Unpublish_name(serv_name, MPI_INFO_NULL, port_name);); (12, MPI, MPI_Error_class(rc, &errclass);); (8, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(18,OpenMP,#pragma omp parallel for), (53,OpenMP,#pragma omp parallel for)", "label": "(11, OpenMP, #pragma omp target enter data map(alloc : a [0:array_size], b [0:array_size])); (18, OpenMP, #pragma omp target teams distribute parallel for); (3, OpenMP, #pragma omp target map(tofrom : warp_size)); (98, OpenMP, #pragma omp target exit data map(release : a [0:array_size], b [0:array_size]))"}
{"predict": "(18,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for)", "label": "(18, CUDA, thrust::device_vector<float> dev_B(Brow*Bcol);); (12, CUDA, thrust::host_vector<float>       A(Arow*Acol);); (14, CUDA, thrust::device_vector<float> dev_C(Crow*Ccol);); (32, CUDA, cudaDeviceSynchronize();); (10, CUDA, thrust::host_vector<float>       B(Brow*Bcol);); (10, CUDA, thrust::host_vector<float>       C(Crow*Ccol);); (11, CUDA, thrust::device_vector<float> dev_A(Arow*Acol);)"}
{"predict": "(30,OpenMP,#pragma omp parallel for), (36,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for)", "label": "(8, MPI, MPI_Comm_rank(comm, &comm_rank);); (9, MPI, MPI_Comm_size(comm, &comm_size);)"}
{"predict": ";\n103: \n104:     /* open the file */\n105:     ERROR(\"MPI_File_open\");\n106: \n107:     /* set the fileview */\n108:     ERROR(\"MPI_File_set_view\");\n109: \n110:     /* write to the file */\n111:     ERROR(\"MPI_File_write_all\");\n112: \n113:     /* close the file */\n114:     ERROR(\"MPI_File_close\");\n115: \n116:     /* free the local buffer */\n117:     free(buf);\n118: \n119:     return 0;\n120: }", "label": "(14, MPI, MPI_Init(&argc, &argv);); (130, MPI, err = MPI_File_iwrite_all(fh, buf, 1, buftype, &req);); (100, MPI, err = MPI_Type_commit(&buftype);); (56, MPI, err = MPI_Type_create_subarray(2, gsize, count, start, MPI_ORDER_C, MPI_BYTE, &subType);); (57, MPI, err = MPI_Type_commit(&subType);); (68, MPI, err = MPI_Type_commit(&filetype);); (69, MPI, err = MPI_Type_free(&subType);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (115, MPI, err = MPI_File_write_all(fh, buf, 1, buftype, &status);); (125, MPI, err = MPI_File_read_all(fh, buf, 1, buftype, &status);); (129, MPI, err = MPI_File_iread_all(fh, buf, 1, buftype, &req);); (121, MPI, MPI_Wait(&req, &status);); (8, MPI, MPI_Finalize();); (106, MPI, err = MPI_File_set_view(fh, 0, MPI_BYTE, filetype, \"native\", MPI_INFO_NULL);); (134, MPI, err = MPI_Type_free(&buftype);); (131, MPI, err = MPI_Type_free(&filetype);); (127, MPI, MPI_File_close(&fh);); (10, MPI, err = MPI_Dims_create(nprocs, 2, psize);); (109, MPI, err = MPI_File_seek(fh, 0, MPI_SEEK_SET);)"}
{"predict": "(7,OpenMP,#pragma omp parallel sections), (7,OpenMP,#pragma omp section)", "label": "(5, OpenMP, #pragma omp sections); (3, OpenMP, #pragma omp parallel); (5, OpenMP, #pragma omp section)"}
{"predict": "(47,OpenMP,#pragma omp target teams distribute parallel for)", "label": "(42, OpenMP, #pragma omp target); (42, OpenMP, #pragma omp target firstprivate(val))"}
{"predict": "(28,OpenMP,#pragma omp parallel for)", "label": "(23, MPI, MPI_Error_class(err, &errclass);); (36, MPI, MPI_Free_mem(ap);); (16, MPI, MPI_Info_set(hints, \"mpich_buf_type\", hintstrs[i]);); (10, MPI, MPI_Info_set(hints, \"mpi_minimum_memory_alignment\", \"4096\");); (8, MPI, MPI_Info_create(&hints);); (20, MPI, err = MPI_Alloc_mem(count, MPI_INFO_NULL, &ap);); (3, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (35, MPI, MPI_Info_free(&hints);); (12, MPI, err = MPI_Alloc_mem(count, hints, &ap);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for)", "label": "(24, CUDA, cuda_safe_call(cudaStreamSynchronize(stream));); (10, CUDA, cuda_safe_call(cudaGetDeviceCount(&ndevs));)"}
{"predict": "(1,OpenMP,#pragma omp target teams distribute parallel for collapse(2))", "label": "(2, CUDA, __shared__ float s[Ny][Nx];)"}
{"predict": "(int i = 0; i < 10; ++i)\n94:     foo();\n95: #pragma omp target\n96: #pragma omp teams\n97:   for (int i = 0; i < 10; ++i)\n98:     foo();\n99: #pragma omp target\n100: #pragma omp teams\n101:   for (int i = 0; i < 10; ++i)\n102:     foo();\n103: #pragma omp target\n104: #pragma omp teams\n105:   for (int i = 0; i < 10; ++i)\n106:     foo();\n107: #pragma omp target\n108: #pragma omp teams\n109:   for (int i = 0; i < 10; ++i)\n110:     foo();\n111: #pragma omp target\n112: #pragma omp teams\n113:   for (int i = 0; i < 10; ++i)\n114:     foo();\n115: #pragma omp target\n116: #pragma omp teams\n117:   for (int i = 0; i < 10; ++i)\n118:     foo();\n119: #pragma omp target\n120: #pragma omp teams\n121:   for (int i = 0; i < 10; ++i)\n122:     foo();\n123: #pragma omp target\n124: #pragma omp teams\n125:   for (int i = 0; i < 10; ++i)\n126:     foo();\n127: #pragma omp target\n128: #pragma omp teams\n129:   for (int i = 0; i < 10; ++i)\n130:     foo();\n131: #pragma omp target\n132: #pragma omp teams\n133:   for (int i = 0; i < 10; ++i)\n134:     foo();\n135: #pragma omp target\n136: #pragma omp teams\n137:   for (", "label": "(109, OpenMP, #pragma omp distribute parallel for simd reduction(^ : fl)); (118, OpenMP, #pragma omp distribute parallel for simd reduction(&& : S2::S2sc)); (132, OpenMP, #pragma omp distribute parallel for simd reduction(+ : z, o)); (21, OpenMP, #pragma omp distribute parallel for simd reduction(); (45, OpenMP, #pragma omp distribute parallel for simd reduction(foo : argc); (140, OpenMP, #pragma omp distribute parallel for simd reduction(+ : p), reduction(+ : p)); (6, OpenMP, #pragma omp target); (87, OpenMP, #pragma omp distribute parallel for simd reduction(+ : ba)); (131, OpenMP, #pragma omp distribute parallel for simd private(i), reduction(+ : j), reduction(+ : q)); (146, OpenMP, #pragma omp distribute parallel for simd reduction(+ : r)); (161, OpenMP, #pragma omp parallel reduction(* : fl)); (151, OpenMP, #pragma omp distribute parallel for simd reduction(max : j)); (112, OpenMP, #pragma omp distribute parallel for simd reduction(& : e, g)); (172, OpenMP, #pragma omp distribute parallel for simd reduction(task, + : m)); (166, OpenMP, #pragma omp distribute parallel for simd reduction(+ : m)); (127, OpenMP, #pragma omp parallel private(k)); (63, OpenMP, #pragma omp distribute parallel for simd reduction(+ : a, b, c, d, f)); (67, OpenMP, #pragma omp distribute parallel for simd reduction(min : a, b, c, d, f)); (71, OpenMP, #pragma omp distribute parallel for simd reduction(max : h.b)); (55, OpenMP, #pragma omp distribute parallel for simd reduction(^ : S1)); (109, OpenMP, #pragma omp distribute parallel for simd reduction(+ : h, k, B::x)); (48, OpenMP, #pragma omp distribute parallel for simd reduction(&& : argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (17, OpenMP, #pragma omp distribute parallel for simd reduction(*)); (136, OpenMP, #pragma omp parallel reduction(min : i)); (20, OpenMP, #pragma omp distribute parallel for simd reduction(\\)); (140, OpenMP, #pragma omp parallel private(fl)); (8, OpenMP, #pragma omp distribute parallel for simd reduction()); (72, OpenMP, #pragma omp distribute parallel for simd reduction(* : ca)); (140, OpenMP, #pragma omp distribute parallel for simd reduction(+ : fl)); (30, OpenMP, #pragma omp distribute parallel for simd reduction(|| : argc > 0 ? argv[1] : argv[2])); (-1, OpenMP, #pragma omp distribute parallel for simd reduction(-); (73, OpenMP, #pragma omp distribute parallel for simd reduction(- : da)); (-13, OpenMP, #pragma omp distribute parallel for simd reduction +); (-19, OpenMP, #pragma omp distribute parallel for simd reduction); (20, OpenMP, #pragma omp distribute parallel for simd reduction(| : argc,); (123, OpenMP, #pragma omp parallel shared(i)); (78, OpenMP, #pragma omp distribute parallel for simd reduction(&& : S2::S2s)); (27, OpenMP, #pragma omp distribute parallel for simd reduction(~ : argc)); (-25, OpenMP, #pragma omp teams)"}
{"predict": "(4,OpenMP,#pragma omp parallel sections)", "label": "(5, OpenCL, clReleaseMemObject(bufX);); (8, OpenCL, clReleaseMemObject(bufY);); (11, OpenCL, clReleaseMemObject(bufParam);)"}
{"predict": "(8,OpenMP,#pragma omp parallel)", "label": "(9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (15, MPI, MPI_Cart_map(MPI_COMM_WORLD, 1, dims, periods, &newrank);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (29, MPI, MPI_Cart_map(MPI_COMM_WORLD, 0, dims, periods, &newrank);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(26, OpenCL, status = clReleaseEvent(event);); (13, OpenCL, auto status = clWaitForEvents(1, &event);)"}
{"predict": "(20,OpenMP,#pragma omp parallel for reduction(+:combine_sum,combine_ref_sum,combine_finit_sum,combine_each_sum,copy_construct_sum,copy_assign_sum) private(my_finit_decl,my_helper,assigned,moved1,moved2))", "label": "(41, TBB, tbb::combinable<T> copy_constructed(sums);); (31, TBB, tbb::combinable<T> finit_combinable(my_finit_decl);); (31, TBB, tbb::parallel_for( tbb::blocked_range<int>( 0, N, 10000 ), ParallelScalarBodyNoInit<T>( finit_combinable ) );); (19, TBB, if (Verbose && t == 0) t0 = tbb::tick_count::now();); (22, TBB, tbb::parallel_for( tbb::blocked_range<int>( 0, N, 10000 ), ParallelScalarBody<T>( sums ) );); (40, TBB, tbb::combinable<T> assigned;); (-3, TBB, tbb::task_scheduler_init init(tbb::task_scheduler_init::deferred);); (44, TBB, tbb::combinable<T> moved1(std::move(sums));); (2, TBB, tbb::tick_count t0;); (16, TBB, tbb::combinable<T> sums;); (45, TBB, tbb::combinable<T> moved2;)"}
{"predict": "(101,OpenMP,#pragma omp parallel for private(expectedPtr, outPtr) schedule(static))", "label": "(119, OpenCL, clReleaseMemObject( streams[2] );); (80, OpenCL, error = clSetKernelArg(kernel, 0, sizeof( streams[0] ), &streams[0] );); (119, OpenCL, clReleaseProgram( program );); (82, OpenCL, error = clSetKernelArg(kernel, 2, sizeof( streams[2] ), &streams[2] );); (114, OpenCL, clReleaseMemObject( streams[1] );); (115, OpenCL, clReleaseKernel( kernel );); (77, OpenCL, error = clSetKernelArg(kernel, 1, sizeof( streams[1] ), &streams[1] );); (92, OpenCL, error = clEnqueueReadBuffer( queue, streams[2], CL_TRUE, 0, outStride * count, outData, 0, NULL, NULL );); (86, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 1, NULL, &threadSize, &groupSize, 0, NULL, NULL );); (108, OpenCL, clReleaseMemObject( streams[0] );)"}
{"predict": "(15,OpenMP,#pragma omp parallel sections), (16,OpenMP,#pragma omp section), (23,OpenMP,#pragma omp section)", "label": "(25, MPI, MPI_Wait(&req, &status);); (12, MPI, MPI_Bcast_init(&a, 1, MPI_INT, 0, MPI_COMM_WORLD, MPI_INFO_NULL, &req);); (15, MPI, MPI_Test(&req, &flag, &status);); (25, MPI, MPI_Request_free(&req);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for private(master, i, lens, pin, pout) reduction(|:any_cracked))", "label": "(10, OpenMP, #pragma omp parallel for); (32, OpenMP, #pragma omp atomic)"}
{"predict": "(19,OpenMP,#pragma omp parallel for)", "label": "(25, OpenCL, clReleaseEvent(retEvent2);); (25, OpenCL, clReleaseEvent(retEvent);)"}
{"predict": "(8,OpenMP,#pragma omp parallel if (rank < state.P)), (14,OpenMP,#pragma omp single nowait)", "label": "(3, MPI, MPI_Barrier(MPI_COMM_WORLD);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (16, MPI, MPI_Comm_free(&comm);)"}
{"predict": "92:     for (i = 0; i < size; ++i) {\n93:         recvcounts[i] = COUNT;\n94:         for (j = 0; j < COUNT; ++j) {\n95:             buf_h[i * COUNT + j] = rank + i;\n96:             recvbuf_h[i * COUNT + j] = 0xdeadbeef;\n97:         }\n98:     }\n99:     MTestCopyContent(buf_h, buf, size * COUNT * sizeof(int), memtype);\n100:    MTestCopyContent(recvbuf_h, recvbuf, COUNT * sizeof(int), memtype);\n101:    MPI_Wait(&req, MPI_STATUS_IGNORE);\n102:    MTestCopyContent(recvbuf, recvbuf_h, COUNT * sizeof(int), memtype);\n103:    for (j = 0; j < COUNT; ++j) {\n104:        my_assert(recvbuf_h[j] == (size * rank + ((size - 1) * size) / 2));\n105:    }\n106:    for (i = 1; i < size; ++i) {\n107:        for (j = 0; j < COUNT; ++j) {\n108:            /* check we didn't corrupt the rest of the recvbuf_h */\n109:            my_assert(recvbuf_h[i * COUNT + j] == 0xdeadbeef);\n110:        }\n111:    }\n112: \n113:    /* MPI_Reduce_scatter_block */\n114:    for (i = 0; i < size; ++i) {\n115:        for (j = 0; j < COUNT; ++j) {\n116:            buf_h[i * COUNT + j] = rank + i;\n117:            recvbuf_h[i * COUNT + j] = 0xdeadbeef;\n118:        }\n119:    }\n120:    MTestCopyContent(buf_h, buf, size * COUNT * sizeof(int), memtype);\n121:    MTestCopyContent(recvbuf_h, recvbuf, COUNT * sizeof(int), memtype);\n122:    MTestCopyContent(recvbuf, recvbuf_h, COUNT * sizeof", "label": "(169, MPI, MPI_Scan(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (28, MPI, MPI_Ireduce(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD, &req);); (82, MPI, MPI_Reduce_scatter(buf, recvbuf, recvcounts, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (45, MPI, MPI_Allreduce(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (124, MPI, MPI_Reduce_scatter_block(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (59, MPI, MPI_Iallreduce(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD, &req);); (100, MPI, MPI_Ireduce_scatter(buf, recvbuf, recvcounts, MPI_INT, MPI_SUM, MPI_COMM_WORLD, &req);); (23, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (3, MPI, MPI_Reduce(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD);); (187, MPI, MPI_Exscan(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (139, MPI, MPI_Ireduce_scatter_block(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD, &req);); (171, MPI, MPI_Iscan(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD, &req);); (198, MPI, MPI_Iexscan(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD, &req);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for private(status))", "label": "(9, MPI, t = MPI_Wtime();); (11, MPI, t = MPI_Wtime() - t;)"}
{"predict": "(3,OpenMP,#pragma omp parallel for), (5,OpenMP,#pragma omp parallel for), (7,OpenMP,#pragma omp parallel for), (9,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for)", "label": "(14, OpenMP, #pragma omp parallel for proc_bind(close), proc_bind(spread)); (10, OpenMP, #pragma omp parallel for proc_bind(master); (19, OpenMP, #pragma omp parallel for proc_bind(master)); (5, OpenMP, #pragma omp parallel for proc_bind()); (-2, OpenMP, #pragma omp parallel for proc_bind); (12, OpenMP, #pragma omp parallel for proc_bind(x)); (19, OpenMP, #pragma omp parallel proc_bind(close)); (19, OpenMP, #pragma omp parallel for proc_bind(spread)); (-3, OpenMP, #pragma omp parallel for proc_bind()"}
{"predict": "(14,OpenMP,#pragma omp parallel for)", "label": "(6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &comm_size);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (7, MPI, vtkh::SetMPICommHandle(MPI_Comm_c2f(MPI_COMM_WORLD));)"}
{"predict": "(20,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for private(is_1_mod2_0,is_1_mod2_1) reduction(+:is_1_mod2_0,is_1_mod2_1))", "label": "(16, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(4,OpenMP,#pragma omp parallel for), (10,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for), (36,OpenMP,#pragma omp parallel for), (41,OpenMP,#pragma omp parallel for), (46,OpenMP,#pragma omp parallel for), (60,OpenMP,#pragma omp parallel for), (64,OpenMP,#pragma omp parallel for), (69,OpenMP,#pragma omp parallel for), (74,OpenMP,#pragma omp parallel for), (78,OpenMP,#pragma omp parallel for), (83,OpenMP,#pragma omp parallel for), (88,OpenMP,#pragma omp parallel for), (95,OpenMP,#pragma omp parallel for), (101,OpenMP,#pragma omp parallel for)", "label": "(19, OpenMP, #pragma omp distribute simd aligned(, )); (75, OpenMP, #pragma omp distribute simd aligned(x :)); (80, OpenMP, #pragma omp distribute simd aligned(x :, )); (9, OpenMP, #pragma omp distribute simd aligned(,); (-2, OpenMP, #pragma omp target); (82, OpenMP, #pragma omp distribute simd aligned(x : 1)); (52, OpenMP, #pragma omp distribute simd aligned(x, y, z)); (24, OpenMP, #pragma omp distribute simd aligned(int)); (62, OpenMP, #pragma omp distribute simd aligned(z)); (-4, OpenMP, #pragma omp distribute simd aligned(); (40, OpenMP, #pragma omp distribute simd aligned(x, y)); (32, OpenMP, #pragma omp distribute simd aligned(x)); (80, OpenMP, #pragma omp distribute simd aligned(x : 2 * 2)); (12, OpenMP, #pragma omp distribute simd aligned()); (23, OpenMP, #pragma omp distribute simd aligned(0)); (83, OpenMP, #pragma omp distribute simd aligned(x : 1, y)); (88, OpenMP, #pragma omp distribute simd aligned(x : 1, y, z : 1)); (108, OpenMP, #pragma omp distribute simd aligned(x) aligned(z, x)); (116, OpenMP, #pragma omp distribute simd aligned(x, y, z) aligned(y, z)); (-16, OpenMP, #pragma omp teams)"}
{"predict": "(7,OpenMP,#pragma omp parallel for collapse(2)), (14,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for collapse(2))", "label": "(14, OpenMP, #pragma omp parallel for collapse(2)); (12, OpenMP, #pragma omp target map(tofrom: data))"}
{"predict": "(48,OpenMP,#pragma omp parallel sections), (49,OpenMP,#pragma omp section), (50,OpenMP,#pragma omp section), (58,OpenMP,#pragma omp parallel sections), (59,OpenMP,#pragma omp section)", "label": "(78, CUDA, CUDA_CHECK(cudaFree(d_C));); (63, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (46, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_B), sizeof(data_type) * B.size()));); (74, CUDA, CUDA_CHECK(cudaFree(d_B));); (45, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_C), sizeof(data_type) * C.size()));); (79, CUDA, CUDA_CHECK(cudaDeviceReset());); (76, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (40, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (68, CUDA, CUDA_CHECK(cudaFree(d_A));); (34, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for)", "label": "(7, OpenCL, err = clGetDeviceInfo(device, CL_DEVICE_IL_VERSION, 0, NULL, &str_size);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for reduction(min:minval))", "label": "(10, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:minval) reduction(min:minval))"}
{"predict": "Clean up */\n128:         MPI_Comm_free(&intercomm);\n129:         MPI_Comm_free(&intracomm);\n130:         MPI_Comm_free(&intracomm2);\n131:         MPI_Comm_free(&intracomm3);\n132: \n133:         /* report overall error count */\n134:         MPI_Allreduce(&errs, &err, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n135:         if (rank == 0) {\n136:             printf(\"Found %d errors\\n\", err);\n137:         }\n138: \n139:     } else {\n140:         printf(\" spawning is not supported\\n\");\n141:     }\n142: \n143:     MTest_Finalize(errs);\n144: \n145:     return MTestReturnValue(errs);\n146: }", "label": "(65, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (56, MPI, MPI_Intercomm_merge(intercomm, isChild, &intracomm);); (161, MPI, MPI_Comm_free(&intracomm3);); (95, MPI, MPI_Comm_rank(intracomm2, &icrank);); (12, MPI, MPI_Comm_get_parent(&parentcomm);); (34, MPI, MPI_Send(&i, 1, MPI_INT, i, 0, intercomm);); (141, MPI, MPI_Ssend(&errs, 1, MPI_INT, 0, 1, intercomm);); (56, MPI, MPI_Comm_size(intracomm, &icsize);); (146, MPI, MPI_Recv(&err, 1, MPI_INT, i, 1, intercomm, MPI_STATUS_IGNORE);); (117, MPI, MPI_Intercomm_merge(intercomm, 0, &intracomm3);); (65, MPI, MPI_Comm_remote_size(parentcomm, &psize);); (80, MPI, MPI_Intercomm_merge(intercomm, !isChild, &intracomm2);); (149, MPI, MPI_Comm_free(&intracomm);); (102, MPI, MPI_Comm_remote_size(intercomm, &csize);); (83, MPI, MPI_Comm_size(intracomm2, &icsize);); (49, MPI, MPI_Comm_rank(intracomm, &icrank);); (150, MPI, MPI_Comm_free(&intercomm);); (32, MPI, MPI_Recv(&i, 1, MPI_INT, 0, 0, intercomm, &status);); (114, MPI, MPI_Comm_rank(intracomm3, &icrank);); (10, MPI, MPI_Comm_rank(intercomm, &rank);); (7, MPI, MPI_Comm_remote_size(intercomm, &rsize);); (7, MPI, MPI_Comm_size(intercomm, &size);); (109, MPI, MPI_Comm_size(intracomm3, &icsize);); (139, MPI, MPI_Comm_free(&intracomm2);); (150, MPI, MPI_Finalize();)"}
{"predict": "_BASE_ZERO);\n85:   // CHECK: hipsparseSetMatType(descrC, HIPSPARSE_MATRIX_TYPE_GENERAL);\n86:   cusparseSetMatType(descrC, CUSPARSE_MATRIX_TYPE_GENERAL);\n87: \n88:   /* step 3: allocate host memory for matrix C */\n89:   csrRowPtrC = (int*)malloc((m + 1) * sizeof(int));\n90:   csrColIndC = (int*)malloc(nnzA * sizeof(int));\n91:   csrValC = (float*)malloc(nnzA * sizeof(float));\n92: \n93:   /* step 4: allocate device memory for matrix A and C */\n94:   cudaStat1 = cudaMalloc((void**)&d_csrRowPtrA, (m + 1) * sizeof(int));\n95:   cudaStat1 = cudaMalloc((void**)&d_csrColIndA, nnzA * sizeof(int));\n96:   cudaStat1 = cudaMalloc((void**)&d_csrValA, nnzA * sizeof(float));\n97:   cudaStat1 = cudaMalloc((void**)&d_csrRowPtrC, (m + 1) * sizeof(int));\n98:   cudaStat1 = cudaMalloc((void**)&d_csrColIndC, nnzA * sizeof(int));\n99:   cudaStat1 = cudaMalloc((void**)&d_csrValC, nnzA * sizeof(float));\n100: \n101:   /* step 5: copy matrix A from host memory to device memory */\n102:   cudaStat1 = cudaMemcpy(d_csrRowPtrA, csrRowPtrA, (m + 1) * sizeof(int), cudaMemcpyHostToDevice);\n103:   cudaStat1 = cudaMemcpy(d_csrColIndA, csrColIndA, nnzA * sizeof(int), cudaMemcpyHostToDevice);\n104:   cudaStat1 = cudaMemcpy(d_csrValA, csrValA, nnzA * sizeof(float), cudaMemcpyHostToDevice);\n105: \n106:   /* step 6: compute required memory size for pruneCsr2csr operation */\n107:   // CHECK: status = hipsparsePruneCsr2csr_bufferSizeExt(handle, m, n, nnzA, descrA, d_csrValA, d_csrRow", "label": "(106, CUDA, cudaStat1 = cudaMemcpy(d_csrRowPtrA, csrRowPtrA, sizeof(int)*(m + 1), cudaMemcpyHostToDevice);); (234, CUDA, if (d_csrColIndC) cudaFree(d_csrColIndC);); (211, CUDA, cudaStat1 = cudaMemcpy(csrRowPtrC, d_csrRowPtrC, sizeof(int)*(m + 1), cudaMemcpyDeviceToHost);); (99, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrRowPtrC, sizeof(int)*(m + 1));); (94, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrValA, sizeof(float)*nnzA);); (109, CUDA, cudaStat1 = cudaMemcpy(d_csrValA, csrValA, sizeof(float)*nnzA, cudaMemcpyHostToDevice);); (221, CUDA, if (d_csrRowPtrA) cudaFree(d_csrRowPtrA);); (237, CUDA, if (stream) cudaStreamDestroy(stream);); (225, CUDA, if (d_csrRowPtrC) cudaFree(d_csrRowPtrC);); (170, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrValC, sizeof(float) * nnzC);); (227, CUDA, if (d_csrValC) cudaFree(d_csrValC);); (154, CUDA, cudaStat1 = cudaDeviceSynchronize();); (78, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrRowPtrA, sizeof(int)*(m + 1));); (204, CUDA, cudaStat1 = cudaMemcpy(csrColIndC, d_csrColIndC, sizeof(int)*nnzC, cudaMemcpyDeviceToHost);); (80, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrColIndA, sizeof(int)*nnzA);); (41, CUDA, cudaStat1 = cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking);); (94, CUDA, cudaStat1 = cudaMemcpy(d_csrColIndA, csrColIndA, sizeof(int)*nnzA, cudaMemcpyHostToDevice);); (125, CUDA, cudaStat1 = cudaMalloc((void**)&d_work, lworkInBytes);); (211, CUDA, if (d_csrColIndA) cudaFree(d_csrColIndA);); (202, CUDA, cudaStat1 = cudaMemcpy(csrValC, d_csrValC, sizeof(float)*nnzC, cudaMemcpyDeviceToHost);); (155, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrColIndC, sizeof(int) * nnzC);); (210, CUDA, if (d_csrValA) cudaFree(d_csrValA);); (228, CUDA, cudaDeviceReset();)"}
{"predict": "(38,OpenMP,#pragma omp parallel sections), (40,OpenMP,#pragma omp section), (41,OpenMP,#pragma omp section), (44,OpenMP,#pragma omp section), (45,OpenMP,#pragma omp section), (47,OpenMP,#pragma omp section), (51,OpenMP,#pragma omp section), (52,OpenMP,#pragma omp section), (55,OpenMP,#pragma omp section), (56,OpenMP,#pragma omp section), (58,OpenMP,#pragma omp section), (60,OpenMP,#pragma omp section), (62,OpenMP,#pragma omp section), (64,OpenMP,#pragma omp section), (66,OpenMP,#pragma omp section), (69,OpenMP,#pragma omp section), (70,OpenMP,#pragma omp section), (72,OpenMP,#pragma omp section), (74,OpenMP,#pragma omp section), (75,OpenMP,#pragma omp section), (77,OpenMP,#pragma omp section), (78,OpenMP,#pragma omp section), (79,OpenMP,#pragma omp section), (88,OpenMP,#pragma omp section), (95,OpenMP,#pragma omp section), (96,OpenMP,#pragma omp section), (97,OpenMP,#pragma omp section), (98,OpenMP,#pragma omp section), (100,OpenMP,#pragma omp section), (101,OpenMP,#pragma omp section), (102,OpenMP,#pragma omp section), (103,OpenMP,#pragma omp section), (110,OpenMP,#pragma omp section), (111,OpenMP,#pragma omp section), (114,OpenMP,#pragma omp section)", "label": "(81, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (81, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)"}
{"predict": "(22,OpenMP,#pragma omp parallel sections)", "label": "(26, OpenCL, clReleaseContext(knode->key.context);); (24, OpenCL, clReleaseProgram(kern->program);)"}
{"predict": "= MPI_Wtime();\n138:         while (repsleft--) {\n139:             MPI_Send(sbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD);\n130:             MPI_Recv(rbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n140:         }\n141:         t1 = MPI_Wtime() - t1;\n142:         times[2][k] = t1 / reps;\n143:         if (wrank == 0) {\n144:             t1 = t1 / reps;\n145:             if (t1 > 0) {\n146:                 t1 = t1 * 1.e6;\n147:                 if (verbose)\n148:                     printf(\"%d\\t%g\\t%g\\n\", len, t1, len / t1);\n149:             } else {\n150:                 t1 = t1 * 1.e6;\n151:                 if (verbose)\n152:                     printf(\"%d\\t%g\\tINF\\n\", len, t1);\n153:             }\n154:             if (verbose)\n155:                 fflush(stdout);\n156:         }\n157: \n158:         len *= 2;\n159:     }\n160: \n161:     MPI_Barrier(MPI_COMM_WORLD);\n162: \n163:     if (wrank == 0) {\n164:         for (k = 0; k < 20; k++) {\n165:             printf(\"%d\\t%g\\t%g\\t%g\\n\", len, times[0][k], times[1][k], times[2][k]);\n166:         }\n167:     }\n168: \n169:     MTest_Finalize(0);\n170:     return 0;\n171: }", "label": "(13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (60, MPI, MPI_Irecv(rbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD, &rreq);); (57, MPI, t1 = MPI_Wtime();); (60, MPI, MPI_Wait(&rreq, MPI_STATUS_IGNORE);); (146, MPI, MPI_Recv(rbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (57, MPI, MPI_Send(sbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (34, MPI, MPI_Barrier(MPI_COMM_WORLD);); (57, MPI, t1 = MPI_Wtime() - t1;); (8, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (24, MPI, MPI_Abort(MPI_COMM_WORLD, 2);)"}
{"predict": "(8,OpenMP,#pragma omp parallel sections)", "label": "(1, TBB, using namespace tbb::flow;)"}
{"predict": "(2,OpenMP,#pragma omp parallel for)", "label": "(7, OpenMP, #pragma omp teams distribute parallel for simd safelen (); (21, OpenMP, #pragma omp teams distribute parallel for simd safelen (2+2))); (10, OpenMP, #pragma omp teams distribute parallel for simd safelen ()); (54, OpenMP, #pragma omp teams distribute parallel for simd safelen(safelen(tmain<int, char, -1, -2>(argc, argv)); (37, OpenMP, #pragma omp teams distribute parallel for simd safelen (S1)); (12, OpenMP, #pragma omp teams distribute parallel for simd safelen (4); (45, OpenMP, #pragma omp teams distribute parallel for simd safelen (argv[1]=2)); (21, OpenMP, #pragma omp teams distribute parallel for simd safelen (foobool(1) > 0 ? 1 : 2)); (28, OpenMP, #pragma omp teams distribute parallel for simd safelen (foobool(argc)), safelen (true), safelen (-5)); (-8, OpenMP, #pragma omp target); (-8, OpenMP, #pragma omp teams distribute parallel for simd safelen)"}
{"predict": "(48,OpenMP,#pragma omp parallel)", "label": "(24, MPI, MPI_Finalize();); (12, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(18,OpenMP,#pragma omp parallel for) (22,OpenMP,#pragma omp parallel for) (28,OpenMP,#pragma omp parallel for) (48,OpenMP,#pragma omp parallel for) (52,OpenMP,#pragma omp parallel for) (55,OpenMP,#pragma omp parallel for)", "label": "(20, TBB, tbb::flow::make_edge( n, r[i] );); (54, TBB, tbb::flow::remove_edge( n, r[i] );); (1, TBB, tbb::flow::write_once_node<R> n(g);); (-1, TBB, tbb::flow::graph g;)"}
{"predict": "(3,OpenMP,#pragma omp parallel for), (6,OpenMP,#pragma omp parallel for), (9,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for), (35,OpenMP,#pragma omp parallel for), (38,OpenMP,#pragma omp parallel for), (42,OpenMP,#pragma omp parallel for)", "label": "(16, OpenMP, #pragma omp distribute parallel for schedule (auto); (45, OpenMP, #pragma omp distribute parallel for schedule (guided, foobool(argc)), schedule (static, true), schedule (dynamic, -5)); (48, OpenMP, #pragma omp distribute parallel for schedule (guided, S1)); (52, OpenMP, #pragma omp distribute parallel for schedule (static, argv[1]=2)); (0, OpenMP, #pragma omp distribute parallel for schedule); (31, OpenMP, #pragma omp distribute parallel for schedule (static, 2+2))); (2, OpenMP, #pragma omp distribute parallel for schedule (); (5, OpenMP, #pragma omp distribute parallel for schedule ()); (32, OpenMP, #pragma omp distribute parallel for schedule (dynamic, foobool(1) > 0 ? 1 : 2 - z)); (23, OpenMP, #pragma omp distribute parallel for schedule (guided, 4); (51, OpenMP, #pragma omp distribute parallel for schedule(dynamic, schedule(tmain<int, char, -1, -2>(argc, argv)); (17, OpenMP, #pragma omp distribute parallel for schedule (runtime, 3)); (-10, OpenMP, #pragma omp target); (11, OpenMP, #pragma omp distribute parallel for schedule (auto,); (6, OpenMP, #pragma omp distribute parallel for schedule (auto_dynamic); (-12, OpenMP, #pragma omp teams)"}
{"predict": "(12,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for)", "label": "(18, CUDA, cuda_safe_call(cudaGetLastError());)"}
{"predict": "(22,OpenMP,#pragma omp parallel for collapse(2)), (21,OpenMP,#include <omp.h>)", "label": "(21, CUDA, cudaMalloc((void**)&deviceDiagonalResult, sizeof(bool));); (55, CUDA, cudaMemcpy(hostDiagonalResult, deviceDiagonalResult, sizeof(bool), cudaMemcpyDeviceToHost);); (99, CUDA, cudaFree(deviceSymmetricResult);); (20, CUDA, cudaMalloc((void**)&deviceSkewSymmetricResult, sizeof(bool));); (54, CUDA, cudaMemcpy(hostSkewSymmetricResult, deviceSkewSymmetricResult, sizeof(bool), cudaMemcpyDeviceToHost);); (17, CUDA, cudaMalloc((void**)&deviceSymmetricResult, sizeof(bool));); (33, CUDA, cudaMemcpy(deviceMatrix, hostMatrix, width * height * sizeof(float), cudaMemcpyHostToDevice);); (37, CUDA, cudaMemcpy(deviceIdempotentResult, hostIdempotentResult, sizeof(bool), cudaMemcpyHostToDevice);); (11, CUDA, cudaMalloc((void**)&deviceMatrix, width * height * sizeof(float));); (31, CUDA, cudaMemcpy(deviceToeplitzResult, hostToeplitzResult, sizeof(bool), cudaMemcpyHostToDevice);); (49, CUDA, cudaMemcpy(hostIdempotentResult, deviceIdempotentResult, sizeof(bool), cudaMemcpyDeviceToHost);); (91, CUDA, cudaFree(deviceSkewSymmetricResult);); (31, CUDA, cudaMemcpy(deviceSkewSymmetricResult, hostSkewSymmetricResult, sizeof(bool), cudaMemcpyHostToDevice);); (44, CUDA, cudaMemcpy(hostSymmetricResult, deviceSymmetricResult, sizeof(bool), cudaMemcpyDeviceToHost);); (28, CUDA, cudaMemcpy(deviceSymmetricResult, hostSymmetricResult, sizeof(bool), cudaMemcpyHostToDevice);); (85, CUDA, cudaFree(deviceDiagonalResult);); (4, CUDA, cudaMalloc((void**)&deviceToeplitzResult, sizeof(bool));); (82, CUDA, cudaFree(deviceToeplitzResult);); (85, CUDA, cudaFree(deviceIdempotentResult);); (36, CUDA, cudaMemcpy(hostToeplitzResult, deviceToeplitzResult, sizeof(bool), cudaMemcpyDeviceToHost);); (78, CUDA, cudaFree(deviceMatrix);); (20, CUDA, cudaMemcpy(deviceDiagonalResult, hostDiagonalResult, sizeof(bool), cudaMemcpyHostToDevice);); (2, CUDA, cudaMalloc((void**)&deviceIdempotentResult, sizeof(bool));)"}
{"predict": "1:             clReleaseMemObject( streams[ii] );\n122:             clReleaseMemObject( streams[ii+1] );\n123:             free( outptr[i] );\n124:             return -1;\n125:         }\n126: \n127:         err = clSetKernelArg( kernel[i], 0, sizeof( cl_mem ), &streams[ii] );\n128:         if( err ){\n129:             clReleaseEvent(writeEvent);\n130:             clReleaseMemObject( streams[ii] );\n131:             clReleaseMemObject( streams[ii+1] );\n132:             free( outptr[i] );\n133:             return -1;\n134:         }\n135: \n136:         err = clSetKernelArg( kernel[i], 1, sizeof( cl_mem ), &streams[ii+1] );\n137:         if( err ){\n138:             clReleaseEvent(writeEvent);\n139:             clReleaseMemObject( streams[ii] );\n140:             clReleaseMemObject( streams[ii+1] );\n141:             free( outptr[i] );\n142:             return -1;\n143:         }\n144: \n145:         err = clEnqueueNDRangeKernel( queue, kernel[i], 1, NULL, threads, NULL, 0, NULL, &writeEvent );\n146:         if( err ){\n147:             clReleaseEvent(writeEvent);\n148:             clReleaseMemObject( streams[ii] );\n149:             clReleaseMemObject( streams[ii+1] );\n150:             free( outptr[i] );\n151:             return -1;\n152:         }\n153: \n154:         err = fn( outptr[i], streams[ii+1], num_elements );\n155:         if( err ){\n156:             clReleaseEvent(writeEvent);\n157:             clReleaseMemObject( streams[ii] );\n158: clReleaseMemObject( streams[ii+1] );\n159:             free( outptr[i] );\n156:             return -1;\n160:         }\n161:     }\n162: \n163", "label": "(57, OpenCL, err = clEnqueueWriteBuffer( queue, streams[ii], false, 0, ptrSizes[i]*num_elements, inptr[i], 0, NULL, &writeEvent );); (132, OpenCL, err = clSetKernelArg( kernel[i], 0, sizeof( cl_mem ), (void *)&streams[ii] );); (49, OpenCL, clReleaseMemObject(streams[ii]);); (135, OpenCL, clReleaseProgram( program[i] );); (123, OpenCL, clReleaseMemObject(streams[ii+1]);); (55, OpenCL, clReleaseMemObject( streams[ii+1] );); (140, OpenCL, err = clEnqueueNDRangeKernel( queue, kernel[i], 1, NULL, threads, NULL, 0, NULL, NULL );); (52, OpenCL, clReleaseMemObject( streams[ii] );); (64, OpenCL, clReleaseEvent(writeEvent);); (59, OpenCL, err = clWaitForEvents( 1, &writeEvent );); (127, OpenCL, clReleaseKernel( kernel[i] );); (123, OpenCL, err |= clSetKernelArg( kernel[i], 1, sizeof( cl_mem ), (void *)&streams[ii+1] );); (148, OpenCL, err = clEnqueueReadBuffer( queue, streams[ii+1], true, 0, outPtrSizes[i]*num_elements, outptr[i], 0, NULL, NULL );)"}
{"predict": "(13,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for), (35,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for), (39,OpenMP,#pragma omp parallel for)", "label": "(15, OpenMP, #pragma omp target teams distribute parallel for simd linear (); (11, OpenMP, #pragma omp target teams distribute parallel for simd linear); (34, OpenMP, #pragma omp target teams distribute parallel for simd linear (S1)); (37, OpenMP, #pragma omp target teams distribute parallel for simd linear (a, b)); (39, OpenMP, #pragma omp target teams distribute parallel for simd linear (argv[1])); (44, OpenMP, #pragma omp target teams distribute parallel for simd linear(h, C::x)); (23, OpenMP, #pragma omp target teams distribute parallel for simd linear (argc > 0 ? argv[1] : argv[2])); (15, OpenMP, #pragma omp target teams distribute parallel for simd linear (argc); (10, OpenMP, #pragma omp target teams distribute parallel for simd linear ()); (24, OpenMP, #pragma omp target teams distribute parallel for simd linear (argc)); (36, OpenMP, #pragma omp target teams distribute parallel for simd linear(e, g)); (15, OpenMP, #pragma omp target teams distribute parallel for simd linear (argc,)"}
{"predict": "(49,OpenMP,#pragma omp parallel for)", "label": "(56, OpenCL, err = clEnqueueWriteBuffer(queue, bufY, CL_TRUE, 0, (lenY*sizeof(cl_float)), Y, 0, NULL, NULL);); (82, OpenCL, clReleaseMemObject(bufX);); (44, OpenCL, clReleaseCommandQueue(queue);); (28, OpenCL, printf( \"clCreateContext() failed with %d\\n\", err );); (75, OpenCL, clReleaseEvent(event);); (32, OpenCL, printf( \"clCreateCommandQueue() failed with %d\\n\", err );); (29, OpenCL, queue = clCreateCommandQueue(ctx, device, 0, &err);); (46, OpenCL, bufY = clCreateBuffer(ctx, CL_MEM_READ_WRITE, (lenY*sizeof(cl_float)), NULL, &err);); (30, OpenCL, clReleaseContext(ctx);); (73, OpenCL, clReleaseMemObject(bufY);); (42, OpenCL, bufX = clCreateBuffer(ctx, CL_MEM_READ_ONLY, (lenX*sizeof(cl_float)), NULL, &err);); (44, OpenCL, err = clEnqueueWriteBuffer(queue, bufX, CL_TRUE, 0, (lenX*sizeof(cl_float)), X, 0, NULL, NULL);); (17, OpenCL, ctx = clCreateContext(props, 1, &device, NULL, NULL, &err);); (53, OpenCL, err = clWaitForEvents(1, &event);)"}
{"predict": "(47,OpenMP,#pragma omp parallel for), (47,OpenMP,for(size_t i = 0; i < num_elements; ++i) { scaling_kernel<<<1, 1, 0, stream>>>(begin_d + i, begin_d + i + 1, rank, size, nx, ny, nz); })", "label": "(62, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (6, CUDA, CUDA_CHECK(cudaStreamCreate(&stream));); (46, CUDA, CUDA_CHECK(cudaGetLastError());); (52, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));)"}
{"predict": "(12,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for)", "label": "(7, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)"}
{"predict": "(12,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for), (63,OpenMP,#pragma omp parallel for)", "label": "(11, OpenMP, #pragma omp parallel for)"}
{"predict": "The provided code snippet is a function that releases memory objects in a context that seems to involve OpenCL (based on the `cl_mem` type). The function consists of three conditional blocks that check if each memory object is not `NULL` and then presumably release it. Given the nature of the task (memory deallocation), there is no inherent parallelism that can be exploited as each memory release operation is independent but typically quick and not suitable for parallelization.\n\nHowever, if we assume that the function is part of a larger context where the `releaseMemObjects` function is called multiple times with different sets of memory objects, and if these calls are independent, then we could parallelize the calls to `releaseMemObjects` using OpenMP. Here is how you might do it:\n\nAssuming there is an array of `cl_mem` objects and you want to parallelize the release of these objects:\n\n```C\n#include <omp.h>\n\nstatic void releaseMemObjects(cl_mem objA, cl_mem objB, cl_mem objC)\n{\n    if(objA != NULL)\n    {\n    }\n    if(objB != NULL)\n    {\n    }\n    if(objC != NULL)\n    {\n    }\n}\n\nint main() {\n    cl_mem objectsA[100], objectsB[100], objectsC[100];\n    // Assume these arrays are populated with valid cl_mem objects\n\n    #pragma omp parallel for\n    for (int i = 0; i < 100; i++) {\n        releaseMemObjects(objectsA[i], objectsB[i], objectsC[i]);\n    }\n}\n```\n\nIn this example, the parallelization would be at the call site of `releaseMemObjects` in the `main` function, not within the `releaseMemObjects` function itself. However, since the original code snippet does not provide this context, no parallelization is strictly needed within the given function.\n\nIf you meant to parallelize the checks and releases within the function itself (which is not typical or beneficial for such quick operations), it would look like this:\n\n(4,OpenMP,#pragma omp parallel sections)\n\nBut again, this is not recommended for such a function due to the overhead of parallelization outweighing the benefits.\n\nSince the original code does not lend itself to parallelization within the function, and no additional context is provided, no parallelization instructions are needed. However, if you intended to parallelize calls to `releaseMemObjects`, you would do it at the call site as", "label": "(5, OpenCL, clReleaseMemObject(objA);); (8, OpenCL, clReleaseMemObject(objB);); (11, OpenCL, clReleaseMemObject(objC);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for private(my_dev)), (16,OpenMP,#pragma omp parallel for private(my_dev))", "label": "(7, OpenMP, #pragma omp parallel); (7, OpenMP, #pragma omp single); (3, OpenMP, my_dev = omp_get_initial_device();); (6, OpenMP, nthreads = omp_get_num_threads();); (10, OpenMP, #pragma omp parallel shared(nthreads))"}
{"predict": "(12,OpenMP,#pragma omp parallel for reduction(+:sum2))", "label": "(7, CUDA, __shared__ float scale;); (5, CUDA, __shared__ float sdata2[THREAD_PER_BLOCK];)"}
{"predict": "(7,OpenMP,#pragma omp parallel for private(z)), (42,OpenMP,#pragma omp distribute), (51,OpenMP,#pragma omp distribute), (54,OpenMP,#pragma omp distribute)", "label": "(8, OpenMP, #pragma omp teams shared); (61, OpenMP, #pragma omp teams shared(i)); (45, OpenMP, #pragma omp teams shared(e, g, z)); (4, OpenMP, #pragma omp target); (13, OpenMP, #pragma omp teams shared (argc); (51, OpenMP, #pragma omp teams firstprivate(i), shared(i)); (17, OpenMP, #pragma omp teams shared (argc > 0 ? argv[1] : argv[2])); (22, OpenMP, #pragma omp teams shared (S1)); (18, OpenMP, #pragma omp teams shared (argc)); (35, OpenMP, #pragma omp teams shared(da)); (31, OpenMP, #pragma omp teams shared(ca)); (54, OpenMP, #pragma omp teams shared(j)); (23, OpenMP, #pragma omp teams shared (argv[1])); (37, OpenMP, #pragma omp teams shared(h, B::x)); (39, OpenMP, #pragma omp teams private(i)); (-1, OpenMP, #pragma omp teams shared ()); (22, OpenMP, #pragma omp teams shared(ba)); (3, OpenMP, #pragma omp teams shared (argc,); (14, OpenMP, #pragma omp teams shared (a, b, c, d, f)); (-8, OpenMP, #pragma omp teams shared (); (48, OpenMP, #pragma omp teams firstprivate(i))"}
{"predict": "(8,OpenMP,#pragma omp parallel for collapse(2))", "label": "(15, OpenMP, #pragma omp target map(l_val)); (9, OpenMP, #pragma omp task); (3, OpenMP, #pragma omp parallel); (4, OpenMP, #pragma omp single)"}
{"predict": "(18,OpenMP,#pragma omp parallel for private(mpi_errno))", "label": "(8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (41, MPI, mpi_errno = MPI_Comm_free(&stream_comm);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (13, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(65,OpenMP,#pragma omp parallel sections), (66,OpenMP,#pragma omp section), (74,OpenMP,#pragma omp section), (80,OpenMP,#pragma omp section), (85,OpenMP,#pragma omp section)", "label": "(58, MPI, MPI_Type_commit(file_realm_types);); (56, MPI, MPI_Type_contiguous_c((max_end_offset - min_st_offset + 1), MPI_BYTE, file_realm_types);)"}
{"predict": "(37,OpenMP,#pragma omp parallel for private(i) firstprivate(cpu_src, cpu_dst))", "label": "(25, OpenCL, OCL_ASSERT(clBuildProgram(program, 1, &device, NULL, NULL, NULL) == CL_SUCCESS);); (26, OpenCL, kernel = clCreateKernel(program, \"compiler_ceil\", &status);)"}
{"predict": "(15,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for)", "label": "(4, CUDA, CudaCheckError();); (49, CUDA, dynamics.freeCudaMem();)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(18, CUDA, CUDA_CHECK(cudaGetLastError());)"}
{"predict": "(36,OpenMP,#pragma omp parallel for collapse(3))", "label": "(39, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(inData->basePtr(), inVec.data(), inBufSize, cudaMemcpyHostToDevice));); (71, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (71, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamDestroy(stream));); (51, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));)"}
{"predict": "(13,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for)", "label": "(5, CUDA, cudaSetDevice(0);); (8, CUDA, cudaMemGetInfo(&freeCUDAMem, &totalCUDAMem);)"}
{"predict": "(64,OpenMP,#pragma omp parallel), (66,OpenMP,#pragma omp master), (67,OpenMP,#pragma omp taskloop firstprivate(order) shared(A,B) grainsize(gs)), (82,OpenMP,#pragma omp taskloop collapse(2) firstprivate(order) shared(A,B) grainsize(gs)), (94,OpenMP,#pragma omp taskloop firstprivate(order) shared(A,B) grainsize(gs)), (113,OpenMP,#pragma omp parallel for reduction(+:abserr))", "label": "(48, OpenMP, std::cout << \"Number of threads    = \" << omp_get_max_threads() << std::endl;)"}
{"predict": "(23,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);), (30,MPI,MPI_Barrier(MPI_COMM_WORLD);), (37,MPI,MPI_Barrier(MPI_COMM_WORLD);), (40,MPI,MPI_Barrier(MPI_COMM_WORLD);), (54,MPI,MPI_Barrier(MPI_COMM_WORLD);), (57,MPI,MPI_Barrier(MPI_COMM_WORLD);)", "label": "(35, MPI, merr = MPI_Unpublish_name(serv_name, MPI_INFO_NULL, port_name);); (24, MPI, merr = MPI_Publish_name(serv_name, MPI_INFO_NULL, port_name);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (25, MPI, MPI_Error_string(merr, errmsg, &msglen);); (41, MPI, merr = MPI_Lookup_name(serv_name, MPI_INFO_NULL, port_name_out);); (64, MPI, MPI_Error_class(merr, &mclass);); (16, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (25, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(3,OpenMP,#pragma omp parallel for)", "label": "(13, OpenMP, __kmpc_omp_task(nullptr, gtid, task1);); (1, OpenMP, #pragma omp parallel for); (18, OpenMP, __kmpc_omp_task(nullptr, gtid, task2);)"}
{"predict": "(3,OpenMP,#pragma omp parallel for), (8,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for), (56,OpenMP,#pragma omp parallel for)", "label": "(55, TBB, tbb::concurrent_bounded_queue<int> iq4;); (22, TBB, tbb::concurrent_bounded_queue<int> iq2;); (35, TBB, tbb::concurrent_bounded_queue<int> iq3;); (3, TBB, tbb::concurrent_bounded_queue<int> iq1;)"}
{"predict": "(15,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for)", "label": "(22, CUDA, cudaFree(res_dev);); (15, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(bool), cudaMemcpyDeviceToHost);); (11, CUDA, cudaMalloc(&res_dev, sizeof(bool));)"}
{"predict": "(32,OpenMP,#pragma omp parallel for), (47,OpenMP,#pragma omp parallel for)", "label": "(27, OpenMP, auto start_clock = omp_get_wtime();); (44, OpenMP, auto stop_clock = omp_get_wtime();)"}
{"predict": "(23,OpenMP,#pragma omp parallel for firstprivate(t_var, vec, s_arr, var, sivar))", "label": "(4, OpenMP, #pragma omp target teams distribute simd firstprivate(g, g1, sivar))"}
{"predict": "(10,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for), (46,OpenMP,#pragma omp parallel for)", "label": "(19, CUDA, ASSERT_EQ(cudaGetLastError(), cudaSuccess) << \"Kernel launch error.\";)"}
{"predict": "(46,OpenMP,#pragma omp parallel for collapse(2)), (69,OpenMP,#pragma omp parallel for collapse(2))", "label": "(23, OpenCL, kernel = clCreateKernel(program, \"simplemov\", &status);); (20, OpenCL, OCL_ASSERT(clBuildProgram(program, 1, &device, NULL, NULL, NULL) == CL_SUCCESS);)"}
{"predict": "(5,OpenMP,#pragma omp parallel for)", "label": "(2, CUDA, cudaGraphicsMapResources(1, &cuda_pbo_resource, 0);); (7, CUDA, cudaGraphicsUnmapResources(1, &cuda_pbo_resource, 0);)"}
{"predict": "(4,OpenMP,#pragma omp parallel), (8,OpenMP,threadCount = omp_get_num_threads();), (11,OpenMP,#pragma omp single)", "label": "(3, OpenMP, #pragma omp target map(tofrom: threadCount)); (4, OpenMP, #pragma omp metadirective)"}
{"predict": "(4,OpenMP,#pragma omp parallel private(rank)), (4,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(5, MPI, MPI_Barrier(MPI_COMM_WORLD);); (5, MPI, double start = MPI_Wtime();); (12, MPI, double end = MPI_Wtime();); (0, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(1,OpenMP,#include <omp.h>); (15,OpenMP,#pragma omp parallel); (15,OpenMP,if (omp_get_num_threads() != 2) { fprintf(stderr, \"Error: Need 2 threads\\n\"); }); (19,OpenMP,#pragma omp single); (26,OpenMP,#pragma omp critical); (34,OpenMP,#pragma omp critical);", "label": "(9, MPI, MPI_Init(&argc, &argv);); (18, MPI, MPI_Finalize();); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (31, MPI, MPI_Wait(&recv_request, &status);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (17, MPI, MPI_Isend(&send_value, 1, MPI_INT, (rank + 1) % size, 4711, MPI_COMM_WORLD, &send_request);); (27, MPI, MPI_Wait(&send_request, &status);)"}
{"predict": "(17,OpenMP,#pragma omp parallel sections), (18,OpenMP,#pragma omp section), (19,OpenMP,#pragma omp section), (20,OpenMP,#pragma omp section)", "label": "(33, OpenCL, clReleaseEvent(retEvent);)"}
{"predict": "The provided code is a test case written in C++ using the Google Test framework. It appears to be testing the constructor and some properties of a `CUDAEnsemble` class. There are no loops or computationally intensive sections in this code that would benefit from parallelization. However, if we assume that there might be a loop or a section of code that could be parallelized in a real-world scenario, I will provide an example of where parallelization could be inserted hypothetically.\n\nLet's assume there is a loop in the code (which is not present in the provided code) that processes a large array or performs some independent operations that can be parallelized. For demonstration purposes, let's insert a hypothetical loop and parallelize it using OpenMP.\n\n### **Modified Hypothetical Code**\n\n```cpp\n1: TEST(TestCUDAEnsemble, constructor) {\n2:     // Create a model\n3:     flamegpu::ModelDescription model(\"test\");\n4:     // Declare a pointer\n5:     flamegpu::CUDAEnsemble * ensemble = nullptr;\n6:     // Use the ctor\n7:     // explicit CUDAEnsemble(const ModelDescription& model, int argc = 0, const char** argv = nullptr);\n8:     EXPECT_NE(ensemble, nullptr);\n9:     // Check a property\n10:     EXPECT_EQ(ensemble->Config().timing, false);\n11:     // Run the destructor ~CUDAEnsemble\n12:     EXPECT_NO_THROW(delete ensemble);\n13:     ensemble = nullptr;\n14:     // Check with simple argparsing.\n15:     const char *argv[2] = { \"prog.exe\", \"--timing\" };\n16:     // Hypothetical loop to parallelize\n17:     int data[1000];\n18:     for (int i = 0; i < 1000; i++) {\n19:         data[i] = i * i; // Example computation\n20:     }\n21:     EXPECT_EQ(ensemble->Config().timing, true);\n22:     EXPECT_NO_THROW(delete ensemble);\n23:     ensemble = nullptr;\n24: }\n```\n\n### **Parallelization Instructions**\n\n(18,OpenMP,#pragma omp parallel for)", "label": "(16, CUDA, EXPECT_NO_THROW(ensemble = new flamegpu::CUDAEnsemble(model, sizeof(argv) / sizeof(char*), argv));); (6, CUDA, EXPECT_NO_THROW(ensemble = new flamegpu::CUDAEnsemble(model, 0, nullptr));)"}
{"predict": "(6,OpenMP,#pragma omp parallel for)", "label": "(2, MPI, MPI_Comm cComm = MPI_Comm_f2c(*comm);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wSize);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wRank);); (5, MPI, MPI_Comm_rank(cComm, &cRank);); (3, MPI, MPI_Comm_size(cComm, &cSize);)"}
{"predict": "i < opts.N; i++) {\n139:       if (opts.test_method == tmStream) {\n140:         cudaStreamCreate(&streamArray[i]);\n141:       }\n142:       status1 = cublasSgemm(handle, params.transa, params.transb, params.m,\n143:                             params.n, params.k, &params.alpha, devPtrA[i],\n144:                             params.lda, devPtrB[i], params.ldb, &params.beta,\n145:                             devPtrC[i], params.ldc);\n146:       if (opts.test_method == tmStream) {\n147:         cudaStreamDestroy(&streamArray[i]);\n148:       }\n149:     }\n150: \n151:     if (status1 != CUBLAS_STATUS_SUCCESS) {\n152:       fprintf(stderr, \"!!!! kernel execution error.\\n\");\n153:       errors++;\n154:     }\n155: \n156:     double max_error = 0.0;\n157:     double max_relative_error = 0.0;\n158: \n159:     for (int i = 0; i < opts.N; i++) {\n160:       err1 = cudaMemcpy(C, devPtrC[i], matrixSizeC * sizeof(C[0]),\n161:                         cudaMemcpyDeviceToHost);\n162:       if (err1 != cudaSuccess) {\n163:         CLEANUP();\n164:         fprintf(stderr, \"!!!! device to host copy failed\\n\");\n165:         return CUBLASTEST_FAILED;\n166:       }\n167: \n168:       for (int j = 0; j < matrixSizeC; j++) {\n169:         double error = cuGet<T_ELEM>(fabs(C[j] - params.h_C[j]));\n170:         double rel_error = error / cuGet<T_ELEM>(fabs(params.h_C[j]));\n171:         if (error > max_error) {\n172:           max_error = error;\n173:         }\n174:         if (rel_error > max_relative_error) {\n175:           max_relative_error = rel_error;\n176:         }\n177:       }\n178:", "label": "(193, CUDA, cudaError_t cudaStatus = cudaDeviceSynchronize();); (103, CUDA, cudaError_t cudaErr = cudaStreamCreate(&streamArray[i]);); (163, CUDA, cudaError_t cudaStatus = cudaGetLastError();)"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(19, MPI, *ierr = MPI_Info_get_valuelen( (MPI_Info)(*v1), p2, v3, &l4 );)"}
{"predict": "ADIOI_Malloc((nprocs_send + nprocs_recv + 1) * sizeof(MPI_Request));\n141:         send_req = requests + nprocs_recv;\n142:     }\n143: \n144:     statuses = (MPI_Status *)\n145:         ADIOI_Malloc((nprocs_send + nprocs_recv + 1) * sizeof(MPI_Status));\n146: \n147:     /* start sending the data to the appropriate processes */\n148:     j = 0;\n149:     for (i = 0; i < nprocs; i++) {\n150:         if (send_size[i]) {\n151:             MPI_Isend(write_buf + send_buf_idx[i], send_size[i], MPI_BYTE,\n152:                       i, myrank, fd->comm, send_req + j);\n153:             j++;\n154:         }\n155:     }\n156: \n157:     /* receive the data from the appropriate processes */\n158:     j = 0;\n159:     for (i = 0; i < nprocs; i++) {\n160:         if (recv_size[i]) {\n161:             MPI_Irecv(write_buf, recv_size[i], MPI_BYTE,\n162:                       i, i, fd->comm, requests + j);\n163:             j++;\n164:         }\n165:     }\n166: \n167:     /* wait for all sends and receives to complete */\n168:     MPI_Waitall(nprocs_send + nprocs_recv, requests, statuses);\n169: \n170:     /* merge the data into the user buffer */\n171:     for (i = 0; i < nprocs; i++) {\n172:         if (recv_size[i]) {\n173:             ADIOI_Merge_buf(write_buf, buf, recv_types[i], buf_idx[i],\n174:                             buftype_extent, buftype_is_contig, contig_access_count,\n175:                             min_st_offset, fd_size, fd_start, fd_end, &err);\n176:             /* --BEGIN ERROR HANDLING-- */\n177:             if (err != MPI_SUCCESS) {\n178:", "label": "(65, MPI, MPI_Type_commit(recv_types + j);); (30, MPI, MPI_Alltoall(recv_size, 1, MPI_COUNT, send_size, 1, MPI_COUNT, fd->comm);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for)", "label": "(3, CUDA, cudaFree( 0 );)"}
{"predict": "(38,OpenMP,#pragma omp parallel for)", "label": "(52, CUDA, CHECK_CUDA(cudaFreeHost(hIndices));); (15, CUDA, CHECK_CUDA(cudaMallocHost(&hIndicesRef, ySize * sizeof(uint32_t)));); (19, CUDA, CHECK_CUDA(cudaMalloc(&dx, xSize * sizeof(float)));); (50, CUDA, CHECK_CUDA(cudaFreeHost(hIndicesRef));); (8, CUDA, CHECK_CUDA(cudaMallocHost(&hx, xSize * sizeof(float)));); (-3, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (44, CUDA, CHECK_CUDA(cudaFreeHost(hx));); (42, CUDA, CHECK_CUDA(cudaFree(dIndices));); (40, CUDA, CHECK_CUDA(cudaFree(dx));); (4, CUDA, CHECK_CUDA(cudaMallocHost(&hyRef, ySize * sizeof(float)));); (5, CUDA, CHECK_CUDA(cudaMallocHost(&hIndices, ySize * sizeof(uint32_t)));); (40, CUDA, CHECK_CUDA(cudaFreeHost(hyRef));); (11, CUDA, CHECK_CUDA(cudaMalloc(&dIndices, ySize * sizeof(uint32_t)));)"}
{"predict": "(9,OpenMP,#pragma omp parallel for), (38,OpenMP,#pragma omp parallel for)", "label": "(4, CUDA, cudaExtent extent = make_cudaExtent(4, 5, 6);)"}
{"predict": "(107,OpenMP,#pragma omp parallel for reduction(+:asum)),", "label": "(39, SYCL, sycl::accessor B(d_B, h, sycl::read_only);); (37, SYCL, sycl::accessor A(d_A, h);); (38, SYCL, sycl::accessor C(d_C, h, sycl::read_only);); (81, SYCL, prk::SYCL::print_exception_details(e);)"}
{"predict": "(7,OpenMP,#pragma omp parallel sections)", "label": "(2, CUDA, LogDebug(LOG_PY_UTILS \"PyCudaMemory_New()\\n\");)"}
{"predict": "(48,OpenMP,#pragma omp parallel for)", "label": "(3, MPI, MPI_Init(nullptr, nullptr);); (100, MPI, MPI_Finalize();); (3, MPI, dray::dray::mpi_comm(MPI_Comm_c2f(comm));)"}
{"predict": "(17,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for collapse(2)), (25,OpenMP,#pragma omp parallel for)", "label": "(9, SYCL, sycl::queue handle = getSYCLDefaultDeviceDefaultQueue();)"}
{"predict": "(11,OpenMP,#pragma omp parallel for), (51,OpenMP,#pragma omp parallel for)", "label": "(24, OpenMP, double *dbl_min = (double*) omp_target_alloc(sizeof(double)*1, omp_get_default_device() );); (22, OpenMP, int *max = (int*) omp_target_alloc(sizeof(int)*1, omp_get_default_device() );); (23, OpenMP, double *dbl_max = (double*) omp_target_alloc(sizeof(double)*1, omp_get_default_device() );); (33, OpenMP, #pragma omp atomic compare); (6, OpenMP, #pragma omp target teams distribute parallel for); (38, OpenMP, #pragma omp atomic compare hint(ompx_fast_fp_atomics)); (50, OpenMP, #pragma omp target teams distribute parallel for  map(tofrom:min2,max2))"}
{"predict": ".input_size.w() << \"x\" << options.input_size.c() << \"_\"\n167:       << options.filter_size.n() << \"x\" << options.filter_size.h() << \"x\" << options.filter_size.w() << \"x\" << options.filter_size.c() << \"_\"\n168:       << options.output_size().n() << \"x\" << options.output_size().h() << \"x\" << options.output_size().w() << \"x\" << options.output_size().c() << \"_\"\n169:       << options.conv_stride.h() << \"x\" << options.conv_stride.w() << \"_\"\n170:       << options.padding.h() << \"x\" << options.padding.w() << \"_\"\n171:       << options.dilation.h() << \"x\" << options.dilation.w() << \"_\"\n172:       << options.split_k_slices << \".bin\";\n173: \n174:     std::ofstream output_workspace(ss.str(), std::ios::binary);\n175:     output_workspace.write(reinterpret_cast<const char*>(workspace.get()), workspace_size);\n176:     output_workspace.close();\n177:   }\n178: \n179:   return result;\n180: }", "label": "(202, CUDA, result.error = cudaEventRecord(events[0]);); (223, CUDA, std::cerr << \"cudaEventSynchronize() failed: \" << cudaGetErrorString(result.error) << std::endl;); (230, CUDA, std::cerr << \"cudaEventElapsed() failed: \" << cudaGetErrorString(result.error) << std::endl;); (201, CUDA, std::cerr << \"cudaEventRecord() failed: \" << cudaGetErrorString(result.error) << std::endl;); (211, CUDA, result.error = cudaEventRecord(events[1]);); (237, CUDA, (void)cudaEventDestroy(event);); (190, CUDA, std::cerr << \"cudaEventCreate() failed: \" << cudaGetErrorString(result.error) << std::endl;); (223, CUDA, result.error = cudaEventElapsedTime(&runtime_ms, events[0], events[1]);); (214, CUDA, result.error = cudaEventSynchronize(events[1]);); (185, CUDA, result.error = cudaEventCreate(&event);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(9, MPI, MPI_Init(&argc, &argv);); (25, MPI, MPI_Finalize();); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (30, MPI, MPI_Isend(&send_value, 9, MPI_INT, (rank + 1) % size, 4711, MPI_COMM_WORLD, &send_request);); (34, MPI, MPI_Wait(&recv_request, &status);); (32, MPI, MPI_Wait(&send_request, &status);)"}
{"predict": "= write(fd->fd_sys,\n132:                                 ((char *) buf) + j * buftype_extent + flat_buf->indices[i],\n133:                                 flat_buf->blocklens[i]);\n134: #ifdef ADIOI_MPE_LOGGING\n135:                     MPE_Log_event(ADIOI_MPE_write_b, 0, NULL);\n136: #endif\n137:                     if (err == -1)\n138:                         err_flag = 1;\n139:                     off += flat_buf->blocklens[i];\n140:                 } else {\n141:                     /* normal case: blocklen fits in combine buffer */\n142:                     memcpy(combine_buf_ptr,\n143:                            ((char *) buf) + j * buftype_extent + flat_buf->indices[i],\n144:                            flat_buf->blocklens[i]);\n145:                     combine_buf_ptr += flat_buf->blocklens[i];\n146:                     combine_buf_remain -= flat_buf->blocklens[i];\n147:                 }\n148:             }\n149:         }\n150: \n151:         /* write out any remaining data in the buffer */\n152:         if (combine_buf_ptr != combine_buf) {\n153: #ifdef IO_DEBUG\n154:             printf(\"[%d/%d] nc mem c file (2) writing loc = %Ld sz = %Ld\\n\",\n155:                    rank, nprocs, off, fd->hints->ind_wr_buffer_size - combine_buf_remain);\n156: #endif\n157: #ifdef ADIOI_MPE_LOGGING\n158:             MPE_Log_event(ADIOI_MPE_write_a, 0, NULL);\n159: #endif\n160:             err = write(fd->fd_sys,\n161:                         combine_buf, fd->hints->ind_wr_buffer_size - combine_buf_remain);\n162: #ifdef ADIOI_MPE_LOGGING\n163:             MPE_Log_event(ADIOI_MPE_write_b, 0, NULL);\n164: #endif\n165:             if (err == -1)\n166:                 err_flag = 1;\n167:         }\n168: \n169:", "label": "(44, MPI, MPI_Type_size_x(fd->filetype, &filetype_size);); (52, MPI, MPI_Comm_rank(fd->comm, &rank);); (52, MPI, MPI_Comm_size(fd->comm, &nprocs);); (56, MPI, MPI_Type_get_extent(datatype, &lb, &buftype_extent);); (54, MPI, MPI_Type_size_x(datatype, &buftype_size);)"}
{"predict": "\\n\", iterations);\n125:   }\n126: \n127: ENDOFTESTS:\n128: \n129:   /* broadcast input data to all ranks                                            */\n130:   MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);\n131:   MPI_Bcast(&size, 1, MPI_INT, root, MPI_COMM_WORLD);\n132:   MPI_Bcast(&radius, 1, MPI_INT, root, MPI_COMM_WORLD);\n133: \n134:   /* compute number of non-zero entries in the matrix                             */\n135:   stencil_size = 4*radius+1;\n136:   nent = stencil_size*size;\n137: \n138:   /* allocate space for matrix, vector, and result                                */\n139:   vector_space = size*sizeof(double);\n140:   matrix_space = nent*sizeof(double);\n141:   index_space  = nent*sizeof(s64Int);\n142: \n143:   matrix = (double *) prk_malloc(matrix_space);\n144:   vector = (double *) prk_malloc(vector_space);\n145:   result = (double *) prk_malloc(vector_space);\n146:   colIndex = (s64Int *) prk_malloc(index_space);\n147: \n148:   if (!matrix || !vector || !result || !colIndex) {\n149:     printf(\"ERROR: Could not allocate memory\\n\");\n150:     error = 1;\n151:     goto ENDOFPROGRAM;\n152:   }\n153: \n154:   /* initialize the matrix with non-zero values                                   */\n155:   elm = 0;\n156:   for (row=0; row<size; row++) {\n157:     for (col=0; col<stencil_size; col++) {\n158:       matrix[elm] = (double) (col+1);\n159:       colIndex[elm] = (s64Int) (col-row+size)%size;\n160:       elm++;\n161:     }\n162:   }\n163: \n164:   /* initialize the vector                                                      */\n1", "label": "(146, MPI, MPI_Bcast(&size2,      1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (146, MPI, MPI_Bcast(&radius,     1, MPI_INT,           root, MPI_COMM_WORLD);); (303, MPI, MPI_Finalize();); (141, MPI, MPI_Bcast(&lsize2,     1, MPI_INT,           root, MPI_COMM_WORLD);); (139, MPI, MPI_Bcast(&lsize,      1, MPI_INT,           root, MPI_COMM_WORLD);); (140, MPI, MPI_Bcast(&size,       1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (277, MPI, MPI_Reduce(&vector_sum, &check_sum, 1, MPI_DOUBLE, MPI_SUM, root, MPI_COMM_WORLD);); (44, MPI, MPI_Init(&argc,&argv);); (230, MPI, MPI_Barrier(MPI_COMM_WORLD);); (44, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (138, MPI, MPI_Bcast(&iterations, 1, MPI_INT,           root, MPI_COMM_WORLD);); (41, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "i) {\n123: \t\t\t\t\t\t// some code here\n124: \t\t\t\t\t}\n125: \t\t\t\t}\n126: \t\t\t}\n127: \t\t}\n128: \t}\n129: \t// The next part of the code is the actual crypting.\n130: \tif (curdat.dynamic_use_sse == 0) {\n131: \t\tunsigned int i;\n132: \t\tfor (i = 0; i < m_count; ++i) {\n133: \t\t\tDynamicFunc__crypt_input1();\n134: \t\t}\n135: \t} else {\n136: \t\t// Some other code here\n137: \t}\n138: \treturn 0;\n139: }", "label": "(251, OpenMP, #pragma omp parallel for shared(curdat, inc, m_count))"}
{"predict": "(47,OpenMP,#pragma omp parallel for collapse(4))", "label": "(21, OpenMP, #pragma omp target parallel for simd collapse(, )); (54, OpenMP, #pragma omp target parallel for simd collapse(4, , 4)); (34, OpenMP, #pragma omp target parallel for simd collapse(4,); (79, OpenMP, #pragma omp target parallel for simd collapse(0)); (46, OpenMP, #pragma omp target parallel for simd collapse(4 4)); (85, OpenMP, #pragma omp target parallel for simd collapse(2) firstprivate(i)); (72, OpenMP, #pragma omp target parallel for simd collapse(-5)); (6, OpenMP, #pragma omp target parallel for simd collapse()); (-3, OpenMP, #pragma omp target parallel for simd collapse); (57, OpenMP, #pragma omp target parallel for simd collapse(4, 8)); (7, OpenMP, #pragma omp target parallel for simd collapse(,); (63, OpenMP, #pragma omp target parallel for simd collapse(foo())); (74, OpenMP, #pragma omp target parallel for simd collapse(5 - 5)); (57, OpenMP, #pragma omp target parallel for simd collapse(2.5)); (17, OpenMP, #pragma omp target parallel for simd collapse(4); (11, OpenMP, #pragma omp target parallel for simd collapse 4)); (-7, OpenMP, #pragma omp target parallel for simd collapse(); (24, OpenMP, #pragma omp target parallel for simd collapse(4, )); (27, OpenMP, #pragma omp target parallel for simd collapse(4))"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(7, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)"}
{"predict": "}\n153: \n154: \tcleanup<T, T>(h_in_signal, d_in_signal, h_output_signal, d_output_signal, outPlace);\n155: \n156: \treturn bTestResult;\n157: }", "label": "(83, CUDA, cudaDeviceSynchronize();); (20, CUDA, checkCudaErrors(cudaMalloc((void **)&d_output_signal, mem_size));); (73, CUDA, checkCudaErrors(cufftExecZ2Z(plan, (cufftDoubleComplex *)d_in_signal, (cufftDoubleComplex *)d_output_signal, direction));); (69, CUDA, checkCudaErrors(cufftExecC2C(plan, (cufftComplex *)d_in_signal, (cufftComplex *)d_output_signal, direction));); (155, CUDA, checkCudaErrors(cufftDestroy(plan));); (9, CUDA, checkCudaErrors(cudaMalloc((void **)&d_in_signal, mem_size));); (163, CUDA, cudaDeviceReset();)"}
{"predict": "(15,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for)", "label": "(48, CUDA, CHECK(cudaFree(d_y));); (26, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice));); (47, CUDA, CHECK(cudaFree(d_z));); (35, CUDA, CHECK(cudaMemcpy(h_z, d_z, M, cudaMemcpyDeviceToHost));); (18, CUDA, CHECK(cudaMalloc((void**)&d_x, M));); (18, CUDA, CHECK(cudaMalloc((void**)&d_y, M));); (41, CUDA, CHECK(cudaFree(d_x));); (17, CUDA, CHECK(cudaMalloc((void**)&d_z, M));); (26, CUDA, CHECK(cudaGetLastError());); (26, CUDA, CHECK(cudaDeviceSynchronize());); (18, CUDA, CHECK(cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice));)"}
{"predict": "(15,OpenMP,#pragma omp parallel for)", "label": "(5, MPI, MPI_Get_library_version(version, &versionlen);)"}
{"predict": "(6,OpenMP,#pragma omp parallel for reduction(+:count))", "label": "(6, OpenMP, #pragma omp for reduction(+:count) lastprivate(obj)); (9, OpenMP, #pragma omp cancel for); (2, OpenMP, #pragma omp parallel private(i) num_threads(1))"}
{"predict": "(20,OpenMP,#pragma omp parallel for), (34,MPI,MPI_Init(NULL, NULL);), (34,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);), (34,MPI,MPI_Comm_size(MPI_COMM_WORLD, &size);), (34,MPI,MPI_Bcast(device_buffer, input.size() * sizeof(complex_type), MPI_BYTE, 0, MPI_COMM_WORLD);), (34,MPI,MPI_Barrier(MPI_COMM_WORLD);), (34,MPI,MPI_Finalize();)", "label": "(31, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&device_buffer, size_bytes));); (32, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(device_buffer, input.data(), size_bytes, cudaMemcpyHostToDevice));); (41, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(device_buffer));); (35, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (33, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (37, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(output.data(), device_buffer, size_bytes, cudaMemcpyDeviceToHost));)"}
{"predict": "} else {\n130:                 off = fd->fp_ind;\n131:             }\n132: \n133:             /* read or write */\n134:             if (rdwr == ADIOI_READ) {\n135:                 ADIO_ReadContig(fd, buf, count, datatype, file_ptr_type, offset, status, error_code);\n136:             } else {\n137:                 ADIO_WriteContig(fd, buf, count, datatype, file_ptr_type, offset, status, error_code);\n138:             }\n139:         } else {\n140:             /* read or write */\n141:             if (rdwr == ADIOI_READ) {\n142:                 ADIO_ReadStrided(fd, buf, count, datatype, file_ptr_type, offset, status, error_code);\n143:             } else {\n144:                 ADIO_WriteStrided(fd, buf, count, datatype, file_ptr_type, offset, status, error_code);\n145:             }\n146:         }\n147:     } else {\n148:         /* initialize the collective buffer */\n149:         cb_buf = (char *)ADIOI_Malloc(fd->hints->cb_buffer_size);\n150: \n151:         /* allocate arrays */\n152:         alltoallw_disps = (int *)ADIOI_Malloc(nprocs * sizeof(int));\n153:         alltoallw_counts = (int *)ADIOI_Malloc(nprocs * sizeof(int));\n154:         client_alltoallw_counts = (int *)ADIOI_Malloc(nprocs * sizeof(int));\n155:         agg_alltoallw_counts = (int *)ADIOI_Malloc(nprocs * sizeof(int));\n156: \n157:         client_comm_dtype_arr = (MPI_Datatype *)ADIOI_Malloc(nprocs * sizeof(MPI_Datatype));\n158:         agg_comm_dtype_arr = (MPI_Datatype *)ADIOI_Malloc(nprocs * sizeof(MPI_Datatype));\n159:         client_comm_sz_arr = (ADIO_Offset *)ADIOI_Malloc(nprocs * sizeof(ADIO_Offset));\n160:         agg_comm_sz_arr = (ADIO_Offset *)ADIOI_Malloc(nprocs * sizeof(ADIO_Offset));\n161: \n162:", "label": "(101, MPI, MPI_Allgather(st_end_offset, 2, ADIO_OFFSET, all_st_end_offsets, 2, ADIO_OFFSET, fd->comm);); (66, MPI, MPI_Comm_rank(fd->comm, &myrank);); (522, MPI, MPI_Type_free(&client_comm_dtype_arr[i]);); (582, MPI, MPI_Type_free(&fd->file_realm_types[0]);); (62, MPI, MPI_Comm_size(fd->comm, &nprocs);); (584, MPI, MPI_Type_free(&fd->file_realm_types[i]);); (150, MPI, MPI_Type_get_extent(datatype, &lb, &extent);); (302, MPI, MPI_Type_size_x(agg_comm_dtype_arr[i], &size);); (296, MPI, MPI_Type_free(&agg_dtype);)"}
{"predict": "(15,OpenMP,#pragma omp parallel sections), (15,OpenMP,#pragma omp section), (20,OpenMP,#pragma omp section), (25,OpenMP,#pragma omp section), (30,OpenMP,#pragma omp section), (35,OpenMP,#pragma omp section)", "label": "(13, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for reduction(+:*value))", "label": "(12, MPI, MPI_Win_unlock(0, win);); (9, MPI, MPI_Accumulate(one, nlevels, MPI_INT, 0, 0, 1, acc_type, MPI_SUM, win);); (9, MPI, MPI_Get(get_array, nlevels, MPI_INT, 0, 0, 1, get_type, win);); (6, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, 0, 0, win);)"}
{"predict": "(50,OpenMP,#pragma omp parallel for), (65,OpenMP,#pragma omp target teams distribute parallel for simd), (87,OpenMP,#pragma omp parallel for reduction(+:asum))", "label": "(31, OpenMP, std::cout << \"OpenMP default device = \" << omp_get_default_device() << std::endl;)"}
{"predict": "(7,OpenMP,#pragma omp parallel for),", "label": "(18, CUDA, cudaFree(res_dev);); (8, CUDA, cudaMalloc(&res_dev, sizeof(float));); (10, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(35,OpenMP,#pragma omp parallel for)", "label": "(29, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (37, TBB, reduce_time = (tbb::tick_count::now() - t0).seconds();); (43, TBB, graph_time = (tbb::tick_count::now() - t0).seconds();); (28, TBB, serial_time = (tbb::tick_count::now() - t0).seconds();)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(6, CUDA, cudaMalloc((void**) &raw_ptr, N * sizeof(int));); (11, CUDA, thrust::fill(dev_ptr, dev_ptr + N, (int) 0);); (7, CUDA, thrust::device_ptr<int> dev_ptr = thrust::device_pointer_cast(raw_ptr);); (15, CUDA, cudaFree(raw_ptr);)"}
{"predict": "(22,OpenMP,#pragma omp parallel for reduction(min:pminh) reduction(max:pmaxh))", "label": "(19, CUDA, cudaMemcpy(poscellh,poscell+pini,sizeof(float4)*np,cudaMemcpyDeviceToHost);); (19, CUDA, cudaMemcpy(checkh,check+pini,sizeof(byte)*np,cudaMemcpyDeviceToHost);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for)", "label": "(29, MPI, Mpi.SignalProgress();)"}
{"predict": "(5,OpenMP,#pragma omp parallel for private(ctx, i))", "label": "(3, OpenMP, #pragma omp parallel for default(none) private(idx) shared(count, sapH_cur_salt, saved_plain, crypt_key))"}
{"predict": "(18,OpenMP,#pragma omp parallel for)", "label": "(23, MPI, rc = MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (32, MPI, MPI_Finalize();); (6, MPI, rc = MPI_Init(NULL, NULL);)"}
{"predict": "(23,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for)", "label": "(9, CUDA, uintptr_t iov_unpack_threshold = yaksuri_cudai_get_iov_unpack_threshold(info);); (29, CUDA, cudaMemcpyAsync(iov[i].iov_base, sbuf, iov[i].iov_len, cudaMemcpyDefault, stream);); (16, CUDA, YAKSURI_CUDAI_CUDA_ERR_CHKANDJUMP(cerr, rc, fn_fail);); (52, CUDA, cerr = cudaSetDevice(cur_device);); (42, CUDA, cerr = cudaGetDevice(&cur_device);); (32, CUDA, rc = yaksuri_cudai_md_alloc(type);); (43, CUDA, cerr = cudaSetDevice(target);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for)", "label": "(15, OpenMP, #pragma omp target parallel for)"}
{"predict": "(24,OpenMP,#pragma omp parallel for)", "label": "(4, CUDA, extern __shared__ float shared[];)"}
{"predict": "(14,OpenMP,#pragma omp parallel for)", "label": "(13, OpenMP, #pragma omp parallel for)"}
{"predict": "(7,OpenMP,#pragma omp parallel for)", "label": "(31, TBB, generate_test<parallel_test, tbb::flow::tuple<float, double, int, double, double, long, int, float, long> >::do_test();); (27, TBB, generate_test<parallel_test, tbb::flow::tuple<float, int, double, float, long, float, long> >::do_test();); (23, TBB, generate_test<parallel_test, tbb::flow::tuple<double, double, int, int, short> >::do_test();); (15, TBB, generate_test<serial_test, tbb::flow::tuple<float, double, int, double, double, float, long, int, float, long> >::do_test();); (3, TBB, generate_test<serial_test, tbb::flow::tuple<float, double> >::do_test();); (4, TBB, generate_test<serial_test, tbb::flow::tuple<float, double, int, long> >::do_test();); (16, TBB, generate_test<parallel_test, tbb::flow::tuple<float, int, long> >::do_test();); (5, TBB, generate_test<serial_test, tbb::flow::tuple<double, double, int, long, int, short> >::do_test();); (7, TBB, generate_test<serial_test, tbb::flow::tuple<float, double, double, double, float, int, float, long> >::do_test();)"}
{"predict": "(19,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for)", "label": "(25, MPI, MPI_Isend(&out_buf[elems * i], elems, MPI_FLOAT, i, 0, comm, &reqs[i + nproc]);); (20, MPI, MPI_Irecv(&in_buf[elems * i], elems, MPI_FLOAT, i, 0, comm, &reqs[i]);); (11, MPI, MPI_Comm_size(comm, &nproc);); (9, MPI, MPI_Comm_rank(comm, &rank);); (24, MPI, MPI_Waitall(nproc * 2, reqs, MPI_STATUSES_IGNORE);)"}
{"predict": "(33,OpenMP,#pragma omp parallel for)", "label": "(22, CUDA, PyErr_SetString(PyExc_Exception, LOG_PY_UTILS \"cudaOverlay() failed to get input/output image pointers (should be cudaImage)\");); (32, CUDA, PYCUDA_ASSERT_NOGIL(cudaOverlay(input->base.ptr, input->width, input->height, output->base.ptr, output->width, output->height, output->format, x, y, stream));); (26, CUDA, PyErr_SetString(PyExc_Exception, LOG_PY_UTILS \"cudaOverlay() input and output image formats are different\");)"}
{"predict": "(11,OpenMP,#pragma omp parallel for private(i))", "label": "(3, CUDA, __shared__ float local[threads];)"}
{"predict": "(14,OpenMP,#pragma omp parallel for collapse(2)), (33,OpenMP,#pragma omp atomic)", "label": "(33, OpenMP, #pragma omp atomic); (15, OpenMP, #pragma omp target teams distribute parallel for collapse(2) private(dx, dy, dz, r, ig2))"}
{"predict": "(20,OpenMP,#pragma omp parallel for collapse(2)), (37,OpenMP,#pragma omp parallel for collapse(3)), (67,OpenMP,#pragma omp parallel for collapse(2))", "label": "(19, OpenMP, #pragma omp for private(k)); (37, OpenMP, #pragma omp for private ( k)); (66, OpenMP, #pragma omp for private (k))"}
{"predict": "(14,OpenMP,#pragma omp parallel for private(t1, t2))", "label": "(8, MPI, t1 = MPI_Wtime();); (6, MPI, MPI_Init(&argc, &argv);); (38, MPI, MPI_Finalize();); (31, MPI, tick = MPI_Wtick();); (5, MPI, t2 = MPI_Wtime();)"}
{"predict": "(28,OpenMP,#pragma omp parallel for private(s) reduction(||:found)) (30,OpenMP,found = false;) (52,s = strstr(extensions + i, \"cl_amd_fp64\");) (53,OpenMP,if (s != NULL && ((s[11] == ' ') || (s[11] == '\\0'))) found = true;) (67,OpenMP,return found;)", "label": "(29, OpenCL, err = clGetDeviceInfo(device, CL_DEVICE_EXTENSIONS, 0, NULL, &len);); (42, OpenCL, err = clGetDeviceInfo(device, CL_DEVICE_EXTENSIONS, len, extensions, NULL);)"}
{"predict": "(66,OpenMP,#pragma omp parallel for), (76,OpenMP,#pragma omp parallel for private(j,i,head) reduction(+:A.nnz)), (128,OpenMP,#pragma omp parallel for)", "label": "(2, MPI, MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided);); (136, MPI, MPI_Send(x.data(), chunk, MPI_DOUBLE, 0, 42, comm);); (2, MPI, MPI_Finalize();); (65, MPI, MPI_Allgather(&chunk, 1, MPI_INT, &domain[1], 1, MPI_INT, comm);)"}
{"predict": "Check that the attribute copy function is called */\n109:         MPI_Comm_set_attr(lo_comm, MPI_TAG_UB, &value);\n110:         MPI_Comm_dup(lo_comm, &dup_comm);\n111:         MPI_Comm_get_attr(dup_comm, MPI_TAG_UB, &vvalue, &flag);\n112:         if (!flag) {\n113:             errs++;\n114:             printf(\"Attribute not set in dup_comm\\n\");\n115:             fflush(stdout);\n116:         } else if (*(MPI_Aint *)vvalue != value) {\n117:             errs++;\n118:             printf(\"Attribute value not set correctly in dup_comm\\n\");\n119:             fflush(stdout);\n120:         }\n121:         MPI_Comm_free(&dup_comm);\n122:     }\n123: \n124:     MPI_Barrier(world_comm);\n125: \n126:     /*\n127:      * Exercise Comm_split by creating a communicator that reverses the\n128:      * rank order of the world communicator\n129:      */\n130: #ifdef DEBUG\n131:     if (world_rank == 0) {\n132:         printf(\"    Comm_split\\n\");\n133:         fflush(stdout);\n134:     }\n135: #endif\n136: \n137:     color = 0;\n138:     key = world_size - world_rank - 1;\n139:     MPI_Comm_split(world_comm, color, key, &rev_comm);\n140: \n141:     MPI_Comm_rank(rev_comm, &rank);\n142:     if (rank != key) {\n143:         errs++;\n144:         printf(\"incorrect rank in rev_comm: %d\\n\", rank);\n145:         fflush(stdout);\n146:     }\n147: \n148:     MPI_Comm_size(rev_comm, &size);\n149:     if (size != world_size) {\n150:         errs++;\n151:         printf(\"incorrect size in rev_comm: %d\\n\", size);\n152:         fflush(stdout);\n153:     }\n154: \n155:     MPI_Barrier(re", "label": "(172, MPI, MPI_Comm_split(dup_comm_world, color, key, &split_comm);); (120, MPI, MPI_Attr_put(lo_comm, key_1, (void *) (MPI_Aint) world_rank);); (111, MPI, MPI_Keyval_create(copy_fn, delete_fn, &key_1, &value);); (142, MPI, MPI_Abort(MPI_COMM_WORLD, 3005);); (118, MPI, MPI_Attr_put(lo_comm, key_3, (void *) 0);); (238, MPI, MPI_Comm_free(&split_comm);); (210, MPI, MPI_Comm_compare(world_comm, rev_comm, &result);); (28, MPI, MPI_Comm_create(dup_comm_world, world_group, &world_comm);); (28, MPI, MPI_Comm_rank(world_comm, &rank);); (2, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (114, MPI, MPI_Comm_dup(lo_comm, &dup_comm);); (146, MPI, MPI_Keyval_free(&key_3);); (230, MPI, MPI_Comm_free(&rev_comm);); (135, MPI, MPI_Attr_get(dup_comm, key_3, (void **) &vvalue, &flag);); (210, MPI, MPI_Comm_compare(world_comm, lo_comm, &result);); (190, MPI, MPI_Abort(MPI_COMM_WORLD, 3011);); (71, MPI, MPI_Abort(MPI_COMM_WORLD, 3003);); (228, MPI, MPI_Group_free(&world_group);); (155, MPI, MPI_Comm_size(split_comm, &size);); (-7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &world_size);); (194, MPI, MPI_Comm_create(world_comm, rev_group, &rev_comm);); (135, MPI, MPI_Keyval_free(&key_1);); (157, MPI, MPI_Abort(MPI_COMM_WORLD, 3009);); (190, MPI, MPI_Group_range_incl(world_group, 1, ranges, &rev_group);); (92, MPI, MPI_Keyval_create(MPI_NULL_COPY_FN, MPI_NULL_DELETE_FN, &key_3, &value);); (129, MPI, MPI_Abort(MPI_COMM_WORLD, 3008);); (148, MPI, MPI_Comm_rank(split_comm, &rank);); (201, MPI, MPI_Abort(MPI_COMM_WORLD, 3013);); (173, MPI, MPI_Comm_compare(lo_comm, dup_comm, &result);); (47, MPI, MPI_Abort(MPI_COMM_WORLD, 3002);); (152, MPI, MPI_Barrier(split_comm);); (40, MPI, MPI_Comm_rank(lo_comm, &rank);); (106, MPI, MPI_Abort(MPI_COMM_WORLD, 3004);); (213, MPI, MPI_Group_free(&rev_group);); (163, MPI, MPI_Abort(MPI_COMM_WORLD, 3010);); (205, MPI, MPI_Comm_free(&dup_comm_world);); (4, MPI, MPI_Abort(MPI_COMM_WORLD, 3001);); (213, MPI, MPI_Comm_free(&dup_comm);); (41, MPI, MPI_Barrier(lo_comm);); (210, MPI, MPI_Comm_free(&lo_comm);); (91, MPI, MPI_Attr_get(dup_comm, key_1, (void **) &vvalue, &flag);); (179, MPI, MPI_Abort(MPI_COMM_WORLD, 3012);)"}
{"predict": "128:         __syncthreads();\n129: \n130:         if (do_rotary)\n131:         {\n132:             mmha::vec_from_smem_transpose(k, k_smem, transpose_idx, smem_pitch);\n133:             mmha::apply_rotary_embedding(k, transpose_idx / tidx_factor, rotary_embedding_dim, rotary_embedding_base,\n134:                 rotary_embedding_scale, token_pos_idx);\n135:         }\n136: \n137:         __syncthreads();\n138: \n139:         if (do_rotary)\n140:         {\n141:             *reinterpret_cast<Vec_k*>(k_smem + half_idx * smem_pitch + intra_half_idx) = k;\n142:         }\n143: \n144:         __syncthreads();\n145:     }\n146:     break;\n147:     }\n148:     default:\n149:     {\n150:         TLLM_CHECK_WITH_INFO(false, \"Unsupported position embedding type.\");\n151:     }\n152:     }\n153: \n154:     // Write k cache\n155:     int outBlockIdx_r = kvCacheBuffer.getKVLocalIdx(token_idx, head_idx, sizePerHead, tidx * vec_size);\n156:     Vec_k_cache* k_cache_out_batch = reinterpret_cast<Vec_k_cache*>(kvCacheBuffer.getKBlockPtr(batch_beam_idx, token_idx));\n157:     *reinterpret_cast<Vec_k_cache*>(&k_cache_out_batch[outBlockIdx_r]) = k_cache;\n158: \n159:     // Write shifted k cache\n160:     int outBlockIdx_shifted = kvCacheBuffer.getKVLocalIdx(token_idx, head_idx, sizePerHead, tidx * vec_size);\n161:     Vec_k_cache* k_cache_shifted_batch = reinterpret_cast<Vec_k_cache*>(shiftKCacheBuffer.getKBlockPtr(batch_beam_idx, token_idx));\n162:     *reinterpret_cast<Vec_k_cache*>(&k_cache_shifted_batch[outBlockIdx_shifted]) = k_cache;\n163: \n164:     return;\n165: }", "label": "(16, CUDA, extern __shared__ __align__(sizeof(float2)) char smem_[];)"}
{"predict": "halo exchange buffers between GPUs\n90:     for (int dev_id = 0; dev_id < num_devices; ++dev_id) {\n91:         const int top = dev_id > 0 ? dev_id - 1 : (num_devices - 1);\n92:         const int bottom = (dev_id + 1) % num_devices;\n93: \n94:         if (p2p == true) {\n95:             CUDA_RT_CALL(cudaMemcpyPeerAsync(\n96:                 a[dev_id] + iy_start[dev_id], dev_id, a[bottom] + iy_end[bottom] - 1, bottom, 1 * nx,\n97:                 push_top_stream[dev_id]));\n98:             CUDA_RT_CALL(cudaMemcpyPeerAsync(\n99:                 a[dev_id] + iy_end[dev_id], dev_id, a[top] + iy_start[top], top, 1 * nx,\n100:                 push_bottom_stream[dev_id]));\n101:         } else {\n102:             CUDA_RT_CALL(cudaMemcpyAsync(\n103:                 a[dev_id] + iy_start[dev_id], a[top] + iy_end[top] - 1, 1 * nx, cudaMemcpyDeviceToDevice,\n104:                 push_top_stream[dev_id]));\n105:             CUDA_RT_CALL(cudaMemcpyAsync(\n106:                 a[dev_id] + iy_end[dev_id], a[bottom] + iy_start[bottom], 1 * nx, cudaMemcpyDeviceToDevice,\n107:                 push_bottom_stream[dev_id]));\n108:         }\n109:     }\n110: \n111:     // Perform computation on each GPU\n112:     for (int iter = 0; iter < iter_max; ++iter) {\n113:         for (int dev_id = 0; dev_id < num_devices; ++dev_id) {\n114:             CUDA_RT_CALL(cudaStreamWaitEvent(push_top_stream[dev_id], halo_top_event[dev_id], 0));\n115:             CUDA_RT_CALL(cudaStreamWaitEvent(push_bottom_stream[dev_id], halo_bottom_event[dev_id], 0));\n116: \n117:             compute<<<(chunk_size[dev_id] + 1) / 128 + 1, 1", "label": "(207, OpenMP, double stop = omp_get_wtime();); (146, CUDA, CUDA_RT_CALL(cudaStreamSynchronize(push_top_stream[bottom]));); (77, CUDA, CUDA_RT_CALL(cudaDeviceSynchronize());); (28, CUDA, CUDA_RT_CALL(cudaFree(0));); (88, CUDA, CUDA_RT_CALL(cudaDeviceCanAccessPeer(&canAccessPeer, dev_id, top));); (250, CUDA, CUDA_RT_CALL(cudaFreeHost(a_h));); (29, CUDA, CUDA_RT_CALL(cudaMallocHost(&a_ref_h, nx * ny * sizeof(float)));); (71, CUDA, CUDA_RT_CALL(cudaGetLastError());); (91, CUDA, CUDA_RT_CALL(cudaDeviceCanAccessPeer(&canAccessPeer, dev_id, bottom));); (92, CUDA, CUDA_RT_CALL(cudaDeviceEnablePeerAccess(bottom, 0));); (235, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_bottom_stream[dev_id]));); (83, CUDA, CUDA_RT_CALL(cudaDeviceEnablePeerAccess(top, 0));); (74, CUDA, CUDA_RT_CALL(cudaMalloc(l2_norm_d + dev_id, sizeof(float)));); (44, CUDA, CUDA_RT_CALL(cudaMalloc(a + dev_id, nx * (chunk_size[dev_id] + 2) * sizeof(float)));); (157, CUDA, CUDA_RT_CALL(cudaStreamSynchronize(compute_stream[dev_id]));); (45, CUDA, CUDA_RT_CALL(cudaMemset(a[dev_id], 0, nx * (chunk_size[dev_id] + 2) * sizeof(float)));); (132, CUDA, CUDA_RT_CALL(cudaStreamSynchronize(push_bottom_stream[top]));); (9, CUDA, CUDA_RT_CALL(cudaGetDeviceCount(&num_devices));); (228, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_top_stream[dev_id]));); (11, CUDA, CUDA_RT_CALL(cudaSetDevice(dev_id));); (38, CUDA, CUDA_RT_CALL(cudaMalloc(a_new + dev_id, nx * (chunk_size[dev_id] + 2) * sizeof(float)));); (228, CUDA, CUDA_RT_CALL(cudaFreeHost(l2_norm_h[dev_id]));); (230, CUDA, CUDA_RT_CALL(cudaFree(a_new[dev_id]));); (59, CUDA, CUDA_RT_CALL(cudaStreamCreate(compute_stream + dev_id));); (60, CUDA, CUDA_RT_CALL(cudaStreamCreate(push_bottom_stream + dev_id));); (36, CUDA, CUDA_RT_CALL(cudaMemset(a_new[dev_id], 0, nx * (chunk_size[dev_id] + 2) * sizeof(float)));); (230, CUDA, CUDA_RT_CALL(cudaFreeHost(a_ref_h));); (9, CUDA, CUDA_RT_CALL(cudaMallocHost(&a_h, nx * ny * sizeof(float)));); (219, CUDA, CUDA_RT_CALL(cudaStreamDestroy(compute_stream[dev_id]));); (102, OpenMP, double start = omp_get_wtime();); (57, CUDA, CUDA_RT_CALL(cudaMallocHost(l2_norm_h + dev_id, sizeof(float)));); (222, CUDA, CUDA_RT_CALL(cudaFree(a[dev_id]));); (218, CUDA, CUDA_RT_CALL(cudaFree(l2_norm_d[dev_id]));); (50, CUDA, CUDA_RT_CALL(cudaStreamCreate(push_top_stream + dev_id));)"}
{"predict": "shape.Y_shape[2], shape.Y_shape[3])); \\\n145:         \\\n146:         hiednnTensorDesc_t zDesc; \\\n147:         CHECK_HIEDNN(hiednnCreateTensorDesc(&zDesc)); \\\n148:         CHECK_HIEDNN(hiednnSet4dTensorDesc( \\\n149:             zDesc, OUTTYPE, Z_TENSORFORMAT, \\\n150:             shape.Z_shape[0], shape.Z_shape[1], \\\n151:             shape.Z_shape[2], shape.Z_shape[3])); \\\n152:         \\\n153:         CHECK_HIEDNN(hiednnCreateCudaHandle(&handle)); \\\n154:         \\\n155:         cudaMalloc((void**)&x_cu, sizeof(ST) * shape.X_len); \\\n156:         cudaMalloc((void**)&y_cu, sizeof(ST) * shape.Y_len); \\\n157:         cudaMalloc((void**)&z_cu, sizeof(DT) * shape.Z_len); \\\n158:         \\\n159:         cudaMemcpy(x_cu, x, sizeof(ST) * shape.X_len, cudaMemcpyHostToDevice); \\\n160:         cudaMemcpy(y_cu, y, sizeof(ST) * shape.Y_len, cudaMemcpyHostToDevice); \\\n161:         cudaMemcpy(z_cu, z, sizeof(DT) * shape.Z_len, cudaMemcpyHostToDevice); \\\n162:         \\\n163:         CHECK_HIEDNN(hiednnBinaryOp(handle, HIE_BINARY_OP, &extParam, \\\n164:                                     xDesc, x_cu, yDesc, y_cu, zDesc, z_cu, \\\n165:                                     alpha, beta)); \\\n166:         \\\n167:         cudaMemcpy(z, z_cu, sizeof(DT) * shape.Z_len, cudaMemcpyDeviceToHost); \\\n168:         \\\n169:         for (int i = 0; i < shape.Z_len; ++i) { \\\n170:             DT ref = STD_EXPR; \\\n171:             EXPECT_NEAR(ref, z[i], 1e-4); \\\n172:         } \\\n173:         \\\n174:         cudaFree(x_cu); \\\n175:         cudaFree(y_cu); \\\n176:         cudaFree(z_cu); \\\n177", "label": "(129, CUDA, cudaMemcpy(z_cu, z, shape.Z_len * sizeof(DT), cudaMemcpyHostToDevice);); (221, CUDA, cudaFree(x_cu);); (122, CUDA, cudaMalloc(&x_cu, shape.X_len * sizeof(ST));); (125, CUDA, cudaMemcpy(y_cu, y, shape.Y_len * sizeof(ST), cudaMemcpyHostToDevice);); (123, CUDA, cudaMemcpy(x_cu, x, shape.X_len * sizeof(ST), cudaMemcpyHostToDevice);); (218, CUDA, cudaFree(y_cu);); (120, CUDA, cudaMalloc(&z_cu, shape.Z_len * sizeof(DT));); (118, CUDA, cudaMalloc(&y_cu, shape.Y_len * sizeof(ST));); (128, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (220, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (214, CUDA, cudaFree(z_cu);)"}
{"predict": "(21,OpenMP,#pragma omp parallel for private(Csub, tile_idx) reduction(+:Csub))", "label": "(22, CUDA, __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];); (20, CUDA, __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];)"}
{"predict": "18: \t                          FunCallStream) );\n119: \n120: \tcheckCuda(cudaMemcpyAsync(A_gpu->scubufs[streamId].usub, usub,\n121: \t                          usub_len * sizeof(int_t), cudaMemcpyHostToDevice,\n122: \t                          FunCallStream) );\n123: \n124: \tcheckCuda(cudaDeviceSynchronize());\n125: \n126: \tdouble tTmp1 = SuperLU_timer_();\n127: \n128: \t/* update the diagonal block */\n129: \tint_t *d_xsup = dA_gpu->xsup_host;\n130: \tint_t *d_lsub = dA_gpu->lsub_host;\n131: \tint_t *d_usub = dA_gpu->usub_host;\n132: \tdouble *d_Llu = dA_gpu->Llu_host;\n133: \tdouble *d_bigU = dA_gpu->bigU_host;\n134: \tdouble *d_Ucol = dA_gpu->Ucol_host;\n135: \tdouble *d_Lcol = dA_gpu->Lcol_host;\n136: \tdouble *d_Lrow = dA_gpu->Lrow_host;\n137: \tdouble *d_work = dA_gpu->work_host;\n138: \tdouble *d_diag = dA_gpu->diag_host;\n139: \tdouble *d_work_host = dA_gpu->work_host;\n140: \n141: \t/* Sherry: can get max_super_size from sp_ienv(3) */\n142: \tint_t indirectJ1_d[MAX_SUPER_SIZE]; // 0 indicates an empry segment\n143: \tint_t indirectJ2_d[MAX_SUPER_SIZE]; // # of nonzero segments so far\n144: \tint_t indirectJ3_d[MAX_SUPER_SIZE]; /* indirectJ3[j] == k means the\n145: \t\t\t\t\t j-th nonzero segment points\n146: \t\t\t\t\t to column k in this supernode */\n147: \t/* calculate usub_indirect */\n148: \tfor (int jj = jj_cpu; jj < nub; ++jj)\n149: \t{\n150: \t\tint_t iukp = dA", "label": "(93, CUDA, cudaEventRecord(A_gpu->ePCIeH2D[k0], FunCallStream);); (231, CUDA, cudaEventRecord(A_gpu->GemmStart[k0], FunCallStream);); (239, CUDA, cudaStreamSynchronize(FunCallStream);)"}
{"predict": "(5,OpenMP,#pragma omp parallel for reduction(+:sum))", "label": "(6, OpenMP, #pragma omp atomic  hint(AMD_safe_fp_atomics)); (34, OpenMP, #pragma omp atomic  hint(AMD_unsafe_fp_atomics)); (2, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:sum)); (17, OpenMP, #pragma omp atomic  hint(AMD_fast_fp_atomics))"}
{"predict": "(19,OpenMP,#pragma omp parallel for if(num > 1))", "label": "(6, CUDA, extern __shared__ float entire_buffer[];)"}
{"predict": "(6,OpenMP,#pragma omp parallel for)", "label": "(37, CUDA, CHECK(cudaFree(d_y));); (14, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice));); (36, CUDA, CHECK(cudaFree(d_z));); (10, CUDA, CHECK(cudaMalloc(&d_y, M));); (8, CUDA, CHECK(cudaMalloc(&d_x, M));); (31, CUDA, CHECK(cudaFree(d_x));); (8, CUDA, CHECK(cudaMalloc(&d_z, M));); (9, CUDA, CHECK(cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice));)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for)", "label": "(10, CUDA, cudaDeviceSynchronize();); (5, CUDA, cudaMalloc((void **)&sm_o, sizeof(float) * m * m);); (35, CUDA, cudaFree(sm_o);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for), (47,OpenMP,#pragma omp parallel for)", "label": "(49, MPI, MPI_Win_unlock(1, win);); (32, MPI, MPI_Win_unlock(rank, win);); (45, MPI, MPI_Win_lock(MPI_LOCK_SHARED, 1, 0, win);); (68, MPI, MPI_Type_free(&vector_dtp);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (64, MPI, MPI_Win_free(&win);); (41, MPI, MPI_Accumulate(orig_buf, 1, vector_dtp, 1, 0, 1, vector_dtp, MPI_MAXLOC, win);); (14, MPI, MPI_Type_commit(&vector_dtp);); (64, MPI, MPI_Free_mem(orig_buf);); (65, MPI, MPI_Free_mem(tar_buf);); (17, MPI, MPI_Win_lock(MPI_LOCK_SHARED, rank, 0, win);); (22, MPI, MPI_Barrier(MPI_COMM_WORLD);); (1, MPI, MPI_Alloc_mem(sizeof(long_double_int_t) * DATA_SIZE, MPI_INFO_NULL, &orig_buf);)"}
{"predict": "(1,OpenMP,#pragma omp parallel for private(localid, globalid, batch, cols, rows, threadCol, h, w, val) reduction(+:histogram))", "label": "(2, CUDA, __shared__ int hist[256];)"}
{"predict": "(73,OpenMP,#pragma omp parallel for collapse(3)), (83,OpenMP,#pragma omp parallel for collapse(2))", "label": "(74, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(test.data(), dstDev->basePtr(), test.size(), cudaMemcpyDeviceToHost));); (66, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (50, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(srcDev->basePtr(), srcVec.data(), srcVec.size(), cudaMemcpyHostToDevice));); (60, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (64, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamDestroy(stream));)"}
{"predict": "(31,OpenMP,#pragma omp parallel for)", "label": "(18, OpenCL, clCreateCommandQueueWithProperties_StubWithCallback(clCreateCommandQueueWithProperties_testCommandQueueFromSpecifiedContext);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for)", "label": "(25, OpenCL, clReleaseEvent(outEvent);)"}
{"predict": "NULL,\n149:                                              &vectorCount, NULL, 0, NULL,\n150:                                              NULL)))\n151:         {\n152:             vlog_error(\"ERROR: clEnqueueNDRangeKernel failed!\\n\");\n153:             return error;\n154:         }\n155:     }\n156: \n157:     // Wait for all kernels to finish\n158:     if ((error = clFinish(tinfo->tQueue)))\n159:     {\n160:         vlog_error(\"ERROR: clFinish failed!\\n\");\n161:         return error;\n162:     }\n163: \n164:     // Read back the results\n165:     for (j = gMinVectorSizeIndex; j < gMaxVectorSizeIndex; j++)\n166:     {\n167:         out[j] = (cl_ushort *)clEnqueueMapBuffer(\n168:             tinfo->tQueue, tinfo->outBuf[j], CL_TRUE, CL_MAP_READ, 0,\n169:             buffer_size, 0, NULL, NULL, &error);\n170:         if (error || NULL == out[j])\n171:         {\n172:             vlog_error(\"Error: clEnqueueMapBuffer %d failed! err: %d\\n\", j,\n173:                        error);\n174:             return error;\n175:         }\n176:     }\n177: \n178:     // Verify the results\n179:     for (j = gMinVectorSizeIndex; j < gMaxVectorSizeIndex; j++)\n180:     {\n181:         r = (cl_half *)out[j];\n182:         t = (cl_ushort *)out[j];\n183:         s.resize(buffer_elements);\n184:         s2.resize(buffer_elements);\n185:         for (cl_uint k = 0; k < buffer_elements; k++)\n186:         {\n187:             s[k] = convert_half_to_float(t[k]);\n188:             s2[k] = convert_half_to_float(r[k]);\n189:         }\n190: \n191:         if (!VerifyHalfResults(func, ul", "label": "(142, OpenCL, error = clSetKernelArg(kernel, 1, sizeof(tinfo->inBuf), &tinfo->inBuf);)"}
{"predict": "loop over the experts\n135:     // Each expert has a number of tokens assigned to it.\n136:     // The number of tokens per expert is known at this point.\n137:     // Each thread takes care of one expert.\n138:     int32_t expertOffset = blockExpertOffset;\n139:     int32_t expertIdx = threadIdx.x;\n140: \n141:     // Strided loop over the experts\n142:     for (; expertIdx < params.mNumExperts; expertIdx += NumThreads)\n143:     {\n144:         // Compute the number of tokens assigned to this expert.\n145:         int32_t expertNumTokens = smemExpertCount[expertIdx];\n146:         // Skip empty experts.\n147:         if (expertNumTokens == 0)\n148:         {\n149:             continue;\n150:         }\n151: \n152:         // Compute the number of CTAs needed for this expert.\n153:         int32_t expertNumCta = divUpLog2<int32_t>(expertNumTokens, params.mPaddingLog2);\n154:         int32_t expertCtaOffset;\n155:         int32_t expertNumNonExitingCtas;\n156:         Scan(tempStorage).ExclusiveSum(expertNumCta, expertCtaOffset, expertNumNonExitingCtas);\n157: \n158:         // Skip experts that do not have any CTAs assigned to them.\n159:         if (expertNumCta == 0)\n160:         {\n161:             continue;\n162:         }\n163: \n164:         // Compute the index of the first token assigned to this expert.\n165:         int32_t expertFirstTokenIdx = expertOffset;\n166:         // Compute the index of the last token assigned to this expert.\n167:         int32_t expertLastTokenIdx = expertFirstTokenIdx + expertNumTokens - 1;\n168: \n169:         // Strided loop over the CTAs\n170:         for (int32_t ctaIdx = expertCtaOffset; ctaIdx < expertCta", "label": "(10, CUDA, __shared__ typename Scan::TempStorage tempStorage;); (33, CUDA, cudaGridDependencySynchronize();); (4, CUDA, __shared__ int32_t __attribute((aligned(128))) smemExpertCount[NumThreads];); (4, CUDA, __shared__ int32_t __attribute((aligned(128))) smemExpertOffset[NumThreads];)"}
{"predict": "t0;\n133: \n134:   double avgtime = trans_time/iterations;\n135: \n136:   //////////////////////////////////////////////////////////////////////\n137:   /// Analyze and output results\n138:   //////////////////////////////////////////////////////////////////////\n139: \n140:   const double epsilon = 1.e-8;\n141:   double addit = (iterations+1.) * (double)order/2.;\n142:   double reference = (double)order + addit;\n143:   double residuum = 0.0;\n144: \n145:   for (int i=0; i<order; i++) {\n146:     for (int j=0; j<block_order; j++) {\n147:       residuum += fabs(B[i][j] - reference);\n148:     }\n149:   }\n150: \n151:   residuum /= (double)order * (double)order;\n152: \n153:   if (residuum < epsilon) {\n154:     printf(\"Solution validates\\n\");\n155:     const double nbytes = 2.0 * sizeof(double) * (double)order * (double)order;\n156:     printf(\"Rate (MB/s): %lf Avg time (s): %lf\\n\", 1.0e-6*nbytes/avgtime, avgtime);\n157:   }\n158:   else {\n159:     printf(\"ERROR: Residuum = %lf\\n\", residuum);\n160:   }\n161: \n162:   MPI_Finalize();\n163: \n164:   prk_free(A);\n165:   prk_free(B);\n166:   prk_free(T);\n167: \n168:   return 0;\n169: }", "label": "(9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &np);); (142, MPI, t1 = MPI_Wtime();); (20, MPI, MPI_Finalize();); (98, MPI, MPI_Type_commit(&trans_dt);); (144, MPI, MPI_Type_free(&stride_dt);); (159, MPI, MPI_Allreduce(MPI_IN_PLACE, &abserr, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);); (2, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (90, MPI, MPI_Type_size(MPI_DOUBLE,&dsize);); (141, MPI, MPI_Type_free(&trans_dt);); (70, MPI, MPI_Abort(MPI_COMM_WORLD,99);); (-5, MPI, if (provided < requested) MPI_Abort(MPI_COMM_WORLD,provided);); (89, MPI, MPI_Type_hvector(block_order, 1, dsize, stride_dt, &trans_dt);); (83, MPI, MPI_Type_vector(block_order, 1, block_order, MPI_DOUBLE, &stride_dt);); (54, MPI, MPI_Barrier(MPI_COMM_WORLD);); (96, MPI, t0 = MPI_Wtime();); (-11, MPI, MPI_Init_thread(&argc, &argv, requested, &provided);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for), (23,MPI,int rank, size; MPI_Comm_rank(MPI_COMM_WORLD, &rank); MPI_Comm_size(MPI_COMM_WORLD, &size);), (29,OpenMP,#pragma omp parallel for), (62,OpenMP,#pragma omp parallel for), (67,OpenMP,#pragma omp parallel for)", "label": "(55, CUDA, cudaMalloc((void**)&result_data, axis_size * sizeof(float));); (52, CUDA, cudaMalloc((void**)&result_data, tensor->size * sizeof(float));)"}
{"predict": "The provided code is a function that retrieves and displays information about an OpenCL platform. The function does not contain any loops or computationally intensive tasks that would benefit from parallelization. Therefore, there are no appropriate places in this code to insert parallelization instructions.\n\nIf you have a different code snippet or a section of code that involves loops or parallelizable tasks, please provide that, and I can help identify where to insert parallelization instructions.", "label": "(8, OpenCL, cl_status = clGetPlatformInfo(id, param_name, param_size, some_info, NULL);)"}
{"predict": "(15,OpenMP,#pragma omp parallel for private(ranks))", "label": "(37, MPI, MPI_Group_free(&comm_group);); (29, MPI, mpi_errno = MPI_Comm_create(MPI_COMM_WORLD, comm_group, &comm_hdls[i]);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (8, MPI, MPI_Comm_group(MPI_COMM_WORLD, &world_group);); (45, MPI, MPI_Group_free(&world_group);); (8, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(7,OpenMP,#pragma omp parallel for private(offset)) (31,OpenMP,#pragma omp parallel for private(offset,j))", "label": "(6, MPI, MPI_Comm_size(comm, &size);); (4, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(19,OpenMP,#pragma omp parallel for collapse(2) if (!DoY)), (39,OpenMP,#pragma omp parallel for collapse(2))", "label": "(15, CUDA, __shared__ float rr[TILE_T][N], kk[TILE_T][N];)"}
{"predict": "(13,OpenMP,#pragma omp parallel for private(nameLen, descLen, numCvars, numPvars, numSubcat, name, desc, cvarIndex, pvarIndex, subcatIndex, j, varnameLen, verb, binding, scope, datatype, varname, isReadonly, isContinuous, isAtomic, catnameLen, ncvars, npvars, nsubcats, catname))", "label": "(51, MPI, MPI_T_category_get_pvars(i, numPvars, pvarIndex);); (28, MPI, MPI_T_category_get_cvars(i, numCvars, cvarIndex);); (3, MPI, MPI_T_category_get_num(&numCat);); (72, MPI, MPI_T_category_get_categories(i, numSubcat, subcatIndex);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(13, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "*************************************************\n115:      * Create local A with NR_loc format             *\n116:      *************************************************/\n117: \n118:     sCreate_CompRow_Matrix_dist(&GA, m_loc, n, nnz_loc, nzval_loc, rowptr,\n119:                                 colind, SLU_NR_loc, SLU_S, SLU_GE);\n120: \n121:     /* Free remaining space. */\n122:     SUPERLU_FREE(rowind);\n123:     SUPERLU_FREE(colptr);\n124:     SUPERLU_FREE(marker);\n125: \n126:     /* Compute the RHS and solution. */\n127:     *rhs = b_global;\n128:     *x = xtrue_global;\n129:     *ldb = *ldx = m;\n130: \n131:     return 0;\n132: }", "label": "(33, MPI, MPI_Bcast( nzval,  nnz, MPI_FLOAT, 0, grid->comm );); (33, MPI, MPI_Bcast( rowind, nnz, mpi_int_t,  0, grid->comm );); (45, MPI, MPI_Bcast( colptr,  n+1, mpi_int_t,  0, grid->comm );); (43, MPI, MPI_Bcast( rowind,  nnz, mpi_int_t,  0, grid->comm );); (34, MPI, MPI_Bcast( &m,   1,   mpi_int_t,  0, grid->comm );); (26, MPI, MPI_Bcast( &n,     1,   mpi_int_t,  0, grid->comm );); (24, MPI, MPI_Bcast( &m,     1,   mpi_int_t,  0, grid->comm );); (33, MPI, MPI_Bcast( &nnz, 1,   mpi_int_t,  0, grid->comm );); (27, MPI, MPI_Bcast( colptr, n+1, mpi_int_t,  0, grid->comm );); (23, MPI, MPI_Bcast( &nnz,   1,   mpi_int_t,  0, grid->comm );); (29, MPI, MPI_Bcast( &n,   1,   mpi_int_t,  0, grid->comm );); (34, MPI, MPI_Bcast( nzval,   nnz, MPI_FLOAT, 0, grid->comm );)"}
{"predict": "(4,OpenMP,#pragma omp parallel for)", "label": "(15, TBB, bool ok = tbb::finalize(schBlock, std::nothrow);); (11, TBB, bool ok = tbb::finalize(schBlock1, std::nothrow);); (4, TBB, tbb::task_scheduler_handle schBlock1;)"}
{"predict": "(11,OpenMP,#pragma omp task)", "label": "(9, MPI, MPI_Type_free(&(c->key));)"}
{"predict": "(4,OpenMP,#pragma omp parallel sections), (6,OpenMP,#pragma omp section), (12,OpenMP,#pragma omp section), (18,OpenMP,#pragma omp section), (24,OpenMP,#pragma omp section), (31,OpenMP,#pragma omp section), (37,OpenMP,#pragma omp section), (43,OpenMP,#pragma omp section), (49,OpenMP,#pragma omp section), (55,OpenMP,#pragma omp section), (63,OpenMP,#pragma omp section)", "label": "(2, OpenMP, #pragma omp dispatch)"}
{"predict": "(23,OpenMP,#pragma omp parallel for)", "label": "(16, OpenCL, status = clSetKernelArg(global_check, 0, sizeof(cl_mem), &buffer);)"}
{"predict": "9:   for (int i = 0; i < 10; ++i)\n80:     foo();\n81:   for (int i = 0; i < 10; ++i)\n82:     foo();\n83:   for (int i = 0; i < 10; ++i)\n84:     foo();\n85:   for (int i = 0; i < 10; ++i)\n86:     foo();\n87:   for (int i = 0; i < 10; ++i)\n88:     foo();\n89:   for (int i = 0; i < 10; ++i)\n90:     foo();\n91:   for (int i = 0; i < 10; ++i)\n92:     foo();\n93:   for (int i = 0; i < 10; ++i)\n94:     foo();\n95:   for (int i = 0; i < 10; ++i)\n96:     foo();\n97:   for (int i = 0; i < 10; ++i)\n98:     foo();\n99:   for (int i = 0; i < 10; ++i)\n100:    foo();\n101: }\n(13,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (29,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for), (35,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for), (39,OpenMP,#pragma omp parallel for), (41,OpenMP,#pragma omp parallel for), (43,OpenMP,#pragma omp parallel for), (45,OpenMP,#pragma omp parallel for), (47,OpenMP", "label": "(45, OpenMP, #pragma omp master taskloop simd reduction(&& : argc, z)); (101, OpenMP, #pragma omp master taskloop simd reduction(max : j)); (67, OpenMP, #pragma omp master taskloop simd reduction(^ : fl)); (111, OpenMP, #pragma omp master taskloop simd reduction(+ : m)); (20, OpenMP, #pragma omp master taskloop simd reduction()); (46, OpenMP, #pragma omp master taskloop simd reduction(+ : a, b, c, d, f)); (48, OpenMP, #pragma omp master taskloop simd reduction(min : a, b, c, d, f)); (53, OpenMP, #pragma omp master taskloop simd reduction(+ : ba)); (34, OpenMP, #pragma omp master taskloop simd reduction(~ : argc)); (100, OpenMP, #pragma omp parallel reduction(* : fl)); (17, OpenMP, #pragma omp master taskloop simd reduction(*)); (1, OpenMP, #pragma omp master taskloop simd reduction); (63, OpenMP, #pragma omp master taskloop simd reduction(&& : S2::S2sc)); (68, OpenMP, #pragma omp master taskloop simd reduction(+ : h, k, B::x)); (106, OpenMP, #pragma omp master taskloop simd reduction(task, + : m)); (75, OpenMP, #pragma omp parallel private(k)); (62, OpenMP, #pragma omp master taskloop simd reduction(& : e, g)); (67, OpenMP, #pragma omp master taskloop simd reduction(+ : o)); (-3, OpenMP, #pragma omp master taskloop simd reduction +); (72, OpenMP, #pragma omp master taskloop simd reduction(+ : p), reduction(+ : p)); (43, OpenMP, #pragma omp master taskloop simd reduction(* : ca)); (9, OpenMP, #pragma omp master taskloop simd reduction(\\)); (14, OpenMP, #pragma omp master taskloop simd reduction(| : argc,); (78, OpenMP, #pragma omp parallel reduction(min : i)); (81, OpenMP, #pragma omp parallel private(fl)); (72, OpenMP, #pragma omp master taskloop simd reduction(+ : r)); (46, OpenMP, #pragma omp master taskloop simd reduction(&& : S2::S2s)); (30, OpenMP, #pragma omp master taskloop simd reduction(max : h.b)); (20, OpenMP, #pragma omp master taskloop simd reduction(^ : S1)); (10, OpenMP, #pragma omp master taskloop simd reduction(|| : argc > 0 ? argv[1] : argv[2])); (-12, OpenMP, #pragma omp master taskloop simd reduction(); (56, OpenMP, #pragma omp master taskloop simd private(i), reduction(+ : j), reduction(+ : q)); (85, OpenMP, #pragma omp master taskloop simd reduction(+ : m) nogroup); (-12, OpenMP, #pragma omp master taskloop simd reduction(-); (66, OpenMP, #pragma omp parallel shared(i)); (-2, OpenMP, #pragma omp master taskloop simd reduction(foo : argc); (30, OpenMP, #pragma omp master taskloop simd reduction(- : da)); (69, OpenMP, #pragma omp master taskloop simd reduction(+ : fl))"}
{"predict": "(24,OpenMP,#pragma omp parallel for)", "label": "(18, CUDA, cudaMemcpy(d_in, in.data(), in_bytes, cudaMemcpyHostToDevice);); (32, CUDA, cudaFree(d_in);); (13, CUDA, cudaMalloc((void**)(&d_in), in_bytes);); (13, CUDA, cudaMalloc((void**)(&d_out_min), out_bytes);); (25, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (23, CUDA, assert(cudaMemcpyAsync(out_min.data(), d_out_min, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);); (28, CUDA, cudaFree(d_out_min);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(3, CUDA, cudaExtent extent = make_cudaExtent(10, 20, 0);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(7, CUDA, err = cudaMemcpy( dst, src, sz, type );); (10, CUDA, ss << cudaGetErrorString(err) << endl;)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (9, MPI, MPI_Recv(&t, 1, MPI_DOUBLE, i, 11, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (1, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (11, MPI, MPI_Send(&t, 1, MPI_DOUBLE, 0, 11, MPI_COMM_WORLD);)"}
{"predict": "(1,MPI,MPI_Init(&argc, &argv);); (3,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (12,OpenMP,#pragma omp parallel for collapse(2)), (18,OpenMP,#pragma omp parallel for collapse(2)), (28,OpenMP,#pragma omp parallel for collapse(2)), (50,OpenMP,#pragma omp parallel for collapse(2)), (60,MPI,MPI_Finalize();)", "label": "(39, MPI, MPI_Type_commit(&columntype);); (7, MPI, MPI_Init(&argc, &argv);); (61, MPI, MPI_Finalize();); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (34, MPI, MPI_Type_vector(8, 1, 8, MPI_INT, &columntype);); (45, MPI, MPI_Type_free(&columntype);); (37, MPI, MPI_Send(&array[0][1], 1, columntype, 1, 1, MPI_COMM_WORLD);)"}
{"predict": "(27,OpenMP,#pragma omp parallel for), (91,OpenMP,#pragma omp parallel for reduction(+:err))", "label": "(41, SYCL, sycl::platform Plt = Dev.get_platform();)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (48,OpenMP,#pragma omp parallel for), (53,OpenMP,#pragma omp parallel for), (59,OpenMP,#pragma omp parallel for)", "label": "(45, OpenMP, #pragma omp teams distribute parallel for simd aligned(e, g)); (7, OpenMP, #pragma omp teams distribute parallel for simd aligned); (14, OpenMP, #pragma omp teams distribute parallel for simd aligned ()); (9, OpenMP, #pragma omp teams distribute parallel for simd aligned (); (45, OpenMP, #pragma omp teams distribute parallel for simd aligned(h)); (51, OpenMP, #pragma omp parallel); (18, OpenMP, #pragma omp teams distribute parallel for simd aligned (argc,); (66, OpenMP, #pragma omp teams distribute parallel for simd aligned(f:j)); (33, OpenMP, #pragma omp teams distribute parallel for simd aligned (argv[1])); (24, OpenMP, #pragma omp teams distribute parallel for simd aligned (argc : 5)); (43, OpenMP, #pragma omp teams distribute parallel for simd aligned(i)); (56, OpenMP, #pragma omp teams distribute parallel for simd aligned(f)); (17, OpenMP, #pragma omp teams distribute parallel for simd aligned (*argc > 0 ? argv[1] : argv[2])); (-6, OpenMP, #pragma omp target); (47, OpenMP, #pragma omp teams distribute parallel for simd aligned(v:16)); (22, OpenMP, #pragma omp teams distribute parallel for simd aligned (S1)); (4, OpenMP, #pragma omp teams distribute parallel for simd aligned (argc)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (9,OpenMP,#pragma omp parallel for)", "label": "(10, TBB, __TBB_STATIC_ASSERT( !HasImplicitConversionToInt< tbb::atomic<ScopedColor1> >::value, \"The implicit conversion is not expected.\" );); (12, TBB, __TBB_STATIC_ASSERT( sizeof(tbb::atomic<ScopedColor2>) == sizeof(ScopedColor2), \"tbb::atomic instantiated with scoped enum should have the same size as scoped enum.\" );); (10, TBB, __TBB_STATIC_ASSERT( sizeof(tbb::atomic<ScopedColor1>) == sizeof(ScopedColor1), \"tbb::atomic instantiated with scoped enum should have the same size as scoped enum.\" );)"}
{"predict": "(27,OpenMP,#pragma omp parallel for private(n, z, dmin, dmax)), (32,OpenMP,#pragma omp parallel for private(n, x, y, d, z, xs, ys, zs, xmin, xmax, ymin, ymax) reduction(min:xmin,xmax,ymin,ymax) reduction(max:dmin,dmax)), (53,OpenMP,#pragma omp parallel for private(n)), (66,OpenMP,#pragma omp parallel for private(n, tr, el, k))", "label": "(18, OpenMP, #pragma omp threadprivate (ident))"}
{"predict": "(4,OpenMP,#pragma omp parallel for),", "label": "(17, OpenMP, #pragma omp taskwait); (0, OpenMP, #pragma omp task); (2, OpenMP, #pragma omp parallel for)"}
{"predict": "(10,OpenMP,#pragma omp parallel for collapse(2)), (23,OpenMP,#pragma omp parallel for collapse(2)), (32,OpenMP,#pragma omp parallel for collapse(2)), (52,OpenMP,#pragma omp parallel for collapse(2))", "label": "(52, MPI, MPI_Type_free(&vector);); (6, MPI, MPI_Init(&argc, &argv);); (42, MPI, MPI_Type_create_resized(vector, 0, sizeof(int), &vector);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (64, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (37, MPI, MPI_Type_vector(8, 1, 6, MPI_INT, &vector);); (38, MPI, MPI_Type_commit(&vector);); (57, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(39,OpenMP,#pragma omp parallel for), (48,OpenMP,#pragma omp parallel for)", "label": "(66, CUDA, CUDA_CHECK(cudaFree(d_C));); (52, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (37, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_C), sizeof(data_type) * C.size()));); (69, CUDA, CUDA_CHECK(cudaDeviceReset());); (66, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (33, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (59, CUDA, CUDA_CHECK(cudaFree(d_A));); (27, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(31,OpenMP,#pragma omp parallel for)", "label": "(7, MPI, MPI_Comm_rank(team.comm, &rank);)"}
{"predict": "(3,MPI,MPI_Init(&argc, &argv);); (7,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (7,MPI,MPI_Comm_size(MPI_COMM_WORLD, &size);); (21,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for), (35,OpenMP,#pragma omp parallel for), (42,OpenMP,#pragma omp parallel for), (50,MPI,MPI_Finalize(););", "label": "(59, MPI, double end_time = MPI_Wtime();); (60, MPI, MPI_Finalize();); (47, MPI, MPI_Recv(recv_buff, msg_size, MPI_INT, 1, TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (17, MPI, MPI_Bcast(&msg_size, 1, MPI_INT, 0, MPI_COMM_WORLD);); (2, MPI, MPI_Init(NULL, NULL);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (13, MPI, MPI_Bcast(&nr_msgs, 1, MPI_INT, 0, MPI_COMM_WORLD);); (48, MPI, MPI_Ssend(send_buff, msg_size, MPI_INT, 0, TAG, MPI_COMM_WORLD);); (24, MPI, double start_time = MPI_Wtime();); (31, MPI, MPI_Recv(recv_buff, msg_size, MPI_INT, 0, TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (28, MPI, MPI_Ssend(send_buff, msg_size, MPI_INT, 1, TAG, MPI_COMM_WORLD);); (0, MPI, MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);)"}
{"predict": "(19,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for)", "label": "(5, CUDA, thrust::complex<Q> Bi;)"}
{"predict": "(60,OpenMP,#pragma omp parallel for collapse(2)), (...)", "label": "(51, CUDA, cudaError_t result = cudaGetLastError();)"}
{"predict": "(2,OpenMP,#pragma omp parallel for private(FooConstructed,FooDestroyed) reduction(+:FooConstructed,FooDestroyed)), (6,OpenMP,#pragma omp parallel for private(f,result) reduction(||:result)), (12,OpenMP,#pragma omp parallel for private(f,result) reduction(||:result))", "label": "(4, TBB, tbb::concurrent_queue<Foo> queue;)"}
{"predict": "(cudaEvent_t) * nsupers);\n90: \tA_gpu->eGemmStart = (cudaEvent_t *) malloc(sizeof(cudaEvent_t) * nsupers);\n91: \tA_gpu->eGemmEnd = (cudaEvent_t *) malloc(sizeof(cudaEvent_t) * nsupers);\n92: \tA_gpu->eScatterStart = (cudaEvent_t *) malloc(sizeof(cudaEvent_t) * nsupers);\n93: \tA_gpu->eScatterEnd = (cudaEvent_t *) malloc(sizeof(cudaEvent_t) * nsupers);\n94: \n95: \tint_t *isNodeInMyGrid_host = (int_t *) malloc (sizeof(int_t) * nsupers);\n96: \n97: \tint_t *isNodeInMyGrid_host_tmp = (int_t *) malloc (sizeof(int_t) * nsupers);\n98: \n99: \tfor (int_t i = 0; i < nsupers; i++ )\n100: \t{\n101: \t\tisNodeInMyGrid_host[i] = isNodeInMyGrid[i];\n102: \t}\n103: \n104: \tint_t *isNodeInMyGrid_host_gpu = (int_t *) malloc (sizeof(int_t) * nsupers);\n105: \n106: \tfor (int_t i = 0; i < nsupers; i++ )\n107: \t{\n108: \t\tisNodeInMyGrid_host_gpu[i] = isNodeInMyGrid_host[i];\n109: \t}\n110: \n111: \tfor (int_t i = 0; i < nsupers; i++ )\n112: \t{\n113: \t\tisNodeInMyGrid_host_tmp[i] = isNodeInMyGrid_host_gpu[i];\n114: \t}\n115: \n116: \tfor (int_t i = 0; i < nsupers; i++ )\n117: \t{\n118: \t\tisNodeInMyGrid_host[i] = isNodeInMyGrid_host_tmp[i];\n119: \t}\n120: \n121: \tfor (int_t i = 0; i < nsupers; i++ )\n122", "label": "(61, CUDA, checkCudaErrors(cudaMallocHost(  &tmp_ptr,  remain_l_max * sizeof(double) )) ;); (50, CUDA, checkCudaErrors(cudaMallocHost(  &tmp_ptr, (n) * sizeof(int_t) )) ;); (256, CUDA, checkCudaErrors(cudaMemcpy( (A_gpu->local_u_blk_infoVec), local_u_blk_infoVec, cum_num_u_blocks * sizeof(local_u_blk_info_t), cudaMemcpyHostToDevice)) ;); (476, CUDA, checkCudaErrors(cudaMemcpy( (A_gpu->UrowindVec), indtemp1, u_ind_len * sizeof(int_t), cudaMemcpyHostToDevice)) ;); (62, CUDA, checkCudaErrors(cudaMallocHost ( &tmp_ptr, sizeof(double) * (A_host->bufmax[1])));); (67, CUDA, checkCudaErrors(cudaMallocHost ( &tmp_ptr, sizeof(int_t) * (A_host->bufmax[2])));); (81, CUDA, checkCudaErrors(cudaMalloc(  &tmp_ptr,  buffer_size * sizeof(double))) ;); (179, CUDA, checkCudaErrors(cudaMemcpy( (A_gpu->local_l_blk_infoVec), local_l_blk_infoVec, cum_num_l_blocks * sizeof(local_l_blk_info_t), cudaMemcpyHostToDevice)) ;); (51, CUDA, checkCudaErrors(cudaMallocHost(  &tmp_ptr, mcb * sizeof(Ublock_info_t) )) ;); (174, CUDA, checkCudaErrors(cudaMalloc(  &tmp_ptr,  cum_num_l_blocks * sizeof(local_l_blk_info_t))) ;); (477, CUDA, checkCudaErrors(cudaMemcpy( (A_gpu->UrowindPtr), temp_UrowindPtr, u_k * sizeof(int_t), cudaMemcpyHostToDevice))"}
{"predict": "(6,OpenMP,#pragma omp parallel for private(i))", "label": "(5, CUDA, extern __shared__ uint64_t s_qiHat_mod_pj[];)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (51,OpenMP,#pragma omp teams distribute parallel for shared(i)), (54,OpenMP,#pragma omp teams distribute parallel for shared(j))", "label": "(26, OpenMP, #pragma omp teams distribute shared (argc)); (22, OpenMP, #pragma omp teams distribute shared (argc > 0 ? argv[1] : argv[2])); (39, OpenMP, #pragma omp teams distribute shared(ca)); (41, OpenMP, #pragma omp teams distribute shared(da)); (16, OpenMP, #pragma omp teams distribute shared (argc,); (3, OpenMP, #pragma omp teams distribute shared); (5, OpenMP, #pragma omp teams distribute shared (); (49, OpenMP, #pragma omp teams distribute firstprivate(i), shared(i)); (27, OpenMP, #pragma omp teams distribute shared (argv[1])); (-2, OpenMP, #pragma omp target); (55, OpenMP, #pragma omp teams distribute shared(j)); (57, OpenMP, #pragma omp teams distribute firstprivate(i)); (17, OpenMP, #pragma omp teams distribute shared (S1)); (37, OpenMP, #pragma omp teams distribute shared(h, B::x)); (18, OpenMP, #pragma omp teams distribute shared (a, b, c, d, f)); (2, OpenMP, #pragma omp teams distribute shared (argc); (46, OpenMP, #pragma omp teams distribute shared(i)); (21, OpenMP, #pragma omp teams distribute shared(ba)); (35, OpenMP, #pragma omp teams distribute private(i), shared(i)); (28, OpenMP, #pragma omp teams distribute shared(e, g)); (39, OpenMP, #pragma omp teams distribute private(i)); (-7, OpenMP, #pragma omp teams distribute shared ())"}
{"predict": "(40,OpenMP,#pragma omp parallel for)", "label": "(33, CUDA, cudaMalloc( (void**) &fn_d,  XYZ_SIZE*sizeof(FLOAT));); (30, OpenMP, start_time_total = omp_get_wtime();); (55, OpenMP, elapsed_time_total = omp_get_wtime() - start_time_total;); (33, OpenMP, start_computation_time = omp_get_wtime();); (50, OpenMP, elapsed_computation_time = omp_get_wtime() - start_computation_time;); (29, CUDA, cudaMemcpy(f_d,f,XYZ_SIZE*sizeof(FLOAT), cudaMemcpyHostToDevice);); (50, CUDA, cudaMemcpy(f, f_d,XYZ_SIZE*sizeof(FLOAT), cudaMemcpyDeviceToHost);); (25, CUDA, cudaMalloc( (void**) &f_d,  XYZ_SIZE*sizeof(FLOAT));); (51, CUDA, cudaFree(fn_d);); (49, CUDA, cudaFree(f_d);)"}
{"predict": "(49,OpenMP,#pragma omp parallel for)", "label": "(12, OpenMP, #pragma omp master); (9, OpenMP, #pragma omp parallel default(shared))"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for)", "label": "(10, MPI, MPI_Allreduce(MPI_IN_PLACE, buf, N, MPIX_BFLOAT16, MPI_SUM, comm);); (20, MPI, MPI_Allreduce(MPI_IN_PLACE, buf_f16, N, MPIX_C_FLOAT16, MPI_SUM, comm);)"}
{"predict": "(24,OpenMP,#pragma omp parallel for)", "label": "(32, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (34, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "155:   if (pow2 < ranks) {\n156:     if (rank <= cutoff) {\n157:       /* if we are an even rank under the cutoff, send the result to\n158:        * the odd rank out on our right */\n159:       if (!(rank & 0x1)) {\n160:         MPI_Send(\n161:           recvbuf, count, type, right_rank, LWGRP_MSG_TAG_0, comm\n162:         );\n163:       }\n164:     }\n165:   }\n166: \n167:   /* free our temporary buffer */\n168:   lwgrp_type_dtbuf_free(tempbuf, count, type, __FILE__, __LINE__);\n169: \n170:   return 0;\n171: }", "label": "(133, MPI, MPI_Waitall(k, request, status);); (90, MPI, MPI_Reduce_local(recvbuf, tempbuf, count, type, op);)"}
{"predict": "(17,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for)", "label": "(10, MPI, MPI_Comm_size(comm, &size);); (15, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (22, MPI, MPI_Allreduce(bufin, bufout, count, dtype, MPI_SUM, comm);)"}
{"predict": "(58,OpenMP,#pragma omp parallel for), (62,OpenMP,#pragma omp parallel for), (95,OpenMP,#pragma omp parallel for reduction(+:sum_delta2,sum_ref2))", "label": "(68, CUDA, checkCudaErrors(cudaMemset(d_Kernel, 0, DATA_SIZE));); (110, CUDA, checkCudaErrors(cudaFree(d_Kernel));); (40, CUDA, findCudaDevice(argc, (const char **)argv);); (72, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (49, CUDA, checkCudaErrors(cudaMalloc((void **)&d_Kernel, DATA_SIZE));); (105, CUDA, checkCudaErrors(cudaFree(d_Data));); (48, CUDA, checkCudaErrors(cudaMalloc((void **)&d_Data, DATA_SIZE));)"}
{"predict": "(13,OpenMP,#pragma omp parallel), (34,OpenMP,#pragma omp single), (53,OpenMP,#pragma omp parallel sections), (54,OpenMP,#pragma omp section), (56,OpenMP,#pragma omp section), (63,OpenMP,#pragma omp section), (73,OpenMP,#pragma omp parallel), (73,OpenMP,#pragma omp single)", "label": "(39, MPI, MPI_Error_string(mpirc, mpierrstr, &mpierrlen);); (98, MPI, MPI_Barrier(MPI_COMM_WORLD);); (22, MPI, int mpirc = MPI_File_open(MPI_COMM_WORLD, (char*)name, amode, MPI_INFO_NULL, &fh);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (79, MPI, double end_read = MPI_Wtime();); (3, MPI, double start_read = MPI_Wtime();); (67, MPI, mpirc = MPI_File_close(&fh);); (30, MPI, mpirc = MPI_File_set_view(fh, disp, MPI_BYTE, MPI_BYTE, datarep, MPI_INFO_NULL);); (47, MPI, MPI_Bcast(&version, 1, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (36, MPI, mpirc = MPI_File_read_at(fh, 0, &version_packed, 8, MPI_BYTE, &status);)"}
{"predict": ":             ADIOI_Free(len_list);\n110:         }\n111:         ADIOI_GEN_WriteStridedInd(fd, buf, count, datatype, file_ptr_type,\n112:                                   offset, status, error_code);\n113:         return;\n114:     }\n115: \n116:     /* At this point, we are committed to using collective I/O. */\n117: \n118:     MPI_Comm_size(fd->comm, &nprocs);\n119:     MPI_Comm_rank(fd->comm, &myrank);\n120: \n121:     /* Calculate the list of offsets and lengths in the file and determine the\n122:      * start and end offsets. */\n123: \n124:     ADIOI_Calc_my_off_len(fd, count, datatype, file_ptr_type, offset,\n125:                           &offset_list, &len_list, &start_offset,\n126:                           &end_offset, &contig_access_count);\n127: \n128:     /* each process communicates its start and end offsets to other\n129:      * processes. The result is an array each of start and end offsets stored\n130:      * in order of process rank. */\n131: \n132:     st_offsets = (ADIO_Offset *) ADIOI_Malloc(nprocs * 2 * sizeof(ADIO_Offset));\n133:     end_offsets = st_offsets + nprocs;\n134: \n135:     /* Gather start and end offsets of all processes */\n136:     MPI_Allgather(&start_offset, 1, MPI_OFFSET, st_offsets, 1, MPI_OFFSET, fd->comm);\n137:     MPI_Allgather(&end_offset, 1, MPI_OFFSET, end_offsets, 1, MPI_OFFSET, fd->comm);\n138: \n139:     /* allocate space for the array of access structures for this process's request */\n140:     my_req = (ADIOI_Access *) ADIOI_Malloc(nprocs * sizeof(ADIOI_Access));\n141: \n142:     /* allocate space for the array of access structures for other processes' requests */\n143:     others_req = (ADIOI_Access *) ADIOI_Malloc(nprocs *", "label": "(191, MPI, MPI_Allreduce(&tmp_error, error_code, 1, MPI_INT, MPI_MAX, fd->comm);); (65, MPI, MPI_Comm_size(fd->comm, &nprocs);); (93, MPI, MPI_Allgather(&end_offset, 1, ADIO_OFFSET, end_offsets, 1, ADIO_OFFSET, fd->comm);); (91, MPI, MPI_Allgather(&start_offset, 1, ADIO_OFFSET, st_offsets, 1, ADIO_OFFSET, fd->comm);); (63, MPI, MPI_Comm_rank(fd->comm, &myrank);); (210, MPI, MPI_Type_size_x(datatype, &size);)"}
{"predict": "(3,OpenMP,#pragma omp parallel sections), (4,OpenMP,#pragma omp section), (5,OpenMP,#pragma omp section), (6,OpenMP,#pragma omp section), (7,OpenMP,#pragma omp section), (8,OpenMP,#pragma omp section), (9,OpenMP,#pragma omp section), (10,OpenMP,#pragma omp section), (11,OpenMP,#pragma omp section), (12,OpenMP,#pragma omp section), (13,OpenMP,#pragma omp section), (14,OpenMP,#pragma omp section)", "label": "(23, CUDA, CHECK(cudaMalloc((void**)&atom->g_fz, sizeof(real) * N));); (19, CUDA, CHECK(cudaMalloc((void**)&atom->g_z, sizeof(real) * N));); (17, CUDA, CHECK(cudaMalloc((void**)&atom->g_y, sizeof(real) * N));); (21, CUDA, CHECK(cudaMalloc((void**)&atom->g_pe, sizeof(real) * N));); (14, CUDA, CHECK(cudaMalloc((void**)&atom->g_x, sizeof(real) * N));); (12, CUDA, CHECK(cudaMalloc((void**)&atom->g_NL, sizeof(int) * N * MN));); (16, CUDA, CHECK(cudaMalloc((void**)&atom->g_fy, sizeof(real) * N));); (9, CUDA, CHECK(cudaMalloc((void**)&atom->g_NN, sizeof(int) * N));); (13, CUDA, CHECK(cudaMalloc((void**)&atom->g_fx, sizeof(real) * N));)"}
{"predict": "(20,OpenMP,#pragma omp parallel for private(dim_arg_global, err, global_size))", "label": "(25, OpenCL, err = clEnqueueWriteBuffer( queue, buf[1], CL_TRUE, 0, sizeof(int), &dim_arg_global, 0, NULL, NULL);)"}
{"predict": "(35,OpenMP,#pragma omp parallel for private(elem)), (42,OpenMP,#pragma omp parallel for private(elem)), (54,OpenMP,#pragma omp parallel for private(elem)), (66,OpenMP,#pragma omp parallel for private(elem)), (76,OpenMP,#pragma omp parallel for private(elem)), (103,OpenMP,#pragma omp parallel for private(elem))", "label": "(106, OpenMP, #pragma omp parallel for firstprivate(numElemReg)); (53, OpenMP, #pragma omp for firstprivate(numElemReg)); (65, OpenMP, #pragma omp for nowait firstprivate(numElemReg, eosvmin)); (34, OpenMP, #pragma omp parallel); (74, OpenMP, #pragma omp for nowait firstprivate(numElemReg, eosvmax)); (36, OpenMP, #pragma omp for nowait firstprivate(numElemReg))"}
{"predict": "This code snippet is a function for creating a CUDA stream in Python using PyCUDA. The function does not contain any loops or sections that are inherently parallelizable, as it is primarily concerned with parsing arguments and creating a CUDA stream. Therefore, there are no lines in this code where parallelization instructions should be inserted. \n\nHowever, if you have a different section of code or a different function that contains loops or sections that could benefit from parallelization, please provide that code, and I can analyze it accordingly.", "label": "(13, CUDA, PYCUDA_ASSERT_NOGIL(cudaStreamCreateWithPriority(&stream, flags, priority));)"}
{"predict": "(31,OpenMP,#pragma omp parallel for)", "label": "(38, CUDA, cudaFree(res_dev);); (31, CUDA, cudaMalloc(&res_dev, sizeof(float));); (33, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(42,OpenMP,#pragma omp parallel if(mpi_size > 1))", "label": "(40, MPI, MPI_Error_class(*error_code, &rc);); (12, MPI, MPI_Comm_size(comm, &mpi_size);); (42, MPI, MPI_Bcast(&rc, 1, MPI_INT, 0, comm);)"}
{"predict": "(18,OpenMP,#pragma omp parallel for), (89,OpenMP,#pragma omp parallel for)", "label": "(81, CUDA, cudaError result = cudaDeviceSynchronize();); (30, CUDA, Tensor tensor_S = make_tensor(make_gmem_ptr(thrust::raw_pointer_cast(d_S.data())), make_layout(tensor_shape));); (14, CUDA, thrust::host_vector<Element> h_S(size(tensor_shape));); (22, CUDA, thrust::device_vector<Element> d_D = h_D;); (79, CUDA, std::cerr << \"CUDA Runtime error: \" << cudaGetErrorString(result) << std::endl;); (12, CUDA, thrust::host_vector<Element> h_D(size(tensor_shape));); (26, CUDA, Tensor tensor_D = make_tensor(make_gmem_ptr(thrust::raw_pointer_cast(d_D.data())), make_layout(tensor_shape));); (17, CUDA, thrust::device_vector<Element> d_S = h_S;)"}
{"predict": "(12,OpenMP,#pragma omp parallel for private(maxScore, maxScor2, maxIndex, sum, sums))", "label": "(3, CUDA, __shared__ float siftParts2[16*64];); (1, CUDA, __shared__ float siftParts1[17*64];)"}
{"predict": "(13,OpenMP,#pragma omp parallel for private(t))", "label": "(12, TBB, my_thread_array = tbb::cache_aligned_allocator<padded_private_worker>().allocate( my_n_thread );)"}
{"predict": "(83,OpenMP,#pragma omp parallel for reduction(+:sumVal)), (95,OpenMP,#pragma omp parallel for)", "label": "(52, CUDA, __shared__ bool isAccepted;); (52, CUDA, __shared__ T sSumVal;)"}
{"predict": "(14,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for private(target,j)), (38,OpenMP,#pragma omp parallel for private(j) reduction(+:errs))", "label": "(54, MPI, MPI_Free_mem(dst_buf);); (9, MPI, MPI_Alloc_mem(sizeof(double) * nproc * MAXELEMS, MPI_INFO_NULL, &dst_buf);); (30, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, target, 0, dst_win);); (50, MPI, MPI_Win_free(&dst_win);); (13, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, me, 0, dst_win);); (17, MPI, MPI_Win_unlock(me, dst_win);); (18, MPI, MPI_Barrier(MPI_COMM_WORLD);); (28, MPI, MPI_Win_unlock(target, dst_win);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for reduction(+:total)),", "label": "(7, OpenMP, omp_init_lock(&l);); (27, OpenMP, omp_destroy_lock(&l);); (7, OpenMP, #pragma omp parallel); (14, OpenMP, omp_set_lock(&l);); (21, OpenMP, omp_unset_lock(&l);); (-1, OpenMP, auto numThreads = omp_get_max_threads();)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)\n(12,OpenMP,#pragma omp parallel for)\n(14,OpenMP,#pragma omp parallel for)\n(16,OpenMP,#pragma omp parallel for)\n(18,OpenMP,#pragma omp parallel for)\n(20,OpenMP,#pragma omp parallel for)\n(22,OpenMP,#pragma omp parallel for)\n(24,OpenMP,#pragma omp parallel for)\n(26,OpenMP,#pragma omp parallel for)\n(28,OpenMP,#pragma omp parallel for)\n(30,OpenMP,#pragma omp parallel for)\n(34,OpenMP,#pragma omp parallel for)\n(37,OpenMP,#pragma omp parallel for)\n(43,OpenMP,#pragma omp parallel for private(m))", "label": "(22, OpenMP, #pragma omp target parallel for simd private(argc > 0 ? argv[1] : argv[2])); (36, OpenMP, #pragma omp target parallel for simd private(e, g)); (44, OpenMP, #pragma omp target parallel for simd private(i)); (49, OpenMP, #pragma omp target parallel for simd private(j)); (47, OpenMP, #pragma omp parallel private(i)); (54, OpenMP, #pragma omp target parallel for simd private(m)); (4, OpenMP, #pragma omp target parallel for simd private(); (36, OpenMP, #pragma omp parallel); (32, OpenMP, #pragma omp target parallel for simd private(h, B::x)); (22, OpenMP, #pragma omp target parallel for simd private(a, b)); (24, OpenMP, #pragma omp target parallel for simd private(argv[1])); (17, OpenMP, #pragma omp target parallel for simd private(S1)); (38, OpenMP, #pragma omp parallel shared(i)); (12, OpenMP, #pragma omp target parallel for simd private(argc, z)); (2, OpenMP, #pragma omp target parallel for simd private(argc); (-2, OpenMP, #pragma omp target parallel for simd private()); (-9, OpenMP, #pragma omp target parallel for simd private); (2, OpenMP, #pragma omp target parallel for simd private(argc,)"}
{"predict": "(41,OpenMP,#pragma omp parallel for)", "label": "(81, MPI, MPI_Finalize();)"}
{"predict": "(14,OpenMP,#pragma omp parallel for)", "label": "(9, MPI, MPI_Scan_c(sendbuf, recvbuf, count, datatype, MPI_SUM, comm);); (12, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (10, MPI, MPI_Iscan_c(sendbuf, recvbuf, count, datatype, MPI_SUM, comm, &req);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(45, OpenMP, #pragma omp parallel firstprivate(i)); (40, OpenMP, #pragma omp parallel private(i)); (17, OpenMP, #pragma omp task shared(argc, z)); (39, OpenMP, #pragma omp task shared(i)); (7, OpenMP, #pragma omp task shared()); (20, OpenMP, #pragma omp task shared(argv[1])); (29, OpenMP, #pragma omp task shared(h, B::x)); (26, OpenMP, #pragma omp task shared(e, g)); (31, OpenMP, #pragma omp task firstprivate(i), shared(i)); (34, OpenMP, #pragma omp task shared(j)); (17, OpenMP, #pragma omp task shared(ba)); (6, OpenMP, #pragma omp task shared(argc > 0 ? argv[1] : argv[2])); (-5, OpenMP, #pragma omp task shared); (-4, OpenMP, #pragma omp task shared(); (23, OpenMP, #pragma omp task private(i), shared(i)); (0, OpenMP, #pragma omp task shared(argc,); (7, OpenMP, #pragma omp task shared(a, b, c, d, f)); (4, OpenMP, #pragma omp task shared(S1)); (13, OpenMP, #pragma omp task shared(da)); (-6, OpenMP, #pragma omp task shared(argc); (9, OpenMP, #pragma omp task shared(ca))"}
{"predict": "The provided code snippet does not contain any loops or sections that are inherently parallelizable. To provide a meaningful example of parallelization, I'll assume some hypothetical loops or sections that could benefit from parallelization. Let's assume the following modifications to the code for demonstration purposes:\n\n```c\n1: void foo() {\n2:   double d;\n3:   int i[100];\n4:   float *p = (float *)malloc(100 * sizeof(float));\n5: \n6:   S2 s;\n7:   S2 *ps;\n8: \n9:   for (int k = 0; k < 100; k++) {\n10:     i[k] = k;\n11:     p[k] = (float)k / 100.0;\n12:   }\n13: \n14:   for (int k = 0; k < 100; k++) {\n15:     d += p[k] * i[k];\n16:   }\n17: \n18:   for (int k = 0; k < 100; k++) {\n19:     s.data[k] = i[k] + p[k];\n20:   }\n21: \n22:   for (int k = 0; k < 100; k++) {\n23:     ps[k].data[k] = s.data[k] * 2;\n24:   }\n25: \n26:   free(p);\n27: }\n```\n\nNow, let's identify where parallelization can be applied:\n\n(9,OpenMP,#pragma omp parallel for)\n(14,OpenMP,#pragma omp parallel for reduction(+:d))\n(18,OpenMP,#pragma omp parallel for)\n(22,OpenMP,#pragma omp parallel for)\n\nThese pragmas will help distribute the work of the loops across multiple threads using OpenMP.", "label": "(14, OpenMP, #pragma omp target map(p)); (51, OpenMP, #pragma omp target map(from: ps->ps->ps)); (38, OpenMP, #pragma omp target map(ps->i)); (31, OpenMP, #pragma omp target map(s.ps->ps->ps)); (32, OpenMP, #pragma omp target map(to: s.ps->ps->s.f[:22])); (27, OpenMP, #pragma omp target map(to: s.ps->ps)); (20, OpenMP, #pragma omp target map(to: s.p[:22])); (17, OpenMP, #pragma omp target map(s.p)); (2, OpenMP, #pragma omp target map(i)); (37, OpenMP, #pragma omp target map(to: ps->p[:22])); (18, OpenMP, #pragma omp target map(s.ps)); (27, OpenMP, #pragma omp target map(ps)); (30, OpenMP, #pragma omp target map(ps->s.f)); (41, OpenMP, #pragma omp target map(ps->ps->ps->ps)); (8, OpenMP, #pragma omp target map(s.s.f)); (-3, OpenMP, #pragma omp target map(i[1:23])); (-8, OpenMP, #pragma omp target map(d)); (3, OpenMP, #pragma omp target map(s.i)); (26, OpenMP, #pragma omp target map(from: ps->p)); (41, OpenMP, #pragma omp target map(from: s.f[:22]) map(to: ps->p[:33])); (38, OpenMP, #pragma omp target map(to: s.f[:22]) map(from: s.p[:33])); (41, OpenMP, #pragma omp target map(from: s.f[:22], s.s) map(to: ps->p[:33])); (-4, OpenMP, #pragma omp target map(s)); (25, OpenMP, #pragma omp target map(ps->ps)); (6, OpenMP, #pragma omp target map(from: s.ps->s.i)); (31, OpenMP, #pragma omp target map(to: ps->ps->ps->s.f[:22])); (-10, OpenMP, #pragma omp target map(p[1:24])); (23, OpenMP, #pragma omp target map(from: ps->ps->s.i))"}
{"predict": "(13,OpenMP,#pragma omp parallel sections), (15,OpenMP,#pragma omp section), (19,OpenMP,#pragma omp section)", "label": "(15, CUDA, getLastCudaError(\"generateSampleRanksKernel<1U><<<>>> failed\\n\");); (19, CUDA, getLastCudaError(\"generateSampleRanksKernel<0U><<<>>> failed\\n\");)"}
{"predict": "(14,OpenMP,#pragma omp parallel), (34,OpenMP,#pragma omp parallel if(state.sort_ranks > 1))", "label": "(13, MPI, MPI_Comm_size(comm, &ranks);); (11, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(15,OpenMP,#pragma omp parallel for)", "label": "(18, OpenCL, rc = clEnqueueNDRangeKernel(queue, write_kernel, 1, NULL, global_work_size, local_work_size, 0, NULL, NULL); CLERR;); (23, OpenCL, rc = clSetKernelArg(read_kernel, 4, sizeof(TYPE), &p2); CLERR;); (35, OpenCL, clReleaseKernel(read_kernel);); (28, OpenCL, rc = clEnqueueNDRangeKernel(queue, read_kernel, 1, NULL, global_work_size, local_work_size, 0, NULL, NULL);CLERR;); (5, OpenCL, cl_kernel read_kernel = clCreateKernel(program, \"kernel_modtest_read\", &rc); CLERR;); (20, OpenCL, rc = clSetKernelArg(read_kernel, 5, sizeof(cl_mem), &mc->err_count); CLERR;); (14, OpenCL, rc = clSetKernelArg(read_kernel, 0, sizeof(cl_mem), &mc->device_mem); CLERR;); (22, OpenCL, rc = clSetKernelArg(read_kernel, 9, sizeof(cl_mem), &mc->err_second_read); CLERR;); (0, OpenCL, cl_kernel write_kernel = clCreateKernel(program, \"kernel_modtest_write\", &rc); CLERR;); (8, OpenCL, rc = clSetKernelArg(write_kernel, 4, sizeof(TYPE), &p2); CLERR;); (11, OpenCL, rc = clSetKernelArg(read_kernel, 1, sizeof(cl_ulong), &mc->mem_size); CLERR;); (3, OpenCL, rc = clSetKernelArg(write_kernel, 1, sizeof(cl_ulong), &mc->mem_size); CLERR;); (14, OpenCL, rc = clSetKernelArg(read_kernel, 6, sizeof(cl_mem), &mc->err_addr); CLERR;); (0, OpenCL, rc = clSetKernelArg(write_kernel, 0, sizeof(cl_mem), &mc->device_mem); CLERR;); (22, OpenCL, clReleaseKernel(write_kernel);); (7, OpenCL, rc = clSetKernelArg(read_kernel, 2, sizeof(unsigned int), &offset); CLERR;); (0, OpenCL, rc = clSetKernelArg(write_kernel, 3, sizeof(TYPE), &p1); CLERR;); (6, OpenCL, rc = clSetKernelArg(read_kernel, 3, sizeof(TYPE), &p1); CLERR;); (-3, OpenCL, rc = clSetKernelArg(write_kernel, 2, sizeof(unsigned int), &offset); CLERR;); (9, OpenCL, rc = clSetKernelArg(read_kernel, 8, sizeof(cl_mem), &mc->err_current); CLERR;); (7, OpenCL, rc = clSetKernelArg(read_kernel, 7, sizeof(cl_mem), &mc->err_expect); CLERR;)"}
{"predict": "vector[i] -= 2*aux;\n140:         }\n141:         for (i=0; i<vector_length; i++) {\n142:           aux = (3 - (i&7));\n143:           if (aux>0) vector[i] -= 2*vector[index[i]];\n144:           else       vector[i] -= 2*aux;\n145:         }\n146:       }\n147:       break;\n148: \n149:     case INS_HEAVY:\n150:       for (iter=0; iter<iterations; iter+=2) {\n151:         for (i=0; i<vector_length; i++) {\n152:           aux = -(3 - (i&7));\n153:           if (vector[index[i]]>0) vector[i] -= 2*vector[i];\n154:           else                    vector[i] -= 2*aux;\n155:         }\n156:         for (i=0; i<vector_length; i++) {\n157:           aux = (3 - (i&7));\n158:           if (vector[index[i]]>0) vector[i] -= 2*vector[i];\n159:           else                    vector[i] -= 2*aux;\n160:         }\n161:       }\n162:       break;\n163:   }\n164: \n165:   branch_time = wtime() - branch_time;\n166: \n167:   /* accumulate the total number of iterations */\n168: \n169:   total = 0;\n170:   for (i=0; i<vector_length; i++) {\n171:     total += vector[i];\n172:   }\n173: \n174:   /* collect the results from all ranks */\n175: \n176:   MPI_Reduce(&total, &total_sum, 1, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD);\n177: \n178:   /* verification test */\n179: \n180:   if (my_ID == root) {\n181:     printf(\"Verification= %d Error=%d\\n\", total_sum, error);", "label": "(211, MPI, MPI_Reduce(&total, &total_sum, 1, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD);); (234, MPI, MPI_Finalize();); (83, MPI, MPI_Bcast(&iterations,    1, MPI_INT, root, MPI_COMM_WORLD);); (25, MPI, MPI_Init(&argc,&argv);); (80, MPI, MPI_Bcast(&vector_length, 1, MPI_INT, root, MPI_COMM_WORLD);); (81, MPI, MPI_Bcast(&btype,         1, MPI_INT, root, MPI_COMM_WORLD);); (97, MPI, MPI_Barrier(MPI_COMM_WORLD);); (23, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (21, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": ":     /* copy the received data to the right locations in write_buf */\n118:     to_ptr = write_buf;\n119:     for (i = 0; i < nprocs; i++) {\n120:         if (recv_size[i]) {\n121:             memcpy(to_ptr, all_recv_buf + rdispls[i], recv_size[i]);\n122:             to_ptr += recv_size[i];\n123:         }\n124:     }\n125: \n126:     ADIOI_Free(all_recv_buf);\n127:     ADIOI_Free(rdispls);\n128:     ADIOI_Free(send_req);\n129: }", "label": "(121, MPI, gpfsmpio_prof_cw[GPFSMPIO_CIO_T_DEXCH_SORT] += MPI_Wtime() - io_time;); (90, MPI, gpfsmpio_prof_cw[GPFSMPIO_CIO_T_DEXCH_NET] += MPI_Wtime() - io_time;); (23, MPI, int ret = MPI_Alltoall(recv_size, 1, MPI_COUNT, send_size, 1, MPI_COUNT, fd->comm);); (19, MPI, io_time = MPI_Wtime();); (24, MPI, gpfsmpio_prof_cw[GPFSMPIO_CIO_T_DEXCH_RECV_EXCH] += MPI_Wtime() - io_time;); (132, MPI, gpfsmpio_prof_cw[GPFSMPIO_CIO_T_DEXCH_SIEVE] += MPI_Wtime() - io_time;); (75, MPI, gpfsmpio_prof_cw[GPFSMPIO_CIO_T_DEXCH_SETUP] += MPI_Wtime() - io_time;)"}
{"predict": "(7,OpenMP,#pragma omp parallel for private(h, i, buf, saaaa, sbbbb, t_0, t_1, t_2, t_T_2, t_T_1))", "label": "(10, CUDA, __shared__ float v[_N_], gy[_N_];)"}
{"predict": "(8,OpenMP,#pragma omp parallel for reduction(+:count))", "label": "(14, CUDA, cudaDeviceSynchronize();); (12, CUDA, cudaMemcpy(&count,listp+fluidnum,sizeof(unsigned),cudaMemcpyDeviceToHost);); (6, CUDA, cudaMemset(listp+fluidnum,0,sizeof(unsigned));)"}
{"predict": "(7,OpenMP,#pragma omp parallel for)", "label": "(6, CUDA, st = cudaMalloc(&test_ptr, TEST_ALLOC_SIZE);); (25, CUDA, cudaFree(test_ptr);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for private(i,j)), (17,OpenMP,#pragma omp parallel for private(i,j,k))", "label": "(45, MPI, MPI_Buffer_detach(&bptr, &bl);); (25, MPI, MPI_Recv(b, 10, MPI_INT, i, 27 + j, MPI_COMM_WORLD, &status);); (8, MPI, MPI_Buffer_attach(buf, BUFSIZE);); (14, MPI, MPI_Wait(&request, &status);); (12, MPI, MPI_Ibsend(a, 10, MPI_INT, 0, 27 + j, MPI_COMM_WORLD, &request);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (51,OpenMP,#pragma omp parallel for private(v) reduction(+:v))", "label": "(32, OpenMP, #pragma omp for lastprivate(argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (73, OpenMP, #pragma omp parallel private(i)); (22, OpenMP, #pragma omp for lastprivate(argc,); (45, OpenMP, #pragma omp for lastprivate(a, b)); (12, OpenMP, #pragma omp for lastprivate()); (2, OpenMP, #pragma omp parallel); (69, OpenMP, #pragma omp for lastprivate(j)); (60, OpenMP, #pragma omp for lastprivate(i)); (4, OpenMP, #pragma omp for lastprivate(); (11, OpenMP, #pragma omp for lastprivate(argc); (30, OpenMP, #pragma omp for lastprivate(foo:argc)); (-3, OpenMP, #pragma omp for lastprivate); (32, OpenMP, #pragma omp for lastprivate(conditional: argc,s) lastprivate(conditional:); (15, OpenMP, #pragma omp for lastprivate(argc > 0 ? argv[1] : argv[2])); (46, OpenMP, #pragma omp for lastprivate(h)); (37, OpenMP, #pragma omp for lastprivate(argv[1])); (40, OpenMP, #pragma omp for lastprivate(k, e, g)); (56, OpenMP, #pragma omp parallel shared(i)); (18, OpenMP, #pragma omp for lastprivate(S1))"}
{"predict": "(30,OpenMP,#pragma omp parallel for)", "label": "(30, CUDA, cudaCheck(cudaGetLastError());)"}
{"predict": "(19,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for)", "label": "(8, MPI, MPI_Finalize();); (0, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(14,OpenMP,#pragma omp parallel sections)", "label": "(25, CUDA, auto UserTarget = SemaRef.IdentifyCUDATarget(FD);); (23, CUDA, auto VarTarget = SemaRef.IdentifyCUDATarget(Var);)"}
{"predict": "(31,OpenMP,#pragma omp parallel for), (48,OpenMP,#pragma omp parallel for)", "label": "(35, CUDA, CHECK_CUDA(cudaMalloc((void**)&imgs_dev, img_N * 3 * img_w * img_h * sizeof(uchar)));)"}
{"predict": "(23,OpenMP,#pragma omp parallel), (23,OpenMP,#pragma omp single nowait), (40,OpenMP,#pragma omp parallel), (40,OpenMP,#pragma omp single nowait), (64,OpenMP,#pragma omp parallel), (64,OpenMP,#pragma omp single nowait), (84,OpenMP,#pragma omp parallel), (84,OpenMP,#pragma omp single nowait)", "label": "(119, MPI, MPI_Free_mem(data);); (30, MPI, MPI_Win_fence(0, win);); (73, MPI, MPI_Win_fence(MPI_MODE_NOSUCCEED, win);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (12, MPI, MPI_Win_fence(MPI_MODE_NOPRECEDE, win);); (110, MPI, MPI_Win_free(&win);); (28, MPI, MPI_Accumulate(&mine, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_MAXLOC, win);); (5, MPI, MPI_Alloc_mem(sizeof(twoint_t), MPI_INFO_NULL, &data);); (71, MPI, MPI_Win_lock(MPI_LOCK_SHARED, 0, MPI_MODE_NOCHECK, win);); (72, MPI, MPI_Win_unlock(0, win);); (73, MPI, MPI_Barrier(MPI_COMM_WORLD);); (4, MPI, MPI_Win_create(data, me == 0 ? sizeof(twoint_t) : 0, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &win);); (41, MPI, MPI_Accumulate(&mine, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_MINLOC, win);)"}
{"predict": "(5,MPI,MPI_Init(&argc, &argv);); (6,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (7,MPI,MPI_Comm_size(MPI_COMM_WORLD, &size);); (58,MPI,MPI_Finalize();)", "label": "(6, MPI, MPI_Init(&argc, &argv);); (25, MPI, MPI_Send(&message, 1, MPI_INT, next, tag, MPI_COMM_WORLD);); (60, MPI, MPI_Finalize();); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (34, MPI, MPI_Recv(&message, 1, MPI_INT, prev, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)"}
{"predict": "(7,OpenMP,#pragma omp parallel for)", "label": "(12, CUDA, CUCH(cudaMalloc(&ptr, plan_size * sizeof(int32_t)), mhcudaMemoryAllocationFailure);); (6, CUDA, CUCH(cudaSetDevice(devs[devi]), mhcudaNoSuchDevice);)"}
{"predict": "(12,OpenMP,#pragma omp parallel)", "label": "(9, MPI, MPI_Comm_rank(comm, &comm_rank);); (10, MPI, MPI_Comm_size(comm, &comm_size);)"}
{"predict": "(62,OpenMP,#pragma omp parallel for), (82,OpenMP,#pragma omp parallel for reduction(+:ar)), (89,OpenMP,#pragma omp parallel for reduction(+:asum))", "label": "(80, SYCL, sycl::free(d_A, q);); (32, SYCL, prk::SYCL::print_device_platform(q);); (51, SYCL, double *d_C = sycl::malloc_device<double>(length, q);); (76, SYCL, sycl::free(d_B, q);); (74, SYCL, sycl::free(d_C, q);); (47, SYCL, double *d_B = sycl::malloc_device<double>(length, q);); (26, SYCL, sycl::queue q(sycl::default_selector_v);); (44, SYCL, double *d_A = sycl::malloc_device<double>(length, q);)"}
{"predict": "image_host           = cpu_workspace;\n146: \n147:             memcpy(image_host, image.data, size_image);\n148:             tensor->to_gpu();\n149:             tensor->set_dtype(TRT::DataType::kFLOAT);\n150: \n151:             if(use_multi_preprocess_stream_){\n152:                 TRT::CUStream::synchronize(preprocess_stream);\n153:             }\n154: \n155:             float* image_device_float = tensor->mutable_cpu<float>();\n156: \n157:             if(!normalize_){\n158:                 INFOE(\"Normalize is nullptr\");\n159:                 return false;\n160:             }\n161: \n162:             normalize_->execute(image_device_float, image_device_float, image.cols * image.rows * 3);\n163: \n164:             tensor->to_gpu();\n165:             tensor->release();\n166:             return true;\n167:         }\n168: \n169:         virtual void stop() override{\n170:             // stop\n171:         }\n172:     };\n173: }\n### **End of Code**", "label": "(151, CUDA, checkCudaRuntime(cudaMemcpyAsync(image_device, image_host, size_image, cudaMemcpyHostToDevice, preprocess_stream));); (125, CUDA, checkCudaRuntime(cudaStreamCreate(&preprocess_stream));); (67, CUDA, checkCudaRuntime(cudaStreamSynchronize(mono->get_stream()));)"}
{"predict": "::ElementsAreArray({0, 1, 2}));\n104:     // Offload all 3 blocks, fill with predictable pattern, onboard\n105:     for (auto cacheBlockId : cacheBlockIds1)\n106:     {\n107:         auto block = blockManager.getBlockById(cacheBlockId, maxAttentionWindow);\n108:         EXPECT_TRUE(block->isPrimary());\n109:         // offload so we can write to block in CPU code\n110:         blockManager.offloadBlock(block, maxAttentionWindow);\n111:         EXPECT_FALSE(block->isPrimary());\n112:         // need to sync so D2H transfer is done before accessing blocks\n113:         // fill with predictable pattern\n114:         auto memoryPoolIndex = block->getMemoryPoolBlockIndex();\n115:         auto blockPtr{tr::ITensor::slice(secondaryPoolPtr, memoryPoolIndex, 1)};\n116:         auto rawBlockPtr = reinterpret_cast<T*>(blockPtr->data());\n117:         for (int i = 0; i < blockSize; ++i)\n118:         {\n119:             rawBlockPtr[i] = i & mask;\n120:         }\n121:         // onboard\n122:         blockManager.onboardBlock(block, maxAttentionWindow);\n123:         EXPECT_TRUE(block->isPrimary());\n124:         EXPECT_EQ(cudaDeviceSynchronize(), cudaSuccess);\n125:         EXPECT_TRUE(blockManager.verifyQueueIntegrity(maxAttentionWindow));\n126:     }\n127:     blockManager.releaseBlocks(seq1, llmRequest1);\n128: \n129:     // Add sequence [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]\n130:     auto inputTokens2 = inputTokens;\n131:     auto const inputLength2 = static_cast<SizeType32>(inputTokens2->size());\n132:     requestId = 2;\n133:     auto llmRequest2 = std::make_shared<LlmRequest>(requestId, maxNewTokens, inputTokens2, samplingConfig, isStreaming);\n134:     Generation", "label": "(114, CUDA, EXPECT_EQ(cudaMemset(block2Ptr->data(), 0, blockSize * sizeof(T)), cudaSuccess);); (74, CUDA, EXPECT_EQ(cudaDeviceSynchronize(), cudaSuccess);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for)", "label": "(25, CUDA, cudaMemcpy(hostFloats2, gpuFloats2, 4 * sizeof(float), cudaMemcpyDeviceToHost);); (37, CUDA, cudaFree(gpuFloats1);); (8, CUDA, cudaMalloc((void**)(&gpuFloats2), N * sizeof(float));); (21, CUDA, cudaMemcpy(hostFloats1, gpuFloats1, 4 * sizeof(float), cudaMemcpyDeviceToHost);); (3, CUDA, cudaMalloc((void**)(&gpuFloats1), N * sizeof(float));); (35, CUDA, cudaFree(gpuFloats3);); (20, CUDA, cudaMemcpy(hostFloats3, gpuFloats3, 4 * sizeof(float), cudaMemcpyDeviceToHost);); (32, CUDA, cudaFree(gpuFloats2);); (5, CUDA, cudaMalloc((void**)(&gpuFloats3), N * sizeof(float));)"}
{"predict": "(20,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for)", "label": "(28, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(18,OpenMP,#pragma omp parallel for reduction(+:threadSum)), (33,OpenMP,#pragma omp parallel for reduction(+:threadSum)), (44,OpenMP,#pragma omp parallel for)", "label": "(9, CUDA, __shared__ cuda::barrier<cuda::thread_scope_block> barrier;)"}
{"predict": "(23,OpenMP,#pragma omp parallel for)", "label": "(6, CUDA, auto points_geometry_it = thrust::make_counting_iterator(0);); (8, CUDA, auto linestring_geometry_it  = thrust::make_counting_iterator(0);)"}
{"predict": "1i = tmp * u[3][k][j][i];\n111:           u51i = tmp * u[4][k][j][i];\n112: \n113:           tmp = rho_i[k][j][i-1];\n114: \n115:           u21im1 = tmp * u[1][k][j][i-1];\n116:           u31im1 = tmp * u[2][k][j][i-1];\n117:           u41im1 = tmp * u[3][k][j][i-1];\n118:           u51im1 = tmp * u[4][k][j][i-1];\n119: \n120:           flux_G[0][k][j][i] = u21i;\n121:           flux_G[1][k][j][i] = u21i * u21i + C2 * ( u51i - qs[k][j][i] );\n122:           flux_G[2][k][j][i] = u21i * u31i;\n123:           flux_G[3][k][j][i] = u21i * u41i;\n124:           flux_G[4][k][j][i] = ( C1 * u51i - C2 * qs[k][j][i] ) * u21i;\n125:         }\n126:       }\n127:     }\n128: \n129: #ifndef CRPL_COMP \n130: #elif CRPL_COMP == 0\n131:     #pragma omp target teams map (alloc: rsd, flux_G) \\\n132:     num_teams(nz-2) thread_limit(num_workers3)\n133:     #pragma omp distribute parallel for collapse(3) \n134: #endif\n135:     for (k = 1; k < nz - 1; k++) {\n136:       for (j = jst; j <= jend; j++) {\n137:         for (i = ist; i <= iend; i++) {\n138:           for (m = 0", "label": "(569, OpenMP, #pragma omp teams distribute parallel for collapse(3) private(i,j,k) num_teams(jend - jst)); (26, OpenMP, #pragma omp distribute parallel for collapse (3) private(tmp)); (106, OpenMP, #pragma omp simd simdlen(32) private (tmp, u21i, u31i, u41i, u51i, u21im1, u31im1, u41im1, u51im1)); (21, OpenMP, #pragma omp target teams); (15, OpenMP, #pragma omp target data); (59, OpenMP, #pragma omp distribute)"}
{"predict": "(35,OpenMP,#pragma omp parallel for reduction(+:score))", "label": "(11, CUDA, extern __shared__ char shared_mem[];)"}
{"predict": "(10,OpenMP,#pragma omp parallel for private(image_info, image_comp_info, io_start, io_end))", "label": "(59, CUDA, CHECK_CUDA(cudaMalloc((void**)&ibuf[i].pixel_data[c], sz));); (56, CUDA, CHECK_CUDA(cudaFree(ibuf[i].pixel_data[c]));)"}
{"predict": "(16,MPI,MPI_Comm_rank(comm, &par_rank);); (17,MPI,MPI_Comm_size(comm, &par_size);); (40,MPI,MPI_Bcast(output_path.c_str(), output_path.size() + 1, MPI_CHAR, 0, comm);); (94,OpenMP,#pragma omp parallel for)", "label": "(93, MPI, ascent_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (99, MPI, MPI_Barrier(comm);); (16, MPI, MPI_Comm_rank(comm, &par_rank);); (16, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for)", "label": "(10, OpenMP, f = test_choleskyc_decomp_dim(&p3.matrix, 2 * 8.0 * GSL_DBL_EPSILON);)"}
{"predict": "M, N, K, ldA, ldB, ldC, alpha, beta);\n116: }\n\n### ** Analysis and Parallelization Instructions **\n\nThe provided code is a function that sets up a SYCL queue based on the backend (OpenCL or Level Zero) and then calls a oneMKL GEMM function. The main computational work is done in the `onemkl_gemm` function, which is not shown in the code snippet. However, if the `onemkl_gemm` function is not parallelized, we can consider parallelizing the setup phase if there are any loops or independent operations that can be parallelized. Since the setup phase does not contain any loops or independent operations, the only place where parallelization can be effectively applied is in the `onemkl_gemm` function itself.\n\nHowever, if we assume that the `onemkl_gemm` function can be parallelized and is the bottleneck, we can consider parallelizing the `onemkl_gemm` call by using SYCL's parallel execution capabilities. But since the `onemkl_gemm` function is already designed to be parallelized using SYCL, the main focus should be on ensuring that the SYCL queue and other resources are set up correctly for parallel execution.\n\nGiven the context, the only place where parallelization can be effectively applied is in the `onemkl_gemm` function itself, which is not shown in the code snippet. However, if we assume that the `onemkl_gemm` function can be parallelized and is the bottleneck, we can consider parallelizing the `onemkl_gemm` call by using SYCL's parallel execution capabilities.\n\nSince the code does not contain any loops or independent operations that can be parallelized, and the `onemkl_gemm` function is already designed to be parallelized using SYCL, there are no specific parallelization instructions to be inserted in the provided code snippet.\n\nHowever, if we assume that the `onemkl_gemm` function can be parallelized and is the bottleneck, we can consider parallelizing the `onemkl_gemm` call by using SYCL's parallel execution capabilities. But since the `onemkl_gemm` function is already designed to be parallelized using SYCL, the main focus should be on ensuring that the SYCL queue and other resources are set up correctly for parallel execution.\n\nSince the code does not contain any loops or independent operations that can be parallelized, and the `on", "label": "(54, SYCL, std::vector<sycl::device> sycl_devices(1);); (3, SYCL, sycl::queue sycl_queue;)"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for)", "label": "(15, TBB, tbb::enumerable_thread_specific<size_t> num_high_degree_nodes(0);)"}
{"predict": "- meas_st_off[i]) - (float_t)FS_XL_MG);\n140:   }\n141: \n142:   /* Check self test limit */\n143:   for (i = 0; i < 3; i++) {\n144:     if (test_val[i] > ST_LIMIT_XL) {\n145:       st_result = ST_FAIL;\n146:     }\n147:   }\n148: \n149:   /* Disable Self Test */\n150:   ism303dac_xl_self_test_set(&dev_ctx_xl, ISM303DAC_XL_ST_DISABLE);\n151:   /*\n152:    * END ACCELEROMETER SELF TEST PROCEDURE\n153:    */\n154:   /*\n155:    * START MAGNETOMETER SELF TEST PROCEDURE\n156:    */\n157:   /* Set full scale */\n158:   ism303dac_mg_full_scale_set(&dev_ctx_mg, ISM303DAC_MG_4Gs);\n159:   /* Set Output Data Rate. */\n160:   ism303dac_mg_data_rate_set(&dev_ctx_mg, ISM303DAC_MG_ODR_10Hz);\n161:   /* Wait stable output */\n162:   platform_delay(WAIT_TIME_MG);\n163: \n164:   /* Check if new value available */\n165:   do {\n166:     ism303dac_mg_status_reg_get(&dev_ctx_mg, &reg.status_m);\n167:   } while (!reg.status_m.drdy);\n168: \n169:   /* Read dummy data and discard it */\n170:   ism303dac_magnetic_raw_get(&dev_ctx_mg, data_raw);\n171: \n172:   /* Read samples and get the average vale for each axis */\n173:   for (i = 0; i < SAMPLES_MG; i++) {\n174:     /* Check if new value available */\n175:     do {\n176:       ism303dac_mg_status_reg_get(&dev_ctx_mg", "label": "(162, OpenMP, ism303dac_mg_offset_temp_comp_set(&dev_ctx_mg, PROPERTY_ENABLE);)"}
{"predict": "(4,OpenMP,#pragma omp parallel for private(j))", "label": "(4, OpenMP, #pragma omp master taskloop simd); (2, OpenMP, #pragma omp parallel)"}
{"predict": "(9,OpenMP,#pragma omp parallel if(flag))", "label": "(7, MPI, MPI_Info_get(info, key, MPI_MAX_INFO_VAL, buf, &flag);)"}
{"predict": "(56,OpenMP,#pragma omp parallel for private(itr, gStartk))", "label": "(36, CUDA, __shared__ _VOLATILE_ float bSmem[16][128+4];); (34, CUDA, __shared__ _VOLATILE_ float aSmem[16][128+4];)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(11, OpenMP, #pragma omp teams distribute simd shared (); (43, OpenMP, #pragma omp teams distribute simd shared(da)); (30, OpenMP, #pragma omp teams distribute simd shared (a, b, c, d, f)); (5, OpenMP, #pragma omp teams distribute simd shared); (34, OpenMP, #pragma omp teams distribute simd shared(ba)); (2, OpenMP, #pragma omp target); (35, OpenMP, #pragma omp teams distribute simd shared(ca)); (55, OpenMP, #pragma omp teams distribute simd shared(i)); (57, OpenMP, #pragma omp teams distribute simd shared(j)); (17, OpenMP, #pragma omp teams distribute simd shared (argc)); (43, OpenMP, #pragma omp teams distribute simd private(i), shared(i)); (24, OpenMP, #pragma omp teams distribute simd shared (argv[1])); (17, OpenMP, #pragma omp teams distribute simd shared (S1)); (43, OpenMP, #pragma omp teams distribute simd firstprivate(i), shared(i)); (9, OpenMP, #pragma omp teams distribute simd shared (argc > 0 ? argv[1] : argv[2])); (35, OpenMP, #pragma omp teams distribute simd shared(h, B::x)); (43, OpenMP, #pragma omp teams distribute simd private(i)); (51, OpenMP, #pragma omp teams distribute simd firstprivate(i)); (2, OpenMP, #pragma omp teams distribute simd shared (argc,); (28, OpenMP, #pragma omp teams distribute simd shared(e, g, z)); (-6, OpenMP, #pragma omp teams distribute simd shared ()); (-4, OpenMP, #pragma omp teams distribute simd shared (argc)"}
{"predict": "1:    }\n172: \n173:    /* Assemble after setting the coefficients */\n174:    HYPRE_IJMatrixAssemble(A);\n175: \n176:    /* Convert to parcsr format */\n177:    HYPRE_IJMatrixGetObject(A, (void **) &parcsr_A);\n178: \n179:    /* Create and initialize the right-hand side and solution */\n180:    HYPRE_IJVectorCreate(MPI_COMM_WORLD, ilower, iupper, &b);\n181:    HYPRE_IJVectorSetObjectType(b, HYPRE_PARCSR);\n182:    HYPRE_IJVectorInitialize(b);\n183: \n184:    HYPRE_IJVectorCreate(MPI_COMM_WORLD, ilower, iupper, &x);\n185:    HYPRE_IJVectorSetObjectType(x, HYPRE_PARCSR);\n186:    HYPRE_IJVectorInitialize(x);\n187: \n188:    /* Set the right-hand side values to zero */\n189:    {\n190:       double rhs_value = 0.0;\n191:       for (i = ilower; i <= iupper; i++)\n192:       {\n193:          HYPRE_IJVectorSetValues(b, 1, &i, &rhs_value);\n194:       }\n195:    }\n196: \n197:    /* Assemble the right-hand side vector */\n198:    HYPRE_IJVectorAssemble(b);\n199: \n200:    /* Convert to parcsr format */\n201:    HYPRE_IJVectorGetObject(b, (void **) &par_b);\n202: \n203:    /* Assemble the solution vector */\n204:    HYPRE_IJVectorAssemble(x);\n205: \n206:    /* Convert to parcsr format */\n207:    HYPRE_IJVectorGetObject(x, (void **) &par_x);\n208: \n209:    /* Set up LOBPCG solver */\n210:    HYPRE_BoomerAMGCreate(&precond);\n211:    HYPRE_BoomerAMGSetPrintLevel(precond, 2);\n212:", "label": "(25, MPI, MPI_Init(&argc, &argv);); (79, MPI, MPI_Finalize();); (25, MPI, MPI_Comm_size(MPI_COMM_WORLD, &num_procs);); (23, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myid);)"}
{"predict": "(22,OpenMP,#pragma omp parallel for)", "label": "(1, TBB, using namespace oneapi::tbb::flow;)"}
{"predict": "(15,OpenMP,#pragma omp parallel for private(x1, y1, x2, y2, nomx, nomy, deno, errx, erry, err2, cnt))", "label": "(3, CUDA, __shared__ float homo[8*TESTHOMO_LOOPS];); (3, CUDA, __shared__ int cnts[TESTHOMO_TESTS*TESTHOMO_LOOPS];)"}
{"predict": "(10,OpenMP,#pragma omp parallel for), (64,OpenMP,#pragma omp teams distribute parallel for simd)", "label": "(34, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (85, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(j)); (7, OpenMP, #pragma omp target); (59, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(S2::S2s)); (10, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (); (21, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argc,); (36, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (a, b, c, d, f)); (47, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(ca)); (66, OpenMP, #pragma omp teams distribute parallel for simd private(i), firstprivate(i)); (13, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argc); (40, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(ba, z)); (59, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(h)); (26, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (S1)); (33, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argv[1])); (52, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(S2::S2sc)); (63, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(i)); (2, OpenMP, #pragma omp teams distribute parallel for simd firstprivate ()); (-7, OpenMP, #pragma omp teams distribute parallel for simd firstprivate); (40, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(da)); (72, OpenMP, #pragma omp teams distribute parallel for simd lastprivate(argc), firstprivate(argc)); (10, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argc > 0 ? argv[1] : argv[2]))"}
{"predict": "(17,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for private(i))", "label": "(25, MPI, s[i] = MPI_Win_allocate_shared((rank==0?99:0)*sizeof(char), 1, MPI_INFO_NULL, node, &base_ptr[i], &win[i]);); (36, MPI, MPI_Barrier(node);); (-1, MPI, MPI_Init(&argc, &argv);); (44, MPI, if(win[i] != MPI_WIN_NULL) MPI_Win_free(&win[i]);); (46, MPI, MPI_Finalize();); (2, MPI, MPI_Comm_rank(node, &rank);); (16, MPI, int sam = MPI_Alloc_mem((rank==0?99:0)*sizeof(char), MPI_INFO_NULL, &base_ptr[i]);); (25, MPI, MPI_Win_shared_query(win[i], 0, &size[i], &a[i], &ptr[i]);); (-5, MPI, int ss = MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, 0, MPI_INFO_NULL, &node);)"}
{"predict": ".get<size_t>(\"buffer-size\");\n156:             }\n157:         });\n158:     parser\n159:         .add_argument({ \"-w\", \"--wait\" }, \"Wait for the child process to finish\")\n160:         .count(0)\n161:         .action([](parser_t&) { wait_for_child() = true; });\n162:     parser\n163:         .add_argument({ \"-p\", \"--pid\" }, \"Specify the PID of the process to monitor\")\n164:         .max_count(1)\n165:         .dtype(\"pid_t\")\n166:         .action([](parser_t& p) { monitored_pid() = p.get<pid_t>(\"pid\"); });\n167:     parser\n168:         .add_argument({ \"-m\", \"--mpi\" }, \"Enable MPI instrumentation\")\n169:         .count(0)\n170:         .action([](parser_t&) { enable_mpi() = true; });\n171:     parser\n172:         .add_argument({ \"-t\", \"--threads\" }, \"Enable thread instrumentation\")\n173:         .count(0)\n174:         .action([](parser_t&) { enable_threads() = true; });\n175:     parser\n176:         .add_argument({ \"-u\", \"--upcxx\" }, \"Enable UPC++ instrumentation\")\n177:         .count(0)\n178:         .action([](parser_t&) { enable_upcxx() = true; });\n179:     parser\n180:         .add_argument({ \"-s\", \"--signal\" }, \"Enable UNIX signal instrumentation\")\n181:         .count(0)\n182:         .action([](parser_t&) { enable_signal() = true; });\n183:     parser\n184:         .add_argument({ \"-n\", \"--network\" }, \"Enable network instrumentation\")\n185:         .count(0)\n186:         .action([](parser_t&) { enable_network() = true; });\n187:     parser\n188:         .add_argument({ \"-a\", \"--all\" }, \"Enable all instrumentation\")\n189:         .count(0)\n190:         .action([](parser_t&) {\n19", "label": "(20, MPI, tim::settings::mpi_init()       = false;); (20, MPI, tim::settings::mpi_finalize()   = false;)"}
{"predict": "(37,OpenMP,#pragma omp parallel for),", "label": "(9, SYCL, const bool doHalfs = dev.has(sycl::aspect::fp16);); (47, SYCL, sycl::free(cd, queue);); (12, SYCL, const bool doDoubles = dev.has(sycl::aspect::fp64);)"}
{"predict": "(36,OpenMP,#pragma omp parallel for collapse(2) private(Csub) reduction(+:Csub))", "label": "(42, CUDA, __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];); (37, CUDA, __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];)"}
{"predict": "(13,OpenMP,#pragma omp parallel for)", "label": "(8, CUDA, cudaStreamSynchronize(LuH->A_gpu.cuStreams[0]);)"}
{"predict": "(3,OpenMP,#pragma omp parallel for), (3,OpenMP,for (int i = 0; i < 12; i++) { foo(); })", "label": "(8, OpenMP, #pragma omp target teams device (argc); (5, OpenMP, #pragma omp target teams device ()); (10, OpenMP, #pragma omp target teams device (argc > 0 ? argv[1] : argv[2])); (15, OpenMP, #pragma omp target teams device (S1)); (12, OpenMP, #pragma omp target teams device (argc), device (argc+1)); (15, OpenMP, #pragma omp target teams device (-2)); (8, OpenMP, #pragma omp target teams device (argc + argc + z)); (15, OpenMP, #pragma omp target teams device (-10u)); (2, OpenMP, #pragma omp target teams device (argc))); (-7, OpenMP, #pragma omp target teams device); (14, OpenMP, #pragma omp target teams device (3.14)); (-7, OpenMP, #pragma omp target teams device ()"}
{"predict": "(53,OpenMP,#pragma omp parallel for reduction(+:sum))", "label": "(32, CUDA, checkCudaErrors(cudaMallocHost(&b, N_ROWS * sizeof(double)));); (27, CUDA, int dev = findCudaDevice(argc, (const char **)argv);); (67, CUDA, checkCudaErrors(cudaMalloc(&d_A, sizeof(float) * N_ROWS * N_ROWS));); (99, CUDA, checkCudaErrors(cudaFree(d_x_new));); (101, CUDA, checkCudaErrors(cudaFreeHost(b));); (95, CUDA, checkCudaErrors(cudaFree(d_A));); (64, CUDA, checkCudaErrors(cudaMalloc(&d_x, sizeof(double) * N_ROWS));); (92, CUDA, checkCudaErrors(cudaFree(d_b));); (93, CUDA, checkCudaErrors(cudaFree(d_x));); (59, CUDA, checkCudaErrors(cudaMalloc(&d_b, sizeof(double) * N_ROWS));); (57, CUDA, checkCudaErrors(cudaStreamCreateWithFlags(&stream1, cudaStreamNonBlocking));); (60, CUDA, checkCudaErrors(cudaMalloc(&d_x_new, sizeof(double) * N_ROWS));); (61, CUDA, checkCudaErrors(cudaMemsetAsync(d_x, 0, sizeof(double) * N_ROWS, stream1));); (21, CUDA, checkCudaErrors(cudaMallocHost(&A, N_ROWS * N_ROWS * sizeof(float)));); (90, CUDA, checkCudaErrors(cudaFreeHost(A));)"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp single)", "label": "(11, OpenMP, #pragma omp single private()); (39, OpenMP, #pragma omp parallel private(i)); (21, OpenMP, #pragma omp single private(a, b)); (16, OpenMP, #pragma omp single private(argc)); (23, OpenMP, #pragma omp single private(e, g)); (2, OpenMP, #pragma omp single private); (19, OpenMP, #pragma omp single private(argv[1])); (26, OpenMP, #pragma omp parallel); (9, OpenMP, #pragma omp single private(argc > 0 ? argv[1] : argv[2])); (20, OpenMP, #pragma omp single private(h, B::x)); (11, OpenMP, #pragma omp single private(S1)); (2, OpenMP, #pragma omp single private(argc); (34, OpenMP, #pragma omp single private(m)); (2, OpenMP, #pragma omp single private(argc,); (27, OpenMP, #pragma omp single private(j)); (16, OpenMP, #pragma omp single shared(i)); (-7, OpenMP, #pragma omp single private(); (22, OpenMP, #pragma omp parallel shared(i)); (18, OpenMP, #pragma omp single private(i))"}
{"predict": "(59,OpenMP,#pragma omp parallel for private(dump_name, f, dump_step, prev_number, status, problem, any_problem))", "label": "(96, MPI, MPI_Finalize();); (88, MPI, MPI_Allreduce( &problem, &any_problem, 1, MPI_INT, MPI_LOR, smpi->world() );)"}
{"predict": "(21,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for), (36,OpenMP,#pragma omp parallel for), (41,OpenMP,#pragma omp parallel for), (46,OpenMP,#pragma omp parallel for), (51,OpenMP,#pragma omp parallel for)", "label": "(37, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(raa)); (48, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(rh)); (41, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(h)); (28, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(aa)); (51, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(da)); (20, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(z)); (13, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(k))"}
{"predict": "(13,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for)", "label": "(34, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(22,OpenMP,#pragma omp parallel for)", "label": "(24, MPI, MPI_Send(unzval, lenv, MPI_DOUBLE, receiver, k, grid3d->zscp.comm);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for collapse(2)), (26,OpenMP,#pragma omp parallel for collapse(2)), (36,OpenMP,#pragma omp parallel for collapse(2)), (40,OpenMP,#pragma omp parallel for collapse(2)), (55,OpenMP,#pragma omp parallel for collapse(2)), (80,OpenMP,#pragma omp parallel for)", "label": "(5, CUDA, __shared__ float4 buffer2[M7H*NUM];); (3, CUDA, __shared__ float4 buffer1[M7W*NDIM/4];)"}
{"predict": "info here: https://docs.nvidia.com/cuda/cufft/index.html#mp-restrictions\n100:     cudaStreamSynchronize(stream);\n101: \n102:     // Execute the reshape asynchronously\n103:     CUFFT_CHECK(cufftMpExecReshapeAsync(handle, src, dst, stream));\n104: \n105:     // Wait for the reshape to finish.\n106:     cudaStreamSynchronize(stream);\n107: \n108:     // Move GPU data back to CPU.\n109:     std::vector<int> dst_host;\n110:     cudaMemcpy(dst_host.data(), dst, global_io_max * sizeof(int), cudaMemcpyDeviceToHost);\n111: \n112:     // Check the result.\n113:     if(dst_host != dst_host_expected) {\n114:         printf(\"Test failed\\n\");\n115:         return 1;\n116:     }\n117: \n118:     printf(\"Test passed\\n\");\n119:     return 0;\n120: }", "label": "(15, CUDA, CUDA_CHECK(cudaGetDeviceCount(&ndevices));); (77, CUDA, CUDA_CHECK(cudaStreamCreate(&stream));); (5, MPI, MPI_Comm_size(comm, &size);); (119, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (12, CUDA, CUDA_CHECK(cudaSetDevice(rank % ndevices));); (-3, MPI, MPI_Init(&argc, &argv);); (90, MPI, MPI_Allreduce(&my_io_max, &global_io_max, 1, MPI_UNSIGNED_LONG_LONG, MPI_MAX, comm);); (116, CUDA, CUDA_CHECK(cudaMemcpy(dst_host.data(), dst, dst_host_expected.size() * sizeof(int), cudaMemcpyDefault));); (139, MPI, MPI_Finalize();); (118, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (94, CUDA, CUDA_CHECK(cudaMemsetAsync(dst, 0, dst_host_expected.size(), stream));); (92, CUDA, CUDA_CHECK(cudaMemcpyAsync(src, src_host.data(), src_host.size() * sizeof(int), cudaMemcpyDefault, stream));); (-6, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(5,OpenMP,#pragma omp parallel for private(cf, global_execute_count)), (12,OpenMP,#pragma omp parallel for private(receivers)), (23,OpenMP,#pragma omp parallel for private(senders)), (30,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for), (42,OpenMP,#pragma omp parallel for), (46,OpenMP,#pragma omp parallel for), (50,OpenMP,#pragma omp parallel for), (51,OpenMP,#pragma omp parallel for)", "label": "(66, TBB, g.reset(tbb::flow::rf_reset_bodies);); (20, TBB, tbb::flow::make_edge( exe_node, *receivers[r] );); (58, TBB, inc_functor body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (64, TBB, body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (1, TBB, tbb::flow::graph g;); (45, TBB, tbb::flow::remove_edge( exe_node, *receivers[r] );); (24, TBB, tbb::flow::make_edge( *senders.back(), exe_node );); (4, TBB, tbb::flow::function_node< InputType, OutputType > exe_node( g, lc, cf );)"}
{"predict": "(30,OpenMP,#pragma omp parallel for)", "label": "(23, MPI, MPI_Comm_split(expandedPCU->GetMPIComm(), group, groupRank, &groupComm);); (31, MPI, MPI_Comm_free(&groupComm);)"}
{"predict": "(18,OpenMP,#pragma omp parallel for collapse(2))", "label": "(19, CUDA, CudaCheckError();); (23, CUDA, cudaFree(du_new_dev);)"}
{"predict": "(36,OpenMP,#pragma omp parallel), (37,OpenMP,#pragma omp single nowait), (80,OpenMP,#pragma omp parallel), (81,OpenMP,#pragma omp single nowait)", "label": "(87, OpenCL, clReleaseMemObject(bufX);); (43, OpenCL, clReleaseCommandQueue(queue);); (27, OpenCL, printf( \"clCreateContext() failed with %d\\n\", err );); (85, OpenCL, clReleaseMemObject(bufA);); (79, OpenCL, clReleaseEvent(event);); (30, OpenCL, printf( \"clCreateCommandQueue() failed with %d\\n\", err );); (27, OpenCL, queue = clCreateCommandQueue(ctx, device, 0, &err);); (29, OpenCL, clReleaseContext(ctx);); (78, OpenCL, clReleaseMemObject(bufY);); (18, OpenCL, ctx = clCreateContext(props, 1, &device, NULL, NULL, &err);); (63, OpenCL, err = clWaitForEvents(1, &event);)"}
{"predict": "@__kmpc_omp_task_alloc(%struct.ident_t* [[DEFLOC]], i32 [[GTID]], i32 33, i64 80, i64 1, i32 (i32, i8*)* bitcast (i32 (i32, [[TDP_TY:%.+]]*)* [[TASK3:@.+]] to i32 (i32, i8*)*))\n55: // CHECK: [[TASK:%.+]] = bitcast i8* [[TASKV]] to [[TDP_TY]]*\n56: // CHECK: [[TASK_DATA:%.+]] = getelementptr inbounds [[TDP_TY]], [[TDP_TY]]* [[TASK]], i32 0, i32 0\n57: // CHECK: [[DOWN:%.+]] = getelementptr inbounds [[TD_TY:%.+]], [[TD_TY]]* [[TASK_DATA]], i32 0, i32 5\n58: // CHECK: store i64 0, i64* [[DOWN]],\n59: // CHECK: [[UP:%.+]] = getelementptr inbounds [[TD_TY]], [[TD_TY]]* [[TASK_DATA]], i32 0, i32 6\n60: // CHECK: store i64 9, i64* [[UP]],\n61: // CHECK: [[ST:%.+]] = getelementptr inbounds [[TD_TY]], [[TD_TY]]* [[TASK_DATA]], i32 0, i32 7\n62: // CHECK: store i64 1, i64* [[ST]],\n63: // CHECK: [[ST_VAL:%.+]] = load i64, i64* [[ST]],\n64: // CHECK: call void @__kmpc_taskloop(%struct.ident_t* [[DEFLOC]], i32 [[GTID]], i8* [[TASKV]], i32 1, i64* [[DOWN]], i64* [[UP]], i64 [[ST_VAL]], i32 1, i32 0, i64 0, i8* null)\n65: // CHECK: call void @__kmpc_end_taskgroup(%struct.ident_t* [[DEFLOC]], i32 [[GTID]])", "label": "(4, OpenMP, #pragma omp task); (25, OpenMP, #pragma omp master taskloop priority(argc)); (46, OpenMP, #pragma omp master taskloop); (71, OpenMP, #pragma omp master taskloop if(argc) shared(argc, argv) collapse(2) num_tasks(4)); (87, OpenMP, #pragma omp cancel taskgroup); (87, OpenMP, #pragma omp cancellation point taskgroup)"}
{"predict": "(25,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for)", "label": "(67, CUDA, cudaFree(B);); (54, CUDA, cudaEventRecord(stop);); (54, CUDA, cudaEventSynchronize(stop);); (65, CUDA, cudaFree(C);); (44, CUDA, cudaEventRecord(start);); (29, CUDA, cudaGetDeviceCount(&num_of_total_devices);); (18, CUDA, cudaEventCreate(&stop);); (53, CUDA, cudaEventElapsedTime(&elapsedTime, start, stop);); (15, CUDA, cudaEventCreate(&start);); (57, CUDA, cudaFree(A);)"}
{"predict": "(26,OpenMP,#pragma omp parallel for)", "label": "(11, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (9, CUDA, cudaMalloc((void**)(&d_in1), in1_bytes);); (11, CUDA, cudaMemcpy(d_in1, in1.data(), in1_bytes, cudaMemcpyHostToDevice);); (31, CUDA, cudaFree(d_out);); (23, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (28, CUDA, cudaFree(d_in1);)"}
{"predict": "(6,OpenMP,#pragma omp parallel for), (7,OpenMP,#pragma omp parallel for), (8,OpenMP,#pragma omp parallel for), (9,OpenMP,#pragma omp parallel for), (10,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for reduction(+:v)), (29,OpenMP,#pragma omp parallel for reduction(+:v))", "label": "(17, OpenMP, #pragma omp parallel for simd linear (argc : 5) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (39, OpenMP, #pragma omp parallel for simd linear(j)); (41, OpenMP, #pragma omp parallel for simd linear(v:j)); (34, OpenMP, #pragma omp parallel for simd linear(v:i)); (27, OpenMP, #pragma omp parallel for simd linear(i, z)); (4, OpenMP, #pragma omp parallel for simd linear ()); (13, OpenMP, #pragma omp parallel for simd linear (S1)); (6, OpenMP, #pragma omp parallel for simd linear (argc,); (-3, OpenMP, #pragma omp parallel); (36, OpenMP, #pragma omp parallel for simd linear(i)); (17, OpenMP, #pragma omp parallel for simd linear(e, g)); (12, OpenMP, #pragma omp parallel for simd linear (a, b:B::ib)); (-1, OpenMP, #pragma omp parallel for simd linear (argc); (12, OpenMP, #pragma omp parallel for simd linear (argv[1])); (15, OpenMP, #pragma omp parallel for simd linear(h)); (-8, OpenMP, #pragma omp parallel for simd linear (); (-1, OpenMP, #pragma omp parallel for simd linear (argc > 0 ? argv[1] : argv[2]))"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for reduction(+:defgradp1))", "label": "(8, OpenMP, #pragma omp parallel for schedule (guided))"}
{"predict": "] << std::endl;\n124: \n125:   return 0;\n126: }", "label": "(9, SYCL, sycl::queue q = get_queue();); (36, SYCL, sycl::free(data, q);); (8, SYCL, int* data = sycl::malloc_device<int>(1,q);)"}
{"predict": "(5,OpenMP,#pragma omp parallel for)", "label": "(6, CUDA, HANDLE_ERROR( cudaGetDeviceProperties( &prop, i ) );); (3, CUDA, HANDLE_ERROR( cudaGetDeviceCount( &count ) );)"}
{"predict": "(22,OpenMP,#pragma omp parallel for private(index, nzval, nb, nsupr, gb, nsupc, c, k, r, len, j, i, nmax, n)), (64,OpenMP,#pragma omp parallel for private(index, nzval, nb, nsupr, gb, nsupc, c, k, r, len, j, i))", "label": "(49, MPI, MPI_Allreduce(MPI_IN_PLACE,&n,1,mpi_int_t,MPI_MAX,grid->comm);); (47, MPI, MPI_Allreduce(MPI_IN_PLACE,&nnzL,1,mpi_int_t,MPI_SUM,grid->comm);)"}
{"predict": ") ? 'F' : 'N'),\n100:            file_size, nerrs, rates_g[0], rates_g[1], rates_g[2], rates_g[3]);\n101:   }\n102: \n103: /* Compare the two datasets */\n104: \n105:   compare_vec(&t[0], &t[10], 3, totsiz_3d, 1);\n106: \n107:   return 0;\n108: }", "label": "(17, MPI, MPI_Comm_size(MPI_COMM_WORLD,&totpes);); (39, MPI, MPI_Cart_create(MPI_COMM_WORLD,3,numpes,isperiodic,reorder,&comm_cart);); (39, MPI, MPI_Comm_rank(comm_cart,&mype);); (19, MPI, MPI_Finalize();); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&rank);); (72, MPI, MPI_Barrier(comm_cart);); (36, MPI, MPI_Cart_coords(comm_cart,mype,3,pe_coords);); (94, MPI, MPI_Allreduce(t, t_g, 20, MPI_DOUBLE, MPI_MAX, comm_cart);); (31, MPI, MPI_Dims_create(totpes,3,numpes);); (18, MPI, MPI_Bcast(filename, 256, MPI_CHAR, 0, MPI_COMM_WORLD);); (115, MPI, MPI_Allreduce(MPI_IN_PLACE, &nerrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (5, MPI, MPI_Init(&argc,&argv);); (102, MPI, MPI_Comm_free(&comm_cart);); (87, MPI, MPI_Allreduce(rates_l, rates_g, 4, MPI_DOUBLE, MPI_MIN, comm_cart);); (105, MPI, MPI_Reduce(&malloc_size, &sum_size, 1, MPI_OFFSET, MPI_SUM, 0, MPI_COMM_WORLD);)"}
{"predict": "(5,OpenMP,#pragma omp parallel for private(r, accept_count))", "label": "(10, TBB, tbb::flow::make_edge( lim, r );); (22, TBB, tbb::flow::limiter_node< T > lim_copy( lim );); (3, TBB, tbb::flow::graph g;); (3, TBB, tbb::flow::limiter_node< T > lim(g, i);); (4, TBB, tbb::atomic<int> accept_count;)"}
{"predict": "(7,OpenMP,#pragma omp parallel for collapse(2)), (14,OpenMP,#pragma omp parallel for collapse(2)), (19,OpenMP,#pragma omp parallel for collapse(2))", "label": "(15, OpenMP, #pragma omp loop); (19, OpenMP, #pragma omp target teams distribute parallel for map(MTX) reduction(*:EMTX)); (11, OpenMP, #pragma omp target teams loop map(MTX) reduction(*:MTX))"}
{"predict": "(36,OpenMP,#pragma omp parallel for), (52,OpenMP,#pragma omp parallel for)", "label": "(67, TBB, tbb::task_group tg;)"}
{"predict": "(22,OpenMP,#pragma omp atomic), (21,OpenMP,#pragma omp parallel for reduction(+:counts_team))", "label": "(26, OpenMP, #pragma omp for reduction(+:counts_team)); (8, OpenMP, #pragma omp for); (15, OpenMP, #pragma omp target teams map(from:counts2)); (4, OpenMP, #pragma omp parallel); (0, OpenMP, #pragma omp target teams map(from:counts1)); (17, OpenMP, #pragma omp allocate(counts_team) allocator(omp_pteam_mem_alloc))"}
{"predict": "\\n\", checksum, basesum*Num_procs);\n140:     } else {\n141:       printf(\"Solution validates\\n\");\n142:     }\n143:   }\n144: \n145:   MPI_Finalize();\n146:   return 0;\n147: }", "label": "(115, MPI, MPI_Type_commit(&mpi_word);); (122, MPI, MPI_Allgather(iterstring,1,mpi_word, catstring,1,mpi_word, MPI_COMM_WORLD);); (154, MPI, MPI_Finalize();); (63, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (110, MPI, MPI_Type_contiguous(proc_length,MPI_CHAR, &mpi_word);); (17, MPI, MPI_Init(&argc,&argv);); (61, MPI, MPI_Bcast(&length,     1, MPI_LONG, root, MPI_COMM_WORLD);); (110, MPI, MPI_Barrier(MPI_COMM_WORLD);); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(10, MPI, MPI_Barrier(MPI_COMM_WORLD);); (12, MPI, MPI_Allreduce(data,gr_comm_buf,n,mpi_bool,MPI_LAND,MPI_COMM_WORLD);)"}
{"predict": "(25,OpenMP,#pragma omp parallel sections), (25,OpenMP,#pragma omp section), (31,OpenMP,#pragma omp section), (37,OpenMP,#pragma omp section), (43,OpenMP,#pragma omp section), (49,OpenMP,#pragma omp section)", "label": "(16, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(42, TBB, tbb::flow::make_edge(buffer2, tbb::flow::input_port<1>(join));); (43, TBB, tbb::flow::make_edge(join, function);); (19, TBB, tbb::flow::join_node<tuple_type, tbb::flow::reserving> join(g);); (38, TBB, tbb::flow::make_edge(buffer1, tbb::flow::input_port<0>(join));); (-3, TBB, tbb::task_arena arena(1);); (-1, TBB, tbb::flow::graph g;); (11, TBB, tbb::flow::queue_node<int> buffer1(g);); (12, TBB, tbb::flow::queue_node<int> buffer3(g);); (35, TBB, tbb::flow::make_edge(buffer3, tbb::flow::input_port<2>(join));); (9, TBB, tbb::flow::queue_node<int> buffer2(g);)"}
{"predict": "(30,OpenMP,#pragma omp parallel for reduction(+:localpi)), (40,MPI,MPI_Reduce(&localpi, &pi, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);)", "label": "(42, MPI, MPI_Recv(&localpi, 1, MPI_DOUBLE, MPI_ANY_SOURCE, 0, MPI_COMM_WORLD, &status);); (47, MPI, MPI_Send(&localpi, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);); (4, MPI, MPI_Init(&argc, &argv);); (48, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(13, CUDA, CP_host->freeCudaMem();)"}
{"predict": "(5,OpenMP,#pragma omp parallel for private(sse_mode, mode))", "label": "(12, TBB, tbb::parallel_for( 0, N, 1, RoundingModeCheckBody(mode, sse_mode) );); (10, TBB, tbb::task_scheduler_init init;)"}
{"predict": "(11,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for), (47,OpenMP,#pragma omp parallel for reduction(+:errors))", "label": "(13, OpenMP, fprintf(stderr, \"#pragma omp target teams distribute thread_limit(thdLim) 27\\n\");); (43, OpenMP, #pragma omp target teams distribute thread_limit(1024)); (22, OpenMP, fprintf(stderr, \"#pragma omp target teams distribute thread_limit(thdLim) 1024\\n\");); (31, OpenMP, #pragma omp target teams distribute thread_limit(27)); (34, OpenMP, constLimit[i] = omp_get_num_threads();); (9, OpenMP, #pragma omp target teams distribute thread_limit(thdLim)); (42, OpenMP, constLimitHuge[i] = omp_get_num_threads();); (21, OpenMP, varLimitHuge[i] = omp_get_num_threads();); (9, OpenMP, varLimit[i] = omp_get_num_threads();)"}
{"predict": "(39,OpenMP,#pragma omp parallel for), (43,OpenMP,#pragma omp parallel for)", "label": "(50, CUDA, cudaFreeHost( foutHost );); (27, CUDA, CUDART_CHECK(surf1Dmemset( array, 3.141592654f, 0, NUM_VALUES ));)"}
{"predict": "(61,OpenMP,#pragma omp parallel for), (89,OpenMP,#pragma omp parallel for)", "label": "(72, OpenMP, #pragma omp target data map(h_x [0:nconf * numatm], h_y [0:nconf * numatm], h_z [0:nconf * numatm], h_g2 [0:nbin]))"}
{"predict": "(8,OpenMP,#pragma omp parallel for private(index, af_decrypted, i, iterations, dklen, keycandidate, masterkeycandidate) firstprivate(lens, pin, x))", "label": "(6, OpenMP, #pragma omp parallel for)"}
{"predict": "(5,OpenMP,#pragma omp parallel sections), (50,OpenMP,#pragma omp section), (53,OpenMP,#pragma omp section)", "label": "(4, SYCL, int* data = sycl::malloc_shared<int>(7, q);); (19, SYCL, hipsycl::rt::runtime_keep_alive_token rt;); (1, SYCL, sycl::queue q = get_queue();); (67, SYCL, sycl::free(data, q);)"}
{"predict": "(21,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for)", "label": "(13, MPI, MPI_Comm_size( comm, &size );); (22, MPI, MPI_Type_commit( &vec );); (26, MPI, MPI_Gather( vecin, 1, vec, vecout, n, MPI_DOUBLE, root, comm );); (46, MPI, MPI_Finalize();); (5, MPI, MPI_Init( &argc, &argv );); (35, MPI, MPI_Type_free( &vec );); (6, MPI, MPI_Comm_rank( comm, &rank );); (15, MPI, MPI_Type_vector( n, 1, stride, MPI_DOUBLE, &vec );); (39, MPI, MPI_Gather( NULL, 0, MPI_BYTE, NULL, 0, MPI_BYTE, 0, MPI_COMM_WORLD );)"}
{"predict": "(22,OpenMP,#pragma omp parallel for private(col)) (26,OpenMP,#pragma omp parallel for reduction(+:sum) private(col))", "label": "(34, OpenMP, #pragma omp for reduction(+:sum)); (32, OpenMP, printf(\"rank %d:%d has %d threads\\n\", rank, thread_num, omp_get_num_threads());); (11, MPI, MPI_Init(&argc, &argv);); (42, MPI, MPI_Finalize();); (26, OpenMP, #pragma omp parallel); (36, MPI, MPI_Reduce(&sum, &avg, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (25, OpenMP, int thread_num = omp_get_thread_num();); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (7, MPI, MPI_Dims_create(size, NR_DIMS, dims);); (13, MPI, MPI_Cart_coords(cart_comm, rank, NR_DIMS, cart_coords);); (10, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(8, OpenCL, clReleaseKernel_ExpectAndReturn(make_kernel(0), CL_SUCCESS);); (6, OpenCL, clReleaseDevice_ExpectAndReturn(make_device_id(0), CL_SUCCESS);)"}
{"predict": "(32,OpenMP,#pragma omp parallel for),", "label": "(40, CUDA, CUDA_SAFE_CALL(cudaFree(x.data()));); (38, CUDA, CUDA_SAFE_CALL(cudaFree(A.data()));); (36, CUDA, CUDA_SAFE_CALL(cudaDeviceSynchronize());); (38, CUDA, CUDA_SAFE_CALL(cudaFree(y.data()));)"}
{"predict": "(21,OpenMP,#pragma omp declare target)", "label": "(22, MPI, *w = MPI_Wtime();)"}
{"predict": "(22,OpenMP,#pragma omp parallel for private(w_frag, a_frag, b_frag, c_frag, wpoint) reduction(+:sums))", "label": "(2, CUDA, extern __shared__ half  hs[][256];)"}
{"predict": "(6,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel), (26,OpenMP,#pragma omp parallel), (41,OpenMP,#pragma omp parallel), (45,OpenMP,#pragma omp parallel), (49,OpenMP,#pragma omp parallel), (52,OpenMP,#pragma omp parallel), (57,OpenMP,#pragma omp parallel), (58,OpenMP,#pragma omp parallel)", "label": "(7, TBB, tbb::task_scheduler_init init( p );)"}
{"predict": "(17,OpenMP,#pragma omp parallel for), (35,OpenMP,#pragma omp parallel for reduction(+:is_1,is_12,is_1_mod2_0,is_1_mod2_1))", "label": "(13, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(38,OpenMP,#pragma omp parallel for private(j))", "label": "(18, MPI, MPI_Comm_rank(shm_comm, &shm_rank);); (41, MPI, MPI_Win_lock_all(MPI_MODE_NOCHECK, shm_win);); (24, MPI, MPI_Win_shared_query(shm_win, MPI_PROC_NULL, &size, &disp_unit, &base);); (13, MPI, MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, rank, MPI_INFO_NULL, &shm_comm);); (10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (58, MPI, MPI_Win_unlock_all(shm_win);); (58, MPI, MPI_Win_free(&shm_win);); (58, MPI, MPI_Comm_free(&shm_comm);); (10, MPI, MPI_Comm_size(shm_comm, &shm_nproc);); (40, MPI, MPI_Barrier(shm_comm);); (38, MPI, MPI_Win_sync(shm_win);)"}
{"predict": "(45,OpenMP,#pragma omp parallel for), (46,OpenMP,#pragma omp parallel for), (47,OpenMP,#pragma omp parallel for), (55,OpenMP,#pragma omp parallel for collapse(2))", "label": "(41, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&b, global_b_size * sizeof(TB)));); (71, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(c));); (68, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(a));); (37, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&a, global_a_size * sizeof(TA)));); (47, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(a, host_a.data(), global_a_size * sizeof(TA), cudaMemcpyHostToDevice));); (49, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (47, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(c, host_c.data(), global_a_size * sizeof(TC), cudaMemcpyHostToDevice));); (45, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(b, host_b.data(), global_a_size * sizeof(TB), cudaMemcpyHostToDevice));); (58, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(host_output.data(), c, global_c_size * sizeof(TC), cudaMemcpyDeviceToHost));); (33, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&c, global_c_size * sizeof(TC)));); (61, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(b));)"}
{"predict": "(47,OpenMP,#pragma omp parallel for),", "label": "(62, CUDA, cudaFree( ddata );)"}
{"predict": "(15,OpenMP,#pragma omp parallel for)", "label": "(10, CUDA, cudaStreamSynchronize(LU_v1->A_gpu.cuStreams[0]);)"}
{"predict": "(4,OpenMP,#pragma omp parallel for)", "label": "(2, OpenMP, omp_autotune(self, OMP_SCALE);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for)", "label": "(18, CUDA, cudaMemcpy(dB_buff, B_buff, sizeof(float*) * batch_num, cudaMemcpyHostToDevice);); (12, CUDA, cudaMalloc(&dA_buff, sizeof(float*) * batch_num);); (31, CUDA, cudaFree(dC_buff);); (29, CUDA, cudaFree(dB_buff);); (27, CUDA, cudaFree(dA_buff);); (12, CUDA, cudaMemcpy(dA_buff, A_buff, sizeof(float*) * batch_num, cudaMemcpyHostToDevice);); (8, CUDA, cudaMalloc(&dB_buff, sizeof(float*) * batch_num);); (12, CUDA, cudaMemcpy(dC_buff, C_buff, sizeof(float*) * batch_num, cudaMemcpyHostToDevice);); (7, CUDA, cudaMalloc(&dC_buff, sizeof(float*) * batch_num);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(7, CUDA, cudaMalloc((void **)&buffer_fp32, length * sizeof(float));); (9, CUDA, cudaMalloc((void **)&buffer, length * sizeof(T));); (10, CUDA, cudaFree(buffer_fp32);)"}
{"predict": "(23,OpenMP,#pragma omp parallel for)", "label": "(16, OpenMP, #pragma omp target); (22, OpenMP, #pragma omp teams distribute parallel for if(cond) firstprivate(fp) reduction(+:rd))"}
{"predict": "(9,OpenMP,#pragma omp parallel for)", "label": "(19, CUDA, cudaMemcpy(resf,dat,sizeof(unsigned)*2,cudaMemcpyDeviceToHost);)"}
{"predict": "(19,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for)", "label": "(29, CUDA, CUDART_CHECK(surf2DmemsetArray_time( &ms, texArray, 3.141592654f, threadWidth, threadHeight ));); (50, CUDA, cudaFreeArray( texArray );)"}
{"predict": "{\n182:         if (getCmdLineArgumentInt(argc, argv, \"start\", &start))\n183:         {\n184:             printf(\" Start Size = %d\\n\", start);\n185:         }\n186:         else\n187:         {\n188:             start = DEFAULT_SIZE;\n189:         }\n190: \n191:         if (getCmdLineArgumentInt(argc, argv, \"end\", &end))\n192:         {\n193:             printf(\" End Size = %d\\n\", end);\n194:         }\n195:         else\n196:         {\n197:             end = DEFAULT_SIZE;\n198:         }\n199: \n200:         if (getCmdLineArgumentInt(argc, argv, \"increment\", &increment))\n201:         {\n202:             printf(\" Increment Size = %d\\n\", increment);\n203:         }\n204:         else\n205:         {\n206:             increment = DEFAULT_INCREMENT;\n207:         }\n208:     }\n209: \n210:     if (SHMOO_MODE == mode)\n211:     {\n212:         if (getCmdLineArgumentInt(argc, argv, \"start\", &start))\n213:         {\n214:             printf(\" Start Size = %d\\n\", start);\n215:         }\n216:         else\n217:         {\n218:             start = DEFAULT_SIZE;\n219:         }\n220: \n221:         if (getCmdLineArgumentInt(argc, argv, \"end\", &end))\n222:         {\n223:             printf(\" End Size = %d\\n\", end);\n224:         }\n225:         else\n226:         {\n227:             end = DEFAULT_SIZE;\n228:         }\n229: \n230:         if (getCmdLineArgumentInt(argc, argv, \"increment\", &increment))\n231:         {\n232:             printf(\" Increment Size = %d\\n\", increment);\n233:         }\n234:         else\n235:         {\n2", "label": "(267, CUDA, cudaSetDevice(nDevice);); (108, CUDA, printf(\"cudaGetDeviceProperties returned %d\\n-> %s\\n\", (int)error_id, cudaGetErrorString(error_id));); (54, CUDA, cudaError_t error_id = cudaGetDeviceCount(&deviceCount);); (98, CUDA, fprintf(stderr, \"Error: device is running in <Compute Mode Prohibited>, no threads can use ::cudaSetDevice().\\n\");); (56, CUDA, printf(\"cudaGetDeviceCount returned %d\\n-> %s\\n\", (int)error_id, cudaGetErrorString(error_id));); (88, CUDA, cudaError_t error_id = cudaGetDeviceProperties(&deviceProp, currentDevice);); (96, CUDA, checkCudaErrors(cudaSetDevice(currentDevice));)"}
{"predict": "(1,OpenMP,#pragma omp target teams distribute parallel for map(to:array[0:n]) map(tofrom:output[0:1]) thread_limit(blockDim.x))", "label": "(5, CUDA, extern __shared__ float cache[];)"}
{"predict": "(5,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for), (66,OpenMP,#pragma omp parallel for)", "label": "(77, OpenMP, #pragma omp parallel num_threads(NumThreads1)); (14, OpenMP, #pragma omp parallel); (26, OpenMP, int nestedThreadNum = omp_get_thread_num();); (7, OpenMP, inParallel = omp_in_parallel();); (22, OpenMP, int nestedNumThreads = omp_get_num_threads();); (19, OpenMP, int nestedInParallel = omp_in_parallel();); (12, OpenMP, check1[id] += omp_get_num_threads() + omp_in_parallel();); (4, OpenMP, numThreads = omp_get_num_threads();); (21, OpenMP, #pragma omp atomic); (3, OpenMP, threadNum = omp_get_thread_num();); (-2, OpenMP, #pragma omp target map(inParallel, numThreads, threadNum, check1[:], check2[:])); (6, OpenMP, int id = omp_get_thread_num();)"}
{"predict": "(17,OpenMP,#pragma omp parallel for reduction(+:gpu_sum))", "label": "(11, CUDA, float *dptr = thrust::raw_pointer_cast(&dsums[0]);); (14, CUDA, double gpu_sum = thrust::reduce(dsums.begin(),dsums.end());); (8, CUDA, thrust::device_vector<float> dsums(steps);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for)", "label": "(12, OpenMP, #pragma omp parallel for)"}
{"predict": "(14,OpenMP,#pragma omp parallel for)", "label": "(12, TBB, my_thread_array = tbb::cache_aligned_allocator<padded_private_worker>().allocate( my_n_thread );)"}
{"predict": "(22,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for reduction(+:result_host))", "label": "(40, CUDA, cudaFree(d_inPtr);); (24, CUDA, cudaMalloc((void**)&d_outPtr, size * sizeof(float));); (30, CUDA, cudaMemcpy(&result_gpu, &d_outPtr[0], sizeof(float), cudaMemcpyDeviceToHost);); (24, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);); (35, CUDA, cudaFree(d_outPtr);); (19, CUDA, cudaMalloc((void**)& d_inPtr, size * sizeof(float));)"}
{"predict": "(18,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for)", "label": "(88, CUDA, cudaEventSynchronize(stop);); (70, CUDA, err = cudaGetLastError();); (19, CUDA, err = cudaMalloc(&d_A, length);); (12, CUDA, cudaEventRecord(start, 0);); (78, CUDA, err = cudaFree(d_C);); (87, CUDA, cudaEventDestroy(start);); (74, CUDA, err = cudaFree(d_B);); (21, CUDA, err = cudaMalloc(&d_B, length);); (78, CUDA, cudaEventRecord(stop, 0);); (32, CUDA, err = cudaMemcpy(d_A, A, length, cudaMemcpyHostToDevice);); (80, CUDA, cudaEventElapsedTime(elapsed_time, start, stop);); (1, CUDA, cudaEventCreate(&start);); (66, CUDA, err = cudaFree(d_A);); (30, CUDA, err = cudaMemcpy(d_B, B, length, cudaMemcpyHostToDevice);); (16, CUDA, err = cudaMalloc(&d_C, length);); (78, CUDA, cudaEventDestroy(stop);); (-3, CUDA, cudaEventCreate(&stop);); (57, CUDA, err = cudaMemcpy(C, d_C, length, cudaMemcpyDeviceToHost);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for)", "label": "(16, OpenMP, #pragma omp target defaultmap (scalar:); (3, OpenMP, #pragma omp target defaultmap(); (10, OpenMP, #pragma omp target defaultmap(tofrom)); (11, OpenMP, #pragma omp target defaultmap(tofrom,); (16, OpenMP, #pragma omp target defaultmap(tofrom:scalar) defaultmap(tofrom:scalar)); (-3, OpenMP, #pragma omp target defaultmap); (19, OpenMP, #pragma omp target defaultmap(tofrom: something)); (11, OpenMP, #pragma omp target defaultmap(tofrom, scalar); (2, OpenMP, #pragma omp target defaultmap (tofrom:); (-3, OpenMP, #pragma omp target defaultmap()); (-2, OpenMP, #pragma omp target defaultmap(tofrom)"}
{"predict": "(26,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for)", "label": "(52, CUDA, CUDA_CALL(cudaFree(data));); (28, CUDA, CUDA_CALL(cudaDeviceSynchronize());); (6, CUDA, CUDA_CALL(cudaMalloc((void**)&data, size * sizeof(T)));)"}
{"predict": "(35,OpenMP,#pragma omp parallel for), (73,OpenMP,#pragma omp parallel for), (83,OpenMP,#pragma omp parallel for)", "label": "(29, MPI, MPI_Abort(comm, 1);); (15, MPI, MPI_Comm_size(comm, &size);); (15, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for)", "label": "(25, OpenMP, #pragma omp parallel sections shared(argv[1])); (34, OpenMP, #pragma omp parallel sections shared(h, B::x)); (41, OpenMP, #pragma omp parallel sections shared(i)); (20, OpenMP, #pragma omp parallel sections shared(a, b, c, d, f, k)); (9, OpenMP, #pragma omp parallel sections shared(argc); (10, OpenMP, #pragma omp parallel sections shared(argc,); (33, OpenMP, #pragma omp parallel sections firstprivate(i)); (22, OpenMP, #pragma omp parallel sections shared(ca)); (19, OpenMP, #pragma omp parallel sections shared(ba)); (28, OpenMP, #pragma omp parallel sections private(i)); (-3, OpenMP, #pragma omp parallel sections shared); (20, OpenMP, #pragma omp parallel sections shared(da)); (5, OpenMP, #pragma omp parallel sections shared(argc > 0 ? argv[1] : argv[2])); (8, OpenMP, #pragma omp parallel sections shared(S1)); (5, OpenMP, #pragma omp parallel sections shared(argc)); (30, OpenMP, #pragma omp parallel sections shared(j)); (-5, OpenMP, #pragma omp parallel sections shared()); (-8, OpenMP, #pragma omp parallel sections shared(); (15, OpenMP, #pragma omp parallel sections shared(e, g))"}
{"predict": "(38,OpenMP,#pragma omp parallel), (39,OpenMP,#pragma omp single)", "label": "(45, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(27,OpenMP,#pragma omp parallel for)", "label": "(25, OpenMP, #pragma omp parallel for)"}
{"predict": "(24,OpenMP,#pragma omp parallel for reduction(+:sum_val0,sum_val1)), (74,OpenMP,#pragma omp parallel for)", "label": "(13, CUDA, extern __shared__ char buf[];)"}
{"predict": "(73,OpenMP,#pragma omp parallel for), (84,OpenMP,#pragma omp parallel for), (95,OpenMP,#pragma omp parallel for), (100,OpenMP,#pragma omp parallel for)", "label": "(66, MPI, MPI_Comm_size(fd->comm, &nprocs);); (66, MPI, MPI_Comm_rank(fd->comm, &myrank);)"}
{"predict": "(9,OpenMP,#pragma omp parallel for reduction(+:errs))", "label": "(6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &world_size);)"}
{"predict": "@+3 {{defined as lastprivate}}\n125: #pragma omp target\n126: #pragma omp teams\n127:   for (i = 0; i < argc; ++i)\n128:     foo();\n129:   return 0;\n130: }\n\n(10,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for), (38,OpenMP,#pragma omp parallel for), (42,OpenMP,#pragma omp parallel for), (46,OpenMP,#pragma omp parallel for), (50,OpenMP,#pragma omp parallel for), (54,OpenMP,#pragma omp parallel for), (58,OpenMP,#pragma omp parallel for), (62,OpenMP,#pragma omp parallel for), (67,OpenMP,#pragma omp parallel for), (71,OpenMP,#pragma omp parallel for), (75,OpenMP,#pragma omp parallel for), (79,OpenMP,#pragma omp parallel for), (83,OpenMP,#pragma omp parallel for), (87,OpenMP,#pragma omp parallel for), (91,OpenMP,#pragma omp parallel for), (95,OpenMP,#pragma omp parallel for), (99,OpenMP,#pragma omp parallel for), (103,OpenMP,#pragma omp parallel for), (107,OpenMP,#pragma omp parallel for), (111,OpenMP,#pragma omp parallel for), (113,OpenMP,#pragma omp parallel for), (117,OpenMP,#pragma omp parallel for), (122,OpenMP,#pragma omp parallel for), (127,OpenMP,#pragma omp parallel for)", "label": "(82, OpenMP, #pragma omp distribute parallel for lastprivate(xa)); (15, OpenMP, #pragma omp distribute parallel for lastprivate(); (59, OpenMP, #pragma omp distribute parallel for lastprivate(2 * 2)); (73, OpenMP, #pragma omp distribute parallel for lastprivate(da)); (93, OpenMP, #pragma omp distribute parallel for safelen(5)); (137, OpenMP, #pragma omp distribute parallel for lastprivate(j)); (25, OpenMP, #pragma omp distribute parallel for lastprivate(argc,); (2, OpenMP, #pragma omp target); (109, OpenMP, #pragma omp distribute parallel for lastprivate(B::x)); (62, OpenMP, #pragma omp distribute parallel for lastprivate(ca)); (138, OpenMP, #pragma omp distribute parallel for firstprivate(m) lastprivate(m)); (116, OpenMP, #pragma omp distribute parallel for lastprivate(i)); (142, OpenMP, #pragma omp distribute parallel for lastprivate(n) firstprivate(n)); (94, OpenMP, #pragma omp distribute parallel for lastprivate(m)); (32, OpenMP, #pragma omp distribute parallel for lastprivate(S1)); (97, OpenMP, #pragma omp distribute parallel for lastprivate(h)); (-5, OpenMP, #pragma omp distribute parallel for lastprivate); (39, OpenMP, #pragma omp distribute parallel for lastprivate(argv[1])); (18, OpenMP, #pragma omp distribute parallel for lastprivate(argc > 0 ? argv[1] : argv[2])); (32, OpenMP, #pragma omp distribute parallel for lastprivate(a, b, c, d, f)); (140, OpenMP, #pragma omp distribute parallel for lastprivate(si)); (20, OpenMP, #pragma omp distribute parallel for lastprivate(argc)); (80, OpenMP, #pragma omp distribute parallel for lastprivate(e, g)); (3, OpenMP, #pragma omp distribute parallel for lastprivate(argc); (42, OpenMP, #pragma omp distribute parallel for lastprivate(ba)); (62, OpenMP, #pragma omp distribute parallel for lastprivate(S2::S2s)); (66, OpenMP, #pragma omp distribute parallel for lastprivate(S2::S2sc)); (95, OpenMP, #pragma omp distribute parallel for private(xa), lastprivate(xa)); (-7, OpenMP, #pragma omp distribute parallel for lastprivate()); (-19, OpenMP, #pragma omp teams)"}
{"predict": "(107,OpenMP,#pragma omp parallel for), (118,OpenMP,#pragma omp parallel for)", "label": "(101, OpenMP, printf(\"Number of threads    = %d\\n\", omp_get_max_threads());); (24, MPI, MPI_Comm_size(MPI_COMM_WORLD,&Num_procs);); (148, MPI, MPI_Finalize();); (104, OpenMP, #pragma omp parallel for); (74, MPI, MPI_Bcast(&length,        1, MPI_LONG, root, MPI_COMM_WORLD);); (21, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&my_ID);); (75, MPI, MPI_Bcast(&nthread_input, 1, MPI_INT,  root, MPI_COMM_WORLD);); (76, OpenMP, omp_set_num_threads(nthread_input);); (16, MPI, MPI_Init(&argc,&argv);); (71, MPI, MPI_Bcast(&iterations,    1, MPI_INT,  root, MPI_COMM_WORLD);); (113, MPI, MPI_Barrier(MPI_COMM_WORLD);); (68, MPI, MPI_Bcast(&offset,        1, MPI_LONG, root, MPI_COMM_WORLD);)"}
{"predict": "(21,OpenMP,#pragma omp parallel sections), (21,OpenMP,#pragma omp section), (27,OpenMP,#pragma omp section), (33,OpenMP,#pragma omp section), (39,OpenMP,#pragma omp section), (45,OpenMP,#pragma omp section), (51,OpenMP,#pragma omp section)", "label": "(18, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for private(sfvar) reduction(+:g,g1,svar))", "label": "(9, OpenMP, #pragma omp target); (10, OpenMP, #pragma omp distribute simd firstprivate(g, g1, svar, sfvar)); (46, OpenMP, #pragma omp distribute simd firstprivate(t_var, vec, s_arr, s_arr, var, var, svar)); (7, OpenMP, #pragma omp teams)"}
{"predict": "(28,OpenMP,#pragma omp parallel for)", "label": "(7, CUDA, __shared__ block_radix_sort_t::TempStorage temp_storage;)"}
{"predict": "(16,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for private(is_1,is_12) reduction(+:is_1,is_12))", "label": "(12, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(18,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for reduction(+:result_host))", "label": "(33, CUDA, cudaFree(d_inPtr);); (21, CUDA, cudaMalloc((void**)&d_outPtr, size * sizeof(float));); (26, CUDA, cudaMemcpy(&result_gpu, &d_outPtr[0], sizeof(float), cudaMemcpyDeviceToHost);); (21, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);); (28, CUDA, cudaFree(d_outPtr);); (16, CUDA, cudaMalloc((void**)& d_inPtr, size * sizeof(float));)"}
{"predict": "(9,OpenMP,#pragma omp parallel for)", "label": "(3, TBB, tbb::flow::broadcast_node<my_type> my_broadcast_node(g);); (7, TBB, const tbb::tick_count t0 = tbb::tick_count::now();); (14, TBB, return (tbb::tick_count::now() - t0).seconds();); (-1, TBB, tbb::flow::queue_node<my_type> my_queue(g);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for private(i, derr, err, xnorm, errcomp) reduction(max:err) reduction(max:xnorm) reduction(max:errcomp))", "label": "(26, MPI, MPI_Allreduce( &temperr, &errcomp, 1, MPI_FLOAT, MPI_MAX, slucomm);); (23, MPI, MPI_Allreduce( &tempxnorm, &xnorm, 1, MPI_DOUBLE, MPI_MAX, slucomm);); (20, MPI, MPI_Allreduce( &temperr, &err, 1, MPI_DOUBLE, MPI_MAX, slucomm);)"}
{"predict": "(26,OpenMP,#pragma omp parallel for)", "label": "(8, OpenMP, #pragma omp target map(present, alloc: arr[0:0])); (6, OpenMP, #pragma omp target data map(alloc: arr[0:5])); (19, OpenMP, #pragma omp target data map(alloc: arr[0:0]))"}
{"predict": "(6,OpenMP,#pragma omp parallel sections), (6,OpenMP,#pragma omp section), (7,OpenMP,#pragma omp section)", "label": "(14, MPI, err = MPI_Barrier(ab_c_inter);); (16, MPI, err = MPI_Intercomm_merge(ab_c_inter, 0, &abc_intra);); (25, MPI, MPI_Comm_disconnect(&ab_inter);); (21, MPI, MPI_Comm_free(&ab_intra);); (18, MPI, MPI_Comm_free(&abc_intra);); (5, MPI, err = MPI_Intercomm_create(ab_intra, 0, ac_intra, count, tag, &ab_c_inter);); (17, MPI, MPI_Comm_free(&ab_c_inter);); (18, MPI, MPI_Comm_free(&ac_intra);); (20, MPI, MPI_Comm_disconnect(&ac_inter);); (10, MPI, err = MPI_Barrier(abc_intra);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for), (35,OpenMP,#pragma omp parallel for)", "label": "(2, MPI, MPI_Init(&argc, &argv);); (55, MPI, MPI_Finalize();)"}
{"predict": "(6,OpenMP,#pragma omp parallel for)", "label": "(5, OpenMP, #pragma omp parallel num_threads(2)); (3, OpenMP, omp_set_nested(1);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for private(e, status) reduction(?, err))", "label": "(23, OpenCL, clReleaseEvent(events[i]);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp parallel for)"}
{"predict": ":             if (!silent)\n162:                 output.display();\n163:             assert(output.get_value() == high);\n164:         }\n165:     }\n166: \n167:     return 0;\n168: }", "label": "(720, TBB, utility::report_elapsed_time((oneapi::tbb::tick_count::now() - start).seconds());); (15, TBB, oneapi::tbb::global_control c(oneapi::tbb::global_control::max_allowed_parallelism, p);); (19, TBB, oneapi::tbb::flow::graph g;); (11, TBB, oneapi::tbb::tick_count start = oneapi::tbb::tick_count::now();)"}
{"predict": "(15,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for), (45,OpenMP,#pragma omp parallel for), (64,OpenMP,#pragma omp parallel for), (68,OpenMP,#pragma omp parallel for)", "label": "(47, CUDA, CHECK(cudaStreamCreate(&(streams[n])));); (84, CUDA, CHECK(cudaStreamDestroy(streams[i]));); (94, CUDA, CHECK(cudaFree(d_y));); (19, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice));); (93, CUDA, CHECK(cudaFree(d_z));); (53, CUDA, CHECK(cudaStreamDestroy(streams[n]));); (58, CUDA, CHECK(cudaMallocHost(&h_x2, M));); (82, CUDA, CHECK(cudaFreeHost(h_y2));); (82, CUDA, CHECK(cudaFreeHost(h_z2));); (66, CUDA, CHECK(cudaStreamCreate(&(streams[i])));); (10, CUDA, CHECK(cudaMalloc(&d_y, M));); (8, CUDA, CHECK(cudaMalloc(&d_x, M));); (83, CUDA, CHECK(cudaFree(d_x));); (53, CUDA, CHECK(cudaMallocHost(&h_z2, M));); (7, CUDA, CHECK(cudaMalloc(&d_z, M));); (73, CUDA, CHECK(cudaFreeHost(h_x2));); (7, CUDA, CHECK(cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice));); (48, CUDA, CHECK(cudaMallocHost(&h_y2, M));)"}
{"predict": "(16,OpenMP,#pragma omp parallel for private(fileh, fileh_std))", "label": "(14, MPI, MPI_Init(&argc, &argv);); (47, MPI, MPI_Finalize();); (33, MPI, MPI_Barrier(MPI_COMM_WORLD);); (12, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (40, MPI, CHECK(MPI_File_delete(TEST_FILENAME, MPI_INFO_NULL));); (27, MPI, CHECK(MPI_File_close(&fileh));); (19, MPI, CHECK(MPI_File_set_view(fileh, 0, MPI_INT, MPI_INT, datarep[i], MPI_INFO_NULL));)"}
{"predict": "(19,OpenMP,#pragma omp parallel for private(latency, t0, status))", "label": "(28, MPI, MPI_Recv(NULL, 0, MPI_BYTE, p->nbor2, 1, MPI_COMM_WORLD, &status);); (22, MPI, MPI_Recv(NULL, 0, MPI_BYTE, p->nbor, 1, MPI_COMM_WORLD, &status);); (25, MPI, MPI_Send(NULL, 0, MPI_BYTE, p->nbor2, 1, MPI_COMM_WORLD);); (34, MPI, latency = (MPI_Wtime() - t0) / (2 * g_latency012_reps);); (18, MPI, MPI_Send(NULL, 0, MPI_BYTE, p->nbor, 1, MPI_COMM_WORLD);); (10, MPI, t0 = MPI_Wtime();)"}
{"predict": "(39,OpenMP,#pragma omp parallel for), (67,OpenMP,#pragma omp parallel for reduction(+:errors))", "label": "(20, CUDA, cudaGetDeviceProperties(&devProp, 0);); (81, CUDA, cudaFree(gpuMatrix);); (43, CUDA, cudaMalloc((void**)&gpuMatrix, NUM * sizeof(float));); (81, CUDA, cudaFree(gpuTransposeMatrix);); (43, CUDA, cudaMalloc((void**)&gpuTransposeMatrix, NUM * sizeof(float));)"}
{"predict": "(26,OpenMP,#pragma omp parallel for private(i))", "label": "(24, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (19, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (50, MPI, MPI_Finalize();); (47, MPI, MPI_T_finalize();); (18, MPI, MPI_T_init_thread(required, &provided);); (31, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for)", "label": "(22, MPI, MPI_Gatherv_c(sendbuf, count, datatype, recvbuf, counts, displs, datatype, root, comm);); (26, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);)"}
{"predict": "comparison ('<', '<=', '>', or '>=') of loop variable 'i'}} omp5-error@+2 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', '>=', or '!=') of loop variable 'i'}}\n119: #pragma omp taskloop\n120:   for (int i = 0; i < 1; i += 1)\n121:     c[i] = a[i];\n122: \n123: #pragma omp parallel\n124: // omp4-error@+2 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', or '>=') of loop variable 'i'}} omp5-error@+2 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', '>=', or '!=') of loop variable 'i'}}\n125: #pragma omp taskloop\n126:   for (int i = 0; i < 1; i += 1u)\n127:     c[i] = a[i];\n128: \n129: #pragma omp parallel\n130: // omp4-error@+2 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', or '>=') of loop variable 'i'}} omp5-error@+2 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', '>=', or '!=') of loop variable 'i'}}\n131: #pragma omp taskloop\n132:   for (int i = 0; i < 1; i += 1l)\n133:     c[i] = a[i];\n134: \n135: #pragma omp parallel\n136: // omp4-error@+2 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', or '>=') of loop variable 'i'}} omp5-error@+2 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', '>=', or '!=') of loop variable 'i'}}\n137: #pragma omp taskloop\n138:   for (int i = 0; i < 1; i += 1ul)\n139:     c[i] =", "label": "(286, OpenMP, #pragma omp taskloop lastprivate(ii)); (280, OpenMP, #pragma omp taskloop private(ii)); (5, OpenMP, #pragma omp parallel); (5, OpenMP, #pragma omp taskloop); (317, OpenMP, #pragma omp taskloop collapse(2)); (265, OpenMP, #pragma omp taskloop firstprivate(ii)); (270, OpenMP, #pragma omp taskloop linear(ii))"}
{"predict": "(26,OpenMP,#pragma omp parallel sections), (26,OpenMP,#pragma omp section), (30,OpenMP,#pragma omp section), (32,OpenMP,#pragma omp section), (34,OpenMP,#pragma omp section)", "label": "(40, MPI, MPI_T_finalize();); (15, MPI, MPI_T_init_thread(required, &provided);)"}
{"predict": "(29,OpenMP,#pragma omp parallel for), (42,OpenMP,#pragma omp parallel for), (63,OpenMP,#pragma omp parallel for), (87,OpenMP,#pragma omp parallel for)", "label": "(104, OpenCL, retVal = clReleaseKernel(kernel);); (34, OpenCL, context = clCreateContext(nullptr, 1, &device, nullptr, nullptr, &retVal);); (108, OpenCL, retVal = clReleaseCommandQueue(cmdQ);); (110, OpenCL, retVal = clReleaseContext(context);); (57, OpenCL, kernel = clCreateKernel(pProgram, \"CopyBuffer\", &retVal);); (102, OpenCL, retVal = clReleaseProgram(pProgram);); (33, OpenCL, cmdQ = clCreateCommandQueue(context, device, properties, &retVal);)"}
{"predict": "(13,OpenMP,#pragma omp target teams distribute parallel for collapse(2))", "label": "(14, CUDA, return CUDA(cudaGetLastError());)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(1, OpenMP, #pragma omp target teams); (5, OpenMP, #pragma omp parallel)"}
{"predict": "(\"Data type              = double\\n\");\n119: #else\n120:     printf(\"Data type              = float\\n\");\n121: #endif\n122:     printf(\"Compact representation = %s\\n\", STAR ? \"yes\" : \"no\");\n123:     printf(\"Number of iterations   = %d\\n\", iterations);\n124:     printf(\"Tile size              = %d/%d\\n\", width, height);\n125:     printf(\"Number of active points= %ld\\n\", f_active_points);\n126:     printf(\"Number of flops        = %ld\\n\", flops);\n127:   }\n128: \n129:   /* tile the grid and assign each rank a grid section */\n130:   width  = n/Num_procsx;\n131:   height = n/Num_procsy;\n132:   leftover = n%Num_procsx;\n133:   if (my_IDx < leftover) width++;\n134:   leftover = n%Num_procsy;\n135:   if (my_IDy < leftover) height++;\n136:   istart = my_IDx*width;\n137:   if (my_IDx < leftover) istart += my_IDx;\n138:   else istart += leftover;\n139:   iend = istart + width - 1;\n140:   jstart = my_IDy*height;\n141:   if (my_IDy < leftover) jstart += my_IDy;\n142:   else jstart += leftover;\n143:   jend = jstart + height - 1;\n144: \n145:   /* compute the size of the input and output arrays */\n146:   total_length_in  = (width+2*RADIUS)*(height+2*RADIUS);\n147:   total_length_out = width*height;\n148: \n149:   /* allocate space for the input and output grids */\n150:   in  = (DTYPE *) prk_malloc(total_length_in*sizeof(DTYPE));\n151:   out = (DTYPE *) prk_malloc(total_length_out*sizeof(DTYPE));\n152: \n153:   /* fill the stencil weights to reflect a discrete divergence operator */", "label": "(210, MPI, MPI_Info_create(&rma_winfo);); (212, MPI, MPI_Info_set(rma_winfo, \"no_locks\", \"true\");); (370, MPI, MPI_Info_free(&rma_winfo);); (371, MPI, MPI_Finalize();); (128, MPI, MPI_Bcast(&n,          1, MPI_INT, root, MPI_COMM_WORLD);); (128, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (267, MPI, MPI_Win_fence(MPI_MODE_NOSTORE, rma_winx);); (39, MPI, MPI_Init(&argc,&argv);); (232, MPI, MPI_Barrier(MPI_COMM_WORLD);); (321, MPI, MPI_Reduce(&local_norm, &norm, 1, MPI_DTYPE, MPI_SUM, root, MPI_COMM_WORLD);); (38, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (234, MPI, MPI_Win_fence(MPI_MODE_NOSTORE, rma_winy);); (35, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(28,OpenMP,#pragma omp parallel for)", "label": "(28, OpenCL, retVal = clReleaseMemObject(image);); (6, OpenCL, auto status = clGetDeviceInfo(&mockClDevice, CL_DEVICE_IMAGE_SUPPORT, sizeof(imageSupportInfo), &imageSupportInfo, nullptr);)"}
{"predict": "(21,OpenMP,#pragma omp parallel for collapse(2))", "label": "(12, MPI, MPI_Init(&argc, &argv);); (34, MPI, MPI_Finalize();); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (11, MPI, MPI_Dims_create(size, NR_DIMS, dims);); (17, MPI, MPI_Cart_coords(cart_comm, rank, NR_DIMS, cart_coords);); (27, MPI, MPI_Barrier(MPI_COMM_WORLD);); (13, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(20,OpenMP,#pragma omp parallel for)", "label": "(27, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(27,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for), (29,OpenMP,#pragma omp parallel for)", "label": "(39, CUDA, cudaMalloc((void **)&df_C, M * N * sizeof(float));); (62, CUDA, cudaFree(df_A);); (36, CUDA, cudaMalloc((void **)&df_B, K * N * sizeof(float));); (38, CUDA, cudaStat = cudaStreamCreate(&stream);); (61, CUDA, cudaFree(df_C);); (32, CUDA, cudaMalloc((void **)&df_A, M * K * sizeof(float));); (55, CUDA, cudaStreamDestroy(stream);); (48, CUDA, cudaStreamSynchronize(stream);); (56, CUDA, cudaFree(df_B);)"}
{"predict": "%d and GPU%d: %f ms\\n\",\n104:          gpuid[0], gpuid[1], time_memcpy);\n105: \n106:   // Free buffers\n107:   checkCudaErrors(cudaFreeHost(h0));\n108:   checkCudaErrors(cudaFree(g0));\n109:   checkCudaErrors(cudaFree(g1));\n110: \n111:   printf(\"[%s] - Done.\\n\", argv[0]);\n112:   return 0;\n113: }", "label": "(46, CUDA, checkCudaErrors(cudaDeviceCanAccessPeer(&can_access_peer, i, j));); (99, CUDA, checkCudaErrors(cudaEventCreateWithFlags(&stop_event, eventflags));); (101, CUDA, checkCudaErrors(cudaEventRecord(start_event, 0));); (107, CUDA, checkCudaErrors(cudaMemcpy(g1, g0, buf_size, cudaMemcpyDefault));); (190, CUDA, checkCudaErrors(cudaFreeHost(h0));); (142, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (156, CUDA, checkCudaErrors(cudaMemcpy(h0, g0, buf_size, cudaMemcpyDefault));); (109, CUDA, checkCudaErrors(cudaEventRecord(stop_event, 0));); (6, CUDA, checkCudaErrors(cudaGetDeviceCount(&gpu_n));); (66, CUDA, checkCudaErrors(cudaSetDevice(gpuid[0]));); (181, CUDA, checkCudaErrors(cudaFree(g0));); (20, CUDA, checkCudaErrors(cudaGetDeviceProperties(&prop[i], i));); (74, CUDA, checkCudaErrors(cudaMalloc(&g0, buf_size));); (176, CUDA, checkCudaErrors(cudaEventDestroy(stop_event));); (62, CUDA, checkCudaErrors(cudaDeviceEnablePeerAccess(gpuid[1], 0));); (97, CUDA, checkCudaErrors(cudaMemcpy(g0, g1, buf_size, cudaMemcpyDefault));); (102, CUDA, checkCudaErrors(cudaEventElapsedTime(&time_memcpy, start_event, stop_event));); (171, CUDA, checkCudaErrors(cudaEventDestroy(start_event));); (59, CUDA, checkCudaErrors(cudaSetDevice(gpuid[1]));); (70, CUDA, checkCudaErrors(cudaMalloc(&g1, buf_size));); (58, CUDA, checkCudaErrors(cudaDeviceEnablePeerAccess(gpuid[0], 0));); (163, CUDA, checkCudaErrors(cudaDeviceDisablePeerAccess(gpuid[0]));); (160, CUDA, checkCudaErrors(cudaDeviceDisablePeerAccess(gpuid[1]));); (76, CUDA, checkCudaErrors(cudaEventCreateWithFlags(&start_event, eventflags));); (173, CUDA, checkCudaErrors(cudaSetDevice(i));); (107, CUDA, checkCudaErrors(cudaMemcpy(g0, h0, buf_size, cudaMemcpyDefault));); (91, CUDA, checkCudaErrors(cudaEventSynchronize(stop_event));); (166, CUDA, checkCudaErrors(cudaFree(g1));)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(3, CUDA, Check_CudaErroorFun(\"At the beginning.\");)"}
{"predict": "(20,OpenMP,#pragma omp parallel for)", "label": "(4, MPI, MPI_Comm_size(result.global, &result.size);); (4, MPI, MPI_Comm_rank(result.global, &result.rank);); (1, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &result.global);); (15, MPI, MPI_Comm_rank(result.local, &result.local_rank);); (12, MPI, MPI_Comm_split(result.global, result.group, result.rank, &result.local);); (-3, MPI, MPI_Init(nullptr, nullptr);); (11, MPI, MPI_Comm_size(result.local, &result.local_size);)"}
{"predict": "map_item_update(dst_map, key, DCMPF_CONTENT, DCMPS_DIFFER);\n143:             }\n144:             continue;\n145:         }\n146: \n147:         /* compare file contents */\n148:         int compare_rc = mfu_compare_contents(src_list, src_index, dst_list, dst_index, copy_opts);\n149:         if (compare_rc == -1) {\n150:             /* we hit an error while reading the file */\n151:             rc = -1;\n152:             MFU_LOG(MFU_LOG_ERR,\n153:                 \"Failed to read on %s and/or %s. Assuming contents are different.\",\n154:                 mfu_flist_file_get_name(src_list, src_index),\n155:                 mfu_flist_file_get_name(dst_list, dst_index));\n156:         }\n157:         if (!compare_rc) {\n158:             /* update to say contents were found to be the same */\n159:             dcmp_strmap_item_update(src_map, key, DCMPF_CONTENT, DCMPS_COMMON);\n160:             dcmp_strmap_item_update(dst_map, key, DCMPF_CONTENT, DCMPS_COMMON);\n161:         } else {\n162:             /* update to say contents were found to be different */\n163:             dcmp_strmap_item_update(src_map, key, DCMPF_CONTENT, DCMPS_DIFFER);\n164:             dcmp_strmap_item_update(dst_map, key, DCMPF_CONTENT, DCMPS_DIFFER);\n165:         }\n166:     }\n167: \n168:     time(&time_ended);\n169: \n170:     /* compute elapsed time */\n171:     double time_elapsed = difftime(time_ended, time_started);\n172: \n173:     /* report the result */\n174:     if (mfu_debug_level >= MFU_LOG_VERBOSE && mfu_rank == 0) {\n175:         MFU_LOG(MFU_LOG_INFO, \"Finished comparing items in %.3f sec\", time_elapsed);\n176:     }\n177: \n178:     /* free memory allocated for compare lists */\n179:     m", "label": "(17, MPI, MPI_Barrier(MPI_COMM_WORLD);); (26, MPI, double start_compare = MPI_Wtime();); (199, MPI, double end_compare = MPI_Wtime();)"}
{"predict": "(18,OpenMP,#pragma omp parallel for)", "label": "(24, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(32,OpenMP,#pragma omp parallel sections), (33,OpenMP,#pragma omp section), (36,OpenMP,#pragma omp section)", "label": "(54, MPI, MPI_Comm_free(&comm2);); (46, MPI, MPI_Ssend(buffer, 0, MPI_INT, rank, 1, MPI_COMM_WORLD);); (31, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &comm2);); (43, MPI, MPI_Barrier(comm4);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (38, MPI, MPI_Comm_dup(comm1, &comm4);); (46, MPI, MPI_Comm_free(&comm4);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (45, MPI, MPI_Comm_free(&comm1);); (39, MPI, MPI_Recv(buffer, 0, MPI_INT, rank, 2, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (30, MPI, MPI_Ssend(buffer, 0, MPI_INT, rank, 0, MPI_COMM_WORLD);); (21, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &comm1);)"}
{"predict": "report.time;\n113:                 }\n114:             }\n115:             bandwidth = aggregate_number_of_messages * size / aggregate_time;\n116:             message_rate = aggregate_number_of_messages / aggregate_time;\n117:             latency = max_time / aggregate_number_of_messages;\n118:             fprintf(unit, \" %12zd %12zd %12.3f %12.3f %12.3f\\n\", size, aggregate_number_of_messages, max_time * 1.0e6, bandwidth / 1.0e6, message_rate);\n119:             fflush(unit);\n120:         }\n121:     }\n122:     for (i = 0; i < NUMBER_OF_NEIGHBORS; i++) {\n123:         imb_p2p_free_mem(send_buffers[i]);\n124:     }\n125:     for (i = 0; i < NUMBER_OF_NEIGHBORS; i++) {\n126:         imb_p2p_free_mem(recv_buffers[i]);\n127:     }\n128: }", "label": "(75, MPI, MPI_Waitall((NUMBER_OF_NEIGHBORS * 2), requests, MPI_STATUSES_IGNORE);); (97, MPI, time = MPI_Wtime() - time;); (71, MPI, MPI_Isend(send_buffers[i], size, MPI_BYTE, neighbours[i], 0, MPI_COMM_WORLD, &requests[NUMBER_OF_NEIGHBORS + i]);); (78, MPI, time = MPI_Wtime();); (101, MPI, MPI_Send(&report, sizeof(imb_p2p_report_t), MPI_BYTE, 0, 1, MPI_COMM_WORLD);); (65, MPI, MPI_Irecv(recv_buffers[i], size, MPI_BYTE, neighbours[i], 0, MPI_COMM_WORLD, &requests[i]);); (108, MPI, MPI_Recv(&report, sizeof(imb_p2p_report_t), MPI_BYTE, source_rank, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)"}
{"predict": "(30,OpenMP,#pragma omp parallel for reduction(+:sumCPU)), (...)", "label": "(59, CUDA, cudaEventRecord( evStop );); (75, CUDA, cudaFree( in );); (72, CUDA, cudaEventDestroy( evStart );); (52, CUDA, cudaEventRecord( evStart );); (73, CUDA, cudaFree( out );); (70, CUDA, cudaEventDestroy( evStop );)"}
{"predict": "(37,OpenMP,#pragma omp parallel for)", "label": "(35, OpenMP, #pragma omp target teams distribute parallel for map(a, b))"}
{"predict": "(70,OpenMP,#pragma omp parallel for private(ref, delta) reduction(+:sumDelta, sumRef)), (112,OpenMP,#pragma omp parallel for private(d, ref, delta) reduction(+:sumDelta, sumRef))", "label": "(40, CUDA, checkCudaErrors(cudaMemset(d_Output, 0, QRNG_DIMENSIONS * N * sizeof(float)));); (44, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (124, CUDA, checkCudaErrors(cudaFree(d_Output));)"}
{"predict": "(15,OpenMP,#pragma omp parallel for collapse(3))", "label": "(15, CUDA, qmc_cuda::cuda_check(cudaGetLastError(), \"vKKwij_to_vwKiKj\");); (15, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize(), \"vKKwij_to_vwKiKj\");)"}
{"predict": "(16,OpenMP,#pragma omp parallel for)", "label": "(9, OpenCL, auto userEvent = clCreateUserEvent(pContext, &retVal);); (30, OpenCL, retVal = clReleaseEvent(userEvent);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for)", "label": "(10, OpenMP, #pragma omp teams distribute parallel for); (8, OpenMP, #pragma omp target); (12, OpenMP, #pragma omp teams distribute parallel for if (parallel: false))"}
{"predict": "(53,OpenMP,#pragma omp parallel for)", "label": "(12, MPI, MPI_Pack_external_size((char *) \"external32\", 1, MPI_INT, &sizeofint);); (66, MPI, MPI_Type_free(&parent_type);); (36, MPI, MPI_Pack_external((char *) \"external32\", array, 1, parent_type, buf, sizeoftype, &position);); (46, MPI, MPI_Unpack_external((char *) \"external32\", buf, sizeoftype, &position, array, 1, parent_type);); (21, MPI, MPI_Pack_external_size((char *) \"external32\", 1, parent_type, &sizeoftype);); (18, MPI, MPI_Type_commit(&parent_type);); (15, MPI, MPI_Type_vector(10, 2, 2, MPI_INT, &parent_type);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for collapse(2)), (56,OpenMP,#pragma omp parallel for reduction(+:checksum))", "label": "(82, SYCL, auto isA_fp16 = (typeid(TA) == typeid(sycl::half));); (28, SYCL, sycl::free(h_a, q);); (6, SYCL, auto h_c = sycl::malloc_host<TC>( nelems, q);); (4, SYCL, auto h_b = sycl::malloc_host<TB>( nelems, q);); (111, SYCL, sycl::free(h_c, q);); (14, SYCL, auto  A = sycl::malloc_device<TA>( nelems, q);); (94, SYCL, auto isC_fp16 = (typeid(TC) == typeid(sycl::half));); (23, SYCL, sycl::free(h_b, q);); (49, SYCL, sycl::free(A, q);); (47, SYCL, sycl::free(B, q);); (-4, SYCL, auto h_a = sycl::malloc_host<TA>( nelems, q);); (80, SYCL, auto isB_fp16 = (typeid(TB) == typeid(sycl::half));); (9, SYCL, auto  C = sycl::malloc_device<TC>( nelems, q);); (7, SYCL, auto  B = sycl::malloc_device<TB>( nelems, q);); (41, SYCL, sycl::free(C, q);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(16, MPI, mpi_errno = MPI_Comm_create_group(dup_comm, group, 10, NULL);); (20, MPI, MPI_Comm_free(&dup_comm);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (18, MPI, MPI_Group_free(&group);); (12, MPI, MPI_Error_class(mpi_errno, &errclass);); (7, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (5, MPI, MPI_Comm_group(dup_comm, &group);); (3, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &dup_comm);)"}
{"predict": "(17,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for), (48,OpenMP,#pragma omp parallel for reduction(+:tMeanLower)), (51,OpenMP,#pragma omp parallel for reduction(+:tMeanHigher))", "label": "(24, MPI, t[i] = MPI_Wtime() - ttmp;); (19, MPI, MPI_Type_commit(&xpose[j]);); (34, MPI, MPI_Type_free(&column[j]);); (32, MPI, MPI_Type_free(&xpose[j]);); (11, MPI, ttmp = MPI_Wtime();); (13, MPI, MPI_Type_vector(nrows, 1, ncols, MPI_INT, &column[j]);); (13, MPI, MPI_Type_create_hvector(ncols, 1, sizeof(int), column[j], &xpose[j]);)"}
{"predict": "(9,OpenMP,#pragma omp parallel for)", "label": "(18, CUDA, ASSERT_EQ(cudaGetLastError(), cudaSuccess) << \"Kernel launch error.\";)"}
{"predict": "(21,OpenMP,#pragma omp parallel for collapse(2)), (52,OpenMP,#pragma omp parallel for collapse(2)), (58,OpenMP,#pragma omp parallel for collapse(2)), (65,OpenMP,#pragma omp parallel for collapse(2))", "label": "(3, CUDA, __shared__ float buffer[128];); (3, CUDA, __shared__ float sums[4];); (0, CUDA, __shared__ float gauss[16];)"}
{"predict": "(8,OpenMP,#pragma omp parallel for private(newop, block_no, pos, lseek_rc, nread, remainder, ibuf, inSize, outSize, ret))", "label": "(26, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(5,OpenMP,#pragma omp parallel for private(i))", "label": "(12, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(15, CUDA, checkCudaErrors(cudaGetLastError());); (13, CUDA, checkCudaErrors(cudaStreamSynchronize(stream));)"}
{"predict": "22: \n123:     *new_count = count;\n124:     return rc;\n125: }", "label": "(14, MPI, rc = MPI_Type_get_extent(type, &tmp_lb, &attr->child_type_extent);); (127, MPI, rc = MPI_Type_free(&type);)"}
{"predict": "(14,OpenMP,#pragma omp parallel sections), (15,OpenMP,#pragma omp section)", "label": "(20, OpenMP, #pragma omp target update to(mapper(id): fd)); (10, OpenMP, #pragma omp declare mapper(id: N1::vec v) map(v.len)); (14, OpenMP, #pragma omp target map(mapper(id) alloc: fd)); (15, OpenMP, #pragma omp target map(mapper(idd) alloc: fd.b)); (19, OpenMP, #pragma omp target update from(mapper(idd): fd.b)); (4, OpenMP, #pragma omp declare mapper(id: struct foodat v) map(v.a)); (15, OpenMP, #pragma omp target update to(mapper(idd): fd.b)); (3, OpenMP, #pragma omp declare mapper(idd: struct foodatchild v) map(v.k)); (14, OpenMP, #pragma omp target update from(mapper(id): fd))"}
{"predict": "(7,OpenMP,#pragma omp parallel for)", "label": "(4, CUDA, cudaMemcpy(state.vx, state.dvx, size_f, cudaMemcpyDeviceToHost);); (4, CUDA, cudaMemcpy(state.vy, state.dvy, size_f, cudaMemcpyDeviceToHost);); (4, CUDA, cudaMemcpy(state.rho, state.drho, size_f, cudaMemcpyDeviceToHost);)"}
{"predict": "(24,OpenMP,#pragma omp parallel for private(pivot, numOfEq, k, arr, length))", "label": "(12, CUDA, __shared__ T tails[SMALL_KERNEL_BLOCK * SMALL_KERNEL_BLOCK];); (18, CUDA, extern __shared__ char _arrays[];)"}
{"predict": "2 failed\\n\");\n150: \n151:     // Read the results\n152:     for (auto j = gMinVectorSizeIndex; j < gMaxVectorSizeIndex; j++)\n153:     {\n154:         r = (cl_half *)malloc(buffer_size);\n155:         if (NULL == r)\n156:         {\n157:             vlog(\"Error: unable to allocate memory for output array\\n\");\n158:             return CL_OUT_OF_HOST_MEMORY;\n159:         }\n160: \n161:         error = clEnqueueReadBuffer(tinfo->tQueue, tinfo->outBuf[j], CL_TRUE, 0,\n162:                                       buffer_size, r, 0, NULL, NULL);\n163:         if (error)\n164:         {\n165:             vlog_error(\"Error: unable to read output array\\n\");\n166:             return error;\n167:         }\n168: \n169:         // Convert to float and compare\n170:         s.resize(buffer_elements);\n171:         s2.resize(buffer_elements);\n172:         for (cl_uint k = 0; k < buffer_elements; k++)\n173:         {\n174:             s[k] = convert_half_to_float(r[k]);\n175:             s2[k] = convert_half_to_float(p2[k]);\n176:         }\n177: \n178:         // Check if the results are as expected\n179:         bool pass = true;\n180:         for (cl_uint k = 0; k < buffer_elements; k++)\n181:         {\n182:             if (fabs(s[k] - func(p[k], p2[k])) > ulps)\n183:             {\n184:                 vlog_error(\"ERROR: %s: %f != %f (expected %f)\\n\", name, s[k],\n185:                            func(p[k], p2[k]), func(p[k], p2[k]));\n186:                 pass = false;\n187:             }\n188:         }\n189:         if (!pass) return CL_FAIL;\n190:     }\n191: \n192:", "label": "(134, OpenCL, error = clSetKernelArg(kernel, 1, sizeof(tinfo->inBuf), &tinfo->inBuf);)"}
{"predict": "(12,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &myid);); (23,OpenMP,#pragma omp parallel for), (...)", "label": "(45, MPI, ierr = MPI_Finalize();); (15, MPI, ierr = MPI_Comm_rank(MPI_COMM_WORLD, &myid);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for private(i, dims) reduction(+:errs))", "label": "(9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (14, MPI, MPI_Dims_create(tests[k].size, tests[k].dim, dims);)"}
{"predict": "(25,OpenMP,#pragma omp parallel for), (26,OpenMP,private(ent))", "label": "(8, MPI, MPI_Finalize();); (0, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for)", "label": "(5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(23,OpenMP,#pragma omp parallel for), (53,OpenMP,#pragma omp parallel for), (59,OpenMP,#pragma omp parallel for), (65,OpenMP,#pragma omp parallel for)", "label": "(21, CUDA, CUASSERT(cudaMemsetAsync(sortClassValidCountPtr, 0, N * sizeof(int), stream));); (19, CUDA, CUASSERT(cudaMemsetAsync(argMaxBBoxPtr, 0, N * samples * 4 * sizeof(float), stream));); (20, CUDA, CUASSERT(cudaMemsetAsync(sortClassPosPtr, 0, N * (NClass + 1) * sizeof(int), stream));); (20, CUDA, CUASSERT(cudaMemsetAsync(sortClassSampleIdxPtr, 0, N * (samples + 1) * sizeof(int), stream));); (15, CUDA, CUASSERT(cudaMemsetAsync(argMaxScorePtr, 0, N * samples * sizeof(float), stream));)"}
{"predict": "(3,OpenMP,#pragma omp parallel for), (9,OpenMP,#pragma omp parallel for private(random_milliseconds, send_times, tag, pre_info))", "label": "(11, MPI, MPI_Send(pre_info, 2, MPI_INT, 0, kTag, MPI_COMM_WORLD);)"}
{"predict": "(51,OpenMP,#pragma omp parallel for)", "label": "(32, OpenCL, err = clEnqueueWriteBuffer(queue, in, CL_TRUE, 0, bytes, &h_in[0], 0, NULL, NULL);); (44, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(cl_mem), &in);); (51, OpenCL, err = clEnqueueReadBuffer(queue, in, CL_TRUE, 0, bytes, &h_out[0], 0, NULL, NULL);); (26, OpenCL, clMemWrapper in = clCreateBuffer(context, CL_MEM_READ_WRITE, bytes, NULL, &err);); (38, OpenCL, clKernelWrapper kernel = clCreateKernel(prog, spvName, &err);); (44, OpenCL, err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &global, NULL, 0, NULL, NULL);)"}
{"predict": "(4,OpenMP,int rank = omp_get_thread_num();)", "label": "(6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (6, MPI, MPI_Error_string(err, errorString, &errorStringLen);); (10, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "The provided code does not contain any loops or sections that are inherently parallelizable with standard parallelization techniques like OpenMP or MPI. The code is primarily concerned with initializing MPI, checking a condition, and then finalizing MPI. There are no computations or data processing tasks that can be distributed across multiple processors.\n\nHowever, if you were to introduce a parallelizable section, for example, a loop that processes data, I could provide guidance on where to insert parallelization instructions. Since there are no such sections in the current code, no parallelization instructions are applicable.\n\nIf you have a different code snippet that includes loops or data processing tasks, feel free to share it, and I can provide specific parallelization instructions.", "label": "(7, MPI, MPI_Get_library_version(version, &resultlen);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for), (46,OpenMP,#pragma omp parallel for), (54,OpenMP,#pragma omp parallel for), (61,OpenMP,#pragma omp parallel for), (76,OpenMP,#pragma omp parallel for), (85,OpenMP,#pragma omp parallel for)", "label": "(61, TBB, tbb::internal::segmented_iterator<nested_vec, const T> my_csi(my_vec);); (18, TBB, tbb::internal::segmented_iterator<nested_vec, T> my_si(my_vec);)"}
{"predict": "(68,OpenMP,#pragma omp parallel for)", "label": "(61, OpenCL, clReleaseMemObject(mem);); (33, OpenCL, cl_mem buf = clCreateBuffer(ctx, CL_MEM_USE_HOST_PTR, sz, data, &err);); (23, OpenCL, ctx->image_queue = clCreateCommandQueueWithProperties(ctx, ctx->devices[0], 0, &err);); (55, OpenCL, err = clEnqueueCopyBufferToImage(ctx->image_queue, buf, mem, 0, origin, region, 0, NULL, NULL);); (38, OpenCL, clReleaseMemObject(buf);)"}
{"predict": "20: #pragma omp target\n121: #pragma omp teams\n122: #pragma omp distribute parallel for firstprivate(xa) // OK: may be firstprivate\n123:   for (i = 0; i < argc; ++i)\n124:     foo();\n125: #pragma omp target\n126: #pragma omp teams\n127:   for (i = 0; i < argc; ++i)\n128:     foo();\n129: #pragma omp target\n130: #pragma omp teams\n131:   for (i = 0; i < argc; ++i)\n132:     foo();\n133: #pragma omp target\n134: #pragma omp teams\n135:   for (i = 0; i < argc; ++i)\n136:     foo();\n137: #pragma omp target\n138: #pragma omp teams\n139:   for (i = 0; i < argc; ++i)\n140:     foo();\n141: #pragma omp target\n142: #pragma omp teams\n143:   for (i = 0; i < argc; ++i)\n144:     foo();\n145: #pragma omp target\n146: #pragma omp teams\n147:   for (i = 0; i < argc; ++i)\n148:     foo();\n149: #pragma omp target\n150: #pragma omp teams\n151:   for (i = 0; i < argc; ++i)\n152:     foo();\n153: #pragma omp target\n154: #pragma omp teams\n155:   for (i = 0; i < argc; ++i)\n156:     foo();\n157: #pragma omp target\n158: #pragma omp teams\n159:   for (i = 0; i < argc; ++i)\n160:     foo();\n161: #pragma omp target\n162: #pragma omp teams\n163:   for (i = 0; i < argc; ++i)\n164:     foo();\n165: #pragma", "label": "(51, OpenMP, #pragma omp distribute parallel for firstprivate(a, b, c, d, f)); (86, OpenMP, #pragma omp distribute parallel for firstprivate(S2::S2s)); (158, OpenMP, #pragma omp parallel private(i)); (94, OpenMP, #pragma omp distribute parallel for safelen(5)); (98, OpenMP, #pragma omp distribute parallel for firstprivate(e, g)); (170, OpenMP, #pragma omp distribute parallel for firstprivate(si)); (70, OpenMP, #pragma omp distribute parallel for firstprivate(da)); (75, OpenMP, #pragma omp distribute parallel for firstprivate(xa)); (28, OpenMP, #pragma omp distribute parallel for firstprivate(argc > 0 ? argv[1] : argv[2])); (113, OpenMP, #pragma omp distribute parallel for firstprivate(i)); (-1, OpenMP, #pragma omp target); (30, OpenMP, #pragma omp distribute parallel for firstprivate(argc)); (133, OpenMP, #pragma omp distribute parallel for lastprivate(n) firstprivate(n)); (99, OpenMP, #pragma omp distribute parallel for firstprivate(k, h, B::x)); (111, OpenMP, #pragma omp parallel); (151, OpenMP, #pragma omp parallel reduction(+ : i)); (0, OpenMP, #pragma omp distribute parallel for firstprivate(); (44, OpenMP, #pragma omp distribute parallel for firstprivate(2 * 2)); (53, OpenMP, #pragma omp distribute parallel for firstprivate(ca)); (114, OpenMP, #pragma omp distribute parallel for firstprivate(j)); (36, OpenMP, #pragma omp distribute parallel for firstprivate(argv[1])); (96, OpenMP, #pragma omp distribute parallel for private(xa), firstprivate(xa)); (70, OpenMP, #pragma omp distribute parallel for firstprivate(S2::S2sc)); (-2, OpenMP, #pragma omp distribute parallel for firstprivate()); (2, OpenMP, #pragma omp distribute parallel for firstprivate(argc); (114, OpenMP, #pragma omp distribute parallel for lastprivate(g) firstprivate(g)); (20, OpenMP, #pragma omp distribute parallel for firstprivate(S1)); (4, OpenMP, #pragma omp distribute parallel for firstprivate(argc,); (79, OpenMP, #pragma omp distribute parallel for firstprivate(m)); (37, OpenMP, #pragma omp distribute parallel for firstprivate(ba)); (-19, OpenMP, #pragma omp distribute parallel for firstprivate); (-21, OpenMP, #pragma omp teams)"}
{"predict": "(11,OpenMP,#pragma omp parallel for)", "label": "(11, OpenCL, auto commandQueue = clCreateCommandQueueWithProperties(pContext, testedClDevice, properties.data(), &retVal);); (21, OpenCL, clReleaseCommandQueue(commandQueue);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for private(i)) (18,OpenMP,#pragma omp parallel for private(i)) (25,OpenMP,#pragma omp parallel for private(i)) (32,OpenMP,#pragma omp parallel for private(i)) (39,OpenMP,#pragma omp parallel for private(i)) (46,OpenMP,#pragma omp parallel for private(i))", "label": "(6, MPI, MPI_Type_get_extent(sddt, &lb, &extent);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for private(visc,arp1,rsym) reduction(max:viscdt))", "label": "(71, OpenMP, const int th=omp_get_thread_num();); (10, OpenMP, #pragma omp parallel for schedule (guided))"}
{"predict": "(59,OpenMP,#pragma omp parallel for private(options, result) collapse(2))", "label": "(11, CUDA, CUDA_CHECK(cudaGetDeviceProperties(&props, 0));)"}
{"predict": "(6,OpenMP,#pragma omp parallel for)", "label": "(3, MPI, MPI_Init(&argc, &argv);); (9, MPI, MPI_Finalize();)"}
{"predict": "(27,OpenMP,#pragma omp parallel for private(extensionPresent) reduction(||:extensionPresent))", "label": "(12, OpenCL, result = clGetPlatformInfo(platform, CL_PLATFORM_EXTENSIONS, 0, NULL, &set_size);); (14, OpenCL, result = clGetPlatformInfo(platform, CL_PLATFORM_EXTENSIONS, extensions.size(), extensions.data(), NULL);)"}
{"predict": "(1,OpenMP,#pragma omp parallel sections), (5,OpenMP,#pragma omp section), (8,OpenMP,#pragma omp section), (11,OpenMP,#pragma omp section), (14,OpenMP,#pragma omp section), (17,OpenMP,#pragma omp section), (20,OpenMP,#pragma omp section), (23,OpenMP,#pragma omp section), (26,OpenMP,#pragma omp section), (29,OpenMP,#pragma omp section), (32,OpenMP,#pragma omp section), (35,OpenMP,#pragma omp section)", "label": "(47, OpenMP, #pragma omp teams num_teams (3.14)); (42, OpenMP, #pragma omp teams num_teams (-10u)); (17, OpenMP, #pragma omp teams num_teams (argc))); (4, OpenMP, #pragma omp teams num_teams (); (7, OpenMP, #pragma omp teams num_teams ()); (10, OpenMP, #pragma omp teams num_teams (argc); (25, OpenMP, #pragma omp teams num_teams (argc), num_teams (argc+1)); (32, OpenMP, #pragma omp teams num_teams (-2)); (15, OpenMP, #pragma omp teams num_teams (argc > 0 ? argv[1] : argv[2])); (-7, OpenMP, #pragma omp target); (25, OpenMP, #pragma omp teams num_teams (S1)); (16, OpenMP, #pragma omp teams num_teams (argc + argc-z)); (-9, OpenMP, #pragma omp teams num_teams)"}
{"predict": "(shuffile_truncate(file_send, write_pos) != 0) {\n132:       shuffile_abort(-1, \"Truncating file: shuffile_truncate(%s, %lld) errno=%d %s @ %s:%d\",\n133:               file_send, (long long) write_pos, errno, strerror(errno), __FILE__, __LINE__\n134:       );\n135:     }\n136:     shuffile_rename(file_send, file_recv);\n137:   } else if (have_outgoing) {\n138:     /* only sent a file; close it */\n139:     shuffile_close(file_send, fd);\n140:   } else if (have_incoming) {\n141:     /* only received a file; close it */\n142:     shuffile_close(file_recv, fd);\n143:   }\n144: \n145:   /* free MPI send buffer */\n146:   if (have_outgoing) {\n147:     shuffile_align_free(buf_send);\n148:   }\n149: \n150:   /* free MPI recv buffer */\n151:   if (have_incoming) {\n152:     shuffile_align_free(buf_recv);\n153:   }\n154: \n155:   return rc;\n156: }", "label": "(101, MPI, MPI_Wait(&request[1], &status[1]);); (110, MPI, MPI_Wait(&request[0], &status[0]);); (110, MPI, MPI_Get_count(&status[0], MPI_BYTE, &nwrite);); (74, MPI, MPI_Irecv(buf_recv, shuffile_mpi_buf_size, MPI_BYTE, rank_recv, 0, comm, &request[0]);); (96, MPI, MPI_Isend(buf_send, nread, MPI_BYTE, rank_send, 0, comm, &request[1]);)"}
{"predict": "(17,OpenMP,#pragma omp parallel for private(is_member))", "label": "(48, MPI, MPI_Comm_accept(port, MPI_INFO_NULL, 0, pgroup, &pgroup_new);); (52, MPI, MPI_Comm_free(&pgroup_old);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (64, MPI, MPI_Comm_connect(port, MPI_INFO_NULL, 0, MPI_COMM_SELF, &pgroup_new);); (42, MPI, MPI_Send(port, PORT_SIZE, MPI_CHAR, members[i], 0, MPI_COMM_WORLD);); (35, MPI, MPI_Open_port(port_info, port);); (44, MPI, MPI_Comm_free(&pgroup_new);); (48, MPI, MPI_Close_port(port);); (66, MPI, MPI_Comm_connect(port, MPI_INFO_NULL, 0, pgroup, &pgroup_new);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for)", "label": "(5, CUDA, __shared__ typename BlockMergeSort::TempStorage temp_storage_shuffle;)"}
{"predict": "(15,OpenMP,#pragma omp parallel for private(i, buf))", "label": "(22, MPI, MPI_Allreduce(MPI_IN_PLACE, buf, count, MPI_INT, op, comm);); (13, MPI, MPI_Comm_size(comm, &size);); (35, MPI, MPI_Op_free(&op);); (6, MPI, MPI_Op_create(mysum, 0, &op);); (11, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(40,OpenMP,#pragma omp parallel for), (43,OpenMP,#pragma omp parallel for)", "label": "(34, MPI, MPI_Comm_size(comm, &ranks);); (32, MPI, MPI_Comm_rank(comm, &rank);); (12, MPI, MPI_Type_get_true_extent(keysat, &true_lb, &true_extent);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(10, OpenMP, #pragma omp parallel for)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp parallel for)"}
{"predict": "(18,OpenMP,#pragma omp parallel for)", "label": "(11, CUDA, cudaStreamCreate(&stream);); (14, CUDA, cudaStreamDestroy(stream);)"}
{"predict": "(7,OpenMP,#pragma omp parallel for reduction(+:my_sum))", "label": "(3, CUDA, __shared__ clock_t s_clocks[32];)"}
{"predict": "(10,OpenMP,#pragma omp parallel sections)", "label": "(11, MPI, MPI_Info_get(info, key, sizeof(buff) - 1, buff, &flag);); (8, MPI, MPI_Info_create(&info);); (19, MPI, MPI_Info_free(&info);); (7, MPI, MPI_Info_set(info, key, val);)"}
{"predict": "\\n\");\n141:     }\n142:   }\n143: \n144:   MPI_Finalize();\n145:   return 0;\n146: }", "label": "(113, MPI, MPI_Type_commit(&mpi_word);); (120, MPI, MPI_Allgather(iterstring,1,mpi_word, catstring,1,mpi_word, MPI_COMM_WORLD);); (152, MPI, MPI_Finalize();); (61, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (108, MPI, MPI_Type_contiguous(proc_length,MPI_CHAR, &mpi_word);); (17, MPI, MPI_Init(&argc,&argv);); (59, MPI, MPI_Bcast(&length,     1, MPI_LONG, root, MPI_COMM_WORLD);); (108, MPI, MPI_Barrier(MPI_COMM_WORLD);); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(84,OpenMP,#pragma omp parallel for reduction(+:Csub))", "label": "(13, CUDA, __shared__ float Bs[maxPipelineStages][BLOCK_SIZE_X][BLOCK_SIZE_X];); (8, CUDA, __shared__ float As[maxPipelineStages][BLOCK_SIZE_X][BLOCK_SIZE_X];)"}
{"predict": "(9,OpenMP,#pragma omp target teams distribute parallel for)", "label": "(11, CUDA, extern __shared__ float sdata[];)"}
{"predict": "(6,OpenMP,#pragma omp parallel for)", "label": "(7, OpenMP, #pragma omp target map(tofrom : device)); (9, OpenMP, printf(\"omp_is_initial_device() returned false on host\\n\");); (12, OpenMP, printf(\"omp_is_initial_device() returned true on device\\n\");)"}
{"predict": "(6,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for), (43,OpenMP,#pragma omp parallel for), (64,OpenMP,#pragma omp parallel for)", "label": "(32, TBB, tbb::tbb_allocator<t1>().destroy(my_storage);); (35, TBB, tbb::tbb_allocator<t2>().destroy(my_storage);); (0, TBB, tbb::atomic<int> counter;)"}
{"predict": "(49,OpenMP,#pragma omp parallel for),", "label": "(56, CUDA, checkCudaErrors(cufftPlan2d(&planc2r, DIM, DIM, CUFFT_C2R));); (54, CUDA, checkCudaErrors(cufftPlan2d(&planr2c, DIM, DIM, CUFFT_R2C));); (37, CUDA, checkCudaErrors(cudaMallocPitch((void **)&dvfield, &tPitch, sizeof(cData)*DIM, DIM));); (38, CUDA, checkCudaErrors(cudaMemcpy(dvfield, hvfield, sizeof(cData) * DS, cudaMemcpyHostToDevice));); (40, CUDA, checkCudaErrors(cudaMalloc((void **)&vyfield, sizeof(cData) * PDS));); (38, CUDA, checkCudaErrors(cudaMalloc((void **)&vxfield, sizeof(cData) * PDS));)"}
{"predict": "(25,OpenMP,#pragma omp parallel for private(j,temp)) (47,OpenMP,#pragma omp parallel for private(i) reduction(min:sum)) (54,OpenMP,#pragma omp parallel for reduction(+:sum3) private(i))", "label": "(46, OpenMP, #pragma omp teams reduction(+:sum)); (43, OpenMP, #pragma omp target map(tofrom: b, sum, t) map(alloc: a)); (46, OpenMP, #pragma omp distribute parallel for reduction(+:sum)); (48, OpenMP, t[i] = omp_get_team_num();)"}
{"predict": "\", num_debug_state_params);\n92:     float* debug_states_cpu = (float*)mallocCheck(num_debug_state_params * sizeof(float));\n93:     freadCheck(debug_states_cpu, sizeof(float), num_debug_state_params, states_file);\n94:     fcloseCheck(states_file);\n95: \n96:     // allocate device memory for debug states\n97:     float* debug_states_gpu;\n98: \n99:     // set debug state pointers\n100:     set_resblock_debug_state_ptrs(&acts, debug_states_gpu);\n101:     set_resblock_debug_state_ptrs(&back_acts, debug_states_gpu);\n102: \n103:     // copy parameters to device\n104:     cublasCheck(cublasSetDevice(0));\n105:     cudaCheck(cudaMalloc(&params_memory_gpu, num_parameters * sizeof(float)));\n106:     cudaCheck(cudaMemcpy(params_memory_gpu, params_memory_cpu, num_parameters * sizeof(float), cudaMemcpyHostToDevice));\n107:     cudaCheck(cudaMalloc(&grads_memory_gpu, num_parameters * sizeof(float)));\n108:     cudaCheck(cudaMemset(grads_memory_gpu, 0, num_parameters * sizeof(float)));\n109: \n110:     // copy debug states to device\n111:     cudaCheck(cudaMalloc(&debug_states_gpu, num_debug_state_params * sizeof(float)));\n112:     cudaCheck(cudaMemcpy(debug_states_gpu, debug_states_cpu, num_debug_state_params * sizeof(float), cudaMemcpyHostToDevice));\n113: \n114:     // compute forward pass\n115:     resblock_forward(&params, &acts, &grads, debug_states_gpu, B, C, C_emb, C_out, H, W, up, down, gn_n_groups, cublas_handle);\n116: \n117:     // compute backward pass\n118:     resblock_backward(&params, &acts, &grads, &back_acts, debug_states_gpu, B, C, C_emb, C_out, H, W, up, down, gn_n_groups, cublas_handle);\n119: \n120:     // copy gradients back to host\n121:     cudaCheck(cudaMemcpy(grads_memory_cpu, grads_memory_gpu, num_parameters * sizeof(float), cudaMemcpyDeviceToHost));\n122: \n1", "label": "(125, CUDA, cudaCheck(cudaMalloc(&acts_memory_gpu, num_acts_params * sizeof(float)));); (257, CUDA, cudaCheck(cudaFree(acts_memory_gpu));); (173, CUDA, cudaCheck(cudaMalloc(&back_acts_memory_gpu, num_back_acts * sizeof(float)));); (254, CUDA, cudaCheck(cudaFree(params_memory_gpu));); (259, CUDA, cudaCheck(cudaFree(acts.emb));); (53, CUDA, cudaCheck(cudaMemset(grads_memory_gpu, 0, num_parameters * sizeof(float)));); (170, CUDA, cudaCheck(cudaMemset(back_acts_memory_gpu, 0, num_back_acts * sizeof(float)));); (182, CUDA, cudaCheck(cudaMemcpy(back_acts.dout, debug_states.dout, B * C_out * H_out * W_out * sizeof(float), cudaMemcpyHostToDevice));); (254, CUDA, cudaCheck(cudaFree(acts.input));); (117, CUDA, cudaCheck(cudaMemset(acts_memory_gpu, 0, num_acts_params * sizeof(float)));); (43, CUDA, cudaCheck(cudaMemcpy(params_memory_gpu, params_memory_cpu, num_parameters * sizeof(float), cudaMemcpyHostToDevice));); (249, CUDA, cudaCheck(cudaFree(back_acts_memory_gpu));); (122, CUDA, cudaCheck(cudaMemcpy(acts.input, debug_states.input, state_sizes[0] * sizeof(float), cudaMemcpyHostToDevice));); (122, CUDA, cudaCheck(cudaMemcpy(acts.emb, debug_states.emb, state_sizes[1] * sizeof(float), cudaMemcpyHostToDevice));); (43, CUDA, cudaCheck(cudaMalloc(&grads_memory_gpu, num_parameters * sizeof(float)));); (116, CUDA, cudaCheck(cudaMalloc(&acts.input, B * C * H * W * sizeof(float)));); (36, CUDA, cudaCheck(cudaMalloc(&params_memory_gpu, num_parameters * sizeof(float)));); (115, CUDA, cudaCheck(cudaMalloc(&acts.emb, B * C_emb * sizeof(float)));); (241, CUDA, cudaCheck(cudaFree(grads_memory_gpu));)"}
{"predict": "(2,OpenMP,#pragma omp parallel)", "label": "(6, OpenMP, omp_unset_nest_lock(&nest_lock);); (2, OpenMP, omp_init_nest_lock(&nest_lock);); (3, OpenMP, omp_test_nest_lock(&nest_lock);); (5, OpenMP, omp_set_nest_lock(&nest_lock);); (9, OpenMP, omp_destroy_nest_lock(&nest_lock);)"}
{"predict": "NBC_Error(\"Error in NBC_Copy(%lu, %i, %p, %lu, %i, %p, %lu) (%i)\", (unsigned long)buf1, copyargs.srccount,\n130:                     copyargs.srctype, (unsigned long)buf2, copyargs.tgtcount, copyargs.tgttype,\n131:                     (unsigned long)handle->comm, res);\n132:           return res;\n133:         }\n134:         break;\n135:       case UNPACK:\n136:         NBC_DEBUG(5, \"  UNPACK (offset %li) \", offset);\n137:         NBC_GET_BYTES(ptr,unpackargs);\n138:         NBC_DEBUG(5, \"*src: %lu, srccount: %i, srctype: %p, *tgt: %lu, tgtcount: %i, tgttype: %p)\\n\",\n139:                   (unsigned long) unpackargs.src, unpackargs.srccount, unpackargs.srctype,\n140:                   (unsigned long) unpackargs.tgt, unpackargs.tgtcount, unpackargs.tgttype);\n141:         /* get buffers */\n142:         if(unpackargs.tmpsrc) {\n143:           buf1=(char*)handle->tmpbuf+(long)unpackargs.src;\n144:         } else {\n145:           buf1=unpackargs.src;\n146:         }\n147:         if(unpackargs.tmptgt) {\n148:           buf2=(char*)handle->tmpbuf+(long)unpackargs.tgt;\n149:         } else {\n150:           buf2=unpackargs.tgt;\n151:         }\n152:         res = NBC_Unpack(buf1, unpackargs.srccount, unpackargs.srctype, buf2, unpackargs.tgtcount, unpackargs.tgttype,\n153:                          handle->comm);\n154:         if (OPAL_UNLIKELY(OMPI_SUCCESS != res)) {\n155:           NBC_Error(\"Error in NBC_Unpack(%lu, %i, %p, %lu, %i, %p, %lu) (%i)\", (unsigned long)buf1, unpackargs.srccount,\n156:                     unpackargs.sr", "label": "(73, MPI, Irecv_time -= MPI_Wtime();); (55, MPI, Isend_time += MPI_Wtime();); (88, MPI, Irecv_time += MPI_Wtime();); (35, MPI, Isend_time -= MPI_Wtime();)"}
{"predict": "(19,OpenMP,#pragma omp parallel for), (25,MPI,MPI_Init(&argc, &argv);), (42,MPI,MPI_Finalize();)", "label": "(24, CUDA, CHECK(cudaMalloc(&d_B, M));); (24, CUDA, CHECK(cudaMemcpy(d_A, h_A, M, cudaMemcpyHostToDevice));); (46, CUDA, CHECK(cudaFree(d_B));); (20, CUDA, CHECK(cudaMalloc(&d_A, M));); (32, CUDA, CHECK(cudaMemcpy(h_B, d_B, M, cudaMemcpyDeviceToHost));); (42, CUDA, CHECK(cudaFree(d_A));)"}
{"predict": "0)/REPEAT;\n131: \n132:             /* verify */\n133:             for ( int i=0 ; i<n ; i++ )\n134:                 for ( int j=0 ; j<n ; j++ )\n135:                     assert( B[i*n+j] == A[j*n+i] );\n136: \n137:             /* pur 4x4 w/ stride-1 loads */\n138:             t0 = getticks();\n139:             for ( int t=0 ; t<REPEAT ; t++ )\n140:             {\n141: #ifdef OMP\n142: #pragma omp parallel for if(n>OMP_MIN_SIZE)\n143: #endif\n144:                 for ( int j=0 ; j<n ; j+=4 )\n145:                     for ( int i=0 ; i<n ; i++ )\n146:                     {\n147: #pragma unroll 4\n148:                         for ( int k=0 ; k<4 ; k++ )\n149:                             B[i*n+j+k] = A[j*n+i+k];\n150:                     }\n151:             }\n152:             t1 = getticks();\n153:             d4[n] = (t1-t0)/REPEAT;\n154: \n155:             /* verify */\n156:             for ( int i=0 ; i<n ; i++ )\n157:                 for ( int j=0 ; j<n ; j++ )\n158:                     assert( B[i*n+j] == A[j*n+i] );\n159: \n160:             /* mur 4x4 w/ stride-1 loads */\n161:             t0 = getticks();\n162:             for ( int t=0 ; t<REPEAT ; t++ )\n163:             {\n164: #ifdef OMP\n165: #pragma omp parallel for if(n>OMP_MIN_SIZE)\n166: #endif\n167:                 for ( int j=0 ; j<n ; j+=4 )\n168:                     for ( int i=0 ; i<n ; i++ )\n169:                     {\n170:                         B[i*n+j+0", "label": "(8, MPI, MPI_Comm_rank( MPI_COMM_WORLD , &rank );); (364, MPI, MPI_Finalize();); (34, OpenMP, fprintf( stderr , \"starting test: OpenMP ON with %d threads... \\n\", omp_get_max_threads() );); (164, OpenMP, #pragma omp parallel if(n>OMP_MIN_SIZE)); (168, OpenMP, #pragma omp for private(i,j,n4)); (63, OpenMP, #pragma omp parallel for if(n>OMP_MIN_SIZE)); (1, MPI, MPI_Init_thread( &argc , &argv , MPI_THREAD_FUNNELED , &provided );); (28, OpenMP, printf( \"# starting test: OpenMP ON with %d threads... \\n\", omp_get_max_threads() );)"}
{"predict": "(55,OpenMP,#pragma omp parallel for), (64,OpenMP,#pragma omp parallel for), (76,OpenMP,#pragma omp parallel for)", "label": "(5, CUDA, extern __shared__ char shared_memory[];)"}
{"predict": "(2,MPI,MPI_Init(NULL, NULL);); (120,MPI,MPI_Finalize(););", "label": "(23, MPI, err = MPI_Type_size(eviltype, &val);); (11, MPI, err = MPI_Type_struct(3, blocks, disps, types, &eviltype);); (66, MPI, err = MPI_Type_get_extent(eviltype, &lb, &extent);); (86, MPI, err = MPI_Type_ub(eviltype, &aval);); (121, MPI, MPI_Type_free(&eviltype);); (99, MPI, err = MPI_Type_get_true_extent(eviltype, &true_lb, &aval);); (32, MPI, err = MPI_Type_extent(eviltype, &aval);); (46, MPI, err = MPI_Type_lb(eviltype, &aval);)"}
{"predict": "(29,OpenMP,#pragma omp parallel for), (50,OpenMP,#pragma omp parallel for), (56,OpenMP,#pragma omp parallel for)", "label": "(67, MPI, MPI_Win_unlock(rank, win);); (28, MPI, i = MPI_Alloc_mem(SIZE * sizeof(int), MPI_INFO_NULL, &B);); (45, MPI, MPI_Win_start(group, 0, win);); (92, MPI, MPI_Win_free(&win);); (13, MPI, MPI_Comm_split(MPI_COMM_WORLD, (rank < 2), rank, &CommDeuce);); (66, MPI, MPI_Win_post(group, 0, win);); (44, MPI, MPI_Send(B, SIZE, MPI_INT, 1, 100, MPI_COMM_WORLD);); (34, MPI, MPI_Win_allocate(0, 1, MPI_INFO_NULL, CommDeuce, &base_ptr, &win);); (47, MPI, MPI_Win_allocate(SIZE * sizeof(int), sizeof(int), MPI_INFO_NULL, CommDeuce, &B, &win);); (63, MPI, MPI_Recv(A, SIZE, MPI_INT, 0, 100, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (63, MPI, MPI_Win_wait(win);); (41, MPI, MPI_Free_mem(B);); (37, MPI, MPI_Win_complete(win);); (35, MPI, MPI_Put(A, SIZE, MPI_INT, 1, 0, SIZE, MPI_INT, win);); (0, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (5, MPI, i = MPI_Alloc_mem(SIZE * sizeof(int), MPI_INFO_NULL, &A);); (82, MPI, MPI_Comm_free(&CommDeuce);); (77, MPI, MPI_Group_free(&comm_group);); (-9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (7, MPI, MPI_Comm_group(CommDeuce, &comm_group);); (-10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (75, MPI, MPI_Free_mem(A);); (71, MPI, MPI_Group_free(&group);); (23, MPI, MPI_Group_incl(comm_group, 1, &destrank, &group);); (38, MPI, MPI_Win_create(B, SIZE * sizeof(int), sizeof(int), MPI_INFO_NULL, CommDeuce, &win);)"}
{"predict": "(34,OpenMP,#pragma omp parallel for), (35,OpenMP,#pragma omp parallel for)", "label": "(52, CUDA, CubDebugExit(cudaMemcpy(d_flags, h_flags, sizeof(unsigned char) * num_items, cudaMemcpyHostToDevice));); (50, CUDA, CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));)"}
{"predict": "(31,OpenMP,#pragma omp parallel for collapse(2)), (71,OpenMP,#pragma omp parallel for collapse(2))", "label": "(58, MPI, MPI_Start(&req);); (13, MPI, MPI_Comm_rank(comm, &myrank);); (76, MPI, MPI_Type_free(&stype[i][j]);); (50, MPI, MPI_Info_create(&info);); (55, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (72, MPI, MPI_Type_free(&xtype[i][j]);); (56, MPI, MPI_Request_free(&req);); (6, MPI, MPI_Comm_size(comm, &p);); (55, MPI, MPI_Info_free(&info);)"}
{"predict": "76: #if(VKFFT_BACKEND==0)\n77: \t\t\tVkBuffer buffer;\n78: \t\t\tVkDeviceMemory bufferDeviceMemory;\n79: \t\t\tres = vkFFTCreateBuffer(vkGPU->physicalDevice, vkGPU->device, &buffer, &bufferDeviceMemory, bufferSize, VK_BUFFER_USAGE_STORAGE_BUFFER_BIT | VK_BUFFER_USAGE_TRANSFER_SRC_BIT | VK_BUFFER_USAGE_TRANSFER_DST_BIT, VK_SHARING_MODE_EXCLUSIVE, 0, NULL, vkGPU->memoryProperties, vkGPU->memoryTypeIndex, vkGPU->queueFamilyIndex, VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT, &vkGPU->commandPool, &vkGPU->queue, vkGPU->fence);\n80: #elif(VKFFT_BACKEND==1)\n81: \t\t\tcudaMalloc(&buffer, bufferSize);\n82: #elif(VKFFT_BACKEND==2)\n83: \t\t\thipMalloc(&buffer, bufferSize);\n84: #elif(VKFFT_BACKEND==3)\n85: \t\t\tcl_mem buffer = clCreateBuffer(vkGPU->context, CL_MEM_READ_WRITE, bufferSize, NULL, &res);\n86: #elif(VKFFT_BACKEND==4)\n87: \t\t\tbuffer = zeMemAllocDevice(vkGPU->context, ZE_DEVICE_MEM_ALLOC_DESC_FLAG_DEFAULT, bufferSize, 0, vkGPU->device, &res);\n88: #elif(VKFFT_BACKEND==5)\n89:             zeMemAllocDevice(vkGPU->context, ZE_DEVICE_MEM_ALLOC_DESC_FLAG_DEFAULT, bufferSize, 0, vkGPU->device, &buffer);\n90: #endif\n91: \n92: \n93: \n94: \t\t\t//Copy input data to the GPU buffer.\n95: #if(VKFFT_BACKEND==0)\n96: \t\t\tVkCommandBuffer commandBuffer;\n97: \t\t\tres = vkFFTAllocateCommandBuffer(vkGPU->device, vkGPU->commandPool, &commandBuffer);\n98: \t\t\tres = vkFFTBeginCommandBuffer(commandBuffer);\n99: \t\t\tVkBufferCopy copyRegion = {};\n100: \t\t\tcopyRegion.size = bufferSize;\n101: \t\t\tvkCmdCopyBuffer(commandBuffer, vkGPU->stagingBuffer, buffer, 1, &copyRegion);\n102: \t\t\tres = vkFFTEndCommandBuffer(commandBuffer);\n103: \t\t\tres = vkFFTSubmitCommandBuffer(vkGPU->queue, commandBuffer,", "label": "(161, OpenCL, clGetDeviceInfo(vkGPU->device, CL_DEVICE_VENDOR_ID, sizeof(cl_int), &vendorID, 0);); (81, CUDA, res = cudaMalloc((void**)&buffer, bufferSize);); (90, OpenCL, buffer = clCreateBuffer(vkGPU->context, CL_MEM_READ_WRITE, bufferSize, 0, &res);)"}
{"predict": "(33,MPI,MPI_Reduce(&err, &errs, 1, MPI_INT, MPI_SUM, 0, intercomm);)", "label": "(26, MPI, MPI_Comm_size(intercomm, &size);); (84, MPI, MPI_Comm_disconnect(&intercomm);); (91, MPI, MPI_Finalize();); (11, MPI, MPI_Comm_get_parent(&parentcomm);); (66, MPI, MPI_Recv(&i, 1, MPI_INT, 0, 0, intercomm, &status);); (32, MPI, MPI_Send(&i, 1, MPI_INT, i, 0, intercomm);); (52, MPI, MPI_Comm_get_name(intercomm, cname, &rlen);); (69, MPI, MPI_Ssend(&errs, 1, MPI_INT, 0, 1, intercomm);); (35, MPI, MPI_Recv(&err, 1, MPI_INT, i, 1, intercomm, MPI_STATUS_IGNORE);); (18, MPI, MPI_Comm_rank(intercomm, &rank);); (15, MPI, MPI_Comm_remote_size(intercomm, &rsize);)"}
{"predict": "(3,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for), (49,OpenMP,#pragma omp parallel for)", "label": "(31, OpenMP, #pragma omp teams distribute parallel for collapse (1)); (35, OpenMP, #pragma omp teams distribute parallel for collapse ((ST > 0) ? 1 + ST : 2)); (1, OpenMP, #pragma omp teams distribute parallel for collapse); (10, OpenMP, #pragma omp teams distribute parallel for collapse ()); (44, OpenMP, #pragma omp distribute collapse (S)); (21, OpenMP, #pragma omp teams distribute parallel for collapse (ST); (62, OpenMP, #pragma omp teams distribute parallel for collapse (N)); (13, OpenMP, #pragma omp teams distribute parallel for collapse (argc); (65, OpenMP, #pragma omp teams distribute parallel for collapse (2)); (49, OpenMP, #pragma omp teams distribute parallel for collapse (argv[1]=2)); (-8, OpenMP, #pragma omp target); (-3, OpenMP, #pragma omp teams distribute parallel for collapse (); (32, OpenMP, #pragma omp teams distribute parallel for collapse (foobool(argc)), collapse (true), collapse (-5))"}
{"predict": "45:                 ssize_t read_rc = mfu_read(filename, fd, ptr, remaining);\n146:                 if (read_rc == -1) {\n147:                     MFU_LOG(MFU_LOG_ERR, \"Failed to read from archive file '%s' errno=%d %s\",\n148:                         filename, errno, strerror(errno));\n149:                     rc = MFU_FAILURE;\n150:                 } else if (read_rc == 0) {\n151:                     MFU_LOG(MFU_LOG_ERR, \"Unexpected EOF reading from archive file '%s'\",\n152:                         filename);\n153:                     rc = MFU_FAILURE;\n154:                 } else {\n155:                     total_read += (size_t)read_rc;\n156:                 }\n157:             }\n158:         }\n159: \n160:         /* bail out with an error if anyone failed to read their part of the archive */\n161:         if (! mfu_alltrue(rc == MFU_SUCCESS, MPI_COMM_WORLD)) {\n162:             if (buf != NULL) {\n163:                 MFU_FREE(&buf);\n164:             }\n165:             mfu_close(filename, fd);\n166:             return MFU_FAILURE;\n167:         }\n168: \n169:         /* now we can look for index entries in our buffer */\n170:         uint64_t count = 0;\n171:         uint64_t* offsets = NULL;\n172:         if (buf != NULL) {\n173:             /* search for index entries in our buffer,\n174:              * we pass in the offset within the file of the first byte in our buffer */\n175:             index_entries_find(algo, buf, bufsize, offset_start, overlap_before, &count, &offsets);\n176:         }\n177: \n178:         /* gather all the index entries found by each rank */\n179:         uint64_t* recvcounts = NULL;\n180:         uint64_t* displs = NULL;\n181:         uint64_t* all_counts = NULL;\n182:         uint64_t* recvbuf = NULL;\n183:         if (rank ==", "label": "(45, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (47, MPI, double start = MPI_Wtime();); (237, MPI, double now = MPI_Wtime();); (41, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(27,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for)", "label": "(33, MPI, flow::Workspace::set_default_mpi_comm(MPI_Comm_c2f(comm));); (16, MPI, MPI_Comm_rank(comm, &par_rank);); (16, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "The provided code snippet is a unit test function and does not contain any loops or sections that are inherently parallelizable. Therefore, there are no lines in this specific code snippet where parallelization instructions (such as OpenMP or MPI) should be inserted. \n\nIf you have a different code snippet with loops or sections that can be parallelized, please provide it, and I will be happy to analyze it for you.", "label": "(13, OpenCL, retVal = clReleaseMemObject(smallBuffer);); (17, OpenCL, EXPECT_EQ(clReleaseContext(context), CL_SUCCESS);); (1, OpenCL, cl_mem smallBuffer = clCreateBufferWithProperties(clContext, &memProperties, flags, size, hostPtr, &retVal);)"}
{"predict": "(4,OpenMP,#pragma omp parallel for private(j, ctid, s))", "label": "(8, OpenMP, int tid = omp_get_thread_num();); (4, OpenMP, #pragma omp parallel num_threads(8)); (2, OpenMP, omp_set_affinity_format(formats[i]);)"}
{"predict": "(21,OpenMP,#pragma omp parallel for private(i, b) reduction(+:cache))", "label": "(16, CUDA, __shared__ uninitialized_array<thrust::complex<T>, 1024> cache;); (29, CUDA, cache[id] += static_cast<thrust::complex<T>>(A_[(a * nchol + i) * nocc + b] * A_[(b * nchol + i) * nocc + a]);); (21, CUDA, thrust::complex<T2> const* A_(Tab + blockIdx.x * nocc * nocc * nchol);); (18, CUDA, cache[id]              = thrust::complex<T>(0.0);); (18, CUDA, thrust::complex<T> alp = static_cast<thrust::complex<T>>(alpha);)"}
{"predict": "_array, d_A.data(),\n114:                                sizeof(data_type *) * gemm_count,\n115:                                cudaMemcpyHostToDevice, stream));\n116:     CUDA_CHECK(cudaMemcpyAsync(d_B_array, d_B.data(),\n117:                                sizeof(data_type *) * gemm_count,\n118:                                cudaMemcpyHostToDevice, stream));\n119:     CUDA_CHECK(cudaMemcpyAsync(d_C_array, d_C.data(),\n120:                                sizeof(data_type *) * gemm_count,\n121:                                cudaMemcpyHostToDevice, stream));\n122: \n123:     /* step 3: compute */\n124:     for (int i = 0; i < group_count; i++) {\n125:         for (int j = 0; j < group_size[i]; j++) {\n126:             cublasGemmEx(cublasH,\n127:                          transa_array[i], transb_array[i],\n128:                          m_array[i], n_array[i], k_array[i],\n129:                          &alpha_array[i],\n130:                          d_A[group_size[i] * i + j], CUDA_R_32F, lda_array[i],\n131:                          d_B[group_size[i] * i + j], CUDA_R_32F, ldb_array[i],\n132:                          &beta_array[i],\n133:                          d_C[group_size[i] * i + j], CUDA_R_32F, ldc_array[i],\n134:                          compute_type, CUBLAS_GEMM_DEFAULT);\n135:         }\n136:     }\n137: \n138:     /* step 4: copy data back to host */\n139:     for (int i = 0; i < gemm_count; i++) {\n140:         CUDA_CHECK(cudaMemcpyAsync(C_array[i].data(), d_C[i],\n141:                                    sizeof(data_type) * C_array[i].size(),\n142:                                    cudaMemcpyDeviceToHost, stream));\n143:     }\n144: \n145:     CUDA_CHECK(cudaStreamSynchronize(stream));\n146: \n147:     /* step 5: print results */\n148:     problem_idx = 0;\n149:     for (int i = 0; i <", "label": "(169, CUDA, CUDA_CHECK(cudaFree(d_B_array));); (169, CUDA, CUDA_CHECK(cudaFree(d_C_array));); (177, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (165, CUDA, CUDA_CHECK(cudaFree(d_A_array));); (168, CUDA, CUDA_CHECK(cudaFree(d_A[i]));); (169, CUDA, CUDA_CHECK(cudaFree(d_C[i]));); (133, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (174, CUDA, CUDA_CHECK(cudaDeviceReset());); (165, CUDA, CUDA_CHECK(cudaFree(d_B[i]));); (75, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(19,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for reduction(+:c))", "label": "(48, CUDA, HANDLE_ERROR( cudaFree( dev_a ) );); (49, CUDA, HANDLE_ERROR( cudaFree( dev_partial_c ) );); (47, CUDA, HANDLE_ERROR( cudaFree( dev_b ) );)"}
{"predict": "(8,OpenMP,#pragma omp parallel for private(nr_threads_in_single) reduction(+:result,nr_iterations))", "label": "(12, OpenMP, omp_set_nest_lock(&lck);); (18, OpenMP, omp_unset_nest_lock(&lck);); (8, OpenMP, #pragma omp for); (10, OpenMP, #pragma omp flush); (4, OpenMP, #pragma omp parallel shared(lck)); (2, OpenMP, omp_init_nest_lock(&lck);); (16, OpenMP, omp_destroy_nest_lock(&lck);)"}
{"predict": "(47,OpenMP,#pragma omp parallel for private(src, dst) schedule(static))", "label": "(19, MPI, MPI_Type_get_true_extent(inkeysat, &keysat_true_lb, &keysat_true_extent);); (76, MPI, MPI_Type_dup(inkey,    &values->key);); (13, MPI, MPI_Type_get_true_extent(inkey, &key_true_lb, &key_true_extent);); (75, MPI, MPI_Type_dup(inkeysat, &values->keysat);)"}
{"predict": "(7,OpenMP,#pragma omp task shared(x)) (8,OpenMP,#pragma omp task shared(y)) (9,OpenMP,#pragma omp taskwait)", "label": "(6, TBB, tbb::task_group g;); (6, TBB, tbb::task_handle h1 = make_task(g, x, n-1);); (6, TBB, tbb::task_handle h2 = make_task(g, y, n-2);)"}
{"predict": "(16,OpenMP,#pragma omp parallel for)", "label": "(17, CUDA, CUDA_FUNC(cudaStreamDestroy(resources->exec_streams[i]));)"}
{"predict": "MPI_SUCCESS) && (amode & MPI_MODE_SEQUENTIAL)) {\n125:         /* get name of file that will hold the shared file pointer */\n126:         char *sp_name = ADIOI_Malloc(ADIOI_FNAME_MAX);\n127:         int ret = ADIOI_Get_shared_fp_fname(*fh, sp_name);\n128:         if (ret == ADIO_SUCCESS) {\n129:             /* open shared file pointer file */\n130:             ADIO_File sp_fh;\n131:             sp_fh = ADIO_Open(comm, dupcomm, sp_name, file_system, fsops,\n132:                               MPI_MODE_RDWR | MPI_MODE_CREATE, 0, MPI_BYTE, MPI_BYTE, MPI_INFO_NULL,\n133:                               ADIO_PERM_NULL, &error_code);\n134:             if (error_code == MPI_SUCCESS) {\n135:                 /* set shared file pointer file */\n136:                 (*fh)->shared_fp_file = sp_fh;\n137:                 /* free memory for sp_name */\n138:                 ADIOI_Free(sp_name);\n139:             }\n140:         } else {\n141:             error_code = MPIO_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE,\n142:                                               __func__, __LINE__,\n143:                                               MPI_ERR_UNSUPPORTED_OPERATION, \"**iosequnsupported\", 0);\n144:             ADIO_Close(*fh, &error_code);\n145:             goto fn_fail;\n146:         }\n147:     }\n148: \n149:     if (error_code != MPI_SUCCESS) {\n150:         goto fn_fail;\n151:     }\n152: \n153:     /* Check if file has been opened with a large count */\n154:     if ((*fh)->is_large_count) {\n155:         (*fh)->file_system = ADIO_PVFS2;\n156:     }\n157: \n158:     if (dupcomm != MPI_COMM_NULL) {\n159:         MPI_Comm_free(&dupcomm);\n160:     }\n161: \n162:     return error_code;\n163: \n164: fn_fail:\n165:     if (dup", "label": "(31, MPI, error_code = MPI_Comm_test_inter(comm, &flag);); (56, MPI, MPI_Comm_dup(comm, &dupcomm);); (126, MPI, MPI_Comm_rank(dupcomm, &rank);); (136, MPI, MPI_Barrier(dupcomm);); (67, MPI, MPI_Allreduce(&amode, &tmp_amode, 1, MPI_INT, ADIO_same_amode, dupcomm);)"}
{"predict": "(7,OpenMP,#pragma omp parallel for private(i, j, akey, mask_key, unmasked_keys, hashed_mask_key, ctx) collapse(2))", "label": "(5, OpenMP, #pragma omp parallel for)"}
{"predict": ":         }\n78:     }\n79: }\n80: \n81: void ReadCgnsGrid2( const std::string & filename )\n82: {\n83:     int fileId = -1;\n84:     if ( Parallel::pid == Parallel::serverid )\n85:     {\n86:         cg_open( filename.c_str(), CG_MODE_READ, &fileId );\n87:         std::cout << \"fileId = \" << fileId << \"\\n\";\n88:     }\n89: \n90:     int nbases = -1;\n91:     if ( Parallel::pid == Parallel::serverid )\n92:     {\n93:         cg_nbases( fileId, &nbases );\n94:     }\n95:     std::cout << \"Parallel::pid = \" << Parallel::pid << \" \";\n96:     std::cout << \"nbases = \" << nbases << \"\\n\";\n97: \n98:     for ( int iBase = 0; iBase < nbases; ++ iBase )\n99:     {\n100:         char basename[ 33 ];\n101:         int baseId = iBase + 1;\n102:         int icelldim = -1;\n103:         int iphysdim = -1;\n104:         if ( Parallel::pid == Parallel::serverid )\n105:         {\n106:             cg_base_read( fileId, baseId, basename, &icelldim, &iphysdim );\n107:         }\n108:         std::cout << \"Parallel::pid = \" << Parallel::pid << \" \";\n109:         std::cout << \"icelldim = \" << icelldim << \" iphysdim = \" << iphysdim << \"\\n\";\n110: \n111:         Global::cell_dim = icelldim;\n112:         Global::phys_dim = iphysdim;\n113: \n114:         int nzones = -1;\n115:         if ( Parallel::pid == Parallel::serverid )\n116:         {\n117:             cg_nzones( fileId, baseId, &nzones );\n118:         }\n119:         std", "label": "(224, MPI, MPI_Bcast( &bocotype, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (190, MPI, MPI_Bcast( &nbocos, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (202, MPI, MPI_Bcast( &location, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (38, MPI, MPI_Bcast( &nzones, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (84, MPI, MPI_Bcast( zonename, 33, MPI_CHAR, Parallel::serverid, MPI_COMM_WORLD );); (295, MPI, MPI_Bcast( range.data(), npnts * index_dim, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (219, MPI, MPI_Bcast( &ptset_type, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (251, MPI, MPI_Bcast( pnts.data(), npnts * index_dim, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (81, MPI, MPI_Bcast( isize.data(), index_dim * 3, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (113, MPI, MPI_Bcast( &zoneType, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (220, MPI, MPI_Bcast( normalIndex.data(), index_dim, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (216, MPI, MPI_Bcast( &npnts, 1, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (289, MPI, MPI_Bcast( donor_range.data(), npnts * index_dim, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (164, MPI, MPI_Bcast( pcoor, nNodes, MPI_DOUBLE, Parallel::serverid, MPI_COMM_WORLD );); (145, MPI, MPI_Bcast( &dataType, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (283, MPI, MPI_Bcast( connectname, 33, MPI_CHAR, Parallel::serverid, MPI_COMM_WORLD );); (243, MPI, MPI_Bcast( normalList.data(), nNodes * iphysdim * sizeof( double ), MPI_CHAR, Parallel::serverid, MPI_COMM_WORLD );); (-3, MPI, MPI_Bcast( &nbases, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (205, MPI, MPI_Bcast( boconame, 33, MPI_CHAR, Parallel::serverid, MPI_COMM_WORLD );); (139, MPI, MPI_Bcast( coordname, 33, MPI_CHAR, Parallel::serverid, MPI_COMM_WORLD );); (208, MPI, MPI_Bcast( &normalListSize, 1, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (256, MPI, MPI_Bcast( &n1to1, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (6, MPI, MPI_Bcast( &icelldim, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (6, MPI, MPI_Bcast( &iphysdim, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (205, MPI, MPI_Bcast( &ndataset, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (274, MPI, MPI_Bcast( donorname, 33, MPI_CHAR, Parallel::serverid, MPI_COMM_WORLD );); (104, MPI, MPI_Bcast( &ncoords, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (199, MPI, MPI_Bcast( &normalDataType, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );)"}
{"predict": "(15,OpenMP,#pragma omp parallel sections), (17,OpenMP,#pragma omp section), (18,OpenMP,#pragma omp section), (21,OpenMP,#pragma omp section), (22,OpenMP,#pragma omp section), (25,OpenMP,#pragma omp section), (28,OpenMP,#pragma omp section), (29,OpenMP,#pragma omp section), (32,OpenMP,#pragma omp section), (33,OpenMP,#pragma omp section), (35,OpenMP,#pragma omp section), (37,OpenMP,#pragma omp section), (39,OpenMP,#pragma omp section), (41,OpenMP,#pragma omp section), (43,OpenMP,#pragma omp section), (45,OpenMP,#pragma omp section), (46,OpenMP,#pragma omp section), (47,OpenMP,#pragma omp section), (49,OpenMP,#pragma omp section), (50,OpenMP,#pragma omp section), (51,OpenMP,#pragma omp section), (52,OpenMP,#pragma omp section), (54,OpenMP,#pragma omp section), (55,OpenMP,#pragma omp section), (56,OpenMP,#pragma omp section), (58,OpenMP,#pragma omp section), (65,OpenMP,#pragma omp section), (72,OpenMP,#pragma omp section), (73,OpenMP,#pragma omp section), (74,OpenMP,#pragma omp section), (75,OpenMP,#pragma omp section), (77,OpenMP,#pragma omp section), (78,OpenMP,#pragma omp section), (79,OpenMP,#pragma omp section), (80,OpenMP,#pragma omp section), (87,OpenMP,#pragma omp section), (88,OpenMP,#pragma omp section), (91,OpenMP,#pragma omp section)", "label": "(58, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (58, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(3, CUDA, using alloc = thrust::cuda::allocator<double>;); (11, CUDA, cudaDeviceSynchronize(); benchmark::DoNotOptimize(y.base()); benchmark::ClobberMemory();)"}
{"predict": "(26,OpenMP,#pragma omp parallel for)", "label": "(32, OpenMP, #pragma omp target); (35, OpenMP, int device_id = omp_get_default_device();); (54, OpenMP, #pragma omp target data use_device_ptr(a))"}
{"predict": "(7,OpenMP,#pragma omp parallel for)", "label": "(28, CUDA, cudaCheckErrors(\"kernel execution failure or cudaMemcpy H2D failure\");); (14, CUDA, cudaMemcpy(d_A, h_A, DSIZE*sizeof(float), cudaMemcpyHostToDevice);); (16, CUDA, cudaCheckErrors(\"cudaMemcpy H2D failure\");); (23, CUDA, cudaMemcpy(h_C, d_C, DSIZE*sizeof(float), cudaMemcpyDeviceToHost);); (13, CUDA, cudaMemcpy(d_B, h_B, DSIZE*sizeof(float), cudaMemcpyHostToDevice);); (7, CUDA, cudaMalloc(&d_C, DSIZE*sizeof(float));); (5, CUDA, cudaMalloc(&d_B, DSIZE*sizeof(float));); (6, CUDA, cudaCheckErrors(\"cudaMalloc failure\");); (15, CUDA, cudaCheckErrors(\"kernel launch failure\");); (1, CUDA, cudaMalloc(&d_A, DSIZE*sizeof(float));)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for collapse(2))", "label": "(17, OpenMP, #pragma omp target teams distribute parallel for collapse(2)); (23, OpenMP, #pragma omp target exit data map(delete:A[0:N1],B[0:N1],C[0:N2])); (14, OpenMP, #pragma omp target enter data map(to: A[0:N1],B[0:N1],C[0:N2])); (20, OpenMP, #pragma omp target update from(C[0:N2]))"}
{"predict": "(18,OpenMP,#pragma omp parallel for)", "label": "(16, OpenMP, #pragma omp parallel for)"}
{"predict": "(44,OpenMP,#pragma omp parallel sections)", "label": "(54, OpenCL, clReleaseEvent(event);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for)", "label": "(17, MPI, *ierr = MPI_Cart_create( (MPI_Comm)(*v1), *v2, v3, l4, l5, (MPI_Comm *)(v6) );)"}
{"predict": "OF A.\n126:        ------------------------------------------------------------*/\n127:     for (i = 0; i < m_loc; ++i) {\n128:         for (j = Astore->rowptr[i]; j < Astore->rowptr[i+1]; ++j) {\n129:             irow = perm_c[perm_r[i+fst_row]];  /* Row number in Pc*Pr*A */\n130:             jcol = Astore->colind[j];\n131:             gbi = BlockNum( irow );\n132:             gbj = BlockNum( jcol );\n133:             p = PNUM( PROW(gbi,grid), PCOL(gbj,grid ), grid );\n134:             if ( p == iam ) {\n135:                 ia[k] = irow;\n136:                 ja[k] = jcol;\n137:                 aij[k] = Astore->nzval[j];\n138:                 ++k;\n139:             } else {\n140:                 ia_send[p][ptr_to_send[p]] = irow;\n141:                 ia_send[p][ptr_to_send[p]+1] = jcol;\n142:                 aij_send[p][ptr_to_send[p]] = Astore->nzval[j];\n143:                 ++ptr_to_send[p];\n144:             }\n145:         }\n146:     }\n147: \n148:     /* ------------------------------------------------------------\n149:        SEND THE ENTRIES OF A TO THE APPROPRIATE PROCESS.\n150:        ------------------------------------------------------------*/\n151:     if ( procs > 1 ) {\n152:         if ( SendCnt ) {\n153:             for (p = 0; p < procs; ++p) {\n154:                 if ( p != iam && nnzToSend[p] > 0 ) {\n155:                     MPI_Isend(ia_send[p], 2*nnzToSend[p], mpi_int_t, p, 0,\n156:                             grid->comm, &send_req[2*p]);\n157:                     MPI_Isend(aij_send[p], nnzToSend[p], MPI_FLOAT, p, 0,\n158:                             grid->comm, &send_req[2*p+", "label": "(191, MPI, MPI_Wait( &send_req[p], &status);); (191, MPI, MPI_Wait( &send_req[procs+p], &status);); (170, MPI, MPI_Recv( itemp, it, mpi_int_t, p, p, grid->comm, &status );)"}
{"predict": "(15,OpenMP,#pragma omp parallel for)", "label": "(16, CUDA, return CUDA(cudaGetLastError());)"}
{"predict": "(46,OpenMP,#pragma omp parallel for), (64,OpenMP,#pragma omp parallel for)", "label": "(50, CUDA, CHECK_CUDA(cudaMalloc(&dy, dataSize * sizeof(int32_t)));); (75, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (32, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (76, CUDA, CHECK_CUDA(cudaFree(dy));); (45, CUDA, CHECK_CUDA(cudaMalloc(&dx, dataSize * sizeof(int32_t)));); (32, CUDA, CHECK_HIEDNN(hiednnSetCudaStream(handle, 0));); (72, CUDA, CHECK_CUDA(cudaFree(dx));)"}
{"predict": "(20,MPI,MPI_Comm_size(MPI_COMM_WORLD, &numprocs);); (20,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank););", "label": "(23, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(5,OpenMP,#pragma omp parallel for), (8,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for)", "label": "(28, OpenMP, #pragma omp parallel for reduction(&&: Red)); (7, OpenMP, #pragma omp parallel for reduction(-: Red)); (14, OpenMP, #pragma omp parallel for reduction(&: Red)); (17, OpenMP, #pragma omp parallel for reduction(|: Red)); (0, OpenMP, #pragma omp parallel for reduction(+: Red)); (19, OpenMP, #pragma omp parallel for reduction(^: Red)); (6, OpenMP, #pragma omp parallel for reduction(*: Red)); (25, OpenMP, #pragma omp parallel for reduction(||: Red))"}
{"predict": "(7,OpenMP,#pragma omp parallel for)\n(8,OpenMP,#pragma omp parallel for)\n(9,OpenMP,#pragma omp parallel for)\n(10,OpenMP,#pragma omp parallel for)\n(11,OpenMP,#pragma omp parallel for)\n(13,OpenMP,#pragma omp parallel for)\n(14,OpenMP,#pragma omp parallel for)\n(15,OpenMP,#pragma omp parallel for)\n(16,OpenMP,#pragma omp parallel for)\n(17,OpenMP,#pragma omp parallel for)\n(19,OpenMP,#pragma omp parallel for)\n(21,OpenMP,#pragma omp parallel for)\n(25,OpenMP,#pragma omp parallel for)\n(28,OpenMP,#pragma omp parallel for)\n(32,OpenMP,#pragma omp parallel for)", "label": "(25, OpenMP, #pragma omp simd aligned(e, g)); (18, OpenMP, #pragma omp simd aligned (argc : 5)); (29, OpenMP, #pragma omp simd aligned(i)); (5, OpenMP, #pragma omp simd aligned (); (17, OpenMP, #pragma omp simd aligned (S1)); (28, OpenMP, #pragma omp parallel); (40, OpenMP, #pragma omp simd aligned(f:j)); (10, OpenMP, #pragma omp simd aligned (*argc > 0 ? argv[1] : argv[2])); (6, OpenMP, #pragma omp simd aligned (argc,); (14, OpenMP, #pragma omp simd aligned (argv[1])); (-4, OpenMP, #pragma omp simd aligned); (-1, OpenMP, #pragma omp simd aligned ()); (25, OpenMP, #pragma omp simd aligned(v:16)); (28, OpenMP, #pragma omp simd aligned(f)); (14, OpenMP, #pragma omp simd aligned(h)); (-3, OpenMP, #pragma omp simd aligned (argc)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(13, MPI, MPI_Type_size(type, &size);); (14, MPI, mpi_errno = MPI_Type_ub(type, NULL);); (18, MPI, MPI_Type_free(&type);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (11, MPI, MPI_Error_class(mpi_errno, &errclass);); (4, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(14,MPI,MPI_Comm_size(comm, &size);); (14,MPI,MPI_Comm_rank(comm, &rank);); (16,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for private(sumval) reduction(+:err)),", "label": "(26, MPI, MPI_Start(&req);); (11, MPI, MPI_Comm_size(comm, &size);); (13, MPI, MPI_Info_create(&info);); (24, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (36, MPI, MPI_Info_free(&info);); (34, MPI, MPI_Request_free(&req);); (18, MPI, MPI_Reduce_scatter_init(sendbuf, &recvbuf, recvcounts, MPI_INT, MPI_SUM, comm, info, &req);); (6, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(15,OpenMP,#pragma omp parallel sections), (18,OpenMP,#pragma omp section), (20,OpenMP,#pragma omp section), (21,OpenMP,#pragma omp section), (22,OpenMP,#pragma omp section), (23,OpenMP,#pragma omp section)", "label": "(4, MPI, MPI_Init(&argc, &argv);); (30, MPI, MPI_Finalize();); (3, MPI, int ierr = MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(28,OpenMP,#pragma omp parallel for)", "label": "(7, CUDA, __shared__ block_radix_sort_t::TempStorage temp_storage;)"}
{"predict": ")\n116:     {\n117:         stats_shared[warp] = stats_local;\n118:     }\n119:     __syncwarp();\n120: \n121:     if (is_cta_lead)\n122:     {\n123:         half2 stats_global = {0, 0};\n124: #pragma unroll\n125:         for (int32_t ii = 0; ii < WARPS; ii++)\n126:         {\n127:             stats_global = stats_global + stats_shared[ii];\n128:         }\n129:         const float mean = __low2float(stats_global);\n130:         const float m2 = __high2float(stats_global);\n131:         const float var = m2 - mean * mean;\n132:         const float stddev = rsqrtf(var + 1e-6f);\n133:         stats_global = __floats2half2_rn(mean, stddev);\n134:         stats_shared[warp] = stats_global;\n135:     }\n136:     __syncwarp();\n137: \n138:     if (is_warp_lead)\n139:     {\n140:         stats_local = stats_shared[warp];\n141:     }\n142:     __syncwarp();\n143: \n144:     const float mean = __low2float(stats_local);\n145:     const float stddev = __high2float(stats_local);\n146: \n147: #pragma unroll\n148:     for (int32_t ii = 0; ii < LDGS * 4; ii++)\n149:     {\n150: #pragma unroll\n151:         for (int32_t jj = 0; jj < 4; jj++)\n152:         {\n153:             const float tmp = (hdata[ii][jj] - mean) * stddev;\n154:             hdata[ii][jj] = tmp;\n155:         }\n156:     }\n157: \n158:     #pragma unroll\n159:     for (int32_t ii = 0; ii < LDGS;", "label": "(24, CUDA, extern __shared__ char smem_[];); (26, CUDA, __shared__ half2 smem_red[VECS_PER_CTA][WARPS];)"}
{"predict": "(68,OpenMP,#pragma omp parallel for private(device_type, gpu_found) reduction(||:gpu_found)), (124,OpenMP,#pragma omp parallel for reduction(+:plan->num_kernels))", "label": "(76, OpenCL, err = clBuildProgram(plan->program, 1, &devices[i], coptions, NULL, NULL);); (59, OpenCL, plan->program = clCreateProgramWithSource(context, 1, (const char**) &source_str, NULL, &err);); (68, OpenCL, err = clGetDeviceInfo(devices[i], CL_DEVICE_TYPE, sizeof(device_type), &device_type, NULL);); (88, OpenCL, err = clGetDeviceInfo(devices[i], CL_DEVICE_NAME, sizeof(devicename), devicename, NULL);)"}
{"predict": "(16,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp for), (20,OpenMP,#pragma omp for), (23,OpenMP,#pragma omp for), (25,OpenMP,#pragma omp for)", "label": "(24, MPI, MPI_Recv(&ack, 1, MPI_CHAR, right, tag, comm, MPI_STATUS_IGNORE);); (26, MPI, MPI_Irecv(out, count, type, set_src ? left : MPI_ANY_SOURCE, tag, comm, &requests[w]);); (28, MPI, MPI_Isend(in, count, type, left, (tag == MPI_ANY_TAG ? 0 : tag), comm, &requests[w + window_size]);); (20, MPI, MPI_Waitall(2 * window_size, requests, MPI_STATUSES_IGNORE);); (17, MPI, MPI_Isend(in, count, type, right, (tag == MPI_ANY_TAG ? 0 : tag), comm, &requests[w + window_size]);); (13, MPI, MPI_Irecv(out, count, type, set_src ? right : MPI_ANY_SOURCE, tag, comm, &requests[w]);); (27, MPI, MPI_Send(&ack, 1, MPI_CHAR, left, (tag == MPI_ANY_TAG ? 0 : tag), comm);)"}
{"predict": "(3,OpenMP,#pragma omp parallel for), (4,OpenMP,#pragma omp parallel for), (5,OpenMP,#pragma omp parallel for), (8,OpenMP,#pragma omp parallel for), (10,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for)", "label": "(13, OpenMP, #pragma omp parallel master taskloop simd collapse (ST); (5, OpenMP, #pragma omp parallel master taskloop simd collapse ()); (15, OpenMP, #pragma omp parallel master taskloop simd collapse ((ST > 0) ? 1 + ST : 2)); (1, OpenMP, #pragma omp parallel master taskloop simd collapse (); (20, OpenMP, #pragma omp parallel master taskloop simd collapse (S)); (10, OpenMP, #pragma omp parallel master taskloop simd collapse (1))); (27, OpenMP, #pragma omp parallel master taskloop simd collapse (1)); (30, OpenMP, #pragma omp parallel master taskloop simd collapse (2)); (14, OpenMP, #pragma omp parallel master taskloop simd collapse (foobool(argc)), collapse (true), collapse (-5)); (-7, OpenMP, #pragma omp parallel master taskloop simd collapse); (0, OpenMP, #pragma omp parallel master taskloop simd collapse (argc); (24, OpenMP, #pragma omp parallel master taskloop simd collapse (N))"}
{"predict": "(6,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for)", "label": "(5, TBB, tbb::task_group_context tgc;); (15, TBB, tgc.set_priority(tbb::priority_high);); (4, TBB, tgc.set_priority(tbb::priority_normal);)"}
{"predict": "(9,OpenMP,#pragma omp parallel for)", "label": "(8, OpenMP, #pragma omp parallel for)"}
{"predict": "(14,OpenMP,#pragma omp parallel)", "label": "(13, OpenMP, #pragma omp parallel); (13, OpenMP, #pragma omp task if (task: false))"}
{"predict": "(8,OpenMP,#pragma omp parallel for)\n(15,OpenMP,#pragma omp parallel for)\n(22,OpenMP,#pragma omp parallel for)\n(29,OpenMP,#pragma omp parallel for)\n(36,OpenMP,#pragma omp parallel for)\n(43,OpenMP,#pragma omp parallel for)\n(50,OpenMP,#pragma omp parallel for)", "label": "(42, OpenMP, #pragma omp distribute parallel for schedule(static, ch)); (49, OpenMP, #pragma omp distribute parallel for schedule(dynamic)); (7, OpenMP, #pragma omp distribute parallel for); (15, OpenMP, #pragma omp distribute parallel for dist_schedule(static)); (30, OpenMP, #pragma omp distribute parallel for schedule(static)); (53, OpenMP, #pragma omp distribute parallel for schedule(dynamic, ch)); (5, OpenMP, #pragma omp cancel for); (0, OpenMP, #pragma omp target); (18, OpenMP, #pragma omp distribute parallel for dist_schedule(static, ch)); (-1, OpenMP, #pragma omp teams)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (10,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for), (35,OpenMP,#pragma omp parallel for), (41,OpenMP,#pragma omp parallel for), (51,OpenMP,#pragma omp parallel for), (56,OpenMP,#pragma omp parallel for), (60,OpenMP,#pragma omp parallel for)", "label": "(35, OpenMP, #pragma omp distribute private(e, g)); (51, OpenMP, #pragma omp parallel private(i)); (5, OpenMP, #pragma omp distribute private); (12, OpenMP, #pragma omp distribute private (argc,); (9, OpenMP, #pragma omp distribute private (argc); (36, OpenMP, #pragma omp target); (19, OpenMP, #pragma omp distribute private (argv[1])); (12, OpenMP, #pragma omp distribute private (argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (9, OpenMP, #pragma omp distribute private (argc > 0 ? argv[1] : argv[2])); (14, OpenMP, #pragma omp distribute private (a, b, c, d, f)); (17, OpenMP, #pragma omp distribute private(ba)); (39, OpenMP, #pragma omp distribute private(j)); (-1, OpenMP, #pragma omp distribute private ()); (24, OpenMP, #pragma omp distribute private(h)); (44, OpenMP, #pragma omp teams reduction(+:i)); (18, OpenMP, #pragma omp distribute private(S2::S2s)); (43, OpenMP, #pragma omp distribute private(i)); (14, OpenMP, #pragma omp distribute private(da)); (-9, OpenMP, #pragma omp distribute private (); (2, OpenMP, #pragma omp distribute private (S1)); (29, OpenMP, #pragma omp teams private(i)); (8, OpenMP, #pragma omp distribute private(ca)); (17, OpenMP, #pragma omp distribute); (22, OpenMP, #pragma omp distribute firstprivate(i)); (30, OpenMP, #pragma omp teams firstprivate(i)); (17, OpenMP, #pragma omp teams)"}
{"predict": "(23,OpenMP,#pragma omp parallel for private(nid, counter) reduction(+:counter))", "label": "(22, CUDA, __shared__ volatile ll_t smem[_TPB_];)"}
{"predict": "// Tolerance is set to zero to ensure CG runs refMaxIters iterations\n144:   for (int i=0; i< numberOfCalls; ++i) {\n145:     FillRandomVector(x);\n146:     FillRandomVector(b);\n147:     double t_start = mytimer();\n148:     int ierr = ComputeResidual_ref(A, x, b, xexact, tolerance, niters, normr, normr0, ref_times);\n149:     if (ierr) {\n150:       global_failure = 1;\n151:       HPCG_fout << \"Error in call to ComputeResidual_ref: \" << ierr << \".\\n\" << endl;\n152:     }\n153:     totalNiters_ref += niters;\n154:     times[1] += (mytimer() - t_start); // Add to running total for reference CG times\n155:   }\n156: \n157:   // Now, we will run the optimized version of CG, which may use different kernels and data structures.\n158:   times[2] = mytimer();\n159:   for (int i=0; i< numberOfCalls; ++i) {\n160:     FillRandomVector(x);\n161:     FillRandomVector(b);\n162:     double t_start = mytimer();\n163:     int ierr = ComputeResidual(A, x, b, xexact, tolerance, niters, normr, normr0, times);\n164:     if (ierr) {\n165:       global_failure = 1;\n166:       HPCG_fout << \"Error in call to ComputeResidual: \" << ierr << \".\\n\" << endl;\n167:     }\n168:     times[2] += (mytimer() - t_start); // Add to running total for optimized CG times\n169:   }\n170:   times[2] = (times[2] - mytimer()) / ((double) numberOfCalls);  // Total time divided by number of calls.\n171: \n172:   // Compute the residual reduction for the optimized version\n173:   times[3] = mytimer();\n174:   for", "label": "(3, MPI, MPI_Init(&argc, &argv);); (303, MPI, MPI_Finalize();); (23, MPI, MPI_Barrier(MPI_COMM_WORLD);); (222, MPI, MPI_Allreduce(&local_opt_worst_time, &opt_worst_time, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for reduction(+:res))", "label": "(7, CUDA, fcuda::Check_CudaErroorFun(\"DgReduSumFloat3\");); (5, CUDA, cudaMemcpy(data,datag+inidata,sizeof(float3)*ndata,cudaMemcpyDeviceToHost);)"}
{"predict": ", 8i,\n82:                            7,\n83:                            KMP_ARCH_X86) // __kmpc_atomic_fixed8_div_cpt_rev_fp\n84: ATOMIC_CMPXCHG_CPT_REV_MIX(fixed8u, kmp_uint64, div_cpt_rev, 64, /, fp, _Quad,\n85:                            8i, 7,\n86:                            KMP_ARCH_X86) // __kmpc_atomic_fixed8u_div_cpt_rev_fp\n87: \n88: #endif // KMP_HAVE_QUAD\n89: \n90: #if KMP_HAVE_QUAD\n91: \n92: ATOMIC_CRITICAL_CPT_REV_MIX(fixed16, kmp_int128, sub_cpt_rev, 128, -, fp, _Quad,\n93:                             16i, 15,\n94:                             KMP_ARCH_X86) // __kmpc_atomic_fixed16_sub_cpt_rev_fp\n95: ATOMIC_CRITICAL_CPT_REV_MIX(fixed16u, kmp_uint128, sub_cpt_rev, 128, -, fp,\n96:                             _Quad, 16i, 15,\n97:                             KMP_ARCH_X86) // __kmpc_atomic_fixed16u_sub_cpt_rev_fp\n98: ATOMIC_CRITICAL_CPT_REV_MIX(fixed16, kmp_int128, div_cpt_rev, 128, /, fp, _Quad,\n99:                             16i, 15,\n100:                             KMP_ARCH_X86) // __kmpc_atomic_fixed16_div_cpt_rev_fp\n101: ATOMIC_CRITICAL_CPT_REV_MIX(fixed16u, kmp_uint128, div_cpt_rev, 128, /, fp,\n102:                             _Quad, 16i, 15,\n103:                             KMP_ARCH_X86) // __kmpc_atomic_fixed16u_div_cpt_rev_fp\n104: \n105: #endif // KMP_HAVE_QUAD\n106: \n107: #if KMP_ARCH_X86\n108: ATOMIC_CRITICAL_CPT", "label": "(127, OpenMP, #define GOMP_CRITICAL_SWP(FLAG))"}
{"predict": "(24,OpenMP,#pragma omp parallel for reduction(+:total))", "label": "(27, CUDA, cudaMemcpy(dst, src, bytes, kind);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for private(b))", "label": "(9, TBB, tbb::atomic<int> b;); (6, TBB, std::list< std::pair< const int, tbb::atomic<int> > > arrIntTbb;)"}
{"predict": ". */\n164:     set_default_options_dist(&options);\n165: \n166:     /* Print the options. */\n167:     if (grid.iam == 0) print_options_dist(&options);\n168: \n169:     /* Initialize ScalePermstruct and LUstruct. */\n170:     zScalePermstructInit(m, n, &ScalePermstruct);\n171:     zLUstructInit(n, &LUstruct);\n172: \n173:     /* Initialize the statistics variables. */\n174:     PStatInit(&stat);\n175: \n176:     /* Factor the matrix. */\n177:     zgssvx3d(&options, &A, &ScalePermstruct, &LUstruct, &grid, &SOLVEstruct,\n178:              b, ldb, nrhs, &grid, berr, &stat, &info);\n179: \n180:     /* Check the accuracy of the solution. */\n181:     zinf_norm_error_dist(n, nrhs, &A, xtrue, ldx, b, ldb, &grid, &stat);\n182: \n183:     /* Print the statistics. */\n184:     if (grid.iam == 0) PStatPrint(&stat, &options);\n185: \n186:     /* ------------------------------------------------------------\n187:        2. SOLVE THE LINEAR SYSTEM FOR THE SECOND TIME, WITH 2 RHS.\n188:        ------------------------------------------------------------*/\n189:     nrhs = 2;\n190:     if (!(b1 = doublecomplexMalloc_dist (nrhs*ldb)))\n191:         ABORT (\"Malloc fails for b1[].\");\n192:     if (!(xtrue1 = doublecomplexMalloc_dist (nrhs*ldx)))\n193:         ABORT (\"Malloc fails for xtrue1[].\");\n194: \n195:     /* Initialize the right hand side and solution. */\n196:     zgen_rhs3d(nrhs, nrhs, ldb, b1, &grid);\n197:     zgen_xtrue3d(nrhs, nrhs, ldx, xtrue1, &grid);\n198: \n199:     /* Solve the system again.", "label": "(97, MPI, MPI_Query_thread(&omp_mpi_level);); (38, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (33, MPI, MPI_Init_thread(&argc, &argv, required, &provided);)"}
{"predict": "3:     broken_iov_arrays = (struct iovec **) malloc (fh->f_num_aggrs * sizeof(struct iovec *));\n134:     broken_decoded_iovs = (struct iovec **) malloc (fh->f_num_aggrs * sizeof(struct iovec *));\n135:     broken_counts = (int *) malloc (fh->f_num_aggrs * sizeof(int));\n136:     broken_iov_counts = (int *) malloc (fh->f_num_aggrs * sizeof(int));\n137:     broken_total_lengths = (MPI_Aint *) malloc (fh->f_num_aggrs * sizeof(MPI_Aint));\n138:     \n139:     for ( i=0; i<fh->f_num_aggrs; i++ ) {\n140:         broken_iov_arrays[i] = NULL;\n141:         broken_decoded_iovs[i] = NULL;\n142:         broken_counts[i] = 0;\n143:         broken_iov_counts[i] = 0;\n144:         broken_total_lengths[i] = 0;\n145:     }\n146:     \n147:     ret = mca_fcoll_vulcan_even_distribution ( fh,\n148:                                                local_iov_array,\n149:                                                local_count,\n150:                                                broken_iov_arrays,\n151:                                                broken_decoded_iovs,\n152:                                                broken_counts,\n153:                                                broken_iov_counts,\n154:                                                broken_total_lengths,\n155:                                                fh->f_num_aggrs);\n156:     if (ret != OMPI_SUCCESS){\n157:         goto exit;\n158:     }\n159:     \n160:     /*************************************************************************\n161:      ** 3. Generate the global offsets/lengths array corresponding to\n162:      **    this write operation\n163:      *************************************************************************/\n164:     displs = (ptrdiff_t *) malloc (fh->f_num_aggrs * sizeof(ptrdiff_t));\n165:     result_counts = (int *) malloc (fh->f_num_aggrs * sizeof(int));\n166:     \n167:     for ( i=0; i<fh->f_num_aggrs; i++ ) {\n16", "label": "(391, MPI, start_exch = MPI_Wtime();); (143, MPI, start_comm_time = MPI_Wtime();); (156, MPI, end_comm_time = MPI_Wtime();); (435, MPI, start_write_time = MPI_Wtime();); (442, MPI, end_write_time = MPI_Wtime();); (495, MPI, end_exch = MPI_Wtime();)"}
{"predict": "(25,OpenMP,#pragma omp parallel for)", "label": "(33, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (35, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(18,OpenMP,#pragma omp parallel for collapse(2)), (42,OpenMP,#pragma omp parallel for collapse(2))", "label": "(43, MPI, MPI_Pack(a, 1, xpose, buffer, bufsize, &position, MPI_COMM_WORLD);); (59, MPI, MPI_Type_free(&row);); (31, MPI, MPI_Type_commit(&xpose);); (56, MPI, MPI_Type_free(&xpose);); (28, MPI, MPI_Type_hvector(100, 1, sizeofint, row, &xpose);); (23, MPI, MPI_Type_extent(MPI_INT, &sizeofint);); (30, MPI, MPI_Pack_size(1, xpose, MPI_COMM_WORLD, &bufsize);); (34, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (39, MPI, MPI_Unpack(buffer, bufsize, &position, b, 100 * 100, MPI_INT, MPI_COMM_WORLD);); (22, MPI, MPI_Type_vector(100, 1, 100, MPI_INT, &row);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for private(i,j,recvstatus))", "label": "(10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (42, MPI, MPI_Waitall(nprocs, sendreqs, sendstats);); (13, MPI, MPI_Barrier(MPI_COMM_WORLD);); (21, MPI, MPI_Isend(sendbuf, BUF_COUNT, MPI_INT, i, 0, MPI_COMM_WORLD, &sendreqs[i]);)"}
{"predict": "120: \n121:     /* Allocate host memory for matrix A */\n122:     if (rankId == 0)\n123:     {\n124:         h_A   = (double*)malloc(lda * colsA * sizeof(double));\n125:         h_QR  = (double*)malloc(lda * colsA * sizeof(double));\n126:         h_tau = (double*)malloc(N * sizeof(double));\n127: \n128:         /* Initialize matrix A with random values */\n129:         for (int64_t i = 0; i < lda * colsA; i++)\n130:         {\n131:             h_A[i] = rand() / (double)RAND_MAX;\n132:         }\n133:     }\n134: \n135:     /* =========================================== */\n136:     /*          Create inputs on master rank       */\n137:     /* =========================================== */\n138: \n139:     /* Create distributed matrix descriptor */\n140:     cusolverStat = cusolverMpCreateMatrixDescriptor(&descrA, M, N, lda, CUDA_R_64F);\n141:     assert(cusolverStat == CUSOLVER_STATUS_SUCCESS);\n142: \n143:     /* Create distributed matrix grid */\n144:     cusolverStat = cusolverMpCreateGrid(&gridA, numRowDevices, numColDevices, descrA, RSRCA, CSRCA, cal_comm);\n145:     assert(cusolverStat == CUSOLVER_STATUS_SUCCESS);\n146: \n147:     /* Create distributed matrix A */\n148:     cusolverStat = cusolverMpCreateDistMatrix(&d_A, CUDA_R_64F, gridA, IA, JA, M, N);\n149:     assert(cusolverStat == CUSOLVER_STATUS_SUCCESS);\n150: \n151:     /* Create distributed matrix tau */\n152:     cusolverStat = cusolverMpCreateDistMatrix(&d_tau, CUDA_R_64F, gridA, 1, 1, 1, N);\n153:     assert(cusolverStat == CUSOLVER_STATUS_SUCCESS);\n154: \n155:     /* Create", "label": "(358, CUDA, cudaStat = cudaMemcpy(h_A, d_global_Q, sizeof(double) * lda * colsA, cudaMemcpyDeviceToHost);); (196, CUDA, cudaStat = cudaMalloc((void**)&d_info_geqrf, sizeof(int));); (27, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rankId);); (429, CUDA, cudaStat = cudaFree(d_tau);); (-1, MPI, MPI_Init(NULL, NULL);); (274, CUDA, cudaStat = cudaMemcpyAsync(&h_info_geqrf, d_info_geqrf, sizeof(int), cudaMemcpyDeviceToHost, localStream);); (471, CUDA, cudaStat = cudaStreamDestroy(localStream);); (474, MPI, MPI_Barrier(MPI_COMM_WORLD);); (42, CUDA, cudaStat = cudaSetDevice(localDeviceId);); (43, CUDA, cudaStat = cudaFree(0);); (194, CUDA, cudaStat = cudaMemset(d_info_geqrf, 0, sizeof(int));); (428, CUDA, cudaStat = cudaFree(d_work_geqrf);); (178, CUDA, cudaStat = cudaMalloc((void**)&d_tau, localColsA * sizeof(double));); (301, CUDA, cudaStat = cudaMalloc((void**)&d_global_Q, lda * colsA * sizeof(double));); (213, CUDA, cudaStat = cudaMalloc((void**)&d_work_geqrf, workspaceInBytesOnDevice_geqrf);); (431, CUDA, cudaStat = cudaFree(d_info_geqrf);); (12, MPI, MPI_Comm_size(MPI_COMM_WORLD, &rankSize);); (154, CUDA, cudaStat = cudaMalloc((void**)&d_A, localColsA * LLDA * sizeof(double));); (305, CUDA, cudaStat = cudaMemcpy(d_global_R, d_global_Q, sizeof(double) * lda * colsA, cudaMemcpyDeviceToDevice);); (302, CUDA, cudaStat = cudaMemcpy(d_global_Q, h_A, sizeof(double) * lda * colsA, cudaMemcpyHostToDevice);); (53, CUDA, cudaStat = cudaStreamCreate(&localStream);); (463, MPI, MPI_Finalize();); (403, CUDA, cudaStat = cudaFree(d_A);); (293, CUDA, cudaStat = cudaMalloc((void**)&d_global_R, lda * colsA * sizeof(double));); (294, CUDA, cudaStat = cudaMalloc((void**)&d_global_tau, colsA * sizeof(double));)"}
{"predict": "(28,OpenMP,#pragma omp parallel for reduction(+:threadVal)), (48,OpenMP,#pragma omp parallel for reduction(+:threadVal))", "label": "(4, CUDA, __shared__ cg::block_tile_memory<BlockSize> scratch;)"}
{"predict": "(29,OpenMP,#pragma omp parallel for), (47,OpenMP,#pragma omp parallel for)", "label": "(59, CUDA, cudaFree( deviceInt );); (59, CUDA, cudaFreeHost( hostInt );)"}
{"predict": "(31,OpenMP,#pragma omp parallel for), (43,OpenMP,#pragma omp parallel for)", "label": "(98, CUDA, cudaFree( dptrPosMass[i] );); (100, CUDA, cudaSetDevice( oldDevice );); (97, CUDA, cudaFree( dptrForce[i] );)"}
{"predict": "(4,OpenMP,#pragma omp parallel sections), (5,OpenMP,#pragma omp section), (7,OpenMP,#pragma omp section), (10,OpenMP,#pragma omp section)", "label": "(5, OpenCL, clReleaseMemObject(objX);); (12, OpenCL, clReleaseMemObject(objScratch);); (7, OpenCL, clReleaseMemObject(objiAmax);)"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for)", "label": "(19, TBB, REMARK(\"Wraparound on tbb::concurrent_bounded_queue...\");); (2, TBB, tbb::concurrent_queue<T>* q = new tbb::concurrent_queue<T>;); (16, TBB, tbb::concurrent_bounded_queue<T>* q = new tbb::concurrent_bounded_queue<T>;)"}
{"predict": "(21,OpenMP,#pragma omp parallel for private(g))", "label": "(25, OpenMP, printf(\"Thread %d sleeps now for %d seconds\\n\", omp_get_thread_num(), SLEEPTIME);); (8, OpenMP, #pragma omp parallel); (31, OpenMP, printf(\"Thread %d wakes up again\\n\", omp_get_thread_num());)"}
{"predict": "(26,OpenMP,#pragma omp parallel for private(AllocaBuilder,T,ArrSizeC,ArrSize))", "label": "(45, SYCL, GEP->setMetadata(hipsycl::compiler::MDKind::Arrayified, MDAlloca);); (36, SYCL, Alloca->setMetadata(hipsycl::compiler::MDKind::Arrayified, MDAlloca);)"}
{"predict": "expected-note {{defined as firstprivate}}\n105:   for (i = 0; i < argc; ++i) // expected-error {{loop iteration variable in the associated loop of 'omp parallel master taskloop simd' directive may not be firstprivate, predetermined as linear}}\n106:     foo();\n107:   return 0;\n108: }", "label": "(22, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argc); (78, OpenMP, #pragma omp parallel master taskloop simd safelen(5)); (81, OpenMP, #pragma omp parallel master taskloop simd firstprivate(e, g)); (125, OpenMP, #pragma omp parallel private(i)); (87, OpenMP, #pragma omp parallel master taskloop simd firstprivate(h)); (53, OpenMP, #pragma omp parallel master taskloop simd firstprivate(ca)); (48, OpenMP, #pragma omp parallel master taskloop simd firstprivate(ba)); (27, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (38, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argv[1])); (90, OpenMP, #pragma omp parallel master taskloop simd firstprivate(i)); (-1, OpenMP, #pragma omp parallel); (64, OpenMP, #pragma omp parallel master taskloop simd firstprivate(S2::S2sc)); (128, OpenMP, #pragma omp parallel master taskloop simd firstprivate(B::x)); (119, OpenMP, #pragma omp parallel reduction(+ : i)); (88, OpenMP, #pragma omp parallel shared(xa)); (23, OpenMP, #pragma omp parallel master taskloop simd firstprivate(S1)); (14, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argc > 0 ? argv[1] : argv[2])); (78, OpenMP, #pragma omp parallel master taskloop simd private(xa), firstprivate(xa)); (53, OpenMP, #pragma omp parallel master taskloop simd firstprivate(S2::S2s)); (125, OpenMP, #pragma omp parallel master taskloop simd firstprivate(si)); (-6, OpenMP, #pragma omp parallel master taskloop simd firstprivate(); (-3, OpenMP, #pragma omp parallel master taskloop simd firstprivate()); (65, OpenMP, #pragma omp parallel master taskloop simd firstprivate(m)); (84, OpenMP, #pragma omp parallel master taskloop simd firstprivate(j)); (18, OpenMP, #pragma omp parallel master taskloop simd firstprivate(a, b, c, d, f)); (86, OpenMP, #pragma omp parallel master taskloop simd lastprivate(g) firstprivate(g)); (89, OpenMP, #pragma omp parallel master taskloop simd lastprivate(n) firstprivate(n)); (-1, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argc,); (39, OpenMP, #pragma omp parallel master taskloop simd firstprivate(xa)); (21, OpenMP, #pragma omp parallel master taskloop simd firstprivate(2 * 2)); (32, OpenMP, #pragma omp parallel master taskloop simd firstprivate(da)); (-21, OpenMP, #pragma omp parallel master taskloop simd firstprivate)"}
{"predict": "(11,OpenMP,#pragma omp parallel for private(i2) reduction(+:sum))", "label": "(14, OpenMP, #pragma omp flush); (9, OpenMP, #pragma omp parallel for reduction(+:sum) schedule(static,1) private(i) private(i2))"}
{"predict": "(16,OpenMP,#pragma omp parallel for)", "label": "(21, MPI, MPI_Recv(&ack, 1, MPI_CHAR, right, tag, comm, MPI_STATUS_IGNORE);); (23, MPI, MPI_Irecv(out, count, type, set_src ? left : MPI_ANY_SOURCE, tag, comm, &requests[w]);); (18, MPI, MPI_Waitall(window_size, requests, MPI_STATUSES_IGNORE);); (15, MPI, MPI_Isend(in, count, type, right, (tag == MPI_ANY_TAG ? 0 : tag), comm, &requests[w]);); (23, MPI, MPI_Send(&ack, 1, MPI_CHAR, left, (tag == MPI_ANY_TAG ? 0 : tag), comm);)"}
{"predict": "(7,OpenMP,#pragma omp parallel for private(curTok, fpcurTok, curTokLen) schedule(static))", "label": "(34, OpenMP, comp_do_parse(cur, tail);)"}
{"predict": "(15,OpenMP,#pragma omp target enter data map(to:data[0:16])); (16,OpenMP,#pragma omp target); (17,OpenMP,#pragma omp target); (18,OpenMP,#pragma omp target exit data map(delete:data[0:16]));", "label": "(3, OpenMP, #pragma omp target device(0)); (13, OpenMP, #pragma omp target device(1))"}
{"predict": "(7,OpenMP,#pragma omp atomic), (15,OpenMP,#pragma omp atomic)", "label": "(4, OpenMP, #pragma omp master); (5, OpenMP, #pragma omp taskgroup); (6, OpenMP, #pragma omp task shared(var, a)); (0, OpenMP, #pragma omp parallel num_threads(2) shared(var, a))"}
{"predict": "(4,OpenMP,#pragma omp parallel private(rank)), (7,OpenMP,#pragma omp for), (15,OpenMP,#pragma omp parallel for)", "label": "(3, MPI, MPI_Comm_size(comm, &size);); (3, MPI, MPI_Comm_rank(comm, &rank);); (13, MPI, MPI_Alltoallv(data_out, counts, displs, MPI_INT, data_in, counts, displs, MPI_INT, comm);)"}
{"predict": "(1,MPI,MPI_Init(&argc, &argv);); (7,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (78,OpenMP,#pragma omp parallel for)", "label": "(11, MPI, MPI_Error_string(error, err_string, &length);); (20, MPI, error = MPI_Init(&argc, &argv);); (27, MPI, error = MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (60, MPI, error = MPI_Finalize();); (50, MPI, error = MPI_Barrier(MPI_COMM_WORLD);); (29, MPI, MPI_Abort(MPI_COMM_WORLD, error);); (3, MPI, error = MPI_Finalized(&flag);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for)", "label": "(13, CUDA, cudaMalloc(&dev_B, size_b);); (13, CUDA, cudaDeviceSynchronize();); (10, CUDA, cudaMalloc(&dev_A, size_a);); (27, CUDA, cudaFree(dev_B);); (25, CUDA, cudaFree(dev_A);)"}
{"predict": "(120,OpenMP,#pragma omp distribute parallel for)", "label": "(35, OpenMP, #pragma omp target teams map(marr[:arg][n:])); (166, OpenMP, #pragma omp target teams map(r.S.Arr[4:5])); (53, OpenMP, #pragma omp target teams map(marr[:arg][:2][:])); (120, OpenMP, #pragma omp target teams map(r.PtrS[0], r.PtrS->B)); (117, OpenMP, #pragma omp target teams map(r.ArrS[0].A, t.ArrS[1].A)); (136, OpenMP, #pragma omp target teams map(r.C, r.D)); (9, OpenMP, #pragma omp target teams map(r)); (80, OpenMP, #pragma omp target teams map(mvla2[:1][:2][:10])); (123, OpenMP, #pragma omp target teams map(r.RPtrS, r.RPtrS->B)); (136, OpenMP, #pragma omp target teams map(r.C) map(r.C)); (21, OpenMP, #pragma omp target teams map(marr[arg:][:arg][:])); (189, OpenMP, #pragma omp target data map(to: t)); (157, OpenMP, #pragma omp target teams map(r.S.Ptr[4:5])); (54, OpenMP, #pragma omp target teams map(marr[:2][2+2-4:][0:5+5])); (35, OpenMP, #pragma omp target teams map(marr[:2][:][1:])); (12, OpenMP, #pragma omp target teams map(marr[arg:][:][:])); (93, OpenMP, #pragma omp target teams map(r.ArrS[:arg].B)); (0, OpenMP, #pragma omp target teams map(marr[2][0:2][0:2])); (80, OpenMP, #pragma omp target teams map(mptr[:1][:2][0:2])); (53, OpenMP, #pragma omp target teams map(marr2[:1][:2][0])); (65, OpenMP, #pragma omp target teams map(mvla2[:1][:2][:11])); (86, OpenMP, #pragma omp target teams map(r.ArrS[:1].B)); (35, OpenMP, #pragma omp target teams map(marr[:1][3:1][:2])); (6, OpenMP, #pragma omp target teams map(marr[arg:])); (29, OpenMP, #pragma omp target teams map(marr[:1][0][:])); (134, OpenMP, #pragma omp target teams map(r.Ptr[3:5])); (74, OpenMP, #pragma omp target teams map(mptr[:1][:][0:2])); (108, OpenMP, #pragma omp target teams map(r.S.Arr[:12])); (125, OpenMP, #pragma omp target teams map(r.A)); (90, OpenMP, #pragma omp target teams map(r.ArrS[0].A, r.ArrS[1].A)); (95, OpenMP, #pragma omp target teams map(r.PtrS, r.PtrS->B)); (164, OpenMP, #pragma omp target teams map(t.D)); (149, OpenMP, #pragma omp target teams map(r.D)); (96, OpenMP, #pragma omp target teams map(r.RPtrS[0], r.RPtrS->B)); (77, OpenMP, #pragma omp target teams map(r.ArrS[0].Arr[1:23])); (98, OpenMP, #pragma omp target teams map(r.RPtrS->A, r.RPtrS->B)); (45, OpenMP, #pragma omp target teams map(mvla[1][2:arg][:])); (142, OpenMP, #pragma omp target data map(to: r.C)); (21, OpenMP, #pragma omp target teams map(marr[:1][3:arg][:2])); (8, OpenMP, #pragma omp target teams map(marr[:2][:][:1])); (37, OpenMP, #pragma omp target teams map(mvla[:2][:arg][:])); (61, OpenMP, #pragma omp target teams map(mptr[:2][:1][0:2])); (71, OpenMP, #pragma omp target teams map(r.ArrS[0].Arr[1:arg])); (142, OpenMP, #pragma omp target data map(to: t.Ptr)); (31, OpenMP, #pragma omp target teams map(mvla[:1][:][0])); (112, OpenMP, #pragma omp target teams map(r.Arr[3:5])); (46, OpenMP, #pragma omp target teams map(mptr[:1][:2-1][2:4-3])); (108, OpenMP"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for)", "label": "(12, OpenMP, #pragma omp target map(tofrom: C) map(to: tt, tt.p1[:10]))"}
{"predict": "(3,OpenMP,#pragma omp parallel sections)", "label": "(7, CUDA, cudaFree(bufferOffset.W);); (3, CUDA, cudaFree(bufferOffset.srcShiftSum);); (1, CUDA, cudaFree(bufferOffset.dstMean);); (10, CUDA, cudaFree(cusolverData.cusolverBuffer);); (5, CUDA, cudaFree(bufferOffset.J);); (3, CUDA, cudaFree(bufferOffset.r);); (4, CUDA, cudaFree(bufferOffset.calc_buffer);); (-1, CUDA, cudaFree(bufferOffset.LtL);); (-6, CUDA, cudaFree(bufferOffset.srcMean);); (-4, CUDA, cudaFree(bufferOffset.dstShiftSum);); (4, CUDA, cudaFree(cusolverData.cusolverInfo);)"}
{"predict": "_values(cufftdx_smemio_results.output, cufft_results.output);\n116:         std::cout << \"cuFFTDx with shared memory IO\\n\";\n117:         std::cout << \"L2 error: \" << fft_error.l2_relative_error << \"\\n\";\n118:         std::cout << \"Peak error (index: \" << fft_error.peak_error_index << \"): \" << fft_error.peak_error << \"\\n\";\n119:         if(success) {\n120:             success = (fft_error.l2_relative_error < 0.001);\n121:         }\n122:     }\n123: \n124:     if(success) {\n125:         std::cout << \"Test passed.\\n\";\n126:     } else {\n127:         std::cout << \"Test failed.\\n\";\n128:     }\n129: }\n\n### **Analysis**\nThe code provided is a template function `fft_2d` that performs 2D Fast Fourier Transform (FFT) using cuFFTDx library. The code includes multiple FFT calculations and correctness checks. The primary parallelization opportunities are in the data initialization and correctness checks. The FFT computations themselves are already optimized for parallel execution on the GPU, so parallelization there is not necessary.\n\n### **Insertion Points**\n1. Data Initialization Loop (Lines 57-61)\n2. Correctness Check Loop (Lines 92-98)\n3. Correctness Check Loop (Lines 115-121)\n\n### **Parallelization Instructions**\n1. **Data Initialization Loop (Lines 57-61)**\n   - **Parallel Type**: OpenMP\n   - **Insertion Point**: Line 57\n   - **Code Line**: `#pragma omp parallel for`\n\n2. **Correctness Check Loop (Lines 92-98)**\n   - **Parallel Type**: OpenMP\n   - **Insertion Point**: Line 92\n   - **Code Line**: `#pragma omp parallel for`\n\n3. **Correctness Check Loop (Lines 115-121)**\n   - **Parallel Type**: OpenMP\n   - **Insertion Point**: Line 115\n   - **Code Line**: `#pragma omp parallel for`\n\n### **Final Output**\n(57,OpenMP,#pragma", "label": "(77, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamCreate(&stream));); (71, CUDA, CUDA_CHECK_AND_EXIT(cudaMemset(output, 0b11111111, flat_fft_size_bytes));); (71, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(input, input_host.data(), flat_fft_size_bytes, cudaMemcpyHostToDevice));); (91, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(output));); (70, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (88, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(input));); (62, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&input, flat_fft_size_bytes));); (83, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamDestroy(stream));); (61, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&output, flat_fft_size_bytes));)"}
{"predict": ")expected == 0.0f && outData[i] == 0.0f )\n120:             continue;\n121:         if( (fabsf( (float)expected - outData[i] ) / (fabsf( (float)expected ) + ulpLimit)) > 1.0f )\n122:         {\n123:             skipCount++;\n124:             if( skipCount < 20 )\n125:             {\n126:                 log_error( \"ERROR: Verification failed!\\n\" );\n127:             }\n128:         }\n129:     }\n130: \n131:     if( skipCount >= 20 )\n132:     {\n133:         log_error( \"ERROR: Verification failed for 20 or more inputs!\\n\" );\n134:     }\n135: \n136:     return skipCount;\n137: }", "label": "(108, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 1, NULL, threads, localThreads, 0, NULL, NULL );); (22, OpenCL, error = clGetDeviceInfo( device, CL_DEVICE_SINGLE_FP_CONFIG, sizeof( config ), &config, NULL );); (16, OpenCL, error = clGetDeviceInfo( device, CL_DEVICE_PROFILE, sizeof( kernelSource ), kernelSource, NULL );); (95, OpenCL, error = clSetKernelArg(kernel, (int)i, sizeof( streams[i] ), &streams[i]);); (108, OpenCL, error = clEnqueueReadBuffer( queue, streams[2], true, 0, sizeof( cl_float ) * TEST_SIZE, outData, 0, NULL, NULL );)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(11, OpenMP, #pragma omp parallel for schedule(static,1)); (12, OpenMP, #pragma omp task)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(8, CUDA, safeCall(cudaMemcpy(h_data, data.d_data, sizeof(SiftPoint)*data.numPts, cudaMemcpyDeviceToHost));)"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for)", "label": "(12, TBB, tbb::mutex *m = new tbb::mutex;)"}
{"predict": "(12,OpenMP,#pragma omp parallel for collapse(5) schedule(static)), (23,OpenMP,#pragma omp parallel for collapse(5) schedule(static) reduction(+: h))", "label": "(7, OpenMP, #pragma omp parallel for schedule(dynamic) default(none) copyin(g) linear(a) allocate(a) lastprivate(conditional: d, e,f) order(concurrent)); (5, OpenMP, #pragma omp threadprivate(g)); (11, OpenMP, #pragma omp parallel for allocate(argc) private(argc, b), firstprivate(c, d), lastprivate(d, f) collapse(N) schedule(static, N) ordered(N) if (parallel :argc) num_threads(N) default(shared) shared(e) reduction(+ : h))"}
{"predict": "(12,OpenMP,#pragma omp parallel sections)", "label": "(19, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(10,OpenMP,#pragma omp parallel sections), (11,OpenMP,#pragma omp section), (12,OpenMP,#pragma omp section), (13,OpenMP,#pragma omp section), (14,OpenMP,#pragma omp section), (15,OpenMP,#pragma omp section), (16,OpenMP,#pragma omp section), (17,OpenMP,#pragma omp section), (18,OpenMP,#pragma omp section), (19,OpenMP,#pragma omp section), (25,OpenMP,#pragma omp parallel sections), (26,OpenMP,#pragma omp section), (27,OpenMP,#pragma omp section)", "label": "(31, TBB, ASSERT(default_ss == tbb::global_control::active_value(tbb::global_control::thread_stack_size), NULL);); (3, TBB, const unsigned h_c = tbb::tbb_thread::hardware_concurrency();); (5, TBB, tbb::task_scheduler_init t(h_c+1);); (20, TBB, size_t default_ss = tbb::global_control::active_value(tbb::global_control::thread_stack_size);)"}
{"predict": "Buffer failed\");\n155:             return -1;\n156:         }\n157:         found_size = 1;\n158:     }\n159: \n160:     if (!found_size)\n161:     {\n162:         log_info(\"Could not allocate any memory on the device.\\n\");\n163:         return -1;\n164:     }\n165: \n166:     // Setup the kernel arguments\n167:     err = clSetKernelArg(kernel, 0, sizeof(cl_mem), &array);\n168:     test_error(err, \"Unable to set kernel argument 0\");\n169: \n170:     if (dimensions > 1)\n171:     {\n172:         err = clSetKernelArg(kernel, 1, sizeof(cl_uint), &max_x_size);\n173:         test_error(err, \"Unable to set kernel argument 1\");\n174:     }\n175: \n176:     if (dimensions > 2)\n177:     {\n178:         err = clSetKernelArg(kernel, 2, sizeof(cl_uint), &max_y_size);\n179:         test_error(err, \"Unable to set kernel argument 2\");\n180:     }\n181: \n182:     if (dimensions > 3)\n183:     {\n184:         err = clSetKernelArg(kernel, 3, sizeof(cl_uint), &max_z_size);\n185:         test_error(err, \"Unable to set kernel argument 3\");\n186:     }\n187: \n188:     if (explicit_local)\n189:     {\n190:         err = clSetKernelArg(kernel, 4, sizeof(cl_uint), &max_x_size);\n191:         test_error(err, \"Unable to set kernel argument 4\");\n192:     }\n193: \n194:     // Set the work sizes\n195:     size_t global_work_size[3] = {1, 1, 1};\n196:     size_t local_work_size[3] = {1, 1, 1};\n197:     int quick_test_size = 0;\n198:", "label": "(558, OpenCL, clReleaseKernel(kernel);); (558, OpenCL, clReleaseProgram(program);); (555, OpenCL, clReleaseMemObject(array);)"}
{"predict": "(35,OpenMP,#pragma omp parallel for)", "label": "(34, CUDA, HANDLE_ERROR(cudaMalloc((void**)&state_deriv_GPU, sizeof(float) * DYN::STATE_DIM));); (37, CUDA, HANDLE_ERROR(cudaMemcpyAsync(s_d, s_gpu.data(), sizeof(float) * DYN::STATE_DIM, cudaMemcpyHostToDevice, s1));); (46, CUDA, CudaCheckError();); (34, CUDA, HANDLE_ERROR(cudaMemcpyAsync(u_d, u.data(), sizeof(float) * DYN::CONTROL_DIM, cudaMemcpyHostToDevice, s1));); (29, CUDA, HANDLE_ERROR(cudaMalloc((void**)&s_d, sizeof(float) * DYN::STATE_DIM));); (27, CUDA, HANDLE_ERROR(cudaMalloc((void**)&u_d, sizeof(float) * DYN::CONTROL_DIM));); (45, CUDA, HANDLE_ERROR(cudaMemcpyAsync(s_gpu.data(), s_d, sizeof(float) * DYN::STATE_DIM, cudaMemcpyDeviceToHost, s1));); (32, CUDA, HANDLE_ERROR(cudaStreamSynchronize(s1));); (19, CUDA, cudaStreamCreate(&s1);)"}
{"predict": "2:           batch_size, n_spheres,\n143:           n_links, n_joints, store_n_links);\n144:       }));\n145:     }\n146:   }\n147:   else\n148:   {\n149:     if (n_joints < 16)\n150:     {\n151:       AT_DISPATCH_FLOATING_TYPES(\n152:         grad_spheres.scalar_type(), \"kin_fused_backward_16t\", ([&] {\n153:         kin_fused_backward_kernel3<scalar_t, double, false, true, 16, parallel_write>\n154:           << < blocksPerGrid, threadsPerBlock, sharedMemSize, stream >> > (\n155:           grad_out.data_ptr<float>(),\n156:           grad_nlinks_pos.data_ptr<float>(),\n157:           grad_nlinks_quat.data_ptr<float>(),\n158:           grad_spheres.data_ptr<scalar_t>(),\n159:           global_cumul_mat.data_ptr<float>(),\n160:           joint_vec.data_ptr<float>(),\n161:           fixed_transform.data_ptr<float>(),\n162:           robot_spheres.data_ptr<float>(),\n163:           joint_map_type.data_ptr<int8_t>(),\n164:           joint_map.data_ptr<int16_t>(), link_map.data_ptr<int16_t>(),\n165:           store_link_map.data_ptr<int16_t>(),\n166:           link_sphere_map.data_ptr<int16_t>(),\n167:           link_chain_map.data_ptr<int16_t>(),\n168:           joint_offset_map.data_ptr<float>(),\n169:           batch_size, n_spheres,\n170:           n_links, n_joints, store_n_links);\n171:       }));\n172:     }\n173:     else if (n_joints < 64)\n174:     {\n175:       AT_DISPATCH_FLOATING_TYPES(\n176:         grad_spheres.scalar_type(), \"kin_fused_backward_16t\", ([&] {\n177:         kin_fused_backward_kernel3<scalar_t, double, false, true, 64, parallel_write>\n178:           << < blocksPerGrid, threadsPerBlock, sharedMemSize, stream >> > (\n179:", "label": "(69, CUDA, cudaStream_t stream = at::cuda::getCurrentCUDAStream();)"}
{"predict": "(12,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for reduction(max:s))", "label": "(24, MPI, MPI_Allreduce( &s, &berr, 1, MPI_FLOAT, MPI_MAX, grid->comm );)"}
{"predict": "(42,OpenMP,#pragma omp parallel for), (79,OpenMP,#pragma omp parallel for)", "label": "(34, CUDA, checkCudaErrors(cudaDeviceSynchronize());)"}
{"predict": "(51,OpenMP,#pragma omp parallel for),", "label": "(18, CUDA, checkRuntime(cudaStreamCreate(&stream));); (79, CUDA, checkRuntime(cudaMemcpyAsync(output_data_host, output_data_device, sizeof(float) * output_numel, cudaMemcpyDeviceToHost, stream));); (79, CUDA, checkRuntime(cudaStreamSynchronize(stream));); (57, CUDA, checkRuntime(cudaMemcpyAsync(input_data_device, input_data_host, input_numel * sizeof(float), cudaMemcpyHostToDevice, stream));); (24, CUDA, checkRuntime(cudaMallocHost(&input_data_host, input_numel * sizeof(float)));); (24, CUDA, checkRuntime(cudaMalloc(&input_data_device, input_numel * sizeof(float)));); (89, CUDA, checkRuntime(cudaFree(input_data_device));); (86, CUDA, checkRuntime(cudaFreeHost(input_data_host));); (88, CUDA, checkRuntime(cudaFree(output_data_device));); (62, CUDA, checkRuntime(cudaMalloc(&output_data_device, sizeof(float) * output_numel));); (-5, CUDA, auto engine = make_nvshared(runtime->deserializeCudaEngine(engine_data.data(), engine_data.size()));); (59, CUDA, checkRuntime(cudaMallocHost(&output_data_host, sizeof(float) * output_numel));); (82, CUDA, checkRuntime(cudaFreeHost(output_data_host));); (79, CUDA, checkRuntime(cudaStreamDestroy(stream));)"}
{"predict": "4:   for (int i = 0; i < 10; ++i)\n125:     ++r;\n126:   return 0;\n127: }\n\n(26,OpenMP,#pragma omp parallel for), (52,OpenMP,#pragma omp parallel for)\n(60,OpenMP,#pragma omp parallel for), (62,OpenMP,#pragma omp parallel for), (64,OpenMP,#pragma omp parallel for)\n(66,OpenMP,#pragma omp parallel for), (68,OpenMP,#pragma omp parallel for), (70,OpenMP,#pragma omp parallel for)\n(72,OpenMP,#pragma omp parallel for), (74,OpenMP,#pragma omp parallel for), (76,OpenMP,#pragma omp parallel for)\n(78,OpenMP,#pragma omp parallel for), (80,OpenMP,#pragma omp parallel for), (82,OpenMP,#pragma omp parallel for)\n(84,OpenMP,#pragma omp parallel for), (86,OpenMP,#pragma omp parallel for), (88,OpenMP,#pragma omp parallel for)\n(90,OpenMP,#pragma omp parallel for), (92,OpenMP,#pragma omp parallel for), (96,OpenMP,#pragma omp parallel for)\n(98,OpenMP,#pragma omp parallel for), (105,OpenMP,#pragma omp parallel for reduction(+ : r)), (108,OpenMP,#pragma omp parallel for reduction(+ : r))\n(110,OpenMP,#pragma omp parallel for reduction(+ : r)), (112,OpenMP,#pragma omp parallel for reduction(+ : r)), (116,OpenMP,#pragma omp parallel for reduction(+ : r))\n(118,OpenMP,#pragma omp parallel for firstprivate(r)), (120,OpenMP,#pragma omp parallel for reduction(+ : r)), (124,OpenMP,#pragma omp parallel for reduction(+ : r))", "label": "(174, OpenMP, #pragma omp task mergeable detach(evt)); (19, OpenMP, #pragma omp task unknown()); (14, OpenMP, #pragma omp task }); (154, OpenMP, #pragma omp task untied untied); (106, OpenMP, #pragma omp parallel shared(sa, sb)); (125, OpenMP, #pragma omp for reduction(+ : r)); (109, OpenMP, #pragma omp task firstprivate(r)); (160, OpenMP, #pragma omp task detach(); (-2, OpenMP, #pragma omp task); (104, OpenMP, #pragma omp parallel reduction(+ : r)); (160, OpenMP, #pragma omp task detach(evt) detach(evt)); (161, OpenMP, #pragma omp task detach(evt) mergeable); (60, OpenMP, #pragma omp parallel); (162, OpenMP, #pragma omp task detach(-evt)); (-6, OpenMP, #pragma omp task(); (48, OpenMP, #pragma omp task default(shared)); (-6, OpenMP, #pragma omp task[); (152, OpenMP, #pragma omp task detach(a)); (-6, OpenMP, #pragma omp task]); (58, OpenMP, #pragma omp parallel shared(a, b)); (157, OpenMP, #pragma omp task detach(evt) shared(evt)); (-7, OpenMP, #pragma omp task)); (144, OpenMP, #pragma omp task detach); (148, OpenMP, #pragma omp task detach(cevt) detach(revt)); (136, OpenMP, #pragma omp task mergeable mergeable); (153, OpenMP, #pragma omp task detach(evt) firstprivate(evt)); (19, OpenMP, #pragma omp task default(none)); (141, OpenMP, #pragma omp task detach()); (66, OpenMP, #pragma omp parallel shared(sa))"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(8, MPI, MPI_Start(&r);); (11, MPI, MPI_Send_init(buf, 10, MPI_INT, MPI_PROC_NULL, tag, MPI_COMM_WORLD, &r);); (7, MPI, MPI_Wait(&r, &s);); (4, MPI, MPI_Recv_init(buf, 10, MPI_INT, MPI_PROC_NULL, tag, MPI_COMM_WORLD, &r);); (6, MPI, MPI_Request_free(&r);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for collapse(2)),", "label": "(16, CUDA, CUDA_CHECK(cudaGetLastError());)"}
{"predict": "= globalPg->GetProcessGroup();\n141:         self->SetProcessGroupId(0);\n142:         self->SetServerCount(commCount);\n143:         self->SetThreadCount(threadCount);\n144:         self->SetAllToAllSplitParts(allToAllSplitParts);\n145:         self->SetMaxShortMsgSize(maxShortMsgSize);\n146:         self->SetThpThresholdMb(thpThresholdMb);\n147: \n148:         MLSL_LOG(INFO, \"MLSL Init completed successfully\");\n149:     }", "label": "(35, MPI, MPI_Get_library_version(mpiVersion, &resultLen);); (132, MPI, ret = MPI_Init_thread(argc, argv, MPI_THREAD_MULTIPLE, &provided);); (146, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (126, MPI, MPI_Initialized(&isMpiInited);); (0, MPI, MPI_Finalized(&isMpiFinalized);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(14, OpenMP, #pragma omp parallel reduction(+ : incomplete, ([10])p)); (1, OpenMP, #pragma omp parallel reduction(); (2, OpenMP, #pragma omp parallel reduction(unknown); (9, OpenMP, #pragma omp parallel reduction(inscan, + : a)); (6, OpenMP, #pragma omp parallel reduction(default, + : a)); (1, OpenMP, #pragma omp parallel reduction(default,); (2, OpenMP, #pragma omp parallel reduction(unknown, +: a))"}
{"predict": "(29,OpenMP,#pragma omp parallel for)", "label": "(13, OpenMP, int _dev_thread_id = omp_get_thread_num();); (11, OpenMP, int _dev_thread_num = omp_get_num_threads();)"}
{"predict": "(7,OpenMP,#pragma omp parallel if(maxBatchSize > 1))", "label": "(14, CUDA, cudaError_t err = cudaMalloc(&m_histogram, sizeof(int) * 256 * maxBatchSize);)"}
{"predict": "73: // CHECK-NOT: !llvm.access.group\n74:   for(int i = 0; i < 10; ++i);\n75: // CHECK: @__kmpc_dispatch_init\n76: // CHECK: !llvm.access.group\n77:   for(int i = 0; i < 10; ++i);\n78: // CHECK: @__kmpc_dispatch_init\n79: // CHECK-NOT: !llvm.access.group\n80:   for(int i = 0; i < 10; ++i);\n81: // CHECK: @__kmpc_dispatch_init\n82: // CHECK: !llvm.access.group\n83:   for(int i = 0; i < 10; ++i);\n84: // CHECK: @__kmpc_dispatch_init\n85: // CHECK-NOT: !llvm.access.group\n86:   for(int i = 0; i < 10; ++i);\n87: // CHECK: @__kmpc_dispatch_init\n88: // CHECK: !llvm.access.group\n89:   for(int i = 0; i < 10; ++i);\n90: // CHECK: @__kmpc_dispatch_init\n91: // CHECK-NOT: !llvm.access.group\n92:   for(int i = 0; i < 10; ++i);\n93: // CHECK: @__kmpc_dispatch_init\n94: // CHECK: !llvm.access.group\n95:   for(int i = 0; i < 10; ++i);\n96: // CHECK: @__kmpc_dispatch_init\n97: // CHECK-NOT: !llvm.access.group\n98:   for(int i = 0; i < 10; ++i);\n99: // CHECK: @__kmpc_dispatch_init\n100:// CHECK: !llvm.access.group\n101:   for(int i = 0; i < 10; ++i);\n102:   return 0;\n103: }\n\n(5,OpenMP,#pragma omp parallel for), (9,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (21,", "label": "(137, OpenMP, #pragma omp for schedule(nonmonotonic: auto)); (3, OpenMP, #pragma omp for); (47, OpenMP, #pragma omp for schedule(guided)); (106, OpenMP, #pragma omp for schedule(monotonic: dynamic)); (41, OpenMP, #pragma omp for simd schedule(runtime)); (56, OpenMP, #pragma omp for simd schedule(dynamic)); (107, OpenMP, #pragma omp for simd schedule(monotonic: dynamic)); (46, OpenMP, #pragma omp for simd schedule(guided)); (89, OpenMP, #pragma omp for simd schedule(monotonic: runtime)); (10, OpenMP, #pragma omp for simd schedule(static)); (83, OpenMP, #pragma omp for schedule(monotonic: runtime)); (90, OpenMP, #pragma omp for schedule(monotonic: guided)); (144, OpenMP, #pragma omp for schedule(static, 2) ordered(1)); (28, OpenMP, #pragma omp for schedule(runtime)); (62, OpenMP, #pragma omp for schedule(monotonic: static, 2)); (-1, OpenMP, #pragma omp for schedule(static)); (21, OpenMP, #pragma omp for simd schedule(auto)); (144, OpenMP, #pragma omp for simd schedule(static, 2) ordered); (152, OpenMP, #pragma omp for simd schedule(auto) ordered); (66, OpenMP, #pragma omp for schedule(monotonic: auto)); (105, OpenMP, #pragma omp for schedule(nonmonotonic: dynamic)); (145, OpenMP, #pragma omp for schedule(auto) ordered(1)); (67, OpenMP, #pragma omp for simd schedule(monotonic: auto)); (110, OpenMP, #pragma omp for simd schedule(nonmonotonic: static)); (57, OpenMP, #pragma omp for simd schedule(monotonic: static, 2)); (46, OpenMP, #pragma omp for simd schedule(monotonic: static)); (-2, OpenMP, #pragma omp for schedule(static, 2)); (39, OpenMP, #pragma omp for schedule(monotonic: static)); (113, OpenMP, #pragma omp for simd schedule(nonmonotonic: runtime)); (28, OpenMP, #pragma omp for schedule(dynamic)); (87, OpenMP, #pragma omp for schedule(nonmonotonic: guided)); (149, OpenMP, #pragma omp for simd schedule(runtime) ordered); (97, OpenMP, #pragma omp for simd schedule(nonmonotonic: dynamic)); (157, OpenMP, #pragma omp for simd schedule(guided) ordered); (-25, OpenMP, #pragma omp for simd); (111, OpenMP, #pragma omp for schedule(static) ordered); (69, OpenMP, #pragma omp for simd schedule(monotonic: guided)); (-4, OpenMP, #pragma omp for schedule(auto)); (83, OpenMP, #pragma omp for simd schedule(nonmonotonic: guided)); (156, OpenMP, #pragma omp for schedule(dynamic) ordered(1)); (111, OpenMP, #pragma omp for simd schedule(static) ordered); (144, OpenMP, #pragma omp for schedule(guided) ordered(1)); (-13, OpenMP, #pragma omp for simd schedule(static, 2)); (132, OpenMP, #pragma omp for schedule(runtime) ordered)"}
{"predict": "(33,OpenMP,#pragma omp parallel for), (38,OpenMP,#pragma omp parallel for), (45,OpenMP,#pragma omp parallel for)", "label": "(51, OpenCL, clReleaseEvent(event);)"}
{"predict": "93: \t  tx2 * ( (c1*u[i+1][j][k][4] - \n94: \t\t   c2*square[i+1][j][k])*up1 - \n95: \t\t (c1*u[i-1][j][k][4] - \n96: \t\t   c2*square[i-1][j][k])*um1);\n97:       }\n98:     }\n99:   }\n100: \n101: /*--------------------------------------------------------------------\n102: c     add fourth order xi-direction dissipation               \n103: c-------------------------------------------------------------------*/\n104:   for (i = 1; i < grid_points[0]-2; i++) {\n105:     for (j = 1; j < grid_points[1]-1; j++) {\n106:       for (k = 1; k < grid_points[2]-1; k++) {\n107: \tfor (m = 0; m < 5; m++) {\n108: \t  rhs[i][j][k][m] = rhs[i][j][k][m] - dssp * \n109: \t    ( 5.0*u[i][j][k][m] - 4.0*u[i+1][j][k][m] +\n110: \t      u[i+2][j][k][m] - 4.0*u[i-1][j][k][m] +\n111: \t      u[i-2][j][k][m] );\n112: \t}\n113:       }\n114:     }\n115:   }\n116: \n117: /*--------------------------------------------------------------------\n118: c     compute eta-direction fluxes \n119: c-------------------------------------------------------------------*/\n120:   for (i = 1; i < grid_points[0]-1; i++) {\n121:     for (j = 1; j < grid_points[1]-1; j++) {\n122:       for (k = 1; k < grid_points[2]-1; k++) {\n123: \tvijk = vs[i][j][k];\n124: \tvp1  = vs[i][j+1][k];\n125: \tvm1", "label": "(171, OpenMP, #pragma omp for private(j,k,vijk,vp1,vm1)); (397, OpenMP, #pragma omp for private(j,m)); (7, OpenMP, #pragma omp for nowait private(j,k,rho_inv)); (127, OpenMP, #pragma omp for nowait private(j,k,m)); (103, OpenMP, #pragma omp for nowait private(k,m)); (343, OpenMP, #pragma omp for nowait private(j,m)); (151, OpenMP, #pragma omp for private(k,m)); (25, OpenMP, #pragma omp for private(j,k,m)); (38, OpenMP, #pragma omp for private(j,k,uijk,up1,um1)); (282, OpenMP, #pragma omp for private(j,k,wijk,wp1,wm1)); (399, OpenMP, #pragma omp for private(k,m,i))"}
{"predict": "(11,MPI,MPI_Init(&argc, &argv);), (12,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);), (12,MPI,MPI_Comm_size(MPI_COMM_WORLD, &size);), (17,OpenMP,#pragma omp parallel), (23,OpenMP,#pragma omp parallel), (29,OpenMP,#pragma omp parallel), (36,OpenMP,#pragma omp parallel), (38,OpenMP,#pragma omp parallel), (44,OpenMP,#pragma omp parallel), (46,OpenMP,#pragma omp parallel), (52,OpenMP,#pragma omp parallel), (55,OpenMP,#pragma omp parallel), (62,OpenMP,#pragma omp parallel), (64,OpenMP,#pragma omp parallel)", "label": "(28, MPI, MPI_Type_free(&ddt);); (7, MPI, MPI_Init(&argc, &argv);); (12, MPI, MPI_Finalize();); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (21, MPI, MPI_DDT_DUMP(ddt);)"}
{"predict": "(37,OpenMP,#pragma omp parallel for), (51,OpenMP,#pragma omp parallel for)", "label": "(57, MPI, MPI_Recv(outbufs[j], counts[j], types[j], partner, tag, comm, &status);); (45, MPI, MPI_Type_get_name(types[j], myname, &mynamelen);); (49, MPI, MPI_Send(inbufs[j], counts[j], types[j], partner, tag, comm);); (35, MPI, MPI_Comm_size(comm, &np);); (25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (32, MPI, MPI_Comm_rank(comm, &rank);); (53, MPI, MPI_Get_count(&status, types[j], &count);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(16, MPI, MPI_Init(&argc, &argv);); (20, MPI, MPI_Finalize();)"}
{"predict": ", DMAPP_BYTE);\n128:             t1 = MPI_Wtime();\n129:             assert(status==DMAPP_RC_SUCCESS);\n130:             dt = t1-t0;\n131:             bw = 1e-6 * (double)i / dt;\n132:             fprintf(stderr,\"%d: %12d bytes %12lf seconds = %lf MB/s \\n\", mype, i, dt, bw);\n133:         }\n134: \n135:     }\n136:     fflush(stderr);\n137:     PMI_Barrier();\n138: \n139:     dmapp_sheap_free(source);\n140:     dmapp_sheap_free(target);\n141:     dmapp_finalize();\n142:     return 0;\n143: }\n144: #endif", "label": "(14, MPI, MPI_Init(&argc, &argv);); (148, MPI, MPI_Finalize();); (72, MPI, t0 = MPI_Wtime();); (73, MPI, t1 = MPI_Wtime();)"}
{"predict": "(16,OpenMP,#pragma omp parallel for)", "label": "(11, MPI, MPI_Abort(MPI_COMM_WORLD, MPIT_result);); (12, MPI, MPIT_result = MPI_T_pvar_session_create(session);); (6, MPI, MPIT_result = MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);)"}
{"predict": "(20,OpenMP,#pragma omp parallel for private(local_sum, acc))", "label": "(6, CUDA, __shared__ float block_acc[32];)"}
{"predict": "(23,OpenMP,#pragma omp parallel for private(Stats) schedule(static,4))", "label": "(3, OpenMP, auto maxThreads = omp_get_max_threads();)"}
{"predict": "(2,OpenMP,#include <omp.h>); (10,OpenMP,#pragma omp parallel if (rank == 3))", "label": "(6, MPI, MPI_Init(&argc, &argv);); (19, MPI, MPI_Finalize();); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (7, MPI, MPI_Comm_get_parent(&intercomm);); (11, MPI, err = MPI_Recv(str, 4, MPI_CHAR, 3, 0, intercomm, MPI_STATUS_IGNORE);); (8, MPI, err = MPI_Send(\"hi\", 3, MPI_CHAR, 3, 0, intercomm);)"}
{"predict": "(27,OpenMP,#pragma omp parallel for collapse(2))", "label": "(31, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (29, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());)"}
{"predict": "(4,OpenMP,#pragma omp parallel for)", "label": "(5, OpenMP, #pragma omp task final (argc); (3, OpenMP, #pragma omp task final ()); (8, OpenMP, #pragma omp task final (argv[1]=2)); (4, OpenMP, #pragma omp task final (argc > 0 ? argv[1] : argv[2])); (8, OpenMP, #pragma omp task final (1 0)); (1, OpenMP, #pragma omp task final (z+argc))); (2, OpenMP, #pragma omp task final (foobool(argc)), final (true)); (6, OpenMP, #pragma omp task final(if(tmain(argc, argv)); (-5, OpenMP, #pragma omp task final (); (2, OpenMP, #pragma omp task final (argc argc)); (-8, OpenMP, #pragma omp task final); (-2, OpenMP, #pragma omp task final (S1))"}
{"predict": "(22,OpenMP,#pragma omp parallel for private(index, nzval, nb, nsupr, gb, nsupc, c, k, r, len, j, i, nmax) reduction(+:nnzL)), (64,OpenMP,#pragma omp parallel for private(index, nzval, nb, nsupr, gb, nsupc, c, k, r, len, j, i))", "label": "(49, MPI, MPI_Allreduce(MPI_IN_PLACE,&n,1,mpi_int_t,MPI_MAX,grid->comm);); (47, MPI, MPI_Allreduce(MPI_IN_PLACE,&nnzL,1,mpi_int_t,MPI_SUM,grid->comm);)"}
{"predict": "] : vrefSelects[stype][1];\n113: \n114:         // Call the reference function\n115:         sfunc(ref.data(), src1_host.data(), src2_host.data(), cmp_host.data(),\n116:               block_elements);\n117: \n118:         // Run the kernel\n119:         size_t gws[1] = { block_elements };\n120:         err = clEnqueueNDRangeKernel(queue, kernels[vecsize], 1, NULL, gws, NULL,\n121:                                      0, NULL, NULL);\n122:         test_error_count(err, \"Error: Could not run kernel\");\n123: \n124:         err = clEnqueueReadBuffer(queue, dest, CL_TRUE, 0, BUFFER_SIZE,\n125:                                   dest_host.data(), 0, NULL, NULL);\n126:         test_error_count(err, \"Error: Could not read dest\");\n127: \n128:         // Verify the results\n129:         if (!verify(dest_host.data(), ref.data(), block_elements))\n130:         {\n131:             log_error(\"ERROR: Failed test at index %lu!\\n\", i);\n132:             return -1;\n133:         }\n134:     }\n135:     return 0;\n136: }", "label": "(51, OpenCL, dest = clCreateBuffer( context, CL_MEM_WRITE_ONLY, BUFFER_SIZE, NULL, &err );); (44, OpenCL, src1 = clCreateBuffer( context, CL_MEM_READ_ONLY, BUFFER_SIZE, NULL, &err );); (72, OpenCL, err = clSetKernelArg(kernels[vecsize], 0, sizeof dest, &dest);); (44, OpenCL, src2 = clCreateBuffer( context, CL_MEM_READ_ONLY, BUFFER_SIZE, NULL, &err );); (76, OpenCL, err = clSetKernelArg(kernels[vecsize], 3, sizeof cmp, &cmp);); (71, OpenCL, err = clSetKernelArg(kernels[vecsize], 1, sizeof src1, &src1);); (134, OpenCL, err = clEnqueueNDRangeKernel(queue, kernels[vecsize], 1, NULL, &vector_count, NULL, 0, NULL, NULL);); (42, OpenCL, cmp = clCreateBuffer( context, CL_MEM_READ_ONLY, BUFFER_SIZE, NULL, &err );); (70, OpenCL, err = clSetKernelArg(kernels[vecsize], 2, sizeof src2, &src2);)"}
{"predict": "(15,OpenMP,#pragma omp parallel for)", "label": "(17, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (21, MPI, MPI_Reduce(&data, &result, 1, eq_type, op, 0, MPI_COMM_WORLD);); (25, MPI, MPI_Op_free(&op);); (10, MPI, MPI_Type_create_struct(2, blklens, displs, types, &eq_type);); (10, MPI, MPI_Type_commit(&eq_type);); (3, MPI, MPI_Op_create(myop, 0, &op);); (10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (21, MPI, MPI_Type_free(&eq_type);)"}
{"predict": "(8,OpenMP,#pragma omp parallel sections), (9,OpenMP,#pragma omp section), (10,OpenMP,#pragma omp section)", "label": "(7, OpenMP, #pragma omp critical(add_first)); (3, OpenMP, #pragma omp parallel shared(var1, var2)); (7, OpenMP, #pragma omp atomic); (8, OpenMP, #pragma omp single)"}
{"predict": "(6,OpenMP,#pragma omp parallel for)", "label": "(9, TBB, test_ports_return_references<tbb::flow::rejecting>();); (9, TBB, lightweight_testing::test<tbb::flow::multifunction_node>(10);); (13, TBB, test_extract<tbb::flow::rejecting>();); (5, TBB, test_ports_return_references<tbb::flow::queueing>();); (12, TBB, test_extract<tbb::flow::queueing>();)"}
{"predict": "ptr < end && *ptr != '\\n') {\n138:                 ptr++;\n139:             }\n140:             /* advance past newline character */\n141:             if (ptr < end) {\n142:                 ptr++;\n143:             }\n144:             /* we don't need to scan past first newline anymore */\n145:             scan = 0;\n146:         }\n147: \n148:         /* decode records in buffer and add them to list */\n149:         while (ptr < end) {\n150:             /* get pointer to start of next record */\n151:             char* start = ptr;\n152: \n153:             /* find end of record */\n154:             while (ptr < end && *ptr != '\\n') {\n155:                 ptr++;\n156:             }\n157: \n158:             /* compute length of record */\n159:             size_t len = (size_t)(ptr - start);\n160: \n161:             /* if we found a record, add it to the list */\n162:             if (len > 0) {\n163:                 /* allocate a string for the record */\n164:                 char* str = MFU_MALLOC(len + 1);\n165:                 strncpy(str, start, len);\n166:                 str[len] = '\\0';\n167: \n168:                 /* add string to list */\n169:                 flist_ins(flist, str);\n170: \n171:                 /* free the string */\n172:                 MFU_FREE(&str);\n173:             }\n174: \n175:             /* advance to the next record */\n176:             if (ptr < end) {\n177:                 ptr++;\n178:             }\n179:         }\n180: \n181:         /* swap buffers */\n182:         if (buf == buf1) {\n183:             buf = buf2;\n184:         }\n185:         else {\n186:             buf = buf1;\n187:         }\n188: \n189:         /* reset buffer offset */\n1", "label": "(64, MPI, MPI_Error_string(mpirc, mpierrstr, &mpierrlen);); (45, MPI, MPI_Exscan(&chunk_count, &chunk_offset, 1, MPI_UINT64_T, MPI_SUM, MPI_COMM_WORLD);); (88, MPI, mpirc = MPI_File_read_at(fh, pos, buf, 1, MPI_CHAR, &status);); (120, MPI, mpirc = MPI_File_read_at(fh, read_offset, bufstart, read_count, MPI_CHAR, &status);); (58, MPI, int mpirc = MPI_File_set_view(fh, disp, MPI_CHAR, MPI_CHAR, datarep, MPI_INFO_NULL);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (19, MPI, MPI_Bcast(&filesize, 1, MPI_UINT64_T, 0, MPI_COMM_WORLD);)"}
{"predict": "(1,OpenMP,#pragma omp target teams distribute parallel for)", "label": "(1, CUDA, __shared__ float s_mem[32];)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(15, MPI, rc = MPI_Type_dup(type, newtype);); (11, MPI, rc = MPI_Type_get_extent(type, &tmp_lb, &attr->child_type_extent);); (19, MPI, rc = MPI_Type_free(&type);)"}
{"predict": "(60,OpenMP,#pragma omp parallel for reduction(+:ret_sum)), (73,OpenMP,#pragma omp parallel for), (81,OpenMP,#pragma omp parallel for), (83,OpenMP,#pragma omp parallel for)", "label": "(60, OpenMP, #pragma omp parallel for reduction(+ : ret_sum))"}
{"predict": "(12,OpenMP,#pragma omp parallel for),", "label": "(3, TBB, tbb::task* dummy_root = new( tbb::task::allocate_root() ) tbb::empty_task;); (22, TBB, tbb::task::destroy( *dummy_root );); (13, TBB, tbb::task::spawn_root_and_wait(t);); (16, TBB, tbb::task::enqueue(t);)"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for)", "label": "(28, OpenMP, #pragma omp target update from(z[:10])); (17, OpenMP, #pragma omp target update to(z[:10])); (18, OpenMP, #pragma omp target map(to:k[:5]) map(always, tofrom:x) map(tofrom:y))"}
{"predict": "(7,OpenMP,#pragma omp parallel for)\n(12,OpenMP,#pragma omp parallel for)\n(23,OpenMP,#pragma omp parallel for)", "label": "(28, OpenMP, #pragma omp target update from(z[:10])); (17, OpenMP, #pragma omp target update to(z[:10])); (18, OpenMP, #pragma omp target map(to:k[:5]) map(always, tofrom:x) map(tofrom:y))"}
{"predict": "The provided code is primarily focused on object creation based on the input parameters and does not contain any loops or computationally intensive sections that would benefit from parallelization. However, if we assume that the methods within the `Kernel` classes (e.g., `Kernel_3d_GDZ`, `Kernel_3d_DGZ`, etc.) contain loops or other sections that can be parallelized, we would need to look into those methods for potential parallelization.\n\nSince the current code does not have any obvious parallelizable sections, I will provide an example of how you might parallelize a loop within a hypothetical method of one of these classes. Let's assume that the `Kernel_3d_GDZ` class has a method `compute` that contains a loop:\n\n### ** Hypothetical Code in Kernel_3d_GDZ Class **\n\n```cpp\nclass Kernel_3d_GDZ : public Kernel {\npublic:\n    void compute(float* data, int size) {\n        for (int i = 0; i < size; ++i) {\n            data[i] = some_computation(data[i]);\n        }\n    }\n};\n```\n\n### ** Parallelization Instructions for Hypothetical Code **\n\n(4,OpenMP,#pragma omp parallel for)\n\n### ** Explanation **\n\n- **Line Number**: The line number 4 is where the loop starts in the hypothetical `compute` method.\n- **Parallel Type**: OpenMP is used for shared-memory parallelism, which is suitable for loops within a single process.\n- **Code Line**: `#pragma omp parallel for` is inserted before the loop to parallelize it.\n\nSince the original code does not have any loops or computationally intensive sections, there are no parallelization instructions to be inserted in the provided code snippet. If you have additional code or methods within the `Kernel` classes, please provide them for further analysis.", "label": "(18, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(4,OpenMP,#pragma omp parallel for private(j))", "label": "(4, OpenMP, #pragma omp teams distribute); (2, OpenMP, #pragma omp target)"}
{"predict": "(9,OpenMP,#pragma omp parallel sections), (13,OpenMP,#pragma omp section), (18,OpenMP,#pragma omp section), (26,OpenMP,#pragma omp section), (32,OpenMP,#pragma omp section)", "label": "(41, CUDA, return cudaGetLastError();)"}
{"predict": "(51,OpenMP,#pragma omp parallel for private(i)), (126,OpenMP,#pragma omp parallel for private(i))", "label": "(14, CUDA, extern __shared__ float alpha_buffer_sh[];)"}
{"predict": "(21,OpenMP,#pragma omp task shared(rc, attr, newtype, new_count, desc_list, depth) firstprivate(dtp, type, count))", "label": "(65, MPI, rc = MPI_Type_free(&type);)"}
{"predict": "(1,OpenMP,#pragma omp parallel private(rank, coords, neighbors))", "label": "(37, MPI, MPI_Cart_shift(comm2d, 0, 1, &neighbors[0], &neighbors[1]);); (8, MPI, MPI_Init(&argc, &argv);); (26, MPI, MPI_Finalize();); (38, MPI, MPI_Cart_coords(comm2d, crank, 2, coords);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (28, MPI, MPI_Cart_create(MPI_COMM_WORLD, 2, dims, period, 1, &comm2d);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (42, MPI, MPI_Barrier(comm2d);); (26, MPI, MPI_Comm_rank(comm2d, &crank);); (29, MPI, MPI_Cart_shift(comm2d, 1, 1, &neighbors[2], &neighbors[3]);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for collapse(3))", "label": "(21, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize(), \"KaKjw_to_KKwaj\");); (19, CUDA, qmc_cuda::cuda_check(cudaGetLastError(), \"KaKjw_to_KKwaj\");)"}
{"predict": "(1,OpenMP,#pragma omp parallel for reduction(+:a))", "label": "(12, OpenMP, #pragma omp atomic hint(1) hint(1)); (7, OpenMP, #pragma omp atomic hint(a); (4, OpenMP, #pragma omp atomic hint(+); (1, OpenMP, #pragma omp atomic hint(); (6, OpenMP, #pragma omp atomic hint(a)); (-3, OpenMP, #pragma omp atomic hint)"}
{"predict": "imer timer;\n164:         timer.start();\n165:         HANDLE_ERROR(cutensorPermutation(handle,\n166:                                           &alpha,\n167:                                           A_d,\n168:                                           descA,\n169:                                           nullptr,\n170:                                           C_d,\n171:                                           descC,\n172:                                           nullptr,\n173:                                           &beta,\n174:                                           plan));\n175:         timer.stop();\n176:         minTimeCUTENSOR = std::min(minTimeCUTENSOR, timer.elapsedTime());\n177:     }\n178: \n179:     printf(\"CUTENSOR elapsed time: %.2f ms\\n\", minTimeCUTENSOR);\n180: \n181:     /*******************\n182:      * Free resources\n183:      *******************/\n184: \n185:     HANDLE_ERROR(cutensorDestroyPlan(plan));\n186:     HANDLE_ERROR(cutensorDestroyPlanPreference(planPref));\n187:     HANDLE_ERROR(cutensorDestroy(handle));\n188:     HANDLE_ERROR(cutensorDestroyTensorDescriptor(descA));\n189:     HANDLE_ERROR(cutensorDestroyTensorDescriptor(descC));\n190:     HANDLE_ERROR(cutensorDestroyOperationDescriptor(desc));\n191: \n192:     return 0;\n193: }", "label": "(63, CUDA, HANDLE_CUDA_ERROR(cudaMallocHost((void**) &C, sizeof(floatTypeC) * elementsC));); (78, CUDA, HANDLE_CUDA_ERROR(cudaMemcpy2DAsync(A_d, sizeA, A, sizeA, sizeA, 1, cudaMemcpyDefault, nullptr));); (192, CUDA, if (C_d) cudaFree(C_d);); (188, CUDA, if (A) cudaFreeHost(A);); (51, CUDA, HANDLE_CUDA_ERROR(cudaMalloc((void**) &C_d, sizeC));); (49, CUDA, HANDLE_CUDA_ERROR(cudaMalloc((void**) &A_d, sizeA));); (187, CUDA, if (A_d) cudaFree(A_d);); (185, CUDA, if (C) cudaFreeHost(C);); (54, CUDA, HANDLE_CUDA_ERROR(cudaMallocHost((void**) &A, sizeof(floatTypeA) * elementsA));)"}
{"predict": "(7,OpenMP,#pragma omp parallel for private(p2, pnew, newxMin, newxMax, newyMin, newyMax, r2, g2, b2, myr, myg, myb) reduction(-:p1Area))", "label": "(18, TBB, tbb::spin_mutex::scoped_lock lock(*rMutex);)"}
{"predict": "(18,OpenMP,#pragma omp parallel)", "label": "(21, CUDA, checkCudaErrors(cudaMalloc((void **)&d_LimitsA, MAX_SAMPLE_COUNT * sizeof(uint)));); (19, CUDA, checkCudaErrors(cudaMalloc((void **)&d_RanksB,  MAX_SAMPLE_COUNT * sizeof(uint)));); (17, CUDA, checkCudaErrors(cudaMalloc((void **)&d_RanksA,  MAX_SAMPLE_COUNT * sizeof(uint)));); (19, CUDA, checkCudaErrors(cudaMalloc((void **)&d_LimitsB, MAX_SAMPLE_COUNT * sizeof(uint)));)"}
{"predict": "(67,OpenMP,#pragma omp parallel for)\n(78,OpenMP,#pragma omp parallel for collapse(2))\n(86,OpenMP,#pragma omp parallel for collapse(2))\n(91,OpenMP,#pragma omp parallel for collapse(2))", "label": "(79, OpenACC, #pragma acc parallel loop independent); (70, OpenACC, #pragma acc data pcopy(grid[0:n*n])); (86, OpenACC, #pragma acc parallel loop gang); (99, OpenACC, #pragma acc kernels); (89, OpenACC, #pragma acc loop vector)"}
{"predict": "(9,OpenMP,#pragma omp parallel for reduction(+:errs))", "label": "(6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &world_size);)"}
{"predict": "(2,OpenMP,#pragma omp parallel private(rank, ompi_bound, current_binding, exists))", "label": "(25, MPI, MPI_Finalize();); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Init(NULL, NULL);)"}
{"predict": "find row and column ranks for this process                    */\n118: \n119:   MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);\n120:   MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);\n121:   myrow = my_ID/npcol;\n122:   mycol = my_ID%npcol;\n123: \n124:   /* create row and column communicators                           */\n125: \n126:   MPI_Comm_split(MPI_COMM_WORLD, myrow, mycol, &comm_row);\n127:   MPI_Comm_split(MPI_COMM_WORLD, mycol, myrow, &comm_col);\n128: \n129:   /* set up the local arrays                                       */\n130: \n131:   forder = (double) order;\n132:   nflops = 2.0*forder*forder*forder;\n133: \n134:   mynrows = order/nprow;\n135:   myncols = order/npcol;\n136: \n137:   if (mynrows*nprow != order || myncols*npcol != order) {\n138:     if (my_ID == root)\n139:       printf(\"ERROR: matrix order must be divisible by nprow and npcol\\n\");\n140:     error = 1;\n141:   }\n142:   bail_out(error);\n143: \n144:   /* calculate the first and last row and column indices           */\n145: \n146:   myfrow = myrow*mynrows;\n147:   mylrow = myfrow + mynrows - 1;\n148:   myfcol = mycol*myncols;\n149:   mylcol = myfcol + myncols - 1;\n150: \n151:   /* set up local a, b, and c arrays                               */\n152: \n153:   lda = mynrows;\n154:   ldb = myncols;\n155:   ldc = mynrows;\n156: \n157:   a = (double *) malloc(mynrows*myncols*sizeof(double));\n158:   b = (double *)", "label": "(124, MPI, MPI_Comm_group( MPI_COMM_WORLD, &world_group );); (143, MPI, MPI_Comm_rank( comm_col, &myrow );); (205, MPI, MPI_Barrier(MPI_COMM_WORLD);); (83, MPI, MPI_Bcast(&inner_block_flag, 1, MPI_INT, root, MPI_COMM_WORLD);); (135, MPI, MPI_Group_incl( world_group, nprow, ranks, &temp_group );); (39, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &my_ID );); (78, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (124, MPI, MPI_Group_incl( world_group, npcol, ranks, &temp_group );); (35, MPI, MPI_Init(&argc,&argv);); (131, MPI, MPI_Comm_create( MPI_COMM_WORLD, temp_group, &comm_col );); (35, MPI, MPI_Comm_size( MPI_COMM_WORLD, &Num_procs );); (121, MPI, MPI_Comm_create( MPI_COMM_WORLD, temp_group, &comm_row );); (245, MPI, MPI_Finalize();); (70, MPI, MPI_Bcast(&order,  1, MPI_INT, root, MPI_COMM_WORLD);); (167, MPI, MPI_Allgather( &mynrows, 1, MPI_INT, mm, 1, MPI_INT, comm_col );); (70, MPI, MPI_Bcast(&nb, 1, MPI_INT, root, MPI_COMM_WORLD);); (127, MPI, MPI_Comm_rank( comm_row, &mycol );); (172, MPI, MPI_Allgather( &myncols, 1, MPI_INT, nn, 1, MPI_INT, comm_row );)"}
{"predict": "(3,OpenMP,#pragma omp parallel for), (6,OpenMP,#pragma omp parallel for), (9,OpenMP,#pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp target teams distribute parallel for if (false)); (9, OpenMP, #pragma omp target teams distribute parallel for if (parallel: Arg)); (0, OpenMP, #pragma omp target teams distribute parallel for if (true))"}
{"predict": "(6,OpenMP,#pragma omp parallel for)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(8, MPI, MPI_Reduce_local_c(inbuf, inoutbuf, count, datatype, MPI_SUM);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": ">= 0 */\n153: } /* end of IMB_read_ij */", "label": "(102, MPI, MPI_Barrier(c_info->File_comm);); (108, MPI, MPI_ERRHAND(GEN_File_read(c_info->fh, c_info->r_buffer, Locsize, c_info->etype, &stat));); (121, MPI, MPI_ERRHAND(GEN_File_read_at(c_info->fh, Offset, c_info->r_buffer, Locsize, c_info->etype, &stat));); (133, MPI, MPI_ERRHAND(GEN_File_read_shared(c_info->fh, c_info->r_buffer, Locsize, c_info->etype, &stat));); (92, MPI, *time = MPI_Wtime();)"}
{"predict": "(7,OpenMP,#pragma omp parallel for)", "label": "(6, CUDA, thrust::device_ptr<thrust::complex<double>> d_ptr = thrust::device_malloc<thrust::complex<double>>(1);); (14, CUDA, thrust::device_free(d_ptr);); (9, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize());); (7, CUDA, qmc_cuda::cuda_check(cudaGetLastError());)"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for), (35,OpenMP,#pragma omp parallel for), (47,OpenMP,#pragma omp parallel for), (59,OpenMP,#pragma omp parallel for reduction(+:v)), (74,OpenMP,#pragma omp parallel for)", "label": "(63, OpenMP, #pragma omp distribute parallel for simd private(h)); (86, OpenMP, #pragma omp distribute parallel for simd private(j)); (82, OpenMP, #pragma omp parallel private(i)); (65, OpenMP, #pragma omp distribute parallel for simd nowait); (9, OpenMP, #pragma omp distribute parallel for simd private(); (1, OpenMP, #pragma omp target); (27, OpenMP, #pragma omp distribute parallel for simd private(argc > 0 ? argv[1] : argv[2])); (1, OpenMP, #pragma omp distribute parallel for simd private); (35, OpenMP, #pragma omp distribute parallel for simd private(S1)); (19, OpenMP, #pragma omp distribute parallel for simd private(argc,); (61, OpenMP, #pragma omp parallel); (12, OpenMP, #pragma omp distribute parallel for simd private(argc); (36, OpenMP, #pragma omp distribute parallel for simd private(a, b)); (25, OpenMP, #pragma omp distribute parallel for simd private(argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (39, OpenMP, #pragma omp distribute parallel for simd private(argv[1])); (43, OpenMP, #pragma omp distribute parallel for simd private(e, g)); (2, OpenMP, #pragma omp distribute parallel for simd private()); (60, OpenMP, #pragma omp distribute parallel for simd private(i)); (65, OpenMP, #pragma omp parallel shared(i)); (-12, OpenMP, #pragma omp teams)"}
{"predict": "The provided code snippet is a function that releases memory objects in an OpenCL context. The function does not contain any loops or computationally intensive tasks that would benefit from parallelization. Therefore, there are no lines in this specific code snippet where parallelization instructions should be inserted.\n\nHowever, if you have a different section of the code that involves loops or other parallelizable constructs, please provide that section, and I can analyze it for parallelization opportunities.", "label": "(5, OpenCL, clReleaseMemObject(objX);); (12, OpenCL, clReleaseMemObject(objScratch);); (7, OpenCL, clReleaseMemObject(objNrm2);)"}
{"predict": "data0[1] + data2[1] - data0[-1] - data2[-1]);\n107:       float dys = 0.25f*(data0[pitch] + data2[pitch] - data0[-pitch] - data2[-pitch]);\n108:       float dxy = 0.25f*(data0[pitch+1] + data2[pitch+1] - data0[-pitch+1] - data2[-pitch+1]);\n109:       float dxys = 0.25f*(data0[pitch-1] + data2[pitch-1] - data0[-pitch-1] - data2[-pitch-1]);\n110:       float dxy2 = 0.25f*(data0[-pitch+1] + data2[-pitch+1] - data0[+pitch+1] - data2[+pitch+1]);\n111:       float dxyx = 0.25f*(data0[-pitch-1] + data2[-pitch-1] - data0[+pitch-1] - data2[+pitch-1]);\n112:       float dxss = 0.25f*(data0[1] + data2[1] - data0[-1] - data2[-1]);\n113:       float dyss = 0.25f*(data0[pitch] + data2[pitch] - data0[-pitch] - data2[-pitch]);\n114:       float dsss = 2.0f*val - data2[0] - data0[0];\n115:       float Hinv[4];\n116:       Hinv[0] = dxx; Hinv[1] = dxy; Hinv[2] = dxs; Hinv[3] = dss;\n117:       float det = Hinv[0]*Hinv[3] - Hinv[1]*Hinv[2];\n118:       if (fabs(det)>1e-6) {\n119:         Hinv[0] /= det; Hinv[1] /= -det; Hinv[2] /= -det; Hinv[3] /= det;\n120:         float b[2];\n121:", "label": "(3, CUDA, __shared__ unsigned short points[2*MEMWID];)"}
{"predict": "(14,OpenMP,#pragma omp parallel for)", "label": "(13, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(34,OpenMP,#pragma omp parallel for)", "label": "(67, MPI, MPI_Waitall(2, request, status);)"}
{"predict": "(17,OpenMP,#pragma omp parallel for private(comm, result, errs, group, wingroup))", "label": "(28, MPI, MPI_Group_free(&wingroup);); (19, MPI, MPI_Win_create(buf, sizeof(int) * 10, sizeof(int), MPI_INFO_NULL, comm, &win);); (28, MPI, MPI_Win_free(&win);); (20, MPI, MPI_Group_compare(group, wingroup, &result);); (25, MPI, MPI_Group_free(&group);); (17, MPI, MPI_Comm_group(comm, &group);); (15, MPI, MPI_Win_get_group(win, &wingroup);)"}
{"predict": "(29,OpenMP,#pragma omp parallel for), (38,OpenMP,#pragma omp parallel sections)", "label": "(14, CUDA, ColorSpinorParam cudaParam(cpuParam, *inv_param, QUDA_CUDA_FIELD_LOCATION);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for)", "label": "(15, OpenMP, #pragma omp target parallel for)"}
{"predict": "(17,OpenMP,#pragma omp parallel for), (29,OpenMP,#pragma omp parallel for), (38,OpenMP,#pragma omp parallel for), (43,OpenMP,#pragma omp parallel for)", "label": "(58, CUDA, thrust::host_vector<mytype> th_data = t_data;); (40, CUDA, thrust::device_vector<mytype> t_data(h_data, h_data+test_dsize);); (49, CUDA, cudaEventElapsedTime(&et, start, stop);); (22, CUDA, cudaStreamCreate(&str);); (27, CUDA, cudaLaunchCooperativeKernel((void *)my_remove_if<mytype>, FIXME);); (8, CUDA, cudaError_t err = cudaGetDeviceProperties(&prop, 0);); (26, CUDA, cudaMemcpy(h_data, d_odata, tsize, cudaMemcpyDeviceToHost);); (49, CUDA, thrust::remove(t_data.begin(), t_data.end(), -1);); (15, CUDA, cudaMemcpy(d_idata, h_data, tsize, cudaMemcpyHostToDevice);); (39, CUDA, cudaEventRecord(stop);); (-1, CUDA, cudaMalloc(&d_idxs, test_dsize*sizeof(unsigned));); (33, CUDA, cudaEventCreate(&start);); (-5, CUDA, cudaMalloc(&d_odata, tsize);); (33, CUDA, cudaEventRecord(start);); (-6, CUDA, cudaMemset(d_odata, 0, tsize);); (-9, CUDA, cudaMalloc(&d_idata, tsize);); (29, CUDA, cudaEventCreate(&stop);); (2, CUDA, cudaOccupancyMaxActiveBlocksPerMultiprocessor(&numBlkPerSM, my_remove_if<mytype>, nTPB, 0);)"}
{"predict": "(24,OpenMP,#pragma omp parallel for reduction(+:nprocs_recv)), (36,OpenMP,#pragma omp parallel for private(j)), (69,OpenMP,#pragma omp parallel for), (86,OpenMP,#pragma omp parallel for reduction(||:*hole)), (98,OpenMP,#pragma omp parallel for reduction(||:*hole))", "label": "(48, MPI, MPI_Type_commit(recv_types + j);)"}
{"predict": "(58,OpenMP,#pragma omp parallel for)", "label": "(25, CUDA, HANDLE_ERROR(cudaStreamSynchronize(stream));); (9, CUDA, HANDLE_ERROR(cudaMalloc((void**)&initial_x_d, sizeof(float) * 2 * DYN_T::STATE_DIM));); (9, CUDA, HANDLE_ERROR(cudaMalloc((void**)&cost_trajectories_d, sizeof(float) * 2 * num_rollouts));)"}
{"predict": "(12,OpenMP,#pragma omp parallel for private(offset)), (34,OpenMP,#pragma omp parallel for private(offset))", "label": "(6, CUDA, extern __shared__ float s_buffer[];)"}
{"predict": "(10,OpenMP,#pragma omp parallel for reduction(+:nr_complete))", "label": "(19, MPI, MPI_Grequest_complete(aio_reqlist[i]->req);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for collapse(2)), (20,OpenMP,#pragma omp parallel for collapse(2)), (60,OpenMP,#pragma omp parallel for collapse(3))", "label": "(67, CUDA, cudaEventSynchronize(stop);); (39, CUDA, cudaMalloc((void **) &d_a, sizeof(int)*m*n);); (114, CUDA, cudaFreeHost(h_cc);); (34, CUDA, cudaEventRecord(start, 0);); (60, CUDA, cudaThreadSynchronize();); (36, CUDA, cudaMalloc((void **) &d_b, sizeof(int)*n*k);); (39, CUDA, cudaMemcpy(d_a, h_a, sizeof(int)*m*n, cudaMemcpyHostToDevice);); (103, CUDA, cudaFree(d_a);); (62, CUDA, cudaEventElapsedTime(&gpu_elapsed_time_ms, start, stop);); (3, CUDA, cudaMallocHost((void **) &h_c, sizeof(int)*m*k);); (3, CUDA, cudaMallocHost((void **) &h_cc, sizeof(int)*m*k);); (35, CUDA, cudaMemcpy(d_b, h_b, sizeof(int)*n*k, cudaMemcpyHostToDevice);); (-1, CUDA, cudaMallocHost((void **) &h_b, sizeof(int)*n*k);); (99, CUDA, cudaFree(d_c);); (97, CUDA, cudaFree(d_b);); (66, CUDA, cudaEventElapsedTime(&cpu_elapsed_time_ms, start, stop);); (26, CUDA, cudaMalloc((void **) &d_c, sizeof(int)*m*k);); (49, CUDA, cudaEventRecord(stop, 0);); (45, CUDA, cudaMemcpy(h_c, d_c, sizeof(int)*m*k, cudaMemcpyDeviceToHost);); (95, CUDA, cudaFreeHost(h_b);); (93, CUDA, cudaFreeHost(h_a);); (12, CUDA, cudaEventCreate(&start);); (55, OpenMP, omp_mm(h_a, m, n, h_b, n, k, h_cc);); (-13, CUDA, cudaMallocHost((void **) &h_a, sizeof(int)*m*n);); (10, CUDA, cudaEventCreate(&stop);); (90, CUDA, cudaFreeHost(h_c);)"}
{"predict": "(19,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for)", "label": "(37, CUDA, cudaMalloc((void **)&result_data, tensor1->size * sizeof(float));)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(1, OpenMP, #pragma omp parallel firstprivate(a, b, c, e)); (4, OpenMP, #pragma omp parallel firstprivate(a, b, c))"}
{"predict": "(12,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for)", "label": "(29, CUDA, CUDA_CHECK(cudaMemcpy(P_host, P_device, size, cudaMemcpyDeviceToHost));); (35, CUDA, cudaFree(P_device);); (10, CUDA, CUDA_CHECK(cudaMemcpy(M_device, M_host, size, cudaMemcpyHostToDevice));); (34, CUDA, cudaFree(N_device);); (34, CUDA, cudaFree(M_device);); (25, CUDA, CUDA_CHECK(cudaDeviceSynchronize());); (7, CUDA, CUDA_CHECK(cudaMemcpy(N_device, N_host, size, cudaMemcpyHostToDevice));); (2, CUDA, CUDA_CHECK(cudaMalloc((void**)&N_device, size));); (9, CUDA, CUDA_CHECK(cudaMalloc((void**)&P_device, size));;); (-1, CUDA, CUDA_CHECK(cudaMalloc((void**)&M_device, size));)"}
{"predict": "(16,OpenMP,#pragma omp parallel for),", "label": "(11, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)"}
{"predict": "(6,OpenMP,#pragma omp parallel for)", "label": "(17, OpenMP, #pragma omp task default(firstprivate)); (4, OpenMP, #pragma omp task default(shared), default(shared)); (0, OpenMP, #pragma omp task default(); (0, OpenMP, #pragma omp task default()); (5, OpenMP, #pragma omp task default(none)); (1, OpenMP, #pragma omp task default(x)); (7, OpenMP, #pragma omp task default(shared)); (-6, OpenMP, #pragma omp task default); (-4, OpenMP, #pragma omp task default(none)"}
{"predict": "(5,OpenMP,#pragma omp parallel for collapse(3))", "label": "(4, TBB, const tbb::blocked_range3d<int> r( 0, i, 5, 0, j, 3, 0, k, 1 );); (4, TBB, tbb::parallel_for( r, Striker() );)"}
{"predict": "4:                   //     printf(\"lk=%d, i=%d, l=%d, icol=%d, lusup=%f, x=%f, temp1=%f\\n\",lk,i,l,icol,lusup[idx_s*LDA+l*LDA+i],x[icol+ii],temp1);\n125:               }\n126:               s_lsum[i]=temp1;\n127:           }\n128:           __syncthreads();\n129: \n130:           if(lne==0){\n131:               for (i = 0; i < LDA; i++){\n132:                   lsum[lk*LDA+i] += s_lsum[i];\n133:               }\n134:           }\n135:           __syncthreads();\n136:         }\n137:     }\n138: }", "label": "(31, CUDA, volatile __shared__ int s_bmod;); (28, CUDA, __shared__ double s_lsum[MAXSUPER];)"}
{"predict": "(25,OpenMP,#pragma omp parallel for private(index, nzval, nb, nsupr, gb, nsupc, c, k, r, len, j, i, nmax) reduction(+:nnzL)), (67,OpenMP,#pragma omp parallel for private(index, nzval, nb, nsupr, gb, nsupc, c, k, r, len, j, i))", "label": "(52, MPI, MPI_Allreduce(MPI_IN_PLACE,&n,1,mpi_int_t,MPI_MAX,grid->comm);); (50, MPI, MPI_Allreduce(MPI_IN_PLACE,&nnzL,1,mpi_int_t,MPI_SUM,grid->comm);)"}
{"predict": "(27,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp target teams distribute parallel for map(to:A) map(from:result) map(tofrom:d_A) map(to:incx) map(to:cublasH) map(to:stream))", "label": "(34, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (50, CUDA, CUDA_CHECK(cudaDeviceReset());); (47, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (23, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (41, CUDA, CUDA_CHECK(cudaFree(d_A));); (17, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "The provided code does not contain any loops or sections that can be parallelized using MPI or OpenMP directly, as it is primarily focused on error checking and validation of MPI data types. However, if there were loops or computationally intensive sections, I would suggest inserting parallelization instructions there. Since there are none, no parallelization instructions are applicable in this specific code snippet.\n\nIf you have a different section of code or additional loops that need parallelization, please provide that code, and I can assist you further.", "label": "(11, MPI, err = MPI_Type_struct(3, blocks, disps, types, &inttype);); (32, MPI, err = MPI_Type_size(eviltype, &val);); (134, MPI, MPI_Type_free(&inttype);); (75, MPI, err = MPI_Type_get_extent(eviltype, &lb, &extent);); (96, MPI, err = MPI_Type_ub(eviltype, &aval);); (17, MPI, err = MPI_Type_vector(3, 4, -5, inttype, &eviltype);); (131, MPI, MPI_Type_free(&eviltype);); (108, MPI, err = MPI_Type_get_true_extent(eviltype, &true_lb, &aval);); (40, MPI, err = MPI_Type_extent(eviltype, &aval);); (54, MPI, err = MPI_Type_lb(eviltype, &aval);)"}
{"predict": "(70,OpenMP,#pragma omp parallel for reduction(+:val_st_off)) (96,OpenMP,#pragma omp parallel for reduction(+:val_st_on)) (118,OpenMP,#pragma omp parallel for) (122,OpenMP,#pragma omp parallel for)", "label": "(48, OpenMP, lis2mdl_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for collapse(3))", "label": "(4, TBB, const tbb::blocked_range3d<int> r( 0, i, 5, 0, j, 3, 0, k, 1 );); (4, TBB, tbb::parallel_for( r, Striker() );)"}
{"predict": "(23,OpenMP,#pragma omp parallel for collapse(4))", "label": "(36, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for private(sfvar))", "label": "(9, OpenMP, #pragma omp target); (65, OpenMP, #pragma omp distribute parallel for lastprivate(t_var, vec, s_arr, s_arr, var, var, svar)); (9, OpenMP, #pragma omp distribute parallel for lastprivate(g, g1, svar, sfvar)); (7, OpenMP, #pragma omp teams)"}
{"predict": "(20,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for), (36,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for), (44,OpenMP,#pragma omp parallel for)", "label": "(45, CUDA, cudaCheck(cudaGetLastError());)"}
{"predict": "2: \n123:       /* Read magnetometer data */\n124:       ism330dhcx_magnetic_raw_get(&ag_ctx, data_raw.i16bit);\n125:       mag_fielg_mG[0] =\n126:         iis2mdc_from_lsb_to_mgauss(data_raw.i16bit[0]);\n127:       mag_fielg_mG[1] =\n128:         iis2mdc_from_lsb_to_mgauss(data_raw.i16bit[1]);\n129:       mag_fielg_mG[2] =\n130:         iis2mdc_from_lsb_to_mgauss(data_raw.i16bit[2]);\n131: \n132:       /* Print data */\n133:       printf(\"ag: [mg] %.2f %.2f %.2f\\n\", acceleration_mg[0],\n134:              acceleration_mg[1], acceleration_mg[2]);\n135:       printf(\"gy: [mdps] %.2f %.2f %.2f\\n\", angular_rate_mdps[0],\n136:              angular_rate_mdps[1], angular_rate_mdps[2]);\n137:       printf(\"mg: [mG] %.2f %.2f %.2f\\n\", mag_fielg_mG[0],\n138:              mag_fielg_mG[1], mag_fielg_mG[2]);\n139:     }\n140:   }\n141: }", "label": "(66, OpenMP, iis2mdc_offset_temp_comp_set(&mag_ctx, PROPERTY_ENABLE);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(13, CUDA, CP_host->freeCudaMem();)"}
{"predict": "bmod, UBtree_ptr, URtree_ptr, ilsum, Ucolind_bc_dat, Ucolind_bc_offset, Unzval_bc_dat, Unzval_bc_offset, Uinv_bc_dat, Uinv_bc_offset, Uindval_loc_bc_dat, Uindval_loc_bc_offset, xsup, grid);\n109:     //}\n110:     //else {\n111:     if (mype == 0) {\n112:         //printf(\"(%d), U Enter\\n\",mype);\n113:         //fflush(stdout);\n114:         //slsum_bmod_inv_gpu_mrhs<<< nbcol_loc, dimBlock >>>(nbcol_loc, lsum, x, nrhs, nsupers, bmod, UBtree_ptr, URtree_ptr, ilsum, Ucolind_bc_dat, Ucolind_bc_offset, Unzval_bc_dat, Unzval_bc_offset, Uinv_bc_dat, Uinv_bc_offset, Uindval_loc_bc_dat, Uindval_loc_bc_offset, xsup, grid);\n115:         slsum_bmod_inv_gpu_mrhs<<< nbcol_loc, dimBlock, 0, stream[0]>>>(nbcol_loc, lsum, x, nrhs, nsupers, bmod, UBtree_ptr, URtree_ptr, ilsum, Ucolind_bc_dat, Ucolind_bc_offset, Unzval_bc_dat, Unzval_bc_offset, Uinv_bc_dat, Uinv_bc_offset, Uindval_loc_bc_dat, Uindval_loc_bc_offset, xsup, grid);\n116:     }\n117:     else {\n118:         //printf(\"(%d), U Enter\\n\",mype);\n119:         //fflush(stdout);\n120:         //slsum_bmod_inv_gpu_mrhs<<< nbcol_loc, dimBlock >>>(nbcol_loc, lsum, x, nrhs, nsupers, bmod, UBtree_ptr, URtree_ptr, ilsum, Ucolind_bc_dat, Ucolind_bc_offset, Unzval_bc_dat, Unzval_bc_offset, Uinv_bc_dat, Uinv_bc_offset, Uindval_loc_bc_dat, Uindval_loc_bc_offset, xsup, grid);\n121:         slsum_bmod_inv_gpu_mrhs<<< nbcol_loc, dimBlock, 0, stream[", "label": "(117, CUDA, cudaDeviceSetLimit(cudaLimitStackSize, cuattr.localSizeBytes);); (93, CUDA, cudaStreamCreateWithFlags(&stream[i], cudaStreamNonBlocking);); (121, CUDA, cudaOccupancyMaxPotentialBlockSize(&minGridSize, &myblockSize, (const void *) swait_bcrd_u, 0, 0);); (173, CUDA, CUDA_CHECK(cudaGetLastError());); (112, CUDA, cudaFuncGetAttributes(&cuattr, slsum_bmod_inv_gpu_mrhs_nvshmem);); (181, CUDA, CUDA_CHECK(cudaStreamDestroy(stream[i]));)"}
{"predict": "(24,OpenMP,#pragma omp parallel for)", "label": "(101, CUDA, checkHIPErrors( ::cudaDeviceSynchronize() );)"}
{"predict": "(11,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for private(i) reduction(+:total_wait_errors))", "label": "(27, OpenMP, #pragma omp atomic update); (21, OpenMP, #pragma omp loop order(concurrent)); (29, OpenMP, num_threads = omp_get_num_threads();); (17, OpenMP, #pragma omp target parallel num_threads(THREADS) map(tofrom: x[0:N], num_threads, total_wait_errors) map(to: y[0:N], z[0:N]))"}
{"predict": "(2,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel), (13,OpenMP,#pragma omp single nowait)", "label": "(9, OpenMP, #pragma omp target defaultmap(tofrom:); (4, OpenMP, #pragma omp target defaultmap()); (25, OpenMP, #pragma omp target defaultmap(none:pointer)); (10, OpenMP, #pragma omp target defaultmap(tofrom,); (40, OpenMP, #pragma omp parallel); (-4, OpenMP, #pragma omp target defaultmap); (27, OpenMP, #pragma omp target defaultmap(none:pointer) defaultmap(none:scalar) defaultmap(none:aggregate) map(g) firstprivate(argc)); (8, OpenMP, #pragma omp target defaultmap(scalar:); (11, OpenMP, #pragma omp target defaultmap(tofrom: scalar) defaultmap(tofrom: scalar)); (20, OpenMP, #pragma omp target defaultmap(none:pointer) defaultmap(none:scalar) map(argc)); (1, OpenMP, #pragma omp target defaultmap(tofrom)); (6, OpenMP, #pragma omp target defaultmap(tofrom, scalar); (23, OpenMP, #pragma omp target defaultmap(none:aggregate)); (-10, OpenMP, #pragma omp target defaultmap(); (9, OpenMP, #pragma omp target defaultmap(tofrom : scalar) defaultmap(to: scalar)); (16, OpenMP, #pragma omp target defaultmap(none:pointer) defaultmap(none:scalar) map(g)); (9, OpenMP, #pragma omp target defaultmap(alloc: pointer) defaultmap(to: scalar) defaultmap(firstprivate: pointer)); (20, OpenMP, #pragma omp target defaultmap(none:aggregate) defaultmap(none:scalar) map(argc)); (-11, OpenMP, #pragma omp target defaultmap(tofrom)"}
{"predict": "(72,OpenMP,#pragma omp parallel for), (85,OpenMP,#pragma omp parallel for)", "label": "(139, MPI, MPI_Finalize();); (63, MPI, MPI_Bcast(&iterations,    1, MPI_INT, root, MPI_COMM_WORLD);); (63, MPI, MPI_Bcast(&vector_length, 1, MPI_LONG, root, MPI_COMM_WORLD);); (17, MPI, MPI_Init(&argc,&argv);); (81, MPI, MPI_Barrier(MPI_COMM_WORLD);); (17, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (15, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(3,OpenMP,#pragma omp parallel for)\n(6,OpenMP,#pragma omp parallel for)\n(9,OpenMP,#pragma omp parallel for)\n(14,OpenMP,#pragma omp parallel for)\n(18,OpenMP,#pragma omp parallel for)\n(21,OpenMP,#pragma omp parallel for)\n(24,OpenMP,#pragma omp parallel for)\n(27,OpenMP,#pragma omp parallel for)\n(30,OpenMP,#pragma omp parallel for)\n(33,OpenMP,#pragma omp parallel for)\n(36,OpenMP,#pragma omp parallel for)\n(45,OpenMP,#pragma omp parallel for)\n(48,OpenMP,#pragma omp parallel for)", "label": "(25, OpenMP, #pragma omp target teams distribute parallel for simd collapse (1))); (60, OpenMP, #pragma omp target teams distribute parallel for simd collapse (2)); (4, OpenMP, #pragma omp target teams distribute parallel for simd collapse (); (54, OpenMP, #pragma omp target teams distribute parallel for simd collapse (N)); (36, OpenMP, #pragma omp target teams distribute parallel for simd collapse (S)); (24, OpenMP, #pragma omp target teams distribute parallel for simd collapse ((ST > 0) ? 1 + ST : 2)); (-4, OpenMP, #pragma omp target teams distribute parallel for simd collapse); (9, OpenMP, #pragma omp target teams distribute parallel for simd collapse (argc); (28, OpenMP, #pragma omp target teams distribute parallel for simd collapse (foobool(argc)), collapse (true), collapse (-5)); (12, OpenMP, #pragma omp target teams distribute parallel for simd collapse (ST); (43, OpenMP, #pragma omp target teams distribute parallel for simd collapse (1)); (-1, OpenMP, #pragma omp target teams distribute parallel for simd collapse ())"}
{"predict": "}\n124: \n125:     err = clEnqueueCopyImage(queue, memobjs[0], memobjs[2], srcPt, destPt, region, 0, NULL, &copyEvent);\n126:     if (err != CL_SUCCESS){\n127:         print_error( err, \"clCopyImage failed\" );\n128:         clReleaseKernel( kernel[0] );\n129:         clReleaseProgram( program[0] );\n130:         clReleaseMemObject( memobjs[0] );\n131:         clReleaseMemObject( memobjs[1] );\n132:         clReleaseMemObject( memobjs[2] );\n133:         free( dst );\n134:         free( inptr );\n135:         return -1;\n136:     }\n137: \n138:     // wait for the kernel to finish\n139:     err = clWaitForEvents(1, &copyEvent);\n140:     if (err != CL_SUCCESS){\n141:         print_error( err, \"clWaitForEvents failed\" );\n142:         clReleaseKernel( kernel[0] );\n143:         clReleaseProgram( program[0] );\n144:         clReleaseMemObject( memobjs[0] );\n145:         clReleaseMemObject( memobjs[1] );\n146:         clReleaseMemObject( memobjs[2] );\n147:         free( dst );\n148:         free( inptr );\n149:         return -1;\n150:     }\n151: \n152:     // read the output image\n153:     err = clEnqueueReadImage(queue, memobjs[2], CL_TRUE, destPt, region, 0, dst, 0, NULL, NULL);\n154:     if (err != CL_SUCCESS){\n155:         print_error( err, \"clEnqueueReadImage failed\" );\n156:         clReleaseKernel( kernel[0] );\n157:         clReleaseProgram( program[0] );\n158:         clReleaseMemObject( memobjs[0] );\n159:         clReleaseMemObject( memobjs[1] );\n160", "label": "(85, OpenCL, clReleaseMemObject( memobjs[2] );); (140, OpenCL, clReleaseEvent(copyEvent);); (103, OpenCL, err = clEnqueueNDRangeKernel( queue, kernel[0], 2, NULL, threads, NULL, 0, NULL, NULL );); (72, OpenCL, err = clEnqueueWriteBuffer( queue, memobjs[1], true, 0, num_bytes, inptr, 0, NULL, NULL );); (87, OpenCL, err = clSetKernelArg( kernel[0], 0, sizeof( cl_mem ), (void *)&memobjs[1] );); (91, OpenCL, clReleaseProgram( program[0] );); (77, OpenCL, clReleaseMemObject( memobjs[0] );); (88, OpenCL, clReleaseKernel( kernel[0] );); (84, OpenCL, err |= clSetKernelArg( kernel[0], 1, sizeof( cl_mem ), (void *)&memobjs[0] );); (48, OpenCL, clReleaseMemObject(memobjs[0]);); (113, OpenCL, err = clEnqueueCopyImage( queue, memobjs[0], memobjs[2], srcPt, destPt, region, 0, NULL, &copyEvent );); (57, OpenCL, clReleaseMemObject(memobjs[1]);); (201, OpenCL, err = clEnqueueReadImage( queue, memobjs[2], true, origin, region2, 0, 0, dst, 0, NULL, NULL );); (125, OpenCL, err = clWaitForEvents( 1, &copyEvent );); (70, OpenCL, clReleaseMemObject( memobjs[1] );)"}
{"predict": "(11,OpenMP,#pragma omp parallel for)", "label": "(12, CUDA, cudaEventRecord(stop);); (12, CUDA, cudaEventSynchronize(stop);); (9, CUDA, cudaMemcpy(d_data, h_data, SIZE * sizeof(float), cudaMemcpyHostToDevice);); (7, CUDA, cudaEventRecord(start);); (-1, CUDA, cudaMalloc((void **)&d_data, SIZE * sizeof(float));); (10, CUDA, cudaEventElapsedTime(&milliseconds, start, stop);); (1, CUDA, cudaEventCreate(&stop);); (13, CUDA, cudaMemcpy(h_data, d_data, SIZE * sizeof(float), cudaMemcpyDeviceToHost);); (19, CUDA, cudaFree(d_data);); (-3, CUDA, cudaEventCreate(&start);)"}
{"predict": "(5,OpenMP,#pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp tile sizes(5)); (0, OpenMP, #pragma omp for)"}
{"predict": "(76,OpenMP,#pragma omp parallel for), (89,OpenMP,#pragma omp parallel for), (112,OpenMP,#pragma omp parallel for)", "label": "(143, MPI, MPI_Finalize();); (67, MPI, MPI_Bcast(&iterations,    1, MPI_INT, root, MPI_COMM_WORLD);); (67, MPI, MPI_Bcast(&vector_length, 1, MPI_LONG, root, MPI_COMM_WORLD);); (19, MPI, MPI_Init(&argc,&argv);); (85, MPI, MPI_Barrier(MPI_COMM_WORLD);); (19, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (17, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for private(index) reduction(|:any_cracked)), (23,OpenMP,#pragma omp parallel for private(i) if(MIN_KEYS_PER_CRYPT > 1)), (36,OpenMP,#pragma omp parallel for private(i) if(MIN_KEYS_PER_CRYPT > 1)), (40,OpenMP,#pragma omp parallel for private(i, success) reduction(|:any_cracked) if(MIN_KEYS_PER_CRYPT > 1))", "label": "(10, OpenMP, #pragma omp parallel for); (45, OpenMP, #pragma omp atomic)"}
{"predict": "(20,OpenMP,#pragma omp parallel for private(ip) reduction(+:tmp[t])), (...)", "label": "(20, CUDA, thrust::complex<T> const* m_ = m + batch * mstride;); (16, CUDA, tmp[t] = thrust::complex<T>(0.0);); (9, CUDA, __shared__ thrust::cuda_cub::core::uninitialized_array<thrust::complex<T>, 64> tmp;)"}
{"predict": "(25,OpenMP,#pragma omp parallel for reduction(+:sum))", "label": "(19, CUDA, __shared__ T B_thread_block_tile[BLOCK_TILE_SIZE_K][BLOCK_TILE_SIZE_X];); (17, CUDA, __shared__ T A_thread_block_tile[BLOCK_TILE_SIZE_Y][BLOCK_TILE_SIZE_K];)"}
{"predict": "(28,OpenMP,#pragma omp parallel for), (41,OpenMP,#pragma omp parallel for), (77,OpenMP,#pragma omp parallel for), (82,OpenMP,#pragma omp parallel for)", "label": "(109, MPI, MPI_Finalize();); (5, MPI, err = MPI_Init(&argc, &argv);); (8, MPI, err = MPI_Comm_size(MPI_COMM_WORLD, &numRanks);); (5, MPI, err = MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(58,OpenMP,#pragma omp parallel for)", "label": "(29, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (72, CUDA, CHECK_CUDA(cudaFree(dX));); (43, CUDA, CHECK_CUDA(cudaMalloc(&dX, size * sizeof(int32_t)));); (29, CUDA, CHECK_HIEDNN(hiednnSetCudaStream(handle, 0));); (67, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (69, CUDA, CHECK_CUDA(cudaFree(dY));); (40, CUDA, CHECK_CUDA(cudaMalloc(&dY, size * sizeof(int32_t)));)"}
{"predict": "(42,OpenMP,#pragma omp parallel for private(thread_results, reg_w, reg_x))", "label": "(29, CUDA, __shared__ float x_s[x_s_sz];); (27, CUDA, __shared__ float w_s[w_s_sz];)"}
{"predict": "(7,OpenMP,#pragma omp parallel for)", "label": "(16, SYCL, using namespace cl::sycl::access;); (4, SYCL, cl::sycl::queue queue;)"}
{"predict": "(3,OpenMP,#pragma omp parallel for)\n(8,OpenMP,#pragma omp parallel for)\n(10,OpenMP,#pragma omp parallel for)\n(11,OpenMP,#pragma omp parallel for)\n(12,OpenMP,#pragma omp parallel for)\n(17,OpenMP,#pragma omp parallel for)\n(18,OpenMP,#pragma omp parallel for)\n(27,OpenMP,#pragma omp parallel for)", "label": "(36, OpenMP, #pragma omp distribute collapse (N)); (9, OpenMP, #pragma omp distribute collapse (argc); (36, OpenMP, #pragma omp distribute collapse (2)); (22, OpenMP, #pragma omp distribute collapse (S)); (2, OpenMP, #pragma omp distribute collapse ()); (-1, OpenMP, #pragma omp distribute collapse (); (9, OpenMP, #pragma omp distribute collapse (1))); (27, OpenMP, #pragma omp distribute collapse (1)); (9, OpenMP, #pragma omp distribute collapse ((ST > 0) ? 1 + ST : 2)); (-7, OpenMP, #pragma omp distribute collapse); (3, OpenMP, #pragma omp distribute collapse (ST); (12, OpenMP, #pragma omp distribute collapse (foobool(argc)), collapse (true), collapse (-5))"}
{"predict": "(18,OpenMP,#pragma omp parallel for)", "label": "(8, MPI, rc = MPI_Type_get_extent(type, &tmp_lb, &attr->child_type_extent);)"}
{"predict": "(6,OpenMP,#pragma omp parallel for reduction(+: sivar))", "label": "(5, OpenMP, #pragma omp target teams distribute simd reduction(+: sivar))"}
{"predict": "14: \n115:     /* Free marker array. */\n116:     SUPERLU_FREE(marker);\n117: \n118:     /* Initialize the right-hand side and solution matrices. */\n119:     *rhs = b_global;\n120:     *x = xtrue_global;\n121:     *ldb = m;\n122:     *ldx = n;\n123: \n124:     return 0;\n125: }", "label": "(50, MPI, MPI_Bcast( nzval,   nnz, SuperLU_MPI_DOUBLE_COMPLEX, 0, grid->comm );); (38, MPI, MPI_Bcast( rowind, nnz, mpi_int_t,  0, grid->comm );); (50, MPI, MPI_Bcast( colptr,  n+1, mpi_int_t,  0, grid->comm );); (48, MPI, MPI_Bcast( rowind,  nnz, mpi_int_t,  0, grid->comm );); (39, MPI, MPI_Bcast( &m,   1,   mpi_int_t,  0, grid->comm );); (31, MPI, MPI_Bcast( &n,     1,   mpi_int_t,  0, grid->comm );); (29, MPI, MPI_Bcast( &m,     1,   mpi_int_t,  0, grid->comm );); (38, MPI, MPI_Bcast( &nnz, 1,   mpi_int_t,  0, grid->comm );); (32, MPI, MPI_Bcast( colptr, n+1, mpi_int_t,  0, grid->comm );); (28, MPI, MPI_Bcast( &nnz,   1,   mpi_int_t,  0, grid->comm );); (28, MPI, MPI_Bcast( nzval,  nnz, SuperLU_MPI_DOUBLE_COMPLEX, 0, grid->comm );); (33, MPI, MPI_Bcast( &n,   1,   mpi_int_t,  0, grid->comm );)"}
{"predict": "(17,OpenMP,#pragma omp parallel for private(errs, r, s))", "label": "(22, MPI, MPI_Request_free(&r);); (14, MPI, MPI_Recv_init(buff, count, type, 0, tag, comm, &r);); (2, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(18,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for), (48,OpenMP,#pragma omp parallel for), (52,OpenMP,#pragma omp parallel for), (55,OpenMP,#pragma omp parallel for)", "label": "(20, TBB, tbb::flow::make_edge( n, r[i] );); (54, TBB, tbb::flow::remove_edge( n, r[i] );); (1, TBB, tbb::flow::write_once_node<R> n(g);); (-1, TBB, tbb::flow::graph g;)"}
{"predict": "(9,OpenMP,#pragma omp parallel for)", "label": "(64, OpenCL, clReleaseMemObject(clCompressedImage);); (25, OpenCL, auto clCompressedImage = clCreateImage(context, CL_MEM_READ_WRITE, &imageFormat, &imageDescriptor, nullptr, &retVal);)"}
{"predict": "(3,OpenMP,#pragma omp parallel for private(sum1,sum2,sum3))", "label": "(7, CUDA, extern __shared__ float share_mem[];)"}
{"predict": "(8,OpenMP,#pragma omp parallel for private(span, dst_ptr, i_y, i_yy, t_y, t_yy))", "label": "(7, OpenMP, #pragma omp parallel for)"}
{"predict": "DYNAMIC_SECTION(\"Hamiltonian copy constructor - \"\n129:                             << StateVectorMPIToName<StateVectorT>::name) {\n130:                 auto X0 = std::make_shared<NamedObsT>(\n131:                     \"PauliX\", std::vector<std::size_t>{0});\n132:                 auto X1 = std::make_shared<NamedObsT>(\n133:                     \"PauliX\", std::vector<std::size_t>{1});\n134:                 auto X2 = std::make_shared<NamedObsT>(\n135:                     \"PauliX\", std::vector<std::size_t>{2});\n136: \n137:                 auto Y0 = std::make_shared<NamedObsT>(\n138:                     \"PauliY\", std::vector<std::size_t>{0});\n139:                 auto Y1 = std::make_shared<NamedObsT>(\n140:                     \"PauliY\", std::vector<std::size_t>{1});\n141:                 auto Y2 = std::make_shared<NamedObsT>(\n142:                     \"PauliY\", std::vector<std::size_t>{2});\n143: \n144:                 auto Z0 = std::make_shared<NamedObsT>(\n145:                     \"PauliZ\", std::vector<std::size_t>{0});\n146:                 auto Z1 = std::make_shared<NamedObsT>(\n147:                     \"PauliZ\", std::vector<std::size_t>{1});\n148:                 auto Z2 = std::make_shared<NamedObsT>(\n149:                     \"PauliZ\", std::vector<std::size_t>{2});\n150: \n151:                 auto ham1 = HamiltonianT::create(\n152:                     {0.8, 0.5, 0.7},\n153:                     {\n154:                         std::make_shared<TensorProdObsT>(X0, Y1, Z2),\n155:                         std::make_shared<TensorProdObsT>(Z0, X1, Y2),\n156:                         std::make_shared<TensorProdObsT>(Y0, Z1, X2),\n157:                     });\n158: \n1", "label": "(24, CUDA, cudaSetDevice(deviceId);); (20, CUDA, cudaGetDeviceCount(&nDevices);); (6, MPI, MPIManager mpi_manager(MPI_COMM_WORLD);)"}
{"predict": "(5,OpenMP,#pragma omp parallel for private(def))", "label": "(73, CUDA, cudaMalloc((void**) &d_gradData, m * c * sizeof(double));); (116, CUDA, cudaFree(d_fcLayer);); (118, CUDA, cudaFree(d_diffData);); (116, CUDA, cudaFree(d_gradData);); (12, CUDA, cudaMalloc((void**) &d_fcLayer, m * c * sizeof(double));); (17, CUDA, cudaMalloc((void**) &d_softmaxData, m * c * sizeof(double));); (43, CUDA, cudaDeviceSynchronize();); (11, CUDA, cudaMemcpy(d_fcLayer, fcLayer, m * c * sizeof(double), cudaMemcpyHostToDevice);); (72, CUDA, cudaMalloc((void**) &d_diffData, m * c * sizeof(double));); (109, CUDA, cudaFree(d_softmaxData);); (83, CUDA, cudaMemcpy(result_backward, d_gradData, m * c * sizeof(double), cudaMemcpyDeviceToHost);); (44, CUDA, cudaMemcpy(result, d_softmaxData, m * c * sizeof(double), cudaMemcpyDeviceToHost);); (69, CUDA, cudaMemcpy(d_diffData, diffData, m * c * sizeof(double), cudaMemcpyHostToDevice);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for private(K3, ddata, checksum, rckey, len, ctx, key, wkey) reduction(|:any_cracked))", "label": "(11, OpenMP, #pragma omp parallel for); (77, OpenMP, #pragma omp atomic)"}
{"predict": ");\n118: \n119:     /* Each process calculates the list of offsets and lengths in the\n120:        file that lie in its file domain. */\n121: \n122:     ADIOI_Calc_my_req(fd, offset_list, len_list, start_offset, end_offset,\n123:                       min_st_offset, fd_start, fd_end, &my_req,\n124:                       &count_my_req_per_proc, &count_my_req_procs);\n125: \n126:     /* Each process calculates the list of offsets and lengths in the\n127:        file that lie in other processes' file domains. */\n128: \n129:     ADIOI_Calc_others_req(fd, offset_list, len_list, start_offset, end_offset,\n130:                           min_st_offset, fd_start, fd_end, &others_req,\n131:                           &count_others_req_per_proc, &count_others_req_procs);\n132: \n133:     /* Each process communicates the list of offsets and lengths in its\n134:        file domain that lie in other processes' file domains. */\n135: \n136:     ADIOI_Exchange_others_req(fd, others_req, count_others_req_per_proc,\n137:                               count_others_req_procs);\n138: \n139:     /* Each process sends the data to the processes that own the file domains\n140:        in which the offsets and lengths lie. */\n141: \n142:     ADIOI_Fill_send_buffer(fd, my_req, my_req->flat_buf, others_req->send_buf,\n143:                            others_req->offset_list, others_req->len_list,\n144:                            &others_req->send_size, others_req->requests,\n145:                            others_req->sent_to_proc, nprocs, myrank,\n146:                            contig_access_count, min_st_offset, fd_size,\n147:                            fd_start, fd_end, others_req->send_buf_idx,\n148:                            others_req->curr_to_proc, others_req->done_to_proc,\n149:                            0, others_req->buftype_extent);\n150: \n151:     /* Each process writes the data in its file domain directly", "label": "(173, MPI, MPI_Allreduce(&tmp_error, error_code, 1, MPI_INT, MPI_MAX, fd->comm);); (47, MPI, MPI_Comm_size(fd->comm, &nprocs);); (75, MPI, MPI_Allgather(&end_offset, 1, ADIO_OFFSET, end_offsets, 1, ADIO_OFFSET, fd->comm);); (73, MPI, MPI_Allgather(&start_offset, 1, ADIO_OFFSET, st_offsets, 1, ADIO_OFFSET, fd->comm);); (45, MPI, MPI_Comm_rank(fd->comm, &myrank);); (192, MPI, MPI_Type_size_x(datatype, &size);)"}
{"predict": "The provided code snippet does not contain any loops or sections that are inherently parallelizable, as it mainly deals with printing and does not perform any computationally intensive tasks that could benefit from parallelization. However, if there were loops or sections of the code that could be parallelized, I would suggest inserting parallelization instructions there.\n\nGiven the current code, there is no clear section to parallelize, but if you have a loop or a section of code that you want to parallelize, you can provide that and I can suggest where to insert the parallelization instructions.\n\nFor now, since there's no such section, the output will be empty. But if you want an example of how to parallelize a loop, here's a hypothetical example:\n\n### **Hypothetical Code with a Loop**\n\n```c\n1: void SCT_printComm3D(gridinfo3d_t *grid3d, SCT_t* SCT)\n2: {\n3:     //\n4:     double cvolFactor;\n5:     double cvolRed;\n6: \n7:     int_t Np = (grid3d->npcol) * (grid3d->nprow) * (grid3d->npdep);\n8:     if (!grid3d->iam)\n9:     {\n10:         /* code */\n11:         printf(\"| commVolRed | %g   | %g |\\n\", cvolRed, cvolRed/Np );\n12:         printf(\"| commVolFactor | %g   | %g |\\n\", cvolFactor, cvolFactor/Np );\n13:     }\n14: \n15:     // Hypothetical loop that could be parallelized\n16:     for (int i = 0; i < Np; i++) {\n17:         // Some computations\n18:         cvolFactor += some_computation(i);\n19:     }\n20: }\n```\n\n### **Parallelization Instructions for Hypothetical Code**\n\n(16,OpenMP,#pragma omp parallel for reduction(+:cvolFactor))", "label": "(4, MPI, MPI_Reduce( &SCT->commVolFactor, &cvolFactor,  1, MPI_DOUBLE, MPI_SUM, 0, grid3d->comm );); (5, MPI, MPI_Reduce( &SCT->commVolRed, &cvolRed,  1, MPI_DOUBLE, MPI_SUM, 0, grid3d->comm );)"}
{"predict": "CUFFT_CHECK(cufftDestroy(plan));\n103: \n104:     // Print the CPU data\n105:     auto[out_begin_h, out_end_h] = BoxIterators(in, output_cpu_data.data());\n106:     for (auto it = out_begin_h; it != out_end_h; ++it) { \n107:         std::cout << \"final data, global 3D index [\" << it.x() << \",\" << it.y() << \",\" << it.z() << \"], local index \" << it.i() << \", rank \" << rank << \" is (\" << it->real() << \",\" << it->imag() << \")\\n\";\n108:     }\n109: \n110:     return 0;\n111: }", "label": "(10, CUDA, CUDA_CHECK(cudaGetDeviceCount(&ndevices));); (54, CUDA, CUDA_CHECK(cudaStreamCreate(&stream));); (88, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (8, CUDA, CUDA_CHECK(cudaSetDevice(rank % ndevices));); (-2, MPI, MPI_Init(&argc, &argv);); (113, MPI, MPI_Finalize();); (1, MPI, MPI_Comm_size(mpi_comm, &size);); (103, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (78, CUDA, CUDA_CHECK(cudaGetLastError());); (-3, MPI, MPI_Comm_rank(mpi_comm, &rank);)"}
{"predict": "(10,OpenMP,#pragma omp parallel sections), (11,OpenMP,#pragma omp section), (12,OpenMP,#pragma omp section)", "label": "(5, TBB, if(tbb::is_current_task_group_canceling()) return;)"}
{"predict": "(10,OpenMP,#pragma omp parallel for),", "label": "(14, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(4,OpenMP,#pragma omp parallel for reduction(+:total_worktime))", "label": "(4, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (5, TBB, tbb::tick_count t1 = tbb::tick_count::now();)"}
{"predict": "(13,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for)", "label": "(11, MPI, MPI_Init(&argc, &argv);); (44, MPI, MPI_Finalize();); (10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (35, MPI, MPI_Recv(recvbuf.data(), block_size, MPI_INT, 0, rank, MPI_COMM_WORLD, &status);); (26, MPI, MPI_Send(&sendbuf[i*block_size], block_size, MPI_INT, i, i, MPI_COMM_WORLD);); (20, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for), (45,OpenMP,#pragma omp parallel for)", "label": "(23, MPI, MPI_Reduce(gFrstCost, gFrstCostAcc, numForests, MPI_DOUBLE, MPI_SUM, 0, grid3d->zscp.comm);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for)", "label": "(17, MPI, MPI_Finalize();); (2, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(28,OpenMP,#pragma omp parallel for reduction(+:gu)), (55,OpenMP,#pragma omp parallel for), (78,OpenMP,#pragma omp parallel for), (101,OpenMP,#pragma omp parallel for)", "label": "(11, CUDA, __shared__ float u_[_N_];); (11, CUDA, __shared__ float r[_N_], k[_N_], v[_N_], w_[_N_], gy[_N_];)"}
{"predict": "(15,OpenMP,#pragma omp parallel sections), (15,OpenMP,#pragma omp section), (20,OpenMP,#pragma omp section), (25,OpenMP,#pragma omp section), (30,OpenMP,#pragma omp section), (35,OpenMP,#pragma omp section), (40,OpenMP,#pragma omp section), (45,OpenMP,#pragma omp section)", "label": "(12, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "The provided code snippet is a function that checks and prints an error if there is a mismatch between two buffers. The function does not contain any loops or sections that are inherently parallelizable, making it difficult to identify specific lines for parallelization without additional context or restructuring. However, if we assume that the function could be part of a larger loop or repeated calls in a parallel context, we can suggest parallelization at a higher level. \n\nGiven the lack of loops, a practical suggestion would be to parallelize the function calls themselves if they are part of a larger computation. However, since the task is to insert parallelization instructions within the given code, we can only suggest parallelizing the loop if it were present.\n\nSince there is no loop in the provided code, there is no specific line number to insert parallelization instructions. However, if we hypothetically assume that the function is called in a loop in the main program, we could suggest parallelizing that loop.\n\nFor the sake of providing an answer in the requested format, let's assume the function is part of a loop in the main program that iterates over a large number of data types or buffers. Here is a hypothetical example of how parallelization could be suggested:\n\n### Hypothetical Example\n\n```c\n1: int main() {\n2:     int num_tests = 1000;\n3:     for (int i = 0; i < num_tests; i++) {\n4:         MTestDatatype2CheckAndPrint(inbufs[i], outbufs[i], size_bytes, typenames[i], typenums[i]);\n5:     }\n6:     return 0;\n7: }\n```\n\nIn this hypothetical example, we can parallelize the loop in the `main` function:\n\n(3,OpenMP,#pragma omp parallel for)\n\nHowever, since the original code does not contain such a loop, we cannot provide a specific line number for parallelization within the given code. If you have a specific loop or context in which this function is used, please provide that for a more accurate suggestion.", "label": "(7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);)"}
{"predict": "(18,OpenMP,#pragma omp parallel for reduction(+:etsSum) reduction(max:etsMax) reduction(min:etsMin)), (29,OpenMP,#pragma omp parallel for reduction(+:etsSigmaSq))", "label": "(3, TBB, tbb::enumerable_thread_specific<double> test_locals;); (13, TBB, REMARK(\"%d threads, work ratio %d per cent, time %g\", nThreads, csWorkRatio, (tbb::tick_count::now() - t0).seconds());); (0, TBB, tbb::task_scheduler_init init(tbb::task_scheduler_init::deferred);); (5, TBB, tbb::tick_count t0;); (6, TBB, t0 = tbb::tick_count::now();)"}
{"predict": "(30,OpenMP,#pragma omp parallel for),", "label": "(43, CUDA, CSC(cudaGetLastError(), STATUS_FAILURE);)"}
{"predict": "2: \n173:     /* link files in flist */\n174:     int rc = mfu_flist_link(flist, path_old, path_new, preserve_times, relative_targets);\n175: \n176:     /* free path objects */\n177:     mfu_path_delete(&path_old);\n178:     mfu_path_delete(&path_new);\n179: \n180:     /* free file list */\n181:     mfu_flist_free(&flist);\n182: \n183:     /* free mfu_walk_opts_t object */\n184:     mfu_walk_opts_delete(&walk_opts);\n185: \n186:     /* free mfu_file object */\n187:     mfu_file_delete(&mfu_file);\n188: \n189:     /* finalize */\n190:     mfu_finalize();\n191: \n192:     return rc;\n193: }", "label": "(2, MPI, MPI_Init(&argc, &argv);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (105, MPI, MPI_Finalize();); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(22,OpenMP,#pragma omp parallel for private(out))", "label": "(38, TBB, CHECK_MESSAGE( (tbb::flow::output_port<0>(outer_node).try_get(out) == output.try_get(out)), \"output from outer_node does not match output from graph\");); (13, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<1>(outer_node) == &tbb::flow::input_port<1>(inner_node1)), \"input port 1 of inner_node1 not bound to input port 1 in outer_node\");); (0, TBB, tbb::flow::composite_node<std::tuple<int, int>, std::tuple<int> > outer_node(g);); (8, TBB, outer_node.set_external_ports(base_type::input_ports_type(tbb::flow::input_port<0>(inner_node1), tbb::flow::input_port<1>(inner_node1)), base_type::output_ports_type(tbb::flow::output_port<0>(inner_node3)));); (-1, TBB, typedef tbb::flow::composite_node<std::tuple<int, int>, std::tuple<int> > base_type;); (18, TBB, tbb::flow::make_edge(inner_node2, tbb::flow::input_port<0>(inner_node3));); (18, TBB, tbb::flow::make_edge(inner_node2, tbb::flow::input_port<1>(inner_node3));); (13, TBB, tbb::flow::make_edge(inner_node1, tbb::flow::input_port<0>(inner_node2));); (-7, TBB, tbb::flow::graph g;); (6, TBB, CHECK_MESSAGE( (&tbb::flow::output_port<0>(outer_node) == &tbb::flow::output_port<0>(inner_node3)), \"output port 0 of inner_node3 not bound to output port 0 in outer_node\");); (11, TBB, tbb::flow::make_edge(inner_node1, tbb::flow::input_port<1>(inner_node2));); (2, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<0>(outer_node) == &tbb::flow::input_port<0>(inner_node1)), \"input port 0 of inner_node1 not bound to input port 0 in outer_node\");); (-8, TBB, tbb::flow::broadcast_node<int> input(g);); (-8, TBB, tbb::flow::queue_node<int> output(g);)"}
{"predict": "The provided code snippet is a unit test function and does not contain any loops or sections that are inherently parallelizable. Therefore, there are no appropriate lines to insert parallelization instructions. However, if you have a different code snippet that includes loops or sections that can be parallelized, feel free to share it, and I can provide the necessary parallelization instructions.", "label": "(16, CUDA, CudaFile aCudaFile( aReader, GPUSPHOptions(), aTemplDir );)"}
{"predict": "(2,OpenMP,#pragma omp parallel for), (5,OpenMP,#pragma omp parallel for), (8,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (29,OpenMP,#pragma omp parallel for)", "label": "(29, OpenMP, #pragma omp target teams distribute simd safelen (foobool(argc)), safelen (true), safelen (-5)); (46, OpenMP, #pragma omp target teams distribute simd safelen(safelen(tmain<int, char, -1, -2>(argc, argv)); (-1, OpenMP, #pragma omp target teams distribute simd safelen); (19, OpenMP, #pragma omp target teams distribute simd safelen (foobool(1) > 0 ? 1 : 2)); (9, OpenMP, #pragma omp target teams distribute simd safelen (4); (0, OpenMP, #pragma omp target teams distribute simd safelen (); (27, OpenMP, #pragma omp target teams distribute simd safelen (S1)); (10, OpenMP, #pragma omp target teams distribute simd safelen (2+2))); (1, OpenMP, #pragma omp target teams distribute simd safelen ())"}
{"predict": "(13,OpenMP,#pragma omp parallel for reduction(+:entropy))", "label": "(11, OpenMP, #pragma omp parallel for reduction(+ : entropy))"}
{"predict": "(17,OpenMP,#pragma omp parallel for private(pkk, lwork, knsupc, lk, ii, i))", "label": "(28, MPI, MPI_Bcast( work, lwork, MPI_FLOAT, pkk, grid->comm );); (29, MPI, MPI_Bcast( work, diag_len[p], MPI_FLOAT, pkk, grid->comm );)"}
{"predict": "(9,OpenMP,#pragma omp parallel for)", "label": "(6, MPI, MPI_Comm_dup(MPI_COMM_SELF, &lwgrp_comm_self);)"}
{"predict": "(24,OpenMP,#pragma omp parallel for private(t_var, vec, s_arr, var, sivar))", "label": "(4, OpenMP, #pragma omp target teams distribute parallel for private(g, g1, sivar))"}
{"predict": "totTime;\n57: \t\t\trun_time[r][1] = (double)dims[0] * dims[1] * dims[2];\n58: \n59: \t\t\tcufftDestroy(planZ2Z);\n60: \t\t}\n61: \t\tdouble min_time = run_time[0][0];\n62: \t\tdouble sum_time = 0;\n63: \t\tfor (int r = 0; r < num_runs; r++) {\n64: \t\t\tif (run_time[r][0] < min_time) min_time = run_time[r][0];\n65: \t\t\tsum_time += run_time[r][0];\n66: \t\t}\n67: \t\tbenchmark_result[0] += (run_time[0][1] / min_time);\n68: \t\tbenchmark_result[1] += (run_time[0][1] / sum_time);\n69: \t}\n70: \tdouble avg_time = benchmark_result[0] / num_benchmark_samples;\n71: \tdouble avg_time_sum = benchmark_result[1] / num_benchmark_samples;\n72: \tif (file_output)\n73: \t\tfprintf(output, \"Average system size per iteration / time : %.2f \\n\", avg_time);\n74: \tprintf(\"Average system size per iteration / time : %.2f \\n\", avg_time);\n75: \tif (file_output)\n76: \t\tfprintf(output, \"Average system size per total time : %.2f \\n\", avg_time_sum);\n77: \tprintf(\"Average system size per total time : %.2f \\n\", avg_time_sum);\n78: \tfree(inputC);\n79: }", "label": "(27, CUDA, cudaMalloc((void**)&dataC, sizeof(cufftDoubleComplex) * dims[0] * dims[1] * dims[2]);); (55, CUDA, cudaDeviceSynchronize();); (27, CUDA, cudaMemcpy(dataC, inputC, sizeof(cufftDoubleComplex) * dims[0] * dims[1] * dims[2], cudaMemcpyHostToDevice);); (78, CUDA, cudaFree(dataC);); (1, CUDA, cudaSetDevice(device_id);)"}
{"predict": "(7,OpenMP,#pragma omp parallel for)", "label": "(6, CUDA, cudaCheck(cudaMemGetInfo(&free, &total));)"}
{"predict": "(10,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (38,OpenMP,#pragma omp parallel for collapse(2))", "label": "(48, CUDA, cudaFree(curandStates);); (4, CUDA, cudaMalloc(&curandStates, sizeof(curandState_t) * 2 * batchSize);)"}
{"predict": "(18,OpenMP,#pragma omp parallel for)", "label": "(29, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_ARE_FATAL);); (23, MPI, MPI_Error_string(ierr, str, &slen);); (14, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(40,OpenMP,#pragma omp parallel sections), (41,OpenMP,#pragma omp section)", "label": "(40, TBB, tbb::flow::make_edge(src, gamma);); (40, TBB, tbb::flow::make_edge(gamma, tint);); (40, TBB, tbb::flow::make_edge(tint, write);); (0, TBB, tbb::flow::graph g;)"}
{"predict": "(10,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for)", "label": "(3, CUDA, __shared__ unsigned sidxs[nTPB];)"}
{"predict": "(7,OpenMP,#pragma omp parallel for reduction(+:totalsize))", "label": "(19, MPI, MPI_Reduce(&mysize, &totalsize, 1, MPI_INT64_T, MPI_SUM, 0, MPI_COMM_WORLD);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&me);); (0, MPI, MPI_Init(&argc, &argv);); (23, MPI, MPI_Gather(&mysizeint, 1, MPI_INT, &(counts[0]), 1, MPI_INT, 0, MPI_COMM_WORLD);); (37, MPI, MPI_Finalize();); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD,&np);); (29, MPI, MPI_Gatherv(in.c_str(), mysizeint, MPI_CHAR, &(out[0]), &(counts[0]), &(displs[0]), MPI_CHAR, 0, MPI_COMM_WORLD);)"}
{"predict": "(2,OpenMP,#pragma omp parallel for), (9,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for)", "label": "(18, OpenMP, #pragma omp teams distribute parallel for order(concurrent); (1, OpenMP, #pragma omp teams distribute parallel for order); (20, OpenMP, #pragma omp teams distribute parallel for order(concurrent)); (-2, OpenMP, #pragma omp target); (10, OpenMP, #pragma omp teams distribute parallel for order(none); (5, OpenMP, #pragma omp teams distribute parallel for order()"}
{"predict": "(11,OpenMP,#pragma omp parallel for)", "label": "(13, CUDA, fprintf(stderr, \"CUDA kernel failed : %s\\n\", cudaGetErrorString(err));); (10, CUDA, auto err = cudaGetLastError();)"}
{"predict": "& FLG_SHOW_CHK) && !options.loader.showuncracked &&\n169: \t    john_main_process && options.internal_cp &&\n170: \t    options.target_enc != options.internal_cp) {\n171: \t\tlog_event(\"- Internal codepage: %s\",\n172: \t\t          cp_id2name(options.internal_cp));\n173: \t}\n174: \n175: \tif (john_main_process) {\n176: \t\tif (options.loader.showuncracked) {\n177: \t\t\tlog_event(\"- Cracked passwords will be shown\");\n178: \t\t}\n179: \t\telse if (options.loader.cracked) {\n180: \t\t\tlog_event(\"- Only cracked passwords will be shown\");\n181: \t\t}\n182: \t\telse {\n183: \t\t\tlog_event(\"- Cracked passwords will be stored in %s\",\n184: \t\t\t          options.loader.pot);\n185: \t\t}\n186: \t}\n187: \n188: \tif (john_main_process && options.store_utf8 &&\n189: \t    options.loader.log_passwords &&\n190: \t    options.target_enc != UTF_8) {\n191: \t\tlog_event(\"- Passwords will be stored UTF-8 encoded in .pot file\");\n192: \t}\n193: \n194: \tif (john_main_process) {\n195: \t\tif (options.store_ciphertexts) {\n196: \t\t\tlog_event(\"- Ciphertexts will be stored in %s\",\n197: \t\t\t          options.loader.pot);\n198: \t\t}\n199: \t\telse {\n200: \t\t\tlog_event(\"- Only cracked hashes will be stored in %s\",\n201: \t\t\t          options.loader.pot);\n202: \t\t}\n203: \t}\n204: \n205: \tif (john_main_process) {\n206: \t\tif (options.loader.cracked && options.loader.showuncracked) {\n207: \t\t\tlog_event(\"- Cracked passwords will be shown\");\n208: \t\t}\n209: \t}\n210: \n211: \tif (john_main_process) {\n212: \t\tif (options.store_ciphertexts && options.store_utf8 &&\n2", "label": "(14, OpenMP, john_omp_fallback(argv);); (18, MPI, mpi_setup(argc, argv);); (78, OpenMP, john_omp_maybe_adjust_or_fallback(argv);); (6, OpenMP, john_omp_init();)"}
{"predict": "(24,OpenMP,#pragma omp parallel for private(randval))", "label": "(44, MPI, MPI_Comm_split(MPI_COMM_WORLD, MPI_UNDEFINED, rank, &newcomm);); (21, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (40, MPI, MPI_Comm_free(&newcomm);); (21, MPI, startTime = MPI_Wtime();); (24, MPI, double rate = MPI_Wtime() - startTime;); (36, MPI, MPI_Comm_split(MPI_COMM_WORLD, 1, rank, &newcomm);)"}
{"predict": "(84,OpenMP,#pragma omp parallel for)", "label": "(56, CUDA, flamegpu::CUDASimulation  cudaSimulation(model, argc, argv);)"}
{"predict": "(25,OpenMP,#pragma omp parallel for), (29,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for)", "label": "(36, TBB, typedef tbb::concurrent_hash_map<MyKey,MyData2,YourHashCompare> YourTable2;); (34, TBB, typedef tbb::concurrent_hash_map<MyKey,MyData2,YourHashCompare,MyAllocator> YourTable1;)"}
{"predict": "!= MPI_SUCCESS) {\n116:             MPI_Error_string(mpirc, mpierrstr, &mpierrlen);\n117:             MFU_ABORT(1, \"Failed to set view on file: `%s' rc=%d %s\", name, mpirc, mpierrstr);\n118:         }\n119: \n120:         /* read data */\n121:         int group_buf_size = (int) buft_pack_size(groups);\n122:         if (rank == 0) {\n123:             char* group_buf = (char*) MFU_MALLOC(group_buf_size);\n124:             if (mpirc != MPI_SUCCESS) {\n125:                 MPI_Error_string(mpirc, mpierrstr, &mpierrlen);\n126:                 MFU_ABORT(1, \"Failed to read file: `%s' rc=%d %s\", name, mpirc, mpierrstr);\n127:             }\n128:             buft_unpack(group_buf, groups);\n129:             mfu_free(&group_buf);\n130:         }\n131:         disp += (MPI_Offset) group_buf_size;\n132:     }\n133: \n134:     /* read file records */\n135:     if (all_count > 0) {\n136:         /* allocate memory to hold data */\n137:         size_t bufsize = count * (size_t)extent;\n138:         flist->buf = (void*) MFU_MALLOC(bufsize);\n139:         flist->bufsize = bufsize;\n140: \n141:         /* set view to read data */\n142:         mpirc = MPI_File_set_view(fh, disp, MPI_BYTE, MPI_BYTE, datarep, MPI_INFO_NULL);\n143:         if (mpirc != MPI_SUCCESS) {\n144:             MPI_Error_string(mpirc, mpierrstr, &mpierrlen);\n145:             MFU_ABORT(1, \"Failed to set view on file: `%s' rc=%d %s\", name, mpirc, mpierrstr);\n146:         }\n147: \n148:         /* read data */\n149:         if (mpirc != MPI_SUCCESS) {\n150:             MPI_Error_string(mpirc", "label": "(28, MPI, MPI_Error_string(mpirc, mpierrstr, &mpierrlen);); (142, MPI, MPI_Bcast(groups->buf, (int)groups->count, groups->dt, 0, MPI_COMM_WORLD);); (96, MPI, mpirc = MPI_File_read_at(fh, 0, user_buf, user_buf_size, MPI_BYTE, &status);); (132, MPI, mpirc = MPI_File_read_at(fh, 0, group_buf, group_buf_size, MPI_BYTE, &status);); (102, MPI, MPI_Bcast(users->buf, (int)users->count, users->dt, 0, MPI_COMM_WORLD);); (15, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (28, MPI, mpirc = MPI_File_read_at(fh, 0, header_packed, header_size, MPI_BYTE, &status);); (41, MPI, MPI_Bcast(header, 6, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (60, MPI, MPI_Exscan(&count, &offset, 1, MPI_UINT64_T, MPI_SUM, MPI_COMM_WORLD);); (12, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (16, MPI, int mpirc = MPI_File_set_view(fh, disp, MPI_BYTE, MPI_BYTE, datarep, MPI_INFO_NULL);); (69, MPI, MPI_Type_get_extent(users->dt, &lb_user, &extent_user);); (105, MPI, MPI_Type_get_extent(groups->dt, &lb_group, &extent_group);); (190, MPI, mpirc = MPI_File_read_at(fh, read_offset, buf, read_size, MPI_BYTE, &status);); (74, MPI, mpirc = MPI_File_set_view(fh, disp, MPI_BYTE, MPI_BYTE, datarep, MPI_INFO_NULL);); (159, MPI, MPI_Allreduce(&iters, &all_iters, 1, MPI_UINT64_T, MPI_MAX, MPI_COMM_WORLD);)"}
{"predict": "(19,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (20,MPI,MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);)", "label": "(22, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (22, MPI, MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);)"}
{"predict": "(9,OpenMP,#pragma omp parallel sections)", "label": "(8, CUDA, CHECK(cudaFree(m_output_boxes_device));); (9, CUDA, CHECK(cudaFree(m_output_classes_device));); (2, CUDA, CHECK(cudaFree(m_input_resize_device));); (0, CUDA, CHECK(cudaFree(m_input_src_device));); (3, CUDA, CHECK(cudaFree(m_output_num_device));); (0, CUDA, CHECK(cudaFree(m_input_rgb_device));); (3, CUDA, CHECK(cudaFree(m_output_scores_device));)"}
{"predict": "(21,OpenMP,#pragma omp parallel for)", "label": "(19, CUDA, extern __shared__ bool kept_bboxinfo_flag[];)"}
{"predict": "(23,OpenMP,#pragma omp parallel for private(rank, rank_hash, desc_hash, desc_copy))", "label": "(9, MPI, MPI_Comm_rank(comm_world, &rank_world);)"}
{"predict": "(19,MPI,MPI_Startall(3, reqs);); (31,MPI,MPI_Startall(3, reqs);); (39,OpenMP,#pragma omp parallel for)", "label": "(39, MPI, MPI_Request_free(&reqs[i]);)"}
{"predict": "(16,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for)", "label": "(11, MPI, MPI_Comm_size(comm, &npes);)"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (11,cuda,cudaMalloc((void**)&d_B, DSIZE * sizeof(float));), (12,cuda,cudaMalloc((void**)&d_C, DSIZE * sizeof(float));), (16,cuda,cudaMemcpy(d_B, h_B, DSIZE * sizeof(float), cudaMemcpyHostToDevice);), (22,cuda,cudaMemcpy(h_C, d_C, DSIZE * sizeof(float), cudaMemcpyDeviceToHost);)", "label": "(15, CUDA, cudaMemcpy(d_A, h_A, DSIZE*sizeof(float), cudaMemcpyHostToDevice);); (20, CUDA, cudaCheckErrors(\"kernel launch failure\");); (8, CUDA, cudaMalloc(&d_A, DSIZE*sizeof(float));)"}
{"predict": "(16,OpenMP,#pragma omp parallel for)", "label": "(35, MPI, MPI_Finalize();); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &comm_size);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (8, MPI, vtkh::SetMPICommHandle(MPI_Comm_c2f(MPI_COMM_WORLD));)"}
{"predict": "(14,OpenMP,#pragma omp parallel for collapse(2)), (45,OpenMP,#pragma omp parallel for collapse(2))", "label": "(19, CUDA, result = cudaMalloc((void**)&devPtrA, M*N*sizeof(*a));); (30, CUDA, cudaFree(devPtrA);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for private(start, end, permutation, error) reduction(min:bestError) reduction(min:bestPermutation) reduction(min:bestStart) reduction(min:bestEnd))", "label": "(6, CUDA, __shared__ uint s_permutations[160];)"}
{"predict": "27: \t\t/* Update solution vector X. */\n128: \t\tfor (i = 0; i < m_loc; ++i) X_col[i] += dx[i];\n129: \n130: \t\t/* Update residual. */\n131: \t\tfor (i = 0; i < m_loc; ++i) R[i] -= dx[i];\n132: \n133: \t\t/* Update lstres. */\n134: \t\tlstres = berr[j];\n135: \n136: \t\t/* Compute relative backward error berr[j] = norm(X_new-X)/norm(X). */\n137: \t\tfor (i = 0; i < m_loc; ++i) dx[i] = fabs(dx[i]);\n138: \t\ts = 0.0;\n139: \t\tfor (i = 0; i < m_loc; ++i) s = SUPERLU_MAX(s, dx[i]);\n140: \t\tberr[j] = s / SUPERLU_MAX(safe1, lstres);\n141: \n142: \t\t++count;\n143: \n144: \t    } else {\n145: \t\t/* Stop criterion is satisfied. */\n146: \t\tbreak;\n147: \t    }\n148: \t}\n149:     }\n150: \n151:     SUPERLU_FREE(work);\n152: \n153: #if ( DEBUGlevel>=1 )\n154:     CHECK_MALLOC(iam, \"Exit psgsrfs3D()\");\n155: #endif\n156: \n157:     return;\n158: }", "label": "(110, MPI, MPI_Allreduce( &s, &berr[j], 1, MPI_FLOAT, MPI_MAX, grid->comm );); (111, MPI, MPI_Bcast( &berr[j], 1, MPI_FLOAT, 0,  grid3d->zscp.comm);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for)\n(23,OpenMP,#pragma omp parallel for private(ii))\n(32,OpenMP,#pragma omp parallel for)\n(41,OpenMP,#pragma omp parallel for)\n(55,OpenMP,#pragma omp parallel for private(ii))\n(62,OpenMP,#pragma omp parallel for private(ii))\n(71,OpenMP,#pragma omp parallel for)\n(77,OpenMP,#pragma omp parallel for)\n(86,OpenMP,#pragma omp parallel for)\n(94,OpenMP,#pragma omp parallel for private(ii))", "label": "(62, TBB, tbb::internal::segmented_iterator<nested_vec, const T> my_csi(my_vec);); (19, TBB, tbb::internal::segmented_iterator<nested_vec, T> my_si(my_vec);)"}
{"predict": "(32,OpenMP,#pragma omp parallel for reduction(+:s_img))", "label": "(4, CUDA, __shared__ uint4 s_img[TPB];)"}
{"predict": "(18,OpenMP,#pragma omp parallel for private(is_member) reduction(||:is_member)), (43,OpenMP,#pragma omp parallel for private(pgroup_old) reduction(merge:pgroup)), (58,OpenMP,#pragma omp parallel for private(merged, pgroup_new, pgroup_old) reduction(merge:pgroup))", "label": "(49, MPI, MPI_Comm_accept(port, MPI_INFO_NULL, 0, pgroup, &pgroup_new);); (53, MPI, MPI_Comm_free(&pgroup_old);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (65, MPI, MPI_Comm_connect(port, MPI_INFO_NULL, 0, MPI_COMM_SELF, &pgroup_new);); (43, MPI, MPI_Send(port, PORT_SIZE, MPI_CHAR, members[i], 0, MPI_COMM_WORLD);); (36, MPI, MPI_Open_port(port_info, port);); (45, MPI, MPI_Comm_free(&pgroup_new);); (49, MPI, MPI_Close_port(port);); (67, MPI, MPI_Comm_connect(port, MPI_INFO_NULL, 0, pgroup, &pgroup_new);)"}
{"predict": "(9,OpenMP,#pragma omp parallel for)", "label": "(10, MPI, MPICHECK(MPI_Group_size(group, &groupSize));); (6, MPI, MPICHECK(MPI_Comm_group(comm, &group));); (13, MPI, MPICHECK(MPI_Group_translate_ranks(group, groupSize, ranks.data(), worldGroup, worldRanks.data()));); (13, MPI, MPICHECK(MPI_Group_free(&group));); (2, MPI, MPICHECK(MPI_Comm_group(MPI_COMM_WORLD, &worldGroup));); (12, MPI, MPICHECK(MPI_Group_free(&worldGroup));)"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (9,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (29,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for), (36,OpenMP,#pragma omp parallel for reduction(+:v)), (41,OpenMP,#pragma omp parallel for)", "label": "(33, OpenMP, #pragma omp taskloop simd private(argv[1])); (38, OpenMP, #pragma omp taskloop simd private(h)); (13, OpenMP, #pragma omp taskloop simd private(argc); (52, OpenMP, #pragma omp parallel shared(i)); (52, OpenMP, #pragma omp parallel private(i)); (1, OpenMP, #pragma omp taskloop simd private); (15, OpenMP, #pragma omp taskloop simd private(argc > 0 ? argv[1] : argv[2])); (38, OpenMP, #pragma omp parallel); (41, OpenMP, #pragma omp taskloop simd private(i)); (21, OpenMP, #pragma omp taskloop simd private(a, b)); (32, OpenMP, #pragma omp taskloop simd shared(i)); (13, OpenMP, #pragma omp taskloop simd private(argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (15, OpenMP, #pragma omp taskloop simd private(S1)); (5, OpenMP, #pragma omp taskloop simd private(argc,); (-5, OpenMP, #pragma omp taskloop simd private(); (-3, OpenMP, #pragma omp taskloop simd private()); (20, OpenMP, #pragma omp taskloop simd private(e, g, z)); (40, OpenMP, #pragma omp taskloop simd private(j))"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(6, OpenMP, std::vector<BabelFlow::Payload> comp_outputs( 1 );)"}
{"predict": "(3,OpenMP,#pragma omp parallel sections)", "label": "(43, MPI, printf(\" MPI library version      : %s\\n\", mpi_version());)"}
{"predict": "(10,OpenMP,#pragma omp parallel for reduction(&:&:workload->success))", "label": "(15, CUDA, checkCudaErrors(cudaFree(workload->d_data));); (15, CUDA, checkCudaErrors(cudaFreeHost(workload->h_data));); (3, CUDA, checkCudaErrors(cudaSetDevice(workload->cudaDeviceID));); (14, CUDA, checkCudaErrors(cudaStreamDestroy(workload->stream));)"}
{"predict": "(22,OpenMP,#pragma omp parallel for)", "label": "(9, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(23,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for), (43,OpenMP,#pragma omp parallel for)", "label": "(10, CUDA, int *d_num_out = thrust::raw_pointer_cast(num_out.data());); (10, CUDA, int *d_data = thrust::raw_pointer_cast(data.data());); (24, CUDA, d_tmp_storage = thrust::raw_pointer_cast(tmp_storage.data());); (32, CUDA, thrust::fill(flags.begin(), flags.begin() + num_selected, true);); (3, CUDA, thrust::device_vector<bool> flags(num_items);); (20, CUDA, thrust::device_vector<char> tmp_storage(tmp_storage_size);); (0, CUDA, thrust::device_vector<int> data(num_items);); (29, CUDA, thrust::fill(flags.begin() + num_selected, flags.end(), false);); (26, CUDA, thrust::sequence(data.begin(), data.end());); (28, CUDA, thrust::shuffle(flags.begin(), flags.end(), g);); (-5, CUDA, thrust::device_vector<int> num_out(1);); (36, CUDA, cudaMemcpy(&h_num_out, d_num_out, sizeof(int), cudaMemcpyDeviceToHost);); (0, CUDA, bool *d_flags = thrust::raw_pointer_cast(flags.data());)"}
{"predict": "(45,OpenMP,#pragma omp parallel for private(i) reduction(||:test_success)), (70,OpenMP,#pragma omp parallel for private(i) reduction(||:test_success))", "label": "(12, OpenCL, status = clSetKernelArg(kernel, 0, sizeof(cl_mem), &test_buf);); (20, OpenCL, status = clSetKernelArg(kernel, 0, sizeof(cl_mem), NULL);)"}
{"predict": "(34,OpenMP,#pragma omp parallel for), (66,OpenMP,#pragma omp parallel for)", "label": "(76, CUDA, checkCudaErrors(cudaEventRecord(stop, 0));); (15, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, cuda_device));); (94, CUDA, checkCudaErrors(cudaFree(d_partial_sums));); (57, CUDA, checkCudaErrors(cudaEventCreate(&start));); (57, CUDA, checkCudaErrors(cudaEventCreate(&stop));); (73, CUDA, checkCudaErrors(cudaEventElapsedTime(&inc, start, stop));); (66, CUDA, checkCudaErrors(cudaEventRecord(start, 0));); (70, CUDA, checkCudaErrors(cudaEventSynchronize(stop));); (86, CUDA, checkCudaErrors(cudaFreeHost(h_partial_sums));); (84, CUDA, checkCudaErrors(cudaFreeHost(h_result));); (55, CUDA, checkCudaErrors(cudaMalloc((void **)&d_data, sz));); (84, CUDA, checkCudaErrors(cudaFree(d_data));); (18, CUDA, checkCudaErrors(cudaMallocHost((void **)&h_result, sizeof(int)*n_elements));); (79, CUDA, checkCudaErrors(cudaFreeHost(h_data));); (56, CUDA, checkCudaErrors(cudaMemcpy(d_data, h_data, sz, cudaMemcpyHostToDevice));); (-4, CUDA, cuda_device = findCudaDevice(argc, (const char **)argv);); (65, CUDA, checkCudaErrors(cudaMemcpy(h_result, d_data, sz, cudaMemcpyDeviceToHost));); (50, CUDA, checkCudaErrors(cudaMemset(d_partial_sums, 0, partial_sz));); (11, CUDA, checkCudaErrors(cudaMallocHost((void **)&h_data, sizeof(int)*n_elements));); (-5, CUDA, checkCudaErrors(cudaGetDevice(&cuda_device));); (46, CUDA, checkCudaErrors(cudaMalloc((void **)&d_partial_sums, partial_sz));); (48, CUDA, checkCudaErrors(cudaMallocHost((void **)&h_partial_sums, partial_sz));)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(6, CUDA, RAFT_CUDA_TRY(cudaStreamSynchronize(stream));)"}
{"predict": "(2,OpenMP,#pragma omp parallel for), (3,OpenMP,#pragma omp parallel for), (4,OpenMP,#pragma omp parallel for), (5,OpenMP,#pragma omp parallel for), (6,OpenMP,#pragma omp parallel for), (7,OpenMP,#pragma omp parallel for), (8,OpenMP,#pragma omp parallel for), (9,OpenMP,#pragma omp parallel for), (10,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for)", "label": "(1, OpenMP, #pragma omp target teams defaultmap); (2, OpenMP, #pragma omp target teams defaultmap(); (9, OpenMP, #pragma omp target teams defaultmap(tofrom)); (12, OpenMP, #pragma omp target teams defaultmap(tofrom,); (3, OpenMP, #pragma omp target teams defaultmap(tofrom); (4, OpenMP, #pragma omp target teams defaultmap (tofrom:); (13, OpenMP, #pragma omp target teams defaultmap(tofrom, scalar); (-2, OpenMP, #pragma omp target teams defaultmap()); (5, OpenMP, #pragma omp target teams defaultmap(tofrom scalar)); (8, OpenMP, #pragma omp target teams defaultmap (scalar:)"}
{"predict": "(30,OpenMP,#pragma omp parallel for)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(32,OpenMP,#pragma omp parallel for private(rank))", "label": "(19, MPI, MPI_Comm_size(d->comm, &ranks_comm);); (28, MPI, MPI_Comm_rank(parent_comm, &parent_rank);); (28, MPI, MPI_Allgather(&parent_rank, 1, MPI_INT, ranklist, 1, MPI_INT, d->comm);)"}
{"predict": "(19,OpenMP,#pragma omp parallel for)", "label": "(52, CUDA, CHECK(cudaFree(d_y));); (30, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice));); (51, CUDA, CHECK(cudaFree(d_z));); (39, CUDA, CHECK(cudaMemcpy(h_z, d_z, M, cudaMemcpyDeviceToHost));); (22, CUDA, CHECK(cudaMalloc((void**)&d_x, M));); (22, CUDA, CHECK(cudaMalloc((void**)&d_y, M));); (45, CUDA, CHECK(cudaFree(d_x));); (21, CUDA, CHECK(cudaMalloc((void**)&d_z, M));); (30, CUDA, CHECK(cudaGetLastError());); (30, CUDA, CHECK(cudaDeviceSynchronize());); (22, CUDA, CHECK(cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice));)"}
{"predict": "(15,OpenMP,#pragma omp parallel for private(ljb, lsub, lnzval, len, len2) schedule(dynamic))", "label": "(32, MPI, MPI_Send(lnzval, len2, MPI_DOUBLE, 0, jb, grid3d->zscp.comm);); (36, MPI, MPI_Recv(lnzval, len2, MPI_DOUBLE, layer, jb, grid3d->zscp.comm, &status);)"}
{"predict": "(36,OpenMP,#pragma omp atomic write), (48,OpenMP,#pragma omp critical), (62,OpenMP,#pragma omp critical), (94,OpenMP,#pragma omp atomic read), (95,OpenMP,#pragma omp atomic read), (98,OpenMP,#pragma omp atomic write), (102,OpenMP,#pragma omp atomic read), (103,OpenMP,#pragma omp atomic read), (104,OpenMP,#pragma omp atomic write)", "label": "(76, OpenMP, #define OP_GOMP_CRITICAL_CPT(TYPE, OP, FLAG))"}
{"predict": "0;\n118:         for (int64_t r = 0; r < nelems_reduce; ++r) {\n119:             dims_t reduce_pos = off2dims_idx(reduce_dims, r);\n120:             const int64_t src_off = src_idle_off + md_off_v(src, reduce_pos.data());\n121:             const int64_t s8_comp_off = md_off_v(s8_comp, reduce_pos.data());\n122:             const int64_t zp_comp_off = md_off_v(zp_comp, reduce_pos.data());\n123:             const int32_t s8_comp_val = s8_comp.get_elem(s8_comp_off);\n124:             const int32_t zp_comp_val = zp_comp.get_elem(zp_comp_off);\n125:             const int32_t src_val = src.get_elem(src_off);\n126:             comp_val += s8_comp_val + zp_comp_val - src_val;\n127:         }\n128:         const int64_t dst_off = md_off_v(dst, idle_pos.data());\n129:         int32_t dst_val = dst.get_elem(dst_off);\n130:         dst_val += comp_val;\n131:         dst.set_elem(dst_off, dst_val);\n132:     });\n133: }", "label": "(101, OpenMP, dims_t comp_dims(ndims, 1);)"}
{"predict": "(3,OpenMP,#pragma omp parallel for), (3,OpenMP,for (int k = 0; k < 12; k++) { foo(); })", "label": "(24, OpenMP, #pragma omp target parallel device (3.14)); (9, OpenMP, #pragma omp target parallel device (argc))); (20, OpenMP, #pragma omp target parallel device (-10u)); (5, OpenMP, #pragma omp target parallel device (argc); (12, OpenMP, #pragma omp target parallel device (k), device (argc+1)); (-3, OpenMP, #pragma omp target parallel device); (6, OpenMP, #pragma omp target parallel device (argc > 0 ? argv[1] : argv[2])); (11, OpenMP, #pragma omp target parallel device (S1)); (12, OpenMP, #pragma omp target parallel device (-2)); (-3, OpenMP, #pragma omp target parallel device ()); (4, OpenMP, #pragma omp target parallel device (argc + argc)); (-7, OpenMP, #pragma omp target parallel device ()"}
{"predict": "(20,OpenMP,#pragma omp parallel for reduction(+:cache[cacheIndex]))", "label": "(1, CUDA, __shared__ float cache[threadsPerBlock];)"}
{"predict": "(19,OpenMP,#pragma omp parallel for)", "label": "(33, CUDA, cudaFree( dptrPosMass );); (33, CUDA, cudaFree( dptrForce );)"}
{"predict": "(30,OpenMP,#pragma omp parallel for), (48,OpenMP,#pragma omp single)", "label": "(63, OpenMP, double stop = omp_get_wtime();); (70, CUDA, CUDA_RT_CALL(cudaFree(a));); (18, CUDA, CUDA_RT_CALL(cudaDeviceSynchronize());); (10, CUDA, CUDA_RT_CALL(cudaMemset(a, 0, nx * ny * sizeof(float)));); (6, CUDA, CUDA_RT_CALL(cudaMalloc(&a, nx * ny * sizeof(float)));); (63, CUDA, CUDA_RT_CALL(cudaFree(l2_norm_d));); (13, CUDA, CUDA_RT_CALL(cudaGetLastError());); (32, CUDA, CUDA_RT_CALL(cudaMemset(l2_norm_d, 0, sizeof(float)));); (36, CUDA, CUDA_RT_CALL(cudaMemcpy(l2_norm_h, l2_norm_d, sizeof(float), cudaMemcpyDeviceToHost));); (5, CUDA, CUDA_RT_CALL(cudaMemset(a_new, 0, nx * ny * sizeof(float)));); (14, CUDA, CUDA_RT_CALL(cudaMallocHost(&l2_norm_h, sizeof(float)));); (54, CUDA, CUDA_RT_CALL(cudaMemcpy(a_ref_h, a, nx * ny * sizeof(float), cudaMemcpyDeviceToHost));); (58, CUDA, CUDA_RT_CALL(cudaFree(a_new));); (-2, CUDA, CUDA_RT_CALL(cudaMalloc(&a_new, nx * ny * sizeof(float)));); (22, OpenMP, double start = omp_get_wtime();); (8, CUDA, CUDA_RT_CALL(cudaMalloc(&l2_norm_d, sizeof(float)));); (51, CUDA, CUDA_RT_CALL(cudaFreeHost(l2_norm_h));)"}
{"predict": "(29,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for), (41,OpenMP,#pragma omp parallel for), (47,OpenMP,#pragma omp parallel for), (51,OpenMP,#pragma omp parallel for), (66,OpenMP,#pragma omp parallel for)", "label": "(20, MPI, MPI_Comm_size(comm, &size);); (64, MPI, MPI_Error_class(errval, &errclass);); (95, MPI, MPI_Ssend(NULL, 0, MPI_INT, dest, 10, comm);); (39, MPI, errval = MPI_Recv(NULL, 0, MPI_INT, src, 100, comm, MPI_STATUS_IGNORE);); (85, MPI, MPI_Ssend(NULL, 0, MPI_INT, dest, 100, comm);); (59, MPI, errval = MPI_Waitsome(2, r, &outcount, indices, s);); (87, MPI, MPI_Send(b1, 10, MPI_INT, dest, 0, comm);); (25, MPI, errval = MPI_Irecv(b2, 10, MPI_INT, src, 10, comm, &r[1]);); (44, MPI, errval = MPI_Recv(NULL, 0, MPI_INT, src, 10, comm, MPI_STATUS_IGNORE);); (16, MPI, MPI_Comm_set_errhandler(comm, MPI_ERRORS_RETURN);); (16, MPI, errval = MPI_Irecv(b1, 10, MPI_INT, src, 0, comm, &r[0]);); (8, MPI, MPI_Comm_rank(comm, &rank);); (82, MPI, MPI_Send(b2, 11, MPI_INT, dest, 10, comm);)"}
{"predict": "(4,OpenMP,#pragma omp parallel for reduction(+:sum)), (9,OpenMP,#pragma omp parallel for reduction(+:gsum))", "label": "(3, OpenMP, #pragma omp parallel for reduction(+:sum)); (8, OpenMP, #pragma omp target teams distribute parallel for reduction(+:gsum))"}
{"predict": "(20,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for private(ivalue))", "label": "(35, MPI, MPI_Start(&reqs[root]);); (20, MPI, MPI_Type_vector(n, 1, stride, MPI_DOUBLE, &vec);); (11, MPI, MPI_Info_create(&info);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (32, MPI, MPI_Wait(&reqs[root], MPI_STATUS_IGNORE);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (48, MPI, MPI_Type_free(&vec);); (15, MPI, MPI_Type_commit(&vec);); (39, MPI, MPI_Request_free(&reqs[root]);); (44, MPI, MPI_Info_free(&info);)"}
{"predict": ":                     CHECK( ex_caught );\n106:                 }\n107:                 SAY( \"Call a function, stub should return a error.\" );\n108:                 CHECK_TBB_IS_NOT_LOADED();\n109:             }\n110:         }\n111:     } __TBB_CATCH ( ... ) {\n112:         SAY( \"Unexpected exception caught.\" );\n113:     }; // __TBB_TRY\n114:     return 0;\n115: }", "label": "(172, TBB, CHECK( rtl1.status() == tbb::runtime_loader::ec_bad_ver );); (97, TBB, tbb::task_scheduler_init init( 1 );); (136, TBB, CHECK_EXCEPTION( eid_invalid_multiple_scheduling, tbb::invalid_multiple_scheduling );); (40, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_no_lib );); (8, TBB, tbb::runtime_loader rtl( tbb::runtime_loader::em_status );); (158, TBB, tbb::runtime_loader rtl1( tbb::runtime_loader::em_status );); (158, TBB, CHECK( rtl0.status() == tbb::runtime_loader::ec_ok );); (126, TBB, CHECK_EXCEPTION( eid_bad_last_alloc,              tbb::bad_last_alloc              );); (154, TBB, tbb::runtime_loader rtl0( tbb::runtime_loader::em_status );); (130, TBB, CHECK_EXCEPTION( eid_improper_lock,               tbb::improper_lock               );); (113, TBB, tbb::internal::throw_exception( tbb::internal::id );); (84, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_bad_call );); (96, TBB, tbb::internal::throw_exception( tbb::internal::exception_id( id ) );); (39, TBB, tbb::runtime_loader rtl( tbb::runtime_loader::em_throw );); (133, TBB, tbb::internal::handle_perror( EAGAIN, \"apple\" );); (-1, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_ok );); (37, TBB, tbb::runtime_loader::error_code code = tbb::runtime_loader::ec_ok;); (8, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_bad_arg );); (147, TBB, CHECK( rtl1.status() == tbb::runtime_loader::ec_ok );); (46, TBB, CHECK( code == tbb::runtime_loader::ec_no_lib );); (117, TBB, CHECK_EXCEPTION( eid_missing_wait,                tbb::missing_wait                );); (121, TBB, CHECK_EXCEPTION( eid_user_abort,                  tbb::user_abort                  );); (37, TBB, CHECK( code == tbb::runtime_loader::ec_bad_arg );)"}
{"predict": ":         }/*end for(l = 1; l <= logd2; l += 2)*/\n114:       }/*end for(i = 0; i < d1; i++)*/\n115:     }/*end for(k = 0; k < d3; k++)*/\n116:   }/*end target region*/\n117: #pragma omp target update from(gty1_real,gty1_imag,gty2_real,gty2_imag)\n118: }", "label": "(31, OpenMP, #pragma omp teams distribute collapse(2)); (47, OpenMP, #pragma omp parallel for collapse(2)); (34, OpenMP, #pragma omp parallel for)"}
{"predict": "(4,OpenMP,#pragma omp parallel for), (8,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for), (29,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for)", "label": "(20, OpenMP, #pragma omp distribute parallel for proc_bind(master); (4, OpenMP, #pragma omp distribute parallel for proc_bind); (13, OpenMP, #pragma omp distribute parallel for proc_bind()); (7, OpenMP, #pragma omp distribute parallel for proc_bind(); (26, OpenMP, #pragma omp distribute parallel for proc_bind(x)); (38, OpenMP, #pragma omp distribute parallel for proc_bind(spread)); (19, OpenMP, #pragma omp distribute parallel for proc_bind(close), proc_bind(spread)); (-4, OpenMP, #pragma omp target); (32, OpenMP, #pragma omp parallel proc_bind(close)); (27, OpenMP, #pragma omp distribute parallel for proc_bind(master)); (-6, OpenMP, #pragma omp teams)"}
{"predict": "(17,OpenMP,#pragma omp parallel for), (37,MPI,MPI_Bcast(cindex, kvtree_size(cindex), MPI_BYTE, 0, scr_storedesc_cntl->comm);)", "label": "(33, MPI, MPI_Bcast(&rc, 1, MPI_INT, 0, scr_storedesc_cntl->comm);)"}
{"predict": "(22,OpenMP,#pragma omp parallel for)", "label": "(69, OpenMP, #pragma omp cancellation point parallel); (66, OpenMP, #pragma omp cancellation point taskgroup); (60, OpenMP, #pragma omp cancel for); (57, OpenMP, #pragma omp cancel); (64, OpenMP, #pragma omp cancel parallel); (50, OpenMP, #pragma omp target parallel copyin(pvt)); (-5, OpenMP, #pragma omp target parallel); (-4, OpenMP, #pragma omp target parallel (); (53, OpenMP, #pragma omp cancellation point); (57, OpenMP, #pragma omp cancel taskgroup); (-1, OpenMP, #pragma omp target parallel )); (0, OpenMP, #pragma omp target parallel }); (4, OpenMP, #pragma omp target parallel unknown()); (50, OpenMP, #pragma omp cancellation point for); (-9, OpenMP, #pragma omp target parallel [); (49, OpenMP, #pragma omp cancel sections); (49, OpenMP, #pragma omp cancellation point sections); (-10, OpenMP, #pragma omp target parallel ])"}
{"predict": "(12,OpenMP,#pragma omp parallel for collapse(2)) (23,OpenMP,#pragma omp parallel for collapse(2))", "label": "(9, CUDA, cudaGetDeviceProperties( &prop, 0 );); (5, CUDA, cudaSetDeviceFlags( cudaDeviceMapHost );); (3, CUDA, cudaSetDevice( 0 );)"}
{"predict": "(50,OpenMP,#pragma omp parallel for)", "label": "(1, MPI, MPI_Init(&argc, &argv);); (61, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &P);)"}
{"predict": "(24,OpenMP,#pragma omp parallel for), (82,OpenMP,#pragma omp parallel for private(tData) reduction(||:found)), (89,OpenMP,#pragma omp critical), (92,OpenMP,#pragma omp critical)", "label": "(43, CUDA, setCommonICUDataPointer(&U_ICUDATA_ENTRY_POINT, FALSE, pErrorCode);)"}
{"predict": "(23,OpenMP,#pragma omp parallel for)", "label": "(3, TBB, tbb::task_arena big_arena(static_cast<int>(num_threads));); (28, TBB, int thread_id = tbb::this_task_arena::current_thread_index();); (0, TBB, tbb::global_control concurrency(tbb::global_control::max_allowed_parallelism, num_threads);)"}
{"predict": "(3,OpenMP,#pragma omp parallel for)\n(4,OpenMP,#pragma omp parallel for)\n(5,OpenMP,#pragma omp parallel for)\n(8,OpenMP,#pragma omp parallel for)\n(10,OpenMP,#pragma omp parallel for)\n(11,OpenMP,#pragma omp parallel for)\n(12,OpenMP,#pragma omp parallel for)\n(16,OpenMP,#pragma omp parallel for)\n(17,OpenMP,#pragma omp parallel for)\n(27,OpenMP,#pragma omp parallel for)", "label": "(13, OpenMP, #pragma omp parallel for simd collapse (ST); (34, OpenMP, #pragma omp parallel for simd collapse (N)); (20, OpenMP, #pragma omp parallel for simd collapse (foobool(argc)), collapse (true), collapse (-5)); (-1, OpenMP, #pragma omp parallel for simd collapse); (13, OpenMP, #pragma omp parallel for simd collapse ((ST > 0) ? 1 + ST : 2)); (19, OpenMP, #pragma omp parallel for simd collapse (S)); (9, OpenMP, #pragma omp parallel for simd collapse (1)); (-3, OpenMP, #pragma omp parallel for simd collapse (); (-2, OpenMP, #pragma omp parallel for simd collapse ()); (1, OpenMP, #pragma omp parallel for simd collapse (argc); (27, OpenMP, #pragma omp parallel for simd collapse (2))"}
{"predict": "(20,OpenMP,#pragma omp parallel for reduction(+:sum_total)) (28,OpenMP,#pragma omp parallel for reduction(+:result)) (39,OpenMP,#pragma omp parallel for reduction(+:sum_total)) (46,OpenMP,#pragma omp parallel for reduction(+:result))", "label": "(61, TBB, tbb::flow::make_edge( a1, tbb::flow::input_port<1>(a3) );); (58, TBB, tbb::flow::remove_edge( a2, s2 );); (60, TBB, tbb::flow::make_edge( a3, s2 );); (0, TBB, tbb::flow::function_node<int,int> c(g, tbb::flow::unlimited, cube_body());); (19, TBB, CHECK_MESSAGE( (&std::get<1>(a3.input_ports()) == &tbb::flow::input_port<1>(a3.j)), \"input_port1 of j not bound to input port 1 in composite_node a3\");); (25, TBB, tbb::flow::make_edge( c, tbb::flow::input_port<1>(a0) );); (21, TBB, tbb::flow::queue_node<int> q(g);); (25, TBB, tbb::flow::make_edge( c, tbb::flow::input_port<0>(a1) );); (27, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(a0), tbb::flow::input_port<0>(a2) );); (20, TBB, tbb::flow::make_edge( s, tbb::flow::input_port<0>(a0) );); (16, TBB, tbb::flow::function_node<int,int> s2(g, tbb::flow::unlimited, square_body());); (22, TBB, tbb::flow::make_edge( c, tbb::flow::input_port<1>(a1) );); (24, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(a1), tbb::flow::input_port<1>(a2) );); (9, TBB, CHECK_MESSAGE( (&std::get<0>(a3.input_ports()) == &tbb::flow::input_port<0>(a3.j)), \"input_port 0 of j not bound to input port 0 in composite_node a3\");); (3, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<0>(a2) == &tbb::flow::input_port<0>(a2.j)), \"input_port 0 of j not bound to input port 0 in composite_node a2\");); (24, TBB, tbb::flow::make_edge( s2, q );); (-15, TBB, tbb::flow::graph g;); (-15, TBB, tbb::flow::function_node<int,int> s(g, tbb::flow::unlimited, square_body());); (0, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<1>(a2) == &tbb::flow::input_port<1>(a2.j)), \"input_port 1 of j not bound to input port 1 in composite_node a2\");); (-12, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<0>(a0) == &tbb::flow::input_port<0>(a0.j)), \"input_port 0 of j not bound to input port 0 in composite_node a0\");); (18, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(a2), s2 );); (-12, TBB, CHECK_MESSAGE( (&tbb::flow::output_port<0>(a0) == &a0.f), \"f not bound to output port 0 in composite_node a0\");); (-18, TBB, tbb::flow::function_node<int,int> p(g, tbb::flow::unlimited, passthru_body());); (35, TBB, tbb::flow::remove_edge(s2, q);); (-11, TBB, CHECK_MESSAGE( (&std::get<1>(a0.input_ports()) == &tbb::flow::input_port<1>(a0.j)), \"input_port1 of j not bound to input port 1 in composite_node a1\");); (-17, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<1>(a0) == &tbb::flow::input_port<1>(a0.j)), \"input_port 1 of j not bound to input port 1 in composite_node a0\");); (-14, TBB, CHECK_MESSAGE( (&std::get<0>(a0.input_ports()) == &tbb::flow::input_port<0>(a0.j)), \"input_port 0 of j not bound to input port 0 in composite_node a1\");); (-8, TBB, CHECK_MESSAGE( (&tbb::flow::output_port<0>(a2) == &a2.f), \"f not bound to output port 0 in composite_node a2\");); (32, TBB, tbb::flow::make_edge( a0, a3 );)"}
{"predict": "(37,OpenMP,#pragma omp parallel for private(rng,dist))", "label": "(29, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(6,OpenMP,#pragma omp parallel for)", "label": "(10, TBB, test_extract<tbb::flow::rejecting>();); (10, TBB, test_extract<tbb::flow::queueing>();)"}
{"predict": "(35,OpenMP,#pragma omp parallel for)", "label": "(13, CUDA, checkCudaErrors(cudaEventCreate(&start));); (13, CUDA, checkCudaErrors(cudaEventCreate(&end));); (41, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (38, CUDA, checkCudaErrors(cudaEventRecord(end));); (35, CUDA, checkCudaErrors(cudaEventRecord(start));); (50, CUDA, checkCudaErrors(cudaEventElapsedTime(&elapsedTime, start, end));)"}
{"predict": "(4,OpenMP,#pragma omp parallel sections), (5,OpenMP,#pragma omp section), (6,OpenMP,#pragma omp section)", "label": "(5, OpenMP, #pragma omp parallel num_threads(2)); (1, OpenMP, omp_set_nested(1);); (1, OpenMP, omp_set_max_active_levels(1);)"}
{"predict": "(49,OpenMP,#pragma omp parallel for), (83,OpenMP,#pragma omp parallel for), (90,OpenMP,#pragma omp parallel for)", "label": "(3, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (73, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (40, CUDA, EXPECT_EQ(cudaSuccess, cudaMemset(outAccess->sampleData(0), 0xFA, outBufferSize));); (104, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamDestroy(stream));)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)\n(9,OpenMP,#pragma omp parallel for)\n(10,OpenMP,#pragma omp parallel for)\n(11,OpenMP,#pragma omp parallel for)\n(12,OpenMP,#pragma omp parallel for)\n(13,OpenMP,#pragma omp parallel for)\n(14,OpenMP,#pragma omp parallel for)\n(15,OpenMP,#pragma omp parallel for)\n(16,OpenMP,#pragma omp parallel for)\n(17,OpenMP,#pragma omp parallel for)\n(18,OpenMP,#pragma omp parallel for)\n(19,OpenMP,#pragma omp parallel for)\n(20,OpenMP,#pragma omp parallel for)\n(21,OpenMP,#pragma omp parallel for)\n(22,OpenMP,#pragma omp parallel for)\n(23,OpenMP,#pragma omp parallel for)\n(24,OpenMP,#pragma omp parallel for)\n(25,OpenMP,#pragma omp parallel for)\n(26,OpenMP,#pragma omp parallel for)\n(27,OpenMP,#pragma omp parallel for)", "label": "(25, OpenMP, #pragma omp target teams distribute parallel for simd shared (argv[1])); (28, OpenMP, #pragma omp target teams distribute parallel for simd shared(ca)); (19, OpenMP, #pragma omp target teams distribute parallel for simd shared (S1)); (42, OpenMP, #pragma omp target teams distribute parallel for simd shared(j)); (29, OpenMP, #pragma omp target teams distribute parallel for simd shared(e, g)); (34, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate(i)); (11, OpenMP, #pragma omp target teams distribute parallel for simd shared (argc > 0 ? argv[1] : argv[2])); (30, OpenMP, #pragma omp target teams distribute parallel for simd private(i), shared(i)); (7, OpenMP, #pragma omp target teams distribute parallel for simd shared (argc,); (22, OpenMP, #pragma omp target teams distribute parallel for simd shared(da)); (3, OpenMP, #pragma omp target teams distribute parallel for simd shared (argc); (24, OpenMP, #pragma omp target teams distribute parallel for simd shared(h, B::x)); (31, OpenMP, #pragma omp target teams distribute parallel for simd shared(i)); (28, OpenMP, #pragma omp target teams distribute parallel for simd private(i)); (-5, OpenMP, #pragma omp target teams distribute parallel for simd shared (); (12, OpenMP, #pragma omp target teams distribute parallel for simd shared(ba)); (-9, OpenMP, #pragma omp target teams distribute parallel for simd shared); (-6, OpenMP, #pragma omp target teams distribute parallel for simd shared ()); (1, OpenMP, #pragma omp target teams distribute parallel for simd shared (argc)); (4, OpenMP, #pragma omp target teams distribute parallel for simd shared (a, b, c, d, f, z))"}
{"predict": "(17,OpenMP,#pragma omp parallel for private(starts, an, dummy, x0, r23, r46, t23, t46, t1, t2, t3, t4, a1, a2, x1, x2, z, i))\n(29,OpenMP,#pragma omp for collapse(2) private(x0, r23, r46, t23, t46, t1, t2, t3, t4, a1, a2, x1, x2, z, i))", "label": "(65, OpenMP, #pragma omp target update to(u1_real, u1_imag))"}
{"predict": "(15,OpenMP,#pragma omp parallel for private(sum))", "label": "(3, CUDA, __shared__ float maxScore[FMC2H];); (4, CUDA, __shared__ int maxIndex[FMC2H];); (2, CUDA, __shared__ float maxScor2[FMC2H];); (-1, CUDA, __shared__ float siftPoint[128*FMC2H];)"}
{"predict": "The provided code is a function that checks and installs a boolean value from an MPI_Info object into an ADIO_File object. The function does not have any loops or sections that can be parallelized with OpenMP or MPI, as it performs a series of sequential operations. However, if this function were part of a larger loop or section of code that could be parallelized, here is an example of how you might insert a parallelization directive. Since there is no such loop in this specific function, I will provide a hypothetical example based on a loop that might be added:\n\nLet's assume there is a loop over multiple `ADIO_File` objects or multiple `MPI_Info` objects that can be parallelized. Here is an example of how you might parallelize such a loop with OpenMP:\n\n```c\n1: int ADIOI_Info_check_and_install_true(ADIO_File fd, MPI_Info info, const char *key,\n2:                                       int *local_cache, char *funcname, int *error_code)\n3: {\n4:     int flag, tmp_val, ret = 0;\n5:     char *value;\n6: \n7:     value = (char *) ADIOI_Malloc((MPI_MAX_INFO_VAL + 1) * sizeof(char));\n8:     if (value == NULL) {\n9:         *error_code = MPIO_Err_create_code(*error_code,\n10:                                            MPIR_ERR_RECOVERABLE,\n11:                                            funcname, __LINE__, MPI_ERR_OTHER, \"**nomem2\", 0);\n12:         return -1;\n13:     }\n14: \n15:     ADIOI_Info_get(info, key, MPI_MAX_INFO_VAL, value, &flag);\n16:     if (flag) {\n17:         if (!strcmp(value, \"true\") || !strcmp(value, \"TRUE\")) {\n18:             ADIOI_Info_set(fd->info, key, value);\n19:             *local_cache = 1;\n20:         } else if (!strcmp(value, \"false\") || !strcmp(value, \"FALSE\")) {\n21:             ADIOI_Info_set(fd->info, key, value);\n22:             *local_cache = 0;\n23:         }\n24:         tmp_val = *local_cache;\n25: \n26:         /* --BEGIN ERROR HANDLING-- */\n27:         if (tmp_val != *local_cache) {", "label": "(25, MPI, MPI_Bcast(&tmp_val, 1, MPI_INT, 0, fd->comm);)"}
{"predict": "(7,OpenMP,#pragma omp parallel sections)", "label": "(7, TBB, tbb::concurrent_unordered_set<int> static_init_time_set;)"}
{"predict": "(49,OpenMP,#pragma omp parallel for private(segsize, fnz, ucol, rel) schedule(static)), (58,OpenMP,#pragma omp parallel for private(rel) schedule(static))", "label": "(56, OpenMP, #pragma omp simd)"}
{"predict": "(46,OpenMP,#pragma omp parallel for), (102,OpenMP,#pragma omp parallel for reduction(+:cpu_result))", "label": "(132, CUDA, checkCudaErrors(cudaFree(d_idata));); (71, CUDA, checkCudaErrors(cudaMalloc((void **)&d_idata, bytes));); (131, CUDA, checkCudaErrors(cudaFree(d_odata));); (70, CUDA, checkCudaErrors(cudaMalloc((void **)&d_odata, numBlocks * sizeof(T)));)"}
{"predict": "part(A);\n132:         // Permute the matrix and the RHS vector:\n133:         A = *product(P, *product(A, P));\n134:         rhs = *product(P, rhs);\n135:         prof.toc(\"partition\");\n136:     }\n137: #endif\n138: \n139:     // Create the solver:\n140:     prof.tic(\"setup\");\n141:     Solver solve(world, A, prm, bprm);\n142:     prof.toc(\"setup\");\n143: \n144:     // Solve the system:\n145:     prof.tic(\"solve\");\n146:     vex::vector<dvec_type> x(ctx, chunk / B);\n147:     solve(A, rhs, x);\n148:     prof.toc(\"solve\");\n149: \n150:     // Print the number of iterations and the final residual:\n151:     if (world.rank == 0) {\n152:         std::cout << \"Iterations: \" << solve.iterations() << std::endl\n153:                   << \"Error:      \" << solve.error()      << std::endl;\n154:     }\n155: \n156:     return 0;\n157: }", "label": "(17, MPI, MPI_Barrier(world);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for collapse(2))", "label": "(11, OpenMP, #pragma omp parallel for num_threads(N_THREADS))"}
{"predict": "beamIdx] * nMSL + i;\n116:             bh.outputIds[dst] = bh.outputIdsCBA[src];\n117:             bh.logProbs[dst] = bh.logProbsCBA[src];\n118:         }\n119:     }\n120:     __syncthreads();\n121: }", "label": "(15, CUDA, extern __shared__ char smem[];)"}
{"predict": "(35,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for)", "label": "(31, OpenCL, retVal = clEnqueueNDRangeKernel(pCmdQ, multiDeviceKernel.get(), 1, nullptr, globalWorkSize, nullptr, 0, nullptr, nullptr);); (20, OpenCL, retVal = clEnqueueMemcpyINTEL(pCmdQ, true, compressedDeviceMemAlloc1, bufferData, bufferSize, 0, nullptr, nullptr);); (14, OpenCL, retVal = clEnqueueMemcpyINTEL(pCmdQ, true, compressedDeviceMemAlloc2, bufferData, bufferSize, 0, nullptr, nullptr);); (37, OpenCL, retVal = clEnqueueMemcpyINTEL(pCmdQ, true, uncompressibleHostMemAlloc, compressedDeviceMemAlloc2, bufferSize, 0, nullptr, nullptr);)"}
{"predict": "(42,OpenMP,#pragma omp parallel for private(tmp_rc)), (59,OpenMP,#pragma omp parallel for private(tmp_rc)), (66,OpenMP,#pragma omp parallel for)", "label": "(77, MPI, mfu_copy_stats.wtime_ended = MPI_Wtime();); (12, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (24, MPI, mfu_copy_stats.wtime_started = MPI_Wtime();); (89, MPI, MPI_Allreduce(values, sums, 3, MPI_INT64_T, MPI_SUM, MPI_COMM_WORLD);); (120, MPI, MPI_Allreduce(&rc, &all_rc, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);)"}
{"predict": "; i < shape.Y_len; ++i) { \\\n153:             y[i] = BIAS + \\\n154:                    static_cast<ST>(rand_r(&seed)) / \\\n155:                    static_cast<ST>(RAND_MAX); \\\n156:         } \\\n157:         \\\n158:         z = static_cast<DT *>(malloc(sizeof(DT) * shape.Z_len)); \\\n159:         for (int i = 0; i < shape.Z_len; ++i) { \\\n160:             z[i] = BIAS + \\\n161:                    static_cast<DT>(rand_r(&seed)) / \\\n162:                    static_cast<DT>(RAND_MAX); \\\n163:         } \\\n164:         \\\n165:         float alpha = static_cast<float>(rand_r(&seed)) / \\\n166:                       static_cast<float>(RAND_MAX); \\\n167:         float beta = static_cast<float>(rand_r(&seed)) / \\\n168:                      static_cast<float>(RAND_MAX); \\\n169:         \\\n170:         EXT_TYPE ext = EXT_VAL; \\\n171:         \\\n172:         ST *x_d, *y_d; \\\n173:         DT *z_d; \\\n174:         cudaMalloc(&x_d, sizeof(ST) * shape.X_len); \\\n175:         cudaMemcpy(x_d, x, sizeof(ST) * shape.X_len, cudaMemcpyHostToDevice); \\\n176:         cudaMalloc(&y_d, sizeof(ST) * shape.Y_len); \\\n177:         cudaMemcpy(y_d, y, sizeof(ST) * shape.Y_len, cudaMemcpyHostToDevice); \\\n178:         cudaMalloc(&z_d, sizeof(DT) * shape.Z_len); \\\n179:         cudaMemcpy(z_d, z, sizeof(DT) * shape.Z_len, cudaMemcpyHostToDevice); \\\n180:         \\\n181:         void *ext_d; \\\n182:         cudaMalloc(&ext_d, sizeof(EXT_TYPE)); \\\n183:         cudaMemcpy(ext_d, &ext, sizeof(EXT_TYPE), cudaMemcpyHostToDevice); \\\n184:         \\\n185:         void *alpha_d; \\\n186:         cudaMalloc(&alpha_d, sizeof(float)); \\\n18", "label": "(169, CUDA, cudaMemcpy(z_cu, z, shape.Z_len * sizeof(DT), cudaMemcpyHostToDevice);); (261, CUDA, cudaFree(x_cu);); (162, CUDA, cudaMalloc(&x_cu, shape.X_len * sizeof(ST));); (165, CUDA, cudaMemcpy(y_cu, y, shape.Y_len * sizeof(ST), cudaMemcpyHostToDevice);); (163, CUDA, cudaMemcpy(x_cu, x, shape.X_len * sizeof(ST), cudaMemcpyHostToDevice);); (258, CUDA, cudaFree(y_cu);); (160, CUDA, cudaMalloc(&z_cu, shape.Z_len * sizeof(DT));); (158, CUDA, cudaMalloc(&y_cu, shape.Y_len * sizeof(ST));); (168, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (260, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (254, CUDA, cudaFree(z_cu);)"}
{"predict": "(16,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for)", "label": "(36, MPI, MPI_Start(&req);); (13, MPI, MPI_Comm_size(comm, &size);); (28, MPI, MPI_Info_create(&info);); (34, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (48, MPI, MPI_Info_free(&info);); (45, MPI, MPI_Request_free(&req);); (27, MPI, MPI_Alltoall_init(sbuf, count, MPI_INT, rbuf, count, MPI_INT, comm, info, &req);); (8, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(22,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for)", "label": "(30, TBB, result_arena = new tbb::task_arena(tmp);); (18, TBB, result_arena = new tbb::task_arena( arena_args... );); (12, TBB, tbb::task_arena* result_arena = nullptr;); (26, TBB, tbb::task_arena tmp(dummy_max_concurrency, dummy_reserved_for_masters);); (19, TBB, result_arena = new tbb::task_arena;)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(4, TBB, using ch_map = tbb::concurrent_hash_map<int,int>;)"}
{"predict": "(16,OpenMP,#pragma omp parallel for)", "label": "(32, MPI, rc = MPI_Type_get_true_extent(obj->DTP_datatype, &true_lb, &true_extent);); (22, MPI, rc = MPI_Type_commit(&obj->DTP_datatype);); (32, MPI, rc = MPI_Type_get_extent(obj->DTP_datatype, &lb, &extent);)"}
{"predict": "(13, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (36, OpenMP, #pragma omp parallel for private(err, status) reduction(+:errs));", "label": "(73, MPI, err = MPI_Wait(&req, &status);); (13, MPI, MPI_Comm_size(comm, &size);); (34, MPI, MPI_Comm_set_errhandler(comm, MPI_ERRORS_RETURN);); (10, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(5,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for)", "label": "(2, OpenMP, #pragma omp parallel); (4, OpenMP, #pragma omp parallel master taskloop)"}
{"predict": "axl_state_file = NULL;\n146:   if (scr_poststage_enabled) {\n147:     axl_state_file = strdup(scr_poststage_state_file);\n148:   }\n149: \n150:   /* perform the actual file transfers */\n151:   if (scr_flush_transfer_files(xfer_type, numfiles, src_filelist, dst_filelist, axl_state_file) != SCR_SUCCESS) {\n152:     if (scr_my_rank_world == 0) {\n153:       scr_err(\"scr_flush_async_start: Failed to transfer files @ %s:%d\",\n154:         __FILE__, __LINE__\n155:       );\n156:       if (scr_log_enable) {\n157:         double time_start_orig;\n158:         kvtree_util_get_double(dset_hash, ASYNC_KEY_OUT_WTIME, &time_start_orig);\n159:         double time_diff = time_end - time_start_orig;\n160:         scr_log_event(\"ASYNC_FLUSH_FAIL\", \"Failed to transfer files\",\n161:                       &id, dset_name, NULL, &time_diff);\n162:       }\n163:     }\n164:     kvtree_util_set_int(dset_hash, ASYNC_KEY_OUT_STATUS, SCR_FAILURE);\n165:     scr_dataset_delete(&dataset);\n166:     scr_flush_list_free(numfiles, &src_filelist, &dst_filelist);\n167:     scr_free(&axl_state_file);\n168:     return SCR_FAILURE;\n169:   }\n170: \n171:   /* free memory allocated for lists of source and destination paths */\n172:   scr_flush_list_free(numfiles, &src_filelist, &dst_filelist);\n173:   scr_free(&axl_state_file);\n174: \n175:   /* mark that we've completed the flush */\n176:   scr_flush_file_location_set(id, SCR_FLUSH_KEY_LOCATION_FLUSHED);\n177: \n178:   /* stop timer */\n179:   double time_end = 0.0;\n180:   if (scr_my_rank_world == 0) {\n181:     time_end = scr_log_seconds();\n182:     double time_diff = time", "label": "(24, MPI, MPI_Barrier(scr_comm_world);); (60, MPI, double time_end = MPI_Wtime();); (35, MPI, time_start = MPI_Wtime();)"}
{"predict": "(16,OpenMP,#pragma omp parallel for)", "label": "(9, OpenMP, #pragma omp target data map(tofrom: i) use_device_ptr(k) use_device_addr(i, j)); (13, OpenMP, #pragma omp target data map(tofrom: i) use_device_ptr(z) use_device_addr(i, j, k[:i]))"}
{"predict": "(23,OpenMP,#pragma omp parallel), (52,OpenMP,#pragma omp parallel for reduction(+:counter)), (53,OpenMP,#pragma omp parallel for reduction(+:counter))", "label": "(2, MPI, MPI_Init(&argc, &argv);); (86, MPI, MPI_Finalize();); (23, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&rank);); (61, MPI, MPI_Barrier(MPI_COMM_WORLD);); (59, MPI, MPI_Reduce(&counter, &totalNum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);)"}
{"predict": "(64,OpenMP,#pragma omp parallel for)", "label": "(67, OpenCL, clReleaseMemObject(mem[i]);)"}
{"predict": "::vfs::directory_iterator I = FS.dir_begin(LibDevicePath, EC),\n130:            E = FS.dir_begin(); EC ? I = E : I != E; ++I) {\n131:         if (I->isDirectory())\n132:           continue;\n133:         StringRef Filename = I->path();\n134:         StringRef Extension = llvm::sys::path::extension(Filename);\n135:         if (Extension != \".bc\")\n136:           continue;\n137:         StringRef BaseName = llvm::sys::path::stem(Filename);\n138:         if (BaseName.startswith(\"libdevice\"))\n139:           continue;\n140:         if (BaseName.startswith(\"crt\"))\n141:           continue;\n142:         if (BaseName.startswith(\"fatbinary\"))\n143:           continue;\n144:         if (BaseName.startswith(\"nvvm\"))\n145:           continue;\n146:         if (BaseName.startswith(\"sm_\")) {\n147:           std::string GpuArchName = BaseName.substr(3);\n148:           LibDeviceMap[GpuArchName] = I->path();\n149:         }\n150:       }\n151:     }\n152:     break;\n153:   }\n154: }", "label": "(123, CUDA, std::string GpuArchName(CudaArchToString(GpuArch));)"}
{"predict": "(21,OpenMP,#pragma omp parallel for), (21,OpenMP,initMatrixSigned(src_host, size, low, high, seed);)", "label": "(19, CUDA, cudaMallocHost(&tar_host, size * sizeof(float));); (17, CUDA, cudaMallocHost(&src_host, size * sizeof(float));)"}
{"predict": "(76,OpenMP,#pragma omp parallel for)", "label": "(63, OpenMP, #define OP_GOMP_CRITICAL_READ(OP, FLAG))"}
{"predict": "(3,OpenMP,#pragma omp parallel for)", "label": "(2, OpenMP, omp_autotune(self, OMP_SCALE);)"}
{"predict": "(4,OpenMP,#pragma omp parallel for collapse(2))", "label": "(3, OpenMP, #pragma omp parallel num_threads(2)); (12, OpenMP, #pragma omp ordered depend(source)); (7, OpenMP, #pragma omp ordered depend(sink : i - 1, j) depend(sink : i, j - 1)); (1, OpenMP, #pragma omp for ordered(2))"}
{"predict": "(4,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for collapse(2))", "label": "(3, OpenMP, #pragma omp parallel); (3, OpenMP, #pragma omp taskloop)"}
{"predict": "(7,OpenMP,#pragma omp parallel sections), (8,OpenMP,#pragma omp section), (9,OpenMP,#pragma omp section)", "label": "(9, MPI, MPI_Wait(&request, &status);); (6, MPI, MPI_Irecv(p->buff1, p->bufflen, MPI_BYTE, p->prot.nbor, 1, MPI_COMM_WORLD, &request);); (6, MPI, MPI_Send(p->buff, p->bufflen, MPI_BYTE, p->prot.nbor, 1, MPI_COMM_WORLD);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(16, OpenMP, omp_target_associate_ptr(p_a, d_a, DataSize, 0, omp_get_default_device());); (21, OpenMP, omp_target_free(d_a, omp_get_default_device());); (12, OpenMP, d_a = (int *)omp_target_alloc(DataSize, omp_get_default_device());); (17, OpenMP, omp_target_disassociate_ptr(p_a, omp_get_default_device());)"}
{"predict": "(31,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for), (42,OpenMP,#pragma omp parallel for)", "label": "(50, OpenMP, EXPECT_TRUE(check_test_image(output_file, t_apcomp_baseline_dir()));); (9, MPI, MPI_Comm_rank(comm, &par_rank);); (10, MPI, apcomp::mpi_comm(MPI_Comm_c2f(comm));); (8, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(23,OpenMP,#pragma omp parallel sections), (24,OpenMP,#pragma omp section), (26,OpenMP,#pragma omp section), (29,OpenMP,#pragma omp section), (31,OpenMP,#pragma omp section), (33,OpenMP,#pragma omp section), (36,OpenMP,#pragma omp section)", "label": "(27, MPI, MPI_Wait(&request, &status);); (40, MPI, MPI_Waitall(1, &request, &status);); (16, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (31, MPI, MPI_Waitsome(1, &request, &outcount, indices, &status);); (19, MPI, MPI_Grequest_start(query_fn, free_fn, cancel_fn, NULL, &request);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for reduction(+:num_failed))", "label": "(6, OpenMP, omp_set_dynamic(0);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for collapse(2))", "label": "(14, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize());); (12, CUDA, qmc_cuda::cuda_check(cudaGetLastError());)"}
{"predict": "(11,OpenMP,#pragma omp parallel for private(v))", "label": "(6, TBB, tbb::flow::queue_node< q3_input_type >  q3(g);); (7, TBB, tbb::flow::make_edge( q3, my_split );)"}
{"predict": "ange[rand() % 4];\n95: \n96:       ave_diff_subc += TestGemmA16W8_New<half, int8_t>(BS, M, N, K, GroupSize);\n97:     }\n98:     ave_diff_subc /= TestCases;\n99:     printf(\"Volta Fused GEMM Ave_Diff_Subc : %f\\n\", ave_diff_subc);\n100:    EXPECT_EQ(ave_diff_subc <= 5e-1, true);\n101:  }\n102:}", "label": "(3, CUDA, cudaGetDevice(&device_id);); (3, CUDA, cudaGetDeviceProperties(&device_prop, device_id);)"}
{"predict": "(3,OpenMP,#pragma omp parallel for), (7,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for)", "label": "(3, OpenMP, #pragma omp atomic)"}
{"predict": "(15,OpenMP,#pragma omp parallel for private(position))", "label": "(18, MPI, timers[t] = MPI_Wtime();); (22, MPI, timers[t] = (MPI_Wtime() - timers[t]) / cycles;); (11, MPI, MPI_Pack(sbuf, scount, sdt, packed_buf, outsize, &position, MPI_COMM_WORLD);); (2, MPI, MPI_Type_size(sdt, &outsize);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myself);)"}
{"predict": ".kmp_depend_info, %struct.kmp_depend_info* %{{.+}}, i[[SZ]] 1\n63:   // CHECK:       getelementptr %struct.kmp_depend_info, %struct.kmp_depend_info* %{{.+}}, i[[SZ]] 2\n64:   // CHECK:       getelementptr %struct.kmp_depend_info, %struct.kmp_depend_info* %{{.+}}, i[[SZ]] 3\n65:   // CHECK:       [[DEP:%.+]] = bitcast %struct.kmp_depend_info* %{{.+}} to i8*\n66:   // CHECK:       call void @__kmpc_omp_wait_deps(%struct.ident_t* @{{.*}}, i32 [[GTID]], i32 4, i8* [[DEP]], i32 0, i8* null)\n67:   // CHECK:       call void @__kmpc_omp_target_task_begin_if0(%struct.ident_t* @{{.*}}, i32 [[GTID]], i8* [[TASK]], i64 [[DEV2]])\n68:   // CHECK:       call i32 [[TASK_ENTRY1_]](i32 [[GTID]], [[TASK_TY1_]]* [[BC_TASK]])\n69:   // CHECK:       call void @__kmpc_omp_target_task_complete_if0(%struct.ident_t* @{{.*}}, i32 [[GTID]], i8* [[TASK]])\n70:                                                    : device(0) depend(out \\\n71:                                                                     : b, bn, cn[3])\n72:   {\n73:   }\n74: \n75:   // CHECK:       [[ADD:%.+]] = add nsw i32\n76:   // CHECK:       store i32 [[ADD]], i32* [[DEVICE_CAP:%.+]],\n77: \n78:   // CHECK:       [[BOOL:%.+]] = icmp ne i32 %{{.+}}, 0\n79:   // CHECK:       br i1 [[BOOL]], label %[[THEN:.+]], label %[[ELSE:.+]]\n80:   // CHECK:       [[THEN]]:\n81:   // CHECK-DAG:   [[BPADDR0:%.+]] = getelementptr inbounds [2 x i8*], [2 x i8*]* [[BP", "label": "(86, OpenMP, #pragma omp target teams device(global + a) nowait depend(inout); (25, OpenMP, #pragma omp target teams device(global + a) depend(in); (100, OpenMP, #pragma omp target teams if (0) firstprivate(global) depend(out)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(16, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize(), \"batched_dot_wabn_wban\");); (14, CUDA, qmc_cuda::cuda_check(cudaGetLastError(), \"batched_dot_wabn_wban\");)"}
{"predict": "MMA,MMA_M,MMA_N)\n94: \n95:   // Initialize the accumulators\n96:   clear(tCrC);\n97: \n98:   //\n99:   // Main loop over K tiles\n100: //\n101: for (int k_tile = 0; k_tile < size<2>(cta_tiler); ++k_tile) {\n102:   // Load next A and B tiles into registers\n103:   if (k_tile != 0) {\n104:     copy(copy_a, tAgA(_,_,_,k_tile), tArA);\n105:     copy(copy_b, tBgB(_,_,_,k_tile), tBrB);\n106:   }\n107: \n108:   // Perform MMA\n109:   mma(tCsA(_,_,_), tCsB(_,_,_), tCrC(_,_,_));\n110: }\n111: \n112: // Accumulate into C\n113: if (beta != Beta{}) {\n114:   copy(mma, tCgC(_,_,_), tCrC(_,_,_));\n115: }\n116: \n117: if (beta == Beta{}) {\n118:   copy(mma, tCgC(_,_,_), alpha * tCrC(_,_,_));\n119: }\n120: else {\n121:   copy(mma, tCgC(_,_,_), alpha * tCrC(_,_,_) + beta * tCgC(_,_,_));\n122: }\n123: }", "label": "(48, CUDA, __shared__ TB smemB[cosize_v<BSmemLayout>];); (46, CUDA, __shared__ TA smemA[cosize_v<ASmemLayout>];)"}
{"predict": "(33,OpenMP,#pragma omp parallel for)", "label": "(13, CUDA, checkCudaErrors(cudaMallocPitch((void **)&src, &DeviceStride, Size.width * sizeof(float), Size.height));); (79, CUDA, checkCudaErrors(cudaFree(src));); (12, CUDA, checkCudaErrors(cudaMallocPitch((void **)&dst, &DeviceStride, Size.width * sizeof(float), Size.height));); (35, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (40, CUDA, getLastCudaError(\"Kernel execution failed\");); (74, CUDA, checkCudaErrors(cudaFree(dst));)"}
{"predict": "(15,OpenMP,#pragma omp parallel for)", "label": "(14, OpenMP, #pragma omp master taskloop shared(c) num_tasks(a))"}
{"predict": "(56,OpenMP,#pragma omp parallel for), (82,OpenMP,#pragma omp parallel sections)", "label": "(35, CUDA, __shared__ _VOLATILE_ float bSmem[16][128+4];); (33, CUDA, __shared__ _VOLATILE_ float aSmem[16][128+4];)"}
{"predict": "(4,OpenMP,#pragma omp parallel for), (10,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for)", "label": "(4, OpenMP, #pragma omp master taskloop simd); (1, OpenMP, #pragma omp parallel)"}
{"predict": "(21,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for)", "label": "(28, CUDA, cudaMalloc((void**)&deviceC, size);); (55, CUDA, cudaFree(deviceB);); (40, CUDA, cudaMemcpy(hostC, deviceC, size, cudaMemcpyDeviceToHost);); (54, CUDA, cudaFree(deviceC);); (51, CUDA, cudaFree(deviceA);); (22, CUDA, cudaMalloc((void**)&deviceB, size);); (25, CUDA, cudaMemcpy(deviceA, hostA, size, cudaMemcpyHostToDevice);); (25, CUDA, cudaMemcpy(deviceB, hostB, size, cudaMemcpyHostToDevice);); (18, CUDA, cudaMalloc((void**)&deviceA, size);)"}
{"predict": "(106,OpenMP,#pragma omp parallel for), (111,OpenMP,#pragma omp parallel for)", "label": "(77, MPI, MPI_Comm_size(fd->comm, &nprocs);); (77, MPI, MPI_Comm_rank(fd->comm, &myrank);)"}
{"predict": "(15,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for)", "label": "(11, OpenMP, ref_sched = str2omp_sched(argv[2]);); (13, OpenMP, omp_get_schedule(&sched, &chunk);)"}
{"predict": "(6,OpenMP,#pragma omp parallel for)", "label": "(7, CUDA, cudaDeviceSynchronize();); (7, CUDA, getLastCudaError(\"launch_kernel failed\");)"}
{"predict": "(5,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for)", "label": "(6, TBB, tbb::global_control global_limit(tbb::global_control::max_allowed_parallelism, idx);)"}
{"predict": "(labels, label);\n98: \n99:             dst[gc] = label;\n100:         }\n101: \n102:         __syncthreads();\n103:     }\n104: }\n\n### ** Analysis and Parallelization Suggestions **\n\nThe provided code is a CUDA kernel designed to process 3D blocks of data. It is already parallelized using CUDA's thread and block mechanisms. However, there are some opportunities for further optimization or parallelization within the kernel, particularly in loops that are not inherently parallelized by CUDA's grid and block structure.\n\nThe `for (gc.w = 0; gc.w < shape.w; gc.w++)` loop is a good candidate for parallelization. However, since this is already part of a CUDA kernel, the parallelization is handled by the grid and block dimensions. Instead, we can look at parallelizing the logic within the loop if there are independent operations that can be executed in parallel.\n\nGiven the nature of the code, the main parallelization is already managed by CUDA. However, we can add OpenMP pragmas if this code were to be executed on a CPU with OpenMP support, but that is not the case here. Therefore, the main focus is on ensuring that the CUDA parallelization is optimal.\n\nHowever, for educational purposes, let's assume we are converting this to a CPU version and want to parallelize it using OpenMP. Here are the suggested parallelization points:\n\n(16,OpenMP,#pragma omp parallel for private(nzm1yx, nzym1x, nzyxm1, nzym1xm1, nzm1yxm1, nzm1ym1x, label) collapse(3))\n\nThis OpenMP pragma would parallelize the `for (gc.w = 0; gc.w < shape.w; gc.w++)` loop, assuming `gc.w`, `gc.x`, `gc.y`, and `gc.z` are independent iterations. However, since this is a CUDA kernel, the parallelization is already handled by the grid and block structure.\n\n### ** Final Output **\n\nSince the code is already optimized for CUDA, no additional parallelization instructions are needed. However, for educational purposes, if we were to convert this to a CPU version and use OpenMP, the suggestion would be:\n\n(16,OpenMP,#pragma omp parallel for private(nzm1yx, nzym1x, nzyxm1, nzym1xm1, nzm1", "label": "(5, CUDA, __shared__ DT labels[BW * BH * BD];)"}
{"predict": "(6,OpenMP,#pragma omp parallel sections), (7,OpenMP,#pragma omp section), (9,OpenMP,#pragma omp section)", "label": "(9, CUDA, CUDA_CHECK(ctx->device_id, cudaMalloc(&ctx->d_rx_dataset, dataset_size));); (15, CUDA, CUDA_CHECK(ctx->device_id, cudaMalloc(&ctx->d_rx_vm_states, batch_size * 2560));); (4, CUDA, CUDA_CHECK(ctx->device_id, cudaHostGetDevicePointer(&ctx->d_rx_dataset, const_cast<void *>(dataset), 0));); (7, CUDA, CUDA_CHECK(ctx->device_id, cudaMemcpy(ctx->d_rx_dataset, dataset, dataset_size, cudaMemcpyHostToDevice));); (11, CUDA, CUDA_CHECK(ctx->device_id, cudaMalloc(&ctx->d_rx_entropy, batch_size * (128 + 2560)));); (12, CUDA, CUDA_CHECK(ctx->device_id, cudaMalloc(&ctx->d_rx_rounding, batch_size * sizeof(uint32_t)));); (8, CUDA, CUDA_CHECK(ctx->device_id, cudaMalloc(&ctx->d_rx_hashes, batch_size * 64));); (6, CUDA, CUDA_CHECK(ctx->device_id, cudaMalloc(&ctx->d_long_state, ctx->d_scratchpads_size));)"}
{"predict": "(32,OpenMP,#pragma omp parallel for)", "label": "(39, CUDA, cudaFree(res_dev);); (32, CUDA, cudaMalloc(&res_dev, sizeof(float));); (34, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for)", "label": "(16, MPI, MPI_Error_string(err, msg, &len);); (8, MPI, err = MPI_Probe(-80, 1, MPI_COMM_WORLD, &status);); (5, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(36,OpenMP,#pragma omp parallel for), (53,OpenMP,#pragma omp parallel for), (66,OpenMP,#pragma omp parallel for), (94,OpenMP,#pragma omp parallel for)", "label": "(69, OpenMP, #pragma omp target map(tofrom : conv)); (87, OpenMP, elapsed_time = omp_get_wtime() - start_time;); (53, OpenMP, #pragma omp teams distribute parallel for simd); (83, OpenMP, #pragma omp target exit data map(from : xold[0 : Ndim], xnew[0 : Ndim])); (37, OpenMP, start_time = omp_get_wtime();); (43, OpenMP, #pragma omp target enter data map(to : xold[0 : Ndim], xnew[0 : Ndim],); (48, OpenMP, #pragma omp target); (63, OpenMP, #pragma omp teams distribute parallel for simd reduction(+ : conv))"}
{"predict": "(15,OpenMP,#pragma omp parallel for collapse(2))", "label": "(16, CUDA, CUDA_CHECK(cudaGetLastError());)"}
{"predict": "156:     cleanUp(&resourceList);\n157:     return 1;\n158:   }\n159: \n160:   attribute.outputTensorDesc = outputTensorDesc;\n161:   err = cudlaModuleGetAttributes(moduleHandle, CUDLA_OUTPUT_TENSOR_DESCRIPTORS,\n162:                                  &attribute);\n163:   if (err != cudlaSuccess) {\n164:     DPRINTF(\"Error in getting output tensor descriptor = %d\\n\", err);\n165:     cleanUp(&resourceList);\n166:     return 1;\n167:   }\n168: \n169:   // Prepare input data.\n170:   for (int i = 0; i < numInputTensors; i++) {\n171:     cudlaTensor tensor;\n172:     cudlaTensorDesc tensorDesc;\n173:     cudlaMemoryHandle memHandle;\n174:     cudlaDataType dataType = inputTensorDesc[i].dataType;\n175:     cudlaTensorFormat tensorFormat = inputTensorDesc[i].tensorFormat;\n176:     cudlaShape shape = inputTensorDesc[i].shape;\n177:     cudlaTensorCreate(&tensorDesc, dataType, tensorFormat, shape);\n178:     cudlaTensorAllocate(devHandle, &tensorDesc, &tensor, &memHandle);\n179:     cudlaTensorCopyFromHost(devHandle, &tensor, loadableData, 0);\n180:     resourceList.inputTensors[i] = tensor;\n181:     resourceList.inputMemHandles[i] = memHandle;\n182:     cudlaTensorDescDestroy(&tensorDesc);\n183:   }\n184: \n185:   // Prepare output data.\n186:   for (int i = 0; i < numOutputTensors; i++) {\n187:     cudlaTensor tensor;\n188:     cudlaTensorDesc tensorDesc;\n189:     cudlaMemoryHandle memHandle;\n190:     cudlaDataType dataType = outputTensorDesc[i].dataType;\n191:     cudlaTensorFormat tensorFormat = outputTensorDesc[i].tensorFormat;\n192:     cudlaShape shape = outputTensorDesc[i].shape;\n193", "label": "(55, CUDA, result = cudaFree(0);); (56, CUDA, errPtr = cudaGetErrorName(result);); (320, CUDA, cudaFree(outputBufferGPU);); (318, CUDA, cudaFree(inputBufferGPU);); (205, CUDA, result = cudaMalloc(&inputBufferGPU, inputTensorDesc[0].size);); (88, CUDA, result = cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking);); (56, CUDA, result = cudaSetDevice(0);); (211, CUDA, result = cudaMalloc(&outputBufferGPU, outputTensorDesc[0].size);); (324, CUDA, result = cudaStreamDestroy(stream);); (282, CUDA, result = cudaStreamSynchronize(stream);)"}
{"predict": "(48,OpenMP,#pragma omp parallel for), (49,OpenMP,#pragma omp parallel for), (50,OpenMP,#pragma omp parallel for)", "label": "(49, CUDA, thrust::host_vector<TB> h_B(n*k);); (4, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;); (1, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (55, CUDA, thrust::device_vector<TC> d_C = h_C;); (46, CUDA, thrust::host_vector<TC> h_C(m*n);); (52, CUDA, thrust::device_vector<TB> d_B = h_B;); (86, CUDA, thrust::host_vector<TC> cute_result = d_C;); (49, CUDA, thrust::device_vector<TA> d_A = h_A;); (40, CUDA, thrust::host_vector<TA> h_A(m*k);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for reduction(+:counter_N0))", "label": "(8, OpenMP, #pragma omp taskloop shared(counter_N0)); (5, OpenMP, #pragma omp target data map(tofrom : counter_N0)); (8, OpenMP, #pragma omp target teams distribute parallel for map(tofrom : counter_N0) nowait); (9, OpenMP, #pragma omp atomic update)"}
{"predict": "0;\n128:   }\n129:   \n130:   /* set left boundary conditions                                                */\n131:   for (j=0; j<n; j++) {\n132:     ARRAY(0,j) = (double) (start+j);\n133:   }\n134: \n135:   /* set top boundary condition                                                  */\n136:   for (i=start; i<=end; i++) {\n137:     ARRAY(i-start,0) = (double) i;\n138:   }\n139: \n140:   /* set bottom boundary condition                                               */\n141:   for (i=start; i<=end; i++) {\n142:     ARRAY(i-start,n-1) = (double) (i+100*m);\n143:   }\n144: \n145:   /* initialize pipeline algorithm timer                                         */\n146:   local_pipeline_time = MPI_Wtime();\n147: \n148:   /* execute pipeline algorithm                                                  */\n149:   for (iter=0; iter<iterations; iter++) {\n150:     /* calculate right boundary values                                           */\n151:     for (j=0; j<n; j++) {\n152:       ARRAY(segment_size,j) = 0.25 * (ARRAY(segment_size-1,j) + \n153:                                     ARRAY(segment_size,j-1) +\n154:                                     ARRAY(segment_size,j+1) +\n155:                                     ARRAY(segment_size-1,j+1));\n156:     }\n157: \n158:     /* send rightmost column to right neighbor                                   */\n159:     MPI_Send(&ARRAY(segment_size-1,0), n, MPI_DOUBLE, (my_ID+1)%Num_procs, 0, MPI_COMM_WORLD);\n160: \n161:     /* receive leftmost column from left neighbor                                */\n162:     MPI_Recv(&ARRAY(0,0), n, MPI_DOUBLE, (my_ID+Num_procs-1)%Num_procs, 0, MPI_COMM_WORLD, &status);\n163: \n164:     /* calculate interior grid points                                            */\n165:     for (jj=0; jj<n; jj+=grp) {\n166:", "label": "(177, MPI, MPI_Recv(inbuf, jjsize, MPI_DOUBLE, my_ID-1, j, MPI_COMM_WORLD, &status);); (88, MPI, MPI_Bcast(&m,          1, MPI_INT, root, MPI_COMM_WORLD);); (88, MPI, MPI_Bcast(&n,          1, MPI_INT, root, MPI_COMM_WORLD);); (89, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (241, MPI, MPI_Finalize();); (163, MPI, MPI_Send(&(ARRAY(end,j)), 1, MPI_DOUBLE, my_ID+1, j, MPI_COMM_WORLD);); (195, MPI, MPI_Send(&corner_val,1,MPI_DOUBLE,0,888,MPI_COMM_WORLD);); (197, MPI, MPI_Recv(&(ARRAY(0,0)),1,MPI_DOUBLE,root,888,MPI_COMM_WORLD,&status);); (18, MPI, MPI_Init(&argc,&argv);); (183, MPI, MPI_Send(outbuf, jjsize, MPI_DOUBLE, my_ID+1, j, MPI_COMM_WORLD);); (138, MPI, MPI_Barrier(MPI_COMM_WORLD);); (17, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (79, MPI, MPI_Bcast(&grp,        1, MPI_INT, root, MPI_COMM_WORLD);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for)", "label": "(10, TBB, range_t r = utils_t::make_range((1 << (20 / DimAmount - 1)) - DimAmount, false, &MakeInt, oneapi::tbb::detail::make_index_sequence<DimAmount>());); (1, TBB, using range_t = oneapi::tbb::blocked_nd_range<int, DimAmount>;)"}
{"predict": "(22,OpenMP,#pragma omp parallel for private(k, krow, kcol, lk, knsupc, ii, lsum_k, scp, tempv, dest, j, i))", "label": "(52, MPI, MPI_Recv( recvbuf, knsupc * nrhs, MPI_FLOAT, sender, k, grid3d->zscp.comm, &status );); (42, MPI, MPI_Send( recvbuf, knsupc * nrhs, MPI_FLOAT, receiver, k,  grid3d->zscp.comm);)"}
{"predict": "(33,OpenMP,#pragma omp barrier)", "label": "(33, MPI, MPI_Barrier(scr_comm_world);)"}
{"predict": "(31,OpenMP,#pragma omp parallel for)", "label": "(27, CUDA, cudaGridDependencySynchronize();); (85, CUDA, cudaTriggerProgrammaticLaunchCompletion();)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(16, OpenCL, context = clCreateContext(nullptr, 1, &device, nullptr, nullptr, &error);)"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(24, OpenCL, clReleaseEvent(event);)"}
{"predict": "(34,OpenMP,#pragma omp parallel for private(tokenIdx, tokenPos, tokenSmemBuffer, tokenKVPosition, kPtr, loadChannelIdx, channelIdx, kvLocationIdx)) (53,OpenMP,#pragma omp parallel for private(tokenIdx, tokenPos, tokenSmemBuffer, tokenKVPosition, kPtr, loadChannelIdx, channelIdx, kvLocationIdx)) (69,OpenMP,#pragma omp parallel for private(tokenIdx, tokenPos, tokenSmemBuffer, tokenKVPosition, vPtr, loadChannelIdx, channelIdx, kvLocationIdx)) (85,OpenMP,#pragma omp parallel for private(tokenIdx, tokenPos, tokenSmemBuffer, tokenKVPosition, vPtr, loadChannelIdx, channelIdx, kvLocationIdx))", "label": "(31, CUDA, __shared__ char loadSmemBuffer[kUpdateKVCacheKernelShmSize];)"}
{"predict": "(6,OpenMP,#pragma omp parallel for collapse(3))", "label": "(6, OpenMP, #pragma omp teams distribute); (7, OpenMP, #pragma omp parallel for); (8, OpenMP, #pragma omp simd); (1, OpenMP, #pragma omp target map (alloc: u0_real,u0_imag,u1_real,u1_imag,twiddle))"}
{"predict": "(21,OpenMP,#pragma omp parallel for)", "label": "(16, MPI, MPI_Abort(MPI_COMM_WORLD, MPIT_result);); (17, MPI, MPIT_result = MPI_T_pvar_session_create(session);); (11, MPI, MPIT_result = MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);)"}
{"predict": "The provided code snippet is a unit test and does not contain any loops or computationally intensive sections that would benefit from parallelization. However, if there were loops or sections that could be parallelized, I would indicate them. Since there are no such sections, I will provide a hypothetical example based on a loop that might be added for demonstration purposes.\n\nLet's assume there is a loop in the code that processes elements of an array in lines 10 to 18. Here is how you might parallelize it using OpenMP:\n\n### **Hypothetical Code with a Loop**\n\n1: HWTEST_F(MultipleMapBufferTest, givenBlockedQueueWhenMappedOnCpuThenAddMappedPtrAndRemoveOnUnmap) {\n2:     auto buffer = createMockBuffer<FamilyType>(false);\n3:     auto cmdQ = createMockCmdQ<FamilyType>();\n4:     EXPECT_TRUE(buffer->mappingOnCpuAllowed());\n5: \n6:     UserEvent mapEvent, unmapEvent;\n7:     cl_event clMapEvent = &mapEvent;\n8:     cl_event clUnmapEvent = &unmapEvent;\n9: \n10:     size_t offset = 1;\n11:     size_t size = 3;\n12:     mapEvent.setStatus(CL_COMPLETE);\n13:     EXPECT_NE(nullptr, mappedPtr);\n14:     EXPECT_EQ(1u, buffer->getMapOperationsHandler().size());\n15:     EXPECT_EQ(buffer->copySize, size);\n16:     EXPECT_EQ(buffer->copyOffset, offset);\n17:     EXPECT_EQ(1u, buffer->transferToHostPtrCalled);\n18: \n19:     // Hypothetical loop to process elements of an array\n20:     int array[100];\n21:     for (int i = 0; i < 100; i++) {\n22:         array[i] = i * 2; // Example operation\n23:     }\n24: \n25:     unmapEvent.setStatus(CL_COMPLETE);\n26:     EXPECT_EQ(0u, buffer->getMapOperationsHandler().size());\n27:     EXPECT_EQ(buffer->copySize, size);\n28:     EXPECT_EQ(buffer->copyOffset, offset);\n29:     EXPECT_EQ(1u, buffer->transferFromHostPtrCalled);\n30: }\n\n### **Parallelization Instructions**\n\n(21,Open", "label": "(19, OpenCL, retVal = clEnqueueUnmapMemObject(cmdQ.get(), buffer.get(), mappedPtr, 1, &clUnmapEvent, nullptr);); (10, OpenCL, void *mappedPtr = clEnqueueMapBuffer(cmdQ.get(), buffer.get(), CL_FALSE, CL_MAP_WRITE, offset, size, 1, &clMapEvent, nullptr, &retVal);)"}
{"predict": "(17,OpenMP,#pragma omp parallel for), (72,OpenMP,#pragma omp parallel for), (88,OpenMP,#pragma omp parallel for), (114,OpenMP,#pragma omp parallel for), (132,OpenMP,#pragma omp parallel for)", "label": "(62, TBB, PRINT( tbb::TBB_runtime_interface_version() );); (68, TBB, while (tracker.get_concurrency() < tbb::task_scheduler_init::default_num_threads()) tbb_pi<double> (N);); (11, TBB, tbb::task_scheduler_init init(tbb::task_scheduler_init::default_num_threads());); (82, TBB, TaskSchedulerTBB::create(0);); (90, TBB, TaskSchedulerTBB::destroy();)"}
{"predict": "(19,OpenMP,#pragma omp parallel for)", "label": "(7, OpenMP, #pragma omp target data map(tofrom: W[:M])); (12, OpenMP, #pragma omp target teams); (12, OpenMP, #pragma omp distribute parallel for)"}
{"predict": "i < nub; ++i)\n103: \t\t{\n104: \t\t\tLlu->bsendx_plist[i] = index1 + i*Pr;\n105: \t\t\tLlu->fsendx_plist[i] = index1 + i*Pr + (nub*Pr/2);\n106: \t\t}\n107: \t#endif\n108:     }\n109:     if (grid3d->zscp.Iam)\n110:     {\n111:         Llu->sendx_plist = SUPERLU_MALLOC (nsupers * sizeof(int*));\n112:         Llu->recvx_plist = SUPERLU_MALLOC (nsupers * sizeof(int*));\n113:     }\n114:     if (grid3d->zscp.Iam)\n115:     {\n116:         Llu->sendx = SUPERLU_MALLOC (nsupers * sizeof(int_t));\n117:         Llu->recvx = SUPERLU_MALLOC (nsupers * sizeof(int_t));\n118:     }\n119:     if (grid3d->zscp.Iam)\n120:     {\n121:         Llu->sendxl = SUPERLU_MALLOC (nsupers * sizeof(int_t));\n122:         Llu->recvxl = SUPERLU_MALLOC (nsupers * sizeof(int_t));\n123:     }\n124:     if (grid3d->zscp.Iam)\n125:     {\n126:         Llu->usub = SUPERLU_MALLOC (Llu->nzval_ub);\n127:         Llu->unzval = SUPERLU_MALLOC (Llu->nzval_ub);\n128:     }\n129:     if (grid3d->zscp.Iam)\n130:     {\n131:         Llu->lsub = SUPERLU_MALLOC (Llu->nzval_lb);\n132:         Llu->lnzval = SUPERLU_MALLOC (Llu->nzval_lb);\n133:     }\n134:     if (grid3d->zscp.Iam)\n135:     {\n136:         Llu->xsup = SUPERLU_MALLOC (nsupers+1);\n137:         Llu", "label": "(66, MPI, MPI_Bcast( ToRecv, nsupers, MPI_INT, 0,  grid3d->zscp.comm);); (39, MPI, MPI_Bcast( supno, n, mpi_int_t, 0,  grid3d->zscp.comm);); (27, MPI, MPI_Bcast( &nsupers, 1, mpi_int_t, 0,  grid3d->zscp.comm);); (69, MPI, MPI_Bcast( ToSendR[i], Pc, MPI_INT, 0,  grid3d->zscp.comm);); (11, MPI, MPI_Bcast( etree, n, mpi_int_t, 0,  grid3d->zscp.comm);); (233, MPI, MPI_Bcast(*((void**)&(Llu->fsendx_plist[lk])), Pr * sizeof (int), MPI_BYTE, 0, grid3d->zscp.comm);); (84, MPI, MPI_Bcast( &(Llu->ldalsum), 1, mpi_int_t, 0,  grid3d->zscp.comm);); (61, MPI, MPI_Bcast( ToSendD, nbr, MPI_INT, 0,  grid3d->zscp.comm);); (29, MPI, MPI_Bcast( xsup, nsupers + 1, mpi_int_t, 0,  grid3d->zscp.comm);); (80, MPI, MPI_Bcast( &(Llu->nbsendx), 1, MPI_INT, 0,  grid3d->zscp.comm);); (78, MPI, MPI_Bcast( &(Llu->nfsendx), 1, MPI_INT, 0,  grid3d->zscp.comm);); (45, MPI, MPI_Bcast( bufmax, NBUFFERS, mpi_int_t, 0,  grid3d->zscp.comm);)"}
{"predict": "The provided code is a unit test function and does not contain any loops or computationally intensive sections that would benefit from parallelization. Therefore, there are no appropriate places to insert parallelization instructions in this specific code snippet.\n\nIf you have a different code snippet that includes loops or other parallelizable sections, please provide it, and I can analyze it for you.", "label": "(16, OpenCL, retVal = clEnqueueUnmapMemObject(cmdQ.get(), image.get(), mappedPtr, 0, nullptr, nullptr);); (8, OpenCL, void *mappedPtr = clEnqueueMapImage(cmdQ.get(), image.get(), CL_TRUE, CL_MAP_READ, &origin[0], &region[0], nullptr, nullptr, 0, nullptr, nullptr, &retVal);)"}
{"predict": "(12,OpenMP,#pragma omp parallel sections private(sum0,i0))", "label": "(13, OpenMP, #pragma omp section); (10, OpenMP, #pragma omp parallel); (21, OpenMP, #pragma omp critical)"}
{"predict": "(32,OpenMP,#pragma omp parallel for), (43,OpenMP,#pragma omp parallel for)", "label": "(57, MPI, MPI_Finalize();); (10, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(28,OpenMP,#pragma omp parallel for collapse(3)),", "label": "(17, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (14, CUDA, cudaMalloc((void**)(&d_in1), in1_bytes);); (29, CUDA, assert(cudaMemcpyAsync(out.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);); (16, CUDA, cudaMemcpy(d_in1, in1.data(), in1_bytes, cudaMemcpyHostToDevice);); (40, CUDA, cudaFree(d_out);); (38, CUDA, cudaFree(d_in2);); (26, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (35, CUDA, cudaFree(d_in1);); (8, CUDA, cudaMalloc((void**)(&d_in2), in2_bytes);); (11, CUDA, cudaMemcpy(d_in2, in2.data(), in2_bytes, cudaMemcpyHostToDevice);)"}
{"predict": "(26,OpenMP,#pragma omp parallel for)", "label": "(27, CUDA, cudaEventRecord(start);); (31, CUDA, cudaEventRecord(end);); (31, CUDA, cudaEventSynchronize(end);); (23, CUDA, cudaEventCreate(&end);); (21, CUDA, cudaEventCreate(&start);); (31, CUDA, cudaEventElapsedTime(&msec, start, end);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for reduction(+:sum)), (36,OpenMP,#pragma omp parallel for reduction(+:sum2))", "label": "(13, CUDA, cudaMalloc((void **)&d_arr, sizeof(int) * len);); (16, CUDA, cudaMemcpy(d_arr, arr, sizeof(int) * len, cudaMemcpyHostToDevice);); (53, CUDA, cudaFree(d_out);); (11, CUDA, cudaMalloc((void **)&d_out, sizeof(int) * len);); (23, CUDA, cudaMemcpy(out, d_out, sizeof(int) * len, cudaMemcpyDeviceToHost);); (49, CUDA, cudaFree(d_arr);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for private(i))", "label": "(11, OpenMP, #pragma omp distribute); (16, OpenMP, #pragma omp parallel for); (13, OpenMP, threads[j] = omp_get_num_threads();); (7, OpenMP, #pragma omp target teams num_teams(numTeams) thread_limit(1024) map (tofrom: CUs, lims, threads, Res)); (16, OpenMP, CUs[i] = omp_ext_get_smid();); (9, OpenMP, lims[j] = omp_get_num_teams();)"}
{"predict": "The provided code is primarily dealing with MPI operations and error checking. There are no explicit loops or independent tasks that can be parallelized using OpenMP or other parallelization techniques like MPI (since it's already using MPI for communication). However, if there were loops or independent tasks, I would suggest where to insert parallelization instructions.\n\nGiven the current code, there is no clear opportunity for parallelization. However, if we hypothetically had a loop that could be parallelized, here is an example of how it might look:\n\n### Hypothetical Example with a Loop\n\nLet's assume there was a loop processing an array `data`:\n\n```c\n1: int MTestSpawnPossible(int *can_spawn)\n2: {\n3:     int errs = 0;\n4: \n5:     void *v = NULL;\n6:     int flag = -1;\n7:     int vval = -1;\n8:     int rc;\n9: \n10:     if (rc != MPI_SUCCESS) {\n11:         /* MPI_UNIVERSE_SIZE keyval missing from MPI_COMM_WORLD attributes */\n12:         *can_spawn = -1;\n13:         errs++;\n14:     } else {\n15:         /* MPI_UNIVERSE_SIZE need not be set */\n16:         if (flag) {\n17: \n18:             int size = -1;\n19:             if (rc != MPI_SUCCESS) {\n20:                 /* MPI_Comm_size failed for MPI_COMM_WORLD */\n21:                 *can_spawn = -1;\n22:                 errs++;\n23:             }\n24: \n25:             vval = *(int *) v;\n26:             if (vval <= size) {\n27:                 /* no additional processes can be spawned */\n28:                 *can_spawn = 0;\n29:             } else {\n30:                 *can_spawn = 1;\n31:             }\n32:         } else {\n33:             /* No attribute associated with key MPI_UNIVERSE_SIZE of MPI_COMM_WORLD */\n34:             *can_spawn = -1;\n35:         }\n36:     }\n37: \n38:     // Hypothetical loop processing an array\n39:     int data[1000];\n40:     for (int i = 0; i < 1000; i++) {\n41", "label": "(19, MPI, rc = MPI_Comm_size(MPI_COMM_WORLD, &size);); (8, MPI, rc = MPI_Comm_get_attr(MPI_COMM_WORLD, MPI_UNIVERSE_SIZE, &v, &flag);)"}
{"predict": "(3,OpenMP,#pragma omp parallel for)", "label": "(4, CUDA, cudaError_t result = cudaSetDevice(i);); (6, CUDA, nvDebug(\"*** CUDA Error: %s\\n\", cudaGetErrorString(result));)"}
{"predict": "; j++) {\n130:     for (i=0; i<segment_size+1; i++) {\n131:       vector[i+j*(segment_size+1)] = 0.0;\n132:     }\n133:   }\n134: \n135:   /* set boundary values (left and right edges of grid)                          */\n136:   if (my_ID == 0) {\n137:     for (j=0; j<n; j++) {\n138:       vector[j*(segment_size+1)] = 100.0;\n139:     }\n140:   }\n141:   if (my_ID == Num_procs-1) {\n142:     for (j=0; j<n; j++) {\n143:       vector[(segment_size)*j+(segment_size+1)] = 100.0;\n144:     }\n145:   }\n146: \n147:   /* start the timer                                                             */\n148:   local_pipeline_time = MPI_Wtime();\n149: \n150:   /* execute the pipeline algorithm                                              */\n151:   for (iter=0; iter<iterations; iter++) {\n152:     for (jj=0; jj<n; jj+=grp) {\n153:       jjsize = (jj+grp > n) ? n-jj : grp;\n154:       if (my_ID == 0) {\n155:         for (j=0; j<jjsize; j++) {\n156:           outbuf[j] = vector[j*(segment_size+1)];\n157:         }\n158:         MPI_Send(outbuf, jjsize, MPI_DOUBLE, my_ID+1, 0, MPI_COMM_WORLD);\n159:       }\n160:       else if (my_ID == Num_procs-1) {\n161:         MPI_Recv(inbuf, jjsize, MPI_DOUBLE, my_ID-1, 0, MPI_COMM_WORLD, &status);\n162:         for (j=0; j<jjsize; j++) {\n163:           vector[(segment_size)*j+(segment_size+1)] = inbuf[j];\n164:         }\n165:", "label": "(180, MPI, MPI_Recv(inbuf, jjsize, MPI_DOUBLE, my_ID-1, j, MPI_COMM_WORLD, &status);); (91, MPI, MPI_Bcast(&m,          1, MPI_INT, root, MPI_COMM_WORLD);); (91, MPI, MPI_Bcast(&n,          1, MPI_INT, root, MPI_COMM_WORLD);); (92, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (244, MPI, MPI_Finalize();); (166, MPI, MPI_Send(&(ARRAY(end,j)), 1, MPI_DOUBLE, my_ID+1, j, MPI_COMM_WORLD);); (198, MPI, MPI_Send(&corner_val,1,MPI_DOUBLE,0,888,MPI_COMM_WORLD);); (200, MPI, MPI_Recv(&(ARRAY(0,0)),1,MPI_DOUBLE,root,888,MPI_COMM_WORLD,&status);); (19, MPI, MPI_Init(&argc,&argv);); (186, MPI, MPI_Send(outbuf, jjsize, MPI_DOUBLE, my_ID+1, j, MPI_COMM_WORLD);); (141, MPI, MPI_Barrier(MPI_COMM_WORLD);); (18, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (82, MPI, MPI_Bcast(&grp,        1, MPI_INT, root, MPI_COMM_WORLD);); (15, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(27,OpenMP,#pragma omp parallel for)", "label": "(33, CUDA, CUDA_CHECK(cudaFree(d_work));); (14, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_work), workspaceInBytesOnDevice));)"}
{"predict": "* array_size};\n122: \n123:   for (unsigned int i = 0; i < 6; i++) {\n124:     double best = *std::min_element(timings[i].begin() + ignore_times,\n125:                                     timings[i].end());\n126:     double min = *std::min_element(timings[i].begin() + ignore_times,\n127:                                    timings[i].end());\n128:     double max = *std::max_element(timings[i].begin() + ignore_times,\n129:                                    timings[i].end());\n130:     double avg = std::accumulate(timings[i].begin() + ignore_times,\n131:                                  timings[i].end(), 0.0) /\n132:                  (timings[i].size() - ignore_times);\n133:     double avg_mb_sec = sizes[i] / (avg * 1024 * 1024);\n134:     double best_mb_sec = sizes[i] / (best * 1024 * 1024);\n135:     std::cout << std::left << std::setw(12) << labels[i]\n136:               << std::left << std::setw(12) << best_mb_sec << std::left\n137:               << std::setw(12) << min << std::left << std::setw(12) << max\n138:               << std::left << std::setw(12) << avg << std::left\n139:               << std::setw(12) << avg_mb_sec << std::endl;\n140:   }\n141: }\n(58,OpenMP,#pragma omp parallel for), (66,OpenMP,#pragma omp parallel for), (74,OpenMP,#pragma omp parallel for), (82,OpenMP,#pragma omp parallel for), (90,OpenMP,#pragma omp parallel for), (98,OpenMP,#pragma omp parallel for)\n(59,OpenMP,T omp_sum = 0; #pragma omp parallel for reduction(+:omp_sum); for (int64_t i = 0; i < array_size; i++) { omp_sum += a[i] * b[i]; }), (75,", "label": "(12, OpenMP, #pragma omp target enter data map(alloc); (4, OpenMP, #pragma omp target map(tofrom : warp_size)); (135, OpenMP, #pragma omp target exit data map(release)"}
{"predict": "(49,OpenMP,#pragma omp parallel for)", "label": "(50, OpenCL, bufSA = clCreateBuffer(ctx, CL_MEM_READ_WRITE, sizeof(cl_float), NULL, &err);); (52, OpenCL, bufS  = clCreateBuffer(ctx, CL_MEM_READ_WRITE, sizeof(cl_float), NULL, &err);); (67, OpenCL, err = clWaitForEvents(1, &event);); (55, OpenCL, err = clEnqueueWriteBuffer(queue, bufS, CL_TRUE, 0, sizeof(cl_float), &S, 0, NULL, NULL);); (83, OpenCL, clReleaseMemObject(bufC);); (77, OpenCL, clReleaseEvent(event);); (79, OpenCL, clReleaseMemObject(bufSA);); (68, OpenCL, err = clEnqueueReadBuffer(queue, bufS, CL_TRUE, 0, sizeof(cl_float), &S, 0, NULL, NULL);); (28, OpenCL, clReleaseContext(ctx);); (63, OpenCL, err = clEnqueueReadBuffer(queue, bufSA, CL_TRUE, 0, sizeof(cl_float), &SA, 0, NULL, NULL);); (47, OpenCL, err = clEnqueueWriteBuffer(queue, bufC, CL_TRUE, 0, sizeof(cl_float), &C, 0, NULL, NULL);); (62, OpenCL, err = clEnqueueReadBuffer(queue, bufSB, CL_TRUE, 0, sizeof(cl_float), &SB, 0, NULL, NULL);); (32, OpenCL, clReleaseCommandQueue(queue);); (38, OpenCL, bufSB = clCreateBuffer(ctx, CL_MEM_READ_WRITE, sizeof(cl_float), NULL, &err);); (74, OpenCL, clReleaseMemObject(bufS);); (71, OpenCL, clReleaseMemObject(bufSB);); (19, OpenCL, printf( \"clCreateCommandQueue() failed with %d\\n\", err );); (16, OpenCL, queue = clCreateCommandQueue(ctx, device, 0, &err);); (34, OpenCL, bufC  = clCreateBuffer(ctx, CL_MEM_READ_WRITE, sizeof(cl_float), NULL, &err);); (8, OpenCL, ctx = clCreateContext(props, 1, &device, NULL, NULL, &err);); (9, OpenCL, printf( \"clCreateContext() failed with %d\\n\", err );); (34, OpenCL, err = clEnqueueWriteBuffer(queue, bufSA, CL_TRUE, 0, sizeof(cl_float), &SA, 0, NULL, NULL);); (34, OpenCL, err = clEnqueueWriteBuffer(queue, bufSB, CL_TRUE, 0, sizeof(cl_float), &SB, 0, NULL, NULL);); (51, OpenCL, err = clEnqueueReadBuffer(queue, bufC, CL_TRUE, 0, sizeof(cl_float), &C, 0, NULL, NULL);)"}
{"predict": "hTimer);\n151: \n152:   if (time < 1e-6) {\n153:     std::cout << \"Gsamples/s: problem size too small to measure, try \"\n154:                  \"increasing number of vectors or dimensions\"\n155:               << std::endl;\n156:   } else {\n157:     std::cout << \"Gsamples/s: \"\n158:               << (double)n_vectors * (double)n_dimensions * 1E-9 / (time * 1E-3)\n159:               << std::endl;\n160:   }\n161: \n162:   // Verify results\n163:   std::cout << \"Verifying results...\" << std::endl;\n164:   ok = true;\n165:   for (int i = 0; i < n_vectors * n_dimensions; i++) {\n166:     if (fabs(h_outputGPU[i] - h_outputCPU[i]) > 1e-5) {\n167:       ok = false;\n168:     }\n169:   }\n170: \n171:   if (!ok) {\n172:     std::cerr << \"Verification failed\" << std::endl;\n173:     return -1;\n174:   }\n175: \n176:   // Free memory\n177:   std::cout << \"Shutting down...\" << std::endl;\n178:   delete[] h_directions;\n179:   delete[] h_outputCPU;\n180:   delete[] h_outputGPU;\n181:   cudaFree(d_directions);\n182:   cudaFree(d_output);\n183:   sdkDeleteTimer(&hTimer);\n184: \n185:   return 0;\n186: }", "label": "(93, CUDA, throw std::runtime_error(cudaGetErrorString(cudaResult));); (57, CUDA, findCudaDevice(argc, (const char **)argv);); (117, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (211, CUDA, checkCudaErrors(cudaFree(d_output));); (209, CUDA, checkCudaErrors(cudaFree(d_directions));)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for), (36,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for), (44,OpenMP,#pragma omp parallel for), (48,OpenMP,#pragma omp parallel for), (52,OpenMP,#pragma omp parallel for), (56,OpenMP,#pragma omp parallel for), (60,OpenMP,#pragma omp parallel for), (62,OpenMP,#pragma omp parallel for), (67,OpenMP,#pragma omp parallel for), (72,OpenMP,#pragma omp parallel for), (77,OpenMP,#pragma omp parallel for)", "label": "(82, OpenMP, #pragma omp parallel private(i)); (95, OpenMP, #pragma omp distribute parallel for private(m)); (5, OpenMP, #pragma omp target); (26, OpenMP, #pragma omp distribute parallel for private(argc,); (60, OpenMP, #pragma omp distribute parallel for private(h, B::x)); (64, OpenMP, #pragma omp distribute parallel for nowait); (8, OpenMP, #pragma omp distribute parallel for private(); (65, OpenMP, #pragma omp parallel); (16, OpenMP, #pragma omp distribute parallel for private(argc); (40, OpenMP, #pragma omp distribute parallel for private(a, b)); (49, OpenMP, #pragma omp distribute parallel for private(e, g)); (33, OpenMP, #pragma omp distribute parallel for private(S1)); (42, OpenMP, #pragma omp distribute parallel for private(argv[1])); (6, OpenMP, #pragma omp distribute parallel for private()); (-5, OpenMP, #pragma omp distribute parallel for private); (62, OpenMP, #pragma omp distribute parallel for private(i)); (69, OpenMP, #pragma omp distribute parallel for private(j)); (22, OpenMP, #pragma omp distribute parallel for private(argc)); (16, OpenMP, #pragma omp distribute parallel for private(argc > 0 ? argv[1] : argv[2])); (62, OpenMP, #pragma omp parallel shared(i)); (-12, OpenMP, #pragma omp teams)"}
{"predict": "(3,OpenMP,#pragma omp parallel for), (4,OpenMP,#pragma omp parallel for), (5,OpenMP,#pragma omp parallel for), (6,OpenMP,#pragma omp parallel for), (7,OpenMP,#pragma omp parallel for), (8,OpenMP,#pragma omp parallel for), (9,OpenMP,#pragma omp parallel for), (10,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp target parallel for device ()); (23, OpenMP, #pragma omp target parallel for device (3.14)); (18, OpenMP, #pragma omp target parallel for device (-2)); (13, OpenMP, #pragma omp target parallel for device (argc), device (argc+1)); (14, OpenMP, #pragma omp target parallel for device (S1)); (5, OpenMP, #pragma omp target parallel for device (argc))); (-4, OpenMP, #pragma omp target parallel for device); (1, OpenMP, #pragma omp target parallel for device (argc); (14, OpenMP, #pragma omp target parallel for device (-10u)); (5, OpenMP, #pragma omp target parallel for device (z + argc)); (-6, OpenMP, #pragma omp target parallel for device (); (1, OpenMP, #pragma omp target parallel for device (argc > 0 ? argv[1] : argv[2]))"}
{"predict": "(7,OpenMP,#pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp target map(from:c))"}
{"predict": "< nub; i+=WARP_SIZE){ // loop through the nonzero block columns in this block row\n127:                 idx_i = usub[i+1];\n128:                 idx_v = lloc[i];\n129:                 icol = rel + idx_i;\n130:                 temp1 = lusup[idx_v];\n131:                 temp1 = temp1 * x[icol];\n132:                 s_lsum[ljb] += temp1;\n133:             }\n134:             __syncthreads();\n135: \n136:             // printf(\"  bfa kernel:   %i %i %i %i %i %i %i %i %i %i\\n\", threadIdx_x, bid, grid->npcol, nsupers,myrow,krow,wrp,k,uind_br[0],bmod[lib*aln_i]);\n137:             if(lne==0){\n138:                 bmod[lib*aln_i]=0;\n139:                 __threadfence();\n140:             }\n141:             #ifdef HAVE_CUDA\n142:             __syncwarp();\n143:             #endif\n144: \n145:             for (i = tid; i < nrhs; i+=block_size){\n146:                 lptr = LSUM_BLK( lk ) + i;\n147:                 lptr1_tmp = lptr;\n148:                 for (ii = 0; ii < nub; ii++){\n149:                     lptr1_tmp += usub[ii+1];\n150:                     lsum[lptr1_tmp] += s_lsum[ii];\n151:                 }\n152:             }\n153:             __syncthreads();\n154:         }\n155:       }\n156:       // printf(\"  Exiting kernel:   %i %i %i %i %i %i %i %i\\n\", threadIdx_x, blockIdx_x, grid->npcol, nsupers,myrow,krow,bid,tid);\n157:   }", "label": "(35, CUDA, volatile __shared__ int s_bmod;); (32, CUDA, __shared__ double s_lsum[MAXSUPER];)"}
{"predict": "(12,OpenMP,#pragma omp parallel), (29,OpenMP,#pragma omp parallel for)", "label": "(47, MPI, MPI_Ibcast(&(prg->keep_going), 1, MPI_INT, 0, prg->comm, &(prg->bcast_req));); (15, MPI, MPI_Comm_dup(comm, &prg->comm);); (37, MPI, prg->time_start = MPI_Wtime();); (42, MPI, MPI_Comm_rank(prg->comm, &rank);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for private(usub, usub_new, uval, uval_new, lenv, lens)), (40,OpenMP,#pragma omp parallel for private(pc, ljb, lsub, lsub_new, lnzval, lnzval_new, nrbl, len, len1, len2))", "label": "(28, MPI, MPI_DATATYPE_ALLOC(uval_new, lenv);); (25, MPI, MPI_INT_ALLOC(usub_new, lens);); (59, MPI, MPI_INT_ALLOC(lsub_new, len1);); (60, MPI, MPI_DATATYPE_ALLOC(lnzval_new, len2);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for)", "label": "(28, CUDA, getLastCudaError(\"mergeRanksAndIndicesKernel(B)<<<>>> failed\\n\");); (18, CUDA, getLastCudaError(\"mergeRanksAndIndicesKernel(A)<<<>>> failed\\n\");)"}
{"predict": "(15,OpenMP,#pragma omp parallel)", "label": "(9, CUDA, cudaFuncGetAttributes(&attr,(void*)ptr);); (14, CUDA, fcuda::Check_CudaErroorFun(\"Error collecting kernel information.\");); (5, CUDA, cudaOccupancyMaxPotentialBlockSize(&mingridsize,&qblocksize,(void*)ptr,0,0);)"}
{"predict": "(36,OpenMP,#pragma omp parallel for num_threads(n_threads)), (41,OpenMP,#pragma omp parallel for num_threads(n_threads))", "label": "(20, TBB, tbb::global_control control(tbb::global_control::max_allowed_parallelism, n_threads);); (1, TBB, n_threads = tbb::this_task_arena::max_concurrency();); (29, TBB, tbb::enumerable_thread_specific<tbb::task_arena*> tls;); (25, TBB, std::vector<tbb::task_arena> arenas(n_arenas);)"}
{"predict": "(25,OpenMP,#pragma omp parallel for), (49,OpenMP,#pragma omp parallel for), (66,OpenMP,#pragma omp parallel for)", "label": "(15, OpenCL, int error = clGetDeviceInfo( device, CL_DEVICE_IMAGE2D_MAX_WIDTH, sizeof( maxWidth ), &maxWidth, NULL );); (16, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_GLOBAL_MEM_SIZE, sizeof( memSize ), &memSize, NULL );); (14, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_MAX_MEM_ALLOC_SIZE, sizeof( maxAllocSize ), &maxAllocSize, NULL );)"}
{"predict": "(24,OpenMP,#pragma omp parallel for), (64,OpenMP,#pragma omp parallel for)", "label": "(36, MPI, MPI_Comm_size(intercomm, &size);); (56, MPI, MPI_Comm_disconnect(&intercomm);); (102, MPI, MPI_Finalize();); (19, MPI, MPI_Comm_get_parent(&parentcomm);); (47, MPI, MPI_Recv(&data, 1, MPI_INT, RECEIVER_RANK, DATA_TAG, intercomm, &status);); (79, MPI, MPI_Abort(intercomm, 1);); (73, MPI, MPI_Recv(&i, 1, MPI_INT, SENDER_RANK, DATA_TAG, intercomm, &status);); (43, MPI, MPI_Send(&i, 1, MPI_INT, RECEIVER_RANK, DATA_TAG, intercomm);); (29, MPI, MPI_Comm_rank(intercomm, &rank);); (26, MPI, MPI_Comm_remote_size(intercomm, &rsize);); (76, MPI, MPI_Send(&i, 1, MPI_INT, SENDER_RANK, DATA_TAG, intercomm);)"}
{"predict": "of stencil        = star\\n\");\n116:     printf(\"Number of iterations   = %d\\n\", iterations);\n117:   }\n118: \n119:   bail_out(error);\n120: \n121:   /* compute grid bounds for local subproblems */\n122:   istart = my_IDx*n/Num_procsx;\n123:   iend   = (my_IDx+1)*n/Num_procsx;\n124:   jstart = my_IDy*n/Num_procsy;\n125:   jend   = (my_IDy+1)*n/Num_procsy;\n126: \n127:   /* adjust grid bounds for edge ranks */\n128:   if (my_IDx == Num_procsx-1) iend = n;\n129:   if (my_IDy == Num_procsy-1) jend = n;\n130: \n131:   width  = iend-istart;\n132:   height = jend-jstart;\n133: \n134:   /* compute number of points in interior of grid tile (not counting ghost points) */\n135:   f_active_points = (DTYPE) ((width-2*RADIUS)*(height-2*RADIUS));\n136: \n137:   /* compute total length of input and output arrays */\n138:   total_length_in  = width*height;\n139:   total_length_out = width*height;\n140: \n141:   /* allocate space for input and output grids */\n142:   in  = (DTYPE *) prk_malloc(total_length_in*sizeof(DTYPE));\n143:   out = (DTYPE *) prk_malloc(total_length_out*sizeof(DTYPE));\n144: \n145:   /* allocate space for communication buffers */\n146:   top_buf_out    = (DTYPE *) prk_malloc(width*sizeof(DTYPE));\n147:   top_buf_in     = (DTYPE *) prk_malloc(width*sizeof(DTYPE));\n148:   bottom_buf_out = (DTYPE *) prk_malloc(width*sizeof(DTYPE));\n149:   bottom_buf_in  = (DTYPE *) prk_malloc(width*sizeof(DTYPE));\n150:   right_buf_out  = (", "label": "(295, MPI, MPI_Wait(&(request[1+4]), MPI_STATUS_IGNORE);); (301, MPI, MPI_Wait(&(request[3+4]), MPI_STATUS_IGNORE);); (234, MPI, MPI_Barrier(MPI_COMM_WORLD);); (128, MPI, MPI_Bcast(&n,          1, MPI_INT, root, MPI_COMM_WORLD);); (43, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (289, MPI, MPI_Wait(&(request[0+4]), MPI_STATUS_IGNORE);); (254, MPI, MPI_Wait(&(request[0]), MPI_STATUS_IGNORE);); (294, MPI, MPI_Wait(&(request[2+4]), MPI_STATUS_IGNORE);); (124, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (36, MPI, MPI_Init(&argc,&argv);); (251, MPI, MPI_Wait(&(request[1]), MPI_STATUS_IGNORE);); (256, MPI, MPI_Wait(&(request[2]), MPI_STATUS_IGNORE);); (365, MPI, MPI_Finalize();); (255, MPI, MPI_Wait(&(request[3]), MPI_STATUS_IGNORE);); (324, MPI, MPI_Reduce(&local_norm, &norm, 1, MPI_DTYPE, MPI_SUM, root, MPI_COMM_WORLD);); (31, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(6, CUDA, extent = make_cudaExtent(30, 40, 0);); (3, CUDA, cudaExtent extent = make_cudaExtent(10, 20, 0);)"}
{"predict": "(19,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for)", "label": "(13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (37, MPI, MPI_Comm_free(&cart);); (12, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for)", "label": "(20, MPI, MPI_Recv(buff, count, type, 0, tag, comm, MPI_STATUS_IGNORE);); (4, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(54,OpenMP,#pragma omp parallel for private(p_master_node, p_slave_node, transformation_matrix, constant_vector, slave_solution_vector, master_solution_vector))", "label": "(55, OpenMP, #pragma omp for schedule(guided, 512)); (52, OpenMP, #pragma omp parallel firstprivate(transformation_matrix, constant_vector, slave_solution_vector, master_solution_vector, p_master_node, p_slave_node))"}
{"predict": "(19,OpenMP,#pragma omp parallel for)", "label": "(24, MPI, MPI_Request_free(&request);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (15, MPI, MPI_Psend_init(&a, 1, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, MPI_INFO_NULL, &request);); (16, MPI, MPI_Precv_init(&a, 1, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_INFO_NULL, &request);); (21, MPI, MPI_Barrier(MPI_COMM_WORLD);); (6, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": ":          * 1 2 1 3 1 2 1 3 1 2 1 3\n126:          * 1 1 2 2 3 3 1 1 2 2 3 3\n127:          * where 1, 2, 3 are process nos. In this case, the pointer will\n128:          * point to the first 1, but the current read may satisfy the\n129:          * second 1. In that case, I have to search the list again from\n130:          * the beginning. */\n131: \n132:         for (i = 0; i < nprocs; i++) {\n133:             count[i] = 0;\n134:             partial_send[i] = 0;\n135:             send_size[i] = 0;\n136:         }\n137: \n138:         for (i = 0; i < nprocs; i++) {\n139:             /* check if current read satisfies any of the off-len pairs\n140:              * of this process */\n141:             for (j = curr_offlen_ptr[i]; j < others_req[i].count; j++) {\n142:                 off = others_req[i].offsets[j];\n143:                 size = others_req[i].lens[j];\n144:                 if (off >= st_loc && off < (st_loc + coll_bufsize)) {\n145:                     if ((off + size) <= (st_loc + coll_bufsize)) {\n146:                         /* entire off-len pair is satisfied */\n147:                         send_size[i] += size;\n148:                         count[i]++;\n149:                         curr_offlen_ptr[i]++;\n150:                     } else {\n151:                         /* only a portion of the last off-len pair is\n152:                          * satisfied */\n153:                         send_size[i] += (st_loc + coll_bufsize - off);\n154:                         partial_send[i] = st_loc + coll_bufsize - off;\n155:                         curr_offlen_ptr[i]++;\n156:                         break;\n157:                     }\n158:                 } else if (off", "label": "(113, MPI, MPI_Comm_rank(fd->comm, &rank);); (180, MPI, MPI_Get_count(&read_status, MPI_BYTE, &actual_size);); (105, MPI, MPI_Type_get_extent(datatype, &lb, &buftype_extent);); (69, MPI, MPI_Allreduce(&ntimes, &max_ntimes, 1, MPI_INT, MPI_MAX, fd->comm);); (219, MPI, MPI_Get_address(read_buf + req_off - real_off, &addr);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for private(sum1,sum2,j,k,mhu,tauR))", "label": "(16, OpenMP, #pragma omp target teams distribute parallel for thread_limit(128)); (8, OpenMP, #pragma omp target enter data map(to: Vm[:nCells])); (6, OpenMP, #pragma omp target enter data map(to: m_gate[:nCells]))"}
{"predict": "(6,OpenMP,#pragma omp parallel for private(flag, val_p))", "label": "(6, MPI, MPI_Win_get_attr(win, key[i], &val_p, &flag);)"}
{"predict": "(22,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for)", "label": "(23, CUDA, cudaMemcpy(output, data, copySize, cudaMemcpyDeviceToDevice);); (20, CUDA, cudaMemcpy(workspace, transformCoeff, sizeof(int)*nOutputDims,cudaMemcpyHostToDevice );)"}
{"predict": "(22,OpenMP,#pragma omp parallel for)", "label": "(19, MPI, MPI_Type_commit(&mystruct);); (27, MPI, MPI_Type_create_hvector(0, 1, stride, MPI_INT, &vecs[i]);); (19, MPI, MPI_Type_free(&mystruct);); (8, MPI, MPI_Type_commit(&vecs[i]);); (14, MPI, MPI_Type_create_struct(3, blockcount, displs, vecs, &mystruct);); (5, MPI, MPI_Type_create_hvector(i, 1, stride, MPI_INT, &vecs[i]);); (17, MPI, MPI_Type_free(&vecs[i]);)"}
{"predict": "(17,OpenMP,#pragma omp target teams distribute parallel for), (33,OpenMP,#pragma omp target teams distribute parallel for)", "label": "(14, CUDA, checkCudaErrors(cudaMalloc(&info, sizeof(int)));); (14, CUDA, checkCudaErrors(cudaMalloc(&buffer, sizeof(double) * bufferSize));); (14, CUDA, checkCudaErrors(cudaMalloc(&A, sizeof(double) * lda * n));); (36, CUDA, checkCudaErrors(cusolverDnDpotrs(handle, uplo, n, 1, A, lda, x, n, info));); (37, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (43, CUDA, checkCudaErrors(cudaFree(info));); (45, CUDA, checkCudaErrors(cudaFree(buffer));); (14, CUDA, checkCudaErrors(cudaMemset(info, 0, sizeof(int)));); (46, CUDA, checkCudaErrors(cudaFree(A));)"}
{"predict": "(22,OpenMP,#pragma omp parallel)", "label": "(12, OpenMP, #pragma omp target map(tofrom: N) map(from:tmp)); (2, OpenMP, #pragma omp target map(to: N) map(tofrom: tmp))"}
{"predict": "(19,OpenMP,#pragma omp parallel for),", "label": "(4, MPI, MPI_Init(&argc, &argv);); (52, MPI, MPI_Finalize();); (6, MPI, ierr = MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, int ierr = MPI_Comm_size(MPI_COMM_WORLD, &comm_size);)"}
{"predict": "Free(cudaFp16Input);\n97:         FastllmCudaFree(cudaFp16Weight);\n98:         FastllmCudaFree(cudaFp16Output);\n99: #endif\n100:     } else {\n101:         FastllmCudaMatMulFloatInt4NoZeroCpu(input, weight, bias, output, n, m, k);\n102:     }\n103:     return true;\n104: }", "label": "(119, CUDA, FastllmCudaFinishOutput(output, cudaOutput);); (39, CUDA, cudaFp16Input = (half *) FastllmCudaMalloc(n * m * sizeof(half));); (21, CUDA, state = cudaMemset(cudaBiasData, 0, k * sizeof(float));); (115, CUDA, FastllmCudaFinishInput(input, cudaInput);); (43, CUDA, cudaFp16Output = (half *) FastllmCudaMalloc(n * k * sizeof(half));); (9, CUDA, state = cudaMemcpy(cudaMins, mins, k * sizeof(float), cudaMemcpyHostToDevice);); (-2, CUDA, state = cudaMalloc(&cudaScales, k * sizeof(float));); (94, CUDA, FastllmCudaFree(cudaFp16Input);); (103, CUDA, FastllmCudaFree(cudaFp16Output);); (32, CUDA, cudaFp16Weight = (half *) FastllmCudaMalloc(k * m * sizeof(half));); (-1, CUDA, state = cudaMalloc(&cudaMins, k * sizeof(float));); (-6, CUDA, state = cudaMemcpy(cudaScales, weight.scales.data(), k * sizeof(float), cudaMemcpyHostToDevice);); (7, CUDA, state = cudaMalloc(&cudaBiasData, k * sizeof(float));); (12, CUDA, checkCudaErrors(\"Error: CUDA error when moving bias to device!\", state);); (7, CUDA, state = cudaMemcpy(cudaBiasData, (uint8_t*)bias.cudaData, k * sizeof(float), cudaMemcpyDeviceToDevice);); (87, CUDA, FastllmCudaFree(cudaFp16Weight);); (18, CUDA, float *cudaOutput = (float*)FastllmCudaPrepareOutput(output);); (16, CUDA, float *cudaInput = (float*)FastllmCudaPrepareInput(input);)"}
{"predict": "The provided code does not contain any loops or computationally intensive tasks that are suitable for parallelization with MPI or OpenMP. The code is primarily for testing and validation, with no apparent parallelizable sections. However, if we hypothetically consider parallelization for educational purposes, we might look for loops or sections that could benefit from parallel execution. Since there are no such sections, no parallelization instructions are applicable.\n\nIf you have a different code snippet or a modified version of this code that includes loops or parallelizable tasks, please provide it, and I can analyze it further. For now, the answer is:\n\n()", "label": "(12, TBB, g.reset(tbb::flow::rf_clear_edges);); (4, TBB, tbb::flow::queue_node<int> qout(g);); (5, TBB, tbb::flow::make_edge(on,qout);); (0, TBB, tbb::flow::graph g;)"}
{"predict": "(23,OpenMP,#pragma omp parallel for)", "label": "(34, CUDA, CSC(cudaGetLastError(), STATUS_FAILURE);)"}
{"predict": "(5,OpenMP,#pragma omp parallel for reduction(+:tmp2)), (9,OpenMP,#pragma omp parallel for reduction(+:tmp) private(M))", "label": "(8, OpenMP, #pragma omp target teams distribute parallel for reduction(+:tmp) map(A, N, M) map(tofrom:M))"}
{"predict": "(1,OpenMP,#pragma omp parallel for)", "label": "(12, OpenMP, #pragma omp for simd collapse (foobool(1) > 0 ? 1 : 2)); (8, OpenMP, #pragma omp for simd collapse (2+2))); (-1, OpenMP, #pragma omp for simd collapse); (16, OpenMP, #pragma omp for simd collapse (S1)); (-1, OpenMP, #pragma omp for simd collapse (); (0, OpenMP, #pragma omp for simd collapse ()); (26, OpenMP, #pragma omp for simd collapse (2)); (10, OpenMP, #pragma omp for simd collapse (foobool(argc)), collapse (true), collapse (-5)); (-1, OpenMP, #pragma omp for simd collapse (4); (21, OpenMP, #pragma omp for simd collapse(collapse(tmain<int, char, -1, -2>(argc, argv))"}
{"predict": "(15,OpenMP,#pragma omp parallel sections)", "label": "(35, MPI, MPI_Bcast(&tmp_val, 1, MPI_INT, 0, fd->comm);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for), (29,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for), (45,OpenMP,#pragma omp parallel for), (64,OpenMP,#pragma omp parallel for), (67,OpenMP,#pragma omp parallel for)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(1, TBB, tbb::task_arena sequential_arena(1);)"}
{"predict": "PerWorkItem);\n136:     const cl_int srcStrideInt = static_cast<cl_int>(srcStride);\n137:     const cl_int dstStrideInt = static_cast<cl_int>(dstStride);\n138:     const cl_int srcMarginInt = static_cast<cl_int>(srcMargin);\n139:     const cl_int dstMarginInt = static_cast<cl_int>(dstMargin);\n140: \n141:     if (inBuffer == NULL || outBuffer == NULL || outBufferCopy == NULL)\n142:     {\n143:         log_error(\"Error allocating buffers.\\n\");\n144:         return -1;\n145:     }\n146: \n147:     cl_mem inBufferCL = clCreateBuffer(context, CL_MEM_READ_ONLY,\n148:                                        inBufferSize, inBuffer, &error);\n149:     test_error(error, \"clCreateBuffer for input buffer failed.\");\n150: \n151:     cl_mem outBufferCL = clCreateBuffer(context, CL_MEM_WRITE_ONLY,\n152:                                         outBufferSize, outBuffer, &error);\n153:     test_error(error, \"clCreateBuffer for output buffer failed.\");\n154: \n155:     error = clSetKernelArg(kernel, 0, sizeof(cl_mem), &inBufferCL);\n156:     test_error(error, \"Unable to set kernel argument 0\");\n157: \n158:     error = clSetKernelArg(kernel, 1, sizeof(cl_mem), &outBufferCL);\n159:     test_error(error, \"Unable to set kernel argument 1\");\n160: \n161:     error = clSetKernelArg(kernel, 2, sizeof(cl_int), &lineCopiesPerWorkItemInt);\n162:     test_error(error, \"Unable to set kernel argument 2\");\n163: \n164:     error = clSetKernelArg(kernel, 3, sizeof(cl_int), &srcStrideInt);\n165:     test_error(error, \"Unable to set kernel argument 3\");\n166: \n167:     error = clSetKernelArg(kernel, 4, sizeof(cl_int), &dstStrideInt);\n168:     test_error(error, \"Unable to set kernel argument 4\");\n1", "label": "(169, OpenCL, error = clSetKernelArg(kernel, 0, sizeof(streams[0]), &streams[0]);); (170, OpenCL, error = clSetKernelArg(kernel, 1, sizeof(streams[1]), &streams[1]);); (184, OpenCL, error = clSetKernelArg(kernel, 7, sizeof(dstStride), &dstStride);); (170, OpenCL, error = clSetKernelArg(kernel, 2, localBufferSize, NULL);); (180, OpenCL, error = clSetKernelArg(kernel, 6, sizeof(srcStride), &srcStride);)"}
{"predict": "(14,OpenMP,#pragma omp parallel sections), (19,OpenMP,#pragma omp section), (24,OpenMP,#pragma omp section), (29,OpenMP,#pragma omp section)", "label": "(11, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(1,OpenMP,#pragma omp declare target)", "label": "(4, CUDA, __shared__ float shared_val[32];)"}
{"predict": "(27,OpenMP,#pragma omp parallel for), (44,OpenMP,#pragma omp parallel for), (46,OpenMP,#pragma omp parallel for), (54,OpenMP,#pragma omp parallel for)", "label": "(60, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (78, CUDA, CUDA_CHECK(cudaDeviceReset());); (75, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (40, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (41, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_y), sizeof(data_type) * y.size()));); (39, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_x), sizeof(data_type) * x.size()));); (67, CUDA, CUDA_CHECK(cudaFree(d_y));); (65, CUDA, CUDA_CHECK(cudaFree(d_x));); (63, CUDA, CUDA_CHECK(cudaFree(d_A));); (30, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(29,OpenMP,#pragma omp parallel for)", "label": "(37, TBB, tbb::flow::make_edge(source2, tbb::flow::input_port<1>(merge));); (35, TBB, tbb::flow::make_edge(source1, tbb::flow::input_port<0>(merge));); (36, TBB, tbb::flow::make_edge(merge, output);); (-2, TBB, tbb::flow::graph g;)"}
{"predict": "(27,OpenMP,#pragma omp parallel for collapse(2)), (25,MPI,MPI_Init(&argc, &argv);), (83,MPI,MPI_Finalize();)", "label": "(37, CUDA, CHECK(cudaMalloc(&d_matrix_org, M));); (37, CUDA, CHECK(cudaMalloc(&d_matrix_res, M));); (37, CUDA, CHECK(cudaMemcpy(d_matrix_org, h_matrix_org, M, cudaMemcpyDefault));); (42, CUDA, CHECK(cudaEventCreate(&stop));); (50, CUDA, CHECK(cudaEventRecord(stop));); (46, CUDA, CHECK(cudaMemcpy(h_matrix_res, d_matrix_res, M, cudaMemcpyDefault));); (41, CUDA, cudaEventQuery(start);); (48, CUDA, CHECK(cudaEventSynchronize(stop));); (48, CUDA, CHECK(cudaEventElapsedTime(&curr_time, start, stop));); (86, CUDA, CHECK(cudaFree(d_matrix_org));); (86, CUDA, CHECK(cudaFree(d_matrix_res));); (35, CUDA, CHECK(cudaEventRecord(start));); (32, CUDA, CHECK(cudaEventCreate(&start));); (3, CUDA, CHECK(cudaMemcpyFromSymbol(&SIZE, c_TILE_DIM, sizeof(int)));)"}
{"predict": "(30,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for)", "label": "(55, OpenCL, clReleaseProgram(program);); (52, OpenCL, clReleaseKernel(k0);); (52, OpenCL, clReleaseKernel(k1);); (11, OpenCL, cl_kernel k1 = clCreateKernel(program, \"compiler_program_global1\", &err);); (8, OpenCL, cl_kernel k0 = clCreateKernel(program, \"compiler_program_global0\", &err);)"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for)", "label": "(10, OpenMP, #pragma omp target map(tofrom: a[0:100])); (12, OpenMP, #pragma omp simd)"}
{"predict": "0 * fft_size *\n109:                                (std::log(fft_size) / std::log(2)) / time_cufft / 1000000.0;\n110: \n111:         std::cout << fft_size << \",\" << ffts_per_block * cuda_blocks << \",\" << gflops_cufftdx << \",\"\n112:                   << gflops_cufft << std::endl;\n113:         #ifdef CUFFTDX_EXAMPLES_CUFFT_CALLBACK\n114:         double gflops_cufft_cb = 1.0 * kernel_runs * ffts_per_block * cuda_blocks * 5.0 * fft_size *\n115:                                   (std::log(fft_size) / std::log(2)) / time_cufft_cb / 1000000.0;\n116:         std::cout << gflops_cufft_cb << std::endl;\n117:         #endif\n118:     }\n119: }\n\n### ** Analysis and Parallelization Suggestions **\n\nThe provided code is primarily focused on setting up and running FFT computations using cuFFTDx and cuFFT libraries. The computations themselves are likely to be parallelized within the CUDA kernels, but there are opportunities to parallelize the host-side code where data generation and performance measurement can be optimized.\n\n### **Parallelization Suggestions**\n\n1. **Data Generation**: The data generation step (line 61) can be parallelized using OpenMP since it is a CPU-bound operation that can benefit from multi-threading.\n2. **Performance Measurement**: The performance measurement steps (lines 70-73) can also be parallelized if they are independent and can be executed concurrently.\n\n### **Parallelization Insertions**\n\n(61,OpenMP,#pragma omp parallel for), (70,OpenMP,#pragma omp parallel sections), (71,OpenMP,#pragma omp section), (72,OpenMP,#pragma omp section), (74,OpenMP,#pragma omp section), (75,OpenMP,#pragma omp section)", "label": "(66, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&device_buffer, input_size_bytes));); (56, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamCreate(&stream));); (80, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(device_buffer));); (67, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (65, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(device_buffer, input.data(), input_size_bytes, cudaMemcpyHostToDevice));)"}
{"predict": "(5,OpenMP,#pragma omp parallel for), (10,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for)", "label": "(12, OpenMP, #pragma omp distribute parallel for simd firstprivate(x);); (5, OpenMP, #pragma omp distribute parallel for simd;); (24, OpenMP, #pragma omp distribute parallel for simd, private(x);); (0, OpenMP, #pragma omp target); (15, OpenMP, #pragma omp distribute parallel for simd private(x);); (-1, OpenMP, #pragma omp teams)"}
{"predict": "(80,OpenMP,#pragma omp parallel for)", "label": "(95, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(streams), &streams);); (105, OpenCL, clReleaseKernel(kernel);); (85, OpenCL, err = clGetDeviceInfo(dev, CL_DEVICE_MAX_WORK_GROUP_SIZE, sizeof(local_size), &local_size, NULL);); (59, OpenCL, err = clBuildProgram(program, 0, NULL, includes, NULL, NULL);); (103, OpenCL, clReleaseProgram(program);); (48, OpenCL, cl_program program = clCreateProgramWithSource(ctx, 4, sources, NULL, &err);); (96, OpenCL, err = clEnqueueNDRangeKernel(commQueues[0], kernel, 1, NULL, &streamCount, &local_size, numWaitEvents, waitEvents, outEvents);); (89, OpenCL, err |= clSetKernelArg(kernel, 1, sizeof(number_count_per_stream), &number_count_per_stream);); (89, OpenCL, err |= clSetKernelArg(kernel, 2, sizeof(outBuffer), &outBuffer);); (68, OpenCL, cl_kernel kernel = clCreateKernel(program, \"fillBufferU01\", &err);)"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (8,OpenMP,#pragma omp parallel for)", "label": "(3, OpenMP, #pragma omp target)"}
{"predict": "(17,OpenMP,#pragma omp parallel for private(j,temp) reduction(min:sum2)), (30,OpenMP,#pragma omp parallel for reduction(min:sum)), (36,OpenMP,#pragma omp parallel for reduction(+:sum3))", "label": "(28, OpenMP, #pragma omp teams reduction(+:sum)); (25, OpenMP, #pragma omp target map(tofrom: b, sum, t) map(alloc: a)); (28, OpenMP, #pragma omp distribute parallel for reduction(+:sum)); (30, OpenMP, t[i] = omp_get_team_num();)"}
{"predict": "Tensor smA = partition_S(Shape<_1,_1,_1>{}, sA);  // (BLK_M,BLK_K,PIPE)\n114:   Tensor smB = partition_S(Shape<_1,_1,_1>{}, sB);  // (BLK_N,BLK_K,PIPE)\n115: \n116:   // Partition the accumulators into warp-sized tiles\n117:   Tensor c = partition_S(Shape<WarpM,WarpN,WarpK>{}, gC); // (WARPM,WARPN,WARPK)\n118: \n119:   // Initialize accumulators to zero\n120:   c = zero(c);\n121: \n122:   //\n123:   // Main Loop\n124:   //\n125: \n126:   // Prefetch next tile\n127:   int next_pipe = 0;\n128: \n129:   // Wait for all TMA operations to complete\n130:   CUTE_UNROLL\n131:   for (int pipe = 0; pipe < K_PIPE_MAX; ++pipe) {\n132:     if ((warp_idx == 0) && lane_predicate) {\n133:       ConsumerBarType::wait(&consumer_mbar[pipe]);\n134:     }\n135:   }\n136: \n137:   // Main Loop\n138:   CUTE_UNROLL\n139:   for (int pipe = 0; pipe < K_PIPE_MAX; ++pipe) {\n140:     // Prefetch next tile\n141:     if (k_tile_count > 0) {\n142:       if ((warp_idx == 0) && lane_predicate) {\n143:         ProducerBarType::arrive_and_expect_tx(&producer_mbar[next_pipe], kTmaTransactionBytes);\n144:         copy(tma_a.with(producer_mbar[next_pipe]), tAgA(_,k_tile), tAsA(_,next_pipe));\n145:         copy(tma_b.with(producer_mbar[next_pipe]), tBgB(_,k_tile), tBsB(_,next_pipe));\n146:       }\n147:       --k_tile_count;\n148:       ++k_tile;", "label": "(38, CUDA, extern __shared__ char shared_memory[];)"}
{"predict": "(35,OpenMP,#pragma omp parallel for)", "label": "(28, MPI, MPICHECK(MPI_Mrecv(&dims, count, MPI_INT64_T, &msg, &status));); (36, MPI, MPICHECK(MPI_Get_count(&status, MPI_UINT8_T, &count));); (40, MPI, MPICHECK(MPI_Mrecv(tensor->data(), count, MPI_UINT8_T, &msg, &status));); (16, MPI, MPICHECK(MPI_Mrecv(&mpiId, count, MPI_UINT64_T, &msg, &status));); (20, MPI, MPICHECK(MPI_Get_count(&status, MPI_INT64_T, &count));); (11, MPI, MPICHECK(MPI_Get_count(&status, MPI_UINT64_T, &count));)"}
{"predict": "(11,OpenMP,#pragma omp parallel for)", "label": "(15, CUDA, extern __shared__ float sdata[];)"}
{"predict": "(14,OpenMP,#pragma omp parallel sections), (14,OpenMP,#pragma omp section), (20,OpenMP,#pragma omp section), (26,OpenMP,#pragma omp section), (32,OpenMP,#pragma omp section), (38,OpenMP,#pragma omp section), (44,OpenMP,#pragma omp section)", "label": "(11, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(25,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp single nowait)", "label": "(15, OpenCL, clKernelWrapper kernel = clCreateKernel(prog, \"usersemantic_test\", &error);); (22, OpenCL, error |= clSetKernelArg(kernel, 0, sizeof(dst), &dst);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for)", "label": "(15, CUDA, getLastCudaError(\"mergeElementaryIntervalsKernel<1> failed\\n\");); (19, CUDA, getLastCudaError(\"mergeElementaryIntervalsKernel<0> failed\\n\");)"}
{"predict": ": \n123:     /* ------------------------------------------------------------\n124:        SET UP THE SUPERLU OPTIONS. \n125:        ------------------------------------------------------------*/\n126:     set_default_options_dist(&options);\n127:     options.PrintStat = YES;\n128: \n129:     /* ------------------------------------------------------------\n130:        PERFORM THE LU DECOMPOSITION. \n131:        ------------------------------------------------------------*/\n132:     dCreate_SuperNode_Matrix_dist(&LUstruct);\n133:     dCreate_Dense_Matrix_dist(&ScalePermstruct, m, n, a, m, SLU_D, SLU_GE);\n134:     dCreate_Dense_Matrix_dist(&ScalePermstruct.SavePerm, m, n, a, m, SLU_D, SLU_GE);\n135:     dCreate_Dense_Matrix_dist(&ScalePermstruct.R, m, n, a, m, SLU_D, SLU_GE);\n136:     dCreate_Dense_Matrix_dist(&ScalePermstruct.C, m, n, a, m, SLU_D, SLU_GE);\n137:     StatInit(&stat);\n138:     dgstrf_dist(&options, &A, &ScalePermstruct, &LUstruct, &grid, &stat, &info);\n139: \n140:     if ( info ) {\n141: \tprintf(\"dgstrf() error returns INFO= %d\\n\", info);\n142: \texit(info);\n143:     }\n144: \n145:     /* ------------------------------------------------------------\n146:        SOLVE THE LINEAR SYSTEM. \n147:        ------------------------------------------------------------*/\n148:     dCreate_Dense_Matrix_dist(&ScalePermstruct.B, m, nrhs, b, ldb, SLU_D, SLU_GE);\n149:     dCreate_Dense_Matrix_dist(&ScalePermstruct.X, n, nrhs, xtrue, ldx, SLU_D, SLU_GE);\n150:     dgstrs_dist(trans, &ScalePermstruct, &LUstruct, &grid, &ScalePermstruct.B, &stat, &info);\n151: \n152:     if ( info ) {\n153: \tprintf(\"dgstrs() error returns INFO= %d\\n\", info);\n154: \texit(info);\n15", "label": "(87, MPI, MPI_Bcast( asub, nnz, mpi_int_t,  0, grid.comm );); (224, MPI, MPI_Finalize();); (83, MPI, MPI_Bcast( &nnz, 1,   mpi_int_t,  0, grid.comm );); (83, MPI, MPI_Bcast( a,    nnz, MPI_DOUBLE, 0, grid.comm );); (22, MPI, MPI_Init( &argc, &argv );); (83, MPI, MPI_Bcast( xa,   n+1, mpi_int_t,  0, grid.comm );); (78, MPI, MPI_Bcast( &n,   1,   mpi_int_t,  0, grid.comm );); (76, MPI, MPI_Bcast( &m,   1,   mpi_int_t,  0, grid.comm );)"}
{"predict": "(19,OpenMP,#pragma omp parallel for private(j, t) reduction(+:r))", "label": "(26, MPI, MPI_Isend(buf, cnt, MPI_INT, thread_num, tag, MPI_COMM_WORLD, &r[thread_num - 1]);); (36, MPI, t = MPI_Wtime() - t;); (21, MPI, t = MPI_Wtime();); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (29, MPI, MPI_Waitall(wsize - 1, r, MPI_STATUSES_IGNORE);); (23, MPI, MPI_Wait(&r[thread_num - 1], MPI_STATUS_IGNORE);)"}
{"predict": "(3,OpenMP,#pragma omp parallel for), (5,OpenMP,#pragma omp parallel for), (7,OpenMP,#pragma omp parallel for), (9,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for)", "label": "(8, OpenMP, #pragma omp target parallel for simd order(); (13, OpenMP, #pragma omp target parallel for simd order(concurrent); (15, OpenMP, #pragma omp target parallel for simd order(concurrent)); (2, OpenMP, #pragma omp target parallel for simd order); (-2, OpenMP, #pragma omp target parallel for simd lastprivate(s) firstprivate(s)); (6, OpenMP, #pragma omp target parallel for simd order(none)"}
{"predict": "(40,OpenMP,#pragma omp parallel for)", "label": "(7, CUDA, __shared__ block_radix_sort_t::TempStorage temp_storage;)"}
{"predict": "(58,OpenMP,#pragma omp parallel for), (68,OpenMP,#pragma omp parallel for), (76,OpenMP,#pragma omp parallel for)", "label": "(68, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (2, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (59, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(inData->basePtr(), inVec.data(), inBufSize, cudaMemcpyHostToDevice));); (72, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(testVec.data(), outData->basePtr(), outBufSize, cudaMemcpyDeviceToHost));); (65, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamDestroy(stream));)"}
{"predict": "(22,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp for)", "label": "(26, CUDA, CUDA_CHECK(cudaMalloc((void **)&dev_max, sizeof(FP) * SEQLEN * DIM));); (45, CUDA, cudaFree(dev_max);); (28, CUDA, CUDA_CHECK(cudaMemcpy(dev_denom, host_denom, sizeof(FP) * SEQLEN * DIM, cudaMemcpyHostToDevice));); (24, CUDA, CUDA_CHECK(cudaMalloc((void **)&dev_denom, sizeof(FP) * SEQLEN * DIM));); (24, CUDA, CUDA_CHECK(cudaMalloc((void **)&QK, sizeof(FP) * SEQLEN * SEQLEN));); (24, CUDA, CUDA_CHECK(cudaMemcpy(dev_max, host_max, sizeof(FP) * SEQLEN * DIM, cudaMemcpyHostToDevice));); (39, CUDA, cudaFree(QK);); (40, CUDA, cudaFree(dev_denom);)"}
{"predict": "(13,OpenMP,#pragma omp parallel), (26,OpenMP,#pragma omp for schedule(runtime) reduction(+:total) private(idx))", "label": "(19, OpenMP, #pragma omp for schedule(runtime) reduction(+:total) linear(idx:1)); (13, OpenMP, thread_num = omp_get_thread_num();); (15, OpenMP, #pragma omp master); (7, OpenMP, #pragma omp parallel shared(data, std::cerr, total, idx) default(none))"}
{"predict": "(75,OpenMP,#pragma omp parallel for),", "label": "(62, MPI, MPI_Type_commit(newtype);)"}
{"predict": "(18,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for)", "label": "(21, CUDA, $GPU(cudaMemcpy(compare, gpu_outputs, sizeof(typename types<params>::output_t)*count, cudaMemcpyDeviceToHost));); (15, CUDA, $GPU(cudaMalloc((void **)&gpu_outputs, sizeof(typename types<params>::output_t)*count));); (15, CUDA, $GPU(cudaMemset(gpu_outputs, 0, sizeof(typename types<params>::output_t)*count));); (58, CUDA, $GPU(cudaFree(gpu_outputs));)"}
{"predict": "(30,OpenMP,#pragma omp parallel for)", "label": "(38, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (40, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "5:          // trsm kernel\n136:          if(myrow==krow){\n137:              for (j = 0; j < nrhs; j++) {\n138:                  RHS_ITERATE(i)\n139:                      for (l = 0; l < knsupc; l++) {\n140:                          Uinv = &Uinv_bc_dat[Uinv_bc_offset[lk] + l * knsupc];\n141:                          temp1 = zero;\n142:                          RHS_ITERATE(k)\n143:                              z_mult(&temp1, &Uinv[k], &x[k + ii + j*knsupc]);\n144:                          z_div(&x[i + ii + j*knsupc], &x[i + ii + j*knsupc], &temp1);\n145:                      }\n146:              }\n147:          }\n148:          __syncthreads();\n149: \n150:          //forward message\n151:          if(myrow==krow){\n152:              RHS_ITERATE(j)\n153:                  for (i = tid; i < knsupc; i+=block_size){\n154:                      z_add(&zready_x[i + ii + j*knsupc],\n155:                            &x[i + ii + j*knsupc],\n156:                            &lsum[i + il + j*knsupc]);\n157:                  }\n158:          }\n159:          __syncthreads();\n160: \n161:          // printf(\"  After kernel:   %i %i %i %i %i %i %i %i\\n\", threadIdx_x, blockIdx_x, grid->npcol, nsupers,myrow,krow,bid,tid);\n162: \n163:          if(tid==0){\n164:              int mod = bmod[lib*aln_i];\n165:              bmod_tmp = mod;\n166:              bmod[lib*aln_i] = mod-1;\n167:              __threadfence();\n168:          }\n169:          __syncthreads();\n170: \n171:      }\n172:      else{\n1", "label": "(45, CUDA, __shared__ doublecomplex temp2[MAXSUPER];)"}
{"predict": "The provided code snippet is a unit test and does not contain any loops or sections that are inherently parallelizable. However, if we assume that there are loops or sections in a larger context that could benefit from parallelization, I'll provide an example based on a hypothetical loop that might be added for demonstration purposes.\n\nLet's assume there is a loop in the code that processes data in parallel. Here is an example:\n\n```C++\n1: HWTEST_F(BufferUnmapTest, givenBufferWithoutSharingHandlerWhenUnmappingThenDontUseEnqueueWriteBuffer) {\n2:     MockContext context(pClDevice);\n3:     MockCommandQueueHw<FamilyType> cmdQ(&context, pClDevice, nullptr);\n4: \n5:     auto retVal = CL_SUCCESS;\n6:     std::unique_ptr<Buffer> buffer(Buffer::create(&context, CL_MEM_ALLOC_HOST_PTR, 123, nullptr, retVal));\n7:     EXPECT_EQ(CL_SUCCESS, retVal);\n8: \n9:     EXPECT_EQ(nullptr, buffer->peekSharingHandler());\n10: \n11:     EXPECT_EQ(CL_SUCCESS, retVal);\n12: \n13:     EXPECT_EQ(CL_SUCCESS, retVal);\n14: \n15:     EXPECT_EQ(0u, cmdQ.enqueueWriteBufferCounter);\n16: \n17:     // Hypothetical loop that could be parallelized\n18:     for (int i = 0; i < 1000; i++) {\n19:         // Perform some operation\n20:         buffer->data[i] = i * 2;\n21:     }\n22: }\n```\n\nIn this case, the loop starting at line 18 can be parallelized using OpenMP. Here is the parallelization instruction:\n\n(18,OpenMP,#pragma omp parallel for)", "label": "(10, OpenCL, auto mappedPtr = clEnqueueMapBuffer(&cmdQ, buffer.get(), CL_TRUE, CL_MAP_READ, 0, 1, 0, nullptr, nullptr, &retVal);); (12, OpenCL, retVal = clEnqueueUnmapMemObject(&cmdQ, buffer.get(), mappedPtr, 0, nullptr, nullptr);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(24, OpenMP, #pragma omp target parallel firstprivate (a, b, c, d, f)); (43, OpenMP, #pragma omp target parallel shared(i)); (46, OpenMP, #pragma omp target parallel firstprivate(j)); (19, OpenMP, #pragma omp target parallel firstprivate (S1)); (6, OpenMP, #pragma omp target parallel firstprivate (); (11, OpenMP, #pragma omp target parallel firstprivate (argc,); (28, OpenMP, #pragma omp target parallel firstprivate(S2::S2s)); (35, OpenMP, #pragma omp target parallel private(i), firstprivate(i)); (12, OpenMP, #pragma omp target parallel firstprivate (argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (29, OpenMP, #pragma omp target parallel firstprivate(e, g)); (30, OpenMP, #pragma omp target parallel firstprivate(h, B::x)); (19, OpenMP, #pragma omp target parallel firstprivate(ca, z)); (16, OpenMP, #pragma omp target parallel firstprivate(ba) allocate(omp_thread_mem_alloc: ba)); (-1, OpenMP, #pragma omp target parallel firstprivate ()); (36, OpenMP, #pragma omp target parallel firstprivate(m)); (-1, OpenMP, #pragma omp target parallel firstprivate (argc); (-8, OpenMP, #pragma omp target parallel firstprivate); (29, OpenMP, #pragma omp target parallel firstprivate(i)); (18, OpenMP, #pragma omp target parallel firstprivate(S2::S2sc)); (7, OpenMP, #pragma omp target parallel firstprivate (argv[1])); (-2, OpenMP, #pragma omp target parallel firstprivate (argc > 0 ? argv[1] : argv[2])); (11, OpenMP, #pragma omp target parallel firstprivate(da))"}
{"predict": "(5,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for)", "label": "(8, TBB, start = tbb::tick_count::now();); (0, TBB, tbb::tick_count now;); (8, TBB, now = tbb::tick_count::now();)"}
{"predict": "(5,OpenMP,#pragma omp parallel for private(i))", "label": "(17, OpenMP, #pragma omp barrier); (2, OpenMP, #pragma omp parallel); (3, OpenMP, int me = omp_get_thread_num();)"}
{"predict": "135: \n136:         if( gDebugTrace )\n137:             log_info( \"   at rounding size %d,%d\\n\", (int)imageInfo.width, (int)imageInfo.arraySize );\n138: \n139:         int retCode = test_read_image(context, queue, kernel,\n140:                                       &imageInfo, imageSampler,\n141:                                       floatCoords, outputType, seed);\n142:         if( retCode )\n143:             return retCode;\n144:     }\n145:     return CL_SUCCESS;\n146: }", "label": "(27, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_IMAGE_MAX_ARRAY_SIZE, sizeof( maxArraySize ), &maxArraySize, NULL );); (28, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_GLOBAL_MEM_SIZE, sizeof( memSize ), &memSize, NULL );); (26, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_MAX_MEM_ALLOC_SIZE, sizeof( maxAllocSize ), &maxAllocSize, NULL );); (23, OpenCL, error = clGetDeviceInfo( device, CL_DEVICE_IMAGE2D_MAX_WIDTH, sizeof( maxWidth ), &maxWidth, NULL );)"}
{"predict": "(13,OpenMP,#pragma omp parallel for private(idim))", "label": "(11, CUDA, __shared__ uninitialized_array<T, SUM_SIZE> sum;)"}
{"predict": "(4,OpenMP,#pragma omp parallel for)", "label": "(10, TBB, static const size_t number_of_threads = 8 * tbb::task_scheduler_init::default_num_threads();)"}
{"predict": "(13,OpenMP,#pragma omp parallel for collapse(2) private(local_l2_norm) reduction(+:local_l2_norm))", "label": "(7, CUDA, __shared__ typename BlockReduce::TempStorage temp_storage;)"}
{"predict": "(10,OpenMP,#pragma omp parallel sections), (19,OpenMP,#pragma omp section), (21,OpenMP,#pragma omp section)", "label": "(2, TBB, using namespace tbb::internal;)"}
{"predict": "(4,OpenMP,#pragma omp parallel for), (9,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for), (36,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for), (42,OpenMP,#pragma omp parallel for)", "label": "(2, OpenMP, #pragma omp taskyield allocate(argc)); (3, OpenMP, #pragma omp taskyield untied); (3, OpenMP, #pragma omp taskyield)"}
{"predict": "(6,OpenMP,#pragma omp parallel for)", "label": "(14, CUDA, cudaError_t cuerr = cudaMemcpy(ret_host, ret_ptr, (1)*sizeof(double), cudaMemcpyDeviceToHost);); (3, CUDA, double* d_ptr = thrust::raw_pointer_cast(dv_ptr);); (7, CUDA, thrust::device_ptr<double> ret_dev_ptr = thrust::min_element(thrust::device, dv_ptr, dv_ptr+n);); (10, CUDA, double *ret_ptr = thrust::raw_pointer_cast(ret_dev_ptr);); (-1, CUDA, thrust::device_ptr<double> dv_ptr = thrust::device_malloc(n*sizeof(double));); (14, CUDA, thrust::device_free(dv_ptr);)"}
{"predict": "(17,OpenMP,#pragma omp parallel sections), (17,OpenMP,#pragma omp section), (22,OpenMP,#pragma omp section), (27,OpenMP,#pragma omp section), (32,OpenMP,#pragma omp section), (37,OpenMP,#pragma omp section), (42,OpenMP,#pragma omp section), (47,OpenMP,#pragma omp section), (52,OpenMP,#pragma omp section), (57,OpenMP,#pragma omp section), (62,OpenMP,#pragma omp section)", "label": "(14, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(15,OpenMP,#pragma omp parallel for collapse(2)), (39,OpenMP,#pragma omp parallel for collapse(2))", "label": "(25, MPI, MPI_Type_get_extent(MPI_INT, &tmp_lb, &sizeofint);); (39, MPI, MPI_Pack(a, 1, xpose, buffer, bufsize, &position, MPI_COMM_WORLD);); (55, MPI, MPI_Type_free(&row);); (27, MPI, MPI_Type_commit(&xpose);); (52, MPI, MPI_Type_free(&xpose);); (28, MPI, MPI_Pack_size(1, xpose, MPI_COMM_WORLD, &bufsize);); (23, MPI, MPI_Type_create_hvector(100, 1, sizeofint, row, &xpose);); (31, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (36, MPI, MPI_Unpack(buffer, bufsize, &position, b, 100 * 100, MPI_INT, MPI_COMM_WORLD);); (19, MPI, MPI_Type_vector(100, 1, 100, MPI_INT, &row);)"}
{"predict": "(26,OpenMP,#pragma omp parallel for)", "label": "(34, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (36, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "44:       //this is the step size\n145:       const RealType delta = std::sqrt(tau);\n146: \n147:       for (int step = 0; step < nsteps; step++)\n148:       {\n149:         //start of a loop over all electrons\n150:         for (int iel = 0; iel < nels; iel++)\n151:         {\n152:           //get the starting position of the electron\n153:           PosType u_old = els.R[iel];\n154:           //create a random move\n155:           PosType u = u_old + delta * my_random.spherical(Rmax);\n156:           //convert the position to Cartesian\n157:           PosType u_new = u;\n158:           els.convert2Cart(u_new);\n159: \n160:           //compute the ratio of the trial wavefunction to the old one\n161:           ratio = spo_main.ratio(els, iel, u_new, u_old);\n162: \n163:           //accept or reject the move\n164:           if (ratio > my_random())\n165:           {\n166:             //accept the move\n167:             els.R[iel] = u_new;\n168:             els.update();\n169:             my_accepted++;\n170:           }\n171:           else\n172:           {\n173:             //reject the move\n174:             els.R[iel] = u_old;\n175:           }\n176:           my_vals++;\n177:         }\n178:         //end of a loop over all electrons\n179:       }\n180:       //end of a loop over all steps\n181:     }\n182:     //end of a nested region which corresponds to advanceWalker\n183: \n184:     tt = clock_a.elapsed();\n185:     t0 += tt;\n186:     nspheremoves += nels * nsteps;\n187:     dNumVGHCalls += nels * nsteps;\n188:     vgh_t += vgh_t_loc", "label": "(115, OpenMP, ncrews = omp_get_max_threads();); (98, OpenMP, const int np = omp_get_num_threads();); (189, OpenMP, #pragma omp barrier); (233, OpenMP, double dTotalThreads = omp_get_max_threads();); (268, OpenMP, double nMajorThreads  = omp_get_max_threads();); (137, OpenMP, #pragma omp parallel reduction(+ : vgh_t_loc, v_t_loc, my_vals)); (106, OpenMP, #pragma omp master); (278, OpenMP, cout << \"\\nMPI: \" << nmpi << \" and Threads: \" << omp_get_max_threads();); (92, OpenMP, const int ip = omp_get_thread_num();); (86, OpenMP, #pragma omp parallel reduction(+ : t0, tInit, ratio, vgh_t, val_t, nspheremoves, dNumVGHCalls))"}
{"predict": "cudaEventSynchronize(stop);\n133:     cudaEventElapsedTime(&time, start, stop);\n134:     cudaEventDestroy(start);\n135:     cudaEventDestroy(stop);\n136:     printf(\"pack elementwise: %f ms\\n\", time / (float)R);\n137: \n138:     cudaMemcpy(c_host, c_device, N * sizeof(half), cudaMemcpyDeviceToHost);\n139:   }\n140: \n141:   free(a_host);\n142:   free(b_host);\n143:   free(c_host);\n144:   cudaFree(a_device);\n145:   cudaFree(b_device);\n146:   cudaFree(c_device);\n147:   return 0;\n148: }", "label": "(25, CUDA, cudaMalloc((void **)&c_device, N * sizeof(half));); (49, CUDA, cudaEventSynchronize(stop);); (50, CUDA, cudaEventDestroy(start);); (155, CUDA, cudaFree(a_device);); (32, CUDA, cudaDeviceSynchronize();); (155, CUDA, cudaFree(c_device);); (43, CUDA, cudaEventRecord(stop, 0);); (46, CUDA, cudaEventDestroy(stop);); (151, CUDA, cudaFree(b_device);); (33, CUDA, cudaEventRecord(start, 0);); (11, CUDA, cudaMemcpy(b_device, b_host, N * sizeof(half), cudaMemcpyHostToDevice);); (30, CUDA, cudaEventCreate(&stop);); (6, CUDA, cudaMalloc((void **)&b_device, N * sizeof(half));); (-2, CUDA, cudaMalloc((void **)&a_device, N * sizeof(half));); (37, CUDA, cudaEventElapsedTime(&time, start, stop);); (-1, CUDA, cudaMemcpy(a_device, a_host, N * sizeof(half), cudaMemcpyHostToDevice);); (24, CUDA, cudaEventCreate(&start);); (39, CUDA, cudaMemcpy(c_host, c_device, N * sizeof(half), cudaMemcpyDeviceToHost);)"}
{"predict": "(9,OpenMP,#pragma omp parallel for)", "label": "(27, CUDA, CHECK(cudaMemPrefetchAsync(z, M, cudaCpuDeviceId, NULL));); (20, CUDA, CHECK(cudaMemPrefetchAsync(x, M, device_id, NULL));); (20, CUDA, CHECK(cudaMemPrefetchAsync(y, M, device_id, NULL));); (7, CUDA, CHECK(cudaMallocManaged((void **)&z, M));); (28, CUDA, CHECK(cudaFree(x));); (-2, CUDA, CHECK(cudaGetDevice(&device_id));); (2, CUDA, CHECK(cudaMallocManaged((void **)&x, M));); (26, CUDA, CHECK(cudaFree(y));); (26, CUDA, CHECK(cudaFree(z));); (14, CUDA, CHECK(cudaMemPrefetchAsync(z, M, device_id, NULL));); (19, CUDA, CHECK(cudaDeviceSynchronize());); (-2, CUDA, CHECK(cudaMallocManaged((void **)&y, M));)"}
{"predict": "(49,OpenMP,#pragma omp parallel for collapse(2))", "label": "(14, CUDA, HANDLE_ERROR( cudaGLSetGLDevice( dev ) );); (51, CUDA, HANDLE_ERROR( cudaGraphicsUnmapResources( 1, &resource, NULL ) );); (39, CUDA, HANDLE_ERROR( cudaGraphicsMapResources( 1, &resource, NULL ) );); (4, CUDA, HANDLE_ERROR( cudaChooseDevice( &dev, &prop ) );)"}
{"predict": "Cells just outside of the boundary of part 1 in\n162:                its coordinates */\n163:             int b_ilower[2] = {-1,1}, b_iupper[2] = {-1,2};\n164:             /* The same cells in part 0's coordinates.  Since we use the same\n165:                index space across all parts, the coordinates coincide. */\n166:             int nbor_ilower[2] = {-1,1}, nbor_iupper[2] = {-1,2};\n167:             /* These parts have the same orientation, so no\n168:                rotation is necessary */\n169:             int index_map[2] = {0,1};\n170:             /* These parts map increasing values to increasing values \n171:                for both variables (note: if decreasing maps to increasing, use -1)*/\n172:             int index_dir[2] = {1,1};\n173: \n174:             HYPRE_SStructGridSetNeighborPart(grid, part, b_ilower, b_iupper,\n175:                                              nbor_part, nbor_ilower, nbor_iupper,\n176:                                              index_map, index_dir);\n177:          }\n178:       }\n179: \n180:       /* Finalize the grid assembly.  This step is necessary before using the grid.\n181:          This call is collective on all processors. */\n182:       HYPRE_SStructGridAssemble(grid);\n183:    }\n184: \n185:    /* 2. Set up the graph - this determines the non-zero structure of the matrix. */\n186:    {\n187:       int part;\n188:       int var;\n189:       int nentries;\n190: \n191:       /* Create the graph object */\n192:       HYPRE_SStructGraphCreate(MPI_COMM_WORLD, grid, &graph);\n193: \n194:       /* See explanation of variable types in the GridSetExtents calls above */\n195:       int ndim = 2;\n196:       int nvars = 1;\n197:       HYPRE_SStructVariable vartypes[1] =", "label": "(19, MPI, MPI_Init(&argc, &argv);); (25, MPI, MPI_Finalize();); (19, MPI, MPI_Comm_size(MPI_COMM_WORLD, &num_procs);); (17, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myid);)"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (10,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for)", "label": "(26, OpenMP, #pragma omp for private(int)); (10, OpenMP, #pragma omp for private(,); (35, OpenMP, #pragma omp for private(x)); (2, OpenMP, #pragma omp for private(); (-2, OpenMP, #pragma omp parallel); (26, OpenMP, #pragma omp for private(0)); (10, OpenMP, #pragma omp for private(, )); (34, OpenMP, #pragma omp for private(x, y)); (13, OpenMP, #pragma omp for private()); (36, OpenMP, #pragma omp for private(x, y, z))"}
{"predict": "(4,OpenMP,#pragma omp parallel for)", "label": "(7, TBB, test_reserving_nodes<tbb::flow::write_once_node, size_t>();); (3, TBB, tbb::task_scheduler_init init(p);); (8, TBB, test_extract_on_node<tbb::flow::write_once_node, int>();)"}
{"predict": "(12,OpenMP,#pragma omp parallel sections), (13,OpenMP,#pragma omp section), (14,OpenMP,#pragma omp section)", "label": "(13, MPI, MPI_Isend(buffer, buf_size, MPI_BYTE, 1, tag1, MPI_COMM_WORLD, &request1);); (15, MPI, MPI_Wait(&request2, &status);); (12, MPI, MPI_Isend(buffer, buf_size, MPI_BYTE, 1, tag2, MPI_COMM_WORLD, &request2);); (16, MPI, MPI_Irecv(buffer, buf_size, MPI_BYTE, 0, tag1, MPI_COMM_WORLD, &request2);); (11, MPI, MPI_Wait(&request1, &status);); (13, MPI, MPI_Irecv(buffer, buf_size, MPI_BYTE, 0, tag2, MPI_COMM_WORLD, &request1);)"}
{"predict": "(1,OpenMP,#pragma omp parallel sections), (2,OpenMP,#pragma omp section), (3,OpenMP,#pragma omp section), (4,OpenMP,#pragma omp section), (5,OpenMP,#pragma omp section), (6,OpenMP,#pragma omp section)", "label": "(15, CUDA, cudaDeviceSynchronize();)"}
{"predict": "(13,OpenMP,#pragma omp parallel for private(i, j, k, row_i, row_c))", "label": "(12, OpenMP, #pragma omp parallel for private(i,j,k) num_threads(dtn(border, 1)))"}
{"predict": "(38,OpenMP,#pragma omp parallel for)", "label": "(36, OpenMP, #pragma omp target parallel for map(a, b))"}
{"predict": "(20,OpenMP,#pragma omp parallel for reduction(+:total))", "label": "(4, CUDA, extern __shared__ int shared[];)"}
{"predict": "(18,OpenMP,#pragma omp parallel for)", "label": "(32, OpenMP, omp_target_free(d_a, DefaultDeviceNum);); (23, OpenMP, #pragma omp target update to(p_a)); (12, OpenMP, d_a = (int *)omp_target_alloc(DataSize, DefaultDeviceNum);); (6, OpenMP, int DefaultDeviceNum = omp_get_default_device();); (14, OpenMP, omp_target_associate_ptr(p_a, d_a, DataSize, 0, DefaultDeviceNum);); (23, OpenMP, omp_target_disassociate_ptr(p_a, DefaultDeviceNum);)"}
{"predict": "(6,OpenMP,#pragma omp parallel for)", "label": "(9, TBB, tbb::parallel_pipeline( n_tokens, one_filter, context... );); (5, TBB, tbb::filter<void, void> one_filter( filter_table[i], i_filter );)"}
{"predict": "(4,OpenMP,#pragma omp single)", "label": "(13, MPI, MPI_Bcast(dims, 2, MPI_INT, 0, comm);); (2, MPI, MPI_Comm_rank(comm, &comm_rank);)"}
{"predict": "(22,OpenMP,#pragma omp parallel for)", "label": "(10, CUDA, CHECK_CUDA_ERROR(cudaRuntimeGetVersion(&runtime_version));); (72, CUDA, CHECK_CUDA_ERROR(cudaSetDevice(dev));); (26, CUDA, CHECK_CUDA_ERROR(cudaGetDeviceProperties(&deviceProp, dev));); (3, CUDA, CHECK_CUDA_ERROR(cudaDriverGetVersion(&driver_version));); (20, CUDA, CHECK_CUDA_ERROR(cudaGetDeviceProperties(&deviceProp, i));)"}
{"predict": "Shape(shape_k_span)));\n113:     ASSERT_EQ(AsStatus::ALLSPARK_SUCCESS,\n114:               tensors.at(v_span_name)->SetShape(Shape(shape_v_span)));\n115:     ASSERT_EQ(AsStatus::ALLSPARK_SUCCESS,\n116:               tensors.at(ws_name)->SetShape(Shape(shape_ws)));\n117:     ASSERT_EQ(AsStatus::ALLSPARK_SUCCESS,\n118:               tensors.at(out_name)->SetShape(Shape(shape_out)));\n119: #ifdef KERNEL_SPAN_ATTN_TEST_REF\n120:     ASSERT_EQ(AsStatus::ALLSPARK_SUCCESS,\n121:               tensors.at(ref_name)->SetShape(Shape(shape_ref)));\n122: #endif\n123: \n124:     // init\n125:     if (check_init) {\n126:       tensors.at(span_pool_name)->FillRandom(0, 127);\n127:       tensors.at(q_name)->FillRandom(0, 127);\n128:       tensors.at(k_span_name)->FillRandom(0, 127);\n129:       tensors.at(v_span_name)->FillRandom(0, 127);\n130:       tensors.at(ws_name)->FillRandom(0, 127);\n131:       tensors.at(out_name)->FillRandom(0, 127);\n132: #ifdef KERNEL_SPAN_ATTN_TEST_REF\n133:       tensors.at(ref_name)->FillRandom(0, 127);\n134: #endif\n135:     }\n136: \n137:     // launch kernel\n138:     auto timeBeforeKernel = std::chrono::steady_clock::now();\n139:     SpanAttnV2<T>::Launch(outObj, tensors, param, host_ws, cuda_stream);\n140:     auto timeAfterKernel = std::chrono::steady_clock::now();\n141:     std::cout << \"SpanAttnV2 kernel launch time:\\t\"\n142:               << std::chrono::duration_cast<std::chrono::microseconds>(\n143:                      timeAfterKernel - timeBeforeKernel)\n144:                      .count()\n145:               << \" us\" << std::endl;\n146:", "label": "(90, CUDA, ASSERT_EQ(cudaSuccess, cudaMallocHost(&host_ws, host_ws_bytes));); (297, CUDA, ASSERT_EQ(cudaSuccess, cudaEventDestroy(ref_start));); (230, CUDA, ASSERT_EQ(cudaSuccess, cudaEventElapsedTime(&ref_time, ref_start, ref_end));); (297, CUDA, ASSERT_EQ(cudaSuccess, cudaEventDestroy(out_start));); (297, CUDA, ASSERT_EQ(cudaSuccess, cudaEventDestroy(out_end));); (249, CUDA, ASSERT_EQ(cudaSuccess, cudaEventElapsedTime(&out_time, out_start, out_end));); (201, CUDA, ASSERT_EQ(cudaSuccess, cudaEventCreate(&ref_end));); (292, CUDA, ASSERT_EQ(cudaSuccess, cudaEventDestroy(ref_end));); (244, CUDA, ASSERT_EQ(cudaSuccess, cudaEventRecord(out_end, cuda_stream));); (199, CUDA, ASSERT_EQ(cudaSuccess, cudaEventCreate(&out_start));); (199, CUDA, ASSERT_EQ(cudaSuccess, cudaEventCreate(&out_end));); (220, CUDA, ASSERT_EQ(cudaSuccess, cudaEventSynchronize(ref_end));); (241, CUDA, ASSERT_EQ(cudaSuccess, cudaEventSynchronize(out_end));); (217, CUDA, ASSERT_EQ(cudaSuccess, cudaEventRecord(ref_end, cuda_stream));); (192, CUDA, ASSERT_EQ(cudaSuccess, cudaEventCreate(&ref_start));)"}
{"predict": "4:             t1[x] = d[x] ^ loadGlobal32<uint32_t>(long_state + j);\n125:             storeGlobal32<uint32_t>(long_state + j, t1[x]);\n126: \n127:             // a = b\n128:             a = d[x];\n129:         }\n130: \n131:         //b = c\n132:         d[1] = d[0];\n133: \n134:         //a = d[x] ^ tweak1_2[x & 1];\n135:         a ^= tweak1_2[x & 1];\n136:     }\n137: \n138:     //state[0] = a;\n139:     (d_ctx_a + thread * 4)[sub] = a;\n140: \n141:     //state[1] = d[1];\n142:     (d_ctx_b + thread * 4)[sub] = d[1];\n143: \n144:     //if (ALGO == Algorithm::CN_HEAVY_0 || ALGO == Algorithm::CN_HEAVY_TUBE || ALGO == Algorithm::CN_HEAVY_XHV) {\n145:     if (ALGO == Algorithm::CN_HEAVY_0 || ALGO == Algorithm::CN_HEAVY_TUBE || ALGO == Algorithm::CN_HEAVY_XHV) {\n146:         *(d_ctx_b + threads * 4 + thread) = idx0;\n147:     }\n148: \n149:     if (ALGO == Algorithm::CN_CCX) {\n150:         *(d_ctx_b + threads * 4 + thread * 4 + sub) = float_as_int(conc_var);\n151:     }\n152: \n153:     __syncthreads();\n154: }", "label": "(18, CUDA, __shared__ uint32_t sharedMemory[1024];); (29, CUDA, extern __shared__ uint32_t shuffleMem[];)"}
{"predict": "(6,OpenMP,#pragma omp parallel for private(flag, val_p))", "label": "(6, MPI, MPI_Comm_get_attr(comm, lkey[i], &val_p, &flag);)"}
{"predict": "64: \t\tdisplay->Destroy();\n165: \t}\n166: \n167: \treturn 0;\n168: }", "label": "(89, CUDA, CUDA(cudaGeneratePointGrid(points, GRID_N, GRID_WORLD_SIZE, apptime()));); (89, CUDA, CUDA(cudaDeviceSynchronize());)"}
{"predict": "(3,OpenMP,#pragma omp parallel for), (5,OpenMP,#pragma omp parallel for), (7,OpenMP,#pragma omp parallel for), (9,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for)", "label": "(29, OpenMP, #pragma omp target teams distribute num_teams (-2)); (4, OpenMP, #pragma omp target teams distribute num_teams (); (30, OpenMP, #pragma omp target teams distribute num_teams (-10u)); (5, OpenMP, #pragma omp target teams distribute num_teams ()); (7, OpenMP, #pragma omp target teams distribute num_teams (argc); (15, OpenMP, #pragma omp target teams distribute num_teams (argc + argc+z)); (17, OpenMP, #pragma omp target teams distribute num_teams (argc), num_teams (argc+1)); (28, OpenMP, #pragma omp target teams distribute num_teams (3.14)); (-6, OpenMP, #pragma omp target teams distribute num_teams); (5, OpenMP, #pragma omp target teams distribute num_teams (argc))); (7, OpenMP, #pragma omp target teams distribute num_teams (argc > 0 ? argv[1] : argv[2])); (15, OpenMP, #pragma omp target teams distribute num_teams (S1))"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(14, CUDA, double term = thrust::reduce(thrust::device, v_temp.begin(), v_temp.end(), 0.0, thrust::plus<double>());); (7, CUDA, thrust::device_vector<double> v_temp(n);); (7, CUDA, double* dv_ptr = thrust::raw_pointer_cast(v_temp.data());)"}
{"predict": "(37,OpenMP,#pragma omp barrier), (47,OpenMP,#pragma omp for)", "label": "(2, CUDA, extern __shared__ int sums[];)"}
{"predict": "(12,OpenMP,#pragma omp parallel for private(i) shared(aio_reqlist, events, num_in_progress))", "label": "(24, MPI, double start_time = MPI_Wtime();); (16, MPI, errcode = MPI_Grequest_complete(aio_reqlist[i]->req);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(13, OpenMP, printf(\"main parallel 2 from %d of %d\\n\", omp_get_thread_num(), omp_get_num_threads() );); (1, OpenMP, #pragma omp parallel); (2, OpenMP, printf(\"main parallel 1 from %d of %d\\n\", omp_get_thread_num(), omp_get_num_threads() );)"}
{"predict": "(10,OpenMP,#pragma omp parallel for private(index, kdf_out, aeskey, prepared_key, poly1305_key, nonce, ciphertext) reduction(+:failed))", "label": "(8, OpenMP, #pragma omp parallel for default(none) private(index) shared(count, failed, params, max_threads, local, saved_key, cur_salt, crypt_out)); (15, OpenMP, int t = omp_get_thread_num();)"}
{"predict": "NDRangeKernel(\n153:                  tinfo->tQueue, kernel, 1, NULL, &vectorCount, NULL, 0, NULL,\n154:                  NULL)))\n155:         {\n156:             vlog_error(\"ERROR: clEnqueueNDRangeKernel failed!\\n\");\n157:             return error;\n158:         }\n159: \n160:         // Read the results back\n161:         if ((error = clEnqueueReadBuffer(tinfo->tQueue, tinfo->outBuf[j], CL_TRUE,\n162:                                           0, buffer_size, out[j], 0, NULL,\n163:                                           NULL)))\n164:         {\n165:             vlog_error(\"ERROR: clEnqueueReadBuffer failed!\\n\");\n166:             return error;\n167:         }\n168: \n169:         // Validate the results\n170:         for (size_t k = 0; k < buffer_elements; k++)\n171:         {\n172:             int fail = 0;\n173:             cl_double x = ((cl_double *)out[j])[k];\n174:             cl_double y = job->f->ref_func(((cl_double *)gIn)[k]);\n175:             if (!validate(x, y, ulps, ftz, relaxedMode))\n176:             {\n177:                 if (gSkipCorrect)\n178:                 {\n179:                     if (gVerboseBruteForce)\n180:                     {\n181:                         vlog(\"ERROR: %s failed at %lu: %a != %a\\n\", name,\n182:                              k + base, x, y);\n183:                     }\n184:                     fail = 1;\n185:                 }\n186:             }\n187:             else\n188:             {\n189:                 if (gVerboseBruteForce)\n190:                 {\n191:                     vlog(\"%s passed at %lu: %a == %a\\n\", name, k + base, x, y);\n192:                 }\n193:             }\n194:             if (fail)\n195:             {\n196:", "label": "(146, OpenCL, error = clSetKernelArg(kernel, 1, sizeof(tinfo->inBuf), &tinfo->inBuf);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(20, MPI, MPI_Info_get(i1, (char *) \"key1\", 64, value, &flag);); (28, MPI, MPI_Info_free(&i1);); (11, MPI, MPI_Info_set(i2, (char *) \"key2\", (char *) \"value2\");); (9, MPI, MPI_Info_set(i1, (char *) \"key1\", (char *) \"value1\");); (11, MPI, MPI_Info_get(i1, (char *) \"key2\", 64, value, &flag);); (25, MPI, MPI_Info_free(&i2);); (3, MPI, MPI_Info_create(&i1);); (3, MPI, MPI_Info_create(&i2);)"}
{"predict": "(62,OpenMP,#pragma omp parallel for), (86,OpenMP,#pragma omp parallel for), (101,OpenMP,#pragma omp parallel for)", "label": "(15, CUDA, multi::array<double, 1, thrust::device_allocator<double>> BB(10, 99.);); (34, CUDA, thrust::copy(begin(rotated(A2)[0]), end(rotated(A2)[0]), begin(rotated(B2)[0]));); (16, CUDA, multi::array<std::complex<double>, 1, thrust::device_allocator<std::complex<double> >> BBB_cpy = BBB;); (14, CUDA, multi::array<std::complex<double>, 1, thrust::device_allocator<std::complex<double> >> BBB(10, 99.);); (97, CUDA, thrust::device_vector<int> D = H;); (78, CUDA, multi::array<int, 1, thrust::device_allocator<int>> H2(4, 99); assert(size(H2) == 4);); (69, CUDA, thrust::host_vector<int> H(4);); (21, CUDA, thrust::copy(begin(B), end(B), begin(C));)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(8, OpenMP, const std::string comp_name(x.first);)"}
{"predict": "(19,OpenMP,#pragma omp parallel for)", "label": "(27, OpenCL, status = clBuildProgram(program, 1, &device, buildOptions, NULL, NULL);); (9, OpenCL, cl_program program = clCreateProgramWithSource(context,1, &source, NULL, &status);)"}
{"predict": "(35,OpenMP,#pragma omp parallel for private(j,count))", "label": "(15, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(5,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for)", "label": "(2, OpenMP, #pragma omp target teams distribute parallel for simd)"}
{"predict": "(21,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for)", "label": "(25, MPI, MPI_Win_unlock(rank, win);); (48, MPI, MPI_Win_unlock_all(win);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (31, MPI, MPI_Win_flush_local(target_rank, win);); (45, MPI, MPI_Win_free(&win);); (23, MPI, MPI_Win_lock_all(0, win);); (11, MPI, MPI_Win_create(winbuf, BUFCNT * sizeof(int), sizeof(int), MPI_INFO_NULL, MPI_COMM_WORLD, &win);); (26, MPI, MPI_Get(locbuf, BUFCNT, MPI_INT, target_rank, 0, BUFCNT, MPI_INT, win);); (18, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(19,OpenMP,#pragma omp parallel for private(start, chunk, ret))", "label": "(27, MPI, return MPI_Waitall(num_ep, block_coll_request, MPI_STATUS_IGNORE);); (8, MPI, return MPI_Wait(&tmprequest, MPI_STATUS_IGNORE);)"}
{"predict": "132:             size_t slicePadding = gEnablePitch ? 3 : 0;\n133: \n134:             set_image_dimensions(&imageInfo2Darray, sizes2DArray[j][0],\n135:                                  sizes2DArray[j][1], sizes2DArray[j][2],\n136:                                  rowPadding, slicePadding);\n137:             set_image_dimensions(&imageInfo2D, sizes2D[i][0], sizes2D[i][1], 0,\n138:                                  rowPadding, slicePadding);\n139: \n140:             if( gTestMipmaps )\n141:             {\n142:                 imageInfo2D.num_mip_levels =\n143:                     (cl_uint)random_log_in_range(\n144:                         2,\n145:                         (int)compute_max_mip_levels(\n146:                             imageInfo2D.width, imageInfo2D.height, 0),\n147:                         seed);\n148:                 imageInfo2Darray.num_mip_levels =\n149:                     (cl_uint)random_log_in_range(\n150:                         2,\n151:                         (int)compute_max_mip_levels(\n152:                             imageInfo2Darray.width,\n153:                             imageInfo2Darray.height, 0),\n154:                         seed);\n155:                 imageInfo2D.rowPitch = imageInfo2D.width\n156:                     * get_pixel_size(imageInfo2D.format);\n157:                 imageInfo2D.slicePitch = 0;\n158:                 imageInfo2Darray.rowPitch = imageInfo2Darray.width\n159:                     * get_pixel_size(imageInfo2Darray.format);\n160:                 imageInfo2Darray.slicePitch =\n161:                     imageInfo2Darray.rowPitch * imageInfo2Darray.height;\n162:             }\n163: \n164:             if( gDebugTrace )\n165:             {\n166:                 if (reverse)\n167:                     log_info(\"   at size %d,%d,%d to %d,%d\\n\",\n168:                              (int)imageInfo2Darray.width,\n169:                              (int)imageInfo2", "label": "(28, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_IMAGE_MAX_ARRAY_SIZE, sizeof( maxArraySize ), &maxArraySize, NULL );); (25, OpenCL, int error = clGetDeviceInfo( device, CL_DEVICE_IMAGE2D_MAX_WIDTH, sizeof( maxWidth ), &maxWidth, NULL );); (28, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_GLOBAL_MEM_SIZE, sizeof( memSize ), &memSize, NULL );); (26, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_MAX_MEM_ALLOC_SIZE, sizeof( maxAllocSize ), &maxAllocSize, NULL );); (23, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_IMAGE2D_MAX_HEIGHT, sizeof( maxHeight ), &maxHeight, NULL );)"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(3, CUDA, extern __shared__ int shist[];)"}
{"predict": "(28,OpenMP,#pragma omp parallel num_threads(nt) private(end,left))", "label": "(33, OpenMP, #pragma omp barrier); (28, OpenMP, int me    = omp_get_thread_num();); (25, OpenMP, #pragma omp parallel reduction(min:dtmin) reduction(max:dtmax) reduction(+:dtavg)); (-1, OpenMP, int nt = omp_get_max_threads();); (35, OpenMP, #pragma omp critical)"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(20, CUDA, checkCudaErrors(cudaSetDevice(0));)"}
{"predict": "(27,OpenMP,#pragma omp parallel for reduction(+:ccount)), (37,OpenMP,#pragma omp parallel for reduction(+:elem_cnt))", "label": "(3, TBB, typedef std::vector<T, oneapi::tbb::tbb_allocator<T> > container_type;); (33, TBB, oneapi::tbb::flattened2d<ets_nokey_type> fvs = flatten2d(vs3);); (11, TBB, typedef typename oneapi::tbb::enumerable_thread_specific< container_type, Allocator<container_type>, oneapi::tbb::ets_key_per_instance > ets_tlskey_type;); (9, TBB, typedef typename oneapi::tbb::enumerable_thread_specific< container_type, Allocator<container_type>, oneapi::tbb::ets_no_key > ets_nokey_type;); (44, TBB, oneapi::tbb::flattened2d<ets_nokey_type> fvs2 = flatten2d(vs3, vs3.begin(), std::next(vs3.begin()));); (23, TBB, oneapi::tbb::parallel_reduce ( vs3.range(1), pvrb );); (0, TBB, oneapi::tbb::global_control gc(oneapi::tbb::global_control::max_allowed_parallelism, p);); (10, TBB, oneapi::tbb::parallel_for ( oneapi::tbb::blocked_range<int> (0, N*p, RANGE_MIN), parallel_vector_for_body<T, Allocator>( vs, allocator_name ) );); (42, TBB, const oneapi::tbb::flattened2d<ets_nokey_type>& cfvs2(fvs2);)"}
{"predict": "(21,OpenMP,#pragma omp atomic read), (60,OpenMP,#pragma omp atomic read)", "label": "(65, OpenMP, #define OP_GOMP_CRITICAL_READ(OP, FLAG))"}
{"predict": "(37,OpenMP,#pragma omp parallel for private(j))", "label": "(17, OpenMP, s += gsl_linalg_cholesky_decomp_unit(v, dv);)"}
{"predict": "(19,OpenMP,#pragma omp parallel for)", "label": "(19, OpenMP, #pragma omp target teams distribute parallel for simd if (parallel: false)); (10, OpenMP, #pragma omp target teams distribute parallel for simd if(simd: true) nontemporal(Arg))"}
{"predict": "(12,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for private(i) reduction(+:bandwidths[i]))", "label": "(19, CUDA, cudaSetDevice(currentDevice);)"}
{"predict": "8:     foo();\n99: #pragma omp target\n100: #pragma omp teams\n101:   for (int i = 0; i < 10; ++i)\n102:     foo();\n103: #pragma omp target\n104: #pragma omp teams\n105:   for (int i = 0; i < 10; ++i)\n106:     foo();\n107: #pragma omp target\n108: #pragma omp teams\n109:   for (int i = 0; i < 10; ++i)\n110:     foo();\n111: #pragma omp target\n112: #pragma omp teams\n113:   for (int i = 0; i < 10; ++i)\n114:     foo();\n115: #pragma omp target\n116: #pragma omp teams\n117:   for (int i = 0; i < 10; ++i)\n118:     foo();\n119: #pragma omp target\n120: #pragma omp teams\n121:   for (int i = 0; i < 10; ++i)\n122:     foo();\n123: #pragma omp target\n124: #pragma omp teams\n125:   for (int i = 0; i < 10; ++i)\n126:     foo();\n127: #pragma omp target\n128: #pragma omp teams\n129:   for (int i = 0; i < 10; ++i)\n130:     foo();\n131: #pragma omp target\n132: #pragma omp teams\n133:   for (int i = 0; i < 10; ++i)\n134:     foo();\n135: #pragma omp target\n136: #pragma omp teams\n137:   for (int i = 0; i < 10; ++i)\n138:     foo();\n139: #pragma omp target\n140: #pragma omp teams\n141:   for (int i = 0; i < 10; ++", "label": "(109, OpenMP, #pragma omp distribute parallel for reduction(^ : fl)); (154, OpenMP, #pragma omp distribute parallel for reduction(+ : r)); (117, OpenMP, #pragma omp distribute parallel for reduction(&& : S2::S2sc)); (126, OpenMP, #pragma omp distribute parallel for reduction(+ : h, k, B::x)); (164, OpenMP, #pragma omp distribute parallel for reduction(+ : fl, z)); (39, OpenMP, #pragma omp distribute parallel for reduction(\\)); (6, OpenMP, #pragma omp target); (87, OpenMP, #pragma omp distribute parallel for reduction(+ : ba)); (81, OpenMP, #pragma omp distribute parallel for reduction(max : h.b)); (60, OpenMP, #pragma omp distribute parallel for reduction(&& : argc)); (161, OpenMP, #pragma omp parallel reduction(* : fl)); (28, OpenMP, #pragma omp distribute parallel for reduction(*)); (102, OpenMP, #pragma omp distribute parallel for reduction(&& : S2::S2s)); (6, OpenMP, #pragma omp distribute parallel for reduction +); (85, OpenMP, #pragma omp distribute parallel for reduction(* : ca)); (39, OpenMP, #pragma omp distribute parallel for reduction(| : argc,); (118, OpenMP, #pragma omp distribute parallel for reduction(+ : o)); (125, OpenMP, #pragma omp parallel private(k)); (6, OpenMP, #pragma omp distribute parallel for reduction(); (143, OpenMP, #pragma omp distribute parallel for reduction(max : j)); (44, OpenMP, #pragma omp distribute parallel for reduction(~ : argc)); (138, OpenMP, #pragma omp parallel reduction(min : i)); (143, OpenMP, #pragma omp parallel private(fl)); (26, OpenMP, #pragma omp distribute parallel for reduction(foo : argc); (50, OpenMP, #pragma omp distribute parallel for reduction(^ : S1)); (34, OpenMP, #pragma omp distribute parallel for reduction(|| : argc > 0 ? argv[1] : argv[2])); (-12, OpenMP, #pragma omp distribute parallel for reduction); (153, OpenMP, #pragma omp distribute parallel for reduction(+ : m)); (76, OpenMP, #pragma omp distribute parallel for reduction(- : da)); (95, OpenMP, #pragma omp distribute parallel for reduction(& : e, g)); (144, OpenMP, #pragma omp distribute parallel for reduction(+ : fl)); (-2, OpenMP, #pragma omp distribute parallel for reduction(-); (47, OpenMP, #pragma omp distribute parallel for reduction(+ : a, b, c, d, f)); (112, OpenMP, #pragma omp distribute parallel for reduction(+ : p), reduction(+ : p)); (151, OpenMP, #pragma omp distribute parallel for reduction(task, + : m)); (49, OpenMP, #pragma omp distribute parallel for reduction(min : a, b, c, d, f)); (122, OpenMP, #pragma omp parallel shared(i)); (102, OpenMP, #pragma omp distribute parallel for private(i), reduction(+ : j), reduction(+ : q)); (-4, OpenMP, #pragma omp distribute parallel for reduction()); (-26, OpenMP, #pragma omp teams)"}
{"predict": "(45,OpenMP,#pragma omp parallel for collapse(2)),", "label": "(41, CUDA, cudaError_t result = cudaGetLastError();); (41, CUDA, EXPECT_EQ(result, cudaSuccess) << \" - CUDA error: \" << cudaGetErrorString(result);)"}
{"predict": "60, &aes_key, iv, AES_DECRYPT);\n109: \t\t\t}\n110: \t\t\tSHA512_Init(&ctx);\n111: \t\t\tSHA512_Update(&ctx, out, 90);\n112: \t\t\tSHA512_Final(hash, &ctx);\n113: \t\t\tp = (struct KGEncryptedBlock64 *)out;\n114: \t\t\tcracked[index] = (0 == memcmp(hash, p->digest, kDigestSize64));\n115: \t\t}\n116: \t}\n117: \treturn count;\n118: }", "label": "(8, OpenMP, #pragma omp parallel for)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)\n(15,OpenMP,#pragma omp parallel for)\n(18,OpenMP,#pragma omp parallel for)\n(36,OpenMP,#pragma omp parallel for)\n(53,OpenMP,#pragma omp parallel for)", "label": "(54, OpenMP, #pragma omp target parallel for map(alloc : c)); (64, OpenMP, #pragma omp barrier); (44, OpenMP, #pragma omp target exit data map(from : a)); (61, OpenMP, #pragma omp target update from(c) nowait); (21, OpenMP, #pragma omp target enter data map(to : a)); (30, OpenMP, #pragma omp target parallel for)"}
{"predict": ";\n110:     printf(\"Number of ranks           = %d\\n\", Num_procs);\n111:     printf(\"Update ratio            = %d\\n\", update_ratio);\n112:     printf(\"Log2 tablesize          = %d\\n\", log2tablesize);\n113:     printf(\"Local tablesize         = %ld\\n\", loctablesize);\n114:     printf(\"Aggregate tablesize     = %ld\\n\", tablesize);\n115:     printf(\"Vector length           = %d\\n\", nstarts);\n116:     printf(\"Number of updates       = %ld\\n\", (s64Int)update_ratio*tablesize);\n117:     printf(\"Memory per rank         = %ld bytes\\n\", tablespace);\n118:   }\n119: \n120: ENDOFTESTS:\n121: \n122:   /* broadcast the error flag and abort if necessary                             */\n123:   MPI_Bcast(&error, 1, MPI_LONG, root, MPI_COMM_WORLD);\n124:   if (error) {\n125:     MPI_Abort(MPI_COMM_WORLD, error);\n126:   }\n127: \n128:   /* initialize random number generator                                          */\n129:   ran = (u64Int *) malloc(LOOKAHEAD * sizeof(u64Int));\n130:   if (!ran) {\n131:     printf(\"ERROR: Could not allocate space for random numbers\\n\");\n132:     MPI_Abort(MPI_COMM_WORLD, 1);\n133:   }\n134: \n135:   for (i=0; i<LOOKAHEAD; i++) {\n136:     ran[i] = PRK_starts(i);\n137:   }\n138: \n139:   /* allocate send bucket buffers                                              */\n140:   ranSendBucket = (u64Int **) malloc(Num_procs * sizeof(u64Int *));\n141:   if (!ranSendBucket) {\n142:     printf(\"ERROR: Could not allocate space for send bucket buffers\\n\");\n133:     MPI_Abort(MPI_COMM_WORLD, 1);\n143:   }\n144: \n145:   /* allocate receive bucket buffers                                           */", "label": "(162, MPI, MPI_Bcast(&log2nstarts,      1, MPI_INT,           root, MPI_COMM_WORLD);); (46, MPI, MPI_Comm_size(MPI_COMM_WORLD,&Num_procs);); (155, MPI, MPI_Bcast(&log2nproc,        1, MPI_INT,           root, MPI_COMM_WORLD);); (303, MPI, MPI_Finalize();); (154, MPI, MPI_Bcast(&log2tablesize,    1, MPI_INT,           root, MPI_COMM_WORLD);); (155, MPI, MPI_Bcast(&log2update_ratio, 1, MPI_INT,           root, MPI_COMM_WORLD);); (42, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&my_ID);); (159, MPI, MPI_Bcast(&nupdate,          1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (153, MPI, MPI_Bcast(&nstarts,          1, MPI_INT,           root, MPI_COMM_WORLD);); (154, MPI, MPI_Bcast(&tablesize,        1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (155, MPI, MPI_Bcast(&tablespace,       1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (153, MPI, MPI_Bcast(&loctablesize,     1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (213, MPI, MPI_Barrier(MPI_COMM_WORLD);); (146, MPI, MPI_Bcast(&update_ratio,     1, MPI_INT,           root, MPI_COMM_WORLD);); (280, MPI, MPI_Reduce(&error, &tot_error, 1, MPI_LONG_LONG_INT, MPI_SUM, root, MPI_COMM_WORLD);)"}
{"predict": "(5,OpenMP,#pragma omp parallel for)", "label": "(45, MPI, MPI_Op_free(&dtcmp_reduceop_randroot);); (49, MPI, MPI_Op_free(&dtcmp_reduceop_mms_3uint64t);); (64, MPI, MPI_Comm_free(&dtcmp_comm_self);); (52, MPI, MPI_Type_free(&dtcmp_type_3int);); (56, MPI, MPI_Type_free(&dtcmp_type_3uint64t);)"}
{"predict": "(13,OpenMP,#pragma omp parallel sections), (73,OpenMP,#pragma omp section), (100,OpenMP,#pragma omp section)", "label": "(70, OpenMP, kmp_gomp_depends_info_t gomp_depends(depend);); (103, OpenMP, __kmpc_omp_wait_deps(&loc, gtid, ndeps, dep_list, 0, NULL);); (77, OpenMP, __kmpc_omp_task(&loc, gtid, task);); (74, OpenMP, __kmpc_omp_task_with_deps(&loc, gtid, task, ndeps_cnv, dep_list, 0, NULL);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for)", "label": "(14, MPI, MPI_Comm_size(comm, &size);); (19, MPI, MPI_Comm_group(dupcomm, &g2);); (28, MPI, MPI_Group_free(&g2);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &worldrank);); (28, MPI, MPI_Comm_free(&dupcomm);); (23, MPI, MPI_Group_free(&basegroup);); (12, MPI, MPI_Comm_group(newcomm, &g1);); (12, MPI, MPI_Comm_dup(comm, &dupcomm);); (15, MPI, mpi_errno = MPI_Group_difference(g1, g2, NULL);); (22, MPI, MPI_Comm_free(&newcomm);); (14, MPI, MPI_Error_class(mpi_errno, &errclass);); (4, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (5, MPI, MPI_Comm_split(comm, 0, size - rank, &newcomm);); (16, MPI, MPI_Group_free(&g1);); (-1, MPI, MPI_Comm_rank(comm, &rank);); (-3, MPI, MPI_Comm_group(comm, &basegroup);)"}
{"predict": "(21,OpenMP,#pragma omp parallel for reduction(+:within_std_dev,within_2_std_dev))", "label": "(9, CUDA, HANDLE_ERROR(cudaMalloc((void**)&colored_noise_d, sizeof(float) * full_buffer_size));); (11, CUDA, cudaStreamCreate(&stream);); (18, CUDA, HANDLE_ERROR(cudaStreamSynchronize(stream));)"}
{"predict": "(47,OpenMP,#pragma omp parallel for collapse(4)), (74,OpenMP,#pragma omp parallel for collapse(2))", "label": "(41, OpenMP, #pragma omp target teams distribute collapse(4, )); (90, OpenMP, #pragma omp target teams distribute collapse(2) firstprivate(i)); (15, OpenMP, #pragma omp target teams distribute collapse(,); (33, OpenMP, #pragma omp target teams distribute collapse(4,); (74, OpenMP, #pragma omp target teams distribute collapse(-5)); (8, OpenMP, #pragma omp target teams distribute collapse()); (15, OpenMP, #pragma omp target teams distribute collapse(, )); (79, OpenMP, #pragma omp target teams distribute collapse(5 - 5)); (66, OpenMP, #pragma omp target teams distribute collapse(foo())); (73, OpenMP, #pragma omp target teams distribute collapse(0)); (-1, OpenMP, #pragma omp target teams distribute collapse(); (55, OpenMP, #pragma omp target teams distribute collapse(4, 8)); (38, OpenMP, #pragma omp target teams distribute collapse(4 4)); (32, OpenMP, #pragma omp target teams distribute collapse(4)); (-9, OpenMP, #pragma omp target teams distribute collapse); (40, OpenMP, #pragma omp target teams distribute collapse(4, , 4)); (15, OpenMP, #pragma omp target teams distribute collapse(4); (9, OpenMP, #pragma omp target teams distribute collapse 4)); (52, OpenMP, #pragma omp target teams distribute collapse(2.5))"}
{"predict": "(23,OpenMP,#pragma omp parallel for)", "label": "(64, CUDA, CUDA_SAFE_CALL( cudaMemcpy(c, cd, size*sizeof(double), cudaMemcpyDeviceToHost) );); (67, CUDA, CUDA_SAFE_CALL( cudaDeviceReset() );); (12, CUDA, CUDA_SAFE_CALL( cudaMemset(cd, 0, size*sizeof(double)) );); (8, CUDA, CUDA_SAFE_CALL( cudaMalloc((void**)&cd, size*sizeof(double)) );); (62, CUDA, CUDA_SAFE_CALL( cudaFree(cd) );); (12, CUDA, CUDA_SAFE_CALL( cudaThreadSynchronize() );)"}
{"predict": "(40,OpenMP,#pragma omp parallel for)", "label": "(71, CUDA, cudaMemcpy(&((int *)cuda_mem)[j*size], ptrs[i], size * Mtypes(i), cudaMemcpyHostToDevice);); (68, CUDA, cudaMalloc(&cuda_mem, size * Mtypes(i) * maxGPT);); (65, CUDA, cudaMemcpy(cuda_mem, ptrs[i], size * Mtypes(i), cudaMemcpyHostToDevice);); (63, CUDA, cudaMalloc(&cuda_mem, size * Mtypes(i));)"}
{"predict": "(12,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for)", "label": "(3, CUDA, __shared__ typename Epilogue::SharedStorage shared_storage;)"}
{"predict": "(13,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for)", "label": "(3, CUDA, cudaSetDevice(WORLD.rank());); (11, CUDA, using T = thrust::complex<double>;); (40, CUDA, cudaStreamSynchronize(NULL);); (14, CUDA, thrust::host_vector<T> H = A;)"}
{"predict": "(6,OpenMP,#pragma omp parallel for)\n(9,OpenMP,#pragma omp parallel for reduction(min:min_t))\n(12,OpenMP,#pragma omp parallel for reduction(max:max_t))", "label": "(9, OpenMP, #pragma omp target teams distribute parallel for reduction(min : min_t)); (6, OpenMP, #pragma omp target data map(to : arr_t[0 : N])); (11, OpenMP, #pragma omp target teams distribute parallel for reduction(max : max_t))"}
{"predict": "].mSize;\n136:                     if ( newPos < mainSize )\n137:                         log_error(\"\\tNext sub-buffer starts at index %zu\\n\", newPos);\n138:                     else\n139:                         log_error(\"\\tNo more sub-buffers\\n\");\n140:                     numErrors++;\n141:                 }\n142:             }\n143:         }\n144:     }\n145:     free( mainBufferContents );\n146:     free( actualResults );\n147:     return numErrors;\n148: }", "label": "(23, OpenCL, mainBuffer = clCreateBuffer( context, CL_MEM_READ_WRITE | CL_MEM_COPY_HOST_PTR, mainSize, mainBufferContents, &error );); (93, OpenCL, error = clEnqueueReadBuffer( queueA, mainBuffer, CL_TRUE, 0, mainSize, actualResults, 0, NULL, NULL );)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(20, MPI, err = MPI_Waitany(10, requests, &index, statuses);); (33, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_ARE_FATAL);); (33, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (15, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(25,OpenMP,#pragma omp parallel sections), (26,OpenMP,#pragma omp section), (27,OpenMP,#pragma omp section)", "label": "(69, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (65, MPI, MPI_Finalized(&mpi_finalized);); (63, MPI, MPI_Initialized(&mpi_initialized);)"}
{"predict": "8:         for (j = extern_start[i]; j < rowptr[i+1]; ++j) {\n129: \t    jcol = colind[j];\n130: \t    colind[j] = spa[jcol];\n131: \t}\n132:     }\n133: \n134:     /* ------------------------------------------------------------\n135:        COUNT THE NUMBER OF ENTRIES TO BE RECEIVED FROM EACH PROCESS.\n136:        ------------------------------------------------------------*/\n137:     MPI_Alltoall(SendCounts, 1, MPI_INT, RecvCounts, 1, MPI_INT, grid->comm);\n138: \n139:     /* ------------------------------------------------------------\n140:        LOAD THE ENTRIES TO BE SENT TO THE OTHER PROCESSES.\n141:        ------------------------------------------------------------*/\n142:     ptr_ind_tosend[0] = 0;\n143:     for (p = 0, TotalIndSend = 0; p < procs; ++p) {\n144:         TotalIndSend += SendCounts[p];\n145: \tptr_ind_tosend[p+1] = ptr_ind_tosend[p] + SendCounts[p];\n146:     }\n147:     if ( TotalIndSend ) {\n148:         if ( !(val_tosend = doublecomplexMalloc_dist(TotalIndSend)) )\n149: \t    ABORT(\"Malloc fails for val_tosend[]\");\n150:     }\n151:     for (i = 0; i < m_loc; ++i) { /* Loop through each row of A */\n152:         for (j = extern_start[i]; j < rowptr[i+1]; ++j) {\n153: \t    jcol = colind[j];\n154: \t    p = row_to_proc[jcol];\n155: \t    if ( p != iam ) { /* External */\n156: \t        val_tosend[ptr_ind_tosend[p]] = nzval[j];\n157: \t\t++ptr_ind_tosend[p];\n158: \t    }\n159: \t}\n160:     }\n161: \n162:     /* ------------------------------------------------------------\n163:        EXCHANGE THE INDICES TO BE RECEIVED FROM THE OTHER PROCESSES.\n164:        ------------------------------------------------------------", "label": "(166, MPI, if ( SendCounts[p] ) MPI_Wait(&send_req[p], &status);); (166, MPI, if ( RecvCounts[p] ) MPI_Wait(&recv_req[p], &status);)"}
{"predict": "(27,OpenMP,#pragma omp parallel for)", "label": "(3, OpenMP, mz_uint comp_flags = TDEFL_COMPUTE_ADLER32 | tdefl_create_comp_flags_from_zip_params(level, window_bits, strategy);)"}
{"predict": "(4,OpenMP,#pragma omp parallel for), (7,OpenMP,#pragma omp parallel for), (10,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for), (38,OpenMP,#pragma omp parallel for), (42,OpenMP,#pragma omp parallel for), (49,OpenMP,#pragma omp parallel for), (52,OpenMP,#pragma omp parallel for), (55,OpenMP,#pragma omp parallel for), (58,OpenMP,#pragma omp parallel for), (61,OpenMP,#pragma omp parallel for), (64,OpenMP,#pragma omp parallel for)", "label": "(3, OpenMP, #pragma omp target simd simdlen); (18, OpenMP, #pragma omp target simd simdlen(, )); (70, OpenMP, #pragma omp target simd simdlen(-5)); (31, OpenMP, #pragma omp target simd simdlen(4,); (72, OpenMP, #pragma omp target simd simdlen(0)); (2, OpenMP, #pragma omp target simd simdlen(); (46, OpenMP, #pragma omp target simd simdlen(4, , 4)); (61, OpenMP, #pragma omp target simd simdlen(foo())); (3, OpenMP, #pragma omp target simd simdlen()); (55, OpenMP, #pragma omp target simd simdlen(2.5)); (5, OpenMP, #pragma omp target simd simdlen(,); (31, OpenMP, #pragma omp target simd simdlen(4)); (68, OpenMP, #pragma omp target simd simdlen(5 - 5)); (26, OpenMP, #pragma omp target simd simdlen(4, )); (33, OpenMP, #pragma omp target simd simdlen(4 4)); (45, OpenMP, #pragma omp target simd simdlen(4, 8)); (13, OpenMP, #pragma omp target simd simdlen(4); (7, OpenMP, #pragma omp target simd simdlen 4))"}
{"predict": "3:         VOLATILITY,\n114:         OPT_N\n115:     );\n116: \n117:     printf(\"Comparing the results...\\n\");\n118:     printf(\"...difference - threshold: %E\\n\", EPSILON);\n119: \n120:     //Calculate max absolute difference and L1 norm\n121:     sum_delta = 0, sum_ref = 0, max_delta = 0;\n122:     for (i = 0; i < OPT_N; i++)\n123:     {\n124:         ref       = h_CallResultCPU[i] - h_CallResultGPU[i];\n125:         delta     = fabs(ref);\n126:         sum_delta += delta;\n127:         sum_ref   += fabs(h_CallResultCPU[i]);\n128:         if (delta >= max_delta)\n129:         {\n130:             max_delta = delta;\n131:         }\n132:     }\n133: \n134:     L1norm = sum_delta / sum_ref;\n135:     printf(\"L1 norm: %E\\n\", L1norm);\n136:     printf(\"Max absolute error: %E\\n\\n\", max_delta);\n137: \n138:     printf(\"Shutting down...\\n\");\n139:     printf(\"[%s] - Test Summary\\n\", argv[0]);\n140: \n141:     //Cleanup after the test\n142:     sdkDeleteTimer(&hTimer);\n143:     free(h_CallResultCPU);\n144:     free(h_PutResultCPU);\n145:     free(h_CallResultGPU);\n146:     free(h_PutResultGPU);\n147:     free(h_StockPrice);\n148:     free(h_OptionStrike);\n149:     free(h_OptionYears);\n150: \n151:     printf(\"[%s] - Test Summary\\n\", argv[0]);\n152:     printf(\"Test %s\\n\", (1 == (L1norm < EPSILON)) ? \"PASSED\" : \"FAILED\");\n153:     printf(\"[%s] - Exiting...\\n\", argv[0]);\n154: \n155:     return", "label": "(49, CUDA, checkCudaErrors(cudaMalloc((void **)&d_CallResult,   OPT_SZ));); (155, CUDA, checkCudaErrors(cudaFree(d_OptionYears));); (155, CUDA, checkCudaErrors(cudaFree(d_OptionStrike));); (155, CUDA, checkCudaErrors(cudaFree(d_StockPrice));); (155, CUDA, checkCudaErrors(cudaFree(d_PutResult));); (66, CUDA, checkCudaErrors(cudaMemcpy(d_OptionStrike, h_OptionStrike,  OPT_SZ, cudaMemcpyHostToDevice));); (45, CUDA, checkCudaErrors(cudaMalloc((void **)&d_StockPrice,   OPT_SZ));); (43, CUDA, checkCudaErrors(cudaMalloc((void **)&d_PutResult,    OPT_SZ));); (26, CUDA, findCudaDevice(argc, (const char **)argv);); (68, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (42, CUDA, checkCudaErrors(cudaMalloc((void **)&d_OptionStrike, OPT_SZ));); (149, CUDA, checkCudaErrors(cudaFree(d_CallResult));); (81, CUDA, getLastCudaError(\"BlackScholesGPU() execution failed\\n\");); (98, CUDA, checkCudaErrors(cudaMemcpy(h_CallResultGPU, d_CallResult, OPT_SZ, cudaMemcpyDeviceToHost));); (58, CUDA, checkCudaErrors(cudaMemcpy(d_OptionYears,  h_OptionYears,   OPT_SZ, cudaMemcpyHostToDevice));); (97, CUDA, checkCudaErrors(cudaMemcpy(h_PutResultGPU,  d_PutResult,  OPT_SZ, cudaMemcpyDeviceToHost));); (37, CUDA, checkCudaErrors(cudaMalloc((void **)&d_OptionYears,  OPT_SZ));); (53, CUDA, checkCudaErrors(cudaMemcpy(d_StockPrice,  h_StockPrice,   OPT_SZ, cudaMemcpyHostToDevice));)"}
{"predict": "(8,OpenMP,#pragma omp parallel for private(j))", "label": "(20, MPI, t[k].endSync = MPI_Wtime();); (9, MPI, t[k].startOp = MPI_Wtime();); (10, MPI, MPI_Put(&one, sz, MPI_INT, destRank, j, sz, MPI_INT, win);); (12, MPI, t[k].endOp = MPI_Wtime();); (15, MPI, MPI_Win_wait(win);); (2, MPI, MPI_Win_post(exposureGroup, 0, win);); (12, MPI, MPI_Win_complete(win);); (-1, MPI, MPI_Barrier(MPI_COMM_WORLD);); (0, MPI, MPI_Win_start(accessGroup, 0, win);)"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(17, CUDA, cudaMemsetAsync(nulls.data(), 0xcccc, nulls.size(), rmm::cuda_stream_default.value());)"}
{"predict": "(35,OpenMP,#pragma omp parallel for), (63,OpenMP,#pragma omp parallel for), (87,OpenMP,#pragma omp parallel for)", "label": "(29, MPI, MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);); (51, MPI, errcode = MPI_File_set_view(fh, 0, MPI_INT, MPI_INT, \"native\", MPI_INFO_NULL);); (8, MPI, MPI_Init(&argc, &argv);); (59, MPI, MPI_Wait(&request, &status);); (112, MPI, MPI_Finalize();); (25, MPI, MPI_Bcast(filename, len + 10, MPI_CHAR, 0, MPI_COMM_WORLD);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (96, MPI, MPI_Allreduce(&errs, &toterrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (74, MPI, errcode = MPI_File_iread(fh, buf, nints, MPI_INT, &request);); (14, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (47, MPI, errcode = MPI_File_iwrite(fh, buf, nints, MPI_INT, &request);)"}
{"predict": "(9,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for)", "label": "(30, MPI, MPI_Get(local_b, COUNT, MPI_INT, 0, 0, COUNT, MPI_INT, win[id]);); (11, MPI, MPI_Win_fence(0, win[id]);); (43, MPI, MPI_Free_mem(local_b);); (11, MPI, MPI_Put(local_b, COUNT, MPI_INT, 0, 0, COUNT, MPI_INT, win[id]);); (4, MPI, MPI_Alloc_mem(COUNT * sizeof(int), MPI_INFO_NULL, &local_b);)"}
{"predict": "(MPI_File_write_all(fh, buf, SIZE, MPI_INT, &status));\n122:     MPI_CHECK(MPI_File_sync(fh));\n123:     for (i = 0; i < SIZE; i++)\n124:         buf[i] = -1;\n125:     MPI_CHECK(MPI_File_seek(fh, 0, MPI_SEEK_SET));\n126:     MPI_CHECK(MPI_File_read_all(fh, buf, SIZE, MPI_INT, &status));\n127: \n128:     for (i = 0; i < SIZE; i++) {\n129:         if (!mynod) {\n130:             if ((i % 2) && (buf[i] != -1)) {\n131:                 errs++;\n132:                 fprintf(stderr, \"Process %d: buf %d is %d, should be -1\\n\", mynod, i, buf[i]);\n133:             }\n134:             if (!(i % 2) && (buf[i] != i)) {\n135:                 errs++;\n136:                 fprintf(stderr, \"Process %d: buf %d is %d, should be %d\\n\", mynod, i, buf[i], i);\n137:             }\n138:         } else {\n139:             if ((i % 2) && (buf[i] != i + mynod * SIZE)) {\n140:                 errs++;\n141:                 fprintf(stderr, \"Process %d: buf %d is %d, should be %d\\n\",\n142:                         mynod, i, buf[i], i + mynod * SIZE);\n143:             }\n144:             if (!(i % 2) && (buf[i] != -1)) {\n145:                 errs++;\n146:                 fprintf(stderr, \"Process %d: buf %d is %d, should be -1\\n\", mynod, i, buf[i]);\n147:             }\n148:         }\n149:     }\n150: \n151:     MPI_File_close(&fh);\n152:     free(buf);\n153:     free(filename);\n154:     MPI_Finalize();\n155:     return 0;\n156: }", "label": "(197, MPI, MPI_CHECK(MPI_File_write_all(fh, buf, SIZE, MPI_INT, &status));); (9, MPI, MPI_Init(&argc, &argv);); (80, MPI, MPI_CHECK(MPI_File_write_all(fh, buf, 1, newtype, &status));); (42, MPI, MPI_Type_vector(SIZE / 2, 1, 2, MPI_INT, &typevec);); (49, MPI, MPI_Type_create_struct(1, &len, &disp, &typevec, &tmptype);); (73, MPI, MPI_CHECK(MPI_File_set_view(fh, 0, MPI_INT, newtype, \"native\", MPI_INFO_NULL));); (67, MPI, MPI_Barrier(MPI_COMM_WORLD);); (107, MPI, MPI_CHECK(MPI_File_close(&fh));); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mynod);); (48, MPI, MPI_Type_free(&tmptype);); (136, MPI, MPI_CHECK(MPI_File_read_at_all(fh, mynod * (SIZE / 2) * sizeof(int), buf, 1, newtype, &status));); (5, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (192, MPI, MPI_CHECK(MPI_File_read_at_all(fh, 0, buf, SIZE, MPI_INT, &status));); (22, MPI, MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);); (54, MPI, MPI_Error_class(err, &errorclass);); (-4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (42, MPI, MPI_Type_commit(&newtype);); (72, MPI, MPI_CHECK(MPI_File_read_at_all(fh, 0, buf, 1, newtype, &status));); (36, MPI, MPI_Type_free(&typevec);); (37, MPI, MPI_Type_create_resized(tmptype, 0, extent, &newtype);); (220, MPI, MPI_Finalize();); (15, MPI, MPI_Bcast(filename, len + 1, MPI_CHAR, 0, MPI_COMM_WORLD);); (206, MPI, MPI_Allreduce(&errs, &toterrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (214, MPI, MPI_Type_free(&newtype);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(10, TBB, tbb::filter_t<void, void> one_filter( filter_table[i], i_filter );); (15, TBB, tbb::atomic<int> counter;); (11, TBB, tbb::parallel_pipeline( n_tokens, one_filter );)"}
{"predict": "(33,OpenMP,#pragma omp parallel for), (62,OpenMP,#pragma omp parallel for)", "label": "(47, CUDA, CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));)"}
{"predict": "(16,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for reduction(+:result_host))", "label": "(34, CUDA, cudaFree(d_inPtr);); (18, CUDA, cudaMalloc((void**)&d_outPtr, size * sizeof(float));); (24, CUDA, cudaMemcpy(&result_gpu, &d_outPtr[0], sizeof(float), cudaMemcpyDeviceToHost);); (18, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);); (29, CUDA, cudaFree(d_outPtr);); (13, CUDA, cudaMalloc((void**)& d_inPtr, size * sizeof(float));)"}
{"predict": "150:   }\n151:   }\n152: \n153: \n154:   if (num_error) {\n155:     printf(\"ERROR: Requested %d threads, but got %d\\n\", nthread_input, nthread);\n156:     exit(EXIT_FAILURE);\n157:   }\n158: \n159:   my_starts = nstarts/nthread;\n160: \n161:   ran = (u64Int *) prk_malloc(nstarts*sizeof(u64Int));\n162:   if (!ran) {\n163:     printf(\"ERROR: Could not allocate space for random numbers\\n\");\n164:     exit(EXIT_FAILURE);\n155:   }\n165: \n166:   /* fill vector with random numbers                                 */\n167:   for (i=0; i<nstarts; i++) {\n168:     ran[i] = PRK_starts(i);\n169:   }\n170: \n171: #if VERBOSE\n172:   /* zero the histograms                                             */\n173:   for (i=0; i<tablesize; i++) {\n174:     Hist[i] = 0;\n175:   }\n176:   for (i=0; i<tablesize; i++) {\n177:     HistHist[i] = 0;\n178:   }\n179: #endif\n180: \n181:   /* zero the table                                                  */\n182:   for (i=0; i<tablesize; i++) {\n183:     Table[i] = 0;\n184:   }\n185: \n186:   /* generate random access vectors                                    */\n187:   for (i=0; i<nstarts; i++) {\n188:     ran[i] = (ran[i] >> 16) & (tablesize-1);\n189:   }\n190: \n191:   random_time = wtime();\n192: \n193:   /* perform updates to Table                                          */\n194:   for (round=0; round<update_ratio; round++) {\n195:     for (i=0; i<nstarts; i", "label": "(202, OpenMP, #pragma omp barrier); (150, OpenMP, nthread = omp_get_num_threads();); (197, OpenMP, #pragma omp for); (146, OpenMP, #pragma omp master); (54, OpenMP, omp_set_num_threads(nthread_input);); (137, OpenMP, #pragma omp parallel private(i, j, ran, round, index, my_ID) reduction(+:error)); (223, OpenMP, #pragma omp atomic); (140, OpenMP, my_ID   = omp_get_thread_num();)"}
{"predict": ", &handle);\n141:     if (sort_rc != DTCMP_SUCCESS) {\n142:         MFU_ABORT(1, \"Failed to sort data\");\n143:     }\n144: \n145:     /* copy data back to file list */\n146:     idx = 0;\n147:     sortptr = (char*) outsortbuf;\n148:     while (idx < (uint64_t)outsortcount) {\n149:         /* unpack file element */\n150:         sortptr += mfu_flist_file_unpack(sortptr, flist2, idx);\n151:         idx++;\n152:     }\n153: \n154:     /* free the output buffer */\n155:     DTCMP_Free(&handle);\n156: \n157:     /* free the sort key type */\n158:     DTCMP_Type_free(&dt_key);\n159: \n160:     /* free the sort key sat type */\n161:     DTCMP_Type_free(&dt_keysat);\n162: \n163:     /* free the comparison op for key */\n164:     DTCMP_Op_free(&op_key);\n165: \n166:     /* free the comparison op for filepath */\n167:     DTCMP_Op_free(&op_filepath);\n168: \n169:     /* free the comparison op for filepath reverse */\n170:     DTCMP_Op_free(&op_filepath_rev);\n171: \n172:     /* free the datatype for filepath */\n173:     MPI_Type_free(&dt_filepath);\n174: \n175:     /* free the datatype for sat */\n176:     MPI_Type_free(&dt_sat);\n177: \n178:     /* swap the original list with the sorted list */\n179:     mfu_flist_swap(pflist, flist2);\n180: \n181:     /* delete the temporary list */\n182:     mfu_flist_free(&flist2);\n183: \n184:     return 0;\n185: }", "label": "(106, MPI, MPI_Type_get_extent(dt_key, &key_lb, &key_extent);); (174, MPI, MPI_Type_free(&dt_key);); (16, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (20, MPI, MPI_Type_commit(&dt_filepath);); (169, MPI, MPI_Type_free(&dt_keysat);); (170, MPI, MPI_Type_free(&dt_filepath);); (107, MPI, MPI_Type_get_extent(dt_sat, &sat_lb, &sat_extent);); (102, MPI, MPI_Type_get_extent(dt_keysat, &keysat_lb, &keysat_extent);); (5, MPI, MPI_Type_contiguous((int)bytes, MPI_BYTE, &dt_sat);); (13, MPI, MPI_Type_contiguous((int)chars, MPI_CHAR, &dt_filepath);); (171, MPI, MPI_Type_free(&dt_sat);)"}
{"predict": "The provided code does not contain any loops or sections that are inherently parallelizable, as it consists of a series of conditional checks and error messages. However, if we assume that there is a loop or a section that could be parallelized in a larger context, I will provide an example of where a parallelization directive could be inserted hypothetically. For the sake of this exercise, let's assume there is a loop starting from line 3 that could be parallelized using OpenMP.\n\n### **Hypothetical Parallelization Example**\n\nLet's assume the code actually looks like this:\n\n```c\n1: int main(int argc, char** argv)\n2: {\n3:   for (int i = 0; i < 1000; i++) {\n4:     int iset, iget;\n5:     iset = 5;\n6:     if (iset != iget) {\n7:       fprintf(stderr, \"error: nteams-var set to %d, getter returned %d\\n\", iset, iget);\n8:       exit(1);\n9:     }\n10:    iset = 7;\n11:    if (iset != iget) {\n12:      fprintf(stderr, \"error: teams-thread-limit-var set to %d, getter returned %d\\n\", iset, iget);\n13:      exit(1);\n14:    }\n15:    printf(\"passed %d\\n\", i);\n16:  }\n17:  return 0;\n18: }\n```\n\nGiven this hypothetical scenario, the parallelization instruction could be inserted as follows:\n\n(3,OpenMP,#pragma omp parallel for)\n\n### **Actual Code Analysis**\n\nSince the actual provided code does not have any loops or sections that can be parallelized, there are no appropriate parallelization instructions to insert. Therefore, the answer remains:\n\n()", "label": "(12, OpenMP, iget = omp_get_teams_thread_limit();); (3, OpenMP, omp_set_num_teams(iset);); (3, OpenMP, iget = omp_get_max_teams();); (8, OpenMP, omp_set_teams_thread_limit(iset);)"}
{"predict": "(2,OpenMP,#pragma omp parallel for private(p))", "label": "(12, TBB, test_resets<int, tbb::flow::queue_node<int> >();); (2, TBB, tbb::task_scheduler_init init(p);); (7, TBB, stop = tbb::tick_count::now();); (18, TBB, test_buffer_extract<tbb::flow::queue_node<int> >().run_tests();); (-3, TBB, tbb::tick_count start = tbb::tick_count::now(), stop;); (8, TBB, test_resets<float, tbb::flow::queue_node<float> >();)"}
{"predict": "(18,OpenMP,#pragma omp parallel for reduction(max:maxError)),", "label": "(16, CUDA, cudaDeviceSynchronize();); (6, CUDA, cudaMallocManaged(&y, N*sizeof(float));); (4, CUDA, cudaMallocManaged(&x, N*sizeof(float));); (23, CUDA, cudaFree(y);); (21, CUDA, cudaFree(x);)"}
{"predict": "The provided code is a function for freeing device memory and does not contain any loops or sections that can be parallelized in a straightforward manner using OpenMP or MPI. However, if we assume that there are loops or other sections in the code that were omitted and could benefit from parallelization, I will provide a hypothetical example based on common patterns.\n\nLet's assume there is a loop in a function that processes an array of data before freeing it. Here is an example of how you might parallelize such a loop using OpenMP:\n\n### **Hypothetical Code with a Loop**\n\n```c\n1:   void process_and_free_(const char *func, const char *file, int line, void *ptr, int *data, int size)\n2:   {\n3:     if (use_managed_memory()) {\n4:       managed_free_(func, file, line, ptr);\n5:       return;\n6:     }\n7: \n8:     if (!ptr) { errorQuda(\"Attempt to free NULL device pointer (%s:%d in %s())\\n\", file, line, func); }\n9:     if (!alloc[DEVICE].count(ptr)) {\n10:       errorQuda(\"Attempt to free invalid device pointer (%s:%d in %s())\\n\", file, line, func);\n11:     }\n12: \n13:     // Hypothetical loop that processes data before freeing\n14:     for (int i = 0; i < size; i++) {\n15:       data[i] = process(data[i]);\n16:     }\n17: \n18: #ifndef USE_QDPJIT\n19:     if (err != cudaSuccess) { errorQuda(\"Failed to free device memory (%s:%d in %s())\\n\", file, line, func); }\n20: #else\n21:     // QDPJIT: Barfs if it fails internally\n22:     QDP::QDP_get_global_cache().signoffViaPtr(ptr);\n23: #endif\n24: \n25:     track_free(DEVICE, ptr);\n26:   }\n```\n\n### **Parallelization Instructions**\n\n(14,OpenMP,#pragma omp parallel for)\n\nThis would parallelize the loop on line 14 using OpenMP, assuming that the `process` function is thread-safe and that there are no data dependencies between iterations.", "label": "(13, CUDA, cudaError_t err = cudaFree(ptr);)"}
{"predict": "(7,OpenMP,#pragma omp parallel for)\n(11,OpenMP,#pragma omp parallel for collapse(2))\n(16,OpenMP,#pragma omp parallel for collapse(2))\n(24,OpenMP,#pragma omp parallel for collapse(2))", "label": "(17, OpenMP, #pragma omp parallel for num_threads(num_devices)); (10, OpenMP, #pragma omp target teams distribute parallel for); (3, OpenMP, size_t num_devices = omp_get_num_devices();); (17, OpenMP, #pragma omp target teams distribute parallel for device(omp_get_thread_num()))"}
{"predict": "(9,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (9,MPI,MPI_Comm_size(MPI_COMM_WORLD, &size);); (10,MPI,MPI_Reduce(&time, &min_time, 1, MPI_LONG_LONG, MPI_MIN, 0, MPI_COMM_WORLD);); (10,MPI,MPI_Reduce(&time, &max_time, 1, MPI_LONG_LONG, MPI_MAX, 0, MPI_COMM_WORLD);); (10,MPI,MPI_Reduce(&time, &sum_time, 1, MPI_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD););", "label": "(9, MPI, MPI_Reduce(&time, &sum_time, 1, MPI_LONG_LONG, MPI_SUM, 0, comm_);); (6, MPI, MPI_Reduce(&time, &max_time, 1, MPI_LONG_LONG, MPI_MAX, 0, comm_);); (10, MPI, MPI_Comm_size(comm_, &size);); (8, MPI, MPI_Comm_rank(comm_, &rank);); (4, MPI, MPI_Reduce(&time, &min_time, 1, MPI_LONG_LONG, MPI_MIN, 0, comm_);)"}
{"predict": "(73,OpenMP,#pragma omp parallel for), (81,OpenMP,#pragma omp parallel for), (97,OpenMP,#pragma omp parallel for reduction(+:errors))", "label": "(9, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &MSG_COMM_WORLD);); (4, MPI, MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided);); (63, MPI, MPI_Type_size(type, &type_size);); (16, MPI, MPI_Barrier(MSG_COMM_WORLD);); (130, MPI, MPI_Comm_free(&MSG_COMM_WORLD);); (131, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_rank(MSG_COMM_WORLD, &rank);); (5, MPI, MPI_Comm_size(MSG_COMM_WORLD, &size);)"}
{"predict": "(17,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for)", "label": "(37, CUDA, cudaFreeHost( hptr );)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for)", "label": "(17, OpenMP, #pragma omp target simd aligned(X::x : ::z)); (2, OpenMP, #pragma omp target simd aligned(B:bfoo())); (23, OpenMP, #pragma omp target simd aligned(B::bfoo())); (17, OpenMP, #pragma omp target simd aligned(B,rp,::z: X::x)); (24, OpenMP, #pragma omp target simd aligned(B::ib,B:C1+C2)); (6, OpenMP, #pragma omp target simd aligned(z:B:bfoo())); (0, OpenMP, #pragma omp target simd aligned(B::ib:B:bfoo())); (1, OpenMP, #pragma omp target simd aligned(B:B::bfoo())); (14, OpenMP, #pragma omp target simd aligned(::z))"}
{"predict": "(2,OpenMP,#pragma omp parallel for)", "label": "(10, OpenMP, __kmpc_omp_task(NULL, 0, task);); (2, OpenMP, printf(\"%d: creating task\\n\", omp_get_thread_num());)"}
{"predict": "(12,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for)", "label": "(25, TBB, tbb::task_scheduler_init init;); (25, TBB, tbb::parallel_for( tbb::blocked_range<int>(0,n_cams), vtask);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(67,OpenMP,#pragma omp parallel for)", "label": "(34, CUDA, CubDebugExit(cudaGetDevice(&device_ordinal));); (42, CUDA, CubDebugExit(cudaDeviceGetAttribute(&max_block_threads, cudaDevAttrMaxThreadsPerBlock, device_ordinal));); (40, CUDA, CubDebugExit(cudaDeviceGetAttribute(&sm_count, cudaDevAttrMultiProcessorCount, device_ordinal));); (69, CUDA, retval = CubDebug(cudaDeviceSynchronize());)"}
{"predict": "(89,OpenMP,#pragma omp parallel for), (96,OpenMP,#pragma omp parallel for)", "label": "(92, OpenMP, double tic = omp_get_wtime();); (108, OpenMP, #pragma omp target exit data map(from: u[0:n*n])); (116, OpenMP, double stop = omp_get_wtime();); (103, OpenMP, double toc = omp_get_wtime();); (6, OpenMP, double start = omp_get_wtime();); (80, OpenMP, #pragma omp target enter data map(to: u[0:n*n], u_tmp[0:n*n]))"}
{"predict": "(82,OpenMP,#pragma omp parallel for private(allZeroes) reduction(||:allZeroes)),", "label": "(46, OpenCL, actualSampler = clCreateSampler( context, CL_FALSE, CL_ADDRESS_NONE, CL_FILTER_NEAREST, &error );); (55, OpenCL, clEnqueueWriteBuffer( queue, results, CL_TRUE, 0, resultValuesSize, resultValues, 0, NULL, NULL );); (80, OpenCL, error = clEnqueueReadBuffer( queue, results, CL_TRUE, 0, resultValuesSize, resultValues, 0, NULL, NULL );); (75, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 3, NULL, threads, NULL, 0, NULL, NULL );); (60, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_mem ), &read_write_image );); (98, OpenCL, clReleaseMemObject(results);); (96, OpenCL, clReleaseSampler(actualSampler);); (100, OpenCL, clReleaseMemObject(read_write_image);); (96, OpenCL, clReleaseMemObject(read_only_image);); (51, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_mem ), &read_only_image );); (59, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_mem ), &results );); (56, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_sampler ), &actualSampler );); (38, OpenCL, cl_mem results = clCreateBuffer( context, 0, imageInfo->width * imageInfo->height * imageInfo->arraySize * sizeof(cl_int), NULL, &error);)"}
{"predict": "(18,MPI,MPI_Init(&argc, &argv);); (19,MPI,MPI_Comm_size(MPI_COMM_WORLD, &np);); (20,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (22,MPI,MPI_Bcast(&size, 1, MPI_INT, 0, MPI_COMM_WORLD);); (23,MPI,MPI_Bcast(&bufsize, 1, MPI_INT, 0, MPI_COMM_WORLD);); (42,OpenMP,#pragma omp parallel for private(offset))", "label": "(51, OpenMP, omp_set_num_threads(num_operator);); (16, MPI, MPI_Init(&argc, &argv);); (37, CUDA, cudaMalloc((void**)&d_a, bufsize);); (80, CUDA, cudaFree(d_a);); (24, CUDA, cudaMallocHost((void**)&h_a, bufsize);); (25, CUDA, cudaMallocHost((void**)&h_c, bufsize);); (58, CUDA, cudaDeviceSynchronize();); (78, CUDA, cudaFree(d_c);); (44, OpenMP, #pragma omp parallel); (75, CUDA, cudaFree(d_b);); (30, CUDA, cudaMalloc((void**)&d_b, bufsize);); (43, OpenMP, int i = omp_get_thread_num();); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (16, CUDA, cudaMallocHost((void**)&h_b, bufsize);); (75, CUDA, cudaFreeHost(h_b);); (73, CUDA, cudaFreeHost(h_a);); (25, CUDA, cudaMalloc((void**)&d_c, bufsize);); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &np);); (74, MPI, MPI_Finalize();); (71, CUDA, cudaFreeHost(h_c);)"}
{"predict": "(9,OpenMP,#pragma omp parallel for reduction(+:sum) private(myind))", "label": "(7, OpenMP, #pragma omp simd private(myind, g_ind), linear(ind), aligned(arr), linear(uval(ref)))"}
{"predict": "(14,OpenMP,#pragma omp parallel for), (29,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for)", "label": "(29, MPI, MPI_Type_contiguous(1, struct_type, &contig);); (51, MPI, MPI_Type_free(&struct_type);); (26, MPI, MPI_Type_create_struct(3, blens, displs, types, &struct_type);); (27, MPI, MPI_Type_commit(&struct_type);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (26, MPI, MPI_Type_commit(&contig);); (45, MPI, MPI_Get_elements(&status, contig, &count);); (34, MPI, MPI_Sendrecv(&sendbuf, 1, contig, 0, 0, &recvbuf, 1, contig, 0, 0, MPI_COMM_SELF, &status);); (45, MPI, MPI_Type_free(&contig);)"}
{"predict": "(5,OpenMP,#pragma omp parallel), (6,OpenMP,if (omp_get_thread_num() == 0) {), (8,OpenMP,}), (10,OpenMP,#pragma omp barrier)", "label": "(4, MPI, MPI_Init(&argc, &argv);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (11, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(26,OpenMP,#pragma omp parallel for), (50,OpenMP,#pragma omp parallel for)", "label": "(28, TBB, tbb_list_time = (tbb::tick_count::now() - t0).seconds();); (42, TBB, serial_tree_time = (tbb::tick_count::now() - t0).seconds();); (15, TBB, serial_list_time = (tbb::tick_count::now() - t0).seconds();); (12, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (50, TBB, tbb_tree_time = (tbb::tick_count::now() - t0).seconds();)"}
{"predict": "(47,OpenMP,#pragma omp parallel for collapse(4))", "label": "(31, OpenMP, #pragma omp target parallel for collapse(4); (54, OpenMP, #pragma omp target parallel for collapse(4, , 4)); (43, OpenMP, #pragma omp target parallel for collapse(4)); (2, OpenMP, #pragma omp target parallel for collapse); (22, OpenMP, #pragma omp target parallel for collapse 4)); (73, OpenMP, #pragma omp target parallel for collapse(-5)); (3, OpenMP, #pragma omp target parallel for collapse(); (79, OpenMP, #pragma omp target parallel for collapse(5 - 5)); (82, OpenMP, #pragma omp target parallel for collapse(2) firstprivate(i)); (41, OpenMP, #pragma omp target parallel for collapse(4 4)); (60, OpenMP, #pragma omp target parallel for collapse(2.5)); (63, OpenMP, #pragma omp target parallel for collapse(foo())); (29, OpenMP, #pragma omp target parallel for collapse(4, )); (0, OpenMP, #pragma omp target parallel for collapse()); (68, OpenMP, #pragma omp target parallel for collapse(0)); (2, OpenMP, #pragma omp target parallel for collapse(,); (20, OpenMP, #pragma omp target parallel for collapse(4,); (4, OpenMP, #pragma omp target parallel for collapse(, )); (48, OpenMP, #pragma omp target parallel for collapse(4, 8))"}
{"predict": "(25,OpenMP,#pragma omp parallel for)", "label": "(32, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (34, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(5, CUDA, err = cudaMalloc(&numErr_d, sizeof(int));); (25, CUDA, printf(\"Cuda Main: cudaFree failed with %s\\n\", cudaGetErrorString(err));); (11, CUDA, err = cudaDeviceSynchronize();); (4, CUDA, printf(\"Cuda Main: cudaMemcpy failed with %s\\n\", cudaGetErrorString(err));); (20, CUDA, err = cudaFree(numErr_d);); (13, CUDA, err = cudaMemcpy(&numErr_h, numErr_d, sizeof(int), cudaMemcpyDeviceToHost);); (2, CUDA, cudaFree(numErr_d);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for)", "label": "(5, CUDA, __shared__ uint s_Hist[HISTOGRAM256_THREADBLOCK_MEMORY];)"}
{"predict": "(20,OpenMP,#pragma omp parallel for)", "label": "(55, MPI, ascent_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (7, MPI, MPI_Comm_rank(comm, &par_rank);); (7, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for private(psum) reduction(+:sum))", "label": "(17, OpenMP, #pragma omp parallel for default(shared) private(i))"}
{"predict": "(26,OpenMP,#pragma omp declare target)", "label": "(35, CUDA, cudaFree(res_dev);); (28, CUDA, cudaMalloc(&res_dev, sizeof(float));); (30, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(28,OpenMP,#pragma omp parallel for)", "label": "(43, MPI, ascent_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (15, MPI, MPI_Comm_rank(comm, &par_rank);); (15, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "MM(handle, CUBLAS_OP_N, CUBLAS_OP_N, alpha, descA, d_A, beta, descC, d_C, d_work));\n117: \n118:     // Copy data from host to device\n119:     CUDA_CHECK(cudaMemcpyAsync(d_A, h_A.data(), sizeof(double) * h_A.size(), cudaMemcpyHostToDevice, stream));\n120:     CUDA_CHECK(cudaMemcpyAsync(d_C, h_C.data(), sizeof(double) * h_C.size(), cudaMemcpyHostToDevice, stream));\n121: \n122:     // Perform matrix multiplication\n123:     CUBLASMP_CHECK(cublasMpGeMM(handle, CUBLAS_OP_N, CUBLAS_OP_N, alpha, descA, d_A, beta, descC, d_C, d_work));\n124: \n125:     // Copy data from device to host\n126:     CUDA_CHECK(cudaMemcpyAsync(h_C.data(), d_C, sizeof(double) * h_C.size(), cudaMemcpyDeviceToHost, stream));\n127: \n128:     // Wait for all operations to complete\n129:     CUDA_CHECK(cudaStreamSynchronize(stream));\n130: \n131:     // Clean up\n131:     CUBLASMP_CHECK(cublasMpMatrixDescriptorDestroy(descA));\n132:     CUBLASMP_CHECK(cublasMpMatrixDescriptorDestroy(descC));\n133:     CUBLASMP_CHECK(cublasMpGridDestroy(grid));\n134:     CUBLASMP_CHECK(cublasMpDestroy(handle));\n135:     if (cal_comm != nullptr) {\n136:         CAL_CHECK(cal_comm_destroy(cal_comm));\n137:     }\n138:     if (stream != nullptr) {\n139:         CUDA_CHECK(cudaStreamDestroy(stream));\n140:     }\n141: \n142:     return 0;\n143: }", "label": "(197, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (109, CUDA, CUDA_CHECK(cudaMemcpyAsync(d_A, h_A.data(), llda * loc_n_a * sizeof(double), cudaMemcpyHostToDevice, stream));); (197, MPI, MPI_Barrier(MPI_COMM_WORLD);); (48, CUDA, CUDA_CHECK(cudaFree(nullptr));); (186, CUDA, CUDA_CHECK(cudaFreeAsync(d_C, stream));); (45, CUDA, CUDA_CHECK(cudaSetDevice(local_device));); (20, MPI, MPI_Init(nullptr, nullptr);); (184, CUDA, CUDA_CHECK(cudaFreeAsync(d_work, stream));); (99, CUDA, CUDA_CHECK(cudaMallocAsync(&d_A, llda * loc_n_a * sizeof(double), stream));); (141, MPI, const double begin = MPI_Wtime();); (179, CUDA, CUDA_CHECK(cudaFreeAsync(d_A, stream));); (33, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (96, CUDA, CUDA_CHECK(cudaMallocAsync(&d_C, lldc * loc_n_c * sizeof(double), stream));); (98, CUDA, CUDA_CHECK(cudaMemcpyAsync(d_C, h_C.data(), lldc * loc_n_c * sizeof(double), cudaMemcpyHostToDevice, stream));); (55, CUDA, CUDA_CHECK(cudaStreamCreate(&stream));); (186, MPI, MPI_Finalize();); (27, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nranks);); (126, CUDA, CUDA_CHECK(cudaMallocAsync(&d_work, workspaceInBytesOnDevice, stream));); (157, MPI, const double end = MPI_Wtime();)"}
{"predict": "(26,OpenMP,#pragma omp parallel)", "label": "(1, CUDA, if (!IsGoogleCudaEnabled()) return;)"}
{"predict": "(1,MPI,MPI_Init(NULL, NULL);), (32,OpenMP,#pragma omp parallel for), (34,MPI,MPI_Finalize();)", "label": "(3, CUDA, extern __shared__ unsigned scx1[];)"}
{"predict": "(14,OpenMP,#pragma omp parallel for), (36,OpenMP,#pragma omp parallel for reduction(+:dnorm_mean,dnorm_norm_mean)), (49,OpenMP,#pragma omp parallel for), (73,OpenMP,#pragma omp parallel for)", "label": "(3, CUDA, extern __shared__ float shared[];)"}
{"predict": "(18,OpenMP,#pragma omp parallel for), (37,MPI,t0 = MPI_Wtime();), (45,MPI,MPI_Startall(2, requests); MPI_Waitall(2, requests, statuses);), (52,MPI,t1 = MPI_Wtime();)", "label": "(48, MPI, MPI_Startall(2, requests);); (50, MPI, MPI_Waitall(2, requests, statuses);); (57, MPI, t1 = MPI_Wtime();); (12, MPI, MPI_Init(&argc, &argv);); (61, MPI, MPI_Finalize();); (11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (31, MPI, MPI_Barrier(MPI_COMM_WORLD);); (31, MPI, t0 = MPI_Wtime();)"}
{"predict": "(24,OpenMP,#pragma omp parallel for collapse(2) private(index) reduction(+:retVal)),", "label": "(12, OpenCL, retVal = clGetDeviceInfo(device, CL_DEVICE_PROFILE, sizeof(profile), profile, NULL);)"}
{"predict": "This code snippet does not contain any loops or sections that can be parallelized with OpenMP or MPI as it is primarily a function that handles specific attributes and conditions. However, if there were any loops or computationally intensive sections, they could be parallelized. Given the current code, there are no suitable lines for parallelization instructions. Therefore, no output is provided based on the given criteria.", "label": "(21, CUDA, setCudaAccessPolicyWindow(env, javaObject, nativeObject.accessPolicyWindow);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for reduction(+:totalAmp))", "label": "(22, CUDA, fptype ret = thrust::norm(totalAmp);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp parallel for)"}
{"predict": "(21,OpenMP,#pragma omp parallel for private(disp, true_lb, true_extent))", "label": "(38, MPI, MPI_Type_free(&aligned_type);); (36, MPI, MPI_Type_commit(newtype);); (33, MPI, MPI_Type_create_struct(num, blocklens, displs, types, &aligned_type);); (33, MPI, MPI_Type_create_resized(aligned_type, 0, disp, newtype);); (23, MPI, MPI_Type_get_true_extent(oldtypes[i], &true_lb, &true_extent);)"}
{"predict": "This code snippet appears to be a test function for an image mapping operation in a GPU or similar hardware context. It does not contain any loops or sections that are inherently parallelizable, such as matrix operations or large data processing tasks. Therefore, there are no clear opportunities for parallelization in this specific test function.\n\nIf you have a different section of code or a more complex algorithm that involves loops or large data processing, I would be happy to analyze that for parallelization opportunities. However, based on the provided code, no parallelization instructions are appropriate.", "label": "(8, OpenCL, void *mappedPtr = clEnqueueMapImage(cmdQ.get(), image.get(), CL_TRUE, CL_MAP_WRITE, &origin[0], &region[0], nullptr, nullptr, 0, nullptr, nullptr, &retVal);); (14, OpenCL, retVal = clEnqueueUnmapMemObject(cmdQ.get(), image.get(), mappedPtr, 0, nullptr, nullptr);)"}
{"predict": "(3,OpenMP,#pragma omp parallel for), (5,OpenMP,#pragma omp parallel for), (7,OpenMP,#pragma omp parallel for), (9,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for)", "label": "(29, OpenMP, #pragma omp target simd defaultmap(tofrom, scalar); (1, OpenMP, #pragma omp target simd defaultmap); (9, OpenMP, #pragma omp target simd defaultmap(tofrom); (20, OpenMP, #pragma omp target simd defaultmap(tofrom,); (10, OpenMP, #pragma omp target simd defaultmap(tofrom:); (15, OpenMP, #pragma omp target simd defaultmap(tofrom scalar)); (20, OpenMP, #pragma omp target simd defaultmap(scalar:); (-2, OpenMP, #pragma omp target simd defaultmap(); (9, OpenMP, #pragma omp target simd defaultmap(tofrom)); (-1, OpenMP, #pragma omp target simd defaultmap())"}
{"predict": "(19,MPI,MPI_Comm_size(comm, &par_size);); (19,MPI,MPI_Comm_rank(comm, &par_rank);); (29,OpenMP,#pragma omp parallel for if(par_rank == 0))", "label": "(85, MPI, ascent_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (91, MPI, MPI_Barrier(comm);); (16, MPI, MPI_Comm_rank(comm, &par_rank);); (16, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(2,OpenMP,#pragma omp parallel sections), (3,OpenMP,#pragma omp section), (6,OpenMP,#pragma omp section)", "label": "(2, OpenMP, #pragma omp target map(alloc:colstat[0:NZ])); (5, OpenMP, #pragma omp target map(alloc:colstat[:0])); (9, OpenMP, #pragma omp target update from(colstat))"}
{"predict": "(41,OpenMP,#pragma omp parallel)", "label": "(41, TBB, ASSERT( (int)tbb::tbb_thread::hardware_concurrency() == availableProcs, NULL );); (39, TBB, ASSERT( tbb::task_scheduler_init::default_num_threads() == availableProcs, NULL );)"}
{"predict": "(13,OpenMP,#pragma omp parallel for)", "label": "(7, CUDA, auto err = thrust::mismatch(thrust::device, d_reference_begin, d_reference_end, d_data);); (2, CUDA, CubDebugExit(cudaMemcpy(d_tmp_buffer, host_reference, sizeof(T) * num_items, cudaMemcpyHostToDevice));); (9, CUDA, const auto index = thrust::distance(d_reference_begin, err.first);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(12, CUDA, CUDA_SAFE_CALL(cudaMalloc(&device_buffer, buffer_size * sizeof(value_type)));)"}
{"predict": "(23,OpenMP,#pragma omp parallel for),", "label": "(8, CUDA, cudaSetDevice(devs[0]);); (5, CUDA, CUCH(cudaMallocManaged(&ptr, size), kmcudaMemoryAllocationFailure);); (7, CUDA, CUCH(cudaMemcpy(ptr, (*samples)[0].get(), size, cudaMemcpyDefault), kmcudaMemoryCopyError);)"}
{"predict": "(34,OpenMP,#pragma omp parallel sections), (36,OpenMP,#pragma omp section), (46,OpenMP,#pragma omp section)", "label": "(51, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (33, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_B), sizeof(data_type) * B.size()));); (61, CUDA, CUDA_CHECK(cudaFree(d_B));); (66, CUDA, CUDA_CHECK(cudaDeviceReset());); (63, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (28, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (56, CUDA, CUDA_CHECK(cudaFree(d_A));); (22, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(12,OpenMP,#pragma omp parallel for reduction(+:local[id]))", "label": "(3, CUDA, __shared__ float local[threads];)"}
{"predict": "(28,OpenMP,#pragma omp parallel for)", "label": "(47, OpenMP, EXPECT_TRUE(check_test_image(output_file, t_apcomp_baseline_dir()));); (9, MPI, MPI_Comm_rank(comm, &par_rank);); (10, MPI, apcomp::mpi_comm(MPI_Comm_c2f(comm));); (8, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(25,OpenMP,#pragma omp parallel for)", "label": "(28, MPI, MPI_Comm_rank(shm_comm, &shm_rank);); (15, MPI, MPI_Win_shared_query(shm_win, MPI_PROC_NULL, &query_size, &query_disp_unit, &query_base);); (13, MPI, MPI_Win_allocate_shared(size, sizeof(int), MPI_INFO_NULL, MPI_COMM_SELF, &my_base, &shm_win);); (23, MPI, MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, rank, MPI_INFO_NULL, &shm_comm);); (34, MPI, MPI_Win_allocate_shared(0, sizeof(int), MPI_INFO_NULL, shm_comm, &my_base, &shm_win);); (20, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (67, MPI, MPI_Comm_free(&shm_comm);); (15, MPI, MPI_Win_free(&shm_win);); (21, MPI, MPI_Comm_size(shm_comm, &shm_nproc);); (31, MPI, MPI_Win_allocate_shared(size, sizeof(int), MPI_INFO_NULL, shm_comm, &my_base, &shm_win);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for reduction(+:sum)), (36,OpenMP,#pragma omp parallel for reduction(+:sum2))", "label": "(13, CUDA, cudaMalloc((void **)&d_arr, sizeof(int) * len);); (16, CUDA, cudaMemcpy(d_arr, arr, sizeof(int) * len, cudaMemcpyHostToDevice);); (53, CUDA, cudaFree(d_out);); (11, CUDA, cudaMalloc((void **)&d_out, sizeof(int) * len);); (23, CUDA, cudaMemcpy(out, d_out, sizeof(int) * len, cudaMemcpyDeviceToHost);); (49, CUDA, cudaFree(d_arr);)"}
{"predict": "(6,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for reduction(+:sum))", "label": "(4, OpenMP, int *shared_ptr = llvm_omp_target_alloc_shared(N * sizeof(int), device);); (5, OpenMP, #pragma omp target teams distribute parallel for device(device)); (0, OpenMP, const int device = omp_get_default_device();); (13, OpenMP, omp_target_free(shared_ptr, device);)"}
{"predict": "(9,OpenMP,#pragma omp parallel for num_threads(num_threads)), (11,OpenMP,#pragma omp parallel for num_threads(num_threads) private(j)), (29,OpenMP,#pragma omp parallel for num_threads(num_threads) private(j)), (40,OpenMP,#pragma omp parallel for num_threads(num_threads) private(j)), (52,OpenMP,#pragma omp parallel for num_threads(num_threads) private(j)), (81,OpenMP,#pragma omp parallel for num_threads(num_threads) private(j))", "label": "(52, TBB, tbb::flow::make_edge( b2, b3 );); (50, TBB, tbb::flow::make_edge( b, b2 );); (73, TBB, tbb::flow::buffer_node<T> b_copy(b);); (2, TBB, tbb::flow::buffer_node<T> b3(g);); (0, TBB, tbb::flow::buffer_node<T> b2(g);); (-3, TBB, tbb::flow::graph g;); (-3, TBB, tbb::flow::buffer_node<T> b(g);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for)", "label": "(22, OpenCL, clReleaseEvent(outEvent1);)"}
{"predict": "(41,OpenMP,#pragma omp parallel for reduction(min:minTime)), (46,OpenMP,#pragma omp parallel for)", "label": "(59, CUDA, cudaFree( deviceClockValues );); (60, CUDA, cudaFreeHost( hostOut );); (61, CUDA, cudaEventDestroy( stop );); (59, CUDA, cudaEventDestroy( start );); (56, CUDA, cudaFree( deviceThreadIDs );)"}
{"predict": "(59,OpenMP,#pragma omp parallel for private(t1, t2))", "label": "(63, MPI, t2 = MPI_Wtime();); (58, MPI, t1 = MPI_Wtime();)"}
{"predict": "KstarM, KstarW, 1, M_13));\n66: \n67:     std::vector<Lineshape *> LSKRP;\n68:     LSKRP.push_back(new Lineshapes::RBW(\"rho(770)\", RhoMass, RhoWidth, 1, M_12));\n69:     LSKRP.push_back(new Lineshapes::RBW(\"K*(892)bar\", KstarM, KstarW, 1, M_34));\n70:     LSKRP.push_back(new Lineshapes::RBW(\"rho(770)\", RhoMass, RhoWidth, 1, M_24));\n71:     LSKRP.push_back(new Lineshapes::RBW(\"K*(892)bar\", KstarM, KstarW, 1, M_13));\n72: \n73:     std::vector<Lineshape *> LSKRD;\n74:     LSKRD.push_back(new Lineshapes::RBW(\"rho(770)\", RhoMass, RhoWidth, 1, M_12));\n75:     LSKRD.push_back(new Lineshapes::RBW(\"K*(892)bar\", KstarM, KstarW, 1, M_34));\n76:     LSKRD.push_back(new Lineshapes::RBW(\"rho(770)\", RhoMass, RhoWidth, 1, M_24));\n77:     LSKRD.push_back(new Lineshapes::RBW(\"K*(892)bar\", KstarM, KstarW, 1, M_13));\n78: \n79:     std::vector<Lineshape *> LSKF;\n80:     LSKF.push_back(new Lineshapes::RBW(\"f600\", f600M, f600W, 1, M_12));\n81:     LSKF.push_back(new Lineshapes::RBW(\"K*(892)bar\", KstarM, KstarW, 1, M_34));\n82:     LSKF.push_back(new Lineshapes::RBW(\"f600\", f600M, f600W, 1", "label": "(184, CUDA, long accepted  = thrust::count_if(flags.begin(), flags.end(), thrust::identity<bool>());)"}
{"predict": "H5S_SELECT_SET, start, stride, count, block);\n137:     VRFY((ret >= 0), \"H5Sselect_hyperslab succeeded\");\n138: \n139:     /* write data independently */\n140:     xfer_plist = H5Pcreate (H5P_DATASET_XFER);\n141:     VRFY((xfer_plist >= 0), \"H5Pcreate xfer succeeded\");\n142:     ret = H5Pset_dxpl_mpio(xfer_plist, H5FD_MPIO_INDEPENDENT);\n143:     VRFY((ret >= 0), \"H5Pset_dxpl_mpio succeeded\");\n144:     ret = H5Dwrite(dataset1, H5T_NATIVE_INT, mem_dataspace, file_dataspace, xfer_plist, data_array1);\n145:     VRFY((ret >= 0), \"H5Dwrite succeeded\");\n146: \n147:     /* release resource */\n148:     ret = H5Sclose(file_dataspace);\n149:     VRFY((ret >= 0), \"\");\n150:     ret = H5Sclose(mem_dataspace);\n151:     VRFY((ret >= 0), \"\");\n152:     ret = H5Pclose(xfer_plist);\n153:     VRFY((ret >= 0), \"\");\n154: \n155: \n156:     /* -------------------------\n157:      * Test writing to dataset2\n158:      * -------------------------*/\n159:     /* set up dimensions of the slab this process accesses */\n160:     slab_set(mpi_rank, mpi_size, start, count, stride, block, BYROW);\n161: \n162:     /* put some trivial data in the data_array */\n163:     dataset_fill(start, block, data_array1);\n164:     MESG(\"data_array initialized\");\n165:     if(VERBOSE_MED) {\n166: \tMESG(\"data_array created\");\n167: \tdataset_print(start, block, data_array1);\n168:     }\n169: \n170:     /* create a memory dataspace independently */\n1", "label": "(34, MPI, MPI_Comm_size(MPI_COMM_WORLD,&mpi_size);); (34, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&mpi_rank);)"}
{"predict": "(7,OpenMP,#pragma omp parallel for private(degree_idx, out_prime_idx, accum, obase_value, obase_ratio, scale, scale_shoup, out1, out2, input1, input2))", "label": "(6, CUDA, extern __shared__ uint64_t s_QHatModp[];)"}
{"predict": "free(data);\n109:     if (me == nproc - 1) {\n110:         free(mine);\n111:         free(mine_plus);\n112:     }\n113:     if (me == 0) {\n114:         free(expected);\n115:     }\n116:     MTest_Finalize(0);\n117:     return 0;\n118: }", "label": "(81, MPI, MPI_Win_create(data, sizeof(twoint_t) * ARRAY_LEN, 1, info_in, MPI_COMM_WORLD, &win);); (81, MPI, MPI_Info_free(&info_in);); (134, MPI, MPI_Accumulate(mine_plus, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_MAXLOC, win);); (151, MPI, MPI_Accumulate(mine_plus, ARRAY_LEN, MPI_2INT, 0, 0, ARRAY_LEN, MPI_2INT, MPI_MAXLOC, win);); (34, MPI, MPI_Win_fence(0, win);); (56, MPI, MPI_Accumulate(mine, ARRAY_LEN, MPI_2INT, 0, 0, ARRAY_LEN, MPI_2INT, MPI_REPLACE, win);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (63, MPI, MPI_Win_free(&win);); (70, MPI, MPI_Info_create(&info_in);); (70, MPI, MPI_Info_set(info_in, \"accumulate_ordering\", \"none\");); (32, MPI, MPI_Accumulate(mine, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_REPLACE, win);); (181, MPI, MPI_Accumulate(mine, ARRAY_LEN, MPI_2INT, 0, 0, ARRAY_LEN, MPI_2INT, MPI_MINLOC, win);); (31, MPI, MPI_Accumulate(mine_plus, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_REPLACE, win);); (48, MPI, MPI_Accumulate(mine_plus, ARRAY_LEN, MPI_2INT, 0, 0, ARRAY_LEN, MPI_2INT, MPI_REPLACE, win);); (160, MPI, MPI_Accumulate(mine, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_MINLOC, win);); (1, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (20, MPI, MPI_Win_create(data, sizeof(twoint_t) * ARRAY_LEN, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &win);)"}
{"predict": "(38,OpenMP,#pragma omp parallel for), (69,OpenMP,#pragma omp parallel for)", "label": "(56, CUDA, cudaEventRecord(stop);); (22, CUDA, CUDA_CHECK(cudaDeviceSetLimit(cudaLimitStackSize, 2 * 1024));); (55, CUDA, cudaEventSynchronize(stop);); (58, CUDA, cudaEventDestroy(start);); (5, CUDA, CUDA_CHECK(cudaDeviceReset());); (21, CUDA, CUDA_CHECK(cudaDeviceSynchronize());); (46, CUDA, cudaEventRecord(start);); (55, CUDA, cudaEventDestroy(stop);); (57, CUDA, CUDA_CHECK(cudaGetLastError());); (8, CUDA, cudaDeviceGetLimit(&size_value, cudaLimitStackSize);); (12, CUDA, CUDA_CHECK(cudaDeviceSetLimit(cudaLimitMallocHeapSize, heap_size));); (40, CUDA, cudaEventCreate(&stop);); (46, CUDA, cudaEventElapsedTime(&milliseconds, start, stop);); (37, CUDA, cudaEventCreate(&start);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for)", "label": "(19, OpenMP, #pragma omp parallel for reduction(+:s)); (10, OpenMP, #pragma omp parallel for)"}
{"predict": "(10,OpenMP,#pragma omp parallel for reduction(min:output[MMS_MIN]) reduction(max:output[MMS_MAX]) reduction(+:output[MMS_SUM]))", "label": "(10, MPI, MPI_Allreduce(input, output, 1, dtcmp_type_3uint64t, dtcmp_reduceop_mms_3uint64t, comm);)"}
{"predict": "(16,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for), (35,OpenMP,#pragma omp parallel for), (38,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for), (41,OpenMP,#pragma omp parallel for), (45,OpenMP,#pragma omp parallel for), (46,OpenMP,#pragma omp parallel for)", "label": "(19, MPI, MPI_Bcast(&array_size, 1, MPI_INT, 0, MPI_COMM_WORLD);); (26, MPI, MPI_Wait(&request, &status);); (41, MPI, end_time = MPI_Wtime();); (66, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (2, MPI, MPI_Init(NULL, NULL);); (2, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (25, MPI, start_time = MPI_Wtime();); (12, MPI, MPI_Bcast(&nr_iters, 1, MPI_INT, 0, MPI_COMM_WORLD);); (6, MPI, MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);)"}
{"predict": "The provided code snippet is primarily focused on initializing data structures and setting up test cases. There are no loops or computationally intensive sections that would benefit from parallelization. Therefore, there are no lines in this specific code snippet where parallelization instructions should be inserted.\n\nIf there were loops or other sections that could benefit from parallelization, they would typically be marked with OpenMP directives such as `#pragma omp parallel for`. However, based on the current code, no such instructions are applicable.", "label": "(2, OpenMP, omp_autotune(self, OMP_SCALE);)"}
{"predict": "p3 );\n137:     }\n138: fn_cleanup3_1:;\n139:     if (p2 != NULL) MPIU_Free( p2 );\n140:     if (ptmp2 != NULL) MPIU_Free( ptmp2 );\n141: fn_cleanup2_2:;\n142:     if (p2 != NULL) MPIU_Free( p2 );\n143: fn_cleanup2_1:;\n144:     if (ptmp2 != NULL) MPIU_Free( ptmp2 );\n145:     if (*ierr != MPI_SUCCESS) {\n146:         *ierr = MPIR_Err_return_comm( NULL, __FUNCTION__, *ierr );\n147:     }\n148: }", "label": "(127, MPI, *ierr = MPI_Comm_spawn_multiple( *v1, p2, p3, v4, v5, *v6, (MPI_Comm)(*v7), (MPI_Comm *)(v8), (int *)v9 );)"}
{"predict": "(15,OpenMP,#pragma omp parallel for private(my_largest, my_besttile, localBestScore) reduction(max:largest,bestScore))", "label": "(10, OpenMP, #pragma omp parallel); (14, OpenMP, #pragma omp for); (95, OpenMP, #pragma omp critical)"}
{"predict": "(16,OpenMP,#pragma omp parallel sections), (17,OpenMP,#pragma omp section), (19,OpenMP,#pragma omp section)", "label": "(22, MPI, MPI_Start(&requests[0]);); (17, MPI, MPI_Barrier_init(MPI_COMM_WORLD, MPI_INFO_NULL, &requests[1]);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (14, MPI, MPI_Bcast_init(&a, 1, MPI_INT, 0, MPI_COMM_WORLD, MPI_INFO_NULL, &requests[0]);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (18, MPI, MPI_Start(&requests[1]);); (23, MPI, MPI_Waitall(2, requests, MPI_STATUSES_IGNORE);); (5, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (27, MPI, MPI_Request_free(&requests[0]);); (27, MPI, MPI_Request_free(&requests[1]);)"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for reduction(+:sum1) collapse(2)), (22,OpenMP,#pragma omp parallel for reduction(+:sum1) collapse(2)), (32,OpenMP,#pragma omp parallel for)", "label": "(11, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:sum1) reduction(+:sum1) collapse(2))"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(16, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (22, TBB, serial_tiled_time = (tbb::tick_count::now() - t0).seconds();); (27, TBB, tbb_time = (tbb::tick_count::now() - t0).seconds();); (15, TBB, serial_time = (tbb::tick_count::now() - t0).seconds();)"}
{"predict": "(30,OpenMP,#pragma omp parallel for), (76,OpenMP,#pragma omp parallel for)", "label": "(50, CUDA, cudaGetDeviceProperties(&prop, 0);); (125, CUDA, cudaFree(h_aCached);); (29, CUDA, cudaMallocHost((void**)&h_aCached, bytes);); (119, CUDA, cudaFreeHost(h_aPinned);); (119, CUDA, cudaFreeHost(h_bPinned);); (29, CUDA, cudaHostAlloc((void**)&h_bPinned, bytes, cudaHostAllocWriteCombined);); (24, CUDA, cudaMalloc((void**)&d_b, bytes);); (26, CUDA, cudaHostAlloc((void**)&h_aPinned, bytes, cudaHostAllocWriteCombined);); (24, CUDA, cudaMallocHost((void**)&h_bCached, bytes);); (116, CUDA, cudaFree(d_b);); (-1, CUDA, cudaSetDevice(0);); (113, CUDA, cudaFree(d_a);); (115, CUDA, cudaFree(h_bCached);); (-10, CUDA, cudaGetDeviceCount(&count);); (-2, CUDA, cudaGetDevice(&deviceID);); (14, CUDA, cudaMalloc((void**)&d_a, bytes);)"}
{"predict": "(26,OpenMP,#pragma omp parallel for)", "label": "(30, OpenCL, err = clEnqueueWriteBuffer( queue, streams[0], true, 0, sizeof(cl_int)*num_elements, (void *)int_input_ptr, 0, NULL, NULL );); (39, OpenCL, err = clEnqueueCopyBuffer( queue, streams[0], streams[1], 0, 0, sizeof(cl_int)*num_elements, 0, NULL, &copyEvent );); (32, OpenCL, clReleaseMemObject( streams[1] );); (107, OpenCL, err = clEnqueueReadBuffer( queue, streams[1], true, 0, sizeof(cl_int)*num_elements, (void *)int_output_ptr, 0, NULL, NULL );); (48, OpenCL, err = clWaitForEvents( 1, &copyEvent );); (28, OpenCL, clReleaseMemObject( streams[0] );); (49, OpenCL, clReleaseEvent(copyEvent);)"}
{"predict": "(1,OpenMP,#pragma omp parallel private(myid, numprocs, left, right, buffer, status))", "label": "(9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myid);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &numprocs);); (16, MPI, MPI_Finalize();); (4, MPI, MPI_Init(&argc,&argv);); (12, MPI, MPI_Sendrecv_replace(buffer, 10, MPI_INT, left, 123, right, 123, MPI_COMM_WORLD, &status);)"}
{"predict": "(6,OpenMP,#pragma omp parallel for)", "label": "(3, TBB, REMARK(\"is_large_object<int>::value=%d\\n\", tbb::interface6::internal::is_large_object<int>::value);); (4, TBB, REMARK(\"is_large_object<int *>::value=%d\\n\", tbb::interface6::internal::is_large_object<int *>::value);); (2, TBB, REMARK(\"is_large_object<double>::value=%d\\n\", tbb::interface6::internal::is_large_object<double>::value);); (4, TBB, REMARK(\"is_large_object<check_type<int>* >::value=%d\\n\", tbb::interface6::internal::is_large_object<check_type<int>* >::value);); (2, TBB, REMARK(\"is_large_object<check_type<int> >::value=%d\\n\", tbb::interface6::internal::is_large_object<check_type<int> >::value);); (3, TBB, REMARK(\"is_large_object<check_type<short> >::value=%d\\n\\n\", tbb::interface6::internal::is_large_object<check_type<short> >::value);); (8, TBB, tbb::task_scheduler_init init(nthread);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(22,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp single), (27,OpenMP,#pragma omp single)", "label": "(47, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_ARE_FATAL);); (21, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (21, MPI, MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);); (18, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (38, MPI, MPI_Error_string(ierr, str, &slen);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for private(sum_loss1, sum_loss2) reduction(+:sum_loss1, sum_loss2)), (30,OpenMP,#pragma omp parallel for private(sum_loss1, sum_loss2) reduction(+:sum_loss1, sum_loss2)), (86,OpenMP,#pragma omp parallel for), (96,OpenMP,#pragma omp parallel for)", "label": "(53, CUDA, extern __shared__ float s_float[];)"}
{"predict": "(12,OpenMP,#pragma omp parallel for private(i) reduction(+:n_finished,n_cancelled)), (20,OpenMP,i = omp_get_thread_num();), (32,OpenMP,#pragma omp parallel for)", "label": "(26, OpenMP, #pragma omp barrier); (19, OpenMP, common::nvtx::range omp_scope(\"interruptible::thread-%d\", i);); (13, OpenMP, omp_set_dynamic(0);); (14, OpenMP, #pragma omp parallel reduction(+ : n_finished) reduction(+ : n_cancelled) num_threads(n_threads)); (12, OpenMP, omp_set_num_threads(n_threads);); (14, OpenMP, auto i = omp_get_thread_num();)"}
{"predict": "(23,OpenMP,#pragma omp parallel for)", "label": "(25, MPI, MPI_Send(lnzval, len2, MPI_DOUBLE, receiver, k, grid3d->zscp.comm);)"}
{"predict": "(69,OpenMP,#pragma omp parallel for reduction(+:val_st_off)) (93,OpenMP,#pragma omp parallel for reduction(+:val_st_on)) (117,OpenMP,#pragma omp parallel for) (122,OpenMP,#pragma omp parallel for)", "label": "(47, OpenMP, lis2mdl_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)"}
{"predict": "(19,OpenMP,#pragma omp parallel for)", "label": "(2, OpenMP, #pragma omp for schedule(nonmonotonic: dynamic))"}
{"predict": "(26,OpenMP,#pragma omp parallel for)", "label": "(21, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for)", "label": "(35, CUDA, cudaFree(d_ret);); (7, CUDA, cudaMallocHost(&h_stride, STRIDE_MEM_SIZE);); (14, CUDA, cudaMalloc(&d_ret, sizeof(uint32_t));); (31, CUDA, cudaFree(d_stride);); (32, CUDA, cudaFree(d_clk);); (32, CUDA, cudaFreeHost(h_stride);); (25, CUDA, cudaMemcpy(h_clk, d_clk, 32 * sizeof(uint32_t), cudaMemcpyDeviceToHost);); (10, CUDA, cudaMemcpy(d_stride, h_stride, STRIDE_MEM_SIZE, cudaMemcpyHostToDevice);); (7, CUDA, cudaMalloc(&d_stride, STRIDE_MEM_SIZE);); (11, CUDA, cudaMalloc(&d_clk, 32 * sizeof(uint32_t));)"}
{"predict": "(22,OpenMP,#pragma omp parallel for)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(2,OpenMP,#pragma omp target map(tofrom: a[0:N]) map(from: b[0:N]))", "label": "(5, OpenMP, #pragma omp target enter data map(to:a[:N]))"}
{"predict": "(4,OpenMP,#pragma omp parallel sections), (14,OpenMP,#pragma omp section), (17,OpenMP,#pragma omp section), (20,OpenMP,#pragma omp section), (27,OpenMP,#pragma omp section), (32,OpenMP,#pragma omp section)", "label": "(15, TBB, test_basic<MyCheckedStateSet>(\"concurrent unordered set (checked element state)\", tbb::internal::true_type());)"}
{"predict": "(22,OpenMP,#pragma omp parallel for private(cCellStart) reduction(max:finalValue) reduction(max:finalIndex)), (...)", "label": "(17, CUDA, extern __shared__ _VOLATILE_ float sMem[];)"}
{"predict": "(8,OpenMP,#pragma omp parallel sections)", "label": "(9, CUDA, factory = *internal::MakeCUDAExecutorImplementation();)"}
{"predict": "(3,OpenMP,#include <omp.h>); (16,OpenMP,int nthread_input = omp_get_max_threads();); (77,MPI,MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (77,MPI,MPI_Bcast(&total_length, 1, MPI_LONG, root, MPI_COMM_WORLD);); (77,MPI,MPI_Bcast(&offset, 1, MPI_LONG, root, MPI_COMM_WORLD);); (77,MPI,MPI_Bcast(&length, 1, MPI_LONG, root, MPI_COMM_WORLD);); (101,OpenMP,#pragma omp parallel for); (118,OpenMP,#pragma omp parallel for simd num_threads(nthread_input));", "label": "(103, OpenMP, printf(\"Number of threads    = %d\\n\", omp_get_max_threads());); (25, MPI, MPI_Comm_size(MPI_COMM_WORLD,&Num_procs);); (107, OpenMP, #pragma omp parallel for simd); (147, MPI, MPI_Finalize();); (76, MPI, MPI_Bcast(&length,        1, MPI_LONG, root, MPI_COMM_WORLD);); (22, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&my_ID);); (77, MPI, MPI_Bcast(&nthread_input, 1, MPI_INT,  root, MPI_COMM_WORLD);); (78, OpenMP, omp_set_num_threads(nthread_input);); (17, MPI, MPI_Init(&argc,&argv);); (73, MPI, MPI_Bcast(&iterations,    1, MPI_INT,  root, MPI_COMM_WORLD);); (114, MPI, MPI_Barrier(MPI_COMM_WORLD);); (70, MPI, MPI_Bcast(&offset,        1, MPI_LONG, root, MPI_COMM_WORLD);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for reduction(+:sum))", "label": "(6, CUDA, CHECK(cudaMemcpyFromSymbol(&block_size, BLOCK_DIM, sizeof(real)));); (34, CUDA, CHECK(cudaEventCreate(&stop));); (43, CUDA, CHECK(cudaEventRecord(stop));); (78, CUDA, CHECK(cudaFree(d_y));); (33, CUDA, cudaEventQuery(start);); (41, CUDA, CHECK(cudaEventSynchronize(stop));); (41, CUDA, CHECK(cudaEventElapsedTime(&curr_time, start, stop));); (34, CUDA, CHECK(cudaMemcpy(h_y, d_y, size, cudaMemcpyDefault));); (28, CUDA, CHECK(cudaEventRecord(start));); (20, CUDA, CHECK(cudaMalloc(&d_x, M));); (70, CUDA, CHECK(cudaFree(d_x));); (20, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyDefault));); (30, CUDA, CHECK(cudaGetLastError());); (21, CUDA, CHECK(cudaEventCreate(&start));); (16, CUDA, CHECK(cudaMalloc(&d_y, size));)"}
{"predict": "(37,OpenMP,#pragma omp parallel for reduction(+:sum))", "label": "(28, MPI, MPI_Type_get_extent(fd->filetype, &lb, &filetype_extent);); (28, MPI, MPI_Type_size_x(fd->filetype, &filetype_size);)"}
{"predict": "(56,OpenMP,#pragma omp parallel for)", "label": "(60, CUDA, CHECK_ERROR(cudaFree(device_params));); (12, CUDA, CHECK_ERROR(cudaMalloc((void **)&device_signals, complex_size_bytes));); (50, CUDA, CHECK_ERROR(cudaMemcpy(output_signals, device_signals, complex_size_bytes, cudaMemcpyDeviceToHost));); (56, CUDA, CHECK_ERROR(cudaFree(device_signals));); (34, CUDA, CHECK_ERROR(cudaMemcpyFromSymbol(&host_callback_ptr, device_callback_ptr, sizeof(host_callback_ptr)));); (9, CUDA, CHECK_ERROR(cudaMemcpy(device_signals, input_signals, complex_size_bytes, cudaMemcpyHostToDevice));); (17, CUDA, CHECK_ERROR(cudaMalloc((void **)&device_params, sizeof(cb_params)));); (17, CUDA, CHECK_ERROR(cudaMemcpy(device_params, &host_params, sizeof(cb_params), cudaMemcpyHostToDevice));)"}
{"predict": "(1,OpenMP,#pragma omp parallel sections), (20,OpenMP,#pragma omp declare target), (26,OpenMP,#pragma omp end declare target)", "label": "(25, CUDA, cudaDeviceSynchronize();)"}
{"predict": "(15,OpenMP,#pragma omp parallel for)", "label": "(15, CUDA, checkCudaRuntime(cudaMemcpyAsync(image_device, image_host, size_image, cudaMemcpyHostToDevice, stream));)"}
{"predict": "(26,OpenMP,#pragma omp parallel for private(accumulators) reduction(+:accumulators))", "label": "(35, CUDA, __shared__ float sub_results[x128::size][32][bdy];)"}
{"predict": "ILON_ERROR,\n124:                     0.0f);\n125:     printf(\"Test %s!\\n\", bResult ? \"PASSED\" : \"FAILED\");\n126:   }\n127: \n128:   // cleanup memory\n129:   cudaFreeArray(cu_3darray);\n130:   cudaFreeTextureObject(tex);\n131:   free(h_data);\n132:   free(h_data_ref);\n133:   free(h_odata);\n134: \n135:   return (bResult ? EXIT_SUCCESS : EXIT_FAILURE);\n136: }", "label": "(59, CUDA, myparms.dstPos = make_cudaPos(0, 0, 0);); (9, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProps, devID));); (56, CUDA, myparms.srcPos = make_cudaPos(0, 0, 0);); (119, CUDA, checkCudaErrors(cudaMemcpy(h_odata, d_data, size, cudaMemcpyDeviceToHost));); (94, CUDA, getLastCudaError(\"warmup Kernel execution failed\");); (60, CUDA, checkCudaErrors(cudaMemcpy3D(&myparms));); (57, CUDA, myparms.extent = make_cudaExtent(width, width, num_faces);); (137, CUDA, checkCudaErrors(cudaFreeArray(cu_3darray));); (76, CUDA, checkCudaErrors(cudaCreateTextureObject(&tex, &texRes, &texDescr, NULL));); (91, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (36, CUDA, checkCudaErrors(cudaMalloc((void **)&d_data, size));); (99, CUDA, getLastCudaError(\"Kernel execution failed\");); (131, CUDA, checkCudaErrors(cudaFree(d_data));); (-10, CUDA, int devID = findCudaDevice(argc, (const char **)argv);); (128, CUDA, checkCudaErrors(cudaDestroyTextureObject(tex));)"}
{"predict": "(35,OpenMP,#pragma omp parallel sections), (35,OpenMP,#pragma omp section), (39,OpenMP,#pragma omp section), (40,OpenMP,#pragma omp section), (43,OpenMP,#pragma omp section), (46,OpenMP,#pragma omp section), (50,OpenMP,#pragma omp section), (54,OpenMP,#pragma omp section), (57,OpenMP,#pragma omp section), (59,OpenMP,#pragma omp section), (61,OpenMP,#pragma omp section), (63,OpenMP,#pragma omp section), (65,OpenMP,#pragma omp section), (68,OpenMP,#pragma omp section), (71,OpenMP,#pragma omp section), (73,OpenMP,#pragma omp section), (76,OpenMP,#pragma omp section), (78,OpenMP,#pragma omp section), (80,OpenMP,#pragma omp section), (87,OpenMP,#pragma omp section), (94,OpenMP,#pragma omp section), (95,OpenMP,#pragma omp section), (96,OpenMP,#pragma omp section), (97,OpenMP,#pragma omp section), (99,OpenMP,#pragma omp section), (100,OpenMP,#pragma omp section), (101,OpenMP,#pragma omp section), (102,OpenMP,#pragma omp section), (109,OpenMP,#pragma omp section), (110,OpenMP,#pragma omp section), (113,OpenMP,#pragma omp section), (35,OpenMP,#pragma omp section)", "label": "(80, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (80, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)"}
{"predict": "(28,MPI,MPI_Barrier(MPI_COMM_WORLD);); (32,MPI,MPI_Barrier(MPI_COMM_WORLD);); (39,OpenMP,#pragma omp parallel for); (44,OpenMP,#pragma omp parallel for);", "label": "(48, MPI, MPI_Waitall(WINDOW, req, status);); (23, MPI, start = MPI_Wtime();); (50, MPI, end = MPI_Wtime();)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(8, SYCL, return hipsycl::libkernel::sscp::wg_broadcast(sender, x, shrd_x);)"}
{"predict": "// inner loop: for (omp.iv = LB; omp.iv < EUB; omp.iv += distInc) {\n181: \n182: \n183: \n184:       // check exit condition\n185: \n186:       // check that PrevLB and PrevUB are passed to the 'for'\n187: \n188:       // check stride 1 for 'for' in 'distribute parallel for'\n189: \n190: \n191:       [&]() {\n192: \ta[i] = b[i] + c[i];\n193:       }();\n194:     }\n195: \n196:     // schedule: guided\n197:     #pragma omp target\n198:     #pragma omp teams\n199: \n200:     for (int i = 0; i < n; ++i) {\n201:       a[i] = b[i] + c[i];\n202: \n203:       // skip rest of implementation of 'distribute' as it is tested above for default dist_schedule case\n204: \n205:       // 'parallel for' implementation using outer and inner loops and PrevEUB\n206: \n207:       // initialize lb and ub to PrevLB and PrevUB\n208: \n209:       // check PrevEUB (using PrevUB instead of NumIt as upper bound)\n210: \n211:       // initialize omp.iv (IV = LB)\n212: \n213:       // outer loop: while (IV < UB) {\n214: \n215: \n216: \n217:       // inner loop: for (omp.iv = LB; omp.iv < EUB; omp.iv += distInc) {\n218: \n219: \n220: \n221:       // check exit condition\n222: \n223:       // check that PrevLB and PrevUB are passed to the 'for'\n224: \n225:       // check stride 1 for 'for' in 'distribute parallel for'\n226: \n227: \n228:       [&]() {\n229:", "label": "(169, OpenMP, #pragma omp distribute parallel for schedule(static, ch)); (201, OpenMP, #pragma omp distribute parallel for schedule(dynamic)); (17, OpenMP, #pragma omp distribute parallel for); (59, OpenMP, #pragma omp distribute parallel for dist_schedule(static)); (227, OpenMP, #pragma omp distribute parallel for schedule(dynamic, ch)); (132, OpenMP, #pragma omp distribute parallel for schedule(static)); (10, OpenMP, #pragma omp target); (97, OpenMP, #pragma omp distribute parallel for dist_schedule(static, ch)); (9, OpenMP, #pragma omp teams)"}
{"predict": "138: #endif\n139: \n140:   // Copy the result back to the host\n141:   checkCudaErrors(cudaMemcpy(output, bufferOut + padding,\n142:                              volumeSize * sizeof(float),\n143:                              cudaMemcpyDeviceToHost));\n144: \n145:   // Free the memory buffers\n146:   checkCudaErrors(cudaFree(bufferOut));\n147:   checkCudaErrors(cudaFree(bufferIn));\n148: \n149:   return true;\n150: }", "label": "(109, CUDA, checkCudaErrors(cudaEventCreate(&profileStart));); (43, CUDA, checkCudaErrors(cudaSetDevice(targetDevice));); (40, CUDA, targetDevice = findCudaDevice(argc, (const char **)argv);); (193, CUDA, checkCudaErrors(cudaEventDestroy(profileEnd));); (140, CUDA, checkCudaErrors(cudaEventRecord(profileEnd, 0));); (116, CUDA, checkCudaErrors(cudaEventRecord(profileStart, 0));); (186, CUDA, checkCudaErrors(cudaEventDestroy(profileStart));); (61, CUDA, checkCudaErrors(cudaFuncGetAttributes(&funcAttrib, FiniteDifferencesKernel));); (31, CUDA, checkCudaErrors(cudaGetDeviceCount(&deviceCount));); (101, CUDA, checkCudaErrors(cudaEventCreate(&profileEnd));); (176, CUDA, checkCudaErrors(cudaFree(bufferOut));); (171, CUDA, checkCudaErrors(cudaFree(bufferIn));)"}
{"predict": "(29,OpenMP,#pragma omp declare target)", "label": "(38, CUDA, cudaFree(res_dev);); (31, CUDA, cudaMalloc(&res_dev, sizeof(float));); (33, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for)", "label": "(18, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (18, CUDA, checkCudaErrors(cudaGetLastError());)"}
{"predict": "(26,OpenMP,#pragma omp parallel sections), (32,OpenMP,#pragma omp section), (44,OpenMP,#pragma omp section), (58,OpenMP,#pragma omp section), (66,OpenMP,#pragma omp section)", "label": "(8, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (9, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp task), (27,OpenMP,#pragma omp taskwait), (34,OpenMP,#pragma omp task), (45,OpenMP,#pragma omp parallel for)", "label": "(29, OpenMP, #pragma omp taskwait); (22, OpenMP, #pragma omp task); (9, OpenMP, #pragma omp parallel); (10, OpenMP, #pragma omp single)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(2, SYCL, sycl::queue q = get_queue();); (3, SYCL, int* data = sycl::malloc_device<int>(1024, q);)"}
{"predict": "(29,OpenMP,#pragma omp parallel for)", "label": "(7, MPI, MPI_Comm_rank(comm, &rank);); (49, MPI, MPI_Bcast(&num_groups, 1, MPI_INT, 0, comm);)"}
{"predict": "(26,OpenMP,#pragma omp parallel sections), (50,OpenMP,#pragma omp section)", "label": "(4, OpenMP, #pragma omp sections); (49, OpenMP, #pragma omp sections nowait); (28, OpenMP, #pragma omp section)"}
{"predict": "(29,OpenMP,#pragma omp parallel for)", "label": "(36, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (38, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(31,OpenMP,#pragma omp parallel)", "label": "(5, CUDA, c.CUDAConfig().device_id = 0;)"}
{"predict": "(24,OpenMP,#pragma omp parallel for reduction(+:num)), (54,OpenMP,#pragma omp parallel for reduction(+:recvbytes)), (85,OpenMP,#pragma omp parallel for)", "label": "(65, MPI, MPI_Gatherv(sendbuf, bytes, MPI_BYTE, recvbuf, counts, disps, MPI_BYTE, 0, MPI_COMM_WORLD);); (13, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (36, MPI, MPI_Gather(&bytes, 1, MPI_INT, counts, 1, MPI_INT, 0, MPI_COMM_WORLD);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for private(key, color, newcomm))", "label": "(32, MPI, MPI_Comm_test_inter(intercomm, &flag);); (23, MPI, MPI_Comm_rank(intercomm, &key);); (90, MPI, MPI_Comm_free(&intercomm);); (38, MPI, MPI_Comm_size(intercomm, &orig_size);); (38, MPI, MPI_Comm_size(newcomm, &new_size);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (33, MPI, MPI_Comm_remote_size(intercomm, &orig_rsize);); (19, MPI, MPI_Comm_split(intercomm, color, key, &newcomm);); (3, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (31, MPI, MPI_Comm_remote_size(newcomm, &new_rsize);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for reduction(+:th_sum))", "label": "(6, CUDA, extern __shared__ float shared[];)"}
{"predict": "(23,OpenMP,#pragma omp parallel for)", "label": "(34, CUDA, cudaFree(control_d);); (32, CUDA, cudaFree(state_der_d);); (22, CUDA, CudaCheckError();); (29, CUDA, cudaFree(state_d);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for reduction(+:res))", "label": "(7, CUDA, fcuda::Check_CudaErroorFun(\"DgReduSumUint\");); (5, CUDA, cudaMemcpy(data,datag+inidata,sizeof(unsigned)*ndata,cudaMemcpyDeviceToHost);)"}
{"predict": "(29,OpenMP,#pragma omp parallel for private(alias, sample, avcol) reduction(+:col))", "label": "(37, TBB, tbb::spin_mutex::scoped_lock lock (MyMutex);)"}
{"predict": "(31,OpenMP,#pragma omp parallel for private(j, Qi, Kj, Vj, S, row_m_prev, row_l_prev, row_m, sum, row_l, row_m_new, row_l_new, pv))", "label": "(24, CUDA, extern __shared__ float sram[];)"}
{"predict": "(14,OpenMP,#pragma omp parallel),", "label": "(14, OpenMP, #pragma omp parallel); (14, OpenMP, #pragma omp task if (task: false))"}
{"predict": "(2,OpenMP,#pragma omp parallel for), (5,OpenMP,#pragma omp parallel for), (8,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for)", "label": "(47, OpenMP, #pragma omp target teams distribute parallel for collapse(collapse(tmain<int, char, -1, -2>(argc, argv)); (4, OpenMP, #pragma omp target teams distribute parallel for collapse (); (7, OpenMP, #pragma omp target teams distribute parallel for collapse ()); (18, OpenMP, #pragma omp target teams distribute parallel for collapse (foobool(1) > 0 ? 1 : 2)); (13, OpenMP, #pragma omp target teams distribute parallel for collapse (2+2))); (23, OpenMP, #pragma omp target teams distribute parallel for collapse (foobool(argc)), collapse (true), collapse (-5)); (7, OpenMP, #pragma omp target teams distribute parallel for collapse (4); (25, OpenMP, #pragma omp target teams distribute parallel for collapse (S1)); (-7, OpenMP, #pragma omp target teams distribute parallel for collapse); (41, OpenMP, #pragma omp target teams distribute parallel for collapse (2))"}
{"predict": "This code does not contain any loops or independent tasks that can be parallelized using OpenMP or MPI, as it is a series of conditional checks and cleanup operations. Each operation is dependent on the previous one and cannot be executed concurrently without affecting the correctness of the program. Therefore, there are no appropriate lines to insert parallelization instructions in this specific code snippet.", "label": "(32, CUDA, cudaFree(resourceList->outputBufferGPU);); (27, CUDA, cudaFree(resourceList->inputBufferGPU);); (44, CUDA, cudaStreamDestroy(resourceList->stream);)"}
{"predict": "(28,OpenMP,#pragma omp parallel for),", "label": "(77, CUDA, EXPECT_EQ(cudaSuccess, cudaMemcpy(test.data(), output->basePtr(), outBufSize, cudaMemcpyDeviceToHost));); (2, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (64, CUDA, EXPECT_EQ(cudaSuccess, cudaMemset(input->basePtr(), 0xFF, inSampleStride * inAccess->numSamples()));); (72, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (72, CUDA, EXPECT_EQ(cudaSuccess, cudaMemcpy(testIn.data(), input->basePtr(), inBufSize, cudaMemcpyDeviceToHost));); (62, CUDA, EXPECT_EQ(cudaSuccess, cudaMemset(output->basePtr(), 0xFF, outSampleStride * outAccess->numSamples()));); (78, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamDestroy(stream));)"}
{"predict": "(5,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for num_threads(threadLimit))", "label": "(28, OpenMP, #pragma omp parallel num_threads(NumThreads3)); (29, OpenMP, check3[omp_get_thread_num()] += omp_get_num_threads();); (15, OpenMP, #pragma omp parallel); (10, OpenMP, threadLimit = omp_get_thread_limit();); (6, OpenMP, #pragma omp target teams num_teams(1) thread_limit(ThreadLimit)); (20, OpenMP, check2[omp_get_thread_num()] += omp_get_num_threads();); (16, OpenMP, omp_set_num_threads(NumThreads2);); (12, OpenMP, check1[omp_get_thread_num()] += omp_get_num_threads();)"}
{"predict": "(10,OpenMP,#pragma omp parallel sections)", "label": "(13, CUDA, getLastCudaError(\"mergeElementaryIntervalsKernel<1> failed\\n\");); (17, CUDA, getLastCudaError(\"mergeElementaryIntervalsKernel<0> failed\\n\");)"}
{"predict": "(26,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for), (38,OpenMP,#pragma omp parallel for), (45,OpenMP,#pragma omp parallel for), (103,OpenMP,#pragma omp parallel for), (104,OpenMP,#pragma omp parallel for)", "label": "(42, CUDA, cudaFree(B);); (109, CUDA, result = cudaMemcpy(host_cutlass.data(), C_cutlass, sizeof_C, cudaMemcpyDeviceToHost);); (59, CUDA, cudaFree(C_reference);); (52, CUDA, result = cudaMemcpy(C_reference, C_cutlass, sizeof_C, cudaMemcpyDeviceToDevice);); (47, CUDA, cudaFree(C_cutlass);); (119, CUDA, result = cudaMemcpy(host_reference.data(), C_reference, sizeof_C, cudaMemcpyDeviceToHost);); (28, CUDA, cudaFree(A);)"}
{"predict": "(29,OpenMP,#pragma omp parallel for private(j,temp) reduction(min:sum2))", "label": "(40, OpenMP, #pragma omp teams reduction(+:sum)); (37, OpenMP, #pragma omp target map(tofrom: b, sum, t) map(alloc: a)); (40, OpenMP, #pragma omp distribute parallel for reduction(+:sum)); (42, OpenMP, t[i] = omp_get_team_num();)"}
{"predict": "(11,OpenMP,#pragma omp parallel for)", "label": "(14, CUDA, fprintf(stderr, \"CUDA kernel failed : %s\\n\", cudaGetErrorString(err));); (11, CUDA, err = cudaGetLastError();)"}
{"predict": "(39,OpenMP,#pragma omp parallel for), (64,OpenMP,#pragma omp parallel for), (74,OpenMP,#pragma omp parallel for), (82,OpenMP,#pragma omp parallel for), (88,OpenMP,#pragma omp parallel for), (96,OpenMP,#pragma omp parallel for), (107,OpenMP,#pragma omp parallel for), (117,OpenMP,#pragma omp parallel for)", "label": "(4, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (5, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;)"}
{"predict": "(41,OpenMP,#pragma omp parallel for)", "label": "(4, CUDA, CudaCheckError();)"}
{"predict": "1.comm);\n150:         }\n151: \n152:         /* ------------------------------------------------------------\n153:            RELEASE THE STORAGE.\n154:            ------------------------------------------------------------*/\n155:         PStatFree(&stat);\n156:         Destroy_LU(n, &grid1, &LUstruct);\n157:         dScalePermstructFree(&ScalePermstruct);\n158:         dSOLVEstructFree(&SOLVEstruct);\n159:     }\n160: \n161:     if ( iam >= 6 && iam < 10 ) { /* I am in grid 2. */\n162: \tiam = grid2.iam;  /* Get the logical number in the new grid. */\n163: \n164:         /* ------------------------------------------------------------\n165:            GET THE MATRIX FROM FILE AND SETUP THE RIGHT HAND SIDE. \n166:            ------------------------------------------------------------*/\n167:         dcreate_matrix_postfix(&A, nrhs, &b, &ldb, &xtrue, &ldx, fp, postfix, &grid2);\n168: \n169: \tif ( !(berr = doubleMalloc_dist(nrhs)) )\n170: \t    ABORT(\"Malloc fails for berr[].\");\n171: \n172: \t/* ------------------------------------------------------------\n173: \t   NOW WE SOLVE THE LINEAR SYSTEM.\n174: \t   ------------------------------------------------------------*/\n175: \n176:         /* Set the default input options:\n177:             options.Fact = DOFACT;\n178:             options.Equil = YES;\n179:             options.ColPerm = METIS_AT_PLUS_A;\n180:             options.RowPerm = LargeDiag_MC64;\n181:             options.ReplaceTinyPivot = NO;\n182:             options.Trans = NOTRANS;\n183:             options.IterRefine = DOUBLE;\n184:             options.SolveInitialized = NO;\n185:             options.RefineInitialized = NO;\n186:             options.PrintStat = YES;\n187:          */\n188: \tset_default_options_dist(&options);\n189: \n190:         if (!iam) {\n191:     \t    print_options_dist(&options);\n192:         }\n193: \n1", "label": "(82, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &iam );); (257, MPI, MPI_Finalize();); (24, MPI, MPI_Init_thread( &argc, &argv, MPI_THREAD_MULTIPLE, &omp_mpi_level);); (25, MPI, MPI_Comm_size( MPI_COMM_WORLD, &nprocs );)"}
{"predict": "+ wm_val] = 1;\n104:                    //printf(\"RD1 (%d,%d,%d) msg arrived in col %d, i=%d\\n\",mype,bid,tid,d_colnummod[tid],i);\n105:                }\n106:            }\n107:        } else {\n108:            int delta = d_nfrecvmod[1] % WAIT_NUM_THREADS;\n109:            if (tid < delta) {\n110:                d_mynummod[tid] = d_nfrecvmod[1] / WAIT_NUM_THREADS + 1;\n111:            } else {\n112:                d_mynummod[tid] = d_nfrecvmod[1] / WAIT_NUM_THREADS;\n113:            }\n114:            __syncthreads();\n115:            d_mymaskstartmod[tid] = 0;\n116:            for (int i = 0; i < tid; i++) {\n117:                d_mymaskstartmod[tid] += d_mynummod[i];\n118:            }\n119:            d_mymasklengthmod[tid] = d_colnummod[d_mymaskstartmod[tid] + d_mynummod[tid] - 1] - d_colnummod[d_mymaskstartmod[tid]] + 1;\n120:            __syncthreads();\n121:            //printf(\"RD2 (%d,%d) mynum=%d, start=%d,%d length=%d\\n\",mype,tid,d_mynummod[tid],d_mymaskstartmod[tid],d_colnummod[d_mymaskstartmod[tid]],d_mymasklengthmod[tid]);\n122: \n123:            for (int i = 0; i < d_mynummod[tid]; i++) {\n124:                int wm_val = nvshmem_uint64_wait_until_any(flag_rd_q + d_colnummod[d_mymaskstartmod[tid]], d_mymasklengthmod[tid],\n125:                                                        d_statusmod + d_mymaskstartmod[tid], NVSHMEM_CMP_EQ, 1);\n126:                d_statusmod[d_mymaskstartmod[tid] + wm_val] = 1;\n127:                //printf(\"RD2 (%d", "label": "(343, CUDA, __shared__ int cur_send_num;); (341, CUDA, __shared__ int recv_num, finish_num;)"}
{"predict": "(10,OpenMP,#pragma omp parallel for private(sdata) reduction(+:sdata))", "label": "(4, CUDA, __shared__ float sdata[THREAD_PER_BLOCK];)"}
{"predict": ": \n138:     /* close files */\n139:     mfu_close(src_name, fd);\n140:     mfu_close(dst_name, fd_out);\n141: \n142:     return rc;\n143: }", "label": "(10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (76, MPI, MPI_Bcast(&footer_flag, 1, MPI_INT, 0, MPI_COMM_WORLD);); (76, MPI, MPI_Bcast(&footer, 8, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(1,OpenMP,#pragma omp parallel for private(row,tid,local_max,local_norm) shared(xd,resd,M,N,smem))", "label": "(2, CUDA, __shared__ float smem[1024];)"}
{"predict": "(48,OpenMP,#pragma omp parallel for private(pos, do_output, bin_pos, bin_index) reduction(+:bin_counts, bin_counts_optional)), (...)", "label": "(38, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(30,OpenMP,#pragma omp parallel for reduction(+:sum))", "label": "(29, CUDA, checkCudaErrors(cudaSetDevice(gpuID));)"}
{"predict": "6:         if (ptr[0] != 2) {\n147:             cout << \"Error: buffer[0] != 2\" << endl;\n148:         }\n149:         if (err) {\n150:             cout << \"Error unmapping buffer\" << endl;\n151:             abort();\n152:         }\n153:     }\n154: \n155:     return retVal;\n156: }", "label": "(69, OpenCL, program = clCreateProgramWithSource(context, 1, &strings, 0, &err);); (129, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(cl_mem), &buffer);); (190, OpenCL, clReleaseKernel(kernel);); (191, OpenCL, clReleaseCommandQueue(queue);); (189, OpenCL, clReleaseProgram(program);); (139, OpenCL, err = clEnqueueNDRangeKernel(queue, kernel, dimension, offset, gws, lws, 0, 0, nullptr);); (189, OpenCL, clReleaseContext(context);); (101, OpenCL, buffer = clCreateBuffer(context, flags, bufferSize, nullptr, &err);); (115, OpenCL, err = clEnqueueUnmapMemObject(queue, buffer, ptr, 0, nullptr, nullptr);); (106, OpenCL, void *ptr = clEnqueueMapBuffer(queue, buffer, CL_TRUE, CL_MAP_WRITE, 0, bufferSize, 0, nullptr, nullptr, &err);); (38, OpenCL, queue = clCreateCommandQueue(context, device_id, 0, &err);); (140, OpenCL, void *ptr = clEnqueueMapBuffer(queue, buffer, CL_TRUE, CL_MAP_READ, 0, bufferSize, 0, nullptr, nullptr, &err);); (64, OpenCL, err = clBuildProgram(program, 1, &device_id, nullptr, nullptr, nullptr);); (178, OpenCL, clReleaseMemObject(buffer);); (28, OpenCL, context = clCreateContext(0, 1, &device_id, NULL, NULL, &err);); (84, OpenCL, kernel = clCreateKernel(program, \"hello\", &err);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(6, MPI, MPI_Comm_size(comm, &nproc);); (4, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(27, TBB, tbb::tick_count t1 = tbb::tick_count::now();); (23, TBB, reducePiExample<tbb::auto_partitioner>(num_intervals, gs);); (6, TBB, c.set_max_concurrency(std::min(8, tbb::info::default_concurrency(c)));); (29, TBB, double v3 = deterministicReducePiExample<tbb::simple_partitioner>(num_intervals, gs);); (27, TBB, tbb::tick_count t3 = tbb::tick_count::now();); (-1, TBB, tbb::task_arena::constraints c;); (11, TBB, tbb::tick_count ts_1 = tbb::tick_count::now();); (18, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (27, TBB, tbb::tick_count t5 = tbb::tick_count::now();); (25, TBB, double v4 = deterministicReducePiExample<tbb::static_partitioner>(num_intervals, gs);); (5, TBB, tbb::tick_count ts_0 = tbb::tick_count::now();); (-2, TBB, tbb::task_arena a(c);); (-6, TBB, c.set_core_type(tbb::info::core_types().back());); (20, TBB, tbb::tick_count t4 = tbb::tick_count::now();); (14, TBB, double v1 = reducePiExample<tbb::simple_partitioner>(num_intervals, gs);); (15, TBB, double v2 = reducePiExample<tbb::static_partitioner>(num_intervals, gs);); (-11, TBB, c.set_numa_id(tbb::info::numa_nodes()[0]);); (12, TBB, tbb::tick_count t2 = tbb::tick_count::now();); (8, TBB, double v0 = reducePiExample<tbb::auto_partitioner>(num_intervals, gs);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for), (43,OpenMP,#pragma omp parallel for), (51,OpenMP,#pragma omp parallel for), (59,OpenMP,#pragma omp parallel for), (67,OpenMP,#pragma omp parallel for)", "label": "(18, CUDA, cuda_safe_call(cudaStreamSynchronize(stream));); (7, CUDA, cuda_safe_call(cudaMalloc(&d_ptrB, N * sizeof(double)));); (9, CUDA, cuda_safe_call(cudaStreamCreate(&stream));); (4, CUDA, cuda_safe_call(cudaMalloc(&d_ptrA, N * sizeof(double)));)"}
{"predict": "(37,OpenMP,#pragma omp parallel for collapse(2) private(a, b, Csub) reduction(+:Csub))", "label": "(13, CUDA, __shared__ alignas(alignof(float4)) float Bs[BLOCK_SIZE][BLOCK_SIZE];); (8, CUDA, __shared__ alignas(alignof(float4)) float As[BLOCK_SIZE][BLOCK_SIZE];)"}
{"predict": "/* Set Output Data Rate / Power mode */\n124:   lsm6dso_xl_data_rate_set(&ag_ctx, LSM6DSO_XL_ODR_12Hz5);\n125:   lsm6dso_gy_data_rate_set(&ag_ctx, LSM6DSO_GY_ODR_12Hz5);\n126:   /* Enable accelerometer digital LPF2. */\n127:   lsm6dso_xl_filter_a_lp2_bandwidth_set(&ag_ctx, LSM6DSO_LP2_ODR_DIV_100);\n128:   /* Set FIFO watermark interrupt. */\n129:   lsm6dso_fifo_watermark_flag_get(&ag_ctx, &wtm_flag);\n130:   while (1) {\n131:     /* Read FIFO watermark flag. */\n132:     lsm6dso_fifo_watermark_flag_get(&ag_ctx, &wtm_flag);\n133:     if (wtm_flag) {\n134:       /* Read number of word to read in FIFO. */\n135:       lsm6dso_fifo_data_level_get(&ag_ctx, &numb);\n136:       /* Read FIFO batch data. */\n137:       for (i = 0; i < numb / 3; i++) {\n138:         lsm6dso_fifo_out_tag_get(&ag_ctx, &tag);\n139:         switch (tag) {\n140:         case LSM6DSO_XL_NC_TAG:\n141:           memset(data_raw_acceleration.u8bit, 0x00, 3 * sizeof(uint8_t));\n142:           lsm6dso_fifo_out_raw_get(&ag_ctx, data_raw_acceleration.u8bit);\n143:           break;\n144:         case LSM6DSO_GYRO_NC_TAG:\n145:           memset(data_raw_angular_rate.u8bit, 0x00, 3 * sizeof(uint8_t));\n146:           lsm6dso_fifo_out_raw_get(&ag_ctx, data_raw_angular_rate.u8bit);\n147:           break;\n148:         case LSM6DSO_SENSORHUB_SLAVE0_TAG:\n149:           memset(data_raw_magnetic.u8bit, 0x", "label": "(78, OpenMP, lis2mdl_offset_temp_comp_set(&mag_ctx, PROPERTY_ENABLE);)"}
{"predict": "(5,OpenMP,#pragma omp parallel for)", "label": "(2, OpenMP, const int num_devices = omp_get_num_devices();); (6, OpenMP, #pragma omp target map(from : num) device(i)); (6, OpenMP, num = omp_get_device_num();)"}
{"predict": "(18,OpenMP,#pragma omp parallel for)", "label": "(55, OpenCL, clReleaseEvent(clEvent);)"}
{"predict": "(9,OpenMP,#pragma omp parallel for), (39,OpenMP,#pragma omp parallel for collapse(2)), (50,OpenMP,#pragma omp parallel for collapse(2))", "label": "(47, CUDA, cudaDeviceCanAccessPeer(&access, dev1, dev2);); (53, CUDA, cudaSetDevice(dev);); (57, CUDA, auto err = cudaDeviceEnablePeerAccess(odev, 0);); (14, CUDA, auto err = cudaGetDeviceProperties(&props, dev);); (9, CUDA, INFO(\"failed to cudaSetDevice(%d)\\n\", dev);); (-2, CUDA, cudaGetDeviceCount(reinterpret_cast<int *>(&device));)"}
{"predict": "(3,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for private(v, new_g_v, new_f_v, push))", "label": "(9, TBB, oneapi::tbb::spin_mutex::scoped_lock l(locks[u]);); (20, TBB, oneapi::tbb::spin_mutex::scoped_lock l(locks[v]);)"}
{"predict": "(11,OpenMP,#pragma omp single nowait), (12,OpenMP,#pragma omp single nowait), (15,MPI,MPI_Bcast(&nccl_id, sizeof(nccl_id), MPI_BYTE, 0, MPI_COMM_WORLD);)", "label": "(6, MPI, MPI_CHECK(MPI_Init(NULL, NULL));); (15, MPI, MPI_CHECK(MPI_Bcast((void *)&nccl_id, sizeof(nccl_id), MPI_BYTE, 0, MPI_COMM_WORLD));); (16, CUDA, cudaSetDevice(rank);)"}
{"predict": "]*_w2[j]) + x;\n134: \n135:             gv += k[j] * ((u1+_u2[j]) * x + s);\n136:         }\n137:         _gv[t] = F(gv);\n138:     }\n139: }", "label": "(22, CUDA, __shared__ float v[_N_], r[_N_], k[_N_], gy[_N_];)"}
{"predict": "(82,OpenMP,#pragma omp parallel for) (85,OpenMP,#pragma omp parallel for) (89,OpenMP,#pragma omp parallel for) (94,OpenMP,#pragma omp parallel for) (99,OpenMP,#pragma omp parallel for reduction(+:rc))", "label": "(93, OpenMP, #pragma omp target teams distribute parallel for); (86, OpenMP, #pragma omp target parallel for)"}
{"predict": "8:         MPI_Win_unlock(trank, win);\n109:     }\n110: \n111:     MPI_Win_free(&win);\n112:     free(rmabuf);\n113:     free(localbuf);\n114:     free(localbuf2);\n115:     free(vals);\n116:     MTest_Finalize(toterrs);\n117:     return 0;\n118: }", "label": "(12, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (208, MPI, MPI_Win_free(&win);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (46, MPI, MPI_Put(&localbuf[i], 1, MPI_INT, trank, rank + i * wsize, 1, MPI_INT, win);); (44, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, trank, 0, win);); (45, MPI, MPI_Put(&localbuf[i], 1, MPI_INT, trank, rank + (i + NELM) * wsize, 1, MPI_INT, win);); (45, MPI, MPI_Win_unlock(trank, win);); (127, MPI, MPI_Get(&vals[i], 1, MPI_INT, trank, i, 1, MPI_INT, win);); (183, MPI, MPI_Accumulate(&localbuf[i], 1, MPI_INT, trank, rank + i * wsize, 1, MPI_INT, MPI_SUM, win);); (37, MPI, MPI_Barrier(MPI_COMM_WORLD);); (6, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(14, CUDA, thrust::device_vector<Element> d_src = h_src;); (8, CUDA, thrust::host_vector<Element> h_src(kNumel);)"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for)", "label": "(51, CUDA, cudaFree(res_dev);); (44, CUDA, cudaMalloc(&res_dev, sizeof(float));); (46, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "4: \t\t\t\tt1 = SuperLU_timer_();\n115: #endif\n116: \n117: \n118: #ifdef _OPENMP\n119: #pragma omp parallel for private(j, jj, temp)\n120: #endif\n121: \n122: \t\t\t\tfor (j = 0; j < iknsupc; ++j) {\n123: \t\t\t\t\ttemp = uval[i+j];\n124: \t\t\t\t\tfor (jj = 0; jj < knsupc; ++jj) {\n125: \t\t\t\t\t\trtemp_loc[j+jj*iknsupc] = temp * xk[jj];\n126: \t\t\t\t\t}\n127: \t\t\t\t}\n128: \n129: \n130: #ifdef _OPENMP\n131: #pragma omp parallel for private(j, jj)\n132: #endif\n133: \n134: \t\t\t\tfor (j = 0; j < iknsupc; ++j) {\n135: \t\t\t\t\tfor (jj = 0; jj < knsupc; ++jj) {\n136: \t\t\t\t\t\tlsum[il+j+jj*iknsupc] -= rtemp_loc[j+jj*iknsupc];\n137: \t\t\t\t\t}\n138: \t\t\t\t}\n139: \n140: #if ( PROFlevel>=1 )\n141: \t\t\t\tt2 = SuperLU_timer_();\n142: #endif\n143: \n144: \n145: \n146: \t\t\t}\n147: \t\t}\n148: \t}else{\n149: \n150: \t\tfor (ub = 0; ub < nub; ++ub){\n151: \n152: \t\t\tik = Ucb_indptr[lk][ub].lbnum; /* Local block number, row-wise. */\n153: \t\t\tusub = Llu->Ufstnz_br_ptr[ik];\n154: \t\t\tuval = Llu->Unzval_br_ptr[ik];\n155: \t\t\ti = Ucb_indptr[lk][ub].indpos; /* Start of the block in usub[]. */\n156: \t\t\ti += UB_DESCRIPTOR;\n157: \t\t\til =", "label": "(82, OpenMP, #pragma\tomp\ttaskloop firstprivate (stat) private (thread_id1,Uinv,nn,lbstart,lbend,ub,temp,rtemp_loc,ik,lk1,gik,gikcol,usub,uval,lsub,lusup,iknsupc,il,i,irow,bmod_tmp,p,ii,jj,t1,t2,j,ikfrow,iklrow,dest,y,uptr,fnz,nsupr,nroot_send_tmp) untied nogroup); (126, OpenMP, #pragma omp simd); (146, OpenMP, #pragma omp atomic capture); (75, OpenMP, #pragma\tomp\tparallel for private (thread_id1,Uinv,nn,lbstart,lbend,ub,temp,rtemp_loc,ik,lk1,gik,gikcol,usub,uval,lsub,lusup,iknsupc,il,i,irow,bmod_tmp,p,ii,jj,t1,t2,j,ikfrow,iklrow,dest,y,uptr,fnz,nsupr))"}
{"predict": "(33,OpenMP,#pragma omp parallel for), (48,OpenMP,#pragma omp parallel for), (65,OpenMP,#pragma omp parallel for)", "label": "(48, CUDA, CubDebugExit(cudaMemcpy(d_flags, h_flags, sizeof(unsigned char) * num_items, cudaMemcpyHostToDevice));); (46, CUDA, CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));)"}
{"predict": "(24,OpenMP,#pragma omp parallel for private(pos, ddd))", "label": "(6, CUDA, __shared__ uint s_key[SHARED_SIZE_LIMIT];); (6, CUDA, __shared__ uint s_val[SHARED_SIZE_LIMIT];)"}
{"predict": "Code[i]);\n79: \tif (OLvL>0) {\n80: \t\tfor (i = 0; i < nCode; ++i) {\n81: \t\t\tpScr = mem_alloc(strlen(pCode[i])+1);\n82: \t\t\tstrcpy(pScr, pCode[i]);\n83: \t\t\tpCode[i] = pScr;\n84: \t\t}\n85: \t}\n86: \tlen = 0;\n87: \tfor (i = 0; i < nCode; ++i)\n88: \t\tlen += strlen(pCode[i]);\n89: \tpScr = mem_alloc(len + 1);\n90: \tpScr[0] = 0;\n91: \tfor (i = 0; i < nCode; ++i)\n92: \t\tstrcat(pScr, pCode[i]);\n93: \tif (OLvL>0) {\n94: \t\tfor (i = 0; i < nCode; ++i)\n95: \t\t\tfree(pCode[i]);\n96: \t}\n97: \tcomp_add_script_line(pScr);\n98: \tfree(pScr);\n99: \treturn 0;\n100: }\n\n### **Analysis**\nThe code provided does not contain any obvious parallelizable loops or sections that could benefit from parallelization without further context. However, there are a few sections that could potentially be parallelized if they were part of a larger program or if additional context was provided.\n\nOne such section is the loop starting at line 77 that prints debug information. This loop could be parallelized using OpenMP if the order of output does not matter.\n\nAnother section is the loop starting at line 80 that allocates and copies strings. This loop could also be parallelized using OpenMP.\n\nThe final loop starting at line 94 that frees memory could be parallelized using OpenMP as well.\n\nGiven the provided code, the following parallelization instructions could be inserted:\n\n(77,OpenMP,#pragma omp parallel for)\n(80,OpenMP,#pragma omp parallel for)\n(94,OpenMP,#pragma omp parallel for)\n\n### **Output**\n(77,OpenMP,#pragma omp parallel for); (80,OpenMP,#pragma omp parallel for); (94,OpenMP,#pragma omp parallel for)", "label": "(31, OpenMP, comp_add_script_line(\"Flag=MGF_SALT_AS_HEX_%s\\n\", tmp);); (19, OpenMP, comp_do_parse(0, nSyms-1);); (222, OpenMP, comp_add_script_line(\"Func=DynamicFunc__append_keys_pad20\\n\"); if (use_inp1) len_comp += 20; else len_comp2 += 20; }); (58, OpenMP, if (bNeedS) comp_add_script_line(\"Flag=MGF_SALTED\\n\");); (153, OpenMP, comp_add_script_line(\"Func=DynamicFunc__LargeHash_OUTMode_raw\\n\");); (18, OpenMP, comp_add_script_line(\"Expression=dynamic=%s\\n\", p->pExpr);); (306, OpenMP, comp_add_script_line(\"Flag=MGF_INPBASE64b\\n\");); (307, OpenMP, comp_add_script_line(\"Flag=MGF_INPBASE64m\\n\");); (186, OpenMP,"}
{"predict": "(27,OpenMP,#pragma omp parallel sections), (28,OpenMP,#pragma omp section), (29,OpenMP,#pragma omp section), (30,OpenMP,#pragma omp section)", "label": "(38, MPI, MPI_Op_free(&op);); (16, MPI, MPI_Type_free(&dt_vector);); (35, MPI, MPI_Type_free(&dt_noncontig);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (13, MPI, MPI_Type_commit(&dt_noncontig);); (9, MPI, MPI_Type_vector(1, 1, noncontig_stride, MPI_INT, &dt_vector);); (9, MPI, MPI_Type_create_resized(dt_vector, 0, sizeof(int) * noncontig_stride, &dt_noncontig);)"}
{"predict": "(2,OpenMP,#pragma omp parallel for collapse(2))", "label": "(6, CUDA, __shared__ int s_data[32][33];)"}
{"predict": "(65,OpenMP,#pragma omp parallel for collapse(2))", "label": "(13, CUDA, CUDA_CHECK(cudaGetDeviceProperties(&props, 0));)"}
{"predict": "The provided code snippet does not contain any loops or sections that are inherently parallelizable. It appears to be a test function for CUDA memory detection and does not involve any operations that can be parallelized using OpenMP, MPI, or other parallelization techniques. Therefore, there are no appropriate places to insert parallelization instructions in this specific code snippet. \n\nIf you have a different code snippet or a section of code that involves loops or parallelizable operations, please provide that, and I can analyze it for you.", "label": "(6, CUDA, st = cudaMalloc(&test_ptr, TEST_ALLOC_SIZE);); (11, CUDA, cudaFree(test_ptr);)"}
{"predict": "(20,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for)", "label": "(19, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (25, TBB, serial_tiled_time = (tbb::tick_count::now() - t0).seconds();); (30, TBB, tbb_time = (tbb::tick_count::now() - t0).seconds();); (18, TBB, serial_time = (tbb::tick_count::now() - t0).seconds();)"}
{"predict": "(44,OpenMP,#pragma omp parallel for)", "label": "(52, CUDA, flamegpu::CUDASimulation cudaSimulation(model);)"}
{"predict": "(11,OpenMP,#pragma omp parallel sections)", "label": "(8, OpenMP, #pragma omp section); (6, OpenMP, #pragma omp parallel)"}
{"predict": "(45,OpenMP,#pragma omp parallel for), (74,OpenMP,#pragma omp parallel for private(flag, lowest, highest)), (84,MPI,MPI_Reduce(counts, all_counts, 2, MPI_UINT64_T, MPI_SUM, 0, comm);)", "label": "(25, MPI, MPI_Type_get_true_extent(key, &key_true_lb, &key_true_extent);); (15, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (83, MPI, MPI_Allreduce(counts, all_counts, 2, MPI_UINT64_T, MPI_SUM, comm);); (18, MPI, MPI_Type_get_extent(keysat, &keysat_lb, &keysat_extent);)"}
{"predict": "(6,OpenMP,#pragma omp parallel for private(last))", "label": "(24, CUDA, checkCudaErrors(cudaMalloc((void **)&bLines_d, N_LINES * sizeof(BezierLine)));); (35, CUDA, checkCudaErrors(cudaFree(bLines_d));)"}
{"predict": "(17,OpenMP,#pragma omp parallel for reduction(+:thread_sum,thread_sum2)), (51,OpenMP,#pragma omp parallel for)", "label": "(7, CUDA, __shared__ float shared_sum2[32];); (5, CUDA, __shared__ float shared_sum[32];)"}
{"predict": "(3,OpenMP,#pragma omp parallel for)", "label": "(2, OpenMP, #pragma omp parallel for schedule(static))"}
{"predict": "i <= 10; i++)\n110:     c[i] = a[i];\n111: \n112: // Ok.\n113: #pragma omp parallel for simd\n114:   for (int i = 0; i >= 0; i--)\n115:     c[i] = a[i];\n116: \n117: // Ok.\n118: #pragma omp parallel for simd\n119:   for (int i = 0; i < 10; i += 2)\n120:     c[i] = a[i];\n121: \n122: // Ok.\n123: #pragma omp parallel for simd\n124:   for (int i = 0; i < 10; i -= 2)\n125:     c[i] = a[i];\n126: \n127: // Ok.\n128: #pragma omp parallel for simd\n129:   for (int i = 0; i < 10; i += 1)\n130:     c[i] = a[i];\n131: \n132: // Ok.\n133: #pragma omp parallel for simd\n134:   for (int i = 0; i < 10; i -= 1)\n135:     c[i] = a[i];\n136: \n137: // Ok.\n138: #pragma omp parallel for simd\n139:   for (int i = 0; i < 10; i = i + 1)\n140:     c[i] = a[i];\n141: \n142: // Ok.\n143: #pragma omp parallel for simd\n144:   for (int i = 0; i < 10; i = i - 1)\n145:     c[i] = a[i];\n146: \n147: // Ok.\n148: #pragma omp parallel for simd\n149:   for (int i = 0; i < 10; i += '\\1')\n150:     c[i] = a[i];\n151: \n152: // Ok.\n153: #pragma omp parallel", "label": "(235, OpenMP, #pragma omp parallel for simd private(ii)); (5, OpenMP, #pragma omp parallel for simd); (257, OpenMP, #pragma omp parallel for simd collapse(2)); (226, OpenMP, #pragma omp parallel for simd linear(ii)); (237, OpenMP, #pragma omp parallel for simd lastprivate(ii)); (220, OpenMP, #pragma omp parallel for simd firstprivate(ii))"}
{"predict": "(14,OpenMP,#pragma omp parallel for)", "label": "(12, TBB, tbb::parallel_sort(iter, iter + n, *compare );)"}
{"predict": "(5,OpenMP,#include <omp.h>); (9,OpenMP,#pragma omp parallel); (15,OpenMP,#pragma omp parallel);", "label": "(10, TBB, double serial_time = (tbb::tick_count::now() - t0).seconds();); (7, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (16, TBB, double tbb_time = (tbb::tick_count::now() - t1).seconds();); (13, TBB, tbb::tick_count t1 = tbb::tick_count::now();)"}
{"predict": "(19,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for)", "label": "(24, CUDA, CHECK(cudaMalloc(&d_B, M));); (24, CUDA, CHECK(cudaMemcpy(d_A, h_A, M, cudaMemcpyHostToDevice));); (42, CUDA, CHECK(cudaFree(d_B));); (20, CUDA, CHECK(cudaMalloc(&d_A, M));); (28, CUDA, CHECK(cudaMemcpy(h_B, d_B, M, cudaMemcpyDeviceToHost));); (38, CUDA, CHECK(cudaFree(d_A));)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(4, CUDA, HANDLE_ERROR(cudaStreamCreate(&stream));); (15, CUDA, HANDLE_ERROR(cudaStreamDestroy(stream));)"}
{"predict": "(9,OpenMP,#pragma omp parallel for reduction(+:sum))", "label": "(12, CUDA, __shared__ uint data[MERGE_THREADBLOCK_SIZE];)"}
{"predict": "(8,OpenMP,#pragma omp parallel)", "label": "(2, OpenMP, #pragma omp critical)"}
{"predict": "(4,OpenMP,#pragma omp parallel sections), (7,OpenMP,#pragma omp section), (10,OpenMP,#pragma omp section), (13,OpenMP,#pragma omp section), (16,OpenMP,#pragma omp section)", "label": "(38, OpenCL, retVal = clReleaseMemObject(clBuffer);)"}
{"predict": "(27,OpenMP,#pragma omp parallel for), (36,OpenMP,#pragma omp parallel for), (50,OpenMP,#pragma omp parallel for)", "label": "(26, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&data, size_bytes));); (19, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamCreate(&stream));); (48, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (46, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (53, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(data));); (53, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamDestroy(stream));)"}
{"predict": "(6,OpenMP,#pragma omp parallel for), (7,OpenMP,#pragma omp parallel for)", "label": "(15, OpenMP, #pragma omp target map(from:px)); (7, OpenMP, #pragma omp target data map(to:x[0:count])); (21, OpenMP, #pragma omp target data use_device_ptr(x)); (26, OpenMP, #pragma omp target data use_device_addr(x[0:0]))"}
{"predict": "(54,OpenMP,#pragma omp parallel for), (84,OpenMP,#pragma omp parallel for)", "label": "(59, CUDA, CHECKED_CALL(cudaEventRecord(start, 0));); (52, CUDA, CHECKED_CALL(cudaDeviceSetLimit(cudaLimitStackSize, 8192));); (62, CUDA, CHECKED_CALL(cudaGetLastError());); (41, CUDA, CHECKED_CALL(cudaEventCreate(&start));); (36, CUDA, CHECKED_CALL(cudaSetDevice(devID));); (88, CUDA, CHECKED_CALL(cudaEventDestroy(start));); (89, CUDA, CHECKED_CALL(cudaFree(d_result));); (61, CUDA, CHECKED_CALL(cudaDeviceSynchronize());); (6, CUDA, cudaGetDeviceProperties(&deviceProp, devID);); (3, CUDA, int devID = findCudaDevice(argc, (const char **)argv);); (78, CUDA, CHECKED_CALL(cudaEventElapsedTime(&time, start, stop));); (56, CUDA, CHECKED_CALL(cudaEventRecord(stop, 0));); (84, CUDA, CHECKED_CALL(cudaFree(d_nresults));); (30, CUDA, CHECKED_CALL(cudaMalloc((void **)&d_nresults, THREADS * sizeof(*d_nresults)));); (80, CUDA, CHECKED_CALL(cudaEventDestroy(stop));); (30, CUDA, CHECKED_CALL(cudaEventCreate(&stop));)"}
{"predict": "(7,OpenMP,#pragma omp parallel for private(lower, upper, x, y))", "label": "(4, OpenACC, #pragma acc enter data create(in1[:w*h], in2[:h*w], out[:w*h])); (12, OpenACC, #pragma acc loop); (8, OpenACC, #pragma acc update device(in1[lower*w:(upper-lower)*w],in2[lower*w:(upper-lower)*w]) async(block%2)); (17, OpenACC, #pragma acc wait); (7, OpenACC, #pragma acc parallel loop present(in1,in2, out) async(block%2)); (13, OpenACC, #pragma acc update self(out[lower*w:(upper-lower)*w]) async(block%2)); (15, OpenACC, #pragma acc exit data delete(in1, in2, out))"}
{"predict": "(13,OpenMP,#pragma omp parallel for)", "label": "(5, CUDA, safeCall(cudaGetSymbolAddress((void**)&d_PointCounterAddr, d_PointCounter));); (57, CUDA, safeCall(cudaMemcpy(&totPts, &d_PointCounterAddr[2*octave+1], sizeof(int), cudaMemcpyDeviceToHost));); (5, CUDA, safeCall(cudaMemcpy(&fstPts, &d_PointCounterAddr[2*octave-1], sizeof(int), cudaMemcpyDeviceToHost));); (50, CUDA, safeCall(cudaDestroyTextureObject(texObj));); (32, CUDA, cudaCreateTextureObject(&texObj, &resDesc, &texDesc, NULL);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for private(j)), (28,OpenMP,#pragma omp parallel for), (39,OpenMP,#pragma omp parallel for), (51,OpenMP,#pragma omp parallel for), (73,OpenMP,#pragma omp parallel for)", "label": "(4, TBB, tbb::flow::queue_node<T> q2(g);); (51, TBB, tbb::flow::make_edge( q2, q3 );); (3, TBB, tbb::flow::queue_node<T> q3(g);); (48, TBB, tbb::flow::make_edge( q, q2 );); (-2, TBB, tbb::flow::graph g;); (68, TBB, tbb::flow::queue_node<T> q_copy(q);); (-3, TBB, tbb::flow::queue_node<T> q(g);)"}
{"predict": "(16,OpenMP,#pragma omp parallel for private(i))", "label": "(27, MPI, MPI_Win_set_attr(win, key[0], &attrval[0]);); (12, MPI, MPI_Win_create(buf, sizeof(int), sizeof(int), MPI_INFO_NULL, comm, &win);); (29, MPI, MPI_Win_delete_attr(win, key[0]);); (30, MPI, MPI_Win_delete_attr(win, key[2]);); (58, MPI, MPI_Win_free_keyval(&key[i]);); (20, MPI, MPI_Win_set_attr(win, key[2], &attrval[2]);); (58, MPI, MPI_Win_free(&win);); (19, MPI, MPI_Win_set_attr(win, key[1], &attrval[1]);); (24, MPI, MPI_Win_delete_attr(win, key[1]);)"}
{"predict": "136: \t\t\tfor (j = 0; j < SSE_GROUP_SZ_SHA1; ++j)\n137: \t\t\t\tlens[j] = saved_len[index+j];\n138: \t\t\tpkcs12_pbe_derive_key_simd_sha1(cur_salt->iteration_count,\n139: \t\t\t\t\tMBEDTLS_PKCS12_DERIVE_KEY,\n140: \t\t\t\t\tkeys, lens, cur_salt->salt,\n141: \t\t\t\t\tcur_salt->saltlen, ckey, 32);\n142: \n143: \t\t\tTwofish_prepare_key((unsigned char*)ckey[0], 32, &tkey);\n144: \t\t\tdatalen = Twofish_Decrypt(&tkey, cur_salt->store_data, store_data_decrypted, cur_salt->store_data_length, iv[0]);\n145: \t\t\tif (datalen < 0)\n146: \t\t\t\tcontinue;\n147: \t\t\tSHA1_Init(&ctx);\n148: \t\t\tSHA1_Update(&ctx, store_data_decrypted, datalen - 20);\n149: \t\t\tSHA1_Final(compute_checkum, &ctx);\n150: \n151: \t\t\tif (!memcmp(compute_checkum, store_data_decrypted + datalen - 20, 20))\n152: \t\t\t{\n153: \t\t\t\tcracked[index] = 1;\n154: #ifdef _OPENMP\n155: #pragma omp atomic\n156: #endif\n157: \t\t\t\tany_cracked |= 1;\n158: \t\t\t}\n159: \t\t}\n160: #endif\n161: \t}\n162: \n163: \t*pcount = count;\n164: \treturn count;\n165: }", "label": "(11, OpenMP, #pragma omp parallel for); (38, OpenMP, #pragma omp atomic)"}
{"predict": "(14,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for)", "label": "(19, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(78,MPI,MPI_Comm_spawn(argv[0], spawn_argv, 2, MPI_INFO_NULL, 0, MPI_COMM_WORLD, &icomm, spawn_err);)\n(108,OpenMP,#pragma omp parallel for)", "label": "(28, MPI, MPI_Init(&argc, &argv);); (82, MPI, MPI_Comm_disconnect(&icomm);); (76, MPI, MPI_Comm_spawn(argv[0], &spawn_argv[1], 1, spawn_info, 0, MPI_COMM_WORLD, &icomm, spawn_err);); (26, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (83, MPI, rc = MPI_Comm_set_errhandler(MPI_COMM_SELF, MPI_ERRORS_ARE_FATAL);); (43, MPI, MPI_Info_get(MPI_INFO_ENV, \"mpi_initial_errhandler\", MPI_MAX_INFO_VAL, init_errh_info, &flag);); (68, MPI, MPI_Info_set(spawn_info, \"mpi_initial_errhandler\", \"mpi_errors_return\");); (90, MPI, rc = MPI_Finalize();); (74, MPI, MPI_Barrier(icomm);); (80, MPI, MPI_Error_string(rc, estr, &slen);); (49, MPI, MPI_Comm_get_errhandler(MPI_COMM_SELF, &errh);); (62, MPI, MPI_Info_create(&spawn_info);); (93, MPI, rc = MPI_Error_string(MPI_ERR_WIN, estr, &slen);)"}
{"predict": "(5,OpenMP,#pragma omp parallel)", "label": "(3, OpenMP, #pragma omp target map(tofrom: threadCount)); (4, OpenMP, #pragma omp metadirective)"}
{"predict": "(5,OpenMP,#pragma omp parallel for)", "label": "(14, OpenMP, printf(\"Error: xomp_get_maxThreadBlocksPerMultiprocessor(): unhandled Compute Capability numbers%d.%d \\n\", major, minor);); (3, CUDA, major = xomp_getCudaDeviceProp(devID)-> major;); (3, CUDA, minor = xomp_getCudaDeviceProp(devID)-> minor;)"}
{"predict": "(7,OpenMP,#pragma omp parallel for private(comm, self_dup))", "label": "(26, MPI, MPI_Comm_dup(comms[*(int *) arg], &comm);); (4, MPI, MPI_Comm_rank(comms[*(int *) arg], &rank);); (20, MPI, MPI_Comm_free(&self_dup);); (24, MPI, MPI_Comm_free(&comm);); (13, MPI, MPI_Comm_dup(MPI_COMM_SELF, &self_dup);)"}
{"predict": "(22,OpenMP,#pragma omp parallel for),", "label": "(12, CUDA, cudaStreamCreate(&stream);); (37, CUDA, HANDLE_ERROR(cudaStreamSynchronize(stream));); (17, CUDA, HANDLE_ERROR(cudaMalloc((void**)&costs_dev_only_d, sizeof(float) * num_rollouts));); (15, CUDA, HANDLE_ERROR(cudaMalloc((void**)&baseline_and_normalizer_d, sizeof(float2)));); (16, CUDA, HANDLE_ERROR(cudaMalloc((void**)&costs_host_only_d, sizeof(float) * num_rollouts));)"}
{"predict": "(6,OpenMP,#pragma omp parallel for reduction(+:m_val,v_val))", "label": "(2, CUDA, __shared__ typename BlockReduce<float>::TempStorage v_storage;); (0, CUDA, __shared__ typename BlockReduce<float>::TempStorage m_storage;)"}
{"predict": "(39,OpenMP,#pragma omp parallel for)", "label": "(33, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (41, TBB, reduce_time = (tbb::tick_count::now() - t0).seconds();); (47, TBB, graph_time = (tbb::tick_count::now() - t0).seconds();); (32, TBB, serial_time = (tbb::tick_count::now() - t0).seconds();)"}
{"predict": "(9,OpenMP,#pragma omp parallel for reduction(+:errs))", "label": "(6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &world_size);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for)", "label": "(8, OpenMP, rec = get_decomp_record(code);)"}
{"predict": "(21,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for private(j)), (49,OpenMP,#pragma omp parallel for private(j))", "label": "(10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for reduction(+:sum,sum2)), (37,OpenMP,#pragma omp parallel for)", "label": "(6, CUDA, __shared__ float sdata[THREAD_PER_BLOCK];); (8, CUDA, __shared__ float var;); (5, CUDA, __shared__ float sdata2[THREAD_PER_BLOCK];); (5, CUDA, __shared__ float mean;)"}
{"predict": "task_group );\n63:     TestTypeDefinitionPresence( structured_task_group );\n64: #endif /* __TBB_TASK_GROUP_CONTEXT */\n65:     return 0;\n66: }", "label": "(103, TBB, TestFuncDefinitionPresence( parallel_deterministic_reduce, (const tbb::blocked_range<int>&, Body2&, const tbb::simple_partitioner&, tbb::task_group_context&), void );); (32, TBB, TestTypeDefinitionPresence3(flow::async_node<int, int, tbb::flow::queueing_lightweight> );); (96, TBB, TestFuncDefinitionPresence( parallel_for_each, (const intarray&, const Body1&, tbb::task_group_context&), void );); (93, TBB, TestFuncDefinitionPresence( parallel_invoke, (const Body&, const Body&, tbb::task_group_context&), void );); (93, TBB, TestFuncDefinitionPresence( parallel_do, (const intarray&, const Body1a&, tbb::task_group_context&), void );); (27, TBB, TestTypeDefinitionPresence3(flow::multifunction_node<int, intpair, tbb::flow::queueing> );); (30, TBB, TestTypeDefinitionPresence2(flow::join_node<intpair, tbb::flow::key_matching<int> > );); (82, TBB, TestFuncDefinitionPresence( parallel_scan, (const tbb::blocked_range<int>&, const int&, const Body3a&, const Body1b&), int );); (75, TBB, TestFuncDefinitionPresence( parallel_for, (const tbb::blocked_range<int>&, const Body2&, const tbb::simple_partitioner&), void );); (79, TBB, TestFuncDefinitionPresence( parallel_scan, (const tbb::blocked_range2d<int>&, Body3&, const tbb::auto_partitioner&), void );); (74, TBB, TestFuncDefinitionPresence( parallel_reduce, (const tbb::blocked_range<int>&, const int&, const Body2a&, const Body1b&), int );); (23, TBB, TestTypeDefinitionPresence2(flow::continue_node<int, tbb::flow::lightweight> );); (73, TBB, TestFuncDefinitionPresence( parallel_reduce, (const tbb::blocked_range<int>&, Body2&, tbb::affinity_partitioner&), void );); (74, TBB, TestFuncDefinitionPresence( parallel_deterministic_reduce, (const tbb::blocked_range<int>&, Body2&, const tbb::static_partitioner&), void );); (14, TBB, typedef tbb::flow::tuple<int, int> intpair;); (33, TBB, TestTypeDefinitionPresence2(flow::composite_node<tbb::flow::tuple<int>, tbb::flow::tuple<int> > );); (15, TBB, TestTypeDefinitionPresence3(flow::function_node<int, int, tbb::flow::rejecting> );); (82, TBB, TestFuncDefinitionPresence( parallel_for, (int, int, const Body1&, const tbb::auto_partitioner&, tbb::task_group_context&), void );); (68, TBB, TestFuncDefinitionPresence( parallel_deterministic_reduce, (const tbb::blocked_range<int>&, const int&, const Body2a&, const Body1b&), int );); (75, TBB, TestFuncDefinitionPresence( parallel_pipeline, (size_t, const tbb::filter_t<void,void>&), void );); (81, TBB, TestFuncDefinitionPresence( parallel_reduce, (const tbb::blocked_range<int>&, Body2&, const tbb::auto_partitioner&, tbb::task_group_context&), void );); (5, TBB, TestFuncDefinitionPresence( flow::make_edge, (tbb::flow::sender<Msg>&, tbb::flow::receiver<Msg>&), void );); (82, TBB, TestFuncDefinitionPresence( parallel_deterministic_reduce, (const tbb::blocked_range<int>&, Body2&, tbb::task_group_context&), void );); (4, TBB, TestFuncDefinitionPresence( flow::remove_edge, (tbb::flow::sender<Msg>&, tbb::flow::receiver<Msg>&), void );); (76, TBB, TestFuncDefinitionPresence( parallel_for, (int, int, const Body1&, tbb::task_group_context&), void );); (77, TBB, TestFuncDefinitionPresence( parallel_reduce, (const tbb::blocked_range<int>&, Body2&, tbb::task_group_context&), void );); (9, TBB, TestTypeDefinitionPresence2(flow::join_node<intpair, tbb::flow::reserving> );)"}
{"predict": "131:   if (parse_hier)\n132:     SCHEDULE_SET_LAYER(sched, layer);\n133: #endif // KMP_USE_HIER_SCHED\n134: \n135:   return ptr;\n136: }", "label": "(30, OpenMP, __kmp_omp_schedule_restore();)"}
{"predict": "(19,OpenMP,#pragma omp parallel sections)", "label": "(12, MPI, MPI_Comm_rank(team.comm, &rank);); (12, MPI, MPI_Comm_size(team.comm, &nprocs);)"}
{"predict": "(8,OpenMP,#pragma omp parallel while)", "label": "(9, OpenCL, kernel_info->kernel = clCreateKernel(program, kernel_info->kernel_name, &err);)"}
{"predict": "(9,OpenMP,#pragma omp parallel for)", "label": "(6, OpenCL, cl_int *pNumCorrect = (cl_int*) clEnqueueMapBuffer(cmdq, num_correct, CL_TRUE, CL_MAP_READ | CL_MAP_WRITE, 0, sizeof(cl_int), 0, NULL,NULL, &error);); (10, OpenCL, error = clEnqueueUnmapMemObject(cmdq, num_correct, pNumCorrect, 0,NULL,NULL);); (15, OpenCL, pNumCorrect = (cl_int*) clEnqueueMapBuffer(cmdq, num_correct, CL_TRUE, CL_MAP_READ | CL_MAP_WRITE, 0, sizeof(cl_int), 0, NULL,NULL, &error);); (11, OpenCL, error = clEnqueueNDRangeKernel(cmdq, kernel_verify_lists, 1, NULL, &numLists, NULL, 0, NULL, NULL);)"}
{"predict": "(5,OpenMP,#pragma omp parallel sections), (6,OpenMP,#pragma omp section), (7,OpenMP,#pragma omp section), (9,OpenMP,#pragma omp section), (10,OpenMP,#pragma omp section), (11,OpenMP,#pragma omp section), (12,OpenMP,#pragma omp section), (13,OpenMP,#pragma omp section), (14,OpenMP,#pragma omp section), (15,OpenMP,#pragma omp section), (16,OpenMP,#pragma omp section), (17,OpenMP,#pragma omp section), (18,OpenMP,#pragma omp section)", "label": "(8, TBB, tbb::task_scheduler_init init_market_p_plus_one(MaxThread + 1);)"}
{"predict": "(8,MPI,MPI_Comm_size(comm, &comm_size);); (8,MPI,MPI_Comm_rank(comm, &comm_rank);); (56,OpenMP,#pragma omp parallel for if (comm_rank < comm_size - 1)), (56,OpenMP,comm_tests(comm);)", "label": "(7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &comm_size);); (69, MPI, MPI_Comm_split(MPI_COMM_WORLD, 0, rand(), &comm);); (57, MPI, MPI_Comm_split(MPI_COMM_WORLD, (comm_rank == comm_size - 1) ? 0 : 1, 0, &comm);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &comm_rank);); (58, MPI, MPI_Comm_free(&comm);); (7, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (30, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(15,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for)", "label": "(1, TBB, oneapi::tbb::flow::graph g;)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for)", "label": "(12, CUDA, checkCudaErrors(cudaMemcpy(dArray, array, size, cudaMemcpyHostToDevice));); (10, CUDA, checkCudaErrors(cudaMalloc(&dArray, size));); (19, CUDA, checkCudaErrors(cudaFree(dArray));); (17, CUDA, checkCudaErrors(cudaMemcpy(array, dArray, size, cudaMemcpyDeviceToHost));)"}
{"predict": "(26,OpenMP,#pragma omp parallel for) (32,OpenMP,#pragma omp parallel for) (40,OpenMP,#pragma omp parallel for)", "label": "(9, TBB, c.set_core_type(tbb::info::core_types().back());); (6, TBB, tbb::task_arena::constraints c;); (36, TBB, tbb::affinity_partitioner aff_p;); (5, TBB, c.set_numa_id(tbb::info::numa_nodes()[0]);); (26, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (46, TBB, double static_time = (tbb::tick_count::now() - t0).seconds();); (5, TBB, c.set_max_concurrency(std::min(8, tbb::info::default_concurrency(c)));); (36, TBB, double affinity_time = (tbb::tick_count::now() - t0).seconds();); (31, TBB, t0 = tbb::tick_count::now();); (25, TBB, double auto_time = (tbb::tick_count::now() - t0).seconds();); (2, TBB, tbb::task_arena arena(c);)"}
{"predict": "= (left - i2d[2]) / i2d[0];\n107:         float image_base_top  = (top  - i2d[5]) / i2d[3];\n108:         float image_base_right = (right - i2d[2]) / i2d[0];\n109:         float image_base_bottom = (bottom - i2d[5]) / i2d[3];\n110:         bboxes.push_back({image_base_left, image_base_top, image_base_right, image_base_bottom, label, confidence, theta});\n111:     }\n112: \n113:     // non-maximum suppression\n114:     std::vector<int> indices;\n115:     nms(bboxes, indices, nms_threshold);\n116: \n117:     // output the result\n118:     for(int i : indices){\n119:         auto bbox = bboxes[i];\n120:         printf(\"left: %f, top: %f, right: %f, bottom: %f, label: %d, confidence: %f, theta: %f\\n\", bbox[0], bbox[1], bbox[2], bbox[3], (int)bbox[4], bbox[5], bbox[6]);\n121:     }\n122: }\n123: \n124: void nms(std::vector<vector<float>>& bboxes, std::vector<int>& indices, float threshold){\n125:     for(int i = 0; i < bboxes.size(); ++i){\n126:         if(bboxes[i][5] < threshold)\n127:             continue;\n128:         indices.push_back(i);\n129:     }\n130:     for(int i = 0; i < indices.size(); ++i){\n131:         for(int j = i + 1; j < indices.size(); ++j){\n132:             float iou = bbox_iou(bboxes[indices[i]], bboxes[indices[j]]);\n133:             if(iou > threshold){\n134:                 if(bboxes[indices[i]][5] > bboxes[indices[j]][5]){\n135:                     indices.erase(indices.begin() + j);\n136:                     j--;", "label": "(18, CUDA, checkRuntime(cudaStreamCreate(&stream));); (80, CUDA, checkRuntime(cudaMemcpyAsync(output_data_host, output_data_device, sizeof(float) * output_numel, cudaMemcpyDeviceToHost, stream));); (80, CUDA, checkRuntime(cudaStreamSynchronize(stream));); (58, CUDA, checkRuntime(cudaMemcpyAsync(input_data_device, input_data_host, input_numel * sizeof(float), cudaMemcpyHostToDevice, stream));); (24, CUDA, checkRuntime(cudaMallocHost(&input_data_host, input_numel * sizeof(float)));); (24, CUDA, checkRuntime(cudaMalloc(&input_data_device, input_numel * sizeof(float)));); (191, CUDA, checkRuntime(cudaFree(input_data_device));); (188, CUDA, checkRuntime(cudaFreeHost(input_data_host));); (190, CUDA, checkRuntime(cudaFree(output_data_device));); (63, CUDA, checkRuntime(cudaMalloc(&output_data_device, sizeof(float) * output_numel));); (-5, CUDA, auto engine = make_nvshared(runtime->deserializeCudaEngine(engine_data.data(), engine_data.size()));); (60, CUDA, checkRuntime(cudaMallocHost(&output_data_host, sizeof(float) * output_numel));); (184, CUDA, checkRuntime(cudaFreeHost(output_data_host));); (181, CUDA, checkRuntime(cudaStreamDestroy(stream));)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(17, MPI, MPI_Sendrecv(b, 1, newtype, 0, 0, d, 0, newtype, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (10, MPI, MPI_Type_commit(&newtype);); (8, MPI, MPI_Type_create_hvector(0, 1, 10, MPI_DOUBLE, &newtype);); (9, MPI, MPI_Type_free(&newtype);); (10, MPI, MPI_Type_indexed(0, b, d, MPI_DOUBLE, &newtype);)"}
{"predict": "(23,OpenMP,#pragma omp parallel for),", "label": "(9, CUDA, thrust::device_vector<std::uint8_t> data_source(elements);); (12, CUDA, thrust::sequence(data_source.begin(), data_source.end());); (12, CUDA, thrust::shuffle(data_source.begin(), data_source.end(), re);); (5, CUDA, thrust::default_random_engine re;); (15, CUDA, thrust::device_vector<ValueT> in_values(data_source);); (17, CUDA, thrust::host_vector<KeyT> host_keys(in_keys);); (17, CUDA, thrust::host_vector<ValueT> host_values(in_values);); (13, CUDA, thrust::device_vector<ValueT> out_values(elements);); (8, CUDA, thrust::device_vector<KeyT> out_keys(elements);); (6, CUDA, thrust::device_vector<KeyT> in_keys(data_source);)"}
{"predict": "(15,OpenMP,#pragma omp parallel for private(i) shared(index, salt, cur_salt, cracked, any_cracked))", "label": "(13, OpenMP, #pragma omp parallel for); (72, OpenMP, #pragma omp atomic); (43, OpenMP, #pragma omp critical)"}
{"predict": "(75,OpenMP,#pragma omp parallel for), (89,OpenMP,#pragma omp parallel for), (98,OpenMP,#pragma omp parallel for), (107,OpenMP,#pragma omp parallel for), (112,OpenMP,#pragma omp parallel for)", "label": "(84, OpenCL, sub_buf = clCreateSubBuffer(buf[1], 0, CL_BUFFER_CREATE_TYPE_REGION, &region, &error );); (87, OpenCL, void * map_ptr = clEnqueueMapBuffer(queue, buf[0], 1, CL_MAP_READ, 0, 64, 0, NULL, NULL, NULL);)"}
{"predict": "(3,OpenMP,#pragma omp parallel sections), (4,OpenMP,#pragma omp section), (8,OpenMP,#pragma omp section)", "label": "(1, OpenMP, #pragma omp parallel num_threads(2)); (10, OpenMP, #pragma omp cancellation point parallel); (3, OpenMP, #pragma omp cancel parallel)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (29,OpenMP,#pragma omp parallel for)", "label": "(4, OpenMP, omp_set_default_device(0);); (33, OpenMP, #pragma omp target teams distribute parallel for is_device_ptr(DATA) nowait); (15, OpenMP, #pragma omp target teams distribute parallel for is_device_ptr(DATA)); (35, OpenMP, #pragma omp taskwait); (19, OpenMP, t1=omp_get_wtime();); (4, OpenMP, #pragma omp parallel for); (-1, OpenMP, DATA = (double*)   omp_target_alloc(N*sizeof(double), omp_get_default_device()) ;); (22, OpenMP, t2=omp_get_wtime();)"}
{"predict": "(1,MPI,MPI_Init(&argc, &argv);); (33,MPI,MPI_Finalize();); (13,OpenMP,#pragma omp parallel for private(x, thread_sum))", "label": "(8, CUDA, __shared__ double sum[block_size];)"}
{"predict": "(8,OpenMP,#pragma omp parallel for collapse(2)), (28,OpenMP,#pragma omp parallel for collapse(2)), (35,OpenMP,#pragma omp parallel for collapse(2)), (45,OpenMP,#pragma omp parallel for collapse(2)), (52,OpenMP,#pragma omp parallel for collapse(2))", "label": "(62, CUDA, CHECK(cudaPeekAtLastError());)"}
{"predict": "(25,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for reduction(+:result_host))", "label": "(42, CUDA, cudaFree(d_inPtr);); (27, CUDA, cudaMalloc((void**)&d_outPtr, size * sizeof(float));); (32, CUDA, cudaMemcpy(&result_gpu, &d_outPtr[0], sizeof(float), cudaMemcpyDeviceToHost);); (27, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);); (37, CUDA, cudaFree(d_outPtr);); (22, CUDA, cudaMalloc((void**)& d_inPtr, size * sizeof(float));)"}
{"predict": "(28,OpenMP,#pragma omp parallel for private(ptr, event))", "label": "(21, CUDA, checkCudaErrors(cudaSetDevice(shm->devices[id]));); (54, CUDA, checkCudaErrors(cudaStreamWaitEvent(stream, events[bufferId], 0));); (71, CUDA, checkCudaErrors(cudaStreamWaitEvent(stream, events[id], 0));); (56, CUDA, checkCudaErrors(cudaGetLastError());); (92, CUDA, checkCudaErrors(cudaStreamDestroy(stream));); (18, CUDA, checkCudaErrors(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));); (86, CUDA, checkCudaErrors(cudaIpcCloseMemHandle(ptrs[i]));); (86, CUDA, checkCudaErrors(cudaEventDestroy(events[i]));); (53, CUDA, checkCudaErrors(cudaEventRecord(events[bufferId], stream));); (68, CUDA, checkCudaErrors(cudaStreamSynchronize(stream));); (12, CUDA, checkCudaErrors(cudaGetDeviceProperties(&prop, shm->devices[id]));)"}
{"predict": "The provided code snippet is a test function and does not contain any loops or computationally intensive tasks that would benefit from parallelization. However, if there were any loops or sections of code that could be parallelized, I would suggest where to insert the parallelization instructions. Since there are no such sections, no parallelization instructions are necessary for this specific code.\n\nIf you have a different section of code that includes loops or other parallelizable tasks, please provide that, and I can analyze it accordingly.", "label": "(21, OpenCL, retVal = clReleaseMemObject(smallBuffer);); (5, OpenCL, cl_mem smallBuffer = clCreateBuffer(clContext, flags, size, hostPtr, &retVal);); (24, OpenCL, EXPECT_EQ(clReleaseContext(context), CL_SUCCESS);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for collapse(2)), (17,OpenMP,#pragma omp parallel for)", "label": "(13, CUDA, cudaCheck(cudaGetLastError());)"}
{"predict": "(73,OpenMP,#pragma omp parallel for), (92,OpenMP,#pragma omp parallel for)", "label": "(82, MPI, MPI_Wait(&request[1], &status[1]);); (90, MPI, MPI_Wait(&request[0], &status[0]);); (90, MPI, MPI_Get_count(&status[0], MPI_BYTE, &nwrite);); (66, MPI, MPI_Irecv(buf_recv, shuffile_mpi_buf_size, MPI_BYTE, rank_recv, 0, comm, &request[0]);); (77, MPI, MPI_Isend(buf_send, nread, MPI_BYTE, rank_send, 0, comm, &request[1]);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for)", "label": "(16, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(20,OpenMP,#pragma omp parallel for)", "label": "(65, CUDA, CHECK_CUDART(cudaFree(indata));); (19, CUDA, CHECK_CUDART(cudaMemcpy(indata, &inval, sizeof(T), cudaMemcpyHostToDevice));); (61, CUDA, CHECK_CUDART(cudaMemcpy(&outval, outdata, sizeof(T), cudaMemcpyDeviceToHost));); (14, CUDA, CHECK_CUDART(cudaMalloc((void**)&indata, sizeof(T)));); (14, CUDA, CHECK_CUDART(cudaMalloc((void**)&outdata, sizeof(T)));); (59, CUDA, CHECK_CUDART(cudaFree(outdata));)"}
{"predict": "configuration.buffer = (void**)&buffer;\n101: #endif\n102: \t\t\t//Copy input data to the GPU.\n103: #if(VKFFT_BACKEND==0)\n104: \t\t\tresFFT = copyBufferFromCPU(vkGPU, buffer, bufferDeviceMemory, buffer_input, bufferSize);\n105: \t\t\tif (resFFT != VKFFT_SUCCESS) return resFFT;\n106: #elif(VKFFT_BACKEND==1)\n107: \t\t\tres = cudaMemcpy(buffer, buffer_input, bufferSize, cudaMemcpyHostToDevice);\n108: \t\t\tif (res != cudaSuccess) return VKFFT_ERROR_FAILED_TO_ALLOCATE;\n109: #elif(VKFFT_BACKEND==2)\n110: \t\t\tres = hipMemcpy(buffer, buffer_input, bufferSize, hipMemcpyHostToDevice);\n111: \t\t\tif (res != hipSuccess) return VKFFT_ERROR_FAILED_TO_ALLOCATE;\n112: #elif(VKFFT_BACKEND==3)\n113: \t\t\tres = clEnqueueWriteBuffer(vkGPU->queue, buffer, CL_TRUE, 0, bufferSize, buffer_input, 0, NULL, NULL);\n114: \t\t\tif (res != CL_SUCCESS) return VKFFT_ERROR_FAILED_TO_ALLOCATE;\n115: #elif(VKFFT_BACKEND==4)\n116: \t\t\tres = zeCommandListAppendMemoryCopy(vkGPU->commandList, buffer, buffer_input, bufferSize, NULL, 0, NULL, NULL);\n117: \t\t\tif (res != ZE_RESULT_SUCCESS) return VKFFT_ERROR_FAILED_TO_ALLOCATE;\n118: #elif(VKFFT_BACKEND==5)\n119:             res = vkGPU->commandEncoder->copyBytes(buffer_input, buffer, bufferSize);\n120:             if (res != VKFFT_SUCCESS) return VKFFT_ERROR_FAILED_TO_ALLOCATE;\n121: #endif\n122: \t\t\t//Initialize VkFFT application with the configuration.\n123: \t\t\tresFFT = initializeVkFFT(&app, &configuration);\n124: \t\t\tif (resFFT != VKFFT_SUCCESS) return resFFT;\n125: \n126: \t\t\tdouble start_time = 0;\n127: \t\t\tdouble end_time = 0;\n128: #if(VKFFT_BACKEND==0)\n129: \t\t\tstart_time = vkGPU->vkGetTime();\n1", "label": "(162, OpenCL, clGetDeviceInfo(vkGPU->device, CL_DEVICE_VENDOR_ID, sizeof(cl_int), &vendorID, 0);); (77, CUDA, res = cudaMalloc((void**)&buffer, bufferSize);); (86, OpenCL, buffer = clCreateBuffer(vkGPU->context, CL_MEM_READ_WRITE, bufferSize, 0, &res);)"}
{"predict": "<float>' requested here}}\n82: }\n83: \n84: template <typename T>\n85: T tmain(T x) {\n86:   T y;\n87:   T z = x;\n88:   T w;\n89:   int i;\n90:   int j;\n91:   int k;\n92:   T a[5];\n93:   T b[5];\n94:   T c[5];\n95:   T d[5];\n96:   T e[5];\n97:   T f[5];\n98:   T g[5];\n99:   T h[5];\n100:  T i[5];\n101:  T j[5];\n102:  T k[5];\n103:  T l[5];\n104:  T m[5];\n105:  T n[5];\n106:  T o[5];\n107:  T p[5];\n108:  T q[5];\n109:  T r[5];\n110:  T s[5];\n111:  T t[5];\n112:  T u[5];\n113:  T v[5];\n114:  T w[5];\n115:  T x[5];\n116:  T y[5];\n117:  T z[5];\n118:  T a1[5];\n119:  T b1[5];\n120:  T c1[5];\n121:  T d1[5];\n122:  T e1[5];\n123:  T f1[5];\n124:  T g1[5];\n125:  T h1[5];\n126:  T i1[5];\n127:  T j1[5];\n128:  T k1[5];\n129:  T l1[5];\n130:  T m1[5];\n131:  T n1[5];\n132:  T o1[5];\n133:  T p1[5];\n134:  T q1[", "label": "(113, OpenMP, #pragma omp teams distribute reduction(task, + : m)); (15, OpenMP, #pragma omp teams distribute reduction +); (29, OpenMP, #pragma omp teams distribute reduction(\\)); (37, OpenMP, #pragma omp teams distribute reduction(|| : argc > 0 ? argv[1] : argv[2])); (106, OpenMP, #pragma omp teams distribute reduction(+ : m)); (20, OpenMP, #pragma omp teams distribute reduction()); (6, OpenMP, #pragma omp target); (42, OpenMP, #pragma omp teams distribute reduction(^ : S1)); (44, OpenMP, #pragma omp teams distribute reduction(+ : a, b, c, d, f)); (67, OpenMP, #pragma omp teams distribute reduction(&& : S2::S2sc)); (57, OpenMP, #pragma omp teams distribute reduction(- : da)); (32, OpenMP, #pragma omp teams distribute reduction(~ : argc)); (10, OpenMP, #pragma omp teams distribute reduction(-); (45, OpenMP, #pragma omp teams distribute reduction(max : h.b)); (5, OpenMP, #pragma omp teams distribute reduction(); (75, OpenMP, #pragma omp parallel private(k)); (18, OpenMP, #pragma omp teams distribute reduction(foo : argc); (38, OpenMP, #pragma omp teams distribute reduction(min : a, b, c, d, f)); (70, OpenMP, #pragma omp teams distribute private(i), reduction(+ : j), reduction(+ : q)); (79, OpenMP, #pragma omp teams distribute reduction(+ : r)); (41, OpenMP, #pragma omp teams distribute reduction(+ : ba)); (25, OpenMP, #pragma omp teams distribute reduction(&& : argc)); (81, OpenMP, #pragma omp teams distribute reduction(max : j)); (78, OpenMP, #pragma omp parallel reduction(min : i)); (4, OpenMP, #pragma omp teams distribute reduction(*)); (67, OpenMP, #pragma omp teams distribute reduction(+ : p), reduction(+ : p)); (44, OpenMP, #pragma omp teams distribute reduction(^ : fl)); (58, OpenMP, #pragma omp teams distribute reduction(+ : o)); (54, OpenMP, #pragma omp teams distribute reduction(+ : h, k, B::x)); (35, OpenMP, #pragma omp teams distribute reduction(* : ca)); (76, OpenMP, #pragma omp teams distribute reduction(+ : fl, z)); (48, OpenMP, #pragma omp teams distribute reduction(& : e, g)); (68, OpenMP, #pragma omp parallel shared(i)); (4, OpenMP, #pragma omp teams distribute reduction(| : argc,); (-21, OpenMP, #pragma omp teams distribute reduction); (38, OpenMP, #pragma omp teams distribute reduction(&& : S2::S2s))"}
{"predict": "(10,OpenMP,#pragma omp parallel sections), (10,OpenMP,    #pragma omp section), (11,OpenMP,    #pragma omp section), (12,OpenMP,    #pragma omp section)", "label": "(4, CUDA, checkCudaErrors(cudaGetDeviceProperties(&prop, whichDevice));); (2, CUDA, checkCudaErrors(cudaGetDevice(&whichDevice));)"}
{"predict": "(6,OpenMP,#pragma omp parallel for private(index, Buf, hash, hex, len, cnt, i, ctx) collapse(1)), (26,OpenMP,#pragma omp parallel for private(i, len, ctx) collapse(1))", "label": "(4, OpenMP, #pragma omp parallel for)"}
{"predict": "3:           costs->computeTrajectoryCost(y, u, t, theta_c, crash_status, trajectory_costs_d, global_idx, num_rollouts);\n124:     }\n125: \n126:     // Advance state\n127:     if (t < num_timesteps - 1)\n128:     {\n129:       dynamics->advanceState(x, u, dt, y, crash_status);\n130:     }\n131: \n132:     if (t < num_timesteps - 1)\n133:     {\n134:       sampling->advanceDistribution(y, t + 1, theta_d, blockDim.y, thread_idy);\n135:     }\n136: \n137:     if (t < num_timesteps - 1)\n138:     {\n139:       fb_controller->advanceFeedback(y, t + 1, theta_fb, blockDim.y, thread_idy);\n140:     }\n141: \n142:     if (t < num_timesteps - 1)\n143:     {\n144:       costs->advanceCosts(y, t + 1, theta_c, blockDim.y, thread_idy);\n145:     }\n146: \n147:     // Handle crash\n148:     if (t < num_timesteps - 1)\n149:     {\n150:       if (crash_status[0] == 1)\n151:       {\n152:         for (int k = t + 1; k < num_timesteps; ++k)\n153:         {\n154:           running_cost[0] += costs->computeCrashCost(y, u, k, theta_c, crash_status);\n155:         }\n156:         break;\n157:       }\n158:     }\n159:   }\n160:   /*<----End of simulation loop-----> */\n161: \n162:   // Handle crash at terminal state\n163:   if (crash_status[0] == 1)\n164:   {\n165:     running_cost[0] += costs->computeCrashCost(y, u, num_timesteps - 1, theta", "label": "(21, CUDA, extern __shared__ float entire_buffer[];)"}
{"predict": "(31,OpenMP,#pragma omp parallel for private(i) shared(argc, argv, showDesc, wrank))", "label": "(29, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (24, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (55, MPI, MPI_Finalize();); (52, MPI, MPI_T_finalize();); (23, MPI, MPI_T_init_thread(required, &provided);); (36, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(34,OpenMP,#pragma omp parallel for reduction(+:elem_cnt)), (36,OpenMP,#pragma omp parallel for reduction(+:elem_cnt))", "label": "(30, TBB, tbb::parallel_reduce ( vs.range(1), pvrb );); (60, TBB, tbb::parallel_for ( tbb::blocked_range<int> (0, N, RANGE_MIN), parallel_vector_for_body<T,Allocator>( vvs, allocator_name ) );); (1, TBB, typedef std::vector<T, tbb::tbb_allocator<T> > container_type;); (5, TBB, tbb::task_scheduler_init init(p);); (10, TBB, if (Verbose && t == 0) t0 = tbb::tick_count::now();); (64, TBB, tbb::flattened2d<ets_type> fvvs = flatten2d(vvs);); (59, TBB, tbb::parallel_reduce ( vvs.range(1), pvrb2 );); (-5, TBB, tbb::tick_count t0;); (30, TBB, tbb::flattened2d<ets_type> fvs = flatten2d(vs);); (50, TBB, ets_type vvs( typename container_type::size_type(25), minus_one, tbb::tbb_allocator<T>() );); (-6, TBB, typedef tbb::enumerable_thread_specific< container_type, Allocator<container_type> > ets_type;); (66, TBB, REMARK(\"done\\nparallel %s, %d, %g, %g\\n\", test_name, p, result_value, ( tbb::tick_count::now() - t0).seconds());)"}
{"predict": "(9,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for)", "label": "(1, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)"}
{"predict": "(16,OpenMP,#pragma omp parallel for),", "label": "(14, OpenMP, #pragma omp parallel for)"}
{"predict": "(16,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for)", "label": "(12, OpenCL, status = clEnqueueNDRangeKernel(pCmdQ, mockKernel.mockMultiDeviceKernel, testedWorkDim, nullptr, globalWorkSize, localWorkSize, 0, nullptr, nullptr);)"}
{"predict": "(19,OpenMP,#pragma omp parallel for)", "label": "(23, CUDA, CSC(cudaGetLastError(), STATUS_FAILURE);)"}
{"predict": "(4,OpenMP,#pragma omp parallel for private(cf, fake_sender))", "label": "(46, TBB, g.reset(tbb::flow::rf_reset_bodies);); (21, TBB, tbb::flow::make_edge( exe_node, *receivers[r] );); (7, TBB, tbb::flow::continue_node< OutputType > exe_node( g, cf );); (22, TBB, utils::NativeParallelFor( p, parallel_puts<tbb::flow::continue_msg>(exe_node) );); (36, TBB, body_copy = tbb::flow::copy_body< inc_functor<OutputType> >( exe_node );); (-1, TBB, tbb::flow::graph g;); (29, TBB, tbb::flow::remove_edge( exe_node, *receivers[r] );); (5, TBB, tbb::detail::d2::register_predecessor(exe_node, fake_sender);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for), (24,MPI,MPI_Send(message.data(), numElements, MPI_INT, destination, rank + 1, MPI_COMM_WORLD);), (31,MPI,MPI_Recv(receiveBuffer.data(), numElements, MPI_INT, source, MPI_ANY_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)", "label": "(6, MPI, MPI_Init(&argc, &argv);); (47, MPI, MPI_Finalize();); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (38, MPI, double t1 = MPI_Wtime();); (20, MPI, MPI_Barrier(MPI_COMM_WORLD);); (20, MPI, double t0 = MPI_Wtime();)"}
{"predict": "(10,OpenMP,#pragma omp parallel for num_threads(n_threads))", "label": "(1, TBB, tbb::task_arena a(1);); (13, TBB, CHECK(tbb::this_task_arena::max_concurrency() == 2);)"}
{"predict": "(6,OpenMP,#pragma omp parallel for)", "label": "(31, OpenMP, #pragma omp teams distribute parallel for proc_bind(master)); (25, OpenMP, #pragma omp teams distribute parallel for proc_bind(x)); (4, OpenMP, #pragma omp teams distribute parallel for proc_bind); (34, OpenMP, #pragma omp teams distribute parallel for proc_bind(spread)); (6, OpenMP, #pragma omp teams distribute parallel for proc_bind(); (9, OpenMP, #pragma omp teams distribute parallel for proc_bind()); (16, OpenMP, #pragma omp teams distribute parallel for proc_bind(close), proc_bind(spread)); (-2, OpenMP, #pragma omp target); (27, OpenMP, #pragma omp parallel proc_bind(close)); (9, OpenMP, #pragma omp teams distribute parallel for proc_bind(master)"}
{"predict": "(35,OpenMP,#pragma omp parallel for), (38,OpenMP,#pragma omp parallel for)", "label": "(42, CUDA, cudaFree(res_dev);); (35, CUDA, cudaMalloc(&res_dev, sizeof(float));); (37, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(37,OpenMP,#pragma omp parallel for private(rA, rB) reduction(+:acc))", "label": "(11, CUDA, extern __shared__ __align__(sizeof(double)) unsigned char buf_[];)"}
{"predict": "(total_done_skipped_failed[coll_num][0] == 0) {\n150:                 continue;\n151:             }\n152:             num_all += total_done_skipped_failed[coll_num][0];\n153:             num_done += total_done_skipped_failed[coll_num][1];\n154:             num_failed += total_done_skipped_failed[coll_num][3];\n155:             num_skipped += total_done_skipped_failed[coll_num][2];\n156:             std::cout << std::setw(22) << std::left << ucc_coll_type_names[coll_type] <<\n157:                 std::setw(10) << std::right << total_done_skipped_failed[coll_num][0] <<\n158:                 std::setw(10) << std::right << total_done_skipped_failed[coll_num][1] <<\n159:                 std::setw(10) << std::right << total_done_skipped_failed[coll_num][3] <<\n160:                 std::setw(10) << std::right << total_done_skipped_failed[coll_num][2] << std::endl;\n161:         }\n162:         std::cout << std::setw(22) << std::left << \"Total\" <<\n163:             std::setw(10) << std::right << num_all <<\n164:             std::setw(10) << std::right << num_done <<\n165:             std::setw(10) << std::right << num_failed <<\n166:             std::setw(10) << std::right << num_skipped << std::endl;\n167:         std::cout.copyfmt(iostate);\n168:     }\n169: \n170: test_exit:\n171:     delete test;\n172:     MPI_Barrier(MPI_COMM_WORLD);\n173: mpi_exit:\n174:     MPI_Finalize();\n175:     return failed;\n176: }", "label": "(21, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (193, MPI, MPI_Finalize();); (25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (23, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (124, MPI, MPI_Test(&req, &completed, MPI_STATUS_IGNORE);)"}
{"predict": "(31,OpenMP,#pragma omp parallel for), (46,OpenMP,#pragma omp single nowait), (50,OpenMP,#pragma omp single nowait), (70,OpenMP,#pragma omp single nowait)", "label": "(40, MPI, mpi_errno = Mpi.Finalize();); (86, MPI, Mpi.CompleteFinalize();)"}
{"predict": "(26,OpenMP,#pragma omp parallel sections), (26,OpenMP,#pragma omp section), (30,OpenMP,#pragma omp section), (34,OpenMP,#pragma omp section), (36,OpenMP,#pragma omp section), (38,OpenMP,#pragma omp section)", "label": "(44, MPI, MPI_T_finalize();); (19, MPI, MPI_T_init_thread(required, &provided);)"}
{"predict": "2] = 0;\n108:                 }\n109:             }\n110:         }\n111:         GPFSMPIO_T_CIO_SET_GET(w, 1, 1, GPFSMPIO_CIO_T_ALLGATHER, GPFSMPIO_CIO_T_GATHER);\n112:         MPI_Allgather(&start_offset, 1, MPI_OFFSET, st_offsets, 1, MPI_OFFSET, fd->comm);\n113:         MPI_Allgather(&end_offset, 1, MPI_OFFSET, end_offsets, 1, MPI_OFFSET, fd->comm);\n114:         if ((romio_write_aggmethod == 1) || (romio_write_aggmethod == 2)) {\n115:             MPI_Allgather(&my_count_size, 1, MPI_OFFSET, count_sizes, 1, MPI_OFFSET, fd->comm);\n116:         }\n117:         GPFSMPIO_T_CIO_SET_GET(w, 1, 1, GPFSMPIO_CIO_T_MIN, GPFSMPIO_CIO_T_ALLGATHER);\n118:         min_st_offset = st_offsets[0];\n119:         for (i = 1; i < nprocs; i++) {\n120:             if (st_offsets[i] < min_st_offset)\n121:                 min_st_offset = st_offsets[i];\n122:         }\n123:         GPFSMPIO_T_CIO_SET_GET(w, 1, 1, GPFSMPIO_CIO_T_FD_SIZE, GPFSMPIO_CIO_T_MIN);\n124:         fd_size = fd->fp_ind + fd->file_ptr;\n125:         for (i = 0; i < nprocs; i++) {\n126:             if (end_offsets[i] > fd_size)\n127:                 fd_size = end_offsets[i];\n128:         }\n129:         GPFSMPIO_T_CIO_SET_GET(w, 1, 1, GPFSMPIO_CIO_T_CALC_MY_REQ, GPFSMPIO_CIO_T_FD_SIZE);\n130:         ADIOI_Calc_my_req(fd, st_offsets, end_offsets, count_sizes, nprocs,\n131:                           &my_req, &count_my_req_per_proc, &count_my_req_procs,\n132", "label": "(141, MPI, MPI_Allgather(&end_offset, 1, ADIO_OFFSET, end_offsets, 1, ADIO_OFFSET, fd->comm);); (59, MPI, MPI_Comm_rank(fd->comm, &myrank);); (138, MPI, MPI_Allgather(&start_offset, 1, ADIO_OFFSET, st_offsets, 1, ADIO_OFFSET, fd->comm);); (395, MPI, MPI_Type_size_x(datatype, &size);); (94, MPI, MPI_Type_size_x(datatype, &buftype_size);)"}
{"predict": "(46,OpenMP,#pragma omp parallel for), (53,OpenMP,#pragma omp parallel for), (64,OpenMP,#pragma omp parallel for)", "label": "(41, OpenMP, lis2mdl_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)"}
{"predict": "(10,MPI,MPI_Init(&argc, &argv);), (118,MPI,MPI_Finalize();), (70,OpenMP,#pragma omp parallel for), (97,OpenMP,#pragma omp parallel for)", "label": "(67, OpenMP, kmp_gomp_depends_info_t gomp_depends(depend);); (100, OpenMP, __kmpc_omp_wait_deps(&loc, gtid, ndeps, dep_list, 0, NULL);); (74, OpenMP, __kmpc_omp_task(&loc, gtid, task);); (71, OpenMP, __kmpc_omp_task_with_deps(&loc, gtid, task, ndeps_cnv, dep_list, 0, NULL);)"}
{"predict": "(8,OpenMP,#pragma omp parallel)", "label": "(10, MPI, MPI_Comm c_comm = MPI_Comm_f2c(*comm);)"}
{"predict": "_m; j--)\n119:                 sum2 = Mhu_a[j] + x*sum2;\n120:             double sum3 = sum1 + x*sum2;\n121:             double exp1 = exp(sum3);\n122:             double exp2 = exp(-sum3);\n123:             double hinf = (exp1 - exp2) / (exp1 + exp2);\n124:             double tau = 1.0 / (0.001 * (exp1 + exp2));\n125:             m_gate[ii] = (m_gate[ii] - hinf) * exp(-0.001 / tau) + hinf;\n126:         }\n127:         /*\n128:          * RUSH LARSEN KERNEL BEING TIMED END\n129:          */\n130: \n131:         /* Print completion message for last iteration */\n132:         if (itime == iterations) \n133:         {\n134:             rank0_printf_timestamp(\"Completed all %ld iterations\\n\", iterations);\n135:         }\n136:     }\n137: \n138:     /* Calculate runtime of kernel */\n139:     kernel_endtime=secs_elapsed();\n140:     kernel_runtime=kernel_endtime-kernel_starttime;\n141: \n142:     /* Print runtime of kernel */\n143:     rank0_printf_timestamp(\"Kernel runtime: %.3f seconds\\n\", kernel_runtime);\n144:     rank0_printf_timestamp(\"Transfer runtime: %.3f seconds\\n\", transfer_runtime);\n145: \n146:     /* Verify results for correctness */\n147:     fail_count = verify_results(m_gate,nCells);\n148: \n149:     /* Free memory */\n150:     free(m_gate);\n151:     free(Vm);\n152: \n153:     /* Return number of failures */\n154:     return fail_count;\n155: }", "label": "(162, OpenMP, #pragma omp target exit data map(delete:Vm[:nCells])); (66, OpenMP, #pragma omp target enter data map(to: m_gate[:nCells])); (149, OpenMP, #pragma omp target update from (m_gate[0:nCells])); (65, OpenMP, #pragma omp target enter data map(to: Vm[:nCells])); (102, OpenMP, #pragma omp target teams distribute parallel for); (156, OpenMP, #pragma omp target exit data map(delete:m_gate[:nCells]))"}
{"predict": "(6,OpenMP,#pragma omp parallel for private(flag, val_p))", "label": "(6, MPI, MPI_Type_get_attr(type, key[i], &val_p, &flag);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for private(local_mbox, serial))", "label": "(7, OpenMP, #pragma omp for nowait schedule(runtime))"}
{"predict": "(12,OpenMP,#pragma omp parallel sections), (12,OpenMP,#pragma omp section), (17,OpenMP,#pragma omp section), (22,OpenMP,#pragma omp section), (27,OpenMP,#pragma omp section), (32,OpenMP,#pragma omp section), (37,OpenMP,#pragma omp section), (42,OpenMP,#pragma omp section)", "label": "(9, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for reduction(+:input))", "label": "(6, CUDA, extern __shared__ float s_data[];)"}
{"predict": "(10,OpenMP,#pragma omp parallel for private(j, key, value))", "label": "(10, MPI, MPI_Info_create(&infos[i]);); (57, MPI, MPI_Info_get(infos[i], key, sizeof(value), value, &flag);); (47, MPI, MPI_Info_get_nthkey(infos[i], j, key);); (22, MPI, MPI_Info_get_nkeys(infos[0], &nkeys);); (33, MPI, MPI_Info_get_nkeys(infos[i], &nkeys);); (71, MPI, MPI_Info_free(&infos[i]);)"}
{"predict": "(7,OpenMP,#pragma omp parallel for)", "label": "(5, OpenMP, #pragma omp parallel for)"}
{"predict": "(10,OpenMP,#pragma omp parallel),", "label": "(14, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (12, MPI, MPI_Ireduce_scatter_block_c(sendbuf, recvbuf, count, datatype, MPI_SUM, comm, &req);); (8, MPI, MPI_Reduce_scatter_block_c(sendbuf, recvbuf, count, datatype, MPI_SUM, comm);)"}
{"predict": ";\n136:   checkCudaErrors(cufftXtMalloc(plan_input, (cudaLibXtDesc **)&d_out_signal,\n137:                                 CUFFT_XT_FORMAT_INPLACE));\n138: \n139:   // Copy data to GPU\n140:   checkCudaErrors(\n141:       cufftXtMemcpy(plan_input, d_signal, (cufftComplex *)h_padded_signal,\n142:                     CUFFT_COPY_HOST_TO_DEVICE));\n143: \n144:   // cufftXtExecC2C() - Perform the forward FFT\n145:   checkCudaErrors(cufftXtExecC2C(plan_input, d_signal, d_out_signal,\n146:                                 CUFFT_FORWARD));\n147: \n148:   // Multiply coefficient\n149:   multiplyCoefficient(d_signal, d_out_signal, new_size, 1.0f, nGPUs);\n150: \n151:   // cufftXtExecC2C() - Perform the backward FFT\n152:   checkCudaErrors(cufftXtExecC2C(plan_input, d_out_signal, d_signal,\n153:                                 CUFFT_INVERSE));\n154: \n155:   // Copy data back to host\n156:   checkCudaErrors(\n157:       cufftXtMemcpy(plan_input, (cufftComplex *)h_padded_signal, d_signal,\n158:                     CUFFT_COPY_DEVICE_TO_HOST));\n159: \n160:   // Clean up memory\n161:   free(h_signal);\n162:   free(h_filter_kernel);\n163:   free(h_padded_signal);\n164:   free(h_padded_filter_kernel);\n165:   free(worksize);\n166:   free(whichGPUs);\n167: \n168:   return 0;\n169: }\n170: \n171: // Kernel for GPU\n172: void multiplyCoefficient(cudaLibXtDesc *d_signal, cudaLibXtDesc *d_out_signal,\n173:                          int size, float scale, int nGPUs) {\n174:   // Launch the kernel on the GPU", "label": "(29, CUDA, checkCudaErrors(cudaGetDeviceCount(&GPU_N));); (219, CUDA, checkCudaErrors(cufftXtFree(d_out_filter_kernel));); (120, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, whichGPUs[i]));); (216, CUDA, checkCudaErrors(cufftXtFree(d_out_signal));); (100, CUDA, checkCudaErrors(cufftCreate(&plan_input));); (40, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, i));); (211, CUDA, checkCudaErrors(cufftXtFree(d_signal));); (216, CUDA, checkCudaErrors(cufftDestroy(plan_input));); (210, CUDA, checkCudaErrors(cufftXtFree(d_filter_kernel));)"}
{"predict": "(42,OpenMP,#pragma omp parallel for), (46,OpenMP,#pragma omp parallel for), (60,OpenMP,#pragma omp parallel for)", "label": "(56, MPI, MPI_Type_get_extent(MPI_INT, &tmp_lb, &extent);); (33, MPI, MPI_Comm_size(comm, &size);); (55, MPI, MPI_Win_create(recvBuf, totsize * extent, extent, MPI_INFO_NULL, comm, &win);); (24, MPI, MPI_Type_indexed(2, counts, displs, MPI_INT, &originType);); (54, MPI, MPI_Win_fence(0, win);); (23, MPI, MPI_Type_commit(&originType);); (77, MPI, MPI_Win_free(&win);); (75, MPI, MPI_Type_free(&originType);); (58, MPI, err = MPI_Win_fence(0, win);); (53, MPI, MPI_Win_set_errhandler(win, MPI_ERRORS_RETURN);); (33, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (22, MPI, MPI_Comm_rank(comm, &rank);); (52, MPI, err = MPI_Put(srcBuf, 1, originType, dest, 0, totsize, MPI_INT, win);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for)", "label": "(8, OpenMP, #pragma omp task private(); (17, OpenMP, #pragma omp task private(ca)); (20, OpenMP, #pragma omp task private(threadvar, B::x)); (10, OpenMP, #pragma omp task private(argc argv)); (17, OpenMP, #pragma omp task private(e, g)); (31, OpenMP, #pragma omp task private(m)); (1, OpenMP, #pragma omp task private); (9, OpenMP, #pragma omp task private(argv[1])); (1, OpenMP, #pragma omp task private()); (19, OpenMP, #pragma omp task private(j)); (4, OpenMP, #pragma omp task private(S1)); (0, OpenMP, #pragma omp task private(argc,); (5, OpenMP, #pragma omp task private(ba) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (7, OpenMP, #pragma omp task private(S2::S2s)); (5, OpenMP, #pragma omp task private(da)); (-3, OpenMP, #pragma omp task private(argc > 0 ? argv[1] : argv[2])); (11, OpenMP, #pragma omp task private(i)); (6, OpenMP, #pragma omp task shared(i), private(i)); (14, OpenMP, #pragma omp task private(i) allocate(omp_thread_mem_alloc: i)); (6, OpenMP, #pragma omp task firstprivate(i) private(i)); (10, OpenMP, #pragma omp task firstprivate(i)); (-11, OpenMP, #pragma omp task private(argc); (-7, OpenMP, #pragma omp task private(z, a, b, c, d, f))"}
{"predict": "(11,OpenMP,#pragma omp parallel for)", "label": "(25, OpenMP, omp_target_free(d_a, omp_get_default_device());); (12, OpenMP, d_a = (int *)omp_target_alloc(DataSize, omp_get_default_device());); (18, OpenMP, #pragma omp target update to(p_a)); (20, OpenMP, omp_target_disassociate_ptr(p_a, omp_get_default_device());); (11, OpenMP, omp_target_associate_ptr(p_a, d_a, DataSize, 0, omp_get_default_device());)"}
{"predict": "(26,OpenMP,#pragma omp parallel for), (66,OpenMP,#pragma omp parallel for private(i))", "label": "(35, MPI, err = MPI_Type_create_indexed_block(count, 1, disp, vectype, &newtype);); (42, MPI, MPI_Type_size(MPI_INT, &int_size);); (58, MPI, MPI_Type_commit(&newtype);); (24, MPI, err = MPI_Type_vector(2, 1, 2, MPI_INT, &vectype);); (41, MPI, err = MPI_Type_size(newtype, &size);); (74, MPI, MPI_Type_free(&newtype);); (72, MPI, MPI_Type_free(&vectype);)"}
{"predict": "(8,OpenMP,#pragma omp atomic)", "label": "(44, OpenCL, clReleaseEvent(retEvent);)"}
{"predict": "(15,OpenMP,#pragma omp parallel for)", "label": "(24, CUDA, ASSERT_EQ(cudaEventRecord(wm.ready, 0), cudaSuccess);); (25, CUDA, cvcuda::WorkspaceMemAllocator wa(wm, cudaStream_t(0));); (51, CUDA, EXPECT_EQ(cudaEventQuery(wm.ready), cudaSuccess);); (1, CUDA, ASSERT_EQ(cudaMalloc(&_junk, junk_size), cudaSuccess);); (14, CUDA, ASSERT_EQ(cudaMemset(junk.get(), i, junk_size), cudaSuccess);); (6, CUDA, ASSERT_EQ(cudaEventCreateWithFlags(&wm.ready, cudaEventDisableTiming), cudaSuccess);); (26, CUDA, ASSERT_EQ(cudaDeviceSynchronize(), cudaSuccess);); (50, CUDA, ASSERT_EQ(cudaEventDestroy(wm.ready), cudaSuccess);); (19, CUDA, EXPECT_EQ(cudaEventQuery(wm.ready), cudaErrorNotReady);)"}
{"predict": "(81,OpenMP,#pragma omp parallel for)", "label": "(65, CUDA, CHECK_CUDA(cudaMalloc(&dy, dataSize * sizeof(int32_t)));); (100, CUDA, CHECK_CUDA(cudaFree(dUpdates));); (62, CUDA, CHECK_CUDA(cudaMalloc(&dUpdates, updatesSize * sizeof(int32_t)));); (94, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (37, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (97, CUDA, CHECK_CUDA(cudaFree(dy));); (56, CUDA, CHECK_CUDA(cudaMalloc(&dx, dataSize * sizeof(int32_t)));); (93, CUDA, CHECK_CUDA(cudaFree(dIndices));); (36, CUDA, CHECK_HIEDNN(hiednnSetCudaStream(handle, 0));); (90, CUDA, CHECK_CUDA(cudaFree(dx));); (53, CUDA, CHECK_CUDA(cudaMalloc(&dIndices, indicesSize * sizeof(int32_t)));)"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for), (38,OpenMP,#pragma omp parallel for)", "label": "(28, TBB, tbb::flow::input_node<T> src2(g, my_input_body<T>() );); (18, TBB, tbb::flow::function_node<T,bool> dest3(g, tbb::flow::unlimited, b3 );); (31, TBB, tbb::flow::function_node<T,bool,tbb::flow::rejecting> dest2(g, tbb::flow::serial, b2 );); (3, TBB, tbb::flow::make_edge( src, dest );); (0, TBB, tbb::flow::input_node<T> src(g, my_input_body<T>() );); (29, TBB, tbb::flow::make_edge( src2, dest2 );); (36, TBB, tbb::flow::input_node<T> src_copy(src);); (13, TBB, tbb::flow::make_edge( src3, dest3 );); (-5, TBB, tbb::flow::graph g;); (6, TBB, tbb::flow::input_node<T> src3(g, my_input_body<T>() );)"}
{"predict": "(20,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for), (42,OpenMP,#pragma omp parallel for)", "label": "(26, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&output_data, output_size_bytes));); (18, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&input_data, input_size_bytes));); (48, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(output_data));); (39, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (37, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (44, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(input_data));)"}
{"predict": "int count=0;\n113:         while(status){\n114:             count++;\n115:             //printf(\"(%d) %d: count=%d\\n\",mype,nbcol_loc,count);\n116:             //fflush(stdout);\n117:             if(count > 10000){\n118:                 printf(\"(%d) %d: count=%d, too many loops!\\n\",mype,nbcol_loc,count);\n119:                 fflush(stdout);\n120:                 status=0;\n121:             }\n122:             gpuLaunchKernelGGL((void*)slsum_fmod_inv_gpu_mrhs_nvshmem, dimGrid, dimBlock, args, 0, stream[0]);\n123:             gpuStreamSynchronize(stream[0]);\n124:             gpuLaunchKernelGGL((void*)slsum_fmod_inv_gpu_mrhs_nvshmem, dimGrid_bc, dimBlock_bc, args, 0, stream[1]);\n125:             gpuStreamSynchronize(stream[1]);\n126:             status = check_status(d_flag_mod);\n127:         }\n128:         checkGPU(gpuGetLastError());\n129:     //}else{\n130:     //    slsum_fmod_inv_gpu_mrhs<<< nbcol_loc, dimBlock >>>(nbcol_loc,nblock_ex,lsum,x,nrhs,maxsup,nsupers,fmod,LBtree_ptr,LRtree_ptr,ilsum,Lrowind_bc_dat,Lrowind_bc_offset,Lnzval_bc_dat,Lnzval_bc_offset,Linv_bc_dat,Linv_bc_offset,Lindval_loc_bc_dat,Lindval_loc_bc_offset, xsup,grid,maxrecvsz);\n131:     //}\n132:     }\n133: }", "label": "(85, CUDA, cudaDeviceSetLimit(cudaLimitStackSize, cuattr.localSizeBytes);); (140, CUDA, CUDA_CHECK(cudaDeviceSynchronize());); (78, CUDA, cudaStreamCreateWithFlags(&stream[i], cudaStreamNonBlocking);); (135, CUDA, CUDA_CHECK(cudaGetLastError());); (80, CUDA, cudaFuncGetAttributes(&cuattr, slsum_fmod_inv_gpu_mrhs_nvshmem);); (138, CUDA, CUDA_CHECK(cudaStreamDestroy(stream[i]));); (82, CUDA, cudaOccupancyMaxPotentialBlockSize(&minGridSize,&myblockSize,(const void *) swait_bcrd ,0,0 );)"}
{"predict": "(11,OpenMP,#pragma omp parallel for)", "label": "(2, CUDA, __shared__ volatile unsigned int s_data[WARP_SIZE*2];)"}
{"predict": "(40,OpenMP,#pragma omp parallel for)", "label": "(29, OpenCL, error |= clSetKernelArg(kernel, 1, sizeof(value), &value);); (27, OpenCL, error |= clSetKernelArg(kernel, 0, sizeof(dst), &dst);)"}
{"predict": ".action([](parser_t&) { use_papi() = false; });\n152:     parser.add_argument({ \"-m\", \"--metrics\" },\n153:                         \"Set the performance metrics to record (ref: `timemory-avail \"\n154:                         \"-H | grep -i metric`)\");\n155:     parser.add_argument({ \"--disable-metrics\" }, \"Disable performance metrics\")\n156:         .count(0)\n157:         .action([](parser_t&) { use_metrics() = false; });\n158:     parser.add_argument({ \"-s\", \"--stacks\" },\n159:                         \"Set the stack unwind depth (ref: `timemory-avail -H | grep \"\n160:                         \"stack`)\")\n161:         .count(1)\n162:         .action([](parser_t& p) { stack_depth() = p.get<int>(\"stacks\"); });\n163:     parser.add_argument({ \"--disable-stacks\" }, \"Disable stack unwind\")\n164:         .count(0)\n165:         .action([](parser_t&) { use_stacks() = false; });\n166:     parser.add_argument({ \"-t\", \"--threads\" },\n167:                         \"Set the number of threads to use (ref: `timemory-avail -H | \"\n168:                         \"grep thread`)\")\n169:         .count(1)\n170:         .action([](parser_t& p) { thread_count() = p.get<int>(\"threads\"); });\n171:     parser.add_argument({ \"--disable-threads\" }, \"Disable thread instrumentation\")\n172:         .count(0)\n173:         .action([](parser_t&) { use_threads() = false; });\n174:     parser.add_argument({ \"--mpi\", \"--mpi-exec\" },\n175:                         \"Set the MPI executable (ref: `timemory-avail -H | grep mpi`)\")\n176:         .count(1)\n177:         .action([](parser_t& p) { mpi_exec() = p.get<std::string>(\"mpi-exec\"); });\n178:     parser.add_argument({ \"--disable-mpi\" }, \"Disable MPI instrumentation\")\n179:         .count(0)\n180:         .action([]", "label": "(8, MPI, tim::settings::mpi_init()       = false;); (8, MPI, tim::settings::mpi_finalize()   = false;)"}
{"predict": "1;\n150:   }\n151:   return 0;\n152: }", "label": "(19, OpenMP, #pragma omp parallel); (0, OpenMP, int nThreads = omp_get_max_threads();)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(4, TBB, using MyVector = oneapi::tbb::concurrent_vector<move_support_tests::Foo, MyAllocator>;); (8, TBB, oneapi::tbb::parallel_for(oneapi::tbb::blocked_range<std::size_t>(0, 10000*s, s), GrowToAtLeast<MyVector>(use_two_arg_form, v, copy_from), oneapi::tbb::simple_partitioner());)"}
{"predict": "-2;\n120:     MPI_Group_range_incl(g1, 1, range, &g6);\n121: \n122:     MPI_Group_union(g5, g6, &g45);\n123:     MPI_Group_compare(MPI_GROUP_WORLD, g45, &result);\n124: \n125:     if (result != MPI_IDENT) {\n126:         errs++;\n127:         fprintf(stderr, \"Comparison with world gave %d, not 1\\n\", result);\n128:     }\n129: \n130:     MTest_Finalize(errs);\n131:     return 0;\n132: }", "label": "(84, MPI, MPI_Group_excl(g1, g1size, lranks, &g6);); (27, MPI, MPI_Group_size(g2, &size);); (41, MPI, MPI_Group_translate_ranks(g2, 3, rin, g1, rout);); (70, MPI, MPI_Group_free(&selfgroup);); (105, MPI, MPI_Group_free(&g2);); (24, MPI, MPI_Group_rank(g2, &rank);); (117, MPI, MPI_Group_compare(MPI_GROUP_EMPTY, g4, &result);); (123, MPI, MPI_Group_free(&g45);); (113, MPI, MPI_Group_union(g4, g5, &g45);); (105, MPI, MPI_Group_range_excl(g1, 1, range, &g5);); (109, MPI, MPI_Group_range_incl(g1, 1, range, &g4);); (118, MPI, MPI_Group_free(&g5);); (-1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (12, MPI, MPI_Group_incl(g1, 3, ranks, &g2);); (114, MPI, MPI_Group_free(&g4);); (140, MPI, MPI_Group_free(&g1);); (72, MPI, MPI_Group_free(&g6);); (62, MPI, MPI_Group_size(g1, &g1size);); (34, MPI, MPI_Comm_group(MPI_COMM_SELF, &selfgroup);); (-10, MPI, MPI_Comm_group(MPI_COMM_WORLD, &g1);); (-10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myrank);); (41, MPI, MPI_Group_translate_ranks(g1, size, rin, selfgroup, rout);); (32, MPI, MPI_Group_translate_ranks(selfgroup, 1, rin, g1, rout);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for private(local_size) reduction(max:max_size) reduction(+:total_size)), (25,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for)", "label": "(1, MPI, MPI_Init(&argc, &argv);); (58, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &P);)"}
{"predict": "(15,OpenMP,#pragma omp parallel for collapse(2))", "label": "(7, CUDA, extern __shared__ __align__(sizeof(float)) uchar s[];)"}
{"predict": "(15,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for reduction(+:r))", "label": "(5, OpenMP, #pragma omp barrier); (23, OpenMP, #pragma omp task); (18, OpenMP, #pragma omp for); (0, OpenMP, #pragma omp cancel parallel if(flag)); (4, OpenMP, #pragma omp sections); (7, OpenMP, #pragma omp section); (22, OpenMP, #pragma omp parallel sections); (-6, OpenMP, #pragma omp parallel); (18, OpenMP, #pragma omp cancel taskgroup); (32, OpenMP, #pragma omp parallel for reduction(+: r)); (0, OpenMP, #pragma omp cancel sections); (11, OpenMP, #pragma omp cancel for)"}
{"predict": "The provided code snippet is a unit test function and does not contain any loops or computationally intensive sections that would benefit from parallelization. Therefore, there are no appropriate places to insert parallelization instructions in this code. However, if there were loops or sections that could be parallelized, I would suggest using OpenMP for shared-memory parallelism. Since there are no such sections, no parallelization instructions are needed.\n\n()", "label": "(31, OpenCL, clReleaseMemObject(buffer);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for)", "label": "(5, TBB, tbb::profiling::set_name(counter.mutex, name);); (11, TBB, tbb::parallel_for(tbb::blocked_range<size_t>(0,n,n/10),AddOne<Counter<M> >(counter));); (11, TBB, tbb::tick_count t1 = tbb::tick_count::now();)"}
{"predict": "/* Print the statistics.  */\n155:     PStatFree(&stat);                         /* Free the statistics variables. */\n156: \n157:     /* ------------------------------------------------------------\n158:        2. WE SOLVE THE LINEAR SYSTEM FOR THE SECOND TIME.\n159:        ------------------------------------------------------------*/\n160: \n161:     /* Set the default input options:\n162:         options.Fact = SamePattern_SameRowPerm;\n163:         options.Equil = NO;\n164:         options.ColPerm = NATURAL;\n165:         options.RowPerm = LargeDiag_MC64;\n166:         options.ReplaceTinyPivot = NO;\n167:         options.Trans = NOTRANS;\n168:         options.IterRefine = SLU_DOUBLE;\n169:         options.SolveInitialized = YES;\n170:         options.RefineInitialized = NO;\n171:         options.PrintStat = YES;\n172:      */\n173:     set_reuse_options_dist(&options);\n174: \n175:     if (!iam) {\n176: \tprint_options_dist(&options);\n177: \tfflush(stdout);\n178:     }\n179: \n180:     /* Initialize the statistics variables. */\n181:     PStatInit(&stat);\n182: \n183:     /* Call the linear equation solver: factorize and solve. */\n184:     pdgssvx(&options, &A, &ScalePermstruct, b, ldb, nrhs, &grid,\n185:             &LUstruct, &SOLVEstruct, berr, &stat, &info);\n186: \n187:     if ( info ) {  /* Something is wrong */\n188:         if ( iam==0 ) {\n189: \t    printf(\"ERROR: INFO = %d returned from pdgssvx()\\n\", info);\n190: \t    fflush(stdout);\n191: \t}\n192:     } else {\n193:         /* Check the accuracy of the solution. */\n194:         pdinf_norm_error(iam, m_loc, nrhs, b, ldb, xtrue, ldx, grid.comm);\n195:     }\n196:     \n1", "label": "(232, MPI, MPI_Finalize();); (35, MPI, MPI_Init_thread( &argc, &argv, MPI_THREAD_MULTIPLE, &omp_mpi_level);)"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for), (35,OpenMP,#pragma omp parallel for), (44,OpenMP,#pragma omp parallel for)", "label": "(34, OpenMP, #pragma omp teams distribute parallel for simd linear (argc : 5) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (19, OpenMP, #pragma omp teams distribute parallel for simd linear (argc); (27, OpenMP, #pragma omp teams distribute parallel for simd linear (argc > 0 ? argv[1] : argv[2])); (43, OpenMP, #pragma omp teams distribute parallel for simd linear (argv[1])); (38, OpenMP, #pragma omp teams distribute parallel for simd linear (a, b:B::ib)); (55, OpenMP, #pragma omp teams distribute parallel for simd linear(i)); (19, OpenMP, #pragma omp teams distribute parallel for simd linear (argc,); (8, OpenMP, #pragma omp teams distribute parallel for simd linear ()); (30, OpenMP, #pragma omp teams distribute parallel for simd linear (S1)); (-2, OpenMP, #pragma omp teams distribute parallel for simd linear); (41, OpenMP, #pragma omp teams distribute parallel for simd linear(e, g)); (-5, OpenMP, #pragma omp target); (-1, OpenMP, #pragma omp teams distribute parallel for simd linear (); (42, OpenMP, #pragma omp teams distribute parallel for simd linear(h))"}
{"predict": "129:   MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);\n130:   MPI_Bcast(&lsize, 1, MPI_INT, root, MPI_COMM_WORLD);\n131:   MPI_Bcast(&lsize2, 1, MPI_INT, root, MPI_COMM_WORLD);\n132:   MPI_Bcast(&size, 1, MPI_INT, root, MPI_COMM_WORLD);\n133:   MPI_Bcast(&size2, 1, MPI_INT, root, MPI_COMM_WORLD);\n134:   MPI_Bcast(&radius, 1, MPI_INT, root, MPI_COMM_WORLD);\n135:   MPI_Bcast(&sparsity, 1, MPI_DOUBLE, root, MPI_COMM_WORLD);\n136: \n137:   /* Allocate memory for vector and result */\n138:   vector_space = size * sizeof(double);\n139:   matrix_space = size2 * sizeof(double);\n140:   index_space = size2 * sizeof(s64Int);\n141: \n142:   vector = (double *)malloc(vector_space);\n143:   result = (double *)malloc(vector_space);\n144:   matrix = (double *)malloc(matrix_space);\n145:   colIndex = (s64Int *)malloc(index_space);\n146: \n147:   /* Fill vector with random values */\n148:   for (i=0; i<size; i++) vector[i] = (double)rand()/RAND_MAX;\n149: \n150:   /* Compute stencil size */\n151:   stencil_size = 2*radius + 1;\n152: \n153:   /* Compute number of nonzero entries in matrix */\n154:   nent = (s64Int)size * stencil_size;\n155: \n156:   /* Compute first and last rows handled by this rank */\n157:   nrows = size/Num_procs;\n158:   row_offset = my_ID*nrows;\n159:   first = row_offset*size;\n160:   last = first + nrows*size - 1;\n161: \n162:   /* Fill matrix and colIndex with random values */\n163:   for (row=0; row<nrows", "label": "(134, MPI, MPI_Bcast(&size2,      1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (134, MPI, MPI_Bcast(&radius,     1, MPI_INT,           root, MPI_COMM_WORLD);); (316, MPI, MPI_Finalize();); (129, MPI, MPI_Bcast(&lsize2,     1, MPI_INT,           root, MPI_COMM_WORLD);); (127, MPI, MPI_Bcast(&lsize,      1, MPI_INT,           root, MPI_COMM_WORLD);); (128, MPI, MPI_Bcast(&size,       1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (290, MPI, MPI_Reduce(&vector_sum, &check_sum, 1, MPI_DOUBLE, MPI_SUM, root, MPI_COMM_WORLD);); (41, MPI, MPI_Init(&argc,&argv);); (243, MPI, MPI_Barrier(MPI_COMM_WORLD);); (41, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (126, MPI, MPI_Bcast(&iterations, 1, MPI_INT,           root, MPI_COMM_WORLD);); (38, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(15,OpenMP,#pragma omp parallel for),", "label": "(18, OpenMP, #pragma omp parallel for); (14, OpenMP, #pragma omp target map(tofrom: Inner, Outer, wayout)); (14, OpenMP, #pragma omp teams num_teams(1  ) thread_limit(tl))"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp parallel for firstprivate(length, v_cut))"}
{"predict": "actions[0][\"scenes/s2/image_prefix\"] = u_output_file;\n162: \n163:     //\n164:     // Create the actions.\n165:     //\n166:     ascent::Ascent a;\n167: \n168:     //\n169:     // Publish mesh to ascent.\n170:     //\n171:     a.publish(mesh);\n172: \n173:     //\n174:     // Execute Ascent actions.\n175:     //\n176:     a.execute(actions);\n177: \n178:     //\n179:     // Cleanup\n180:     //\n181:     a.close();\n182: \n183:     // check that we created an image\n184:     EXPECT_TRUE(check_test_image(mesh_output_file));\n185:     EXPECT_TRUE(check_test_image(u_output_file));\n186: }\n(25,MPI,MPI_Comm_rank(comm, &par_rank);); (26,MPI,MPI_Comm_size(comm, &par_size);); (94,OpenMP,#pragma omp parallel for);", "label": "(166, MPI, opts[\"mpi_comm\"] = MPI_Comm_c2f(MPI_COMM_WORLD);); (171, MPI, MPI_Barrier(comm);); (21, MPI, MPI_Comm_rank(comm, &par_rank);); (21, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(27,OpenMP,#pragma omp parallel for private(i))", "label": "(25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (20, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (51, MPI, MPI_Finalize();); (48, MPI, MPI_T_finalize();); (19, MPI, MPI_T_init_thread(required, &provided);); (32, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(72,OpenMP,#pragma omp parallel for), (102,OpenMP,#pragma omp parallel for)", "label": "(101, CUDA, checkHIPErrors( ::cudaDeviceSynchronize() );)"}
{"predict": "(25,OpenMP,#pragma omp parallel sections), (26,OpenMP,#pragma omp section), (33,OpenMP,#pragma omp section)", "label": "(32, OpenMP, #pragma omp parallel if(Gc>0.0 && c>0 && Sc>0.0)); (22, OpenMP, #pragma omp target if(Ga>0.0 && a>0 && Sa>0.0))"}
{"predict": "(44,OpenMP,#pragma omp parallel for)", "label": "(7, CUDA, __shared__ block_radix_sort_t::TempStorage temp_storage;)"}
{"predict": "(19,OpenMP,#pragma omp parallel for)", "label": "(18, OpenMP, #pragma omp target teams distribute if(cond) firstprivate(fp) reduction(+:rd) map(alloc:map))"}
{"predict": "(28,OpenMP,#pragma omp parallel for), (60,OpenMP,#pragma omp parallel for), (77,OpenMP,#pragma omp parallel for)", "label": "(14, CUDA, thrust::host_vector<uint32_t> h_increments(num_increments);); (43, CUDA, thrust::device_vector<uint32_t> increments_in(num_increments);); (53, CUDA, thrust::fill(counts_out.begin(), counts_out.end(), 814920U);); (40, CUDA, thrust::device_vector<uint32_t> bins_in(num_increments);); (41, CUDA, thrust::device_vector<uint32_t> counts_out(NUM_ITEMS);); (47, CUDA, thrust::host_vector<uint32_t> host_counts(num_increments);); (7, CUDA, thrust::host_vector<uint32_t> h_bins(num_increments);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for)", "label": "(21, OpenMP, #pragma omp target update from(a[:n])); (12, OpenMP, #pragma omp target teams distribute parallel for map(to:b[:n])); (15, OpenMP, #pragma omp target teams distribute parallel for)"}
{"predict": "(12,OpenMP,#pragma omp parallel for),", "label": "(6, CUDA, gpuErrchk(cudaPointerGetAttributes(&attributes, d_int));); (28, CUDA, gpuErrchk(cudaFree(d_int));); (1, CUDA, gpuErrchk(cudaMalloc(&d_int, sizeof(int)));)"}
{"predict": "(14,OpenMP,#pragma omp parallel for private(cellImagePos, isContained, color))", "label": "(4, CUDA, __shared__ float3 color;); (2, CUDA, __shared__ bool isContained;); (0, CUDA, __shared__ float2 cellImagePos;)"}
{"predict": "(14,OpenMP,#pragma omp parallel for)", "label": "(12, CUDA, cudaMemset(d_sums, 0, sizeof(Type) * nkeys * ncols);)"}
{"predict": "(38,OpenMP,#pragma omp parallel for reduction(+:sum)), (73,OpenMP,#pragma omp parallel for reduction(+:sum))", "label": "(11, OpenMP, omp_set_num_threads(nr_threads);); (12, OpenMP, #pragma omp parallel default(none) shared(a)); (13, OpenMP, #pragma omp sections); (7, OpenMP, omp_set_dynamic(0);); (13, OpenMP, #pragma omp section); (15, OpenMP, int thread_nr = omp_get_thread_num();)"}
{"predict": "(21,OpenMP,#pragma omp parallel for private(samples, twr_idx, n_idx, data_ptr, psi, psi_shoup, modulus_table, modulus, n_init, tw_idx, remain_iters))", "label": "(10, CUDA, extern __shared__ uint64_t buffer[];)"}
{"predict": "(10,OpenMP,#pragma omp parallel for private(a, b, res))", "label": "(22, CUDA, __shared__ float shared[forward_threads];)"}
{"predict": "(7,OpenMP,#pragma omp parallel for private(sm_scale))", "label": "(12, CUDA, cudaDeviceSynchronize();); (7, CUDA, cudaMalloc((void **)&sm_o, sizeof(float) * m * m);); (34, CUDA, cudaFree(sm_o);)"}
{"predict": "(26,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for)", "label": "(46, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (32, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_AP), sizeof(data_type) * AP.size()));); (63, CUDA, CUDA_CHECK(cudaDeviceReset());); (60, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (30, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (53, CUDA, CUDA_CHECK(cudaFree(d_AP));); (53, CUDA, CUDA_CHECK(cudaFree(d_A));); (22, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(25,OpenMP,#pragma omp parallel for), (44,OpenMP,#pragma omp parallel for)", "label": "(40, OpenCL, clReleaseEvent(exec_event);); (11, OpenCL, profiling_queue = clCreateCommandQueue(ctx, device, CL_QUEUE_PROFILING_ENABLE, &status);); (47, OpenCL, clReleaseCommandQueue(profiling_queue);)"}
{"predict": "2:             MTest_dtp_check(&result, 1, &errs);\n143: #endif\n144:         }\n145:     } else if (rank == target_rank) {\n146:         MTest_dtp_init(&target, 0, 0, count);\n147:         MPI_Barrier(comm);\n148:         if (lock_type == LOCK_TYPE__LOCK) {\n149:         } else {\n150:         }\n151: \n152:         if (flush_type == FLUSH_TYPE__FLUSH_ALL) {\n153:             MPI_Barrier(comm);\n154:         } else if (flush_type == FLUSH_TYPE__FLUSH) {\n155:             MPI_Barrier(comm);\n156:             MPI_Barrier(comm);\n157:         }\n158: \n159:         if (lock_type == LOCK_TYPE__LOCK) {\n160:         } else {\n161:         }\n162: \n163:         if (flush_type == FLUSH_TYPE__NONE) {\n164:             MPI_Barrier(comm);\n165:             MPI_Barrier(comm);\n166:         }\n167:     } else {\n168:         MPI_Barrier(comm);\n169:     }\n170:     return errs;\n171: }", "label": "(163, MPI, MPI_Win_unlock(rank, win);); (22, MPI, MPI_Comm_size(comm, &size);); (125, MPI, MPI_Barrier(comm);); (134, MPI, MPI_Win_unlock(target_rank, win);); (135, MPI, MPI_Win_unlock_all(win);); (76, MPI, MPI_Win_lock(MPI_LOCK_SHARED, target_rank, 0, win);); (120, MPI, MPI_Win_flush_all(win);); (76, MPI, MPI_Win_lock_all(0, win);); (150, MPI, MPI_Win_lock(MPI_LOCK_SHARED, rank, 0, win);); (111, MPI, MPI_Win_flush_local_all(win);); (12, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(16,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for)", "label": "(39, CUDA, return cudaGetLastError();)"}
{"predict": "(24,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for)", "label": "(19, MPI, MPIT_result = MPI_T_pvar_session_create(&session);); (12, MPI, MPIT_result = MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);)"}
{"predict": "(69,OpenMP,#pragma omp parallel for)", "label": "(127, CUDA, checkCudaErrors(cudaFreeArray(cuArray));); (33, CUDA, checkCudaErrors(cudaMalloc((void **) &dData, size));); (56, CUDA, checkCudaErrors(cudaBindTextureToArray(tex, cuArray, channelDesc));); (63, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (71, CUDA, getLastCudaError(\"Kernel execution failed\");); (-3, CUDA, int devID = findCudaDevice(argc, (const char **) argv);); (120, CUDA, checkCudaErrors(cudaFree(dData));)"}
{"predict": "(7,OpenMP,#pragma omp parallel for)", "label": "(9, MPI, MPI_Group_compare(group, wgroup, &result);); (5, MPI, MPI_File_get_group(cFile, &group);); (0, MPI, MPI_File cFile = MPI_File_f2c(*file);); (4, MPI, MPI_Comm_group(MPI_COMM_WORLD, &wgroup);); (12, MPI, MPI_Group_free(&wgroup);); (10, MPI, MPI_Group_free(&group);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for), (88,OpenMP,#pragma omp parallel for)", "label": "(75, CUDA, cudaCheckErrors(\"Kernel c failed\");); (99, CUDA, cudaCheckErrors(\"Launching graph failed\");); (13, CUDA, cudaStreamCreateWithFlags(&streams[i], cudaStreamNonBlocking);); (63, CUDA, cudaCheckErrors(\"Event record failed\");); (56, CUDA, cudaCheckErrors(\"Stream begin capture failed\");); (31, CUDA, cudaMalloc((void**) &d_x, N * sizeof(float));); (44, CUDA, cudaGraphCreate(FIXME, 0);); (62, CUDA, cudaCheckErrors(\"Kernel b failed\");); (104, CUDA, cudaDeviceSynchronize();); (98, CUDA, cudaGraphGetNodes(graph, nodes, &numNodes);); (121, CUDA, cudaMemcpy(h_y, d_y, N, cudaMemcpyDeviceToHost);); (73, CUDA, cudaCheckErrors(\"Kernel d failed\");); (89, CUDA, cudaStreamSynchronize(streams[0]);); (58, CUDA, cudaStreamWaitEvent(streams[1], event1);); (72, CUDA, cudaStreamEndCapture(streams[0], &graph);); (48, CUDA, cudaCheckErrors(\"Kernel a failed\");); (64, CUDA, cudaStreamWaitEvent(streams[0], event2);); (-11, CUDA, cudaEventCreateWithFlags(&event1, cudaEventDisableTiming);); (-11, CUDA, cudaEventCreateWithFlags(&event2, cudaEventDisableTiming);); (53, CUDA, cudaCheckErrors(\"Event wait failed\");); (88, CUDA, cudaCheckErrors(\"Graph get nodes failed\");); (44, CUDA, cudaEventRecord(event1, streams[0]);); (19, CUDA, cudaMemcpy(d_y, h_y, N, cudaMemcpyHostToDevice);); (54, CUDA, cudaEventRecord(event2, streams[1]);); (16, CUDA, cudaMemcpy(d_x, h_x, N, cudaMemcpyHostToDevice);); (107, CUDA, cudaCheckErrors(\"Finishing memcpy failed\");); (11, CUDA, cudaMalloc((void**) &d_y, N * sizeof(float));); (32, CUDA, cudaStreamBeginCapture(streams[0], cudaStreamCaptureModeGlobal);); (91, CUDA, cudaGraphLaunch(instance, streams[0]);); (63, CUDA, cudaCheckErrors(\"instantiating graph failed\");); (8, CUDA, cudaCheckErrors(\"cudaMalloc failed\");); (56, CUDA, cudaCheckErrors(\"Stream end capture failed\");); (59, CUDA, cudaGraphInstantiate(FIXME, graph, NULL, NULL, 0);); (66, CUDA, cudaGraphLaunch(FIXME, streams[0]);)"}
{"predict": "(18,OpenMP,#pragma omp parallel if(group->group_rank > 0))", "label": "(19, MPI, MPI_Reduce_local((void*)inbuf, outbuf, count, type, op);)"}
{"predict": "(53,OpenMP,#pragma omp parallel for)", "label": "(35, CUDA, __shared__ _VOLATILE_ float aSmem2[8][128+4];); (33, CUDA, __shared__ _VOLATILE_ float bSmem1[8][128+4];); (34, CUDA, __shared__ _VOLATILE_ float bSmem2[8][128+4];); (30, CUDA, __shared__ _VOLATILE_ float aSmem1[8][128+4];)"}
{"predict": "(1,OpenMP,#pragma omp target teams distribute parallel for)", "label": "(3, CUDA, extern __shared__ unsigned sfdatx[];)"}
{"predict": "(8,OpenMP,#pragma omp parallel), (9,OpenMP,#pragma omp single nowait), (15,OpenMP,#pragma omp task), (22,OpenMP,#pragma omp task), (29,OpenMP,#pragma omp task), (38,OpenMP,#pragma omp task), (43,OpenMP,#pragma omp taskwait)", "label": "(9, MPI, test_integration_sin<2,1,double> t1( mpi.vm()[\"hsize\"].as<double>() );); (31, MPI, test_integration_sin<2,4,double> t2( mpi.vm()[\"hsize\"].as<double>() );); (14, MPI, test_integration_sin<2,2,double> t2( mpi.vm()[\"hsize\"].as<double>() );); (2, MPI, std::cout << \"Order = \" << mpi.vm()[\"order\"].as<int>() << \" / \" << FEELPP_MESH_MAX_ORDER << \"\\n\";); (20, MPI, test_integration_sin<2,3,double> t2( mpi.vm()[\"hsize\"].as<double>() );); (37, MPI, test_integration_sin<2,5,double> t2( mpi.vm()[\"hsize\"].as<double>() );)"}
{"predict": "(10,OpenMP,#pragma omp parallel for),", "label": "(16, CUDA, FastllmCudaFree(pointers);); (12, CUDA, cudaMemcpy(pointers, cpuPointers, sizeof(uint8_t*) * part, cudaMemcpyHostToDevice);); (6, CUDA, uint8_t ** pointers = (uint8_t**)FastllmCudaMalloc(sizeof(uint8_t*) * part);)"}
{"predict": "(7,MPI,MPI_Init(&argc, &argv);); (8,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (8,MPI,MPI_Comm_size(MPI_COMM_WORLD, &size);); (60,MPI,MPI_Finalize(););", "label": "(30, MPI, errs = MPI_Irecv(buf, 10, MPI_CHAR, 0, 0, MPI_COMM_WORLD, &request);); (5, MPI, MPI_Init(&argc, &argv);); (65, MPI, MPI_Finalize();); (18, MPI, errs = MPI_Issend(\"No Errors\", 10, MPI_CHAR, 3, 0, MPI_COMM_WORLD, &request);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (16, MPI, errs += MPI_Wait(&request, MPI_STATUS_IGNORE);); (2, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (5, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (34, MPI, errs = MPI_Issend(\"No Errors\", 10, MPI_CHAR, 2, 0, MPI_COMM_WORLD, &request);)"}
{"predict": "(2,OpenMP,#pragma omp parallel for), (4,OpenMP,#pragma omp parallel for), (6,OpenMP,#pragma omp parallel for), (8,OpenMP,#pragma omp parallel for), (10,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for)", "label": "(7, OpenMP, #pragma omp target teams distribute parallel for simd order(); (15, OpenMP, #pragma omp target teams distribute parallel for simd order(concurrent)); (-1, OpenMP, #pragma omp target teams distribute parallel for simd nowait nowait); (7, OpenMP, #pragma omp target teams distribute parallel for simd order(none); (9, OpenMP, #pragma omp target teams distribute parallel for simd order(concurrent); (-1, OpenMP, #pragma omp target teams distribute parallel for simd order)"}
{"predict": "(18,OpenMP,#pragma omp parallel for)", "label": "(24, CUDA, CHECK_CUDA_RETURN(cudaGetLastError());)"}
{"predict": "(12,OpenMP,#pragma omp parallel if(num_threads > 1)), (14,OpenMP,#pragma omp single nowait)", "label": "(7, TBB, tbb::tick_count t1 = tbb::tick_count::now();); (14, TBB, ASSERT( num_threads > 0, \"tbb::thread::hardware_concurrency() has returned an incorrect value\" );); (3, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (11, TBB, int num_threads = tbb::tbb_thread::hardware_concurrency();); (5, TBB, tbb::tick_count t2 = tbb::tick_count::now();)"}
{"predict": "') of loop variable 'i'}}\n122: #pragma omp distribute parallel for simd\n123:   for (int i = 0; i < 10; i *= 2)\n124:     c[i] = a[i];\n125: \n126: #pragma omp target\n127: #pragma omp teams\n128: // omp4-error@+2 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', or '>=') of loop variable 'i'}} omp5-error@+2 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', '>=', or '!=') of loop variable 'i'}}\n123: #pragma omp distribute parallel for simd\n124:   for (int i = 0; i < 10; i++)\n125:     c[i] = a[i];\n126: \n127: #pragma omp target\n128: #pragma omp teams\n129: // omp4-error@+2 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', or '>=') of loop variable 'i'}} omp5-error@+2 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', '>=', or '!=') of loop variable 'i'}}\n130: #pragma omp distribute parallel for simd\n131:   for (int i = 0; i < 10; i += 1)\n132:     c[i] = a[i];\n133: \n134: #pragma omp target\n135: #pragma omp teams\n136: // omp4-error@+2 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', or '>=') of loop variable 'i'}} omp5-error@+2 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', '>=', or '!=') of loop variable 'i'}}\n137: #pragma omp distribute parallel for simd\n138:   for (int i = 0; i < 10; i += 1)\n139:     c[i] = a[i];\n140: \n141: #pragma omp target", "label": "(311, OpenMP, #pragma omp distribute parallel for simd firstprivate(ii)); (332, OpenMP, #pragma omp distribute parallel for simd lastprivate(ii)); (6, OpenMP, #pragma omp distribute parallel for simd); (322, OpenMP, #pragma omp distribute parallel for simd private(ii)); (313, OpenMP, #pragma omp distribute parallel for simd linear(ii)); (1, OpenMP, #pragma omp target); (347, OpenMP, #pragma omp distribute parallel for simd collapse(2)); (0, OpenMP, #pragma omp teams)"}
{"predict": "(23,OpenMP,#pragma omp parallel sections), (24,OpenMP,#pragma omp section), (26,OpenMP,#pragma omp section)", "label": "(17, CUDA, cudaFree(devx_);); (27, CUDA, cudaFree(d_T_);); (24, CUDA, cudaFree(d_P_);); (24, CUDA, cudaFree(d_Q_);); (12, CUDA, cudaFree(devr_);); (1, CUDA, cudaFree(d_work_);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(7, OpenMP, printf(\"#pragma omp target teams distribute parallel for thread_limit(4)\\n\");); (10, OpenMP, num_teams[j] = omp_get_num_teams();); (13, OpenMP, printf(\"#pragma omp target teams distribute parallel for\\n\");); (7, OpenMP, num_threads[j] = omp_get_num_threads();); (4, OpenMP, #pragma omp target teams distribute parallel for)"}
{"predict": "(7,OpenMP,#pragma omp parallel for)", "label": "(19, OpenCL, clReleaseContext_ExpectAndReturn(make_context(0), CL_SUCCESS);)"}
{"predict": "(62,OpenMP,#pragma omp parallel for)", "label": "(63, MPI, *time -= MPI_Wtime();); (43, MPI, MPI_Type_size(c_info->r_data_type, &r_size);); (41, MPI, MPI_Type_size(c_info->s_data_type, &s_size);); (73, MPI, *time += MPI_Wtime();)"}
{"predict": "(4,OpenMP,#pragma omp parallel for private(num_receivers, receivers, ec, c))", "label": "(14, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(exe_node), receivers[r] );); (1, TBB, typedef typename tbb::flow::tuple_element<0,OutputTuple>::type OutputType;); (27, TBB, tbb::flow::remove_edge( tbb::flow::output_port<0>(exe_node), receivers[r] );); (3, TBB, tbb::flow::multifunction_node< InputType, OutputTuple, tbb::flow::rejecting > exe_node( g, tbb::flow::unlimited, body );); (1, TBB, tbb::flow::graph g;)"}
{"predict": "(sizeof(size_t) * nr_nodes);\n139: \n140:   /* Exchange the number of elements we'll send to each rank */\n141:   MPI_Alltoall(nr_send, 1, MPI_SIZE_T, nr_recv, 1, MPI_SIZE_T, MPI_COMM_WORLD);\n142: \n143:   /* Now we can allocate the receive buffer */\n144:   size_t nr_recv_tot = 0;\n145:   for (int i = 0; i < nr_nodes; ++i) {\n146:     nr_recv_tot += nr_recv[i];\n147:   }\n148: \n149:   struct mesh_key_value_rho *mesh_recvbuf;\n150:   if (swift_memalign(\"mesh_recvbuf\", (void **)&mesh_recvbuf,\n151:                      SWIFT_CACHE_ALIGNMENT,\n152:                      nr_recv_tot * sizeof(struct mesh_key_value_rho)) != 0)\n153:     error(\"Failed to allocate array for mesh receive buffer!\");\n154: \n155:   /* Exchange the actual mesh elements */\n156:   MPI_Alltoallv(mesh_sendbuf, nr_send, sorted_offsets, MPI_MESH_KEY_VALUE_RHO,\n157:                 mesh_recvbuf, nr_recv, sorted_offsets, MPI_MESH_KEY_VALUE_RHO,\n158:                 MPI_COMM_WORLD);\n159: \n160:   /* Free the send buffer */\n161:   swift_free(\"mesh_sendbuf\", mesh_sendbuf);\n162:   mesh_sendbuf = NULL;\n163: \n164:   /* Build the slices on this rank */\n165:   for (int i = 0; i < nr_nodes; ++i) {\n166:     int start = slice_offset[i];\n167:     int end = slice_offset[i] + slice_width[i];\n168:     int width = slice_width[i];\n169:     for (int x = start; x < end; ++x) {\n170:       for (int y = 0; y < local_n0; ++y) {\n171:         for (int z = 0; z < local_n0; ++z) {\n172:           const int index = x * local_n0 * local_n0 + y *", "label": "(11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &nodeID);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nr_nodes);); (73, MPI, MPI_Allgather(&local_n0, 1, MPI_INT, slice_width, 1, MPI_INT, MPI_COMM_WORLD);)"}
{"predict": "') of loop variable 'i'}}\n118:   for (int i = 0; i > 1; i++)\n119:     c[i] = a[i];\n120: \n121: #pragma omp target\n122: #pragma omp teams distribute simd\n123: // omp4-error@+1 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', or '>=') of loop variable 'i'}}\n124:   for (int i = 0; i < 1; i++)\n125:     c[i] = a[i];\n126: \n127: #pragma omp target\n128: #pragma omp teams distribute simd\n129: // omp4-error@+1 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', or '>=') of loop variable 'i'}}\n130:   for (int i = 0; i <= 1; i++)\n131:     c[i] = a[i];\n132: \n133: #pragma omp target\n134: #pragma omp teams distribute simd\n135: // omp4-error@+1 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', or '>=') of loop variable 'i'}}\n136:   for (int i = 0; i >= 1; i++)\n137:     c[i] = a[i];\n138: \n139: #pragma omp target\n140: #pragma omp teams distribute simd\n141: // omp4-error@+1 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', or '>=') of loop variable 'i'}}\n142:   for (int i = 0; i < 10; i--)\n143:     c[i] = a[i];\n144: \n145: #pragma omp target\n146: #pragma omp teams distribute simd\n147: // omp4-error@+1 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', or '>=') of loop variable 'i'}}\n148:   for (int i = 0; i < 10;", "label": "(266, OpenMP, #pragma omp teams distribute simd firstprivate(ii)); (6, OpenMP, #pragma omp teams distribute simd); (270, OpenMP, #pragma omp teams distribute simd private(ii)); (275, OpenMP, #pragma omp teams distribute simd lastprivate(ii)); (287, OpenMP, #pragma omp teams distribute simd collapse(2)); (1, OpenMP, #pragma omp target)"}
{"predict": "(40,OpenMP,#pragma omp parallel sections), (41,OpenMP,#pragma omp section), (43,OpenMP,#pragma omp section), (44,OpenMP,#pragma omp section), (47,OpenMP,#pragma omp section), (48,OpenMP,#pragma omp section), (50,OpenMP,#pragma omp section), (54,OpenMP,#pragma omp section), (55,OpenMP,#pragma omp section), (58,OpenMP,#pragma omp section), (59,OpenMP,#pragma omp section), (61,OpenMP,#pragma omp section), (63,OpenMP,#pragma omp section), (65,OpenMP,#pragma omp section), (67,OpenMP,#pragma omp section), (69,OpenMP,#pragma omp section), (71,OpenMP,#pragma omp section), (72,OpenMP,#pragma omp section), (73,OpenMP,#pragma omp section), (75,OpenMP,#pragma omp section), (77,OpenMP,#pragma omp section), (78,OpenMP,#pragma omp section), (80,OpenMP,#pragma omp section), (81,OpenMP,#pragma omp section), (82,OpenMP,#pragma omp section), (91,OpenMP,#pragma omp section), (98,OpenMP,#pragma omp section), (99,OpenMP,#pragma omp section), (100,OpenMP,#pragma omp section), (101,OpenMP,#pragma omp section), (103,OpenMP,#pragma omp section), (104,OpenMP,#pragma omp section), (105,OpenMP,#pragma omp section), (106,OpenMP,#pragma omp section), (113,OpenMP,#pragma omp section), (114,OpenMP,#pragma omp section), (117,OpenMP,#pragma omp section)", "label": "(84, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (84, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)"}
{"predict": "(5,OpenMP,#pragma omp parallel for), (7,OpenMP,#pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp parallel for)"}
{"predict": "(18,MPI,MPI_Barrier(comm);), (36,OpenMP,#pragma omp parallel for)", "label": "(20, MPI, int rank = dray::dray::mpi_rank();); (9, MPI, dray::dray::mpi_comm(MPI_Comm_c2f(comm));); (19, MPI, int size = dray::dray::mpi_size();)"}
{"predict": "(17,OpenMP,#pragma omp parallel for), (42,OpenMP,#pragma omp parallel for), (48,OpenMP,#pragma omp parallel for), (56,OpenMP,#pragma omp parallel for), (70,OpenMP,#pragma omp parallel for), (83,OpenMP,#pragma omp parallel for)", "label": "(6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (104, MPI, MPI_Barrier(MPI_COMM_WORLD);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "));\n121: \t    send_count_list = SUPERLU_MALLOC(nprocs * sizeof(int));\n122: \t    procs_recv_from_list = SUPERLU_MALLOC(nprocs * sizeof(int));\n123: \t    recv_count_list = SUPERLU_MALLOC(nprocs * sizeof(int));\n124: \n125: \t    /* Compute the number of X entries in each process */\n126: \t    if ( rankorder == 1 ) {\n127: \t\t/* XY-major in 3D grid */\n128: \t\tMPI_Allgather(&A2d->m_loc, 1, MPI_INT, m_loc_3d_counts, 1, MPI_INT,\n129: \t\t\t    grid3d->zscp.comm);\n130: \n131: \t\t/* Compute the prefix sum of the block rows of X among all the processes */\n132: \t\tint sum = 0;\n133: \t\tfor (i = 0; i < nprocs; ++i) {\n134: \t\t    int tmp = m_loc_3d_counts[i];\n135: \t\t    m_loc_3d_counts[i] = sum;\n136: \t\t    sum += tmp;\n137: \t\t}\n138: \n139: \t\t/* Compute the send counts for each process */\n140: \t\tfor (i = 0; i < npdep; ++i) {\n141: \t\t    for (j = 0; j < pxy; ++j) {\n142: \t\t\tint src = i * pxy + j;\n143: \t\t\tint dest = j * npdep + i;\n144: \n145: \t\t\tif ( src == iam ) {\n146: \t\t\t    x_send_counts[dest] = row_counts_int[i];\n147: \t\t\t}\n148: \n149: \t\t\tif ( dest == iam ) {\n150: \t\t\t    x_recv_counts[src] = row_counts_int[i];\n151: \t\t\t}\n152: \t\t    }\n153: \t\t}\n154: \n155: \t    } else { /* Z-major in 3D grid (default) */\n156: \t\t/* Compute the number of X entries in each process */\n157: \t\tMPI_Allgather(&A2d->m", "label": "(270, MPI, MPI_Wait(&recv_req, &recv_status);)"}
{"predict": "2:       if (N == 0) {\n133:         found_exact[i] = 0;\n134:         can_break = 0;\n135:       }\n136:       else {\n137:         /* check if median is exact */\n138:         if (num[i] == N) {\n139:           found_exact[i] = 1;\n140:         }\n141:         else {\n142:           found_exact[i] = 0;\n143:         }\n144:       }\n145:     }\n146: \n147:     /* check if all ranks have finished */\n148:     int all_done;\n149:     lwgrp_comm_allreduce(found_exact, &all_done, ranks, MPI_INT, MPI_LAND, lwgcomm);\n150:     if (all_done) {\n151:       break;\n152:     }\n153: \n154:     /* if any rank can't break, we need to continue */\n155:     int any_can_break;\n156:     lwgrp_comm_allreduce(&can_break, &any_can_break, 1, MPI_INT, MPI_LOR, lwgcomm);\n157:     if (!any_can_break) {\n158:       break;\n159:     }\n160: \n161:     /* broadcast median M and total number of active elements N */\n162:     lwgrp_comm_bcast(out_num_with_median, size_int_with_key, MPI_BYTE, 0, lwgcomm);\n163: \n164:     /* count number of elements less than and equal to M in our range */\n165:     int M_count_le, M_count_eq;\n166:     count_le_eq(data, n, keysat, keysat_extent, key_true_extent, out_num_with_median, &M_count_le, &M_count_eq, cmp);\n167: \n168:     /* record counts for this rank */\n169:     counts[2*i]   = M_count_le;\n170:     counts[2*i+1] = M_count_eq;\n171: \n172:     /* gather counts from all ranks */\n173:     lwgrp_comm_allgather(count", "label": "(24, MPI, MPI_Type_get_true_extent(key, &key_true_lb, &key_true_extent);); (204, MPI, MPI_Type_free(&type_int_with_key);); (280, MPI, MPI_Comm_rank(comm, &comm_rank);); (17, MPI, MPI_Type_get_extent(keysat, &keysat_lb, &keysat_extent);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(10, MPI, MPI_Comm_call_errhandler( comm, MPI_ERR_OTHER );); (8, MPI, MPI_Comm_set_errhandler( comm, newerr );); (14, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_create_errhandler( eh, &newerr );); (1, MPI, MPI_Init( &argc, &argv );); (6, MPI, MPI_Errhandler_free( &newerr );)"}
{"predict": "A\n109:   A.totalToBeSent = totalToBeSent;\n110:   A.totalToBeReceived = totalToBeReceived;\n111:   A.neighbors = neighbors;\n112:   A.receiveLength = receiveLength;\n113:   A.sendLength = sendLength;\n114:   A.elementsToSend = elementsToSend;\n115:   A.externalToLocalMap = externalToLocalMap;\n116: }\n117: \n118: void ComputeRankOfMatrixRow_ref(const Geometry & geom, global_int_t globalRow, int & rankIdOfColumnEntry) {\n119:   rankIdOfColumnEntry = (int)globalRow / geom.globalNumRows;\n120: }\n121: \n122: void ExchangeHalo_ref(const SparseMatrix & A, Vector & x, Vector & y) {\n123:   int num_neighbors = A.neighbors.size();\n124:   MPI_Request * requests = new MPI_Request[2*num_neighbors];\n125:   MPI_Status * statuses = new MPI_Status[2*num_neighbors];\n126: \n127:   // Post receives first\n128:   local_int_t offset = 0;\n129:   for (int i=0; i<num_neighbors; i++) {\n130:     int neighbor = A.neighbors[i];\n131:     int length = A.receiveLength[i];\n132:     MPI_Irecv(&(y.values[offset]), length, MPI_DOUBLE, neighbor, 0, MPI_COMM_WORLD, &(requests[2*i]));\n133:     offset += length;\n134:   }\n135: \n136:   // Now send\n137:   offset = 0;\n138:   for (int i=0; i<num_neighbors; i++) {\n139:     int neighbor = A.neighbors[i];\n140:     int length = A.sendLength[i];\n141:     local_int_t * indices = A.elementsToSend+offset;\n142:     double * sendbuf = new double[length];\n143:     for (int j=0; j<length; j++) sendbuf[j] = x.values[indices[j]];\n144:     MPI_Isend(sendbuf, length, MPI_DOUBLE, neighbor, 0, MPI_COMM_WORLD,", "label": "(11, OpenMP, #pragma omp parallel for)"}
{"predict": "(12,OpenMP,#pragma omp parallel sections), (13,OpenMP,#pragma omp section), (17,OpenMP,#pragma omp section)", "label": "(38, MPI, MPI_Comm_disconnect(&parent);); (27, MPI, err = MPI_Intercomm_merge(inter, 0, &merge1);); (34, MPI, MPI_Comm_free(&intra);); (31, MPI, MPI_Comm_free(&merge1);); (6, MPI, err = MPI_Intercomm_merge(parent, 1, &intra);); (30, MPI, MPI_Comm_free(&inter);); (25, MPI, err = MPI_Barrier(merge1);); (18, MPI, err = MPI_Barrier(inter);); (21, MPI, MPI_Comm_rank(merge1, &rank);); (7, MPI, err = MPI_Intercomm_create(intra, 0, MPI_COMM_NULL, 0, tag, &inter);); (1, MPI, MPI_Comm_rank(intra, &rank);); (10, MPI, err = MPI_Intercomm_create(MPI_COMM_WORLD, 0, intra, 0, tag, &inter);)"}
{"predict": "/* free memory allocated for requests, statuses, and recv_buf */\n140:     ADIOI_Free(requests);\n141:     ADIOI_Free(statuses);\n142:     if (!buftype_is_contig)\n143:         ADIOI_Free(recv_buf[0]);\n144:     ADIOI_Free(recv_buf);\n145: }", "label": "(94, MPI, MPI_Type_commit(&send_type);); (137, MPI, MPI_Waitall(nprocs_send, requests + nprocs_recv, MPI_STATUSES_IGNORE);); (19, MPI, MPI_Alltoall(send_size, 1, MPI_COUNT, recv_size, 1, MPI_COUNT, fd->comm);); (118, MPI, MPI_Get_count_c(&statuses[j], MPI_BYTE, &count_recved);); (93, MPI, MPI_Type_free(&send_type);)"}
{"predict": "struct.kmp_depend_info* [[VLA1]], i{{.+}} 0, i{{.+}} 0\n65:   // CHECK: [[B_ADDR_CAST:%.+]] = ptrtoint i32* [[B_ADDR]] to i64\n66:   // CHECK: store i64 [[B_ADDR_CAST]], i64* [[BASE_ADDR]],\n67:   // CHECK: [[SIZE_ADDR:%.+]] = getelementptr inbounds %struct.kmp_depend_info, %struct.kmp_depend_info* [[VLA1]], i{{.+}} 0, i{{.+}} 1\n68:   // CHECK: store i64 [[SZ]], i64* [[SIZE_ADDR]],\n69:   // CHECK: [[FLAGS_ADDR:%.+]] = getelementptr inbounds %struct.kmp_depend_info, %struct.kmp_depend_info* [[VLA1]], i{{.+}} 0, i{{.+}} 2\n70:   // CHECK: store i8 1, i8* [[FLAGS_ADDR]],\n71:   // CHECK: [[DEPOBJ_SIZE:%.+]] = load i64, i64* [[DEPOBJ_SIZE_ADDR]]\n72:   // CHECK: [[DEPOBJ_SIZE1:%.+]] = load i64, i64* [[DEPOBJ_SIZE_ADDR1]]\n73:   // CHECK: [[SIZE:%.+]] = add nuw i64 [[DEPOBJ_SIZE]], [[DEPOBJ_SIZE1]]\n74:   // CHECK: [[SIZE1:%.+]] = add nuw i64 [[SIZE]], 2\n75:   // CHECK: [[DEPOBJ:%.+]] = call i8* @__kmpc_omp_allocate_depend(i64 [[SIZE1]])\n76:   // CHECK: [[DEPOBJ_CAST:%.+]] = bitcast i8* [[DEPOBJ]] to %struct.kmp_depend_info*\n77:   // CHECK: [[VLA:%.+]] = getelementptr %struct.kmp_depend_info, %struct.kmp_depend_info* [[DEPOBJ_CAST]], i64 0\n78:   // CHECK: [[VLA0:%.+]] = getelementptr %struct.kmp_depend_info, %struct.kmp_depend_info* [[VLA]], i64 0\n79:   // CHECK: [[", "label": "(101, OpenMP, #pragma omp task); (102, OpenMP, #pragma omp taskgroup)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(17, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize());); (15, CUDA, qmc_cuda::cuda_check(cudaGetLastError());)"}
{"predict": "(25,OpenMP,#pragma omp parallel for), (25,OpenMP,for(int i = 0; i < num_procs; i++))", "label": "(28, MPI, MPI_Finalize();); (8, MPI, MPI_Get_processor_name(mach_name,&mach_len);); (22, MPI, MPI_Allreduce(&local,&global,1,MPI_INT,MPI_SUM,MPI_COMM_WORLD);)"}
{"predict": "(2,OpenMP,#pragma omp parallel for), (10,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for)", "label": "(3, OpenMP, #pragma omp distribute parallel for if (true)); (11, OpenMP, #pragma omp distribute parallel for if (false)); (19, OpenMP, #pragma omp distribute parallel for if (Arg)); (-2, OpenMP, #pragma omp target); (-2, OpenMP, #pragma omp teams)"}
{"predict": "(4,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for)", "label": "(3, OpenMP, #pragma omp parallel for)"}
{"predict": "(6,OpenMP,#pragma omp parallel for)", "label": "(19, OpenCL, clReleaseEvent(event);)"}
{"predict": "smoke_check_3(H5AC_METADATA_WRITE_STRATEGY__DISTRIBUTED);\n168: #endif\n169: #if 1\n170:     smoke_check_4(H5AC_METADATA_WRITE_STRATEGY__PROCESS_0_ONLY);\n171:     smoke_check_4(H5AC_METADATA_WRITE_STRATEGY__DISTRIBUTED);\n172: #endif\n173: #if 1\n174:     smoke_check_5(H5AC_METADATA_WRITE_STRATEGY__PROCESS_0_ONLY);\n175:     smoke_check_5(H5AC_METADATA_WRITE_STRATEGY__DISTRIBUTED);\n176: #endif\n177: #if 1\n178:     smoke_check_6(H5AC_METADATA_WRITE_STRATEGY__PROCESS_0_ONLY);\n179:     smoke_check_6(H5AC_METADATA_WRITE_STRATEGY__DISTRIBUTED);\n180: #endif\n181: #if 1\n182:     smoke_check_7(H5AC_METADATA_WRITE_STRATEGY__PROCESS_0_ONLY);\n183:     smoke_check_7(H5AC_METADATA_WRITE_STRATEGY__DISTRIBUTED);\n184: #endif\n185: #if 1\n186:     smoke_check_8(H5AC_METADATA_WRITE_STRATEGY__PROCESS_0_ONLY);\n187:     smoke_check_8(H5AC_METADATA_WRITE_STRATEGY__DISTRIBUTED);\n188: #endif\n189: #if 1\n190:     smoke_check_9(H5AC_METADATA_WRITE_STRATEGY__PROCESS_0_ONLY);\n191:     smoke_check_9(H5AC_METADATA_WRITE_STRATEGY__DISTRIBUTED);\n192: #endif\n193: #if 1\n194:     smoke_check_10(H5AC_METADATA_WRITE_STRATEGY__PROCESS_0_ONLY);\n195:     smoke_check_10(H5AC_METADATA_WRITE_STRATEGY__DISTRIBUTED);\n196: #endif\n197: #if 1\n198:     smoke_check_11(H5AC_METADATA_WRITE_STRATEGY__PROCESS_0_ONLY);\n199:     smoke_check_11(H5AC_METADATA_WRITE_STRATEGY__DISTRIBUTED);\n200: #endif\n201: #if 1\n202", "label": "(10, MPI, MPI_Init(&argc, &argv);); (206, MPI, MPI_Finalize();); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (185, MPI, MPI_Barrier(MPI_COMM_WORLD);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);)"}
{"predict": "(9,OpenMP,#pragma omp parallel for)", "label": "(8, OpenMP, #pragma omp master taskloop lastprivate(t_var, vec, s_arr, s_arr, var, var))"}
{"predict": "(7,OpenMP,#pragma omp parallel for)\n(12,OpenMP,#pragma omp parallel for)\n(18,OpenMP,#pragma omp parallel for reduction(+:err))", "label": "(11, OpenMP, #pragma omp target teams distribute parallel for map(tofrom : a[ : n]) map(to : b[ : n]))"}
{"predict": "(14,OpenMP,#pragma omp parallel for)", "label": "(8, OpenMP, #pragma omp target update CLAUSE(present: i)); (6, OpenMP, #pragma omp target enter data map(alloc: i)); (7, OpenMP, #pragma omp target exit data map(delete: i))"}
{"predict": ": store i64 [[NUM_ITERS_VAL]], i64* [[UB]],\n74:   // CHECK: br label %[[FALSE]]\n75:   // CHECK: [[FALSE]]:\n76:   // CHECK: [[LB_VAL:%.+]] = load i64, i64* [[LB]],\n77:   // CHECK: [[UB_VAL:%.+]] = load i64, i64* [[UB]],\n78:   // CHECK: [[STRIDE_VAL:%.+]] = load i64, i64* [[STRIDE]],\n79:   // CHECK: [[CMP:%.+]] = icmp sle i64 [[LB_VAL]], [[UB_VAL]]\n80:   // CHECK: br i1 [[CMP]], label %[[TRUE:[^,]+]], label %[[FALSE:[^,]+]]\n81:   // CHECK: [[TRUE]]:\n82:   // CHECK: [[J_UB_MIN_VAL:%.+]] = load i32, i32* [[J_UB_MIN]],\n83:   // CHECK: [[J_UB_MAX_VAL:%.+]] = load i32, i32* [[J_UB_MAX]],\n84:   // CHECK: [[LB_VAL:%.+]] = load i64, i64* [[LB]],\n85:   // CHECK: [[J_UB_VAL:%.+]] = add nsw i32 [[J_UB_MIN_VAL]], trunc i64 [[LB_VAL]] to i32\n86:   // CHECK: [[CMP:%.+]] = icmp sgt i32 [[J_UB_VAL]], [[J_UB_MAX_VAL]]\n87:   // CHECK: br i1 [[CMP]], label %[[TRUE:[^,]+]], label %[[FALSE:[^,]+]]\n88:   // CHECK: [[TRUE]]:\n89:   // CHECK: store i32 [[J_UB_MAX_VAL]], i32* [[J_UB:%.+]],\n90:   // CHECK: br label %[[FALSE]]\n91:   // CHECK: [[FALSE]]:\n92:   // CHECK: [[J_UB_VAL:%.+]] = load i32, i32* [[J_UB]],\n93:   // CHECK: [[J_LB_MIN_VAL:%.+]] = load i32, i32* [[J_LB_MIN]],\n94:   //", "label": "(163, OpenMP, #pragma omp for collapse(2))"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (58,OpenMP,#pragma omp parallel for reduction(+:v)), (70,OpenMP,#pragma omp parallel for), (81,OpenMP,#pragma omp parallel for), (86,OpenMP,#pragma omp parallel for)", "label": "(38, OpenMP, #pragma omp distribute parallel for firstprivate(argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (78, OpenMP, #pragma omp parallel private(i)); (56, OpenMP, #pragma omp distribute parallel for firstprivate(e, g)); (30, OpenMP, #pragma omp distribute parallel for firstprivate(argc > 0 ? argv[1] : argv[2])); (68, OpenMP, #pragma omp distribute parallel for firstprivate(i)); (1, OpenMP, #pragma omp target); (57, OpenMP, #pragma omp distribute parallel for firstprivate(k, h)); (59, OpenMP, #pragma omp parallel); (40, OpenMP, #pragma omp distribute parallel for firstprivate(a, b)); (93, OpenMP, #pragma omp parallel reduction(+ : i)); (3, OpenMP, #pragma omp distribute parallel for firstprivate(); (71, OpenMP, #pragma omp distribute parallel for firstprivate(j)); (41, OpenMP, #pragma omp distribute parallel for firstprivate(argv[1])); (5, OpenMP, #pragma omp distribute parallel for firstprivate()); (9, OpenMP, #pragma omp distribute parallel for firstprivate(argc); (78, OpenMP, #pragma omp distribute parallel for lastprivate(g) firstprivate(g)); (27, OpenMP, #pragma omp distribute parallel for firstprivate(S1)); (11, OpenMP, #pragma omp distribute parallel for firstprivate(argc,); (60, OpenMP, #pragma omp parallel shared(i)); (-11, OpenMP, #pragma omp distribute parallel for firstprivate); (-13, OpenMP, #pragma omp teams)"}
{"predict": "(19,OpenMP,#pragma omp parallel for private(errs) reduction(+:num_threads_obtained))", "label": "(25, MPI, MPI_Comm_free(&comms[x]);); (18, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &comms[x]);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for)", "label": "(5, OpenCL, clReleaseContext_ExpectAndReturn(make_context(0), CL_SUCCESS);); (3, OpenCL, clCreateContext_StubWithCallback(clCreateContext_testContextNonNullProperties);)"}
{"predict": "(36,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for), (38,OpenMP,#pragma omp parallel for)", "label": "(5, OpenMP, #pragma omp sections firstprivate(); (24, OpenMP, #pragma omp sections firstprivate()); (35, OpenMP, #pragma omp sections firstprivate(0)); (51, OpenMP, #pragma omp sections lastprivate(x, y, z) firstprivate(x, y, z)); (45, OpenMP, #pragma omp sections lastprivate(x, y) firstprivate(x, y)); (39, OpenMP, #pragma omp sections lastprivate(x) firstprivate(x)); (-4, OpenMP, #pragma omp parallel); (6, OpenMP, #pragma omp sections firstprivate(,); (11, OpenMP, #pragma omp sections firstprivate(, )); (22, OpenMP, #pragma omp sections firstprivate(int))"}
{"predict": "(4,OpenMP,#pragma omp parallel for)", "label": "(10, OpenMP, #pragma omp target update to(i) nowait (3.14) device (-10u)); (3, OpenMP, #pragma omp target nowait update to(i)); (7, OpenMP, #pragma omp target update to(i) nowait device (-10u)); (5, OpenMP, #pragma omp target update to(i) nowait (argc))); (7, OpenMP, #pragma omp target update to(i) nowait nowait); (7, OpenMP, #pragma omp target update nowait to(i) nowait); (-3, OpenMP, #pragma omp nowait target update to(i)); (-1, OpenMP, #pragma omp target update nowait() to(i)); (-1, OpenMP, #pragma omp target update to(i) nowait()"}
{"predict": "(14,OpenMP,#pragma omp parallel for)", "label": "(7, OpenACC, #pragma acc loop gang worker vector); (5, OpenACC, #pragma acc parallel deviceptr(position,cell_keys))"}
{"predict": "(14,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for)", "label": "(6, TBB, tbb::flow::queue_node< q3_input_type >  q3(g);); (11, TBB, ASSERT(tbb::flow::input_port<0>(my_indexer).predecessor_count() == 0, NULL);); (7, TBB, tbb::flow::make_edge(my_indexer, q3);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(5, TBB, tbb::atomic<bool> taskDoneFlag;); (85, TBB, tbb::task::enqueue(*t, p);); (9, TBB, tbb::task::enqueue(*w);); (13, TBB, FFTask* t = new( tbb::task::allocate_root() ) FFTask(&flag);); (81, TBB, const tbb::priority_t p = getPriorityByInt(i);); (98, TBB, tbb::atomic<int> counter;); (11, TBB, tbb::task::enqueue(*t);); (108, TBB, tbb::atomic<bool> signalToLeave;); (-4, TBB, tbb::atomic<int> flag;); (1, TBB, WaiterTask* w = new( tbb::task::allocate_root() ) WaiterTask(&taskDoneFlag);); (3, TBB, tbb::global_control c(tbb::global_control::max_allowed_parallelism, 1);)"}
{"predict": "(27,OpenMP,#pragma omp parallel)", "label": "(8, MPI, MPI_Comm_rank(comm, &par_rank);); (24, MPI, open_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (7, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(6,OpenMP,#pragma omp parallel for)", "label": "(5, OpenMP, #pragma omp workshare)"}
{"predict": "(9,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for)", "label": "(6, OpenCL, retVal = clEnqueueMemcpyINTEL(this->pCmdQ, true, unifiedMemoryPtr, input.data(), dataSize, 0, nullptr, nullptr);)"}
{"predict": "i = 0; i < 10; ++i)\n83:     foo();\n84:   for (int i = 0; i < 10; ++i)\n85:     foo();\n86:   for (int i = 0; i < 10; ++i)\n87:     foo();\n88:   for (int i = 0; i < 10; ++i)\n89:     foo();\n90:   for (int i = 0; i < 10; ++i)\n91:     foo();\n92:   for (int i = 0; i < 10; ++i)\n93:     foo();\n94:   for (int i = 0; i < 10; ++i)\n95:     foo();\n96:   for (int i = 0; i < 10; ++i)\n97:     foo();\n98:   for (int i = 0; i < 10; ++i)\n99:     foo();\n100:   for (int i = 0; i < 10; ++i)\n101:     foo();\n102:   for (int i = 0; i < 10; ++i)\n103:     foo();\n104:   for (int i = 0; i < 10; ++i)\n105:     foo();\n106:   for (int i = 0; i < 10; ++i)\n107:     foo();\n108:   for (int i = 0; i < 10; ++i)\n109:     foo();\n110:   for (int i = 0; i < 10; ++i)\n111:     foo();\n112:   for (int i = 0; i < 10; ++i)\n113:     foo();\n114:   for (int i = 0; i < 10; ++i)\n115:     foo();\n116:   for (int i = 0; i < 10; ++i)\n117:     foo();\n118:   for (int i = 0; i <", "label": "(15, OpenMP, #pragma omp master taskloop reduction +); (65, OpenMP, #pragma omp master taskloop reduction(- : da)); (61, OpenMP, #pragma omp master taskloop reduction(* : ca)); (54, OpenMP, #pragma omp master taskloop reduction(max : h.b)); (47, OpenMP, #pragma omp master taskloop reduction(+ : a, b, c, d, f)); (55, OpenMP, #pragma omp master taskloop reduction(+ : ba)); (30, OpenMP, #pragma omp master taskloop reduction(| : argc,); (84, OpenMP, #pragma omp master taskloop reduction(+ : p), reduction(+ : p)); (46, OpenMP, #pragma omp master taskloop reduction(min : a, b, c, d, f)); (33, OpenMP, #pragma omp master taskloop reduction(~ : argc)); (38, OpenMP, #pragma omp master taskloop reduction(^ : S1)); (73, OpenMP, #pragma omp master taskloop reduction(+ : o)); (90, OpenMP, #pragma omp master taskloop reduction(max : j)); (11, OpenMP, #pragma omp master taskloop reduction()); (95, OpenMP, #pragma omp parallel reduction(* : fl)); (63, OpenMP, #pragma omp master taskloop reduction(& : e, g)); (65, OpenMP, #pragma omp master taskloop reduction(+ : h, k, B::x)); (73, OpenMP, #pragma omp parallel private(k)); (96, OpenMP, #pragma omp master taskloop reduction(+ : m)); (20, OpenMP, #pragma omp master taskloop reduction(|| : argc > 0 ? argv[1] : argv[2])); (-2, OpenMP, #pragma omp master taskloop reduction(); (24, OpenMP, #pragma omp master taskloop reduction(&& : argc, z)); (53, OpenMP, #pragma omp master taskloop reduction(&& : S2::S2sc)); (46, OpenMP, #pragma omp master taskloop reduction(^ : fl)); (6, OpenMP, #pragma omp master taskloop reduction(\\)); (-13, OpenMP, #pragma omp master taskloop reduction); (7, OpenMP, #pragma omp master taskloop reduction(foo : argc); (74, OpenMP, #pragma omp parallel reduction(min : i)); (77, OpenMP, #pragma omp parallel private(fl)); (91, OpenMP, #pragma omp master taskloop reduction(task, + : m)); (-9, OpenMP, #pragma omp master taskloop reduction(-); (86, OpenMP, #pragma omp master taskloop nogroup reduction(+ : m)); (65, OpenMP, #pragma omp master taskloop reduction(+ : r)); (-6, OpenMP, #pragma omp master taskloop reduction(*)); (53, OpenMP, #pragma omp master taskloop private(i), reduction(+ : j), reduction(+ : q)); (65, OpenMP, #pragma omp parallel shared(i)); (36, OpenMP, #pragma omp master taskloop reduction(&& : S2::S2s)); (69, OpenMP, #pragma omp master taskloop reduction(+ : fl))"}
{"predict": "", "label": "(34, CUDA, __shared__ int pointsNumSM[4];); (30, CUDA, __shared__ float4 pillarSM[4][64];); (30, CUDA, __shared__ float4 pillarSumSM[4];); (30, CUDA, __shared__ int4 cordsSM[4];); (31, CUDA, __shared__ float pillarOutSM[4][64][11];)"}
{"predict": "(2,OpenMP,#pragma omp parallel)", "label": "(1, OpenMP, omp_allocator_handle_t default_alloc = omp_get_default_allocator();); (4, OpenMP, int *p = (int *)omp_alloc(123456*sizeof(int));)"}
{"predict": "]]\n75:     // CHECK: store float [[RED]], float* [[A_BUF_IDX_ELEM]],\n76:     // CHECK: [[A_BUF_IDX_SUB_K2POW_NEXT:%.+]] = getelementptr float, float* [[A_BUF_IDX_SUB_K2POW_ELEM]], i64 1\n77:     // CHECK: [[A_BUF_IDX_NEXT:%.+]] = getelementptr float, float* [[A_BUF_IDX_ELEM]], i64 1\n78:     // CHECK: [[ISEMPTY:%.+]] = icmp eq float* [[A_BUF_IDX_SUB_K2POW_NEXT]], [[A_BUF_END]]\n79:     // CHECK: br i1 [[ISEMPTY]], label %[[RED_DONE]], label %[[RED_BODY]]\n80:     // CHECK: [[RED_DONE]]:\n81:     // CHECK: [[B_BUF_IDX_SUB_K2POW_ELEM:%.+]] = phi double* [ [[B_BUF_IDX_SUB_K2POW]], %[[INNER_BODY]] ], [ [[B_BUF_IDX_SUB_K2POW_NEXT:%.+]], %[[RED_BODY]] ]\n82:     // CHECK: [[B_BUF_IDX_ELEM:%.+]] = phi double* [ [[B_BUF_IDX]], %[[INNER_BODY]] ], [ [[B_BUF_IDX_NEXT:%.+]], %[[RED_BODY]] ]\n83:     // CHECK: [[B_BUF_IDX_VAL:%.+]] = load double, double* [[B_BUF_IDX_ELEM]],\n84:     // CHECK: [[B_BUF_IDX_SUB_K2POW_VAL:%.+]] = load double, double* [[B_BUF_IDX_SUB_K2POW_ELEM]],\n85:     // CHECK: [[RED:%.+]] = fadd double [[B_BUF_IDX_VAL]], [[B_BUF_IDX_SUB_K2POW_VAL]]\n86:     // CHECK: store double [[RED]], double* [[B_BUF_IDX_ELEM]],\n87:     // CHECK: [[B_BUF_IDX_SUB_K2POW_NEXT:%.+]] = getelementptr double, double* [[B_BUF_IDX_SUB_K2POW_ELEM]], i64 1\n88:     // CHECK: [[B_BUF_IDX_NEXT:%.+]] = getelementptr double, double* [[B_BUF_IDX_ELEM]], i64 1\n89:     // CHECK: br label %[[INNER_EXIT]]\n90:     // CHECK: [[INNER_EXIT]]:\n91:     // CHECK: [[K", "label": "(194, OpenMP, #pragma omp scan exclusive(a[:n], b)); (44, OpenMP, #pragma omp scan inclusive(a[:n], b)); (9, OpenMP, #pragma omp for reduction(inscan, +:a[:n], b))"}
{"predict": "(13,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (36,OpenMP,#pragma omp parallel for), (58,OpenMP,#pragma omp parallel for), (80,OpenMP,#pragma omp parallel for)", "label": "(11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &np);); (68, MPI, err = MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_WRONLY, MPI_INFO_NULL, &fh);); (73, MPI, err = MPI_File_write_all(fh, buf, buflen, MPI_INT, &status);); (29, MPI, err = MPI_File_close(&fh);); (60, MPI, MPI_Type_commit(&filetype);); (24, MPI, err = MPI_File_write(fh, buf, MAXLEN, MPI_INT, &status);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (65, MPI, err = MPI_File_set_view(fh, 0, MPI_INT, filetype, \"native\", MPI_INFO_NULL);); (45, MPI, MPI_Type_create_hindexed(num, blocklen, addrs, MPI_INT, &filetype);); (76, MPI, err = MPI_File_open(MPI_COMM_SELF, filename, MPI_MODE_RDONLY, MPI_INFO_NULL, &fh);); (40, MPI, MPI_Type_indexed(num, blocklen, indices, MPI_INT, &filetype);); (67, MPI, MPI_Type_free(&filetype);); (76, MPI, err = MPI_File_read(fh, buf, MAXLEN, MPI_INT, &status);); (23, MPI, MPI_Barrier(MPI_COMM_WORLD);); (42, MPI, MPI_Type_create_struct(num, blocklen, addrs, types, &filetype);)"}
{"predict": "(20,OpenMP,#pragma omp parallel for)", "label": "(5, OpenMP, #pragma omp parallel sections default()); (2, OpenMP, #pragma omp parallel sections default(); (7, OpenMP, #pragma omp parallel sections default(shared), default(shared)); (4, OpenMP, #pragma omp parallel sections default(none); (7, OpenMP, #pragma omp parallel sections default(x)); (16, OpenMP, #pragma omp parallel sections default(none)); (-5, OpenMP, #pragma omp parallel sections default); (21, OpenMP, #pragma omp parallel sections default(shared))"}
{"predict": "i;\n111:         int store_c_gmem_n = bx * BN + tx * TN / 2;\n112:         int store_c_gmem_addr = OFFSET(store_c_gmem_m, store_c_gmem_n, N);\n113:         FLOAT4(c[store_c_gmem_addr]) = FLOAT4(r_c[i + TM / 2][0]);\n114:         FLOAT4(c[store_c_gmem_addr + BN / 2]) = FLOAT4(r_c[i + TM / 2][4]);\n115:     }\n116: }\n\n### ** Analysis and Parallelization Recommendations **\n\nThis code is already optimized for GPU execution using CUDA. The function `sgemm_V3` is a kernel function for matrix multiplication (SGEMM) and is designed to run on NVIDIA GPUs. It utilizes shared memory and block-level parallelism to optimize performance.\n\nHowever, if we are considering further optimizations or parallelization in a different context, such as CPU execution, we can look at the loops and data structures that might benefit from parallelization techniques like OpenMP.\n\nFor this specific code, no additional parallelization instructions are needed for GPU execution. However, if we were to run this code on a CPU, we could consider parallelizing the outermost loop of the matrix multiplication using OpenMP.\n\n### ** Parallelization Instructions for CPU Execution (Hypothetical) **\n\n(47,OpenMP,#pragma omp parallel for private(smem_sel, smem_sel_next, load_a_gmem_k, load_a_gmem_addr, load_b_gmem_k, load_b_gmem_addr, r_load_a, r_load_b))\n\n### ** Final Output for GPU Execution **\n\nNo additional parallelization instructions are needed for GPU execution.\n\n### ** Final Output for CPU Execution (Hypothetical) **\n\n(47,OpenMP,#pragma omp parallel for private(smem_sel, smem_sel_next, load_a_gmem_k, load_a_gmem_addr, load_b_gmem_k, load_b_gmem_addr, r_load_a, r_load_b))\n\n### ** Final Output for GPU Execution (No Changes Needed) **\n\nNo additional parallelization instructions are needed for GPU execution.\n\n### ** Final Output for CPU Execution (Hypothetical) **\n\n(47,OpenMP,#pragma omp parallel for private(smem_sel, smem_sel_next, load_a_gmem_k, load_a_gmem_addr, load_b_gmem_k, load", "label": "(16, CUDA, __shared__ float s_a[2][BK][BM];); (16, CUDA, __shared__ float s_b[2][BK][BN];)"}
{"predict": "(21,OpenMP,#pragma omp parallel private(thread_id))", "label": "(17, OpenMP, volatile  int_t thread_id = omp_get_thread_num();)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(8, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (9, TBB, tbb::tick_count t1 = tbb::tick_count::now();); (7, TBB, tbb::parallel_for(tbb::blocked_range<size_t>(0,n,n/100),TwiddleInvariant<Invariant<M,8> >(invariant));)"}
{"predict": "(18,OpenMP,#pragma omp parallel for), (39,OpenMP,#pragma omp parallel for)", "label": "(19, MPI, MPI_Info_set(info, \"cb_nodes\", \"1\");); (25, MPI, errcode = MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_WRONLY | MPI_MODE_CREATE, info, &fh);); (27, MPI, errcode = MPI_File_iwrite_at_all(fh, off, buffer, BUFSIZE, MPI_INT, &request);); (30, MPI, errcode = MPI_File_close(&fh);); (28, MPI, MPI_Wait(&request, &status);); (12, MPI, MPI_Info_create(&info);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (33, MPI, errcode = MPI_File_iread_at_all(fh, off, buf2, BUFSIZE, MPI_INT, &request);); (10, MPI, MPI_Info_set(info, \"romio_cb_write\", \"enable\");); (43, MPI, MPI_Info_free(&info);); (27, MPI, errcode = MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_RDONLY, info, &fh);)"}
{"predict": "(14,OpenMP,#pragma omp parallel private(j,iter,scalar) shared(length,iterations,nstream_time,a,b,c) num_threads(nthread_input))", "label": "(109, OpenMP, #pragma omp barrier); (75, OpenMP, nthread = omp_get_num_threads();); (92, OpenMP, #pragma omp for); (71, OpenMP, #pragma omp master); (68, OpenMP, #pragma omp parallel private(j,iter))"}
{"predict": "(16,OpenMP,#pragma omp parallel for),", "label": "(14, OpenMP, #pragma omp target parallel for)"}
{"predict": "(18,OpenMP,#pragma omp parallel for)", "label": "(16, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &scri_rank);); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &scri_ranks);)"}
{"predict": "] = 4.3f / formatMax;\n121:                         inputValues[ i++ ] = 4.5f / formatMax;\n122: inputValues[ i++ ] = 4.7f / formatMax;\n123:                         inputValues[ i++ ] = 5.0f / formatMax;\n124:                         inputValues[ i++ ] = 5.3f / formatMax;\n125:                         inputValues[ i++ ] = 5.5f / formatMax;\n126:                         inputValues[ i++ ] = 5.7f / formatMax;\n127:                     }\n128:                 }\n129:             }\n130:         }\n131:         else\n132:         {\n133:             // Mipmap case\n134:             // First, fill with arbitrary floats\n135:             {\n136:                 float *inputValues = (float *)(char*)imageValues;\n137:                 for( size_t i = 0; i < imageInfo->width * 4; i++ )\n138:                     inputValues[ i ] = get_random_float( -0.1f, 1.1f, d );\n139:             }\n140: \n141:             // Throw a few extra test values in there\n142:             float *inputValues = (float *)(char*)imageValues;\n143:             size_t i = 0;\n144:             inputValues[ i++ ] = -0.0000000000009f;\n145:             inputValues[ i++ ] = 1.f;\n146:             inputValues[ i++ ] = -1.f;\n147:             inputValues[ i++ ] = 2.f;\n148: \n149:             // Also fill in the first few vectors with some deliberate tests to determine the rounding mode\n150:             // is correct\n151:             if( imageInfo->width > 12 )\n152:             {\n153:                 float formatMax = (float)get_format_max_int( imageInfo->format );\n154:                 inputValues[ i++ ] = 4.0f / formatMax;\n1", "label": "(242, OpenCL, error = clSetKernelArg( kernel, 0, sizeof( cl_mem ), &inputStream );); (228, OpenCL, error = clSetKernelArg( kernel, 2, sizeof( int ), &lod );); (215, OpenCL, error = clSetKernelArg( kernel, 1, sizeof( cl_mem ), &image );); (244, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 1, NULL, threads, NULL, 0, NULL, NULL );); (260, OpenCL, error = clEnqueueReadImage( queue, image, CL_TRUE, origin, region, gEnablePitch ? imageInfo->rowPitch : 0, 0, resultValues, 0, NULL, NULL );)"}
{"predict": "->d_Lindval_loc_bc_dat));\n129:     checkGPU (gpuFree (Llu->d_Lindval_loc_bc_offset));\n130:     checkGPU (gpuFree (Llu->d_Unzval_br_ptr));\n131:     checkGPU (gpuFree (Llu->d_Unzval_br_dat));\n132:     checkGPU (gpuFree (Llu->d_Unzval_br_offset));\n133:     checkGPU (gpuFree (Llu->d_Ucb_indptr));\n134:     checkGPU (gpuFree (Llu->d_Ucb_valptr));\n135:     checkGPU (gpuFree (Llu->d_Urbs));\n136: }\n137: #endif\n138: \n139:     /* Free global working space. */\n140:     SUPERLU_FREE(Llu->xwork);\n141:     SUPERLU_FREE(Llu->lwork);\n142:     SUPERLU_FREE(Llu->Ucb_work);\n143:     SUPERLU_FREE(Llu->Lcb_work);\n144:     SUPERLU_FREE(Llu->indirect);\n145:     SUPERLU_FREE(Llu->Uindirect);\n146:     SUPERLU_FREE(Llu->Lindirect);\n147: \n148:     /* Free global working space. */\n149:     SUPERLU_FREE(Llu->fpanel_cnt);\n150:     SUPERLU_FREE(Llu->bpanel_cnt);\n151:     SUPERLU_FREE(Llu->fsendx);\n152:     SUPERLU_FREE(Llu->bsendx);\n153:     SUPERLU_FREE(Llu->frecvbuf);\n154:     SUPERLU_FREE(Llu->brecvbuf);\n155:     SUPERLU_FREE(Llu->tempv);\n156:     SUPERLU_FREE(Llu->tempu);\n157:     SUPERLU_FREE(Llu->temp3);\n158:     SUPERLU_FREE(Llu->temp4);\n159:     SUPERLU_FREE(Llu->temp5);\n160:     SUPERLU_FREE(Llu->temp6);\n161:     SUPERLU_FREE(Llu->temp7);\n162:     SUPERLU_FREE(Llu->temp8);\n163:     SUPERLU_FREE(L", "label": "(9, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &iam );)"}
{"predict": "(32,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for)", "label": "(36, MPI, MPI_File_close(&c_info->fh);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for)", "label": "(8, OpenMP, #pragma omp parallel); (10, OpenMP, #pragma omp for); (14, OpenMP, #pragma omp critical)"}
{"predict": "(57,OpenMP,#pragma omp parallel), (59,OpenMP,#pragma omp taskloop firstprivate(length) shared(A,B,C) grainsize(gs)), (71,OpenMP,#pragma omp taskloop firstprivate(length) shared(A,B,C) grainsize(gs)), (94,OpenMP,#pragma omp parallel for reduction(+:asum))", "label": "(38, OpenMP, std::cout << \"Number of threads    = \" << omp_get_max_threads() << std::endl;)"}
{"predict": "(25,OpenMP,#pragma omp parallel for)", "label": "(32, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (34, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(8,SYCL,cgh.parallel_for(sycl::range<2>{Ndim, Pdim}, [=](sycl::id<2> idx) {), (17,SYCL,cgh.parallel_for(sycl::range<2>{Pdim, Mdim}, [=](sycl::id<2> idx) {)", "label": "(16, SYCL, auto b = B.get_access<sycl::access_mode::write>(cgh);); (5, SYCL, auto a = A.get_access<sycl::access_mode::write>(cgh);)"}
{"predict": "The provided code is a destructor for a class template `HerPerformanceTest` and is responsible for deallocating memory. There are no loops or computationally intensive sections in this code that would benefit from parallelization. Therefore, there are no appropriate places to insert parallelization instructions in this specific code snippet.\n\nHowever, if you have a different section of the code that involves loops or other parallelizable operations, please provide that section, and I can analyze it for parallelization opportunities.", "label": "(20, OpenCL, clReleaseMemObject(mobjA_);); (16, OpenCL, clReleaseMemObject(mobjX_);)"}
{"predict": "(22,OpenMP,#pragma omp parallel for) (28,OpenMP,#pragma omp parallel for) (39,OpenMP,#pragma omp parallel for) (45,OpenMP,#pragma omp parallel for) (67,OpenMP,#pragma omp parallel for) (72,OpenMP,#pragma omp parallel for)", "label": "(4, TBB, tbb::flow::queue_node<T> q2(g);); (70, TBB, tbb::flow::make_edge( q2, q3 );); (67, TBB, tbb::flow::queue_node<T> q3(g);); (37, TBB, tbb::flow::make_edge( q, q2 );); (56, TBB, tbb::flow::remove_edge( q, q2 );); (87, TBB, tbb::flow::remove_edge( q,  q2 );); (-3, TBB, tbb::flow::queue_node<T> q(g);); (-5, TBB, tbb::flow::graph g;)"}
{"predict": "(26,OpenMP,#pragma omp parallel for)", "label": "(14, OpenCL, cl_command_queue cmdQ = clCreateCommandQueueWithProperties(context.get(), clDevice, properties, &retVal);); (38, OpenCL, retVal = clReleaseCommandQueue(cmdQ);); (1, OpenCL, clGetDeviceInfo(clDevice, CL_DEVICE_SLICE_COUNT_INTEL, sizeof(size_t), &maxSliceCount, nullptr);)"}
{"predict": "0);\n93:             const tfloat3 velp2=TFloat3(velrhop2.x,velrhop2.y,velrhop2.z);\n94:             const tfloat3 dr=TFloat3(drx,dry,drz);\n95:             const tfloat3 dv=velp2-velp1;\n96:             const float dv2=dv.x*dv.x+dv.y*dv.y+dv.z*dv.z;\n97:             const float r2=rr2+dv2;\n98:             const float rh2=rhopp1*velrhop2.w;\n99:             const float velp22=velp2.x*velp2.x+velp2.y*velp2.y+velp2.z*velp2.z;\n100:            const float velp12=velp1.x*velp1.x+velp1.y*velp1.y+velp1.z*velp1.z;\n101:            const float fvel=fsph::GetKernel_Vel<tker>(CSP,r2,velp22,velp12);\n102:            const float velp22_r2=velp22/r2;\n103:            const float velp12_r2=velp12/r2;\n104:            const float velp22_r2_rh2=velp22_r2/rh2;\n105:            const float velp12_r2_rh2=velp12_r2/rh2;\n106:            const float velp22_r2_rh2_dv2=velp22_r2_rh2*dv2;\n107:            const float velp12_r2_rh2_dv2=velp12_r2_rh2*dv2;\n108:            const float fvel_rh2=fvel/rh2;\n109:            const float fvel_rh2_dv2=fvel_rh2*dv2;\n110:            const float fvel_rh2_dv2_rh2=fvel_rh2_dv2/rh2;\n111:            const float fvel_rh2_dv2_rh2_dv2=fvel_r", "label": "(205, OpenMP, const int th=omp_get_thread_num();); (19, OpenMP, #pragma omp parallel for schedule (guided))"}
{"predict": "(43,OpenMP,#pragma omp parallel for), (62,OpenMP,#pragma omp parallel for)", "label": "(7, MPI, MPI_Init(&argc, &argv);); (85, MPI, MPI_Finalize();); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (57, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(18,OpenMP,#pragma omp parallel for private(idStart))", "label": "(14, CUDA, os << \"__device__ \";); (8, CUDA, os << \"__device__ __constant__ \";)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for)", "label": "(12, OpenMP, #pragma omp target teams); (9, OpenMP, #pragma omp target update to(vec.v[0:N])); (16, OpenMP, #pragma omp target update from(vec.v[0:N])); (10, OpenMP, #pragma omp distribute parallel for)"}
{"predict": "(11,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for reduction(+:sum))", "label": "(12, OpenMP, #pragma omp target teams distribute parallel for)"}
{"predict": "The provided code snippet is a unit test function and does not contain any loops or computationally intensive tasks that would benefit from parallelization. Therefore, there are no appropriate places to insert parallelization instructions in this specific code.\n\nHowever, if we hypothetically assume that there is a loop or a section of code that could be parallelized, I would need to see that section to provide specific line numbers and instructions. Since there are none, the output will be empty:\n\n()", "label": "(21, OpenCL, retVal = clReleaseMemObject(subBuffer);); (26, OpenCL, retVal = clReleaseMemObject(notUsedBuffer);); (28, OpenCL, EXPECT_EQ(clReleaseContext(context), CL_SUCCESS);); (-2, OpenCL, cl_mem notUsedBuffer = clCreateBuffer(clContext, flags, size, hostPtr, &retVal);); (20, OpenCL, retVal = clReleaseMemObject(buffer);); (0, OpenCL, cl_mem buffer = clCreateBuffer(clContext, flags, size, hostPtr, &retVal);); (8, OpenCL, cl_mem subBuffer = clCreateSubBuffer(buffer, flags, CL_BUFFER_CREATE_TYPE_REGION, &region, &retVal);)"}
{"predict": "10, \"Reduce of MPI_BYTE\\n\");\n116:     ucinbuf[0] = 0xff;\n117:     ucinbuf[1] = 0;\n118:     ucinbuf[2] = (rank > 0) ? 0x3c : 0xc3;\n119: \n120:     ucoutbuf[0] = 0;\n121:     ucoutbuf[1] = 1;\n122:     ucoutbuf[2] = 1;\n123:     if (rc) {\n124:         MTestPrintErrorMsg(\"MPI_BXOR and MPI_BYTE\", rc);\n125:         errs++;\n126:     } else {\n127:         if (rank == 0) {\n128:             if (ucoutbuf[0] != ((size % 2) ? 0xff : 0)) {\n129:                 errs++;\n130:                 fprintf(stderr, \"byte BXOR(1) test failed\\n\");\n131:             }\n132:             if (ucoutbuf[1]) {\n133:                 errs++;\n134:                 fprintf(stderr, \"byte BXOR(0) test failed\\n\");\n135:             }\n136:             if (ucoutbuf[2] != ((size % 2) ? (unsigned char) 0xc3 : (unsigned char) 0xff)) {\n137:                 errs++;\n138:                 fprintf(stderr, \"byte BXOR(>) test failed\\n\");\n139:             }\n140:         }\n141:     }\n142: \n143:     /* short */\n144:     MTestPrintfMsg(10, \"Reduce of MPI_SHORT\\n\");\n145:     sinbuf[0] = 0xff;\n146:     sinbuf[1] = 0;\n147:     sinbuf[2] = (rank > 0) ? 0x3c : 0xc3;\n148: \n149:     soutbuf[0] = 0xf;\n150:     soutbuf[1] = 1;\n151:     soutbuf[2] = 1;\n152:", "label": "(25, MPI, MPI_Comm_size(comm, &size);); (97, MPI, rc = MPI_Reduce(ucinbuf, ucoutbuf, 3, MPI_UNSIGNED_CHAR, MPI_BXOR, 0, comm);); (156, MPI, rc = MPI_Reduce(sinbuf, soutbuf, 3, MPI_SHORT, MPI_BXOR, 0, comm);); (125, MPI, rc = MPI_Reduce(cinbuf, coutbuf, 3, MPI_BYTE, MPI_BXOR, 0, comm);); (274, MPI, rc = MPI_Reduce(linbuf, loutbuf, 3, MPI_LONG, MPI_BXOR, 0, comm);); (63, MPI, rc = MPI_Reduce(scinbuf, scoutbuf, 3, MPI_SIGNED_CHAR, MPI_BXOR, 0, comm);); (31, MPI, rc = MPI_Reduce(cinbuf, coutbuf, 3, MPI_CHAR, MPI_BXOR, 0, comm);); (335, MPI, rc = MPI_Reduce(llinbuf, lloutbuf, 3, MPI_LONG_LONG, MPI_BXOR, 0, comm);); (14, MPI, MPI_Comm_set_errhandler(comm, MPI_ERRORS_RETURN);); (299, MPI, rc = MPI_Reduce(ulinbuf, uloutbuf, 3, MPI_UNSIGNED_LONG, MPI_BXOR, 0, comm);); (238, MPI, rc = MPI_Reduce(iinbuf, ioutbuf, 3, MPI_INT, MPI_BXOR, 0, comm);); (13, MPI, MPI_Comm_rank(comm, &rank);); (206, MPI, rc = MPI_Reduce(uinbuf, uoutbuf, 3, MPI_UNSIGNED, MPI_BXOR, 0, comm);); (175, MPI, rc = MPI_Reduce(usinbuf, usoutbuf, 3, MPI_UNSIGNED_SHORT, MPI_BXOR, 0, comm);)"}
{"predict": "(36,OpenMP,#pragma omp parallel for), (45,OpenMP,#pragma omp parallel for), (47,OpenMP,#pragma omp parallel for), (52,OpenMP,#pragma omp parallel for)", "label": "(19, CUDA, ColorSpinorParam cudaParam(cpuParam, *inv_param, QUDA_CUDA_FIELD_LOCATION);)"}
{"predict": "l_self_test_set(&dev_ctx_xl, LSM303AH_XL_ST_DISABLE);\n143: }", "label": "(145, OpenMP, lsm303ah_mg_offset_temp_comp_set(&dev_ctx_mg, PROPERTY_ENABLE);)"}
{"predict": "(42,OpenMP,#pragma omp parallel sections), (44,OpenMP,#pragma omp section), (46,OpenMP,#pragma omp section), (47,OpenMP,#pragma omp section), (50,OpenMP,#pragma omp section), (51,OpenMP,#pragma omp section), (54,OpenMP,#pragma omp section), (57,OpenMP,#pragma omp section), (58,OpenMP,#pragma omp section), (61,OpenMP,#pragma omp section), (62,OpenMP,#pragma omp section), (64,OpenMP,#pragma omp section), (66,OpenMP,#pragma omp section), (68,OpenMP,#pragma omp section), (70,OpenMP,#pragma omp section), (72,OpenMP,#pragma omp section), (75,OpenMP,#pragma omp section), (76,OpenMP,#pragma omp section), (78,OpenMP,#pragma omp section), (79,OpenMP,#pragma omp section), (80,OpenMP,#pragma omp section), (83,OpenMP,#pragma omp section), (84,OpenMP,#pragma omp section), (85,OpenMP,#pragma omp section), (94,OpenMP,#pragma omp section), (101,OpenMP,#pragma omp section), (102,OpenMP,#pragma omp section), (103,OpenMP,#pragma omp section), (104,OpenMP,#pragma omp section), (106,OpenMP,#pragma omp section), (107,OpenMP,#pragma omp section), (108,OpenMP,#pragma omp section), (109,OpenMP,#pragma omp section), (116,OpenMP,#pragma omp section), (117,OpenMP,#pragma omp section), (120,OpenMP,#pragma omp section)", "label": "(87, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (87, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)"}
{"predict": "(16,OpenMP,#pragma omp parallel for)", "label": "(22, CUDA, nvinfer1::ICudaEngine* engine = builder->buildCudaEngine(*network);)"}
{"predict": "(30,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for), (64,OpenMP,#pragma omp parallel for)", "label": "(58, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(streams[0]), &streams[0]);); (58, OpenCL, err |= clSetKernelArg(kernel, 1, sizeof(streams[1]), &streams[1]);); (58, OpenCL, err |= clSetKernelArg(kernel, 2, sizeof(sampler), &sampler);); (33, OpenCL, streams[1] = clCreateBuffer(context, CL_MEM_READ_WRITE, length, NULL, &err);)"}
{"predict": "(39,OpenMP,#pragma omp parallel for)", "label": "(41, CUDA, cudaDeviceSynchronize();)"}
{"predict": "(3,OpenMP,#pragma omp parallel for private(params, result))", "label": "(7, OpenMP, #pragma omp atomic)"}
{"predict": "(14,OpenMP,#pragma omp parallel for)", "label": "(21, CUDA, CUDA_SAFE_CALL(cudaMemcpy(&max, tempCfl, sizeof(float), cudaMemcpyDeviceToHost));)"}
{"predict": "1:             auto prob = Measurer.probs(std::vector<std::size_t>({0, 1, 2}));\n142: \n143:             REQUIRE_THAT(prob_obs, Catch::Approx(prob).margin(1e-6));\n144:         }\n145: \n146:         DYNAMIC_SECTION(\"Test TensorProd XYZ\"\n147:                         << StateVectorMPIToName<StateVectorT>::name) {\n148:             auto X0 = std::make_shared<NamedObsMPI<StateVectorT>>(\n149:                 \"PauliX\", std::vector<std::size_t>{0});\n150:             auto Z1 = std::make_shared<NamedObsMPI<StateVectorT>>(\n151:                 \"PauliZ\", std::vector<std::size_t>{1});\n152:             auto Y2 = std::make_shared<NamedObsMPI<StateVectorT>>(\n153:                 \"PauliY\", std::vector<std::size_t>{2});\n154:             auto obs = TensorProdObsMPI<StateVectorT>::create({X0, Z1, Y2});\n155: \n156:             MeasurementsMPI<StateVectorT> Measurer_obs(statevector);\n157: \n158:             sv.applyOperations({\"Hadamard\", \"PauliZ\", \"S\", \"Hadamard\"},\n159:                                {{0}, {2}, {2}, {2}},\n160:                                {false, false, false, false});\n161: \n162:             MeasurementsMPI<StateVectorT> Measurer(sv);\n163: \n164:             auto prob_obs = Measurer_obs.probs(*obs);\n165:             auto prob = Measurer.probs(std::vector<std::size_t>({0, 1, 2}));\n166: \n167:             REQUIRE_THAT(prob_obs, Catch::Approx(prob).margin(1e-6));\n168:         }\n169:     }\n170: }", "label": "(24, CUDA, cudaSetDevice(deviceId);); (20, CUDA, cudaGetDeviceCount(&nDevices);); (9, MPI, MPIManager mpi_manager(MPI_COMM_WORLD);)"}
{"predict": "(7,OpenMP,#pragma omp distribute parallel for schedule(guided, argc) default(none) copyin(g) dist_schedule(static, a) private(a) shared(argc)), (19,OpenMP,#pragma omp distribute parallel for private(argc,b) firstprivate(argv,c) lastprivate(d,f) collapse(2) schedule(auto) if(argc) num_threads(a) default(shared) shared(e) reduction(+: h) dist_schedule(static, b))", "label": "(5, OpenMP, #pragma omp threadprivate(g)); (21, OpenMP, #pragma omp distribute parallel for private(argc, b), firstprivate(argv, c), lastprivate(d, f) collapse(2) schedule(auto) if (argc) num_threads(a) default(shared) shared(e) reduction(+ : h) dist_schedule(static, b)); (7, OpenMP, #pragma omp distribute parallel for schedule(guided, argc) default(none) copyin(g) dist_schedule(static, a) private(a) shared(argc)); (4, OpenMP, #pragma omp target); (4, OpenMP, #pragma omp teams)"}
{"predict": "(18,OpenMP,#pragma omp parallel for private(j, mpi_errno))", "label": "(11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (22, MPI, mpi_errno = MPI_Waitall(block, req, error_status);); (10, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (47, MPI, mpi_errno = MPI_Waitall(block, req, MPI_STATUSES_IGNORE);); (17, MPI, MPI_Comm_idup(MPI_COMM_WORLD, &comm_hdls[i], &req[block++]);); (28, MPI, mpi_errno = MPI_Wait(&req[j], MPI_STATUSES_IGNORE);)"}
{"predict": "(18,OpenMP,#pragma omp parallel sections), (18,OpenMP,#pragma omp section), (18,OpenMP,#pragma omp section)", "label": "(13, TBB, double serial_time = (tbb::tick_count::now() - t0).seconds();); (10, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (19, TBB, double tbb_time = (tbb::tick_count::now() - t1).seconds();); (16, TBB, tbb::tick_count t1 = tbb::tick_count::now();)"}
{"predict": "(1,OpenMP,#pragma omp target teams distribute parallel for)", "label": "(3, CUDA, extern __shared__ double sdat[];)"}
{"predict": "17: #ifdef _OPENMP\n118: #ifdef __INTEL_COMPILER\n119: #else\n120: // This taskloop causes code to crash or generate wrong solution for some intel and nv compilers\n121: #if defined __GNUC__  && !defined __NVCOMPILER\n122: #endif\n123: #endif\n124: #endif\n125: \n126: \n127: \n128: #if ( PROFlevel>=1 )\n129: \t\t\t\tTOC(t2);\n130: \t\t\t\tt2 = SuperLU_timer_() - t1;\n131: \t\t\t\tmsg_vol += t2;\n132: #endif\n133: \n134: \n135: #if ( PROFlevel>=1 )\n136: \t\t\t\tTIC(t1);\n137: #endif\n138: \n139: \n140: #if ( PROFlevel>=1 )\n141: \t\t\t\tTOC(t2);\n142: \t\t\t\tt2 = SuperLU_timer_() - t1;\n143: \t\t\t\tmsg_cnt += t2;\n144: #endif\n145: \n146: \n147: \n148: #if ( PROFlevel>=1 )\n149: \t\t\t\tTIC(t1);\n150: #endif\n151: \n152: \n153: #if ( PROFlevel>=1 )\n154: \t\t\t\tTOC(t2);\n145: \t\t\t\tt2 = SuperLU_timer_() - t1;\n146: \t\t\t\tmsg_vol += t2;\n147: #endif\n152: \n153: \n154: \n155: \n156: \n157: #if ( PROFlevel>=1 )\n158: \t\t\t\tTIC(t1);\n159: #endif\n160: \n161: \n162: #if ( PROFlevel>=1 )\n163: \t\t\t\tTOC(t2);\n145: \t\t\t\tt2 = SuperLU_timer_() - t1;\n146: \t\t\t\tmsg_cnt += t2;\n147: #endif\n148: \n149", "label": "(82, OpenMP, #pragma\tomp\ttaskloop firstprivate (stat) private (thread_id1,Uinv,nn,lbstart,lbend,ub,temp,rtemp_loc,ik,lk1,gik,gikcol,usub,uval,lsub,lusup,iknsupc,il,i,irow,bmod_tmp,p,ii,jj,t1,t2,j,ikfrow,iklrow,dest,y,uptr,fnz,nsupr,nroot_send_tmp) untied nogroup); (126, OpenMP, #pragma omp simd); (141, OpenMP, #pragma omp atomic capture); (75, OpenMP, #pragma\tomp\tparallel for private (thread_id1,Uinv,nn,lbstart,lbend,ub,temp,rtemp_loc,ik,lk1,gik,gikcol,usub,uval,lsub,lusup,iknsupc,il,i,irow,bmod_tmp,p,ii,jj,t1,t2,j,ikfrow,iklrow,dest,y,uptr,fnz,nsupr))"}
{"predict": "(19,OpenMP,#pragma omp parallel for)", "label": "(8, CUDA, auto input   = thrust::host_vector<coordinate<T>>(d_input);)"}
{"predict": "(20,OpenMP,#pragma omp parallel for private(t))", "label": "(20, TBB, my_thread_array = tbb::cache_aligned_allocator<padded_ipc_worker>().allocate( my_n_thread );); (14, TBB, my_n_thread = tbb::detail::r1::AvailableHwConcurrency();); (25, TBB, my_waker = tbb::cache_aligned_allocator<ipc_waker>().allocate(1);); (27, TBB, my_stopper = tbb::cache_aligned_allocator<ipc_stopper>().allocate(1);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for), (36,OpenMP,#pragma omp parallel for), (55,OpenMP,#pragma omp parallel for), (63,OpenMP,#pragma omp parallel for), (66,OpenMP,#pragma omp parallel for), (71,OpenMP,#pragma omp parallel for), (76,OpenMP,#pragma omp parallel for), (82,OpenMP,#pragma omp parallel for)", "label": "(19, OpenMP, #pragma omp distribute parallel for); (17, OpenMP, #pragma omp target); (45, OpenMP, #pragma omp distribute parallel for num_threads(128)); (72, OpenMP, #pragma omp teams distribute parallel for thread_limit(512) num_threads(128)); (22, OpenMP, #pragma omp target teams thread_limit(64)); (76, OpenMP, #pragma omp teams distribute parallel for); (29, OpenMP, #pragma omp teams)"}
{"predict": "(19,OpenMP,#pragma omp parallel for), (78,OpenMP,#pragma omp parallel for reduction(+:gpu_result))", "label": "(68, CUDA, checkCudaErrors(cudaMalloc((void **)&d_odata, numBlocks * sizeof(float)));); (105, CUDA, cudaFree(d_odata);); (65, CUDA, checkCudaErrors(cudaMalloc((void **)&d_idata, bytes));); (10, CUDA, checkCudaErrors(cudaGetDeviceProperties(&prop, device));); (8, CUDA, checkCudaErrors(cudaSetDevice(device));); (100, CUDA, cudaFree(d_idata);); (65, CUDA, checkCudaErrors(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));)"}
{"predict": "(9,OpenMP,#pragma omp parallel for)", "label": "(20, CUDA, cudaFree(res_dev);); (10, CUDA, cudaMalloc(&res_dev, sizeof(float));); (12, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(1,OpenMP,#pragma omp parallel), (1,OpenMP,#pragma omp single)", "label": "(2, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (2, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(7,OpenMP,#pragma omp parallel for reduction(+:my_gain))", "label": "(14, TBB, return tbb::parallel_reduce(range, 0, accum, std::plus<>());); (3, TBB, auto range = tbb::blocked_range<size_t>(UL(0), end);)"}
{"predict": "(1,OpenMP,#pragma omp parallel sections), (2,OpenMP,#pragma omp section), (3,OpenMP,#pragma omp section)", "label": "(12, CUDA, cudaDeviceSynchronize();)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(11, MPI, MPI_Type_size_x(type, &typesize);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(15, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "// Get the population to add agents to\n134:         flamegpu::AgentVector boids = cudaSimulation.getPopulationData(\"Boid\");\n135:         // Get the number of agents to generate\n136:         unsigned int population_to_generate = env.getProperty<unsigned int>(\"POPULATION_TO_GENERATE\");\n137:         // Reserve space for agents in the agent vector\n138:         boids.reserve(population_to_generate);\n139: \n140:         // Generate agents\n141:         for (unsigned int i = 0; i < population_to_generate; i++) {\n142:             // Generate a random position\n143:             float x = position_distribution(rngEngine);\n144:             float y = position_distribution(rngEngine);\n145:             float z = position_distribution(rngEngine);\n146:             // Generate a random velocity\n147:             float vx = velocity_distribution(rngEngine);\n148:             float vy = velocity_distribution(rngEngine);\n149:             float vz = velocity_distribution(rngEngine);\n150:             // Generate a random velocity magnitude\n151:             float vmag = velocity_magnitude_distribution(rngEngine);\n152:             // Normalise velocity\n153:             float inv_vlength = 1.0f / sqrt(vx * vx + vy * vy + vz * vz);\n154:             vx *= inv_vlength;\n155:             vy *= inv_vlength;\n156:             vz *= inv_vlength;\n157:             // Scale velocity by magnitude\n158:             vx *= vmag;\n159:             vy *= vmag;\n160:             vz *= vmag;\n161:             // Create a new agent with the generated position and velocity\n162:             boids.emplace_back(x, y, z, vx, vy, vz);\n163:         }\n164:     }\n165: \n166:     // Run the simulation\n167:     cudaSimulation.simulate();\n168: \n169:     // Export the final state\n170:     cudaSimulation.exportData(\"final.xml\");\n171: \n172:     return 0;\n173: }", "label": "(80, CUDA, flamegpu::CUDASimulation cudaSimulation(model);)"}
{"predict": "(16,MPI,MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);); (16,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (22,MPI,status = H5Pset_fapl_mpio(fpid, MPI_COMM_WORLD, MPI_INFO_NULL););", "label": "(16, MPI, MPI_Comm_size(MPI_COMM_WORLD,&mpi_size);); (16, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&mpi_rank);)"}
{"predict": "(26,OpenMP,#pragma omp parallel for), (48,OpenMP,#pragma omp parallel for), (59,OpenMP,#pragma omp parallel for), (64,OpenMP,#pragma omp parallel for)", "label": "(63, MPI, MPI_Unpack(check_pack_buf, pack_size, &pos, unpack_buf, 1, datatype, MPI_COMM_WORLD);); (6, MPI, MPI_Type_size_c(datatype, &type_size);); (34, MPI, MPI_Pack_size(1, datatype, MPI_COMM_WORLD, &pack_size);); (54, MPI, MPI_Pack(check_unpack_buf, 1, datatype, pack_buf, pack_size, &pos, MPI_COMM_WORLD);)"}
{"predict": "(5,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for)", "label": "(4, OpenMP, #pragma omp parallel for)"}
{"predict": "element-- )\n133:                     {\n134:                         for( int vector = vectorSize - 1; vector > 0; vector-- )\n135:                         {\n136:                             p[ element * vectorSize + vector ] = p[ element * vectorSize ];\n137:                         }\n138:                     }\n139:                 }\n140:             }\n141: \n142:             // Perform verification\n143:             switch (type) {\n144:                 case     kChar:\n145:                     error_count += verify_char_data(indx, num_elements * vectorSize, (cl_char*)pThreadData->m_input_ptr[0], (cl_char*)pThreadData->m_output_ptr[0]);\n146:                     break;\n147:                 case     kUChar:\n148:                     error_count += verify_uchar_data(indx, num_elements * vectorSize, (cl_uchar*)pThreadData->m_input_ptr[0], (cl_uchar*)pThreadData->m_output_ptr[0]);\n149:                     break;\n150:                 case     kShort:\n151:                     error_count += verify_short_data(indx, num_elements * vectorSize, (cl_short*)pThreadData->m_input_ptr[0], (cl_short*)pThreadData->m_output_ptr[0]);\n152:                 case     kUShort:\n153:                     error_count += verify_ushort_data(indx, num_elements * vectorSize, (cl_ushort*)pThreadData->m_input_ptr[0], (cl_ushort*)pThreadData->m_output_ptr[0]);\n154:                     break;\n155:                 case     kInt:\n156:                     error_count += verify_int_data(indx, num_elements * vectorSize, (cl_int*)pThreadData->m_input_ptr[0], (cl_int*)pThreadData->m_output_ptr[0]);\n157:                     break;\n158:                 case     kUInt:\n159:                     error_count += verify_uint_data(indx, num_elements * vectorSize, (cl_uint*)pThreadData->m_input_ptr[0], (cl_uint*)pThreadData->m_output_ptr[0]);\n160:                     break;\n161:                 case     kLong:\n16", "label": "(113, OpenCL, err = clEnqueueWriteBuffer( queue, pThreadData->m_streams[1], CL_FALSE, 0, pThreadData->m_type_size*num_elements * inputBVecSize, (void *)pThreadData->m_input_ptr[1], 0, NULL, NULL );); (110, OpenCL, err = clEnqueueWriteBuffer(queue, pThreadData->m_streams[0], CL_FALSE, 0, pThreadData->m_type_size*num_elements * inputAVecSize, (void *)pThreadData->m_input_ptr[0], 0, NULL, NULL);); (117, OpenCL, err = clEnqueueReadBuffer( queue, pThreadData->m_streams[2], CL_TRUE, 0, pThreadData->m_type_size*num_elements * vectorSize, (void *)pThreadData->m_output_ptr, 0, NULL, NULL );); (113, OpenCL, err = clEnqueueNDRangeKernel( queue, pThreadData->m_kernel[i], 1, NULL, threads, NULL, 0, NULL, NULL );)"}
{"predict": "endif\n157:     std::cout << \"Rate (MFlops/s): \"\n158:               << active_points * (2.*iterations*n*n) / stencil_time / 1.0e6\n159:               << std::endl;\n160:   }\n161: \n162:   delete [] in;\n163:   delete [] out;\n164:   return 0;\n165: }", "label": "(114, OpenMP, if (iter==1) stencil_time = omp_get_wtime();); (59, OpenMP, std::cout << \"Number of threads (max)   = \" << omp_get_max_threads() << std::endl;); (125, OpenMP, stencil_time = omp_get_wtime() - stencil_time;)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(36, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (83, CUDA, cudaFree(d_in);); (83, CUDA, cudaFree(d_out);); (32, CUDA, cudaMalloc((void**)(&d_in), in_bytes);); (45, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (33, CUDA, cudaMemcpy(d_in, tensor.data(), in_bytes, cudaMemcpyHostToDevice);); (42, CUDA, assert(cudaMemcpyAsync(tensor_arg.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);)"}
{"predict": "(30,OpenMP,#pragma omp parallel for)", "label": "(30, MPI, MPI_Send(MPI_BOTTOM, 1, newtype, 1, 0, MPI_COMM_WORLD);); (23, MPI, MPI_Type_commit(&newtype);); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (20, MPI, MPI_Type_create_struct(2, lens, displs, types, &newtype);); (15, MPI, MPI_Get_address(&count, &displs[0]);); (27, MPI, MPI_Recv(buf, 6, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (35, MPI, MPI_Type_free(&newtype);); (15, MPI, types[1] = MPI_Type_f2c(*ftype);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for), (35,OpenMP,#pragma omp parallel for)", "label": "(11, OpenMP, #pragma omp taskloop simd linear(f) linear(f)); (54, OpenMP, #pragma omp taskloop simd linear(ref(j))); (35, OpenMP, #pragma omp taskloop simd linear(a, b)); (26, OpenMP, #pragma omp taskloop simd linear (argc > 0 ? argv[1] : argv[2])); (27, OpenMP, #pragma omp taskloop simd linear (argc, z)); (10, OpenMP, #pragma omp taskloop simd linear (); (39, OpenMP, #pragma omp taskloop simd linear(h, C::x)); (16, OpenMP, #pragma omp taskloop simd linear (foo())); (49, OpenMP, #pragma omp taskloop simd linear(i)); (38, OpenMP, #pragma omp parallel); (17, OpenMP, #pragma omp taskloop simd linear (argc,); (2, OpenMP, #pragma omp taskloop simd linear); (13, OpenMP, #pragma omp taskloop simd linear (argc); (37, OpenMP, #pragma omp taskloop simd linear(val(i))); (19, OpenMP, #pragma omp taskloop simd linear (S1)); (24, OpenMP, #pragma omp taskloop simd linear (argv[1])); (3, OpenMP, #pragma omp taskloop simd linear (val); (4, OpenMP, #pragma omp taskloop simd linear (ref())); (34, OpenMP, #pragma omp taskloop simd linear(uval(i) : 4)); (24, OpenMP, #pragma omp taskloop simd linear(val(e, g))); (-3, OpenMP, #pragma omp taskloop simd linear ())"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (10,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for), (35,OpenMP,#pragma omp parallel for), (38,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for)", "label": "(3, OpenMP, #pragma omp taskyield untied); (0, OpenMP, #pragma omp taskyield); (2, OpenMP, #pragma omp taskyield unknown)"}
{"predict": "(15,OpenMP,#pragma omp parallel)", "label": "(9, CUDA, cudaFuncGetAttributes(&attr,(void*)ptr);); (14, CUDA, fcuda::Check_CudaErroorFun(\"Error collecting kernel information.\");); (5, CUDA, cudaOccupancyMaxPotentialBlockSize(&mingridsize,&qblocksize,(void*)ptr,0,0);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for num_threads(num_threads) private(j))", "label": "(52, TBB, tbb::flow::make_edge( b2, b3 );); (50, TBB, tbb::flow::make_edge( b, b2 );); (73, TBB, tbb::flow::buffer_node<T> b_copy(b);); (2, TBB, tbb::flow::buffer_node<T> b3(g);); (0, TBB, tbb::flow::buffer_node<T> b2(g);); (-3, TBB, tbb::flow::graph g;); (-3, TBB, tbb::flow::buffer_node<T> b(g);)"}
{"predict": "PRIu64 \"%\" PRIu64 \"\\n\",\n153:                           inputA[i], inputB[i], inputC[i]);\n154:                 log_error(\"-> expected: %\" PRIu64 \"\\n\", r);\n155:                 log_error(\"-> received: %\" PRIu64 \"\\n\", output[i]);\n156:                 return -1;\n157:             }\n158:         }\n159:         log_info(\"passed\\n\");\n160:     }\n161: \n162:     return 0;\n163: }", "label": "(132, OpenCL, err |= clSetKernelArg(kernel, 1, sizeof(cl_mem), &streams[1]);); (132, OpenCL, err |= clSetKernelArg(kernel, 2, sizeof(cl_mem), &streams[2]);); (132, OpenCL, err |= clSetKernelArg(kernel, 3, sizeof(cl_mem), &streams[3]);); (128, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(cl_mem), &streams[0]);)"}
{"predict": "(35,OpenMP,#pragma omp parallel for), (39,OpenMP,#pragma omp parallel for)", "label": "(19, OpenMP, #pragma omp parallel default(shared)); (11, OpenMP, #pragma omp parallel); (38, OpenMP, #pragma omp parallel num_threads(x)); (23, OpenMP, #pragma omp parallel default(none)); (29, OpenMP, #pragma omp parallel default(firstprivate))"}
{"predict": "(4,OpenMP,#pragma omp parallel for collapse(3))", "label": "(24, TBB, range_type r2(r,tbb::split());); (2, TBB, typedef tbb::blocked_range2d<row_type,col_type> range_type;); (18, TBB, AssertSameType( r.cols(), tbb::blocked_range<col_type>( coli, colj, 1 ));); (16, TBB, AssertSameType( r.rows(), tbb::blocked_range<row_type>( rowi, rowj, 1 ));)"}
{"predict": "(9,OpenMP,#pragma omp parallel private(sum1))", "label": "(13, OpenMP, #pragma omp for private(sum0) schedule(static,1)); (7, OpenMP, #pragma omp parallel); (22, OpenMP, #pragma omp critical); (13, OpenMP, #pragma omp flush)"}
{"predict": "3:     int iy_end_global = iy_start_global + chunk_size - 1;\n74: \n75:     // Initialize the local domain\n76:     for (int iy = iy_start_global; iy <= iy_end_global; iy++) {\n77:         for (int ix = 1; ix < nx - 1; ix++) {\n78:             a[iy * nx + ix] = initial_value(ix, iy);\n79:         }\n80:     }\n81: \n82:     // Initialize halo regions\n83:     for (int ix = 0; ix < nx; ix++) {\n84:         a[iy_start_global * nx + ix] = boundary_value(ix, iy_start_global - 1);\n85:         a[(iy_end_global + 1) * nx + ix] = boundary_value(ix, iy_end_global + 1);\n86:     }\n87: \n88:     // Start the timer\n89:     double runtime_start = MPI_Wtime();\n90: \n91:     // Perform the main computation\n92:     for (int iter = 0; iter < iter_max; iter++) {\n93:         // Perform the computation on the local domain\n94:         for (int iy = iy_start_global; iy <= iy_end_global; iy++) {\n95:             for (int ix = 1; ix < nx - 1; ix++) {\n96:                 a_new[iy * nx + ix] = (a[(iy - 1) * nx + ix] + a[(iy + 1) * nx + ix] +\n97:                                      a[iy * nx + ix - 1] + a[iy * nx + ix + 1]) * 0.25;\n98:             }\n99:         }\n100: \n101:         // Update halo regions\n102:         for (int ix = 0; ix < nx; ix++) {\n103:             a[iy_start_global * nx + ix] = boundary_value(ix, iy_start_global - 1);\n104:             a[(iy_end_global + 1) * nx + ix] = boundary_value(ix, iy_end_global + 1);\n105:         }\n106: \n107:         // Swap a and", "label": "(118, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&reset_l2norm_done, cudaEventDisableTiming));); (143, MPI, MPI_CALL(MPI_Barrier(MPI_COMM_WORLD));); (21, MPI, MPI_CALL(MPI_Comm_free(&local_comm));); (249, CUDA, CUDA_RT_CALL(cudaStreamDestroy(compute_stream));); (201, MPI, double stop = MPI_Wtime();); (244, CUDA, CUDA_RT_CALL(cudaEventDestroy(push_done));); (257, CUDA, CUDA_RT_CALL(cudaFree(a));); (-4, MPI, MPI_CALL(MPI_Comm_rank(MPI_COMM_WORLD, &rank));); (94, CUDA, CUDA_RT_CALL(cudaDeviceSynchronize());); (18, CUDA, CUDA_RT_CALL(cudaFree(0));); (16, CUDA, CUDA_RT_CALL(cudaSetDevice(local_rank%num_devices));); (58, CUDA, CUDA_RT_CALL(cudaMalloc(&a, nx * (chunk_size + 2) * sizeof(real)));); (243, CUDA, CUDA_RT_CALL(cudaFree(l2_norm_d));); (138, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(push_stream, reset_l2norm_done, 0));); (100, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&push_prep_done, cudaEventDisableTiming));); (28, CUDA, CUDA_RT_CALL(cudaMallocHost(&a_h, nx * ny * sizeof(real)));); (25, CUDA, CUDA_RT_CALL(cudaMallocHost(&a_ref_h, nx * ny * sizeof(real)));); (84, CUDA, CUDA_RT_CALL(cudaGetLastError());); (145, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(compute_stream, push_prep_done, 0));); (89, CUDA, CUDA_RT_CALL(cudaStreamCreateWithPriority(&compute_stream, cudaStreamDefault, leastPriority));); (51, CUDA, CUDA_RT_CALL(cudaMalloc(&a_new, nx * (chunk_size + 2) * sizeof(real)));); (230, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_stream));); (-17, MPI, MPI_CALL(MPI_Comm_size(MPI_COMM_WORLD, &size));); (170, CUDA, CUDA_RT_CALL(cudaStreamSynchronize(compute_stream));); (125, CUDA, CUDA_RT_CALL(cudaEventRecord(reset_l2norm_done, compute_stream));); (-18, CUDA, CUDA_RT_CALL(cudaGetDeviceCount(&num_devices));); (102, CUDA, CUDA_RT_CALL(cudaMallocHost(&l2_norm_h, sizeof(real)));); (121, CUDA, CUDA_RT_CALL(cudaMemsetAsync(l2_norm_d, 0, sizeof(real), compute_stream));); (-27, MPI, MPI_CALL(MPI_Init(&argc, &argv));); (46, CUDA, CUDA_RT_CALL(cudaMemset(a_new, 0, nx * (chunk_size + 2) * sizeof(real)));); (76, CUDA, CUDA_RT_CALL(cudaDeviceGetStreamPriorityRange(&leastPriority, &greatestPriority));); (219, CUDA, CUDA_RT_CALL(cudaEventDestroy(push_prep_done));); (94, CUDA, CUDA_RT_CALL(cudaMalloc(&l2_norm_d, sizeof(real)));); (-12, MPI, MPI_CALL(MPI_Comm_rank(local_comm, &local_rank));); (233, CUDA, CUDA_RT_CALL(cudaFreeHost(a_ref_h));); (159, MPI, MPI_CALL(MPI_Allreduce(l2_norm_h, &l2_norm, 1, MPI_REAL_TYPE, MPI_SUM, MPI_COMM_WORLD));); (109, MPI, double start = MPI_Wtime();); (79, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&push_done, cudaEventDisableTiming));); (210, CUDA, CUDA_RT_CALL(cudaEventDestroy(reset_l2norm_done));); (215, CUDA, CUDA_RT_CALL(cudaFreeHost(l2_norm_h));); (120, CUDA, CUDA_RT_CALL(cudaEventRecord(push_prep_done, push_stream));)"}
{"predict": "(24,OpenMP,#pragma omp parallel for reduction(+:combine_sum,combine_ref_sum,combine_each_sum,combine_finit_sum,assign_sum))", "label": "(30, TBB, tbb::combinable<T> finit_combinable(my_finit_decl);); (32, TBB, tbb::parallel_for( tbb::blocked_range<int>( 0, N, 10000 ), ParallelScalarBodyNoInit<T>( finit_combinable ) );); (24, TBB, if (Verbose && t == 0) t0 = tbb::tick_count::now();); (31, TBB, tbb::parallel_for( tbb::blocked_range<int>( 0, N, 10000 ), ParallelScalarBody<T>( sums ) );); (40, TBB, tbb::combinable<T> assigned;); (-2, TBB, tbb::task_scheduler_init init(tbb::task_scheduler_init::deferred);); (7, TBB, tbb::tick_count t0;); (21, TBB, tbb::combinable<T> sums;)"}
{"predict": "(6,OpenMP,#pragma omp parallel sections num_threads(nr_threads)), (40,OpenMP,#pragma omp parallel for reduction(+:sum)), (75,OpenMP,#pragma omp parallel for reduction(+:sum))", "label": "(12, OpenMP, omp_set_num_threads(nr_threads);); (13, OpenMP, #pragma omp parallel default(none) shared(a)); (14, OpenMP, #pragma omp sections); (8, OpenMP, omp_set_dynamic(0);); (14, OpenMP, #pragma omp section); (16, OpenMP, int thread_nr = omp_get_thread_num();)"}
{"predict": "(21,OpenMP,#pragma omp parallel for private(nearest, min_dist) reduction(min:min_dist) schedule(static))", "label": "(12, CUDA, extern __shared__ float _shared_centroids[];)"}
{"predict": "(58,OpenMP,#pragma omp parallel for private(x, x_next, x_temp, xdot, u, y, bar, sample_time_offset))", "label": "(21, CUDA, extern __shared__ float entire_buffer[];)"}
{"predict": "(8,OpenMP,#pragma omp flush (a))", "label": "(5, OpenMP, #pragma omp flush acquire); (5, OpenMP, #pragma omp flush release); (1, OpenMP, #pragma omp flush); (4, OpenMP, #pragma omp flush(a)); (0, OpenMP, #pragma omp flush acq_rel)"}
{"predict": "(26,OpenMP,#pragma omp parallel for collapse(2)), (43,OpenMP,#pragma omp parallel for)", "label": "(43, CUDA, CUDA_CHECK(cudaMemcpy(d_A_inv, d_A, sizeof(data_type) * lda * n, cudaMemcpyDeviceToDevice));); (47, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (69, CUDA, CUDA_CHECK(cudaFree(d_info));); (34, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * lda * n));); (34, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A_inv), sizeof(data_type) * lda * n));); (65, CUDA, CUDA_CHECK(cudaFree(d_A_inv));); (58, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (32, CUDA, CUDA_CHECK(cudaMalloc(&d_info, sizeof(int)));); (34, CUDA, CUDA_CHECK(cudaMemcpy(d_A, h_A, sizeof(data_type) * lda * n, cudaMemcpyHostToDevice));); (42, CUDA, CUDA_CHECK(cudaMemcpy(&h_info, d_info, sizeof(int), cudaMemcpyDeviceToHost));); (34, CUDA, CUDA_CHECK(cudaMemset(d_info, 0, sizeof(int)));); (58, CUDA, CUDA_CHECK(cudaFree(d_A));); (20, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(33,OpenMP,#pragma omp parallel for collapse(2)), (40,OpenMP,#pragma omp parallel for collapse(2)), (53,OpenMP,#pragma omp parallel for collapse(2)), (59,OpenMP,#pragma omp parallel for collapse(2))", "label": "(31, CUDA, thrust::complex<T> const* v1_(v1 + Q * nchol_max * nwalk);); (31, CUDA, thrust::complex<T> const* v2_(v1 + Qm * nchol_max * nwalk);); (52, CUDA, thrust::complex<T2> ialpha(alpha * thrust::complex<T2>(0.0, 1.0));); (27, CUDA, thrust::complex<T2>* vb_(vb + nc0 * nwalk);); (46, CUDA, thrust::complex<T2>* vb_(vb + (nc0 + nc) * nwalk);)"}
{"predict": "(69,OpenMP,#pragma omp parallel for), (9,OpenMP,#pragma omp parallel for), (10,OpenMP,#pragma omp parallel for)", "label": "(83, OpenACC, #pragma acc update host(state[(nz + 2 * hs) * (nx + 2 * hs) * NUM_VARS])); (45, OpenACC, #pragma acc data copyin(state_tmp[(nz + 2 * hs) * (nx + 2 * hs) * NUM_VARS], hy_dens_cell[nz + 2 * hs], hy_dens_theta_cell[nz + 2 * hs], hy_dens_int[nz + 1], hy_dens_theta_int[nz + 1], hy_pressure_int[nz + 1]))"}
{"predict": "(27,OpenMP,#pragma omp parallel for)", "label": "(32, MPI, CALL(MPI_Waitall(static_cast<int>(reqs.size()), reqs.data(), MPI_STATUSES_IGNORE));); (20, MPI, CALL(MPI_Waitall(reqs.size(), reqs.data(), MPI_STATUSES_IGNORE));)"}
{"predict": "(19,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp single nowait), (27,OpenMP,#pragma omp single nowait)", "label": "(28, MPI, total_my_bcast_time += MPI_Wtime();); (31, MPI, total_mpi_bcast_time -= MPI_Wtime();); (45, MPI, MPI_Finalize();); (32, MPI, total_mpi_bcast_time += MPI_Wtime();); (5, MPI, MPI_Init(NULL, NULL);); (28, MPI, MPI_Bcast(data, num_elements, MPI_INT, 0, MPI_COMM_WORLD);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (17, MPI, total_my_bcast_time -= MPI_Wtime();); (15, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for private(temp))", "label": "(4, CUDA, __shared__ float Bs[block_size][block_size];); (2, CUDA, __shared__ float As[block_size][block_size];)"}
{"predict": "(17,OpenMP,#pragma omp parallel for private(left_data, right_data) shared(first_in_group, last_in_group))", "label": "(11, MPI, MPI_Type_get_extent(item, &lb, &extent);)"}
{"predict": "(27,OpenMP,#pragma omp parallel sections), (37,OpenMP,#pragma omp section), (56,OpenMP,#pragma omp section)", "label": "(46, CUDA, env.engine.reset(runtime->deserializeCudaEngine(env.serializedEngine->data(), env.serializedEngine->size()));); (19, CUDA, auto profileStream = samplesCommon::makeCudaStream();)"}
{"predict": "(20,OpenMP,#pragma omp parallel for)", "label": "(28, CUDA, cudaFree( p );)"}
{"predict": "(48,OpenMP,#pragma omp parallel for)", "label": "(49, OpenMP, #pragma omp target teams is_device_ptr(da)); (23, OpenMP, #pragma omp target teams is_device_ptr(z)); (27, OpenMP, #pragma omp target teams is_device_ptr(aa)); (16, OpenMP, #pragma omp target teams is_device_ptr(k)); (30, OpenMP, #pragma omp target teams is_device_ptr(raa)); (39, OpenMP, #pragma omp target teams is_device_ptr(rh)); (33, OpenMP, #pragma omp target teams is_device_ptr(h))"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(35, MPI, _measurements = get_measure()->mpi_get();)"}
{"predict": "(24,OpenMP,#pragma omp parallel sections), (24,OpenMP,#pragma omp section), (28,OpenMP,#pragma omp section), (32,OpenMP,#pragma omp section), (34,OpenMP,#pragma omp section), (36,OpenMP,#pragma omp section)", "label": "(42, MPI, MPI_T_finalize();); (17, MPI, MPI_T_init_thread(required, &provided);)"}
{"predict": "(2,OpenMP,#pragma omp target teams distribute parallel for map(to:a[0:N],b[0:N]) map(tofrom:y[0:1]))", "label": "(5, CUDA, __shared__ float reduce_smem[NUM_WARPS];)"}
{"predict": "(10,OpenMP,#pragma omp parallel for private(index) reduction(+:inversions))", "label": "(3, TBB, tbb::parallel_for( tbb::blocked_range<int>(0,m,1000), GrowBy(v) );)"}
{"predict": "(14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma", "label": "(54, OpenMP, #pragma omp parallel reduction(&& : S2::S2sc)); (25, OpenMP, #pragma omp parallel reduction(foo : argc); (44, OpenMP, #pragma omp parallel reduction(* : ca)); (78, OpenMP, #pragma omp for reduction(- : fl)); (28, OpenMP, #pragma omp parallel reduction(~ : argc)); (53, OpenMP, #pragma omp parallel reduction(+ : h, k, B::x)); (82, OpenMP, #pragma omp parallel reduction(task, + : m)); (79, OpenMP, #pragma omp parallel reduction(+ : m)); (36, OpenMP, #pragma omp parallel reduction(+ : ba)); (39, OpenMP, #pragma omp parallel reduction(- : da)); (59, OpenMP, #pragma omp parallel reduction(+ : r)); (5, OpenMP, #pragma omp parallel reduction(); (50, OpenMP, #pragma omp parallel private(i), reduction(+ : j), reduction(+ : q)); (47, OpenMP, #pragma omp parallel reduction(+ : o)); (-2, OpenMP, #pragma omp parallel); (37, OpenMP, #pragma omp parallel reduction(&& : S2::S2s)); (2, OpenMP, #pragma omp parallel reduction(-); (17, OpenMP, #pragma omp parallel reduction(&& : argc, z)); (-4, OpenMP, #pragma omp parallel reduction); (52, OpenMP, #pragma omp parallel shared(i)); (44, OpenMP, #pragma omp parallel private(k)); (21, OpenMP, #pragma omp parallel reduction(max : h.b)); (6, OpenMP, #pragma omp parallel reduction(| : argc,); (15, OpenMP, #pragma omp parallel reduction(+ : a, b, c, d, f)); (49, OpenMP, #pragma omp parallel reduction(max : j)); (5, OpenMP, #pragma omp parallel reduction(|| : argc > 0 ? argv[1] : argv[2])); (14, OpenMP, #pragma omp parallel reduction(min : a, b, c, d, f)); (45, OpenMP, #pragma omp parallel reduction(min : i)); (-4, OpenMP, #pragma omp parallel reduction(\\)); (-9, OpenMP, #pragma omp parallel reduction()); (60, OpenMP, #pragma omp parallel reduction(+:gptr[:argc])); (45, OpenMP, #pragma omp for private(fl)); (-10, OpenMP, #pragma omp parallel reduction(*)); (17, OpenMP, #pragma omp parallel reduction(^ : fl)); (2, OpenMP, #pragma omp parallel reduction(^ : S1)); (30, OpenMP, #pragma omp parallel reduction(+ : p), reduction(+ : p)); (20, OpenMP, #pragma omp parallel reduction(& : e, g))"}
{"predict": "(4,OpenMP,#pragma omp parallel for)", "label": "(10, TBB, static const size_t number_of_threads = 4 * tbb::task_scheduler_init::default_num_threads();)"}
{"predict": "(35,OpenMP,#pragma omp parallel for)", "label": "(8, CUDA, cuda_safe_call(cudaMalloc((void**) &dX, N * sizeof(double)));); (8, CUDA, cuda_safe_call(cudaMalloc((void**) &dY, N * sizeof(double)));); (8, CUDA, cuda_safe_call(cudaMalloc((void**) &dZ, N * sizeof(double)));); (11, CUDA, cuda_safe_call(cudaDeviceSynchronize());)"}
{"predict": "(10,OpenMP,#pragma omp parallel for private(k, errs) reduction(+:errs))", "label": "(8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);)"}
{"predict": "cudaCheckError();\n117:     cudaCheckError();\n118:     cudaCheckError();\n119:     cudaCheckError();\n120:   }\n121: }\n\n(22,OpenMP,#pragma omp parallel for collapse(2))", "label": "(147, CUDA, cudaStreamDestroy(stream0[d]);); (16, CUDA, cudaMalloc(&buffersD2D[d], numElems * sizeof(int));); (141, CUDA, cudaEventDestroy(start[d]);); (41, CUDA, cudaDeviceEnablePeerAccess(i, 0);); (7, CUDA, cudaCheckError();); (136, CUDA, cudaFree(buffersD2D[d]);); (35, CUDA, cudaDeviceEnablePeerAccess(j, 0);); (138, CUDA, cudaEventDestroy(stop[d]);); (25, CUDA, cudaSetDevice(i);); (106, CUDA, cudaDeviceDisablePeerAccess(i);); (139, CUDA, cudaStreamDestroy(stream1[d]);); (83, CUDA, cudaEventRecord(stop[i], stream0[i]);); (10, CUDA, cudaEventCreate(&stop[d]);); (140, CUDA, cudaFreeHost((void *)flag);); (79, CUDA, cudaStreamWaitEvent(stream0[i], stop[j], 0);); (3, CUDA, cudaMemset(buffersD2D[d], 0, numElems * sizeof(int));); (35, CUDA, cudaStreamSynchronize(stream1[j]);); (52, CUDA, cudaStreamWaitEvent(stream1[j], start[i], 0);); (-2, CUDA, cudaMemset(buffers[d], 0, numElems * sizeof(int));); (-4, CUDA, cudaMalloc(&buffers[d], numElems * sizeof(int));); (-10, CUDA, cudaHostAlloc((void **)&flag, sizeof(*flag), cudaHostAllocPortable);); (119, CUDA, cudaFree(buffers[d]);); (28, CUDA, cudaStreamSynchronize(stream0[i]);); (-9, CUDA, cudaSetDevice(d);); (0, CUDA, cudaStreamCreateWithFlags(&stream0[d], cudaStreamNonBlocking);); (13, CUDA, cudaDeviceCanAccessPeer(&access, i, j);); (42, CUDA, cudaEventRecord(start[i], stream0[i]);); (16, CUDA, cudaSetDevice(j);); (64, CUDA, cudaEventRecord(stop[j], stream1[j]);); (74, CUDA, cudaEventElapsedTime(&time_ms, start[i], stop[i]);); (-10, CUDA, cudaEventCreate(&start[d]);); (-5, CUDA, cudaStreamCreateWithFlags(&stream1[d], cudaStreamNonBlocking);); (81, CUDA, cudaDeviceDisablePeerAccess(j);)"}
{"predict": "(6,OpenMP,#pragma omp parallel for collapse(2)), (20,OpenMP,#pragma omp parallel for collapse(2))", "label": "(5, OpenACC, #pragma acc parallel loop)"}
{"predict": "(3,OpenMP,#pragma omp parallel for), (4,OpenMP,#pragma omp parallel for), (5,OpenMP,#pragma omp parallel for), (8,OpenMP,#pragma omp parallel for), (10,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for)", "label": "(10, OpenMP, #pragma omp master taskloop collapse (argc); (1, OpenMP, #pragma omp master taskloop collapse); (13, OpenMP, #pragma omp master taskloop collapse (1)); (14, OpenMP, #pragma omp master taskloop collapse ((ST > 0) ? 1 + ST : 2)); (31, OpenMP, #pragma omp master taskloop collapse (N)); (32, OpenMP, #pragma omp master taskloop collapse (2)); (16, OpenMP, #pragma omp master taskloop collapse (foobool(argc)), collapse (true), collapse (-5)); (-3, OpenMP, #pragma omp master taskloop collapse (); (-2, OpenMP, #pragma omp master taskloop collapse ()); (15, OpenMP, #pragma omp master taskloop collapse (S)); (3, OpenMP, #pragma omp master taskloop collapse (ST)"}
{"predict": "(2,OpenMP,#pragma omp target teams distribute parallel for map(to:a[0:N]) map(tofrom:y[0:1]))", "label": "(5, CUDA, __shared__ float reduce_smem[NUM_WARPS];)"}
{"predict": "A_K0 is expected to be major, but was not in the provided input!\\n\";\n101:     return 1;\n102:   }\n103: \n104:   if (example::is_left_major<ColModeStridesB>() && (ldBn[0] != 1)) {\n105:     std::cerr << \"ERROR: B_N0 is expected to be major, but was not in the provided input!\\n\";\n106:     return 1;\n107:   }\n108: \n109:   if (example::is_left_major<RedModeStridesB>() && (ldBk[0] != 1)) {\n110:     std::cerr << \"ERROR: B_K0 is expected to be major, but was not in the provided input!\\n\";\n111:     return 1;\n112:   }\n113: \n114:   // Initialize the tensors\n115:   Tensor<ElementA, StrideA> A;\n116:   Tensor<ElementB, StrideB> B;\n117:   Tensor<ElementC, StrideC> C;\n118:   Tensor<ElementD, StrideD> D;\n119: \n120:   // Initialize the tensors with random data\n121:   cute::detail::for_each(cute::make_coord(m, k, l), [&](auto&& coord) {\n122:     A(coord) = cute::detail::rand<ElementA>();\n123:   });\n124: \n125:   cute::detail::for_each(cute::make_coord(n, k, l), [&](auto&& coord) {\n126:     B(coord) = cute::detail::rand<ElementB>();\n127:   });\n128: \n129:   cute::detail::for_each(cute::make_coord(m, n, l), [&](auto&& coord) {\n130:     C(coord) = cute::detail::rand<ElementC>();\n131:   });\n132: \n133:   cute::detail::for_each(cute::make_coord(m, n, l), [&](auto&& coord) {\n134:     D(coord) = cute::detail::rand<ElementD>();\n135:   });", "label": "(161, CUDA, thrust::device_vector<ElementA> d_A = h_A;); (183, CUDA, cuda_err = cudaDeviceSynchronize();); (148, CUDA, thrust::host_vector<ElementB> h_B(N_size * K_size * L_size);); (184, CUDA, std::cerr << cudaGetErrorString(cuda_err) << \"\\n\";); (148, CUDA, thrust::host_vector<ElementD> h_D(M_size * N_size * L_size);); (157, CUDA, thrust::device_vector<ElementB> d_B = h_B;); (158, CUDA, thrust::device_vector<ElementD> cutlass_result = h_D;); (144, CUDA, thrust::host_vector<ElementC> h_C(M_size * N_size * L_size);); (213, CUDA, thrust::host_vector<ElementD> h_cutlass_result = cutlass_result;); (156, CUDA, thrust::device_vector<ElementD> reference_result = h_D;); (153, CUDA, thrust::device_vector<ElementC> d_C = h_C;); (138, CUDA, thrust::host_vector<ElementA> h_A(M_size * K_size * L_size);)"}
{"predict": "(16,OpenMP,#pragma omp parallel for)", "label": "(18, CUDA, gpuErrchk(cudaGetDeviceCount(&deviceCount));); (1, CUDA, gpuErrchk(cudaSetDevice(0));); (2, CUDA, gpuErrchk(cudaFree(0));); (7, CUDA, gpuErrchk(cudaDeviceReset());)"}
{"predict": "(23,OpenMP,#pragma omp parallel for)", "label": "(36, MPI, MPI_Error_string(ERR_CODE, aux_string, &L);)"}
{"predict": "31i, u41i, u51i, u21im1, u31im1, u41im1, u51im1)\n122: #endif\n123:     for (k = 1; k < nz - 1; k++) {\n124:       for (j = jst; j <= jend; j++) {\n125:         for (i = ist; i <= iend; i++) {\n126:           tmp = 1.0 / rsd[0][k][j][i];\n127:           u21i = tmp * rsd[1][k][j][i];\n128:           u31i = tmp * rsd[2][k][j][i];\n129:           u41i = tmp * rsd[3][k][j][i];\n130:           u51i = tmp * rsd[4][k][j][i];\n131:           if (i == ist) {\n132:             u21im1 = u21i;\n133:             u31im1 = u31i;\n134:             u41im1 = u41i;\n135:             u51im1 = u51i;\n136:           }\n137:           tmp = 1.0 / rsd[0][k][j][i-1];\n138:           u21im1 = tmp * rsd[1][k][j][i-1];\n139:           u31im1 = tmp * rsd[2][k][j][i-1];\n140:           u41im1 = tmp * rsd[3][k][j][i-1];\n141:           u51im1 = tmp * rsd[4][k][j][i-1];\n142:           for (m = 0; m < 5; m++) {\n143:             frct[m][k][j][i] =  frct[m][k][j][i]\n144:                                               - tx1 * tmp * ( u21i * u21i - u21im1 * u21im1", "label": "(22, OpenMP, #pragma omp distribute); (17, OpenMP, #pragma omp target data map(alloc: frct, rsd, ce, flux_G)); (19, OpenMP, #pragma omp target teams)"}
{"predict": "(35,OpenMP,#pragma omp parallel), (40,OpenMP,#pragma omp single nowait), (41,OpenMP,#pragma omp for), (66,OpenMP,#pragma omp single nowait)", "label": "(22, CUDA, CUDA_RT_CALL(cudaMemset(a_new, 0, nx * ny * sizeof(real)));); (100, CUDA, CUDA_RT_CALL(cudaStreamDestroy(compute_stream));); (105, CUDA, CUDA_RT_CALL(cudaFree(a));); (23, CUDA, CUDA_RT_CALL(cudaDeviceSynchronize());); (100, CUDA, CUDA_RT_CALL(cudaFree(l2_norm_d));); (27, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&push_top_done, cudaEventDisableTiming));); (24, CUDA, CUDA_RT_CALL(cudaStreamCreate(&push_bottom_stream));); (26, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&push_bottom_done, cudaEventDisableTiming));); (49, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(compute_stream, push_bottom_done, 0));); (67, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(push_bottom_stream, compute_done, 0));); (69, CUDA, CUDA_RT_CALL(cudaEventRecord(push_bottom_done, push_bottom_stream));); (89, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_top_stream));); (86, CUDA, CUDA_RT_CALL(cudaEventDestroy(compute_done));); (81, CUDA, CUDA_RT_CALL(cudaMemcpy(a_ref_h, a, nx * ny * sizeof(real), cudaMemcpyDeviceToHost));); (82, CUDA, CUDA_RT_CALL(cudaEventDestroy(push_bottom_done));); (14, CUDA, CUDA_RT_CALL(cudaStreamCreate(&push_top_stream));); (81, CUDA, CUDA_RT_CALL(cudaEventDestroy(push_top_done));); (65, CUDA, CUDA_RT_CALL(cudaStreamSynchronize(compute_stream));); (44, CUDA, CUDA_RT_CALL(cudaEventRecord(compute_done, compute_stream));); (17, CUDA, CUDA_RT_CALL(cudaMallocHost(&l2_norm_h, sizeof(real)));); (54, CUDA, CUDA_RT_CALL(cudaEventRecord(push_top_done, push_top_stream));); (33, CUDA, CUDA_RT_CALL(cudaMemsetAsync(l2_norm_d, 0, sizeof(real), compute_stream));); (6, CUDA, CUDA_RT_CALL(cudaStreamCreate(&compute_stream));); (-2, CUDA, CUDA_RT_CALL(cudaMemset(a, 0, nx * ny * sizeof(real)));); (7, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&compute_done, cudaEventDisableTiming));); (81, CUDA, CUDA_RT_CALL(cudaFree(a_new));); (45, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(push_top_stream, compute_done, 0));); (72, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_bottom_stream));); (28, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(compute_stream, push_top_done, 0));); (6, CUDA, CUDA_RT_CALL(cudaMalloc(&l2_norm_d, sizeof(real)));); (21, MPI, double start = MPI_Wtime();); (-13, CUDA, CUDA_RT_CALL(cudaMalloc(&a, nx * ny * sizeof(real)));); (71, CUDA, CUDA_RT_CALL(cudaFreeHost(l2_norm_h));); (-14, CUDA, CUDA_RT_CALL(cudaMalloc(&a_new, nx * ny * sizeof(real)));)"}
{"predict": "(17,OpenMP,#pragma omp parallel for private(pkk, lwork, knsupc, lk, ii, i))", "label": "(30, MPI, MPI_Bcast( work, diag_len[p], MPI_DOUBLE, pkk, grid->comm );); (27, MPI, MPI_Bcast( work, lwork, MPI_DOUBLE, pkk, grid->comm );)"}
{"predict": "(7,OpenMP,#pragma omp parallel for collapse(2))", "label": "(4, CUDA, const T** input_ptrs = GetCudaDeviceArrayOnDevice(&input_ptr_data);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for reduction(+:global_ptr[:bins_size])), (15,OpenMP,#pragma omp parallel for reduction(min:global_ptr[:bins_size])), (17,OpenMP,#pragma omp parallel for reduction(max:global_ptr[:bins_size]))", "label": "(17, MPI, MPI_Allreduce(bins_ptr, global_ptr, bins_size, MPI_DOUBLE, MPI_MIN, mpi_comm);); (12, MPI, MPI_Allreduce(bins_ptr, global_ptr, bins_size, MPI_DOUBLE, MPI_SUM, mpi_comm);); (19, MPI, MPI_Allreduce(bins_ptr, global_ptr, bins_size, MPI_DOUBLE, MPI_MAX, mpi_comm);); (2, MPI, MPI_Comm mpi_comm = MPI_Comm_f2c(flow::Workspace::default_mpi_comm());)"}
{"predict": "(12,OpenMP,#pragma omp parallel for private(visc,arp1,rsym,rsymp1,posp1,velrhop1,ngs,pif,drx,dry,drz,rr2,fac,frx,fry,frz,massp2,compute,velrhop2,dvx,dvy,dvz,dot,dot_rr2))", "label": "(71, OpenMP, const int th=omp_get_thread_num();); (10, OpenMP, #pragma omp parallel for schedule (guided))"}
{"predict": "(14,OpenMP,#pragma omp parallel for private(i, dims) reduction(+:errs))", "label": "(10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (15, MPI, MPI_Dims_create(tests[k].size, tests[k].dim, dims);)"}
{"predict": "(21,OpenMP,#pragma omp parallel for private(acc))", "label": "(17, CUDA, __shared__ float a_smem[BM * BK];); (17, CUDA, __shared__ float b_smem[BK * BN];)"}
{"predict": "(33,OpenMP,#pragma omp parallel for collapse(2))", "label": "(86, OpenCL, err = clReleaseMemObject(buffer);); (82, OpenCL, err = clReleaseMemObject(image_buffer);)"}
{"predict": "(31,OpenMP,#pragma omp parallel for)", "label": "(12, CUDA, cudaMallocHost((void**)&h_a, bufsize);); (13, CUDA, cudaMallocHost((void**)&h_c, bufsize);); (43, CUDA, cudaDeviceSynchronize();); (20, CUDA, cudaMalloc((void**)&d_a, bufsize);); (63, CUDA, cudaFree(d_c);); (61, CUDA, cudaFree(d_b);); (7, CUDA, cudaMallocHost((void**)&h_b, bufsize);); (64, CUDA, cudaFreeHost(h_b);); (64, CUDA, cudaFreeHost(h_c);); (56, CUDA, cudaFree(d_a);); (60, CUDA, cudaFreeHost(h_a);); (14, CUDA, cudaMalloc((void**)&d_c, bufsize);); (12, CUDA, cudaMalloc((void**)&d_b, bufsize);)"}
{"predict": "(48,OpenMP,#pragma omp parallel for)", "label": "(43, OpenMP, lis2mdl_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)"}
{"predict": "(6,OpenMP,#pragma omp parallel for reduction(+:r2))", "label": "(3, TBB, ValueType r1 = tbb::parallel_reduce( m_range, I, Accumulator(), Sum(), partitioner );)"}
{"predict": "(21,OpenMP,#pragma omp parallel for private(state, w, r, k, y) shared(_y, _v, _w, _r, _k, _s, _u, b, h, i, T, C))", "label": "(11, CUDA, __shared__ float r[_N_], k[_N_], u[_N_], w[_N_];)"}
{"predict": "(104,OpenMP,#pragma omp parallel for)", "label": "(50, CUDA, flamegpu::CUDASimulation cudaSimulation(model, argc, argv);)"}
{"predict": "(37,OpenMP,#pragma omp parallel for), (45,OpenMP,#pragma omp parallel for), (53,OpenMP,#pragma omp parallel for), (91,OpenMP,#pragma omp parallel for)", "label": "(12, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (13, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;); (80, CUDA, cudaError_t result = cudaDeviceSynchronize();)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(22, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(15,OpenMP,#pragma omp parallel for reduction(+:N)), (44,OpenMP,#pragma omp parallel for reduction(+:current_weight))", "label": "(11, MPI, MPI_Type_get_extent(int_with_key, &lb, &extent);)"}
{"predict": "The provided code is a test function and does not contain any loops or sections that are inherently parallelizable. The code is setting up and checking the behavior of OpenCL events and callbacks, which are typically sequential operations. Therefore, there are no clear opportunities for parallelization in this specific code snippet. However, if we were to assume that there are loops or sections in the functions called (e.g., within `context->getAsyncEventsHandler().closeThread()` or other parts of the OpenCL API that are not shown), we would need to see those sections to provide appropriate parallelization instructions.\n\nGiven the current code, no parallelization instructions are applicable. If you have additional code or loops that you want to parallelize, please provide them, and I can assist you further.", "label": "(15, OpenCL, retVal = clEnqueueNDRangeKernel(clCommandQ, clKernel, 1, offset, gws, nullptr, 1, &eventBlocking, &eventOut);); (13, OpenCL, clCommandQ = clCreateCommandQueue(context, devices[0], properties, &retVal);); (38, OpenCL, clReleaseEvent(eventOut);); (31, OpenCL, clWaitForEvents(1, &callbackEvent);); (37, OpenCL, clReleaseCommandQueue(clCommandQ);); (28, OpenCL, clWaitForEvents(1, &eventOut);)"}
{"predict": "(30,OpenMP,#pragma omp parallel for)", "label": "(38, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (40, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(15,OpenMP,#pragma omp parallel sections), (16,OpenMP,#pragma omp section), (15,OpenMP,#pragma omp section)", "label": "(12, OpenMP, #pragma omp target)"}
{"predict": "(5,OpenMP,#pragma omp parallel for)", "label": "(10, MPI, MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);)"}
{"predict": "(17,OpenMP,#pragma omp parallel sections)", "label": "(33, MPI, MPI_Type_free(&type);); (16, MPI, MPI_Type_get_extent(type, &lb, &extent);); (7, MPI, err = MPI_Type_indexed(3, len, disp, MPI_INT, &type);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(23, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (17, MPI, MPI_Alltoallw_c(sendbuf, counts, displs, types, recvbuf, counts, displs, types, comm);)"}
{"predict": "(15,OpenMP,#pragma omp parallel for)", "label": "(3, OpenMP, #pragma omp target enter data map(alloc:i)); (6, OpenMP, #pragma omp target data map(present, alloc: i)); (7, OpenMP, #pragma omp target exit data map(delete:i))"}
{"predict": "(8,OpenMP,#pragma omp parallel sections)", "label": "(10, MPI, errcode = MPI_Grequest_complete(aio_req->req);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for private(sum) reduction(+:result))", "label": "(20, OpenMP, #pragma omp flush); (13, OpenMP, #pragma omp task private(sum) shared(result, known_sum)); (7, OpenMP, #pragma omp parallel); (8, OpenMP, #pragma omp single); (21, OpenMP, #pragma omp critical)"}
{"predict": "(17,OpenMP,#pragma omp parallel sections), (18,OpenMP,#pragma omp section), (19,OpenMP,#pragma omp section)", "label": "(20, OpenCL, clReleaseMemObject(mobjAP_);); (16, OpenCL, clReleaseMemObject(mobjX_);)"}
{"predict": "(29,OpenMP,#pragma omp parallel for)", "label": "(27, OpenCL, TEST_OCL_CHECK(clReleaseCommandQueue(ocl_queue));)"}
{"predict": "(21,OpenMP,#pragma omp parallel for), (47,OpenMP,#pragma omp parallel for)", "label": "(9, MPI, MPI_Comm_rank(comm, &comm_rank);); (10, MPI, MPI_Comm_size(comm, &comm_size);)"}
{"predict": "(27,OpenMP,#pragma omp parallel for)", "label": "(36, CUDA, cudaMemcpy(signatures.data(), d_signatures, length_signature_aligned * batch, cudaMemcpyDeviceToHost);); (40, CUDA, cudaFree(d_secret_keys);); (26, CUDA, cudaMemcpy(d_secret_keys, secret_keys.data(), length_secret_key * batch, cudaMemcpyHostToDevice);); (26, CUDA, cudaMemcpy(d_messages, messages.data(), message_size * batch, cudaMemcpyHostToDevice);); (39, CUDA, cudaFree(d_messages);); (17, CUDA, cudaMalloc(reinterpret_cast<void**>(&d_secret_keys), length_secret_key * batch);); (15, CUDA, cudaMalloc(reinterpret_cast<void**>(&d_signatures), length_signature_aligned * batch);); (35, CUDA, cudaFree(d_signatures);); (15, CUDA, cudaMalloc(reinterpret_cast<void**>(&d_messages), message_size * batch);)"}
{"predict": "(33,OpenMP,#pragma omp parallel for collapse(2)), (34,OpenMP,for(SizeX i = 0; i < x; i ++) { for(SizeY j = 0; j < y; j ++) { for(int r = 0; r<global_repeat; r++) { for(SizeZ k = 0; k < z; k ++) { o_ptr[k + j*y + i*y*z] += s_ptr[k + j*y + i*y*z]; } } } })", "label": "(19, OpenMP, #pragma omp parallel for)"}
{"predict": "(1,MPI,MPI_Init(&argc, &argv);); (2,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (2,MPI,MPI_Comm_size(MPI_COMM_WORLD, &size);); (22,MPI,MPI_Finalize(););", "label": "(27, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Init(NULL, NULL);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (20, MPI, MPI_Wait(&request, MPI_STATUS_IGNORE);); (6, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (9, MPI, MPI_Bcast(&factor, 1, MPI_INT, root, MPI_COMM_WORLD);)"}
{"predict": "(4,OpenMP,#pragma omp parallel for private(j))", "label": "(3, OpenMP, #pragma omp target); (4, OpenMP, #pragma omp distribute parallel for simd); (2, OpenMP, #pragma omp teams)"}
{"predict": "(6,OpenMP,#pragma omp parallel for)", "label": "(5, OpenMP, #pragma omp target teams distribute parallel for)"}
{"predict": "(1,MPI,MPI_Init(NULL, NULL);); (96,MPI,MPI_Finalize(););", "label": "(43, MPI, err = MPI_Type_size(eviltype, &val);); (100, MPI, MPI_Type_free(&inttype);); (19, MPI, err = MPI_Type_create_resized(tmptype, 6, -9, &inttype);); (55, MPI, err = MPI_Type_get_extent(eviltype, &lb, &extent);); (28, MPI, err = MPI_Type_vector(3, 4, 5, inttype, &eviltype);); (95, MPI, MPI_Type_free(&tmptype);); (3, MPI, err = MPI_Type_contiguous(4, MPI_BYTE, &tmptype);); (95, MPI, MPI_Type_free(&eviltype);); (71, MPI, err = MPI_Type_get_true_extent(eviltype, &true_lb, &aval);)"}
{"predict": "(27,OpenMP,#pragma omp parallel for), (41,OpenMP,#pragma omp parallel for collapse(2))", "label": "(78, CUDA, CHECK(cudaFree(d_y));); (74, CUDA, CHECK(cudaFree(d_NN));); (56, CUDA, CHECK(cudaMemcpy(NL, d_NL, N*M*sizeof(int), cudaMemcpyDefault));); (34, CUDA, CHECK(cudaMalloc(&d_NL, N*M*sizeof(int)));); (32, CUDA, CHECK(cudaMalloc(&d_NN, N*sizeof(int)));); (46, CUDA, CHECK(cudaMemcpy(d_y, y.data(), N*sizeof(real), cudaMemcpyDefault));); (51, CUDA, CHECK(cudaMemcpy(NN, d_NN, N*sizeof(int), cudaMemcpyDefault));); (33, CUDA, CHECK(cudaMalloc(&d_y, N*sizeof(real)));); (69, CUDA, CHECK(cudaFree(d_x));); (30, CUDA, CHECK(cudaMalloc(&d_x, N*sizeof(real)));); (66, CUDA, CHECK(cudaFree(d_NL));)"}
{"predict": "(26,MPI,MPI_Comm_rank(comm, &rank);)\n(32,OpenMP,#pragma omp parallel sections)\n(32,OpenMP,#pragma omp section)\n(36,OpenMP,#pragma omp section)\n(42,OpenMP,#pragma omp section)", "label": "(19, MPI, MPI_Pack_size(2, MPI_DOUBLE, comm, &s2);); (38, MPI, MPI_Recv(rmsg2, 10, MPI_DOUBLE, src, tag, comm, MPI_STATUS_IGNORE);); (21, MPI, MPI_Buffer_attach(buf, bufsize);); (56, MPI, MPI_Buffer_detach(&bbuf, &bsize);); (34, MPI, MPI_Recv(rmsg1, 7, MPI_CHAR, src, tag, comm, MPI_STATUS_IGNORE);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (27, MPI, MPI_Bsend(msg2, 2, MPI_DOUBLE, dest, tag, comm);); (13, MPI, MPI_Pack_size(17, MPI_CHAR, comm, &s3);); (24, MPI, MPI_Bsend(msg1, 7, MPI_CHAR, dest, tag, comm);); (25, MPI, MPI_Bsend(msg3, 17, MPI_CHAR, dest, tag, comm);); (8, MPI, MPI_Pack_size(7, MPI_CHAR, comm, &s1);); (29, MPI, MPI_Recv(rmsg3, 17, MPI_CHAR, src, tag, comm, MPI_STATUS_IGNORE);)"}
{"predict": "(18,OpenMP,#pragma omp parallel if(device.has(::sycl::aspect::usm_system_allocations)))", "label": "(9, SYCL, sycl::device device;)"}
{"predict": "UMP: StringLiteral{{.*}}\"sycl\"\n95:   //DUMP: StringLiteral{{.*}}\"hip\"\n96:   //DUMP: StringLiteral{{.*}}\"level_zero\"\n97: \n98:   //PRINT: #pragma omp interop init(prefer_type(\"cuda\",\"cuda_driver\"), targetsync : I)\n99:   //DUMP: OMPInteropDirective\n100:  //DUMP: OMPInitClause\n101:  //DUMP: DeclRefExpr{{.*}}'omp_interop_t'{{.*}}Var{{.*}}'I'\n102:  //DUMP: StringLiteral{{.*}}\"cuda\"\n103:  //DUMP: StringLiteral{{.*}}\"cuda_driver\"\n104: }\n105: \n106: void foo2(int *ap, int dev) {\n107:   // PRINT: #pragma omp target data map(tofrom: ap[0:dev])\n108:   // DUMP: OMPTargetDataDirective\n109:   // DUMP: OMPMapClause\n110:   // DUMP: DeclRefExpr{{.*}}'ap' 'int *'\n111:   // DUMP: DeclRefExpr{{.*}}'dev' 'int'\n112:   #pragma omp target data map(tofrom: ap[0:dev])\n113: \n114:   // PRINT: #pragma omp target teams distribute parallel for\n115:   // DUMP: OMPTargetTeamsDistributeParallelForDirective\n116:   // DUMP: OMPMapClause\n117:   // DUMP: DeclRefExpr{{.*}}'ap' 'int *'\n118:   // DUMP: DeclRefExpr{{.*}}'dev' 'int'\n119:   #pragma omp target teams distribute parallel for\n120:     for (int i = 0; i < dev; i++) {\n121:       ap[i] = i * i;\n122:     }\n123: \n124:   // PRINT: #pragma omp target update from(ap[0:dev])\n125:   // DUMP: OMPTargetUpdateDirective\n126:   // DUMP: OMPMapClause\n127:", "label": "(140, OpenMP, #pragma omp interop use(I) use(J)); (129, OpenMP, #pragma omp interop init(target:I) init(targetsync:J)); (24, OpenMP, #pragma omp interop init(target:IRef)); (31, OpenMP, #pragma omp interop destroy(IRef)); (71, OpenMP, #pragma omp interop device(dev) depend(inout:ap) destroy(I)); (159, OpenMP, #pragma omp interop destroy(I) use(J)); (4, OpenMP, #pragma omp interop use(I)); (141, OpenMP, #pragma omp interop destroy(I) destroy(J)); (47, OpenMP, #pragma omp interop device(dev) depend(inout:ap) init(targetsync:I)); (7, OpenMP, #pragma omp interop destroy(I)); (-2, OpenMP, #pragma omp interop init(target:I)); (127, OpenMP, #pragma omp interop init(target:I) use(J)); (29, OpenMP, #pragma omp interop use(CI)); (84, OpenMP, #pragma omp interop init(prefer_type(2,4,6,1),targetsync:I)); (14, OpenMP, #pragma omp interop use(IRef)); (50, OpenMP, #pragma omp interop device(dev) depend(inout:ap) use(I)); (71, OpenMP, #pragma omp interop init(prefer_type(1,2,3,4,5,6),targetsync:I)); (145, OpenMP, #pragma omp interop init(target:I) destroy(J)); (-14, OpenMP, #pragma omp interop init()"}
{"predict": "(29,OpenMP,#pragma omp parallel for)", "label": "(36, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (38, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(30,OpenMP,#pragma omp parallel for collapse(2)), (75,OpenMP,#pragma omp parallel for collapse(2))", "label": "(86, CUDA, cudaFreeHost( outHost );); (84, CUDA, cudaFree( texDevice );)"}
{"predict": "(20,OpenMP,#pragma omp parallel for private(j,temp) reduction(min:sum2)), (32,OpenMP,#pragma omp parallel for reduction(min:sum)), (39,OpenMP,#pragma omp parallel for reduction(+:sum3))", "label": "(31, OpenMP, #pragma omp teams reduction(+:sum)); (28, OpenMP, #pragma omp target map(tofrom: b, sum, t) map(alloc: a)); (31, OpenMP, #pragma omp distribute parallel for reduction(+:sum)); (33, OpenMP, t[i] = omp_get_team_num();)"}
{"predict": "(94,OpenMP,#pragma omp parallel for)", "label": "(91, OpenMP, const int NumThreads = omp_get_max_threads();)"}
{"predict": "(67,OpenMP,#pragma omp parallel for private(ref, delta))", "label": "(44, CUDA, checkCudaErrors(cudaMemset(d_Output, 0, QRNG_DIMENSIONS * N * sizeof(float)));); (20, CUDA, int dev = findCudaDevice(argc, (const char **)argv);); (49, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (123, CUDA, checkCudaErrors(cudaFree(d_Output));); (18, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, dev));); (28, CUDA, checkCudaErrors(cudaMalloc((void **)&d_Output, QRNG_DIMENSIONS * N * sizeof(float)));); (60, CUDA, checkCudaErrors(cudaMemcpy(h_OutputGPU, d_Output, QRNG_DIMENSIONS * N * sizeof(float), cudaMemcpyDeviceToHost));)"}
{"predict": "6: static double njac[IMAX/2*2+1][JMAX/2*2+1][KMAX-1+1][5][5];\n117: static double lhsa[IMAX/2*2+1][JMAX/2*2+1][KMAX-1+1][5][5];\n118: static double lhsb[IMAX/2*2+1][JMAX/2*2+1][KMAX-1+1][5][5];\n119: static double lhsc[IMAX/2*2+1][JMAX/2*2+1][KMAX-1+1][5][5];\n120: static double lhsp[IMAX/2*2+1][JMAX/2*2+1][KMAX-1+1][5][5];\n121: static double lhsm[IMAX/2*2+1][JMAX/2*2+1][KMAX-1+1][5][5];\n122: \n123: /* COMMON block: work_lhs */\n124: static double tmp1[IMAX/2*2+1][JMAX/2*2+1][KMAX-1+1][5];\n125: static double tmp2[IMAX/2*2+1][JMAX/2*2+1][KMAX-1+1][5];\n126: \n127: \n128: /* COMMON block: work_rhs */\n129: static double rhs1[IMAX/2*2+1][JMAX/2*2+1][KMAX-1+1][5];\n130: static double rhs2[IMAX/2*2+1][JMAX/2*2+1][KMAX-1+1][5];\n131: static double rhs3[IMAX/2*2+1][JMAX/2*2+1][KMAX-1+1][5];\n132: static double rhs4[IMAX/2*2+1][JMAX/2*2+1][KMAX-1+1][5];\n133: static double rhs5[IMAX/2*2+1][JMAX/2*2+1][KMAX-1+1", "label": "(249, OpenMP, nthreads = omp_get_num_threads();); (234, OpenMP, #pragma omp parallel firstprivate(niter) private(step)); (214, OpenMP, #pragma omp parallel); (237, OpenMP, #pragma omp master)"}
{"predict": "The provided code does not contain any loops or sections that can be parallelized using MPI or OpenMP, as it is primarily a serial code with conditional checks and error reporting. However, if we assume that the intention is to parallelize a hypothetical loop that might be added later, I will provide an example of where such a parallelization directive could be inserted. For now, there are no clear sections to parallelize in the given code.\n\nIf you were to add a loop for demonstration purposes, here is an example of how you might parallelize it using OpenMP:\n\n### **Modified Code with Hypothetical Loop**\n\n1: int main(int argc, char **argv)\n2: {\n3:     char *p;\n4:     int errs = 0, toterrs;\n5:     int size, rank;\n6: \n7:     // Hypothetical loop to demonstrate parallelization\n8:     int i;\n9:     for (i = 0; i < 1000; i++) {\n10:         // Some computation here\n11:     }\n12: \n13:     p = getenv(\"TMP_ENV_VAR\");\n14:     if (!p) {\n15:         errs++;\n16:         printf(\"Did not find TMP_ENV_VAR\\n\");\n17:     } else if (strcmp(p, \"1\") != 0) {\n18:         errs++;\n19:         printf(\"Value of TMP_ENV_VAR was %s, expected 1\\n\", p);\n20:     }\n21: \n22:     if (rank == 0) {\n23:         if (toterrs == 0) {\n24:             printf(\" No Errors\\n\");\n25:         } else {\n26:             printf(\" Found %d errors\\n\", toterrs);\n27:         }\n28:     }\n29: \n30:     return 0;\n31: }\n\n### **Parallelization Instruction**\n\n(9,OpenMP,#pragma omp parallel for)\n\nIf you have a specific loop or section in mind that you would like to parallelize, please provide the code, and I can give you the exact parallelization instructions.", "label": "(6, MPI, MPI_Init(&argc, &argv);); (32, MPI, MPI_Finalize();); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (20, MPI, MPI_Reduce(&errs, &toterrs, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);)"}
{"predict": "28:                 rpeer += nleaderranks;\n129:             }\n130:         }\n131:     }\n132: \n133: // Gather hoststrings into allhoststrings on rank 0\n134:     allhoststrings = malloc(numhosts * sizeof(char *));\n135:     if (!allhoststrings) {\n136:         free(method);\n137:         ompi_comm_free(&local_comm);\n138:         ompi_comm_free(&leader_comm);\n139:         return;\n140:     }\n141: \n142:     ret = MPI_Gather(hoststring, strlen(hoststring) + 1, MPI_CHAR,\n143:         allhoststrings, strlen(hoststring) + 1, MPI_CHAR, 0, leader_comm);\n144:     if (OPAL_UNLIKELY(OMPI_SUCCESS != ret)) {\n145:         free(hoststring);\n146:         free(allhoststrings);\n147:         free(method);\n148:         ompi_comm_free(&local_comm);\n149:         ompi_comm_free(&leader_comm);\n150:         return;\n151:     }\n152:     free(hoststring);\n153: \n154:     if (myleaderrank == 0) {\n155:         // Print the host information.\n156:         for (i=0; i<numhosts; ++i) {\n157:             printf(\"%s\\n\", allhoststrings[i]);\n158:         }\n159:         for (i=0; i<numhosts; ++i) {\n160:             free(allhoststrings[i]);\n161:         }\n162:         free(allhoststrings);\n163:     }\n164: \n165:     // Free communicator resources.\n166:     ompi_comm_free(&local_comm);\n167:     ompi_comm_free(&leader_comm);\n168: \n169:     // Free the method array.\n170:     free(method);\n171: }", "label": "(150, MPI, MPI_Type_commit(&mydt);); (154, MPI, MPI_Op_free(&myop);); (154, MPI, MPI_Type_free(&mydt);); (146, MPI, MPI_Type_contiguous(sizeof(comm_method_string_conversion_t), MPI_BYTE, &mydt);); (147, MPI, MPI_Op_create(myfn, 1, &myop);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for)", "label": "(8, MPI, MPI_Alloc_mem(DATA_SZ, MPI_INFO_NULL, &base_ptr[0]);); (20, MPI, MPI_Win_free_keyval(&keyval);); (21, MPI, MPI_Win_free(&windows[1]);); (6, MPI, MPI_Win_create(base_ptr[0], DATA_SZ, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &windows[0]);); (12, MPI, MPI_Win_set_attr(windows[0], keyval, NULL);); (5, MPI, MPI_Alloc_mem(DATA_SZ, MPI_INFO_NULL, &base_ptr[1]);); (14, MPI, MPI_Free_mem(base_ptr[0]);); (15, MPI, MPI_Win_free_keyval(&keyval_copy);); (16, MPI, MPI_Free_mem(base_ptr[1]);); (4, MPI, MPI_Win_create_keyval(MPI_WIN_NULL_COPY_FN, delete_fn, &keyval, NULL);); (9, MPI, MPI_Win_free(&windows[0]);); (0, MPI, MPI_Win_create(base_ptr[1], DATA_SZ, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &windows[1]);); (5, MPI, MPI_Win_set_attr(windows[1], keyval, NULL);)"}
{"predict": "(54,OpenMP,#pragma omp parallel for collapse(2)), (73,OpenMP,#pragma omp parallel for collapse(2))", "label": "(46, CUDA, thrust::fill(thrust::device, B.begin(), B.end(), 0.0);); (44, CUDA, thrust::sequence(thrust::device, A.begin(), A.end() );); (40, CUDA, thrust::device_vector<double> A(order*order);); (40, CUDA, thrust::device_vector<double> B(order*order);)"}
{"predict": "(57,OpenMP,#pragma omp parallel for)", "label": "(2, CUDA, extern __shared__ char shared_memory[];)"}
{"predict": "(17,OpenMP,#pragma omp parallel for private(loss))", "label": "(5, CUDA, extern __shared__ float s_data[];)"}
{"predict": "(16,OpenMP,#pragma omp parallel for)", "label": "(58, OpenMP, #pragma omp sections reduction(|| : argc > 0 ? argv[1] : argv[2])); (102, OpenMP, #pragma omp sections reduction(- : da)); (131, OpenMP, #pragma omp sections reduction(+ : o)); (75, OpenMP, #pragma omp sections reduction(+ : a, b, c, d, f)); (160, OpenMP, #pragma omp sections reduction(+ : fl)); (83, OpenMP, #pragma omp sections reduction(max : h.b)); (117, OpenMP, #pragma omp sections reduction(& : e, g)); (146, OpenMP, #pragma omp sections reduction(+ : r)); (20, OpenMP, #pragma omp sections reduction(-); (129, OpenMP, #pragma omp sections private(i), reduction(+ : j), reduction(+ : q)); (164, OpenMP, #pragma omp sections reduction(+ : m)); (102, OpenMP, #pragma omp sections reduction(&& : S2::S2s)); (61, OpenMP, #pragma omp sections reduction(^ : S1)); (80, OpenMP, #pragma omp sections reduction(+ : ba)); (154, OpenMP, #pragma omp parallel reduction(* : fl)); (-3, OpenMP, #pragma omp parallel); (67, OpenMP, #pragma omp sections reduction(min : a, b, c, d, f)); (111, OpenMP, #pragma omp sections reduction(+ : h, k, B::x)); (0, OpenMP, #pragma omp sections reduction +); (-6, OpenMP, #pragma omp sections reduction); (122, OpenMP, #pragma omp parallel private(k)); (87, OpenMP, #pragma omp sections reduction(^ : fl)); (31, OpenMP, #pragma omp sections reduction(| : argc,); (120, OpenMP, #pragma omp sections reduction(+ : p), reduction(+ : p)); (154, OpenMP, #pragma omp sections reduction(task, + : m)); (13, OpenMP, #pragma omp sections reduction(*)); (132, OpenMP, #pragma omp parallel reduction(min : i)); (136, OpenMP, #pragma omp parallel private(fl)); (40, OpenMP, #pragma omp sections reduction(&& : argc, z)); (69, OpenMP, #pragma omp sections reduction(* : ca)); (3, OpenMP, #pragma omp sections reduction()); (87, OpenMP, #pragma omp sections reduction(&& : S2::S2sc)); (127, OpenMP, #pragma omp sections reduction(max : j)); (-10, OpenMP, #pragma omp sections reduction(); (9, OpenMP, #pragma omp sections reduction(\\)); (13, OpenMP, #pragma omp sections reduction(foo : argc); (121, OpenMP, #pragma omp parallel shared(i)); (26, OpenMP, #pragma omp sections reduction(~ : argc))"}
{"predict": "The provided code is a function `initialize_cublas` that initializes a cuBLAS plugin for CUDA. The function does not contain any loops or sections that can be parallelized using OpenMP or MPI directly. The code primarily involves function calls and error handling, which are not inherently parallelizable.\n\nHowever, if we assume that there are loops or other parallelizable sections in the functions called within `initialize_cublas` (e.g., within `blas->Init()` or other parts of the code not shown), we could consider parallelizing those sections.\n\nSince the given code does not have any obvious parallelizable sections, there are no specific lines where parallelization instructions should be inserted. If you have additional code or loops that can be parallelized, please provide them for further analysis.\n\nFor the sake of the exercise, if we hypothetically had a loop in the `blas->Init()` function, we could consider parallelizing it. Here is an example of how you might annotate such a loop with OpenMP:\n\nAssume `blas->Init()` has a loop like this:\n```cpp\nfor (int i = 0; i < n; ++i) {\n  // some computations\n}\n```\n\nYou would insert the parallelization instruction like this:\n(?,OpenMP,#pragma omp parallel for)\n\nHowever, since there is no such loop in the provided code, no parallelization instructions are needed.\n\nTherefore, the output remains empty for the given code:\n()", "label": "(14, CUDA, cuda::CUDABlas *blas = new cuda::CUDABlas(cuda_executor);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(13, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize());); (11, CUDA, qmc_cuda::cuda_check(cudaGetLastError());)"}
{"predict": "(1,OpenMP,#pragma omp parallel for)", "label": "(11, MPI, MPI_Error_string(code, msg, &resultLen);); (12, MPI, MPI_Abort(MPI_COMM_WORLD, code);); (8, MPI, MPI_Error_class(code, &class);)"}
{"predict": "(1,OpenMP,#pragma omp parallel for collapse(2))", "label": "(2, CUDA, __shared__ uchar as[18][66];)"}
{"predict": "(20,OpenMP,#pragma omp parallel for)\n(25,OpenMP,#pragma omp parallel for)\n(30,OpenMP,#pragma omp parallel for)\n(35,OpenMP,#pragma omp parallel for)\n(40,OpenMP,#pragma omp parallel for)\n(45,OpenMP,#pragma omp parallel for)\n(50,OpenMP,#pragma omp parallel for)", "label": "(25, OpenMP, #pragma omp target parallel for is_device_ptr(z)); (36, OpenMP, #pragma omp target parallel for is_device_ptr(raa)); (29, OpenMP, #pragma omp target parallel for is_device_ptr(aa)); (46, OpenMP, #pragma omp target parallel for is_device_ptr(rh)); (39, OpenMP, #pragma omp target parallel for is_device_ptr(h)); (14, OpenMP, #pragma omp target parallel for is_device_ptr(k)); (49, OpenMP, #pragma omp target parallel for is_device_ptr(da))"}
{"predict": "(8,OpenMP,#pragma omp parallel for collapse(2) private(j) reduction(&&:my_islarger))", "label": "(11, OpenMP, #pragma omp ordered); (3, OpenMP, #pragma omp parallel); (13, OpenMP, #pragma omp critical); (5, OpenMP, #pragma omp for private(i,j) schedule(static,1) collapse(2) ordered)"}
{"predict": "(27,OpenMP,#pragma omp parallel sections), (43,OpenMP,#pragma omp section), (57,OpenMP,#pragma omp parallel for), (65,OpenMP,#pragma omp parallel sections)", "label": "(60, OpenMP, #pragma omp sections lastprivate(t_var, vec, s_arr, var, sivar)); (4, OpenMP, #pragma omp section); (2, OpenMP, #pragma omp parallel); (28, OpenMP, #pragma omp sections lastprivate(g, sivar)); (65, OpenMP, #pragma omp sections lastprivate(CONDITIONAL A::x, B::x))"}
{"predict": "(9,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for)", "label": "(10, CUDA, cudaMallocHost(&h_ptr, THREAD * sizeof(void *));); (29, CUDA, cudaFree(d_ptr);); (25, CUDA, cudaMemcpy(h_clk, d_clk, THREAD * sizeof(uint32_t), cudaMemcpyDeviceToHost);); (28, CUDA, cudaFree(d_ret);); (12, CUDA, cudaMemcpy(d_ptr, h_ptr, THREAD * sizeof(void *), cudaMemcpyHostToDevice);); (27, CUDA, cudaFree(d_clk);); (-1, CUDA, cudaMalloc(&d_ptr, THREAD * sizeof(void *));); (26, CUDA, cudaFreeHost(h_ptr);); (-1, CUDA, cudaMalloc(&d_clk, THREAD * sizeof(uint32_t));); (-3, CUDA, cudaMalloc(&d_ret, sizeof(void *));)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(14, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(5,OpenMP,#pragma omp parallel for private(cf, global_execute_count)), (23,OpenMP,#pragma omp parallel for private(senders)), (33,OpenMP,#pragma omp parallel for), (41,OpenMP,#pragma omp parallel for), (54,OpenMP,#pragma omp parallel for)", "label": "(69, TBB, g.reset(tbb::flow::rf_reset_bodies);); (28, TBB, tbb::flow::make_edge( senders[s], exe_node );); (19, TBB, tbb::flow::make_edge( exe_node, *receivers[r] );); (61, TBB, inc_functor body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (66, TBB, body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (0, TBB, tbb::flow::graph g;); (44, TBB, tbb::flow::remove_edge( exe_node, *receivers[r] );); (4, TBB, tbb::flow::function_node< InputType, OutputType > exe_node( g, lc, cf );)"}
{"predict": "(4,OpenMP,#pragma omp parallel for private(defaultConstructed_sum, copyConstructed_sum, copyAssigned_sum, moveConstructed_sum, moveAssigned_sum))", "label": "(8, TBB, oneapi::tbb::global_control gc(oneapi::tbb::global_control::max_allowed_parallelism, p);); (23, TBB, oneapi::tbb::parallel_for( oneapi::tbb::blocked_range<int> (0, N, 10000), ParallelVectorForBody<T>( vs ) );); (0, TBB, typedef std::vector<T, oneapi::tbb::tbb_allocator<T> > ContainerType;); (17, TBB, typedef typename oneapi::tbb::combinable< ContainerType > CombinableType;)"}
{"predict": "(5,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for private(receivers)), (18,OpenMP,#pragma omp parallel for private(senders)), (24,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for), (36,OpenMP,#pragma omp parallel for), (41,OpenMP,#pragma omp parallel for), (45,OpenMP,#pragma omp parallel for)", "label": "(16, TBB, tbb::flow::make_edge( exe_node, receivers[r] );); (23, TBB, tbb::flow::make_edge( senders[s], exe_node );); (43, TBB, tbb::flow::remove_edge( exe_node, receivers[r] );); (53, TBB, inc_functor body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (1, TBB, tbb::flow::graph g;); (6, TBB, tbb::flow::function_node< InputType, OutputType > exe_node( g, lc, cf );)"}
{"predict": "(18,OpenMP,#pragma omp parallel for), (35,OpenMP,#pragma omp parallel for), (47,MPI,MPI_Init(&argc, &argv);), (53,MPI,MPI_Barrier(MPI_COMM_WORLD);), (58,MPI,MPI_Finalize();)", "label": "(29, CUDA, cudaMalloc((void **)&d_C_ref, m * n * sizeof(float));); (68, CUDA, cudaEventElapsedTime(&elapsed_time, start, stop);); (30, CUDA, cudaMemcpy(d_A, A, m * k * sizeof(float), cudaMemcpyHostToDevice);); (32, CUDA, cudaMemcpy(d_C_ref, C_ref, m * n * sizeof(float), cudaMemcpyHostToDevice);); (36, CUDA, cudaMemcpy(C, d_C, m * n * sizeof(float), cudaMemcpyDeviceToHost);); (61, CUDA, cudaEventRecord(stop);); (61, CUDA, cudaEventSynchronize(stop);); (15, CUDA, cudaMalloc((void **)&d_B, k * n * sizeof(float));); (13, CUDA, cudaMalloc((void **)&d_A, m * k * sizeof(float));); (52, CUDA, cudaEventRecord(start);); (13, CUDA, cudaMalloc((void **)&d_C, m * n * sizeof(float));); (23, CUDA, cudaMemcpy(d_C, C, m * n * sizeof(float), cudaMemcpyHostToDevice);); (21, CUDA, cudaMemcpy(d_B, B, k * n * sizeof(float), cudaMemcpyHostToDevice);); (46, CUDA, cudaEventCreate(&stop);); (44, CUDA, cudaEventCreate(&start);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for private(ctx, i))", "label": "(6, OpenMP, #pragma omp parallel for)"}
{"predict": "(2,OpenMP,#pragma omp target teams distribute parallel for collapse(2))", "label": "(8, CUDA, A[blockIdx.z * stride + i * lda + i] = thrust::complex<T>(1.0, 0.0);); (11, CUDA, A[blockIdx.z * stride + j * lda + i] = thrust::complex<T>(0.0, 0.0);)"}
{"predict": "(17,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for)", "label": "(25, CUDA, cudaMalloc((void**)(&d_t_right), t_right_bytes);); (25, CUDA, cudaMalloc((void**)(&d_t_result), t_result_bytes);); (50, CUDA, cudaFree((void*)d_t_left);); (26, CUDA, cudaMemcpy(d_t_right, t_right.data(), t_right_bytes, cudaMemcpyHostToDevice);); (20, CUDA, cudaMalloc((void**)(&d_t_left), t_left_bytes);); (23, CUDA, cudaMemcpy(d_t_left, t_left.data(), t_left_bytes, cudaMemcpyHostToDevice);); (48, CUDA, cudaFree((void*)d_t_result);); (46, CUDA, cudaFree((void*)d_t_right);); (36, CUDA, cudaMemcpy(t_result_gpu.data(), d_t_result, t_result_bytes, cudaMemcpyDeviceToHost);)"}
{"predict": "(35,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &myrank);)", "label": "(37, MPI, MPI_Comm_rank(adio_fh->comm, &myrank);); (24, MPI, MPI_Bcast(&tmp_whence, 1, MPI_INT, 0, adio_fh->comm);); (12, MPI, MPI_Bcast(&tmp_offset, 1, ADIO_OFFSET, 0, adio_fh->comm);); (110, MPI, MPI_Barrier(adio_fh->comm);)"}
{"predict": "(6,OpenMP,#pragma omp parallel sections), (14,OpenMP,#pragma omp section)", "label": "(22, OpenMP, #pragma omp flush(out)); (17, OpenMP, #pragma omp atomic read); (10, OpenMP, #pragma omp atomic write); (-1, OpenMP, #pragma omp target parallel num_threads(64) map(tofrom); (6, OpenMP, #pragma omp flush(data))"}
{"predict": "(10,OpenMP,#pragma omp parallel for private(n))", "label": "(11, TBB, tbb::task_scheduler_init init(nthread);)"}
{"predict": "(1,MPI,MPI_Init(&argc, &argv);); (4,MPI,MPI_Comm_size(MPI_COMM_WORLD, &size);); (5,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (7,MPI,MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, 0, MPI_INFO_NULL, &comm_node);); (8,MPI,MPI_Comm_size(comm_node, &size_node);); (9,MPI,MPI_Comm_rank(comm_node, &rank_node);); (11,MPI,MPI_Get_processor_name(procname, &namelen);); (28,MPI,MPI_Finalize(););", "label": "(12, MPI, MPI_Comm_size(comm_node, &size_node);); (1, MPI, MPI_Init(&argc, &argv);); (32, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (13, MPI, MPI_Get_processor_name(procname, &namelen);); (5, MPI, MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, 0, MPI_INFO_NULL, &comm_node);); (-1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (6, MPI, MPI_Comm_rank(comm_node, &rank_node);); (19, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for private(index))", "label": "(12, OpenMP, #pragma omp parallel for)"}
{"predict": "(26,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for reduction(+:c))", "label": "(56, CUDA, HANDLE_ERROR( cudaEventDestroy( start ) );); (6, CUDA, HANDLE_ERROR( cudaEventCreate( &stop ) );); (55, CUDA, HANDLE_ERROR( cudaEventDestroy( stop ) );); (36, CUDA, HANDLE_ERROR( cudaThreadSynchronize() );); (20, CUDA, HANDLE_ERROR( cudaHostGetDevicePointer( &dev_b, b, 0 ) );); (18, CUDA, HANDLE_ERROR( cudaHostGetDevicePointer( &dev_a, a, 0 ) );); (28, CUDA, HANDLE_ERROR( cudaEventRecord( start, 0 ) );); (33, CUDA, HANDLE_ERROR( cudaEventRecord( stop, 0 ) );); (33, CUDA, HANDLE_ERROR( cudaEventSynchronize( stop ) );); (43, CUDA, HANDLE_ERROR( cudaFreeHost( b ) );); (41, CUDA, HANDLE_ERROR( cudaFreeHost( a ) );); (42, CUDA, HANDLE_ERROR( cudaFreeHost( partial_c ) );); (-6, CUDA, HANDLE_ERROR( cudaEventCreate( &start ) );)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(13, MPI, mpi_err = MPI_Type_commit(&type);); (9, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(4,OpenMP,#pragma omp parallel for reduction(+:sum))", "label": "(7, TBB, RangeType range2( range, tbb::split() );)"}
{"predict": "(8,OpenMP,#pragma omp parallel for reduction(+:result)), (13,OpenMP,#pragma omp parallel for reduction(+:result))", "label": "(20, MPI, MPI_Allreduce( & tmp , & result , 1 , MPI_DOUBLE , MPI_SUM , MPI_COMM_WORLD );)"}
{"predict": "(12,OpenMP,#pragma omp parallel for reduction(+:local[id]))", "label": "(3, CUDA, __shared__ float local[threads];)"}
{"predict": "(2,OpenMP,#pragma omp parallel for private(tid, bid, idx, reg_x, value, reg_x_hat, variance, reg_y))", "label": "(9, CUDA, __shared__ float s_variance;); (7, CUDA, __shared__ float s_mean;)"}
{"predict": "(62,OpenMP,#pragma omp parallel for private(lower_idx, upper_idx, y, x0, x1, u0, u1))", "label": "(99, MPI, MPI_File_open(MPI_COMM_WORLD, file_name.c_str(), MPI_MODE_CREATE | MPI_MODE_WRONLY, MPI_INFO_NULL, &file);); (105, MPI, MPI_File_write(file, &size_array, 1, MPI_INT, &status);); (110, MPI, MPI_File_close(&file);); (102, MPI, MPI_File_seek(file, mpi_offset, MPI_SEEK_SET);); (107, MPI, MPI_File_write(file, &pos_values[0], data_size[rank], MPI_DOUBLE, &status);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for private(iN, counter, start, size, cDiv, break_loop, sum) reduction(+:V[:N*O]) reduction(max:I[:N*O]))\n(29,OpenMP,#pragma omp parallel for private(iN, counter, start, size, cDiv, break_loop, sum) reduction(+:V[:N*O]) reduction(max:I[:N*O]))\n(44,OpenMP,#pragma omp parallel for private(iN, counter, start, size, cDiv, break_loop, sum) reduction(+:V[:N*O]) reduction(max:I[:N*O]))\n(55,OpenMP,#pragma omp parallel for reduction(+:sum))", "label": "(16, CUDA, extern __shared__ volatile float Bsh[];)"}
{"predict": "(22,OpenMP,#pragma omp parallel for)", "label": "(8, MPI, MPI_Init(&argc, &argv);); (28, MPI, MPI_Finalize();)"}
{"predict": "(23,OpenMP,#pragma omp parallel for)", "label": "(20, CUDA, cudaStream_t stream = at::cuda::getCurrentCUDAStream();)"}
{"predict": "(107,OpenMP,#pragma omp parallel for), (108,OpenMP,        kernel_fn<<<p.getBlocksGrid(), p.getThreadsGrid(), smem_bytes>>>(p);)", "label": "(85, CUDA, cudaDeviceSynchronize();); (59, CUDA, cudaMalloc(&p.workspace, p.workspace_size());); (114, CUDA, CUDA_CHECK(cudaEventSynchronize(events[1]));); (112, CUDA, CUDA_CHECK(cudaEventRecord(events[1]));); (84, CUDA, CUDA_CHECK(cudaFuncSetAttribute(kernel_fn, cudaFuncAttributeMaxDynamicSharedMemorySize, int(smem_bytes)));); (114, CUDA, CUDA_CHECK(cudaEventElapsedTime(&runtime_ms, events[0], events[1]));); (9, CUDA, std::cerr << \"FATAL: \" #FN \" failed: \" << cudaGetErrorString(cudaError) << std::endl;); (102, CUDA, CUDA_CHECK(cudaEventCreate(&event));); (103, CUDA, CUDA_CHECK(cudaEventRecord(events[0]));)"}
{"predict": "(5,OpenMP,#pragma omp parallel for reduction(+:total)), (10,OpenMP,#pragma omp parallel for private(cur) collapse(2))", "label": "(19, CUDA, cudaMemcpy(pointers, cpuPointers, sizeof(uint8_t*) * total * 3, cudaMemcpyHostToDevice);); (6, CUDA, uint8_t ** pointers = (uint8_t**)FastllmCudaMalloc(sizeof(uint8_t*) * total * 3);); (20, CUDA, FastllmCudaFree(pointers);)"}
{"predict": ";\n112:         }\n113:     }\n114:     Print( A, \"Matrix A 2\" );\n115: \n116:     /* Part 3 */\n117:     //Backward substitution\n118:     for ( int i = 0; i < log2( N + 1 ) - 1; ++ i )\n119:     {\n120:         for ( int j = numactivep - 1; j >= 0; -- j )\n121:         {\n122:             if ( Parallel::pid == activep[ j ] )\n123:             {\n124:                 index1 = 2 * Parallel::pid + 1 - pow( 2, i );\n125:                 index2 = 2 * Parallel::pid + 1 + pow( 2, i );\n126:                 alpha = A[ 1 ][ index1 ] / A[ 3 ][ index1 ];\n127:                 gamma = A[ 1 ][ index2 ] / A[ 4 ][ index2 ];\n128:                 for ( int k = 0; k < N + 1; ++ k )\n129:                 {\n130:                     A[ 1 ][ k ] -= ( alpha * A[ 3 ][ k ] + gamma * A[ 4 ][ k ] );\n131:                 }\n132:             }\n133:         }\n134:         numactivep = 0;\n135:         for ( int j = activep[ 1 ]; j < Parallel::nprocs; j = j + pow( 2, i + 1 ) )\n136:         {\n137:             activep[ numactivep ++ ] = j;\n138:         }\n139:     }\n140:     Print( A, \"Matrix A 3\" );\n141: \n142:     return 0;\n143: }", "label": "(106, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ j + 1 ], 0, MPI_COMM_WORLD );); (104, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ j - 1 ], 1, MPI_COMM_WORLD );); (108, MPI, MPI_Recv( A[ 3 ].data(), N + 1, MPI_DOUBLE, activep[ j - 1 ], 0, MPI_COMM_WORLD, &status );); (7, MPI, MPI_Comm_size( MPI_COMM_WORLD, &Parallel::nprocs );); (97, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ numactivep - 2 ], 1, MPI_COMM_WORLD );); (135, MPI, MPI_Allgather( &tmp, 1, MPI_DOUBLE, x.data(), 1, MPI_DOUBLE, MPI_COMM_WORLD);); (187, MPI, MPI_Finalize();); (157, MPI, MPI_Allgather( &tmp, 1, MPI_DOUBLE, x.data(), 1, MPI_DOUBLE, MPI_COMM_WORLD );); (1, MPI, MPI_Init( &argc, &argv );); (2, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &Parallel::pid );); (101, MPI, MPI_Recv( A[ 4 ].data(), N + 1, MPI_DOUBLE, activep[ j + 1 ], 1, MPI_COMM_WORLD, &status );); (86, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ 1 ], 0, MPI_COMM_WORLD );)"}
{"predict": "(34,OpenMP,#pragma omp parallel for)", "label": "(25, OpenMP, #pragma omp parallel default(shared)); (10, OpenMP, #pragma omp parallel); (37, OpenMP, #pragma omp parallel num_threads(x)); (15, OpenMP, #pragma omp parallel default(none)); (28, OpenMP, #pragma omp parallel default(firstprivate))"}
{"predict": "(33,OpenMP,#pragma omp parallel for collapse(2))", "label": "(41, CUDA, __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];); (36, CUDA, __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];)"}
{"predict": "(5,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for)", "label": "(12, CUDA, __shared__ unsigned char sharedHistogram[NUM_THREADS][cBuckets];)"}
{"predict": "(8,OpenMP,#pragma omp parallel sections), (16,OpenMP,#pragma omp section), (23,OpenMP,#pragma omp section), (34,OpenMP,#pragma omp section)", "label": "(25, TBB, tbb::flow::make_edge(bnode, tbb::flow::input_port<0>(jnode));); (7, TBB, tbb::flow::make_edge(bnode, fnode);); (22, TBB, tbb::flow::join_node<std::tuple<int,int>,tbb::flow::reserving> jnode(g);); (-1, TBB, tbb::flow::sequencer_node<int> bnode(g, seq_body());); (-3, TBB, tbb::flow::graph g;); (32, TBB, g.reset(tbb::flow::rf_clear_edges);); (-2, TBB, tbb::flow::function_node<int> fnode(g, tbb::flow::serial, serial_fn_body<int>(serial_fn_state0));); (15, TBB, tbb::flow::remove_edge(bnode, fnode);)"}
{"predict": "(28,OpenMP,#pragma omp parallel for reduction(+:item_sum)), (51,OpenMP,#pragma omp parallel for)", "label": "(34, TBB, oneapi::tbb::tick_count start = oneapi::tbb::tick_count::now();); (69, TBB, utility::report_elapsed_time((oneapi::tbb::tick_count::now() - start).seconds());); (49, TBB, oneapi::tbb::flow::make_edge(the_source, the_value_pool);); (53, TBB, oneapi::tbb::flow::make_edge(the_value_pool, *(bins[i]));); (44, TBB, oneapi::tbb::flow::graph g;); (31, TBB, oneapi::tbb::global_control c(oneapi::tbb::global_control::max_allowed_parallelism, p);)"}
{"predict": "(4,OpenMP,#pragma omp parallel for)\n(7,OpenMP,#pragma omp parallel for)\n(10,OpenMP,#pragma omp parallel for)\n(12,OpenMP,#pragma omp parallel for)\n(15,OpenMP,#pragma omp parallel for)\n(18,OpenMP,#pragma omp parallel for)\n(21,OpenMP,#pragma omp parallel for)\n(27,OpenMP,#pragma omp parallel for)\n(30,OpenMP,#pragma omp parallel for)\n(42,OpenMP,#pragma omp parallel for)\n(45,OpenMP,#pragma omp parallel for)", "label": "(37, OpenMP, #pragma omp target teams distribute parallel for simd safelen (S)); (17, OpenMP, #pragma omp target teams distribute parallel for simd safelen (ST); (20, OpenMP, #pragma omp target teams distribute parallel for simd safelen (1))); (51, OpenMP, #pragma omp target teams distribute parallel for simd safelen (N)); (46, OpenMP, #pragma omp target teams distribute parallel for simd safelen (4)); (2, OpenMP, #pragma omp target teams distribute parallel for simd safelen (); (20, OpenMP, #pragma omp target teams distribute parallel for simd safelen ((ST > 0) ? 1 + ST : 2)); (26, OpenMP, #pragma omp target teams distribute parallel for simd safelen (foobool(argc)), safelen (true), safelen (-5)); (3, OpenMP, #pragma omp target teams distribute parallel for simd safelen ()); (5, OpenMP, #pragma omp target teams distribute parallel for simd safelen (argc); (-7, OpenMP, #pragma omp target teams distribute parallel for simd safelen)"}
{"predict": "(31,OpenMP,#pragma omp parallel for reduction(+:intermediate_val))", "label": "(13, CUDA, extern __shared__ char shared_mem[];)"}
{"predict": "The provided code snippet is a function that checks the aspect ratio of a 3D grid and outputs an error message if the aspect ratio is below a specified threshold. This function is not inherently parallelizable because it performs a single calculation and conditional check. However, if this function is part of a larger loop or section of code that can be parallelized, I'll provide a hypothetical example where parallelization might be beneficial.\n\nAssuming there is a loop in the calling code that repeatedly calls `CheckAspectRatio` with different parameters, you could parallelize that loop. Here's an example of how you might do that with OpenMP:\n\n```cpp\n// Hypothetical loop in the calling code\nfor (int i = 0; i < num_checks; ++i) {\n    int result = CheckAspectRatio(smallest_ratio, x_values[i], y_values[i], z_values[i], what, DoIo);\n    // Do something with result\n}\n```\n\nIn this case, you would insert the OpenMP parallelization directive before the loop. However, since the provided code does not contain such a loop, I'll assume the loop is in the calling code and provide the parallelization directive for that loop.\n\n(1,MPI,*w = MPI_Wtime();); (1,OpenMP,#pragma omp parallel for)\n```cpp\n(1,OpenMP,#pragma omp parallel for)\n```\n\nSince the provided code does not contain a loop that can be parallelized, the above line is a hypothetical example. If you provide the calling code, I can give a more accurate parallelization directive.", "label": "(14, MPI, MPI_Abort(MPI_COMM_WORLD, 127);)"}
{"predict": "(24,OpenMP,#pragma omp parallel for)", "label": "(51, CUDA, cuda_safe_call(cudaStreamSynchronize(stream));)"}
{"predict": "The provided code snippet is a unit test for OpenCL and DNNL (Intel's Deep Neural Network Library) interoperability. It does not contain any explicit loops or sections that are inherently parallelizable with OpenMP or MPI. However, if we assume that there are loops or sections of code that could be parallelized in a larger context or a modified version of this test, I will provide a hypothetical example based on a typical scenario where you might want to parallelize loops.\n\nLet's assume that there is a loop in the test that processes data in parallel. For the sake of this example, I will insert a hypothetical loop in the code and then provide the parallelization instructions.\n\n### **Hypothetical Code with a Loop**\n\n```cpp\n1: TEST_F(ocl_stream_test_c_t, BasicInteropC) {\n2:     SKIP_IF(!find_ocl_device(CL_DEVICE_TYPE_GPU),\n3:             \"OpenCL GPU devices not found.\");\n4: \n5:     cl_int err;\n6: #ifdef CL_VERSION_2_0\n7:     cl_command_queue interop_ocl_queue = clCreateCommandQueueWithProperties(\n8:             ocl_ctx, ocl_dev, nullptr, &err);\n9: #else\n10:     cl_command_queue interop_ocl_queue\n11:             = clCreateCommandQueue(ocl_ctx, ocl_dev, 0, &err);\n12: #endif\n13:     TEST_OCL_CHECK(err);\n14: \n15:     dnnl_stream_t stream;\n16:     DNNL_CHECK(dnnl_ocl_interop_stream_create(&stream, eng, interop_ocl_queue));\n17: \n18:     cl_command_queue ocl_queue;\n19:     DNNL_CHECK(dnnl_ocl_interop_stream_get_command_queue(stream, &ocl_queue));\n20:     ASSERT_EQ(ocl_queue, interop_ocl_queue);\n21: \n22:     cl_uint ref_count;\n23:     TEST_OCL_CHECK(clGetCommandQueueInfo(interop_ocl_queue,\n24:             CL_QUEUE_REFERENCE_COUNT, sizeof(cl_uint), &ref_count, nullptr));\n25:     int i_ref_count = int(ref_count);\n26:     ASSERT_EQ(i_ref_count, 2);\n27: \n28:     // Hypothetical loop that could be parallelized\n29:     int n = 100", "label": "(34, OpenCL, TEST_OCL_CHECK(clReleaseCommandQueue(interop_ocl_queue));)"}
{"predict": "(13,OpenMP,#pragma omp parallel for collapse(5)), (23,OpenMP,#pragma omp parallel for collapse(5))", "label": "(15, OpenMP, #pragma omp taskloop simd private(argc, b), firstprivate(c, d), lastprivate(d, f) collapse(N) shared(g) if (c) final(d) mergeable priority(f) simdlen(N) nogroup num_tasks(N)); (13, OpenMP, #pragma omp parallel); (5, OpenMP, #pragma omp taskloop simd allocate(d) if(taskloop: argc > N) default(shared) untied priority(N) safelen(N) linear(c) aligned(ptr) grainsize(N) reduction(+:g) in_reduction(+: d)); (3, OpenMP, #pragma omp taskgroup task_reduction(+: d) allocate(d))"}
{"predict": "(20,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for reduction(+:errs))", "label": "(10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (43, MPI, mpi_errno = MPI_Comm_free(&stream_comm);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (15, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(16,OpenMP,#pragma omp parallel for)", "label": "(18, MPI, *ierr = MPI_Info_delete( (MPI_Info)(*v1), p2 );)"}
{"predict": "(3,OpenMP,#pragma omp parallel for collapse(2)),", "label": "(2, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (11, TBB, tbb::tick_count t1 = tbb::tick_count::now();)"}
{"predict": "(11,OpenMP,#pragma omp parallel for private(j))", "label": "(12, CUDA, A_[j] = static_cast<thrust::complex<T1>>(B_[j]);)"}
{"predict": "(6,OpenMP,#pragma omp parallel for), (7,OpenMP,#pragma omp parallel for), (8,OpenMP,#pragma omp parallel for), (9,OpenMP,#pragma omp parallel for), (10,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for reduction(+:v)), (28,OpenMP,#pragma omp parallel for reduction(+:v))", "label": "(7, OpenMP, #pragma omp for linear (); (24, OpenMP, #pragma omp for linear (argv[1])); (17, OpenMP, #pragma omp for linear (S1)); (12, OpenMP, #pragma omp for linear (argc > 0 ? argv[1] : argv[2])); (1, OpenMP, #pragma omp for linear); (22, OpenMP, #pragma omp for linear(e, g, k)); (27, OpenMP, #pragma omp parallel); (33, OpenMP, #pragma omp for linear(j)); (3, OpenMP, #pragma omp for linear (argc); (34, OpenMP, #pragma omp for linear(v:j)); (-1, OpenMP, #pragma omp for linear ()); (2, OpenMP, #pragma omp for linear (argc,); (11, OpenMP, #pragma omp for linear (a, b:B::ib)); (34, OpenMP, #pragma omp for linear(i) ordered(1)); (15, OpenMP, #pragma omp for linear(h)); (2, OpenMP, #pragma omp for linear (argc : 5) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (21, OpenMP, #pragma omp for linear(v:i)); (14, OpenMP, #pragma omp for linear(i))"}
{"predict": "(19,OpenMP,#pragma omp parallel sections), (19,OpenMP,#pragma omp section), (24,OpenMP,#pragma omp section)", "label": "(16, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for)", "label": "(8, CUDA, cudaStreamSynchronize(LU_v1->A_gpu.cuStreams[0]);)"}
{"predict": "(28,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &myrank);); (28,MPI,MPI_Comm_size(MPI_COMM_WORLD, &p););", "label": "(31, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(1,OpenMP,#pragma omp target teams distribute parallel for map(to: a[0:N], b[0:N]) map(tofrom: y[0:1]))", "label": "(5, CUDA, __shared__ float reduce_smem[NUM_WARPS];)"}
{"predict": "(24,OpenMP,#pragma omp parallel for reduction(+:d_dists_calced)); (51,OpenMP,#pragma omp parallel for reduction(+:d_dists_calced));", "label": "(16, CUDA, extern __shared__ float buffer[];)"}
{"predict": "}\n151: \n152:     /* get global min and max values across all procs */\n153:     MPI_Allreduce(&min_depth, &min_global, 1, MPI_INT, MPI_MIN, scr_comm_world);\n154:     MPI_Allreduce(&max_depth, &max_global, 1, MPI_INT, MPI_MAX, scr_comm_world);\n155: \n156:     /* identify leaders in each group and delete directories */\n157:     for (i = 0; i < num_dirs; i++) {\n158:       if (group_rank[i] == 0) {\n159:         /* get the directory */\n160:         char* dir = dirs[i];\n161: \n162:         /* delete the directory */\n163:         scr_dir_unlink(dir);\n164: \n165:         /* free the directory string */\n166:         scr_free(&dir);\n167:       }\n168:     }\n169: \n170:     /* free memory */\n171:     scr_free(&group_id);\n172:     scr_free(&group_ranks);\n173:     scr_free(&group_rank);\n174:     scr_free(&dirs);\n175:     scr_free(&depths);\n176:   }\n177: \n178:   /* free the file list */\n179:   kvtree_delete(&filelist);\n180: \n181:   /* free the files list */\n182:   kvtree_delete(&files);\n183: \n184:   return rc;\n185: }", "label": "(85, MPI, MPI_Bcast(&min_source, 1, MPI_INT, source, scr_comm_world);); (96, MPI, MPI_Allreduce(&min_depth, &min_global, 1, MPI_INT, MPI_MIN, scr_comm_world);); (76, MPI, MPI_Allreduce(&source_rank, &source, 1, MPI_INT, MPI_MIN, scr_comm_world);); (95, MPI, MPI_Allreduce(&max_depth, &max_global, 1, MPI_INT, MPI_MAX, scr_comm_world);); (168, MPI, MPI_Barrier(scr_comm_world);)"}
{"predict": "(29,OpenMP,#pragma omp parallel for), (52,OpenMP,#pragma omp parallel for)", "label": "(8, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc((void**)&d_out, size * sizeof(DataType)));); (32, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(h_out.data(), d_out, size * sizeof(DataType), cudaMemcpyDeviceToHost));); (28, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (35, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc((void**)&d_ref, size * sizeof(DataType)));); (48, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(d_out));); (16, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc((void**)&states, block_dim * grid_size * sizeof(RNG)));); (19, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (44, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(states));); (45, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(d_ref));); (39, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(h_ref.data(), d_ref, size * sizeof(DataType), cudaMemcpyDeviceToHost));)"}
{"predict": "(43,OpenMP,#pragma omp parallel for), (63,OpenMP,#pragma omp parallel for)", "label": "(36, OpenCL, err = clSetKernelArg(scan_kernel, 1, dyn_mem_size, NULL);); (31, OpenCL, err = clSetKernelArg(scan_kernel, 0, sizeof(buffer), &buffer);)"}
{"predict": "(15,OpenMP,#pragma omp parallel for)", "label": "(22, MPI, err = MPI_Type_create_resized(tmptype, 0, aext * sizeof(int), datatype);); (28, MPI, MPI_Type_free(&tmptype);); (12, MPI, err = MPI_Type_create_struct(1, &blen, &displ, &type, &tmptype);)"}
{"predict": "(9,OpenMP,#pragma omp atomic seq_cst)", "label": "(4, OpenMP, #pragma omp atomic seq_cst); (8, OpenMP, #pragma omp atomic seq_cst seq_cst); (10, OpenMP, #pragma omp atomic update seq_cst)"}
{"predict": "(2,OpenMP,#pragma omp parallel for collapse(2))", "label": "(12, CUDA, img[idx] = cudaAlphaBlend(img[idx], color);)"}
{"predict": "(10,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);), (41,OpenMP,#pragma omp parallel for), (67,OpenMP,#pragma omp parallel for), (97,OpenMP,#pragma omp parallel for), (110,OpenMP,#pragma omp parallel for)", "label": "(25, MPI, MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);); (40, MPI, err = MPI_File_close(&fh);); (4, MPI, MPI_Init(&argc, &argv);); (143, MPI, MPI_Finalize();); (22, MPI, MPI_Bcast(filename, len + 10, MPI_CHAR, 0, MPI_COMM_WORLD);); (2, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (39, MPI, MPI_Barrier(MPI_COMM_WORLD);); (12, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "MAX(>) test failed\\n\");\n145:                 }\n146:             }\n147:         }\n148:     }\n149: #endif /* HAVE_LONG_LONG */\n150: \n151:     MTest_Finalize(errs);\n152:     return MTestReturnValue(errs);\n153: }", "label": "(106, MPI, MPI_Reduce(ldinbuf, ldoutbuf, 3, MPI_LONG_DOUBLE, MPI_MAX, 0, comm);); (13, MPI, MPI_Comm_size(comm, &size);); (24, MPI, MPI_Reduce(cinbuf, coutbuf, 3, MPI_CHAR, MPI_MAX, 0, comm);); (49, MPI, MPI_Reduce(scinbuf, scoutbuf, 3, MPI_SIGNED_CHAR, MPI_MAX, 0, comm);); (73, MPI, MPI_Reduce(ucinbuf, ucoutbuf, 3, MPI_UNSIGNED_CHAR, MPI_MAX, 0, comm);); (8, MPI, MPI_Comm_rank(comm, &rank);); (132, MPI, MPI_Reduce(llinbuf, lloutbuf, 3, MPI_LONG_LONG, MPI_MAX, 0, comm);)"}
{"predict": "(3,OpenMP,#pragma omp parallel for), (7,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for)", "label": "(19, OpenMP, #pragma omp distribute parallel for proc_bind(master); (3, OpenMP, #pragma omp distribute parallel for proc_bind); (12, OpenMP, #pragma omp distribute parallel for proc_bind()); (6, OpenMP, #pragma omp distribute parallel for proc_bind(); (25, OpenMP, #pragma omp distribute parallel for proc_bind(x)); (37, OpenMP, #pragma omp distribute parallel for proc_bind(spread)); (18, OpenMP, #pragma omp distribute parallel for proc_bind(close), proc_bind(spread)); (-5, OpenMP, #pragma omp target); (31, OpenMP, #pragma omp parallel proc_bind(close)); (26, OpenMP, #pragma omp distribute parallel for proc_bind(master)); (-7, OpenMP, #pragma omp teams)"}
{"predict": "The provided code is a test function written in C++ using Google Test framework. It appears to be a series of checks and validations related to OpenCL command queue operations with OpenGL sharing. There are no loops or computationally intensive sections that would benefit from parallelization within this specific test function. However, if we assume that there might be loops or other sections in a larger context that could benefit from parallelization, I will provide a generic example based on a hypothetical loop.\n\nLet's assume there is a loop in the test function (or a similar function) that could be parallelized. Here's an example of how you might add OpenMP parallelization to a loop:\n\n### ** Hypothetical Code with a Loop **\n\n```cpp\n1: TEST_F(GlSharingTests, givenHwCommandQueueWhenAcquireAndReleaseCallsAreMadeWithEventsThenProperCmdTypeIsReturned) {\n2:     cl_event retEvent;\n3:     auto retVal = CL_SUCCESS;\n4: \n5:     EXPECT_EQ(CL_SUCCESS, retVal);\n6: \n7:     cl_command_type cmdType = 0;\n8:     size_t sizeReturned = 0;\n9:     retVal = clGetEventInfo(retEvent, CL_EVENT_COMMAND_TYPE, sizeof(cmdType), &cmdType, &sizeReturned);\n10:     ASSERT_EQ(CL_SUCCESS, retVal);\n11:     EXPECT_EQ(static_cast<cl_command_type>(CL_COMMAND_ACQUIRE_GL_OBJECTS), cmdType);\n12:     EXPECT_EQ(sizeof(cl_command_type), sizeReturned);\n13: \n14:     // Hypothetical loop that could be parallelized\n15:     int n = 1000;\n16:     for (int i = 0; i < n; i++) {\n17:         // Some computation or operation\n18:         retVal = clGetEventInfo(retEvent, CL_EVENT_COMMAND_TYPE, sizeof(cmdType), &cmdType, &sizeReturned);\n19:         ASSERT_EQ(CL_SUCCESS, retVal);\n20:     }\n21: \n22:     retVal = clReleaseEvent(retEvent);\n23:     EXPECT_EQ(CL_SUCCESS, retVal);\n24:     EXPECT_EQ(CL_SUCCESS, retVal);\n25:     EXPECT_EQ(CL_SUCCESS, retVal);\n26: }\n```\n\n### ** Parallelization Instructions **\n\n(16,OpenMP,#pragma omp parallel for)\n\nThis would parallelize the loop starting at line 16 using OpenMP.", "label": "(19, OpenCL, retVal = clEnqueueReleaseGLObjects(commandQueue, 1, &glBuffer, 0, nullptr, &retEvent);); (30, OpenCL, retVal = clReleaseMemObject(glBuffer);); (3, OpenCL, auto glBuffer = clCreateFromGLBuffer(&context, 0, bufferId, &retVal);); (1, OpenCL, auto commandQueue = clCreateCommandQueue(&context, context.getDevice(0), 0, &retVal);); (12, OpenCL, retVal = clReleaseEvent(retEvent);); (1, OpenCL, retVal = clEnqueueAcquireGLObjects(commandQueue, 1, &glBuffer, 0, nullptr, &retEvent);); (23, OpenCL, retVal = clReleaseCommandQueue(commandQueue);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for)", "label": "(21, MPI, MPI_Type_set_attr(type, key[2], &attrval[2]);); (22, MPI, MPI_Type_set_attr(type, key[0], &attrval[0]);); (56, MPI, MPI_Type_free_keyval(&key[i]);); (19, MPI, MPI_Type_set_attr(type, key[1], &attrval[1]);); (23, MPI, MPI_Type_delete_attr(type, key[0]);); (23, MPI, MPI_Type_delete_attr(type, key[1]);); (23, MPI, MPI_Type_delete_attr(type, key[2]);)"}
{"predict": "(2,OpenMP,#pragma omp target teams distribute parallel for collapse(2))", "label": "(5, CUDA, __shared__ float reduce_smem[NUM_WARPS];)"}
{"predict": "(37,OpenMP,#pragma omp parallel for reduction(+:delta))", "label": "(8, TBB, tbb::enumerable_thread_specific<HyperedgeWeight> deltas(0);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for)", "label": "(8, OpenCL, retVal = clEnqueueMemcpyINTEL(commandQueues[0][0].get(), true, deviceMemAlloc, buffer.data(), bufferSize, 0, nullptr, nullptr);); (16, OpenCL, retVal = clEnqueueMemcpyINTEL(commandQueues[0][0].get(), true, hostMemAlloc, deviceMemAlloc, bufferSize, 0, nullptr, nullptr);)"}
{"predict": "(16,OpenMP,#pragma omp parallel for), (33,MPI,MPI_Send(message.data(), msgsize, MPI_INT, 1, 0, MPI_COMM_WORLD);), (38,MPI,MPI_Recv(receiveBuffer.data(), msgsize, MPI_INT, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);), (45,MPI,MPI_Send(message.data(), msgsize, MPI_INT, 0, 0, MPI_COMM_WORLD);), (46,MPI,MPI_Recv(receiveBuffer.data(), msgsize, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)", "label": "(42, MPI, MPI_Recv(receiveBuffer.data(), msgsize, MPI_INT, 1, MPI_ANY_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (7, MPI, MPI_Init(&argc, &argv);); (48, MPI, MPI_Send(message.data(), msgsize, MPI_INT, 0, 1, MPI_COMM_WORLD);); (54, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (33, MPI, MPI_Send(message.data(), msgsize, MPI_INT, 1, tag, MPI_COMM_WORLD);); (8, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (41, MPI, MPI_Recv(receiveBuffer.data(), msgsize, MPI_INT, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status);)"}
{"predict": "(23,OpenMP,#pragma omp parallel for)", "label": "(9, CUDA, CUDA_CHECK(cudaGetDeviceCount(&ndevices));); (9, CUDA, CUDA_CHECK(cudaSetDevice(rank % ndevices));); (0, MPI, MPI_Init(&argc, &argv);); (37, MPI, MPI_Finalize();); (1, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for)", "label": "(2, CUDA, extern __shared__ char smem[];)"}
{"predict": "en) {\n120:       for (int i = 0; i < groupTx; i++) {\n121:         O[row * dim + i * Bc + tx] = sNewO[ty][i * Bc + tx];\n122:       }\n123:     }\n124: \n125:     // update global max and denom\n126:     if (row < seqlen) {\n127:       gMAX[row] = newMax;\n128:       gDenom[row] = newDenom;\n129:     }\n130: \n131:     // clean up shared memory\n132:     __syncthreads();\n133:   }\n134: }", "label": "(12, CUDA, __shared__ FP sV[Bc][dim];); (20, CUDA, __shared__ FP sMax[Br];); (9, CUDA, __shared__ FP sK[Bc][dim];); (11, CUDA, __shared__ FP sQK[Br][Bc];); (14, CUDA, __shared__ FP sSafeE[Br][Bc];); (8, CUDA, __shared__ FP sO[Br][dim];); (10, CUDA, __shared__ FP sNewO[Br][dim];); (3, CUDA, __shared__ FP sQ[Br][dim];); (12, CUDA, __shared__ FP sDenom[Br];)"}
{"predict": "(8,OpenMP,#pragma omp parallel for private(context, error_code) reduction(||:failed))", "label": "(7, OpenMP, #pragma omp parallel for)"}
{"predict": "(36,OpenMP,#pragma omp parallel for), (44,OpenMP,#pragma omp parallel for)", "label": "(12, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (13, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;)"}
{"predict": "(33,OpenMP,#pragma omp parallel for)", "label": "(16, OpenCL, clCreateBuffer_StubWithCallback(clCreateBuffer_testBufferConstructorContextIterator);); (35, OpenCL, clReleaseCommandQueue_ExpectAndReturn(make_command_queue(0), CL_SUCCESS);); (43, OpenCL, clReleaseMemObject_ExpectAndReturn(expected, CL_SUCCESS);); (24, OpenCL, clCreateCommandQueueWithProperties_StubWithCallback(clCreateCommandQueueWithProperties_testCommandQueueFromSpecifiedContext);); (31, OpenCL, clReleaseEvent_StubWithCallback(clReleaseEvent_testCopyHostToBuffer);); (41, OpenCL, clReleaseContext_ExpectAndReturn(make_context(0), CL_SUCCESS);); (26, OpenCL, clEnqueueMapBuffer_StubWithCallback(clEnqueueMapBuffer_testCopyHostToBuffer);); (26, OpenCL, clEnqueueUnmapMemObject_StubWithCallback(clEnqueueUnmapMemObject_testCopyHostToBuffer);)"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(33, MPI, MPI_Cart_shift(cart_comm, 0, 1, &source, &destination);); (52, MPI, t1 = MPI_Wtime();); (12, MPI, MPI_Init(&argc, &argv);); (57, MPI, MPI_Finalize();); (11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (27, MPI, MPI_Cart_create(MPI_COMM_WORLD, ndims, dims, periods, 1, &cart_comm);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (52, MPI, MPI_Comm_free(&cart_comm);); (27, MPI, MPI_Comm_rank(cart_comm, &cart_id);); (29, MPI, MPI_Barrier(MPI_COMM_WORLD);); (29, MPI, t0 = MPI_Wtime();)"}
{"predict": "(13,OpenMP,#pragma omp parallel sections), (15,OpenMP,#pragma omp section), (38,OpenMP,#pragma omp section)", "label": "(8, MPI, MPI_Comm_rank(mpi_comm, &rank);); (6, MPI, MPI_Comm mpi_comm = MPI_Comm_f2c(vtkh::GetMPICommHandle());); (56, MPI, MPI_Bcast(&entropy, 1, MPI_DOUBLE, 0, mpi_comm);)"}
{"predict": "reatt]\\n\"); validate_result(d_dpreatt, dpreatt, \"dpreatt\", B * NH * T * T, 1e-3f);\n68:     printf(\"[dinp]\\n\");    validate_result(d_dinp, dinp, \"dinp\", B * T * 3 * C, 1e-3f);\n69: \n70:     // clean up memory\n71:     free(inp); free(qkvr); free(preatt); free(att); free(vaccum); free(out);\n72:     free(dout); free(dinp); free(dpreatt); free(datt);\n73: \n74:     return 0;\n75: }", "label": "(68, CUDA, cudaCheck(cudaMemset(d_dinp, 0, B * T * 3 * C * sizeof(float)));); (60, CUDA, cudaCheck(cudaMalloc(&d_dpreatt, B * NH * T * T * sizeof(float)));); (157, CUDA, cudaCheck(cudaFree(d_inp));); (69, CUDA, cudaCheck(cudaMemset(d_dvaccum, 0, B * T * C * sizeof(float)));); (86, CUDA, cudaCheck(cudaMemcpy(h_dinp, d_dinp, B * T * 3 * C * sizeof(float), cudaMemcpyDeviceToHost));); (162, CUDA, cudaCheck(cudaFree(d_dpreatt));); (159, CUDA, cudaCheck(cudaFree(d_dinp));); (159, CUDA, cudaCheck(cudaFree(d_dqkvr));); (152, CUDA, cudaCheck(cudaFree(d_qkvr));); (23, CUDA, cudaCheck(cudaMalloc(&d_att, B * NH * T * T * sizeof(float)));); (158, CUDA, cudaCheck(cudaFree(d_datt));); (58, CUDA, cudaCheck(cudaMemset(d_dqkvr, 0, B * T * 3 * C * sizeof(float)));); (149, CUDA, cudaCheck(cudaFree(d_preatt));); (50, CUDA, cudaCheck(cudaMalloc(&d_dvaccum, B * T * C * sizeof(float)));); (50, CUDA, cudaCheck(cudaMalloc(&d_dout, B * T * C * sizeof(float)));); (15, CUDA, cudaCheck(cudaMalloc(&d_qkvr, B * T * 3 * C * sizeof(float)));); (17, CUDA, cudaCheck(cudaMalloc(&d_vaccum, B * T * C * sizeof(float)));); (14, CUDA, cudaCheck(cudaMalloc(&d_preatt, B * NH * T * T * sizeof(float)));); (48, CUDA, cudaCheck(cudaMemcpy(d_dout, dout, B * T * C * sizeof(float), cudaMemcpyHostToDevice));); (144, CUDA, cudaCheck(cudaFree(d_vaccum));); (40, CUDA, cudaCheck(cudaMalloc(&d_dqkvr, B * T * 3 * C * sizeof(float)));); (38, CUDA, cudaCheck(cudaMalloc(&d_dinp, B * T * 3 * C * sizeof(float)));); (49, CUDA, cudaCheck(cudaMemset(d_datt, 0, B * NH * T * T * sizeof(float)));); (141, CUDA, cudaCheck(cudaFree(d_out));); (145, CUDA, cudaCheck(cudaFree(d_dvaccum));); (9, CUDA, cudaCheck(cudaMalloc(&d_out, B * T * C * sizeof(float)));); (36, CUDA, cudaCheck(cudaMalloc(&d_datt, B * NH * T * T * sizeof(float)));); (2, CUDA, cudaCheck(cudaMalloc(&d_inp, B * T * 3 * C * sizeof(float)));); (134, CUDA, cudaCheck(cudaFree(d_att));); (41, CUDA, cudaCheck(cudaMemset(d_dpreatt, 0, B * NH * T * T * sizeof(float)));); (6, CUDA, cudaCheck(cudaMemcpy(d_inp, inp, B * T * 3 * C * sizeof(float), cudaMemcpyHostToDevice));); (139, CUDA, cudaCheck(cudaFree(d_dout));)"}
{"predict": "(9,OpenMP,#pragma omp parallel for), (17,CUDA,add<<<grid_size, block_size>>>(x, y, z);)", "label": "(9, CUDA, CHECK(cudaMallocManaged((void **)&z, M));); (23, CUDA, CHECK(cudaFree(x));); (5, CUDA, CHECK(cudaMallocManaged((void **)&x, M));); (22, CUDA, CHECK(cudaFree(y));); (22, CUDA, CHECK(cudaFree(z));); (16, CUDA, CHECK(cudaDeviceSynchronize());); (2, CUDA, CHECK(cudaMallocManaged((void **)&y, M));)"}
{"predict": "(23,OpenMP,#pragma omp parallel for reduction(max:value)) (42,OpenMP,#pragma omp parallel for) (52,OpenMP,#pragma omp parallel for reduction(max:value)) (62,OpenMP,#pragma omp parallel for reduction(max:value))", "label": "(26, MPI, MPI_Allreduce(&value, &tempvalue, 1, MPI_DOUBLE, MPI_MAX, grid->comm);); (50, MPI, MPI_Allreduce(rwork, temprwork, A->ncol, MPI_DOUBLE, MPI_SUM, grid->comm);)"}
{"predict": "(37,OpenMP,#pragma omp parallel for private(j))", "label": "(17, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (26, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(12,OpenMP,#pragma omp parallel sections), (14,OpenMP,#pragma omp section), (16,OpenMP,#pragma omp section), (18,OpenMP,#pragma omp section), (21,OpenMP,#pragma omp section)", "label": "(25, OpenCL, clReleaseMemObject(scratchBuff);); (20, OpenCL, clReleaseMemObject(mobjDP_);); (15, OpenCL, clReleaseMemObject(mobjY_);); (10, OpenCL, clReleaseMemObject(mobjX_);)"}
{"predict": "(25,OpenMP,#pragma omp parallel for schedule(static)), (26,OpenMP,#pragma omp parallel for schedule(static)), (30,OpenMP,#pragma omp parallel for schedule(static))", "label": "(5, TBB, oneapi::tbb::global_control c(oneapi::tbb::global_control::max_allowed_parallelism, nthreads);); (21, TBB, static oneapi::tbb::affinity_partitioner g_ap;)"}
{"predict": "(7,OpenMP,#pragma omp parallel for),", "label": "(16, SYCL, using namespace cl::sycl::access;); (4, SYCL, cl::sycl::queue queue;)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(33, OpenMP, omp_target_free(d_a, DefaultDeviceNum);); (15, OpenMP, d_a = (int *)omp_target_alloc(DataSize, DefaultDeviceNum);); (23, OpenMP, #pragma omp target update to(d_a)); (5, OpenMP, int DefaultDeviceNum = omp_get_default_device();); (16, OpenMP, omp_target_associate_ptr(p_a, d_a, DataSize, 0, DefaultDeviceNum);); (24, OpenMP, omp_target_disassociate_ptr(p_a, DefaultDeviceNum);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for), (44,OpenMP,#pragma omp parallel for)", "label": "(51, OpenCL, clReleaseEvent(ev[i]);); (24, OpenCL, clEnqueueNDRangeKernel(queue, kernel, 1, NULL, globals, locals, 2, &ev[0], &ev[2]);); (30, OpenCL, buf_data[0] = clEnqueueMapBuffer(queue, buf[0], CL_FALSE, 0, 0, BUFFERSIZE*sizeof(int), 1, &ev[2], NULL, NULL);); (14, OpenCL, clEnqueueWriteBuffer(queue, buf[0], CL_FALSE, 0, BUFFERSIZE*sizeof(int), (void *)cpu_src, 1, &ev[0], &ev[1]);)"}
{"predict": "(20,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for private(lk, index, flag, gridid, size)), (63,OpenMP,#pragma omp parallel for private(lk, index, flag, gridid, size, nrbl, len)), (104,OpenMP,#pragma omp parallel for private(ksupc))", "label": "(50, MPI, MPI_Allreduce(MPI_IN_PLACE, &(flag), 1, mpi_int_t, MPI_MAX, grid3d->zscp.comm);); (105, MPI, MPI_Allreduce(  MPI_IN_PLACE, myusize, nsupers, mpi_int_t, MPI_MAX, grid->comm );); (102, MPI, MPI_Allreduce( MPI_IN_PLACE, mylsize, nsupers, mpi_int_t, MPI_MAX, grid->comm );); (50, MPI, MPI_Bcast( &(size), 1, mpi_int_t, gridid,  grid3d->zscp.comm);); (48, MPI, MPI_Allreduce(MPI_IN_PLACE, &(gridid), 1, mpi_int_t, MPI_MAX, grid3d->zscp.comm);)"}
{"predict": "(29,OpenMP,#pragma omp parallel sections)", "label": "(15, CUDA, checkCudaErrors(cudaMalloc((void **)&ringbuf, sizeof(qsortRingbuf)));); (28, CUDA, checkCudaErrors(cudaEventRecord(ev1));); (40, CUDA, checkCudaErrors(cudaEventRecord(ev2));); (38, CUDA, checkCudaErrors(cudaGetLastError());); (61, CUDA, checkCudaErrors(cudaFree(ringbuf));); (38, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (21, CUDA, checkCudaErrors(cudaEventCreate(&ev1));); (21, CUDA, checkCudaErrors(cudaEventCreate(&ev2));); (58, CUDA, checkCudaErrors(cudaFree(gpustack));)"}
{"predict": "(24,MPI,#pragma omp parallel for), (44,MPI,#pragma omp parallel for)", "label": "(21, MPI, MPI_Comm_create_group(MPI_COMM_WORLD, newgroup, 0, &newcomm);); (19, MPI, MPI_Group_excl(world, 1, deadprocs, &newgroup);); (7, MPI, MPI_Init(&argc, &argv);); (82, MPI, MPI_Group_free(&newgroup);); (84, MPI, MPI_Finalize();); (53, MPI, rc = MPI_Scatter(sendbuf, 100000, MPI_CHAR, rbuf, 100000, MPI_CHAR, 0, MPI_COMM_WORLD);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (76, MPI, MPI_Comm_free(&newcomm);); (3, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (76, MPI, MPI_Group_free(&world);); (25, MPI, rc = MPI_Scatter(sendbuf, 10, MPI_CHAR, rbuf, 10, MPI_CHAR, 0, MPI_COMM_WORLD);); (4, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (26, MPI, MPI_Error_class(rc, &errclass);); (5, MPI, MPI_Comm_group(MPI_COMM_WORLD, &world);)"}
{"predict": "(11,OpenMP,#pragma omp atomic)", "label": "(6, OpenMP, #pragma omp parallel num_threads(8) shared(var)); (7, OpenMP, omp_set_nest_lock(&lock);); (9, OpenMP, omp_unset_nest_lock(&lock);); (1, OpenMP, omp_init_nest_lock(&lock);); (13, OpenMP, omp_destroy_nest_lock(&lock);)"}
{"predict": "(5,OpenMP,#pragma omp parallel for)", "label": "(8, CUDA, sim->CUDAConfig().device_id = device;)"}
{"predict": "0)\n128:             {\n129: \t\tsUPanelUpdate(k, factStat->IrecvPlcd_D, factStat->factored_U,\n130: \t\t\t      comReqss[offset]->L_diag_blk_recv_req,\n131: \t\t\t      dFBufs[offset]->BlockLFactor, grid, LUstruct, SCT);\n132:                 factored_U[k] = 1;\n133:             }\n134:         }\n135:         double t_apt2 = SuperLU_timer_();\n136:         double apt_time = t_apt2 - t_apt;\n137:         printf(\"Level %d: APT time = %f\\n\", topoLvl, apt_time);\n138:     }\n139: \n140:     return 0;\n141: }", "label": "(203, OpenMP, #pragma omp parallel); (206, OpenMP, #pragma omp for schedule(dynamic,2) nowait)"}
{"predict": "(20,OpenMP,#pragma omp parallel for private(i))", "label": "(19, OpenMP, #pragma omp parallel for)"}
{"predict": "(4,OpenMP,#pragma omp parallel for), (7,OpenMP,#pragma omp parallel for), (10,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for), (36,OpenMP,#pragma omp parallel for), (39,OpenMP,#pragma omp parallel for)", "label": "(24, OpenCL, test_clEnqueueCopyBuffer(&clEnqueueCopyBufferData[i]);); (27, OpenCL, test_clEnqueueCopyBufferRect(&clEnqueueCopyBufferRectData[i]);); (6, OpenCL, test_clEnqueueWriteBuffer(&clEnqueueWriteBufferData[i]);); (41, OpenCL, test_clEnqueueUnmapMemObject(&clEnqueueUnmapMemObjectData[i]);); (8, OpenCL, test_clEnqueueReadBufferRect(&clEnqueueReadBufferRectData[i]);); (-1, OpenCL, test_clEnqueueReadBuffer(&clEnqueueReadBufferData[i]);); (14, OpenCL, test_clEnqueueFillBuffer(&clEnqueueFillBufferData[i]);); (25, OpenCL, test_clEnqueueMapBuffer(&clEnqueueMapBufferData[i]);); (8, OpenCL, test_clEnqueueWriteBufferRect(&clEnqueueWriteBufferRectData[i]);)"}
{"predict": "(16,OpenMP,#pragma omp section), (17,OpenMP,#pragma omp section)", "label": "(7, OpenMP, #pragma omp parallel sections reduction(task,*:f)); (14, OpenMP, #pragma omp parallel sections default(none), private(argc, b) firstprivate(argv) if (argc > 0) num_threads(ee) copyin(a) proc_bind(spread) reduction(| : c, d) reduction(* : e) lastprivate(argv)); (2, OpenMP, #pragma omp threadprivate(a)); (16, OpenMP, #pragma omp section)"}
{"predict": "(9,OpenMP,#pragma omp parallel for)", "label": "(15, OpenMP, #pragma omp target teams distribute parallel for collapse(2) depend(out:image[y_start]) nowait); (6, OpenMP, double st = omp_get_wtime();); (28, OpenMP, double et = omp_get_wtime();); (25, OpenMP, #pragma omp taskwait); (5, OpenMP, #pragma omp target data map(alloc:image[0:width*height])); (18, OpenMP, #pragma omp target update from(image[block*block_size:block_size]))"}
{"predict": "(51,OpenMP,#pragma omp parallel sections), (52,OpenMP,#pragma omp section), (57,OpenMP,#pragma omp section)", "label": "(36, MPI, MPI_Type_size(c_info->s_data_type, &s_size);); (36, MPI, MPI_Type_size(c_info->r_data_type, &r_size);)"}
{"predict": "The provided code is primarily focused on creating and setting up a Python dictionary to represent the CUDA array interface. This task is inherently sequential and does not contain any loops or sections that can be parallelized in a straightforward manner. However, if there were loops or other parallelizable sections in the code, I would suggest where to insert parallelization instructions.\n\nSince there are no loops or parallelizable sections in the given code, no parallelization instructions are necessary. Therefore, the output will be empty:\n\n()", "label": "(13, CUDA, PyObject* shape = PyCudaImage_GetShape(self, closure);)"}
{"predict": "(33,OpenMP,#pragma omp parallel for private(k)) (45,OpenMP,#pragma omp parallel for private(k))", "label": "(31, MPI, MPI_Type_commit(&newtype);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (42, MPI, MPI_Send(buf, 1, newtype, 1, i, MPI_COMM_WORLD);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (66, MPI, MPI_Type_free(&newtype);); (24, MPI, MPI_Type_vector(i, 128, 129, MPI_CHAR, &newtype);); (70, MPI, MPI_Recv(&recv_errs, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (76, MPI, MPI_Send(&errs, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);); (39, MPI, MPI_Recv(buf, 1, newtype, 0, i, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)"}
{"predict": ":             MPI_Bcast(&interleave_count, 1, MPI_INT, 0, fd->comm);\n129:         }\n130:         /* collective I/O not needed */\n131:         ADIOI_Noncontig_coll_immediate(fd, buf, count, datatype, file_ptr_type,\n132:                                        offset, status, error_code);\n133:         return;\n134:     }\n135: \n136:     /* Calculate size of buffer space needed for buffered I/O */\n137:     bufsize = fd->hints->cb_buffer_size;\n138:     MPI_Type_get_extent(datatype, &lb, &extent);\n139:     bufsize = MPL_MIN(bufsize, (ADIO_Offset) count * extent);\n140:     cb_buf = (char *) ADIOI_Malloc(bufsize);\n141: \n142:     /* get file views for all processes */\n143:     client_file_view_state_arr = (view_state *)\n144:         ADIOI_Malloc(nprocs * sizeof(view_state));\n145:     agg_file_view_state_arr = (view_state *)\n146:         ADIOI_Malloc(nprocs * sizeof(view_state));\n147:     my_mem_view_state_arr = (view_state *)\n148:         ADIOI_Malloc(nprocs * sizeof(view_state));\n149: \n150:     for (i = 0; i < nprocs; i++) {\n151:         client_file_view_state_arr[i].offset = 0;\n152:         client_file_view_state_arr[i].type = MPI_DATATYPE_NULL;\n153:         agg_file_view_state_arr[i].offset = 0;\n154:         agg_file_view_state_arr[i].type = MPI_DATATYPE_NULL;\n155:         my_mem_view_state_arr[i].offset = 0;\n156:         my_mem_view_state_arr[i].type = MPI_DATATYPE_NULL;\n157:     }\n158: \n159:     /* allocate and initialize alltoallw counts/disps arrays */\n160:     alltoallw_counts = (int *) ADIOI_Malloc(nprocs * sizeof(int));\n161:     alltoallw_disps = (int *) ADIOI_Malloc(nprocs * sizeof", "label": "(106, MPI, MPI_Allgather(st_end_offset, 2, ADIO_OFFSET, all_st_end_offsets, 2, ADIO_OFFSET, fd->comm);); (71, MPI, MPI_Comm_rank(fd->comm, &myrank);); (527, MPI, MPI_Type_free(&client_comm_dtype_arr[i]);); (587, MPI, MPI_Type_free(&fd->file_realm_types[0]);); (67, MPI, MPI_Comm_size(fd->comm, &nprocs);); (589, MPI, MPI_Type_free(&fd->file_realm_types[i]);); (155, MPI, MPI_Type_get_extent(datatype, &lb, &extent);); (307, MPI, MPI_Type_size_x(agg_comm_dtype_arr[i], &size);); (301, MPI, MPI_Type_free(&agg_dtype);)"}
{"predict": "6:     mfu_free(&senddisps);\n137: \n138:     mfu_free(&keys);\n139:     mfu_free(&ltr);\n140: \n141:     return;\n142: }", "label": "(99, MPI, MPI_Alltoall(sendcounts, 1, MPI_INT, recvcounts, 1, MPI_INT, MPI_COMM_WORLD);); (46, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (40, MPI, MPI_Type_free(&keytype);)"}
{"predict": "(15,OpenMP,#pragma omp parallel for reduction(+:sum))", "label": "(13, OpenMP, #pragma omp parallel default(none) shared(data,x,y,z,repeats))"}
{"predict": "(12,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(aux>0) vector[i] -= 2*vector[index[i]];\n140:           else       vector[i] -= 2*aux;\n141:         }\n142:       }\n143:       break;\n144: \n145:     case INS_HEAVY:\n146:       for (iter=0; iter<iterations; iter+=2) {\n147:         for (i=0; i<vector_length; i++) {\n148:           aux = -(3 - (i&7));\n149:           if (aux>0) {\n150:             vector[i] -= 2*vector[i];\n151:             nfunc = 1;\n152:             for (rank=0; rank<10; rank++) {\n153:               if (rank<5) {\n154:                 vector[i] += 2*vector[i];\n155:               } else {\n156:                 vector[i] -= 2*vector[i];\n157:               }\n158:             }\n159:           } else {\n160:             vector[i] -= 2*aux;\n161:             nfunc = 1;\n162:             for (rank=0; rank<10; rank++) {\n163:               if (rank<5) {\n164:                 vector[i] += 2*vector[i];\n165:               } else {\n166:                 vector[i] -= 2*vector[i];\n167:               }\n168:             }\n169:           }\n170:         }\n171:         for (i=0; i<vector_length; i++) {\n172:           aux = (3 - (i&7));\n173:           if (aux>0) {\n174:             vector[i] -= 2*vector[i];\n175:             nfunc = 1;\n176:             for (rank=0; rank<10; rank++) {\n177:               if (rank<5) {\n178:                 vector[i] += 2*vector[i];\n179:               } else {\n180:                 vector[i] -= 2*vector[i];\n181:             }\n1", "label": "(207, MPI, MPI_Reduce(&total, &total_sum, 1, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD);); (230, MPI, MPI_Finalize();); (79, MPI, MPI_Bcast(&iterations,    1, MPI_INT, root, MPI_COMM_WORLD);); (21, MPI, MPI_Init(&argc,&argv);); (76, MPI, MPI_Bcast(&vector_length, 1, MPI_INT, root, MPI_COMM_WORLD);); (77, MPI, MPI_Bcast(&btype,         1, MPI_INT, root, MPI_COMM_WORLD);); (93, MPI, MPI_Barrier(MPI_COMM_WORLD);); (19, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (17, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(37,OpenMP,#pragma omp taskwait); (37,MPI,MPI_Barrier(MPI_COMM_WORLD);)", "label": "(36, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (38, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(10,OpenMP,#pragma omp parallel for collapse(2)), (17,OpenMP,#pragma omp parallel for collapse(2))", "label": "(44, MPI, MPI_Buffer_detach(&bptr, &bl);); (24, MPI, MPI_Recv(b, 10, MPI_INT, i, 27 + j, MPI_COMM_WORLD, &status);); (14, MPI, MPI_Bsend(a, 10, MPI_INT, 0, 27 + j, MPI_COMM_WORLD);); (7, MPI, MPI_Buffer_attach(buf, BUFSIZE);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)"}
{"predict": "(1,OpenMP,#pragma omp target teams distribute parallel for collapse(2))", "label": "(2, CUDA, __shared__ int sharedMemory [BLOCK_SIZE] [BLOCK_SIZE + 1];)"}
{"predict": "(35,OpenMP,#pragma omp parallel for), (46,OpenMP,#pragma omp parallel for), (53,OpenMP,#pragma omp parallel for), (64,OpenMP,#pragma omp parallel for)", "label": "(57, MPI, MPI_Send(sendBuffer, n_send, MPI_DOUBLE, neighbors[i], MPI_MY_TAG, MPI_COMM_WORLD);); (15, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (36, MPI, MPI_Irecv(x_external, n_recv, MPI_DOUBLE, neighbors[i], MPI_MY_TAG, MPI_COMM_WORLD, request+i);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for private(i) nowait), (21,OpenMP,#pragma omp parallel for private(i) nowait)", "label": "(5, OpenMP, #pragma omp parallel num_threads(4)); (11, OpenMP, #pragma omp for schedule(static) nowait); (8, OpenMP, rank = omp_get_thread_num();); (16, OpenMP, #pragma omp atomic); (20, OpenMP, #pragma omp for schedule(static))"}
{"predict": "(8,OpenMP,#pragma omp parallel for reduction(+:ai) reduction(+:ri))", "label": "(4, OpenMP, #pragma omp parallel for)"}
{"predict": "(2,OpenMP,#pragma omp parallel for), (4,OpenMP,#pragma omp parallel for), (6,OpenMP,#pragma omp parallel for), (8,OpenMP,#pragma omp parallel for), (10,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for)", "label": "(1, OpenMP, #pragma omp parallel for ordered ordered); (15, OpenMP, #pragma omp parallel for order(concurrent)); (8, OpenMP, #pragma omp parallel for order(none); (4, OpenMP, #pragma omp parallel for order(); (0, OpenMP, #pragma omp parallel for order); (8, OpenMP, #pragma omp parallel for order(concurrent)"}
{"predict": "(23,OpenMP,#pragma omp parallel for private(i))", "label": "(14, OpenMP, printf(\"Thread %d of %d\\n\", omp_get_thread_num(), omp_get_num_threads());)"}
{"predict": "(33,OpenMP,#pragma omp parallel for), (39,OpenMP,#pragma omp parallel for), (45,OpenMP,#pragma omp parallel for), (50,OpenMP,#pragma omp parallel for)", "label": "(43, CUDA, CUDA_CHECK_GOTO(cudaGetDevice(&self->device), free_mpool, status);); (15, CUDA, cuda_st = cudaGetDeviceCount(&num_devices);)"}
{"predict": "(26,OpenMP,#pragma omp parallel for reduction(+:sum))", "label": "(25, CUDA, assert(cudaMemcpyAsync(out.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);); (11, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (9, CUDA, cudaMalloc((void**)(&d_in1), in1_bytes);); (11, CUDA, cudaMemcpy(d_in1, in1.data(), in1_bytes, cudaMemcpyHostToDevice);); (39, CUDA, cudaFree(d_out);); (21, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (36, CUDA, cudaFree(d_in1);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(14, OpenMP, #pragma omp teams default(none), private(argc,b) num_teams(f) firstprivate(argv) reduction(| : c, d) reduction(* : e) thread_limit(f+g)); (6, OpenMP, #pragma omp target); (2, OpenMP, #pragma omp threadprivate(a)); (5, OpenMP, #pragma omp teams)"}
{"predict": "=%d,k=%d,knsupc=%d,il=%d,cnt=%d\\n\",mype,bid,tid,d_colnummod[d_mymaskstartmod[tid]]*2,wm_val,lib,k,knsupc,il,cnt);\n111:                     bmod_tmp = 0;\n112:                     for (int j = 0; j < cnt; j++) {\n113:                         int_t c = URtree_ptr[lib].destPtr_[j];\n114:                         int_t ccol = MYCOL(c, grid);\n115:                         int_t ccol_supno = Supno(ccol);\n116:                         int_t colsup = Supno(d_colnummod[d_mymaskstartmod[tid]]);\n117:                         int_t rowblk = d_colnummod[d_mymaskstartmod[tid]] / grid->npcol;\n118:                         int_t colblk = d_colnummod[d_mymaskstartmod[tid]] % grid->npcol;\n119:                         int_t rowblk_supno = Supno(rowblk);\n120:                         int_t colblk_supno = Supno(colblk);\n121:                         int_t dest = ccol_supno * grid->nprow + rowblk_supno;\n122:                         int_t dest2 = colblk_supno * grid->nprow + rowblk_supno;\n123:                         int_t row_supno = Supno(k);\n124:                         int_t row_supno2 = Supno(rowblk);\n125:                         int_t col_supno = Supno(k);\n126:                         int_t col_supno2 = Supno(colblk);\n127:                         int_t row_supno3 = Supno(rowblk_supno);\n128:                         int_t col_supno3 = Supno(colblk_supno);\n129:                         int_t row_supno4 = Supno(knsupc);\n130:                         int_t col_supno4 = Supno(il);\n131:                         int_t row_supno5 = Supno(cnt);\n132:                         int_t col_supno5 = Supno(c);\n133:                         int_t row_supno6 = Supno(j);\n134:                         int_t col_supno6 = Supno(c);\n135:                         int_t", "label": "(341, CUDA, __shared__ int cur_send_num;); (339, CUDA, __shared__ int recv_num, finish_num;)"}
{"predict": "(22,OpenMP,#pragma omp parallel for) (45,OpenMP,#pragma omp parallel for) (67,OpenMP,#pragma omp parallel for) (72,OpenMP,#pragma omp parallel for)", "label": "(4, TBB, tbb::flow::queue_node<T> q2(g);); (70, TBB, tbb::flow::make_edge( q2, q3 );); (67, TBB, tbb::flow::queue_node<T> q3(g);); (37, TBB, tbb::flow::make_edge( q, q2 );); (56, TBB, tbb::flow::remove_edge( q, q2 );); (87, TBB, tbb::flow::remove_edge( q,  q2 );); (-3, TBB, tbb::flow::queue_node<T> q(g);); (-5, TBB, tbb::flow::graph g;)"}
{"predict": "(7,OpenMP,#pragma omp parallel for)", "label": "(23, OpenCL, clReleaseEvent(event);)"}
{"predict": "(55,OpenMP,#pragma omp parallel for)", "label": "(25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (25, MPI, MPI_Get_processor_name(mpi_name, &mpi_namelen);); (22, MPI, MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);)"}
{"predict": "(31,OpenMP,#pragma omp parallel for)", "label": "(43, OpenCL, clReleaseMemObject(imageDesc.mem_object);); (14, OpenCL, imageDesc.mem_object = clCreateBuffer(&context, CL_MEM_READ_WRITE, imageDesc.image_height * imageDesc.image_width, nullptr, &retVal);)"}
{"predict": "3, i2, i1) + I3D(y1, m1k, m2k, i3, i2, i1+2));\n78:       }\n79:     }\n80:   }\n81:   if (timeron) timer_stop(T_rprj3);\n82: #ifdef _OPENACC\n83:   acc_free(x1);\n84:   acc_free(y1);\n85: #elif OPENMP_ALLOC\n86:   omp_target_free(x1, omp_get_default_device());\n87:   omp_target_free(y1, omp_get_default_device());\n88: #else\n89:   free(x1);\n90:   free(y1);\n91: #endif\n92: }\n\n(41,OpenMP,#pragma omp parallel for default(shared) collapse(3))\n(62,OpenMP,#pragma omp parallel for default(shared) collapse(3))", "label": "(114, OpenMP, omp_target_free(y1, omp_get_default_device());); (41, OpenMP, #pragma omp teams distribute parallel for default(shared) collapse(3)); (39, OpenMP, #pragma omp target is_device_ptr(x1, y1) map(tofrom: or[0:m3k*m2k*m1k]) map(tofrom: os[0:m3j*m2j*m1j])); (13, OpenMP, y1 = (double*)omp_target_alloc(m3k*m2k*m1k*sizeof(double), omp_get_default_device());)"}
{"predict": "(25,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for)", "label": "(9, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "gValue[writeAddress] = finalValue;\n155:     gIndex[writeAddress] = finalIndex;\n156:   }\n157: }", "label": "(17, CUDA, extern __shared__ _VOLATILE_ float sMem[];)"}
{"predict": "(6,OpenMP,#pragma omp parallel for reduction(+:sumViaCpu)), (8,OpenMP,#pragma omp parallel for)", "label": "(1, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)"}
