{"predict": "(20, CUDA, CUDA_CHECK(cudaMalloc((void **)&dev_max, sizeof(FP) * SEQLEN * DIM));); (40, CUDA, cudaFree(dev_max);); (23, CUDA, CUDA_CHECK(cudaMemcpy(dev_denom, host_denom, sizeof(FP) * SEQLEN * DIM, cudaMemcpyHostToDevice));); (19, CUDA, CUDA_CHECK(cudaMalloc((void **)&dev_denom, sizeof(FP) * SEQLEN * DIM));); (19, CUDA, CUDA_CHECK(cudaMalloc((void **)&QK, sizeof(FP) * SEQLEN * SEQLEN));); (19, CUDA, CUDA_CHECK(cudaMemcpy(dev_max, host_max, sizeof(FP) * SEQLEN * DIM, cudaMemcpyHostToDevice));); (33, CUDA, cudaFree(QK);); (34, CUDA, cudaFree(dev_denom);)", "label": "(20, CUDA, CUDA_CHECK(cudaMalloc((void **)&dev_max, sizeof(FP) * SEQLEN * DIM));); (39, CUDA, cudaFree(dev_max);); (22, CUDA, CUDA_CHECK(cudaMemcpy(dev_denom, host_denom, sizeof(FP) * SEQLEN * DIM, cudaMemcpyHostToDevice));); (18, CUDA, CUDA_CHECK(cudaMalloc((void **)&dev_denom, sizeof(FP) * SEQLEN * DIM));); (18, CUDA, CUDA_CHECK(cudaMalloc((void **)&QK, sizeof(FP) * SEQLEN * SEQLEN));); (18, CUDA, CUDA_CHECK(cudaMemcpy(dev_max, host_max, sizeof(FP) * SEQLEN * DIM, cudaMemcpyHostToDevice));); (33, CUDA, cudaFree(QK);); (34, CUDA, cudaFree(dev_denom);)"}
{"predict": "(15, OpenMP, #pragma omp critical(out))", "label": "(32, OpenMP, #pragma omp critical(cerr)); (15, OpenMP, #pragma omp critical(out))"}
{"predict": "(5, MPI, MPI_T_init_thread(required, &provided);); (7, MPI, MPI_Abort(MPI_COMM_WORLD, -1);); (30, MPI, MPI_T_finalize();); (7, MPI, MPI_Finalize();)", "label": "(31, MPI, MPI_T_finalize();); (4, MPI, MPI_T_init_thread(required, &provided);); (6, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(23, SYCL, sycl::free(data, sycl::default_selector_v);); (16, SYCL, sycl::memory_bench(data, N, MB_PER_THREAD, ELS_PER_THREAD, sycl::default_selector_v);)", "label": "(19, OpenMP, #pragma omp parallel for simd reduction(+:totalSum) schedule(guided) aligned(data: 64) num_threads(N_THREADS)); (10, OpenMP, #pragma omp parallel for num_threads(N_THREADS))"}
{"predict": "(15, CUDA, cudaMemcpy(printfbuf_local+hdr->fmtoffset, bufptr+hdr->fmtoffset, CUPRINTF_MAX_LEN,cudaMemcpyDeviceToHost);)", "label": "(15, CUDA, cudaMemcpy(printfbuf_local, bufptr, CUPRINTF_MAX_LEN, cudaMemcpyDeviceToHost);)"}
{"predict": "(39, OpenMP, #pragma omp parallel for firstprivate(x, y)); (46, OpenMP, #pragma omp parallel for lastprivate(x, y, z) firstprivate(x, y, z)); (20, OpenMP, #pragma omp parallel for firstprivate()); (10, OpenMP, #pragma omp parallel for firstprivate(,); (39, OpenMP, #pragma omp parallel for lastprivate(x, y) firstprivate(x, y)); (22, OpenMP, #pragma omp parallel for firstprivate(int)); (-2, OpenMP, #pragma omp parallel for firstprivate()", "label": "(26, OpenMP, #pragma omp parallel sections firstprivate(int)); (10, OpenMP, #pragma omp parallel sections firstprivate(,); (14, OpenMP, #pragma omp parallel sections firstprivate(, )); (34, OpenMP, #pragma omp parallel sections lastprivate(x) firstprivate(x)); (17, OpenMP, #pragma omp parallel sections firstprivate()); (36, OpenMP, #pragma omp parallel sections lastprivate(x, y) firstprivate(x, y)); (25, OpenMP, #pragma omp parallel sections firstprivate(0)); (-3, OpenMP, #pragma omp parallel sections firstprivate(); (37, OpenMP, #pragma omp parallel sections lastprivate(x, y, z) firstprivate(x, y, z))"}
{"predict": "(17, MPI, MPI_Send(a, 0, MPI_INT, 0, 0, MPI_COMM_WORLD);); (26, MPI, MPI_Irecv(b, 0, MPI_INT, 0, 0, MPI_COMM_WORLD, &request);); (20, MPI, MPI_Wait(&request, &status);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (19, MPI, MPI_Get_count(&status, MPI_INT, &count);); (2, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(29, MPI, MPI_Recv(NULL, 0, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &status);); (16, MPI, MPI_Isend(a, 0, MPI_INT, rank, 0, MPI_COMM_WORLD, &request);); (24, MPI, MPI_Wait(&request, &status);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (24, MPI, MPI_Isend(NULL, 0, MPI_INT, rank, 0, MPI_COMM_WORLD, &request);); (14, MPI, MPI_Get_count(&status, MPI_INT, &count);); (12, MPI, MPI_Recv(b, 1, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &status);)"}
{"predict": "(14, OpenCL, retVal = clReleaseMemObject(image);)", "label": "(14, OpenCL, retVal = clReleaseMemObject(image);)"}
{"predict": "(10, OpenMP, error(\"samples must be defined\");)", "label": "(60, OpenMP, #pragma omp parallel for)"}
{"predict": "(17, CUDA, CHECK(cudaMalloc((void**) &anchorsDev, 4 * params.anchorsRatioCount * params.anchorsScaleCount * sizeof(float)));)", "label": "(17, CUDA, CHECK(cudaMalloc((void**) &anchorsDev, 4 * params.anchorsRatioCount * params.anchorsScaleCount * sizeof(float)));)"}
{"predict": "(3, CUDA, extern __shared__ float entire_buffer[];)", "label": "(3, CUDA, extern __shared__ float entire_buffer[];)"}
{"predict": "(24, CUDA, cudaFree(res_dev);); (17, CUDA, cudaMalloc(&res_dev, sizeof(float));); (19, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)", "label": "(24, CUDA, cudaFree(res_dev);); (17, CUDA, cudaMalloc(&res_dev, sizeof(float));); (19, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(16, OpenCL, auto b0 = clCreateBuffer(context, CL_MEM_READ_WRITE, 20, nullptr, nullptr);); (16, OpenCL, auto b1 = clCreateBuffer(context, CL_MEM_READ_WRITE, 20, nullptr, nullptr);); (35, OpenCL, clReleaseEvent(gatingEvent);); (22, OpenCL, clEnqueueMapBuffer(cmdQ, b0, CL_TRUE, CL_MAP_READ, 0, 10, 0, nullptr, nullptr, &retVal);); (33, OpenCL, clReleaseMemObject(b1);); (28, OpenCL, retVal = clEnqueueNDRangeKernel(cmdQ, pCmdQ->getDevice().getSpecializedDevice<ClDevice>(), 1, 0, &one, 0, 0, nullptr, &gatingEvent);); (9, OpenCL, auto b0 = clCreateBuffer(context, CL_MEM_READ_WRITE, 20, nullptr, nullptr);); (10, OpenCL, auto b1 = clCreateBuffer(context, CL_MEM_READ_WRITE, 20, nullptr, nullptr);); (24, OpenCL, retVal = clEnqueueMapBuffer(cmdQ, b1, CL_TRUE, CL_MAP_READ, 0, 10, 0, nullptr, nullptr, &retVal);); (28, OpenCL, clReleaseMemObject(b0);); (10, OpenCL, retVal = clEnqueueMapBuffer(cmdQ, b0, CL_TRUE, CL_MAP_READ, 0, 10, 0, nullptr, nullptr, &retVal);); (14, OpenCL, retVal = clEnqueueMapBuffer(cmdQ, b1, CL_TRUE, CL_MAP_READ, 0, 10, 0, nullptr, nullptr, &retVal);); (26, OpenCL, clReleaseMemObject(b0);)", "label": "(16, OpenCL, auto b1 = clCreateBuffer(context, CL_MEM_READ_WRITE, 20, nullptr, nullptr);); (24, OpenCL, void *ptr2 = clEnqueueMapBuffer(&cmdQ, b2, CL_TRUE, CL_MAP_READ, 0, 8, 0, nullptr, nullptr, nullptr);); (32, OpenCL, clReleaseMemObject(b2);); (17, OpenCL, void *ptr1 = clEnqueueMapBuffer(&cmdQ, b1, CL_FALSE, CL_MAP_READ, 0, 8, 1, &gatingEvent, nullptr, nullptr);); (29, OpenCL, clReleaseMemObject(b1);); (31, OpenCL, clReleaseEvent(gatingEvent);); (25, OpenCL, clEnqueueUnmapMemObject(pCmdQ, b2, ptr2, 0, nullptr, nullptr);); (12, OpenCL, auto gatingEvent = clCreateUserEvent(context, nullptr);); (13, OpenCL, clEnqueueUnmapMemObject(&cmdQ, b1, ptr1, 0, nullptr, nullptr);); (8, OpenCL, auto b2 = clCreateBuffer(context, CL_MEM_READ_WRITE, 20, nullptr, nullptr);)"}
{"predict": "(49, CUDA, cudaEventRecord(stop, NULL);); (49, CUDA, cudaEventSynchronize(stop);); (39, CUDA, cudaEventRecord(start, NULL);); (116, CUDA, cudaFree(gpuMatrix);); (50, CUDA, cudaEventElapsedTime(&eventMs, start, stop);); (39, CUDA, cudaMemcpy(gpuMatrix, Matrix, NUM * sizeof(float), cudaMemcpyHostToDevice);); (35, CUDA, cudaEventRecord(start, NULL);); (112, CUDA, cudaFree(gpuTransposeMatrix);); (5, CUDA, cudaGetDeviceProperties(&devProp, 0);); (14, CUDA, cudaEventCreate(&stop);); (46, CUDA, cudaMemcpy(TransposeMatrix, gpuTransposeMatrix, NUM * sizeof(float), cudaMemcpyDeviceToHost);); (39, CUDA, cudaEventSynchronize(start);); (6, CUDA, cudaEventCreate(&start);); (45, CUDA, cudaEventRecord(stop, NULL);); (29, CUDA, cudaMalloc((void**)&gpuMatrix, NUM * sizeof(float));); (29, CUDA, cudaMalloc((void**)&gpuTransposeMatrix, NUM * sizeof(float));)", "label": "(53, CUDA, cudaEventSynchronize(stop);); (86, CUDA, cudaMemcpy(TransposeMatrix, gpuTransposeMatrix, NUM * sizeof(float), cudaMemcpyDeviceToHost);); (9, CUDA, cudaGetDeviceProperties(&devProp, 0);); (44, CUDA, cudaMemcpy(gpuMatrix, Matrix, NUM * sizeof(float), cudaMemcpyHostToDevice);); (114, CUDA, cudaFree(gpuMatrix);); (32, CUDA, cudaMalloc((void**)&gpuMatrix, NUM * sizeof(float));); (49, CUDA, cudaEventElapsedTime(&eventMs, start, stop);); (36, CUDA, cudaEventRecord(start, NULL);); (112, CUDA, cudaFree(gpuTransposeMatrix);); (42, CUDA, cudaEventRecord(stop, NULL);); (10, CUDA, cudaEventCreate(&stop);); (28, CUDA, cudaMalloc((void**)&gpuTransposeMatrix, NUM * sizeof(float));); (6, CUDA, cudaEventCreate(&start);)"}
{"predict": "(5, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (55, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (70, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamDestroy(stream));)", "label": "(56, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (4, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (35, CUDA, ASSERT_EQ(cudaSuccess, cudaMemsetAsync(dstData->basePtr(), 0, dstBufSize * sizeof(T), stream));); (54, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamDestroy(stream));)"}
{"predict": "(38, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (44, MPI, MPI_Allgather(&item_offset, 1, MPI_INT, rank_disps, 1, MPI_INT, MPI_COMM_WORLD);)", "label": "(38, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (44, MPI, MPI_Allgather(&item_offset, 1, MPI_INT, rank_disps, 1, MPI_INT, MPI_COMM_WORLD);); (80, MPI, reduce_start = MPI_Wtime();)"}
{"predict": "(59, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 1, NULL, threads, localThreads, 0, NULL, NULL );); (47, OpenCL, error = clSetKernelArg(kernel, 0, sizeof( streams[0] ), &streams[0] );); (48, OpenCL, error = clSetKernelArg(kernel, 1, sizeof( streams[1] ), &streams[1] );); (60, OpenCL, error = clEnqueueReadBuffer( queue, streams[1], true, 0, sizeof( cl_double ) * TEST_SIZE, outData, 0, NULL, NULL );)", "label": "(58, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 1, NULL, threads, localThreads, 0, NULL, NULL );); (46, OpenCL, error = clSetKernelArg( kernel, 0, sizeof( streams[ 0 ] ), &streams[0] );); (60, OpenCL, error = clEnqueueReadBuffer( queue, streams[1], true, 0, sizeof( cl_double ) * TEST_SIZE, outData, 0, NULL, NULL );); (46, OpenCL, error = clSetKernelArg( kernel, 1, sizeof( streams[ 1 ] ), &streams[1] );)"}
{"predict": "(26, CUDA, cudaMemcpy(d_A, A, m * k * sizeof(float), cudaMemcpyHostToDevice);); (29, CUDA, cudaMemcpy(d_C, C, m * n * sizeof(float), cudaMemcpyHostToDevice);); (54, CUDA, cudaFree(d_A);); (20, CUDA, cudaMalloc((void **)&d_A, m * k * sizeof(float));); (21, CUDA, cudaMalloc((void **)&d_C, m * n * sizeof(float));); (56, CUDA, cudaFree(d_C);); (53, CUDA, cudaFree(d_B);); (17, CUDA, cudaMalloc((void **)&d_B, k * n * sizeof(float));); (20, CUDA, cudaMemcpy(d_B, B, k * n * sizeof(float), cudaMemcpyHostToDevice);)", "label": "(26, CUDA, cudaMemcpy(d_A, A, m * k * sizeof(float), cudaMemcpyHostToDevice);); (56, CUDA, cudaFree(d_A);); (31, CUDA, cudaMemcpy(C, d_C, m * n * sizeof(float), cudaMemcpyDeviceToHost);); (55, CUDA, cudaFree(d_B);); (55, CUDA, cudaFree(d_C);); (17, CUDA, cudaMalloc((void **)&d_B, k * n * sizeof(float));); (15, CUDA, cudaMalloc((void **)&d_A, m * k * sizeof(float));); (16, CUDA, cudaMalloc((void **)&d_C, m * n * sizeof(float));); (20, CUDA, cudaMemcpy(d_C, C, m * n * sizeof(float), cudaMemcpyHostToDevice);); (18, CUDA, cudaMemcpy(d_B, B, k * n * sizeof(float), cudaMemcpyHostToDevice);)"}
{"predict": "( cudaCheck(cudaMalloc(&d_dpreatt, B * T * C * sizeof(float)));); (34, CUDA, cudaCheck(cudaMalloc(&d_qkvr, B * T * 3 * C * sizeof(float)));); (38, CUDA, cudaCheck(cudaMalloc(&d_dout, B * T * C * sizeof(float)));); (39, CUDA, cudaCheck(cudaMalloc(&d_dinp, B * T * 3 * C * sizeof(float)));); (39, CUDA, cudaCheck(cudaMalloc(&d_dqkvr, B * T * 3 * C * sizeof(float)));); (34, CUDA, cudaCheck(cudaMalloc(&d_preatt, B * T * C * sizeof(float)));); (35, CUDA, cudaCheck(cudaMalloc(&d_att, B * T * C * sizeof(float)));); (39, CUDA, cudaCheck(cudaMalloc(&d_datt, B * T * C * sizeof(float)));); (39, CUDA, cudaCheck(cudaMalloc(&d_scratch, cuda_num_SMs * (2 * C + 1) * sizeof(float)));); (28, CUDA, cudaCheck(cudaMalloc(&d_out, B * T * C * sizeof(float)));); (31, CUDA, cudaCheck(cudaMalloc(&d_dout, B * T * C * sizeof(float)));); (39, CUDA, cudaCheck(cudaMalloc(&d_dqkvr, B * T * 3 * C * sizeof(float)));); (38, CUDA, cudaCheck(cudaMalloc(&d_dinp, B * T * 3 * C * sizeof(float)));); (27, CUDA, cudaCheck(cudaMalloc(&d_qkvr, B * T * 3 * C * sizeof(float)));); (28, CUDA, cudaCheck(cudaMalloc(&d_preatt, B * T * C * sizeof(float)));); (23, CUDA, cudaCheck(cudaMalloc(&d_inp, B * T * 3 * C * sizeof(float)));); (37, CUDA, cudaCheck(cudaMalloc(&d_datt, B * T * C * sizeof(float)));); (38, CUDA, cudaCheck(cudaMalloc(&d_dqkvr, B * T * 3 * C * sizeof(float)));); (39, CUDA, cudaCheck(cudaMalloc(&d_dout, B * T * C * sizeof(float)));); (34, CUDA, cudaCheck(cudaMalloc(&d_dinp, B * T * 3", "label": "(45, CUDA, cudaCheck(cudaMemset(d_dinp, 0, B * T * 3 * C * sizeof(float)));); (47, CUDA, cudaCheck(cudaMalloc(&d_dpreatt, B * NH * T * T * sizeof(float)));); (99, CUDA, cudaCheck(cudaFree(d_inp));); (38, CUDA, cudaCheck(cudaMemset(d_att, 0, B * NH * T * T * sizeof(float)));); (105, CUDA, cudaCheck(cudaFree(d_dpreatt));); (102, CUDA, cudaCheck(cudaFree(d_dinp));); (102, CUDA, cudaCheck(cudaFree(d_dqkvr));); (28, CUDA, cudaCheck(cudaMemset(d_qkvr, 0, B * T * 3 * C * sizeof(float)));); (31, CUDA, cudaCheck(cudaMemset(d_preatt, 0, B * NH * T * T * sizeof(float)));); (93, CUDA, cudaCheck(cudaFree(d_qkvr));); (30, CUDA, cudaCheck(cudaMalloc(&d_att, B * NH * T * T * sizeof(float)));); (99, CUDA, cudaCheck(cudaFree(d_datt));); (35, CUDA, cudaCheck(cudaMemset(d_dqkvr, 0, B * T * 3 * C * sizeof(float)));); (24, CUDA, cudaCheck(cudaMemset(d_out, 0, B * T * C * sizeof(float)));); (90, CUDA, cudaCheck(cudaFree(d_preatt));); (27, CUDA, cudaCheck(cudaMalloc(&d_dout, B * T * C * sizeof(float)));); (18, CUDA, cudaCheck(cudaMalloc(&d_qkvr, B * T * 3 * C * sizeof(float)));); (16, CUDA, cudaCheck(cudaMemcpy(d_inp, qkv, B * T * 3 * C * sizeof(float), cudaMemcpyHostToDevice));); (35, CUDA, cudaCheck(cudaMemset(d_scratch, 0, B * T * C * sizeof(float)));); (19, CUDA, cudaCheck(cudaMalloc(&d_preatt, B * NH * T * T * sizeof(float)));); (91, CUDA, cudaCheck(cudaFree(d_scratch));); (22, CUDA, cudaCheck(cudaMemcpy(d_dout, dout, B * T * C * sizeof(float), cudaMemcpyHostToDevice));); (81, CUDA, cudaCheck(cudaFree(d_out));); (23, CUDA, cudaCheck(cudaMalloc(&d_dqkvr, B * T * 3 * C * sizeof(float)));); (20, CUDA, cudaCheck(cudaMalloc(&d_dinp, B * T * 3 * C * sizeof(float)));); (26, CUDA, cudaCheck(cudaMemset(d_datt, 0, B * NH * T * T * sizeof(float)));); (26, CUDA, cudaCheck(cudaMalloc(&d_scratch, B * T * C * sizeof(float)));); (9, CUDA, cudaCheck(cudaMalloc(&d_out, B * T * C * sizeof(float)));); (22, CUDA, cudaCheck(cudaMalloc(&d_datt, B * NH * T * T * sizeof(float)));); (3, CUDA, cudaCheck(cudaMalloc(&d_inp, B * T * 3 * C * sizeof(float)));); (75, CUDA, cudaCheck(cudaFree(d_att));); (18, CUDA, cudaCheck(cudaMemset(d_dpreatt, 0, B * NH * T * T * sizeof(float)));); (74, CUDA, cudaCheck(cudaFree(d_dout));)"}
{"predict": "(48, MPI, MPI_Buffer_detach(&bptr, &bl);); (28, MPI, MPI_Recv(b, 10, MPI_INT, i, 27 + j, MPI_COMM_WORLD, &status);); (15, MPI, MPI_Bsend(a, 10, MPI_INT, 0, 27 + j, MPI_COMM_WORLD);); (6, MPI, MPI_Buffer_attach(buf, BUFSIZE);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (21, MPI, MPI_Wait(&request, &status);); (18, MPI, MPI_Irecv(b, 10, MPI_INT, i, 27 + j, MPI_COMM_WORLD, &request);)", "label": "(48, MPI, MPI_Buffer_detach(&bptr, &bl);); (17, MPI, MPI_Start(&request);); (27, MPI, MPI_Recv(b, 10, MPI_INT, i, 27 + j, MPI_COMM_WORLD, &status);); (8, MPI, MPI_Buffer_attach(buf, BUFSIZE);); (15, MPI, MPI_Wait(&request, &status);); (15, MPI, MPI_Request_free(&request);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (6, MPI, MPI_Bsend_init(a, 10, MPI_INT, 0, 27 + j, MPI_COMM_WORLD, &request);)"}
{"predict": "(31, MPI, auto err = MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided);); (39, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (39, MPI, MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);); (113, MPI, MPI_Allreduce(&run_time, &max_run_time, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);); (247, MPI, MPI_Finalize();); (27, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(31, MPI, auto err = MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided);); (119, MPI, MPI_Finalize();); (37, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (40, MPI, ascent_opt[\"mpi_comm\"] = MPI_Comm_c2f(MPI_COMM_WORLD);); (111, MPI, MPI_Reduce(&run_time, &max_run_time, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);); (33, MPI, MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);)"}
{"predict": "(15, CUDA, CUDASimulation cudaSimulation(model);)", "label": "(15, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(44, TBB, double t1d_auto = pforTranspose2d<tbb::auto_partitioner>(N, a, b, gs);); (47, TBB, pforTranspose2d<tbb::auto_partitioner>(N, a, b, gs);); (4, TBB, c.set_core_type(tbb::info::core_types().back());); (1, TBB, tbb::task_arena::constraints c;); (52, TBB, checkCudaErrors(cudaFreeArray(a));); (42, TBB, pforTranspose2d<tbb::simple_partitioner>(N, a, b, gs);); (0, TBB, c.set_numa_id(tbb::info::numa_nodes()[0]);); (2, TBB, c.set_max_concurrency(std::min(8, tbb::info::default_concurrency(c)));); (52, TBB, checkCudaErrors(cudaFreeArray(b));); (-3, TBB, tbb::task_arena a(c);); (29, TBB, pforTranspose2d<tbb::auto_partitioner>(N, a, b, gs);); (29, TBB, double t1d_simple = pforTranspose2d<tbb::simple_partitioner>(N, a, b, gs);); (37, TBB, double t2d_auto = pforTranspose2d<tbb::auto_partitioner>(N, a, b, gs);); (37, TBB, pforTranspose2d<tbb::simple_partitioner>(N, a, b, gs);); (38, TBB, double t2d_simple = pforTranspose2d<tbb::simple_partitioner>(N, a, b, gs);); (5, TBB, c.set_partitioner(tbb::info::default_partitioner());)", "label": "(47, TBB, double t2d_auto = pforTranspose2d<tbb::auto_partitioner>(N, a, b, gs);); (45, TBB, pforTranspose2d<tbb::auto_partitioner>(N, a, b, gs);); (4, TBB, c.set_core_type(tbb::info::core_types().back());); (1, TBB, tbb::task_arena::constraints c;); (48, TBB, double t2d_simple = pforTranspose2d<tbb::simple_partitioner>(N, a, b, gs);); (37, TBB, double t1d_simple = pforTranspose<tbb::simple_partitioner>(N, a, b, gs);); (-1, TBB, c.set_numa_id(tbb::info::numa_nodes()[0]);); (29, TBB, pforTranspose<tbb::auto_partitioner>(N, a, b, gs);); (29, TBB, double t1d_auto = pforTranspose<tbb::auto_partitioner>(N, a, b, gs);); (32, TBB, pforTranspose<tbb::simple_partitioner>(N, a, b, gs);); (-2, TBB, c.set_max_concurrency(std::min(8, tbb::info::default_concurrency(c)));); (-2, TBB, tbb::task_arena a(c);); (39, TBB, pforTranspose2d<tbb::simple_partitioner>(N, a, b, gs);)"}
{"predict": "(15, CUDA, __shared__ FP sMax[Br];); (23, CUDA, __shared__ FP sV[Bc][dim];); (19, CUDA, __shared__ FP sO[Br][dim];); (23, CUDA, __shared__ FP sDenom[Br];); (17, CUDA, __shared__ FP sK[Bc][Br];); (12, CUDA, __shared__ FP sQK[Br][Bc];); (17, CUDA, __shared__ FP sSafeE[Br][Bc];); (11, CUDA, __shared__ FP sV[Br][Bc];); (16, CUDA, __shared__ FP sO[Br][Bc];); (11, CUDA, __shared__ FP sK[Br][Bc];); (16, CUDA, __shared__ FP sDenom[Br];); (11, CUDA, __shared__ FP sQ[Br][dim];); (13, CUDA, __shared__ FP sMax[Br];)", "label": "(12, CUDA, __shared__ FP sV[Bc][dim];); (19, CUDA, __shared__ FP sMax[Br];); (9, CUDA, __shared__ FP sK[Bc][dim];); (12, CUDA, __shared__ FP sQK[Br][Bc];); (13, CUDA, __shared__ FP sSafeE[Br][Bc];); (9, CUDA, __shared__ FP sO[Br][dim];); (4, CUDA, __shared__ FP sQ[Br][dim];); (12, CUDA, __shared__ FP sDenom[Br];)"}
{"predict": "(22, CUDA, cudaSetDevice(device_id_);)", "label": "(22, CUDA, CudaCheck(cudaSetDevice(device_id_));)"}
{"predict": "(7, OpenCL, void *mappedPtr = clEnqueueMapBuffer(cmdQ.get(), buffer.get(), CL_FALSE, CL_MAP_WRITE, offset, size, 0, nullptr, nullptr, &retVal);); (13, OpenCL, retVal = clEnqueueUnmapMemObject(cmdQ.get(), buffer.get(), mappedPtr, 0, nullptr, nullptr);)", "label": "(14, OpenCL, retVal = clEnqueueUnmapMemObject(cmdQ.get(), buffer.get(), mappedPtr, 0, nullptr, nullptr);); (6, OpenCL, void *mappedPtr = clEnqueueMapBuffer(cmdQ.get(), buffer.get(), CL_FALSE, CL_MAP_WRITE, offset, size, 0, nullptr, nullptr, nullptr);)"}
{"predict": "(65, OpenMP, #pragma omp target shared(i)); (67, OpenMP, #pragma omp target private(j)); (53, OpenMP, #pragma omp target shared(h, B::x)); (26, OpenMP, #pragma omp target shared(argc > 0? argv[1] : argv[2])); (17, OpenMP, #pragma omp target shared(argc); (37, OpenMP, #pragma omp target shared(a, b)); (32, OpenMP, #pragma omp target shared(argv[1])); (60, OpenMP, #pragma omp target shared(i)); (50, OpenMP, #pragma omp target shared(i)); (38, OpenMP, #pragma omp target shared(e, g)); (0, OpenMP, #pragma omp target shared(); (48, OpenMP, #pragma omp parallel shared(i)); (-3, OpenMP, #pragma omp target shared); (17, OpenMP, #pragma omp target shared(argc, z)); (3, OpenMP, #pragma omp target shared()); (59, OpenMP, #pragma omp target private(i)); (14, OpenMP, #pragma omp target shared(argc,); (4, OpenMP, #pragma omp target shared(argc,); (22, OpenMP, #pragma omp target shared(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (17, OpenMP, #pragma omp target shared(argc > 0? omp_get_default_mem_alloc:() : omp_get_default_mem_alloc:()); (-5, OpenMP, #pragma omp target shared(argc); (40, OpenMP, #pragma omp target shared(j))", "label": "(69, OpenMP, #pragma omp parallel private(i)); (42, OpenMP, #pragma omp master taskloop lastprivate(a, b)); (49, OpenMP, #pragma omp master taskloop lastprivate(z, e, g)); (16, OpenMP, #pragma omp master taskloop lastprivate(argc); (2, OpenMP, #pragma omp parallel); (18, OpenMP, #pragma omp master taskloop lastprivate(argc,); (25, OpenMP, #pragma omp master taskloop lastprivate(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (0, OpenMP, #pragma omp master taskloop lastprivate); (47, OpenMP, #pragma omp master taskloop lastprivate(h)); (18, OpenMP, #pragma omp master taskloop lastprivate(argc > 0? argv[1] : argv[2])); (64, OpenMP, #pragma omp master taskloop lastprivate(i)); (51, OpenMP, #pragma omp master taskloop allocate(omp_thread_mem_alloc: i) lastprivate(i)); (27, OpenMP, #pragma omp master taskloop lastprivate(S1)); (57, OpenMP, #pragma omp master taskloop lastprivate(j)); (-3, OpenMP, #pragma omp master taskloop lastprivate(); (32, OpenMP, #pragma omp master taskloop lastprivate(argv[1])); (-1, OpenMP, #pragma omp master taskloop lastprivate()); (18, OpenMP, #pragma omp master taskloop lastprivate(conditional: argc) lastprivate(conditional:); (50, OpenMP, #pragma omp parallel shared(i))"}
{"predict": "(5, CUDA, cudaStream_t stream = create_and_populate_stream(b, n, m, idxs, dataset, temp, n_threads);); (52, CUDA, CUDA_CHECK_ERRORS();)", "label": "(5, CUDA, cudaStream_t stream = at::cuda::getCurrentCUDAStream();)"}
{"predict": "(6, MPI, MPI_Send(sbuf, size, MPI_BYTE, rbuf, tagno, MPI_COMM_WORLD);); (13, MPI, MPI_Send(NULL, 0, MPI_BYTE, from, 100, MPI_COMM_WORLD);); (4, MPI, MPI_Isend(sbuf, size, MPI_BYTE, rbuf, tagno, MPI_COMM_WORLD, &request);); (0, MPI, MPI_Recv(NULL, 0, MPI_BYTE, to, 100, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (11, MPI, MPI_Recv(NULL, 0, MPI_BYTE, from, 100, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (2, MPI, MPI_Wait(&request, MPI_STATUS_IGNORE);)", "label": "(13, MPI, MPI_Recv(NULL, 0, MPI_BYTE, to, 100, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (11, MPI, MPI_Send(NULL, 0, MPI_BYTE, from, 100, MPI_COMM_WORLD);); (4, MPI, MPI_Irecv(rbuf, size, MPI_BYTE, from, tagno, MPI_COMM_WORLD, &request);); (18, MPI, MPI_Send(sbuf, size, MPI_BYTE, to, tagno, MPI_COMM_WORLD);); (20, MPI, MPI_Wait(&request, MPI_STATUS_IGNORE);)"}
{"predict": "(13, MPI, MPI_Abort(MPI_COMM_WORLD, MPIT_result);); (14, MPI, MPIT_result = MPI_T_pvar_session_create(session);); (8, MPI, MPIT_result = MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);)", "label": "(13, MPI, MPI_Abort(MPI_COMM_WORLD, MPIT_result);); (14, MPI, MPIT_result = MPI_T_pvar_session_create(session);); (8, MPI, MPIT_result = MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);)"}
{"predict": "(8, OpenMP, #pragma omp for schedule(MY_SCHEDULE)); (3, OpenMP, #pragma omp target map(tofrom: a, b)); (8, OpenMP, #pragma omp atomic)", "label": "(9, OpenMP, #pragma omp atomic); (3, OpenMP, #pragma omp parallel); (5, OpenMP, #pragma omp for schedule(MY_SCHEDULE))"}
{"predict": "(14, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (11, CUDA, CUDA_CHECK(cudaGetDevice(&current_device_id));); (11, CUDA, CUDA_CHECK(cudaGetDeviceProperties(&props, current_device_id));)", "label": "(14, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (11, CUDA, CUDA_CHECK(cudaGetDevice(&current_device_id));); (11, CUDA, CUDA_CHECK(cudaGetDeviceProperties(&props, current_device_id));)"}
{"predict": "(9, CUDA, err = cudaFree(handle->d_workspace);); (3, CUDA, cudaError_t err = cudaFree(handle->c_workspace);)", "label": "(4, CUDA, auto err = cudaFree(handle->deviceWs);); (8, CUDA, err = cudaEventDestroy(handle->wsMutex);)"}
{"predict": "(18, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(18, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(87, OpenMP, int OLvL = omp_get_level();); (90, OpenMP, comp_add_script_line(\"Test=%s:%s\\n\", _p->pName, gen_pw);)", "label": "(39, OpenMP, comp_add_script_line(\"Func=DynamicFunc__\" #C \"_crypt_input%d_to_output1_FINAL\\n\",side);); (18, OpenMP, else                 comp_add_script_line(\"Func=DynamicFunc__append_salt%s\\n\", side==2?\"2\":\"\");); (9, OpenMP, comp_add_script_line(\"Func=DynamicFunc__clean_input2_kwik\\n\");); (61, OpenMP, comp_add_script_line(\"MaxInputLen=110\\n\");); (5, OpenMP, comp_add_script_line(\"Func=DynamicFunc__set_input_len_%d\\n\", keys_hex_len);)"}
{"predict": "(50, OpenCL, clReleaseEvent(event);)", "label": "(50, OpenCL, clReleaseEvent(event);)"}
{"predict": "(17, CUDA, ColorSpinorParam cudaParam(cpuParam, inv_param, QUDA_CUDA_FIELD_LOCATION);)", "label": "(17, CUDA, ColorSpinorParam cudaParam(cpuParam, inv_param, QUDA_CUDA_FIELD_LOCATION);)"}
{"predict": "(63, MPI, MPI_Send(buf_send, size_send, MPI_BYTE, rank_send, 0, comm);); (40, MPI, MPI_Waitall(num_req, request, status);); (43, MPI, MPI_Irecv(&size_recv, 1, MPI_INT, rank_recv, 0, comm, &request[0]);); (43, MPI, MPI_Irecv(buf_recv, size_recv, MPI_BYTE, rank_recv, 0, comm, &request[1]);); (36, MPI, MPI_Send(&size_send, 1, MPI_INT, rank_send, 0, comm);); (58, MPI, MPI_Send(buf_send, size_send, MPI_BYTE, rank_send, 1, comm);)", "label": "(55, MPI, MPI_Irecv(buf_recv, size_recv, MPI_BYTE, rank_recv, 0, comm, &request[num_req]);); (25, MPI, MPI_Irecv(&size_recv, 1, MPI_INT, rank_recv, 0, comm, &request[num_req]);); (60, MPI, MPI_Isend(buf_send, size_send, MPI_BYTE, rank_send, 0, comm, &request[num_req]);); (38, MPI, MPI_Isend(&size_send, 1, MPI_INT, rank_send, 0, comm, &request[num_req]);); (41, MPI, MPI_Waitall(num_req, request, status);)"}
{"predict": "(3, CUDA, extern __shared__ float shared[];)", "label": "(3, CUDA, extern __shared__ float shared[];)"}
{"predict": "(4, OpenMP, #pragma omp parallel for)", "label": "(4, OpenMP, #pragma omp parallel for)"}
{"predict": "(36, OpenMP, #pragma omp distribute simd safelen (1))); (44, OpenMP, #pragma omp distribute simd safelen ((ST > 0)? 1 + ST : 2)); (9, OpenMP, #pragma omp distribute simd safelen (); (75, OpenMP, #pragma omp distribute simd safelen (N)); (1, OpenMP, #pragma omp distribute simd safelen); (12, OpenMP, #pragma omp distribute simd safelen ()); (70, OpenMP, #pragma omp parallel safelen (N)); (34, OpenMP, #pragma omp distribute simd safelen (foobool(1) > 0? 1 : 2)); (20, OpenMP, #pragma omp distribute simd safelen (ST); (54, OpenMP, #pragma omp distribute simd safelen (argv[1]=2)); (0, OpenMP, #pragma omp distribute simd safelen (argc); (14, OpenMP, #pragma omp distribute simd safelen (1))); (6, OpenMP, #pragma omp distribute simd safelen (argc))); (10, OpenMP, #pragma omp distribute simd safelen (ST); (54, OpenMP, #pragma omp distribute simd safelen (S)); (21, OpenMP, #pragma omp distribute simd safelen (N)); (-14, OpenMP, #pragma omp target); (-14, OpenMP, #pragma omp teams)", "label": "(11, OpenMP, #pragma omp distribute simd safelen (); (71, OpenMP, #pragma omp distribute simd safelen (4)); (3, OpenMP, #pragma omp distribute simd safelen); (19, OpenMP, #pragma omp distribute simd safelen (argc); (51, OpenMP, #pragma omp distribute simd safelen (S)); (44, OpenMP, #pragma omp distribute simd safelen (foobool(argc)), safelen (true), safelen (-5)); (72, OpenMP, #pragma omp distribute simd safelen (N)); (27, OpenMP, #pragma omp distribute simd safelen (1))); (58, OpenMP, #pragma omp distribute simd safelen (argv[1]=2)); (-6, OpenMP, #pragma omp target); (18, OpenMP, #pragma omp distribute simd safelen (ST); (29, OpenMP, #pragma omp distribute simd safelen ((ST > 0)? 1 + ST : 2)); (5, OpenMP, #pragma omp distribute simd safelen ()); (-9, OpenMP, #pragma omp teams)"}
{"predict": "(6, OpenMP, #pragma omp parallel for simd private); (29, OpenMP, #pragma omp parallel for simd private(a, b)); (49, OpenMP, #pragma omp parallel for simd private(i)); (43, OpenMP, #pragma omp parallel for simd shared(i)); (14, OpenMP, #pragma omp parallel for simd private(argc,); (17, OpenMP, #pragma omp parallel for simd private(argc > 0? argv[1] : argv[2])); (50, OpenMP, #pragma omp parallel private(i)); (51, OpenMP, #pragma omp parallel for simd private(j)); (37, OpenMP, #pragma omp parallel for simd private(h)); (53, OpenMP, #pragma omp parallel for simd private(i)); (39, OpenMP, #pragma omp parallel shared(i)); (14, OpenMP, #pragma omp parallel for simd private(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (16, OpenMP, #pragma omp parallel for simd private(S1)); (-3, OpenMP, #pragma omp parallel); (22, OpenMP, #pragma omp parallel for simd private(e, g, z)); (0, OpenMP, #pragma omp parallel for simd private()); (17, OpenMP, #pragma omp parallel for simd private(argv[1])); (24, OpenMP, #pragma omp parallel for simd private(argv[1])); (11, OpenMP, #pragma omp parallel for simd private(argc) allocate(omp_thread_mem_alloc: argc)); (22, OpenMP, #pragma omp parallel for simd private(h)); (1, OpenMP, #pragma omp parallel for simd private(argc); (28, OpenMP, #pragma omp parallel for simd private(); (27, OpenMP, #pragma omp parallel shared(i))", "label": "(33, OpenMP, #pragma omp parallel for simd private(argv[1])); (29, OpenMP, #pragma omp parallel for simd private(z, a, b)); (47, OpenMP, #pragma omp parallel for simd private(i)); (15, OpenMP, #pragma omp parallel for simd private(argc,); (23, OpenMP, #pragma omp parallel for simd private(S1)); (50, OpenMP, #pragma omp parallel shared(i)); (33, OpenMP, #pragma omp parallel for simd private(h)); (49, OpenMP, #pragma omp parallel private(i)); (-2, OpenMP, #pragma omp parallel); (12, OpenMP, #pragma omp parallel for simd private(argc > 0? argv[1] : argv[2])); (26, OpenMP, #pragma omp parallel for simd private(e, g)); (-2, OpenMP, #pragma omp parallel for simd private); (30, OpenMP, #pragma omp parallel for simd nowait); (44, OpenMP, #pragma omp parallel for simd private(j)); (10, OpenMP, #pragma omp parallel for simd private(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (0, OpenMP, #pragma omp parallel for simd private(argc); (-4, OpenMP, #pragma omp parallel for simd private()"}
{"predict": "(4, OpenMP, #pragma omp master taskloop); (2, OpenMP, #pragma omp parallel)", "label": "(3, OpenMP, #pragma omp parallel); (3, OpenMP, #pragma omp master taskloop)"}
{"predict": "(25, MPI, MPI_Comm_rank(comm_row, &mycol);); (25, MPI, MPI_Comm_rank(comm_col, &myrow);); (25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)", "label": "(25, MPI, MPI_Comm_rank(comm_row, &mycol);); (25, MPI, MPI_Comm_rank(comm_col, &myrow);); (25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(56, OpenMP, #pragma omp simd)", "label": "(56, OpenMP, #pragma omp simd)"}
{"predict": "(54, MPI, MPI_Comm_size(comm, &size);); (61, MPI, MPI_Barrier(comm);); (28, MPI, MPI_Type_get_extent(dtp.DTP_base_type, &lb, &extent);); (50, MPI, MPI_Win_create(target.buf, target.maxbufsize, 1, MPI_INFO_NULL, comm, &win);); (69, MPI, MPI_Win_free(&win);); (46, MPI, MPI_Comm_rank(comm, &rank);)", "label": "(52, MPI, MPI_Comm_size(comm, &size);); (62, MPI, MPI_Barrier(comm);); (52, MPI, MPI_Win_create(target.buf, target.maxbufsize, base_type_size, MPI_INFO_NULL, comm, &win);); (69, MPI, MPI_Win_free(&win);); (25, MPI, MPI_Type_get_extent(dtp.DTP_base_type, &lb, &extent);); (46, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(1, CUDA, __shared__ float cache[threadsPerBlock];)", "label": "(1, CUDA, __shared__ float cache[threadsPerBlock];)"}
{"predict": "(117, OpenMP, double stop = omp_get_wtime();); (154, MPI, MPI_Bcast( &Xnorm, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD );); (152, MPI, MPI_Bcast( &Anorm, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD );); (153, MPI, MPI_Bcast( &Rnorm, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD );); (115, OpenMP, double start = omp_get_wtime();); (134, MPI, MPI_Allreduce( &Xnorm, &Xnorm, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD );); (134, MPI, MPI_Allreduce( &Anorm, &Anorm, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD );); (134, MPI, MPI_Allreduce( &Rnorm, &Rnorm, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD );); (119, OpenMP, #pragma omp parallel for); (140, MPI, MPI_Bcast( ipiv2, n, MPI_INT, 0, MPI_COMM_WORLD );); (138, MPI, MPI_Bcast( ipiv, n, MPI_INT, 0, MPI_COMM_WORLD );); (143, MPI, MPI_Bcast( &n, 1, MPI_INT, 0, MPI_COMM_WORLD );); (140, MPI, MPI_Bcast( &nb, 1, MPI_INT, 0, MPI_COMM_WORLD );); (143, MPI, MPI_Bcast( &ldx, 1, MPI_LONG, 0, MPI_COMM_WORLD );); (143, MPI, MPI_Bcast( &ldb, 1, MPI_LONG, 0, MPI_COMM_WORLD );); (136, MPI, MPI_Bcast( &n, 1, MPI_INT, 0, MPI_COMM_WORLD );); (143, MPI, MPI_Bcast( &klt, 1, MPI_INT, 0, MPI_COMM_WORLD );); (143, MPI, MPI_Bcast( &kut, 1, MPI_INT, 0, MPI_COMM_WORLD );); (136, MPI, MPI_Bcast( &ldx, 1, MPI_LONG, 0, MPI_COMM_WORLD );); (143, MPI, MPI_Bcast( &ldt, 1, MPI_LONG, 0, MPI_COMM_WORLD );); (136, MPI, MPI", "label": "(117, OpenMP, plasma_time_t stop = omp_get_wtime();); (113, OpenMP, plasma_time_t start = omp_get_wtime();)"}
{"predict": "(7, CUDA, __shared__ float block_acc[32];)", "label": "(7, CUDA, __shared__ float block_acc[32];)"}
{"predict": "(2, OpenMP, #pragma omp for collapse(2) schedule(runtime))", "label": "(3, OpenMP, #pragma omp for collapse(2) schedule(runtime))"}
{"predict": "(6, OpenMP, #pragma omp target parallel for simd linear); (29, OpenMP, #pragma omp target parallel for simd linear(argv[1])); (60, OpenMP, #pragma omp target parallel for simd linear(j)); (44, OpenMP, #pragma omp target parallel for simd linear(i)); (34, OpenMP, #pragma omp target parallel for simd linear(h)); (48, OpenMP, #pragma omp target parallel for simd linear(v : i)); (4, OpenMP, #pragma omp target parallel for simd linear(); (13, OpenMP, #pragma omp target parallel for simd linear(argc,); (18, OpenMP, #pragma omp target parallel for simd linear(argc > 0? argv[1] : argv[2])); (22, OpenMP, #pragma omp target parallel for simd linear(a, b : B::ib)); (1, OpenMP, #pragma omp target parallel for simd linear()); (26, OpenMP, #pragma omp target parallel for simd linear(e, g)); (31, OpenMP, #pragma omp target parallel for simd linear(z, v : i)); (44, OpenMP, #pragma omp target parallel for simd linear(v : j)); (49, OpenMP, #pragma omp target parallel for simd linear(v : j)); (4, OpenMP, #pragma omp target parallel for simd linear(argc); (48, OpenMP, #pragma omp target parallel for simd linear(i)); (43, OpenMP, #pragma omp target parallel for simd linear(j)); (20, OpenMP, #pragma omp target parallel for simd linear); (3, OpenMP, #pragma omp target parallel for simd linear(argc,)", "label": "(32, OpenMP, #pragma omp target parallel for simd linear(a, b : B::ib)); (23, OpenMP, #pragma omp target parallel for simd linear(argc : 5) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (4, OpenMP, #pragma omp target parallel for simd linear); (12, OpenMP, #pragma omp target parallel for simd linear(argc); (40, OpenMP, #pragma omp target parallel for simd linear(i)); (33, OpenMP, #pragma omp target parallel for simd linear(e, g, z)); (45, OpenMP, #pragma omp target parallel for simd linear(v : i)); (34, OpenMP, #pragma omp target parallel for simd linear(h)); (39, OpenMP, #pragma omp parallel); (12, OpenMP, #pragma omp target parallel for simd linear(argc > 0? argv[1] : argv[2])); (-1, OpenMP, #pragma omp target parallel for simd linear(); (7, OpenMP, #pragma omp target parallel for simd linear(argc,); (49, OpenMP, #pragma omp target parallel for simd linear(v : j)); (22, OpenMP, #pragma omp target parallel for simd linear(argv[1])); (13, OpenMP, #pragma omp target parallel for simd linear(S1)); (42, OpenMP, #pragma omp target parallel for simd linear(j)); (-4, OpenMP, #pragma omp target parallel for simd linear())"}
{"predict": "(25, OpenMP, #pragma omp parallel for)", "label": "(25, OpenMP, #pragma omp parallel for)"}
{"predict": "(26, OpenMP, #pragma omp target simd collapse 4)); (57, OpenMP, #pragma omp target simd collapse(4,, 4)); (4, OpenMP, #pragma omp target simd collapse); (54, OpenMP, #pragma omp target simd collapse(4, 8)); (39, OpenMP, #pragma omp target simd collapse(4)); (30, OpenMP, #pragma omp target simd collapse(4,); (72, OpenMP, #pragma omp target simd collapse(0)); (7, OpenMP, #pragma omp target simd collapse()); (63, OpenMP, #pragma omp target simd collapse(foo())); (-3, OpenMP, #pragma omp target simd collapse(); (69, OpenMP, #pragma omp target simd collapse(-5)); (14, OpenMP, #pragma omp target simd collapse(, )); (2, OpenMP, #pragma omp target simd collapse(,); (36, OpenMP, #pragma omp target simd collapse(4 4)); (16, OpenMP, #pragma omp target simd collapse 4)); (51, OpenMP, #pragma omp target simd collapse(2.5))", "label": "(41, OpenMP, #pragma omp target simd collapse(4, )); (12, OpenMP, #pragma omp target simd collapse()); (48, OpenMP, #pragma omp target simd collapse(4 4)); (63, OpenMP, #pragma omp target simd collapse(4, 8)); (82, OpenMP, #pragma omp target simd collapse(5 - 5)); (4, OpenMP, #pragma omp target simd collapse(); (15, OpenMP, #pragma omp target simd collapse(, )); (10, OpenMP, #pragma omp target simd collapse(,); (74, OpenMP, #pragma omp target simd collapse(0)); (36, OpenMP, #pragma omp target simd collapse(4)); (45, OpenMP, #pragma omp target simd collapse(4,, 4)); (63, OpenMP, #pragma omp target simd collapse(foo())); (66, OpenMP, #pragma omp target simd collapse(-5)); (23, OpenMP, #pragma omp target simd collapse(4,); (17, OpenMP, #pragma omp target simd collapse(4); (11, OpenMP, #pragma omp target simd collapse 4)); (-11, OpenMP, #pragma omp target simd collapse); (53, OpenMP, #pragma omp target simd collapse(2.5))"}
{"predict": "(97, OpenMP, nthreads = omp_get_num_threads();); (82, OpenMP, #pragma omp parallel firstprivate(niter) private(step)); (62, OpenMP, #pragma omp parallel); (85, OpenMP, #pragma omp master)", "label": "(97, OpenMP, nthreads = omp_get_num_threads();); (82, OpenMP, #pragma omp parallel firstprivate(niter) private(step)); (62, OpenMP, #pragma omp parallel); (85, OpenMP, #pragma omp master)"}
{"predict": "(35, OpenMP, iis3dhhc_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)", "label": "(35, OpenMP, iis3dhhc_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)"}
{"predict": "(60, OpenMP, #pragma omp target exit data nowait map(from: c) depend(in : argc, argv[i:argc], x[:]) map(release: d)); (68, OpenMP, #pragma omp target exit data nowait depend(in : argc, argv[i:argc], x[:]) map(always,release: e)); (69, OpenMP, #pragma omp target exit data nowait depend(in : argc, argv[i:argc], x[:]) map(release: c) if(b>g)); (59, OpenMP, #pragma omp target exit data nowait map(from: x[0:10], c) depend(in : argc, argv[i:argc], x[:])); (68, OpenMP, #pragma omp target exit data nowait depend(in : argc, argv[i:argc], x[:]) map(from: a) if(b>g) nowait); (59, OpenMP, #pragma omp target exit data nowait map(from: c) depend(in : argc, argv[i:argc], x[:]) nowait); (54, OpenMP, #pragma omp target exit data nowait map(from: x[0:10], c) depend(in : argc, argv[i:argc], x[:])); (60, OpenMP, #pragma omp target exit data nowait map(from: c) depend(in : argc, argv[i:argc], x[:]) nowait); (62, OpenMP, #pragma omp target exit data nowait map(always,release: e)); (67, OpenMP, #pragma omp target exit data nowait depend(in : argc, argv[i:argc], x[:]) map(release: c) nowait if(b>g)); (54, OpenMP, #pragma omp target exit data nowait map(from: a) depend(in : argc, argv[i:argc], x[:]) if(target exit data: b)); (55, OpenMP, #pragma omp target exit data nowait map(from: c) depend(in : argc, argv[i:argc], x[:]) nowait); (57, OpenMP, #pragma omp target exit data nowait depend(in : argc, argv[i:argc], x[:]) map(release: c) nowait if(b>g)); (62, OpenMP, #pragma omp target exit data nowait depend(in : argc, argv[i:argc], x[:]) map(always,release: e)); (50, OpenMP, #", "label": "(72, OpenMP, #pragma omp target exit data depend(in : argc,argv[i:argc],x[:]) map(release: c) nowait if(b>g)); (17, OpenMP, #pragma omp target exit data map(release: c) if(b>g)); (52, OpenMP, #pragma omp target exit data nowait map(from: c) map(release: d)); (57, OpenMP, #pragma omp target exit data nowait depend(in : argc,argv[i:argc],x[:]) map(from: a)); (1, OpenMP, #pragma omp target exit data map(from: a)); (16, OpenMP, #pragma omp target exit data map(from: x[0:10], c)); (24, OpenMP, #pragma omp target exit data map(from: c) map(release: d)); (41, OpenMP, #pragma omp target exit data map(release: c) nowait if(b>g)); (58, OpenMP, #pragma omp target exit data map(from: a) if (b > g) nowait depend(in : argc,argv[i:argc],x[:])); (72, OpenMP, #pragma omp target exit data nowait depend(in : argc,argv[i:argc],x[:]) map(always,release: e)); (5, OpenMP, #pragma omp target exit data map(from: c)); (58, OpenMP, #pragma omp target exit data map(from: c) depend(in : argc,argv[i:argc],x[:]) nowait); (30, OpenMP, #pragma omp target exit data map(from: a) if (b > g) nowait); (38, OpenMP, #pragma omp target exit data nowait map(from: x[0:10], c)); (19, OpenMP, #pragma omp target exit data map(always,release: e)); (30, OpenMP, #pragma omp target exit data map(from: c) nowait); (62, OpenMP, #pragma omp target exit data nowait map(from: c) depend(in : argc,argv[i:argc],x[:]) map(release: d)); (58, OpenMP, #pragma omp target exit data nowait map(from: x[0:10], c) depend(in : argc,argv[i:argc],x[:])); (-6, OpenMP, #pragma omp target exit data map(from: a) if (b > g)); (5, OpenMP, #pragma omp target exit data map(delete: x[0:10])); (43, OpenMP, #pragma omp target exit data nowait map(from: a) depend(in : argc,argv[i:argc],x[:]) if (target exit data: b)); (36, OpenMP, #pragma omp target exit data nowait map(always,release: e)); (17, OpenMP, #pragma omp target exit data nowait map(from: a) if (target exit data: b)); (-14, OpenMP, #pragma omp target exit data map(from: a) if (target exit data: b)); (12, OpenMP, #pragma omp target exit data nowait map(from: a)); (2, OpenMP, #pragma omp target exit data map(always, delete: x[0:10]))"}
{"predict": "(61, OpenCL, TEST_OCL_CHECK(clReleaseCommandQueue(ocl_queue));)", "label": "(62, OpenCL, TEST_OCL_CHECK(clReleaseEvent(read_buffer_event));); (64, OpenCL, TEST_OCL_CHECK(clReleaseCommandQueue(ocl_queue));); (52, OpenCL, TEST_OCL_CHECK(clWaitForEvents(1, &read_buffer_event));); (61, OpenCL, TEST_OCL_CHECK(clReleaseEvent(eltwise_event));); (59, OpenCL, TEST_OCL_CHECK(clReleaseEvent(write_buffer_event));)"}
{"predict": "(6, CUDA, cudaExtent extent = make_cudaExtent(4, 5, 6);); (9, CUDA, extent = make_cudaExtent(2, 3, 4);); (12, CUDA, extent = make_cudaExtent(3, 4, 5);); (26, CUDA, extent = make_cudaExtent(4, 5, 6);); (29, CUDA, extent = make_cudaExtent(0, 4, 5);); (36, CUDA, extent = make_cudaExtent(4, 5, 0);); (27, CUDA, extent = make_cudaExtent(1, 4, 5);); (6, CUDA, extent = make_cudaExtent(1, 3, 4);); (37, CUDA, extent = make_cudaExtent(2, 3, 0);); (7, CUDA, extent = make_cudaExtent(2, 3, 4);); (10, CUDA, extent = make_cudaExtent(3, 4, 4);); (5, CUDA, extent = make_cudaExtent(3, 4, 5);)", "label": "(10, CUDA, extent = make_cudaExtent(4, 5, 0);); (3, CUDA, cudaExtent extent = make_cudaExtent(4, 5, 6);)"}
{"predict": "(16, CUDA, cudaMalloc((void**)&device_array, num_bytes);); (55, CUDA, cudaMemcpy(host_array, device_array, num_bytes, cudaMemcpyDeviceToHost);); (38, CUDA, cudaDeviceSynchronize();); (44, CUDA, cudaError_t error = cudaGetLastError();); (47, CUDA, cudaErrorString(error);)", "label": "(16, CUDA, cudaMalloc((void**)&device_array, num_bytes);); (49, CUDA, printf(\"CUDA Error: %s\\n\", cudaGetErrorString(error));); (35, CUDA, cudaThreadSynchronize();); (42, CUDA, cudaError_t error = cudaGetLastError();); (66, CUDA, cudaFree(device_array);); (54, CUDA, cudaMemcpy(host_array, device_array, num_bytes, cudaMemcpyDeviceToHost);)"}
{"predict": "(25, OpenMP, int threadId = omp_get_thread_num();); (59, OpenMP, #pragma omp parallel for)", "label": "(19, OpenMP, int numThreads = omp_get_max_threads();); (25, OpenMP, #pragma omp parallel for); (35, OpenMP, int threadId = omp_get_thread_num();)"}
{"predict": "(56, MPI, MPI_Finalize();); (22, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (30, MPI, MPI_Bcast(&params, 1, *params_type, root, MPI_COMM_WORLD);); (12, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (10, MPI, MPI_Init_thread(NULL, NULL, MPI_THREAD_FUNNELED, &thread_level);)", "label": "(56, MPI, MPI_Finalize();); (22, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (29, MPI, MPI_Bcast(&params, 1, *params_type, root, MPI_COMM_WORLD);); (14, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (10, MPI, MPI_Init_thread(NULL, NULL, MPI_THREAD_FUNNELED, &thread_level);)"}
{"predict": "(25, MPI, MPI_Bcast( &params.load_imbalance, 1, MPI_FLOAT, 0, MPI_COMM_WORLD );); (25, MPI, MPI_Bcast( &params.nx, 1, MPI_INT, 0, MPI_COMM_WORLD );); (26, MPI, MPI_Bcast( &params.numa, 1, MPI_INT, 0, MPI_COMM_WORLD );); (24, MPI, MPI_Bcast( &params.mv_overlap_comm_comp, 1, MPI_INT, 0, MPI_COMM_WORLD );); (25, MPI, MPI_Bcast( &params.use_locking, 1, MPI_INT, 0, MPI_COMM_WORLD );); (22, MPI, MPI_Bcast( &params.numthreads, 1, MPI_INT, 0, MPI_COMM_WORLD );); (23, MPI, MPI_Bcast( &params(use_elem_mat_fields), 1, MPI_INT, 0, MPI_COMM_WORLD );); (25, MPI, MPI_Bcast( &params.device, 1, MPI_INT, 0, MPI_COMM_WORLD );); (23, MPI, MPI_Bcast( &params.num_devices, 1, MPI_INT, 0, MPI_COMM_WORLD );); (19, MPI, MPI_Bcast( &params.nz, 1, MPI_INT, 0, MPI_COMM_WORLD );); (20, MPI, MPI_Bcast( &params.elemsize, 1, MPI_INT, 0, MPI_COMM_WORLD );); (21, MPI, MPI_Bcast( &params.use_elem_group_size, 1, MPI_INT, 0, MPI_COMM_WORLD );); (22, MPI, MPI_Bcast( &params.enable_verif, 1, MPI_INT, 0, MPI_COMM_WORLD );)", "label": "(23, MPI, MPI_Bcast(&fparams[0], 1, MPI_FLOAT, 0, MPI_COMM_WORLD);); (6, MPI, MPI_Bcast(&iparams[0], num_int_params, MPI_INT, 0, MPI_COMM_WORLD);)"}
{"predict": "(3, MPI, MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &mpi_threads_provided);); (35, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (1, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (28, MPI, MPI_Recv(&shut_down, 1, MPI_BOOL, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)", "label": "(35, MPI, MPI_Finalize();); (2, MPI, MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &mpi_threads_provided);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);)"}
{"predict": "(16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (143, MPI, MPI_Allreduce(&starting_pos, &pos, 1, MPI_UINT64_T, MPI_MAX, MPI_COMM_WORLD);); (139, MPI, MPI_Allreduce(&max_offset, &pos, 1, MPI_UINT64_T, MPI_MAX, MPI_COMM_WORLD);)", "label": "(141, MPI, MPI_Exscan(&max_offset, &starting_pos, 1, MPI_UINT64_T, MPI_MAX, MPI_COMM_WORLD);); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (258, MPI, MPI_Allreduce(&pos, &max_pos, 1, MPI_UINT64_T, MPI_MAX, MPI_COMM_WORLD);); (13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(57, CUDA, cudaEventRecord(end, s);); (41, CUDA, cudaEventDestroy(end);); (40, CUDA, cudaEventDestroy(begin);); (5, CUDA, cudaStreamCreate(&s);); (5, CUDA, cudaEventCreate(&begin);); (63, CUDA, cudaEventSynchronize(end);); (1, CUDA, cudaEventCreate(&end);); (24, CUDA, cudaEventRecord(begin, s);); (24, CUDA, cudaEventElapsedTime(&time, begin, end);); (38, CUDA, cudaStreamDestroy(s);)", "label": "(72, CUDA, cudaStreamDestroy(s);); (5, CUDA, cudaStreamCreate(&s);); (31, CUDA, cudaEventRecord(end, s);); (31, CUDA, cudaEventSynchronize(end);); (23, CUDA, cudaEventRecord(begin, s);); (66, CUDA, cudaEventDestroy(end);); (3, CUDA, cudaEventCreate(&end);); (63, CUDA, cudaEventDestroy(begin);); (0, CUDA, cudaEventCreate(&begin);); (27, CUDA, cudaEventElapsedTime(&time, begin, end);)"}
{"predict": "(11, MPI, dray::dray::mpi_comm(MPI_Comm_c2f(comm));)", "label": "(11, MPI, ::dray::dray::mpi_comm(MPI_Comm_c2f(comm));)"}
{"predict": "(11, OpenMP, #pragma omp target parallel defaultmap(tofrom, scalar); (29, OpenMP, #pragma omp target defaultmap(none:aggregate)); (47, OpenMP, #pragma omp target map(g) defaultmap(none:pointer)); (54, OpenMP, #pragma omp target update from(g) defaultmap(none:aggregate)); (22, OpenMP, #pragma omp target parallel defaultmap(none:aggregate) defaultmap(none:scalar) map(argc)); (10, OpenMP, #pragma omp target parallel defaultmap(tofrom:scalar) defaultmap(tofrom:scalar)); (31, OpenMP, #pragma omp target defaultmap(none:aggregate) defaultmap(none:scalar) map(g)); (13, OpenMP, #pragma omp target parallel defaultmap(tofrom scalar)); (36, OpenMP, #pragma omp target defaultmap(none:aggregate) map(bazPtr)); (44, OpenMP, #pragma omp target update to(g) defaultmap(none:aggregate)); (18, OpenMP, #pragma omp target parallel defaultmap(none:aggregate)); (-3, OpenMP, #pragma omp target parallel defaultmap(tofrom); (11, OpenMP, #pragma omp target parallel defaultmap(tofrom,)", "label": "(12, OpenMP, #pragma omp target parallel defaultmap(tofrom,); (1, OpenMP, #pragma omp target parallel defaultmap(); (14, OpenMP, #pragma omp target parallel defaultmap(tofrom, scalar); (24, OpenMP, #pragma omp target parallel defaultmap(none:pointer) defaultmap(none:scalar) map(g)); (35, OpenMP, #pragma omp target defaultmap(none:aggregate)); (26, OpenMP, #pragma omp target parallel defaultmap(none:aggregate) defaultmap(none:scalar) map(argc)); (8, OpenMP, #pragma omp target parallel defaultmap (scalar:); (47, OpenMP, #pragma omp target defaultmap(none:pointer)); (32, OpenMP, #pragma omp parallel); (14, OpenMP, #pragma omp target parallel defaultmap(none:pointer)); (0, OpenMP, #pragma omp target parallel defaultmap(tofrom)); (-7, OpenMP, #pragma omp target parallel defaultmap()); (7, OpenMP, #pragma omp target parallel defaultmap(tofrom:scalar) defaultmap(to:scalar)); (8, OpenMP, #pragma omp target parallel defaultmap(alloc:pointer) defaultmap(to:scalar) defaultmap(firstprivate:pointer)); (15, OpenMP, #pragma omp target parallel defaultmap(none:pointer) defaultmap(none:scalar) defaultmap(none:aggregate) map(g) firstprivate(argc)); (-7, OpenMP, #pragma omp target parallel defaultmap (tofrom:); (9, OpenMP, #pragma omp target parallel defaultmap(none:pointer) defaultmap(none:scalar) map(argc)); (-11, OpenMP, #pragma omp target parallel defaultmap(tofrom)"}
{"predict": "(57, MPI, MPI_Allreduce(&my_bytes, &max_bytes, 1, MPI_UNSIGNED_LONG, MPI_MAX, comm);)", "label": "(57, MPI, MPI_Allreduce(&my_bytes, &max_bytes, 1, MPI_UNSIGNED_LONG, MPI_MAX, comm);)"}
{"predict": "(18, CUDA, __shared__ typename BlockReduceT::TempStorage temp_storage;)", "label": "(15, CUDA, __shared__ typename BlockReduceT::TempStorage temp_storage;)"}
{"predict": "(7, OpenMP, int tid = omp_get_thread_num();)", "label": "(7, OpenMP, int tid = omp_get_thread_num();)"}
{"predict": "(14, OpenMP, return comp_by_key(lv, rv, key, by_number, ascendingly, casesensitively);)", "label": "(14, OpenMP, return comp_raw(lv, rv, by_number, ascendingly, casesensitively);)"}
{"predict": "(15, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (13, CUDA, cudaMalloc((void**)(&d_in1), in1_bytes);); (16, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (30, CUDA, assert(cudaMemcpy(out.data(), d_out, out_bytes, cudaMemcpyDeviceToHost) == cudaSuccess);); (45, CUDA, cudaStreamSynchronize(context.device().stream());); (14, CUDA, cudaMemcpy(d_in1, in1.data(), in1_bytes, cudaMemcpyHostToDevice);); (40, CUDA, cudaFree(d_in1);); (41, CUDA, cudaFree(d_out);); (11, CUDA, cudaMemcpy(d_in2, in2.data(), in2_bytes, cudaMemcpyHostToDevice);); (7, CUDA, cudaMalloc((void**)(&d_in2), in2_bytes);); (39, CUDA, cudaFree(d_in2);)", "label": "(27, CUDA, assert(cudaMemcpy(out.data(), d_out, out_bytes, cudaMemcpyDeviceToHost) == cudaSuccess);); (15, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (12, CUDA, cudaMalloc((void**)(&d_in1), in1_bytes);); (69, CUDA, assert(cudaMemcpyAsync(out.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, context.device().stream()) == cudaSuccess);); (14, CUDA, cudaMemcpy(d_in1, in1.data(), in1_bytes, cudaMemcpyHostToDevice);); (10, CUDA, cudaMalloc((void**)(&d_in2), in2_bytes);); (67, CUDA, assert(cudaStreamSynchronize(context.device().stream()) == cudaSuccess);); (12, CUDA, cudaMemcpy(d_in2, in2.data(), in2_bytes, cudaMemcpyHostToDevice);)"}
{"predict": "(11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (17, MPI, MPI_Barrier(MPI_COMM_WORLD);); (5, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (19, MPI, MPI_Barrier(MPI_COMM_WORLD);); (13, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (4, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(6, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)", "label": "(6, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(12, CUDA, __shared__ thrust::cuda_cub::core::uninitialized_array<T, 1024> tmp;)", "label": "(15, CUDA, __shared__ thrust::cuda_cub::core::uninitialized_array<T, 1024> tmp;)"}
{"predict": "(65, CUDA, cudaFree( outGPU );); (63, CUDA, cudaEventDestroy( evStop );); (61, CUDA, cudaEventDestroy( evStart );); (62, CUDA, cudaFree( inGPU );)", "label": "(65, CUDA, cudaFree( outGPU );); (63, CUDA, cudaEventDestroy( evStop );); (64, CUDA, cudaFree( inGPU );); (60, CUDA, cudaEventDestroy( evStart );)"}
{"predict": "(2, CUDA, __shared__ float maxScore[FMC2H];); (2, CUDA, __shared__ float maxScor2[FMC2H];); (2, CUDA, __shared__ int maxIndex[FMC2H];)", "label": "(3, CUDA, __shared__ float maxScore[FMC2H];); (1, CUDA, __shared__ float siftPoint[128];); (2, CUDA, __shared__ float maxScor2[FMC2H];); (2, CUDA, __shared__ int maxIndex[FMC2H];)"}
{"predict": "(174, OpenMP, #pragma omp simd); (112, OpenMP, #pragma\tomp\ttaskloop private (lptr1,luptr1,nlb1,thread_id1,lsub1,lusup1,nsupr1,Linv,nn,lbstart,lbend,luptr_tmp1,nbrow,lb,lptr1_tmp,rtemp_loc,nbrow_ref,lptr,nbrow1,ik,rel,lk,iknsupc,il,i,irow,fmod_tmp,ikcol,p,ii,jj,t1,t2,j) untied); (229, OpenMP, #pragma omp simd lastprivate(irow))", "label": "(174, OpenMP, #pragma omp simd); (112, OpenMP, #pragma\tomp\ttaskloop private (lptr1,luptr1,nlb1,thread_id1,lsub1,lusup1,nsupr1,Linv,nn,lbstart,lbend,luptr_tmp1,nbrow,lb,lptr1_tmp,rtemp_loc,nbrow_ref,lptr,nbrow1,ik,rel,lk,iknsupc,il,i,irow,fmod_tmp,ikcol,p,ii,jj,t1,t2,j) untied); (229, OpenMP, #pragma omp simd lastprivate(irow))"}
{"predict": "(10, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (11, MPI, rc = MPI_Comm_connect(port_name, MPI_INFO_NULL, 0, MPI_COMM_WORLD, &comm);); (12, MPI, MPI_Error_class(rc, &errclass);); (9, MPI, MPI_Comm_disconnect(&comm);)", "label": "(12, MPI, rc = MPI_Unpublish_name(serv_name, MPI_INFO_NULL, port_name);); (12, MPI, MPI_Error_class(rc, &errclass);); (8, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(11, OpenMP, #pragma omp target enter data map(alloc : a [0:array_size], b [0:array_size])); (3, OpenMP, #pragma omp target map(tofrom : warp_size)); (18, OpenMP, #pragma omp target teams distribute parallel for); (3, OpenMP, #pragma omp target map(tofrom : a [0:array_size], b [0:array_size]))", "label": "(11, OpenMP, #pragma omp target enter data map(alloc : a [0:array_size], b [0:array_size])); (18, OpenMP, #pragma omp target teams distribute parallel for); (3, OpenMP, #pragma omp target map(tofrom : warp_size)); (98, OpenMP, #pragma omp target exit data map(release : a [0:array_size], b [0:array_size]))"}
{"predict": "(18, CUDA, thrust::device_vector<float> dev_B(Brow*Bcol);); (12, CUDA, thrust::host_vector<float>       A(Arow*Acol);); (16, CUDA, thrust::device_vector<float> dev_C(Crow*Ccol);); (32, CUDA, cudaDeviceSynchronize();); (10, CUDA, thrust::host_vector<float>       B(Brow*Bcol);); (10, CUDA, thrust::host_vector<float>       C(Crow*Ccol);); (12, CUDA, thrust::device_vector<float> dev_A(Arow*Acol);)", "label": "(18, CUDA, thrust::device_vector<float> dev_B(Brow*Bcol);); (12, CUDA, thrust::host_vector<float>       A(Arow*Acol);); (14, CUDA, thrust::device_vector<float> dev_C(Crow*Ccol);); (32, CUDA, cudaDeviceSynchronize();); (10, CUDA, thrust::host_vector<float>       B(Brow*Bcol);); (10, CUDA, thrust::host_vector<float>       C(Crow*Ccol);); (11, CUDA, thrust::device_vector<float> dev_A(Arow*Acol);)"}
{"predict": "(7, MPI, MPI_Comm_rank(comm, &comm_rank);); (8, MPI, MPI_Comm_size(comm, &comm_size);)", "label": "(8, MPI, MPI_Comm_rank(comm, &comm_rank);); (9, MPI, MPI_Comm_size(comm, &comm_size);)"}
{"predict": "(14, MPI, MPI_Init(&argc, &argv);); (18, MPI, MPI_Finalize();); (13, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (55, MPI, err = MPI_Type_commit(&subType);); (55, MPI, err = MPI_Type_free(&subType);); (121, MPI, MPI_Barrier(MPI_COMM_WORLD);); (55, MPI, err = MPI_Type_commit(&filetype);); (55, MPI, err = MPI_Type_commit(&buftype);); (105, MPI, err = MPI_File_set_view(fh, 0, MPI_BYTE, filetype, \"native\", MPI_INFO_NULL);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (120, MPI, MPI_Iwrite(buf, buf_len, MPI_BYTE, &req);); (150, MPI, err = MPI_File_read_all(fh, buf, buf_len, MPI_BYTE, &status);); (112, MPI, MPI_Wait(&req, &status);); (120, MPI, err = MPI_File_seek(fh, 0, MPI_SEEK_SET);); (150, MPI, err = MPI_File_close(&fh);); (110, MPI, err = MPI_File_write_all(fh, buf, buf_len, MPI_BYTE, &status);); (39, MPI, MPI_Type_create_subarray(2, gsize, count, start, MPI_ORDER_C, MPI_BYTE, &subType);); (127, MPI, err = MPI_File_seek(fh, 0, MPI_SEEK_SET);); (21, MPI, MPI_Dims_create(nprocs, 2, psize);); (61, MPI, MPI_Type_free(&filetype);); (41, MPI, err = MPI_Type_commit(&filetype);); (59, MPI, err = MPI_Type_free(&buftype);); (127, MPI, err = MPI_File_write_all(fh, buf, buf_len, MPI_BYTE, &status);); (57, MPI, err = MPI_Type_commit(&filetype);); (36, MPI, err = MPI_Type_commit(&subType);); (108, MPI, err = MPI_File_read_all(fh, buf, buf_len, MPI_BYTE, &status);); (61, MPI, MPI_Type_create_struct(NVARS, array_of_blocklengths, array_of_displacements, array_of_types, &filetype);); (2, MPI", "label": "(14, MPI, MPI_Init(&argc, &argv);); (130, MPI, err = MPI_File_iwrite_all(fh, buf, 1, buftype, &req);); (100, MPI, err = MPI_Type_commit(&buftype);); (56, MPI, err = MPI_Type_create_subarray(2, gsize, count, start, MPI_ORDER_C, MPI_BYTE, &subType);); (57, MPI, err = MPI_Type_commit(&subType);); (68, MPI, err = MPI_Type_commit(&filetype);); (69, MPI, err = MPI_Type_free(&subType);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (115, MPI, err = MPI_File_write_all(fh, buf, 1, buftype, &status);); (125, MPI, err = MPI_File_read_all(fh, buf, 1, buftype, &status);); (129, MPI, err = MPI_File_iread_all(fh, buf, 1, buftype, &req);); (121, MPI, MPI_Wait(&req, &status);); (8, MPI, MPI_Finalize();); (106, MPI, err = MPI_File_set_view(fh, 0, MPI_BYTE, filetype, \"native\", MPI_INFO_NULL);); (134, MPI, err = MPI_Type_free(&buftype);); (131, MPI, err = MPI_Type_free(&filetype);); (127, MPI, MPI_File_close(&fh);); (10, MPI, err = MPI_Dims_create(nprocs, 2, psize);); (109, MPI, err = MPI_File_seek(fh, 0, MPI_SEEK_SET);)"}
{"predict": "(4, OpenMP, #pragma omp sections); (5, OpenMP, #pragma omp section)", "label": "(5, OpenMP, #pragma omp sections); (3, OpenMP, #pragma omp parallel); (5, OpenMP, #pragma omp section)"}
{"predict": "(44, OpenMP, #pragma omp target); (46, OpenMP, #pragma omp target firstprivate(val))", "label": "(42, OpenMP, #pragma omp target); (42, OpenMP, #pragma omp target firstprivate(val))"}
{"predict": "(24, MPI, err = MPI_Allreduce(MPI_IN_PLACE, ap, count, MPI_CHAR, MPI_SUM, MPI_COMM_WORLD);); (8, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (17, MPI, MPI_Info_create(&hints);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (40, MPI, MPI_Info_free(&hints);); (17, MPI, MPI_Info_set(hints, hintstrs[i], count);); (22, MPI, MPI_Error_class(err, &errclass);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &world_size);); (12, MPI, err = MPI_Alloc_mem(count, MPI_INFO_NULL, &ap);); (34, MPI, MPI_Free_mem(ap);)", "label": "(23, MPI, MPI_Error_class(err, &errclass);); (36, MPI, MPI_Free_mem(ap);); (16, MPI, MPI_Info_set(hints, \"mpich_buf_type\", hintstrs[i]);); (10, MPI, MPI_Info_set(hints, \"mpi_minimum_memory_alignment\", \"4096\");); (8, MPI, MPI_Info_create(&hints);); (20, MPI, err = MPI_Alloc_mem(count, MPI_INFO_NULL, &ap);); (3, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (35, MPI, MPI_Info_free(&hints);); (12, MPI, err = MPI_Alloc_mem(count, hints, &ap);)"}
{"predict": "(11, CUDA, cuda_safe_call(cudaGetDeviceCount(&ndevs));)", "label": "(24, CUDA, cuda_safe_call(cudaStreamSynchronize(stream));); (10, CUDA, cuda_safe_call(cudaGetDeviceCount(&ndevs));)"}
{"predict": "(2, CUDA, __shared__ float s[Ny][Nx];)", "label": "(2, CUDA, __shared__ float s[Ny][Nx];)"}
{"predict": "(79, OpenMP, #pragma omp distribute parallel for private(S2::S2sc)); (69, OpenMP, #pragma omp distribute parallel for private(ba)); (50, OpenMP, #pragma omp distribute parallel for private (argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (39, OpenMP, #pragma omp distribute parallel for private (argc, z)); (100, OpenMP, #pragma omp distribute parallel for private(e, g)); (121, OpenMP, #pragma omp distribute parallel for private(k), private(k)); (130, OpenMP, #pragma omp distribute parallel for private(p), private(p)); (31, OpenMP, #pragma omp distribute parallel for private (argc > 0? argv[1] : argv[2])); (76, OpenMP, #pragma omp distribute parallel for private(h)); (105, OpenMP, #pragma omp distribute parallel for private(i)); (120, OpenMP, #pragma omp distribute parallel for private(r)); (130, OpenMP, #pragma omp distribute parallel for private(fl)); (84, OpenMP, #pragma omp distribute parallel for private(m)); (49, OpenMP, #pragma omp distribute parallel for private (argv[1])); (58, OpenMP, #pragma omp distribute parallel for private(ca)); (35, OpenMP, #pragma omp distribute parallel for private (a, b, c, d, f)); (16, OpenMP, #pragma omp distribute parallel for private (argc); (25, OpenMP, #pragma omp distribute parallel for private (argc) allocate(omp_thread_mem_alloc: argc)); (18, OpenMP, #pragma omp distribute parallel for private (argc,); (45, OpenMP, #pragma omp distribute parallel for private (S1)); (32, OpenMP, #pragma omp distribute parallel for private (argc) allocate(omp_default_mem_alloc: argc)); (129, OpenMP, #pragma omp distribute parallel for private(fl)); (2, OpenMP, #pragma omp distribute parallel for private (); (57, OpenMP, #pragma omp distribute parallel for private(da)); (72, OpenMP, #pragma omp distribute parallel for private(j)); (23, OpenMP, #pragma omp distribute parallel for private (argc) allocate(omp_default_mem_alloc: argc));", "label": "(109, OpenMP, #pragma omp distribute parallel for simd reduction(^ : fl)); (118, OpenMP, #pragma omp distribute parallel for simd reduction(&& : S2::S2sc)); (132, OpenMP, #pragma omp distribute parallel for simd reduction(+ : z, o)); (21, OpenMP, #pragma omp distribute parallel for simd reduction(); (45, OpenMP, #pragma omp distribute parallel for simd reduction(foo : argc); (140, OpenMP, #pragma omp distribute parallel for simd reduction(+ : p), reduction(+ : p)); (6, OpenMP, #pragma omp target); (87, OpenMP, #pragma omp distribute parallel for simd reduction(+ : ba)); (131, OpenMP, #pragma omp distribute parallel for simd private(i), reduction(+ : j), reduction(+ : q)); (146, OpenMP, #pragma omp distribute parallel for simd reduction(+ : r)); (161, OpenMP, #pragma omp parallel reduction(* : fl)); (151, OpenMP, #pragma omp distribute parallel for simd reduction(max : j)); (112, OpenMP, #pragma omp distribute parallel for simd reduction(& : e, g)); (172, OpenMP, #pragma omp distribute parallel for simd reduction(task, + : m)); (166, OpenMP, #pragma omp distribute parallel for simd reduction(+ : m)); (127, OpenMP, #pragma omp parallel private(k)); (63, OpenMP, #pragma omp distribute parallel for simd reduction(+ : a, b, c, d, f)); (67, OpenMP, #pragma omp distribute parallel for simd reduction(min : a, b, c, d, f)); (71, OpenMP, #pragma omp distribute parallel for simd reduction(max : h.b)); (55, OpenMP, #pragma omp distribute parallel for simd reduction(^ : S1)); (109, OpenMP, #pragma omp distribute parallel for simd reduction(+ : h, k, B::x)); (48, OpenMP, #pragma omp distribute parallel for simd reduction(&& : argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (17, OpenMP, #pragma omp distribute parallel for simd reduction(*)); (136, OpenMP, #pragma omp parallel reduction(min : i)); (20, OpenMP, #pragma omp distribute parallel for simd reduction(\\)); (140, OpenMP, #pragma omp parallel private(fl)); (8, OpenMP, #pragma omp distribute parallel for simd reduction()); (72, OpenMP, #pragma omp distribute parallel for simd reduction(* : ca)); (140, OpenMP, #pragma omp distribute parallel for simd reduction(+ : fl)); (30, OpenMP, #pragma omp distribute parallel for simd reduction(|| : argc > 0? argv[1] : argv[2])); (-1, OpenMP, #pragma omp distribute parallel for simd reduction(-); (73, OpenMP, #pragma omp distribute parallel for simd reduction(- : da)); (-13, OpenMP, #pragma omp distribute parallel for simd reduction +); (-19, OpenMP, #pragma omp distribute parallel for simd reduction); (20, OpenMP, #pragma omp distribute parallel for simd reduction(| : argc,); (123, OpenMP, #pragma omp parallel shared(i)); (78, OpenMP, #pragma omp distribute parallel for simd reduction(&& : S2::S2s)); (27, OpenMP, #pragma omp distribute parallel for simd reduction(~ : argc)); (-25, OpenMP, #pragma omp teams)"}
{"predict": "(13, OpenCL, clReleaseMemObject(bufParam);); (8, OpenCL, clReleaseMemObject(bufY);); (3, OpenCL, clReleaseMemObject(bufX);)", "label": "(5, OpenCL, clReleaseMemObject(bufX);); (8, OpenCL, clReleaseMemObject(bufY);); (11, OpenCL, clReleaseMemObject(bufParam);)"}
{"predict": "(19, MPI, MPI_Cart_create(MPI_COMM_WORLD, 1, dims, periods, 0, &newcomm);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (49, MPI, MPI_Comm_free(&newcomm);); (32, MPI, MPI_Cart_coords(newcomm, 0, 0, NULL);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (17, MPI, MPI_Comm_rank(newcomm, &newrank);); (10, MPI, MPI_Comm_set_name(newcomm, \"Unwanted newrank\");); (23, MPI, MPI_Barrier(MPI_COMM_WORLD);); (8, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (15, MPI, MPI_Cart_map(MPI_COMM_WORLD, 1, dims, periods, &newrank);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (29, MPI, MPI_Cart_map(MPI_COMM_WORLD, 0, dims, periods, &newrank);)"}
{"predict": "(14, OpenCL, auto status = clWaitForEvents(1, &event);); (25, OpenCL, status = clReleaseEvent(event);)", "label": "(26, OpenCL, status = clReleaseEvent(event);); (13, OpenCL, auto status = clWaitForEvents(1, &event);)"}
{"predict": "(3, TBB, typedef tbb::combinable<T> Container;); (37, TBB, tbb::combinable<T> copy_constructed;); (47, TBB, tbb::combinable<T> moved2;); (44, TBB, tbb::combinable<T> moved1(std::move(sums));); (17, TBB, if (Verbose && t == 0) t0 = tbb::tick_count::now();); (-1, TBB, tbb::task_scheduler_init init(tbb::task_scheduler_init::deferred);); (39, TBB, tbb::combinable<T> assigned;); (22, TBB, tbb::combinable<T> sums;); (13, TBB, tbb::tick_count t0;)", "label": "(41, TBB, tbb::combinable<T> copy_constructed(sums);); (31, TBB, tbb::combinable<T> finit_combinable(my_finit_decl);); (31, TBB, tbb::parallel_for( tbb::blocked_range<int>( 0, N, 10000 ), ParallelScalarBodyNoInit<T>( finit_combinable ) );); (19, TBB, if (Verbose && t == 0) t0 = tbb::tick_count::now();); (22, TBB, tbb::parallel_for( tbb::blocked_range<int>( 0, N, 10000 ), ParallelScalarBody<T>( sums ) );); (40, TBB, tbb::combinable<T> assigned;); (-3, TBB, tbb::task_scheduler_init init(tbb::task_scheduler_init::deferred);); (44, TBB, tbb::combinable<T> moved1(std::move(sums));); (2, TBB, tbb::tick_count t0;); (16, TBB, tbb::combinable<T> sums;); (45, TBB, tbb::combinable<T> moved2;)"}
{"predict": "(93, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 1, NULL, &threadSize, &groupSize, 0, NULL, NULL );); (80, OpenCL, error = clSetKernelArg( kernel, 0, sizeof( streams[0] ), &streams[0] );); (81, OpenCL, error = clSetKernelArg( kernel, 1, sizeof( streams[1] ), &streams[1] );); (82, OpenCL, error = clSetKernelArg( kernel, 2, sizeof( streams[2] ), &streams[2] );)", "label": "(119, OpenCL, clReleaseMemObject( streams[2] );); (80, OpenCL, error = clSetKernelArg(kernel, 0, sizeof( streams[0] ), &streams[0] );); (119, OpenCL, clReleaseProgram( program );); (82, OpenCL, error = clSetKernelArg(kernel, 2, sizeof( streams[2] ), &streams[2] );); (114, OpenCL, clReleaseMemObject( streams[1] );); (115, OpenCL, clReleaseKernel( kernel );); (77, OpenCL, error = clSetKernelArg(kernel, 1, sizeof( streams[1] ), &streams[1] );); (92, OpenCL, error = clEnqueueReadBuffer( queue, streams[2], CL_TRUE, 0, outStride * count, outData, 0, NULL, NULL );); (86, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 1, NULL, &threadSize, &groupSize, 0, NULL, NULL );); (108, OpenCL, clReleaseMemObject( streams[0] );)"}
{"predict": "(27, MPI, MPI_Wait(&req, &status);); (12, MPI, MPI_Bcast_init(&a, 1, MPI_INT, 0, MPI_COMM_WORLD, MPI_INFO_NULL, &req);); (15, MPI, MPI_Test(&req, &flag, &status);); (25, MPI, MPI_Request_free(&req);)", "label": "(25, MPI, MPI_Wait(&req, &status);); (12, MPI, MPI_Bcast_init(&a, 1, MPI_INT, 0, MPI_COMM_WORLD, MPI_INFO_NULL, &req);); (15, MPI, MPI_Test(&req, &flag, &status);); (25, MPI, MPI_Request_free(&req);)"}
{"predict": "(10, OpenMP, #pragma omp parallel for); (32, OpenMP, #pragma omp atomic)", "label": "(10, OpenMP, #pragma omp parallel for); (32, OpenMP, #pragma omp atomic)"}
{"predict": "(25, OpenCL, clReleaseEvent(retEvent);); (25, OpenCL, clReleaseEvent(retEvent2);)", "label": "(25, OpenCL, clReleaseEvent(retEvent2);); (25, OpenCL, clReleaseEvent(retEvent);)"}
{"predict": "(7, MPI, MPI_Comm_rank(comm, &rank);); (2, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(3, MPI, MPI_Barrier(MPI_COMM_WORLD);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (16, MPI, MPI_Comm_free(&comm);)"}
{"predict": "(135, MPI, MPI_Reduce_scatter_block(recvbuf_h, buf, COUNT, MPI_INT, MPI_SUM, root, comm);); (104, MPI, MPI_Ireduce_scatter_block(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, root, comm, &req);); (143, MPI, MPI_Iallreduce(MPI_IN_PLACE, recvbuf, COUNT, MPI_INT, MPI_SUM, root, comm, &req);); (135, MPI, MPI_Reduce_scatter_block(buf, recvbuf_h, COUNT, MPI_INT, MPI_SUM, root, comm);); (150, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (119, MPI, MPI_Allreduce(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, root, comm);); (104, MPI, MPI_Ireduce_scatter(recvbuf, buf, recvcounts, MPI_INT, MPI_SUM, root, comm, &req);); (133, MPI, MPI_Reduce_scatter_block(recvbuf_h, buf, COUNT, MPI_INT, MPI_SUM, root, comm);); (138, MPI, MPI_Reduce_scatter_block(buf, recvbuf_h, COUNT, MPI_INT, MPI_SUM, root, comm);); (118, MPI, MPI_Allreduce(MPI_IN_PLACE, recvbuf, COUNT, MPI_INT, MPI_SUM, root, comm);); (141, MPI, MPI_Iallreduce(buf, MPI_IN_PLACE, COUNT, MPI_INT, MPI_SUM, root, comm, &req);); (127, MPI, MPI_Reduce_scatter_block(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, root, comm);); (99, MPI, MPI_Ireduce_scatter_block(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, root, comm, &req);); (127, MPI, MPI_Reduce_scatter_block(buf, recvbuf_h, COUNT, MPI_INT, MPI_SUM, root, comm);); (115, MPI, MPI_Allreduce(MPI_IN_PLACE, buf, COUNT, MPI_INT, MPI_SUM, root, comm);); (100, MPI, MPI_Ireduce_scatter_block(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, root, comm, &req);); (130, MPI, MPI_Reduce_scatter_block(buf, recvbuf_h, COUNT, MPI_INT, MPI_SUM, root, comm);); (109, MPI, MPI_Ireduce_scatter_block(buf, recvbuf", "label": "(169, MPI, MPI_Scan(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (28, MPI, MPI_Ireduce(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD, &req);); (82, MPI, MPI_Reduce_scatter(buf, recvbuf, recvcounts, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (45, MPI, MPI_Allreduce(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (124, MPI, MPI_Reduce_scatter_block(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (59, MPI, MPI_Iallreduce(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD, &req);); (100, MPI, MPI_Ireduce_scatter(buf, recvbuf, recvcounts, MPI_INT, MPI_SUM, MPI_COMM_WORLD, &req);); (23, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (3, MPI, MPI_Reduce(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD);); (187, MPI, MPI_Exscan(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (139, MPI, MPI_Ireduce_scatter_block(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD, &req);); (171, MPI, MPI_Iscan(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD, &req);); (198, MPI, MPI_Iexscan(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD, &req);)"}
{"predict": "(10, MPI, t = MPI_Wtime();); (12, MPI, t = MPI_Wtime() - t;)", "label": "(9, MPI, t = MPI_Wtime();); (11, MPI, t = MPI_Wtime() - t;)"}
{"predict": "(14, OpenMP, #pragma omp target teams distribute parallel for proc_bind(master); (21, OpenMP, #pragma omp target teams distribute parallel for proc_bind(spread)); (0, OpenMP, #pragma omp target teams distribute parallel for proc_bind); (6, OpenMP, #pragma omp target teams distribute parallel for proc_bind()); (11, OpenMP, #pragma omp target teams distribute parallel for proc_bind(close), proc_bind(spread)); (6, OpenMP, #pragma omp target teams distribute parallel for proc_bind(master); (15, OpenMP, #pragma omp target teams distribute parallel for proc_bind(x)); (18, OpenMP, #pragma omp target teams distribute parallel for proc_bind(master)); (13, OpenMP, #pragma omp target teams distribute parallel for proc_bind(close))", "label": "(14, OpenMP, #pragma omp parallel for proc_bind(close), proc_bind(spread)); (10, OpenMP, #pragma omp parallel for proc_bind(master); (19, OpenMP, #pragma omp parallel for proc_bind(master)); (5, OpenMP, #pragma omp parallel for proc_bind()); (-2, OpenMP, #pragma omp parallel for proc_bind); (12, OpenMP, #pragma omp parallel for proc_bind(x)); (19, OpenMP, #pragma omp parallel proc_bind(close)); (19, OpenMP, #pragma omp parallel for proc_bind(spread)); (-3, OpenMP, #pragma omp parallel for proc_bind()"}
{"predict": "(6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &comm_size);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (7, MPI, vtkh::SetMPICommHandle(MPI_Comm_c2f(MPI_COMM_WORLD));)", "label": "(6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &comm_size);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (7, MPI, vtkh::SetMPICommHandle(MPI_Comm_c2f(MPI_COMM_WORLD));)"}
{"predict": "(16, CUDA, CUDASimulation cudaSimulation(model);)", "label": "(16, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(127, OpenMP, #pragma omp distribute simd aligned(x, y, z) aligned(y, z)); (44, OpenMP, #pragma omp distribute simd aligned(x :)); (95, OpenMP, #pragma omp distribute simd aligned(x : 1, y)); (3, OpenMP, #pragma omp distribute simd aligned(); (29, OpenMP, #pragma omp distribute simd aligned(int)); (20, OpenMP, #pragma omp distribute simd aligned()); (125, OpenMP, #pragma omp distribute simd aligned(x, y, z) aligned(z, y, x)); (5, OpenMP, #pragma omp distribute simd aligned(,); (24, OpenMP, #pragma omp distribute simd aligned(0)); (34, OpenMP, #pragma omp distribute simd aligned(x)); (1, OpenMP, #pragma omp distribute simd aligned(, )); (37, OpenMP, #pragma omp distribute simd aligned(x, y)); (46, OpenMP, #pragma omp distribute simd aligned(x : 1)); (51, OpenMP, #pragma omp distribute simd aligned(x : 1, y)); (91, OpenMP, #pragma omp distribute simd aligned(x : 2 * 2)); (69, OpenMP, #pragma omp distribute simd aligned(x : 1, y, 1)); (62, OpenMP, #pragma omp distribute simd aligned(x : 2 * 1)); (-9, OpenMP, #pragma omp target); (0, OpenMP, #pragma omp distribute simd aligned(,); (72, OpenMP, #pragma omp distribute simd aligned(x : 1, y, 0)); (-10, OpenMP, #pragma omp teams)", "label": "(19, OpenMP, #pragma omp distribute simd aligned(, )); (75, OpenMP, #pragma omp distribute simd aligned(x :)); (80, OpenMP, #pragma omp distribute simd aligned(x :, )); (9, OpenMP, #pragma omp distribute simd aligned(,); (-2, OpenMP, #pragma omp target); (82, OpenMP, #pragma omp distribute simd aligned(x : 1)); (52, OpenMP, #pragma omp distribute simd aligned(x, y, z)); (24, OpenMP, #pragma omp distribute simd aligned(int)); (62, OpenMP, #pragma omp distribute simd aligned(z)); (-4, OpenMP, #pragma omp distribute simd aligned(); (40, OpenMP, #pragma omp distribute simd aligned(x, y)); (32, OpenMP, #pragma omp distribute simd aligned(x)); (80, OpenMP, #pragma omp distribute simd aligned(x : 2 * 2)); (12, OpenMP, #pragma omp distribute simd aligned()); (23, OpenMP, #pragma omp distribute simd aligned(0)); (83, OpenMP, #pragma omp distribute simd aligned(x : 1, y)); (88, OpenMP, #pragma omp distribute simd aligned(x : 1, y, z : 1)); (108, OpenMP, #pragma omp distribute simd aligned(x) aligned(z, x)); (116, OpenMP, #pragma omp distribute simd aligned(x, y, z) aligned(y, z)); (-16, OpenMP, #pragma omp teams)"}
{"predict": "(25, OpenMP, #pragma omp target exit data map(from: data)); (12, OpenMP, #pragma omp target enter data map(to: data)); (11, OpenMP, #pragma omp target map(tofrom: data[0:N][0:N]))", "label": "(14, OpenMP, #pragma omp parallel for collapse(2)); (12, OpenMP, #pragma omp target map(tofrom: data))"}
{"predict": "(78, CUDA, CUDA_CHECK(cudaFree(d_C));); (63, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (46, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_B), sizeof(data_type) * B.size()));); (74, CUDA, CUDA_CHECK(cudaFree(d_B));); (45, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_C), sizeof(data_type) * C.size()));); (79, CUDA, CUDA_CHECK(cudaDeviceReset());); (76, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (40, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (68, CUDA, CUDA_CHECK(cudaFree(d_A));); (34, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)", "label": "(78, CUDA, CUDA_CHECK(cudaFree(d_C));); (63, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (46, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_B), sizeof(data_type) * B.size()));); (74, CUDA, CUDA_CHECK(cudaFree(d_B));); (45, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_C), sizeof(data_type) * C.size()));); (79, CUDA, CUDA_CHECK(cudaDeviceReset());); (76, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (40, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (68, CUDA, CUDA_CHECK(cudaFree(d_A));); (34, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(7, OpenCL, err = clGetDeviceInfo(device, CL_DEVICE_IL_VERSION, 0, NULL, &str_size);)", "label": "(7, OpenCL, err = clGetDeviceInfo(device, CL_DEVICE_IL_VERSION, 0, NULL, &str_size);)"}
{"predict": "(10, OpenMP, #pragma omp target teams distribute parallel for map(tofrom: minval)); (10, OpenMP, #pragma omp target teams distribute parallel for map(to: c[:array_size]))", "label": "(10, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:minval) reduction(min:minval))"}
{"predict": "(59, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (50, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (88, MPI, MPI_Comm_split(MPI_COMM_WORLD, isChild, wrank, &intracomm2);); (87, MPI, MPI_Comm_rank(intracomm2, &icrank);); (89, MPI, MPI_Comm_size(intracomm2, &icsize);); (87, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (95, MPI, MPI_Comm_size(MPI_COMM_WORLD, &csize);); (109, MPI, MPI_Comm_split(MPI_COMM_WORLD, (rank < 0), wrank, &intracomm3);); (153, MPI, MPI_Comm_free(&intercomm);); (53, MPI, MPI_Comm_rank(intracomm, &icrank);); (54, MPI, MPI_Comm_size(intracomm, &icsize);); (148, MPI, MPI_Comm_free(&intracomm3);); (148, MPI, MPI_Comm_free(&intracomm2);); (53, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (134, MPI, MPI_Ssend(&errs, 1, MPI_INT, 0, 0, intercomm);); (139, MPI, MPI_Recv(&err, 1, MPI_INT, i, 0, intercomm, MPI_STATUS_IGNORE);); (44, MPI, MPI_Intercomm_create(intracomm, 0, MPI_COMM_WORLD, 0, 123, &intercomm);); (10, MPI, MPI_Comm_get_parent(&parentcomm);); (130, MPI, MPI_Comm_free(&intracomm);); (35, MPI, MPI_Send(&i, 1, MPI_INT, i, 0, intercomm);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (35, MPI, MPI_Recv(&i, 1, MPI_INT, 0, 0, intercomm, &status);); (55, MPI, MPI_Comm_split(MPI_COMM_WORLD, (rank == 0), wrank, &intracomm);)", "label": "(65, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (56, MPI, MPI_Intercomm_merge(intercomm, isChild, &intracomm);); (161, MPI, MPI_Comm_free(&intracomm3);); (95, MPI, MPI_Comm_rank(intracomm2, &icrank);); (12, MPI, MPI_Comm_get_parent(&parentcomm);); (34, MPI, MPI_Send(&i, 1, MPI_INT, i, 0, intercomm);); (141, MPI, MPI_Ssend(&errs, 1, MPI_INT, 0, 1, intercomm);); (56, MPI, MPI_Comm_size(intracomm, &icsize);); (146, MPI, MPI_Recv(&err, 1, MPI_INT, i, 1, intercomm, MPI_STATUS_IGNORE);); (117, MPI, MPI_Intercomm_merge(intercomm, 0, &intracomm3);); (65, MPI, MPI_Comm_remote_size(parentcomm, &psize);); (80, MPI, MPI_Intercomm_merge(intercomm,!isChild, &intracomm2);); (149, MPI, MPI_Comm_free(&intracomm);); (102, MPI, MPI_Comm_remote_size(intercomm, &csize);); (83, MPI, MPI_Comm_size(intracomm2, &icsize);); (49, MPI, MPI_Comm_rank(intracomm, &icrank);); (150, MPI, MPI_Comm_free(&intercomm);); (32, MPI, MPI_Recv(&i, 1, MPI_INT, 0, 0, intercomm, &status);); (114, MPI, MPI_Comm_rank(intracomm3, &icrank);); (10, MPI, MPI_Comm_rank(intercomm, &rank);); (7, MPI, MPI_Comm_remote_size(intercomm, &rsize);); (7, MPI, MPI_Comm_size(intercomm, &size);); (109, MPI, MPI_Comm_size(intracomm3, &icsize);); (139, MPI, MPI_Comm_free(&intracomm2);); (150, MPI, MPI_Finalize();)"}
{"predict": "(109, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrColIndC, sizeof(int) * nnzC);); (244, CUDA, if (d_csrRowPtrC) cudaFree(d_csrRowPtrC);); (219, CUDA, cudaStat1 = cudaDeviceSynchronize();); (102, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrValC, sizeof(float) * nnzC);); (97, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrRowPtrC, sizeof(int) * (m + 1));); (112, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrValA, sizeof(float) * nnzA);); (94, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrColIndA, sizeof(int) * nnzA);); (96, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrValA, sizeof(float) * nnzA);); (102, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrColIndC, sizeof(int) * nnzC);); (99, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrColIndA, sizeof(int) * nnzA);); (73, CUDA, cudaStat1 = cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking);); (231, CUDA, if (d_csrColIndA) cudaFree(d_csrColIndA);); (233, CUDA, if (d_csrColIndC) cudaFree(d_csrColIndC);); (234, CUDA, if (d_csrValC) cudaFree(d_csrValC);); (235, CUDA, if (d_csrValA) cudaFree(d_csrValA);); (231, CUDA, if (d_csrRowPtrA) cudaFree(d_csrRowPtrA);); (232, CUDA, if (d_csrRowPtrC) cudaFree(d_csrRowPtrC);); (75, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrRowPtrA, sizeof(int)*(m + 1));); (229, CUDA, if (d_csrColIndA) cudaFree(d_csrColIndA);); (231, CUDA, if (d_csrValA) cudaFree(d_c", "label": "(106, CUDA, cudaStat1 = cudaMemcpy(d_csrRowPtrA, csrRowPtrA, sizeof(int)*(m + 1), cudaMemcpyHostToDevice);); (234, CUDA, if (d_csrColIndC) cudaFree(d_csrColIndC);); (211, CUDA, cudaStat1 = cudaMemcpy(csrRowPtrC, d_csrRowPtrC, sizeof(int)*(m + 1), cudaMemcpyDeviceToHost);); (99, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrRowPtrC, sizeof(int)*(m + 1));); (94, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrValA, sizeof(float)*nnzA);); (109, CUDA, cudaStat1 = cudaMemcpy(d_csrValA, csrValA, sizeof(float)*nnzA, cudaMemcpyHostToDevice);); (221, CUDA, if (d_csrRowPtrA) cudaFree(d_csrRowPtrA);); (237, CUDA, if (stream) cudaStreamDestroy(stream);); (225, CUDA, if (d_csrRowPtrC) cudaFree(d_csrRowPtrC);); (170, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrValC, sizeof(float) * nnzC);); (227, CUDA, if (d_csrValC) cudaFree(d_csrValC);); (154, CUDA, cudaStat1 = cudaDeviceSynchronize();); (78, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrRowPtrA, sizeof(int)*(m + 1));); (204, CUDA, cudaStat1 = cudaMemcpy(csrColIndC, d_csrColIndC, sizeof(int)*nnzC, cudaMemcpyDeviceToHost);); (80, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrColIndA, sizeof(int)*nnzA);); (41, CUDA, cudaStat1 = cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking);); (94, CUDA, cudaStat1 = cudaMemcpy(d_csrColIndA, csrColIndA, sizeof(int)*nnzA, cudaMemcpyHostToDevice);); (125, CUDA, cudaStat1 = cudaMalloc((void**)&d_work, lworkInBytes);); (211, CUDA, if (d_csrColIndA) cudaFree(d_csrColIndA);); (202, CUDA, cudaStat1 = cudaMemcpy(csrValC, d_csrValC, sizeof(float)*nnzC, cudaMemcpyDeviceToHost);); (155, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrColIndC, sizeof(int) * nnzC);); (210, CUDA, if (d_csrValA) cudaFree(d_csrValA);); (228, CUDA, cudaDeviceReset();)"}
{"predict": "(81, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (81, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)", "label": "(81, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (81, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)"}
{"predict": "(23, SYCL, sycl::free(knode, kcache->context);)", "label": "(26, OpenCL, clReleaseContext(knode->key.context);); (24, OpenCL, clReleaseProgram(kern->program);)"}
{"predict": "(61, MPI, MPI_Send(sbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD);); (58, MPI, t1 = MPI_Wtime();); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (58, MPI, MPI_Recv(rbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (30, MPI, MPI_Alloc_mem(maxlen, MPI_INFO_NULL, &rbuf);); (30, MPI, MPI_Alloc_mem(maxlen, MPI_INFO_NULL, &sbuf);); (56, MPI, t1 = MPI_Wtime() - t1;); (145, MPI, MPI_Recv(rbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (52, MPI, MPI_Send(sbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD);); (56, MPI, MPI_Barrier(MPI_COMM_WORLD);); (27, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (128, MPI, MPI_Send(sbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD);); (0, MPI, MPI_Mrecv(sbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD, &rreq);); (128, MPI, MPI_Recv(rbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (3, MPI, MPI_Comm_set_name(MPI_COMM_WORLD, \" MPI_COMM_WORLD\");); (60, MPI, MPI_Free_mem(rbuf);); (60, MPI, MPI_Free_mem(sbuf);); (128, MPI, MPI_Wait(&rreq, MPI_STATUS_IGNORE);)", "label": "(13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (60, MPI, MPI_Irecv(rbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD, &rreq);); (57, MPI, t1 = MPI_Wtime();); (60, MPI, MPI_Wait(&rreq, MPI_STATUS_IGNORE);); (146, MPI, MPI_Recv(rbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (57, MPI, MPI_Send(sbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (34, MPI, MPI_Barrier(MPI_COMM_WORLD);); (57, MPI, t1 = MPI_Wtime() - t1;); (8, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (24, MPI, MPI_Abort(MPI_COMM_WORLD, 2);)"}
{"predict": "(1, TBB, using namespace tbb::flow;)", "label": "(1, TBB, using namespace tbb::flow;)"}
{"predict": "(21, OpenMP, #pragma omp teams distribute parallel for simd safelen ()); (22, OpenMP, #pragma omp teams distribute parallel for simd safelen (2+2))); (40, OpenMP, #pragma omp teams distribute parallel for simd safelen (S1)); (27, OpenMP, #pragma omp teams distribute parallel for simd safelen (foobool(1) > 0? 1 : 2)); (49, OpenMP, #pragma omp teams distribute parallel for simd safelen (argv[1]=2)); (31, OpenMP, #pragma omp teams distribute parallel for simd safelen (foobool(argc)), safelen (true), safelen (-5)); (8, OpenMP, #pragma omp teams distribute parallel for simd safelen ()); (50, OpenMP, #pragma omp teams distribute parallel for simd safelen(safelen(tmain<int, char, -1, -2>(argc, argv)); (-3, OpenMP, #pragma omp teams distribute parallel for simd safelen); (-7, OpenMP, #pragma omp target); (15, OpenMP, #pragma omp teams distribute parallel for simd safelen (4); (8, OpenMP, #pragma omp teams distribute parallel for simd safelen (2+2))); (35, OpenMP, #pragma omp teams distribute parallel for simd safelen (1))", "label": "(7, OpenMP, #pragma omp teams distribute parallel for simd safelen (); (21, OpenMP, #pragma omp teams distribute parallel for simd safelen (2+2))); (10, OpenMP, #pragma omp teams distribute parallel for simd safelen ()); (54, OpenMP, #pragma omp teams distribute parallel for simd safelen(safelen(tmain<int, char, -1, -2>(argc, argv)); (37, OpenMP, #pragma omp teams distribute parallel for simd safelen (S1)); (12, OpenMP, #pragma omp teams distribute parallel for simd safelen (4); (45, OpenMP, #pragma omp teams distribute parallel for simd safelen (argv[1]=2)); (21, OpenMP, #pragma omp teams distribute parallel for simd safelen (foobool(1) > 0? 1 : 2)); (28, OpenMP, #pragma omp teams distribute parallel for simd safelen (foobool(argc)), safelen (true), safelen (-5)); (-8, OpenMP, #pragma omp target); (-8, OpenMP, #pragma omp teams distribute parallel for simd safelen)"}
{"predict": "(24, MPI, MPI_Finalize();); (12, MPI, MPI_Init(&argc,&argv);)", "label": "(24, MPI, MPI_Finalize();); (12, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(3, TBB, tbb::flow::write_once_node<R> n(g);); (53, TBB, tbb::flow::remove_edge( n, r[i] );); (0, TBB, tbb::flow::graph g;); (17, TBB, tbb::flow::make_edge( n, r[i] );)", "label": "(20, TBB, tbb::flow::make_edge( n, r[i] );); (54, TBB, tbb::flow::remove_edge( n, r[i] );); (1, TBB, tbb::flow::write_once_node<R> n(g);); (-1, TBB, tbb::flow::graph g;)"}
{"predict": "(39, OpenMP, #pragma omp distribute parallel for schedule (dynamic, foobool(1) > 0? 1 : 2 - z)); (46, OpenMP, #pragma omp distribute parallel for schedule (guided, foobool(argc)), schedule (static, true), schedule (dynamic, -5)); (27, OpenMP, #pragma omp distribute parallel for schedule (static, 2+2))); (50, OpenMP, #pragma omp distribute parallel for schedule (static, S1)); (39, OpenMP, #pragma omp distribute parallel for schedule (guided, argv[1]=2)); (53, OpenMP, #pragma omp distribute parallel for schedule(dynamic, schedule(tmain<int, char, -1, -2>(argc, argv)); (20, OpenMP, #pragma omp distribute parallel for schedule (runtime, 3)); (7, OpenMP, #pragma omp distribute parallel for schedule ()); (42, OpenMP, #pragma omp distribute parallel for schedule (guided, foobool(argc) > 0? 1 : 2)); (-5, OpenMP, #pragma omp distribute parallel for schedule); (-6, OpenMP, #pragma omp target); (4, OpenMP, #pragma omp distribute parallel for schedule (auto); (17, OpenMP, #pragma omp distribute parallel for schedule (guided, 4); (-9, OpenMP, #pragma omp teams)", "label": "(16, OpenMP, #pragma omp distribute parallel for schedule (auto); (45, OpenMP, #pragma omp distribute parallel for schedule (guided, foobool(argc)), schedule (static, true), schedule (dynamic, -5)); (48, OpenMP, #pragma omp distribute parallel for schedule (guided, S1)); (52, OpenMP, #pragma omp distribute parallel for schedule (static, argv[1]=2)); (0, OpenMP, #pragma omp distribute parallel for schedule); (31, OpenMP, #pragma omp distribute parallel for schedule (static, 2+2))); (2, OpenMP, #pragma omp distribute parallel for schedule (); (5, OpenMP, #pragma omp distribute parallel for schedule ()); (32, OpenMP, #pragma omp distribute parallel for schedule (dynamic, foobool(1) > 0? 1 : 2 - z)); (23, OpenMP, #pragma omp distribute parallel for schedule (guided, 4); (51, OpenMP, #pragma omp distribute parallel for schedule(dynamic, schedule(tmain<int, char, -1, -2>(argc, argv)); (17, OpenMP, #pragma omp distribute parallel for schedule (runtime, 3)); (-10, OpenMP, #pragma omp target); (11, OpenMP, #pragma omp distribute parallel for schedule (auto,); (6, OpenMP, #pragma omp distribute parallel for schedule (auto_dynamic); (-12, OpenMP, #pragma omp teams)"}
{"predict": "(28, CUDA, cuda_safe_call(cudaStreamSynchronize(stream));)", "label": "(18, CUDA, cuda_safe_call(cudaGetLastError());)"}
{"predict": "(34, CUDA, cudaMalloc((void**)&deviceSkewSymmetricResult, sizeof(bool));); (46, CUDA, cudaMemcpy(hostDiagonalResult, deviceDiagonalResult, sizeof(bool), cudaMemcpyDeviceToHost);); (46, CUDA, cudaMemcpy(hostSymmetricResult, deviceSymmetricResult, sizeof(bool), cudaMemcpyDeviceToHost);); (45, CUDA, cudaMemcpy(hostSkewSymmetricResult, deviceSkewSymmetricResult, sizeof(bool), cudaMemcpyDeviceToHost);); (34, CUDA, cudaMalloc((void**)&deviceIdempotentResult, sizeof(bool));); (31, CUDA, cudaMalloc((void**)&deviceDiagonalResult, sizeof(bool));); (29, CUDA, cudaMalloc((void**)&deviceMatrix, width * height * sizeof(float));); (33, CUDA, cudaMalloc((void**)&deviceSkewSymmetricResult, sizeof(bool));); (44, CUDA, cudaMemcpy(hostIdempotentResult, deviceIdempotentResult, sizeof(bool), cudaMemcpyDeviceToHost);); (26, CUDA, cudaMalloc((void**)&deviceToplitzResult, sizeof(bool));); (43, CUDA, cudaMemcpy(hostSkewSymmetricResult, deviceSkewSymmetricResult, sizeof(bool), cudaMemcpyDeviceToHost);); (24, CUDA, cudaMalloc((void**)&deviceSymmetricResult, sizeof(bool));); (26, CUDA, cudaMalloc((void**)&deviceToplitzResult, sizeof(bool));); (83, CUDA, cudaFree(deviceToplitzResult);); (83, CUDA, cudaFree(deviceDiagonalResult);); (83, CUDA, cudaFree(deviceSymmetricResult);); (20, CUDA, cudaMalloc((void**)&deviceSkewSymmetricResult, sizeof(bool));); (83, CUDA, cudaFree(deviceIdempotentResult);); (83, CUDA, cudaFree(deviceSkewSymmetricResult);); (80, CUDA, cudaFree(deviceMatrix);); (18, CUDA, cudaMalloc((void**)&deviceToplitzResult, sizeof(bool));); (81, CUDA, cudaFree(deviceToplitzResult);); (18, CUDA, cudaMalloc((void**)&deviceToplitzResult, sizeof(bool));); (81, CUDA, cudaFree(deviceSkewSymmetricResult);); (79, CUDA, cudaFree(deviceDiagonalResult);); (79, CUDA, cudaFree(deviceToplitzResult);); (78, CUDA, cudaFree(deviceSymmetricResult);); (17, CUDA, cuda", "label": "(21, CUDA, cudaMalloc((void**)&deviceDiagonalResult, sizeof(bool));); (55, CUDA, cudaMemcpy(hostDiagonalResult, deviceDiagonalResult, sizeof(bool), cudaMemcpyDeviceToHost);); (99, CUDA, cudaFree(deviceSymmetricResult);); (20, CUDA, cudaMalloc((void**)&deviceSkewSymmetricResult, sizeof(bool));); (54, CUDA, cudaMemcpy(hostSkewSymmetricResult, deviceSkewSymmetricResult, sizeof(bool), cudaMemcpyDeviceToHost);); (17, CUDA, cudaMalloc((void**)&deviceSymmetricResult, sizeof(bool));); (33, CUDA, cudaMemcpy(deviceMatrix, hostMatrix, width * height * sizeof(float), cudaMemcpyHostToDevice);); (37, CUDA, cudaMemcpy(deviceIdempotentResult, hostIdempotentResult, sizeof(bool), cudaMemcpyHostToDevice);); (11, CUDA, cudaMalloc((void**)&deviceMatrix, width * height * sizeof(float));); (31, CUDA, cudaMemcpy(deviceToeplitzResult, hostToeplitzResult, sizeof(bool), cudaMemcpyHostToDevice);); (49, CUDA, cudaMemcpy(hostIdempotentResult, deviceIdempotentResult, sizeof(bool), cudaMemcpyDeviceToHost);); (91, CUDA, cudaFree(deviceSkewSymmetricResult);); (31, CUDA, cudaMemcpy(deviceSkewSymmetricResult, hostSkewSymmetricResult, sizeof(bool), cudaMemcpyHostToDevice);); (44, CUDA, cudaMemcpy(hostSymmetricResult, deviceSymmetricResult, sizeof(bool), cudaMemcpyDeviceToHost);); (28, CUDA, cudaMemcpy(deviceSymmetricResult, hostSymmetricResult, sizeof(bool), cudaMemcpyHostToDevice);); (85, CUDA, cudaFree(deviceDiagonalResult);); (4, CUDA, cudaMalloc((void**)&deviceToeplitzResult, sizeof(bool));); (82, CUDA, cudaFree(deviceToeplitzResult);); (85, CUDA, cudaFree(deviceIdempotentResult);); (36, CUDA, cudaMemcpy(hostToeplitzResult, deviceToeplitzResult, sizeof(bool), cudaMemcpyDeviceToHost);); (78, CUDA, cudaFree(deviceMatrix);); (20, CUDA, cudaMemcpy(deviceDiagonalResult, hostDiagonalResult, sizeof(bool), cudaMemcpyHostToDevice);); (2, CUDA, cudaMalloc((void**)&deviceIdempotentResult, sizeof(bool));)"}
{"predict": "(67, writeEvent, 1, NULL, threads, NULL, 0, NULL, &err);); (58, OpenCL, err = clEnqueueWriteBuffer( queue, streams[ii], true, 0, ptrSizes[i]*num_elements, inptr[i], 0, NULL, &writeEvent );); (139, OpenCL, err = clEnqueueNDRangeKernel( queue, kernel[i], 1, NULL, threads, NULL, 0, NULL, NULL );); (133, OpenCL, err |= clSetKernelArg( kernel[i], 0, sizeof( cl_mem ), (void *)&streams[ii] );); (133, OpenCL, err |= clSetKernelArg( kernel[i], 1, sizeof( cl_mem ), (void *)&streams[ii+1] );); (134, OpenCL, err |= clSetKernelArg( kernel[i], 2, sizeof( cl_mem ), (void *)&streams[0] );); (135, OpenCL, err |= clSetKernelArg( kernel[i], 3, sizeof( cl_mem ), (void *)&streams[1] );); (127, OpenCL, err |= clSetKernelArg( kernel[i], 2, sizeof( cl_mem ), (void *)&streams[0] );); (135, OpenCL, err |= clSetKernelArg( kernel[i], 4, sizeof( cl_mem ), (void *)&streams[2] );); (135, OpenCL, err |= clSetKernelArg( kernel[i], 5, sizeof( cl_mem ), (void *)&streams[3] );); (133, OpenCL, err |= clSetKernelArg( kernel[i], 3, sizeof( cl_mem ), (void *)&streams[1] );); (133, OpenCL, err |= clSetKernelArg( kernel[i], 4, sizeof( cl_mem ), (void *)&streams[2] );); (129, OpenCL, err |= clSetKernelArg( kernel[i], 1, sizeof( cl_mem ), (void *)&streams[ii+1] );); (131, OpenCL, err |= clSetKernelArg( kernel[i], 5, sizeof( cl_mem ), (void *)&streams[3] );); (124, OpenCL, err |= clSetKernelArg( kernel[i], 0, sizeof( cl_mem ), (void *)&streams[ii] );); (127, OpenCL, err |= clSet", "label": "(57, OpenCL, err = clEnqueueWriteBuffer( queue, streams[ii], false, 0, ptrSizes[i]*num_elements, inptr[i], 0, NULL, &writeEvent );); (132, OpenCL, err = clSetKernelArg( kernel[i], 0, sizeof( cl_mem ), (void *)&streams[ii] );); (49, OpenCL, clReleaseMemObject(streams[ii]);); (135, OpenCL, clReleaseProgram( program[i] );); (123, OpenCL, clReleaseMemObject(streams[ii+1]);); (55, OpenCL, clReleaseMemObject( streams[ii+1] );); (140, OpenCL, err = clEnqueueNDRangeKernel( queue, kernel[i], 1, NULL, threads, NULL, 0, NULL, NULL );); (52, OpenCL, clReleaseMemObject( streams[ii] );); (64, OpenCL, clReleaseEvent(writeEvent);); (59, OpenCL, err = clWaitForEvents( 1, &writeEvent );); (127, OpenCL, clReleaseKernel( kernel[i] );); (123, OpenCL, err |= clSetKernelArg( kernel[i], 1, sizeof( cl_mem ), (void *)&streams[ii+1] );); (148, OpenCL, err = clEnqueueReadBuffer( queue, streams[ii+1], true, 0, outPtrSizes[i]*num_elements, outptr[i], 0, NULL, NULL );)"}
{"predict": "(39, OpenMP, #pragma omp target teams distribute parallel for simd linear (val(a, b))); (46, OpenMP, #pragma omp target teams distribute parallel for simd linear(h, C::x)); (43, OpenMP, #pragma omp target teams distribute parallel for simd linear(e, g)); (28, OpenMP, #pragma omp target teams distribute parallel for simd linear (val(argc,); (49, OpenMP, #pragma omp target teams distribute parallel for simd linear(i)); (50, OpenMP, #pragma omp target teams distribute parallel for simd linear(j)); (39, OpenMP, #pragma omp target teams distribute parallel for simd linear(val(darr[0:1], darr[2:3])); (26, OpenMP, #pragma omp target teams distribute parallel for simd linear (val(argc)); (11, OpenMP, #pragma omp target teams distribute parallel for simd linear ()); (44, OpenMP, #pragma omp target teams distribute parallel for simd linear(iarr[0:2])); (29, OpenMP, #pragma omp target teams distribute parallel for simd linear (S1)); (16, OpenMP, #pragma omp target teams distribute parallel for simd linear (val); (39, OpenMP, #pragma omp target teams distribute parallel for simd linear(val(darr[:1], darr[:2])); (22, OpenMP, #pragma omp target teams distribute parallel for simd linear (argc)); (13, OpenMP, #pragma omp target teams distribute parallel for simd linear (foo()); (-4, OpenMP, #pragma omp target teams distribute parallel for simd linear); (27, OpenMP, #pragma omp target teams distribute parallel for simd linear (val(a, b, c))); (-6, OpenMP, #pragma omp target teams distribute parallel for simd linear(); (5, OpenMP, #pragma omp target teams distribute parallel for simd linear (val); (32, OpenMP, #pragma omp target teams distribute parallel for simd linear(val(darr[0:2], darr[2:4])); (12, OpenMP, #pragma omp target teams distribute parallel for simd linear (val()); (33, OpenMP, #pragma omp target teams distribute parallel for simd linear(val(darr[2:4], darr[3:5]))", "label": "(15, OpenMP, #pragma omp target teams distribute parallel for simd linear (); (11, OpenMP, #pragma omp target teams distribute parallel for simd linear); (34, OpenMP, #pragma omp target teams distribute parallel for simd linear (S1)); (37, OpenMP, #pragma omp target teams distribute parallel for simd linear (a, b)); (39, OpenMP, #pragma omp target teams distribute parallel for simd linear (argv[1])); (44, OpenMP, #pragma omp target teams distribute parallel for simd linear(h, C::x)); (23, OpenMP, #pragma omp target teams distribute parallel for simd linear (argc > 0? argv[1] : argv[2])); (15, OpenMP, #pragma omp target teams distribute parallel for simd linear (argc); (10, OpenMP, #pragma omp target teams distribute parallel for simd linear ()); (24, OpenMP, #pragma omp target teams distribute parallel for simd linear (argc)); (36, OpenMP, #pragma omp target teams distribute parallel for simd linear(e, g)); (15, OpenMP, #pragma omp target teams distribute parallel for simd linear (argc,)"}
{"predict": "(56, OpenCL, err = clEnqueueWriteBuffer(queue, bufY, CL_TRUE, 0, (lenY*sizeof(cl_float)), Y, 0, NULL, NULL);); (82, OpenCL, clReleaseMemObject(bufX);); (69, OpenCL, err = clWaitForEvents(1, &event);); (33, OpenCL, clReleaseContext(ctx);); (53, OpenCL, err = clEnqueueWriteBuffer(queue, bufX, CL_TRUE, 0, (lenX*sizeof(cl_float)), X, 0, NULL, NULL);); (27, OpenCL, printf( \"clCreateCommandQueue() failed with %d\\n\", err );); (24, OpenCL, queue = clCreateCommandQueue(ctx, device, 0, &err);); (27, OpenCL, clReleaseCommandQueue(queue);); (17, OpenCL, ctx = clCreateContext(props, 1, &device, NULL, NULL, &err);); (45, OpenCL, bufY = clCreateBuffer(ctx, CL_MEM_READ_WRITE, (lenY*sizeof(cl_float)), NULL, &err);); (72, OpenCL, clReleaseEvent(event);); (43, OpenCL, bufX = clCreateBuffer(ctx, CL_MEM_READ_ONLY, (lenX*sizeof(cl_float)), NULL, &err);); (44, OpenCL, err = clEnqueueWriteBuffer(queue, bufX, CL_TRUE, 0, (lenX*sizeof(cl_float)), X, 0, NULL, NULL);); (17, OpenCL, printf( \"clCreateContext() failed with %d\\n\", err );); (53, OpenCL, err = clEnqueueReadBuffer(queue, bufY, CL_TRUE, 0, (lenY*sizeof(cl_float)), Y, 0, NULL, NULL);); (17, OpenCL, clReleaseCommandQueue(queue);)", "label": "(56, OpenCL, err = clEnqueueWriteBuffer(queue, bufY, CL_TRUE, 0, (lenY*sizeof(cl_float)), Y, 0, NULL, NULL);); (82, OpenCL, clReleaseMemObject(bufX);); (44, OpenCL, clReleaseCommandQueue(queue);); (28, OpenCL, printf( \"clCreateContext() failed with %d\\n\", err );); (75, OpenCL, clReleaseEvent(event);); (32, OpenCL, printf( \"clCreateCommandQueue() failed with %d\\n\", err );); (29, OpenCL, queue = clCreateCommandQueue(ctx, device, 0, &err);); (46, OpenCL, bufY = clCreateBuffer(ctx, CL_MEM_READ_WRITE, (lenY*sizeof(cl_float)), NULL, &err);); (30, OpenCL, clReleaseContext(ctx);); (73, OpenCL, clReleaseMemObject(bufY);); (42, OpenCL, bufX = clCreateBuffer(ctx, CL_MEM_READ_ONLY, (lenX*sizeof(cl_float)), NULL, &err);); (44, OpenCL, err = clEnqueueWriteBuffer(queue, bufX, CL_TRUE, 0, (lenX*sizeof(cl_float)), X, 0, NULL, NULL);); (17, OpenCL, ctx = clCreateContext(props, 1, &device, NULL, NULL, &err);); (53, OpenCL, err = clWaitForEvents(1, &event);)"}
{"predict": "(7, CUDA, CUDA_CHECK(cudaStreamCreate(&stream));); (60, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (47, CUDA, CUDA_CHECK(cudaGetLastError());); (47, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));)", "label": "(62, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (6, CUDA, CUDA_CHECK(cudaStreamCreate(&stream));); (46, CUDA, CUDA_CHECK(cudaGetLastError());); (52, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));)"}
{"predict": "(7, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)", "label": "(7, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)"}
{"predict": "(9, CUDA, cudaSetDevice(devs[devi]);)", "label": "(11, OpenMP, #pragma omp parallel for)"}
{"predict": "(9, OpenCL, clReleaseMemObject(objB);); (4, OpenCL, clReleaseMemObject(objA);); (11, OpenCL, clReleaseMemObject(objC);)", "label": "(5, OpenCL, clReleaseMemObject(objA);); (8, OpenCL, clReleaseMemObject(objB);); (11, OpenCL, clReleaseMemObject(objC);)"}
{"predict": "(6, OpenMP, #pragma omp parallel shared(nthreads)); (6, OpenMP, #pragma omp single); (1, OpenMP, omp_set_default_device(0);); (10, OpenMP, #pragma omp parallel shared(nthreads, my_dev))", "label": "(7, OpenMP, #pragma omp parallel); (7, OpenMP, #pragma omp single); (3, OpenMP, my_dev = omp_get_initial_device();); (6, OpenMP, nthreads = omp_get_num_threads();); (10, OpenMP, #pragma omp parallel shared(nthreads))"}
{"predict": "(6, CUDA, __shared__ float sdata2[THREAD_PER_BLOCK];); (4, CUDA, __shared__ float scale;)", "label": "(7, CUDA, __shared__ float scale;); (5, CUDA, __shared__ float sdata2[THREAD_PER_BLOCK];)"}
{"predict": "(67, OpenMP, #pragma omp teams shared(j)); (38, OpenMP, #pragma omp teams shared (argc, z)); (21, OpenMP, #pragma omp teams shared (argc); (4, OpenMP, #pragma omp teams shared); (29, OpenMP, #pragma omp teams shared (a, b, c, d, f)); (16, OpenMP, #pragma omp teams shared (argc,); (51, OpenMP, #pragma omp teams private(i)); (59, OpenMP, #pragma omp teams shared(i)); (4, OpenMP, #pragma omp teams shared (); (19, OpenMP, #pragma omp teams shared (argc > 0? argv[1] : argv[2])); (46, OpenMP, #pragma omp teams firstprivate(i), shared(i)); (50, OpenMP, #pragma omp teams shared(j)); (39, OpenMP, #pragma omp teams shared(h, B::x)); (22, OpenMP, #pragma omp teams shared (argv[1])); (1, OpenMP, #pragma omp teams shared ()); (4, OpenMP, #pragma omp teams shared (argc); (23, OpenMP, #pragma omp teams shared(ba)); (34, OpenMP, #pragma omp teams shared(e, g)); (16, OpenMP, #pragma omp teams shared (S1)); (57, OpenMP, #pragma omp teams shared(i)); (-11, OpenMP, #pragma omp target); (22, OpenMP, #pragma omp teams shared(ca)); (29, OpenMP, #pragma omp teams shared(da)); (-14, OpenMP, #pragma omp teams shared); (2, OpenMP, #pragma omp teams shared (argc,); (11, OpenMP, #pragma omp teams shared (argv[0])); (17, OpenMP, #pragma omp teams shared(da)); (-11, OpenMP, #pragma omp teams shared (argc)", "label": "(8, OpenMP, #pragma omp teams shared); (61, OpenMP, #pragma omp teams shared(i)); (45, OpenMP, #pragma omp teams shared(e, g, z)); (4, OpenMP, #pragma omp target); (13, OpenMP, #pragma omp teams shared (argc); (51, OpenMP, #pragma omp teams firstprivate(i), shared(i)); (17, OpenMP, #pragma omp teams shared (argc > 0? argv[1] : argv[2])); (22, OpenMP, #pragma omp teams shared (S1)); (18, OpenMP, #pragma omp teams shared (argc)); (35, OpenMP, #pragma omp teams shared(da)); (31, OpenMP, #pragma omp teams shared(ca)); (54, OpenMP, #pragma omp teams shared(j)); (23, OpenMP, #pragma omp teams shared (argv[1])); (37, OpenMP, #pragma omp teams shared(h, B::x)); (39, OpenMP, #pragma omp teams private(i)); (-1, OpenMP, #pragma omp teams shared ()); (22, OpenMP, #pragma omp teams shared(ba)); (3, OpenMP, #pragma omp teams shared (argc,); (14, OpenMP, #pragma omp teams shared (a, b, c, d, f)); (-8, OpenMP, #pragma omp teams shared (); (48, OpenMP, #pragma omp teams firstprivate(i))"}
{"predict": "(9, OpenMP, #pragma omp target map(l_val)); (5, OpenMP, #pragma omp parallel); (10, OpenMP, #pragma omp task); (1, OpenMP, const int num_threads = omp_get_max_threads();); (10, OpenMP, #pragma omp atomic)", "label": "(15, OpenMP, #pragma omp target map(l_val)); (9, OpenMP, #pragma omp task); (3, OpenMP, #pragma omp parallel); (4, OpenMP, #pragma omp single)"}
{"predict": "(8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (14, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (41, MPI, mpi_errno = MPI_Comm_free(&stream_comm);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (13, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(58, MPI, MPI_Type_commit(file_realm_types);); (56, MPI, MPI_Type_contiguous_c((max_end_offset - min_st_offset + 1), MPI_BYTE, file_realm_types);)", "label": "(58, MPI, MPI_Type_commit(file_realm_types);); (56, MPI, MPI_Type_contiguous_c((max_end_offset - min_st_offset + 1), MPI_BYTE, file_realm_types);)"}
{"predict": "(25, OpenCL, OCL_ASSERT(clBuildProgram(program, 1, &device, NULL, NULL, NULL) == CL_SUCCESS);)", "label": "(25, OpenCL, OCL_ASSERT(clBuildProgram(program, 1, &device, NULL, NULL, NULL) == CL_SUCCESS);); (26, OpenCL, kernel = clCreateKernel(program, \"compiler_ceil\", &status);)"}
{"predict": "(4, CUDA, CudaCheckError();)", "label": "(4, CUDA, CudaCheckError();); (49, CUDA, dynamics.freeCudaMem();)"}
{"predict": "(18, CUDA, CUDA_CHECK(cudaGetLastError());)", "label": "(18, CUDA, CUDA_CHECK(cudaGetLastError());)"}
{"predict": "(72, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (39, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(inData->basePtr(), inVec.data(), inBufSize, cudaMemcpyHostToDevice));); (52, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (39, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamDestroy(stream));)", "label": "(39, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(inData->basePtr(), inVec.data(), inBufSize, cudaMemcpyHostToDevice));); (71, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (71, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamDestroy(stream));); (51, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));)"}
{"predict": "(8, CUDA, cudaMemGetInfo(&freeCUDAMem, &totalCUDAMem);)", "label": "(5, CUDA, cudaSetDevice(0);); (8, CUDA, cudaMemGetInfo(&freeCUDAMem, &totalCUDAMem);)"}
{"predict": "(48, OpenMP, std::cout << \"Number of threads    = \" << omp_get_max_threads() << std::endl;)", "label": "(48, OpenMP, std::cout << \"Number of threads    = \" << omp_get_max_threads() << std::endl;)"}
{"predict": "(27, MPI, merr = MPI_Publish_name(serv_name, MPI_INFO_NULL, port_name);); (31, MPI, merr = MPI_Unpublish_name(serv_name, MPI_INFO_NULL, port_name);); (66, MPI, MPI_Error_class(merr, &mclass);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (19, MPI, MPI_Barrier(MPI_COMM_WORLD);); (40, MPI, merr = MPI_Lookup_name(serv_name, MPI_INFO_NULL, port_name_out);); (20, MPI, MPI_Error_string(merr, errmsg, &msglen);)", "label": "(35, MPI, merr = MPI_Unpublish_name(serv_name, MPI_INFO_NULL, port_name);); (24, MPI, merr = MPI_Publish_name(serv_name, MPI_INFO_NULL, port_name);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (25, MPI, MPI_Error_string(merr, errmsg, &msglen);); (41, MPI, merr = MPI_Lookup_name(serv_name, MPI_INFO_NULL, port_name_out);); (64, MPI, MPI_Error_class(merr, &mclass);); (16, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (25, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(2, OpenMP, #pragma omp parallel for); (5, OpenMP, #pragma omp task shared(data1, data2))", "label": "(13, OpenMP, __kmpc_omp_task(nullptr, gtid, task1);); (1, OpenMP, #pragma omp parallel for); (18, OpenMP, __kmpc_omp_task(nullptr, gtid, task2);)"}
{"predict": "(3, TBB, tbb::task_scheduler_init init(nthreads);); (54, TBB, tbb::concurrent_bounded_queue<int> iq4;); (22, TBB, tbb::concurrent_bounded_queue<int> iq2;); (15, TBB, tbb::concurrent_bounded_queue<int> iq1;); (34, TBB, tbb::concurrent_bounded_queue<int> iq3;); (73, TBB, tbb::concurrent_bounded_queue<int> iq4;); (52, TBB, tbb::concurrent_bounded_queue<int> iq4;); (57, TBB, tbb::concurrent_bounded_queue<int> iq5;); (56, TBB, tbb::concurrent_bounded_queue<int> iq4;); (61, TBB, tbb::concurrent_bounded_queue<int> iq6;); (52, TBB, tbb::concurrent_bounded_queue<int> iq4;); (55, TBB, tbb::concurrent_bounded_queue<int> iq5;); (59, TBB, tbb::concurrent_bounded_queue<int> iq6;); (54, TBB, tbb::concurrent_bounded_queue<int> iq4;); (55, TBB, tbb::concurrent_bounded_queue<int> iq5;); (56, TBB, tbb::concurrent_bounded_queue<int> iq6;); (62, TBB, tbb::concurrent_bounded_queue<int> iq7;); (57, TBB, tbb::concurrent_bounded_queue<int> iq5;); (52, TBB, tbb::concurrent_bounded_queue<int> iq4;); (53, TBB, tbb::concurrent_bounded_queue<int> iq5;); (54, TBB, tbb::concurrent_bounded_queue<int> iq6;); (61, TBB, tbb::concurrent_bounded_queue<int> iq7;); (54, TBB, tbb::concurrent_bounded_queue<int> iq5;); (55, TBB, tbb::concurrent_bounded_queue<int> iq6;); (61, TBB, tbb::concurrent_bounded_queue<int> iq7;); (51, TBB, tbb::concurrent_bounded_queue<int", "label": "(55, TBB, tbb::concurrent_bounded_queue<int> iq4;); (22, TBB, tbb::concurrent_bounded_queue<int> iq2;); (35, TBB, tbb::concurrent_bounded_queue<int> iq3;); (3, TBB, tbb::concurrent_bounded_queue<int> iq1;)"}
{"predict": "(22, CUDA, cudaFree(res_dev);); (15, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(bool), cudaMemcpyDeviceToHost);); (11, CUDA, cudaMalloc(&res_dev, sizeof(bool));)", "label": "(22, CUDA, cudaFree(res_dev);); (15, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(bool), cudaMemcpyDeviceToHost);); (11, CUDA, cudaMalloc(&res_dev, sizeof(bool));)"}
{"predict": "(45, OpenMP, auto stop_clock = omp_get_wtime();); (26, OpenMP, auto start_clock = omp_get_wtime();)", "label": "(27, OpenMP, auto start_clock = omp_get_wtime();); (44, OpenMP, auto stop_clock = omp_get_wtime();)"}
{"predict": "(4, OpenMP, #pragma omp target teams distribute simd firstprivate(g, g1, sivar))", "label": "(4, OpenMP, #pragma omp target teams distribute simd firstprivate(g, g1, sivar))"}
{"predict": "(19, CUDA, ASSERT_EQ(cudaGetLastError(), cudaSuccess) << \"Kernel launch error.\");)", "label": "(19, CUDA, ASSERT_EQ(cudaGetLastError(), cudaSuccess) << \"Kernel launch error.\";)"}
{"predict": "(22, OpenCL, OCL_ASSERT(clBuildProgram(program, 1, &device, NULL, NULL, &status) == CL_SUCCESS);); (24, OpenCL, OCL_ASSERT(clReleaseProgram(program) == CL_SUCCESS);)", "label": "(23, OpenCL, kernel = clCreateKernel(program, \"simplemov\", &status);); (20, OpenCL, OCL_ASSERT(clBuildProgram(program, 1, &device, NULL, NULL, NULL) == CL_SUCCESS);)"}
{"predict": "(1, CUDA, cudaGraphicsMapResources(1, &cuda_pbo_resource, 0);); (12, CUDA, cudaGraphicsUnmapResources(1, &cuda_pbo_resource, 0);)", "label": "(2, CUDA, cudaGraphicsMapResources(1, &cuda_pbo_resource, 0);); (7, CUDA, cudaGraphicsUnmapResources(1, &cuda_pbo_resource, 0);)"}
{"predict": "(3, OpenMP, #pragma omp target map(tofrom: threadCount)); (4, OpenMP, #pragma omp metadirective)", "label": "(3, OpenMP, #pragma omp target map(tofrom: threadCount)); (4, OpenMP, #pragma omp metadirective)"}
{"predict": "(4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (13, MPI, double end = MPI_Wtime();); (0, MPI, double start = MPI_Wtime();)", "label": "(5, MPI, MPI_Barrier(MPI_COMM_WORLD);); (5, MPI, double start = MPI_Wtime();); (12, MPI, double end = MPI_Wtime();); (0, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(22, MPI, MPI_Send(&send_value, 1, MPI_INT, (rank + 1) % size, 4711, MPI_COMM_WORLD);); (30, MPI, MPI_Wait(&send_request, &status);); (31, MPI, MPI_Wait(&recv_request, &status);); (6, MPI, MPI_Init(&argc, &argv);); (37, MPI, MPI_Send(&send_value, 1, MPI_INT, (rank + 1) % size, 4711, MPI_COMM_WORLD);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (29, MPI, MPI_Recv(&recv_value, 1, MPI_INT, (rank + 1) % size, 4711, MPI_COMM_WORLD, &status);); (13, MPI, MPI_Finalize();); (9, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(9, MPI, MPI_Init(&argc, &argv);); (18, MPI, MPI_Finalize();); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (31, MPI, MPI_Wait(&recv_request, &status);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (17, MPI, MPI_Isend(&send_value, 1, MPI_INT, (rank + 1) % size, 4711, MPI_COMM_WORLD, &send_request);); (27, MPI, MPI_Wait(&send_request, &status);)"}
{"predict": "(33, OpenCL, clReleaseEvent(retEvent);)", "label": "(33, OpenCL, clReleaseEvent(retEvent);)"}
{"predict": "(7, CUDA, EXPECT_EQ(cudaSuccess, cudaEnsembleConstructor(model, argc, (const char**)argv));)", "label": "(16, CUDA, EXPECT_NO_THROW(ensemble = new flamegpu::CUDAEnsemble(model, sizeof(argv) / sizeof(char*), argv));); (6, CUDA, EXPECT_NO_THROW(ensemble = new flamegpu::CUDAEnsemble(model, 0, nullptr));)"}
{"predict": "(6, MPI, MPI_Comm cComm = MPI_Comm_f2c(*comm);); (2, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wSize);); (2, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wRank);); (3, MPI, MPI_Comm_rank(cComm, &cRank);); (3, MPI, MPI_Comm_size(cComm, &cSize);)", "label": "(2, MPI, MPI_Comm cComm = MPI_Comm_f2c(*comm);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wSize);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wRank);); (5, MPI, MPI_Comm_rank(cComm, &cRank);); (3, MPI, MPI_Comm_size(cComm, &cSize);)"}
{"predict": "(107, CUDA, cudaError_t cudaErr = cudaStreamCreate(&streamArray[i]);); (170, CUDA, cudaError_t err = cudaStreamSynchronize(streamArray[i]);); (175, CUDA, cudaError_t err1 = cudaStreamDestroy(streamArray[i]);)", "label": "(193, CUDA, cudaError_t cudaStatus = cudaDeviceSynchronize();); (103, CUDA, cudaError_t cudaErr = cudaStreamCreate(&streamArray[i]);); (163, CUDA, cudaError_t cudaStatus = cudaGetLastError();)"}
{"predict": "(19, MPI, *ierr = MPI_Info_get_valuelen( (MPI_Info)(*v1), p2, v3, v4 );); (4, MPI, MPI_Info_set( (MPI_Info)(*v1), p2, &l4 );)", "label": "(19, MPI, *ierr = MPI_Info_get_valuelen( (MPI_Info)(*v1), p2, v3, &l4 );)"}
{"predict": "(65, MPI, MPI_Type_commit(recv_types + j);); (29, MPI, MPI_Alltoall(send_size, 1, MPI_COUNT, recv_size, 1, MPI_COUNT, fd->comm);)", "label": "(65, MPI, MPI_Type_commit(recv_types + j);); (30, MPI, MPI_Alltoall(recv_size, 1, MPI_COUNT, send_size, 1, MPI_COUNT, fd->comm);)"}
{"predict": "(5, OpenMP, printf( \"Using %d threads.\\n\", omp_get_max_threads() );); (12, OpenMP, #pragma omp parallel for schedule(dynamic,1))", "label": "(3, CUDA, cudaFree( 0 );)"}
{"predict": "(52, CUDA, CHECK_CUDA(cudaFreeHost(hIndices));); (15, CUDA, CHECK_CUDA(cudaMallocHost(&hIndicesRef, ySize * sizeof(uint32_t)));); (50, CUDA, CHECK_CUDA(cudaFreeHost(hIndicesRef));); (9, CUDA, CHECK_CUDA(cudaMallocHost(&hx, xSize * sizeof(float)));); (50, CUDA, CHECK_CUDA(cudaFreeHost(hx));); (10, CUDA, CHECK_CUDA(cudaMallocHost(&hyRef, ySize * sizeof(float)));); (11, CUDA, CHECK_CUDA(cudaMallocHost(&hIndices, ySize * sizeof(uint32_t)));); (15, CUDA, CHECK_CUDA(cudaMalloc(&dx, xSize * sizeof(float)));); (47, CUDA, CHECK_CUDA(cudaFreeHost(hyRef));); (13, CUDA, CHECK_CUDA(cudaMalloc(&dIndices, ySize * sizeof(uint32_t)));); (-2, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (42, CUDA, CHECK_CUDA(cudaFreeHost(hx));); (42, CUDA, CHECK_CUDA(cudaFree(dIndices));); (40, CUDA, CHECK_CUDA(cudaFree(dx));); (5, CUDA, CHECK_CUDA(cudaMallocHost(&hIndicesRef, ySize * sizeof(uint32_t)));); (38, CUDA, CHECK_CUDA(cudaFreeHost(hIndices));); (39, CUDA, CHECK_CUDA(cudaFree(dIndices));); (3, CUDA, CHECK_CUDA(cudaMallocHost(&hyRef, ySize * sizeof(float)));); (1, CUDA, CHECK_CUDA(cudaMallocHost(&hIndices, ySize * sizeof(uint32_t)));); (37, CUDA, CHECK_CUDA(cudaFreeHost(hIndicesRef));); (-7, CUDA, CHECK_CUDA(cudaMallocHost(&hx, ySize * sizeof(float)));); (3, CUDA, CHECK_CUDA(cudaMalloc(&dx, ySize * sizeof(float)));); (35, CUDA, CHECK_CUDA(cudaFreeHost(hIndices));); (-10, CUDA, CHECK_CUDA(cudaMallocHost(&hhyRef, ySize * sizeof(float)));); (-9, CUDA, CHECK_CUDA(cudaMallocHost(&hhyRef, ySize * sizeof(float)));); (-11, CUDA, CHECK_CUDA(cudaMallocHost(&hIndicesRef, ySize * sizeof(uint32_t)));); (-10, CUDA, CHECK_CUDA(cudaMallocHost(&hIndices, ySize * sizeof(uint32_t)));); (-9, CUDA, CHECK_CUDA(cudaMallocHost(&h", "label": "(52, CUDA, CHECK_CUDA(cudaFreeHost(hIndices));); (15, CUDA, CHECK_CUDA(cudaMallocHost(&hIndicesRef, ySize * sizeof(uint32_t)));); (19, CUDA, CHECK_CUDA(cudaMalloc(&dx, xSize * sizeof(float)));); (50, CUDA, CHECK_CUDA(cudaFreeHost(hIndicesRef));); (8, CUDA, CHECK_CUDA(cudaMallocHost(&hx, xSize * sizeof(float)));); (-3, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (44, CUDA, CHECK_CUDA(cudaFreeHost(hx));); (42, CUDA, CHECK_CUDA(cudaFree(dIndices));); (40, CUDA, CHECK_CUDA(cudaFree(dx));); (4, CUDA, CHECK_CUDA(cudaMallocHost(&hyRef, ySize * sizeof(float)));); (5, CUDA, CHECK_CUDA(cudaMallocHost(&hIndices, ySize * sizeof(uint32_t)));); (40, CUDA, CHECK_CUDA(cudaFreeHost(hyRef));); (11, CUDA, CHECK_CUDA(cudaMalloc(&dIndices, ySize * sizeof(uint32_t)));)"}
{"predict": "(3, CUDA, cudaExtent extent = make_cudaExtent(4, 5, 6);)", "label": "(4, CUDA, cudaExtent extent = make_cudaExtent(4, 5, 6);)"}
{"predict": "(38, SYCL, sycl::accessor B(d_B, h, sycl::no_init);); (40, SYCL, sycl::accessor C(d_C, h, sycl::no_init);); (37, SYCL, sycl::accessor A(d_A, h, sycl::no_init);)", "label": "(39, SYCL, sycl::accessor B(d_B, h, sycl::read_only);); (37, SYCL, sycl::accessor A(d_A, h);); (38, SYCL, sycl::accessor C(d_C, h, sycl::read_only);); (81, SYCL, prk::SYCL::print_exception_details(e);)"}
{"predict": "(2, CUDA, LogDebug(LOG_PY_UTILS \"PyCudaMemory_New()\\n\");)", "label": "(2, CUDA, LogDebug(LOG_PY_UTILS \"PyCudaMemory_New()\\n\");)"}
{"predict": "(3, MPI, MPI_Init(NULL,NULL);); (99, MPI, MPI_Finalize();)", "label": "(3, MPI, MPI_Init(nullptr, nullptr);); (100, MPI, MPI_Finalize();); (3, MPI, dray::dray::mpi_comm(MPI_Comm_c2f(comm));)"}
{"predict": "(9, SYCL, sycl::queue handle = getSYCLDefaultDeviceDefaultQueue();)", "label": "(9, SYCL, sycl::queue handle = getSYCLDefaultDeviceDefaultQueue();)"}
{"predict": "(52, OpenMP, #pragma omp target teams distribute parallel for  map(tofrom:int_array[0:N],min2[0:1],max2[0:1])); (29, OpenMP, #pragma omp target teams distribute parallel for  map(tofrom:min[0:1],max[0:1])); (64, OpenMP, omp_target_free(int_array, omp_get_default_device());); (64, OpenMP, omp_target_free(dbl_array, omp_get_default_device());); (28, OpenMP, #pragma omp atomic  hint(ompx_fast_fp_atomics)); (25, OpenMP, #pragma omp target data map(tofrom: int_array[0:N], min[0:1], max[0:1])); (57, OpenMP, #pragma omp target teams distribute parallel for  map(tofrom:int_array[0:N],min[0:1],max[0:1]))", "label": "(24, OpenMP, double *dbl_min = (double*) omp_target_alloc(sizeof(double)*1, omp_get_default_device() );); (22, OpenMP, int *max = (int*) omp_target_alloc(sizeof(int)*1, omp_get_default_device() );); (23, OpenMP, double *dbl_max = (double*) omp_target_alloc(sizeof(double)*1, omp_get_default_device() );); (33, OpenMP, #pragma omp atomic compare); (6, OpenMP, #pragma omp target teams distribute parallel for); (38, OpenMP, #pragma omp atomic compare hint(ompx_fast_fp_atomics)); (50, OpenMP, #pragma omp target teams distribute parallel for  map(tofrom:min2,max2))"}
{"predict": "(246, CUDA, result.error = cudaEventRecord(events[0]);); (267, CUDA, std::cerr << \"cudaEventSynchronize() failed: \" << cudaGetErrorString(result.error) << std::endl;); (274, CUDA, std::cerr << \"cudaEventElapsed() failed: \" << cudaGetErrorString(result.error) << std::endl;); (245, CUDA, std::cerr << \"cudaEventRecord() failed: \" << cudaGetErrorString(result.error) << std::endl;); (255, CUDA, result.error = cudaEventRecord(events[1]);); (281, CUDA, (void)cudaEventDestroy(event);); (234, CUDA, std::cerr << \"cudaEventCreate() failed: \" << cudaGetErrorString(result.error) << std::endl;); (267, CUDA, result.error = cudaEventElapsedTime(&runtime_ms, events[0], events[1]);); (258, CUDA, result.error = cudaEventSynchronize(events[1]);); (229, CUDA, result.error = cudaEventCreate(&event);)", "label": "(202, CUDA, result.error = cudaEventRecord(events[0]);); (223, CUDA, std::cerr << \"cudaEventSynchronize() failed: \" << cudaGetErrorString(result.error) << std::endl;); (230, CUDA, std::cerr << \"cudaEventElapsed() failed: \" << cudaGetErrorString(result.error) << std::endl;); (201, CUDA, std::cerr << \"cudaEventRecord() failed: \" << cudaGetErrorString(result.error) << std::endl;); (211, CUDA, result.error = cudaEventRecord(events[1]);); (237, CUDA, (void)cudaEventDestroy(event);); (190, CUDA, std::cerr << \"cudaEventCreate() failed: \" << cudaGetErrorString(result.error) << std::endl;); (223, CUDA, result.error = cudaEventElapsedTime(&runtime_ms, events[0], events[1]);); (214, CUDA, result.error = cudaEventSynchronize(events[1]);); (185, CUDA, result.error = cudaEventCreate(&event);)"}
{"predict": "(22, MPI, MPI_Recv(recv_value, 10, MPI_INT, (rank + size - 1) % size, 4711, MPI_COMM_WORLD, &status);); (8, MPI, MPI_Init(&argc, &argv);); (44, MPI, MPI_Finalize();); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (34, MPI, MPI_Wait(&recv_request, &status);); (31, MPI, MPI_Send(send_value, 10, MPI_INT, (rank + 1) % size, 4711, MPI_COMM_WORLD);); (19, MPI, MPI_Send(send_value, 10, MPI_INT, (rank + 1) % size, 4711, MPI_COMM_WORLD);); (20, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (22, MPI, MPI_Wait(&send_request, &status);)", "label": "(9, MPI, MPI_Init(&argc, &argv);); (25, MPI, MPI_Finalize();); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (30, MPI, MPI_Isend(&send_value, 9, MPI_INT, (rank + 1) % size, 4711, MPI_COMM_WORLD, &send_request);); (34, MPI, MPI_Wait(&recv_request, &status);); (32, MPI, MPI_Wait(&send_request, &status);)"}
{"predict": "(44, MPI, MPI_Type_size_x(fd->filetype, &filetype_size);); (53, MPI, MPI_Comm_rank(fd->comm, &rank);); (53, MPI, MPI_Comm_size(fd->comm, &nprocs);); (57, MPI, MPI_Type_size_x(datatype, &buftype_size);); (43, MPI, MPI_Type_extent(fd->filetype, &filetype_extent);); (56, MPI, MPI_Type_extent(datatype, &buftype_extent);); (43, MPI, MPI_Type_size_x(buftype, &etype_size);); (45, MPI, MPI_Type_size_x(new_buftype, &new_buftype_size);); (43, MPI, MPI_Type_size_x(new_fuftype, &new_fuftype_size);); (44, MPI, MPI_Type_size_x(new_buftype, &new_buftype_size);)", "label": "(44, MPI, MPI_Type_size_x(fd->filetype, &filetype_size);); (52, MPI, MPI_Comm_rank(fd->comm, &rank);); (52, MPI, MPI_Comm_size(fd->comm, &nprocs);); (56, MPI, MPI_Type_get_extent(datatype, &lb, &buftype_extent);); (54, MPI, MPI_Type_size_x(datatype, &buftype_size);)"}
{"predict": "(145, MPI, MPI_Bcast(&size2,      1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (145, MPI, MPI_Bcast(&radius,     1, MPI_INT,           root, MPI_COMM_WORLD);); (305, MPI, MPI_Finalize();); (141, MPI, MPI_Bcast(&lsize2,     1, MPI_INT,           root, MPI_COMM_WORLD);); (139, MPI, MPI_Bcast(&lsize,      1, MPI_INT,           root, MPI_COMM_WORLD);); (140, MPI, MPI_Bcast(&size,       1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (278, MPI, MPI_Reduce(&vector_sum, &check_sum, 1, MPI_DOUBLE, MPI_SUM, root, MPI_COMM_WORLD);); (45, MPI, MPI_Init(&argc,&argv);); (231, MPI, MPI_Barrier(MPI_COMM_WORLD);); (45, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (139, MPI, MPI_Bcast(&iterations, 1, MPI_INT,           root, MPI_COMM_WORLD);); (42, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)", "label": "(146, MPI, MPI_Bcast(&size2,      1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (146, MPI, MPI_Bcast(&radius,     1, MPI_INT,           root, MPI_COMM_WORLD);); (303, MPI, MPI_Finalize();); (141, MPI, MPI_Bcast(&lsize2,     1, MPI_INT,           root, MPI_COMM_WORLD);); (139, MPI, MPI_Bcast(&lsize,      1, MPI_INT,           root, MPI_COMM_WORLD);); (140, MPI, MPI_Bcast(&size,       1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (277, MPI, MPI_Reduce(&vector_sum, &check_sum, 1, MPI_DOUBLE, MPI_SUM, root, MPI_COMM_WORLD);); (44, MPI, MPI_Init(&argc,&argv);); (230, MPI, MPI_Barrier(MPI_COMM_WORLD);); (44, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (138, MPI, MPI_Bcast(&iterations, 1, MPI_INT,           root, MPI_COMM_WORLD);); (41, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(252, OpenMP, __nonMP_DynamicFunc__input2_handle_error();)", "label": "(251, OpenMP, #pragma omp parallel for shared(curdat, inc, m_count))"}
{"predict": "(25, OpenMP, #pragma omp target parallel for simd collapse 4)); (41, OpenMP, #pragma omp target parallel for simd collapse(4, )); (72, OpenMP, #pragma omp target parallel for simd collapse(-5)); (8, OpenMP, #pragma omp target parallel for simd collapse()); (63, OpenMP, #pragma omp target parallel for simd collapse(4, 8)); (4, OpenMP, #pragma omp target parallel for simd collapse(); (89, OpenMP, #pragma omp target parallel for simd collapse(2) firstprivate(i)); (81, OpenMP, #pragma omp target parallel for simd collapse(2)); (70, OpenMP, #pragma omp target parallel for simd collapse(0)); (22, OpenMP, #pragma omp target parallel for simd collapse(4); (30, OpenMP, #pragma omp target parallel for simd collapse(4,); (52, OpenMP, #pragma omp target parallel for simd collapse(4,, 4)); (39, OpenMP, #pragma omp target parallel for simd collapse(4 4)); (2, OpenMP, #pragma omp target parallel for simd collapse(,); (69, OpenMP, #pragma omp target parallel for simd collapse(5 - 5)); (-14, OpenMP, #pragma omp target parallel for simd collapse); (-6, OpenMP, #pragma omp target parallel for simd collapse(, )); (22, OpenMP, #pragma omp target parallel for simd collapse(4,); (29, OpenMP, #pragma omp target parallel for simd collapse(4)); (54, OpenMP, #pragma omp target parallel for simd collapse(2.5))", "label": "(21, OpenMP, #pragma omp target parallel for simd collapse(, )); (54, OpenMP, #pragma omp target parallel for simd collapse(4,, 4)); (34, OpenMP, #pragma omp target parallel for simd collapse(4,); (79, OpenMP, #pragma omp target parallel for simd collapse(0)); (46, OpenMP, #pragma omp target parallel for simd collapse(4 4)); (85, OpenMP, #pragma omp target parallel for simd collapse(2) firstprivate(i)); (72, OpenMP, #pragma omp target parallel for simd collapse(-5)); (6, OpenMP, #pragma omp target parallel for simd collapse()); (-3, OpenMP, #pragma omp target parallel for simd collapse); (57, OpenMP, #pragma omp target parallel for simd collapse(4, 8)); (7, OpenMP, #pragma omp target parallel for simd collapse(,); (63, OpenMP, #pragma omp target parallel for simd collapse(foo())); (74, OpenMP, #pragma omp target parallel for simd collapse(5 - 5)); (57, OpenMP, #pragma omp target parallel for simd collapse(2.5)); (17, OpenMP, #pragma omp target parallel for simd collapse(4); (11, OpenMP, #pragma omp target parallel for simd collapse 4)); (-7, OpenMP, #pragma omp target parallel for simd collapse(); (24, OpenMP, #pragma omp target parallel for simd collapse(4, )); (27, OpenMP, #pragma omp target parallel for simd collapse(4))"}
{"predict": "(7, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)", "label": "(7, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)"}
{"predict": "(72, CUDA, checkCudaErrors(cufftExecC2C(plan, (cufftComplex *)d_in_signal, (cufftComplex *)d_output_signal));); (73, CUDA, checkCudaErrors(cufftExecZ2Z(plan, (cufftDoubleComplex*)d_in_signal, (cufftDoubleComplex*)d_output_signal));); (20, CUDA, checkCudaErrors(cudaMalloc((void **)&d_output_signal, mem_size));); (165, CUDA, cudaDeviceReset();); (71, CUDA, getLastCudaError(\"cufftExecC2C failed\");); (72, CUDA, getLastCudaError(\"cufftExecZ2Z failed\");); (8, CUDA, checkCudaErrors(cudaMalloc((void **)&d_in_signal, mem_size));); (154, CUDA, checkCudaErrors(cufftDestroy(plan));); (159, CUDA, cudaFree(d_in_signal);); (159, CUDA, cudaFree(d_output_signal);); (4, CUDA, checkCudaErrors(cudaMemcpy(d_in_signal, h_in_signal, mem_size, cudaMemcpyHostToDevice));)", "label": "(83, CUDA, cudaDeviceSynchronize();); (20, CUDA, checkCudaErrors(cudaMalloc((void **)&d_output_signal, mem_size));); (73, CUDA, checkCudaErrors(cufftExecZ2Z(plan, (cufftDoubleComplex *)d_in_signal, (cufftDoubleComplex *)d_output_signal, direction));); (69, CUDA, checkCudaErrors(cufftExecC2C(plan, (cufftComplex *)d_in_signal, (cufftComplex *)d_output_signal, direction));); (155, CUDA, checkCudaErrors(cufftDestroy(plan));); (9, CUDA, checkCudaErrors(cudaMalloc((void **)&d_in_signal, mem_size));); (163, CUDA, cudaDeviceReset();)"}
{"predict": "(26, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice));); (34, CUDA, CHECK(cudaMemcpy(h_z, d_z, M, cudaMemcpyDeviceToHost));); (23, CUDA, CHECK(cudaMalloc((void**)&d_z, M));); (20, CUDA, CHECK(cudaMalloc((void**)&d_x, M));); (46, CUDA, CHECK(cudaFree(d_y));); (24, CUDA, CHECK(cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice));); (43, CUDA, CHECK(cudaFree(d_z));); (19, CUDA, CHECK(cudaMalloc((void**)&d_y, M));); (38, CUDA, CHECK(cudaFree(d_x));); (40, CUDA, CHECK(cudaFree(d_z));)", "label": "(48, CUDA, CHECK(cudaFree(d_y));); (26, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice));); (47, CUDA, CHECK(cudaFree(d_z));); (35, CUDA, CHECK(cudaMemcpy(h_z, d_z, M, cudaMemcpyDeviceToHost));); (18, CUDA, CHECK(cudaMalloc((void**)&d_x, M));); (18, CUDA, CHECK(cudaMalloc((void**)&d_y, M));); (41, CUDA, CHECK(cudaFree(d_x));); (17, CUDA, CHECK(cudaMalloc((void**)&d_z, M));); (26, CUDA, CHECK(cudaGetLastError());); (26, CUDA, CHECK(cudaDeviceSynchronize());); (18, CUDA, CHECK(cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice));)"}
{"predict": "(5, MPI, MPI_Init(&argc, &argv);); (122, MPI, MPI_Finalize();); (4, MPI, MPI_Get_library_version(version, &versionlen);)", "label": "(5, MPI, MPI_Get_library_version(version, &versionlen);)"}
{"predict": "(4, OpenMP, #pragma omp target map(tofrom: obj, count)); (5, OpenMP, #pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp for reduction(+:count) lastprivate(obj)); (9, OpenMP, #pragma omp cancel for); (2, OpenMP, #pragma omp parallel private(i) num_threads(1))"}
{"predict": "(31, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&device_buffer, size_bytes));); (33, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(device_buffer, input.data(), size_bytes, cudaMemcpyHostToDevice));); (40, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(device_buffer));); (30, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (30, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (37, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(output.data(), device_buffer, size_bytes, cudaMemcpyDeviceToHost));)", "label": "(31, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&device_buffer, size_bytes));); (32, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(device_buffer, input.data(), size_bytes, cudaMemcpyHostToDevice));); (41, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(device_buffer));); (35, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (33, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (37, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(output.data(), device_buffer, size_bytes, cudaMemcpyDeviceToHost));)"}
{"predict": "(67, MPI, MPI_Comm_rank(fd->comm, &myrank);); (102, MPI, MPI_Allgather(st_end_offset, 2, ADIO_OFFSET, all_st_end_offsets, 2, ADIO_OFFSET, fd->comm);); (258, MPI, MPI_Type_free(&client_comm_dtype_arr[i]);); (325, MPI, MPI_Type_free(&fd->file_realm_types[0]);); (325, MPI, MPI_Type_free(&fd->file_realm_types[i]);); (60, MPI, MPI_Comm_size(fd->comm, &nprocs);); (256, MPI, MPI_Type_free(&agg_dtype);); (217, MPI, MPI_Type_get_extent(datatype, &lb, &extent);); (218, MPI, MPI_Type_size_x(datatype, &size);); (305, MPI, MPI_Type_free(&client_alltoallw_counts);); (305, MPI, MPI_Type_free(&agg_alltoallw_counts);); (226, MPI, MPI_Type_size_x(agg_comm_dtype_arr[i], &size);); (50, MPI, MPI_Type_commit(&client_comm_dtype_arr[i]);); (94, MPI, MPI_Allgather(&min_st_offset, 1, ADIO_OFFSET, &min_st_offset, 1, ADIO_OFFSET, fd->comm);); (305, MPI, MPI_Type_free(&fd->file_realm_types[i]);); (226, MPI, MPI_Type_free(&client_mem_view_state_arr[i]);); (227, MPI, MPI_Type_free(&my_mem_view_state_arr[i]);); (42, MPI, MPI_Type_commit(&agg_dtype);); (305, MPI, MPI_Type_free(&fd->file_realm_types[1]);); (226, MPI, MPI_Type_free(&fd->file_realm_types[0]);); (51, MPI, MPI_Type_contiguous_x(agg_rank, MPI_BYTE, &agg_dtype);); (295, MPI, MPI_Type_free(&client_comm_statuses);); (301, MPI, MPI_Type_free(&fd->file_realm_types[i]);); (226, MPI, MPI_Type_free(&client_alltoallw_p);); (53, MPI, MPI_Type_size_x(agg_dtype, &size);); (295, MPI, MPI_Type_free(&client_mem_view_state_arr[1]);); (226, MPI, MPI_Type_free(&client_alltoallw_count);); (281, MPI, MPI_Type_free", "label": "(101, MPI, MPI_Allgather(st_end_offset, 2, ADIO_OFFSET, all_st_end_offsets, 2, ADIO_OFFSET, fd->comm);); (66, MPI, MPI_Comm_rank(fd->comm, &myrank);); (522, MPI, MPI_Type_free(&client_comm_dtype_arr[i]);); (582, MPI, MPI_Type_free(&fd->file_realm_types[0]);); (62, MPI, MPI_Comm_size(fd->comm, &nprocs);); (584, MPI, MPI_Type_free(&fd->file_realm_types[i]);); (150, MPI, MPI_Type_get_extent(datatype, &lb, &extent);); (302, MPI, MPI_Type_size_x(agg_comm_dtype_arr[i], &size);); (296, MPI, MPI_Type_free(&agg_dtype);)"}
{"predict": "(13, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(13, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(9, MPI, MPI_Win_unlock(0, win);); (8, MPI, MPI_Get(get_array, nlevels, MPI_INT, 0, 0, nlevels, MPI_INT, win);); (9, MPI, MPI_Win_lock(MPI_LOCK_SHARED, 0, 0, win);)", "label": "(12, MPI, MPI_Win_unlock(0, win);); (9, MPI, MPI_Accumulate(one, nlevels, MPI_INT, 0, 0, 1, acc_type, MPI_SUM, win);); (9, MPI, MPI_Get(get_array, nlevels, MPI_INT, 0, 0, 1, get_type, win);); (6, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, 0, 0, win);)"}
{"predict": "(40, OpenMP, std::cout << \"Number of threads    = \" << omp_get_max_threads() << std::endl;)", "label": "(31, OpenMP, std::cout << \"OpenMP default device = \" << omp_get_default_device() << std::endl;)"}
{"predict": "(18, CUDA, cudaFree(res_dev);); (8, CUDA, cudaMalloc(&res_dev, sizeof(float));); (10, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)", "label": "(18, CUDA, cudaFree(res_dev);); (8, CUDA, cudaMalloc(&res_dev, sizeof(float));); (10, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(29, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (37, TBB, reduce_time = (tbb::tick_count::now() - t0).seconds();); (43, TBB, graph_time = (tbb::tick_count::now() - t0).seconds();); (26, TBB, serial_time = (tbb::tick_count::now() - t0).seconds();)", "label": "(29, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (37, TBB, reduce_time = (tbb::tick_count::now() - t0).seconds();); (43, TBB, graph_time = (tbb::tick_count::now() - t0).seconds();); (28, TBB, serial_time = (tbb::tick_count::now() - t0).seconds();)"}
{"predict": "(13, CUDA, thrust::device_ptr<int> dev_ptr(raw_ptr);); (16, CUDA, thrust::raw_pointer_cast(dev_ptr);); (4, CUDA, raw_ptr = thrust::raw_pointer_cast(&raw_ptr[0]);); (17, CUDA, thrust::device_free(raw_ptr);); (1, CUDA, cudaMalloc(&raw_ptr, N * sizeof(int));)", "label": "(6, CUDA, cudaMalloc((void**) &raw_ptr, N * sizeof(int));); (11, CUDA, thrust::fill(dev_ptr, dev_ptr + N, (int) 0);); (7, CUDA, thrust::device_ptr<int> dev_ptr = thrust::device_pointer_cast(raw_ptr);); (15, CUDA, cudaFree(raw_ptr);)"}
{"predict": "(19, CUDA, cudaMemcpy(poscellh,poscell+pini,sizeof(float4)*np,cudaMemcpyDeviceToHost);); (19, CUDA, cudaMemcpy(checkh,checkcell+pini,sizeof(byte)*np,cudaMemcpyDeviceToHost);)", "label": "(19, CUDA, cudaMemcpy(poscellh,poscell+pini,sizeof(float4)*np,cudaMemcpyDeviceToHost);); (19, CUDA, cudaMemcpy(checkh,check+pini,sizeof(byte)*np,cudaMemcpyDeviceToHost);)"}
{"predict": "(18, MPI, Mpi.SignalProgress();)", "label": "(29, MPI, Mpi.SignalProgress();)"}
{"predict": "(3, OpenMP, #pragma omp parallel for default(none) private(idx) shared(count, sapH_cur_salt, saved_plain, crypt_key))", "label": "(3, OpenMP, #pragma omp parallel for default(none) private(idx) shared(count, sapH_cur_salt, saved_plain, crypt_key))"}
{"predict": "(23, MPI, rc = MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (7, MPI, rc = MPI_Init(&argc, &argv);); (30, MPI, MPI_Finalize();)", "label": "(23, MPI, rc = MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (32, MPI, MPI_Finalize();); (6, MPI, rc = MPI_Init(NULL, NULL);)"}
{"predict": "(18, CUDA, YAKSURI_CUDAI_CUDA_ERR_CHKANDJUMP(cerr, rc, fn_fail);); (8, CUDA, uintptr_t iov_unpack_threshold = yaksuri_cudai_get_iov_unpack_threshold(info);); (28, CUDA, cerr = yaksuri_cudai_md_alloc(type);); (52, CUDA, cerr = cudaSetDevice(cur_device);); (42, CUDA, cerr = cudaGetDevice(&cur_device);); (32, CUDA, rc = yaksuri_cudai_md_alloc(iov[i]);)", "label": "(9, CUDA, uintptr_t iov_unpack_threshold = yaksuri_cudai_get_iov_unpack_threshold(info);); (29, CUDA, cudaMemcpyAsync(iov[i].iov_base, sbuf, iov[i].iov_len, cudaMemcpyDefault, stream);); (16, CUDA, YAKSURI_CUDAI_CUDA_ERR_CHKANDJUMP(cerr, rc, fn_fail);); (52, CUDA, cerr = cudaSetDevice(cur_device);); (42, CUDA, cerr = cudaGetDevice(&cur_device);); (32, CUDA, rc = yaksuri_cudai_md_alloc(type);); (43, CUDA, cerr = cudaSetDevice(target);)"}
{"predict": "(15, OpenMP, #pragma omp target teams distribute parallel for num_threads(8))", "label": "(15, OpenMP, #pragma omp target parallel for)"}
{"predict": "(4, CUDA, extern __shared__ float shared[];)", "label": "(4, CUDA, extern __shared__ float shared[];)"}
{"predict": "(2, OpenMP, #pragma omp parallel for)", "label": "(13, OpenMP, #pragma omp parallel for)"}
{"predict": "(31, TBB, generate_test<parallel_test, tbb::flow::tuple<float, double, int, double, float, long, int, float, long> >::do_test();); (18, TBB, generate_test<serial_test, tbb::flow::tuple<float, double, int, double, float, int, float, long, int, float, long> >::do_test();); (7, TBB, generate_test<serial_test, tbb::flow::tuple<float, double> >::do_test();); (23, TBB, generate_test<serial_test, tbb::flow::tuple<float, double, int, int, float, int, float, long, int, float, long> >::do_test();); (7, TBB, generate_test<serial_test, tbb::flow::tuple<double, float, int, double, float, int, float, long, int, float, long> >::do_test();); (15, TBB, generate_test<serial_test, tbb::flow::tuple<float, int, float, int, float, int, float, long, int, float, long> >::do_test();); (23, TBB, generate_test<serial_test, tbb::flow::tuple<double, float, int, int, float, int, float, long, int, float, long> >::do_test();); (18, TBB, generate_test<serial_test, tbb::flow::tuple<float, int, int, int, float, int, float, long, int, float, long> >::do_test();); (2, TBB, generate_test<serial_test, tbb::flow::tuple<float, double, int, int, float, int, float, long, int, float, long> >::do_test();); (7, TBB, generate_test<serial_test, tbb::flow::tuple<double, float, int, int, float, int, float, long, int, float, long> >::do_test();)", "label": "(31, TBB, generate_test<parallel_test, tbb::flow::tuple<float, double, int, double, double, long, int, float, long> >::do_test();); (27, TBB, generate_test<parallel_test, tbb::flow::tuple<float, int, double, float, long, float, long> >::do_test();); (23, TBB, generate_test<parallel_test, tbb::flow::tuple<double, double, int, int, short> >::do_test();); (15, TBB, generate_test<serial_test, tbb::flow::tuple<float, double, int, double, double, float, long, int, float, long> >::do_test();); (3, TBB, generate_test<serial_test, tbb::flow::tuple<float, double> >::do_test();); (4, TBB, generate_test<serial_test, tbb::flow::tuple<float, double, int, long> >::do_test();); (16, TBB, generate_test<parallel_test, tbb::flow::tuple<float, int, long> >::do_test();); (5, TBB, generate_test<serial_test, tbb::flow::tuple<double, double, int, long, int, short> >::do_test();); (7, TBB, generate_test<serial_test, tbb::flow::tuple<float, double, double, double, float, int, float, long> >::do_test();)"}
{"predict": "(24, MPI, MPI_Start(&reqs[i]);); (18, MPI, MPI_Barrier(comm);); (25, MPI, MPI_Waitall(2 * nproc, reqs, MPI_STATUSES_IGNORE);); (28, MPI, MPI_Comm_free(&comm);); (8, MPI, MPI_Comm_size(comm, &nproc);); (6, MPI, MPI_Comm_rank(comm, &rank);); (18, MPI, MPI_Request_free(&reqs[i]);); (15, MPI, MPI_Start(&reqs[req_id]);)", "label": "(25, MPI, MPI_Isend(&out_buf[elems * i], elems, MPI_FLOAT, i, 0, comm, &reqs[i + nproc]);); (20, MPI, MPI_Irecv(&in_buf[elems * i], elems, MPI_FLOAT, i, 0, comm, &reqs[i]);); (11, MPI, MPI_Comm_size(comm, &nproc);); (9, MPI, MPI_Comm_rank(comm, &rank);); (24, MPI, MPI_Waitall(nproc * 2, reqs, MPI_STATUSES_IGNORE);)"}
{"predict": "(28, CUDA, PyErr_SetString(PyExc_Exception, LOG_PY_UTILS \"cudaOverlay() input and output image formats are different\");); (21, CUDA, PyErr_SetString(PyExc_Exception, LOG_PY_UTILS \"cudaOverlay() failed to get input/output image pointers (should be cudaImage)\");); (31, CUDA, PYCUDA_ASSERT_NOGIL(cudaOverlay(input->base.ptr, output->base.ptr, input->width, input->height, x, y, stream));)", "label": "(22, CUDA, PyErr_SetString(PyExc_Exception, LOG_PY_UTILS \"cudaOverlay() failed to get input/output image pointers (should be cudaImage)\");); (32, CUDA, PYCUDA_ASSERT_NOGIL(cudaOverlay(input->base.ptr, input->width, input->height, output->base.ptr, output->width, output->height, output->format, x, y, stream));); (26, CUDA, PyErr_SetString(PyExc_Exception, LOG_PY_UTILS \"cudaOverlay() input and output image formats are different\");)"}
{"predict": "(3, CUDA, __shared__ float local[threads];)", "label": "(3, CUDA, __shared__ float local[threads];)"}
{"predict": "(33, OpenMP, #pragma omp atomic); (16, OpenMP, #pragma omp parallel for private(dx, dy, dz, r, ig2))", "label": "(33, OpenMP, #pragma omp atomic); (15, OpenMP, #pragma omp target teams distribute parallel for collapse(2) private(dx, dy, dz, r, ig2))"}
{"predict": "(19, OpenMP, #pragma omp for private(j,k,isize))", "label": "(19, OpenMP, #pragma omp for private(k)); (37, OpenMP, #pragma omp for private ( k)); (66, OpenMP, #pragma omp for private (k))"}
{"predict": "(7, MPI, MPI_Init(&argc, &argv);); (39, MPI, MPI_Finalize();); (6, MPI, MPI_Wtime(&t1);); (6, MPI, MPI_Wtime(&t2);); (31, MPI, MPI_Wtick(&tick);)", "label": "(8, MPI, t1 = MPI_Wtime();); (6, MPI, MPI_Init(&argc, &argv);); (38, MPI, MPI_Finalize();); (31, MPI, tick = MPI_Wtick();); (5, MPI, t2 = MPI_Wtime();)"}
{"predict": "(44, OpenCL, err = clGetDeviceInfo(device, CL_DEVICE_EXTENSIONS, v * sizeof(s), extensions, &len);); (28, OpenCL, err = clGetDeviceInfo(device, CL_DEVICE_EXTENSIONS, 0, NULL, &len);)", "label": "(29, OpenCL, err = clGetDeviceInfo(device, CL_DEVICE_EXTENSIONS, 0, NULL, &len);); (42, OpenCL, err = clGetDeviceInfo(device, CL_DEVICE_EXTENSIONS, len, extensions, NULL);)"}
{"predict": "(4, MPI, MPI_Finalize();); (1, MPI, MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided);)", "label": "(2, MPI, MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided);); (136, MPI, MPI_Send(x.data(), chunk, MPI_DOUBLE, 0, 42, comm);); (2, MPI, MPI_Finalize();); (65, MPI, MPI_Allgather(&chunk, 1, MPI_INT, &domain[1], 1, MPI_INT, comm);)"}
{"predict": "(140, MPI, MPI_Comm_free(&dup_comm);); (134, MPI, MPI_Attr_get(dup_comm, key_1, &vvalue, &flag);); (27, MPI, MPI_Comm_dup(dup_comm_world, &dup_comm_world);); (168, MPI, MPI_Comm_free(&world_comm);); (42, MPI, MPI_Comm_group(dup_comm_world, &world_group);); (135, MPI, MPI_Attr_get(dup_comm, key_3, &vvalue, &flag);); (166, MPI, MPI_Group_free(&rev_group);); (17, MPI, MPI_Comm_size(dup_comm_world, &world_size);); (71, MPI, MPI_Group_free(&lo_group);); (46, MPI, MPI_Group_incl(world_group, 1, &n, &lo_group);); (153, MPI, MPI_Comm_free(&lo_comm);); (98, MPI, MPI_Attr_put(lo_comm, key_1, (void *) (MPI_Aint) world_rank);); (152, MPI, MPI_Group_free(&world_group);); (96, MPI, MPI_Attr_put(lo_comm, key_3, (void *) (MPI_Aint) world_rank);); (39, MPI, MPI_Barrier(world_comm);); (8, MPI, MPI_Comm_rank(dup_comm_world, &world_rank);); (143, MPI, MPI_Comm_free(&dup_comm_world);); (153, MPI, MPI_Comm_free(&rev_comm);); (22, MPI, MPI_Comm_create(world_comm, lo_group, &rev_comm);); (9, MPI, MPI_Comm_set_name(dup_comm_world, (char *) \"Dup of world\");); (8, MPI, MPI_Comm_set_name(lo_comm, (char *) \"Half world\");); (59, MPI, MPI_Group_incl(world_group, 1, &world_rank, &rev_group);); (2, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &dup_comm_world);); (140, MPI, MPI_Attr_free(&key_1);); (142, MPI, MPI_Attr_free(&key_3);); (12, MPI, MPI_Comm_rank(lo_comm, &rank);); (3, MPI, MPI_Comm_group(lo_comm, &lo_group);); (12, MPI, MPI_Comm_set_name(dup_comm, (char *) \"Dup of lo\");); (53, MPI, MPI", "label": "(172, MPI, MPI_Comm_split(dup_comm_world, color, key, &split_comm);); (120, MPI, MPI_Attr_put(lo_comm, key_1, (void *) (MPI_Aint) world_rank);); (111, MPI, MPI_Keyval_create(copy_fn, delete_fn, &key_1, &value);); (142, MPI, MPI_Abort(MPI_COMM_WORLD, 3005);); (118, MPI, MPI_Attr_put(lo_comm, key_3, (void *) 0);); (238, MPI, MPI_Comm_free(&split_comm);); (210, MPI, MPI_Comm_compare(world_comm, rev_comm, &result);); (28, MPI, MPI_Comm_create(dup_comm_world, world_group, &world_comm);); (28, MPI, MPI_Comm_rank(world_comm, &rank);); (2, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (114, MPI, MPI_Comm_dup(lo_comm, &dup_comm);); (146, MPI, MPI_Keyval_free(&key_3);); (230, MPI, MPI_Comm_free(&rev_comm);); (135, MPI, MPI_Attr_get(dup_comm, key_3, (void **) &vvalue, &flag);); (210, MPI, MPI_Comm_compare(world_comm, lo_comm, &result);); (190, MPI, MPI_Abort(MPI_COMM_WORLD, 3011);); (71, MPI, MPI_Abort(MPI_COMM_WORLD, 3003);); (228, MPI, MPI_Group_free(&world_group);); (155, MPI, MPI_Comm_size(split_comm, &size);); (-7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &world_size);); (194, MPI, MPI_Comm_create(world_comm, rev_group, &rev_comm);); (135, MPI, MPI_Keyval_free(&key_1);); (157, MPI, MPI_Abort(MPI_COMM_WORLD, 3009);); (190, MPI, MPI_Group_range_incl(world_group, 1, ranges, &rev_group);); (92, MPI, MPI_Keyval_create(MPI_NULL_COPY_FN, MPI_NULL_DELETE_FN, &key_3, &value);); (129, MPI, MPI_Abort(MPI_COMM_WORLD, 3008);); (148, MPI, MPI_Comm_rank(split_comm, &rank);); (201, MPI, MPI_Abort(MPI_COMM_WORLD, 3013);); (173, MPI, MPI_Comm_compare(lo_comm, dup_comm, &result);); (47, MPI, MPI_Abort(MPI_COMM_WORLD, 3002);); (152, MPI, MPI_Barrier(split_comm);); (40, MPI, MPI_Comm_rank(lo_comm, &rank);); (106, MPI, MPI_Abort(MPI_COMM_WORLD, 3004);); (213, MPI, MPI_Group_free(&rev_group);); (163, MPI, MPI_Abort(MPI_COMM_WORLD, 3010);); (205, MPI, MPI_Comm_free(&dup_comm_world);); (4, MPI, MPI_Abort(MPI_COMM_WORLD, 3001);); (213, MPI, MPI_Comm_free(&dup_comm);); (41, MPI, MPI_Barrier(lo_comm);); (210, MPI, MPI_Comm_free(&lo_comm);); (91, MPI, MPI_Attr_get(dup_comm, key_1, (void **) &vvalue, &flag);); (179, MPI, MPI_Abort(MPI_COMM_WORLD, 3012);)"}
{"predict": "(102, CUDA, extern __shared__ __align__(sizeof(float2)) char smem_[];)", "label": "(16, CUDA, extern __shared__ __align__(sizeof(float2)) char smem_[];)"}
{"predict": "(96, OpenMP, double stop = omp_get_wtime();); (78, CUDA, CUDA_RT_CALL(cudaDeviceSynchronize());); (29, CUDA, CUDA_RT_CALL(cudaFree(0));); (91, CUDA, CUDA_RT_CALL(cudaDeviceCanAccessPeer(&canAccessPeer, dev_id, top));); (249, CUDA, CUDA_RT_CALL(cudaFreeHost(a_h));); (32, CUDA, CUDA_RT_CALL(cudaMallocHost(&a_ref_h, nx * ny * sizeof(float)));); (74, CUDA, CUDA_RT_CALL(cudaGetLastError());); (94, CUDA, CUDA_RT_CALL(cudaDeviceCanAccessPeer(&canAccessPeer, dev_id, bottom));); (94, CUDA, CUDA_RT_CALL(cudaDeviceEnablePeerAccess(bottom, 0));); (234, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_bottom_stream[dev_id]));); (86, CUDA, CUDA_RT_CALL(cudaDeviceEnablePeerAccess(top, 0));); (77, CUDA, CUDA_RT_CALL(cudaMalloc(l2_norm_d + dev_id, sizeof(float)));); (49, CUDA, CUDA_RT_CALL(cudaMalloc(a + dev_id, nx * (chunk_size[dev_id] + 2) * sizeof(float)));); (50, CUDA, CUDA_RT_CALL(cudaMemset(a[dev_id], 0, nx * (chunk_size[dev_id] + 2) * sizeof(float)));); (14, CUDA, CUDA_RT_CALL(cudaGetDeviceCount(&num_devices));); (229, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_top_stream[dev_id]));); (16, CUDA, CUDA_RT_CALL(cudaSetDevice(dev_id));); (43, CUDA, CUDA_RT_CALL(cudaMalloc(a_new + dev_id, nx * (chunk_size[dev_id] + 2) * sizeof(float)));); (229, CUDA, CUDA_RT_CALL(cudaFreeHost(l2_norm_h[dev_id]));); (231, CUDA, CUDA_RT_CALL(cudaFree(a_new[dev_id]));); (64, CUDA, CUDA_RT_CALL(cudaStreamCreate(compute_stream + dev_id));); (65, CUDA, CUDA_RT_CALL(cudaStreamCreate(push_bottom_stream + dev_id));); (41, CUDA, CUDA_RT_CALL(cudaMemset(a_new[dev_id], 0, nx * (chunk_size[dev_id] + 2) * sizeof(float)));); (231, CUDA, CUDA_RT_CALL(cudaFreeHost(a_ref_h));); (14, CUDA, CUDA", "label": "(207, OpenMP, double stop = omp_get_wtime();); (146, CUDA, CUDA_RT_CALL(cudaStreamSynchronize(push_top_stream[bottom]));); (77, CUDA, CUDA_RT_CALL(cudaDeviceSynchronize());); (28, CUDA, CUDA_RT_CALL(cudaFree(0));); (88, CUDA, CUDA_RT_CALL(cudaDeviceCanAccessPeer(&canAccessPeer, dev_id, top));); (250, CUDA, CUDA_RT_CALL(cudaFreeHost(a_h));); (29, CUDA, CUDA_RT_CALL(cudaMallocHost(&a_ref_h, nx * ny * sizeof(float)));); (71, CUDA, CUDA_RT_CALL(cudaGetLastError());); (91, CUDA, CUDA_RT_CALL(cudaDeviceCanAccessPeer(&canAccessPeer, dev_id, bottom));); (92, CUDA, CUDA_RT_CALL(cudaDeviceEnablePeerAccess(bottom, 0));); (235, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_bottom_stream[dev_id]));); (83, CUDA, CUDA_RT_CALL(cudaDeviceEnablePeerAccess(top, 0));); (74, CUDA, CUDA_RT_CALL(cudaMalloc(l2_norm_d + dev_id, sizeof(float)));); (44, CUDA, CUDA_RT_CALL(cudaMalloc(a + dev_id, nx * (chunk_size[dev_id] + 2) * sizeof(float)));); (157, CUDA, CUDA_RT_CALL(cudaStreamSynchronize(compute_stream[dev_id]));); (45, CUDA, CUDA_RT_CALL(cudaMemset(a[dev_id], 0, nx * (chunk_size[dev_id] + 2) * sizeof(float)));); (132, CUDA, CUDA_RT_CALL(cudaStreamSynchronize(push_bottom_stream[top]));); (9, CUDA, CUDA_RT_CALL(cudaGetDeviceCount(&num_devices));); (228, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_top_stream[dev_id]));); (11, CUDA, CUDA_RT_CALL(cudaSetDevice(dev_id));); (38, CUDA, CUDA_RT_CALL(cudaMalloc(a_new + dev_id, nx * (chunk_size[dev_id] + 2) * sizeof(float)));); (228, CUDA, CUDA_RT_CALL(cudaFreeHost(l2_norm_h[dev_id]));); (230, CUDA, CUDA_RT_CALL(cudaFree(a_new[dev_id]));); (59, CUDA, CUDA_RT_CALL(cudaStreamCreate(compute_stream + dev_id));); (60, CUDA, CUDA_RT_CALL(cudaStreamCreate(push_bottom_stream + dev_id));); (36, CUDA, CUDA_RT_CALL(cudaMemset(a_new[dev_id], 0, nx * (chunk_size[dev_id] + 2) * sizeof(float)));); (230, CUDA, CUDA_RT_CALL(cudaFreeHost(a_ref_h));); (9, CUDA, CUDA_RT_CALL(cudaMallocHost(&a_h, nx * ny * sizeof(float)));); (219, CUDA, CUDA_RT_CALL(cudaStreamDestroy(compute_stream[dev_id]));); (102, OpenMP, double start = omp_get_wtime();); (57, CUDA, CUDA_RT_CALL(cudaMallocHost(l2_norm_h + dev_id, sizeof(float)));); (222, CUDA, CUDA_RT_CALL(cudaFree(a[dev_id]));); (218, CUDA, CUDA_RT_CALL(cudaFree(l2_norm_d[dev_id]));); (50, CUDA, CUDA_RT_CALL(cudaStreamCreate(push_top_stream + dev_id));)"}
{"predict": "(129, CUDA, cudaMemcpy(z_cu, z, shape.Z_len * sizeof(DT), cudaMemcpyHostToDevice);); (241, CUDA, cudaFree(x_cu);); (122, CUDA, cudaMalloc(&x_cu, shape.X_len * sizeof(ST));); (125, CUDA, cudaMemcpy(y_cu, y, shape.Y_len * sizeof(ST), cudaMemcpyHostToDevice);); (123, CUDA, cudaMemcpy(x_cu, x, shape.X_len * sizeof(ST), cudaMemcpyHostToDevice);); (238, CUDA, cudaFree(y_cu);); (120, CUDA, cudaMalloc(&z_cu, shape.Z_len * sizeof(DT));); (118, CUDA, cudaMalloc(&y_cu, shape.Y_len * sizeof(ST));); (128, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (240, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (234, CUDA, cudaFree(z_cu);)", "label": "(129, CUDA, cudaMemcpy(z_cu, z, shape.Z_len * sizeof(DT), cudaMemcpyHostToDevice);); (221, CUDA, cudaFree(x_cu);); (122, CUDA, cudaMalloc(&x_cu, shape.X_len * sizeof(ST));); (125, CUDA, cudaMemcpy(y_cu, y, shape.Y_len * sizeof(ST), cudaMemcpyHostToDevice);); (123, CUDA, cudaMemcpy(x_cu, x, shape.X_len * sizeof(ST), cudaMemcpyHostToDevice);); (218, CUDA, cudaFree(y_cu);); (120, CUDA, cudaMalloc(&z_cu, shape.Z_len * sizeof(DT));); (118, CUDA, cudaMalloc(&y_cu, shape.Y_len * sizeof(ST));); (128, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (220, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (214, CUDA, cudaFree(z_cu);)"}
{"predict": "(21, CUDA, __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];); (21, CUDA, __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];)", "label": "(22, CUDA, __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];); (20, CUDA, __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];)"}
{"predict": "(188, CUDA, checkCuda(cudaMemcpyAsync(A_gpu->scubufs[streamId].Remain_info, Remain_info, RemainBlk * sizeof(Remain_info_t), cudaMemcpyHostToDevice, FunCallStream));); (97, CUDA, checkCuda(cudaMemcpyAsync(A_gpu->scubufs[streamId].usub_IndirectJ3, A_gpu->scubufs[streamId].usub_IndirectJ3_host, ncols * sizeof(int_t), cudaMemcpyHostToDevice, FunCallStream));); (99, CUDA, checkCuda(cudaMemcpyAsync(A_gpu->scubufs[streamId].Remain_L_buff, Remain_L_buff, Remain_lbuf_send_size * sizeof(double), cudaMemcpyHostToDevice, FunCallStream));); (104, CUDA, checkCuda(cudaMemcpyAsync(A_gpu->scubufs[streamId].bigU, bigU, bigu_send_size * sizeof(double), cudaMemcpyHostToDevice, FunCallStream));); (174, CUDA, checkCuda(cudaMemcpyAsync(A_gpu->scubufs[streamId].lsub, lsub, lsub_len * sizeof(int_t), cudaMemcpyHostToDevice, FunCallStream));); (175, CUDA, checkCuda(cudaMemcpyAsync(A_gpu->scubufs[streamId].usub, usub, usub_len * sizeof(int_t), cudaMemcpyHostToDevice, FunCallStream));); (99, CUDA, checkCuda(cudaMemcpyAsync(A_gpu->scubufs[streamId].Ublock_info, Ublock_info, mcb * sizeof(Ublock_info_t), cudaMemcpyHostToDevice, FunCallStream));); (180, CUDA, checkCuda(cudaMemcpyAsync(A_gpu->scubufs[streamId].indirectJ1, indirectJ1, MAX_SUPER_SIZE * sizeof(int_t), cudaMemcpyHostToDevice, FunCallStream));); (99, CUDA, checkCuda(cudaMemcpyAsync(A_gpu->scubufs[streamId].bigU, bigU, bigu_send_size * sizeof(double), cudaMemcpyHostToDevice, FunCallStream));); (93, CUDA, checkCuda(cudaMemcpyAsync(A_gpu->scubufs[streamId].usub_IndirectJ2, A_gpu->scubufs[streamId].usub_IndirectJ2_host, MAX_SUPER_SIZE * sizeof(int_t), cudaMemcpyHostToDevice, FunCallStream));); (180, CUDA, checkCuda(cudaMemcpyAsync(A_gpu->scubufs[stream", "label": "(93, CUDA, cudaEventRecord(A_gpu->ePCIeH2D[k0], FunCallStream);); (231, CUDA, cudaEventRecord(A_gpu->GemmStart[k0], FunCallStream);); (239, CUDA, cudaStreamSynchronize(FunCallStream);)"}
{"predict": "(33, OpenMP, #pragma omp atomic  hint(AMD_uncontended)); (19, OpenMP, #pragma omp atomic  hint(AMD_fast_fp_atomics)); (3, OpenMP, #pragma omp atomic  hint(AMD_safe_fp_atomics)); (37, OpenMP, #pragma omp atomic  hint(AMD_unsafe_fp_atomics)); (0, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:sum))", "label": "(6, OpenMP, #pragma omp atomic  hint(AMD_safe_fp_atomics)); (34, OpenMP, #pragma omp atomic  hint(AMD_unsafe_fp_atomics)); (2, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:sum)); (17, OpenMP, #pragma omp atomic  hint(AMD_fast_fp_atomics))"}
{"predict": "(4, CUDA, extern __shared__ float entire_buffer[];)", "label": "(6, CUDA, extern __shared__ float entire_buffer[];)"}
{"predict": "(13, CUDA, CHECK(cudaMalloc(&d_y, M));); (36, CUDA, CHECK(cudaFree(d_y));); (14, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice));); (35, CUDA, CHECK(cudaFree(d_z));); (9, CUDA, CHECK(cudaMalloc(&d_z, M));); (7, CUDA, CHECK(cudaMalloc(&d_x, M));); (10, CUDA, CHECK(cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice));); (30, CUDA, CHECK(cudaFree(d_x));)", "label": "(37, CUDA, CHECK(cudaFree(d_y));); (14, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice));); (36, CUDA, CHECK(cudaFree(d_z));); (10, CUDA, CHECK(cudaMalloc(&d_y, M));); (8, CUDA, CHECK(cudaMalloc(&d_x, M));); (31, CUDA, CHECK(cudaFree(d_x));); (8, CUDA, CHECK(cudaMalloc(&d_z, M));); (9, CUDA, CHECK(cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice));)"}
{"predict": "(10, CUDA, cudaDeviceSynchronize();); (5, CUDA, cudaMalloc((void **)&sm_o, sizeof(float) * m * m);); (35, CUDA, cudaFree(sm_o);)", "label": "(10, CUDA, cudaDeviceSynchronize();); (5, CUDA, cudaMalloc((void **)&sm_o, sizeof(float) * m * m);); (35, CUDA, cudaFree(sm_o);)"}
{"predict": "(47, MPI, MPI_Win_lock(MPI_LOCK_SHARED, 1, 0, win);); (22, MPI, MPI_Type_commit(&vector_dtp);); (44, MPI, MPI_Win_unlock(1, win);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (65, MPI, MPI_Free_mem(orig_buf);); (65, MPI, MPI_Free_mem(tar_buf);); (39, MPI, MPI_Win_lock(MPI_LOCK_SHARED, 0, 0, win);); (15, MPI, MPI_Win_attach(win, orig_buf, sizeof(long_double_int_t) * DATA_SIZE);); (61, MPI, MPI_Win_detach(win, vector_dtp);); (36, MPI, MPI_Barrier(MPI_COMM_WORLD);); (57, MPI, MPI_Win_free(&win);); (18, MPI, MPI_Win_attach(win, tar_buf, sizeof(long_double_int_t) * DATA_SIZE);); (9, MPI, MPI_Alloc_mem(sizeof(long_double_int_t) * DATA_SIZE, MPI_INFO_NULL, &orig_buf);)", "label": "(49, MPI, MPI_Win_unlock(1, win);); (32, MPI, MPI_Win_unlock(rank, win);); (45, MPI, MPI_Win_lock(MPI_LOCK_SHARED, 1, 0, win);); (68, MPI, MPI_Type_free(&vector_dtp);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (64, MPI, MPI_Win_free(&win);); (41, MPI, MPI_Accumulate(orig_buf, 1, vector_dtp, 1, 0, 1, vector_dtp, MPI_MAXLOC, win);); (14, MPI, MPI_Type_commit(&vector_dtp);); (64, MPI, MPI_Free_mem(orig_buf);); (65, MPI, MPI_Free_mem(tar_buf);); (17, MPI, MPI_Win_lock(MPI_LOCK_SHARED, rank, 0, win);); (22, MPI, MPI_Barrier(MPI_COMM_WORLD);); (1, MPI, MPI_Alloc_mem(sizeof(long_double_int_t) * DATA_SIZE, MPI_INFO_NULL, &orig_buf);)"}
{"predict": "(2, CUDA, __shared__ int hist[256];)", "label": "(2, CUDA, __shared__ int hist[256];)"}
{"predict": "(70, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (66, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (50, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(srcDev->basePtr(), srcVec.data(), srcVec.size(), cudaMemcpyHostToDevice));); (68, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(test.data(), dstDev->basePtr(), test.size(), cudaMemcpyDeviceToHost));); (61, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamDestroy(stream));)", "label": "(74, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(test.data(), dstDev->basePtr(), test.size(), cudaMemcpyDeviceToHost));); (66, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (50, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(srcDev->basePtr(), srcVec.data(), srcVec.size(), cudaMemcpyHostToDevice));); (60, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (64, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamDestroy(stream));)"}
{"predict": "(18, OpenCL, clCreateCommandQueue_StubWithCallback(clCreateCommandQueue_testCommandQueueFromSpecifiedContext);); (39, OpenCL, clReleaseContext_ExpectAndReturn(expected_context, CL_SUCCESS);); (39, OpenCL, clReleaseCommandQueue_ExpectAndReturn(expected, CL_SUCCESS);); (27, OpenCL, clReleasePlatform_ExpectAndReturn(expected_platform, CL_SUCCESS);)", "label": "(18, OpenCL, clCreateCommandQueueWithProperties_StubWithCallback(clCreateCommandQueueWithProperties_testCommandQueueFromSpecifiedContext);)"}
{"predict": "(25, OpenCL, clReleaseEvent(outEvent);)", "label": "(25, OpenCL, clReleaseEvent(outEvent);)"}
{"predict": "(142, OpenCL, error = clSetKernelArg(kernel, 1, sizeof(tinfo->inBuf), &tinfo->inBuf);)", "label": "(142, OpenCL, error = clSetKernelArg(kernel, 1, sizeof(tinfo->inBuf), &tinfo->inBuf);)"}
{"predict": "(32, CUDA, cudaGridDependencySynchronize();); (18, CUDA, __shared__ int32_t smemExpertCount[NumThreadsPerCluster];); (29, CUDA, cudaTriggerProgrammaticLaunchCompletion();); (11, CUDA, __shared__ typename Scan::TempStorage tempStorage;)", "label": "(10, CUDA, __shared__ typename Scan::TempStorage tempStorage;); (33, CUDA, cudaGridDependencySynchronize();); (4, CUDA, __shared__ int32_t __attribute((aligned(128))) smemExpertCount[NumThreads];); (4, CUDA, __shared__ int32_t __attribute((aligned(128))) smemExpertOffset[NumThreads];)"}
{"predict": "(8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &np);); (157, MPI, MPI_Finalize();); (92, MPI, MPI_Type_hvector(block_order, 1, MPI_Aint(0), stride_dt, &trans_dt);); (159, MPI, MPI_Type_free(&stride_dt);); (159, MPI, MPI_Type_free(&trans_dt);); (93, MPI, MPI_Type_commit(&trans_dt);); (0, MPI, MPI_Init_thread(&argc, &argv, requested, &provided);); (135, MPI, MPI_Allreduce(MPI_IN_PLACE, &abserr, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (95, MPI, t0 = MPI_Wtime();); (55, MPI, MPI_Barrier(MPI_COMM_WORLD);); (135, MPI, MPI_Allreduce(&t1, &t0, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);); (58, MPI, t1 = MPI_Wtime();); (84, MPI, MPI_Type_vector(block_order, 1, block_order, MPI_DOUBLE, &stride_dt);); (121, MPI, MPI_Allreduce(&A[0], &B[order], 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);); (53, MPI, MPI_Finalized(&provided);)", "label": "(9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &np);); (142, MPI, t1 = MPI_Wtime();); (20, MPI, MPI_Finalize();); (98, MPI, MPI_Type_commit(&trans_dt);); (144, MPI, MPI_Type_free(&stride_dt);); (159, MPI, MPI_Allreduce(MPI_IN_PLACE, &abserr, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);); (2, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (90, MPI, MPI_Type_size(MPI_DOUBLE,&dsize);); (141, MPI, MPI_Type_free(&trans_dt);); (70, MPI, MPI_Abort(MPI_COMM_WORLD,99);); (-5, MPI, if (provided < requested) MPI_Abort(MPI_COMM_WORLD,provided);); (89, MPI, MPI_Type_hvector(block_order, 1, dsize, stride_dt, &trans_dt);); (83, MPI, MPI_Type_vector(block_order, 1, block_order, MPI_DOUBLE, &stride_dt);); (54, MPI, MPI_Barrier(MPI_COMM_WORLD);); (96, MPI, t0 = MPI_Wtime();); (-11, MPI, MPI_Init_thread(&argc, &argv, requested, &provided);)"}
{"predict": "(55, CUDA, cudaMalloc((void **)&result_data, tensor->size * sizeof(float));); (55, CUDA, cudaMemcpy(result_data, tensor->data, tensor->size * sizeof(float), cudaMemcpyHostToDevice);); (52, CUDA, cudaMalloc((void **)&result_data, tensor->size * sizeof(float));)", "label": "(55, CUDA, cudaMalloc((void**)&result_data, axis_size * sizeof(float));); (52, CUDA, cudaMalloc((void**)&result_data, tensor->size * sizeof(float));)"}
{"predict": "(8, OpenCL, CL_CHECK(cl_status);); (8, OpenCL, CL_CHECK(clGetPlatformInfo(id, param_name, param_size, some_info, &cl_status));)", "label": "(8, OpenCL, cl_status = clGetPlatformInfo(id, param_name, param_size, some_info, NULL);)"}
{"predict": "(10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (10, MPI, MPI_Comm_group(MPI_COMM_WORLD, &world_group);); (28, MPI, mpi_errno = MPI_Comm_create(MPI_COMM_WORLD, comm_group, &comm_hdls[i]);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (46, MPI, MPI_Group_free(&world_group);); (26, MPI, MPI_Group_free(&comm_group);)", "label": "(37, MPI, MPI_Group_free(&comm_group);); (29, MPI, mpi_errno = MPI_Comm_create(MPI_COMM_WORLD, comm_group, &comm_hdls[i]);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (8, MPI, MPI_Comm_group(MPI_COMM_WORLD, &world_group);); (45, MPI, MPI_Group_free(&world_group);); (8, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(6, MPI, MPI_Comm_size(comm, &size);); (4, MPI, MPI_Comm_rank(comm, &rank);)", "label": "(6, MPI, MPI_Comm_size(comm, &size);); (4, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(15, CUDA, __shared__ float rr[TILE_T][TILE_T], kk[TILE_T][TILE_T];)", "label": "(15, CUDA, __shared__ float rr[TILE_T][N], kk[TILE_T][N];)"}
{"predict": "(49, MPI, MPI_T_category_get_pvars(i, numPvars, pvarIndex);); (26, MPI, MPI_T_category_get_cvars(i, numCvars, cvarIndex);); (3, MPI, MPI_T_category_get_num(&numCat);); (73, MPI, MPI_T_category_get_categories(i, numSubcat, subcatIndex);)", "label": "(51, MPI, MPI_T_category_get_pvars(i, numPvars, pvarIndex);); (28, MPI, MPI_T_category_get_cvars(i, numCvars, cvarIndex);); (3, MPI, MPI_T_category_get_num(&numCat);); (72, MPI, MPI_T_category_get_categories(i, numSubcat, subcatIndex);)"}
{"predict": "(13, CUDA, CUDASimulation cudaSimulation(model);)", "label": "(13, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(45, MPI, MPI_Bcast( nzval,   nnz, MPI_FLOAT, 0, grid->comm );); (45, MPI, MPI_Bcast( rowind, nnz, mpi_int_t,  0, grid->comm );); (34, MPI, MPI_Bcast( &nnz, 1,   mpi_int_t,  0, grid->comm );); (45, MPI, MPI_Bcast( colptr,  n+1, mpi_int_t,  0, grid->comm );); (43, MPI, MPI_Bcast( rowind,  nnz, mpi_int_t,  0, grid->comm );); (34, MPI, MPI_Bcast( &m,   1,   mpi_int_t,  0, grid->comm );); (27, MPI, MPI_Bcast( &n,     1,   mpi_int_t,  0, grid->comm );); (25, MPI, MPI_Bcast( &m,     1,   mpi_int_t,  0, grid->comm );); (33, MPI, MPI_Bcast( &nnz, 1,   mpi_int_t,  0, grid->comm );); (27, MPI, MPI_Bcast( colptr, n+1, mpi_int_t,  0, grid->comm );); (23, MPI, MPI_Bcast( &n,   1,   mpi_int_t,  0, grid->comm );); (30, MPI, MPI_Bcast( &m,   1,   mpi_int_t,  0, grid->comm );); (33, MPI, MPI_Bcast( nzval,   nnz, MPI_FLOAT, 0, grid->comm );); (23, MPI, MPI_Bcast( &nnz,   1,   mpi_int_t,  0, grid->comm );); (30, MPI, MPI_Bcast( &n,   1,   mpi_int_t,  0, grid->comm );); (22, MPI, MPI_Bcast( rowind,  nnz, mpi_int_t,  0, grid->comm );); (27, MPI, MPI_Bcast( &m,   1,   mpi_int_t,  0, grid->comm );); (30, MPI, MPI_Bcast( nzval,", "label": "(33, MPI, MPI_Bcast( nzval,  nnz, MPI_FLOAT, 0, grid->comm );); (33, MPI, MPI_Bcast( rowind, nnz, mpi_int_t,  0, grid->comm );); (45, MPI, MPI_Bcast( colptr,  n+1, mpi_int_t,  0, grid->comm );); (43, MPI, MPI_Bcast( rowind,  nnz, mpi_int_t,  0, grid->comm );); (34, MPI, MPI_Bcast( &m,   1,   mpi_int_t,  0, grid->comm );); (26, MPI, MPI_Bcast( &n,     1,   mpi_int_t,  0, grid->comm );); (24, MPI, MPI_Bcast( &m,     1,   mpi_int_t,  0, grid->comm );); (33, MPI, MPI_Bcast( &nnz, 1,   mpi_int_t,  0, grid->comm );); (27, MPI, MPI_Bcast( colptr, n+1, mpi_int_t,  0, grid->comm );); (23, MPI, MPI_Bcast( &nnz,   1,   mpi_int_t,  0, grid->comm );); (29, MPI, MPI_Bcast( &n,   1,   mpi_int_t,  0, grid->comm );); (34, MPI, MPI_Bcast( nzval,   nnz, MPI_FLOAT, 0, grid->comm );)"}
{"predict": "(9, TBB, tbb::finalize(schBlock1, std::nothrow);); (4, TBB, tbb::task_scheduler_handle schBlock = tbb::task_scheduler_handle{tbb::attach{});); (5, TBB, tbb::task_scheduler_handle schBlock1 = tbb::task_scheduler_handle{tbb::attach{});)", "label": "(15, TBB, bool ok = tbb::finalize(schBlock, std::nothrow);); (11, TBB, bool ok = tbb::finalize(schBlock1, std::nothrow);); (4, TBB, tbb::task_scheduler_handle schBlock1;)"}
{"predict": "(10, OpenMP, c->op_free = DTCMP_Op_free(c->op);)", "label": "(9, MPI, MPI_Type_free(&(c->key));)"}
{"predict": "(27, OpenMP, #pragma omp dispatch ret); (1, OpenMP, #pragma omp dispatch)", "label": "(2, OpenMP, #pragma omp dispatch)"}
{"predict": "(16, OpenCL, status |= clSetKernelArg(global_check, 0, sizeof(cl_mem), &buffer);)", "label": "(16, OpenCL, status = clSetKernelArg(global_check, 0, sizeof(cl_mem), &buffer);)"}
{"predict": "(59, OpenMP, #pragma omp master taskloop simd reduction(+ : ba)); (52, OpenMP, #pragma omp master taskloop simd reduction(+ : a, b, c, d, f)); (46, OpenMP, #pragma omp master taskloop simd reduction(+ : a, b)); (69, OpenMP, #pragma omp master taskloop simd reduction(^ : fl)); (58, OpenMP, #pragma omp master taskloop simd reduction(* : ca)); (43, OpenMP, #pragma omp master taskloop simd reduction(+ : ad)); (72, OpenMP, #pragma omp master taskloop simd reduction(&& : S2::S2sc)); (1, OpenMP, #pragma omp master taskloop simd reduction); (97, OpenMP, #pragma omp master taskloop simd reduction(+ : fl)); (51, OpenMP, #pragma omp master taskloop simd reduction(- : da)); (29, OpenMP, #pragma omp master taskloop simd reduction(\\)); (68, OpenMP, #pragma omp master taskloop simd reduction(& : e, g)); (59, OpenMP, #pragma omp master taskloop simd reduction(&& : S2::S2s)); (16, OpenMP, #pragma omp master taskloop simd reduction(foo : argc); (69, OpenMP, #pragma omp master taskloop simd reduction(+ : h, k, B::x)); (39, OpenMP, #pragma omp master taskloop simd reduction(min : a, b, c, d, f)); (76, OpenMP, #pragma omp parallel reduction(* : fl)); (5, OpenMP, #pragma omp master taskloop simd reduction()); (54, OpenMP, #pragma omp master taskloop simd reduction(max : h.b)); (33, OpenMP, #pragma omp master taskloop simd reduction(^ : S1)); (15, OpenMP, #pragma omp master taskloop simd reduction(|| : argc > 0? argv[1] : argv[2])); (18, OpenMP, #pragma omp master taskloop simd reduction(&& : argc, z)); (57, OpenMP, #pragma omp master taskloop simd reduction(+ : o)); (1, OpenMP, #pragma omp master taskloop simd reduction(*)); (72, OpenMP, #pragma omp master taskloop simd reduction(+ : m)); (64, OpenMP, #pragma omp parallel shared(i)); (38, OpenMP, #", "label": "(45, OpenMP, #pragma omp master taskloop simd reduction(&& : argc, z)); (101, OpenMP, #pragma omp master taskloop simd reduction(max : j)); (67, OpenMP, #pragma omp master taskloop simd reduction(^ : fl)); (111, OpenMP, #pragma omp master taskloop simd reduction(+ : m)); (20, OpenMP, #pragma omp master taskloop simd reduction()); (46, OpenMP, #pragma omp master taskloop simd reduction(+ : a, b, c, d, f)); (48, OpenMP, #pragma omp master taskloop simd reduction(min : a, b, c, d, f)); (53, OpenMP, #pragma omp master taskloop simd reduction(+ : ba)); (34, OpenMP, #pragma omp master taskloop simd reduction(~ : argc)); (100, OpenMP, #pragma omp parallel reduction(* : fl)); (17, OpenMP, #pragma omp master taskloop simd reduction(*)); (1, OpenMP, #pragma omp master taskloop simd reduction); (63, OpenMP, #pragma omp master taskloop simd reduction(&& : S2::S2sc)); (68, OpenMP, #pragma omp master taskloop simd reduction(+ : h, k, B::x)); (106, OpenMP, #pragma omp master taskloop simd reduction(task, + : m)); (75, OpenMP, #pragma omp parallel private(k)); (62, OpenMP, #pragma omp master taskloop simd reduction(& : e, g)); (67, OpenMP, #pragma omp master taskloop simd reduction(+ : o)); (-3, OpenMP, #pragma omp master taskloop simd reduction +); (72, OpenMP, #pragma omp master taskloop simd reduction(+ : p), reduction(+ : p)); (43, OpenMP, #pragma omp master taskloop simd reduction(* : ca)); (9, OpenMP, #pragma omp master taskloop simd reduction(\\)); (14, OpenMP, #pragma omp master taskloop simd reduction(| : argc,); (78, OpenMP, #pragma omp parallel reduction(min : i)); (81, OpenMP, #pragma omp parallel private(fl)); (72, OpenMP, #pragma omp master taskloop simd reduction(+ : r)); (46, OpenMP, #pragma omp master taskloop simd reduction(&& : S2::S2s)); (30, OpenMP, #pragma omp master taskloop simd reduction(max : h.b)); (20, OpenMP, #pragma omp master taskloop simd reduction(^ : S1)); (10, OpenMP, #pragma omp master taskloop simd reduction(|| : argc > 0? argv[1] : argv[2])); (-12, OpenMP, #pragma omp master taskloop simd reduction(); (56, OpenMP, #pragma omp master taskloop simd private(i), reduction(+ : j), reduction(+ : q)); (85, OpenMP, #pragma omp master taskloop simd reduction(+ : m) nogroup); (-12, OpenMP, #pragma omp master taskloop simd reduction(-); (66, OpenMP, #pragma omp parallel shared(i)); (-2, OpenMP, #pragma omp master taskloop simd reduction(foo : argc); (30, OpenMP, #pragma omp master taskloop simd reduction(- : da)); (69, OpenMP, #pragma omp master taskloop simd reduction(+ : fl))"}
{"predict": "(34, CUDA, cudaFree(d_out_min);); (17, CUDA, cudaMemcpy(d_in, in.data(), in_bytes, cudaMemcpyHostToDevice);); (31, CUDA, cudaFree(d_in);); (12, CUDA, cudaMalloc((void**)(&d_in), in_bytes);); (25, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (11, CUDA, cudaMalloc((void**)(&d_out_min), out_bytes);); (22, CUDA, assert(cudaMemcpyAsync(out_min.data(), d_out_min, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);)", "label": "(18, CUDA, cudaMemcpy(d_in, in.data(), in_bytes, cudaMemcpyHostToDevice);); (32, CUDA, cudaFree(d_in);); (13, CUDA, cudaMalloc((void**)(&d_in), in_bytes);); (13, CUDA, cudaMalloc((void**)(&d_out_min), out_bytes);); (25, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (23, CUDA, assert(cudaMemcpyAsync(out_min.data(), d_out_min, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);); (28, CUDA, cudaFree(d_out_min);)"}
{"predict": "(3, CUDA, cudaExtent extent = make_cudaExtent(10, 20, 0);)", "label": "(3, CUDA, cudaExtent extent = make_cudaExtent(10, 20, 0);)"}
{"predict": "(7, CUDA, err = cudaSyncMemcpy( dst, src, sz, type, file, line );)", "label": "(7, CUDA, err = cudaMemcpy( dst, src, sz, type );); (10, CUDA, ss << cudaGetErrorString(err) << endl;)"}
{"predict": "(4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (12, MPI, MPI_Recv(&t, 1, MPI_DOUBLE, i, 11, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (6, MPI, MPI_Send(&t, 1, MPI_DOUBLE, 0, 11, MPI_COMM_WORLD);)", "label": "(4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (9, MPI, MPI_Recv(&t, 1, MPI_DOUBLE, i, 11, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (1, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (11, MPI, MPI_Send(&t, 1, MPI_DOUBLE, 0, 11, MPI_COMM_WORLD);)"}
{"predict": "(40, MPI, MPI_Type_commit(&columntype);); (7, MPI, MPI_Init(&argc, &argv);); (61, MPI, MPI_Finalize();); (40, MPI, MPI_Send(&array[0][1], 1, columntype, 1, 1, MPI_COMM_WORLD);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (46, MPI, MPI_Type_free(&columntype);); (32, MPI, MPI_Type_vector(8, 1, 8, MPI_INT, &columntype);); (32, MPI, MPI_Type_commit(&columntype);)", "label": "(39, MPI, MPI_Type_commit(&columntype);); (7, MPI, MPI_Init(&argc, &argv);); (61, MPI, MPI_Finalize();); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (34, MPI, MPI_Type_vector(8, 1, 8, MPI_INT, &columntype);); (45, MPI, MPI_Type_free(&columntype);); (37, MPI, MPI_Send(&array[0][1], 1, columntype, 1, 1, MPI_COMM_WORLD);)"}
{"predict": "(47, SYCL, sycl::accessor B(B, myAlloc, sycl::no_init);); (47, SYCL, sycl::accessor C(C, myAlloc, sycl::no_init);); (53, SYCL, sycl::mutex::scoped_lock lock(myMutex);); (42, SYCL, sycl::accessor A(A, myAlloc, sycl::no_init);); (46, SYCL, sycl::mutex myMutex;); (49, SYCL, sycl::accessor ptr_l0_queue_h(ptr_l0_queue_h, sycl::no_init);); (50, SYCL, sycl::accessor ptr_icl_h(ptr_icl_h, sycl::no_init);)", "label": "(41, SYCL, sycl::platform Plt = Dev.get_platform();)"}
{"predict": "(8, OpenMP, #pragma omp teams distribute parallel for simd aligned (); (65, OpenMP, #pragma omp teams distribute parallel for simd aligned(f:j)); (40, OpenMP, #pragma omp teams distribute parallel for simd aligned(e, g)); (49, OpenMP, #pragma omp teams distribute parallel for simd aligned(i)); (10, OpenMP, #pragma omp teams distribute parallel for simd aligned ()); (30, OpenMP, #pragma omp teams distribute parallel for simd aligned (S1)); (13, OpenMP, #pragma omp teams distribute parallel for simd aligned (argc); (16, OpenMP, #pragma omp teams distribute parallel for simd aligned (*argc > 0? argv[1] : argv[2])); (34, OpenMP, #pragma omp teams distribute parallel for simd aligned(h)); (0, OpenMP, #pragma omp teams distribute parallel for simd aligned); (38, OpenMP, #pragma omp teams distribute parallel for simd aligned(argv[1])); (22, OpenMP, #pragma omp teams distribute parallel for simd aligned (argc : 5)); (44, OpenMP, #pragma omp parallel); (47, OpenMP, #pragma omp teams distribute parallel for simd aligned(v:16)); (51, OpenMP, #pragma omp teams distribute parallel for simd aligned(f)); (0, OpenMP, #pragma omp teams distribute parallel for simd aligned (argc,); (7, OpenMP, #pragma omp teams distribute parallel for simd aligned (argc,); (14, OpenMP, #pragma omp teams distribute parallel for simd aligned (argc : 5)); (54, OpenMP, #pragma omp teams distribute parallel for simd aligned(f:j)); (18, OpenMP, #pragma omp teams distribute parallel for simd aligned (argc : 5 + 5)); (-10, OpenMP, #pragma omp target); (31, OpenMP, #pragma omp teams distribute parallel for simd aligned(f))", "label": "(45, OpenMP, #pragma omp teams distribute parallel for simd aligned(e, g)); (7, OpenMP, #pragma omp teams distribute parallel for simd aligned); (14, OpenMP, #pragma omp teams distribute parallel for simd aligned ()); (9, OpenMP, #pragma omp teams distribute parallel for simd aligned (); (45, OpenMP, #pragma omp teams distribute parallel for simd aligned(h)); (51, OpenMP, #pragma omp parallel); (18, OpenMP, #pragma omp teams distribute parallel for simd aligned (argc,); (66, OpenMP, #pragma omp teams distribute parallel for simd aligned(f:j)); (33, OpenMP, #pragma omp teams distribute parallel for simd aligned (argv[1])); (24, OpenMP, #pragma omp teams distribute parallel for simd aligned (argc : 5)); (43, OpenMP, #pragma omp teams distribute parallel for simd aligned(i)); (56, OpenMP, #pragma omp teams distribute parallel for simd aligned(f)); (17, OpenMP, #pragma omp teams distribute parallel for simd aligned (*argc > 0? argv[1] : argv[2])); (-6, OpenMP, #pragma omp target); (47, OpenMP, #pragma omp teams distribute parallel for simd aligned(v:16)); (22, OpenMP, #pragma omp teams distribute parallel for simd aligned (S1)); (4, OpenMP, #pragma omp teams distribute parallel for simd aligned (argc)"}
{"predict": "(2, TBB, Test<tbb::atomic<ScopedColor> >::mpi_type(tbb::internal::MPI_Type::atomic<ScopedColor>());)", "label": "(10, TBB, __TBB_STATIC_ASSERT(!HasImplicitConversionToInt< tbb::atomic<ScopedColor1> >::value, \"The implicit conversion is not expected.\" );); (12, TBB, __TBB_STATIC_ASSERT( sizeof(tbb::atomic<ScopedColor2>) == sizeof(ScopedColor2), \"tbb::atomic instantiated with scoped enum should have the same size as scoped enum.\" );); (10, TBB, __TBB_STATIC_ASSERT( sizeof(tbb::atomic<ScopedColor1>) == sizeof(ScopedColor1), \"tbb::atomic instantiated with scoped enum should have the same size as scoped enum.\" );)"}
{"predict": "(26, OpenMP, #pragma omp parallel for private(xmin, xmax, ymin, ymax, dmin, dmax)); (102, OpenMP, #pragma omp parallel for private(i, j, k, n, p, q, r, s, t, u, v, w, x, y, z, d, xs, ys, zs, tr, el, node, d, z, s, y, x, r, q, p, s, t, u, v, w, x, y, z, d, xs, ys, zs, tr, el, node, d, z, s, y, x, r, q, p, s, t, u, v, w, x, y, z, d, xs, ys, zs, tr, el, node, d, z, s, y, x, r, q, p, s, t, u, v, w, x, y, z, d, xs, ys, zs, tr, el, node, d, z, s, y, x, r, q, p, s, t, u, v, w, x, y, z, d, xs, ys, zs, tr, el, node, d, z, s, y, x, r, q, p, s, t, u, v, w, x, y, z, d, xs, ys, zs, tr, el,(102, OpenMP, #pragma omp flush); (100, OpenMP, #pragma omp critical); (25, OpenMP, #pragma omp target data map(tofrom: matrix[0:nc*nr]) map(tofrom: nodes[0:nc*nr]))", "label": "(18, OpenMP, #pragma omp threadprivate (ident))"}
{"predict": "(16, OpenMP, #pragma omp taskwait); (0, OpenMP, #pragma omp task); (0, OpenMP, #pragma omp parallel); (9, OpenMP, #pragma omp task shared(i)); (0, OpenMP, #pragma omp master)", "label": "(17, OpenMP, #pragma omp taskwait); (0, OpenMP, #pragma omp task); (2, OpenMP, #pragma omp parallel for)"}
{"predict": "(7, MPI, MPI_Init(&argc, &argv);); (64, MPI, MPI_Finalize();); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (40, MPI, MPI_Type_vector(8, 1, 6, MPI_INT, &vector);); (40, MPI, MPI_Type_commit(&vector);); (46, MPI, MPI_Type_free(&vector);)", "label": "(52, MPI, MPI_Type_free(&vector);); (6, MPI, MPI_Init(&argc, &argv);); (42, MPI, MPI_Type_create_resized(vector, 0, sizeof(int), &vector);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (64, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (37, MPI, MPI_Type_vector(8, 1, 6, MPI_INT, &vector);); (38, MPI, MPI_Type_commit(&vector);); (57, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(53, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (37, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_C), sizeof(data_type) * C.size()));); (69, CUDA, CUDA_CHECK(cudaDeviceReset());); (66, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (33, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (59, CUDA, CUDA_CHECK(cudaFree(d_C));); (57, CUDA, CUDA_CHECK(cudaFree(d_A));); (27, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)", "label": "(66, CUDA, CUDA_CHECK(cudaFree(d_C));); (52, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (37, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_C), sizeof(data_type) * C.size()));); (69, CUDA, CUDA_CHECK(cudaDeviceReset());); (66, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (33, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (59, CUDA, CUDA_CHECK(cudaFree(d_A));); (27, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(7, MPI, MPI_Comm_rank(team.comm, &rank);)", "label": "(7, MPI, MPI_Comm_rank(team.comm, &rank);)"}
{"predict": "(35, MPI, MPI_Recv(recv_buff, msg_size, MPI_INT, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (6, MPI, MPI_Init(&argc, &argv);); (54, MPI, MPI_Finalize();); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (33, MPI, MPI_Send(send_buff, msg_size, MPI_INT, 0, 0, MPI_COMM_WORLD);); (47, MPI, MPI_Barrier(MPI_COMM_WORLD);); (4, MPI, MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);); (48, MPI, double end_time = MPI_Wtime();); (-2, MPI, double start_time = MPI_Wtime();)", "label": "(59, MPI, double end_time = MPI_Wtime();); (60, MPI, MPI_Finalize();); (47, MPI, MPI_Recv(recv_buff, msg_size, MPI_INT, 1, TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (17, MPI, MPI_Bcast(&msg_size, 1, MPI_INT, 0, MPI_COMM_WORLD);); (2, MPI, MPI_Init(NULL, NULL);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (13, MPI, MPI_Bcast(&nr_msgs, 1, MPI_INT, 0, MPI_COMM_WORLD);); (48, MPI, MPI_Ssend(send_buff, msg_size, MPI_INT, 0, TAG, MPI_COMM_WORLD);); (24, MPI, double start_time = MPI_Wtime();); (31, MPI, MPI_Recv(recv_buff, msg_size, MPI_INT, 0, TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (28, MPI, MPI_Ssend(send_buff, msg_size, MPI_INT, 1, TAG, MPI_COMM_WORLD);); (0, MPI, MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);)"}
{"predict": "(9, CUDA, cache.try_put(thrust::complex<Q>(0.0, 0.0));); (13, CUDA, __shared__ thrust::complex<Q> Bi;)", "label": "(5, CUDA, thrust::complex<Q> Bi;)"}
{"predict": "(51, CUDA, cudaError_t result = cudaGetLastError();)", "label": "(51, CUDA, cudaError_t result = cudaGetLastError();)"}
{"predict": "(1, TBB, typedef tbb::concurrent_queue<Foo> Queue;)", "label": "(4, TBB, tbb::concurrent_queue<Foo> queue;)"}
{"predict": "A(193, CUDA, checkCudaErrors(cudaEventSynchronize(A_gpu->ePCIeD2H_Start));); (99, CUDA, checkCudaErrors(cudaEventCreate(&(A_gpu->ePCIeD2H_End)));); (193, CUDA, checkCudaErrors(cudaEventRecord(A_gpu->ePCIeD2H_Start, sluGPU->nCudaStreams[streamId]));); (183, CUDA, checkCudaErrors(cudaEventRecord(A_gpu->ePCIeH2D, sluGPU->nCudaStreams[streamId]));); (99, CUDA, checkCudaErrors(cudaEventCreate(&(A(183, CUDA, checkCudaErrors(cudaEventRecord(A_gpu->ePCIeD2H, sluGPU->nCudaStreams[streamId]));); (175, CUDA, checkCudaErrors(cudaEventRecord(A_gpu->ePCIeH2D, sluGPU->nCudaStreams[streamId]));); (175, CUDA, checkCudaErrors(cudaEventSynchronize(A_gpu->ePCIeH2D));); (180, CUDA, checkCudaErrors(cudaEventRecord(A_gpu->ePCIeD2H_Start, sluGPU->nCudaStreams[streamId]));); (184, CUDA, checkCudaErrors(cudaEventRecord(A_gpu->ScatterEnd, sluGPU->nCudaStreams[streamId]));); (175, CUDA, checkCudaErrors(cudaEventSynchronize(A_gpu->ePCIeD2H_Start));); (164, CUDA, checkCudaErrors(cudaEventRecord(A_gpu->ePCIeH2D_Start, sluGPU->nCudaStreams[streamId]));); (93, CUDA, checkCudaErrors(cudaEventCreate(&(A(185, CUDA, checkCudaErrors(cudaEventRecord(A_gpu->ePCIeD2H_End, sluGPU->nCudaStreams[streamId]));); (181, CUDA, checkCudaErrors(cudaEventRecord(A_gpu->ePCIeD2H_Start, sluGPU->nCudaStreams[streamId]));); (175, CUDA, checkCudaErrors(cudaEventSynchronize(A_gpu->ePCIeD2H));); (164, CUDA, checkCudaErrors(cudaEventRecord(A_gpu->ePCIeD2H_Start, sluGPU->nCudaStreams[streamId]));); (84, CUDA, checkCudaErrors(cudaEventCreate(&(A(176,", "label": "(61, CUDA, checkCudaErrors(cudaMallocHost(  &tmp_ptr,  remain_l_max * sizeof(double) )) ;); (50, CUDA, checkCudaErrors(cudaMallocHost(  &tmp_ptr, (n) * sizeof(int_t) )) ;); (256, CUDA, checkCudaErrors(cudaMemcpy( (A_gpu->local_u_blk_infoVec), local_u_blk_infoVec, cum_num_u_blocks * sizeof(local_u_blk_info_t), cudaMemcpyHostToDevice)) ;); (476, CUDA, checkCudaErrors(cudaMemcpy( (A_gpu->UrowindVec), indtemp1, u_ind_len * sizeof(int_t), cudaMemcpyHostToDevice)) ;); (62, CUDA, checkCudaErrors(cudaMallocHost ( &tmp_ptr, sizeof(double) * (A_host->bufmax[1])));); (67, CUDA, checkCudaErrors(cudaMallocHost ( &tmp_ptr, sizeof(int_t) * (A_host->bufmax[2])));); (81, CUDA, checkCudaErrors(cudaMalloc(  &tmp_ptr,  buffer_size * sizeof(double))) ;); (179, CUDA, checkCudaErrors(cudaMemcpy( (A_gpu->local_l_blk_infoVec), local_l_blk_infoVec, cum_num_l_blocks * sizeof(local_l_blk_info_t), cudaMemcpyHostToDevice)) ;); (51, CUDA, checkCudaErrors(cudaMallocHost(  &tmp_ptr, mcb * sizeof(Ublock_info_t) )) ;); (174, CUDA, checkCudaErrors(cudaMalloc(  &tmp_ptr,  cum_num_l_blocks * sizeof(local_l_blk_info_t))) ;); (477, CUDA, checkCudaErrors(cudaMemcpy( (A_gpu->UrowindPtr), temp_UrowindPtr, u_k * sizeof(int_t), cudaMemcpyHostToDevice)) ;); (102, CUDA, checkCudaErrors(cudaEventCreate(&(A_gpu->ePCIeH2D[i])));); (481, CUDA, checkCudaErrors(cudaMemcpy( (A_gpu"}
{"predict": "(5, CUDA, extern __shared__ uint64_t s_qiHat_mod_pj[];)", "label": "(5, CUDA, extern __shared__ uint64_t s_qiHat_mod_pj[];)"}
{"predict": "(70, OpenMP, #pragma omp teams distribute private(j)); (69, OpenMP, #pragma omp teams distribute shared(j)); (66, OpenMP, #pragma omp teams distribute firstprivate(j)); (53, OpenMP, #pragma omp teams distribute shared(e, g)); (16, OpenMP, #pragma omp teams distribute shared (argc)); (43, OpenMP, #pragma omp teams distribute shared(ba)); (50, OpenMP, #pragma omp teams distribute shared(h, B::x)); (39, OpenMP, #pragma omp teams distribute shared(da)); (26, OpenMP, #pragma omp teams distribute shared (a, b, c, d, f)); (17, OpenMP, #pragma omp teams distribute shared (argc)); (4, OpenMP, #pragma omp teams distribute shared); (54, OpenMP, #pragma omp teams distribute shared(i)); (22, OpenMP, #pragma omp teams distribute shared (argc)); (59, OpenMP, #pragma omp teams distribute firstprivate(i)); (-3, OpenMP, #pragma omp target); (16, OpenMP, #pragma omp teams distribute shared (argc > 0? argv[1] : argv[2])); (59, OpenMP, #pragma omp teams distribute shared(i)); (46, OpenMP, #pragma omp teams distribute shared(j)); (24, OpenMP, #pragma omp teams distribute shared (argv[1])); (35, OpenMP, #pragma omp teams distribute shared(ca)); (30, OpenMP, #pragma omp teams distribute shared(da)); (5, OpenMP, #pragma omp teams distribute shared (argc); (0, OpenMP, #pragma omp teams distribute shared ()); (26, OpenMP, #pragma omp teams distribute shared(h, C::x)); (17, OpenMP, #pragma omp teams distribute shared (S1)); (-10, OpenMP, #pragma omp teams distribute shared (); (-5, OpenMP, #pragma omp teams distribute shared (argc); (46, OpenMP, #pragma omp teams distribute shared(j)); (35, OpenMP, #pragma omp teams distribute private(i)); (20, OpenMP, #pragma omp teams distribute shared(i)); (51, OpenMP, #pragma omp teams distribute shared(i)); (49, OpenMP, #pragma omp teams distribute shared(i)); (27, OpenMP, #pragma omp teams distribute shared(ca)); (-24, OpenMP, #pragma omp target", "label": "(26, OpenMP, #pragma omp teams distribute shared (argc)); (22, OpenMP, #pragma omp teams distribute shared (argc > 0? argv[1] : argv[2])); (39, OpenMP, #pragma omp teams distribute shared(ca)); (41, OpenMP, #pragma omp teams distribute shared(da)); (16, OpenMP, #pragma omp teams distribute shared (argc,); (3, OpenMP, #pragma omp teams distribute shared); (5, OpenMP, #pragma omp teams distribute shared (); (49, OpenMP, #pragma omp teams distribute firstprivate(i), shared(i)); (27, OpenMP, #pragma omp teams distribute shared (argv[1])); (-2, OpenMP, #pragma omp target); (55, OpenMP, #pragma omp teams distribute shared(j)); (57, OpenMP, #pragma omp teams distribute firstprivate(i)); (17, OpenMP, #pragma omp teams distribute shared (S1)); (37, OpenMP, #pragma omp teams distribute shared(h, B::x)); (18, OpenMP, #pragma omp teams distribute shared (a, b, c, d, f)); (2, OpenMP, #pragma omp teams distribute shared (argc); (46, OpenMP, #pragma omp teams distribute shared(i)); (21, OpenMP, #pragma omp teams distribute shared(ba)); (35, OpenMP, #pragma omp teams distribute private(i), shared(i)); (28, OpenMP, #pragma omp teams distribute shared(e, g)); (39, OpenMP, #pragma omp teams distribute private(i)); (-7, OpenMP, #pragma omp teams distribute shared ())"}
{"predict": "(62, CUDA, cudaFree(f_d);); (52, CUDA, cudaDeviceSynchronize();); (60, CUDA, cudaFree(fn_d);); (26, CUDA, cudaMalloc((void**)&f_d, NX*NY*NZ*sizeof(FLOAT));); (26, CUDA, cudaMalloc((void**)&fn_d, NX*NY*NZ*sizeof(FLOAT));); (27, CUDA, cudaMalloc((void**)&f_d, NX*NY*NZ*sizeof(FLOAT));); (30, CUDA, start_computation_time = omp_get_wtime();); (25, CUDA, cudaMalloc((void**)&fn_d, NX*NY*NZ*sizeof(FLOAT));); (49, CUDA, elapsed_time_total = omp_get_wtime() - start_time_total;); (49, CUDA, elapsed_computation_time = omp_get_wtime() - start_computation_time;); (27, CUDA, start_time_total = omp_get_wtime();); (23, CUDA, cudaMemset(f_d,0,sizeof(FLOAT)*NX*NY*NZ);); (23, CUDA, cudaMemset(fn_d,0,sizeof(FLOAT)*NX*NY*NZ);)", "label": "(33, CUDA, cudaMalloc( (void**) &fn_d,  XYZ_SIZE*sizeof(FLOAT));); (30, OpenMP, start_time_total = omp_get_wtime();); (55, OpenMP, elapsed_time_total = omp_get_wtime() - start_time_total;); (33, OpenMP, start_computation_time = omp_get_wtime();); (50, OpenMP, elapsed_computation_time = omp_get_wtime() - start_computation_time;); (29, CUDA, cudaMemcpy(f_d,f,XYZ_SIZE*sizeof(FLOAT), cudaMemcpyHostToDevice);); (50, CUDA, cudaMemcpy(f, f_d,XYZ_SIZE*sizeof(FLOAT), cudaMemcpyDeviceToHost);); (25, CUDA, cudaMalloc( (void**) &f_d,  XYZ_SIZE*sizeof(FLOAT));); (51, CUDA, cudaFree(fn_d);); (49, CUDA, cudaFree(f_d);)"}
{"predict": "(12, OpenMP, #pragma omp master); (9, OpenMP, #pragma omp parallel default(shared))", "label": "(12, OpenMP, #pragma omp master); (9, OpenMP, #pragma omp parallel default(shared))"}
{"predict": "(21, MPI, MPI_Allreduce(buf_f16, buf, N, MPI_C_FLOAT16, MPI_SUM, MPI_COMM_WORLD);); (10, MPI, MPI_Allreduce(buf, buf, N, MPIX_BFLOAT16, MPI_SUM, MPI_COMM_WORLD);)", "label": "(10, MPI, MPI_Allreduce(MPI_IN_PLACE, buf, N, MPIX_BFLOAT16, MPI_SUM, comm);); (20, MPI, MPI_Allreduce(MPI_IN_PLACE, buf_f16, N, MPIX_C_FLOAT16, MPI_SUM, comm);)"}
{"predict": "(32, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (34, CUDA, CUDA_CHECK(cudaFree(d_data));)", "label": "(32, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (34, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(134, MPI, MPI_Waitall(k, request, status);); (88, MPI, MPI_Reduce_local(recvbuf, tempbuf, count, type, op);); (88, MPI, MPI_Reduce_local(recvbuf, recvbuf, count, type, op);)", "label": "(133, MPI, MPI_Waitall(k, request, status);); (90, MPI, MPI_Reduce_local(recvbuf, tempbuf, count, type, op);)"}
{"predict": "(11, MPI, MPI_Comm_size(comm, &size);); (24, MPI, MPI_Allgather(bufin, count, dtype, bufout, count, dtype, comm);); (15, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(10, MPI, MPI_Comm_size(comm, &size);); (15, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (22, MPI, MPI_Allreduce(bufin, bufout, count, dtype, MPI_SUM, comm);)"}
{"predict": "(66, CUDA, checkCudaErrors(cudaMemset(d_Kernel, 0, DATA_SIZE));); (110, CUDA, checkCudaErrors(cudaFree(d_Kernel));); (42, CUDA, findCudaDevice(argc, (const char **)argv);); (71, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (49, CUDA, checkCudaErrors(cudaMalloc((void **)&d_Kernel, DATA_SIZE));); (107, CUDA, checkCudaErrors(cudaFree(d_Data));); (46, CUDA, checkCudaErrors(cudaMalloc((void **)&d_Data, DATA_SIZE));)", "label": "(68, CUDA, checkCudaErrors(cudaMemset(d_Kernel, 0, DATA_SIZE));); (110, CUDA, checkCudaErrors(cudaFree(d_Kernel));); (40, CUDA, findCudaDevice(argc, (const char **)argv);); (72, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (49, CUDA, checkCudaErrors(cudaMalloc((void **)&d_Kernel, DATA_SIZE));); (105, CUDA, checkCudaErrors(cudaFree(d_Data));); (48, CUDA, checkCudaErrors(cudaMalloc((void **)&d_Data, DATA_SIZE));)"}
{"predict": "(33, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (77, MPI, double end_read = MPI_Wtime();); (40, MPI, MPI_Bcast(&version, 1, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (43, MPI, MPI_Error_string(mpirc, mpierrstr, &mpierrlen);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (19, MPI, MPI_Error_string(mpirc, mpierrstr, &mpierrlen);); (67, MPI, mpirc = MPI_File_close(&fh);); (28, MPI, MPI_File_set_view(fh, disp, MPI_UINT64_T, MPI_UINT64_T, datarep, MPI_INFO_NULL);); (17, MPI, MPI_File_open(MPI_COMM_WORLD, (char*)name, amode, MPI_INFO_NULL, &fh);); (31, MPI, MPI_Bcast(&version_packed, 1, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (76, MPI, double start_read = MPI_Wtime();); (2, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(39, MPI, MPI_Error_string(mpirc, mpierrstr, &mpierrlen);); (98, MPI, MPI_Barrier(MPI_COMM_WORLD);); (22, MPI, int mpirc = MPI_File_open(MPI_COMM_WORLD, (char*)name, amode, MPI_INFO_NULL, &fh);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (79, MPI, double end_read = MPI_Wtime();); (3, MPI, double start_read = MPI_Wtime();); (67, MPI, mpirc = MPI_File_close(&fh);); (30, MPI, mpirc = MPI_File_set_view(fh, disp, MPI_BYTE, MPI_BYTE, datarep, MPI_INFO_NULL);); (47, MPI, MPI_Bcast(&version, 1, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (36, MPI, mpirc = MPI_File_read_at(fh, 0, &version_packed, 8, MPI_BYTE, &status);)"}
{"predict": "(147, MPI, MPI_Allreduce(&tmp_error, error_code, 1, MPI_INT, MPI_MAX, fd->comm);); (67, MPI, MPI_Comm_size(fd->comm, &nprocs);); (92, MPI, MPI_Allgather(&end_offset, 1, ADIO_OFFSET, end_offsets, 1, ADIO_OFFSET, fd->comm);); (92, MPI, MPI_Allgather(&start_offset, 1, ADIO_OFFSET, st_offsets, 1, ADIO_OFFSET, fd->comm);); (64, MPI, MPI_Comm_rank(fd->comm, &myrank);); (137, MPI, MPI_Type_size_x(datatype, &size);)", "label": "(191, MPI, MPI_Allreduce(&tmp_error, error_code, 1, MPI_INT, MPI_MAX, fd->comm);); (65, MPI, MPI_Comm_size(fd->comm, &nprocs);); (93, MPI, MPI_Allgather(&end_offset, 1, ADIO_OFFSET, end_offsets, 1, ADIO_OFFSET, fd->comm);); (91, MPI, MPI_Allgather(&start_offset, 1, ADIO_OFFSET, st_offsets, 1, ADIO_OFFSET, fd->comm);); (63, MPI, MPI_Comm_rank(fd->comm, &myrank);); (210, MPI, MPI_Type_size_x(datatype, &size);)"}
{"predict": "(16, CUDA, CHECK(cudaMalloc(&atom->g_N, sizeof(real)));); (23, CUDA, CHECK(cudaMalloc(&atom->g_pe, sizeof(real)));); (19, CUDA, CHECK(cudaMalloc(&atom->g_fx, sizeof(real)));); (20, CUDA, CHECK(cudaMalloc(&atom->g_z, 6 * sizeof(real)));); (20, CUDA, CHECK(cudaMalloc(&atom->g_vx, N * sizeof(real)));); (21, CUDA, CHECK(cudaMalloc(&atom->g_fy, sizeof(real)));); (17, CUDA, CHECK(cudaMalloc(&atom->g_x, N * sizeof(real)));); (20, CUDA, CHECK(cudaMalloc(&atom->g_vz, N * sizeof(real)));); (17, CUDA, CHECK(cudaMalloc(&atom->g_y, N * sizeof(real)));); (20, CUDA, CHECK(cudaMalloc(&atom->g_fz, sizeof(real)));); (15, CUDA, CHECK(cudaMalloc(&atom->g_MN, sizeof(real)));); (18, CUDA, CHECK(cudaMalloc(&atom->g_vy, N * sizeof(real)));); (15, CUDA, CHECK(cudaMalloc(&atom->g_NN, sizeof(real)));); (20, CUDA, CHECK(cudaMalloc(&atom->g_fy, sizeof(real)));); (15, CUDA, CHECK(cudaMalloc(&atom->g_y, N * sizeof(real)));); (16, CUDA, CHECK(cudaMalloc(&atom->g_z, 3 * sizeof(real)));); (17, CUDA, CHECK(cudaMalloc(&atom->g_vx, N * sizeof(real)));); (14, CUDA, CHECK(cudaMalloc(&atom->g_x, 3 * sizeof(real)));); (17, CUDA, CHECK(cudaMalloc(&atom->g_vy, N * sizeof(real)));); (14, CUDA, CHECK(cudaMalloc(&atom->g_y, 3 * sizeof(real)));); (15, CUDA, CHECK(cudaMalloc(&atom->g_z, 6 * sizeof(real)));); (14, CUDA, CHECK(cudaMalloc(&atom->g_x, N * sizeof(real)));); (15, CUDA, CHECK(cudaMalloc(&atom->g_vz, N * sizeof(real)));); (16, CUDA, CHECK(cudaMalloc(&atom->g_fz, sizeof(real)));); (13, CUDA, CHECK(cudaMalloc(&atom->g_fx, sizeof(real)));); (16, CUDA, CHECK(cudaMalloc(&atom->g_vx", "label": "(23, CUDA, CHECK(cudaMalloc((void**)&atom->g_fz, sizeof(real) * N));); (19, CUDA, CHECK(cudaMalloc((void**)&atom->g_z, sizeof(real) * N));); (17, CUDA, CHECK(cudaMalloc((void**)&atom->g_y, sizeof(real) * N));); (21, CUDA, CHECK(cudaMalloc((void**)&atom->g_pe, sizeof(real) * N));); (14, CUDA, CHECK(cudaMalloc((void**)&atom->g_x, sizeof(real) * N));); (12, CUDA, CHECK(cudaMalloc((void**)&atom->g_NL, sizeof(int) * N * MN));); (16, CUDA, CHECK(cudaMalloc((void**)&atom->g_fy, sizeof(real) * N));); (9, CUDA, CHECK(cudaMalloc((void**)&atom->g_NN, sizeof(int) * N));); (13, CUDA, CHECK(cudaMalloc((void**)&atom->g_fx, sizeof(real) * N));)"}
{"predict": "(25, OpenCL, err = clEnqueueWriteBuffer( queue, buf[1], CL_TRUE, 0, sizeof(int), &dim_arg_global, 0, NULL, NULL);)", "label": "(25, OpenCL, err = clEnqueueWriteBuffer( queue, buf[1], CL_TRUE, 0, sizeof(int), &dim_arg_global, 0, NULL, NULL);)"}
{"predict": "(103, OpenMP, #pragma omp for firstprivate(numElemReg)); (36, OpenMP, #pragma omp for nowait firstprivate(numElemReg)); (50, OpenMP, #pragma omp for firstprivate(numElemReg, rho0, q_cut, e_cut, rho0, numElemReg))", "label": "(106, OpenMP, #pragma omp parallel for firstprivate(numElemReg)); (53, OpenMP, #pragma omp for firstprivate(numElemReg)); (65, OpenMP, #pragma omp for nowait firstprivate(numElemReg, eosvmin)); (34, OpenMP, #pragma omp parallel); (74, OpenMP, #pragma omp for nowait firstprivate(numElemReg, eosvmax)); (36, OpenMP, #pragma omp for nowait firstprivate(numElemReg))"}
{"predict": "(13, CUDA, PYCUDA_ASSERT_NOGIL(cudaStreamCreate(&stream, flags | cudaStreamCreatePortable));)", "label": "(13, CUDA, PYCUDA_ASSERT_NOGIL(cudaStreamCreateWithPriority(&stream, flags, priority));)"}
{"predict": "(38, CUDA, cudaFree(res_dev);); (31, CUDA, cudaMalloc(&res_dev, sizeof(float));); (33, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)", "label": "(38, CUDA, cudaFree(res_dev);); (31, CUDA, cudaMalloc(&res_dev, sizeof(float));); (33, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(13, MPI, MPI_Comm_size(comm, &mpi_size);); (45, MPI, rc = MPI_File_open(comm, fd->filename, amode, fd->fs_info, &cont->fh);)", "label": "(40, MPI, MPI_Error_class(*error_code, &rc);); (12, MPI, MPI_Comm_size(comm, &mpi_size);); (42, MPI, MPI_Bcast(&rc, 1, MPI_INT, 0, comm);)"}
{"predict": "(28, CUDA, Tensor tensor_S = make_tensor(make_gmem_ptr(thrust::raw_pointer_cast(d_S.data())), make_layout(tensor_shape));); (28, CUDA, Tensor tensor_D = make_tensor(make_gmem_ptr(thrust::raw_pointer_cast(d_D.data())), make_layout(tensor_shape));); (29, CUDA, cudaDeviceSynchronize();); (79, CUDA, cudaError_t result = cudaGetLastError();); (15, CUDA, thrust::host_vector<Element> h_S(size_t);); (17, CUDA, thrust::host_vector<Element> h_D(size_t);); (23, CUDA, thrust::device_vector<Element> d_D = h_D;); (77, CUDA, std::cerr << \"CUDA Runtime error: \" << cudaGetErrorString(result) << std::endl;); (20, CUDA, thrust::device_vector<Element> d_S = h_S;)", "label": "(81, CUDA, cudaError result = cudaDeviceSynchronize();); (30, CUDA, Tensor tensor_S = make_tensor(make_gmem_ptr(thrust::raw_pointer_cast(d_S.data())), make_layout(tensor_shape));); (14, CUDA, thrust::host_vector<Element> h_S(size(tensor_shape));); (22, CUDA, thrust::device_vector<Element> d_D = h_D;); (79, CUDA, std::cerr << \"CUDA Runtime error: \" << cudaGetErrorString(result) << std::endl;); (12, CUDA, thrust::host_vector<Element> h_D(size(tensor_shape));); (26, CUDA, Tensor tensor_D = make_tensor(make_gmem_ptr(thrust::raw_pointer_cast(d_D.data())), make_layout(tensor_shape));); (17, CUDA, thrust::device_vector<Element> d_S = h_S;)"}
{"predict": "(2, CUDA, __shared__ float4 siftParts1[17*16];); (2, CUDA, __shared__ float4 siftParts2[16*16];)", "label": "(3, CUDA, __shared__ float siftParts2[16*64];); (1, CUDA, __shared__ float siftParts1[17*64];)"}
{"predict": "(12, TBB, my_thread_array = tbb::cache_aligned_allocator<padded_private_worker>().allocate( my_n_thread );)", "label": "(12, TBB, my_thread_array = tbb::cache_aligned_allocator<padded_private_worker>().allocate( my_n_thread );)"}
{"predict": "(14, CUDA, __shared__ float sSumVal;)", "label": "(52, CUDA, __shared__ bool isAccepted;); (52, CUDA, __shared__ T sSumVal;)"}
{"predict": "(10, MPI, MPI_Alloc_mem(sizeof(double) * nproc * MAXELEMS, MPI_INFO_NULL, &dst_buf);); (54, MPI, MPI_Free_mem(dst_buf);); (34, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, target, 0, dst_win);); (17, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, me, 0, dst_win);); (25, MPI, MPI_Win_unlock(me, dst_win);); (29, MPI, MPI_Win_flush(dst_buf, dst_win);); (25, MPI, MPI_Win_flush(target, dst_win);); (18, MPI, MPI_Win_unlock(target, dst_win);); (7, MPI, MPI_Win_set(dst_buf, 0, sizeof(double) * nproc * MAXELEMS);); (50, MPI, MPI_Win_free(&dst_win);); (17, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(54, MPI, MPI_Free_mem(dst_buf);); (9, MPI, MPI_Alloc_mem(sizeof(double) * nproc * MAXELEMS, MPI_INFO_NULL, &dst_buf);); (30, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, target, 0, dst_win);); (50, MPI, MPI_Win_free(&dst_win);); (13, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, me, 0, dst_win);); (17, MPI, MPI_Win_unlock(me, dst_win);); (18, MPI, MPI_Barrier(MPI_COMM_WORLD);); (28, MPI, MPI_Win_unlock(target, dst_win);)"}
{"predict": "(10, OpenMP, int numThreads = omp_get_max_threads();); (7, OpenMP, #pragma omp parallel); (12, OpenMP, omp_set_lock(&l);); (7, OpenMP, #pragma omp single); (22, OpenMP, omp_unset_lock(&l);); (1, OpenMP, omp_set_num_threads(numThreads);); (5, OpenMP, omp_init_lock(&l);)", "label": "(7, OpenMP, omp_init_lock(&l);); (27, OpenMP, omp_destroy_lock(&l);); (7, OpenMP, #pragma omp parallel); (14, OpenMP, omp_set_lock(&l);); (21, OpenMP, omp_unset_lock(&l);); (-1, OpenMP, auto numThreads = omp_get_max_threads();)"}
{"predict": "(38, OpenMP, #pragma omp target parallel for simd private(h, B::x)); (9, OpenMP, #pragma omp target parallel for simd private(); (53, OpenMP, #pragma omp target parallel for simd private(m)); (31, OpenMP, #pragma omp target parallel for simd private(argv[1])); (14, OpenMP, #pragma omp target parallel for simd private(argc,); (36, OpenMP, #pragma omp target parallel for simd private(i)); (23, OpenMP, #pragma omp target parallel for simd private(a, b)); (19, OpenMP, #pragma omp target parallel for simd private(argc)); (43, OpenMP, #pragma omp target parallel for simd private(j)); (8, OpenMP, #pragma omp target parallel for simd private(argc); (51, OpenMP, #pragma omp target parallel for simd private(i)); (0, OpenMP, #pragma omp target parallel for simd private()); (37, OpenMP, #pragma omp parallel shared(i)); (23, OpenMP, #pragma omp target parallel for simd private(e, g)); (27, OpenMP, #pragma omp target parallel for simd private(k, z)); (13, OpenMP, #pragma omp target parallel for simd private(argc)); (19, OpenMP, #pragma omp target parallel for simd private(S1)); (36, OpenMP, #pragma omp parallel private(i)); (-14, OpenMP, #pragma omp target parallel for simd private); (3, OpenMP, #pragma omp target parallel for simd private(argc > 0? argv[1] : argv[2])); (-8, OpenMP, #pragma omp target parallel for simd private(argc)", "label": "(22, OpenMP, #pragma omp target parallel for simd private(argc > 0? argv[1] : argv[2])); (36, OpenMP, #pragma omp target parallel for simd private(e, g)); (44, OpenMP, #pragma omp target parallel for simd private(i)); (49, OpenMP, #pragma omp target parallel for simd private(j)); (47, OpenMP, #pragma omp parallel private(i)); (54, OpenMP, #pragma omp target parallel for simd private(m)); (4, OpenMP, #pragma omp target parallel for simd private(); (36, OpenMP, #pragma omp parallel); (32, OpenMP, #pragma omp target parallel for simd private(h, B::x)); (22, OpenMP, #pragma omp target parallel for simd private(a, b)); (24, OpenMP, #pragma omp target parallel for simd private(argv[1])); (17, OpenMP, #pragma omp target parallel for simd private(S1)); (38, OpenMP, #pragma omp parallel shared(i)); (12, OpenMP, #pragma omp target parallel for simd private(argc, z)); (2, OpenMP, #pragma omp target parallel for simd private(argc); (-2, OpenMP, #pragma omp target parallel for simd private()); (-9, OpenMP, #pragma omp target parallel for simd private); (2, OpenMP, #pragma omp target parallel for simd private(argc,)"}
{"predict": "(81, MPI, MPI_Finalize();)", "label": "(81, MPI, MPI_Finalize();)"}
{"predict": "(12, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (12, MPI, MPI_Iscan_c(sendbuf, recvbuf, count, datatype, MPI_SUM, comm, &req);); (8, MPI, MPI_Exscan_c(sendbuf, recvbuf, count, datatype, MPI_SUM, comm);)", "label": "(9, MPI, MPI_Scan_c(sendbuf, recvbuf, count, datatype, MPI_SUM, comm);); (12, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (10, MPI, MPI_Iscan_c(sendbuf, recvbuf, count, datatype, MPI_SUM, comm, &req);)"}
{"predict": "(19, OpenMP, #pragma omp task shared (argc)); (6, OpenMP, #pragma omp task shared); (31, OpenMP, #pragma omp task shared(i)); (19, OpenMP, #pragma omp task shared (S1)); (36, OpenMP, #pragma omp task shared(j)); (7, OpenMP, #pragma omp task shared (argc); (24, OpenMP, #pragma omp task shared(ba)); (17, OpenMP, #pragma omp task shared (a, b, c, d, f)); (4, OpenMP, #pragma omp task shared ()); (19, OpenMP, #pragma omp task shared (argv[1])); (32, OpenMP, #pragma omp task shared(i, z)); (5, OpenMP, #pragma omp task shared (argc > 0? argv[1] : argv[2])); (18, OpenMP, #pragma omp task shared(h, B::x)); (21, OpenMP, #pragma omp task shared(e, g)); (0, OpenMP, #pragma omp task shared (argc); (23, OpenMP, #pragma omp task private(i), shared(i)); (8, OpenMP, #pragma omp task shared(ca)); (23, OpenMP, #pragma omp task shared(j)); (-5, OpenMP, #pragma omp task shared (argc,); (2, OpenMP, #pragma omp task shared (argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (-8, OpenMP, #pragma omp task shared (); (-8, OpenMP, #pragma omp task shared (argc)", "label": "(45, OpenMP, #pragma omp parallel firstprivate(i)); (40, OpenMP, #pragma omp parallel private(i)); (17, OpenMP, #pragma omp task shared(argc, z)); (39, OpenMP, #pragma omp task shared(i)); (7, OpenMP, #pragma omp task shared()); (20, OpenMP, #pragma omp task shared(argv[1])); (29, OpenMP, #pragma omp task shared(h, B::x)); (26, OpenMP, #pragma omp task shared(e, g)); (31, OpenMP, #pragma omp task firstprivate(i), shared(i)); (34, OpenMP, #pragma omp task shared(j)); (17, OpenMP, #pragma omp task shared(ba)); (6, OpenMP, #pragma omp task shared(argc > 0? argv[1] : argv[2])); (-5, OpenMP, #pragma omp task shared); (-4, OpenMP, #pragma omp task shared(); (23, OpenMP, #pragma omp task private(i), shared(i)); (0, OpenMP, #pragma omp task shared(argc,); (7, OpenMP, #pragma omp task shared(a, b, c, d, f)); (4, OpenMP, #pragma omp task shared(S1)); (13, OpenMP, #pragma omp task shared(da)); (-6, OpenMP, #pragma omp task shared(argc); (9, OpenMP, #pragma omp task shared(ca))"}
{"predict": "(37, OpenMP, #pragma omp target exit data map(from: s2::S2sc)); (22, OpenMP, #pragma omp target exit data map(from: s2::S2s)); (3, OpenMP, #pragma omp target exit data map(from: s2::S2sc)); (30, OpenMP, #pragma omp target exit data map(from: s2::S2s)); (45, OpenMP, #pragma omp target exit data map(from: s2::S2sc) nowait); (36, OpenMP, #pragma omp target exit data map(from: s2::S2s) nowait); (13, OpenMP, #pragma omp target exit data map(from: s2::S2sc)); (40, OpenMP, #pragma omp target exit data map(from: s2::S2sc) nowait depend(inout: s2::S2sc)); (31, OpenMP, #pragma omp target exit data map(from: s2::S2s) nowait depend(inout: s2::S2s)); (14, OpenMP, #pragma omp target exit data map(from: s2::S2sc)); (15, OpenMP, #pragma omp target exit data map(from: s2::S2s)); (22, OpenMP, #pragma omp target exit data map(from: s2::S2sc)); (17, OpenMP, #pragma omp target exit data map(from: s2::S2s)); (24, OpenMP, #pragma omp target exit data map(from: s2::S2sc)); (39, OpenMP, #pragma omp target exit data map(from: s2::S2sc) nowait depend(inout: s2::S2sc) nowait); (46, OpenMP, #pragma omp target exit data map(from: s2::S2sc) nowait depend(inout: s2::S2sc) nowait depend(in: s2::S2sc)); (29, OpenMP, #pragma omp target exit data map(from: s2::S2s) nowait depend(inout: s2::S2s)); (36, OpenMP, #pragma omp target exit data map(from: s2::S2sc) nowait depend(inout: s2::S2sc) nowait depend(in: s2::S2sc)); (49, OpenMP", "label": "(14, OpenMP, #pragma omp target map(p)); (51, OpenMP, #pragma omp target map(from: ps->ps->ps)); (38, OpenMP, #pragma omp target map(ps->i)); (31, OpenMP, #pragma omp target map(s.ps->ps->ps)); (32, OpenMP, #pragma omp target map(to: s.ps->ps->s.f[:22])); (27, OpenMP, #pragma omp target map(to: s.ps->ps)); (20, OpenMP, #pragma omp target map(to: s.p[:22])); (17, OpenMP, #pragma omp target map(s.p)); (2, OpenMP, #pragma omp target map(i)); (37, OpenMP, #pragma omp target map(to: ps->p[:22])); (18, OpenMP, #pragma omp target map(s.ps)); (27, OpenMP, #pragma omp target map(ps)); (30, OpenMP, #pragma omp target map(ps->s.f)); (41, OpenMP, #pragma omp target map(ps->ps->ps->ps)); (8, OpenMP, #pragma omp target map(s.s.f)); (-3, OpenMP, #pragma omp target map(i[1:23])); (-8, OpenMP, #pragma omp target map(d)); (3, OpenMP, #pragma omp target map(s.i)); (26, OpenMP, #pragma omp target map(from: ps->p)); (41, OpenMP, #pragma omp target map(from: s.f[:22]) map(to: ps->p[:33])); (38, OpenMP, #pragma omp target map(to: s.f[:22]) map(from: s.p[:33])); (41, OpenMP, #pragma omp target map(from: s.f[:22], s.s) map(to: ps->p[:33])); (-4, OpenMP, #pragma omp target map(s)); (25, OpenMP, #pragma omp target map(ps->ps)); (6, OpenMP, #pragma omp target map(from: s.ps->s.i)); (31, OpenMP, #pragma omp target map(to: ps->ps->ps->s.f[:22])); (-10, OpenMP, #pragma omp target map(p[1:24])); (23, OpenMP, #pragma omp target map(from: ps->ps->s.i))"}
{"predict": "(15, CUDA, getLastCudaError(\"generateSampleRanksKernel<1> failed\\n\");); (19, CUDA, getLastCudaError(\"generateSampleRanksKernel<0> failed\\n\");)", "label": "(15, CUDA, getLastCudaError(\"generateSampleRanksKernel<1U><<<>>> failed\\n\");); (19, CUDA, getLastCudaError(\"generateSampleRanksKernel<0U><<<>>> failed\\n\");)"}
{"predict": "(13, MPI, MPI_Comm_size(comm, &ranks);); (11, MPI, MPI_Comm_rank(comm, &rank);)", "label": "(13, MPI, MPI_Comm_size(comm, &ranks);); (11, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(8, OpenCL, cl_kernel init_kernel = clCreateKernel(program, \"kernel_init\", &rc); CLERR;); (23, OpenCL, rc = clSetKernelArg(check_kernel, 2, sizeof(cl_mem), &mc->err_count); CLERR;); (24, OpenCL, rc = clSetKernelArg(check_kernel, 4, sizeof(cl_mem), &mc->err_expect); CLERR;); (25, OpenCL, rc = clSetKernelArg(check_kernel, 5, sizeof(cl_mem), &mc->err_current); CLERR;); (26, OpenCL, rc = clEnqueueNDRangeKernel(queue, check_kernel, 1, NULL, global_work_size, local_work_size, 0, NULL, NULL);CLERR;); (5, OpenCL, cl_kernel check_kernel = clCreateKernel(program, \"kernel_check\", &rc); CLERR;); (26, OpenCL, rc = clSetKernelArg(check_kernel, 7, sizeof(cl_mem), &mc->err_second_read); CLERR;); (11, OpenCL, rc = clSetKernelArg(check_kernel, 0, sizeof(cl_mem), &mc->device_mem); CLERR;); (16, OpenCL, rc = clSetKernelArg(check_kernel, 3, sizeof(cl_mem), &mc->err_addr); CLERR;); (11, OpenCL, rc = clSetKernelArg(check_kernel, 1, sizeof(cl_ulong), &mc->mem_size); CLERR;); (12, OpenCL, rc = clSetKernelArg(check_kernel, 2, sizeof(cl_mem), &mc->err_count); CLERR;); (13, OpenCL, rc = clSetKernelArg(check_kernel, 4, sizeof(cl_mem), &mc->err_expect); CLERR;); (14, OpenCL, rc = clSetKernelArg(check_kernel, 5, sizeof(cl_mem), &mc->err_current); CLERR;); (9, OpenCL, rc = clSetKernelArg(check_kernel, 3, sizeof(cl_mem), &mc->err_addr); CLERR;); (12, OpenCL, rc = clEnqueueNDRangeKernel(queue, check_kernel, 1, NULL, global_work_size, local_work_size, 0, NULL, NULL);CLERR;); (3, OpenCL, rc = clSetKernelArg(init_kernel, 1, sizeof", "label": "(18, OpenCL, rc = clEnqueueNDRangeKernel(queue, write_kernel, 1, NULL, global_work_size, local_work_size, 0, NULL, NULL); CLERR;); (23, OpenCL, rc = clSetKernelArg(read_kernel, 4, sizeof(TYPE), &p2); CLERR;); (35, OpenCL, clReleaseKernel(read_kernel);); (28, OpenCL, rc = clEnqueueNDRangeKernel(queue, read_kernel, 1, NULL, global_work_size, local_work_size, 0, NULL, NULL);CLERR;); (5, OpenCL, cl_kernel read_kernel = clCreateKernel(program, \"kernel_modtest_read\", &rc); CLERR;); (20, OpenCL, rc = clSetKernelArg(read_kernel, 5, sizeof(cl_mem), &mc->err_count); CLERR;); (14, OpenCL, rc = clSetKernelArg(read_kernel, 0, sizeof(cl_mem), &mc->device_mem); CLERR;); (22, OpenCL, rc = clSetKernelArg(read_kernel, 9, sizeof(cl_mem), &mc->err_second_read); CLERR;); (0, OpenCL, cl_kernel write_kernel = clCreateKernel(program, \"kernel_modtest_write\", &rc); CLERR;); (8, OpenCL, rc = clSetKernelArg(write_kernel, 4, sizeof(TYPE), &p2); CLERR;); (11, OpenCL, rc = clSetKernelArg(read_kernel, 1, sizeof(cl_ulong), &mc->mem_size); CLERR;); (3, OpenCL, rc = clSetKernelArg(write_kernel, 1, sizeof(cl_ulong), &mc->mem_size); CLERR;); (14, OpenCL, rc = clSetKernelArg(read_kernel, 6, sizeof(cl_mem), &mc->err_addr); CLERR;); (0, OpenCL, rc = clSetKernelArg(write_kernel, 0, sizeof(cl_mem), &mc->device_mem); CLERR;); (22, OpenCL, clReleaseKernel(write_kernel);); (7, OpenCL, rc = clSetKernelArg(read_kernel, 2, sizeof(unsigned int), &offset); CLERR;); (0, OpenCL, rc = clSetKernelArg(write_kernel, 3, sizeof(TYPE), &p1); CLERR;); (6, OpenCL, rc = clSetKernelArg(read_kernel, 3, sizeof(TYPE), &p1); CLERR;); (-3, OpenCL, rc = clSetKernelArg(write_kernel, 2, sizeof(unsigned int), &offset); CLERR;); (9, OpenCL, rc = clSetKernelArg(read_kernel, 8, sizeof(cl_mem), &mc->err_current); CLERR;); (7, OpenCL, rc = clSetKernelArg(read_kernel, 7, sizeof(cl_mem), &mc->err_expect); CLERR;)"}
{"predict": "(83, MPI, MPI_Bcast(&btype,         1, MPI_INTEGER, root, MPI_COMM_WORLD);); (238, MPI, MPI_Finalize();); (83, MPI, MPI_Bcast(&iterations,    1, MPI_INTEGER, root, MPI_COMM_WORLD);); (217, MPI, MPI_Reduce(&total, &total_sum, 1, MPI_INTEGER, MPI_SUM, root, MPI_COMM_WORLD);); (24, MPI, MPI_Init(&argc,&argv);); (98, MPI, MPI_Barrier(MPI_COMM_WORLD);); (24, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (77, MPI, MPI_Bcast(&vector_length, 1, MPI_INTEGER, root, MPI_COMM_WORLD);); (21, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)", "label": "(211, MPI, MPI_Reduce(&total, &total_sum, 1, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD);); (234, MPI, MPI_Finalize();); (83, MPI, MPI_Bcast(&iterations,    1, MPI_INT, root, MPI_COMM_WORLD);); (25, MPI, MPI_Init(&argc,&argv);); (80, MPI, MPI_Bcast(&vector_length, 1, MPI_INT, root, MPI_COMM_WORLD);); (81, MPI, MPI_Bcast(&btype,         1, MPI_INT, root, MPI_COMM_WORLD);); (97, MPI, MPI_Barrier(MPI_COMM_WORLD);); (23, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (21, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(24, MPI, MPI_Alltoall(recv_size, 1, MPI_COUNT, send_size, 1, MPI_COUNT, fd->comm);); (80, MPI, MPI_Waitall(nprocs_recv, send_req, statuses);); (24, MPI, MPI_Waitall(2, send_req, &status);); (80, MPI, MPI_Waitall(2, recv_req, &status);); (19, MPI, MPI_Send(&send_size[0], 1, MPI_COUNT, 0, 0, fd->comm);); (17, MPI, MPI_Recv(&recv_size[0], 1, MPI_COUNT, 0, 0, fd->comm, &status);)", "label": "(121, MPI, gpfsmpio_prof_cw[GPFSMPIO_CIO_T_DEXCH_SORT] += MPI_Wtime() - io_time;); (90, MPI, gpfsmpio_prof_cw[GPFSMPIO_CIO_T_DEXCH_NET] += MPI_Wtime() - io_time;); (23, MPI, int ret = MPI_Alltoall(recv_size, 1, MPI_COUNT, send_size, 1, MPI_COUNT, fd->comm);); (19, MPI, io_time = MPI_Wtime();); (24, MPI, gpfsmpio_prof_cw[GPFSMPIO_CIO_T_DEXCH_RECV_EXCH] += MPI_Wtime() - io_time;); (132, MPI, gpfsmpio_prof_cw[GPFSMPIO_CIO_T_DEXCH_SIEVE] += MPI_Wtime() - io_time;); (75, MPI, gpfsmpio_prof_cw[GPFSMPIO_CIO_T_DEXCH_SETUP] += MPI_Wtime() - io_time;)"}
{"predict": "(9, CUDA, __shared__ float v[_N_], gy[_N_];)", "label": "(10, CUDA, __shared__ float v[_N_], gy[_N_];)"}
{"predict": "(14, CUDA, cudaDeviceSynchronize();); (12, CUDA, cudaMemcpy(&count,listp+fluidnum,sizeof(unsigned),cudaMemcpyDeviceToHost);); (6, CUDA, cudaMemset(listp+fluidnum,0,sizeof(unsigned));)", "label": "(14, CUDA, cudaDeviceSynchronize();); (12, CUDA, cudaMemcpy(&count,listp+fluidnum,sizeof(unsigned),cudaMemcpyDeviceToHost);); (6, CUDA, cudaMemset(listp+fluidnum,0,sizeof(unsigned));)"}
{"predict": "(6, CUDA, st = cudaMalloc(&test_ptr, TEST_ALLOC_SIZE);); (26, CUDA, cudaFree(test_ptr);)", "label": "(6, CUDA, st = cudaMalloc(&test_ptr, TEST_ALLOC_SIZE);); (25, CUDA, cudaFree(test_ptr);)"}
{"predict": "(44, MPI, MPI_Buffer_detach(&bptr, &bl);); (28, MPI, MPI_Recv(b, 10, MPI_INT, i, 27 + j, MPI_COMM_WORLD, &status);); (14, MPI, MPI_Bsend(a, 10, MPI_INT, 0, 27 + j, MPI_COMM_WORLD);); (5, MPI, MPI_Buffer_attach(buf, BUFSIZE);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (18, MPI, MPI_Wait(&request, &status);); (13, MPI, MPI_Irecv(b, 10, MPI_INT, i, 27 + j, MPI_COMM_WORLD, &request);)", "label": "(45, MPI, MPI_Buffer_detach(&bptr, &bl);); (25, MPI, MPI_Recv(b, 10, MPI_INT, i, 27 + j, MPI_COMM_WORLD, &status);); (8, MPI, MPI_Buffer_attach(buf, BUFSIZE);); (14, MPI, MPI_Wait(&request, &status);); (12, MPI, MPI_Ibsend(a, 10, MPI_INT, 0, 27 + j, MPI_COMM_WORLD, &request);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)"}
{"predict": "(61, OpenMP, #pragma omp for lastprivate(i)); (69, OpenMP, #pragma omp parallel private(i)); (42, OpenMP, #pragma omp for lastprivate(argv[1])); (4, OpenMP, #pragma omp for lastprivate); (20, OpenMP, #pragma omp for lastprivate(argc > 0? argv[1] : argv[2])); (52, OpenMP, #pragma omp for lastprivate(h)); (36, OpenMP, #pragma omp for lastprivate(conditional: s,argc) lastprivate(conditional:); (42, OpenMP, #pragma omp for lastprivate(e, g)); (19, OpenMP, #pragma omp for lastprivate(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (24, OpenMP, #pragma omp for lastprivate(argc) allocate(omp_thread_mem_alloc: argc)); (46, OpenMP, #pragma omp for lastprivate(i)); (29, OpenMP, #pragma omp for lastprivate(foo:argc)); (0, OpenMP, #pragma omp for lastprivate(); (-5, OpenMP, #pragma omp parallel); (25, OpenMP, #pragma omp for lastprivate(conditional: argc,s) lastprivate(conditional:); (29, OpenMP, #pragma omp for lastprivate(k)); (4, OpenMP, #pragma omp for lastprivate(argc); (59, OpenMP, #pragma omp for lastprivate(j)); (14, OpenMP, #pragma omp for lastprivate(argc) allocate(omp_default_mem_alloc: argc)); (43, OpenMP, #pragma omp parallel shared(i)); (46, OpenMP, #pragma omp for lastprivate)", "label": "(32, OpenMP, #pragma omp for lastprivate(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (73, OpenMP, #pragma omp parallel private(i)); (22, OpenMP, #pragma omp for lastprivate(argc,); (45, OpenMP, #pragma omp for lastprivate(a, b)); (12, OpenMP, #pragma omp for lastprivate()); (2, OpenMP, #pragma omp parallel); (69, OpenMP, #pragma omp for lastprivate(j)); (60, OpenMP, #pragma omp for lastprivate(i)); (4, OpenMP, #pragma omp for lastprivate(); (11, OpenMP, #pragma omp for lastprivate(argc); (30, OpenMP, #pragma omp for lastprivate(foo:argc)); (-3, OpenMP, #pragma omp for lastprivate); (32, OpenMP, #pragma omp for lastprivate(conditional: argc,s) lastprivate(conditional:); (15, OpenMP, #pragma omp for lastprivate(argc > 0? argv[1] : argv[2])); (46, OpenMP, #pragma omp for lastprivate(h)); (37, OpenMP, #pragma omp for lastprivate(argv[1])); (40, OpenMP, #pragma omp for lastprivate(k, e, g)); (56, OpenMP, #pragma omp parallel shared(i)); (18, OpenMP, #pragma omp for lastprivate(S1))"}
{"predict": "(30, CUDA, cudaCheck(cudaGetLastError());)", "label": "(30, CUDA, cudaCheck(cudaGetLastError());)"}
{"predict": "(8, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)", "label": "(8, MPI, MPI_Finalize();); (0, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(55, CUDA, SemaRef.tryCaptureCUDAGlobal(Var, Loc, Sema::TryCapture_Implicit, \"device\");)", "label": "(25, CUDA, auto UserTarget = SemaRef.IdentifyCUDATarget(FD);); (23, CUDA, auto VarTarget = SemaRef.IdentifyCUDATarget(Var);)"}
{"predict": "(37, CUDA, CHECK_CUDA(cudaMalloc((void**)&imgs_dev, img_N * 3 * img_w * img_h * sizeof(uchar)));)", "label": "(35, CUDA, CHECK_CUDA(cudaMalloc((void**)&imgs_dev, img_N * 3 * img_w * img_h * sizeof(uchar)));)"}
{"predict": "(10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (78, MPI, MPI_Win_unlock(0, win);); (12, MPI, MPI_Win_create(data, sizeof(twoint_t)*2, sizeof(twoint_t), MPI_INFO_NULL, MPI_COMM_WORLD, &win);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (26, MPI, MPI_Win_fence(0, win);); (110, MPI, MPI_Win_free(&win);); (108, MPI, MPI_Barrier(MPI_COMM_WORLD);); (45, MPI, MPI_Win_unlock_all(win);); (15, MPI, MPI_Win_lock_all(0, win);); (15, MPI, MPI_Win_lock(MPI_LOCK_SHARED, 0, MPI_MODE_NOCHECK, win);); (25, MPI, MPI_Win_lock(0, win);); (16, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, 0, MPI_MODE_NOCHECK, win);); (16, MPI, MPI_Win_lock(MPI_LOCK_SHARED, 0, MPI_MODE_NOCHECK, win);); (30, MPI, MPI_Accumulate(&mine, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_MAXLOC, win);); (45, MPI, MPI_Accumulate(&mine, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_MINLOC, win);); (18, MPI, MPI_Win_unlock(0, win);); (1, MPI, MPI_Alloc_mem(sizeof(twoint_t)*2, MPI_INFO_NULL, &data);); (108, MPI, MPI_Free_mem(data);); (22, MPI, MPI_Win_lock(MPI_LOCK_SHARED, 0, MPI_MODE_NOCHECK, win);); (9, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, 0, MPI_MODE_NOCHECK, win);); (9, MPI, MPI_Win_lock(MPI_LOCK_SHARED, 0, MPI_MODE_NOCHECK, win);); (51, MPI, MPI_Win_lock_all(0, win);); (-10, MPI, MPI_MTest_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (69, MPI, MPI_Win_lock(MPI_LOCK_SHARED, 0, MPI_MODE", "label": "(119, MPI, MPI_Free_mem(data);); (30, MPI, MPI_Win_fence(0, win);); (73, MPI, MPI_Win_fence(MPI_MODE_NOSUCCEED, win);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (12, MPI, MPI_Win_fence(MPI_MODE_NOPRECEDE, win);); (110, MPI, MPI_Win_free(&win);); (28, MPI, MPI_Accumulate(&mine, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_MAXLOC, win);); (5, MPI, MPI_Alloc_mem(sizeof(twoint_t), MPI_INFO_NULL, &data);); (71, MPI, MPI_Win_lock(MPI_LOCK_SHARED, 0, MPI_MODE_NOCHECK, win);); (72, MPI, MPI_Win_unlock(0, win);); (73, MPI, MPI_Barrier(MPI_COMM_WORLD);); (4, MPI, MPI_Win_create(data, me == 0? sizeof(twoint_t) : 0, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &win);); (41, MPI, MPI_Accumulate(&mine, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_MINLOC, win);)"}
{"predict": "(5, MPI, MPI_Init(&argc, &argv);); (60, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (22, MPI, MPI_Send(&message, 1, MPI_INT, next, tag, MPI_COMM_WORLD);); (54, MPI, MPI_Send(&message, 1, MPI_INT, 0, tag, MPI_COMM_WORLD);); (43, MPI, MPI_Recv(&message, 1, MPI_INT, prev, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)", "label": "(6, MPI, MPI_Init(&argc, &argv);); (25, MPI, MPI_Send(&message, 1, MPI_INT, next, tag, MPI_COMM_WORLD);); (60, MPI, MPI_Finalize();); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (34, MPI, MPI_Recv(&message, 1, MPI_INT, prev, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)"}
{"predict": "(11, CUDA, CUCH(cudaMalloc(&ptr, plan_size * sizeof(int32_t)), mhcudaMallocError);)", "label": "(12, CUDA, CUCH(cudaMalloc(&ptr, plan_size * sizeof(int32_t)), mhcudaMemoryAllocationFailure);); (6, CUDA, CUCH(cudaSetDevice(devs[devi]), mhcudaNoSuchDevice);)"}
{"predict": "(9, MPI, MPI_Comm_rank(comm, &comm_rank);); (10, MPI, MPI_Comm_size(comm, &comm_size);)", "label": "(9, MPI, MPI_Comm_rank(comm, &comm_rank);); (10, MPI, MPI_Comm_size(comm, &comm_size);)"}
{"predict": "(50, SYCL, double *d_B = sycl::malloc_device<double>(length, q);); (52, SYCL, double *d_C = sycl::malloc_device<double>(length, q);); (106, SYCL, sycl::free(d_A, q);); (84, SYCL, sycl::free(d_B, q);); (84, SYCL, sycl::free(d_C, q);); (46, SYCL, double *d_A = sycl::malloc_device<double>(length, q);); (27, SYCL, prk::SYCL::print_device_platform(q);); (20, SYCL, sycl::queue q(sycl::default_selector_v);)", "label": "(80, SYCL, sycl::free(d_A, q);); (32, SYCL, prk::SYCL::print_device_platform(q);); (51, SYCL, double *d_C = sycl::malloc_device<double>(length, q);); (76, SYCL, sycl::free(d_B, q);); (74, SYCL, sycl::free(d_C, q);); (47, SYCL, double *d_B = sycl::malloc_device<double>(length, q);); (26, SYCL, sycl::queue q(sycl::default_selector_v);); (44, SYCL, double *d_A = sycl::malloc_device<double>(length, q);)"}
{"predict": "(151, CUDA, checkCudaRuntime(cudaMemcpyAsync(image_device, image_host, size_image, cudaMemcpyHostToDevice, preprocess_stream));); (127, CUDA, checkCudaRuntime(cudaStreamCreate(&preprocess_stream));); (67, CUDA, checkCudaRuntime(cudaStreamSynchronize(mono->get_stream()));)", "label": "(151, CUDA, checkCudaRuntime(cudaMemcpyAsync(image_device, image_host, size_image, cudaMemcpyHostToDevice, preprocess_stream));); (125, CUDA, checkCudaRuntime(cudaStreamCreate(&preprocess_stream));); (67, CUDA, checkCudaRuntime(cudaStreamSynchronize(mono->get_stream()));)"}
{"predict": "(84, CUDA, cudaDeviceSynchronize();); (12, CUDA, using namespace tr::CudaCheckError();)", "label": "(114, CUDA, EXPECT_EQ(cudaMemset(block2Ptr->data(), 0, blockSize * sizeof(T)), cudaSuccess);); (74, CUDA, EXPECT_EQ(cudaDeviceSynchronize(), cudaSuccess);)"}
{"predict": "(46, CUDA, cudaFree(gpuFloats3);); (43, CUDA, cudaFree(gpuFloats2);); (21, CUDA, cudaMemcpy(hostFloats2, gpuFloats2, 4 * sizeof(float), cudaMemcpyDeviceToHost);); (21, CUDA, cudaMemcpy(hostFloats3, gpuFloats3, 4 * sizeof(float), cudaMemcpyDeviceToHost);); (4, CUDA, cudaMalloc((void**)(&gpuFloats1), N * sizeof(float));); (23, CUDA, cudaDeviceSynchronize();); (39, CUDA, cudaFree(gpuFloats1);); (6, CUDA, cudaMalloc((void**)(&gpuFloats3), N * sizeof(float));); (3, CUDA, cudaMalloc((void**)(&gpuFloats2), N * sizeof(float));); (16, CUDA, cudaMemcpy(hostFloats1, gpuFloats1, 4 * sizeof(float), cudaMemcpyDeviceToHost);)", "label": "(25, CUDA, cudaMemcpy(hostFloats2, gpuFloats2, 4 * sizeof(float), cudaMemcpyDeviceToHost);); (37, CUDA, cudaFree(gpuFloats1);); (8, CUDA, cudaMalloc((void**)(&gpuFloats2), N * sizeof(float));); (21, CUDA, cudaMemcpy(hostFloats1, gpuFloats1, 4 * sizeof(float), cudaMemcpyDeviceToHost);); (3, CUDA, cudaMalloc((void**)(&gpuFloats1), N * sizeof(float));); (35, CUDA, cudaFree(gpuFloats3);); (20, CUDA, cudaMemcpy(hostFloats3, gpuFloats3, 4 * sizeof(float), cudaMemcpyDeviceToHost);); (32, CUDA, cudaFree(gpuFloats2);); (5, CUDA, cudaMalloc((void**)(&gpuFloats3), N * sizeof(float));)"}
{"predict": "(28, CUDA, CUDASimulation cudaSimulation(model);)", "label": "(28, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(9, CUDA, __shared__ double barrier;)", "label": "(9, CUDA, __shared__ cuda::barrier<cuda::thread_scope_block> barrier;)"}
{"predict": "(6, CUDA, auto points_geometry_it = thrust::make_counting_iterator(0);); (8, CUDA, auto linestring_geometry_it  = thrust::make_counting_iterator(0);)", "label": "(6, CUDA, auto points_geometry_it = thrust::make_counting_iterator(0);); (8, CUDA, auto linestring_geometry_it  = thrust::make_counting_iterator(0);)"}
{"predict": "(25, OpenMP, #pragma omp distribute parallel for collapse(3)); (26, OpenMP, #pragma omp target teams map (alloc: rsd, flux_G)); (56, OpenMP, #pragma omp target teams map (alloc: u, qs) private (k, j, m, tmp, u21, u31, u41, u51, u21im1, u31im1, u41im1, u51im1))", "label": "(569, OpenMP, #pragma omp teams distribute parallel for collapse(3) private(i,j,k) num_teams(jend - jst)); (26, OpenMP, #pragma omp distribute parallel for collapse (3) private(tmp)); (106, OpenMP, #pragma omp simd simdlen(32) private (tmp, u21i, u31i, u41i, u51i, u21im1, u31im1, u41im1, u51im1)); (21, OpenMP, #pragma omp target teams); (15, OpenMP, #pragma omp target data); (59, OpenMP, #pragma omp distribute)"}
{"predict": "(11, CUDA, extern __shared__ char shared_mem[];)", "label": "(11, CUDA, extern __shared__ char shared_mem[];)"}
{"predict": "(59, CUDA, CHECK_CUDA(cudaMalloc((void**)&ibuf[i].pixel_data[c], sz));); (56, CUDA, CHECK_CUDA(cudaFree(ibuf[i].pixel_data[c]));)", "label": "(59, CUDA, CHECK_CUDA(cudaMalloc((void**)&ibuf[i].pixel_data[c], sz));); (56, CUDA, CHECK_CUDA(cudaFree(ibuf[i].pixel_data[c]));)"}
{"predict": "(93, MPI, ascent_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (99, MPI, MPI_Barrier(comm);); (17, MPI, MPI_Comm_rank(comm, &par_rank);); (17, MPI, MPI_Comm_size(comm, &par_size);)", "label": "(93, MPI, ascent_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (99, MPI, MPI_Barrier(comm);); (16, MPI, MPI_Comm_rank(comm, &par_rank);); (16, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(10, OpenMP, f = test_cholesky_decomp_dim(p3);)", "label": "(10, OpenMP, f = test_choleskyc_decomp_dim(&p3.matrix, 2 * 8.0 * GSL_DBL_EPSILON);)"}
{"predict": "(32, SYCL, sycl::queue q = getSYCLDefaultDeviceDefaultQueue();)", "label": "(54, SYCL, std::vector<sycl::device> sycl_devices(1);); (3, SYCL, sycl::queue sycl_queue;)"}
{"predict": "(3, TBB, tbb::enumerable_thread_specific<size_t> num_high_degree_nodes(0);); (15, TBB, tbb::parallel_for(tbb::blocked_range<HypernodeID>(0, num_nodes, 1000), node_degree_graph);)", "label": "(15, TBB, tbb::enumerable_thread_specific<size_t> num_high_degree_nodes(0);)"}
{"predict": "(162, OpenMP, ism303dac_mg_offset_temp_comp_set(&dev_ctx_mg, PROPERTY_ENABLE);)", "label": "(162, OpenMP, ism303dac_mg_offset_temp_comp_set(&dev_ctx_mg, PROPERTY_ENABLE);)"}
{"predict": "(4, OpenMP, #pragma omp master taskloop simd); (2, OpenMP, #pragma omp parallel)", "label": "(4, OpenMP, #pragma omp master taskloop simd); (2, OpenMP, #pragma omp parallel)"}
{"predict": "(8, MPI, MPI_Info_get_string(info, key, &flag, buf, &length);)", "label": "(7, MPI, MPI_Info_get(info, key, MPI_MAX_INFO_VAL, buf, &flag);)"}
{"predict": "(36, CUDA, __shared__ _VOLATILE_ float bSmem[16][128+4];); (34, CUDA, __shared__ _VOLATILE_ float aSmem[16][128+4];)", "label": "(36, CUDA, __shared__ _VOLATILE_ float bSmem[16][128+4];); (34, CUDA, __shared__ _VOLATILE_ float aSmem[16][128+4];)"}
{"predict": "(44, OpenMP, #pragma omp teams distribute parallel for shared(ba)); (29, OpenMP, #pragma omp teams distribute parallel for shared (argc, z)); (14, OpenMP, #pragma omp teams distribute parallel for shared ()); (3, OpenMP, #pragma omp target); (51, OpenMP, #pragma omp teams distribute parallel for private(i), shared(i)); (38, OpenMP, #pragma omp teams distribute parallel for shared(da)); (17, OpenMP, #pragma omp teams distribute parallel for shared (argc); (22, OpenMP, #pragma omp teams distribute parallel for shared (argc > 0? argv[1] : argv[2])); (9, OpenMP, #pragma omp teams distribute parallel for shared (argc); (52, OpenMP, #pragma omp teams distribute parallel for shared(i)); (43, OpenMP, #pragma omp teams distribute parallel for firstprivate(i), shared(i)); (59, OpenMP, #pragma omp teams distribute parallel for shared(j)); (30, OpenMP, #pragma omp teams distribute parallel for shared(ca)); (53, OpenMP, #pragma omp teams distribute parallel for shared(i)); (-3, OpenMP, #pragma omp teams distribute parallel for shared (); (29, OpenMP, #pragma omp teams distribute parallel for shared(da)); (10, OpenMP, #pragma omp teams distribute parallel for shared (argc,); (17, OpenMP, #pragma omp teams distribute parallel for shared (a, b, c, d, f)); (14, OpenMP, #pragma omp teams distribute parallel for shared (argc)); (37, OpenMP, #pragma omp teams distribute parallel for shared(h, B::x)); (-12, OpenMP, #pragma omp teams distribute parallel for shared); (5, OpenMP, #pragma omp teams distribute parallel for shared (argc))); (-8, OpenMP, #pragma omp teams distribute parallel for shared (argc)", "label": "(11, OpenMP, #pragma omp teams distribute simd shared (); (43, OpenMP, #pragma omp teams distribute simd shared(da)); (30, OpenMP, #pragma omp teams distribute simd shared (a, b, c, d, f)); (5, OpenMP, #pragma omp teams distribute simd shared); (34, OpenMP, #pragma omp teams distribute simd shared(ba)); (2, OpenMP, #pragma omp target); (35, OpenMP, #pragma omp teams distribute simd shared(ca)); (55, OpenMP, #pragma omp teams distribute simd shared(i)); (57, OpenMP, #pragma omp teams distribute simd shared(j)); (17, OpenMP, #pragma omp teams distribute simd shared (argc)); (43, OpenMP, #pragma omp teams distribute simd private(i), shared(i)); (24, OpenMP, #pragma omp teams distribute simd shared (argv[1])); (17, OpenMP, #pragma omp teams distribute simd shared (S1)); (43, OpenMP, #pragma omp teams distribute simd firstprivate(i), shared(i)); (9, OpenMP, #pragma omp teams distribute simd shared (argc > 0? argv[1] : argv[2])); (35, OpenMP, #pragma omp teams distribute simd shared(h, B::x)); (43, OpenMP, #pragma omp teams distribute simd private(i)); (51, OpenMP, #pragma omp teams distribute simd firstprivate(i)); (2, OpenMP, #pragma omp teams distribute simd shared (argc,); (28, OpenMP, #pragma omp teams distribute simd shared(e, g, z)); (-6, OpenMP, #pragma omp teams distribute simd shared ()); (-4, OpenMP, #pragma omp teams distribute simd shared (argc)"}
{"predict": "(25, MPI, MPI_Init(&argc, &argv);); (80, MPI, MPI_Finalize();); (25, MPI, MPI_Comm_size(MPI_COMM_WORLD, &num_procs);); (25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myid);)", "label": "(25, MPI, MPI_Init(&argc, &argv);); (79, MPI, MPI_Finalize();); (25, MPI, MPI_Comm_size(MPI_COMM_WORLD, &num_procs);); (23, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myid);)"}
{"predict": "(1, TBB, using namespace oneapi::tbb::flow;)", "label": "(1, TBB, using namespace oneapi::tbb::flow;)"}
{"predict": "(3, CUDA, __shared__ float cnts[TESTHOMO_TESTS*TESTHOMO_LOOPS];); (3, CUDA, __shared__ float homo[TESTHOMO_LOOPS*8];)", "label": "(3, CUDA, __shared__ float homo[8*TESTHOMO_LOOPS];); (3, CUDA, __shared__ int cnts[TESTHOMO_TESTS*TESTHOMO_LOOPS];)"}
{"predict": "(9, OpenMP, #pragma omp teams distribute parallel for simd firstprivate); (63, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(S2::S2sc)); (46, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(ba)); (33, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (51, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(da, z)); (0, OpenMP, #pragma omp target); (83, OpenMP, #pragma omp teams distribute parallel for simd lastprivate(argc), firstprivate(argc)); (76, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(j)); (31, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argv[1])); (4, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (); (60, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(h)); (41, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(ca)); (17, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argc > 0? argv[1] : argv[2])); (22, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argc) allocate(omp_thread_mem_alloc: argc)); (53, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(i)); (28, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (a, b, c, d, f)); (0, OpenMP, #pragma omp teams distribute parallel for simd firstprivate ()); (15, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argc) uses_allocators(omp_thread_mem_alloc) allocate(omp_thread_mem_alloc: argc)); (3, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argc); (31, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(2 * 2)); (54, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(i)); (17, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (S1)); (40, OpenMP,", "label": "(34, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (85, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(j)); (7, OpenMP, #pragma omp target); (59, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(S2::S2s)); (10, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (); (21, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argc,); (36, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (a, b, c, d, f)); (47, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(ca)); (66, OpenMP, #pragma omp teams distribute parallel for simd private(i), firstprivate(i)); (13, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argc); (40, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(ba, z)); (59, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(h)); (26, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (S1)); (33, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argv[1])); (52, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(S2::S2sc)); (63, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(i)); (2, OpenMP, #pragma omp teams distribute parallel for simd firstprivate ()); (-7, OpenMP, #pragma omp teams distribute parallel for simd firstprivate); (40, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(da)); (72, OpenMP, #pragma omp teams distribute parallel for simd lastprivate(argc), firstprivate(argc)); (10, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argc > 0? argv[1] : argv[2]))"}
{"predict": "(3, MPI, MPI_Init(&argc, &argv);); (46, MPI, MPI_Finalize();); (22, MPI, s[i] = MPI_Win_allocate_shared((rank==0?99:0)*sizeof(char), 1, MPI_INFO_NULL, node, &base_ptr[i], &win[i]);); (2, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (42, MPI, MPI_Win_free(&win[i]);); (30, MPI, MPI_Barrier(node);); (0, MPI, MPI_Comm_size(node, &ss);); (-2, MPI, MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, 0, MPI_INFO_NULL, &node);); (12, MPI, MPI_Win_get_attr(&win, MPI_WIN_BASE, &ptr[i], 0);)", "label": "(25, MPI, s[i] = MPI_Win_allocate_shared((rank==0?99:0)*sizeof(char), 1, MPI_INFO_NULL, node, &base_ptr[i], &win[i]);); (36, MPI, MPI_Barrier(node);); (-1, MPI, MPI_Init(&argc, &argv);); (44, MPI, if(win[i]!= MPI_WIN_NULL) MPI_Win_free(&win[i]);); (46, MPI, MPI_Finalize();); (2, MPI, MPI_Comm_rank(node, &rank);); (16, MPI, int sam = MPI_Alloc_mem((rank==0?99:0)*sizeof(char), MPI_INFO_NULL, &base_ptr[i]);); (25, MPI, MPI_Win_shared_query(win[i], 0, &size[i], &a[i], &ptr[i]);); (-5, MPI, int ss = MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, 0, MPI_INFO_NULL, &node);)"}
{"predict": "(20, MPI, tim::settings::mpi_init()       = false;); (20, MPI, tim::settings::mpi_finalize()   = false;)", "label": "(20, MPI, tim::settings::mpi_init()       = false;); (20, MPI, tim::settings::mpi_finalize()   = false;)"}
{"predict": "(47, SYCL, sycl::free(c, queue);); (46, SYCL, sycl::free(cd, queue);)", "label": "(9, SYCL, const bool doHalfs = dev.has(sycl::aspect::fp16);); (47, SYCL, sycl::free(cd, queue);); (12, SYCL, const bool doDoubles = dev.has(sycl::aspect::fp64);)"}
{"predict": "(42, CUDA, __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];); (37, CUDA, __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];)", "label": "(42, CUDA, __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];); (37, CUDA, __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];)"}
{"predict": "(8, CUDA, cudaStreamSynchronize(LuH->A_gpu.cuStreams[0]);)", "label": "(8, CUDA, cudaStreamSynchronize(LuH->A_gpu.cuStreams[0]);)"}
{"predict": "(14, OpenMP, #pragma omp target if (argc argc)); (25, OpenMP, #pragma omp target if (argc argc) if (argc)); (2, OpenMP, #pragma omp target if (); (7, OpenMP, #pragma omp target if (argc > 0? argv[1] : argv[2])); (12, OpenMP, #pragma omp target if (1 0)); (5, OpenMP, #pragma omp target if (argc); (18, OpenMP, #pragma omp target if(target : argc); (9, OpenMP, #pragma omp target if(if(tbb::task::is_current_task_group_canceling())); (-2, OpenMP, #pragma omp target if ()); (15, OpenMP, #pragma omp target if(target : argc) if (argc)); (8, OpenMP, #pragma omp target if(target : argc) if (for:argc)); (15, OpenMP, #pragma omp target if(target : argc/z)); (0, OpenMP, #pragma omp target if (argc))); (-12, OpenMP, #pragma omp target if); (-11, OpenMP, #pragma omp target if (argc))); (-2, OpenMP, #pragma omp target if (argc))); (-12, OpenMP, #pragma omp target if (argc)", "label": "(8, OpenMP, #pragma omp target teams device (argc); (5, OpenMP, #pragma omp target teams device ()); (10, OpenMP, #pragma omp target teams device (argc > 0? argv[1] : argv[2])); (15, OpenMP, #pragma omp target teams device (S1)); (12, OpenMP, #pragma omp target teams device (argc), device (argc+1)); (15, OpenMP, #pragma omp target teams device (-2)); (8, OpenMP, #pragma omp target teams device (argc + argc + z)); (15, OpenMP, #pragma omp target teams device (-10u)); (2, OpenMP, #pragma omp target teams device (argc))); (-7, OpenMP, #pragma omp target teams device); (14, OpenMP, #pragma omp target teams device (3.14)); (-7, OpenMP, #pragma omp target teams device ()"}
{"predict": "(30, CUDA, checkCudaErrors(cudaMallocHost(&b, N_ROWS * sizeof(double)));); (28, CUDA, checkCudaErrors(cudaMallocHost(&A, N_ROWS * N_ROWS * sizeof(float)));); (65, CUDA, checkCudaErrors(cudaMalloc(&d_A, sizeof(float) * N_ROWS * N_ROWS));); (98, CUDA, checkCudaErrors(cudaFree(d_x_new));); (98, CUDA, checkCudaErrors(cudaFree(d_b));); (96, CUDA, checkCudaErrors(cudaFree(d_A));); (63, CUDA, checkCudaErrors(cudaMalloc(&d_x, sizeof(double) * N_ROWS));); (66, CUDA, checkCudaErrors(cudaStreamCreateWithFlags(&stream1, cudaStreamNonBlocking));); (63, CUDA, checkCudaErrors(cudaMalloc(&d_x_new, sizeof(double) * N_ROWS));); (94, CUDA, checkCudaErrors(cudaFreeHost(b));); (92, CUDA, checkCudaErrors(cudaFreeHost(A));); (57, CUDA, checkCudaErrors(cudaMalloc(&d_b, sizeof(double) * N_ROWS));); (57, CUDA, checkCudaErrors(cudaMalloc(&d_x_new, sizeof(double) * N_ROWS));); (51, CUDA, checkCudaErrors(cudaFree(d_x));); (51, CUDA, checkCudaErrors(cudaFreeHost(b));); (91, CUDA, checkCudaErrors(cudaFree(d_A));); (52, CUDA, checkCudaErrors(cudaFreeHost(A));); (52, CUDA, checkCudaErrors(cudaFree(d_x));); (49, CUDA, checkCudaErrors(cudaFree(d_b));); (49, CUDA, checkCudaErrors(cudaFree(d_x_new));); (84, CUDA, checkCudaErrors(cudaFree(d_x_new));); (83, CUDA, checkCudaErrors(cudaFree(d_A));); (83, CUDA, checkCudaErrors(cudaFree(d_b));); (84, CUDA, checkCudaErrors(cudaFree(d_x));); (54, CUDA, checkCudaErrors(cudaStreamDestroy(stream1));); (50, CUDA, checkCudaErrors(cudaFreeHost(b));); (51, CUDA, checkCudaErrors(cudaFreeHost(A));); (83, CUDA, checkCudaErrors(cudaFreeHost(b));); (18, CUDA, checkCudaErrors(cudaMallocHost(&b, N_ROWS", "label": "(32, CUDA, checkCudaErrors(cudaMallocHost(&b, N_ROWS * sizeof(double)));); (27, CUDA, int dev = findCudaDevice(argc, (const char **)argv);); (67, CUDA, checkCudaErrors(cudaMalloc(&d_A, sizeof(float) * N_ROWS * N_ROWS));); (99, CUDA, checkCudaErrors(cudaFree(d_x_new));); (101, CUDA, checkCudaErrors(cudaFreeHost(b));); (95, CUDA, checkCudaErrors(cudaFree(d_A));); (64, CUDA, checkCudaErrors(cudaMalloc(&d_x, sizeof(double) * N_ROWS));); (92, CUDA, checkCudaErrors(cudaFree(d_b));); (93, CUDA, checkCudaErrors(cudaFree(d_x));); (59, CUDA, checkCudaErrors(cudaMalloc(&d_b, sizeof(double) * N_ROWS));); (57, CUDA, checkCudaErrors(cudaStreamCreateWithFlags(&stream1, cudaStreamNonBlocking));); (60, CUDA, checkCudaErrors(cudaMalloc(&d_x_new, sizeof(double) * N_ROWS));); (61, CUDA, checkCudaErrors(cudaMemsetAsync(d_x, 0, sizeof(double) * N_ROWS, stream1));); (21, CUDA, checkCudaErrors(cudaMallocHost(&A, N_ROWS * N_ROWS * sizeof(float)));); (90, CUDA, checkCudaErrors(cudaFreeHost(A));)"}
{"predict": "(14, OpenMP, #pragma omp single private(argc,); (23, OpenMP, #pragma omp single private(a, b)); (4, OpenMP, #pragma omp single private); (21, OpenMP, #pragma omp single private(argv[1])); (22, OpenMP, #pragma omp single private(e, g)); (3, OpenMP, #pragma omp single private(); (22, OpenMP, #pragma omp single private(h, B::x)); (29, OpenMP, #pragma omp parallel shared(i)); (16, OpenMP, #pragma omp single private(S1)); (37, OpenMP, #pragma omp single private(m)); (39, OpenMP, #pragma omp single private(m)); (26, OpenMP, #pragma omp single private(j)); (17, OpenMP, #pragma omp single private(e, g)); (20, OpenMP, #pragma omp single private(i)); (21, OpenMP, #pragma omp single private(i)); (0, OpenMP, #pragma omp single private(argc); (11, OpenMP, #pragma omp single private(h,)); (22, OpenMP, #pragma omp parallel); (23, OpenMP, #pragma omp single private(i)); (14, OpenMP, #pragma omp single private(e, g, h, B::x)); (-2, OpenMP, #pragma omp single private(argc > 0? argv[1] : argv[2])); (11, OpenMP, #pragma omp single private(e, g)); (-11, OpenMP, #pragma omp single private()); (-10, OpenMP, #pragma omp single private(argc)", "label": "(11, OpenMP, #pragma omp single private()); (39, OpenMP, #pragma omp parallel private(i)); (21, OpenMP, #pragma omp single private(a, b)); (16, OpenMP, #pragma omp single private(argc)); (23, OpenMP, #pragma omp single private(e, g)); (2, OpenMP, #pragma omp single private); (19, OpenMP, #pragma omp single private(argv[1])); (26, OpenMP, #pragma omp parallel); (9, OpenMP, #pragma omp single private(argc > 0? argv[1] : argv[2])); (20, OpenMP, #pragma omp single private(h, B::x)); (11, OpenMP, #pragma omp single private(S1)); (2, OpenMP, #pragma omp single private(argc); (34, OpenMP, #pragma omp single private(m)); (2, OpenMP, #pragma omp single private(argc,); (27, OpenMP, #pragma omp single private(j)); (16, OpenMP, #pragma omp single shared(i)); (-7, OpenMP, #pragma omp single private(); (22, OpenMP, #pragma omp parallel shared(i)); (18, OpenMP, #pragma omp single private(i))"}
{"predict": "(13, MPI, MPI_Comm_rank( smpi->world(), &my_rank );); (88, MPI, MPI_Recv( &prev_number, 1, MPI_UNSIGNED, (smpi->getRank()+smpi->filesize()-1)%smpi->filesize(), 0, MPI_COMM_WORLD, &status );); (83, MPI, MPI_Send( &dump_number, 1, MPI_UNSIGNED, (smpi->getRank()+1) % smpi->filesize(), 0, MPI_COMM_WORLD );); (87, MPI, MPI_Allreduce( &problem, &any_problem, 1, MPI_INT, MPI_LOR, MPI_COMM_WORLD );)", "label": "(96, MPI, MPI_Finalize();); (88, MPI, MPI_Allreduce( &problem, &any_problem, 1, MPI_INT, MPI_LOR, smpi->world() );)"}
{"predict": "(50, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(rh)); (39, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(h)); (46, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(da)); (20, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(k)); (25, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(aa)); (28, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(raa)); (53, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(k,i,j)); (15, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(z)); (49, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(j)); (56, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(da)); (43, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(rh)); (26, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(raa)); (17, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(aa))", "label": "(37, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(raa)); (48, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(rh)); (41, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(h)); (28, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(aa)); (51, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(da)); (20, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(z)); (13, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(k))"}
{"predict": "(34, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(34, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(24, MPI, MPI_Send(unzval, lenv, MPI_DOUBLE, receiver, k, grid3d->zscp.comm);)", "label": "(24, MPI, MPI_Send(unzval, lenv, MPI_DOUBLE, receiver, k, grid3d->zscp.comm);)"}
{"predict": "(4, CUDA, __shared__ float4 buffer2[M7H*NUM];); (2, CUDA, __shared__ float4 buffer1[M7W*NDIM/4];)", "label": "(5, CUDA, __shared__ float4 buffer2[M7H*NUM];); (3, CUDA, __shared__ float4 buffer1[M7W*NDIM/4];)"}
{"predict": "(13, CUDA, CUDA_CHECK(cudaGetDeviceCount(&ndevices));); (13, CUDA, CUDA_CHECK(cudaSetDevice(rank % ndevices));); (2, MPI, MPI_Init(&argc, &argv);); (151, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (157, MPI, MPI_Finalize();); (72, CUDA, CUDA_CHECK(cudaStreamCreate(&stream));); (89, CUDA, CUDA_CHECK(cudaMemsetAsync(dst, 0, global_io_max * sizeof(int), stream));); (3, MPI, MPI_Comm_rank(comm, &rank);); (3, MPI, MPI_Comm_size(comm, &size);); (96, CUDA, CUDA_CHECK(cudaMemcpyAsync(src, src.data(), global_io_max * sizeof(int), cudaMemcpyHostToDevice, stream));); (139, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (96, CUDA, CUDA_CHECK(cudaMemcpyAsync(dst, dst.data(), global_io_max * sizeof(int), cudaMemcpyHostToDevice, stream));); (139, CUDA, CUDA_CHECK(cudaMemcpyAsync(src, src.data(), global_io_max * sizeof(int), cudaMemcpyHostToDevice, stream));); (133, CUDA, CUDA_CHECK(cudaMemcpyAsync(dst, dst.data(), global_io_max * sizeof(int), cudaMemcpyHostToDevice, stream));); (58, CUDA, CUDA_CHECK(cudaMemsetAsync(src, 0, global_io_max * sizeof(int), stream));); (135, CUDA, CUDA_CHECK(cudaMemcpyAsync(dst, dst.data(), global_io_max * sizeof(int), cudaMemcpyHostToDevice, stream));); (135, CUDA, CUDA_CHECK(cudaMemcpyAsync(src, src.data(), global_io_max * sizeof(int), cudaMemcpyHostToDevice, stream));); (1, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (56, CUDA, CUDA_CHECK(cudaMemsetAsync(dst, 0, global_io_max * sizeof(int), stream));); (133, CUDA, CUDA_CHECK(cudaMemcpyAsync(src, src.data(), global_io_max * sizeof(int), cudaMemcpyHostToDevice, stream));); (133, CUDA, CUDA_CHECK(cudaMemcpyAsync(dst, dst.data(), global_io_max * sizeof(int), cudaMemcpyHostToDevice, stream));); (133, CUDA, CUDA_CHECK(cudaMemcpyAsync(src, src.data(), global_io_max * sizeof(int), cudaMemcpyHostToDevice, stream));); (133, CUDA, CUDA_CHECK(cudaMemcpyAsync(dst, dst.data(),(136, CUDA, CUDA_CHECK(cudaMemcpyAsync(src, src.data", "label": "(15, CUDA, CUDA_CHECK(cudaGetDeviceCount(&ndevices));); (77, CUDA, CUDA_CHECK(cudaStreamCreate(&stream));); (5, MPI, MPI_Comm_size(comm, &size);); (119, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (12, CUDA, CUDA_CHECK(cudaSetDevice(rank % ndevices));); (-3, MPI, MPI_Init(&argc, &argv);); (90, MPI, MPI_Allreduce(&my_io_max, &global_io_max, 1, MPI_UNSIGNED_LONG_LONG, MPI_MAX, comm);); (116, CUDA, CUDA_CHECK(cudaMemcpy(dst_host.data(), dst, dst_host_expected.size() * sizeof(int), cudaMemcpyDefault));); (139, MPI, MPI_Finalize();); (118, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (94, CUDA, CUDA_CHECK(cudaMemsetAsync(dst, 0, dst_host_expected.size(), stream));); (92, CUDA, CUDA_CHECK(cudaMemcpyAsync(src, src_host.data(), src_host.size() * sizeof(int), cudaMemcpyDefault, stream));); (-6, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(11, TBB, tbb::flow::multifunction_node< InputType, OutputType > exe_node( g, lc, cf );); (49, TBB, tbb::flow::remove_edge( tbb::flow::output_port<0>(exe_node), *receivers[r] );); (0, TBB, tbb::flow::graph g;); (18, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(exe_node), *receivers[r] );); (28, TBB, tbb::flow::make_edge( *senders.back(), exe_node );); (62, TBB, g.reset(tbb::flow::rf_reset_bodies);); (55, TBB, inc_functor body_copy = tbb::flow::copy_body<inc_functor>( exe_node );)", "label": "(66, TBB, g.reset(tbb::flow::rf_reset_bodies);); (20, TBB, tbb::flow::make_edge( exe_node, *receivers[r] );); (58, TBB, inc_functor body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (64, TBB, body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (1, TBB, tbb::flow::graph g;); (45, TBB, tbb::flow::remove_edge( exe_node, *receivers[r] );); (24, TBB, tbb::flow::make_edge( *senders.back(), exe_node );); (4, TBB, tbb::flow::function_node< InputType, OutputType > exe_node( g, lc, cf );)"}
{"predict": "(23, MPI, MPI_Comm_dup(m->getMPIComm(), &groupComm);)", "label": "(23, MPI, MPI_Comm_split(expandedPCU->GetMPIComm(), group, groupRank, &groupComm);); (31, MPI, MPI_Comm_free(&groupComm);)"}
{"predict": "(8, CUDA, HANDLE_ERROR(cudaMemcpy(du_new_dev, du_new_host.data(), sizeof(float) * du_new_host.size(), cudaMemcpyHostToDevice));); (23, CUDA, cudaFree(du_new_dev);)", "label": "(19, CUDA, CudaCheckError();); (23, CUDA, cudaFree(du_new_dev);)"}
{"predict": "(44, OpenCL, clReleaseCommandQueue(queue);); (28, OpenCL, printf( \"clCreateContext() failed with %d\\n\", err );); (86, OpenCL, clReleaseMemObject(bufX);); (82, OpenCL, clReleaseMemObject(bufA);); (79, OpenCL, clReleaseEvent(event);); (30, OpenCL, printf( \"clCreateCommandQueue() failed with %d\\n\", err );); (27, OpenCL, queue = clCreateCommandQueue(ctx, device, 0, &err);); (29, OpenCL, clReleaseContext(ctx);); (19, OpenCL, ctx = clCreateContext(props, 1, &device, NULL, NULL, &err);); (64, OpenCL, err = clWaitForEvents(1, &event);); (77, OpenCL, clReleaseMemObject(bufY);)", "label": "(87, OpenCL, clReleaseMemObject(bufX);); (43, OpenCL, clReleaseCommandQueue(queue);); (27, OpenCL, printf( \"clCreateContext() failed with %d\\n\", err );); (85, OpenCL, clReleaseMemObject(bufA);); (79, OpenCL, clReleaseEvent(event);); (30, OpenCL, printf( \"clCreateCommandQueue() failed with %d\\n\", err );); (27, OpenCL, queue = clCreateCommandQueue(ctx, device, 0, &err);); (29, OpenCL, clReleaseContext(ctx);); (78, OpenCL, clReleaseMemObject(bufY);); (18, OpenCL, ctx = clCreateContext(props, 1, &device, NULL, NULL, &err);); (63, OpenCL, err = clWaitForEvents(1, &event);)"}
{"predict": "(86, OpenMP, #pragma omp master taskloop nogroup grainsize(argc)); (25, OpenMP, #pragma omp master taskloop priority(argc)); (163, OpenMP, #pragma omp master taskloop priority(5)); (165, OpenMP, #pragma omp master taskloop priority(10)); (53, OpenMP, #pragma omp master taskloop priority(5)); (166, OpenMP, #pragma omp master taskloop priority(20)); (41, OpenMP, #pragma omp master taskloop priority(5) nogroup); (55, OpenMP, #pragma omp master taskloop priority(10)); (56, OpenMP, #pragma omp master taskloop priority(20)); (61, OpenMP, #pragma omp master taskloop priority(10) nogroup); (12, OpenMP, #pragma omp master taskloop priority(argc); (165, OpenMP, #pragma omp master taskloop priority(10)); (36, OpenMP, #pragma omp master taskloop priority(10) nowait); (15, OpenMP, #pragma omp master taskloop priority(argc,); (32, OpenMP, #pragma omp master taskloop priority(5) nowait); (153, OpenMP, #pragma omp master taskloop priority(2)); (34, OpenMP, #pragma omp master taskloop priority(20) nowait); (57, OpenMP, #pragma omp master taskloop priority(20) nogroup); (22, OpenMP, #pragma omp master taskloop priority(argc > 0? argv[1] : argv[2])); (-5, OpenMP, #pragma omp master taskloop priority); (56, OpenMP, #pragma omp master taskloop priority(10) nogroup); (153, OpenMP, #pragma omp master taskloop priority(10)); (64, OpenMP, #pragma omp master taskloop priority(2)); (13, OpenMP, #pragma omp master taskloop priority(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (38, OpenMP, #pragma omp master taskloop priority(20) nowait); (6, OpenMP, #pragma omp master taskloop priority(argc) allocate(omp_thread_mem_alloc: argc)); (9, OpenMP, #pragma omp master task", "label": "(4, OpenMP, #pragma omp task); (25, OpenMP, #pragma omp master taskloop priority(argc)); (46, OpenMP, #pragma omp master taskloop); (71, OpenMP, #pragma omp master taskloop if(argc) shared(argc, argv) collapse(2) num_tasks(4)); (87, OpenMP, #pragma omp cancel taskgroup); (87, OpenMP, #pragma omp cancellation point taskgroup)"}
{"predict": "(68, CUDA, cudaFree(B);); (55, CUDA, cudaEventSynchronize(stop);); (55, CUDA, cudaEventElapsedTime(&elapsedTime, start, stop);); (63, CUDA, cudaFree(C);); (62, CUDA, cudaFree(A);); (17, CUDA, cudaEventCreate(&stop);); (44, CUDA, cudaEventRecord(stop);); (44, CUDA, cudaEventSynchronize(stop);); (15, CUDA, cudaEventRecord(start);); (17, CUDA, cudaEventDestroy(start);); (53, CUDA, cudaEventDestroy(stop);); (60, CUDA, cudaFree(devices[0]);); (60, CUDA, cudaFree(devices[1]);); (11, CUDA, cudaEventCreate(&start);); (59, CUDA, cudaFree(devices[0]);); (34, CUDA, cudaEventRecord(start);); (1, CUDA, cudaGetDeviceCount(&num_of_total_devices);)", "label": "(67, CUDA, cudaFree(B);); (54, CUDA, cudaEventRecord(stop);); (54, CUDA, cudaEventSynchronize(stop);); (65, CUDA, cudaFree(C);); (44, CUDA, cudaEventRecord(start);); (29, CUDA, cudaGetDeviceCount(&num_of_total_devices);); (18, CUDA, cudaEventCreate(&stop);); (53, CUDA, cudaEventElapsedTime(&elapsedTime, start, stop);); (15, CUDA, cudaEventCreate(&start);); (57, CUDA, cudaFree(A);)"}
{"predict": "(11, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (9, CUDA, cudaMalloc((void**)(&d_in1), in1_bytes);); (11, CUDA, cudaMemcpy(d_in1, in1.data(), in1_bytes, cudaMemcpyHostToDevice);); (31, CUDA, cudaFree(d_out);); (29, CUDA, cudaFree(d_in1);); (22, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);)", "label": "(11, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (9, CUDA, cudaMalloc((void**)(&d_in1), in1_bytes);); (11, CUDA, cudaMemcpy(d_in1, in1.data(), in1_bytes, cudaMemcpyHostToDevice);); (31, CUDA, cudaFree(d_out);); (23, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (28, CUDA, cudaFree(d_in1);)"}
{"predict": "(9, OpenMP, #pragma omp parallel master taskloop simd linear (); (39, OpenMP, #pragma omp parallel master taskloop simd linear(uval(j))); (5, OpenMP, #pragma omp parallel master taskloop simd linear); (31, OpenMP, #pragma omp parallel master taskloop simd linear(h, z)); (16, OpenMP, #pragma omp parallel master taskloop simd linear (val(a, b):B::ib)); (7, OpenMP, #pragma omp parallel master taskloop simd linear (val); (8, OpenMP, #pragma omp parallel master taskloop simd linear (val()); (3, OpenMP, #pragma omp parallel); (22, OpenMP, #pragma omp parallel master taskloop simd linear(v : i)); (15, OpenMP, #pragma omp parallel master taskloop simd linear(e, g)); (16, OpenMP, #pragma omp parallel master taskloop simd linear(h)); (11, OpenMP, #pragma omp parallel master taskloop simd linear (argv[1])); (4, OpenMP, #pragma omp parallel master taskloop simd linear ()); (23, OpenMP, #pragma omp parallel master taskloop simd allocate(omp_thread_mem_alloc: i) linear(i)); (16, OpenMP, #pragma omp parallel master taskloop simd linear(i)); (29, OpenMP, #pragma omp parallel master taskloop simd linear(ref(j))); (4, OpenMP, #pragma omp parallel master taskloop simd linear (val :); (17, OpenMP, #pragma omp parallel master taskloop simd linear(v : j))", "label": "(17, OpenMP, #pragma omp parallel for simd linear (argc : 5) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (39, OpenMP, #pragma omp parallel for simd linear(j)); (41, OpenMP, #pragma omp parallel for simd linear(v:j)); (34, OpenMP, #pragma omp parallel for simd linear(v:i)); (27, OpenMP, #pragma omp parallel for simd linear(i, z)); (4, OpenMP, #pragma omp parallel for simd linear ()); (13, OpenMP, #pragma omp parallel for simd linear (S1)); (6, OpenMP, #pragma omp parallel for simd linear (argc,); (-3, OpenMP, #pragma omp parallel); (36, OpenMP, #pragma omp parallel for simd linear(i)); (17, OpenMP, #pragma omp parallel for simd linear(e, g)); (12, OpenMP, #pragma omp parallel for simd linear (a, b:B::ib)); (-1, OpenMP, #pragma omp parallel for simd linear (argc); (12, OpenMP, #pragma omp parallel for simd linear (argv[1])); (15, OpenMP, #pragma omp parallel for simd linear(h)); (-8, OpenMP, #pragma omp parallel for simd linear (); (-1, OpenMP, #pragma omp parallel for simd linear (argc > 0? argv[1] : argv[2]))"}
{"predict": "(8, OpenMP, #pragma omp parallel for schedule (static) if(n>OMP_LIMIT_COMPUTESTEP))", "label": "(8, OpenMP, #pragma omp parallel for schedule (guided))"}
{"predict": "(9, SYCL, sycl::queue q = get_queue();); (36, SYCL, sycl::free(data, q);); (8, SYCL, int* data = sycl::malloc_device<int>(1,q);)", "label": "(9, SYCL, sycl::queue q = get_queue();); (36, SYCL, sycl::free(data, q);); (8, SYCL, int* data = sycl::malloc_device<int>(1,q);)"}
{"predict": "(3, CUDA, cudaGetDeviceProperties( &prop, 0 );); (4, CUDA, cudaGetDeviceCount( &count );)", "label": "(6, CUDA, HANDLE_ERROR( cudaGetDeviceProperties( &prop, i ) );); (3, CUDA, HANDLE_ERROR( cudaGetDeviceCount( &count ) );)"}
{"predict": "(49, MPI, MPI_Allreduce(MPI_IN_PLACE,&n,1,mpi_int_t,MPI_MAX,grid->comm);); (47, MPI, MPI_Allreduce(MPI_IN_PLACE,&nnzL,1,mpi_int_t,MPI_SUM,grid->comm);)", "label": "(49, MPI, MPI_Allreduce(MPI_IN_PLACE,&n,1,mpi_int_t,MPI_MAX,grid->comm);); (47, MPI, MPI_Allreduce(MPI_IN_PLACE,&nnzL,1,mpi_int_t,MPI_SUM,grid->comm);)"}
{"predict": "(17, MPI, MPI_Init(&argc, &argv);); (20, MPI, MPI_Finalize();); (39, MPI, MPI_Cart_create(MPI_COMM_WORLD,3,numpes,isperiodic,reorder,&comm_cart);); (39, MPI, MPI_Comm_rank(comm_cart,&mype);); (17, MPI, MPI_Comm_size(MPI_COMM_WORLD, &totpes);); (70, MPI, MPI_Barrier(comm_cart);); (35, MPI, MPI_Cart_coords(comm_cart,mype,3,pe_coords);); (92, MPI, MPI_Allreduce(t, t_g, 20, MPI_DOUBLE, MPI_MAX, comm_cart);); (28, MPI, MPI_Dims_create(totpes,3,numpes);); (15, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (110, MPI, MPI_Allreduce(MPI_IN_PLACE, &nerrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (103, MPI, MPI_Comm_free(&comm_cart);); (88, MPI, MPI_Allreduce(rates_l, rates_g, 4, MPI_DOUBLE, MPI_MIN, comm_cart);); (105, MPI, MPI_Reduce(&malloc_size, &sum_size, 1, MPI_OFFSET, MPI_SUM, 0, MPI_COMM_WORLD);)", "label": "(17, MPI, MPI_Comm_size(MPI_COMM_WORLD,&totpes);); (39, MPI, MPI_Cart_create(MPI_COMM_WORLD,3,numpes,isperiodic,reorder,&comm_cart);); (39, MPI, MPI_Comm_rank(comm_cart,&mype);); (19, MPI, MPI_Finalize();); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&rank);); (72, MPI, MPI_Barrier(comm_cart);); (36, MPI, MPI_Cart_coords(comm_cart,mype,3,pe_coords);); (94, MPI, MPI_Allreduce(t, t_g, 20, MPI_DOUBLE, MPI_MAX, comm_cart);); (31, MPI, MPI_Dims_create(totpes,3,numpes);); (18, MPI, MPI_Bcast(filename, 256, MPI_CHAR, 0, MPI_COMM_WORLD);); (115, MPI, MPI_Allreduce(MPI_IN_PLACE, &nerrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (5, MPI, MPI_Init(&argc,&argv);); (102, MPI, MPI_Comm_free(&comm_cart);); (87, MPI, MPI_Allreduce(rates_l, rates_g, 4, MPI_DOUBLE, MPI_MIN, comm_cart);); (105, MPI, MPI_Reduce(&malloc_size, &sum_size, 1, MPI_OFFSET, MPI_SUM, 0, MPI_COMM_WORLD);)"}
{"predict": "(10, TBB, tbb::flow::make_edge( lim, r );); (22, TBB, tbb::flow::limiter_node< T > lim_copy( lim );); (3, TBB, tbb::flow::graph g;); (3, TBB, tbb::flow::limiter_node< T > lim(g, i);); (4, TBB, tbb::atomic<int> accept_count;)", "label": "(10, TBB, tbb::flow::make_edge( lim, r );); (22, TBB, tbb::flow::limiter_node< T > lim_copy( lim );); (3, TBB, tbb::flow::graph g;); (3, TBB, tbb::flow::limiter_node< T > lim(g, i);); (4, TBB, tbb::atomic<int> accept_count;)"}
{"predict": "(20, OpenMP, #pragma omp loop); (12, OpenMP, #pragma omp loop reduction(*:MTX)); (16, OpenMP, #pragma omp combined); (26, OpenMP, #pragma omp loop reduction(*:EMTX))", "label": "(15, OpenMP, #pragma omp loop); (19, OpenMP, #pragma omp target teams distribute parallel for map(MTX) reduction(*:EMTX)); (11, OpenMP, #pragma omp target teams loop map(MTX) reduction(*:MTX))"}
{"predict": "(59, TBB, tbb::task_group tg;)", "label": "(67, TBB, tbb::task_group tg;)"}
{"predict": "(9, OpenMP, #pragma omp for reduction(+:counts_team)); (4, OpenMP, #pragma omp target teams map(from:counts1)); (15, OpenMP, #pragma omp target teams map(from:counts2)); (21, OpenMP, #pragma omp for reduction(+:counts_team)); (4, OpenMP, #pragma omp parallel); (0, OpenMP, #pragma omp target data map(from:counts1,counts2))", "label": "(26, OpenMP, #pragma omp for reduction(+:counts_team)); (8, OpenMP, #pragma omp for); (15, OpenMP, #pragma omp target teams map(from:counts2)); (4, OpenMP, #pragma omp parallel); (0, OpenMP, #pragma omp target teams map(from:counts1)); (17, OpenMP, #pragma omp allocate(counts_team) allocator(omp_pteam_mem_alloc))"}
{"predict": "(116, MPI, MPI_Type_commit(&mpi_word);); (123, MPI, MPI_Allgather(iterstring,1,mpi_word, catstring,1,mpi_word, MPI_COMM_WORLD);); (155, MPI, MPI_Finalize();); (64, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (111, MPI, MPI_Barrier(MPI_COMM_WORLD);); (18, MPI, MPI_Init(&argc,&argv);); (62, MPI, MPI_Bcast(&length,     1, MPI_LONG, root, MPI_COMM_WORLD);); (111, MPI, MPI_Allgather(MPI_BOTTOM,1,mpi_word, catstring,1,mpi_word, MPI_COMM_WORLD);); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (60, MPI, MPI_Bcast(&my_ID, 1, MPI_INT, root, MPI_COMM_WORLD);); (102, MPI, MPI_Type_contiguous(proc_length,MPI_CHAR, &mpi_word);); (13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)", "label": "(115, MPI, MPI_Type_commit(&mpi_word);); (122, MPI, MPI_Allgather(iterstring,1,mpi_word, catstring,1,mpi_word, MPI_COMM_WORLD);); (154, MPI, MPI_Finalize();); (63, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (110, MPI, MPI_Type_contiguous(proc_length,MPI_CHAR, &mpi_word);); (17, MPI, MPI_Init(&argc,&argv);); (61, MPI, MPI_Bcast(&length,     1, MPI_LONG, root, MPI_COMM_WORLD);); (110, MPI, MPI_Barrier(MPI_COMM_WORLD);); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(10, MPI, MPI_Barrier(MPI_COMM_WORLD);); (15, MPI, MPI_Allreduce(MPI_IN_PLACE,data,n, MPIBool_t,MPI_BAND,MPI_COMM_WORLD);); (13, MPI, MPI_Waitall(2,mpiRequest,mpiStatus);)", "label": "(10, MPI, MPI_Barrier(MPI_COMM_WORLD);); (12, MPI, MPI_Allreduce(data,gr_comm_buf,n,mpi_bool,MPI_LAND,MPI_COMM_WORLD);)"}
{"predict": "(16, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(16, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(2, TBB, tbb::task_arena arena(wait_message);); (36, TBB, tbb::flow::make_edge(tbb::flow::output_port<0>(join_node), buffer1);); (43, TBB, tbb::flow::make_edge(tbb::flow::output_port<0>(join_node), buffer3);); (34, TBB, tbb::flow::make_edge(tbb::flow::output_port<1>(join_node), buffer2);); (15, TBB, tbb::flow::function_node<tuple_type, tbb::flow::rejecting> function(g, tbb::flow::serial, &function);); (0, TBB, tbb::flow::graph g;); (60, TBB, tbb::flow::remove_edge(tbb::flow::output_port<0>(join_node), buffer1);); (13, TBB, tbb::flow::join_node<tuple_type, tbb::flow::queueing> join_node(g, tbb::flow::serial, &function);); (59, TBB, tbb::flow::remove_edge(tbb::flow::output_port<1>(join_node), buffer2);); (59, TBB, tbb::flow::remove_edge(tbb::flow::output_port<2>(join_node), buffer3);); (-2, TBB, tbb::flow::buffer_node<int> buffer2(g);); (-5, TBB, tbb::flow::graph g;); (27, TBB, tbb::flow::make_edge(join_node, buffer3);); (-9, TBB, tbb::flow::graph g;); (53, TBB, tbb::flow::make_edge(join_node, buffer1);); (-12, TBB, tbb::task_arena arena(1);); (4, TBB, tbb::flow::make_edge(tbb::flow::output_port<0>(join_node), buffer2);); (-18, TBB, tbb::flow::graph g;); (-12, TBB, tbb::flow::function_node<tuple_type, tbb::flow::rejecting> function(g, tbb::flow::serial, &function);); (-6, TBB, tbb::flow::buffer_node<int> buffer3(g);); (-9, TBB, tbb::flow", "label": "(42, TBB, tbb::flow::make_edge(buffer2, tbb::flow::input_port<1>(join));); (43, TBB, tbb::flow::make_edge(join, function);); (19, TBB, tbb::flow::join_node<tuple_type, tbb::flow::reserving> join(g);); (38, TBB, tbb::flow::make_edge(buffer1, tbb::flow::input_port<0>(join));); (-3, TBB, tbb::task_arena arena(1);); (-1, TBB, tbb::flow::graph g;); (11, TBB, tbb::flow::queue_node<int> buffer1(g);); (12, TBB, tbb::flow::queue_node<int> buffer3(g);); (35, TBB, tbb::flow::make_edge(buffer3, tbb::flow::input_port<2>(join));); (9, TBB, tbb::flow::queue_node<int> buffer2(g);)"}
{"predict": "(7, MPI, MPI_Init(&argc, &argv);); (49, MPI, MPI_Finalize();); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (46, MPI, MPI_Reduce(&localpi, &pi, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);); (28, MPI, MPI_Exscan(&localpi, &pi, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);); (41, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)", "label": "(42, MPI, MPI_Recv(&localpi, 1, MPI_DOUBLE, MPI_ANY_SOURCE, 0, MPI_COMM_WORLD, &status);); (47, MPI, MPI_Send(&localpi, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);); (4, MPI, MPI_Init(&argc, &argv);); (48, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(13, CUDA, CP_host->freeCudaMem();)", "label": "(13, CUDA, CP_host->freeCudaMem();)"}
{"predict": "(14, TBB, tbb::task_scheduler_init init( 1 );); (2, TBB, tbb::task_scheduler_init init( tbb::task_scheduler_init::deferred );)", "label": "(12, TBB, tbb::parallel_for( 0, N, 1, RoundingModeCheckBody(mode, sse_mode) );); (10, TBB, tbb::task_scheduler_init init;)"}
{"predict": "(18, OpenMP, varLimit[i] = omp_get_var_limit();); (25, OpenMP, varLimitHuge[i] = omp_get_var_limitHuge();); (12, OpenMP, #pragma omp target teams distribute thread_limit(27)); (40, OpenMP, constLimit[i] = omp_get_const_limit();); (47, OpenMP, #pragma omp target teams distribute thread_limit(1024)); (6, OpenMP, #pragma omp target teams distribute thread_limit(12)); (30, OpenMP, constLimitHuge[i] = omp_get_const_limitHuge();); (20, OpenMP, #pragma omp target teams distribute thread_limit(1024)); (8, OpenMP, #pragma omp target teams distribute thread_limit(N)); (40, OpenMP, #pragma omp target teams distribute thread_limit(27)); (44, OpenMP, #pragma omp target teams distribute thread_limit(18))", "label": "(13, OpenMP, fprintf(stderr, \"#pragma omp target teams distribute thread_limit(thdLim) 27\\n\");); (43, OpenMP, #pragma omp target teams distribute thread_limit(1024)); (22, OpenMP, fprintf(stderr, \"#pragma omp target teams distribute thread_limit(thdLim) 1024\\n\");); (31, OpenMP, #pragma omp target teams distribute thread_limit(27)); (34, OpenMP, constLimit[i] = omp_get_num_threads();); (9, OpenMP, #pragma omp target teams distribute thread_limit(thdLim)); (42, OpenMP, constLimitHuge[i] = omp_get_num_threads();); (21, OpenMP, varLimitHuge[i] = omp_get_num_threads();); (9, OpenMP, varLimit[i] = omp_get_num_threads();)"}
{"predict": "(29, CUDA, cuda(HostFree( foutHost ));); (28, CUDA, cudaFreeArray( array );)", "label": "(50, CUDA, cudaFreeHost( foutHost );); (27, CUDA, CUDART_CHECK(surf1Dmemset( array, 3.141592654f, 0, NUM_VALUES ));)"}
{"predict": "(72, OpenMP, #pragma omp target data map(h_x [0:nconf * numatm], h_y [0:nconf * numatm], h_z [0:nconf * numatm], h_g2 [0:nbin]))", "label": "(72, OpenMP, #pragma omp target data map(h_x [0:nconf * numatm], h_y [0:nconf * numatm], h_z [0:nconf * numatm], h_g2 [0:nbin]))"}
{"predict": "(6, OpenMP, #pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp parallel for)"}
{"predict": "(3, SYCL, sycl::queue q = get_queue();); (18, SYCL, hipsycl::rt::runtime_keep_alive_token rt;); (2, SYCL, int* data = sycl::malloc_shared<int>(7, q);); (67, SYCL, sycl::free(data, q);)", "label": "(4, SYCL, int* data = sycl::malloc_shared<int>(7, q);); (19, SYCL, hipsycl::rt::runtime_keep_alive_token rt;); (1, SYCL, sycl::queue q = get_queue();); (67, SYCL, sycl::free(data, q);)"}
{"predict": "(30, MPI, MPI_Gather( vecin, 1, vec, vecout, 1, vec, root, comm );); (10, MPI, MPI_Comm_size( comm, &size );); (21, MPI, MPI_Type_vector( n, 1, stride, MPI_DOUBLE, &vec );); (43, MPI, MPI_Gather( NULL, 0, MPI_BYTE, NULL, 0, MPI_BYTE, 0, MPI_COMM_WORLD );); (42, MPI, MPI_Comm_rank( comm, &rank );); (38, MPI, MPI_Type_free( &vec );); (18, MPI, MPI_Type_commit( &vec );); (5, MPI, MPI_Init( &argc, &argv );); (42, MPI, MPI_Finalize();); (8, MPI, MPI_Comm_rank( comm, &rank );); (5, MPI, MPI_Comm_size( comm, &size );)", "label": "(13, MPI, MPI_Comm_size( comm, &size );); (22, MPI, MPI_Type_commit( &vec );); (26, MPI, MPI_Gather( vecin, 1, vec, vecout, n, MPI_DOUBLE, root, comm );); (46, MPI, MPI_Finalize();); (5, MPI, MPI_Init( &argc, &argv );); (35, MPI, MPI_Type_free( &vec );); (6, MPI, MPI_Comm_rank( comm, &rank );); (15, MPI, MPI_Type_vector( n, 1, stride, MPI_DOUBLE, &vec );); (39, MPI, MPI_Gather( NULL, 0, MPI_BYTE, NULL, 0, MPI_BYTE, 0, MPI_COMM_WORLD );)"}
{"predict": "(12, MPI, MPI_Init(&argc, &argv);); (40, MPI, MPI_Finalize();); (12, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (12, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (12, MPI, MPI_Dims_create(size, NR_DIMS, dims);); (19, MPI, MPI_Cart_coords(cart_comm, rank, NR_DIMS, cart_coords);); (25, MPI, MPI_Barrier(MPI_COMM_WORLD);); (25, MPI, MPI_Reduce(&sum, &avg, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);); (16, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(34, OpenMP, #pragma omp for reduction(+:sum)); (32, OpenMP, printf(\"rank %d:%d has %d threads\\n\", rank, thread_num, omp_get_num_threads());); (11, MPI, MPI_Init(&argc, &argv);); (42, MPI, MPI_Finalize();); (26, OpenMP, #pragma omp parallel); (36, MPI, MPI_Reduce(&sum, &avg, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (25, OpenMP, int thread_num = omp_get_thread_num();); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (7, MPI, MPI_Dims_create(size, NR_DIMS, dims);); (13, MPI, MPI_Cart_coords(cart_comm, rank, NR_DIMS, cart_coords);); (10, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(8, OpenCL, clReleaseKernel_ExpectAndReturn(make_kernel(0), CL_SUCCESS);); (6, OpenCL, clReleaseDevice_ExpectAndReturn(make_device_id(0), CL_SUCCESS);)", "label": "(8, OpenCL, clReleaseKernel_ExpectAndReturn(make_kernel(0), CL_SUCCESS);); (6, OpenCL, clReleaseDevice_ExpectAndReturn(make_device_id(0), CL_SUCCESS);)"}
{"predict": "(40, CUDA, CUDA_SAFE_CALL(cudaFree(p_y));); (38, CUDA, CUDA_SAFE_CALL(cudaFree(p_x));); (36, CUDA, CUDA_SAFE_CALL(cudaFree(p_A));)", "label": "(40, CUDA, CUDA_SAFE_CALL(cudaFree(x.data()));); (38, CUDA, CUDA_SAFE_CALL(cudaFree(A.data()));); (36, CUDA, CUDA_SAFE_CALL(cudaDeviceSynchronize());); (38, CUDA, CUDA_SAFE_CALL(cudaFree(y.data()));)"}
{"predict": "(22, MPI, *w = MPI_Wtime();)", "label": "(22, MPI, *w = MPI_Wtime();)"}
{"predict": "(2, CUDA, __shared__ half hs[][256];)", "label": "(2, CUDA, extern __shared__ half  hs[][256];)"}
{"predict": "(7, TBB, tbb::task_scheduler_init init( p );)", "label": "(7, TBB, tbb::task_scheduler_init init( p );)"}
{"predict": "(13, CUDA, CUDASimulation cudaSimulation(model);)", "label": "(13, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(17, MPI, MPI_Comm_rank(shm_comm, &shm_rank);); (24, MPI, MPI_Win_query(shm_win);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (16, MPI, MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, rank, MPI_INFO_NULL, &shm_comm);); (59, MPI, MPI_Comm_free(&shm_comm);); (15, MPI, MPI_Comm_size(shm_comm, &shm_nproc);); (22, MPI, MPI_Win_get_attr(shm_win, MPI_PROC_NULL, &base, &disp_unit);); (42, MPI, MPI_Win_lock_all(MPI_MODE_NOCHECK, shm_win);); (42, MPI, MPI_Barrier(shm_comm);); (57, MPI, MPI_Win_free(&shm_win);); (27, MPI, MPI_Win_lock_all(MPI_MODE_NOCHECK, MPI_COMM_WORLD);); (3, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(18, MPI, MPI_Comm_rank(shm_comm, &shm_rank);); (41, MPI, MPI_Win_lock_all(MPI_MODE_NOCHECK, shm_win);); (24, MPI, MPI_Win_shared_query(shm_win, MPI_PROC_NULL, &size, &disp_unit, &base);); (13, MPI, MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, rank, MPI_INFO_NULL, &shm_comm);); (10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (58, MPI, MPI_Win_unlock_all(shm_win);); (58, MPI, MPI_Win_free(&shm_win);); (58, MPI, MPI_Comm_free(&shm_comm);); (10, MPI, MPI_Comm_size(shm_comm, &shm_nproc);); (40, MPI, MPI_Barrier(shm_comm);); (38, MPI, MPI_Win_sync(shm_win);)"}
{"predict": "(64, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(host_output.data(), c, global_c_size * sizeof(TC), cudaMemcpyDeviceToHost));); (50, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(b, host_b.data(), global_b_size * sizeof(TB), cudaMemcpyHostToDevice));); (50, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(c, host_c.data(), global_c_size * sizeof(TC), cudaMemcpyHostToDevice));); (37, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&b,  global_b_size * sizeof(TB)));); (50, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (69, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(a));); (37, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&c,  global_c_size * sizeof(TC)));); (50, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(a, host_a.data(), global_a_size * sizeof(TA), cudaMemcpyHostToDevice));); (63, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(c));); (29, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&a,  global_a_size * sizeof(TA)));); (61, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(b));)", "label": "(41, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&b, global_b_size * sizeof(TB)));); (71, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(c));); (68, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(a));); (37, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&a, global_a_size * sizeof(TA)));); (47, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(a, host_a.data(), global_a_size * sizeof(TA), cudaMemcpyHostToDevice));); (49, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (47, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(c, host_c.data(), global_a_size * sizeof(TC), cudaMemcpyHostToDevice));); (45, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(b, host_b.data(), global_a_size * sizeof(TB), cudaMemcpyHostToDevice));); (58, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(host_output.data(), c, global_c_size * sizeof(TC), cudaMemcpyDeviceToHost));); (33, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&c, global_c_size * sizeof(TC)));); (61, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(b));)"}
{"predict": "(61, CUDA, cudaFree( ddata );)", "label": "(62, CUDA, cudaFree( ddata );)"}
{"predict": "(10, CUDA, cudaStreamSynchronize(LU_v1->A_gpu.cuStreams[0]);)", "label": "(10, CUDA, cudaStreamSynchronize(LU_v1->A_gpu.cuStreams[0]);)"}
{"predict": "(2, OpenMP, omp_autotune(self, OMP_SCALE);)", "label": "(2, OpenMP, omp_autotune(self, OMP_SCALE);)"}
{"predict": "(13, CUDA, cudaMalloc(&dB_buff, B COL * B_ROW * sizeof(float)*batch_num);); (34, CUDA, cudaFree(dB_buff);); (15, CUDA, cudaMalloc(&dC_buff, C COL * C_ROW * sizeof(float)*batch_num);); (12, CUDA, cudaMalloc(&dA_buff, A COL * A_ROW * sizeof(float)*batch_num);); (25, CUDA, cudaFree(dA_buff);); (25, CUDA, cudaFree(dC_buff);); (22, CUDA, cudaFree(dB_buff);)", "label": "(18, CUDA, cudaMemcpy(dB_buff, B_buff, sizeof(float*) * batch_num, cudaMemcpyHostToDevice);); (12, CUDA, cudaMalloc(&dA_buff, sizeof(float*) * batch_num);); (31, CUDA, cudaFree(dC_buff);); (29, CUDA, cudaFree(dB_buff);); (27, CUDA, cudaFree(dA_buff);); (12, CUDA, cudaMemcpy(dA_buff, A_buff, sizeof(float*) * batch_num, cudaMemcpyHostToDevice);); (8, CUDA, cudaMalloc(&dB_buff, sizeof(float*) * batch_num);); (12, CUDA, cudaMemcpy(dC_buff, C_buff, sizeof(float*) * batch_num, cudaMemcpyHostToDevice);); (7, CUDA, cudaMalloc(&dC_buff, sizeof(float*) * batch_num);)"}
{"predict": "(7, CUDA, cudaMalloc(&buffer_fp32, length * sizeof(float));); (12, CUDA, cudaFree(buffer_fp32);); (5, CUDA, cudaMalloc(&buffer, length * sizeof(T));)", "label": "(7, CUDA, cudaMalloc((void **)&buffer_fp32, length * sizeof(float));); (9, CUDA, cudaMalloc((void **)&buffer, length * sizeof(T));); (10, CUDA, cudaFree(buffer_fp32);)"}
{"predict": "(22, OpenMP, #pragma omp target); (23, OpenMP, #pragma omp teams distribute parallel for if(cond) firstprivate(fp) reduction(+:rd))", "label": "(16, OpenMP, #pragma omp target); (22, OpenMP, #pragma omp teams distribute parallel for if(cond) firstprivate(fp) reduction(+:rd))"}
{"predict": "(19, CUDA, cudaMemcpy(resf,dat,sizeof(unsigned)*2,cudaMemcpyDeviceToHost);)", "label": "(19, CUDA, cudaMemcpy(resf,dat,sizeof(unsigned)*2,cudaMemcpyDeviceToHost);)"}
{"predict": "(51, CUDA, cudaFreeArray( texArray );); (28, CUDA, CUDART_CHECK(tex2Dmemset( texArray, 3.141592654f ) );); (28, CUDA, CUDART_CHECK( MeasureTimes( &ms, &Bandwidth, texArray, 3.141592654f ) );)", "label": "(29, CUDA, CUDART_CHECK(surf2DmemsetArray_time( &ms, texArray, 3.141592654f, threadWidth, threadHeight ));); (50, CUDA, cudaFreeArray( texArray );)"}
{"predict": "(94, CUDA, cudaError_t error_id = cudaGetDeviceProperties(&deviceProp, currentDevice);); (57, CUDA, cudaError_t error_id = cudaGetDeviceCount(&deviceCount);); (59, CUDA, printf(\"cudaGetDeviceCount returned %d\\n-> %s\\n\", (int)error_id, cudaGetErrorString(error_id));); (91, CUDA, checkCudaErrors(cudaSetDevice(currentDevice));); (105, CUDA, cudaSetDevice(device);)", "label": "(267, CUDA, cudaSetDevice(nDevice);); (108, CUDA, printf(\"cudaGetDeviceProperties returned %d\\n-> %s\\n\", (int)error_id, cudaGetErrorString(error_id));); (54, CUDA, cudaError_t error_id = cudaGetDeviceCount(&deviceCount);); (98, CUDA, fprintf(stderr, \"Error: device is running in <Compute Mode Prohibited>, no threads can use ::cudaSetDevice().\\n\");); (56, CUDA, printf(\"cudaGetDeviceCount returned %d\\n-> %s\\n\", (int)error_id, cudaGetErrorString(error_id));); (88, CUDA, cudaError_t error_id = cudaGetDeviceProperties(&deviceProp, currentDevice);); (96, CUDA, checkCudaErrors(cudaSetDevice(currentDevice));)"}
{"predict": "(1, CUDA, __shared__ float cache[64];)", "label": "(5, CUDA, extern __shared__ float cache[];)"}
{"predict": "(9, OpenMP, #pragma omp target map(inParallel, numThreads, threadNum, check1[:], check2[:])); (16, OpenMP, #pragma omp parallel); (20, OpenMP, #pragma omp atomic); (8, OpenMP, #pragma omp teams); (10, OpenMP, #pragma omp target parallel for); (69, OpenMP, #pragma omp target parallel for map(check1[:], check2[:])); (15, OpenMP, int nestedInParallel = omp_in_parallel();); (15, OpenMP, int nestedNumThreads = omp_get_num_threads();); (15, OpenMP, int nestedThreadNum = omp_get_thread_num();); (69, OpenMP, int NumThreads1 = omp_get_num_threads();); (8, OpenMP, int inParallel = omp_in_parallel();); (8, OpenMP, int numThreads = omp_get_num_threads();); (8, OpenMP, int threadNum = omp_get_thread_num();); (1, OpenMP, #pragma omp target map(tofrom: inParallel)); (8, OpenMP, #pragma omp target parallel for map(tofrom: inParallel, numThreads, threadNum, check1[:], check2[:]))", "label": "(77, OpenMP, #pragma omp parallel num_threads(NumThreads1)); (14, OpenMP, #pragma omp parallel); (26, OpenMP, int nestedThreadNum = omp_get_thread_num();); (7, OpenMP, inParallel = omp_in_parallel();); (22, OpenMP, int nestedNumThreads = omp_get_num_threads();); (19, OpenMP, int nestedInParallel = omp_in_parallel();); (12, OpenMP, check1[id] += omp_get_num_threads() + omp_in_parallel();); (4, OpenMP, numThreads = omp_get_num_threads();); (21, OpenMP, #pragma omp atomic); (3, OpenMP, threadNum = omp_get_thread_num();); (-2, OpenMP, #pragma omp target map(inParallel, numThreads, threadNum, check1[:], check2[:])); (6, OpenMP, int id = omp_get_thread_num();)"}
{"predict": "(16, CUDA, double gpu_sum = thrust::reduce(dptr, dptr+steps*sizeof(float));); (10, CUDA, float *dptr = thrust::raw_pointer_cast(&dsums[0]);); (13, CUDA, cudaDeviceSynchronize();); (7, CUDA, thrust::device_vector<float> dsums(steps);)", "label": "(11, CUDA, float *dptr = thrust::raw_pointer_cast(&dsums[0]);); (14, CUDA, double gpu_sum = thrust::reduce(dsums.begin(),dsums.end());); (8, CUDA, thrust::device_vector<float> dsums(steps);)"}
{"predict": "(9, OpenMP, #pragma omp parallel for)", "label": "(12, OpenMP, #pragma omp parallel for)"}
{"predict": "(12, TBB, my_thread_array = tbb::cache_aligned_allocator<padded_private_worker>().allocate( my_n_thread );)", "label": "(12, TBB, my_thread_array = tbb::cache_aligned_allocator<padded_private_worker>().allocate( my_n_thread );)"}
{"predict": "(40, CUDA, cudaFree(d_inPtr);); (23, CUDA, cudaMalloc((void**)&d_inPtr, size * sizeof(float));); (29, CUDA, cudaMemcpy(&result_gpu, &d_outPtr[0], sizeof(float), cudaMemcpyDeviceToHost);); (24, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);); (35, CUDA, cudaFree(d_outPtr);); (20, CUDA, cudaMalloc((void**)&d_outPtr, size * sizeof(float));)", "label": "(40, CUDA, cudaFree(d_inPtr);); (24, CUDA, cudaMalloc((void**)&d_outPtr, size * sizeof(float));); (30, CUDA, cudaMemcpy(&result_gpu, &d_outPtr[0], sizeof(float), cudaMemcpyDeviceToHost);); (24, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);); (35, CUDA, cudaFree(d_outPtr);); (19, CUDA, cudaMalloc((void**)& d_inPtr, size * sizeof(float));)"}
{"predict": "(38, CUDA, err = cudaEventRecord(start, 0);); (83, CUDA, cudaEventDestroy(start);); (84, CUDA, cudaEventDestroy(stop);); (14, CUDA, err = cudaMalloc(&d_A, length);); (81, CUDA, cudaEventElapsedTime(elapsed_time, start, stop);); (22, CUDA, err = cudaMalloc(&d_C, length);); (18, CUDA, err = cudaMalloc(&d_B, length);); (34, CUDA, err = cudaEventRecord(stop, 0);); (29, CUDA, err = cudaEventSynchronize(stop);); (66, CUDA, err = cudaEventRecord(stop);); (21, CUDA, err = cudaMemcpy(d_B, B, length, cudaMemcpyHostToDevice);); (13, CUDA, err = cudaEventCreate(&stop);); (70, CUDA, err = cudaFree(d_A);); (71, CUDA, err = cudaFree(d_B);); (69, CUDA, err = cudaFree(d_C);); (7, CUDA, err = cudaEventCreate(&start);); (18, CUDA, err = cudaMemcpy(d_A, A, length, cudaMemcpyHostToDevice);)", "label": "(88, CUDA, cudaEventSynchronize(stop);); (70, CUDA, err = cudaGetLastError();); (19, CUDA, err = cudaMalloc(&d_A, length);); (12, CUDA, cudaEventRecord(start, 0);); (78, CUDA, err = cudaFree(d_C);); (87, CUDA, cudaEventDestroy(start);); (74, CUDA, err = cudaFree(d_B);); (21, CUDA, err = cudaMalloc(&d_B, length);); (78, CUDA, cudaEventRecord(stop, 0);); (32, CUDA, err = cudaMemcpy(d_A, A, length, cudaMemcpyHostToDevice);); (80, CUDA, cudaEventElapsedTime(elapsed_time, start, stop);); (1, CUDA, cudaEventCreate(&start);); (66, CUDA, err = cudaFree(d_A);); (30, CUDA, err = cudaMemcpy(d_B, B, length, cudaMemcpyHostToDevice);); (16, CUDA, err = cudaMalloc(&d_C, length);); (78, CUDA, cudaEventDestroy(stop);); (-3, CUDA, cudaEventCreate(&stop);); (57, CUDA, err = cudaMemcpy(C, d_C, length, cudaMemcpyDeviceToHost);)"}
{"predict": "(18, OpenMP, #pragma omp target update to(mapper)); (19, OpenMP, #pragma omp target update from(mapper)); (15, OpenMP, #pragma omp target map(mapper)); (18, OpenMP, #pragma omp target update to(mapper) from(mapper)); (11, OpenMP, #pragma omp target enter data map(mapper)); (12, OpenMP, #pragma omp target enter data map(mapper) to(mapper)); (15, OpenMP, #pragma omp target update from(mapper)); (4, OpenMP, #pragma omp target enter data map(mapper) to(mapper)); (5, OpenMP, #pragma omp target enter data map(mapper) to(mapper) from(mapper)); (8, OpenMP, #pragma omp target enter data map(mapper) to(mapper) from(mapper)); (11, OpenMP, #pragma omp target update to(mapper)); (-3, OpenMP, #pragma omp target enter data map(mapper)); (6, OpenMP, #pragma omp target enter data map(mapper) from(mapper)); (-9, OpenMP, #pragma omp target enter data map(mapper); (-6, OpenMP, #pragma omp target enter data map(mapper) (); (-9, OpenMP, #pragma omp target enter data map(mapper)); (-3, OpenMP, #pragma omp target enter data map(mapper) to(mapper))", "label": "(16, OpenMP, #pragma omp target defaultmap (scalar:); (3, OpenMP, #pragma omp target defaultmap(); (10, OpenMP, #pragma omp target defaultmap(tofrom)); (11, OpenMP, #pragma omp target defaultmap(tofrom,); (16, OpenMP, #pragma omp target defaultmap(tofrom:scalar) defaultmap(tofrom:scalar)); (-3, OpenMP, #pragma omp target defaultmap); (19, OpenMP, #pragma omp target defaultmap(tofrom: something)); (11, OpenMP, #pragma omp target defaultmap(tofrom, scalar); (2, OpenMP, #pragma omp target defaultmap (tofrom:); (-3, OpenMP, #pragma omp target defaultmap()); (-2, OpenMP, #pragma omp target defaultmap(tofrom)"}
{"predict": "(29, CUDA, CUDA_CALL(cudaDeviceSynchronize());); (7, CUDA, CUDA_CALL(cudaMalloc((void**)&data, size * sizeof(T)));); (50, CUDA, CUDA_CALL(cudaFree(data));)", "label": "(52, CUDA, CUDA_CALL(cudaFree(data));); (28, CUDA, CUDA_CALL(cudaDeviceSynchronize());); (6, CUDA, CUDA_CALL(cudaMalloc((void**)&data, size * sizeof(T)));)"}
{"predict": "(16, MPI, MPI_Comm_size(comm, &size);); (28, MPI, MPI_Abort(comm, 1);); (15, MPI, MPI_Comm_rank(comm, &rank);)", "label": "(29, MPI, MPI_Abort(comm, 1);); (15, MPI, MPI_Comm_size(comm, &size);); (15, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(22, OpenMP, #pragma omp parallel sections shared (a, b, c, d, f, k)); (27, OpenMP, #pragma omp parallel sections shared(h, B::x)); (20, OpenMP, #pragma omp parallel sections shared (argv[1])); (39, OpenMP, #pragma omp parallel sections firstprivate(i)); (26, OpenMP, #pragma omp parallel sections shared(e, g)); (29, OpenMP, #pragma omp parallel sections shared(i)); (8, OpenMP, #pragma omp parallel sections shared ()); (39, OpenMP, #pragma omp parallel sections shared(j)); (8, OpenMP, #pragma omp parallel sections shared (argc); (25, OpenMP, #pragma omp parallel sections private(i)); (10, OpenMP, #pragma omp parallel sections shared (argc)); (1, OpenMP, #pragma omp parallel sections shared (); (12, OpenMP, #pragma omp parallel sections shared (argc > 0? argv[1] : argv[2])); (-3, OpenMP, #pragma omp parallel sections shared); (27, OpenMP, #pragma omp parallel sections shared(j)); (14, OpenMP, #pragma omp parallel sections shared(ca)); (5, OpenMP, #pragma omp parallel sections shared (S1)); (16, OpenMP, #pragma omp parallel sections shared(da))", "label": "(25, OpenMP, #pragma omp parallel sections shared(argv[1])); (34, OpenMP, #pragma omp parallel sections shared(h, B::x)); (41, OpenMP, #pragma omp parallel sections shared(i)); (20, OpenMP, #pragma omp parallel sections shared(a, b, c, d, f, k)); (9, OpenMP, #pragma omp parallel sections shared(argc); (10, OpenMP, #pragma omp parallel sections shared(argc,); (33, OpenMP, #pragma omp parallel sections firstprivate(i)); (22, OpenMP, #pragma omp parallel sections shared(ca)); (19, OpenMP, #pragma omp parallel sections shared(ba)); (28, OpenMP, #pragma omp parallel sections private(i)); (-3, OpenMP, #pragma omp parallel sections shared); (20, OpenMP, #pragma omp parallel sections shared(da)); (5, OpenMP, #pragma omp parallel sections shared(argc > 0? argv[1] : argv[2])); (8, OpenMP, #pragma omp parallel sections shared(S1)); (5, OpenMP, #pragma omp parallel sections shared(argc)); (30, OpenMP, #pragma omp parallel sections shared(j)); (-5, OpenMP, #pragma omp parallel sections shared()); (-8, OpenMP, #pragma omp parallel sections shared(); (15, OpenMP, #pragma omp parallel sections shared(e, g))"}
{"predict": "(45, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(45, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(25, OpenMP, #pragma omp parallel for)", "label": "(25, OpenMP, #pragma omp parallel for)"}
{"predict": "(13, CUDA, extern __shared__ __align__(sizeof(float)) unsigned char buf[];)", "label": "(13, CUDA, extern __shared__ char buf[];)"}
{"predict": "(66, MPI, MPI_Comm_size(fd->comm, &nprocs);); (66, MPI, MPI_Comm_rank(fd->comm, &myrank);)", "label": "(66, MPI, MPI_Comm_size(fd->comm, &nprocs);); (66, MPI, MPI_Comm_rank(fd->comm, &myrank);)"}
{"predict": "(6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &world_size);)", "label": "(6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &world_size);)"}
{"predict": "(138, OpenMP, #pragma omp distribute parallel for lastprivate(j)); (44, OpenMP, #pragma omp distribute parallel for lastprivate(argv[1])); (19, OpenMP, #pragma omp distribute parallel for lastprivate(argc); (60, OpenMP, #pragma omp distribute parallel for lastprivate(ba)); (55, OpenMP, #pragma omp distribute parallel for lastprivate(ca)); (27, OpenMP, #pragma omp distribute parallel for lastprivate(argc > 0? argv[1] : argv[2])); (18, OpenMP, #pragma omp distribute parallel for lastprivate(argc,); (51, OpenMP, #pragma omp distribute parallel for lastprivate(da)); (32, OpenMP, #pragma omp distribute parallel for lastprivate(S1)); (2, OpenMP, #pragma omp distribute parallel for lastprivate); (59, OpenMP, #pragma omp distribute parallel for lastprivate(xa)); (36, OpenMP, #pragma omp distribute parallel for lastprivate(a, b, c, d, f)); (83, OpenMP, #pragma omp distribute parallel for lastprivate(h, B::x)); (96, OpenMP, #pragma omp distribute parallel for lastprivate(i)); (39, OpenMP, #pragma omp distribute parallel for lastprivate(2 * 2)); (140, OpenMP, #pragma omp distribute parallel for lastprivate(si)); (18, OpenMP, #pragma omp distribute parallel for lastprivate(argc)); (45, OpenMP, #pragma omp distribute parallel for lastprivate(S2::S2s)); (92, OpenMP, #pragma omp distribute parallel for lastprivate(m)); (51, OpenMP, #pragma omp distribute parallel for lastprivate(S2::S2sc)); (29, OpenMP, #pragma omp distribute parallel for lastprivate(conditional: argc) lastprivate(conditional:); (130, OpenMP, #pragma omp distribute parallel for firstprivate(m) lastprivate(m)); (39, OpenMP, #pragma omp distribute parallel for lastprivate(S2::S2sc)); (71, OpenMP, #pragma omp distribute parallel for lastprivate(e, g)); (4, OpenMP, #pragma omp distribute parallel for lastprivate(argc); (59, OpenMP, #pragma omp distribute parallel for lastprivate(S1)); (63, OpenMP, #pragma omp distribute parallel for lastprivate(S2::S2s)); (77, OpenMP, #pragma omp distribute parallel", "label": "(82, OpenMP, #pragma omp distribute parallel for lastprivate(xa)); (15, OpenMP, #pragma omp distribute parallel for lastprivate(); (59, OpenMP, #pragma omp distribute parallel for lastprivate(2 * 2)); (73, OpenMP, #pragma omp distribute parallel for lastprivate(da)); (93, OpenMP, #pragma omp distribute parallel for safelen(5)); (137, OpenMP, #pragma omp distribute parallel for lastprivate(j)); (25, OpenMP, #pragma omp distribute parallel for lastprivate(argc,); (2, OpenMP, #pragma omp target); (109, OpenMP, #pragma omp distribute parallel for lastprivate(B::x)); (62, OpenMP, #pragma omp distribute parallel for lastprivate(ca)); (138, OpenMP, #pragma omp distribute parallel for firstprivate(m) lastprivate(m)); (116, OpenMP, #pragma omp distribute parallel for lastprivate(i)); (142, OpenMP, #pragma omp distribute parallel for lastprivate(n) firstprivate(n)); (94, OpenMP, #pragma omp distribute parallel for lastprivate(m)); (32, OpenMP, #pragma omp distribute parallel for lastprivate(S1)); (97, OpenMP, #pragma omp distribute parallel for lastprivate(h)); (-5, OpenMP, #pragma omp distribute parallel for lastprivate); (39, OpenMP, #pragma omp distribute parallel for lastprivate(argv[1])); (18, OpenMP, #pragma omp distribute parallel for lastprivate(argc > 0? argv[1] : argv[2])); (32, OpenMP, #pragma omp distribute parallel for lastprivate(a, b, c, d, f)); (140, OpenMP, #pragma omp distribute parallel for lastprivate(si)); (20, OpenMP, #pragma omp distribute parallel for lastprivate(argc)); (80, OpenMP, #pragma omp distribute parallel for lastprivate(e, g)); (3, OpenMP, #pragma omp distribute parallel for lastprivate(argc); (42, OpenMP, #pragma omp distribute parallel for lastprivate(ba)); (62, OpenMP, #pragma omp distribute parallel for lastprivate(S2::S2s)); (66, OpenMP, #pragma omp distribute parallel for lastprivate(S2::S2sc)); (95, OpenMP, #pragma omp distribute parallel for private(xa), lastprivate(xa)); (-7, OpenMP, #pragma omp distribute parallel for lastprivate()); (-19, OpenMP, #pragma omp teams)"}
{"predict": "(25, MPI, MPI_Comm_size(MPI_COMM_WORLD,&Num_procs);); (131, OpenMP, #pragma omp parallel for reduction(+:error)); (76, MPI, MPI_Bcast(&iterations,1, MPI_INT, root, MPI_COMM_WORLD);); (74, MPI, MPI_Bcast(&offset,1, MPI_LONG, root, MPI_COMM_WORLD);); (21, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&my_ID);); (77, OpenMP, omp_set_num_threads(nthread_input);); (18, MPI, MPI_Init(&argc,&argv);); (130, MPI, MPI_Finalize();); (71, MPI, MPI_Bcast(&length,1, MPI_LONG, root, MPI_COMM_WORLD);); (113, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(101, OpenMP, printf(\"Number of threads    = %d\\n\", omp_get_max_threads());); (24, MPI, MPI_Comm_size(MPI_COMM_WORLD,&Num_procs);); (148, MPI, MPI_Finalize();); (104, OpenMP, #pragma omp parallel for); (74, MPI, MPI_Bcast(&length,        1, MPI_LONG, root, MPI_COMM_WORLD);); (21, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&my_ID);); (75, MPI, MPI_Bcast(&nthread_input, 1, MPI_INT,  root, MPI_COMM_WORLD);); (76, OpenMP, omp_set_num_threads(nthread_input);); (16, MPI, MPI_Init(&argc,&argv);); (71, MPI, MPI_Bcast(&iterations,    1, MPI_INT,  root, MPI_COMM_WORLD);); (113, MPI, MPI_Barrier(MPI_COMM_WORLD);); (68, MPI, MPI_Bcast(&offset,        1, MPI_LONG, root, MPI_COMM_WORLD);)"}
{"predict": "(18, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(18, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(9, OpenMP, #pragma omp target); (10, OpenMP, #pragma omp distribute firstprivate(g, g1, svar, sfvar)); (47, OpenMP, #pragma omp distribute firstprivate(t_var, vec, s_arr, s_arr, var, var, svar)); (7, OpenMP, #pragma omp teams)", "label": "(9, OpenMP, #pragma omp target); (10, OpenMP, #pragma omp distribute simd firstprivate(g, g1, svar, sfvar)); (46, OpenMP, #pragma omp distribute simd firstprivate(t_var, vec, s_arr, s_arr, var, var, svar)); (7, OpenMP, #pragma omp teams)"}
{"predict": "(7, CUDA, __shared__ block_radix_sort_t::TempStorage temp_storage;)", "label": "(7, CUDA, __shared__ block_radix_sort_t::TempStorage temp_storage;)"}
{"predict": "(12, CUDA, CUDASimulation cudaSimulation(model);)", "label": "(12, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(22, CUDA, cudaMalloc((void**)&d_outPtr, size * sizeof(float));); (25, CUDA, cudaMemcpy(&result_gpu, &d_outPtr[0], sizeof(float), cudaMemcpyDeviceToHost);); (22, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);); (29, CUDA, cudaFree(d_inPtr);); (29, CUDA, cudaFree(d_outPtr);); (18, CUDA, cudaMalloc((void**)& d_inPtr, size * sizeof(float));)", "label": "(33, CUDA, cudaFree(d_inPtr);); (21, CUDA, cudaMalloc((void**)&d_outPtr, size * sizeof(float));); (26, CUDA, cudaMemcpy(&result_gpu, &d_outPtr[0], sizeof(float), cudaMemcpyDeviceToHost);); (21, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);); (28, CUDA, cudaFree(d_outPtr);); (16, CUDA, cudaMalloc((void**)& d_inPtr, size * sizeof(float));)"}
{"predict": "(6, TBB, tbb::flow::broadcast_node<my_type> my_broadcast_node(g);); (14, TBB, double barrier = tbb::tick_count::now();); (1, TBB, tbb::flow::queue_node<my_type> my_queue(g);); (13, TBB, return (tbb::tick_count::now() - t0).seconds();); (-2, TBB, tbb::tick_count t0;); (5, TBB, using namespace tbb::flow;)", "label": "(3, TBB, tbb::flow::broadcast_node<my_type> my_broadcast_node(g);); (7, TBB, const tbb::tick_count t0 = tbb::tick_count::now();); (14, TBB, return (tbb::tick_count::now() - t0).seconds();); (-1, TBB, tbb::flow::queue_node<my_type> my_queue(g);)"}
{"predict": "(27, MPI, MPI_Allreduce( &temperr, &errcomp, 1, MPI_FLOAT, MPI_MAX, slucomm);); (24, MPI, MPI_Allreduce( &tempxnorm, &xnorm, 1, MPI_DOUBLE, MPI_MAX, slucomm);); (21, MPI, MPI_Allreduce( &temperr, &err, 1, MPI_DOUBLE, MPI_MAX, slucomm);)", "label": "(26, MPI, MPI_Allreduce( &temperr, &errcomp, 1, MPI_FLOAT, MPI_MAX, slucomm);); (23, MPI, MPI_Allreduce( &tempxnorm, &xnorm, 1, MPI_DOUBLE, MPI_MAX, slucomm);); (20, MPI, MPI_Allreduce( &temperr, &err, 1, MPI_DOUBLE, MPI_MAX, slucomm);)"}
{"predict": "(22, OpenMP, #pragma omp target data map(alloc: arr[0:0])); (6, OpenMP, #pragma omp target map(present, alloc: arr[0:0])); (21, OpenMP, #pragma omp target exit data map(from: arr[0:0]))", "label": "(8, OpenMP, #pragma omp target map(present, alloc: arr[0:0])); (6, OpenMP, #pragma omp target data map(alloc: arr[0:5])); (19, OpenMP, #pragma omp target data map(alloc: arr[0:0]))"}
{"predict": "(20, MPI, MPI_Comm_free(&abc_intra);); (17, MPI, MPI_Comm_free(&ac_intra);); (13, MPI, MPI_Barrier(inter);); (19, MPI, MPI_Intercomm_merge(inter, 0, &abc_intra);); (18, MPI, MPI_Comm_free(&ab_c_inter);); (18, MPI, MPI_Comm_free(&ab_intra);); (18, MPI, MPI_Comm_free(&ac_intra);); (18, MPI, MPI_Comm_free(&ab_inter);); (4, MPI, MPI_Intercomm_create( ac_intra, 0, MPI_COMM_WORLD, 0, 0, &inter );); (18, MPI, MPI_Comm_free(&inter);); (3, MPI, MPI_Comm_free(&ab_intra);); (10, MPI, MPI_Comm_free(&abc_intra);); (6, MPI, MPI_Comm_free(&ac_intra);); (2, MPI, MPI_Comm_free(&ab_c_inter);); (5, MPI, MPI_Comm_free(&ac_intra);)", "label": "(14, MPI, err = MPI_Barrier(ab_c_inter);); (16, MPI, err = MPI_Intercomm_merge(ab_c_inter, 0, &abc_intra);); (25, MPI, MPI_Comm_disconnect(&ab_inter);); (21, MPI, MPI_Comm_free(&ab_intra);); (18, MPI, MPI_Comm_free(&abc_intra);); (5, MPI, err = MPI_Intercomm_create(ab_intra, 0, ac_intra, count, tag, &ab_c_inter);); (17, MPI, MPI_Comm_free(&ab_c_inter);); (18, MPI, MPI_Comm_free(&ac_intra);); (20, MPI, MPI_Comm_disconnect(&ac_inter);); (10, MPI, err = MPI_Barrier(abc_intra);)"}
{"predict": "(56, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)", "label": "(2, MPI, MPI_Init(&argc, &argv);); (55, MPI, MPI_Finalize();)"}
{"predict": "(4, OpenMP, #pragma omp parallel num_threads(2)); (9, OpenMP, #pragma omp atomic)", "label": "(5, OpenMP, #pragma omp parallel num_threads(2)); (3, OpenMP, omp_set_nested(1);)"}
{"predict": "(26, OpenCL, err = clReleaseEvent(events[i]);)", "label": "(23, OpenCL, clReleaseEvent(events[i]);)"}
{"predict": "(6, OpenMP, #pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp parallel for)"}
{"predict": "(19, TBB, tbb::global_control c(tbb::global_control::max_allowed_parallelism, p);); (12, TBB, utility::report_elapsed_time((tbb::tick_count::now() - mainStartTime).seconds());); (0, TBB, tbb::tick_count mainStartTime = tbb::tick_count::now();); (20, TBB, tbb::flow::graph g;)", "label": "(720, TBB, utility::report_elapsed_time((oneapi::tbb::tick_count::now() - start).seconds());); (15, TBB, oneapi::tbb::global_control c(oneapi::tbb::global_control::max_allowed_parallelism, p);); (19, TBB, oneapi::tbb::flow::graph g;); (11, TBB, oneapi::tbb::tick_count start = oneapi::tbb::tick_count::now();)"}
{"predict": "(45, CUDA, CHECK(cudaStreamCreate(&(streams[n])));); (82, CUDA, CHECK(cudaStreamDestroy(streams[i]));); (92, CUDA, CHECK(cudaFree(d_y));); (20, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice));); (17, CUDA, CHECK(cudaMalloc(&d_y, M));); (91, CUDA, CHECK(cudaFree(d_z));); (55, CUDA, CHECK(cudaStreamDestroy(streams[n]));); (63, CUDA, CHECK(cudaMallocHost(&h_x2, M));); (80, CUDA, CHECK(cudaFreeHost(h_y2));); (80, CUDA, CHECK(cudaFreeHost(h_z2));); (64, CUDA, CHECK(cudaStreamCreate(&(streams[i])));); (10, CUDA, CHECK(cudaMalloc(&d_x, M));); (66, CUDA, CHECK(cudaMemcpyAsync(d_y, h_y2, M, cudaMemcpyHostToDevice, streams[i]));); (7, CUDA, CHECK(cudaMalloc(&d_z, M));); (72, CUDA, CHECK(cudaFree(d_x));); (5, CUDA, CHECK(cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice));); (55, CUDA, CHECK(cudaFreeHost(h_x2));); (54, CUDA, CHECK(cudaFreeHost(h_y2));); (44, CUDA, CHECK(cudaMallocHost(&h_z2, M));); (5, CUDA, CHECK(cudaFree(d_x));); (-2, CUDA, CHECK(cudaMallocHost(&h_y, M));); (67, CUDA, CHECK(cudaFreeHost(h_x2));); (65, CUDA, CHECK(cudaFreeHost(h_y2));); (65, CUDA, CHECK(cudaFreeHost(h_z2));); (44, CUDA, CHECK(cudaMemcpyAsync(d_x, h_x2, M, cudaMemcpyHostToDevice, streams[i]));); (-12, CUDA, CHECK(cudaMallocHost(&h_x, M));); (-9, CUDA, CHECK(cudaMallocHost(&h_y, M));); (1, CUDA, CHECK(cudaFree(d_y));); (19, CUDA, CHECK(cudaMallocHost(&h_y2, M));); (17, CUDA, CHECK(cudaMallocHost(&h_x2, M));); (45, CUDA, CHECK(cudaFreeHost(h_x));); (-18, CUDA, CHECK(cudaMallocHost(&h_z, M));); (1, CUDA, CHECK(cudaFreeHost(h_x));", "label": "(47, CUDA, CHECK(cudaStreamCreate(&(streams[n])));); (84, CUDA, CHECK(cudaStreamDestroy(streams[i]));); (94, CUDA, CHECK(cudaFree(d_y));); (19, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice));); (93, CUDA, CHECK(cudaFree(d_z));); (53, CUDA, CHECK(cudaStreamDestroy(streams[n]));); (58, CUDA, CHECK(cudaMallocHost(&h_x2, M));); (82, CUDA, CHECK(cudaFreeHost(h_y2));); (82, CUDA, CHECK(cudaFreeHost(h_z2));); (66, CUDA, CHECK(cudaStreamCreate(&(streams[i])));); (10, CUDA, CHECK(cudaMalloc(&d_y, M));); (8, CUDA, CHECK(cudaMalloc(&d_x, M));); (83, CUDA, CHECK(cudaFree(d_x));); (53, CUDA, CHECK(cudaMallocHost(&h_z2, M));); (7, CUDA, CHECK(cudaMalloc(&d_z, M));); (73, CUDA, CHECK(cudaFreeHost(h_x2));); (7, CUDA, CHECK(cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice));); (48, CUDA, CHECK(cudaMallocHost(&h_y2, M));)"}
{"predict": "(14, MPI, MPI_Init(&argc, &argv);); (46, MPI, MPI_Finalize();); (13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (23, MPI, MPI_File_set_view(fileh, 0, MPI_INT, MPI_INT, datarep[i], MPI_INFO_NULL);); (29, MPI, MPI_File_close(&fileh);); (38, MPI, MPI_File_delete(TEST_FILENAME, MPI_INFO_NULL);)", "label": "(14, MPI, MPI_Init(&argc, &argv);); (47, MPI, MPI_Finalize();); (33, MPI, MPI_Barrier(MPI_COMM_WORLD);); (12, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (40, MPI, CHECK(MPI_File_delete(TEST_FILENAME, MPI_INFO_NULL));); (27, MPI, CHECK(MPI_File_close(&fileh));); (19, MPI, CHECK(MPI_File_set_view(fileh, 0, MPI_INT, MPI_INT, datarep[i], MPI_INFO_NULL));)"}
{"predict": "(22, MPI, MPI_Recv(NULL, 0, MPI_BYTE, p->nbor, 1, MPI_COMM_WORLD, &status);); (19, MPI, MPI_Send(NULL, 0, MPI_BYTE, p->nbor, 1, MPI_COMM_WORLD);)", "label": "(28, MPI, MPI_Recv(NULL, 0, MPI_BYTE, p->nbor2, 1, MPI_COMM_WORLD, &status);); (22, MPI, MPI_Recv(NULL, 0, MPI_BYTE, p->nbor, 1, MPI_COMM_WORLD, &status);); (25, MPI, MPI_Send(NULL, 0, MPI_BYTE, p->nbor2, 1, MPI_COMM_WORLD);); (34, MPI, latency = (MPI_Wtime() - t0) / (2 * g_latency012_reps);); (18, MPI, MPI_Send(NULL, 0, MPI_BYTE, p->nbor, 1, MPI_COMM_WORLD);); (10, MPI, t0 = MPI_Wtime();)"}
{"predict": "(22, CUDA, cudaGetDeviceProperties(&devProp, 0);); (45, CUDA, cudaMalloc((void**)&gpuMatrix, NUM * sizeof(float));); (45, CUDA, cudaMalloc((void**)&gpuTransposeMatrix, NUM * sizeof(float));); (81, CUDA, cudaFree(gpuMatrix);); (81, CUDA, cudaFree(gpuTransposeMatrix);)", "label": "(20, CUDA, cudaGetDeviceProperties(&devProp, 0);); (81, CUDA, cudaFree(gpuMatrix);); (43, CUDA, cudaMalloc((void**)&gpuMatrix, NUM * sizeof(float));); (81, CUDA, cudaFree(gpuTransposeMatrix);); (43, CUDA, cudaMalloc((void**)&gpuTransposeMatrix, NUM * sizeof(float));)"}
{"predict": "(24, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (19, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (50, MPI, MPI_Finalize();); (47, MPI, MPI_T_finalize();); (18, MPI, MPI_T_init_thread(required, &provided);); (31, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(24, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (19, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (50, MPI, MPI_Finalize();); (47, MPI, MPI_T_finalize();); (18, MPI, MPI_T_init_thread(required, &provided);); (31, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(22, MPI, MPI_Gatherv_c(sendbuf, count, datatype, recvbuf, counts, displs, datatype, root, comm);); (26, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);)", "label": "(22, MPI, MPI_Gatherv_c(sendbuf, count, datatype, recvbuf, counts, displs, datatype, root, comm);); (26, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);)"}
{"predict": "(8, OpenMP, #pragma omp taskloop); (6, OpenMP, #pragma omp parallel); (304, OpenMP, #pragma omp taskloop collapse(2)); (271, OpenMP, #pragma omp taskloop collapse(3)); (285, OpenMP, #pragma omp taskloop collapse(4)); (275, OpenMP, #pragma omp taskloop collapse(2)); (290, OpenMP, #pragma omp taskloop collapse(5)); (294, OpenMP, #pragma omp taskloop collapse(2)); (300, OpenMP, #pragma omp taskloop collapse(3)); (294, OpenMP, #pragma omp taskloop collapse(4)); (267, OpenMP, #pragma omp taskloop collapse(3)); (270, OpenMP, #pragma omp taskloop collapse(4)); (275, OpenMP, #pragma omp taskloop collapse(5)); (280, OpenMP, #pragma omp taskloop collapse(2)); (295, OpenMP, #pragma omp taskloop collapse(2)); (300, OpenMP, #pragma omp taskloop collapse(5)); (267, OpenMP, #pragma omp taskloop collapse(2)); (268, OpenMP, #pragma omp taskloop collapse(4)); (269, OpenMP, #pragma omp taskloop collapse(5)); (284, OpenMP, #pragma omp taskloop collapse(2)); (300, OpenMP, #pragma omp taskloop collapse(2)); (267, OpenMP, #pragma omp taskloop collapse(3)); (268, OpenMP, #pragma omp taskloop collapse(4)); (275, OpenMP, #pragma omp taskloop collapse(2)); (284, OpenMP, #pragma omp taskloop collapse(3)); (300, OpenMP, #pragma omp taskloop collapse(4)); (267, OpenMP, #pragma omp taskloop collapse(5)); (284, OpenMP, #pragma omp taskloop collapse(5)); (300, OpenMP, #pragma omp taskloop collapse(2)); (267, OpenMP, #pragma omp taskloop collapse(2)); (274, OpenMP, #pragma omp taskloop collapse(3)); (275, OpenMP, #pragma omp taskloop collapse(4)); (284, OpenMP, #pragma omp taskloop collapse(5)); (299, OpenMP, #pragma omp taskloop collapse(3)); (268, OpenMP, #pragma omp", "label": "(286, OpenMP, #pragma omp taskloop lastprivate(ii)); (280, OpenMP, #pragma omp taskloop private(ii)); (5, OpenMP, #pragma omp parallel); (5, OpenMP, #pragma omp taskloop); (317, OpenMP, #pragma omp taskloop collapse(2)); (265, OpenMP, #pragma omp taskloop firstprivate(ii)); (270, OpenMP, #pragma omp taskloop linear(ii))"}
{"predict": "(40, MPI, MPI_T_finalize();); (15, MPI, MPI_T_init_thread(required, &provided);)", "label": "(40, MPI, MPI_T_finalize();); (15, MPI, MPI_T_init_thread(required, &provided);)"}
{"predict": "(108, OpenCL, retVal = clReleaseKernel(kernel);); (30, OpenCL, context = clCreateContext(nullptr, 1, &device, nullptr, nullptr, &retVal);); (108, OpenCL, retVal = clReleaseCommandQueue(cmdQ);); (108, OpenCL, retVal = clReleaseContext(context);); (58, OpenCL, kernel = clCreateKernel(pProgram, \"CopyBuffer\", &retVal);); (33, OpenCL, cmdQ = clCreateCommandQueue(context, device, properties, &retVal);); (101, OpenCL, retVal = clReleaseProgram(pProgram);)", "label": "(104, OpenCL, retVal = clReleaseKernel(kernel);); (34, OpenCL, context = clCreateContext(nullptr, 1, &device, nullptr, nullptr, &retVal);); (108, OpenCL, retVal = clReleaseCommandQueue(cmdQ);); (110, OpenCL, retVal = clReleaseContext(context);); (57, OpenCL, kernel = clCreateKernel(pProgram, \"CopyBuffer\", &retVal);); (102, OpenCL, retVal = clReleaseProgram(pProgram);); (33, OpenCL, cmdQ = clCreateCommandQueue(context, device, properties, &retVal);)"}
{"predict": "(14, CUDA, return CUDA(cudaGetLastError());)", "label": "(14, CUDA, return CUDA(cudaGetLastError());)"}
{"predict": "(3, OpenMP, #pragma omp target teams); (5, OpenMP, #pragma omp parallel)", "label": "(1, OpenMP, #pragma omp target teams); (5, OpenMP, #pragma omp parallel)"}
{"predict": "(236, MPI, MPI_Barrier(MPI_COMM_WORLD);); (131, MPI, MPI_Bcast(&n,          1, MPI_INT, root, MPI_COMM_WORLD);); (44, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (292, MPI, MPI_Win_free(&rma_win);); (129, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (129, MPI, MPI_Bcast(&group_size, 1, MPI_INT, root, MPI_COMM_WORLD);); (37, MPI, MPI_Init(&argc,&argv);); (291, MPI, MPI_Info_free(&rma_winfo);); (192, MPI, MPI_Info_set(rma_winfo, \"no_locks\", \"true\");); (192, MPI, MPI_Info_set(rma_winfo, \"accumulate_ordering\", \"none\");); (192, MPI, MPI_Info_set(rma_winfo, \"accumulate_ops\", \"same_op\");); (265, MPI, MPI_Win_detach(rma_win, &rma_winfo);); (295, MPI, MPI_Finalize();); (256, MPI, MPI_Win_detach(&rma_win, &rma_winfo);); (232, MPI, MPI_Barrier(MPI_COMM_WORLD);); (256, MPI, MPI_Win_free(&rma_win);); (29, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);); (182, MPI, MPI_Info_create(&rma_winfo);); (23, MPI, MPI_Win_create(&bottom_buf_in, total_length_in, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &rma_win);); (24, MPI, MPI_Win_create(&top_buf_out,  total_length_out, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &rma_win);); (185, MPI, MPI_Info_set(rma_winfo, \"rma_type\", \"grid\");); (183, MPI, MPI_Info_set(rma_winfo, \"rma_direction\", \"x\");); (25, MPI, MPI_Win_create(&right_buf_out,  total_length_out, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &rma_win);); (140, MPI, MPI_Bcast(&bail_out, 1, MPI_INT, root, MPI_COMM_WORLD);); (277, MPI", "label": "(210, MPI, MPI_Info_create(&rma_winfo);); (212, MPI, MPI_Info_set(rma_winfo, \"no_locks\", \"true\");); (370, MPI, MPI_Info_free(&rma_winfo);); (371, MPI, MPI_Finalize();); (128, MPI, MPI_Bcast(&n,          1, MPI_INT, root, MPI_COMM_WORLD);); (128, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (267, MPI, MPI_Win_fence(MPI_MODE_NOSTORE, rma_winx);); (39, MPI, MPI_Init(&argc,&argv);); (232, MPI, MPI_Barrier(MPI_COMM_WORLD);); (321, MPI, MPI_Reduce(&local_norm, &norm, 1, MPI_DTYPE, MPI_SUM, root, MPI_COMM_WORLD);); (38, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (234, MPI, MPI_Win_fence(MPI_MODE_NOSTORE, rma_winy);); (35, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(7, OpenCL, auto status = clGetDeviceInfo(&mockClDevice, CL_DEVICE_IMAGE_SUPPORT, sizeof(imageSupportInfo), &imageSupportInfo, nullptr);); (27, OpenCL, retVal = clReleaseMemObject(image);)", "label": "(28, OpenCL, retVal = clReleaseMemObject(image);); (6, OpenCL, auto status = clGetDeviceInfo(&mockClDevice, CL_DEVICE_IMAGE_SUPPORT, sizeof(imageSupportInfo), &imageSupportInfo, nullptr);)"}
{"predict": "(15, MPI, MPI_Cart_coords(cart_comm, rank, NR_DIMS, cart_coords);); (36, MPI, MPI_Finalize();); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (11, MPI, MPI_Dims_create(size, NR_DIMS, dims);); (29, MPI, MPI_Barrier(MPI_COMM_WORLD);); (13, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(12, MPI, MPI_Init(&argc, &argv);); (34, MPI, MPI_Finalize();); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (11, MPI, MPI_Dims_create(size, NR_DIMS, dims);); (17, MPI, MPI_Cart_coords(cart_comm, rank, NR_DIMS, cart_coords);); (27, MPI, MPI_Barrier(MPI_COMM_WORLD);); (13, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(27, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(27, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(59, CUDA, cudaFree(df_A);); (59, CUDA, cudaFree(df_B);); (59, CUDA, cudaFree(df_C);); (35, CUDA, cudaMalloc((void **)&df_B, M * N * sizeof(float));); (42, CUDA, cudaStat = cudaStreamSetFlags(stream, cudaStreamNonBlocking);); (34, CUDA, cudaMalloc((void **)&df_C, M * N * sizeof(float));); (39, CUDA, cudaStat = cudaStreamCreate(&stream);); (31, CUDA, cudaMalloc((void **)&df_A, M * K * sizeof(float));)", "label": "(39, CUDA, cudaMalloc((void **)&df_C, M * N * sizeof(float));); (62, CUDA, cudaFree(df_A);); (36, CUDA, cudaMalloc((void **)&df_B, K * N * sizeof(float));); (38, CUDA, cudaStat = cudaStreamCreate(&stream);); (61, CUDA, cudaFree(df_C);); (32, CUDA, cudaMalloc((void **)&df_A, M * K * sizeof(float));); (55, CUDA, cudaStreamDestroy(stream);); (48, CUDA, cudaStreamSynchronize(stream);); (56, CUDA, cudaFree(df_B);)"}
{"predict": "(179, CUDA, checkCudaErrors(cudaFreeHost(h0));); (101, CUDA, checkCudaErrors(cudaEventRecord(start_event, gpuid[0]));); (119, CUDA, checkCudaErrors(cudaMalloc(&buf, buf_size));); (174, CUDA, checkCudaErrors(cudaEventSynchronize(stop_event));); (175, CUDA, checkCudaErrors(cudaEventElapsedTime(&time_memcpy, start_event, stop_event));); (178, CUDA, checkCudaErrors(cudaEventDestroy(stop_event));); (103, CUDA, checkCudaErrors(cudaEventRecord(stop_event, gpuid[1]));); (91, CUDA, checkCudaErrors(cudaEventCreate(&stop_event));); (81, CUDA, checkCudaErrors(cudaSetDevice(gpuid[1]));); (93, CUDA, checkCudaErrors(cudaEventCreate(&start_event));); (178, CUDA, checkCudaErrors(cudaFree(buf));); (91, CUDA, checkCudaErrors(cudaEventFlags(eventflags));); (79, CUDA, checkCudaErrors(cudaMalloc(&g1, buf_size));); (91, CUDA, checkCudaErrors(cudaEventSynchronize(start_event));); (165, CUDA, checkCudaErrors(cudaEventDestroy(start_event));); (75, CUDA, checkCudaErrors(cudaMalloc(&g0, buf_size));); (31, CUDA, checkCudaErrors(cudaSetDevice(i));); (93, CUDA, checkCudaErrors(cudaEventElapsedTime(&time_memcpy, start_event, stop_event));); (3, CUDA, checkCudaErrors(cudaGetDeviceCount(&gpu_n));); (153, CUDA, checkCudaErrors(cudaEventSynchronize(stop_event));); (74, CUDA, checkCudaErrors(cudaMallocHost(&h1, buf_size));); (29, CUDA, checkCudaErrors(cudaGetDeviceProperties(&prop[i], i));); (117, CUDA, checkCudaErrors(cudaMemcpy(g0, h0, buf_size, cudaMemcpyHostToDevice));); (165, CUDA, checkCudaErrors(cudaFree(g1));); (2, CUDA, checkCudaErrors(cudaSetDevice(gpuid[0]));); (93, CUDA, checkCudaErrors(cudaMemcpy(g1, h1, buf_size, cudaMemcpyHostToDevice));); (62, CUDA, checkCudaErrors(cudaDeviceEnablePeerAccess(gpuid[1], 0));); (159, CUDA, checkCuda", "label": "(46, CUDA, checkCudaErrors(cudaDeviceCanAccessPeer(&can_access_peer, i, j));); (99, CUDA, checkCudaErrors(cudaEventCreateWithFlags(&stop_event, eventflags));); (101, CUDA, checkCudaErrors(cudaEventRecord(start_event, 0));); (107, CUDA, checkCudaErrors(cudaMemcpy(g1, g0, buf_size, cudaMemcpyDefault));); (190, CUDA, checkCudaErrors(cudaFreeHost(h0));); (142, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (156, CUDA, checkCudaErrors(cudaMemcpy(h0, g0, buf_size, cudaMemcpyDefault));); (109, CUDA, checkCudaErrors(cudaEventRecord(stop_event, 0));); (6, CUDA, checkCudaErrors(cudaGetDeviceCount(&gpu_n));); (66, CUDA, checkCudaErrors(cudaSetDevice(gpuid[0]));); (181, CUDA, checkCudaErrors(cudaFree(g0));); (20, CUDA, checkCudaErrors(cudaGetDeviceProperties(&prop[i], i));); (74, CUDA, checkCudaErrors(cudaMalloc(&g0, buf_size));); (176, CUDA, checkCudaErrors(cudaEventDestroy(stop_event));); (62, CUDA, checkCudaErrors(cudaDeviceEnablePeerAccess(gpuid[1], 0));); (97, CUDA, checkCudaErrors(cudaMemcpy(g0, g1, buf_size, cudaMemcpyDefault));); (102, CUDA, checkCudaErrors(cudaEventElapsedTime(&time_memcpy, start_event, stop_event));); (171, CUDA, checkCudaErrors(cudaEventDestroy(start_event));); (59, CUDA, checkCudaErrors(cudaSetDevice(gpuid[1]));); (70, CUDA, checkCudaErrors(cudaMalloc(&g1, buf_size));); (58, CUDA, checkCudaErrors(cudaDeviceEnablePeerAccess(gpuid[0], 0));); (163, CUDA, checkCudaErrors(cudaDeviceDisablePeerAccess(gpuid[0]));); (160, CUDA, checkCudaErrors(cudaDeviceDisablePeerAccess(gpuid[1]));); (76, CUDA, checkCudaErrors(cudaEventCreateWithFlags(&start_event, eventflags));); (173, CUDA, checkCudaErrors(cudaSetDevice(i));); (107, CUDA, checkCudaErrors(cudaMemcpy(g0, h0, buf_size, cudaMemcpyDefault));); (91, CUDA, checkCudaErrors(cudaEventSynchronize(stop_event));); (166, CUDA, checkCudaErrors(cudaFree(g1));)"}
{"predict": "(3, CUDA, Check_CudaErroorFun(\"At the beginning.\");)", "label": "(3, CUDA, Check_CudaErroorFun(\"At the beginning.\");)"}
{"predict": "(2, MPI, MPI_Comm_size(MPI_COMM_WORLD, &result.size);); (2, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &result.rank);); (13, MPI, MPI_Comm_rank(result.local, &lrank);); (0, MPI, MPI_Init(&argc, &argv);); (26, MPI, MPI_Finalize();); (11, MPI, MPI_Bcast(&lrank, 1, MPI_INT, 0, result.local);)", "label": "(4, MPI, MPI_Comm_size(result.global, &result.size);); (4, MPI, MPI_Comm_rank(result.global, &result.rank);); (1, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &result.global);); (15, MPI, MPI_Comm_rank(result.local, &result.local_rank);); (12, MPI, MPI_Comm_split(result.global, result.group, result.rank, &result.local);); (-3, MPI, MPI_Init(nullptr, nullptr);); (11, MPI, MPI_Comm_size(result.local, &result.local_size);)"}
{"predict": "(18, MPI, MPI_Barrier(MPI_COMM_WORLD);); (25, MPI, time_started = MPI_Wtime();); (167, MPI, double wtime_ended = MPI_Wtime();); (25, MPI, time_ended = MPI_Wtime();); (157, MPI, MPI_Allreduce(&tmp_rc, &all_rc, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);)", "label": "(17, MPI, MPI_Barrier(MPI_COMM_WORLD);); (26, MPI, double start_compare = MPI_Wtime();); (199, MPI, double end_compare = MPI_Wtime();)"}
{"predict": "(24, CUDA, CUDASimulation cudaSimulation(model);)", "label": "(24, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(52, MPI, MPI_Comm_free(&comm2);); (46, MPI, MPI_Ssend(buffer, 0, MPI_INT, rank, 1, MPI_COMM_WORLD);); (31, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &comm2);); (50, MPI, MPI_Comm_free(&comm4);); (43, MPI, MPI_Barrier(comm4);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (40, MPI, MPI_Recv(buffer, 0, MPI_INT, rank, 2, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (27, MPI, MPI_Ssend(buffer, 0, MPI_INT, rank, 0, MPI_COMM_WORLD);); (46, MPI, MPI_Comm_free(&comm1);); (19, MPI, MPI_Comm_dup(comm2, &comm4);); (27, MPI, MPI_Recv(buffer, 0, MPI_INT, rank, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (5, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (22, MPI, MPI_Comm_dup(comm1, &comm2);); (-2, MPI, MPI_Init_thread(&argc, &argv, MPI_THREAD_SERIALIZED, &provided);); (17, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &comm1);)", "label": "(54, MPI, MPI_Comm_free(&comm2);); (46, MPI, MPI_Ssend(buffer, 0, MPI_INT, rank, 1, MPI_COMM_WORLD);); (31, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &comm2);); (43, MPI, MPI_Barrier(comm4);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (38, MPI, MPI_Comm_dup(comm1, &comm4);); (46, MPI, MPI_Comm_free(&comm4);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (45, MPI, MPI_Comm_free(&comm1);); (39, MPI, MPI_Recv(buffer, 0, MPI_INT, rank, 2, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (30, MPI, MPI_Ssend(buffer, 0, MPI_INT, rank, 0, MPI_COMM_WORLD);); (21, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &comm1);)"}
{"predict": "(70, MPI, MPI_Waitall((NUMBER_OF_NEIGHBORS * 2), requests, MPI_STATUSES_IGNORE);); (81, MPI, time = MPI_Wtime() - time;); (64, MPI, time = MPI_Wtime();); (65, MPI, MPI_Irecv(recv_buffers[i], size, MPI_BYTE, neighbours[i], 0, MPI_COMM_WORLD, &requests[i]);); (65, MPI, MPI_Isend(send_buffers[i], size, MPI_BYTE, neighbours[i], 0, MPI_COMM_WORLD, &requests[NUMBER_OF_NEIGHBORS + i]);); (99, MPI, MPI_Send(&report, sizeof(imb_p2p_report_t), MPI_BYTE, 0, 1, MPI_COMM_WORLD);); (109, MPI, MPI_Recv(&report, sizeof(imb_p2p_report_t), MPI_BYTE, source_rank, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)", "label": "(75, MPI, MPI_Waitall((NUMBER_OF_NEIGHBORS * 2), requests, MPI_STATUSES_IGNORE);); (97, MPI, time = MPI_Wtime() - time;); (71, MPI, MPI_Isend(send_buffers[i], size, MPI_BYTE, neighbours[i], 0, MPI_COMM_WORLD, &requests[NUMBER_OF_NEIGHBORS + i]);); (78, MPI, time = MPI_Wtime();); (101, MPI, MPI_Send(&report, sizeof(imb_p2p_report_t), MPI_BYTE, 0, 1, MPI_COMM_WORLD);); (65, MPI, MPI_Irecv(recv_buffers[i], size, MPI_BYTE, neighbours[i], 0, MPI_COMM_WORLD, &requests[i]);); (108, MPI, MPI_Recv(&report, sizeof(imb_p2p_report_t), MPI_BYTE, source_rank, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)"}
{"predict": "(59, CUDA, cudaEventRecord( evStop );); (41, CUDA, cudaEventRecord( evStart );)", "label": "(59, CUDA, cudaEventRecord( evStop );); (75, CUDA, cudaFree( in );); (72, CUDA, cudaEventDestroy( evStart );); (52, CUDA, cudaEventRecord( evStart );); (73, CUDA, cudaFree( out );); (70, CUDA, cudaEventDestroy( evStop );)"}
{"predict": "(35, OpenMP, #pragma omp target teams distribute parallel for map(a, b))", "label": "(35, OpenMP, #pragma omp target teams distribute parallel for map(a, b))"}
{"predict": "(40, CUDA, checkCudaErrors(cudaMemset(d_Output, 0, QRNG_DIMENSIONS * N * sizeof(float)));); (44, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (124, CUDA, checkCudaErrors(cudaFree(d_Output));)", "label": "(40, CUDA, checkCudaErrors(cudaMemset(d_Output, 0, QRNG_DIMENSIONS * N * sizeof(float)));); (44, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (124, CUDA, checkCudaErrors(cudaFree(d_Output));)"}
{"predict": "(15, CUDA, qmc_cuda::cuda_check(cudaGetLastError(), \"vKKwij_to_vwKiKj\");); (15, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize(), \"vKKwij_to_vwKiKj\");)", "label": "(15, CUDA, qmc_cuda::cuda_check(cudaGetLastError(), \"vKKwij_to_vwKiKj\");); (15, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize(), \"vKKwij_to_vwKiKj\");)"}
{"predict": "(9, OpenCL, auto userEvent = clCreateUserEvent(pContext, &retVal);); (30, OpenCL, retVal = clReleaseEvent(userEvent);)", "label": "(9, OpenCL, auto userEvent = clCreateUserEvent(pContext, &retVal);); (30, OpenCL, retVal = clReleaseEvent(userEvent);)"}
{"predict": "(10, OpenMP, #pragma omp teams distribute parallel for); (8, OpenMP, #pragma omp target); (12, OpenMP, #pragma omp teams distribute parallel for if (parallel: false))", "label": "(10, OpenMP, #pragma omp teams distribute parallel for); (8, OpenMP, #pragma omp target); (12, OpenMP, #pragma omp teams distribute parallel for if (parallel: false))"}
{"predict": "(26, MPI, MPI_Pack_external_size((char *) \"external32\", 1, MPI_INT, &sizeoftype);); (37, MPI, MPI_Pack_external((char *) \"external32\", array, 1, MPI_INT, buf, sizeoftype, MPI_BYTE, &position);); (9, MPI, MPI_Pack_external_size((char *) \"external32\", 0, MPI_INT, &sizeofint);); (63, MPI, MPI_Type_free(&parent_type);); (18, MPI, MPI_Type_commit(&parent_type);); (45, MPI, MPI_Unpack_external((char *) \"external32\", buf, sizeoftype, MPI_BYTE, array, 1, MPI_INT, &position);); (18, MPI, MPI_Pack_external((char *) \"external32\", array, 1, MPI_INT, buf, sizeoftype, MPI_BYTE, &position);); (62, MPI, MPI_Datatype_free(&parent_type);); (16, MPI, MPI_Type_contiguous(10, MPI_INT, &parent_type);)", "label": "(12, MPI, MPI_Pack_external_size((char *) \"external32\", 1, MPI_INT, &sizeofint);); (66, MPI, MPI_Type_free(&parent_type);); (36, MPI, MPI_Pack_external((char *) \"external32\", array, 1, parent_type, buf, sizeoftype, &position);); (46, MPI, MPI_Unpack_external((char *) \"external32\", buf, sizeoftype, &position, array, 1, parent_type);); (21, MPI, MPI_Pack_external_size((char *) \"external32\", 1, parent_type, &sizeoftype);); (18, MPI, MPI_Type_commit(&parent_type);); (15, MPI, MPI_Type_vector(10, 2, 2, MPI_INT, &parent_type);)"}
{"predict": "(8, SYCL, auto  h_b = sycl::malloc_host<TB>( nelems, q);); (18, SYCL, auto  B = sycl::malloc_device<TB>( nelems, q);); (6, SYCL, auto  h_a = sycl::malloc_host<TA>( nelems, q);); (49, SYCL, sycl::free(h_a, q);); (50, SYCL, sycl::free(h_c, q);); (48, SYCL, sycl::free(h_b, q);); (15, SYCL, auto  C = sycl::malloc_device(TC>( nelems, q);); (9, SYCL, auto  h_c = sycl::malloc_host<TC>( nelems, q);); (16, SYCL, auto  A = sycl::malloc_device(TA>( nelems, q);); (44, SYCL, sycl::free(A, q);); (42, SYCL, sycl::free(B, q);); (44, SYCL, sycl::free(C, q);)", "label": "(82, SYCL, auto isA_fp16 = (typeid(TA) == typeid(sycl::half));); (28, SYCL, sycl::free(h_a, q);); (6, SYCL, auto h_c = sycl::malloc_host<TC>( nelems, q);); (4, SYCL, auto h_b = sycl::malloc_host<TB>( nelems, q);); (111, SYCL, sycl::free(h_c, q);); (14, SYCL, auto  A = sycl::malloc_device<TA>( nelems, q);); (94, SYCL, auto isC_fp16 = (typeid(TC) == typeid(sycl::half));); (23, SYCL, sycl::free(h_b, q);); (49, SYCL, sycl::free(A, q);); (47, SYCL, sycl::free(B, q);); (-4, SYCL, auto h_a = sycl::malloc_host<TA>( nelems, q);); (80, SYCL, auto isB_fp16 = (typeid(TB) == typeid(sycl::half));); (9, SYCL, auto  C = sycl::malloc_device<TC>( nelems, q);); (7, SYCL, auto  B = sycl::malloc_device<TB>( nelems, q);); (41, SYCL, sycl::free(C, q);)"}
{"predict": "(20, MPI, MPI_Comm_free(&dup_comm);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (17, MPI, MPI_Group_free(&group);); (6, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (9, MPI, mpi_errno = MPI_Comm_create_group(dup_comm, group, 0, NULL);); (10, MPI, MPI_Error_class(mpi_errno, &errclass);); (4, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &dup_comm);); (5, MPI, MPI_Comm_group(dup_comm, &group);)", "label": "(16, MPI, mpi_errno = MPI_Comm_create_group(dup_comm, group, 10, NULL);); (20, MPI, MPI_Comm_free(&dup_comm);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (18, MPI, MPI_Group_free(&group);); (12, MPI, MPI_Error_class(mpi_errno, &errclass);); (7, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (5, MPI, MPI_Comm_group(dup_comm, &group);); (3, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &dup_comm);)"}
{"predict": "(18, MPI, MPI_Type_commit(&xpose[j]);); (33, MPI, MPI_Type_free(&column[j]);); (18, MPI, MPI_Put(&ttmp, 1, MPI_DOUBLE, 0, 0, 1, xpose[j], MPI_COMM_WORLD);); (32, MPI, MPI_Put(&tmean, 1, MPI_DOUBLE, 0, size, 1, MPI_DOUBLE, 0, 1, xpose[j], MPI_COMM_WORLD);); (32, MPI, MPI_Put(&tMeanLower, 1, MPI_DOUBLE, 0, 1, MPI_DOUBLE, 0, 1, xpose[j], MPI_COMM_WORLD);); (33, MPI, MPI_Put(&tMeanHigher, 1, MPI_DOUBLE, 0, 1, MPI_DOUBLE, 0, 1, xpose[j], MPI_COMM_WORLD);); (11, MPI, MPI_Type_contiguous(100, MPI_DOUBLE, &column[j]);); (13, MPI, MPI_Type_size(xpose[j], &size);); (29, MPI, MPI_Type_free(&xpose[j]);)", "label": "(24, MPI, t[i] = MPI_Wtime() - ttmp;); (19, MPI, MPI_Type_commit(&xpose[j]);); (34, MPI, MPI_Type_free(&column[j]);); (32, MPI, MPI_Type_free(&xpose[j]);); (11, MPI, ttmp = MPI_Wtime();); (13, MPI, MPI_Type_vector(nrows, 1, ncols, MPI_INT, &column[j]);); (13, MPI, MPI_Type_create_hvector(ncols, 1, sizeof(int), column[j], &xpose[j]);)"}
{"predict": "(18, CUDA, cudaDeviceSynchronize();)", "label": "(18, CUDA, ASSERT_EQ(cudaGetLastError(), cudaSuccess) << \"Kernel launch error.\";)"}
{"predict": "(3, CUDA, __shared__ float buffer[128];); (3, CUDA, __shared__ float sums[4];); (0, CUDA, __shared__ float gauss[16];)", "label": "(3, CUDA, __shared__ float buffer[128];); (3, CUDA, __shared__ float sums[4];); (0, CUDA, __shared__ float gauss[16];)"}
{"predict": "(7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (23, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(26, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(4, MPI, MPI_Recv(buffer, n, MPI_INT, task, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)", "label": "(12, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(15, CUDA, checkCudaErrors(cudaGetLastError());); (13, CUDA, checkCudaErrors(cudaStreamSynchronize(stream));)", "label": "(15, CUDA, checkCudaErrors(cudaGetLastError());); (13, CUDA, checkCudaErrors(cudaStreamSynchronize(stream));)"}
{"predict": "(14, MPI, rc = MPI_Type_get_extent(type, &tmp_lb, &attr->child_type_extent);); (127, MPI, rc = MPI_Type_free(&type);)", "label": "(14, MPI, rc = MPI_Type_get_extent(type, &tmp_lb, &attr->child_type_extent);); (127, MPI, rc = MPI_Type_free(&type);)"}
{"predict": "(9, OpenMP, #pragma omp declare mapper(id: N1::vec v) map(v.len)); (14, OpenMP, #pragma omp target map(mapper(id), to: fd) map(close mapper(id), from: fd))", "label": "(20, OpenMP, #pragma omp target update to(mapper(id): fd)); (10, OpenMP, #pragma omp declare mapper(id: N1::vec v) map(v.len)); (14, OpenMP, #pragma omp target map(mapper(id) alloc: fd)); (15, OpenMP, #pragma omp target map(mapper(idd) alloc: fd.b)); (19, OpenMP, #pragma omp target update from(mapper(idd): fd.b)); (4, OpenMP, #pragma omp declare mapper(id: struct foodat v) map(v.a)); (15, OpenMP, #pragma omp target update to(mapper(idd): fd.b)); (3, OpenMP, #pragma omp declare mapper(idd: struct foodatchild v) map(v.k)); (14, OpenMP, #pragma omp target update from(mapper(id): fd))"}
{"predict": "(14, CUDA, CUDA_CHECK(cudaGetLastError());)", "label": "(4, CUDA, cudaMemcpy(state.vx, state.dvx, size_f, cudaMemcpyDeviceToHost);); (4, CUDA, cudaMemcpy(state.vy, state.dvy, size_f, cudaMemcpyDeviceToHost);); (4, CUDA, cudaMemcpy(state.rho, state.drho, size_f, cudaMemcpyDeviceToHost);)"}
{"predict": "(12, CUDA, __shared__ T tails[SMALL_KERNEL_BLOCK * SMALL_KERNEL_BLOCK];); (19, CUDA, extern __shared__ char _arrays[];)", "label": "(12, CUDA, __shared__ T tails[SMALL_KERNEL_BLOCK * SMALL_KERNEL_BLOCK];); (18, CUDA, extern __shared__ char _arrays[];)"}
{"predict": "(134, OpenCL, error = clSetKernelArg(kernel, 1, sizeof(tinfo->inBuf), &tinfo->inBuf);)", "label": "(134, OpenCL, error = clSetKernelArg(kernel, 1, sizeof(tinfo->inBuf), &tinfo->inBuf);)"}
{"predict": "(47, MPI, MPI_Finalize();); (21, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myid);); (20, MPI, MPI_Comm_size(MPI_COMM_WORLD, &myid);)", "label": "(45, MPI, ierr = MPI_Finalize();); (15, MPI, ierr = MPI_Comm_rank(MPI_COMM_WORLD, &myid);)"}
{"predict": "(10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (15, MPI, MPI_Dims_create(tests[k].size, tests[k].dim, dims);)", "label": "(9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (14, MPI, MPI_Dims_create(tests[k].size, tests[k].dim, dims);)"}
{"predict": "(2, MPI, MPI_Init(&argc,&argv);)", "label": "(8, MPI, MPI_Finalize();); (0, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(18, CUDA, CUASSERT(cudaMemsetAsync(sortClassValidCountPtr, 0, N * sizeof(int), stream));)", "label": "(21, CUDA, CUASSERT(cudaMemsetAsync(sortClassValidCountPtr, 0, N * sizeof(int), stream));); (19, CUDA, CUASSERT(cudaMemsetAsync(argMaxBBoxPtr, 0, N * samples * 4 * sizeof(float), stream));); (20, CUDA, CUASSERT(cudaMemsetAsync(sortClassPosPtr, 0, N * (NClass + 1) * sizeof(int), stream));); (20, CUDA, CUASSERT(cudaMemsetAsync(sortClassSampleIdxPtr, 0, N * (samples + 1) * sizeof(int), stream));); (15, CUDA, CUASSERT(cudaMemsetAsync(argMaxScorePtr, 0, N * samples * sizeof(float), stream));)"}
{"predict": "(16, MPI, MPI_Send(sendbuf.data(), kSendLength, MPI_INT, 0, tag, MPI_COMM_WORLD);)", "label": "(11, MPI, MPI_Send(pre_info, 2, MPI_INT, 0, kTag, MPI_COMM_WORLD);)"}
{"predict": "(49, OpenCL, err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &global, NULL, 0, NULL, NULL);); (27, OpenCL, clMemWrapper in = clCreateBuffer(context, CL_MEM_READ_ONLY, bytes, NULL, &err);); (51, OpenCL, err = clEnqueueReadBuffer(queue, in, CL_TRUE, 0, bytes, &h_out[0], 0, NULL, NULL);); (46, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(cl_mem), &in);); (40, OpenCL, clKernelWrapper kernel = clCreateKernel(prog, spvName, &err);); (27, OpenCL, err = clEnqueueWriteBuffer(queue, in, CL_TRUE, 0, bytes, &h_in[0], 0, NULL, NULL);)", "label": "(32, OpenCL, err = clEnqueueWriteBuffer(queue, in, CL_TRUE, 0, bytes, &h_in[0], 0, NULL, NULL);); (44, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(cl_mem), &in);); (51, OpenCL, err = clEnqueueReadBuffer(queue, in, CL_TRUE, 0, bytes, &h_out[0], 0, NULL, NULL);); (26, OpenCL, clMemWrapper in = clCreateBuffer(context, CL_MEM_READ_WRITE, bytes, NULL, &err);); (38, OpenCL, clKernelWrapper kernel = clCreateKernel(prog, spvName, &err);); (44, OpenCL, err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &global, NULL, 0, NULL, NULL);)"}
{"predict": "(7, MPI, MPI_Error_string(err, errorString, &errorStringLen);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (6, MPI, MPI_Error_string(err, errorString, &errorStringLen);); (10, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(7, MPI, MPI_Get_library_version(version, &resultlen);)", "label": "(7, MPI, MPI_Get_library_version(version, &resultlen);)"}
{"predict": "(59, TBB, tbb::detail::d1::segmented_iterator<nested_vec, const T> my_csi(my_vec);); (19, TBB, tbb::detail::d1::segmented_iterator<nested_vec, T> my_si(my_vec);)", "label": "(61, TBB, tbb::internal::segmented_iterator<nested_vec, const T> my_csi(my_vec);); (18, TBB, tbb::internal::segmented_iterator<nested_vec, T> my_si(my_vec);)"}
{"predict": "(39, OpenCL, clReleaseMemObject(buf);); (56, OpenCL, err = clEnqueueMapImage(ctx->image_queue, mem, CL_TRUE, CL_MAP_READ, origin, region, &aligned_slice_pitch, data, 0, NULL, NULL, &err);); (61, OpenCL, clReleaseMemObject(mem);); (20, OpenCL, err = clCreateImage(ctx, flags, *fmt, &intel_fmt, &bpp, &tiling, data, errcode_ret);); (28, OpenCL, err = clEnqueueMapHost(ctx->image_queue, data, 0, NULL, NULL, &err);)", "label": "(61, OpenCL, clReleaseMemObject(mem);); (33, OpenCL, cl_mem buf = clCreateBuffer(ctx, CL_MEM_USE_HOST_PTR, sz, data, &err);); (23, OpenCL, ctx->image_queue = clCreateCommandQueueWithProperties(ctx, ctx->devices[0], 0, &err);); (55, OpenCL, err = clEnqueueCopyBufferToImage(ctx->image_queue, buf, mem, 0, origin, region, 0, NULL, NULL);); (38, OpenCL, clReleaseMemObject(buf);)"}
{"predict": "(20, OpenMP, #pragma omp distribute parallel for firstprivate()); (8, OpenMP, #pragma omp distribute parallel for firstprivate); (50, OpenMP, #pragma omp distribute parallel for firstprivate(argv[1])); (51, OpenMP, #pragma omp distribute parallel for firstprivate(2 * 2)); (21, OpenMP, #pragma omp distribute parallel for firstprivate(argc); (154, OpenMP, #pragma omp parallel private(i)); (9, OpenMP, #pragma omp distribute parallel for firstprivate(); (168, OpenMP, #pragma omp parallel reduction(+ : i)); (100, OpenMP, #pragma omp distribute parallel for firstprivate(h, B::x)); (29, OpenMP, #pragma omp distribute parallel for firstprivate(argc > 0? argv[1] : argv[2])); (3, OpenMP, #pragma omp target); (164, OpenMP, #pragma omp parallel shared(xa)); (97, OpenMP, #pragma omp distribute parallel for firstprivate(m)); (58, OpenMP, #pragma omp distribute parallel for firstprivate(da)); (53, OpenMP, #pragma omp distribute parallel for firstprivate(S2::S2sc)); (64, OpenMP, #pragma omp distribute parallel for firstprivate(xa)); (38, OpenMP, #pragma omp distribute parallel for firstprivate(a, b, c, d, f)); (99, OpenMP, #pragma omp distribute parallel for private(xa), firstprivate(xa)); (69, OpenMP, #pragma omp distribute parallel for firstprivate(S2::S2s)); (28, OpenMP, #pragma omp distribute parallel for firstprivate(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (39, OpenMP, #pragma omp distribute parallel for firstprivate(2 * 2)); (145, OpenMP, #pragma omp distribute parallel for lastprivate(g) firstprivate(g)); (0, OpenMP, #pragma omp distribute parallel for firstprivate(argc); (13, OpenMP, #pragma omp distribute parallel for firstprivate(argc,); (58, OpenMP, #pragma omp distribute parallel for safelen(5)); (16, OpenMP, #pragma omp distribute parallel for firstprivate(argc > 0? argv[1] : argv", "label": "(51, OpenMP, #pragma omp distribute parallel for firstprivate(a, b, c, d, f)); (86, OpenMP, #pragma omp distribute parallel for firstprivate(S2::S2s)); (158, OpenMP, #pragma omp parallel private(i)); (94, OpenMP, #pragma omp distribute parallel for safelen(5)); (98, OpenMP, #pragma omp distribute parallel for firstprivate(e, g)); (170, OpenMP, #pragma omp distribute parallel for firstprivate(si)); (70, OpenMP, #pragma omp distribute parallel for firstprivate(da)); (75, OpenMP, #pragma omp distribute parallel for firstprivate(xa)); (28, OpenMP, #pragma omp distribute parallel for firstprivate(argc > 0? argv[1] : argv[2])); (113, OpenMP, #pragma omp distribute parallel for firstprivate(i)); (-1, OpenMP, #pragma omp target); (30, OpenMP, #pragma omp distribute parallel for firstprivate(argc)); (133, OpenMP, #pragma omp distribute parallel for lastprivate(n) firstprivate(n)); (99, OpenMP, #pragma omp distribute parallel for firstprivate(k, h, B::x)); (111, OpenMP, #pragma omp parallel); (151, OpenMP, #pragma omp parallel reduction(+ : i)); (0, OpenMP, #pragma omp distribute parallel for firstprivate(); (44, OpenMP, #pragma omp distribute parallel for firstprivate(2 * 2)); (53, OpenMP, #pragma omp distribute parallel for firstprivate(ca)); (114, OpenMP, #pragma omp distribute parallel for firstprivate(j)); (36, OpenMP, #pragma omp distribute parallel for firstprivate(argv[1])); (96, OpenMP, #pragma omp distribute parallel for private(xa), firstprivate(xa)); (70, OpenMP, #pragma omp distribute parallel for firstprivate(S2::S2sc)); (-2, OpenMP, #pragma omp distribute parallel for firstprivate()); (2, OpenMP, #pragma omp distribute parallel for firstprivate(argc); (114, OpenMP, #pragma omp distribute parallel for lastprivate(g) firstprivate(g)); (20, OpenMP, #pragma omp distribute parallel for firstprivate(S1)); (4, OpenMP, #pragma omp distribute parallel for firstprivate(argc,); (79, OpenMP, #pragma omp distribute parallel for firstprivate(m)); (37, OpenMP, #pragma omp distribute parallel for firstprivate(ba)); (-19, OpenMP, #pragma omp distribute parallel for firstprivate); (-21, OpenMP, #pragma omp teams)"}
{"predict": "(11, OpenCL, auto commandQueue = clCreateCommandQueueWithProperties(context.get(), clDevice, properties, &retVal);); (21, OpenCL, clReleaseCommandQueue(commandQueue);)", "label": "(11, OpenCL, auto commandQueue = clCreateCommandQueueWithProperties(pContext, testedClDevice, properties.data(), &retVal);); (21, OpenCL, clReleaseCommandQueue(commandQueue);)"}
{"predict": "(10, MPI, MPI_Type_get_extent(sddt, &lb, &extent);); (10, MPI, MPI_Type_size(rddt, &size);)", "label": "(6, MPI, MPI_Type_get_extent(sddt, &lb, &extent);)"}
{"predict": "(70, OpenMP, const int th=omp_get_thread_num();); (10, OpenMP, #pragma omp parallel for schedule (guided))", "label": "(71, OpenMP, const int th=omp_get_thread_num();); (10, OpenMP, #pragma omp parallel for schedule (guided))"}
{"predict": "(11, CUDA, CUDA_CHECK(cudaGetDeviceProperties(&props, 0));)", "label": "(11, CUDA, CUDA_CHECK(cudaGetDeviceProperties(&props, 0));)"}
{"predict": "(3, MPI, MPI_Init(&argc, &argv);); (9, MPI, MPI_Finalize();)", "label": "(3, MPI, MPI_Init(&argc, &argv);); (9, MPI, MPI_Finalize();)"}
{"predict": "(15, OpenCL, result = clGetPlatformInfo(platform, CL_PLATFORM_EXTENSIONS, set_size, &extensions[0], NULL);); (10, OpenCL, result = clGetPlatformInfo(platform, CL_PLATFORM_EXTENSIONS, 0, NULL, &set_size);)", "label": "(12, OpenCL, result = clGetPlatformInfo(platform, CL_PLATFORM_EXTENSIONS, 0, NULL, &set_size);); (14, OpenCL, result = clGetPlatformInfo(platform, CL_PLATFORM_EXTENSIONS, extensions.size(), extensions.data(), NULL);)"}
{"predict": "(44, OpenMP, #pragma omp teams distribute dist_schedule (static, S1)); (19, OpenMP, #pragma omp teams distribute dist_schedule (argc))); (27, OpenMP, #pragma omp teams distribute dist_schedule (static, argc > 0? argv[1] : argv[2])); (30, OpenMP, #pragma omp teams distribute dist_schedule (static), dist_schedule (static, z+1)); (39, OpenMP, #pragma omp teams distribute dist_schedule (static, argv[1]=2)); (46, OpenMP, #pragma omp teams distribute dist_schedule (static, argv[1] = -1)); (49, OpenMP, #pragma omp teams distribute dist_schedule (static, tmain<int, -1>(argc)); (34, OpenMP, #pragma omp teams distribute dist_schedule (static, a, b)); (1, OpenMP, #pragma omp teams distribute dist_schedule (); (44, OpenMP, #pragma omp teams distribute dist_schedule (static, argv[1] = 0)); (23, OpenMP, #pragma omp teams distribute dist_schedule (static, static); (26, OpenMP, #pragma omp teams distribute dist_schedule (static, 1 + z)); (7, OpenMP, #pragma omp teams distribute dist_schedule (static,); (18, OpenMP, #pragma omp teams distribute dist_schedule (static, argc))); (-3, OpenMP, #pragma omp teams distribute dist_schedule ()); (-9, OpenMP, #pragma omp target); (32, OpenMP, #pragma omp teams distribute dist_schedule (static, 1)); (33, OpenMP, #pragma omp teams distribute dist_schedule (static, 2)); (20, OpenMP, #pragma omp teams distribute dist_schedule (static, 1 + z)); (-11, OpenMP, #pragma omp teams distribute dist_schedule); (-2, OpenMP, #pragma omp teams distribute dist_schedule (static)", "label": "(47, OpenMP, #pragma omp teams num_teams (3.14)); (42, OpenMP, #pragma omp teams num_teams (-10u)); (17, OpenMP, #pragma omp teams num_teams (argc))); (4, OpenMP, #pragma omp teams num_teams (); (7, OpenMP, #pragma omp teams num_teams ()); (10, OpenMP, #pragma omp teams num_teams (argc); (25, OpenMP, #pragma omp teams num_teams (argc), num_teams (argc+1)); (32, OpenMP, #pragma omp teams num_teams (-2)); (15, OpenMP, #pragma omp teams num_teams (argc > 0? argv[1] : argv[2])); (-7, OpenMP, #pragma omp target); (25, OpenMP, #pragma omp teams num_teams (S1)); (16, OpenMP, #pragma omp teams num_teams (argc + argc-z)); (-9, OpenMP, #pragma omp teams num_teams)"}
{"predict": "(180, MPI, MPI_Waitall(2, request, status);); (77, MPI, MPI_Irecv(buf_recv, count, MPI_BYTE, rank_recv, 0, comm, &request[0]);); (100, MPI, MPI_Isend(buf_send, count, MPI_BYTE, rank_send, 0, comm, &request[1]);); (101, MPI, MPI_Waitall(2, request, status);); (108, MPI, MPI_Irecv(buf_recv, count, MPI_BYTE, rank_recv, 0, comm, &request[1]);); (101, MPI, MPI_Irecv(buf_recv, count, MPI_BYTE, rank_recv, 0, comm, &request[0]);); (76, MPI, MPI_Irecv(buf_recv, count, MPI_BYTE, rank_recv, 0, comm, &request[0]);); (99, MPI, MPI_Isend(buf_send, count, MPI_BYTE, rank_send, 0, comm, &request[1]);); (74, MPI, MPI_Irecv(buf_recv, count, MPI_BYTE, rank_recv, 0, comm, &request[1]);); (77, MPI, MPI_Isend(buf_send, count, MPI_BYTE, rank_send, 0, comm, &request[0]);)", "label": "(101, MPI, MPI_Wait(&request[1], &status[1]);); (110, MPI, MPI_Wait(&request[0], &status[0]);); (110, MPI, MPI_Get_count(&status[0], MPI_BYTE, &nwrite);); (74, MPI, MPI_Irecv(buf_recv, shuffile_mpi_buf_size, MPI_BYTE, rank_recv, 0, comm, &request[0]);); (96, MPI, MPI_Isend(buf_send, nread, MPI_BYTE, rank_send, 0, comm, &request[1]);)"}
{"predict": "(7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (50, MPI, MPI_Comm_free(&pgroup_new);); (39, MPI, MPI_Open_port(port_info, port);); (39, MPI, MPI_Send(port, PORT_SIZE, MPI_CHAR, members[1], 0, MPI_COMM_WORLD);); (49, MPI, MPI_Intercomm_merge(pgroup_new, 0, &pgroup);); (39, MPI, MPI_Comm_free(&pgroup_old);); (39, MPI, MPI_Comm_free(&pgroup_new);); (57, MPI, MPI_Recv(port, PORT_SIZE, MPI_CHAR, members[0], 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (29, MPI, MPI_Comm_set_name(*group, \"PGroup\");); (26, MPI, MPI_Barrier(MPI_COMM_WORLD);); (44, MPI, MPI_Comm_free(&pgroup_old);); (57, MPI, MPI_Comm_create(MPI_COMM_WORLD, port_info, &pgroup_new);); (11, MPI, MPI_Info_create(&port_info);); (5, MPI, MPI_Comm_group(MPI_COMM_WORLD, &old_group);); (11, MPI, MPI_Info_set(port_info, \"ompi_global_scope\", \"true\");); (46, MPI, MPI_Comm_free(&pgroup_new);); (46, MPI, MPI_Comm_free(&old_group);); (36, MPI, MPI_Comm_free(&pgroup_old);); (26, MPI, MPI_Send(port, PORT_SIZE, MPI_CHAR, members[0], 0, MPI_COMM_WORLD);); (38, MPI, MPI_Comm_free(&pgroup_new);); (19, MPI, MPI_Comm_free(&pgroup);); (44, MPI, MPI_Intercomm_merge(pgroup_new, 1, &pgroup);); (-3, MPI, MPI_Comm_create_group(MPI_COMM_WORLD, port_info, 0, &pgroup);); (36, MPI, MPI_Comm_free(&old_group);); (47, MPI, MPI_Info_free(&port_info);)", "label": "(48, MPI, MPI_Comm_accept(port, MPI_INFO_NULL, 0, pgroup, &pgroup_new);); (52, MPI, MPI_Comm_free(&pgroup_old);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (64, MPI, MPI_Comm_connect(port, MPI_INFO_NULL, 0, MPI_COMM_SELF, &pgroup_new);); (42, MPI, MPI_Send(port, PORT_SIZE, MPI_CHAR, members[i], 0, MPI_COMM_WORLD);); (35, MPI, MPI_Open_port(port_info, port);); (44, MPI, MPI_Comm_free(&pgroup_new);); (48, MPI, MPI_Close_port(port);); (66, MPI, MPI_Comm_connect(port, MPI_INFO_NULL, 0, pgroup, &pgroup_new);)"}
{"predict": "(5, CUDA, __shared__ typename BlockMergeSort::TempStorage temp_storage_shuffle;)", "label": "(5, CUDA, __shared__ typename BlockMergeSort::TempStorage temp_storage_shuffle;)"}
{"predict": "(15, MPI, MPI_Comm_size(comm, &size);); (21, MPI, MPI_Allreduce(buf, &result, 1, MPI_INT, op, comm);); (8, MPI, MPI_Op_create((MPI_User_function *) addem, 1, &op);); (32, MPI, MPI_Op_free(&op);); (8, MPI, MPI_Comm_rank(comm, &rank);); (18, MPI, MPI_Op_free(&op);)", "label": "(22, MPI, MPI_Allreduce(MPI_IN_PLACE, buf, count, MPI_INT, op, comm);); (13, MPI, MPI_Comm_size(comm, &size);); (35, MPI, MPI_Op_free(&op);); (6, MPI, MPI_Op_create(mysum, 0, &op);); (11, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(14, MPI, MPI_Type_get_true_extent(keysat, &true_lb, &true_extent);); (32, MPI, MPI_Comm_size(comm, &ranks);); (30, MPI, MPI_Comm_rank(comm, &rank);)", "label": "(34, MPI, MPI_Comm_size(comm, &ranks);); (32, MPI, MPI_Comm_rank(comm, &rank);); (12, MPI, MPI_Type_get_true_extent(keysat, &true_lb, &true_extent);)"}
{"predict": "(10, OpenMP, #pragma omp parallel for)", "label": "(10, OpenMP, #pragma omp parallel for)"}
{"predict": "(6, OpenMP, #pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp parallel for)"}
{"predict": "(15, CUDA, cudaStreamDestroy(stream);); (10, CUDA, cudaStreamCreate(&stream);)", "label": "(11, CUDA, cudaStreamCreate(&stream);); (14, CUDA, cudaStreamDestroy(stream);)"}
{"predict": "(3, CUDA, __shared__ clock_t s_clocks[32];)", "label": "(3, CUDA, __shared__ clock_t s_clocks[32];)"}
{"predict": "(10, MPI, MPI_Info_get(info, key, sizeof(buff) - 1, buff, &flag);); (7, MPI, MPI_Info_create(&info);); (19, MPI, MPI_Info_free(&info);)", "label": "(11, MPI, MPI_Info_get(info, key, sizeof(buff) - 1, buff, &flag);); (8, MPI, MPI_Info_create(&info);); (19, MPI, MPI_Info_free(&info);); (7, MPI, MPI_Info_set(info, key, val);)"}
{"predict": "(111, MPI, MPI_Type_commit(&mpi_word);); (148, MPI, MPI_Finalize();); (64, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (111, MPI, MPI_Barrier(MPI_COMM_WORLD);); (62, MPI, MPI_Bcast(&length,     1, MPI_LONG, root, MPI_COMM_WORLD);); (108, MPI, MPI_Type_contiguous(proc_length,MPI_CHAR, &mpi_word);); (17, MPI, MPI_Init(&argc,&argv);); (95, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);); (127, MPI, MPI_Allreduce(&basesum, &checksum, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (14, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);)", "label": "(113, MPI, MPI_Type_commit(&mpi_word);); (120, MPI, MPI_Allgather(iterstring,1,mpi_word, catstring,1,mpi_word, MPI_COMM_WORLD);); (152, MPI, MPI_Finalize();); (61, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (108, MPI, MPI_Type_contiguous(proc_length,MPI_CHAR, &mpi_word);); (17, MPI, MPI_Init(&argc,&argv);); (59, MPI, MPI_Bcast(&length,     1, MPI_LONG, root, MPI_COMM_WORLD);); (108, MPI, MPI_Barrier(MPI_COMM_WORLD);); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(13, CUDA, __shared__ float Bs[maxPipelineStages][BLOCK_SIZE_X][BLOCK_SIZE_X];); (8, CUDA, __shared__ float As[maxPipelineStages][BLOCK_SIZE_X][BLOCK_SIZE_X];)", "label": "(13, CUDA, __shared__ float Bs[maxPipelineStages][BLOCK_SIZE_X][BLOCK_SIZE_X];); (8, CUDA, __shared__ float As[maxPipelineStages][BLOCK_SIZE_X][BLOCK_SIZE_X];)"}
{"predict": "(11, CUDA, extern __shared__ float sdata[];)", "label": "(11, CUDA, extern __shared__ float sdata[];)"}
{"predict": "(7, OpenMP, #pragma omp target map(from: device))", "label": "(7, OpenMP, #pragma omp target map(tofrom : device)); (9, OpenMP, printf(\"omp_is_initial_device() returned false on host\\n\");); (12, OpenMP, printf(\"omp_is_initial_device() returned true on device\\n\");)"}
{"predict": "(3, TBB, tbb::atomic<int> output_counter;); (1, TBB, tbb::atomic<int> counter;)", "label": "(32, TBB, tbb::tbb_allocator<t1>().destroy(my_storage);); (35, TBB, tbb::tbb_allocator<t2>().destroy(my_storage);); (0, TBB, tbb::atomic<int> counter;)"}
{"predict": "(44, CUDA, checkCudaErrors(cudaMalloc((void **)&vyfield, sizeof(cData) * PDS));); (42, CUDA, checkCudaErrors(cudaMalloc((void **)&vxfield, sizeof(cData) * PDS));); (46, CUDA, checkCudaErrors(cudaMalloc((void **)&vzfield, sizeof(cData) * PDS));); (44, CUDA, checkCudaErrors(cudaMalloc((void **)&v_xfield, sizeof(cData) * PDS));); (45, CUDA, checkCudaErrors(cudaMalloc((void **)&v_yfield, sizeof(cData) * PDS));); (50, CUDA, checkCudaErrors(cufftPlan2d(&planc2r, DIM, DIM, CUFFT_C2R));); (48, CUDA, checkCudaErrors(cufftPlan2d(&planr2c, DIM, DIM, CUFFT_R2C));); (44, CUDA, checkCudaErrors(cudaMalloc((void **)&v_zfield, sizeof(cData) * PDS));); (39, CUDA, checkCudaErrors(cudaMemcpy(vyfield, hvfield, sizeof(cData) * DS, cudaMemcpyHostToDevice));); (40, CUDA, checkCudaErrors(cudaMemcpy(vzfield, hvfield, sizeof(cData) * DS, cudaMemcpyHostToDevice));); (37, CUDA, checkCudaErrors(cudaMemcpy(vxfield, hvfield, sizeof(cData) * DS, cudaMemcpyHostToDevice));); (51, CUDA, checkCudaErrors(cufftPlan3d(&planr2c, DIM, DIM, CUFFT_R2C));); (52, CUDA, checkCudaErrors(cufftPlan3d(&planr2f, DIM, DIM, CUFFT_R2F));); (49, CUDA, checkCudaErrors(cufftPlan3d(&planc2r, DIM, DIM, CUFFT_C2R));); (50, CUDA, checkCudaErrors(cufftPlan3d(&planf2c, DIM, DIM, CUFFT_F2C));); (54, CUDA, checkCudaErrors(cufftPlan2d(&planr2d, DIM, DIM, CUFFT_R2D));); (54, CUDA, checkCudaErrors(cufftPlan2d(&planf2r, DIM, DIM, CUFFT_F2R", "label": "(56, CUDA, checkCudaErrors(cufftPlan2d(&planc2r, DIM, DIM, CUFFT_C2R));); (54, CUDA, checkCudaErrors(cufftPlan2d(&planr2c, DIM, DIM, CUFFT_R2C));); (37, CUDA, checkCudaErrors(cudaMallocPitch((void **)&dvfield, &tPitch, sizeof(cData)*DIM, DIM));); (38, CUDA, checkCudaErrors(cudaMemcpy(dvfield, hvfield, sizeof(cData) * DS, cudaMemcpyHostToDevice));); (40, CUDA, checkCudaErrors(cudaMalloc((void **)&vyfield, sizeof(cData) * PDS));); (38, CUDA, checkCudaErrors(cudaMalloc((void **)&vxfield, sizeof(cData) * PDS));)"}
{"predict": "(46, OpenMP, #pragma omp teams reduction(+:sum)); (44, OpenMP, #pragma omp target map(tofrom: b, sum, t) map(alloc: a)); (47, OpenMP, #pragma omp distribute parallel for reduction(+:sum)); (49, OpenMP, t[i] = omp_get_team_num();)", "label": "(46, OpenMP, #pragma omp teams reduction(+:sum)); (43, OpenMP, #pragma omp target map(tofrom: b, sum, t) map(alloc: a)); (46, OpenMP, #pragma omp distribute parallel for reduction(+:sum)); (48, OpenMP, t[i] = omp_get_team_num();)"}
{"predict": "(53, CUDA, cudaCheck(cudaMalloc((void**)&params_memory_gpu, num_parameters * sizeof(float)));); (55, CUDA, cudaCheck(cudaMalloc((void**)&grads_memory_gpu, num_parameters * sizeof(float)));); (163, CUDA, cudaCheck(cudaFree(params_memory_gpu));); (163, CUDA, cudaCheck(cudaFree(grads_memory_gpu));); (54, CUDA, cudaCheck(cudaMemcpy(params_memory_gpu, params_memory_cpu, num_parameters * sizeof(float), cudaMemcpyHostToDevice));); (163, CUDA, cudaCheck(cudaFree(back_acts_memory_gpu));); (156, CUDA, cudaCheck(cudaFree(acts_memory_gpu));); (51, CUDA, cudaCheck(cudaMemcpy(grads_memory_gpu, grads_memory_cpu, num_parameters * sizeof(float), cudaMemcpyHostToDevice));); (156, CUDA, cudaCheck(cudaFree(params_acts_gpu));); (49, CUDA, cudaCheck(cudaMemcpy(back_acts_memory_gpu, back_acts_cpu, num_parameters * sizeof(float), cudaMemcpyHostToDevice));); (44, CUDA, cudaCheck(cudaMalloc((void**)&back_acts_memory_gpu, num_parameters * sizeof(float)));); (152, CUDA, cudaCheck(cudaFree(grads_acts_gpu));); (153, CUDA, cudaCheck(cudaFree(acts_back_gpu));); (142, CUDA, cudaCheck(cudaFree(acts_memory_gpu));); (143, CUDA, cudaCheck(cudaFree(acts_count_gpu));); (156, CUDA, cudaCheck(cudaFree(back_acts_cpu));); (39, CUDA, cudaCheck(cudaMalloc((void**)&acts_count_gpu, num_debug_state_params * sizeof(float)));); (39, CUDA, cudaCheck(cudaMalloc((void**)&acts_memory_gpu, num_debug_state_params * sizeof(float)));); (142, CUDA, cudaCheck(cudaFree(acts_params_gpu));); (39, CUDA, cudaCheck(cudaMalloc((void**)&acts_params_gpu, num_debug_state_params * sizeof(float)));); (36, CUDA, cudaCheck(cudaMalloc((void**)&acts_count_gpu, num_debug_state_params * sizeof(float)));); (141, CUDA, cudaCheck(cudaFree(acts_count_gpu));)", "label": "(125, CUDA, cudaCheck(cudaMalloc(&acts_memory_gpu, num_acts_params * sizeof(float)));); (257, CUDA, cudaCheck(cudaFree(acts_memory_gpu));); (173, CUDA, cudaCheck(cudaMalloc(&back_acts_memory_gpu, num_back_acts * sizeof(float)));); (254, CUDA, cudaCheck(cudaFree(params_memory_gpu));); (259, CUDA, cudaCheck(cudaFree(acts.emb));); (53, CUDA, cudaCheck(cudaMemset(grads_memory_gpu, 0, num_parameters * sizeof(float)));); (170, CUDA, cudaCheck(cudaMemset(back_acts_memory_gpu, 0, num_back_acts * sizeof(float)));); (182, CUDA, cudaCheck(cudaMemcpy(back_acts.dout, debug_states.dout, B * C_out * H_out * W_out * sizeof(float), cudaMemcpyHostToDevice));); (254, CUDA, cudaCheck(cudaFree(acts.input));); (117, CUDA, cudaCheck(cudaMemset(acts_memory_gpu, 0, num_acts_params * sizeof(float)));); (43, CUDA, cudaCheck(cudaMemcpy(params_memory_gpu, params_memory_cpu, num_parameters * sizeof(float), cudaMemcpyHostToDevice));); (249, CUDA, cudaCheck(cudaFree(back_acts_memory_gpu));); (122, CUDA, cudaCheck(cudaMemcpy(acts.input, debug_states.input, state_sizes[0] * sizeof(float), cudaMemcpyHostToDevice));); (122, CUDA, cudaCheck(cudaMemcpy(acts.emb, debug_states.emb, state_sizes[1] * sizeof(float), cudaMemcpyHostToDevice));); (43, CUDA, cudaCheck(cudaMalloc(&grads_memory_gpu, num_parameters * sizeof(float)));); (116, CUDA, cudaCheck(cudaMalloc(&acts.input, B * C * H * W * sizeof(float)));); (36, CUDA, cudaCheck(cudaMalloc(&params_memory_gpu, num_parameters * sizeof(float)));); (115, CUDA, cudaCheck(cudaMalloc(&acts.emb, B * C_emb * sizeof(float)));); (241, CUDA, cudaCheck(cudaFree(grads_memory_gpu));)"}
{"predict": "(10, OpenMP, omp_set_nest_lock(&nest_lock);); (4, OpenMP, omp_init_nest_lock(&nest_lock);); (1, OpenMP, omp_set_dynamic(0);); (9, OpenMP, omp_unset_nest_lock(&nest_lock);)", "label": "(6, OpenMP, omp_unset_nest_lock(&nest_lock);); (2, OpenMP, omp_init_nest_lock(&nest_lock);); (3, OpenMP, omp_test_nest_lock(&nest_lock);); (5, OpenMP, omp_set_nest_lock(&nest_lock);); (9, OpenMP, omp_destroy_nest_lock(&nest_lock);)"}
{"predict": "(56, MPI, MPI_Wait(handle->req_array+handle->req_count-1, MPI_STATUS_IGNORE);); (164, MPI, MPI_Wait(&handle->req[i]);); (164, MPI, MPI_Request_free(&handle->req[i]);)", "label": "(73, MPI, Irecv_time -= MPI_Wtime();); (55, MPI, Isend_time += MPI_Wtime();); (88, MPI, Irecv_time += MPI_Wtime();); (35, MPI, Isend_time -= MPI_Wtime();)"}
{"predict": "(24, CUDA, CHECK(cudaMalloc(&d_B, M));); (24, CUDA, CHECK(cudaMemcpy(d_A, h_A, M, cudaMemcpyHostToDevice));); (46, CUDA, CHECK(cudaFree(d_B));); (20, CUDA, CHECK(cudaMalloc(&d_A, M));); (32, CUDA, CHECK(cudaMemcpy(h_B, d_B, M, cudaMemcpyDeviceToHost));); (42, CUDA, CHECK(cudaFree(d_A));)", "label": "(24, CUDA, CHECK(cudaMalloc(&d_B, M));); (24, CUDA, CHECK(cudaMemcpy(d_A, h_A, M, cudaMemcpyHostToDevice));); (46, CUDA, CHECK(cudaFree(d_B));); (20, CUDA, CHECK(cudaMalloc(&d_A, M));); (32, CUDA, CHECK(cudaMemcpy(h_B, d_B, M, cudaMemcpyDeviceToHost));); (42, CUDA, CHECK(cudaFree(d_A));)"}
{"predict": "(65, OpenMP, #pragma omp parallel for if(n>OMP_MIN_SIZE)); (5, MPI, MPI_Init_thread( &argc, &argv, MPI_THREAD_MULTIPLE, &provided );); (6, MPI, MPI_Comm_size( MPI_COMM_WORLD, &rank );); (35, OpenMP, printf( \"# starting test: OpenMP... \\n\" );); (147, OpenMP, #pragma omp target map( B[:N],A[:N] )); (174, MPI, MPI_Finalize();); (2, MPI, MPI_Comm_set_errhandler( MPI_COMM_WORLD, MPI_ERRORS_RETURN );)", "label": "(8, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &rank );); (364, MPI, MPI_Finalize();); (34, OpenMP, fprintf( stderr, \"starting test: OpenMP ON with %d threads... \\n\", omp_get_max_threads() );); (164, OpenMP, #pragma omp parallel if(n>OMP_MIN_SIZE)); (168, OpenMP, #pragma omp for private(i,j,n4)); (63, OpenMP, #pragma omp parallel for if(n>OMP_MIN_SIZE)); (1, MPI, MPI_Init_thread( &argc, &argv, MPI_THREAD_FUNNELED, &provided );); (28, OpenMP, printf( \"# starting test: OpenMP ON with %d threads... \\n\", omp_get_max_threads() );)"}
{"predict": "(5, CUDA, extern __shared__ char shared_memory[];)", "label": "(5, CUDA, extern __shared__ char shared_memory[];)"}
{"predict": "(12, MPI, err = MPI_Type_struct(3, blocks, disps, types, &eviltype);); (22, MPI, err = MPI_Type_size(eviltype, &val);); (66, MPI, err = MPI_Type_get_extent(eviltype, &lb, &extent);); (100, MPI, err = MPI_Type_get_true_extent(eviltype, &true_lb, &aval);); (120, MPI, MPI_Type_free(&eviltype);); (36, MPI, err = MPI_Type_extent(eviltype, &aval);); (49, MPI, err = MPI_Type_lb(eviltype, &aval);)", "label": "(23, MPI, err = MPI_Type_size(eviltype, &val);); (11, MPI, err = MPI_Type_struct(3, blocks, disps, types, &eviltype);); (66, MPI, err = MPI_Type_get_extent(eviltype, &lb, &extent);); (86, MPI, err = MPI_Type_ub(eviltype, &aval);); (121, MPI, MPI_Type_free(&eviltype);); (99, MPI, err = MPI_Type_get_true_extent(eviltype, &true_lb, &aval);); (32, MPI, err = MPI_Type_extent(eviltype, &aval);); (46, MPI, err = MPI_Type_lb(eviltype, &aval);)"}
{"predict": "(59, MPI, MPI_Win_unlock(rank, win);); (41, MPI, MPI_Win_start(group, 0, win);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (37, MPI, MPI_Put(A + i, 1, MPI_INT, 1, i, 1, MPI_INT, win);); (52, MPI, MPI_Win_post(group, 0, win);); (35, MPI, MPI_Win_allocate(0, 1, MPI_INFO_NULL, CommDeuce, &base_ptr, &win);); (79, MPI, MPI_Win_free(&win);); (77, MPI, MPI_Group_free(&comm_group);); (8, MPI, MPI_Comm_split(MPI_COMM_WORLD, (rank < 2), rank, &CommDeuce);); (78, MPI, MPI_Comm_free(&CommDeuce);); (11, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (44, MPI, MPI_Send(A, SIZE, MPI_INT, 1, 0, MPI_COMM_WORLD);); (44, MPI, MPI_Recv(B, SIZE, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (34, MPI, MPI_Win_complete(win);); (20, MPI, MPI_Win_create(B, SIZE * sizeof(int), sizeof(int), MPI_INFO_NULL, CommDeuce, &win);); (54, MPI, MPI_Win_wait(win);); (50, MPI, MPI_Win_create(NULL, 0, 1, MPI_INFO_NULL, CommDeuce, &win);); (71, MPI, MPI_Free_mem(A);); (22, MPI, MPI_Win_fence(MPI_MODE_NOPRECEDE);); (71, MPI, MPI_Free_mem(B);); (-2, MPI, MPI_Comm_group(CommDeuce, &comm_group);); (8, MPI, i = MPI_Alloc_mem(SIZE * sizeof(int), MPI_INFO_NULL, &B);); (58, MPI, MPI_Win_fence(MPI_MODE_NOSUCCEED);); (28, MPI, MPI_Win_allocate(SIZE * sizeof(int), sizeof(int), MPI_INFO_NULL, CommDeuce, &i, &win);); (18, MPI, MPI_Win_fence(MPI_MODE_NOPRECE", "label": "(67, MPI, MPI_Win_unlock(rank, win);); (28, MPI, i = MPI_Alloc_mem(SIZE * sizeof(int), MPI_INFO_NULL, &B);); (45, MPI, MPI_Win_start(group, 0, win);); (92, MPI, MPI_Win_free(&win);); (13, MPI, MPI_Comm_split(MPI_COMM_WORLD, (rank < 2), rank, &CommDeuce);); (66, MPI, MPI_Win_post(group, 0, win);); (44, MPI, MPI_Send(B, SIZE, MPI_INT, 1, 100, MPI_COMM_WORLD);); (34, MPI, MPI_Win_allocate(0, 1, MPI_INFO_NULL, CommDeuce, &base_ptr, &win);); (47, MPI, MPI_Win_allocate(SIZE * sizeof(int), sizeof(int), MPI_INFO_NULL, CommDeuce, &B, &win);); (63, MPI, MPI_Recv(A, SIZE, MPI_INT, 0, 100, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (63, MPI, MPI_Win_wait(win);); (41, MPI, MPI_Free_mem(B);); (37, MPI, MPI_Win_complete(win);); (35, MPI, MPI_Put(A, SIZE, MPI_INT, 1, 0, SIZE, MPI_INT, win);); (0, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (5, MPI, i = MPI_Alloc_mem(SIZE * sizeof(int), MPI_INFO_NULL, &A);); (82, MPI, MPI_Comm_free(&CommDeuce);); (77, MPI, MPI_Group_free(&comm_group);); (-9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (7, MPI, MPI_Comm_group(CommDeuce, &comm_group);); (-10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (75, MPI, MPI_Free_mem(A);); (71, MPI, MPI_Group_free(&group);); (23, MPI, MPI_Group_incl(comm_group, 1, &destrank, &group);); (38, MPI, MPI_Win_create(B, SIZE * sizeof(int), sizeof(int), MPI_INFO_NULL, CommDeuce, &win);)"}
{"predict": "(51, CUDA, CubDebugExit(cudaMemcpy(d_flags, h_flags, sizeof(unsigned char) * num_items, cudaMemcpyHostToDevice));); (49, CUDA, CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));)", "label": "(52, CUDA, CubDebugExit(cudaMemcpy(d_flags, h_flags, sizeof(unsigned char) * num_items, cudaMemcpyHostToDevice));); (50, CUDA, CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));)"}
{"predict": "(14, MPI, MPI_Comm_rank(comm, &myrank);); (78, MPI, MPI_Type_free(&stype[i][j]);); (75, MPI, MPI_Type_free(&xtype[i][j]);); (56, MPI, MPI_Start(&req);); (56, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (57, MPI, MPI_Request_free(&req);); (7, MPI, MPI_Comm_size(comm, &p);); (50, MPI, MPI_Info_create(&info);); (56, MPI, MPI_Info_free(&info);)", "label": "(58, MPI, MPI_Start(&req);); (13, MPI, MPI_Comm_rank(comm, &myrank);); (76, MPI, MPI_Type_free(&stype[i][j]);); (50, MPI, MPI_Info_create(&info);); (55, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (72, MPI, MPI_Type_free(&xtype[i][j]);); (56, MPI, MPI_Request_free(&req);); (6, MPI, MPI_Comm_size(comm, &p);); (55, MPI, MPI_Info_free(&info);)"}
{"predict": "(179, OpenCL, clGetDeviceInfo(vkGPU->device, CL_DEVICE_VENDOR_ID, sizeof(cl_int), &vendorID, 0);); (80, CUDA, res = cudaMalloc((void**)&buffer, bufferSize);); (95, OpenCL, buffer = clCreateBuffer(vkGPU->context, CL_MEM_READ_WRITE, bufferSize, 0, &res);)", "label": "(161, OpenCL, clGetDeviceInfo(vkGPU->device, CL_DEVICE_VENDOR_ID, sizeof(cl_int), &vendorID, 0);); (81, CUDA, res = cudaMalloc((void**)&buffer, bufferSize);); (90, OpenCL, buffer = clCreateBuffer(vkGPU->context, CL_MEM_READ_WRITE, bufferSize, 0, &res);)"}
{"predict": "(92, MPI, MPI_Finalize();); (14, MPI, MPI_Comm_get_parent(&parentcomm);); (37, MPI, MPI_Send(&i, 1, MPI_INT, i, 0, intercomm);); (37, MPI, MPI_Recv(&err, 1, MPI_INT, i, 1, intercomm, MPI_STATUS_IGNORE);); (23, MPI, MPI_Comm_size(intercomm, &size);); (55, MPI, MPI_Comm_get_name(intercomm, cname, &rlen);); (87, MPI, MPI_Comm_free(&intercomm);); (72, MPI, MPI_Ssend(&errs, 1, MPI_INT, 0, 1, intercomm);); (19, MPI, MPI_Comm_rank(intercomm, &rank);); (18, MPI, MPI_Comm_remote_size(intercomm, &rsize);); (65, MPI, MPI_Recv(&i, 1, MPI_INT, 0, 0, intercomm, &status);); (45, MPI, MPI_Comm_set_name(intercomm, (char *) \"Single\");)", "label": "(26, MPI, MPI_Comm_size(intercomm, &size);); (84, MPI, MPI_Comm_disconnect(&intercomm);); (91, MPI, MPI_Finalize();); (11, MPI, MPI_Comm_get_parent(&parentcomm);); (66, MPI, MPI_Recv(&i, 1, MPI_INT, 0, 0, intercomm, &status);); (32, MPI, MPI_Send(&i, 1, MPI_INT, i, 0, intercomm);); (52, MPI, MPI_Comm_get_name(intercomm, cname, &rlen);); (69, MPI, MPI_Ssend(&errs, 1, MPI_INT, 0, 1, intercomm);); (35, MPI, MPI_Recv(&err, 1, MPI_INT, i, 1, intercomm, MPI_STATUS_IGNORE);); (18, MPI, MPI_Comm_rank(intercomm, &rank);); (15, MPI, MPI_Comm_remote_size(intercomm, &rsize);)"}
{"predict": "(47, OpenMP, #pragma omp teams distribute parallel for collapse (S)); (24, OpenMP, #pragma omp teams distribute parallel for collapse (ST); (31, OpenMP, #pragma omp teams distribute parallel for collapse (1)); (62, OpenMP, #pragma omp teams distribute parallel for collapse (N)); (45, OpenMP, #pragma omp teams distribute parallel for collapse (argv[1]=2)); (0, OpenMP, #pragma omp teams distribute parallel for collapse); (32, OpenMP, #pragma omp teams distribute parallel for collapse ((ST > 0)? 1 + ST : 2)); (64, OpenMP, #pragma omp teams distribute parallel for collapse (2)); (53, OpenMP, #pragma omp teams distribute parallel for collapse (1)); (6, OpenMP, #pragma omp teams distribute parallel for collapse ()); (15, OpenMP, #pragma omp teams distribute parallel for collapse (argc); (10, OpenMP, #pragma omp teams distribute parallel for collapse (argc); (-7, OpenMP, #pragma omp target); (0, OpenMP, #pragma omp teams distribute parallel for collapse (); (16, OpenMP, #pragma omp teams distribute parallel for collapse (ST)", "label": "(31, OpenMP, #pragma omp teams distribute parallel for collapse (1)); (35, OpenMP, #pragma omp teams distribute parallel for collapse ((ST > 0)? 1 + ST : 2)); (1, OpenMP, #pragma omp teams distribute parallel for collapse); (10, OpenMP, #pragma omp teams distribute parallel for collapse ()); (44, OpenMP, #pragma omp distribute collapse (S)); (21, OpenMP, #pragma omp teams distribute parallel for collapse (ST); (62, OpenMP, #pragma omp teams distribute parallel for collapse (N)); (13, OpenMP, #pragma omp teams distribute parallel for collapse (argc); (65, OpenMP, #pragma omp teams distribute parallel for collapse (2)); (49, OpenMP, #pragma omp teams distribute parallel for collapse (argv[1]=2)); (-8, OpenMP, #pragma omp target); (-3, OpenMP, #pragma omp teams distribute parallel for collapse (); (32, OpenMP, #pragma omp teams distribute parallel for collapse (foobool(argc)), collapse (true), collapse (-5))"}
{"predict": "(213, MPI, MPI_Allreduce(&total_count, out_count, 1, MPI_UINT64_T, MPI_SUM, MPI_COMM_WORLD);); (44, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (42, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(45, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (47, MPI, double start = MPI_Wtime();); (237, MPI, double now = MPI_Wtime();); (41, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(37, MPI, MPI_Comm_rank(comm, &par_rank);); (37, MPI, MPI_Comm_size(comm, &par_size);)", "label": "(33, MPI, flow::Workspace::set_default_mpi_comm(MPI_Comm_c2f(comm));); (16, MPI, MPI_Comm_rank(comm, &par_rank);); (16, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(3, OpenCL, cl_mem smallBuffer = clCreateBuffer(clContext, CL_MEM_READ_WRITE, 1024 * 1024, nullptr, &retVal);); (12, OpenCL, retVal = clReleaseMemObject(smallBuffer);)", "label": "(13, OpenCL, retVal = clReleaseMemObject(smallBuffer);); (17, OpenCL, EXPECT_EQ(clReleaseContext(context), CL_SUCCESS);); (1, OpenCL, cl_mem smallBuffer = clCreateBufferWithProperties(clContext, &memProperties, flags, size, hostPtr, &retVal);)"}
{"predict": "(8, OpenMP, int tid = omp_get_thread_num();); (4, OpenMP, #pragma omp parallel num_threads(8)); (2, OpenMP, omp_set_affinity_format(formats[i]);)", "label": "(8, OpenMP, int tid = omp_get_thread_num();); (4, OpenMP, #pragma omp parallel num_threads(8)); (2, OpenMP, omp_set_affinity_format(formats[i]);)"}
{"predict": "(16, CUDA, __shared__ uninitialized_array<thrust::complex<T>, 1024> cache;); (28, CUDA, cache[id] += static_cast<thrust::complex<T>>(A_[i * nchol + b] * B_[i * nchol + a]);); (20, CUDA, thrust::complex<T2> const* A_(Tab + blockIdx.x * nocc * nocc * nchol * A);); (18, CUDA, cache[id] = thrust::complex<T>(0.0);); (20, CUDA, cache[id] += static_cast<thrust::complex<T>>(A_[i * nchol + a] * B_[i * nchol + b]);); (10, CUDA, __shared__ uninitialized_array<thrust::complex<T>, 1024> cache;); (15, CUDA, cache[id] = thrust::complex<T>(alpha);); (12, CUDA, cache[id] = thrust::complex<T>(0.0);); (16, CUDA, cache[id] += static_cast<thrust::complex<T>>(B_[i * nchol + b] * A_[i * nchol + a]);); (9, CUDA, cache[id] = thrust::complex<T2>(0.0);)", "label": "(16, CUDA, __shared__ uninitialized_array<thrust::complex<T>, 1024> cache;); (29, CUDA, cache[id] += static_cast<thrust::complex<T>>(A_[(a * nchol + i) * nocc + b] * A_[(b * nchol + i) * nocc + a]);); (21, CUDA, thrust::complex<T2> const* A_(Tab + blockIdx.x * nocc * nocc * nchol);); (18, CUDA, cache[id]              = thrust::complex<T>(0.0);); (18, CUDA, thrust::complex<T> alp = static_cast<thrust::complex<T>>(alpha);)"}
{"predict": "(184, CUDA, CUDA_CHECK(cudaFree(d_B_array));); (184, CUDA, CUDA_CHECK(cudaFree(d_C_array));); (229, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (180, CUDA, CUDA_CHECK(cudaFree(d_A_array));); (174, CUDA, CUDA_CHECK(cudaFree(d_A[i]));); (229, CUDA, CUDA_CHECK(cudaDeviceReset());); (174, CUDA, CUDA_CHECK(cudaFree(d_B[i]));); (76, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));); (164, CUDA, CUDA_CHECK(cudaFree(d_C[i]));); (160, CUDA, CUDA_CHECK(cudaFree(d_A[i]));)", "label": "(169, CUDA, CUDA_CHECK(cudaFree(d_B_array));); (169, CUDA, CUDA_CHECK(cudaFree(d_C_array));); (177, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (165, CUDA, CUDA_CHECK(cudaFree(d_A_array));); (168, CUDA, CUDA_CHECK(cudaFree(d_A[i]));); (169, CUDA, CUDA_CHECK(cudaFree(d_C[i]));); (133, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (174, CUDA, CUDA_CHECK(cudaDeviceReset());); (165, CUDA, CUDA_CHECK(cudaFree(d_B[i]));); (75, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(49, CUDA, HANDLE_ERROR( cudaFree( dev_partial_c ) );); (52, CUDA, HANDLE_ERROR( cudaFree( dev_b ) );); (50, CUDA, HANDLE_ERROR( cudaFree( dev_a ) );)", "label": "(48, CUDA, HANDLE_ERROR( cudaFree( dev_a ) );); (49, CUDA, HANDLE_ERROR( cudaFree( dev_partial_c ) );); (47, CUDA, HANDLE_ERROR( cudaFree( dev_b ) );)"}
{"predict": "(10, OpenMP, #pragma omp for); (10, OpenMP, #pragma omp flush); (5, OpenMP, #pragma omp parallel shared(result, nr_iterations, i)); (10, OpenMP, #pragma omp critical)", "label": "(12, OpenMP, omp_set_nest_lock(&lck);); (18, OpenMP, omp_unset_nest_lock(&lck);); (8, OpenMP, #pragma omp for); (10, OpenMP, #pragma omp flush); (4, OpenMP, #pragma omp parallel shared(lck)); (2, OpenMP, omp_init_nest_lock(&lck);); (16, OpenMP, omp_destroy_nest_lock(&lck);)"}
{"predict": "(15, MPI, MPI_Type_get_true_extent(inkey, &key_true_lb, &key_true_extent);); (18, MPI, MPI_Type_get_true_extent(inkeysat, &keysat_true_lb, &keysat_true_extent);)", "label": "(19, MPI, MPI_Type_get_true_extent(inkeysat, &keysat_true_lb, &keysat_true_extent);); (76, MPI, MPI_Type_dup(inkey,    &values->key);); (13, MPI, MPI_Type_get_true_extent(inkey, &key_true_lb, &key_true_extent);); (75, MPI, MPI_Type_dup(inkeysat, &values->keysat);)"}
{"predict": "(6, TBB, tbb::task_group g;); (6, TBB, tbb::task* h1 = new(tbb::task::allocate_root()) tbb::empty_task;); (6, TBB, tbb::task* h2 = new(tbb::task::allocate_root()) tbb::empty_task;); (9, TBB, tbb::task::spawn_root_and_wait(g);); (9, TBB, x = tbb::task::x(g);); (9, TBB, y = tbb::task::y(g);)", "label": "(6, TBB, tbb::task_group g;); (6, TBB, tbb::task_handle h1 = make_task(g, x, n-1);); (6, TBB, tbb::task_handle h2 = make_task(g, y, n-2);)"}
{"predict": "(17, CUDA, CUDA_FUNC(cudaStreamDestroy(resources->exec_streams[i]));)", "label": "(17, CUDA, CUDA_FUNC(cudaStreamDestroy(resources->exec_streams[i]));)"}
{"predict": "(31, MPI, error_code = MPI_Comm_test_inter(comm, &flag);); (56, MPI, MPI_Comm_dup(comm, &dupcomm);); (126, MPI, MPI_Comm_rank(dupcomm, &rank);); (135, MPI, MPI_Barrier(dupcomm);); (67, MPI, MPI_Allreduce(&amode, &tmp_amode, 1, MPI_INT, ADIO_same_amode, dupcomm);)", "label": "(31, MPI, error_code = MPI_Comm_test_inter(comm, &flag);); (56, MPI, MPI_Comm_dup(comm, &dupcomm);); (126, MPI, MPI_Comm_rank(dupcomm, &rank);); (136, MPI, MPI_Barrier(dupcomm);); (67, MPI, MPI_Allreduce(&amode, &tmp_amode, 1, MPI_INT, ADIO_same_amode, dupcomm);)"}
{"predict": "(5, OpenMP, #pragma omp parallel for)", "label": "(5, OpenMP, #pragma omp parallel for)"}
{"predict": "(14, MPI, MPI_Bcast( &nbases, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (28, MPI, MPI_Bcast( &iphysdim, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (26, MPI, MPI_Bcast( &icelldim, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (38, MPI, MPI_Bcast( &nzones, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (86, MPI, MPI_Bcast( zonename, 33, MPI_CHAR, Parallel::serverid, MPI_COMM_WORLD );); (85, MPI, MPI_Bcast( isize.data(), index_dim * 3, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (28, MPI, MPI_Bcast( &ipriddim, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (25, MPI, MPI_Bcast( &iprowdim, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (18, MPI, MPI_Bcast( &bocotype, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (14, MPI, MPI_Bcast( &bocodim, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (9, MPI, MPI_Bcast( &nbocos, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (14, MPI, MPI_Bcast( &location, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (9, MPI, MPI_Bcast( &ptset_type, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (12, MPI, MPI_Bcast( &ptset_dim, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (16, MPI, MPI_Bcast( &ptset_base, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (4, MPI, MPI_Bcast( &dataType, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (16, MPI, MPI_Bcast( &ptset_name, 33, MPI_CHAR, Parallel::serverid, MPI_COMM_WORLD );); (7, MPI,", "label": "(224, MPI, MPI_Bcast( &bocotype, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (190, MPI, MPI_Bcast( &nbocos, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (202, MPI, MPI_Bcast( &location, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (38, MPI, MPI_Bcast( &nzones, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (84, MPI, MPI_Bcast( zonename, 33, MPI_CHAR, Parallel::serverid, MPI_COMM_WORLD );); (295, MPI, MPI_Bcast( range.data(), npnts * index_dim, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (219, MPI, MPI_Bcast( &ptset_type, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (251, MPI, MPI_Bcast( pnts.data(), npnts * index_dim, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (81, MPI, MPI_Bcast( isize.data(), index_dim * 3, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (113, MPI, MPI_Bcast( &zoneType, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (220, MPI, MPI_Bcast( normalIndex.data(), index_dim, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (216, MPI, MPI_Bcast( &npnts, 1, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (289, MPI, MPI_Bcast( donor_range.data(), npnts * index_dim, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (164, MPI, MPI_Bcast( pcoor, nNodes, MPI_DOUBLE, Parallel::serverid, MPI_COMM_WORLD );); (145, MPI, MPI_Bcast( &dataType, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (283, MPI, MPI_Bcast( connectname, 33, MPI_CHAR, Parallel::serverid, MPI_COMM_WORLD );); (243, MPI, MPI_Bcast( normalList.data(), nNodes * iphysdim * sizeof( double ), MPI_CHAR, Parallel::serverid, MPI_COMM_WORLD );); (-3, MPI, MPI_Bcast( &nbases, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (205, MPI, MPI_Bcast( boconame, 33, MPI_CHAR, Parallel::serverid, MPI_COMM_WORLD );); (139, MPI, MPI_Bcast( coordname, 33, MPI_CHAR, Parallel::serverid, MPI_COMM_WORLD );); (208, MPI, MPI_Bcast( &normalListSize, 1, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (256, MPI, MPI_Bcast( &n1to1, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (6, MPI, MPI_Bcast( &icelldim, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (6, MPI, MPI_Bcast( &iphysdim, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (205, MPI, MPI_Bcast( &ndataset, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (274, MPI, MPI_Bcast( donorname, 33, MPI_CHAR, Parallel::serverid, MPI_COMM_WORLD );); (104, MPI, MPI_Bcast( &ncoords, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (199, MPI, MPI_Bcast( &normalDataType, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );)"}
{"predict": "(58, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (58, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)", "label": "(58, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (58, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)"}
{"predict": "(3, CUDA, using alloc = thrust::cuda::allocator<double>;); (11, CUDA, cudaDeviceSynchronize(); benchmark::DoNotOptimize(base(y)); benchmark::ClobberMemory();)", "label": "(3, CUDA, using alloc = thrust::cuda::allocator<double>;); (11, CUDA, cudaDeviceSynchronize(); benchmark::DoNotOptimize(y.base()); benchmark::ClobberMemory();)"}
{"predict": "(32, OpenMP, #pragma omp target); (55, OpenMP, #pragma omp target data use_device_ptr(a))", "label": "(32, OpenMP, #pragma omp target); (35, OpenMP, int device_id = omp_get_default_device();); (54, OpenMP, #pragma omp target data use_device_ptr(a))"}
{"predict": "(23, CUDA, cudaMemcpy(h_C, d_C, DSIZE*sizeof(float), cudaMemcpyDeviceToHost);); (14, CUDA, cudaMemcpy(d_B, h_B, DSIZE*sizeof(float), cudaMemcpyHostToDevice);); (24, CUDA, cudaFree(d_A);); (10, CUDA, cudaMemcpy(d_A, h_A, DSIZE*sizeof(float), cudaMemcpyHostToDevice);); (21, CUDA, cudaFree(d_B);); (21, CUDA, cudaFree(d_C);); (7, CUDA, cudaMalloc(&d_C, DSIZE*sizeof(float));); (3, CUDA, cudaMalloc(&d_A, DSIZE*sizeof(float));); (3, CUDA, cudaMalloc(&d_B, DSIZE*sizeof(float));)", "label": "(28, CUDA, cudaCheckErrors(\"kernel execution failure or cudaMemcpy H2D failure\");); (14, CUDA, cudaMemcpy(d_A, h_A, DSIZE*sizeof(float), cudaMemcpyHostToDevice);); (16, CUDA, cudaCheckErrors(\"cudaMemcpy H2D failure\");); (23, CUDA, cudaMemcpy(h_C, d_C, DSIZE*sizeof(float), cudaMemcpyDeviceToHost);); (13, CUDA, cudaMemcpy(d_B, h_B, DSIZE*sizeof(float), cudaMemcpyHostToDevice);); (7, CUDA, cudaMalloc(&d_C, DSIZE*sizeof(float));); (5, CUDA, cudaMalloc(&d_B, DSIZE*sizeof(float));); (6, CUDA, cudaCheckErrors(\"cudaMalloc failure\");); (15, CUDA, cudaCheckErrors(\"kernel launch failure\");); (1, CUDA, cudaMalloc(&d_A, DSIZE*sizeof(float));)"}
{"predict": "(16, OpenMP, #pragma omp target teams loop map(from: C[0:N2]) map(to: A[0:N1], B[0:N1]))", "label": "(17, OpenMP, #pragma omp target teams distribute parallel for collapse(2)); (23, OpenMP, #pragma omp target exit data map(delete:A[0:N1],B[0:N1],C[0:N2])); (14, OpenMP, #pragma omp target enter data map(to: A[0:N1],B[0:N1],C[0:N2])); (20, OpenMP, #pragma omp target update from(C[0:N2]))"}
{"predict": "(16, OpenMP, #pragma omp parallel for)", "label": "(16, OpenMP, #pragma omp parallel for)"}
{"predict": "(56, OpenCL, clReleaseEvent(event);)", "label": "(54, OpenCL, clReleaseEvent(event);)"}
{"predict": "(16, MPI, *v6 = MPI_Cart_create( (MPI_Comm)(*v1), *v2, l4, &l5, 0, &ierr );)", "label": "(17, MPI, *ierr = MPI_Cart_create( (MPI_Comm)(*v1), *v2, v3, l4, l5, (MPI_Comm *)(v6) );)"}
{"predict": "(191, MPI, MPI_Wait( &send_req[p], &status);); (191, MPI, MPI_Wait( &send_req[procs+p], &status);); (170, MPI, MPI_Recv( itemp, it, mpi_int_t, p, p, grid->comm, &status );)", "label": "(191, MPI, MPI_Wait( &send_req[p], &status);); (191, MPI, MPI_Wait( &send_req[procs+p], &status);); (170, MPI, MPI_Recv( itemp, it, mpi_int_t, p, p, grid->comm, &status );)"}
{"predict": "(16, CUDA, return CUDA(cudaGetLastError());)", "label": "(16, CUDA, return CUDA(cudaGetLastError());)"}
{"predict": "(50, CUDA, CHECK_CUDA(cudaMalloc(&dy, dataSize * sizeof(int32_t)));); (75, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (32, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (75, CUDA, CHECK_CUDA(cudaFree(dy));); (46, CUDA, CHECK_CUDA(cudaMalloc(&dx, dataSize * sizeof(int32_t)));); (32, CUDA, CHECK_HIEDNN(hiednnSetCudaStream(handle, 0));); (72, CUDA, CHECK_CUDA(cudaFree(dx));)", "label": "(50, CUDA, CHECK_CUDA(cudaMalloc(&dy, dataSize * sizeof(int32_t)));); (75, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (32, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (76, CUDA, CHECK_CUDA(cudaFree(dy));); (45, CUDA, CHECK_CUDA(cudaMalloc(&dx, dataSize * sizeof(int32_t)));); (32, CUDA, CHECK_HIEDNN(hiednnSetCudaStream(handle, 0));); (72, CUDA, CHECK_CUDA(cudaFree(dx));)"}
{"predict": "(23, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(23, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(26, OpenMP, #pragma omp parallel for reduction(&&: Red)); (21, OpenMP, #pragma omp parallel for reduction(|: Red)); (16, OpenMP, #pragma omp parallel for reduction(&& : Red)); (7, OpenMP, #pragma omp parallel for reduction(-: Red)); (0, OpenMP, #pragma omp parallel for reduction(+: Red)); (23, OpenMP, #pragma omp parallel for reduction(^: Red)); (8, OpenMP, #pragma omp parallel for reduction(*: Red)); (25, OpenMP, #pragma omp parallel for reduction(||: Red)); (16, OpenMP, #pragma omp parallel for reduction(min: Red)); (9, OpenMP, #pragma omp parallel for reduction(^: Red)); (2, OpenMP, #pragma omp parallel for reduction(|: Red)); (25, OpenMP, #pragma omp parallel for reduction(&& : Red)); (16, OpenMP, #pragma omp parallel for reduction(max: Red)); (-4, OpenMP, #pragma omp parallel for reduction(+: Red)); (11, OpenMP, #pragma omp parallel for reduction(+ : Red))", "label": "(28, OpenMP, #pragma omp parallel for reduction(&&: Red)); (7, OpenMP, #pragma omp parallel for reduction(-: Red)); (14, OpenMP, #pragma omp parallel for reduction(&: Red)); (17, OpenMP, #pragma omp parallel for reduction(|: Red)); (0, OpenMP, #pragma omp parallel for reduction(+: Red)); (19, OpenMP, #pragma omp parallel for reduction(^: Red)); (6, OpenMP, #pragma omp parallel for reduction(*: Red)); (25, OpenMP, #pragma omp parallel for reduction(||: Red))"}
{"predict": "(11, OpenMP, #pragma omp simd aligned ()); (18, OpenMP, #pragma omp simd aligned (*argc > 0? argv[1] : argv[2])); (7, OpenMP, #pragma omp simd aligned (argc); (22, OpenMP, #pragma omp simd aligned(e, g)); (39, OpenMP, #pragma omp simd aligned(f:j)); (12, OpenMP, #pragma omp simd aligned (argc : 5)); (22, OpenMP, #pragma omp simd aligned(i)); (-2, OpenMP, #pragma omp simd aligned); (4, OpenMP, #pragma omp simd aligned (argc,); (23, OpenMP, #pragma omp parallel); (27, OpenMP, #pragma omp simd aligned(v:16)); (10, OpenMP, #pragma omp simd aligned (S1)); (15, OpenMP, #pragma omp simd aligned(h)); (28, OpenMP, #pragma omp simd aligned(f)); (3, OpenMP, #pragma omp simd aligned (argc : argc)); (8, OpenMP, #pragma omp simd aligned (argv[1])); (-2, OpenMP, #pragma omp simd aligned (argc,)", "label": "(25, OpenMP, #pragma omp simd aligned(e, g)); (18, OpenMP, #pragma omp simd aligned (argc : 5)); (29, OpenMP, #pragma omp simd aligned(i)); (5, OpenMP, #pragma omp simd aligned (); (17, OpenMP, #pragma omp simd aligned (S1)); (28, OpenMP, #pragma omp parallel); (40, OpenMP, #pragma omp simd aligned(f:j)); (10, OpenMP, #pragma omp simd aligned (*argc > 0? argv[1] : argv[2])); (6, OpenMP, #pragma omp simd aligned (argc,); (14, OpenMP, #pragma omp simd aligned (argv[1])); (-4, OpenMP, #pragma omp simd aligned); (-1, OpenMP, #pragma omp simd aligned ()); (25, OpenMP, #pragma omp simd aligned(v:16)); (28, OpenMP, #pragma omp simd aligned(f)); (14, OpenMP, #pragma omp simd aligned(h)); (-3, OpenMP, #pragma omp simd aligned (argc)"}
{"predict": "(13, MPI, MPI_Type_size(type, &size);); (19, MPI, MPI_Type_free(&type);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (12, MPI, mpi_errno = MPI_Type_ub(type, NULL);); (10, MPI, MPI_Error_class(mpi_errno, &errclass);); (5, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(13, MPI, MPI_Type_size(type, &size);); (14, MPI, mpi_errno = MPI_Type_ub(type, NULL);); (18, MPI, MPI_Type_free(&type);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (11, MPI, MPI_Error_class(mpi_errno, &errclass);); (4, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(24, MPI, MPI_Start(&req);); (11, MPI, MPI_Comm_size(comm, &size);); (24, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (25, MPI, MPI_Request_free(&req);); (12, MPI, MPI_Info_create(&info);); (24, MPI, MPI_Info_free(&info);); (7, MPI, MPI_Comm_rank(comm, &rank);); (18, MPI, MPI_Reduce_scatter_init(sendbuf, recvbuf, recvcounts, MPI_INT, MPI_SUM, comm, info, &req);); (34, MPI, MPI_Reduce_scatter_finalize(&req);)", "label": "(26, MPI, MPI_Start(&req);); (11, MPI, MPI_Comm_size(comm, &size);); (13, MPI, MPI_Info_create(&info);); (24, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (36, MPI, MPI_Info_free(&info);); (34, MPI, MPI_Request_free(&req);); (18, MPI, MPI_Reduce_scatter_init(sendbuf, &recvbuf, recvcounts, MPI_INT, MPI_SUM, comm, info, &req);); (6, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(4, MPI, MPI_Init(&argc, &argv);); (30, MPI, MPI_Finalize();); (3, MPI, int ierr = MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(4, MPI, MPI_Init(&argc, &argv);); (30, MPI, MPI_Finalize();); (3, MPI, int ierr = MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(7, CUDA, __shared__ block_radix_sort_t::TempStorage temp_storage;)", "label": "(7, CUDA, __shared__ block_radix_sort_t::TempStorage temp_storage;)"}
{"predict": "(24, CUDA, extern __shared__ char smem_[];); (27, CUDA, __shared__ half2 smem_red[VECS_PER_CTA][WARPS];)", "label": "(24, CUDA, extern __shared__ char smem_[];); (26, CUDA, __shared__ half2 smem_red[VECS_PER_CTA][WARPS];)"}
{"predict": "(83, OpenCL, err = clGetDeviceInfo(devices[i], CL_DEVICE_NAME, sizeof(devicename), devicename, NULL);); (69, OpenCL, err = clGetDeviceInfo(devices[i], CL_DEVICE_TYPE, sizeof(device_type), &device_type, NULL);)", "label": "(76, OpenCL, err = clBuildProgram(plan->program, 1, &devices[i], coptions, NULL, NULL);); (59, OpenCL, plan->program = clCreateProgramWithSource(context, 1, (const char**) &source_str, NULL, &err);); (68, OpenCL, err = clGetDeviceInfo(devices[i], CL_DEVICE_TYPE, sizeof(device_type), &device_type, NULL);); (88, OpenCL, err = clGetDeviceInfo(devices[i], CL_DEVICE_NAME, sizeof(devicename), devicename, NULL);)"}
{"predict": "(22, MPI, MPI_Recv(out, 1, type, set_src? left : right, tag, comm, MPI_STATUS_IGNORE);); (24, MPI, MPI_Send(in, 1, type, set_src? left : right, tag, comm);); (18, MPI, MPI_Send(in, 1, type, set_src? right : left, tag, comm);); (27, MPI, MPI_Recv(in, 1, type, set_src? right : left, tag, comm, MPI_STATUS_IGNORE);); (18, MPI, MPI_Waitall(2 * window_size, requests, MPI_STATUSES_IGNORE);); (22, MPI, MPI_Send(out, 1, type, set_src? right : left, tag, comm);)", "label": "(24, MPI, MPI_Recv(&ack, 1, MPI_CHAR, right, tag, comm, MPI_STATUS_IGNORE);); (26, MPI, MPI_Irecv(out, count, type, set_src? left : MPI_ANY_SOURCE, tag, comm, &requests[w]);); (28, MPI, MPI_Isend(in, count, type, left, (tag == MPI_ANY_TAG? 0 : tag), comm, &requests[w + window_size]);); (20, MPI, MPI_Waitall(2 * window_size, requests, MPI_STATUSES_IGNORE);); (17, MPI, MPI_Isend(in, count, type, right, (tag == MPI_ANY_TAG? 0 : tag), comm, &requests[w + window_size]);); (13, MPI, MPI_Irecv(out, count, type, set_src? right : MPI_ANY_SOURCE, tag, comm, &requests[w]);); (27, MPI, MPI_Send(&ack, 1, MPI_CHAR, left, (tag == MPI_ANY_TAG? 0 : tag), comm);)"}
{"predict": "(2, OpenMP, #pragma omp parallel master taskloop simd collapse); (21, OpenMP, #pragma omp parallel master taskloop simd collapse (foobool(argc)), collapse (true), collapse (-5)); (11, OpenMP, #pragma omp parallel master taskloop simd collapse (ST); (22, OpenMP, #pragma omp parallel master taskloop simd collapse (S)); (30, OpenMP, #pragma omp parallel master taskloop simd collapse (N)); (2, OpenMP, #pragma omp parallel master taskloop simd collapse ()); (29, OpenMP, #pragma omp parallel master taskloop simd collapse (2)); (4, OpenMP, #pragma omp parallel master taskloop simd collapse (argc); (8, OpenMP, #pragma omp parallel master taskloop simd collapse ((ST > 0)? 1 + ST : 2)); (-6, OpenMP, #pragma omp parallel master taskloop simd collapse (); (5, OpenMP, #pragma omp parallel master taskloop simd collapse (1))); (-8, OpenMP, #pragma omp parallel master taskloop simd collapse)", "label": "(13, OpenMP, #pragma omp parallel master taskloop simd collapse (ST); (5, OpenMP, #pragma omp parallel master taskloop simd collapse ()); (15, OpenMP, #pragma omp parallel master taskloop simd collapse ((ST > 0)? 1 + ST : 2)); (1, OpenMP, #pragma omp parallel master taskloop simd collapse (); (20, OpenMP, #pragma omp parallel master taskloop simd collapse (S)); (10, OpenMP, #pragma omp parallel master taskloop simd collapse (1))); (27, OpenMP, #pragma omp parallel master taskloop simd collapse (1)); (30, OpenMP, #pragma omp parallel master taskloop simd collapse (2)); (14, OpenMP, #pragma omp parallel master taskloop simd collapse (foobool(argc)), collapse (true), collapse (-5)); (-7, OpenMP, #pragma omp parallel master taskloop simd collapse); (0, OpenMP, #pragma omp parallel master taskloop simd collapse (argc); (24, OpenMP, #pragma omp parallel master taskloop simd collapse (N))"}
{"predict": "(2, TBB, tbb::task_group_context tgc;); (5, TBB, tbb::task_group_context tgc;)", "label": "(5, TBB, tbb::task_group_context tgc;); (15, TBB, tgc.set_priority(tbb::priority_high);); (4, TBB, tgc.set_priority(tbb::priority_normal);)"}
{"predict": "(8, OpenMP, #pragma omp parallel for)", "label": "(8, OpenMP, #pragma omp parallel for)"}
{"predict": "(13, OpenMP, #pragma omp parallel); (13, OpenMP, #pragma omp task if (task: false))", "label": "(13, OpenMP, #pragma omp parallel); (13, OpenMP, #pragma omp task if (task: false))"}
{"predict": "(44, OpenMP, #pragma omp distribute parallel for schedule(dynamic, no chunk)); (23, OpenMP, #pragma omp distribute parallel for schedule(dist_schedule, static, no chunk)); (8, OpenMP, #pragma omp distribute parallel for schedule); (29, OpenMP, #pragma omp distribute parallel for schedule(static, no chunk)); (34, OpenMP, #pragma omp distribute parallel for schedule(static, chunk)); (49, OpenMP, #pragma omp distribute parallel for schedule(dynamic, chunk)); (4, OpenMP, #pragma omp target); (51, OpenMP, #pragma omp parallel for schedule(dynamic, chunk)); (0, OpenMP, #pragma omp teams)", "label": "(42, OpenMP, #pragma omp distribute parallel for schedule(static, ch)); (49, OpenMP, #pragma omp distribute parallel for schedule(dynamic)); (7, OpenMP, #pragma omp distribute parallel for); (15, OpenMP, #pragma omp distribute parallel for dist_schedule(static)); (30, OpenMP, #pragma omp distribute parallel for schedule(static)); (53, OpenMP, #pragma omp distribute parallel for schedule(dynamic, ch)); (5, OpenMP, #pragma omp cancel for); (0, OpenMP, #pragma omp target); (18, OpenMP, #pragma omp distribute parallel for dist_schedule(static, ch)); (-1, OpenMP, #pragma omp teams)"}
{"predict": "(37, OpenMP, #pragma omp distribute private(i)); (25, OpenMP, #pragma omp target); (54, OpenMP, #pragma omp distribute firstprivate(i)); (4, OpenMP, #pragma omp target); (22, OpenMP, #pragma omp teams private(i)); (8, OpenMP, #pragma omp teams private(argc); (43, OpenMP, #pragma omp distribute private(j)); (16, OpenMP, #pragma omp teams private(argc)); (35, OpenMP, #pragma omp teams); (19, OpenMP, #pragma omp teams private(S2::S2sc)); (0, OpenMP, #pragma omp teams private(); (19, OpenMP, #pragma omp teams private(da)); (57, OpenMP, #pragma omp distribute); (22, OpenMP, #pragma omp teams private(e, g)); (27, OpenMP, #pragma omp parallel private(i)); (3, OpenMP, #pragma omp teams private(argc,); (51, OpenMP, #pragma omp teams reduction(+:i)); (57, OpenMP, #pragma omp teams reduction(+:j)); (5, OpenMP, #pragma omp teams private(argc > 0? argv[1] : argv[2])); (44, OpenMP, #pragma omp distribute private(i)); (7, OpenMP, #pragma omp teams private(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (-11, OpenMP, #pragma omp teams private); (20, OpenMP, #pragma omp teams shared(i)); (13, OpenMP, #pragma omp teams private(S2::S2s))", "label": "(35, OpenMP, #pragma omp distribute private(e, g)); (51, OpenMP, #pragma omp parallel private(i)); (5, OpenMP, #pragma omp distribute private); (12, OpenMP, #pragma omp distribute private (argc,); (9, OpenMP, #pragma omp distribute private (argc); (36, OpenMP, #pragma omp target); (19, OpenMP, #pragma omp distribute private (argv[1])); (12, OpenMP, #pragma omp distribute private (argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (9, OpenMP, #pragma omp distribute private (argc > 0? argv[1] : argv[2])); (14, OpenMP, #pragma omp distribute private (a, b, c, d, f)); (17, OpenMP, #pragma omp distribute private(ba)); (39, OpenMP, #pragma omp distribute private(j)); (-1, OpenMP, #pragma omp distribute private ()); (24, OpenMP, #pragma omp distribute private(h)); (44, OpenMP, #pragma omp teams reduction(+:i)); (18, OpenMP, #pragma omp distribute private(S2::S2s)); (43, OpenMP, #pragma omp distribute private(i)); (14, OpenMP, #pragma omp distribute private(da)); (-9, OpenMP, #pragma omp distribute private (); (2, OpenMP, #pragma omp distribute private (S1)); (29, OpenMP, #pragma omp teams private(i)); (8, OpenMP, #pragma omp distribute private(ca)); (17, OpenMP, #pragma omp distribute); (22, OpenMP, #pragma omp distribute firstprivate(i)); (30, OpenMP, #pragma omp teams firstprivate(i)); (17, OpenMP, #pragma omp teams)"}
{"predict": "(22, CUDA, __shared__ ll_t smem[_TPB_];)", "label": "(22, CUDA, __shared__ volatile ll_t smem[_TPB_];)"}
{"predict": "(3, MPI, MPI_Init(&argc, &argv);); (24, MPI, MPI_Finalize();); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)", "label": "(3, MPI, MPI_Init(&argc, &argv);); (303, MPI, MPI_Finalize();); (23, MPI, MPI_Barrier(MPI_COMM_WORLD);); (222, MPI, MPI_Allreduce(&local_opt_worst_time, &opt_worst_time, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);)"}
{"predict": "(7, CUDA, cudaMemcpy(data,datag+inidata,sizeof(float3)*ndata,cudaMemcpyDeviceToHost);); (6, CUDA, fcuda::Check_CudaErroorFun(\"DgReduSumFloat3\");)", "label": "(7, CUDA, fcuda::Check_CudaErroorFun(\"DgReduSumFloat3\");); (5, CUDA, cudaMemcpy(data,datag+inidata,sizeof(float3)*ndata,cudaMemcpyDeviceToHost);)"}
{"predict": "(134, OpenMP, #define GOMP_CRITICAL_SWP(FLAG))", "label": "(127, OpenMP, #define GOMP_CRITICAL_SWP(FLAG))"}
{"predict": "(24, CUDA, cudaMemcpy(dst, src, bytes, cudaMemcpyDeviceToDevice);); (27, CUDA, cudaEventRecord(stopEvent, 0);); (27, CUDA, cudaEventSynchronize(stopEvent);); (29, CUDA, cudaEventElapsedTime(&time, startEvent, stopEvent);); (18, CUDA, cudaEventRecord(startEvent, 0);)", "label": "(27, CUDA, cudaMemcpy(dst, src, bytes, kind);)"}
{"predict": "(8, TBB, for ( std::list< std::pair<const int, int> >::iterator it = arrIntTbb.begin(); it!= arrIntTbb.end(); ++it, ++b ) *it = std::make_pair( i, b );); (11, TBB, TestConstructorWithBigType<tbb::flow::interface11::internal::Wrapper>( arrIntTbb );); (11, TBB, TestConstructorWithBigType<tbb::flow::interface11::internal::Wrapper>( arrRefInt );); (12, TBB, TestConstructorWithBigType<tbb::flow::interface11::internal::Wrapper>( arrIntRef );); (11, TBB, TestConstructorWithBigType<tbb::flow::interface11::internal::Wrapper>( arrWkWk );); (12, TBB, TestConstructorWithBigType<tbb::flow::interface11::internal::Wrapper>( arrShrShr );)", "label": "(9, TBB, tbb::atomic<int> b;); (6, TBB, std::list< std::pair< const int, tbb::atomic<int> > > arrIntTbb;)"}
{"predict": "(97, MPI, MPI_Query_thread(&omp_mpi_level);); (38, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (34, MPI, MPI_Init_thread(&argc, &argv, required, &provided);)", "label": "(97, MPI, MPI_Query_thread(&omp_mpi_level);); (38, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (33, MPI, MPI_Init_thread(&argc, &argv, required, &provided);)"}
{"predict": "(443, MPI, start_exch = MPI_Wtime();); (145, MPI, start_comm_time = MPI_Wtime();); (192, MPI, end_comm_time = MPI_Wtime();); (494, MPI, start_write_time = MPI_Wtime();); (500, MPI, end_write_time = MPI_Wtime();); (536, MPI, end_exch = MPI_Wtime();)", "label": "(391, MPI, start_exch = MPI_Wtime();); (143, MPI, start_comm_time = MPI_Wtime();); (156, MPI, end_comm_time = MPI_Wtime();); (435, MPI, start_write_time = MPI_Wtime();); (442, MPI, end_write_time = MPI_Wtime();); (495, MPI, end_exch = MPI_Wtime();)"}
{"predict": "(33, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (35, CUDA, CUDA_CHECK(cudaFree(d_data));)", "label": "(33, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (35, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(44, MPI, MPI_Pack(a, 1, xpose, buffer, bufsize, &position, MPI_COMM_WORLD);); (31, MPI, MPI_Type_commit(&xpose);); (58, MPI, MPI_Type_free(&row);); (38, MPI, MPI_Pack_size(1, xpose, MPI_COMM_WORLD, &bufsize);); (29, MPI, MPI_Type_hvector(100, 1, sizeofint, row, &xpose);); (26, MPI, MPI_Type_extent(MPI_INT, &sizeofint);); (53, MPI, MPI_Type_free(&xpose);); (29, MPI, MPI_Pack(&(a[0][0]), 1, xpose, buffer, bufsize, &position, MPI_COMM_WORLD);); (38, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (19, MPI, MPI_Type_contiguous(100 * 100, MPI_INT, &row);); (39, MPI, MPI_Unpack(buffer, bufsize, &position, b, 100 * 100, MPI_INT, MPI_COMM_WORLD);)", "label": "(43, MPI, MPI_Pack(a, 1, xpose, buffer, bufsize, &position, MPI_COMM_WORLD);); (59, MPI, MPI_Type_free(&row);); (31, MPI, MPI_Type_commit(&xpose);); (56, MPI, MPI_Type_free(&xpose);); (28, MPI, MPI_Type_hvector(100, 1, sizeofint, row, &xpose);); (23, MPI, MPI_Type_extent(MPI_INT, &sizeofint);); (30, MPI, MPI_Pack_size(1, xpose, MPI_COMM_WORLD, &bufsize);); (34, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (39, MPI, MPI_Unpack(buffer, bufsize, &position, b, 100 * 100, MPI_INT, MPI_COMM_WORLD);); (22, MPI, MPI_Type_vector(100, 1, 100, MPI_INT, &row);)"}
{"predict": "(25, MPI, MPI_Waitall(nprocs, sendreqs, sendstats);); (22, MPI, MPI_Alltoall(sendbuf, BUF_COUNT, MPI_INT, recvbuf, BUF_COUNT, MPI_INT, MPI_COMM_WORLD);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (40, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (42, MPI, MPI_Waitall(nprocs, sendreqs, sendstats);); (13, MPI, MPI_Barrier(MPI_COMM_WORLD);); (21, MPI, MPI_Isend(sendbuf, BUF_COUNT, MPI_INT, i, 0, MPI_COMM_WORLD, &sendreqs[i]);)"}
{"predict": "(4, MPI, MPI_Init(&argc, &argv);); (206, CUDA, cudaStat = cudaMalloc((void**)&d_info_geqrf, sizeof(int));); (92, CUDA, cudaStat = cudaMalloc((void**)&d_work_geqrf, workspaceInBytesOnDevice_geqrf);); (51, CUDA, cudaStat = cudaSetDevice(localDeviceId);); (205, MPI, MPI_Bcast(d_info_geqrf, 1, MPI_INT, 0, MPI_COMM_WORLD);); (69, CUDA, cudaStat = cudaStreamCreate(&localStream);); (233, CUDA, cudaStat = cudaMalloc((void**)&d_global_tau, sizeof(double) * numColDevices);); (321, CUDA, cudaStat = cudaFree(d_work_geqrf);); (94, CUDA, cudaStat = cudaMalloc((void**)&d_info_geqrf, sizeof(int));); (307, CUDA, cudaStat = cudaFree(d_info_geqrf);); (313, CUDA, cudaStat = cudaFree(d_tau);); (219, CUDA, cudaStat = cudaMalloc((void**)&d_global_Q, sizeof(double) * numRowDevices);); (219, CUDA, cudaStat = cudaMalloc((void**)&d_global_R, sizeof(double) * numRowDevices);); (220, CUDA, cudaStat = cudaMalloc((void**)&d_global_tau, sizeof(double) * numColDevices);); (44, CUDA, cudaStat = cudaSetDevice(localDeviceId);); (302, CUDA, cudaStat = cudaFree(d_info_geqrf);); (317, CUDA, cudaStat = cudaFree(d_global_tau);); (314, CUDA, cudaStat = cudaFree(d_global_R);); (307, CUDA, cudaStat = cudaFree(d_global_Q);); (41, CUDA, cudaStat = cudaMalloc((void**)&d_tau, sizeof(double) * numColDevices);); (292, CUDA, cudaStat = cudaFree(d_work);); (20, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rankId);); (18, MPI, MPI_Comm_size(MPI_COMM_WORLD, &rankSize);); (302, CUDA, cudaStat = cudaFree(d_A);); (13, CUDA, cudaStat = cudaMalloc((void**)&d_A, sizeof(double) * lda * colsA);); (289, CUDA, cudaStat = cudaFree(d_QR);); (290, CUDA,", "label": "(358, CUDA, cudaStat = cudaMemcpy(h_A, d_global_Q, sizeof(double) * lda * colsA, cudaMemcpyDeviceToHost);); (196, CUDA, cudaStat = cudaMalloc((void**)&d_info_geqrf, sizeof(int));); (27, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rankId);); (429, CUDA, cudaStat = cudaFree(d_tau);); (-1, MPI, MPI_Init(NULL, NULL);); (274, CUDA, cudaStat = cudaMemcpyAsync(&h_info_geqrf, d_info_geqrf, sizeof(int), cudaMemcpyDeviceToHost, localStream);); (471, CUDA, cudaStat = cudaStreamDestroy(localStream);); (474, MPI, MPI_Barrier(MPI_COMM_WORLD);); (42, CUDA, cudaStat = cudaSetDevice(localDeviceId);); (43, CUDA, cudaStat = cudaFree(0);); (194, CUDA, cudaStat = cudaMemset(d_info_geqrf, 0, sizeof(int));); (428, CUDA, cudaStat = cudaFree(d_work_geqrf);); (178, CUDA, cudaStat = cudaMalloc((void**)&d_tau, localColsA * sizeof(double));); (301, CUDA, cudaStat = cudaMalloc((void**)&d_global_Q, lda * colsA * sizeof(double));); (213, CUDA, cudaStat = cudaMalloc((void**)&d_work_geqrf, workspaceInBytesOnDevice_geqrf);); (431, CUDA, cudaStat = cudaFree(d_info_geqrf);); (12, MPI, MPI_Comm_size(MPI_COMM_WORLD, &rankSize);); (154, CUDA, cudaStat = cudaMalloc((void**)&d_A, localColsA * LLDA * sizeof(double));); (305, CUDA, cudaStat = cudaMemcpy(d_global_R, d_global_Q, sizeof(double) * lda * colsA, cudaMemcpyDeviceToDevice);); (302, CUDA, cudaStat = cudaMemcpy(d_global_Q, h_A, sizeof(double) * lda * colsA, cudaMemcpyHostToDevice);); (53, CUDA, cudaStat = cudaStreamCreate(&localStream);); (463, MPI, MPI_Finalize();); (403, CUDA, cudaStat = cudaFree(d_A);); (293, CUDA, cudaStat = cudaMalloc((void**)&d_global_R, lda * colsA * sizeof(double));); (294, CUDA, cudaStat = cudaMalloc((void**)&d_global_tau, colsA * sizeof(double));)"}
{"predict": "(4, CUDA, __shared__ T sdata[MultiWarpGroupSize];)", "label": "(4, CUDA, __shared__ cg::block_tile_memory<BlockSize> scratch;)"}
{"predict": "(60, CUDA, cudaFree( deviceInt );); (60, CUDA, cudaFreeHost( hostInt );)", "label": "(59, CUDA, cudaFree( deviceInt );); (59, CUDA, cudaFreeHost( hostInt );)"}
{"predict": "(100, CUDA, cudaFree( dptrPosMass[i] );); (100, CUDA, cudaFree( dptrForce[i] );); (25, CUDA, cudaSetDevice( i );)", "label": "(98, CUDA, cudaFree( dptrPosMass[i] );); (100, CUDA, cudaSetDevice( oldDevice );); (97, CUDA, cudaFree( dptrForce[i] );)"}
{"predict": "(9, OpenCL, clReleaseMemObject(objiAmax);); (4, OpenCL, clReleaseMemObject(objX);); (11, OpenCL, clReleaseMemObject(objScratch);)", "label": "(5, OpenCL, clReleaseMemObject(objX);); (12, OpenCL, clReleaseMemObject(objScratch);); (7, OpenCL, clReleaseMemObject(objiAmax);)"}
{"predict": "(3, TBB, tbb::concurrent_queue<T> q;); (17, TBB, tbb::concurrent_bounded_queue<T> q;)", "label": "(19, TBB, REMARK(\"Wraparound on tbb::concurrent_bounded_queue...\");); (2, TBB, tbb::concurrent_queue<T>* q = new tbb::concurrent_queue<T>;); (16, TBB, tbb::concurrent_bounded_queue<T>* q = new tbb::concurrent_bounded_queue<T>;)"}
{"predict": "(23, OpenMP, #pragma omp parallel); (9, OpenMP, #pragma omp parallel nevents(4)); (25, OpenMP, #pragma omp single)", "label": "(25, OpenMP, printf(\"Thread %d sleeps now for %d seconds\\n\", omp_get_thread_num(), SLEEPTIME);); (8, OpenMP, #pragma omp parallel); (31, OpenMP, printf(\"Thread %d wakes up again\\n\", omp_get_thread_num());)"}
{"predict": "(47, SYCL, GEP->setMetadata(hipsycl::compiler::MDKind::Arrayified, MDAlloca);); (48, SYCL, Alloca->setMetadata(hipsycl::compiler::MDKind::Arrayified, MDAlloca);)", "label": "(45, SYCL, GEP->setMetadata(hipsycl::compiler::MDKind::Arrayified, MDAlloca);); (36, SYCL, Alloca->setMetadata(hipsycl::compiler::MDKind::Arrayified, MDAlloca);)"}
{"predict": "(70, OpenMP, #pragma omp parallel master taskloop simd safelen(5)); (45, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argv[1])); (59, OpenMP, #pragma omp parallel master taskloop simd firstprivate(da)); (10, OpenMP, #pragma omp parallel master taskloop simd firstprivate); (3, OpenMP, #pragma omp parallel); (13, OpenMP, #pragma omp parallel master taskloop simd firstprivate()); (45, OpenMP, #pragma omp parallel master taskloop simd firstprivate(2 * 2)); (92, OpenMP, #pragma omp parallel master taskloop simd private(xa), firstprivate(xa)); (57, OpenMP, #pragma omp parallel master taskloop simd firstprivate(xa)); (77, OpenMP, #pragma omp parallel master taskloop simd firstprivate(h, B::x)); (1, OpenMP, #pragma omp parallel master taskloop simd firstprivate(); (28, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argc)); (13, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argc); (16, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argc,); (43, OpenMP, #pragma omp parallel master taskloop simd firstprivate(ba)); (96, OpenMP, #pragma omp parallel master taskloop simd lastprivate(g) firstprivate(g)); (67, OpenMP, #pragma omp parallel master taskloop simd firstprivate(m)); (73, OpenMP, #pragma omp parallel master taskloop simd firstprivate(e, g)); (0, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argc); (58, OpenMP, #pragma omp parallel master taskloop simd firstprivate(S2::S2s)); (82, OpenMP, #pragma omp parallel master taskloop simd firstprivate(j)); (43, OpenMP, #pragma omp parallel master taskloop simd firstprivate(S2::S2sc)); (93, OpenMP, #pragma omp parallel master taskloop simd firstprivate(n) lastprivate(n)); (17, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argc > 0? argv[1] : argv[2])); (20, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default", "label": "(22, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argc); (78, OpenMP, #pragma omp parallel master taskloop simd safelen(5)); (81, OpenMP, #pragma omp parallel master taskloop simd firstprivate(e, g)); (125, OpenMP, #pragma omp parallel private(i)); (87, OpenMP, #pragma omp parallel master taskloop simd firstprivate(h)); (53, OpenMP, #pragma omp parallel master taskloop simd firstprivate(ca)); (48, OpenMP, #pragma omp parallel master taskloop simd firstprivate(ba)); (27, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (38, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argv[1])); (90, OpenMP, #pragma omp parallel master taskloop simd firstprivate(i)); (-1, OpenMP, #pragma omp parallel); (64, OpenMP, #pragma omp parallel master taskloop simd firstprivate(S2::S2sc)); (128, OpenMP, #pragma omp parallel master taskloop simd firstprivate(B::x)); (119, OpenMP, #pragma omp parallel reduction(+ : i)); (88, OpenMP, #pragma omp parallel shared(xa)); (23, OpenMP, #pragma omp parallel master taskloop simd firstprivate(S1)); (14, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argc > 0? argv[1] : argv[2])); (78, OpenMP, #pragma omp parallel master taskloop simd private(xa), firstprivate(xa)); (53, OpenMP, #pragma omp parallel master taskloop simd firstprivate(S2::S2s)); (125, OpenMP, #pragma omp parallel master taskloop simd firstprivate(si)); (-6, OpenMP, #pragma omp parallel master taskloop simd firstprivate(); (-3, OpenMP, #pragma omp parallel master taskloop simd firstprivate()); (65, OpenMP, #pragma omp parallel master taskloop simd firstprivate(m)); (84, OpenMP, #pragma omp parallel master taskloop simd firstprivate(j)); (18, OpenMP, #pragma omp parallel master taskloop simd firstprivate(a, b, c, d, f)); (86, OpenMP, #pragma omp parallel master taskloop simd lastprivate(g) firstprivate(g)); (89, OpenMP, #pragma omp parallel master taskloop simd lastprivate(n) firstprivate(n)); (-1, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argc,); (39, OpenMP, #pragma omp parallel master taskloop simd firstprivate(xa)); (21, OpenMP, #pragma omp parallel master taskloop simd firstprivate(2 * 2)); (32, OpenMP, #pragma omp parallel master taskloop simd firstprivate(da)); (-21, OpenMP, #pragma omp parallel master taskloop simd firstprivate)"}
{"predict": "(10, OpenMP, #pragma omp parallel for private(i) schedule(static,1)); (12, OpenMP, #pragma omp flush)", "label": "(14, OpenMP, #pragma omp flush); (9, OpenMP, #pragma omp parallel for reduction(+:sum) schedule(static,1) private(i) private(i2))"}
{"predict": "(23, MPI, MPI_Recv(out, count, type, set_src? left : right, tag, comm, MPI_STATUS_IGNORE);); (17, MPI, MPI_Isend(in, count, type, set_src? right : left, tag, comm, &requests[w]);); (22, MPI, MPI_Send(&ack, 1, MPI_CHAR, set_src? right : left, tag, comm);); (18, MPI, MPI_Waitall(window_size, requests, MPI_STATUSES_IGNORE);); (15, MPI, MPI_Recv(&ack, 1, MPI_CHAR, set_src? left : right, tag, comm, MPI_STATUS_IGNORE);)", "label": "(21, MPI, MPI_Recv(&ack, 1, MPI_CHAR, right, tag, comm, MPI_STATUS_IGNORE);); (23, MPI, MPI_Irecv(out, count, type, set_src? left : MPI_ANY_SOURCE, tag, comm, &requests[w]);); (18, MPI, MPI_Waitall(window_size, requests, MPI_STATUSES_IGNORE);); (15, MPI, MPI_Isend(in, count, type, right, (tag == MPI_ANY_TAG? 0 : tag), comm, &requests[w]);); (23, MPI, MPI_Send(&ack, 1, MPI_CHAR, left, (tag == MPI_ANY_TAG? 0 : tag), comm);)"}
{"predict": "(3, OpenMP, comp_parse();)", "label": "(34, OpenMP, comp_do_parse(cur, tail);)"}
{"predict": "(3, OpenMP, #pragma omp target device(global_device)); (13, OpenMP, #pragma omp target device(0))", "label": "(3, OpenMP, #pragma omp target device(0)); (13, OpenMP, #pragma omp target device(1))"}
{"predict": "(16, OpenMP, #pragma omp taskwait); (2, OpenMP, #pragma omp parallel num_threads(2) shared(var, a)); (5, OpenMP, #pragma omp task shared(var, a)); (1, OpenMP, #pragma omp master)", "label": "(4, OpenMP, #pragma omp master); (5, OpenMP, #pragma omp taskgroup); (6, OpenMP, #pragma omp task shared(var, a)); (0, OpenMP, #pragma omp parallel num_threads(2) shared(var, a))"}
{"predict": "(3, MPI, MPI_Comm_size(comm, &size);); (3, MPI, MPI_Comm_rank(comm, &rank);); (13, MPI, MPI_Alltoallv(data_out, counts, displs, MPI_INT, data_in, counts, displs, MPI_INT, comm);)", "label": "(3, MPI, MPI_Comm_size(comm, &size);); (3, MPI, MPI_Comm_rank(comm, &rank);); (13, MPI, MPI_Alltoallv(data_out, counts, displs, MPI_INT, data_in, counts, displs, MPI_INT, comm);)"}
{"predict": "(20, MPI, MPI_Error_string(error, err_string, &length);); (7, MPI, MPI_Init(&argc, &argv);); (28, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (50, MPI, error = MPI_Barrier(MPI_COMM_WORLD);); (59, MPI, error = MPI_Finalize();); (6, MPI, error = MPI_Finalized(&flag);); (16, MPI, MPI_Error_string(error, err_string, &length);); (18, MPI, MPI_Abort(MPI_COMM_WORLD, error);)", "label": "(11, MPI, MPI_Error_string(error, err_string, &length);); (20, MPI, error = MPI_Init(&argc, &argv);); (27, MPI, error = MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (60, MPI, error = MPI_Finalize();); (50, MPI, error = MPI_Barrier(MPI_COMM_WORLD);); (29, MPI, MPI_Abort(MPI_COMM_WORLD, error);); (3, MPI, error = MPI_Finalized(&flag);)"}
{"predict": "(13, CUDA, cudaMalloc(&dev_B, size_b);); (30, CUDA, cudaFree(dev_B);); (18, CUDA, cudaDeviceSynchronize();); (27, CUDA, cudaFree(dev_A);); (7, CUDA, cudaMalloc(&dev_A, size_a);)", "label": "(13, CUDA, cudaMalloc(&dev_B, size_b);); (13, CUDA, cudaDeviceSynchronize();); (10, CUDA, cudaMalloc(&dev_A, size_a);); (27, CUDA, cudaFree(dev_B);); (25, CUDA, cudaFree(dev_A);)"}
{"predict": "(123, OpenMP, #pragma omp target teams map(r, t, s, u, r, t, s, u, n, m, p, q, r, t, s, u, 0, n, m, p, q, r, t, s, u, -1, -2, n, m, p, q, r, t, s, u, n, m, p, q, r, t, s, u, 123, OpenMP, #pragma omp target teams map(r, t, s, u, r, t, s, u, n, m, p, q, r, t, s, u, 0, n, m, p, q, r, t, s, u, -1, -2, n, m, p, q, r, t, s, u, n, m, p, q, r, t, s, u, 102, OpenMP, #pragma omp target teams map(r, t, s, u, r, t, s, u, n, m, p, q, r, t, s, u, 0, n, m, p, q, r, t, s, u, -1, -2, n, m, p, q, r, t, s, u, n, m, p, q, r, t, s, u, 123, OpenMP, #pragma omp target teams map(r, t, s, u, r, t, s, u, n, m, p, q, r, t, s, u, 0, n, m, p, q, r, t, s, u, (-1, -(139, OpenMP, #pragma omp target teams map(r, t, s, u, r, t, s, u, n, m, p, q, r, t, s, u, 0, n, m, p, q, r, t, s, u, (-1, -2, n, m, p, q, r, t, s, u, n, m, p, q, r, t, s, u, 139, OpenMP, #pragma omp target teams map(r, t, s, u, r, t, s, u, n, m, p, q, r, t, s", "label": "(35, OpenMP, #pragma omp target teams map(marr[:arg][n:])); (166, OpenMP, #pragma omp target teams map(r.S.Arr[4:5])); (53, OpenMP, #pragma omp target teams map(marr[:arg][:2][:])); (120, OpenMP, #pragma omp target teams map(r.PtrS[0], r.PtrS->B)); (117, OpenMP, #pragma omp target teams map(r.ArrS[0].A, t.ArrS[1].A)); (136, OpenMP, #pragma omp target teams map(r.C, r.D)); (9, OpenMP, #pragma omp target teams map(r)); (80, OpenMP, #pragma omp target teams map(mvla2[:1][:2][:10])); (123, OpenMP, #pragma omp target teams map(r.RPtrS, r.RPtrS->B)); (136, OpenMP, #pragma omp target teams map(r.C) map(r.C)); (21, OpenMP, #pragma omp target teams map(marr[arg:][:arg][:])); (189, OpenMP, #pragma omp target data map(to: t)); (157, OpenMP, #pragma omp target teams map(r.S.Ptr[4:5])); (54, OpenMP, #pragma omp target teams map(marr[:2][2+2-4:][0:5+5])); (35, OpenMP, #pragma omp target teams map(marr[:2][:][1:])); (12, OpenMP, #pragma omp target teams map(marr[arg:][:][:])); (93, OpenMP, #pragma omp target teams map(r.ArrS[:arg].B)); (0, OpenMP, #pragma omp target teams map(marr[2][0:2][0:2])); (80, OpenMP, #pragma omp target teams map(mptr[:1][:2][0:2])); (53, OpenMP, #pragma omp target teams map(marr2[:1][:2][0])); (65, OpenMP, #pragma omp target teams map(mvla2[:1][:2][:11])); (86, OpenMP, #pragma omp target teams map(r.ArrS[:1].B)); (35, OpenMP, #pragma omp target teams map(marr[:1][3:1][:2])); (6, OpenMP, #pragma omp target teams map(marr[arg:])); (29, OpenMP, #pragma omp target teams map(marr[:1][0][:])); (134, OpenMP, #pragma omp target teams map(r.Ptr[3:5])); (74, OpenMP, #pragma omp target teams map(mptr[:1][:][0:2])); (108, OpenMP, #pragma omp target teams map(r.S.Arr[:12])); (125, OpenMP, #pragma omp target teams map(r.A)); (90, OpenMP, #pragma omp target teams map(r.ArrS[0].A, r.ArrS[1].A)); (95, OpenMP, #pragma omp target teams map(r.PtrS, r.PtrS->B)); (164, OpenMP, #pragma omp target teams map(t.D)); (149, OpenMP, #pragma omp target teams map(r.D)); (96, OpenMP, #pragma omp target teams map(r.RPtrS[0], r.RPtrS->B)); (77, OpenMP, #pragma omp target teams map(r.ArrS[0].Arr[1:23])); (98, OpenMP, #pragma omp target teams map(r.RPtrS->A, r.RPtrS->B)); (45, OpenMP, #pragma omp target teams map(mvla[1][2:arg][:])); (142, OpenMP, #pragma omp target data map(to: r.C)); (21, OpenMP, #pragma omp target teams map(marr[:1][3:arg][:2])); (8, OpenMP, #pragma omp target teams map(marr[:2][:][:1])); (37, OpenMP, #pragma omp target teams map(mvla[:2][:arg][:])); (61, OpenMP, #pragma omp target teams map(mptr[:2][:1][0:2])); (71, OpenMP, #pragma omp target teams map(r.ArrS[0].Arr[1:arg])); (142, OpenMP, #pragma omp target data map(to: t.Ptr)); (31, OpenMP, #pragma omp target teams map(mvla[:1][:][0])); (112, OpenMP, #pragma omp target teams map(r.Arr[3:5])); (46, OpenMP, #pragma omp target teams map(mptr[:1][:2-1][2:4-3])); (108, OpenMP, #pragma omp target teams map(r.Arr)); (123, OpenMP, #pragma omp target teams map(r.S.Ptr[:])); (21, OpenMP, #pragma omp target teams map(marr[:1][:2][0])); (-7, OpenMP, #pragma omp target teams map(marr[:2][:1][:])); (125, OpenMP, #pragma omp target teams map(u.B)); (111, OpenMP, #pragma omp target teams map(r.ArrS[3:5].Arr[6:7])); (-16, OpenMP, #pragma omp target teams map(marr[:][:arg][n:])); (97, OpenMP, #pragma omp target teams map(r.C, t.C)); (62, OpenMP, #pragma omp target teams map(r.ArrS[0].Error)); (-15, OpenMP, #pragma omp target teams map(marr[n:m][:arg][n:])); (90, OpenMP, #pragma omp target teams map(r.C, r.S)); (135, OpenMP, #pragma omp target data map"}
{"predict": "(12, OpenMP, #pragma omp target teams); (14, OpenMP, #pragma omp target teams distribute parallel for)", "label": "(12, OpenMP, #pragma omp target map(tofrom: C) map(to: tt, tt.p1[:10]))"}
{"predict": "(6, CUDA, cudaFree(cusolverData.cusolverInfo);); (6, CUDA, cudaFree(cusolverData.cusolverParams);); (3, CUDA, cudaFree(cusolverData.cusolverData);); (0, CUDA, cudaFree(cusolverData.hue);); (1, CUDA, cudaFree(cusolverData.saturation);); (2, CUDA, cudaFree(cusolverData.cusolverBuffer);)", "label": "(7, CUDA, cudaFree(bufferOffset.W);); (3, CUDA, cudaFree(bufferOffset.srcShiftSum);); (1, CUDA, cudaFree(bufferOffset.dstMean);); (10, CUDA, cudaFree(cusolverData.cusolverBuffer);); (5, CUDA, cudaFree(bufferOffset.J);); (3, CUDA, cudaFree(bufferOffset.r);); (4, CUDA, cudaFree(bufferOffset.calc_buffer);); (-1, CUDA, cudaFree(bufferOffset.LtL);); (-6, CUDA, cudaFree(bufferOffset.srcMean);); (-4, CUDA, cudaFree(bufferOffset.dstShiftSum);); (4, CUDA, cudaFree(cusolverData.cusolverInfo);)"}
{"predict": "(71, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&output, flat_fft_size_bytes));); (71, CUDA, CUDA_CHECK_AND_EXIT(cudaMemset(output, 0b11111111, flat_fft_size_bytes));); (76, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamCreate(&stream));); (71, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (76, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(cufftdx_results.input, input_host.data(), flat_fft_size_bytes, cudaMemcpyHostToDevice));); (69, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(input, input_host.data(), flat_fft_size_bytes, cudaMemcpyHostToDevice));); (64, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&input, flat_fft_size_bytes));); (85, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamDestroy(stream));); (88, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(output));); (84, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(input));); (58, CUDA, CUDA_CHECK_AND_EXIT(cudaMemset(input, 0b11111111, flat_fft_size_bytes));)", "label": "(77, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamCreate(&stream));); (71, CUDA, CUDA_CHECK_AND_EXIT(cudaMemset(output, 0b11111111, flat_fft_size_bytes));); (71, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(input, input_host.data(), flat_fft_size_bytes, cudaMemcpyHostToDevice));); (91, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(output));); (70, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (88, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(input));); (62, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&input, flat_fft_size_bytes));); (83, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamDestroy(stream));); (61, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&output, flat_fft_size_bytes));)"}
{"predict": "(107, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 1, NULL, threads, localThreads, 0, NULL, NULL );); (17, OpenCL, error = clGetDeviceInfo( device, CL_DEVICE_VERSION, sizeof( kernelSource ), &kernelSource, NULL );); (23, OpenCL, error = clGetDeviceInfo( device, CL_DEVICE_SINGLE_FP_CONFIG, sizeof( config ), &config, NULL );); (94, OpenCL, error = clSetKernelArg(kernel, i, sizeof( streams[i] ), &streams[i]);); (108, OpenCL, error = clEnqueueReadBuffer( queue, streams[2], true, 0, sizeof( cl_float ) * TEST_SIZE, outData, 0, NULL, NULL );)", "label": "(108, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 1, NULL, threads, localThreads, 0, NULL, NULL );); (22, OpenCL, error = clGetDeviceInfo( device, CL_DEVICE_SINGLE_FP_CONFIG, sizeof( config ), &config, NULL );); (16, OpenCL, error = clGetDeviceInfo( device, CL_DEVICE_PROFILE, sizeof( kernelSource ), kernelSource, NULL );); (95, OpenCL, error = clSetKernelArg(kernel, (int)i, sizeof( streams[i] ), &streams[i]);); (108, OpenCL, error = clEnqueueReadBuffer( queue, streams[2], true, 0, sizeof( cl_float ) * TEST_SIZE, outData, 0, NULL, NULL );)"}
{"predict": "(12, OpenMP, #pragma omp parallel for schedule(static,1)); (12, OpenMP, #pragma omp task)", "label": "(11, OpenMP, #pragma omp parallel for schedule(static,1)); (12, OpenMP, #pragma omp task)"}
{"predict": "(10, CUDA, cudaMallocManaged((void **)&h_data, sizeof(SiftPoint)*data.maxPts);)", "label": "(8, CUDA, safeCall(cudaMemcpy(h_data, data.d_data, sizeof(SiftPoint)*data.numPts, cudaMemcpyDeviceToHost));)"}
{"predict": "(15, OpenMP, #pragma omp target data map(m[:n])); (16, OpenMP, #pragma omp target exit data map(from: a[:n]))", "label": "(12, TBB, tbb::mutex *m = new tbb::mutex;)"}
{"predict": "(7, OpenMP, #pragma omp parallel for schedule(dynamic) default(none) copyin(g) linear(a) allocate(a) lastprivate(conditional: d, e, f) order(concurrent)); (11, OpenMP, #pragma omp parallel for allocate(argc) private(argc, b), firstprivate(c, d), lastprivate(d, f) collapse(N) schedule(static, N) ordered(N) if (parallel :argc) num_threads(N) default(shared) shared(e) reduction(+ : h)); (13, OpenMP, #pragma omp parallel for lastprivate(conditional: g) order(concurrent))", "label": "(7, OpenMP, #pragma omp parallel for schedule(dynamic) default(none) copyin(g) linear(a) allocate(a) lastprivate(conditional: d, e,f) order(concurrent)); (5, OpenMP, #pragma omp threadprivate(g)); (11, OpenMP, #pragma omp parallel for allocate(argc) private(argc, b), firstprivate(c, d), lastprivate(d, f) collapse(N) schedule(static, N) ordered(N) if (parallel :argc) num_threads(N) default(shared) shared(e) reduction(+ : h))"}
{"predict": "(19, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(19, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(35, TBB, ASSERT(default_ss == tbb::global_control::active_value(tbb::global_control::thread_stack_size), NULL);); (4, TBB, tbb::task_scheduler_init t(h_c);); (1, TBB, harness_graph_multifunction_executor<tbb::task_scheduler_init>::execute_count = 0;); (18, TBB, size_t default_ss = tbb::global_control::active_value(tbb::global_control::thread_stack_size);); (3, TBB, ASSERT(h_c == tbb::global_control::active_value(tbb::global_control::hardware_concurrency), NULL);)", "label": "(31, TBB, ASSERT(default_ss == tbb::global_control::active_value(tbb::global_control::thread_stack_size), NULL);); (3, TBB, const unsigned h_c = tbb::tbb_thread::hardware_concurrency();); (5, TBB, tbb::task_scheduler_init t(h_c+1);); (20, TBB, size_t default_ss = tbb::global_control::active_value(tbb::global_control::thread_stack_size);)"}
{"predict": "(149, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(cl_mem), &array);); (170, OpenCL, clReleaseMemObject(array);)", "label": "(558, OpenCL, clReleaseKernel(kernel);); (558, OpenCL, clReleaseProgram(program);); (555, OpenCL, clReleaseMemObject(array);)"}
{"predict": "(35, CUDA, HANDLE_ERROR(cudaMemcpyAsync(s_d, s_cpu.data(), sizeof(float) * DYN::STATE_DIM, cudaMemcpyHostToDevice, s1));); (35, CUDA, HANDLE_ERROR(cudaMemcpyAsync(u_d, u.data(), sizeof(float) * DYN::CONTROL_DIM, cudaMemcpyHostToDevice, s1));); (35, CUDA, HANDLE_ERROR(cudaMemcpyAsync(state_deriv_GPU, state_deriv_cpu.data(), sizeof(float) * DYN::STATE_DIM, cudaMemcpyHostToDevice, s1));); (28, CUDA, HANDLE_ERROR(cudaMalloc((void**)&state_deriv_GPU, sizeof(float) * DYN::STATE_DIM));); (52, CUDA, cudaFree(state_deriv_GPU);); (29, CUDA, HANDLE_ERROR(cudaMalloc((void**)&s_d, sizeof(float) * DYN::STATE_DIM));); (25, CUDA, HANDLE_ERROR(cudaMalloc((void**)&u_d, sizeof(float) * DYN::CONTROL_DIM));); (49, CUDA, cudaFree(s_d);); (50, CUDA, cudaFree(u_d);); (35, CUDA, HANDLE_ERROR(cudaStreamSynchronize(s1));); (48, CUDA, cudaFree(state_deriv_GPU);); (23, CUDA, HANDLE_ERROR(cudaStreamCreate(&s1));)", "label": "(34, CUDA, HANDLE_ERROR(cudaMalloc((void**)&state_deriv_GPU, sizeof(float) * DYN::STATE_DIM));); (37, CUDA, HANDLE_ERROR(cudaMemcpyAsync(s_d, s_gpu.data(), sizeof(float) * DYN::STATE_DIM, cudaMemcpyHostToDevice, s1));); (46, CUDA, CudaCheckError();); (34, CUDA, HANDLE_ERROR(cudaMemcpyAsync(u_d, u.data(), sizeof(float) * DYN::CONTROL_DIM, cudaMemcpyHostToDevice, s1));); (29, CUDA, HANDLE_ERROR(cudaMalloc((void**)&s_d, sizeof(float) * DYN::STATE_DIM));); (27, CUDA, HANDLE_ERROR(cudaMalloc((void**)&u_d, sizeof(float) * DYN::CONTROL_DIM));); (45, CUDA, HANDLE_ERROR(cudaMemcpyAsync(s_gpu.data(), s_d, sizeof(float) * DYN::STATE_DIM, cudaMemcpyDeviceToHost, s1));); (32, CUDA, HANDLE_ERROR(cudaStreamSynchronize(s1));); (19, CUDA, cudaStreamCreate(&s1);)"}
{"predict": "(69, CUDA, cudaStream_t stream = at::cuda::getCurrentCUDAStream();)", "label": "(69, CUDA, cudaStream_t stream = at::cuda::getCurrentCUDAStream();)"}
{"predict": "(23, MPI, MPI_Allreduce( &s, &berr, 1, MPI_FLOAT, MPI_MAX, grid->comm );)", "label": "(24, MPI, MPI_Allreduce( &s, &berr, 1, MPI_FLOAT, MPI_MAX, grid->comm );)"}
{"predict": "(41, CUDA, cudaDeviceSynchronize();); (7, CUDA, cudaSetDevice(0);); (15, CUDA, cudaMalloc(&dev_cgrid.data().get(),psize*sizeof(float));); (15, CUDA, cudaMalloc(&dev_cpgrid.data().get(),psize*sizeof(float));); (38, CUDA, cudaProfilerStart();); (39, CUDA, cudaProfilerStop();); (15, CUDA, cudaMalloc(&states.data().get(),psize*sizeof(curandState));); (9, CUDA, cudaMalloc(&cgrid.data().get(),psize*sizeof(float));); (13, CUDA, cudaMalloc(&d2::curandState.data().get(),psize*sizeof(curandState));); (8, CUDA, cudaMalloc(&b1::x.data().get(),psize*sizeof(float));); (8, CUDA, cudaMalloc(&b1::y.data().get(),psize*sizeof(float));); (9, CUDA, cudaMalloc(&b2::x.data().get(),psize*sizeof(float));); (10, CUDA, cudaMalloc(&b2::y.data().get(),psize*sizeof(float));); (6, CUDA, cudaMalloc(&a1::x.data().get(),psize*sizeof(float));); (13, CUDA, cudaMalloc(&d1::curandState.data().get(),psize*sizeof(curandState));); (6, CUDA, cudaMalloc(&a1::y.data().get(),psize*sizeof(float));); (8, CUDA, cudaMalloc(&d2::curandState.data().get(),psize*sizeof(curandState));); (4, CUDA, cudaMalloc(&a2::x.data().get(),psize*sizeof(float));); (7, CUDA, cudaMalloc(&d1::curandState.data().get(),psize*sizeof(curandState));); (4, CUDA, cudaMalloc(&a2::y.data().get(),psize*sizeof(float));); (6, CUDA, cudaMalloc(&b1::z.data().get(),psize*sizeof(float));); (11, CUDA, cudaMalloc(&d2::curandState.data().get(),psize*sizeof(curandState));); (6, CUDA, cudaMalloc(&b2::z.data().get(),psize*sizeof(float));); (3, CUDA, cudaMalloc(&b1::x.data().get(),psize*sizeof(float));); (10, CUDA, cudaMalloc(&d1::curandState", "label": "(34, CUDA, checkCudaErrors(cudaDeviceSynchronize());)"}
{"predict": "(19, CUDA, checkRuntime(cudaStreamCreate(&stream));); (80, CUDA, checkRuntime(cudaStreamSynchronize(stream));); (58, CUDA, checkRuntime(cudaMemcpyAsync(input_data_device, input_data_host, input_numel * sizeof(float), cudaMemcpyHostToDevice, stream));); (25, CUDA, checkRuntime(cudaMallocHost(&input_data_host, input_numel * sizeof(float)));); (25, CUDA, checkRuntime(cudaMalloc(&input_data_device, input_numel * sizeof(float)));); (72, CUDA, checkRuntime(cudaMalloc(&output_data_device, sizeof(float) * output_numel));); (-2, CUDA, auto engine = make_nvshared(runtime->deserializeCudaEngine(engine_data.data(), engine_data.size()));); (69, CUDA, checkRuntime(cudaMallocHost(&output_data_host, sizeof(float) * output_numel));); (1, CUDA, checkRuntime(cudaSetDevice(0));); (82, CUDA, checkRuntime(cudaStreamDestroy(stream));)", "label": "(18, CUDA, checkRuntime(cudaStreamCreate(&stream));); (79, CUDA, checkRuntime(cudaMemcpyAsync(output_data_host, output_data_device, sizeof(float) * output_numel, cudaMemcpyDeviceToHost, stream));); (79, CUDA, checkRuntime(cudaStreamSynchronize(stream));); (57, CUDA, checkRuntime(cudaMemcpyAsync(input_data_device, input_data_host, input_numel * sizeof(float), cudaMemcpyHostToDevice, stream));); (24, CUDA, checkRuntime(cudaMallocHost(&input_data_host, input_numel * sizeof(float)));); (24, CUDA, checkRuntime(cudaMalloc(&input_data_device, input_numel * sizeof(float)));); (89, CUDA, checkRuntime(cudaFree(input_data_device));); (86, CUDA, checkRuntime(cudaFreeHost(input_data_host));); (88, CUDA, checkRuntime(cudaFree(output_data_device));); (62, CUDA, checkRuntime(cudaMalloc(&output_data_device, sizeof(float) * output_numel));); (-5, CUDA, auto engine = make_nvshared(runtime->deserializeCudaEngine(engine_data.data(), engine_data.size()));); (59, CUDA, checkRuntime(cudaMallocHost(&output_data_host, sizeof(float) * output_numel));); (82, CUDA, checkRuntime(cudaFreeHost(output_data_host));); (79, CUDA, checkRuntime(cudaStreamDestroy(stream));)"}
{"predict": "(20, OpenMP, #pragma omp task unknown()); (54, OpenMP, #pragma omp task default(none)); (141, OpenMP, #pragma omp task firstprivate(r)); (62, OpenMP, #pragma omp task default(shared)); (54, OpenMP, #pragma omp task default(shared)); (6, OpenMP, #pragma omp task (); (72, OpenMP, #pragma omp parallel shared(sa)); (102, OpenMP, #pragma omp task default(shared) firstprivate(r)); (59, OpenMP, #pragma omp task default(none) firstprivate(a)); (77, OpenMP, #pragma omp parallel); (3, OpenMP, #pragma omp task ()); (92, OpenMP, #pragma omp parallel reduction(+ : r)); (80, OpenMP, #pragma omp parallel shared(b, r)); (45, OpenMP, #pragma omp task default(none) shared(a)); (4, OpenMP, #pragma omp task (default); (10, OpenMP, #pragma omp task (default none); (54, OpenMP, #pragma omp task default(none) shared(b, r)); (83, OpenMP, #pragma omp task default(shared) private(r)); (91, OpenMP, #pragma omp task default(shared) firstprivate(sb)); (13, OpenMP, #pragma omp task default(shared)); (17, OpenMP, #pragma omp task default(x)); (62, OpenMP, #pragma omp task default(none) shared(a, b, r)); (36, OpenMP, #pragma omp task default(shared) default(shared)); (38, OpenMP, #pragma omp task default(none)); (-1, OpenMP, #pragma omp task (shared); (75, OpenMP, #pragma omp task default(firstprivate) shared(r)); (21, OpenMP, #pragma omp task default(none)); (53, OpenMP, #pragma omp task default(none) shared(r)); (60, OpenMP, #pragma omp task default(none) firstprivate(b, r)); (72, OpenMP, #pragma omp task default(shared) firstprivate(sa)); (-10, OpenMP, #pragma omp task); (17, OpenMP, #pragma omp task default(shared) default(none)); (58, OpenMP, #pragma omp task default(shared) firstprivate(da))", "label": "(174, OpenMP, #pragma omp task mergeable detach(evt)); (19, OpenMP, #pragma omp task unknown()); (14, OpenMP, #pragma omp task }); (154, OpenMP, #pragma omp task untied untied); (106, OpenMP, #pragma omp parallel shared(sa, sb)); (125, OpenMP, #pragma omp for reduction(+ : r)); (109, OpenMP, #pragma omp task firstprivate(r)); (160, OpenMP, #pragma omp task detach(); (-2, OpenMP, #pragma omp task); (104, OpenMP, #pragma omp parallel reduction(+ : r)); (160, OpenMP, #pragma omp task detach(evt) detach(evt)); (161, OpenMP, #pragma omp task detach(evt) mergeable); (60, OpenMP, #pragma omp parallel); (162, OpenMP, #pragma omp task detach(-evt)); (-6, OpenMP, #pragma omp task(); (48, OpenMP, #pragma omp task default(shared)); (-6, OpenMP, #pragma omp task[); (152, OpenMP, #pragma omp task detach(a)); (-6, OpenMP, #pragma omp task]); (58, OpenMP, #pragma omp parallel shared(a, b)); (157, OpenMP, #pragma omp task detach(evt) shared(evt)); (-7, OpenMP, #pragma omp task)); (144, OpenMP, #pragma omp task detach); (148, OpenMP, #pragma omp task detach(cevt) detach(revt)); (136, OpenMP, #pragma omp task mergeable mergeable); (153, OpenMP, #pragma omp task detach(evt) firstprivate(evt)); (19, OpenMP, #pragma omp task default(none)); (141, OpenMP, #pragma omp task detach()); (66, OpenMP, #pragma omp parallel shared(sa))"}
{"predict": "(7, MPI, MPI_Probe(buf, 10, MPI_INT, tag, MPI_ANY_TAG, MPI_COMM_WORLD, &r);)", "label": "(8, MPI, MPI_Start(&r);); (11, MPI, MPI_Send_init(buf, 10, MPI_INT, MPI_PROC_NULL, tag, MPI_COMM_WORLD, &r);); (7, MPI, MPI_Wait(&r, &s);); (4, MPI, MPI_Recv_init(buf, 10, MPI_INT, MPI_PROC_NULL, tag, MPI_COMM_WORLD, &r);); (6, MPI, MPI_Request_free(&r);)"}
{"predict": "(17, CUDA, CUDA_CHECK(cudaGetLastError());)", "label": "(16, CUDA, CUDA_CHECK(cudaGetLastError());)"}
{"predict": "(36, MPI, MPI_Get_version(mpiVersion, &resultLen);); (128, MPI, MPI_Initialized(&isMpiInited);); (129, MPI, MPI_Comm_size(MPI_COMM_WORLD, &np);); (145, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (123, MPI, MPI_Init_thread(argc, (char***)argv, MPI_THREAD_MULTIPLE, &provided);)", "label": "(35, MPI, MPI_Get_library_version(mpiVersion, &resultLen);); (132, MPI, ret = MPI_Init_thread(argc, argv, MPI_THREAD_MULTIPLE, &provided);); (146, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (126, MPI, MPI_Initialized(&isMpiInited);); (0, MPI, MPI_Finalized(&isMpiFinalized);)"}
{"predict": "(8, OpenMP, #pragma omp target enter data map(to : a) if (p)); (1, OpenMP, #pragma omp target enter data map(to : a) if (true)); (6, OpenMP, #pragma omp target enter data map(to : a) if (p > 0? 1 : 2)); (11, OpenMP, #pragma omp target enter data map(to : a) if(target enter data : true)); (12, OpenMP, #pragma omp target enter data map(to : a) if(target enter data : -1)); (0, OpenMP, #pragma omp target enter data map(to : a) if (p))); (-3, OpenMP, #pragma omp target enter data map(to : a) if ()); (4, OpenMP, #pragma omp target enter data map(to : a) if (S1)); (7, OpenMP, #pragma omp target enter data map(to : a) if(target enter data : 1))", "label": "(14, OpenMP, #pragma omp parallel reduction(+ : incomplete, ([10])p)); (1, OpenMP, #pragma omp parallel reduction(); (2, OpenMP, #pragma omp parallel reduction(unknown); (9, OpenMP, #pragma omp parallel reduction(inscan, + : a)); (6, OpenMP, #pragma omp parallel reduction(default, + : a)); (1, OpenMP, #pragma omp parallel reduction(default,); (2, OpenMP, #pragma omp parallel reduction(unknown, +: a))"}
{"predict": "(12, OpenMP, int _dev_thread_num = omp_get_thread_num();); (12, OpenMP, int _dev_thread_id = omp_get_thread_id();)", "label": "(13, OpenMP, int _dev_thread_id = omp_get_thread_num();); (11, OpenMP, int _dev_thread_num = omp_get_num_threads();)"}
{"predict": "(13, CUDA, cudaError_t err = cudaMalloc(&m_histogram, sizeof(int) * 256 * maxBatchSize);)", "label": "(14, CUDA, cudaError_t err = cudaMalloc(&m_histogram, sizeof(int) * 256 * maxBatchSize);)"}
{"predict": "(57, OpenMP, #pragma omp target teams distribute parallel for simd schedule(static,); (77, OpenMP, #pragma omp target teams distribute parallel for simd schedule(static, 1)); (20, OpenMP, #pragma omp target teams distribute parallel for simd schedule(static,); (7, OpenMP, #pragma omp target teams distribute parallel for simd schedule()); (0, OpenMP, #pragma omp target teams distribute parallel for simd schedule); (54, OpenMP, #pragma omp target teams distribute parallel for simd schedule(static)); (91, OpenMP, #pragma omp target teams distribute parallel for simd schedule(dynamic)); (62, OpenMP, #pragma omp target teams distribute parallel for simd schedule(static, 1)); (17, OpenMP, #pragma omp target teams distribute parallel for simd schedule(static, 1)); (28, OpenMP, #pragma omp target teams distribute parallel for simd schedule(static, 1)); (29, OpenMP, #pragma omp target teams distribute parallel for simd schedule(dynamic)); (4, OpenMP, #pragma omp target teams distribute parallel for simd schedule(static); (82, OpenMP, #pragma omp target teams distribute parallel for simd schedule(guided)); (35, OpenMP, #pragma omp target teams distribute parallel for simd schedule(dynamic)); (20, OpenMP, #pragma omp target teams distribute parallel for simd schedule(guided)); (54, OpenMP, #pragma omp target teams distribute parallel for simd schedule(dynamic, 1)); (-7, OpenMP, #pragma omp target teams distribute parallel for simd schedule(); (61, OpenMP, #pragma omp target teams distribute parallel for simd schedule(guided, 1)); (76, OpenMP, #pragma omp target teams distribute parallel for simd schedule(static, 1)); (25, OpenMP, #pragma omp target teams distribute parallel for simd schedule(static, 1)); (36, OpenMP, #pragma omp target teams distribute parallel for simd schedule(dynamic, 1)); (39, OpenMP, #pragma omp target teams distribute parallel for simd schedule(guided, 1)); (20, OpenMP, #pragma omp target teams distribute parallel for simd schedule(dynamic)); (0, OpenMP, #pragma omp target teams distribute parallel for simd schedule(auto)); (52, OpenMP, #pragma omp target teams distribute parallel for simd schedule(static, 1, 1)); (57, OpenMP, #pragma omp target teams distribute parallel for simd schedule(dynamic, 1, 1", "label": "(137, OpenMP, #pragma omp for schedule(nonmonotonic: auto)); (3, OpenMP, #pragma omp for); (47, OpenMP, #pragma omp for schedule(guided)); (106, OpenMP, #pragma omp for schedule(monotonic: dynamic)); (41, OpenMP, #pragma omp for simd schedule(runtime)); (56, OpenMP, #pragma omp for simd schedule(dynamic)); (107, OpenMP, #pragma omp for simd schedule(monotonic: dynamic)); (46, OpenMP, #pragma omp for simd schedule(guided)); (89, OpenMP, #pragma omp for simd schedule(monotonic: runtime)); (10, OpenMP, #pragma omp for simd schedule(static)); (83, OpenMP, #pragma omp for schedule(monotonic: runtime)); (90, OpenMP, #pragma omp for schedule(monotonic: guided)); (144, OpenMP, #pragma omp for schedule(static, 2) ordered(1)); (28, OpenMP, #pragma omp for schedule(runtime)); (62, OpenMP, #pragma omp for schedule(monotonic: static, 2)); (-1, OpenMP, #pragma omp for schedule(static)); (21, OpenMP, #pragma omp for simd schedule(auto)); (144, OpenMP, #pragma omp for simd schedule(static, 2) ordered); (152, OpenMP, #pragma omp for simd schedule(auto) ordered); (66, OpenMP, #pragma omp for schedule(monotonic: auto)); (105, OpenMP, #pragma omp for schedule(nonmonotonic: dynamic)); (145, OpenMP, #pragma omp for schedule(auto) ordered(1)); (67, OpenMP, #pragma omp for simd schedule(monotonic: auto)); (110, OpenMP, #pragma omp for simd schedule(nonmonotonic: static)); (57, OpenMP, #pragma omp for simd schedule(monotonic: static, 2)); (46, OpenMP, #pragma omp for simd schedule(monotonic: static)); (-2, OpenMP, #pragma omp for schedule(static, 2)); (39, OpenMP, #pragma omp for schedule(monotonic: static)); (113, OpenMP, #pragma omp for simd schedule(nonmonotonic: runtime)); (28, OpenMP, #pragma omp for schedule(dynamic)); (87, OpenMP, #pragma omp for schedule(nonmonotonic: guided)); (149, OpenMP, #pragma omp for simd schedule(runtime) ordered); (97, OpenMP, #pragma omp for simd schedule(nonmonotonic: dynamic)); (157, OpenMP, #pragma omp for simd schedule(guided) ordered); (-25, OpenMP, #pragma omp for simd); (111, OpenMP, #pragma omp for schedule(static) ordered); (69, OpenMP, #pragma omp for simd schedule(monotonic: guided)); (-4, OpenMP, #pragma omp for schedule(auto)); (83, OpenMP, #pragma omp for simd schedule(nonmonotonic: guided)); (156, OpenMP, #pragma omp for schedule(dynamic) ordered(1)); (111, OpenMP, #pragma omp for simd schedule(static) ordered); (144, OpenMP, #pragma omp for schedule(guided) ordered(1)); (-13, OpenMP, #pragma omp for simd schedule(static, 2)); (132, OpenMP, #pragma omp for schedule(runtime) ordered)"}
{"predict": "(52, OpenCL, clReleaseEvent(event);)", "label": "(51, OpenCL, clReleaseEvent(event);)"}
{"predict": "(9, OpenMP, #pragma omp parallel for private(j,k,m,rho_inv,us,vs,ws,square,qs))", "label": "(171, OpenMP, #pragma omp for private(j,k,vijk,vp1,vm1)); (397, OpenMP, #pragma omp for private(j,m)); (7, OpenMP, #pragma omp for nowait private(j,k,rho_inv)); (127, OpenMP, #pragma omp for nowait private(j,k,m)); (103, OpenMP, #pragma omp for nowait private(k,m)); (343, OpenMP, #pragma omp for nowait private(j,m)); (151, OpenMP, #pragma omp for private(k,m)); (25, OpenMP, #pragma omp for private(j,k,m)); (38, OpenMP, #pragma omp for private(j,k,uijk,up1,um1)); (282, OpenMP, #pragma omp for private(j,k,wijk,wp1,wm1)); (399, OpenMP, #pragma omp for private(k,m,i))"}
{"predict": "(9, MPI, MPI_Init(&argc, &argv);); (13, MPI, MPI_Finalize();); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (15, MPI, MPI_Type_free(&ddt);); (7, MPI, MPI_DDT_DUMP(ddt);); (17, MPI, MPI_Type_commit(&ddt);)", "label": "(28, MPI, MPI_Type_free(&ddt);); (7, MPI, MPI_Init(&argc, &argv);); (12, MPI, MPI_Finalize();); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (21, MPI, MPI_DDT_DUMP(ddt);)"}
{"predict": "(46, MPI, MPI_Comm_rank(comm, &rank);); (46, MPI, MPI_Comm_size(comm, &np);); (54, MPI, MPI_Recv(inbufs[j], counts[j], MPI_BYTE, partner, tag, comm, &status);); (45, MPI, MPI_Send(outbufs[j], counts[j], MPI_BYTE, 0, tag, comm);); (26, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (54, MPI, MPI_Get_count(&status, MPI_BYTE, &count);); (44, MPI, MPI_Comm_set_errhandler(comm, MPI_ERRORS_RETURN);); (38, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(57, MPI, MPI_Recv(outbufs[j], counts[j], types[j], partner, tag, comm, &status);); (45, MPI, MPI_Type_get_name(types[j], myname, &mynamelen);); (49, MPI, MPI_Send(inbufs[j], counts[j], types[j], partner, tag, comm);); (35, MPI, MPI_Comm_size(comm, &np);); (25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (32, MPI, MPI_Comm_rank(comm, &rank);); (53, MPI, MPI_Get_count(&status, types[j], &count);)"}
{"predict": "(17, MPI, MPI_Init(&argc, &argv);); (21, MPI, MPI_Finalize();)", "label": "(16, MPI, MPI_Init(&argc, &argv);); (20, MPI, MPI_Finalize();)"}
{"predict": "(14, MPI, MPI_Init(&argc, &argv);); (148, MPI, MPI_Finalize();); (72, MPI, t0 = MPI_Wtime();); (73, MPI, t1 = MPI_Wtime();)", "label": "(14, MPI, MPI_Init(&argc, &argv);); (148, MPI, MPI_Finalize();); (72, MPI, t0 = MPI_Wtime();); (73, MPI, t1 = MPI_Wtime();)"}
{"predict": "(11, MPI, MPI_Abort(MPI_COMM_WORLD, MPIT_result);); (12, MPI, MPIT_result = MPI_T_pvar_session_create(session);); (6, MPI, MPIT_result = MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);)", "label": "(11, MPI, MPI_Abort(MPI_COMM_WORLD, MPIT_result);); (12, MPI, MPIT_result = MPI_T_pvar_session_create(session);); (6, MPI, MPIT_result = MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);)"}
{"predict": "(6, CUDA, __shared__ float block_acc[32];)", "label": "(6, CUDA, __shared__ float block_acc[32];)"}
{"predict": "(12, OpenMP, int maxThreads = omp_get_max_threads();)", "label": "(3, OpenMP, auto maxThreads = omp_get_max_threads();)"}
{"predict": "(6, MPI, MPI_Init(&argc, &argv);); (19, MPI, MPI_Finalize();); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (7, MPI, MPI_Comm_get_parent(&intercomm);); (10, MPI, err = MPI_Recv(str, 3, MPI_CHAR, 4, 0, intercomm, MPI_STATUS_IGNORE);); (8, MPI, err = MPI_Send(\"hi\", 3, MPI_CHAR, 4, 0, intercomm);)", "label": "(6, MPI, MPI_Init(&argc, &argv);); (19, MPI, MPI_Finalize();); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (7, MPI, MPI_Comm_get_parent(&intercomm);); (11, MPI, err = MPI_Recv(str, 4, MPI_CHAR, 3, 0, intercomm, MPI_STATUS_IGNORE);); (8, MPI, err = MPI_Send(\"hi\", 3, MPI_CHAR, 3, 0, intercomm);)"}
{"predict": "(30, CUDA, CUDA_CHECK(cudaGetLastError());)", "label": "(31, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (29, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());)"}
{"predict": "(2, OpenMP, #pragma omp target enter data map(tofrom: tmain, foo, z)); (3, OpenMP, #pragma omp target enter data map(tofrom: tmain, foo, z) device (-10u)); (3, OpenMP, #pragma omp target enter data map(tofrom: tmain, foo, z) device (3.14)); (0, OpenMP, #pragma omp target enter data map(tofrom: tmain, foo, z) device (argc))); (1, OpenMP, #pragma omp target enter data map(tofrom: tmain, foo, z) device (argc), device (argc+1)); (2, OpenMP, #pragma omp target enter data map(tofrom: tmain, foo, z) device (-2)); (-3, OpenMP, #pragma omp target enter data map(tofrom: tmain, foo, z) device (); (-6, OpenMP, #pragma omp target enter data map(tofrom: tmain, foo, z) device); (-7, OpenMP, #pragma omp target enter data map(tofrom: tmain, foo, z) device)", "label": "(5, OpenMP, #pragma omp task final (argc); (3, OpenMP, #pragma omp task final ()); (8, OpenMP, #pragma omp task final (argv[1]=2)); (4, OpenMP, #pragma omp task final (argc > 0? argv[1] : argv[2])); (8, OpenMP, #pragma omp task final (1 0)); (1, OpenMP, #pragma omp task final (z+argc))); (2, OpenMP, #pragma omp task final (foobool(argc)), final (true)); (6, OpenMP, #pragma omp task final(if(tmain(argc, argv)); (-5, OpenMP, #pragma omp task final (); (2, OpenMP, #pragma omp task final (argc argc)); (-8, OpenMP, #pragma omp task final); (-2, OpenMP, #pragma omp task final (S1))"}
{"predict": "(49, MPI, MPI_Allreduce(MPI_IN_PLACE,&n,1,mpi_int_t,MPI_MAX,grid->comm);); (47, MPI, MPI_Allreduce(MPI_IN_PLACE,&nnzL,1,mpi_int_t,MPI_SUM,grid->comm);)", "label": "(49, MPI, MPI_Allreduce(MPI_IN_PLACE,&n,1,mpi_int_t,MPI_MAX,grid->comm);); (47, MPI, MPI_Allreduce(MPI_IN_PLACE,&nnzL,1,mpi_int_t,MPI_SUM,grid->comm);)"}
{"predict": "(141, OpenCL, err = clSetKernelArg(kernels[vecsize], 0, sizeof(src1), &src1);); (142, OpenCL, err = clSetKernelArg(kernels[vecsize], 2, sizeof(src2), &src2);); (143, OpenCL, err = clSetKernelArg(kernels[vecsize], 3, sizeof(cmptype), &cmptype);); (144, OpenCL, err = clSetKernelArg(kernels[vecsize], 4, sizeof(dest), &dest);); (168, OpenCL, err = clEnqueueReadBuffer(queue, dest, CL_TRUE, 0, BUFFER_SIZE, dest_host.data(), 0, NULL, NULL);); (45, OpenCL, src1 = clCreateBuffer(context, CL_MEM_READ_WRITE, BUFFER_SIZE, NULL, &err);); (49, OpenCL, dest = clCreateBuffer(context, CL_MEM_READ_WRITE, BUFFER_SIZE, NULL, &err);); (72, OpenCL, err = clSetKernelArg(kernels[0], 4, sizeof(dest), &dest);); (73, OpenCL, err = clSetKernelArg(kernels[1], 4, sizeof(dest), &dest);); (143, OpenCL, err = clEnqueueReadBuffer(queue, src2, CL_TRUE, 0, BUFFER_SIZE, src2_host.data(), 0, NULL, NULL);); (71, OpenCL, err = clSetKernelArg(kernels[2], 4, sizeof(dest), &dest);); (153, OpenCL, err = clSetKernelArg(kernels[3], 4, sizeof(dest), &dest);); (56, OpenCL, err = clSetKernelArg(kernels[0], 1, sizeof(src2), &src2);); (157, OpenCL, err = clSetKernelArg(kernels[4], 4, sizeof(dest), &dest);); (63, OpenCL, err = clSetKernelArg(kernels[1], 2, sizeof(src2), &src2);); (58, OpenCL, err = clSetKernelArg(kernels[0], 0, sizeof(src1), &src1);); (143, OpenCL, err = clEnqueueReadBuffer(queue, src1, CL_TRUE, 0, BUFFER_SIZE, src1_host.data(), 0, NULL, NULL);", "label": "(51, OpenCL, dest = clCreateBuffer( context, CL_MEM_WRITE_ONLY, BUFFER_SIZE, NULL, &err );); (44, OpenCL, src1 = clCreateBuffer( context, CL_MEM_READ_ONLY, BUFFER_SIZE, NULL, &err );); (72, OpenCL, err = clSetKernelArg(kernels[vecsize], 0, sizeof dest, &dest);); (44, OpenCL, src2 = clCreateBuffer( context, CL_MEM_READ_ONLY, BUFFER_SIZE, NULL, &err );); (76, OpenCL, err = clSetKernelArg(kernels[vecsize], 3, sizeof cmp, &cmp);); (71, OpenCL, err = clSetKernelArg(kernels[vecsize], 1, sizeof src1, &src1);); (134, OpenCL, err = clEnqueueNDRangeKernel(queue, kernels[vecsize], 1, NULL, &vector_count, NULL, 0, NULL, NULL);); (42, OpenCL, cmp = clCreateBuffer( context, CL_MEM_READ_ONLY, BUFFER_SIZE, NULL, &err );); (70, OpenCL, err = clSetKernelArg(kernels[vecsize], 2, sizeof src2, &src2);)"}
{"predict": "(16, MPI, MPI_Type_commit(&eq_type);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (18, MPI, MPI_Op_free(&op);); (13, MPI, MPI_Type_create_struct(2, blklens, displs, types, &eq_type);); (13, MPI, MPI_Type_commit(&eq_type);); (16, MPI, MPI_Op_create((MPI_User_function *) addem, 1, &op);); (16, MPI, MPI_Type_free(&eq_type);); (11, MPI, MPI_Type_contiguous(100, MPI_CHAR, &eq_type);); (16, MPI, MPI_Reduce(&data, &result, 1, eq_type, op, 0, MPI_COMM_WORLD);)", "label": "(17, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (21, MPI, MPI_Reduce(&data, &result, 1, eq_type, op, 0, MPI_COMM_WORLD);); (25, MPI, MPI_Op_free(&op);); (10, MPI, MPI_Type_create_struct(2, blklens, displs, types, &eq_type);); (10, MPI, MPI_Type_commit(&eq_type);); (3, MPI, MPI_Op_create(myop, 0, &op);); (10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (21, MPI, MPI_Type_free(&eq_type);)"}
{"predict": "(4, OpenMP, #pragma omp target map(tofrom: var1, var2))", "label": "(7, OpenMP, #pragma omp critical(add_first)); (3, OpenMP, #pragma omp parallel shared(var1, var2)); (7, OpenMP, #pragma omp atomic); (8, OpenMP, #pragma omp single)"}
{"predict": "(13, TBB, test_extract<tbb::flow::rejecting>();); (13, TBB, test_extract<tbb::flow::queueing>();); (15, TBB, test_extract<tbb::flow::rejecting_and_precedes_api>();); (7, TBB, lightweight_testing::test<tbb::flow::function_node>(10);); (13, TBB, test_extract<tbb::flow::queueing_and_precedes_api>();); (7, TBB, test_concurrency<tbb::flow::function_node>(10);)", "label": "(9, TBB, test_ports_return_references<tbb::flow::rejecting>();); (9, TBB, lightweight_testing::test<tbb::flow::multifunction_node>(10);); (13, TBB, test_extract<tbb::flow::rejecting>();); (5, TBB, test_ports_return_references<tbb::flow::queueing>();); (12, TBB, test_extract<tbb::flow::queueing>();)"}
{"predict": "(60, MPI, MPI_Error_string(mpirc, mpierrstr, &mpierrlen);); (58, MPI, int mpirc = MPI_File_set_view(fh, disp, MPI_CHAR, MPI_CHAR, datarep, MPI_INFO_NULL);); (25, MPI, MPI_Bcast(&filesize, 1, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (89, MPI, MPI_Error_string(mpirc, mpierrstr, &mpierrlen);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (213, MPI, MPI_Bcast(&read_count, 1, MPI_INT, 0, MPI_COMM_WORLD);); (119, MPI, mpirc = MPI_File_read_at(fh, read_offset, bufstart, read_count, MPI_CHAR, &status);); (83, MPI, mpirc = MPI_File_read_at(fh, pos, buf, read_count, MPI_CHAR, &status);); (62, MPI, MPI_Barrier(MPI_COMM_WORLD);); (18, MPI, MPI_Bcast(&chunk_count, 1, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (18, MPI, MPI_Bcast(&remainder, 1, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (62, MPI, int mpierr = MPI_File_set_view(fh, 0, MPI_CHAR, MPI_CHAR, datarep, MPI_INFO_NULL);); (18, MPI, MPI_Bcast(&chunk_offset, 1, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (204, MPI, MPI_Allreduce(&read_count, &all_count, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (9, MPI, MPI_Bcast(&last_offset, 1, MPI_OFFSET, 0, MPI_COMM_WORLD);); (93, MPI, MPI_Allreduce(&scan, &done, 1, MPI_INT, MPI_LAND, MPI_COMM_WORLD);); (31, MPI, MPI_Scan(&chunk_count, &chunk_offset, 1, MPI_UINT64_T, MPI_SUM, MPI_COMM_WORLD);); (0, MPI, MPI_Bcast(&readable, 1, MPI_INT, 0, MPI_COMM_WORLD);); (205, MPI, MPI_File_close(&fh);); (60, MPI, MPI_Error_string(mpirc, mpi", "label": "(64, MPI, MPI_Error_string(mpirc, mpierrstr, &mpierrlen);); (45, MPI, MPI_Exscan(&chunk_count, &chunk_offset, 1, MPI_UINT64_T, MPI_SUM, MPI_COMM_WORLD);); (88, MPI, mpirc = MPI_File_read_at(fh, pos, buf, 1, MPI_CHAR, &status);); (120, MPI, mpirc = MPI_File_read_at(fh, read_offset, bufstart, read_count, MPI_CHAR, &status);); (58, MPI, int mpirc = MPI_File_set_view(fh, disp, MPI_CHAR, MPI_CHAR, datarep, MPI_INFO_NULL);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (19, MPI, MPI_Bcast(&filesize, 1, MPI_UINT64_T, 0, MPI_COMM_WORLD);)"}
{"predict": "(1, CUDA, __shared__ float s_mem[32];)", "label": "(1, CUDA, __shared__ float s_mem[32];)"}
{"predict": "(12, MPI, rc = MPI_Type_get_extent(type, &tmp_lb, &attr->child_type_extent);); (13, MPI, rc = MPI_Type_free(&type);); (19, MPI, rc = MPI_Type_free(&attr->child_type);)", "label": "(15, MPI, rc = MPI_Type_dup(type, newtype);); (11, MPI, rc = MPI_Type_get_extent(type, &tmp_lb, &attr->child_type_extent);); (19, MPI, rc = MPI_Type_free(&type);)"}
{"predict": "(60, OpenMP, #pragma omp parallel for reduction(+ : ret_sum))", "label": "(60, OpenMP, #pragma omp parallel for reduction(+ : ret_sum))"}
{"predict": "(2, TBB, tbb::task* dummy_root = new( tbb::task::allocate_root() ) tbb::empty_task;); (18, TBB, tbb::task::spawn( t );)", "label": "(3, TBB, tbb::task* dummy_root = new( tbb::task::allocate_root() ) tbb::empty_task;); (22, TBB, tbb::task::destroy( *dummy_root );); (13, TBB, tbb::task::spawn_root_and_wait(t);); (16, TBB, tbb::task::enqueue(t);)"}
{"predict": "(32, OpenMP, #pragma omp target update from(z[:10])); (17, OpenMP, #pragma omp target update to(z[:10])); (18, OpenMP, #pragma omp target map(to:k[:5]) map(always, tofrom:x) map(tofrom:y))", "label": "(28, OpenMP, #pragma omp target update from(z[:10])); (17, OpenMP, #pragma omp target update to(z[:10])); (18, OpenMP, #pragma omp target map(to:k[:5]) map(always, tofrom:x) map(tofrom:y))"}
{"predict": "(29, OpenMP, #pragma omp target update from(z[:10])); (17, OpenMP, #pragma omp target update to(z[:10])); (18, OpenMP, #pragma omp target map(to:k[:5]) map(always, tofrom:x) map(tofrom:y))", "label": "(28, OpenMP, #pragma omp target update from(z[:10])); (17, OpenMP, #pragma omp target update to(z[:10])); (18, OpenMP, #pragma omp target map(to:k[:5]) map(always, tofrom:x) map(tofrom:y))"}
{"predict": "(19, CUDA, return new Kernel_Cyrush();); (1, CUDA, return new Kernel();); (20, CUDA, return new Kernel_Dims_create(num_dims, DIM_SIZE);); (17, CUDA, return new Kernel_Pack_dim(num_dims, DIM_SIZE);)", "label": "(18, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(3, OpenMP, #pragma omp target); (3, OpenMP, #pragma omp teams distribute)", "label": "(4, OpenMP, #pragma omp teams distribute); (2, OpenMP, #pragma omp target)"}
{"predict": "(41, CUDA, return cudaGetLastError();)", "label": "(41, CUDA, return cudaGetLastError();)"}
{"predict": "(23, CUDA, __shared__ float alpha_buffer_sh[32];)", "label": "(14, CUDA, extern __shared__ float alpha_buffer_sh[];)"}
{"predict": "(66, MPI, rc = MPI_Type_free(&type);); (23, MPI, rc = MPI_Type_commit(newtype);)", "label": "(65, MPI, rc = MPI_Type_free(&type);)"}
{"predict": "(9, MPI, MPI_Init(&argc, &argv);); (46, MPI, MPI_Comm_free(&comm2d);); (30, MPI, MPI_Cart_create(MPI_COMM_WORLD, 2, dims, period, 0, &comm2d);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (35, MPI, MPI_Cart_coords(comm2d, irank, 2, coords);); (35, MPI, MPI_Neighbor_allgather(MPI_IN_PLACE, 0, MPI_DATATYPE_NULL, neighbors, 1, MPI_INT, comm2d);); (27, MPI, MPI_Comm_rank(comm2d, &crank);); (20, MPI, MPI_Finalize();)", "label": "(37, MPI, MPI_Cart_shift(comm2d, 0, 1, &neighbors[0], &neighbors[1]);); (8, MPI, MPI_Init(&argc, &argv);); (26, MPI, MPI_Finalize();); (38, MPI, MPI_Cart_coords(comm2d, crank, 2, coords);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (28, MPI, MPI_Cart_create(MPI_COMM_WORLD, 2, dims, period, 1, &comm2d);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (42, MPI, MPI_Barrier(comm2d);); (26, MPI, MPI_Comm_rank(comm2d, &crank);); (29, MPI, MPI_Cart_shift(comm2d, 1, 1, &neighbors[2], &neighbors[3]);)"}
{"predict": "(21, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize(), \"KaKjw_to_KKwaj\");); (19, CUDA, qmc_cuda::cuda_check(cudaGetLastError(), \"KaKjw_to_KKwaj\");)", "label": "(21, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize(), \"KaKjw_to_KKwaj\");); (19, CUDA, qmc_cuda::cuda_check(cudaGetLastError(), \"KaKjw_to_KKwaj\");)"}
{"predict": "(3, OpenMP, #pragma omp target hint()); (12, OpenMP, #pragma omp target hint(a)); (5, OpenMP, #pragma omp target hint(alloc)); (0, OpenMP, #pragma omp target hint(); (9, OpenMP, #pragma omp target hint(a, b)); (5, OpenMP, #pragma omp target hint(argc)); (1, OpenMP, #pragma omp target hint(argc,); (5, OpenMP, #pragma omp target hint(S1)); (0, OpenMP, #pragma omp target hint(argc > 0? argv[1] : argv[2]))", "label": "(12, OpenMP, #pragma omp atomic hint(1) hint(1)); (7, OpenMP, #pragma omp atomic hint(a); (4, OpenMP, #pragma omp atomic hint(+); (1, OpenMP, #pragma omp atomic hint(); (6, OpenMP, #pragma omp atomic hint(a)); (-3, OpenMP, #pragma omp atomic hint)"}
{"predict": "(62, CUDA, HANDLE_CUDA_ERROR(cudaMallocHost((void**) &C,      sizeC));); (55, CUDA, HANDLE_CUDA_ERROR(cudaMalloc((void**) &C_d,   sizeC));); (63, CUDA, HANDLE_CUDA_ERROR(cudaMallocHost((void**) &A,      sizeA));); (52, CUDA, HANDLE_CUDA_ERROR(cudaMalloc((void**) &A_d,   sizeA));); (187, CUDA, HANDLE_CUDA_ERROR(cudaFree(A_d));); (187, CUDA, HANDLE_CUDA_ERROR(cudaFreeHost(A));); (183, CUDA, HANDLE_CUDA_ERROR(cudaFree(C_d));); (184, CUDA, HANDLE_CUDA_ERROR(cudaFreeHost(C));); (55, CUDA, HANDLE_CUDA_ERROR(cudaMallocHost((void**) &B,      sizeC));)", "label": "(63, CUDA, HANDLE_CUDA_ERROR(cudaMallocHost((void**) &C, sizeof(floatTypeC) * elementsC));); (78, CUDA, HANDLE_CUDA_ERROR(cudaMemcpy2DAsync(A_d, sizeA, A, sizeA, sizeA, 1, cudaMemcpyDefault, nullptr));); (192, CUDA, if (C_d) cudaFree(C_d);); (188, CUDA, if (A) cudaFreeHost(A);); (51, CUDA, HANDLE_CUDA_ERROR(cudaMalloc((void**) &C_d, sizeC));); (49, CUDA, HANDLE_CUDA_ERROR(cudaMalloc((void**) &A_d, sizeA));); (187, CUDA, if (A_d) cudaFree(A_d);); (185, CUDA, if (C) cudaFreeHost(C);); (54, CUDA, HANDLE_CUDA_ERROR(cudaMallocHost((void**) &A, sizeof(floatTypeA) * elementsA));)"}
{"predict": "(18, TBB, tbb::spin_mutex::scoped_lock lock(*rMutex);)", "label": "(18, TBB, tbb::spin_mutex::scoped_lock lock(*rMutex);)"}
{"predict": "(21, CUDA, cudaMalloc(&d_SrcVal, sizeof(uint) * MAX_SAMPLE_COUNT);); (18, CUDA, cudaMalloc(&d_SrcKey, sizeof(uint) * MAX_SAMPLE_COUNT);); (19, CUDA, cudaMalloc(&d_DstVal, sizeof(uint) * MAX_SAMPLE_COUNT);); (17, CUDA, cudaMalloc(&d_DstKey, sizeof(uint) * MAX_SAMPLE_COUNT);); (19, CUDA, cudaMalloc(&d_LimitsB, sizeof(uint) * 2 * MAX_SAMPLE_COUNT);); (17, CUDA, cudaMalloc(&d_LimitsA, sizeof(uint) * 2 * MAX_SAMPLE_COUNT);); (18, CUDA, cudaMalloc(&d_RanksB, sizeof(uint) * MAX_SAMPLE_COUNT);); (16, CUDA, cudaMalloc(&d_RanksA, sizeof(uint) * MAX_SAMPLE_COUNT);)", "label": "(21, CUDA, checkCudaErrors(cudaMalloc((void **)&d_LimitsA, MAX_SAMPLE_COUNT * sizeof(uint)));); (19, CUDA, checkCudaErrors(cudaMalloc((void **)&d_RanksB,  MAX_SAMPLE_COUNT * sizeof(uint)));); (17, CUDA, checkCudaErrors(cudaMalloc((void **)&d_RanksA,  MAX_SAMPLE_COUNT * sizeof(uint)));); (19, CUDA, checkCudaErrors(cudaMalloc((void **)&d_LimitsB, MAX_SAMPLE_COUNT * sizeof(uint)));)"}
{"predict": "(80, OpenMP, #pragma omp barrier); (100, OpenMP, #pragma omp target data map(tofrom: grid[0:n*n])); (70, OpenMP, #pragma omp target enter data map(to: grid[0:n*n])); (100, OpenMP, #pragma omp target exit data map(from: grid[0:n*n]))", "label": "(79, OpenACC, #pragma acc parallel loop independent); (70, OpenACC, #pragma acc data pcopy(grid[0:n*n])); (86, OpenACC, #pragma acc parallel loop gang); (99, OpenACC, #pragma acc kernels); (89, OpenACC, #pragma acc loop vector)"}
{"predict": "(6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &world_size);)", "label": "(6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &world_size);)"}
{"predict": "(7, MPI, MPI_Init(&argc, &argv);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (24, MPI, MPI_Finalize();)", "label": "(25, MPI, MPI_Finalize();); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Init(NULL, NULL);)"}
{"predict": "(124, MPI, MPI_Comm_group( MPI_COMM_WORLD, &world_group );); (140, MPI, MPI_Comm_rank( comm_col, &myrow );); (202, MPI, MPI_Barrier(MPI_COMM_WORLD);); (83, MPI, MPI_Bcast(&inner_block_flag, 1, MPI_INT, root, MPI_COMM_WORLD);); (137, MPI, MPI_Group_incl( world_group, nprow, ranks, &temp_group );); (40, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &my_ID );); (79, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (78, MPI, MPI_Bcast(&order,  1, MPI_INT, root, MPI_COMM_WORLD);); (125, MPI, MPI_Group_incl( world_group, npcol, ranks, &temp_group );); (35, MPI, MPI_Init(&argc,&argv);); (137, MPI, MPI_Comm_create( MPI_COMM_WORLD, temp_group, &comm_col );); (34, MPI, MPI_Comm_size( MPI_COMM_WORLD, &Num_procs );); (122, MPI, MPI_Comm_create( MPI_COMM_WORLD, temp_group, &comm_row );); (247, MPI, MPI_Finalize();); (71, MPI, MPI_Bcast(&nb, 1, MPI_INT, root, MPI_COMM_WORLD);); (166, MPI, MPI_Allgather( &mynrows, 1, MPI_INT, mm, 1, MPI_INT, comm_col );); (71, MPI, MPI_Bcast(&nprow, 1, MPI_INT, root, MPI_COMM_WORLD);); (137, MPI, MPI_Comm_rank( comm_row, &mycol );); (164, MPI, MPI_Allgather( &myncols, 1, MPI_INT, nn, 1, MPI_INT, comm_row );); (71, MPI, MPI_Bcast(&npcol, 1, MPI_INT, root, MPI_COMM_WORLD);); (229, MPI, MPI_Reduce( &work1, &work2, 1, MPI_DOUBLE, MPI_SUM, root, MPI_COMM_WORLD );); (231, MPI, MPI_Allgather( &avgtime, 1, MPI_DOUBLE, &avgtime, 1, MPI_DOUBLE, comm_row );); (64, MPI, MPI_Bcast(&a, b, c, lda, ldb, ldc,", "label": "(124, MPI, MPI_Comm_group( MPI_COMM_WORLD, &world_group );); (143, MPI, MPI_Comm_rank( comm_col, &myrow );); (205, MPI, MPI_Barrier(MPI_COMM_WORLD);); (83, MPI, MPI_Bcast(&inner_block_flag, 1, MPI_INT, root, MPI_COMM_WORLD);); (135, MPI, MPI_Group_incl( world_group, nprow, ranks, &temp_group );); (39, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &my_ID );); (78, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (124, MPI, MPI_Group_incl( world_group, npcol, ranks, &temp_group );); (35, MPI, MPI_Init(&argc,&argv);); (131, MPI, MPI_Comm_create( MPI_COMM_WORLD, temp_group, &comm_col );); (35, MPI, MPI_Comm_size( MPI_COMM_WORLD, &Num_procs );); (121, MPI, MPI_Comm_create( MPI_COMM_WORLD, temp_group, &comm_row );); (245, MPI, MPI_Finalize();); (70, MPI, MPI_Bcast(&order,  1, MPI_INT, root, MPI_COMM_WORLD);); (167, MPI, MPI_Allgather( &mynrows, 1, MPI_INT, mm, 1, MPI_INT, comm_col );); (70, MPI, MPI_Bcast(&nb, 1, MPI_INT, root, MPI_COMM_WORLD);); (127, MPI, MPI_Comm_rank( comm_row, &mycol );); (172, MPI, MPI_Allgather( &myncols, 1, MPI_INT, nn, 1, MPI_INT, comm_row );)"}
{"predict": "(9, OpenMP, #pragma omp target teams distribute parallel for if (parallel: Arg)); (6, OpenMP, #pragma omp target teams distribute parallel for if (false)); (1, OpenMP, #pragma omp target teams distribute parallel for if (true)); (0, OpenMP, #pragma omp target teams distribute parallel for if (false))", "label": "(6, OpenMP, #pragma omp target teams distribute parallel for if (false)); (9, OpenMP, #pragma omp target teams distribute parallel for if (parallel: Arg)); (0, OpenMP, #pragma omp target teams distribute parallel for if (true))"}
{"predict": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(8, MPI, MPI_Reduce_local(inbuf, inoutbuf, count, datatype, MPI_SUM);)", "label": "(8, MPI, MPI_Reduce_local_c(inbuf, inoutbuf, count, datatype, MPI_SUM);)"}
{"predict": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(100, MPI, MPI_Barrier(c_info->File_comm);); (122, MPI, MPI_File_read_at(fh, Offset, c_info->r_buffer, Locsize, c_info->etype, &stat);); (95, MPI, MPI_File_read(fh, c_info->r_buffer, Locsize, c_info->etype, &stat);); (69, MPI, MPI_Bcast( &time_inner, 1, MPI_INT, 0, c_info->communicator );); (132, MPI, MPI_File_read_ordered(fh, c_info->r_buffer, Locsize, c_info->etype, &stat);); (94, MPI, MPI_File_read_shared(fh, c_info->r_buffer, Locsize, c_info->etype, &stat);); (117, MPI, MPI_File_read_at_all(fh, Offset, c_info->r_buffer, Locsize, c_info->etype, &stat);)", "label": "(102, MPI, MPI_Barrier(c_info->File_comm);); (108, MPI, MPI_ERRHAND(GEN_File_read(c_info->fh, c_info->r_buffer, Locsize, c_info->etype, &stat));); (121, MPI, MPI_ERRHAND(GEN_File_read_at(c_info->fh, Offset, c_info->r_buffer, Locsize, c_info->etype, &stat));); (133, MPI, MPI_ERRHAND(GEN_File_read_shared(c_info->fh, c_info->r_buffer, Locsize, c_info->etype, &stat));); (92, MPI, *time = MPI_Wtime();)"}
{"predict": "(6, CUDA, thrust::device_ptr<thrust::complex<double>> d_ptr = thrust::device_malloc<thrust::complex<double>>(1);); (13, CUDA, thrust::device_free(d_ptr);); (9, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize());); (7, CUDA, qmc_cuda::cuda_check(cudaGetLastError());)", "label": "(6, CUDA, thrust::device_ptr<thrust::complex<double>> d_ptr = thrust::device_malloc<thrust::complex<double>>(1);); (14, CUDA, thrust::device_free(d_ptr);); (9, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize());); (7, CUDA, qmc_cuda::cuda_check(cudaGetLastError());)"}
{"predict": "(87, OpenMP, #pragma omp parallel private(i)); (6, OpenMP, #pragma omp distribute parallel for simd private); (50, OpenMP, #pragma omp distribute parallel for simd private(argv[1])); (54, OpenMP, #pragma omp distribute parallel for simd private(e, g)); (63, OpenMP, #pragma omp distribute parallel for simd private(i)); (16, OpenMP, #pragma omp distribute parallel for simd private(argc); (37, OpenMP, #pragma omp distribute parallel for simd private(a, b)); (40, OpenMP, #pragma omp distribute parallel for simd private(S1)); (73, OpenMP, #pragma omp distribute parallel for simd private(j)); (21, OpenMP, #pragma omp distribute parallel for simd private(argc > 0? argv[1] : argv[2])); (26, OpenMP, #pragma omp distribute parallel for simd private(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (29, OpenMP, #pragma omp distribute parallel for simd private(conditional: argc) private(conditional:); (54, OpenMP, #pragma omp distribute parallel for simd shared(i)); (-5, OpenMP, #pragma omp target); (62, OpenMP, #pragma omp parallel); (75, OpenMP, #pragma omp distribute parallel for simd private(i)); (76, OpenMP, #pragma omp parallel shared(i)); (11, OpenMP, #pragma omp distribute parallel for simd private(argc,); (-4, OpenMP, #pragma omp teams)", "label": "(63, OpenMP, #pragma omp distribute parallel for simd private(h)); (86, OpenMP, #pragma omp distribute parallel for simd private(j)); (82, OpenMP, #pragma omp parallel private(i)); (65, OpenMP, #pragma omp distribute parallel for simd nowait); (9, OpenMP, #pragma omp distribute parallel for simd private(); (1, OpenMP, #pragma omp target); (27, OpenMP, #pragma omp distribute parallel for simd private(argc > 0? argv[1] : argv[2])); (1, OpenMP, #pragma omp distribute parallel for simd private); (35, OpenMP, #pragma omp distribute parallel for simd private(S1)); (19, OpenMP, #pragma omp distribute parallel for simd private(argc,); (61, OpenMP, #pragma omp parallel); (12, OpenMP, #pragma omp distribute parallel for simd private(argc); (36, OpenMP, #pragma omp distribute parallel for simd private(a, b)); (25, OpenMP, #pragma omp distribute parallel for simd private(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (39, OpenMP, #pragma omp distribute parallel for simd private(argv[1])); (43, OpenMP, #pragma omp distribute parallel for simd private(e, g)); (2, OpenMP, #pragma omp distribute parallel for simd private()); (60, OpenMP, #pragma omp distribute parallel for simd private(i)); (65, OpenMP, #pragma omp parallel shared(i)); (-12, OpenMP, #pragma omp teams)"}
{"predict": "(13, OpenCL, clReleaseMemObject(objScratch);); (8, OpenCL, clReleaseMemObject(objNrm2);); (3, OpenCL, clReleaseMemObject(objX);)", "label": "(5, OpenCL, clReleaseMemObject(objX);); (12, OpenCL, clReleaseMemObject(objScratch);); (7, OpenCL, clReleaseMemObject(objNrm2);)"}
{"predict": "(4, CUDA, __shared__ unsigned short points[2*MEMWID];)", "label": "(3, CUDA, __shared__ unsigned short points[2*MEMWID];)"}
{"predict": "(13, CUDA, CUDASimulation cudaSimulation(model);)", "label": "(13, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(68, MPI, MPI_Waitall(2, request, status);)", "label": "(67, MPI, MPI_Waitall(2, request, status);)"}
{"predict": "(22, MPI, result = MPI_Win_get_group(win);); (18, MPI, MPI_Win_create(buf, sizeof(int) * 10, sizeof(int), MPI_INFO_NULL, comm, &win);); (23, MPI, MPI_Win_free(&win);); (16, MPI, MPI_Comm_group(comm, &wingroup);); (18, MPI, MPI_Win_set_group(win, wingroup);); (23, MPI, MPI_Group_free(&wingroup);); (21, MPI, MPI_Group_free(&group);)", "label": "(28, MPI, MPI_Group_free(&wingroup);); (19, MPI, MPI_Win_create(buf, sizeof(int) * 10, sizeof(int), MPI_INFO_NULL, comm, &win);); (28, MPI, MPI_Win_free(&win);); (20, MPI, MPI_Group_compare(group, wingroup, &result);); (25, MPI, MPI_Group_free(&group);); (17, MPI, MPI_Comm_group(comm, &group);); (15, MPI, MPI_Win_get_group(win, &wingroup);)"}
{"predict": "(14, CUDA, ColorSpinorParam cudaParam(cpuParam, *inv_param, QUDA_CUDA_FIELD_LOCATION);)", "label": "(14, CUDA, ColorSpinorParam cudaParam(cpuParam, *inv_param, QUDA_CUDA_FIELD_LOCATION);)"}
{"predict": "(15, OpenMP, #pragma omp target parallel for)", "label": "(15, OpenMP, #pragma omp target parallel for)"}
{"predict": "(58, CUDA, thrust::host_vector<mytype> th_data = t_data;); (40, CUDA, thrust::device_vector<mytype> t_data(h_data, h_data+test_dsize);); (49, CUDA, cudaEventElapsedTime(&et, start, stop);); (49, CUDA, cudaEventSynchronize(stop);); (19, CUDA, cudaStreamCreate(&str);); (27, CUDA, cudaLaunchCooperativeKernel((void *)my_remove_if<mytype>, FIXME);); (41, CUDA, cudaEventRecord(stop);); (4, CUDA, cudaError_t err = cudaGetDeviceProperties(&prop, 0);); (40, CUDA, cudaEventSynchronize(start);); (30, CUDA, cudaEventCreate(&stop);); (32, CUDA, cudaEventRecord(start);); (29, CUDA, cudaEventCreate(&start);); (49, CUDA, thrust::remove(t_data.begin(), t_data.end(), -1);); (-2, CUDA, cudaSetDevice(0);); (8, CUDA, cudaMemcpy(d_idata, h_data, tsize, cudaMemcpyHostToDevice);); (6, CUDA, cudaMemcpy(d_odata, h_data, tsize, cudaMemcpyHostToDevice);); (3, CUDA, cudaMemcpy(d_idxs, h_data, tsize, cudaMemcpyHostToDevice);); (46, CUDA, cudaEventDestroy(start);); (44, CUDA, cudaEventDestroy(stop);); (8, CUDA, cudaStreamSynchronize(str);); (-10, CUDA, cudaMalloc(&d_idxs, tsize);); (38, CUDA, cudaEventElapsedTime(&et, start, stop);); (-9, CUDA, cudaMalloc(&d_odata, tsize);); (-11, CUDA, cudaMalloc(&d_idata, tsize);); (1, CUDA, cudaMemset(d_odata, -1, tsize);); (2, CUDA, cudaMemset(d_idxs, -1, tsize);); (38, CUDA, cudaEventSynchronize(stop);); (39, CUDA, cudaEventDestroy(finished);); (21, CUDA, cudaEventRecord(finished);); (-15, CUDA, cudaSetDevice(0);); (22, CUDA, cudaEventCreate(&finished);); (-7, CUDA, cudaMemset(d_idata, -1, tsize);); (-18, CUDA, cudaMalloc(&d_data, tsize);); (3", "label": "(58, CUDA, thrust::host_vector<mytype> th_data = t_data;); (40, CUDA, thrust::device_vector<mytype> t_data(h_data, h_data+test_dsize);); (49, CUDA, cudaEventElapsedTime(&et, start, stop);); (22, CUDA, cudaStreamCreate(&str);); (27, CUDA, cudaLaunchCooperativeKernel((void *)my_remove_if<mytype>, FIXME);); (8, CUDA, cudaError_t err = cudaGetDeviceProperties(&prop, 0);); (26, CUDA, cudaMemcpy(h_data, d_odata, tsize, cudaMemcpyDeviceToHost);); (49, CUDA, thrust::remove(t_data.begin(), t_data.end(), -1);); (15, CUDA, cudaMemcpy(d_idata, h_data, tsize, cudaMemcpyHostToDevice);); (39, CUDA, cudaEventRecord(stop);); (-1, CUDA, cudaMalloc(&d_idxs, test_dsize*sizeof(unsigned));); (33, CUDA, cudaEventCreate(&start);); (-5, CUDA, cudaMalloc(&d_odata, tsize);); (33, CUDA, cudaEventRecord(start);); (-6, CUDA, cudaMemset(d_odata, 0, tsize);); (-9, CUDA, cudaMalloc(&d_idata, tsize);); (29, CUDA, cudaEventCreate(&stop);); (2, CUDA, cudaOccupancyMaxActiveBlocksPerMultiprocessor(&numBlkPerSM, my_remove_if<mytype>, nTPB, 0);)"}
{"predict": "(49, MPI, MPI_Type_commit(recv_types + j);); (29, MPI, MPI_Type_commit(&recv_types[0]);)", "label": "(48, MPI, MPI_Type_commit(recv_types + j);)"}
{"predict": "(10, CUDA, HANDLE_ERROR(cudaMalloc((void**)&initial_x_d, sizeof(float) * 2 * DYN_T::STATE_DIM));); (12, CUDA, HANDLE_ERROR(cudaMalloc((void**)&num_rollouts_d, sizeof(int)));); (8, CUDA, HANDLE_ERROR(cudaMalloc((void**)&num_timesteps_d, sizeof(int)));); (10, CUDA, HANDLE_ERROR(cudaMalloc((void**)&num_distributions_d, sizeof(int)));); (8, CUDA, HANDLE_ERROR(cudaMalloc((void**)&sampler_data_d, sizeof(int) * num_rollouts * num_timesteps * num_distributions));); (11, CUDA, HANDLE_ERROR(cudaMalloc((void**)&num_threads_d, sizeof(int)));); (8, CUDA, HANDLE_ERROR(cudaMalloc((void**)&stream, sizeof(cudaStream)));); (9, CUDA, HANDLE_ERROR(cudaMalloc((void**)&num_rollouts_d, sizeof(int)));); (6, CUDA, HANDLE_ERROR(cudaMalloc((void**)&num_distributions_d, sizeof(int)));); (8, CUDA, HANDLE_ERROR(cudaMalloc((void**)&sampler_type_d, sizeof(int)));); (7, CUDA, HANDLE_ERROR(cudaMalloc((void**)&num_timesteps_d, sizeof(int)));); (9, CUDA, HANDLE_ERROR(cudaMalloc((void**)&num_threads_d, sizeof(int)));); (5, CUDA, HANDLE_ERROR(cudaMalloc((void**)&num_rollouts_d, sizeof(int)));); (8, CUDA, HANDLE_ERROR(cudaMalloc((void**)&num_deterministic_d, sizeof(int)));); (10, CUDA, HANDLE_ERROR(cudaMalloc((void**)&num_deterministic_d, sizeof(int)));); (4, CUDA, HANDLE_ERROR(cudaMalloc((void**)&num_timesteps_d, sizeof(int)));); (5, CUDA, HANDLE_ERROR(cudaMalloc((void**)&num_distributions_d, sizeof(int)));); (6, CUDA, HANDLE_ERROR(cudaMalloc((void**)&num_thread_y_d, sizeof(int)));); (8, CUDA, HANDLE_ERROR(cudaMalloc((void**)&num_deterministic_d, sizeof(int)));); (4, CUDA, HANDLE_ERROR(cudaMalloc((void**)&num_thread_x_d, sizeof(int)));); (8, CUDA, HANDLE_ERROR(cudaMalloc((void**)&num_deterministic_d, sizeof(int)));); (9, CUDA, HANDLE_ERROR(cudaMalloc((void**)&num_deterministic_d, sizeof(int)));); (3, CUDA, HANDLE_ERROR(cudaMalloc((void**)&num_distributions_d, sizeof(int)));); (10, CUDA,", "label": "(25, CUDA, HANDLE_ERROR(cudaStreamSynchronize(stream));); (9, CUDA, HANDLE_ERROR(cudaMalloc((void**)&initial_x_d, sizeof(float) * 2 * DYN_T::STATE_DIM));); (9, CUDA, HANDLE_ERROR(cudaMalloc((void**)&cost_trajectories_d, sizeof(float) * 2 * num_rollouts));)"}
{"predict": "(6, CUDA, __shared__ float s_buffer[2 * BLOCK_DIM];)", "label": "(6, CUDA, extern __shared__ float s_buffer[];)"}
{"predict": "(18, MPI, MPI_Grequest_complete(aio_reqlist[i]->req);)", "label": "(19, MPI, MPI_Grequest_complete(aio_reqlist[i]->req);)"}
{"predict": "(70, CUDA, cudaEventSynchronize(stop);); (39, CUDA, cudaMalloc((void **) &d_c, sizeof(int)*m*k);); (111, CUDA, cudaFreeHost(h_c);); (62, CUDA, cudaEventRecord(start, 0);); (39, CUDA, cudaMalloc((void **) &d_b, sizeof(int)*n*k);); (7, CUDA, cudaMallocHost((void **) &h_c, sizeof(int)*m*k);); (105, CUDA, cudaFree(d_a);); (60, CUDA, cudaEventElapsedTime(&gpu_elapsed_time_ms, start, stop);); (105, CUDA, cudaFree(d_c);); (105, CUDA, cudaFree(d_b);); (62, CUDA, cudaEventElapsedTime(&cpu_elapsed_time_ms, start, stop);); (17, CUDA, cudaEventCreate(&stop);); (102, CUDA, cudaFreeHost(h_b);); (3, CUDA, cudaMallocHost((void **) &h_b, sizeof(int)*n*k);); (101, CUDA, cudaFreeHost(h_a);); (18, CUDA, cudaEventRecord(stop, 0);); (22, CUDA, cudaEventCreate(&start);); (0, CUDA, cudaMallocHost((void **) &h_a, sizeof(int)*m*n);); (24, CUDA, cudaMalloc((void **) &d_a, sizeof(int)*m*n);); (97, CUDA, cudaFreeHost(h_k);); (2, CUDA, cudaMallocHost((void **) &h_k, sizeof(int)*n*k);); (55, CUDA, cudaEventDestroy(start);); (55, CUDA, cudaEventDestroy(stop);); (93, CUDA, cudaFreeHost(h_a);); (91, CUDA, cudaFreeHost(h_n);); (-2, CUDA, cudaMallocHost((void **) &h_n, sizeof(int)*m*k);); (51, CUDA, cudaEventElapsedTime(&time_elapsed_ms, start, stop);); (89, CUDA, cudaFreeHost(h_m);); (-8, CUDA, cudaMallocHost((void **) &m, sizeof(int)*m*k);); (-7, CUDA, cudaMallocHost((void **) &n, sizeof(int)*n*k);); (51, CUDA, cudaEventSynchronize(start);); (-1, CUDA, cudaEventCreate(&time_start);); (-14, CUDA, cudaMallocHost((void **) &h", "label": "(67, CUDA, cudaEventSynchronize(stop);); (39, CUDA, cudaMalloc((void **) &d_a, sizeof(int)*m*n);); (114, CUDA, cudaFreeHost(h_cc);); (34, CUDA, cudaEventRecord(start, 0);); (60, CUDA, cudaThreadSynchronize();); (36, CUDA, cudaMalloc((void **) &d_b, sizeof(int)*n*k);); (39, CUDA, cudaMemcpy(d_a, h_a, sizeof(int)*m*n, cudaMemcpyHostToDevice);); (103, CUDA, cudaFree(d_a);); (62, CUDA, cudaEventElapsedTime(&gpu_elapsed_time_ms, start, stop);); (3, CUDA, cudaMallocHost((void **) &h_c, sizeof(int)*m*k);); (3, CUDA, cudaMallocHost((void **) &h_cc, sizeof(int)*m*k);); (35, CUDA, cudaMemcpy(d_b, h_b, sizeof(int)*n*k, cudaMemcpyHostToDevice);); (-1, CUDA, cudaMallocHost((void **) &h_b, sizeof(int)*n*k);); (99, CUDA, cudaFree(d_c);); (97, CUDA, cudaFree(d_b);); (66, CUDA, cudaEventElapsedTime(&cpu_elapsed_time_ms, start, stop);); (26, CUDA, cudaMalloc((void **) &d_c, sizeof(int)*m*k);); (49, CUDA, cudaEventRecord(stop, 0);); (45, CUDA, cudaMemcpy(h_c, d_c, sizeof(int)*m*k, cudaMemcpyDeviceToHost);); (95, CUDA, cudaFreeHost(h_b);); (93, CUDA, cudaFreeHost(h_a);); (12, CUDA, cudaEventCreate(&start);); (55, OpenMP, omp_mm(h_a, m, n, h_b, n, k, h_cc);); (-13, CUDA, cudaMallocHost((void **) &h_a, sizeof(int)*m*n);); (10, CUDA, cudaEventCreate(&stop);); (90, CUDA, cudaFreeHost(h_c);)"}
{"predict": "(37, CUDA, cudaMalloc((void **)&result_data, tensor1->size * sizeof(float));)", "label": "(37, CUDA, cudaMalloc((void **)&result_data, tensor1->size * sizeof(float));)"}
{"predict": "(1, OpenMP, #pragma omp parallel firstprivate(a, b, c))", "label": "(1, OpenMP, #pragma omp parallel firstprivate(a, b, c, e)); (4, OpenMP, #pragma omp parallel firstprivate(a, b, c))"}
{"predict": "(29, CUDA, CUDA_CHECK(cudaMemcpy(P_host, P_device, size, cudaMemcpyDeviceToHost));); (35, CUDA, cudaFree(P_device);); (10, CUDA, CUDA_CHECK(cudaMemcpy(M_device, M_host, size, cudaMemcpyHostToDevice));); (34, CUDA, cudaFree(N_device);); (34, CUDA, cudaFree(M_device);); (25, CUDA, CUDA_CHECK(cudaDeviceSynchronize());); (7, CUDA, CUDA_CHECK(cudaMemcpy(N_device, N_host, size, cudaMemcpyHostToDevice));); (9, CUDA, CUDA_CHECK(cudaMalloc((void**)&P_device, size));); (3, CUDA, CUDA_CHECK(cudaMalloc((void**)&N_device, size));); (1, CUDA, CUDA_CHECK(cudaMalloc((void**)&M_device, size));)", "label": "(29, CUDA, CUDA_CHECK(cudaMemcpy(P_host, P_device, size, cudaMemcpyDeviceToHost));); (35, CUDA, cudaFree(P_device);); (10, CUDA, CUDA_CHECK(cudaMemcpy(M_device, M_host, size, cudaMemcpyHostToDevice));); (34, CUDA, cudaFree(N_device);); (34, CUDA, cudaFree(M_device);); (25, CUDA, CUDA_CHECK(cudaDeviceSynchronize());); (7, CUDA, CUDA_CHECK(cudaMemcpy(N_device, N_host, size, cudaMemcpyHostToDevice));); (2, CUDA, CUDA_CHECK(cudaMalloc((void**)&N_device, size));); (9, CUDA, CUDA_CHECK(cudaMalloc((void**)&P_device, size));;); (-1, CUDA, CUDA_CHECK(cudaMalloc((void**)&M_device, size));)"}
{"predict": "(11, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)", "label": "(11, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)"}
{"predict": "(2, OpenMP, #pragma omp task default); (4, OpenMP, #pragma omp task default(none)); (8, OpenMP, #pragma omp task default(firstprivate)); (12, OpenMP, #pragma omp task default(shared), default(shared)); (0, OpenMP, #pragma omp task default()", "label": "(17, OpenMP, #pragma omp task default(firstprivate)); (4, OpenMP, #pragma omp task default(shared), default(shared)); (0, OpenMP, #pragma omp task default(); (0, OpenMP, #pragma omp task default()); (5, OpenMP, #pragma omp task default(none)); (1, OpenMP, #pragma omp task default(x)); (7, OpenMP, #pragma omp task default(shared)); (-6, OpenMP, #pragma omp task default); (-4, OpenMP, #pragma omp task default(none)"}
{"predict": "(4, TBB, tbb::task::spawn( ParallelTestFunctor() );)", "label": "(4, TBB, const tbb::blocked_range3d<int> r( 0, i, 5, 0, j, 3, 0, k, 1 );); (4, TBB, tbb::parallel_for( r, Striker() );)"}
{"predict": "(31, CUDA, __shared__ double temp2[MAXSUPER];); (28, CUDA, __shared__ double s_lsum[MAXSUPER];)", "label": "(31, CUDA, volatile __shared__ int s_bmod;); (28, CUDA, __shared__ double s_lsum[MAXSUPER];)"}
{"predict": "(52, MPI, MPI_Allreduce(MPI_IN_PLACE,&n,1,mpi_int_t,MPI_MAX,grid->comm);); (50, MPI, MPI_Allreduce(MPI_IN_PLACE,&nnzL,1,mpi_int_t,MPI_SUM,grid->comm);)", "label": "(52, MPI, MPI_Allreduce(MPI_IN_PLACE,&n,1,mpi_int_t,MPI_MAX,grid->comm);); (50, MPI, MPI_Allreduce(MPI_IN_PLACE,&nnzL,1,mpi_int_t,MPI_SUM,grid->comm);)"}
{"predict": "(34, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (50, CUDA, CUDA_CHECK(cudaDeviceReset());); (47, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (23, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (41, CUDA, CUDA_CHECK(cudaFree(d_A));); (17, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)", "label": "(34, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (50, CUDA, CUDA_CHECK(cudaDeviceReset());); (47, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (23, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (41, CUDA, CUDA_CHECK(cudaFree(d_A));); (17, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(11, MPI, err = MPI_Type_struct(3, blocks, disps, types, &inttype);); (32, MPI, err = MPI_Type_size(eviltype, &val);); (133, MPI, MPI_Type_free(&inttype);); (19, MPI, err = MPI_Type_vector(3, 4, -5, inttype, &eviltype);); (76, MPI, err = MPI_Type_get_extent(eviltype, &lb, &extent);); (95, MPI, err = MPI_Type_ub(eviltype, &aval);); (131, MPI, MPI_Type_free(&eviltype);); (108, MPI, err = MPI_Type_get_true_extent(eviltype, &true_lb, &aval);); (40, MPI, err = MPI_Type_extent(eviltype, &aval);); (54, MPI, err = MPI_Type_lb(eviltype, &aval);)", "label": "(11, MPI, err = MPI_Type_struct(3, blocks, disps, types, &inttype);); (32, MPI, err = MPI_Type_size(eviltype, &val);); (134, MPI, MPI_Type_free(&inttype);); (75, MPI, err = MPI_Type_get_extent(eviltype, &lb, &extent);); (96, MPI, err = MPI_Type_ub(eviltype, &aval);); (17, MPI, err = MPI_Type_vector(3, 4, -5, inttype, &eviltype);); (131, MPI, MPI_Type_free(&eviltype);); (108, MPI, err = MPI_Type_get_true_extent(eviltype, &true_lb, &aval);); (40, MPI, err = MPI_Type_extent(eviltype, &aval);); (54, MPI, err = MPI_Type_lb(eviltype, &aval);)"}
{"predict": "(48, OpenMP, lis2mdl_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)", "label": "(48, OpenMP, lis2mdl_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)"}
{"predict": "(4, TBB, const oneapi::tbb::blocked_range3d<int> r( 0, i, 5, 0, j, 3, 0, k, 1 );); (4, TBB, oneapi::tbb::parallel_for( r, Striker() );)", "label": "(4, TBB, const tbb::blocked_range3d<int> r( 0, i, 5, 0, j, 3, 0, k, 1 );); (4, TBB, tbb::parallel_for( r, Striker() );)"}
{"predict": "(36, CUDA, CUDASimulation cudaSimulation(model);)", "label": "(36, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(9, OpenMP, #pragma omp target); (10, OpenMP, #pragma omp distribute parallel for simd lastprivate(g, g1, svar, sfvar)); (69, OpenMP, #pragma omp distribute parallel for simd lastprivate(t_var, vec, s_arr, s_arr, var, var, svar)); (7, OpenMP, #pragma omp teams)", "label": "(9, OpenMP, #pragma omp target); (65, OpenMP, #pragma omp distribute parallel for lastprivate(t_var, vec, s_arr, s_arr, var, var, svar)); (9, OpenMP, #pragma omp distribute parallel for lastprivate(g, g1, svar, sfvar)); (7, OpenMP, #pragma omp teams)"}
{"predict": "(42, CUDA, cudaCheck(cudaGetLastError());)", "label": "(45, CUDA, cudaCheck(cudaGetLastError());)"}
{"predict": "(66, OpenMP, iis2mdc_offset_temp_comp_set(&mag_ctx, PROPERTY_ENABLE);)", "label": "(66, OpenMP, iis2mdc_offset_temp_comp_set(&mag_ctx, PROPERTY_ENABLE);)"}
{"predict": "(13, CUDA, CP_host->freeCudaMem();)", "label": "(13, CUDA, CP_host->freeCudaMem();)"}
{"predict": "(116, CUDA, cudaFuncGetAttributes(&cuattr, slsum_bmod_inv_gpu_mrhs_nvshmem);); (116, CUDA, cudaOccupancyMaxPotentialBlockSize(&minGridSize, &myblockSize, (const void *)slsum_bmod_inv_gpu_mrhs_nvshmem, 0, 0);); (116, CUDA, cudaDeviceSetLimit(cudaLimitStackSize, cuattr.localSizeBytes);); (94, CUDA, cudaStreamCreateWithFlags(&stream[i], cudaStreamNonBlocking);); (115, CUDA, cudaDeviceSetLimit(cudaLimitStackSize, 4 * 1024);)", "label": "(117, CUDA, cudaDeviceSetLimit(cudaLimitStackSize, cuattr.localSizeBytes);); (93, CUDA, cudaStreamCreateWithFlags(&stream[i], cudaStreamNonBlocking);); (121, CUDA, cudaOccupancyMaxPotentialBlockSize(&minGridSize, &myblockSize, (const void *) swait_bcrd_u, 0, 0);); (173, CUDA, CUDA_CHECK(cudaGetLastError());); (112, CUDA, cudaFuncGetAttributes(&cuattr, slsum_bmod_inv_gpu_mrhs_nvshmem);); (181, CUDA, CUDA_CHECK(cudaStreamDestroy(stream[i]));)"}
{"predict": "(32, CUDA, __shared__ ReductionFloat sharedMemory[ kWorkgroupSize ];)", "label": "(101, CUDA, checkHIPErrors( ::cudaDeviceSynchronize() );)"}
{"predict": "(20, OpenMP, #pragma omp parallel num_threads(THREADS)); (29, OpenMP, #pragma omp barrier); (28, OpenMP, #pragma omp master); (17, OpenMP, #pragma omp target data map(to: x[0:N], y[0:N], z[0:N])); (29, OpenMP, #pragma omp target exit data map(from: x[0:N], y[0:N], z[0:N]))", "label": "(27, OpenMP, #pragma omp atomic update); (21, OpenMP, #pragma omp loop order(concurrent)); (29, OpenMP, num_threads = omp_get_num_threads();); (17, OpenMP, #pragma omp target parallel num_threads(THREADS) map(tofrom: x[0:N], num_threads, total_wait_errors) map(to: y[0:N], z[0:N]))"}
{"predict": "(9, OpenMP, #pragma omp target defaultmap(tofrom)); (22, OpenMP, #pragma omp target defaultmap(none:aggregate) defaultmap(none:scalar) map(argc)); (12, OpenMP, #pragma omp target defaultmap(tofrom,); (1, OpenMP, #pragma omp target defaultmap(); (30, OpenMP, #pragma omp target defaultmap(none:aggregate)); (3, OpenMP, #pragma omp target defaultmap()); (18, OpenMP, #pragma omp target defaultmap(none:aggregate) defaultmap(none:scalar) map(argc)); (6, OpenMP, #pragma omp target defaultmap(tofrom scalar)); (15, OpenMP, #pragma omp target defaultmap(none:aggregate) defaultmap(none:scalar) map(g)); (7, OpenMP, #pragma omp target defaultmap(none:aggregate) defaultmap(none:scalar) map(tofrom); (-3, OpenMP, #pragma omp target defaultmap(alloc)); (23, OpenMP, #pragma omp target defaultmap(none:aggregate) map(vla[:argc])); (-8, OpenMP, #pragma omp target defaultmap); (-5, OpenMP, #pragma omp target defaultmap()); (-10, OpenMP, #pragma omp target defaultmap()", "label": "(9, OpenMP, #pragma omp target defaultmap(tofrom:); (4, OpenMP, #pragma omp target defaultmap()); (25, OpenMP, #pragma omp target defaultmap(none:pointer)); (10, OpenMP, #pragma omp target defaultmap(tofrom,); (40, OpenMP, #pragma omp parallel); (-4, OpenMP, #pragma omp target defaultmap); (27, OpenMP, #pragma omp target defaultmap(none:pointer) defaultmap(none:scalar) defaultmap(none:aggregate) map(g) firstprivate(argc)); (8, OpenMP, #pragma omp target defaultmap(scalar:); (11, OpenMP, #pragma omp target defaultmap(tofrom: scalar) defaultmap(tofrom: scalar)); (20, OpenMP, #pragma omp target defaultmap(none:pointer) defaultmap(none:scalar) map(argc)); (1, OpenMP, #pragma omp target defaultmap(tofrom)); (6, OpenMP, #pragma omp target defaultmap(tofrom, scalar); (23, OpenMP, #pragma omp target defaultmap(none:aggregate)); (-10, OpenMP, #pragma omp target defaultmap(); (9, OpenMP, #pragma omp target defaultmap(tofrom : scalar) defaultmap(to: scalar)); (16, OpenMP, #pragma omp target defaultmap(none:pointer) defaultmap(none:scalar) map(g)); (9, OpenMP, #pragma omp target defaultmap(alloc: pointer) defaultmap(to: scalar) defaultmap(firstprivate: pointer)); (20, OpenMP, #pragma omp target defaultmap(none:aggregate) defaultmap(none:scalar) map(argc)); (-11, OpenMP, #pragma omp target defaultmap(tofrom)"}
{"predict": "(140, MPI, MPI_Finalize();); (62, MPI, MPI_Bcast(&iterations,    1, MPI_INT, root, MPI_COMM_WORLD);); (62, MPI, MPI_Bcast(&vector_length, 1, MPI_LONG, root, MPI_COMM_WORLD);); (17, MPI, MPI_Init(&argc,&argv);); (81, MPI, MPI_Barrier(MPI_COMM_WORLD);); (17, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (15, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)", "label": "(139, MPI, MPI_Finalize();); (63, MPI, MPI_Bcast(&iterations,    1, MPI_INT, root, MPI_COMM_WORLD);); (63, MPI, MPI_Bcast(&vector_length, 1, MPI_LONG, root, MPI_COMM_WORLD);); (17, MPI, MPI_Init(&argc,&argv);); (81, MPI, MPI_Barrier(MPI_COMM_WORLD);); (17, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (15, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(19, OpenMP, #pragma omp target teams distribute parallel for simd collapse (ST); (22, OpenMP, #pragma omp target teams distribute parallel for simd collapse (1))); (25, OpenMP, #pragma omp target teams distribute parallel for simd collapse ((ST > 0)? 1 + ST : 2)); (34, OpenMP, #pragma omp target teams distribute parallel for simd collapse (S)); (39, OpenMP, #pragma omp target teams distribute parallel for simd collapse (foobool(argc)), collapse (true), collapse (-5)); (50, OpenMP, #pragma omp target teams distribute parallel for simd collapse (N)); (52, OpenMP, #pragma omp target teams distribute parallel for simd collapse (2)); (0, OpenMP, #pragma omp target teams distribute parallel for simd collapse (); (4, OpenMP, #pragma omp target teams distribute parallel for simd collapse ()); (25, OpenMP, #pragma omp target teams distribute parallel for simd collapse (0)); (6, OpenMP, #pragma omp target teams distribute parallel for simd collapse (argc); (47, OpenMP, #pragma omp target teams distribute parallel for simd collapse (N)); (49, OpenMP, #pragma omp target teams distribute parallel for simd collapse (2)); (-11, OpenMP, #pragma omp target teams distribute parallel for simd collapse)", "label": "(25, OpenMP, #pragma omp target teams distribute parallel for simd collapse (1))); (60, OpenMP, #pragma omp target teams distribute parallel for simd collapse (2)); (4, OpenMP, #pragma omp target teams distribute parallel for simd collapse (); (54, OpenMP, #pragma omp target teams distribute parallel for simd collapse (N)); (36, OpenMP, #pragma omp target teams distribute parallel for simd collapse (S)); (24, OpenMP, #pragma omp target teams distribute parallel for simd collapse ((ST > 0)? 1 + ST : 2)); (-4, OpenMP, #pragma omp target teams distribute parallel for simd collapse); (9, OpenMP, #pragma omp target teams distribute parallel for simd collapse (argc); (28, OpenMP, #pragma omp target teams distribute parallel for simd collapse (foobool(argc)), collapse (true), collapse (-5)); (12, OpenMP, #pragma omp target teams distribute parallel for simd collapse (ST); (43, OpenMP, #pragma omp target teams distribute parallel for simd collapse (1)); (-1, OpenMP, #pragma omp target teams distribute parallel for simd collapse ())"}
{"predict": "ILING(123, OpenCL, err = clEnqueueCopyImage( queue, memobjs[0], memobjs[2], 0, srcPt, destPt, region, 0, NULL, &copyEvent );); (89, OpenCL, err |= clSetKernelArg( kernel[0], 0, sizeof( cl_mem ), (void *)&memobjs[0] );); (89, OpenCL, err |= clSetKernelArg( kernel[0], 1, sizeof( cl_mem ), (void *)&memobjs[1] );); (128, OpenCL, err = clWaitForEvents( 1, &copyEvent );); (86, OpenCL, err |= clSetKernelArg( kernel[0], 2, sizeof( cl_mem ), (void *)&memobjs[2] );); (93, OpenCL, err = clEnqueueNDRangeKernel(queue, kernel[0], 2, NULL, threads, NULL, 0, NULL, NULL);); (137, OpenCL, clReleaseEvent( copyEvent );); (64, OpenCL, err = clEnqueueWriteBuffer(queue, memobjs[1], false, 0, num_bytes, inptr, 0, NULL, NULL);); (58, OpenCL, clReleaseMemObject(memobjs[0]);); (128, OpenCL, err = clReleaseEvent( copyEvent );); (58, OpenCL, clReleaseMemObject(memobjs[1]);); (17, OpenCL, clReleaseMemObject( memobjs[2] );); (81, OpenCL, clReleaseProgram( program[0] );); (79, OpenCL, clReleaseKernel( kernel[0] );); (51, OpenCL, err |= clSetKernelArg( kernel[0], 3, sizeof( cl_mem ), (void *)&memobjs[2] );); (48, OpenCL, err |= clSetKernelArg( kernel[0], 2, sizeof( cl_mem ), (void *)&memobjs[1] );); (47, OpenCL, err |= clSetKernelArg( kernel[0], 1, sizeof( cl_mem ), (void *)&memobjs[0] );); (44, OpenCL, err = clSetKernelArg( kernel[0], 0, sizeof( cl_mem ), (void *)&memobjs[0] );); (49, OpenCL, err |= clSet", "label": "(85, OpenCL, clReleaseMemObject( memobjs[2] );); (140, OpenCL, clReleaseEvent(copyEvent);); (103, OpenCL, err = clEnqueueNDRangeKernel( queue, kernel[0], 2, NULL, threads, NULL, 0, NULL, NULL );); (72, OpenCL, err = clEnqueueWriteBuffer( queue, memobjs[1], true, 0, num_bytes, inptr, 0, NULL, NULL );); (87, OpenCL, err = clSetKernelArg( kernel[0], 0, sizeof( cl_mem ), (void *)&memobjs[1] );); (91, OpenCL, clReleaseProgram( program[0] );); (77, OpenCL, clReleaseMemObject( memobjs[0] );); (88, OpenCL, clReleaseKernel( kernel[0] );); (84, OpenCL, err |= clSetKernelArg( kernel[0], 1, sizeof( cl_mem ), (void *)&memobjs[0] );); (48, OpenCL, clReleaseMemObject(memobjs[0]);); (113, OpenCL, err = clEnqueueCopyImage( queue, memobjs[0], memobjs[2], srcPt, destPt, region, 0, NULL, &copyEvent );); (57, OpenCL, clReleaseMemObject(memobjs[1]);); (201, OpenCL, err = clEnqueueReadImage( queue, memobjs[2], true, origin, region2, 0, 0, dst, 0, NULL, NULL );); (125, OpenCL, err = clWaitForEvents( 1, &copyEvent );); (70, OpenCL, clReleaseMemObject( memobjs[1] );)"}
{"predict": "(13, CUDA, cudaEventRecord(stop);); (13, CUDA, cudaEventSynchronize(stop);); (9, CUDA, cudaMemcpy(d_data, h_data, SIZE * sizeof(float), cudaMemcpyHostToDevice);); (23, CUDA, cudaFree(d_data);); (4, CUDA, cudaEventCreate(&stop);); (11, CUDA, cudaEventElapsedTime(&milliseconds, start, stop);); (6, CUDA, cudaEventRecord(start);); (-2, CUDA, cudaEventCreate(&start);); (16, CUDA, cudaEventDestroy(stop);); (3, CUDA, cudaMemcpy(&d_data[0], h_data, SIZE * sizeof(float), cudaMemcpyHostToDevice);); (11, CUDA, cudaEventDestroy(start);); (-3, CUDA, cudaEventCreate(&d_data);)", "label": "(12, CUDA, cudaEventRecord(stop);); (12, CUDA, cudaEventSynchronize(stop);); (9, CUDA, cudaMemcpy(d_data, h_data, SIZE * sizeof(float), cudaMemcpyHostToDevice);); (7, CUDA, cudaEventRecord(start);); (-1, CUDA, cudaMalloc((void **)&d_data, SIZE * sizeof(float));); (10, CUDA, cudaEventElapsedTime(&milliseconds, start, stop);); (1, CUDA, cudaEventCreate(&stop);); (13, CUDA, cudaMemcpy(h_data, d_data, SIZE * sizeof(float), cudaMemcpyDeviceToHost);); (19, CUDA, cudaFree(d_data);); (-3, CUDA, cudaEventCreate(&start);)"}
{"predict": "(1, OpenMP, #pragma omp for); (9, OpenMP, #pragma omp tile sizes(5))", "label": "(6, OpenMP, #pragma omp tile sizes(5)); (0, OpenMP, #pragma omp for)"}
{"predict": "(142, MPI, MPI_Finalize();); (22, MPI, MPI_Init(&argc,&argv);); (85, MPI, MPI_Barrier(MPI_COMM_WORLD);); (22, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (139, MPI, MPI_Reduce(&local_reduce_time, &avgtime, 1, MPI_DOUBLE, MPI_SUM, root, MPI_COMM_WORLD);); (20, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)", "label": "(143, MPI, MPI_Finalize();); (67, MPI, MPI_Bcast(&iterations,    1, MPI_INT, root, MPI_COMM_WORLD);); (67, MPI, MPI_Bcast(&vector_length, 1, MPI_LONG, root, MPI_COMM_WORLD);); (19, MPI, MPI_Init(&argc,&argv);); (85, MPI, MPI_Barrier(MPI_COMM_WORLD);); (19, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (17, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(10, OpenMP, #pragma omp parallel for); (45, OpenMP, #pragma omp atomic)", "label": "(10, OpenMP, #pragma omp parallel for); (45, OpenMP, #pragma omp atomic)"}
{"predict": "(16, CUDA, tmp[t] = thrust::complex<T>(0.0);); (17, CUDA, thrust::complex<T> const* m_(m + batch * lda * mstride + t);); (21, CUDA, tmp[t] += log(piv_[ip] == (ip + 1));); (14, CUDA, int t0 = omp_get_thread_num();); (34, CUDA, tmp[t] = thrust::complex<T>(1.0, 0.0);); (12, CUDA, thrust::complex<T> LogOverlapFactor;); (13, CUDA, *det + batch = thrust::complex<T>(0.0);)", "label": "(20, CUDA, thrust::complex<T> const* m_ = m + batch * mstride;); (16, CUDA, tmp[t] = thrust::complex<T>(0.0);); (9, CUDA, __shared__ thrust::cuda_cub::core::uninitialized_array<thrust::complex<T>, 64> tmp;)"}
{"predict": "(19, CUDA, __shared__ T B_thread_block_tile[BLOCK_TILE_SIZE_K][BLOCK_TILE_SIZE_X];); (17, CUDA, __shared__ T A_thread_block_tile[BLOCK_TILE_SIZE_Y][BLOCK_TILE_SIZE_K];)", "label": "(19, CUDA, __shared__ T B_thread_block_tile[BLOCK_TILE_SIZE_K][BLOCK_TILE_SIZE_X];); (17, CUDA, __shared__ T A_thread_block_tile[BLOCK_TILE_SIZE_Y][BLOCK_TILE_SIZE_K];)"}
{"predict": "(108, MPI, MPI_Finalize();); (5, MPI, err = MPI_Init(&argc, &argv);); (8, MPI, err = MPI_Comm_size(MPI_COMM_WORLD, &numRanks);); (5, MPI, err = MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(109, MPI, MPI_Finalize();); (5, MPI, err = MPI_Init(&argc, &argv);); (8, MPI, err = MPI_Comm_size(MPI_COMM_WORLD, &numRanks);); (5, MPI, err = MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(29, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (72, CUDA, CHECK_CUDA(cudaFree(dX));); (43, CUDA, CHECK_CUDA(cudaMalloc(&dX, size * sizeof(int32_t)));); (29, CUDA, CHECK_HIEDNN(hiednnSetCudaStream(handle, 0));); (67, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (69, CUDA, CHECK_CUDA(cudaFree(dY));); (40, CUDA, CHECK_CUDA(cudaMalloc(&dY, size * sizeof(int32_t)));)", "label": "(29, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (72, CUDA, CHECK_CUDA(cudaFree(dX));); (43, CUDA, CHECK_CUDA(cudaMalloc(&dX, size * sizeof(int32_t)));); (29, CUDA, CHECK_HIEDNN(hiednnSetCudaStream(handle, 0));); (67, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (69, CUDA, CHECK_CUDA(cudaFree(dY));); (40, CUDA, CHECK_CUDA(cudaMalloc(&dY, size * sizeof(int32_t)));)"}
{"predict": "(29, CUDA, __shared__ float x_s[x_s_sz];); (27, CUDA, __shared__ float w_s[w_s_sz];)", "label": "(29, CUDA, __shared__ float x_s[x_s_sz];); (27, CUDA, __shared__ float w_s[w_s_sz];)"}
{"predict": "(16, SYCL, using namespace cl::sycl::access;); (4, SYCL, cl::sycl::queue queue;)", "label": "(16, SYCL, using namespace cl::sycl::access;); (4, SYCL, cl::sycl::queue queue;)"}
{"predict": "(34, OpenMP, #pragma omp distribute collapse (N)); (2, OpenMP, #pragma omp distribute collapse); (7, OpenMP, #pragma omp distribute collapse (ST); (10, OpenMP, #pragma omp distribute collapse (1))); (31, OpenMP, #pragma omp distribute collapse (2)); (20, OpenMP, #pragma omp distribute collapse (S)); (7, OpenMP, #pragma omp distribute collapse ((ST > 0)? 1 + ST : 2)); (0, OpenMP, #pragma omp distribute collapse ()); (11, OpenMP, #pragma omp distribute collapse (foobool(argc)), collapse (true), collapse (-5)); (-6, OpenMP, #pragma omp distribute collapse (); (13, OpenMP, #pragma omp distribute collapse (foobool(1) > 0? 1 : 2)); (6, OpenMP, #pragma omp distribute collapse (argc)", "label": "(36, OpenMP, #pragma omp distribute collapse (N)); (9, OpenMP, #pragma omp distribute collapse (argc); (36, OpenMP, #pragma omp distribute collapse (2)); (22, OpenMP, #pragma omp distribute collapse (S)); (2, OpenMP, #pragma omp distribute collapse ()); (-1, OpenMP, #pragma omp distribute collapse (); (9, OpenMP, #pragma omp distribute collapse (1))); (27, OpenMP, #pragma omp distribute collapse (1)); (9, OpenMP, #pragma omp distribute collapse ((ST > 0)? 1 + ST : 2)); (-7, OpenMP, #pragma omp distribute collapse); (3, OpenMP, #pragma omp distribute collapse (ST); (12, OpenMP, #pragma omp distribute collapse (foobool(argc)), collapse (true), collapse (-5))"}
{"predict": "(8, MPI, rc = MPI_Type_get_extent(type, &tmp_lb, &attr->child_type_extent);)", "label": "(8, MPI, rc = MPI_Type_get_extent(type, &tmp_lb, &attr->child_type_extent);)"}
{"predict": "(5, OpenMP, #pragma omp target teams distribute simd reduction(+: sivar))", "label": "(5, OpenMP, #pragma omp target teams distribute simd reduction(+: sivar))"}
{"predict": "(39, MPI, MPI_Bcast( rowind, nnz, mpi_int_t,  0, grid->comm );); (36, MPI, MPI_Bcast( &nnz,   1,   mpi_int_t,  0, grid->comm );); (50, MPI, MPI_Bcast( colptr,  n+1, mpi_int_t,  0, grid->comm );); (48, MPI, MPI_Bcast( rowind,  nnz, mpi_int_t,  0, grid->comm );); (39, MPI, MPI_Bcast( &m,   1,   mpi_int_t,  0, grid->comm );); (31, MPI, MPI_Bcast( &n,     1,   mpi_int_t,  0, grid->comm );); (29, MPI, MPI_Bcast( &m,     1,   mpi_int_t,  0, grid->comm );); (38, MPI, MPI_Bcast( &nnz, 1,   mpi_int_t,  0, grid->comm );); (32, MPI, MPI_Bcast( colptr, n+1, mpi_int_t,  0, grid->comm );); (29, MPI, MPI_Bcast( nzval,  nnz, SuperLU_MPI_DOUBLE_COMPLEX, 0, grid->comm );); (34, MPI, MPI_Bcast( &n,   1,   mpi_int_t,  0, grid->comm );); (39, MPI, MPI_Bcast( nzval,   nnz, SuperLU_MPI_DOUBLE_COMPLEX, 0, grid->comm );)", "label": "(50, MPI, MPI_Bcast( nzval,   nnz, SuperLU_MPI_DOUBLE_COMPLEX, 0, grid->comm );); (38, MPI, MPI_Bcast( rowind, nnz, mpi_int_t,  0, grid->comm );); (50, MPI, MPI_Bcast( colptr,  n+1, mpi_int_t,  0, grid->comm );); (48, MPI, MPI_Bcast( rowind,  nnz, mpi_int_t,  0, grid->comm );); (39, MPI, MPI_Bcast( &m,   1,   mpi_int_t,  0, grid->comm );); (31, MPI, MPI_Bcast( &n,     1,   mpi_int_t,  0, grid->comm );); (29, MPI, MPI_Bcast( &m,     1,   mpi_int_t,  0, grid->comm );); (38, MPI, MPI_Bcast( &nnz, 1,   mpi_int_t,  0, grid->comm );); (32, MPI, MPI_Bcast( colptr, n+1, mpi_int_t,  0, grid->comm );); (28, MPI, MPI_Bcast( &nnz,   1,   mpi_int_t,  0, grid->comm );); (28, MPI, MPI_Bcast( nzval,  nnz, SuperLU_MPI_DOUBLE_COMPLEX, 0, grid->comm );); (33, MPI, MPI_Bcast( &n,   1,   mpi_int_t,  0, grid->comm );)"}
{"predict": "(15, MPI, MPI_Recv(buff, count, type, 0, tag, comm, &s);); (23, MPI, MPI_Cancel(&r);); (12, MPI, MPI_Send(&buff[0], count, type, 1, tag, comm);); (4, MPI, MPI_Comm_rank(comm, &rank);); (19, MPI, MPI_Request_free(&r);)", "label": "(22, MPI, MPI_Request_free(&r);); (14, MPI, MPI_Recv_init(buff, count, type, 0, tag, comm, &r);); (2, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(3, TBB, tbb::flow::write_once_node<R> n(g);); (20, TBB, tbb::flow::make_edge( n, r[i] );); (53, TBB, tbb::flow::remove_edge( n, r[i] );); (-1, TBB, tbb::flow::graph g;)", "label": "(20, TBB, tbb::flow::make_edge( n, r[i] );); (54, TBB, tbb::flow::remove_edge( n, r[i] );); (1, TBB, tbb::flow::write_once_node<R> n(g);); (-1, TBB, tbb::flow::graph g;)"}
{"predict": "(26, OpenCL, retVal = clCreateImage2DFromBuffer(&compressedImage, compressedBuffer.get(), 0, &imageFormat, &imageDescriptor, nullptr, &retVal);); (63, OpenCL, clReleaseMemObject(compressedBuffer);); (61, OpenCL, clReleaseMemObject(compressedImage);)", "label": "(64, OpenCL, clReleaseMemObject(clCompressedImage);); (25, OpenCL, auto clCompressedImage = clCreateImage(context, CL_MEM_READ_WRITE, &imageFormat, &imageDescriptor, nullptr, &retVal);)"}
{"predict": "(2, CUDA, __shared__ float share_mem[InputPropSharedMemorySize];)", "label": "(7, CUDA, extern __shared__ float share_mem[];)"}
{"predict": "(7, OpenMP, #pragma omp parallel for)", "label": "(7, OpenMP, #pragma omp parallel for)"}
{"predict": "(20, CUDA, cudaSetDevice(deviceId);); (20, CUDA, cudaGetDeviceCount(&nDevices);)", "label": "(24, CUDA, cudaSetDevice(deviceId);); (20, CUDA, cudaGetDeviceCount(&nDevices);); (6, MPI, MPIManager mpi_manager(MPI_COMM_WORLD);)"}
{"predict": "(74, CUDA, cudaMalloc((void**) &d_diffData, m * c * sizeof(double));); (115, CUDA, cudaFree(d_diffData);); (78, CUDA, cudaMemcpy(d_diffData, diffData, m * c * sizeof(double), cudaMemcpyHostToDevice);); (58, CUDA, cudaDeviceSynchronize();); (54, CUDA, cudaMemcpy(result, d_softmaxData, m * c * sizeof(double), cudaMemcpyDeviceToHost);); (106, CUDA, cudaFree(d_gradData);); (106, CUDA, cudaFree(d_diffData);); (106, CUDA, cudaFree(d_sumData);); (14, CUDA, cudaMalloc((void**) &dsoftmaxData, m * c * sizeof(double));); (77, CUDA, cudaMemcpy(d_gradData, result_backward, m * c * sizeof(double), cudaMemcpyDeviceToHost);); (76, CUDA, cudaDeviceSynchronize();); (76, CUDA, cudaMemcpy(result_backward, d_gradData, m * c * sizeof(double), cudaMemcpyDeviceToHost);); (14, CUDA, cudaMemcpy(d_fcLayer, fcLayer, m * c * sizeof(double), cudaMemcpyHostToDevice);); (100, CUDA, cudaFree(d_srcData);); (64, CUDA, cudaMalloc((void**) &d_gradData, m * c * sizeof(double));); (98, CUDA, cudaFree(d_sumData);); (100, CUDA, cudaFree(d_gradData);); (65, CUDA, cudaFree(d_fcLayer);); (4, CUDA, cudaMalloc((void**) &d_fcLayer, m * c * sizeof(double));); (64, CUDA, cudaFree(dsoftmaxData);); (58, CUDA, cudaMemcpy(d_srcData, fcLayer, m * c * sizeof(double), cudaMemcpyHostToDevice);); (58, CUDA, cudaDeviceSynchronize();); (57, CUDA, cudaMemcpy(d_sumData, result, m * c * sizeof(double), cudaMemcpyDeviceToHost);); (58, CUDA, cudaMemcpy(d_gradData, result, m * c * sizeof(double), cudaMemcpyDeviceToHost);); (-4, CUDA, cudaMemcpy(d_sumData, fcLayer, m * c * sizeof(double), cudaMemcpyHostToDevice);); (-8, CUDA, cudaMalloc((void**) &d_srcData, m * c * sizeof(double));); (71, CUDA, cudaFree(d_srcData);)", "label": "(73, CUDA, cudaMalloc((void**) &d_gradData, m * c * sizeof(double));); (116, CUDA, cudaFree(d_fcLayer);); (118, CUDA, cudaFree(d_diffData);); (116, CUDA, cudaFree(d_gradData);); (12, CUDA, cudaMalloc((void**) &d_fcLayer, m * c * sizeof(double));); (17, CUDA, cudaMalloc((void**) &d_softmaxData, m * c * sizeof(double));); (43, CUDA, cudaDeviceSynchronize();); (11, CUDA, cudaMemcpy(d_fcLayer, fcLayer, m * c * sizeof(double), cudaMemcpyHostToDevice);); (72, CUDA, cudaMalloc((void**) &d_diffData, m * c * sizeof(double));); (109, CUDA, cudaFree(d_softmaxData);); (83, CUDA, cudaMemcpy(result_backward, d_gradData, m * c * sizeof(double), cudaMemcpyDeviceToHost);); (44, CUDA, cudaMemcpy(result, d_softmaxData, m * c * sizeof(double), cudaMemcpyDeviceToHost);); (69, CUDA, cudaMemcpy(d_diffData, diffData, m * c * sizeof(double), cudaMemcpyHostToDevice);)"}
{"predict": "(11, OpenMP, #pragma omp parallel for); (77, OpenMP, #pragma omp atomic)", "label": "(11, OpenMP, #pragma omp parallel for); (77, OpenMP, #pragma omp atomic)"}
{"predict": "(189, MPI, MPI_Allreduce(&tmp_error, error_code, 1, MPI_INT, MPI_MAX, fd->comm);); (47, MPI, MPI_Comm_size(fd->comm, &nprocs);); (147, MPI, MPI_Allgather(&end_offset, 1, ADIO_OFFSET, end_offsets, 1, ADIO_OFFSET, fd->comm);); (73, MPI, MPI_Allgather(&start_offset, 1, ADIO_OFFSET, st_offsets, 1, ADIO_OFFSET, fd->comm);); (45, MPI, MPI_Comm_rank(fd->comm, &myrank);); (182, MPI, MPI_Type_size_x(datatype, &size);)", "label": "(173, MPI, MPI_Allreduce(&tmp_error, error_code, 1, MPI_INT, MPI_MAX, fd->comm);); (47, MPI, MPI_Comm_size(fd->comm, &nprocs);); (75, MPI, MPI_Allgather(&end_offset, 1, ADIO_OFFSET, end_offsets, 1, ADIO_OFFSET, fd->comm);); (73, MPI, MPI_Allgather(&start_offset, 1, ADIO_OFFSET, st_offsets, 1, ADIO_OFFSET, fd->comm);); (45, MPI, MPI_Comm_rank(fd->comm, &myrank);); (192, MPI, MPI_Type_size_x(datatype, &size);)"}
{"predict": "(6, MPI, MPI_Reduce( &SCT->commVolFactor, &cvolFactor, 1, MPI_DOUBLE, MPI_SUM, 0, grid3d->comm );); (6, MPI, MPI_Reduce( &SCT->commVolRed, &cvolRed, 1, MPI_DOUBLE, MPI_SUM, 0, grid3d->comm );)", "label": "(4, MPI, MPI_Reduce( &SCT->commVolFactor, &cvolFactor,  1, MPI_DOUBLE, MPI_SUM, 0, grid3d->comm );); (5, MPI, MPI_Reduce( &SCT->commVolRed, &cvolRed,  1, MPI_DOUBLE, MPI_SUM, 0, grid3d->comm );)"}
{"predict": "(2, MPI, MPI_Init(&argc, &argv);); (52, CUDA, CUDA_CHECK(cudaStreamCreate(&stream));); (113, MPI, MPI_Finalize();); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (109, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (5, CUDA, CUDA_CHECK(cudaGetDeviceCount(&ndevices));); (5, CUDA, CUDA_CHECK(cudaSetDevice(rank % ndevices));); (80, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));)", "label": "(10, CUDA, CUDA_CHECK(cudaGetDeviceCount(&ndevices));); (54, CUDA, CUDA_CHECK(cudaStreamCreate(&stream));); (88, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (8, CUDA, CUDA_CHECK(cudaSetDevice(rank % ndevices));); (-2, MPI, MPI_Init(&argc, &argv);); (113, MPI, MPI_Finalize();); (1, MPI, MPI_Comm_size(mpi_comm, &size);); (103, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (78, CUDA, CUDA_CHECK(cudaGetLastError());); (-3, MPI, MPI_Comm_rank(mpi_comm, &rank);)"}
{"predict": "(10, TBB, tbb::task_group g;)", "label": "(5, TBB, if(tbb::is_current_task_group_canceling()) return;)"}
{"predict": "(14, CUDA, CUDASimulation cudaSimulation(model);)", "label": "(14, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(4, TBB, tbb::tick_count t0;); (5, TBB, tbb::tick_count t1;); (2, TBB, if( trial==0 ) t0 = tbb::tick_count::now();); (5, TBB, REMARK(\"trial %i, worktime %g, delta %g\\n\", trial, worktime, delta);)", "label": "(4, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (5, TBB, tbb::tick_count t1 = tbb::tick_count::now();)"}
{"predict": "(11, MPI, MPI_Init(&argc, &argv);); (44, MPI, MPI_Finalize();); (10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (35, MPI, MPI_Recv(recvbuf.data(), block_size, MPI_INT, 0, rank, MPI_COMM_WORLD, &status);); (26, MPI, MPI_Send(&sendbuf[i*block_size], block_size, MPI_INT, i, i, MPI_COMM_WORLD);); (19, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)", "label": "(11, MPI, MPI_Init(&argc, &argv);); (44, MPI, MPI_Finalize();); (10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (35, MPI, MPI_Recv(recvbuf.data(), block_size, MPI_INT, 0, rank, MPI_COMM_WORLD, &status);); (26, MPI, MPI_Send(&sendbuf[i*block_size], block_size, MPI_INT, i, i, MPI_COMM_WORLD);); (20, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(23, MPI, MPI_Reduce(gFrstCost, gFrstCostAcc, 1, MPI_DOUBLE, MPI_SUM, 0, grid3d->zscp.comm);)", "label": "(23, MPI, MPI_Reduce(gFrstCost, gFrstCostAcc, numForests, MPI_DOUBLE, MPI_SUM, 0, grid3d->zscp.comm);)"}
{"predict": "(17, MPI, MPI_Finalize();); (2, MPI, MPI_Init(&argc,&argv);)", "label": "(17, MPI, MPI_Finalize();); (2, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(11, CUDA, __shared__ float u_[_N_];); (10, CUDA, __shared__ float r[_N_], k[_N_], v[_N_], w_[_N_], gy[_N_];)", "label": "(11, CUDA, __shared__ float u_[_N_];); (11, CUDA, __shared__ float r[_N_], k[_N_], v[_N_], w_[_N_], gy[_N_];)"}
{"predict": "(12, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(12, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);)", "label": "(7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);)"}
{"predict": "(2, TBB, tbb::enumerable_thread_specific<double> test_locals;); (5, TBB, tbb::task_scheduler_init init(tbb::task_scheduler_init::deferred);)", "label": "(3, TBB, tbb::enumerable_thread_specific<double> test_locals;); (13, TBB, REMARK(\"%d threads, work ratio %d per cent, time %g\", nThreads, csWorkRatio, (tbb::tick_count::now() - t0).seconds());); (0, TBB, tbb::task_scheduler_init init(tbb::task_scheduler_init::deferred);); (5, TBB, tbb::tick_count t0;); (6, TBB, t0 = tbb::tick_count::now();)"}
{"predict": "(43, CUDA, CSC(cudaGetLastError(), STATUS_FAILURE);)", "label": "(43, CUDA, CSC(cudaGetLastError(), STATUS_FAILURE);)"}
{"predict": "(3, MPI, MPI_Init(&argc, &argv);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (104, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(2, MPI, MPI_Init(&argc, &argv);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (105, MPI, MPI_Finalize();); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(14, TBB, tbb::flow::make_edge(outer_node/*inner_node2*/, inner_node3);); (12, TBB, tbb::flow::make_edge(outer_node/*inner_node1*/, inner_node2);); (5, TBB, tbb::flow::input_node<int> input(g, snode_body(4));); (1, TBB, tbb::flow::function_node<int> output(g, tbb::flow::unlimited, output_body());); (3, TBB, tbb::flow::function_node<int> outer_node(g, tbb::flow::unlimited, outer_body());); (8, TBB, tbb::flow::make_edge(input, tbb::flow::input_port<0>(outer_node)/*inner_node1*/);); (3, TBB, tbb::flow::graph g;)", "label": "(38, TBB, CHECK_MESSAGE( (tbb::flow::output_port<0>(outer_node).try_get(out) == output.try_get(out)), \"output from outer_node does not match output from graph\");); (13, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<1>(outer_node) == &tbb::flow::input_port<1>(inner_node1)), \"input port 1 of inner_node1 not bound to input port 1 in outer_node\");); (0, TBB, tbb::flow::composite_node<std::tuple<int, int>, std::tuple<int> > outer_node(g);); (8, TBB, outer_node.set_external_ports(base_type::input_ports_type(tbb::flow::input_port<0>(inner_node1), tbb::flow::input_port<1>(inner_node1)), base_type::output_ports_type(tbb::flow::output_port<0>(inner_node3)));); (-1, TBB, typedef tbb::flow::composite_node<std::tuple<int, int>, std::tuple<int> > base_type;); (18, TBB, tbb::flow::make_edge(inner_node2, tbb::flow::input_port<0>(inner_node3));); (18, TBB, tbb::flow::make_edge(inner_node2, tbb::flow::input_port<1>(inner_node3));); (13, TBB, tbb::flow::make_edge(inner_node1, tbb::flow::input_port<0>(inner_node2));); (-7, TBB, tbb::flow::graph g;); (6, TBB, CHECK_MESSAGE( (&tbb::flow::output_port<0>(outer_node) == &tbb::flow::output_port<0>(inner_node3)), \"output port 0 of inner_node3 not bound to output port 0 in outer_node\");); (11, TBB, tbb::flow::make_edge(inner_node1, tbb::flow::input_port<1>(inner_node2));); (2, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<0>(outer_node) == &tbb::flow::input_port<0>(inner_node1)), \"input port 0 of inner_node1 not bound to input port 0 in outer_node\");); (-8, TBB, tbb::flow::broadcast_node<int> input(g);); (-8, TBB, tbb::flow::queue_node<int> output(g);)"}
{"predict": "(16, CUDA, CudaFile aCudaFile( aReader, GPUSPHOptions(), aTemplDir );)", "label": "(16, CUDA, CudaFile aCudaFile( aReader, GPUSPHOptions(), aTemplDir );)"}
{"predict": "(24, OpenMP, #pragma omp target teams distribute simd safelen (foobool(1) > 0? 1 : 2)); (29, OpenMP, #pragma omp target teams distribute simd safelen (S1)); (47, OpenMP, #pragma omp target teams distribute simd safelen(safelen(tmain<int, char, -1, -2>(argc, argv)); (-1, OpenMP, #pragma omp target teams distribute simd safelen); (2, OpenMP, #pragma omp target teams distribute simd safelen (); (27, OpenMP, #pragma omp target teams distribute simd safelen (foobool(argc)), safelen (true), safelen (-5)); (6, OpenMP, #pragma omp target teams distribute simd safelen (4); (0, OpenMP, #pragma omp target teams distribute simd safelen ()); (9, OpenMP, #pragma omp target teams distribute simd safelen (2+2))); (5, OpenMP, #pragma omp target teams distribute simd safelen ()); (43, OpenMP, #pragma omp target teams distribute simd safelen(safelen(tmain<int, char, -1, -2>(argc, argv)); (35, OpenMP, #pragma omp target teams distribute simd safelen(safelen(tmain<int, char, 12, 4>(argc, argv)); (26, OpenMP, #pragma omp target teams distribute simd safelen (argc argc)); (3, OpenMP, #pragma omp target teams distribute simd safelen (foobool(1) > 0? 0 : 2)); (7, OpenMP, #pragma omp target teams distribute simd safelen (6))", "label": "(29, OpenMP, #pragma omp target teams distribute simd safelen (foobool(argc)), safelen (true), safelen (-5)); (46, OpenMP, #pragma omp target teams distribute simd safelen(safelen(tmain<int, char, -1, -2>(argc, argv)); (-1, OpenMP, #pragma omp target teams distribute simd safelen); (19, OpenMP, #pragma omp target teams distribute simd safelen (foobool(1) > 0? 1 : 2)); (9, OpenMP, #pragma omp target teams distribute simd safelen (4); (0, OpenMP, #pragma omp target teams distribute simd safelen (); (27, OpenMP, #pragma omp target teams distribute simd safelen (S1)); (10, OpenMP, #pragma omp target teams distribute simd safelen (2+2))); (1, OpenMP, #pragma omp target teams distribute simd safelen ())"}
{"predict": "(11, OpenMP, #pragma omp parallel for)", "label": "(11, OpenMP, #pragma omp parallel for reduction(+ : entropy))"}
{"predict": "(30, MPI, MPI_Bcast( work, lwork, MPI_FLOAT, pkk, grid->comm );); (27, MPI, MPI_Bcast( &lwork, 1, MPI_INT, pkk, grid->comm );)", "label": "(28, MPI, MPI_Bcast( work, lwork, MPI_FLOAT, pkk, grid->comm );); (29, MPI, MPI_Bcast( work, diag_len[p], MPI_FLOAT, pkk, grid->comm );)"}
{"predict": "(6, MPI, MPI_Comm_dup(MPI_COMM_SELF, &lwgrp_comm_self);)", "label": "(6, MPI, MPI_Comm_dup(MPI_COMM_SELF, &lwgrp_comm_self);)"}
{"predict": "(4, OpenMP, #pragma omp target teams distribute parallel for private(g, g1, sivar))", "label": "(4, OpenMP, #pragma omp target teams distribute parallel for private(g, g1, sivar))"}
{"predict": "(27, CUDA, cudaMalloc((void**)&dataC, sizeof(cufftDoubleComplex) * dims[0] * dims[1] * dims[2]);); (55, CUDA, cudaDeviceSynchronize();); (27, CUDA, cudaMemcpy(dataC, inputC, sizeof(cufftDoubleComplex) * dims[0] * dims[1] * dims[2], cudaMemcpyHostToDevice);); (80, CUDA, cudaFree(dataC);); (4, CUDA, cudaSetDevice(device_id);)", "label": "(27, CUDA, cudaMalloc((void**)&dataC, sizeof(cufftDoubleComplex) * dims[0] * dims[1] * dims[2]);); (55, CUDA, cudaDeviceSynchronize();); (27, CUDA, cudaMemcpy(dataC, inputC, sizeof(cufftDoubleComplex) * dims[0] * dims[1] * dims[2], cudaMemcpyHostToDevice);); (78, CUDA, cudaFree(dataC);); (1, CUDA, cudaSetDevice(device_id);)"}
{"predict": "(8, CUDA, cudaFree(prevFree);); (5, CUDA, cudaMemGetInfo(&free, &total);); (7, CUDA, cudaMalloc(&newlyAllocated, 1024 * 1024);)", "label": "(6, CUDA, cudaCheck(cudaMemGetInfo(&free, &total));)"}
{"predict": "(5, CUDA, cudaMalloc(&curandStates, sizeof(curandState_t) * batchSize);); (48, CUDA, cudaFree(curandStates);)", "label": "(48, CUDA, cudaFree(curandStates);); (4, CUDA, cudaMalloc(&curandStates, sizeof(curandState_t) * 2 * batchSize);)"}
{"predict": "(16, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (23, MPI, MPI_Error_string(ierr, str, &slen);)", "label": "(29, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_ARE_FATAL);); (23, MPI, MPI_Error_string(ierr, str, &slen);); (14, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(40, TBB, tbb::flow::make_edge(src, gamma);); (40, TBB, tbb::flow::make_edge(gamma, tint);); (40, TBB, tbb::flow::make_edge(tint, write);); (0, TBB, tbb::flow::graph g;)", "label": "(40, TBB, tbb::flow::make_edge(src, gamma);); (40, TBB, tbb::flow::make_edge(gamma, tint);); (40, TBB, tbb::flow::make_edge(tint, write);); (0, TBB, tbb::flow::graph g;)"}
{"predict": "(3, CUDA, __shared__ unsigned sidxs[nTPB];)", "label": "(3, CUDA, __shared__ unsigned sidxs[nTPB];)"}
{"predict": "(5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &np);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (16, MPI, MPI_Allreduce(&mysize, &totalsize, 1, MPI_UINT64_T, MPI_SUM, MPI_COMM_WORLD);); (18, MPI, MPI_Allreduce(&mysizeint, &out.size(), 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (23, MPI, MPI_Allgather(&me, 1, MPI_INT, &counts[0], 1, MPI_INT, MPI_COMM_WORLD);); (23, MPI, MPI_Allgather(&np, 1, MPI_INT, &displs[0], 1, MPI_INT, MPI_COMM_WORLD);); (-1, MPI, MPI_Init(&argc, &argv);); (38, MPI, MPI_Finalize();); (20, MPI, MPI_Allgather(&mysize, 1, MPI_UINT64_T, &counts[1], 1, MPI_INT, MPI_COMM_WORLD);); (10, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(19, MPI, MPI_Reduce(&mysize, &totalsize, 1, MPI_INT64_T, MPI_SUM, 0, MPI_COMM_WORLD);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&me);); (0, MPI, MPI_Init(&argc, &argv);); (23, MPI, MPI_Gather(&mysizeint, 1, MPI_INT, &(counts[0]), 1, MPI_INT, 0, MPI_COMM_WORLD);); (37, MPI, MPI_Finalize();); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD,&np);); (29, MPI, MPI_Gatherv(in.c_str(), mysizeint, MPI_CHAR, &(out[0]), &(counts[0]), &(displs[0]), MPI_CHAR, 0, MPI_COMM_WORLD);)"}
{"predict": "(10, OpenMP, #pragma omp teams distribute parallel for order(); (15, OpenMP, #pragma omp teams distribute parallel for order(none); (19, OpenMP, #pragma omp teams distribute parallel for order(concurrent)); (6, OpenMP, #pragma omp teams distribute parallel for order); (20, OpenMP, #pragma omp teams distribute parallel for order(concurrent); (7, OpenMP, #pragma omp teams distribute parallel for order()); (-4, OpenMP, #pragma omp target); (13, OpenMP, #pragma omp teams distribute parallel for order(concurrent); (-5, OpenMP, #pragma omp teams distribute parallel for order)", "label": "(18, OpenMP, #pragma omp teams distribute parallel for order(concurrent); (1, OpenMP, #pragma omp teams distribute parallel for order); (20, OpenMP, #pragma omp teams distribute parallel for order(concurrent)); (-2, OpenMP, #pragma omp target); (10, OpenMP, #pragma omp teams distribute parallel for order(none); (5, OpenMP, #pragma omp teams distribute parallel for order()"}
{"predict": "(13, CUDA, fprintf(stderr, \"CUDA kernel failed : %s\\n\", cudaGetErrorString(err));); (10, CUDA, auto err = cudaGetLastError();)", "label": "(13, CUDA, fprintf(stderr, \"CUDA kernel failed : %s\\n\", cudaGetErrorString(err));); (10, CUDA, auto err = cudaGetLastError();)"}
{"predict": "(79, OpenMP, john_omp_autotune();)", "label": "(14, OpenMP, john_omp_fallback(argv);); (18, MPI, mpi_setup(argc, argv);); (78, OpenMP, john_omp_maybe_adjust_or_fallback(argv);); (6, OpenMP, john_omp_init();)"}
{"predict": "(40, MPI, MPI_Comm_create(MPI_COMM_WORLD, MPI_UNDEFINED, &newcomm);); (22, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (27, MPI, MPI_Barrier(MPI_COMM_WORLD);); (27, MPI, startTime = MPI_Wtime();); (49, MPI, MPI_Comm_free(&newcomm);); (20, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (44, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (17, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (18, MPI, MPI_Comm_set_errhandler(newcomm, MPI_ERRORS_RETURN);)", "label": "(44, MPI, MPI_Comm_split(MPI_COMM_WORLD, MPI_UNDEFINED, rank, &newcomm);); (21, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (40, MPI, MPI_Comm_free(&newcomm);); (21, MPI, startTime = MPI_Wtime();); (24, MPI, double rate = MPI_Wtime() - startTime;); (36, MPI, MPI_Comm_split(MPI_COMM_WORLD, 1, rank, &newcomm);)"}
{"predict": "(57, CUDA, flamegpu::CUDASimulation cudaSimulation(model);)", "label": "(56, CUDA, flamegpu::CUDASimulation  cudaSimulation(model, argc, argv);)"}
{"predict": "(19, TBB, using YourTable2 = oneapi::tbb::concurrent_hash_map<MyKey,MyData2,YourHashCompare>;); (17, TBB, using YourTable1 = oneapi::tbb::concurrent_hash_map<MyKey,MyData2,YourHashCompare>;); (1, TBB, using MyTable = oneapi::tbb::concurrent_hash_map<MyKey,MyData2>;)", "label": "(36, TBB, typedef tbb::concurrent_hash_map<MyKey,MyData2,YourHashCompare> YourTable2;); (34, TBB, typedef tbb::concurrent_hash_map<MyKey,MyData2,YourHashCompare,MyAllocator> YourTable1;)"}
{"predict": "(33, MPI, MPI_Error_string(mpirc, mpierrstr, &mpierrlen);); (109, MPI, MPI_Type_get_extent(groups->dt, &lb_group, &extent_group);); (20, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (18, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (23, MPI, MPI_Error_string(mpirc, mpierrstr, &mpierrlen);); (91, MPI, mpirc = MPI_File_read_at(fh, 0, users->buf, user_buf, user_buf_size, users->dt, &status);); (78, MPI, mpirc = MPI_File_set_view(fh, disp, MPI_BYTE, MPI_BYTE, datarep, MPI_INFO_NULL);); (125, MPI, mpirc = MPI_File_read_at(fh, 0, groups->buf, group_buf, group_buf_size, groups->dt, &status);); (22, MPI, mpirc = MPI_File_read_at(fh, 0, header, header, 6, MPI_UINT64_T, &status);); (29, MPI, mpirc = MPI_File_read_at(fh, 0, header_packed, header_packed, 6, MPI_UINT64_T, &status);); (19, MPI, MPI_Bcast(header, 6, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (81, MPI, MPI_Bcast(users->buf, bufsize_user, users->dt, 0, MPI_COMM_WORLD);); (88, MPI, MPI_Bcast(&all_count, 1, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (30, MPI, MPI_Bcast(&ranks, 1, MPI_INT, 0, MPI_COMM_WORLD);); (28, MPI, MPI_Bcast(&rank, 1, MPI_INT, 0, MPI_COMM_WORLD);); (105, MPI, MPI_Bcast(&groups->count, 1, MPI_INT, 0, MPI_COMM_WORLD);); (105, MPI, MPI_Bcast(&groups->chars, 1, MPI_INT, 0, MPI_COMM_WORLD);); (56, MPI, MPI_Exscan(&count, &offset, 1, MPI_UINT64_T, MPI_SUM, MPI_COMM_WORLD);); (79, MPI, MPI_Bcast(&user_buf_size, 1, MPI_INT, 0, MPI_COMM_WORLD););", "label": "(28, MPI, MPI_Error_string(mpirc, mpierrstr, &mpierrlen);); (142, MPI, MPI_Bcast(groups->buf, (int)groups->count, groups->dt, 0, MPI_COMM_WORLD);); (96, MPI, mpirc = MPI_File_read_at(fh, 0, user_buf, user_buf_size, MPI_BYTE, &status);); (132, MPI, mpirc = MPI_File_read_at(fh, 0, group_buf, group_buf_size, MPI_BYTE, &status);); (102, MPI, MPI_Bcast(users->buf, (int)users->count, users->dt, 0, MPI_COMM_WORLD);); (15, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (28, MPI, mpirc = MPI_File_read_at(fh, 0, header_packed, header_size, MPI_BYTE, &status);); (41, MPI, MPI_Bcast(header, 6, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (60, MPI, MPI_Exscan(&count, &offset, 1, MPI_UINT64_T, MPI_SUM, MPI_COMM_WORLD);); (12, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (16, MPI, int mpirc = MPI_File_set_view(fh, disp, MPI_BYTE, MPI_BYTE, datarep, MPI_INFO_NULL);); (69, MPI, MPI_Type_get_extent(users->dt, &lb_user, &extent_user);); (105, MPI, MPI_Type_get_extent(groups->dt, &lb_group, &extent_group);); (190, MPI, mpirc = MPI_File_read_at(fh, read_offset, buf, read_size, MPI_BYTE, &status);); (74, MPI, mpirc = MPI_File_set_view(fh, disp, MPI_BYTE, MPI_BYTE, datarep, MPI_INFO_NULL);); (159, MPI, MPI_Allreduce(&iters, &all_iters, 1, MPI_UINT64_T, MPI_MAX, MPI_COMM_WORLD);)"}
{"predict": "(22, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (22, MPI, MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);)", "label": "(22, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (22, MPI, MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);)"}
{"predict": "(12, CUDA, CHECK(cudaFree(m_output_boxes_device));); (12, CUDA, CHECK(cudaFree(m_output_scores_device));); (6, CUDA, CHECK(cudaFree(m_max_val_device));); (0, CUDA, CHECK(cudaFree(m_input_resize_device));); (10, CUDA, CHECK(cudaFree(m_output_classes_device));); (4, CUDA, CHECK(cudaFree(m_min_val_device));); (-2, CUDA, CHECK(cudaFree(m_input_src_device));); (0, CUDA, CHECK(cudaFree(m_input_rgb_device));); (1, CUDA, CHECK(cudaFree(m_input_hwc_device));); (-4, CUDA, CHECK(cudaFree(m_input_norm_device));)", "label": "(8, CUDA, CHECK(cudaFree(m_output_boxes_device));); (9, CUDA, CHECK(cudaFree(m_output_classes_device));); (2, CUDA, CHECK(cudaFree(m_input_resize_device));); (0, CUDA, CHECK(cudaFree(m_input_src_device));); (3, CUDA, CHECK(cudaFree(m_output_num_device));); (0, CUDA, CHECK(cudaFree(m_input_rgb_device));); (3, CUDA, CHECK(cudaFree(m_output_scores_device));)"}
{"predict": "(20, CUDA, __shared__ bool kept_bboxinfo_flag[TSIZE];)", "label": "(19, CUDA, extern __shared__ bool kept_bboxinfo_flag[];)"}
{"predict": "(9, MPI, MPI_Comm_rank(comm_world, &rank_world);)", "label": "(9, MPI, MPI_Comm_rank(comm_world, &rank_world);)"}
{"predict": "(39, MPI, MPI_Request_free(&reqs[i]);); (39, MPI, MPI_Status_free(&stats[i]);)", "label": "(39, MPI, MPI_Request_free(&reqs[i]);)"}
{"predict": "(11, MPI, MPI_Comm_size(comm, &npes);)", "label": "(11, MPI, MPI_Comm_size(comm, &npes);)"}
{"predict": "(23, CUDA, cudaMemcpy(h_C, d_C, DSIZE*sizeof(float), cudaMemcpyDeviceToHost);); (15, CUDA, cudaMemcpy(d_A, h_A, DSIZE*sizeof(float), cudaMemcpyHostToDevice);); (15, CUDA, cudaMemcpy(d_B, h_B, DSIZE*sizeof(float), cudaMemcpyHostToDevice);); (8, CUDA, cudaMalloc(&d_A, DSIZE*sizeof(float));); (10, CUDA, cudaCheckErrors(\"cudaMalloc failure\");); (6, CUDA, cudaMalloc(&d_B, DSIZE*sizeof(float));); (24, CUDA, cudaCheckErrors(\"kernel execution failure or cudaMemcpy H2D failure\");); (8, CUDA, cudaCheckErrors(\"cudaMalloc failure\");); (15, CUDA, cudaCheckErrors(\"cudaMemcpy H2D failure\");); (18, CUDA, cudaCheckErrors(\"vadd kernel launch failure\");)", "label": "(15, CUDA, cudaMemcpy(d_A, h_A, DSIZE*sizeof(float), cudaMemcpyHostToDevice);); (20, CUDA, cudaCheckErrors(\"kernel launch failure\");); (8, CUDA, cudaMalloc(&d_A, DSIZE*sizeof(float));)"}
{"predict": "(34, MPI, MPI_Finalize();); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &comm_size);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (30, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(35, MPI, MPI_Finalize();); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &comm_size);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (8, MPI, vtkh::SetMPICommHandle(MPI_Comm_c2f(MPI_COMM_WORLD));)"}
{"predict": "(19, CUDA, result = cudaMalloc((void**)&devPtrA, M*N*sizeof(*a));); (30, CUDA, cudaFree(devPtrA);)", "label": "(19, CUDA, result = cudaMalloc((void**)&devPtrA, M*N*sizeof(*a));); (30, CUDA, cudaFree(devPtrA);)"}
{"predict": "(6, CUDA, __shared__ uint s_permutations[160];)", "label": "(6, CUDA, __shared__ uint s_permutations[160];)"}
{"predict": "(110, MPI, MPI_Allreduce( &s, &berr[j], 1, MPI_FLOAT, MPI_MAX, grid->comm );); (148, MPI, MPI_Allreduce( &s, &berr[j], 1, MPI_FLOAT, MPI_MIN, grid->comm );)", "label": "(110, MPI, MPI_Allreduce( &s, &berr[j], 1, MPI_FLOAT, MPI_MAX, grid->comm );); (111, MPI, MPI_Bcast( &berr[j], 1, MPI_FLOAT, 0,  grid3d->zscp.comm);)"}
{"predict": "(20, TBB, tbb::detail::d1::segmented_iterator<nested_vec, const T> my_si(my_vec);); (62, TBB, tbb::detail::d1::segmented_iterator<nested_vec, T> my_csi(my_vec);)", "label": "(62, TBB, tbb::internal::segmented_iterator<nested_vec, const T> my_csi(my_vec);); (19, TBB, tbb::internal::segmented_iterator<nested_vec, T> my_si(my_vec);)"}
{"predict": "(4, CUDA, __shared__ uchar4 s_img[256];)", "label": "(4, CUDA, __shared__ uint4 s_img[TPB];)"}
{"predict": "(47, MPI, MPI_Open_port(port_info, port);); (42, MPI, MPI_Comm_free(&pgroup_new);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (51, MPI, MPI_Recv(port, PORT_SIZE, MPI_CHAR, members[0], 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (37, MPI, MPI_Intercomm_merge(pgroup_new, 0 /* LOW */, &pgroup);); (39, MPI, MPI_Comm_free(&pgroup_old);); (78, MPI, MPI_Barrier(MPI_COMM_WORLD);); (27, MPI, MPI_Send(port, PORT_SIZE, MPI_CHAR, members[0], 0, MPI_COMM_WORLD);); (54, MPI, MPI_Comm_create(MPI_COMM_WORLD, merged, &pgroup_new);); (13, MPI, MPI_Info_create(&port_info);); (54, MPI, MPI_Info_free(&port_info);); (30, MPI, MPI_Comm_free(&pgroup);); (19, MPI, MPI_Comm_dup(MPI_COMM_SELF, &pgroup);)", "label": "(49, MPI, MPI_Comm_accept(port, MPI_INFO_NULL, 0, pgroup, &pgroup_new);); (53, MPI, MPI_Comm_free(&pgroup_old);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (65, MPI, MPI_Comm_connect(port, MPI_INFO_NULL, 0, MPI_COMM_SELF, &pgroup_new);); (43, MPI, MPI_Send(port, PORT_SIZE, MPI_CHAR, members[i], 0, MPI_COMM_WORLD);); (36, MPI, MPI_Open_port(port_info, port);); (45, MPI, MPI_Comm_free(&pgroup_new);); (49, MPI, MPI_Close_port(port);); (67, MPI, MPI_Comm_connect(port, MPI_INFO_NULL, 0, pgroup, &pgroup_new);)"}
{"predict": "(13, MPI, MPI_Allgather(ranks.begin(), ranks.end(), MPI_INT, worldRanks.begin(), ranks.end(), MPI_INT, comm);); (7, MPI, MPI_Comm_group(comm, &group);); (14, MPI, MPI_Group_free(&group);); (2, MPI, MPI_Comm_size(comm, &groupSize);); (4, MPI, MPI_Comm_group(MPI_COMM_WORLD, &worldGroup);); (12, MPI, MPI_Group_free(&worldGroup);)", "label": "(10, MPI, MPICHECK(MPI_Group_size(group, &groupSize));); (6, MPI, MPICHECK(MPI_Comm_group(comm, &group));); (13, MPI, MPICHECK(MPI_Group_translate_ranks(group, groupSize, ranks.data(), worldGroup, worldRanks.data()));); (13, MPI, MPICHECK(MPI_Group_free(&group));); (2, MPI, MPICHECK(MPI_Comm_group(MPI_COMM_WORLD, &worldGroup));); (12, MPI, MPICHECK(MPI_Group_free(&worldGroup));)"}
{"predict": "(29, OpenMP, #pragma omp taskloop simd private(a, b)); (53, OpenMP, #pragma omp parallel shared(i)); (17, OpenMP, #pragma omp taskloop simd private(argc,); (4, OpenMP, #pragma omp taskloop simd private); (37, OpenMP, #pragma omp taskloop simd private(h)); (1, OpenMP, #pragma omp taskloop simd private(); (29, OpenMP, #pragma omp taskloop simd private(e, g, z)); (49, OpenMP, #pragma omp taskloop simd private(j)); (22, OpenMP, #pragma omp taskloop simd private(argv[1])); (30, OpenMP, #pragma omp taskloop simd shared(i)); (40, OpenMP, #pragma omp parallel); (7, OpenMP, #pragma omp taskloop simd private(argc > 0? argv[1] : argv[2])); (46, OpenMP, #pragma omp taskloop simd private(i)); (8, OpenMP, #pragma omp taskloop simd private(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (-1, OpenMP, #pragma omp taskloop simd private()); (19, OpenMP, #pragma omp taskloop simd private(S1)); (36, OpenMP, #pragma omp parallel shared(i)); (27, OpenMP, #pragma omp taskloop simd private(i)); (30, OpenMP, #pragma omp taskloop simd private(i)); (37, OpenMP, #pragma omp taskloop simd private(i)); (15, OpenMP, #pragma omp taskloop simd private(foo:argc)); (20, OpenMP, #pragma omp taskloop simd private(i)); (-10, OpenMP, #pragma omp taskloop simd private(argc)", "label": "(33, OpenMP, #pragma omp taskloop simd private(argv[1])); (38, OpenMP, #pragma omp taskloop simd private(h)); (13, OpenMP, #pragma omp taskloop simd private(argc); (52, OpenMP, #pragma omp parallel shared(i)); (52, OpenMP, #pragma omp parallel private(i)); (1, OpenMP, #pragma omp taskloop simd private); (15, OpenMP, #pragma omp taskloop simd private(argc > 0? argv[1] : argv[2])); (38, OpenMP, #pragma omp parallel); (41, OpenMP, #pragma omp taskloop simd private(i)); (21, OpenMP, #pragma omp taskloop simd private(a, b)); (32, OpenMP, #pragma omp taskloop simd shared(i)); (13, OpenMP, #pragma omp taskloop simd private(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (15, OpenMP, #pragma omp taskloop simd private(S1)); (5, OpenMP, #pragma omp taskloop simd private(argc,); (-5, OpenMP, #pragma omp taskloop simd private(); (-3, OpenMP, #pragma omp taskloop simd private()); (20, OpenMP, #pragma omp taskloop simd private(e, g, z)); (40, OpenMP, #pragma omp taskloop simd private(j))"}
{"predict": "(7, OpenMP, std::vector<BabelFlow::Payload> comp_outputs( 1 );)", "label": "(6, OpenMP, std::vector<BabelFlow::Payload> comp_outputs( 1 );)"}
{"predict": "(1, MPI, MPI_Greetings();)", "label": "(43, MPI, printf(\" MPI library version      : %s\\n\", mpi_version());)"}
{"predict": "(16, CUDA, checkCudaErrors(cudaFree(workload->d_data));); (4, CUDA, checkCudaErrors(cudaSetDevice(workload->cudaDeviceID));); (14, CUDA, checkCudaErrors(cudaFreeHost(workload->h_data));)", "label": "(15, CUDA, checkCudaErrors(cudaFree(workload->d_data));); (15, CUDA, checkCudaErrors(cudaFreeHost(workload->h_data));); (3, CUDA, checkCudaErrors(cudaSetDevice(workload->cudaDeviceID));); (14, CUDA, checkCudaErrors(cudaStreamDestroy(workload->stream));)"}
{"predict": "(9, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)", "label": "(9, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(12, CUDA, int d_num_out = thrust::count(data.begin(), data.end(), false);); (29, CUDA, thrust::fill(data.begin() + num_selected, data.end(), 0);); (20, CUDA, thrust::shuffle(data.begin(), data.end(), g);); (10, CUDA, int d_flags = thrust::reduce(data.begin(), data.end(), 0, thrust::plus<int>());); (22, CUDA, thrust::fill(flags.begin(), flags.end(), 0);); (0, CUDA, thrust::host_vector<int> data(num_items);); (38, CUDA, thrust::shuffle(flags.begin(), flags.end(), g);); (-1, CUDA, thrust::host_vector<int> flags(num_items);); (16, CUDA, int d_data = thrust::reduce(data.begin(), data.end(), 0, thrust::plus<int>());); (36, CUDA, thrust::default_random_engine g;); (-3, CUDA, thrust::host_vector<int> num_out(1);)", "label": "(10, CUDA, int *d_num_out = thrust::raw_pointer_cast(num_out.data());); (10, CUDA, int *d_data = thrust::raw_pointer_cast(data.data());); (24, CUDA, d_tmp_storage = thrust::raw_pointer_cast(tmp_storage.data());); (32, CUDA, thrust::fill(flags.begin(), flags.begin() + num_selected, true);); (3, CUDA, thrust::device_vector<bool> flags(num_items);); (20, CUDA, thrust::device_vector<char> tmp_storage(tmp_storage_size);); (0, CUDA, thrust::device_vector<int> data(num_items);); (29, CUDA, thrust::fill(flags.begin() + num_selected, flags.end(), false);); (26, CUDA, thrust::sequence(data.begin(), data.end());); (28, CUDA, thrust::shuffle(flags.begin(), flags.end(), g);); (-5, CUDA, thrust::device_vector<int> num_out(1);); (36, CUDA, cudaMemcpy(&h_num_out, d_num_out, sizeof(int), cudaMemcpyDeviceToHost);); (0, CUDA, bool *d_flags = thrust::raw_pointer_cast(flags.data());)"}
{"predict": "(26, OpenCL, status = clSetKernelArg(kernel, 1, sizeof(cl_mem), &result_buf);)", "label": "(12, OpenCL, status = clSetKernelArg(kernel, 0, sizeof(cl_mem), &test_buf);); (20, OpenCL, status = clSetKernelArg(kernel, 0, sizeof(cl_mem), NULL);)"}
{"predict": "(79, CUDA, checkCudaErrors(cudaFree(d_partial_sums));); (14, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, cuda_device));); (8, CUDA, cuda_device = findCudaDevice(argc, (const char **)argv);); (62, CUDA, checkCudaErrors(cudaEventRecord(stop, 0));); (82, CUDA, checkCudaErrors(cudaFreeHost(h_partial_sums));); (69, CUDA, checkCudaErrors(cudaEventElapsedTime(&inc, start, stop));); (62, CUDA, checkCudaErrors(cudaEventSynchronize(stop));); (77, CUDA, checkCudaErrors(cudaFreeHost(h_result));); (68, CUDA, checkCudaErrors(cudaEventDestroy(start));); (73, CUDA, checkCudaErrors(cudaFree(d_data));); (18, CUDA, checkCudaErrors(cudaMalloc((void **)&d_partial_sums, partial_sz));); (62, CUDA, checkCudaErrors(cudaEventDestroy(stop));); (64, CUDA, checkCudaErrors(cudaMemcpy(h_result, d_data, sz, cudaMemcpyDeviceToHost));); (15, CUDA, checkCudaErrors(cudaMemcpy(d_data, h_data, sz, cudaMemcpyHostToDevice));); (10, CUDA, checkCudaErrors(cudaMalloc((void **)&d_data, sz));); (56, CUDA, checkCudaErrors(cudaEventRecord(start, 0));); (16, CUDA, checkCudaErrors(cudaMemset(d_partial_sums, 0, partial_sz));); (-4, CUDA, checkCudaErrors(cudaGetDevice(&cuda_device));); (62, CUDA, checkCudaErrors(cudaFreeHost(h_data));); (6, CUDA, checkCudaErrors(cudaMallocHost(&h_result, sz));); (52, CUDA, checkCudaErrors(cudaEventCreate(&stop));); (4, CUDA, checkCudaErrors(cudaMallocHost(&h_partial_sums, partial_sz));); (49, CUDA, checkCudaErrors(cudaEventCreate(&start));); (4, CUDA, checkCudaErrors(cudaMallocHost(&h_data, sz));)", "label": "(76, CUDA, checkCudaErrors(cudaEventRecord(stop, 0));); (15, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, cuda_device));); (94, CUDA, checkCudaErrors(cudaFree(d_partial_sums));); (57, CUDA, checkCudaErrors(cudaEventCreate(&start));); (57, CUDA, checkCudaErrors(cudaEventCreate(&stop));); (73, CUDA, checkCudaErrors(cudaEventElapsedTime(&inc, start, stop));); (66, CUDA, checkCudaErrors(cudaEventRecord(start, 0));); (70, CUDA, checkCudaErrors(cudaEventSynchronize(stop));); (86, CUDA, checkCudaErrors(cudaFreeHost(h_partial_sums));); (84, CUDA, checkCudaErrors(cudaFreeHost(h_result));); (55, CUDA, checkCudaErrors(cudaMalloc((void **)&d_data, sz));); (84, CUDA, checkCudaErrors(cudaFree(d_data));); (18, CUDA, checkCudaErrors(cudaMallocHost((void **)&h_result, sizeof(int)*n_elements));); (79, CUDA, checkCudaErrors(cudaFreeHost(h_data));); (56, CUDA, checkCudaErrors(cudaMemcpy(d_data, h_data, sz, cudaMemcpyHostToDevice));); (-4, CUDA, cuda_device = findCudaDevice(argc, (const char **)argv);); (65, CUDA, checkCudaErrors(cudaMemcpy(h_result, d_data, sz, cudaMemcpyDeviceToHost));); (50, CUDA, checkCudaErrors(cudaMemset(d_partial_sums, 0, partial_sz));); (11, CUDA, checkCudaErrors(cudaMallocHost((void **)&h_data, sizeof(int)*n_elements));); (-5, CUDA, checkCudaErrors(cudaGetDevice(&cuda_device));); (46, CUDA, checkCudaErrors(cudaMalloc((void **)&d_partial_sums, partial_sz));); (48, CUDA, checkCudaErrors(cudaMallocHost((void **)&h_partial_sums, partial_sz));)"}
{"predict": "(5, CUDA, RAFT_CUDA_TRY(cudaStreamSynchronize(stream));)", "label": "(6, CUDA, RAFT_CUDA_TRY(cudaStreamSynchronize(stream));)"}
{"predict": "(11, OpenMP, #pragma omp target teams distribute parallel for simd if (foobool(argc)), if (true)); (6, OpenMP, #pragma omp target teams distribute parallel for simd if (argc > 0? argv[1] : argv[2])); (17, OpenMP, #pragma omp target teams distribute parallel for simd if(target : argc) if (simd:argc)); (0, OpenMP, #pragma omp target teams distribute parallel for simd if ()); (1, OpenMP, #pragma omp target teams distribute parallel for simd if (argc); (2, OpenMP, #pragma omp target teams distribute parallel for simd if (argc))); (7, OpenMP, #pragma omp target teams distribute parallel for simd if(target : argc); (12, OpenMP, #pragma omp target teams distribute parallel for simd if(target : argc) if (argc)); (3, OpenMP, #pragma omp target teams distribute parallel for simd if (S1)); (8, OpenMP, #pragma omp target teams distribute parallel for simd if(target : argc) if (for:argc)); (-2, OpenMP, #pragma omp target teams distribute parallel for simd if (argc argc)); (-11, OpenMP, #pragma omp target teams distribute parallel for simd if)", "label": "(1, OpenMP, #pragma omp target teams defaultmap); (2, OpenMP, #pragma omp target teams defaultmap(); (9, OpenMP, #pragma omp target teams defaultmap(tofrom)); (12, OpenMP, #pragma omp target teams defaultmap(tofrom,); (3, OpenMP, #pragma omp target teams defaultmap(tofrom); (4, OpenMP, #pragma omp target teams defaultmap (tofrom:); (13, OpenMP, #pragma omp target teams defaultmap(tofrom, scalar); (-2, OpenMP, #pragma omp target teams defaultmap()); (5, OpenMP, #pragma omp target teams defaultmap(tofrom scalar)); (8, OpenMP, #pragma omp target teams defaultmap (scalar:)"}
{"predict": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(19, MPI, MPI_Comm_size(d->comm, &ranks_comm);); (28, MPI, MPI_Comm_rank(parent_comm, &parent_rank);); (28, MPI, MPI_Allgather(&parent_rank, 1, MPI_INT, ranklist, 1, MPI_INT, d->comm);)", "label": "(19, MPI, MPI_Comm_size(d->comm, &ranks_comm);); (28, MPI, MPI_Comm_rank(parent_comm, &parent_rank);); (28, MPI, MPI_Allgather(&parent_rank, 1, MPI_INT, ranklist, 1, MPI_INT, d->comm);)"}
{"predict": "(27, CUDA, CHECK(cudaMalloc((void**)&d_y, M));); (27, CUDA, CHECK(cudaMalloc((void**)&d_z, M));); (49, CUDA, CHECK(cudaFree(d_y));); (49, CUDA, CHECK(cudaFree(d_z));); (26, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice));); (37, CUDA, CHECK(cudaMemcpy(h_z, d_z, M, cudaMemcpyDeviceToHost));); (22, CUDA, CHECK(cudaMalloc((void**)&d_x, M));); (44, CUDA, CHECK(cudaFree(d_x));); (25, CUDA, CHECK(cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice));)", "label": "(52, CUDA, CHECK(cudaFree(d_y));); (30, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice));); (51, CUDA, CHECK(cudaFree(d_z));); (39, CUDA, CHECK(cudaMemcpy(h_z, d_z, M, cudaMemcpyDeviceToHost));); (22, CUDA, CHECK(cudaMalloc((void**)&d_x, M));); (22, CUDA, CHECK(cudaMalloc((void**)&d_y, M));); (45, CUDA, CHECK(cudaFree(d_x));); (21, CUDA, CHECK(cudaMalloc((void**)&d_z, M));); (30, CUDA, CHECK(cudaGetLastError());); (30, CUDA, CHECK(cudaDeviceSynchronize());); (22, CUDA, CHECK(cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice));)"}
{"predict": "(32, MPI, MPI_Send(lnzval, len2, MPI_DOUBLE, 0, jb, grid3d->zscp.comm);); (36, MPI, MPI_Recv(lnzval, len2, MPI_DOUBLE, layer, jb, grid3d->zscp.comm, &status);)", "label": "(32, MPI, MPI_Send(lnzval, len2, MPI_DOUBLE, 0, jb, grid3d->zscp.comm);); (36, MPI, MPI_Recv(lnzval, len2, MPI_DOUBLE, layer, jb, grid3d->zscp.comm, &status);)"}
{"predict": "(76, OpenMP, #define OP_GOMP_CRITICAL_CPT(TYPE, OP, FLAG))", "label": "(76, OpenMP, #define OP_GOMP_CRITICAL_CPT(TYPE, OP, FLAG))"}
{"predict": "(142, OpenMP, comp_val += src.get_f32_elem(src_off) * dst.get_f32_elem(dst_off);); (140, OpenMP, comp_val -= src.get_f32_elem(src_off) * dst.get_f32_elem(dst_off);); (143, OpenMP, comp_val += dst.get_f32_elem(dst_off) * src.get_f32_elem(src_off);); (140, OpenMP, comp_val -= dst.get_f32_elem(dst_off) * src.get_f32_elem(src_off);); (143, OpenMP, comp_val += src.get_f32_elem(src_off) * dst.get_f32_elem(dst_off);); (140, OpenMP, comp_val -= src.get_f32_elem(src_off) * dst.get_f32_elem(dst_off);); (141, OpenMP, comp_val += dst.get_f32_elem(dst_off) * src.get_f32_elem(src_off);); (140, OpenMP, comp_val -= dst.get_f32_elem(dst_off) * src.get_f32_elem(src_off);); (143, OpenMP, comp_val += src.get_f32_elem(src_off) * dst.get(140, OpenMP, comp_val += dst.get_f32_elem(dst_off) * src.get_f32_elem(src_off);); (138, OpenMP, comp_val -= dst.get_f32_elem(dst_off) * src.get_f32_elem(src_off);); (143, OpenMP, comp_val += dst.get_f32_elem(dst_off) * src.get(f32_elem(src_off);); (138, OpenMP, comp_val += src.get_f32_elem(src_off) * dst.get_f32_elem(dst_off);); (143, OpenMP, comp_val += dst.get(f32_elem(dst_off);); (138, OpenMP, comp_val -= src.get_f32_elem(src_off) * dst.get_f32_elem(dst_off);); (143, OpenMP, comp_val += src.get(f32_elem(src_off);); (134, OpenMP, comp_val += dst.get_f32_elem(dst_off) * src.get_f32_elem(dst_off);); (142, OpenMP, comp_val += dst.get(f32_elem(dst_off);); (133, OpenMP, comp_val -= dst.get_f32_elem(dst_off) * src.get_f32_elem(dst_off);); (138, OpenMP, comp_val += src.get(f32_elem(src_off);); (143, OpenMP, comp_val += dst.get(f32_elem(dst", "label": "(101, OpenMP, dims_t comp_dims(ndims, 1);)"}
{"predict": "(18, OpenMP, #pragma omp target enter data map(to:k[:4])); (11, OpenMP, #pragma omp target enter data map(to:k[:4])); (20, OpenMP, #pragma omp target enter data map(to:k[:4], j)); (9, OpenMP, #pragma omp target enter data map(to:k[:4])); (0, OpenMP, #pragma omp target enter data map(to:k)); (11, OpenMP, #pragma omp target enter data map(to:k[:4] j)); (12, OpenMP, #pragma omp target enter data map(to:k[:2][:2])); (17, OpenMP, #pragma omp target enter data map(to:k[:4], j, k[:4])); (12, OpenMP, #pragma omp target enter data map(to:k[:2][:2] j)); (13, OpenMP, #pragma omp target enter data map(to:k[:2][:2], j)); (14, OpenMP, #pragma omp target enter data map(to:k[:2][:2] j, k[:2][:2] j)); (-4, OpenMP, #pragma omp target enter data map(to:k[); (-5, OpenMP, #pragma omp target enter data map(to:k(); (-2, OpenMP, #pragma omp target enter data map(to:k)); (3, OpenMP, #pragma omp target enter data map(to:k[:4]; (-6, OpenMP, #pragma omp target enter data map(to:k()); (-3, OpenMP, #pragma omp target enter data map(to:k[:); (3, OpenMP, #pragma omp target enter data map(to:k[:4 4])); (-11, OpenMP, #pragma omp target enter data map(to:k)", "label": "(24, OpenMP, #pragma omp target parallel device (3.14)); (9, OpenMP, #pragma omp target parallel device (argc))); (20, OpenMP, #pragma omp target parallel device (-10u)); (5, OpenMP, #pragma omp target parallel device (argc); (12, OpenMP, #pragma omp target parallel device (k), device (argc+1)); (-3, OpenMP, #pragma omp target parallel device); (6, OpenMP, #pragma omp target parallel device (argc > 0? argv[1] : argv[2])); (11, OpenMP, #pragma omp target parallel device (S1)); (12, OpenMP, #pragma omp target parallel device (-2)); (-3, OpenMP, #pragma omp target parallel device ()); (4, OpenMP, #pragma omp target parallel device (argc + argc)); (-7, OpenMP, #pragma omp target parallel device ()"}
{"predict": "(2, CUDA, __shared__ float cache[threadsPerBlock];)", "label": "(1, CUDA, __shared__ float cache[threadsPerBlock];)"}
{"predict": "(34, CUDA, cudaFree( dptrPosMass );); (34, CUDA, cudaFree( dptrForce );)", "label": "(33, CUDA, cudaFree( dptrPosMass );); (33, CUDA, cudaFree( dptrForce );)"}
{"predict": "(62, OpenMP, double stop = omp_get_wtime();); (69, CUDA, CUDA_RT_CALL(cudaFree(a));); (18, CUDA, CUDA_RT_CALL(cudaDeviceSynchronize());); (10, CUDA, CUDA_RT_CALL(cudaMemset(a, 0, nx * ny * sizeof(float)));); (6, CUDA, CUDA_RT_CALL(cudaMalloc(&a, nx * ny * sizeof(float)));); (62, CUDA, CUDA_RT_CALL(cudaFree(l2_norm_d));); (13, CUDA, CUDA_RT_CALL(cudaGetLastError());); (32, CUDA, CUDA_RT_CALL(cudaMemset(l2_norm_d, 0, sizeof(float)));); (36, CUDA, CUDA_RT_CALL(cudaMemcpy(l2_norm_h, l2_norm_d, sizeof(float), cudaMemcpyDeviceToHost));); (5, CUDA, CUDA_RT_CALL(cudaMemset(a_new, 0, nx * ny * sizeof(float)));); (14, CUDA, CUDA_RT_CALL(cudaMallocHost(&l2_norm_h, sizeof(float)));); (54, CUDA, CUDA_RT_CALL(cudaMemcpy(a_ref_h, a, nx * ny * sizeof(float), cudaMemcpyDeviceToHost));); (58, CUDA, CUDA_RT_CALL(cudaFree(a_new));); (-2, CUDA, CUDA_RT_CALL(cudaMalloc(&a_new, nx * ny * sizeof(float)));); (22, OpenMP, double start = omp_get_wtime();); (7, CUDA, CUDA_RT_CALL(cudaMalloc(&l2_norm_d, sizeof(float)));); (51, CUDA, CUDA_RT_CALL(cudaFreeHost(l2_norm_h));)", "label": "(63, OpenMP, double stop = omp_get_wtime();); (70, CUDA, CUDA_RT_CALL(cudaFree(a));); (18, CUDA, CUDA_RT_CALL(cudaDeviceSynchronize());); (10, CUDA, CUDA_RT_CALL(cudaMemset(a, 0, nx * ny * sizeof(float)));); (6, CUDA, CUDA_RT_CALL(cudaMalloc(&a, nx * ny * sizeof(float)));); (63, CUDA, CUDA_RT_CALL(cudaFree(l2_norm_d));); (13, CUDA, CUDA_RT_CALL(cudaGetLastError());); (32, CUDA, CUDA_RT_CALL(cudaMemset(l2_norm_d, 0, sizeof(float)));); (36, CUDA, CUDA_RT_CALL(cudaMemcpy(l2_norm_h, l2_norm_d, sizeof(float), cudaMemcpyDeviceToHost));); (5, CUDA, CUDA_RT_CALL(cudaMemset(a_new, 0, nx * ny * sizeof(float)));); (14, CUDA, CUDA_RT_CALL(cudaMallocHost(&l2_norm_h, sizeof(float)));); (54, CUDA, CUDA_RT_CALL(cudaMemcpy(a_ref_h, a, nx * ny * sizeof(float), cudaMemcpyDeviceToHost));); (58, CUDA, CUDA_RT_CALL(cudaFree(a_new));); (-2, CUDA, CUDA_RT_CALL(cudaMalloc(&a_new, nx * ny * sizeof(float)));); (22, OpenMP, double start = omp_get_wtime();); (8, CUDA, CUDA_RT_CALL(cudaMalloc(&l2_norm_d, sizeof(float)));); (51, CUDA, CUDA_RT_CALL(cudaFreeHost(l2_norm_h));)"}
{"predict": "(54, MPI, errval = MPI_Recv(NULL, 0, MPI_INT, src, 100, comm, MPI_STATUS_IGNORE);); (40, MPI, errval = MPI_Recv(NULL, 0, MPI_INT, src, 10, comm, MPI_STATUS_IGNORE);); (18, MPI, MPI_Comm_size(comm, &size);); (51, MPI, MPI_Ssend(NULL, 0, MPI_INT, dest, 100, comm);); (62, MPI, errval = MPI_Waitsome(2, r, s);); (18, MPI, MPI_Irecv(b2, 10, MPI_INT, src, 10, comm, &r[1]);); (16, MPI, MPI_Irecv(b1, 10, MPI_INT, src, 0, comm, &r[0]);); (87, MPI, MPI_Ssend(NULL, 0, MPI_INT, dest, 10, comm);); (50, MPI, MPI_Ssend(NULL, 0, MPI_INT, dest, 100, comm);); (14, MPI, MPI_Comm_set_errhandler(comm, MPI_ERRORS_RETURN);); (43, MPI, MPI_Ssend(NULL, 0, MPI_INT, dest, 0, comm);); (85, MPI, MPI_Ssend(NULL, 0, MPI_INT, dest, 10, comm);); (43, MPI, MPI_Ssend(NULL, 0, MPI_INT, dest, 10, comm);); (49, MPI, MPI_Ssend(NULL, 0, MPI_INT, dest, 100, comm);); (15, MPI, errval = MPI_Irecv(NULL, 0, MPI_INT, src, 100, comm, &r[1]);); (57, MPI, MPI_Error_class(errval, &errclass);); (10, MPI, MPI_Comm_rank(comm, &rank);); (51, MPI, MPI_Error_string(errval, &errstr, &errval);); (44, MPI, MPI_Ssend(NULL, 0, MPI_INT, dest, 10, comm);); (8, MPI, errval = MPI_Irecv(NULL, 0, MPI_INT, src, 0, comm, &r[0]);); (14, MPI, errval = MPI_Irecv(b2, 10, MPI_INT, src, 100, comm, &r[1]););", "label": "(20, MPI, MPI_Comm_size(comm, &size);); (64, MPI, MPI_Error_class(errval, &errclass);); (95, MPI, MPI_Ssend(NULL, 0, MPI_INT, dest, 10, comm);); (39, MPI, errval = MPI_Recv(NULL, 0, MPI_INT, src, 100, comm, MPI_STATUS_IGNORE);); (85, MPI, MPI_Ssend(NULL, 0, MPI_INT, dest, 100, comm);); (59, MPI, errval = MPI_Waitsome(2, r, &outcount, indices, s);); (87, MPI, MPI_Send(b1, 10, MPI_INT, dest, 0, comm);); (25, MPI, errval = MPI_Irecv(b2, 10, MPI_INT, src, 10, comm, &r[1]);); (44, MPI, errval = MPI_Recv(NULL, 0, MPI_INT, src, 10, comm, MPI_STATUS_IGNORE);); (16, MPI, MPI_Comm_set_errhandler(comm, MPI_ERRORS_RETURN);); (16, MPI, errval = MPI_Irecv(b1, 10, MPI_INT, src, 0, comm, &r[0]);); (8, MPI, MPI_Comm_rank(comm, &rank);); (82, MPI, MPI_Send(b2, 11, MPI_INT, dest, 10, comm);)"}
{"predict": "(9, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:sum) reduction(+:gsum)); (2, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:sum))", "label": "(3, OpenMP, #pragma omp parallel for reduction(+:sum)); (8, OpenMP, #pragma omp target teams distribute parallel for reduction(+:gsum))"}
{"predict": "(20, MPI, MPI_Type_vector(n, 1, stride, MPI_DOUBLE, &vec);); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (32, MPI, MPI_Start(&reqs[root]);); (18, MPI, MPI_Type_commit(&vec);); (45, MPI, MPI_Type_free(&vec);); (29, MPI, MPI_Wait(&reqs[root], MPI_STATUS_IGNORE);); (27, MPI, MPI_Request_free(&reqs[root]);); (18, MPI, MPI_Info_create(&info);); (43, MPI, MPI_Info_free(&info);)", "label": "(35, MPI, MPI_Start(&reqs[root]);); (20, MPI, MPI_Type_vector(n, 1, stride, MPI_DOUBLE, &vec);); (11, MPI, MPI_Info_create(&info);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (32, MPI, MPI_Wait(&reqs[root], MPI_STATUS_IGNORE);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (48, MPI, MPI_Type_free(&vec);); (15, MPI, MPI_Type_commit(&vec);); (39, MPI, MPI_Request_free(&reqs[root]);); (44, MPI, MPI_Info_free(&info);)"}
{"predict": "( \"(171, TBB, CHECK( rtl1.status() == tbb::runtime_loader::ec_bad_ver );); (97, TBB, tbb::task_scheduler_init init( 1 );); (136, TBB, CHECK_EXCEPTION( eid_invalid_multiple_scheduling, tbb::invalid_multiple_scheduling );); (40, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_no_lib );); (8, TBB, tbb::runtime_loader rtl( tbb::runtime_loader::em_status );); (158, TBB, tbb::runtime_loader rtl1( tbb::runtime_loader::em_status );); (158, TBB, CHECK( rtl0.status() == tbb::runtime_loader::ec_ok );); (126, TBB, CHECK_EXCEPTION( eid_bad_last_alloc,              tbb::bad_last_alloc              );); (153, TBB, tbb::runtime_loader rtl0( tbb::runtime_loader::em_status );); (130, TBB, CHECK_EXCEPTION( eid_improper_lock,               tbb::improper_lock               );); (113, TBB, tbb::internal::throw_exception( tbb::internal::id );); (84, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_bad_call );); (96, TBB, tbb::internal::throw_exception( tbb::internal::exception_id( id ) );); (39, TBB, tbb::runtime_loader rtl( tbb::runtime_loader::em_throw );); (8, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_ok );); (38, TBB, CHECK( code == tbb::runtime_loader::ec_ok );); (146, TBB, CHECK( rtl1.status() == tbb::runtime_loader::ec_ok );); (8, TBB, CHECK( code == tbb::runtime_loader::ec_bad_arg );); (152, TBB, CHECK( rtl1.status() == tbb::runtime_loader::ec_bad_ver );); (38, TBB, tbb::runtime_loader::error_code code = tbb::runtime_loader::ec_ok;); (7, TBB, CHECK( code == tbb::runtime_loader::ec_bad_arg );); (39, TBB, CHECK( code == tbb::runtime_loader::ec_no_lib );); (2, TBB, CHECK", "label": "(172, TBB, CHECK( rtl1.status() == tbb::runtime_loader::ec_bad_ver );); (97, TBB, tbb::task_scheduler_init init( 1 );); (136, TBB, CHECK_EXCEPTION( eid_invalid_multiple_scheduling, tbb::invalid_multiple_scheduling );); (40, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_no_lib );); (8, TBB, tbb::runtime_loader rtl( tbb::runtime_loader::em_status );); (158, TBB, tbb::runtime_loader rtl1( tbb::runtime_loader::em_status );); (158, TBB, CHECK( rtl0.status() == tbb::runtime_loader::ec_ok );); (126, TBB, CHECK_EXCEPTION( eid_bad_last_alloc,              tbb::bad_last_alloc              );); (154, TBB, tbb::runtime_loader rtl0( tbb::runtime_loader::em_status );); (130, TBB, CHECK_EXCEPTION( eid_improper_lock,               tbb::improper_lock               );); (113, TBB, tbb::internal::throw_exception( tbb::internal::id );); (84, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_bad_call );); (96, TBB, tbb::internal::throw_exception( tbb::internal::exception_id( id ) );); (39, TBB, tbb::runtime_loader rtl( tbb::runtime_loader::em_throw );); (133, TBB, tbb::internal::handle_perror( EAGAIN, \"apple\" );); (-1, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_ok );); (37, TBB, tbb::runtime_loader::error_code code = tbb::runtime_loader::ec_ok;); (8, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_bad_arg );); (147, TBB, CHECK( rtl1.status() == tbb::runtime_loader::ec_ok );); (46, TBB, CHECK( code == tbb::runtime_loader::ec_no_lib );); (117, TBB, CHECK_EXCEPTION( eid_missing_wait,                tbb::missing_wait                );); (121, TBB, CHECK_EXCEPTION( eid_user_abort,                  tbb::user_abort                  );); (37, TBB, CHECK( code == tbb::runtime_loader::ec_bad_arg );)"}
{"predict": "(29, OpenMP, #pragma omp target teams map(alloc: gty1_real,gty1_imag,gty2_real,gty2_imag, u1_real, u1_imag, u_real,u_imag))", "label": "(31, OpenMP, #pragma omp teams distribute collapse(2)); (47, OpenMP, #pragma omp parallel for collapse(2)); (34, OpenMP, #pragma omp parallel for)"}
{"predict": "(22, OpenMP, #pragma omp distribute parallel for proc_bind(master); (6, OpenMP, #pragma omp distribute parallel for proc_bind); (40, OpenMP, #pragma omp distribute parallel for proc_bind(spread)); (31, OpenMP, #pragma omp distribute parallel for proc_bind(x)); (34, OpenMP, #pragma omp parallel proc_bind(close)); (7, OpenMP, #pragma omp distribute parallel for proc_bind()); (12, OpenMP, #pragma omp distribute parallel for proc_bind()); (17, OpenMP, #pragma omp distribute parallel for proc_bind(close), proc_bind(spread)); (-4, OpenMP, #pragma omp target); (36, OpenMP, #pragma omp parallel proc_bind(close)); (21, OpenMP, #pragma omp distribute parallel for proc_bind(master)); (-5, OpenMP, #pragma omp teams)", "label": "(20, OpenMP, #pragma omp distribute parallel for proc_bind(master); (4, OpenMP, #pragma omp distribute parallel for proc_bind); (13, OpenMP, #pragma omp distribute parallel for proc_bind()); (7, OpenMP, #pragma omp distribute parallel for proc_bind(); (26, OpenMP, #pragma omp distribute parallel for proc_bind(x)); (38, OpenMP, #pragma omp distribute parallel for proc_bind(spread)); (19, OpenMP, #pragma omp distribute parallel for proc_bind(close), proc_bind(spread)); (-4, OpenMP, #pragma omp target); (32, OpenMP, #pragma omp parallel proc_bind(close)); (27, OpenMP, #pragma omp distribute parallel for proc_bind(master)); (-6, OpenMP, #pragma omp teams)"}
{"predict": "(33, MPI, MPI_Bcast(&rc, 1, MPI_INT, 0, scr_storedesc_cntl->comm);)", "label": "(33, MPI, MPI_Bcast(&rc, 1, MPI_INT, 0, scr_storedesc_cntl->comm);)"}
{"predict": "(4, OpenMP, #pragma omp target parallel ]); (7, OpenMP, #pragma omp target parallel }); (56, OpenMP, #pragma omp target parallel copyin(pvt)); (60, OpenMP, #pragma omp target parallel copyin(mvt)); (54, OpenMP, #pragma omp target parallel copyin(i)); (-2, OpenMP, #pragma omp target parallel (); (-7, OpenMP, #pragma omp target parallel); (0, OpenMP, #pragma omp target parallel ]); (8, OpenMP, #pragma omp target parallel unknown())", "label": "(69, OpenMP, #pragma omp cancellation point parallel); (66, OpenMP, #pragma omp cancellation point taskgroup); (60, OpenMP, #pragma omp cancel for); (57, OpenMP, #pragma omp cancel); (64, OpenMP, #pragma omp cancel parallel); (50, OpenMP, #pragma omp target parallel copyin(pvt)); (-5, OpenMP, #pragma omp target parallel); (-4, OpenMP, #pragma omp target parallel (); (53, OpenMP, #pragma omp cancellation point); (57, OpenMP, #pragma omp cancel taskgroup); (-1, OpenMP, #pragma omp target parallel )); (0, OpenMP, #pragma omp target parallel }); (4, OpenMP, #pragma omp target parallel unknown()); (50, OpenMP, #pragma omp cancellation point for); (-9, OpenMP, #pragma omp target parallel [); (49, OpenMP, #pragma omp cancel sections); (49, OpenMP, #pragma omp cancellation point sections); (-10, OpenMP, #pragma omp target parallel ])"}
{"predict": "(7, CUDA, cudaGetDeviceProperties( &prop, 0 );)", "label": "(9, CUDA, cudaGetDeviceProperties( &prop, 0 );); (5, CUDA, cudaSetDeviceFlags( cudaDeviceMapHost );); (3, CUDA, cudaSetDevice( 0 );)"}
{"predict": "(2, MPI, MPI_Init( &argc, &argv );); (63, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &rank );); (1, MPI, MPI_Comm_size( MPI_COMM_WORLD, &P );)", "label": "(1, MPI, MPI_Init(&argc, &argv);); (61, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &P);)"}
{"predict": "(47, CUDA, setCommonICUDataPointer(gCommonICUDataArray[commonDataIndex]);)", "label": "(43, CUDA, setCommonICUDataPointer(&U_ICUDATA_ENTRY_POINT, FALSE, pErrorCode);)"}
{"predict": "(2, TBB, tbb::task_arena big_arena(num_threads);)", "label": "(3, TBB, tbb::task_arena big_arena(static_cast<int>(num_threads));); (28, TBB, int thread_id = tbb::this_task_arena::current_thread_index();); (0, TBB, tbb::global_control concurrency(tbb::global_control::max_allowed_parallelism, num_threads);)"}
{"predict": "(21, OpenMP, #pragma omp parallel for simd collapse (foobool(argc)), collapse (true), collapse (-5)); (22, OpenMP, #pragma omp parallel for simd collapse (S)); (34, OpenMP, #pragma omp parallel for simd collapse (2)); (29, OpenMP, #pragma omp parallel for simd collapse (N)); (1, OpenMP, #pragma omp parallel for simd collapse (); (10, OpenMP, #pragma omp parallel for simd collapse (ST); (4, OpenMP, #pragma omp parallel for simd collapse (argc); (1, OpenMP, #pragma omp parallel for simd collapse ()); (4, OpenMP, #pragma omp parallel for simd collapse (1)); (27, OpenMP, #pragma omp parallel for simd collapse (1)); (-5, OpenMP, #pragma omp parallel for simd collapse); (15, OpenMP, #pragma omp parallel for simd collapse (foobool(1) > 0? 1 : 2)); (3, OpenMP, #pragma omp parallel for simd collapse ((ST > 0)? 1 + ST : 2))", "label": "(13, OpenMP, #pragma omp parallel for simd collapse (ST); (34, OpenMP, #pragma omp parallel for simd collapse (N)); (20, OpenMP, #pragma omp parallel for simd collapse (foobool(argc)), collapse (true), collapse (-5)); (-1, OpenMP, #pragma omp parallel for simd collapse); (13, OpenMP, #pragma omp parallel for simd collapse ((ST > 0)? 1 + ST : 2)); (19, OpenMP, #pragma omp parallel for simd collapse (S)); (9, OpenMP, #pragma omp parallel for simd collapse (1)); (-3, OpenMP, #pragma omp parallel for simd collapse (); (-2, OpenMP, #pragma omp parallel for simd collapse ()); (1, OpenMP, #pragma omp parallel for simd collapse (argc); (27, OpenMP, #pragma omp parallel for simd collapse (2))"}
{"predict": "(17, TBB, tbb::flow::make_edge( a1, a2 );); (18, TBB, tbb::flow::make_edge( a3, a2 );); (20, TBB, tbb::flow::make_edge( s, c );); (17, TBB, tbb::flow::make_edge( a0, a1 );); (25, TBB, tbb::flow::make_edge( s2, q );); (13, TBB, tbb::flow::make_edge( a2, a3 );); (54, TBB, CHECK_MESSAGE( (tbb::flow::input_port<0>(a2).my_successors.empty()), \"after reset(a2), a2 has a successor.\");); (53, TBB, CHECK_MESSAGE( (tbb::flow::input_port<0>(a3).my_successors.empty()), \"after reset(a3), a3 has a successor.\");); (9, TBB, tbb::flow::make_edge( a0, a3 );); (3, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<0>(a0).my_successors.empty()), \"after reset(a0), a0 has a successor.\");); (53, TBB, CHECK_MESSAGE( (tbb::flow::input_port<0>(s2).my_successors.empty()), \"after reset(s2), s2 has a successor.\");); (50, TBB, CHECK_MESSAGE( (tbb::flow::input_port<0>(c).my_successors.empty()), \"after reset(c), c has a successor.\");); (3, TBB, tbb::flow::make_edge( a1, a3 );); (13, TBB, tbb::flow::make_edge( s, a0 );); (9, TBB, tbb::flow::make_edge( c, s );); (4, TBB, tbb::flow::make_edge( a0, a2 );); (14, TBB, tbb::flow::make_edge( a3, s );); (-11, TBB, tbb::flow::graph g;); (27, TBB, tbb::flow::remove_edge( s2, q );); (-11, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<0>(a3).my_successors.empty()), \"after", "label": "(61, TBB, tbb::flow::make_edge( a1, tbb::flow::input_port<1>(a3) );); (58, TBB, tbb::flow::remove_edge( a2, s2 );); (60, TBB, tbb::flow::make_edge( a3, s2 );); (0, TBB, tbb::flow::function_node<int,int> c(g, tbb::flow::unlimited, cube_body());); (19, TBB, CHECK_MESSAGE( (&std::get<1>(a3.input_ports()) == &tbb::flow::input_port<1>(a3.j)), \"input_port1 of j not bound to input port 1 in composite_node a3\");); (25, TBB, tbb::flow::make_edge( c, tbb::flow::input_port<1>(a0) );); (21, TBB, tbb::flow::queue_node<int> q(g);); (25, TBB, tbb::flow::make_edge( c, tbb::flow::input_port<0>(a1) );); (27, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(a0), tbb::flow::input_port<0>(a2) );); (20, TBB, tbb::flow::make_edge( s, tbb::flow::input_port<0>(a0) );); (16, TBB, tbb::flow::function_node<int,int> s2(g, tbb::flow::unlimited, square_body());); (22, TBB, tbb::flow::make_edge( c, tbb::flow::input_port<1>(a1) );); (24, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(a1), tbb::flow::input_port<1>(a2) );); (9, TBB, CHECK_MESSAGE( (&std::get<0>(a3.input_ports()) == &tbb::flow::input_port<0>(a3.j)), \"input_port 0 of j not bound to input port 0 in composite_node a3\");); (3, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<0>(a2) == &tbb::flow::input_port<0>(a2.j)), \"input_port 0 of j not bound to input port 0 in composite_node a2\");); (24, TBB, tbb::flow::make_edge( s2, q );); (-15, TBB, tbb::flow::graph g;); (-15, TBB, tbb::flow::function_node<int,int> s(g, tbb::flow::unlimited, square_body());); (0, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<1>(a2) == &tbb::flow::input_port<1>(a2.j)), \"input_port 1 of j not bound to input port 1 in composite_node a2\");); (-12, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<0>(a0) == &tbb::flow::input_port<0>(a0.j)), \"input_port 0 of j not bound to input port 0 in composite_node a0\");); (18, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(a2), s2 );); (-12, TBB, CHECK_MESSAGE( (&tbb::flow::output_port<0>(a0) == &a0.f), \"f not bound to output port 0 in composite_node a0\");); (-18, TBB, tbb::flow::function_node<int,int> p(g, tbb::flow::unlimited, passthru_body());); (35, TBB, tbb::flow::remove_edge(s2, q);); (-11, TBB, CHECK_MESSAGE( (&std::get<1>(a0.input_ports()) == &tbb::flow::input_port<1>(a0.j)), \"input_port1 of j not bound to input port 1 in composite_node a1\");); (-17, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<1>(a0) == &tbb::flow::input_port<1>(a0.j)), \"input_port 1 of j not bound to input port 1 in composite_node a0\");); (-14, TBB, CHECK_MESSAGE( (&std::get<0>(a0.input_ports()) == &tbb::flow::input_port<0>(a0.j)), \"input_port 0 of j not bound to input port 0 in composite_node a1\");); (-8, TBB, CHECK_MESSAGE( (&tbb::flow::output_port<0>(a2) == &a2.f), \"f not bound to output port 0 in composite_node a2\");); (32, TBB, tbb::flow::make_edge( a0, a3 );)"}
{"predict": "(29, CUDA, CUDASimulation cudaSimulation(model);)", "label": "(29, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(10, TBB, test_concurrency( tbb::task_scheduler_init::automatic );); (10, TBB, test_concurrency( tbb::task_scheduler_init::flow );); (10, TBB, test_concurrency( tbb::task_scheduler_init::graph );); (10, TBB, test_concurrency( tbb::task_scheduler_init::allreduce );); (10, TBB, test_concurrency( tbb::task_scheduler_init::allreduce_unique );); (10, TBB, test_concurrency( tbb::task_scheduler_init::allreduce_sum );); (7, TBB, test_concurrency( tbb::task_scheduler_init::allreduce(p) );); (7, TBB, test_concurrency( tbb::task_scheduler_init::allreduce(0, p) );); (8, TBB, test_concurrency( tbb::task_scheduler_init::allreduce(+ :, p) );); (8, TBB, test_concurrency( tbb::task_scheduler_init::allreduce(+ : 0, p) );); (7, TBB, test_concurrency( tbb::task_scheduler_init::allreduce(+);); (8, TBB, test_concurrency( tbb::task_scheduler_init::allreduce(+ : 0));); (7, TBB, test_concurrency( tbb::task_scheduler_init::allreduce(+ : 1));); (7, TBB, test_concurrency( tbb::task_scheduler_init::allreduce(+ : 2));); (7, TBB, test_concurrency( tbb::task_scheduler_init::allreduce(+ : 3));); (7, TBB, test_concurrency( tbb::task_scheduler_init::allreduce(+ : 4));); (7, TBB, test_concurrency( tbb::task_scheduler_init::allreduce(+ : 5));); (7, TBB, test_concurrency( tbb::task_scheduler_init::allreduce(+ : 6));); (7, TBB, test_concurrency( tbb::task_scheduler_init::allreduce(+ : 7));); (7, TBB, test_concurrency( tbb::task_scheduler_init::allreduce(+ : 8));); (7, TBB, test_concurrency( tbb::task_scheduler_init::allreduce(+ : 9));); (4, TBB, test_concurrency( tbb::", "label": "(10, TBB, test_extract<tbb::flow::rejecting>();); (10, TBB, test_extract<tbb::flow::queueing>();)"}
{"predict": "(41, CUDA, checkCudaErrors(cudaEventRecord(start));); (47, CUDA, checkCudaErrors(cudaEventElapsedTime(&elapsedTime, start, end));); (54, CUDA, checkCudaErrors(cudaEventSynchronize(end));); (55, CUDA, checkCudaErrors(cudaEventDestroy(start));); (14, CUDA, checkCudaErrors(cudaEventCreate(&end));); (39, CUDA, checkCudaErrors(cudaEventRecord(end));); (53, CUDA, checkCudaErrors(cudaEventDestroy(end));); (7, CUDA, checkCudaErrors(cudaEventCreate(&start));); (37, CUDA, checkCudaErrors(cudaEventSynchronize(start));)", "label": "(13, CUDA, checkCudaErrors(cudaEventCreate(&start));); (13, CUDA, checkCudaErrors(cudaEventCreate(&end));); (41, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (38, CUDA, checkCudaErrors(cudaEventRecord(end));); (35, CUDA, checkCudaErrors(cudaEventRecord(start));); (50, CUDA, checkCudaErrors(cudaEventElapsedTime(&elapsedTime, start, end));)"}
{"predict": "(3, OpenMP, omp_set_num_threads(2);); (4, OpenMP, #pragma omp parallel num_threads(2)); (-1, OpenMP, omp_set_dynamic(1);)", "label": "(5, OpenMP, #pragma omp parallel num_threads(2)); (1, OpenMP, omp_set_nested(1);); (1, OpenMP, omp_set_max_active_levels(1);)"}
{"predict": "(3, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (72, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (39, CUDA, EXPECT_EQ(cudaSuccess, cudaMemset(outAccess->sampleData(0), 0xFA, outBufferSize));); (1, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamDestroy(stream));)", "label": "(3, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (73, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (40, CUDA, EXPECT_EQ(cudaSuccess, cudaMemset(outAccess->sampleData(0), 0xFA, outBufferSize));); (104, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamDestroy(stream));)"}
{"predict": "(27, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate(S2::S2sc)); (39, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate(i)); (20, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate (argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (7, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate()); (30, OpenMP, #pragma omp target teams distribute parallel for simd shared(i)); (33, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate(j)); (22, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate(da)); (13, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate (argc, z)); (8, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate (argc); (35, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate(i)); (22, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate(S2::S2s)); (15, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate(ba)); (18, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate(ca)); (-3, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate (); (4, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate (argc > 0? argv[1] : argv[2])); (7, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate (a, b, c, d, f)); (-12, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate); (15, OpenMP, #pragma omp target teams distribute parallel for simd safelen(5)); (6, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate (argv[1])); (17, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate(h, B::x)); (22, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate(r)); (7, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate(2 * 2))", "label": "(25, OpenMP, #pragma omp target teams distribute parallel for simd shared (argv[1])); (28, OpenMP, #pragma omp target teams distribute parallel for simd shared(ca)); (19, OpenMP, #pragma omp target teams distribute parallel for simd shared (S1)); (42, OpenMP, #pragma omp target teams distribute parallel for simd shared(j)); (29, OpenMP, #pragma omp target teams distribute parallel for simd shared(e, g)); (34, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate(i)); (11, OpenMP, #pragma omp target teams distribute parallel for simd shared (argc > 0? argv[1] : argv[2])); (30, OpenMP, #pragma omp target teams distribute parallel for simd private(i), shared(i)); (7, OpenMP, #pragma omp target teams distribute parallel for simd shared (argc,); (22, OpenMP, #pragma omp target teams distribute parallel for simd shared(da)); (3, OpenMP, #pragma omp target teams distribute parallel for simd shared (argc); (24, OpenMP, #pragma omp target teams distribute parallel for simd shared(h, B::x)); (31, OpenMP, #pragma omp target teams distribute parallel for simd shared(i)); (28, OpenMP, #pragma omp target teams distribute parallel for simd private(i)); (-5, OpenMP, #pragma omp target teams distribute parallel for simd shared (); (12, OpenMP, #pragma omp target teams distribute parallel for simd shared(ba)); (-9, OpenMP, #pragma omp target teams distribute parallel for simd shared); (-6, OpenMP, #pragma omp target teams distribute parallel for simd shared ()); (1, OpenMP, #pragma omp target teams distribute parallel for simd shared (argc)); (4, OpenMP, #pragma omp target teams distribute parallel for simd shared (a, b, c, d, f, z))"}
{"predict": "(32, OpenMP, #pragma omp target teams distribute parallel for); (28, OpenMP, #pragma omp target data map(to: u1[0:d3*d2*(d1+1)], starts[0:d3])); (40, OpenMP, #pragma omp target teams distribute parallel for collapse(2))", "label": "(65, OpenMP, #pragma omp target update to(u1_real, u1_imag))"}
{"predict": "(2, CUDA, __shared__ float maxScore[FMC2H];); (2, CUDA, __shared__ float maxScor2[FMC2H];); (2, CUDA, __shared__ int maxIndex[FMC2H];)", "label": "(3, CUDA, __shared__ float maxScore[FMC2H];); (4, CUDA, __shared__ int maxIndex[FMC2H];); (2, CUDA, __shared__ float maxScor2[FMC2H];); (-1, CUDA, __shared__ float siftPoint[128*FMC2H];)"}
{"predict": "(18, MPI, MPI_Bcast(value, MPI_MAX_INFO_VAL, MPI_CHAR, 0, fd->comm);)", "label": "(25, MPI, MPI_Bcast(&tmp_val, 1, MPI_INT, 0, fd->comm);)"}
{"predict": "(8, TBB, tbb::task_scheduler_init init(1);); (2, TBB, tbb::enumerable_thread_specific< tbb::task_scheduler_init > static_init_time_set;)", "label": "(7, TBB, tbb::concurrent_unordered_set<int> static_init_time_set;)"}
{"predict": "(56, OpenMP, #pragma omp simd)", "label": "(56, OpenMP, #pragma omp simd)"}
{"predict": "(72, CUDA, checkCudaErrors(cudaMalloc((void **) &d_idata, bytes));); (133, CUDA, cudaFree(d_idata);); (73, CUDA, checkCudaErrors(cudaMalloc((void **) &d_odata, numBlocks * sizeof(T)));); (130, CUDA, cudaFree(d_odata);); (67, CUDA, checkCudaErrors(cudaMemcpy(d_odata, h_idata, numBlocks * sizeof(T), cudaMemcpyHostToDevice));)", "label": "(132, CUDA, checkCudaErrors(cudaFree(d_idata));); (71, CUDA, checkCudaErrors(cudaMalloc((void **)&d_idata, bytes));); (131, CUDA, checkCudaErrors(cudaFree(d_odata));); (70, CUDA, checkCudaErrors(cudaMalloc((void **)&d_odata, numBlocks * sizeof(T)));)"}
{"predict": "(18, MPI, MPI_Barrier(world);)", "label": "(17, MPI, MPI_Barrier(world);)"}
{"predict": "(2, TBB, using namespace tbb::task_scheduler_init;)", "label": "(11, OpenMP, #pragma omp parallel for num_threads(N_THREADS))"}
{"predict": "(15, CUDA, extern __shared__ char smem[];)", "label": "(15, CUDA, extern __shared__ char smem[];)"}
{"predict": "(32, OpenCL, retVal = clEnqueueNDRangeKernel(pCmdQ, multiDeviceKernel.get(), 1, nullptr, globalWorkSize, nullptr, 0, nullptr, nullptr);); (20, OpenCL, retVal = clEnqueueMemcpyINTEL(pCmdQ, true, compressedDeviceMemAlloc1, bufferData, bufferSize, 0, nullptr, nullptr);); (38, OpenCL, retVal = clEnqueueMemcpyINTEL(pCmdQ, true, uncompressibleHostMemAlloc, compressedDeviceMemAlloc2, bufferSize, 0, nullptr, nullptr);); (14, OpenCL, retVal = clEnqueueMemcpyINTEL(pCmdQ, true, compressedDeviceMemAlloc2, bufferData, bufferSize, 0, nullptr, nullptr);)", "label": "(31, OpenCL, retVal = clEnqueueNDRangeKernel(pCmdQ, multiDeviceKernel.get(), 1, nullptr, globalWorkSize, nullptr, 0, nullptr, nullptr);); (20, OpenCL, retVal = clEnqueueMemcpyINTEL(pCmdQ, true, compressedDeviceMemAlloc1, bufferData, bufferSize, 0, nullptr, nullptr);); (14, OpenCL, retVal = clEnqueueMemcpyINTEL(pCmdQ, true, compressedDeviceMemAlloc2, bufferData, bufferSize, 0, nullptr, nullptr);); (37, OpenCL, retVal = clEnqueueMemcpyINTEL(pCmdQ, true, uncompressibleHostMemAlloc, compressedDeviceMemAlloc2, bufferSize, 0, nullptr, nullptr);)"}
{"predict": "(122, MPI, MPI_Allreduce(&rc, &all_rc, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);); (12, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (89, MPI, MPI_Allreduce(values, sums, 3, MPI_INT64_T, MPI_SUM, MPI_COMM_WORLD);); (76, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(77, MPI, mfu_copy_stats.wtime_ended = MPI_Wtime();); (12, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (24, MPI, mfu_copy_stats.wtime_started = MPI_Wtime();); (89, MPI, MPI_Allreduce(values, sums, 3, MPI_INT64_T, MPI_SUM, MPI_COMM_WORLD);); (120, MPI, MPI_Allreduce(&rc, &all_rc, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);)"}
{"predict": "(179, CUDA, cudaMemcpy(z_cu, z, shape.Z_len * sizeof(DT), cudaMemcpyHostToDevice);); (271, CUDA, cudaFree(x_cu);); (172, CUDA, cudaMalloc(&x_cu, shape.X_len * sizeof(ST));); (175, CUDA, cudaMemcpy(y_cu, y, shape.Y_len * sizeof(ST), cudaMemcpyHostToDevice);); (173, CUDA, cudaMemcpy(x_cu, x, shape.X_len * sizeof(ST), cudaMemcpyHostToDevice);); (268, CUDA, cudaFree(y_cu);); (170, CUDA, cudaMalloc(&z_cu, shape.Z_len * sizeof(DT));); (168, CUDA, cudaMalloc(&y_cu, shape.Y_len * sizeof(ST));); (178, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (270, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (264, CUDA, cudaFree(z_cu);)", "label": "(169, CUDA, cudaMemcpy(z_cu, z, shape.Z_len * sizeof(DT), cudaMemcpyHostToDevice);); (261, CUDA, cudaFree(x_cu);); (162, CUDA, cudaMalloc(&x_cu, shape.X_len * sizeof(ST));); (165, CUDA, cudaMemcpy(y_cu, y, shape.Y_len * sizeof(ST), cudaMemcpyHostToDevice);); (163, CUDA, cudaMemcpy(x_cu, x, shape.X_len * sizeof(ST), cudaMemcpyHostToDevice);); (258, CUDA, cudaFree(y_cu);); (160, CUDA, cudaMalloc(&z_cu, shape.Z_len * sizeof(DT));); (158, CUDA, cudaMalloc(&y_cu, shape.Y_len * sizeof(ST));); (168, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (260, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (254, CUDA, cudaFree(z_cu);)"}
{"predict": "(35, MPI, MPI_Start(&req);); (14, MPI, MPI_Comm_size(comm, &size);); (28, MPI, MPI_Info_create(&info);); (49, MPI, MPI_Info_free(&info);); (28, MPI, MPI_Bcast_init(sbuf, count, MPI_INT, 0, comm, info, &req);); (12, MPI, MPI_Comm_rank(comm, &rank);); (34, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (45, MPI, MPI_Request_free(&req);); (43, MPI, MPI_Info_free(&info);)", "label": "(36, MPI, MPI_Start(&req);); (13, MPI, MPI_Comm_size(comm, &size);); (28, MPI, MPI_Info_create(&info);); (34, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (48, MPI, MPI_Info_free(&info);); (45, MPI, MPI_Request_free(&req);); (27, MPI, MPI_Alltoall_init(sbuf, count, MPI_INT, rbuf, count, MPI_INT, comm, info, &req);); (8, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(19, TBB, tbb::task_arena* result_arena = new( tbb::task_arena::allocate(arena_args..., 0) ) tbb::task_arena(arena_args..., 0);); (27, TBB, result_arena = new( tbb::task_arena::allocate(arena_args..., 1) ) tbb::task_arena(arena_args..., 1);)", "label": "(30, TBB, result_arena = new tbb::task_arena(tmp);); (18, TBB, result_arena = new tbb::task_arena( arena_args... );); (12, TBB, tbb::task_arena* result_arena = nullptr;); (26, TBB, tbb::task_arena tmp(dummy_max_concurrency, dummy_reserved_for_masters);); (19, TBB, result_arena = new tbb::task_arena;)"}
{"predict": "(7, TBB, using namespace oneapi::tbb::concurrent_hash_map;)", "label": "(4, TBB, using ch_map = tbb::concurrent_hash_map<int,int>;)"}
{"predict": "(32, MPI, rc = MPI_Type_get_true_extent(obj->DTP_datatype, &true_lb, &true_extent);); (33, MPI, rc = MPI_Type_get_extent(obj->DTP_datatype, &lb, &extent);)", "label": "(32, MPI, rc = MPI_Type_get_true_extent(obj->DTP_datatype, &true_lb, &true_extent);); (22, MPI, rc = MPI_Type_commit(&obj->DTP_datatype);); (32, MPI, rc = MPI_Type_get_extent(obj->DTP_datatype, &lb, &extent);)"}
{"predict": "(14, MPI, MPI_Comm_size(comm, &size);); (60, MPI, MPI_Wait(&req, &status);); (70, MPI, MPI_Error_string(err, &err_string, &err);); (33, MPI, MPI_Comm_set_errhandler(comm, MPI_ERRORS_RETURN);); (12, MPI, MPI_Comm_rank(comm, &rank);)", "label": "(73, MPI, err = MPI_Wait(&req, &status);); (13, MPI, MPI_Comm_size(comm, &size);); (34, MPI, MPI_Comm_set_errhandler(comm, MPI_ERRORS_RETURN);); (10, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(4, OpenMP, #pragma omp parallel master taskloop); (0, OpenMP, #pragma omp parallel)", "label": "(2, OpenMP, #pragma omp parallel); (4, OpenMP, #pragma omp parallel master taskloop)"}
{"predict": "(24, MPI, MPI_Barrier(scr_comm_world);); (219, MPI, double time_end = MPI_Wtime();); (34, MPI, time_start = MPI_Wtime();)", "label": "(24, MPI, MPI_Barrier(scr_comm_world);); (60, MPI, double time_end = MPI_Wtime();); (35, MPI, time_start = MPI_Wtime();)"}
{"predict": "(10, OpenMP, #pragma omp target data map(tofrom: i) use_device_ptr(k) use_device_addr(i,j)); (14, OpenMP, #pragma omp target data map(tofrom: i) use_device_ptr(z) use_device_addr(i,j,k[:i])); (15, OpenMP, #pragma omp target data map(tofrom: i) use_device_addr(k,i,j) use_device_ptr(z))", "label": "(9, OpenMP, #pragma omp target data map(tofrom: i) use_device_ptr(k) use_device_addr(i, j)); (13, OpenMP, #pragma omp target data map(tofrom: i) use_device_ptr(z) use_device_addr(i, j, k[:i]))"}
{"predict": "(4, MPI, MPI_Init(&argc, &argv);); (88, MPI, MPI_Finalize();); (23, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (60, MPI, MPI_Barrier(MPI_COMM_WORLD);); (60, MPI, MPI_Reduce(&counter, &totalNum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);)", "label": "(2, MPI, MPI_Init(&argc, &argv);); (86, MPI, MPI_Finalize();); (23, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&rank);); (61, MPI, MPI_Barrier(MPI_COMM_WORLD);); (59, MPI, MPI_Reduce(&counter, &totalNum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);)"}
{"predict": "(67, OpenCL, clReleaseMemObject(mem[i]);)", "label": "(67, OpenCL, clReleaseMemObject(mem[i]);)"}
{"predict": "(146, CUDA, CudaArch GpuArch = static_cast<CudaArch>(Arch);)", "label": "(123, CUDA, std::string GpuArchName(CudaArchToString(GpuArch));)"}
{"predict": "(32, CUDA, cudaDeviceSynchronize();); (19, CUDA, cudaMalloc((void**)&src_host, size * sizeof(float));); (33, CUDA, cudaMemcpy(src_test.data(), src_host, size * sizeof(float), cudaMemcpyDeviceToHost);); (18, CUDA, cudaMalloc((void**)&tar_host, size * sizeof(float));); (30, CUDA, cudaFree(tar_host);); (28, CUDA, cudaFree(src_host);)", "label": "(19, CUDA, cudaMallocHost(&tar_host, size * sizeof(float));); (17, CUDA, cudaMallocHost(&src_host, size * sizeof(float));)"}
{"predict": "(63, OpenMP, #define OP_GOMP_CRITICAL_READ(OP, FLAG))", "label": "(63, OpenMP, #define OP_GOMP_CRITICAL_READ(OP, FLAG))"}
{"predict": "(2, OpenMP, omp_autotune(self, OMP_SCALE);)", "label": "(2, OpenMP, omp_autotune(self, OMP_SCALE);)"}
{"predict": "(3, OpenMP, #pragma omp target teams distribute parallel for collapse(2) map(a[:2][:2]))", "label": "(3, OpenMP, #pragma omp parallel num_threads(2)); (12, OpenMP, #pragma omp ordered depend(source)); (7, OpenMP, #pragma omp ordered depend(sink : i - 1, j) depend(sink : i, j - 1)); (1, OpenMP, #pragma omp for ordered(2))"}
{"predict": "(4, OpenMP, #pragma omp taskloop); (2, OpenMP, #pragma omp parallel)", "label": "(3, OpenMP, #pragma omp parallel); (3, OpenMP, #pragma omp taskloop)"}
{"predict": "(9, MPI, MPI_Start(&request);); (12, MPI, MPI_Wait(&request, &status);); (7, MPI, MPI_Request_free(&request);); (5, MPI, MPI_Send_init(p->buff, p->bufflen, MPI_BYTE, p->prot.nbor, 1, MPI_COMM_WORLD, &request);); (8, MPI, MPI_Recv_init(p->buff1, p->bufflen, MPI_BYTE, p->prot.nbor, 1, MPI_COMM_WORLD, &request);)", "label": "(9, MPI, MPI_Wait(&request, &status);); (6, MPI, MPI_Irecv(p->buff1, p->bufflen, MPI_BYTE, p->prot.nbor, 1, MPI_COMM_WORLD, &request);); (6, MPI, MPI_Send(p->buff, p->bufflen, MPI_BYTE, p->prot.nbor, 1, MPI_COMM_WORLD);)"}
{"predict": "(26, OpenMP, omp_target_free(d_a, omp_get_default_device());); (13, OpenMP, d_a = (int *)omp_target_alloc(DataSize, omp_get_default_device());); (18, OpenMP, #pragma omp target disassociate_ptr(p_a)); (13, OpenMP, omp_target_associate_ptr(p_a, d_a, DataSize, 0, omp_get_default_device());)", "label": "(16, OpenMP, omp_target_associate_ptr(p_a, d_a, DataSize, 0, omp_get_default_device());); (21, OpenMP, omp_target_free(d_a, omp_get_default_device());); (12, OpenMP, d_a = (int *)omp_target_alloc(DataSize, omp_get_default_device());); (17, OpenMP, omp_target_disassociate_ptr(p_a, omp_get_default_device());)"}
{"predict": "(50, OpenMP, EXPECT_TRUE(check_test_image(output_file, t_apcomp_baseline_dir()));); (9, MPI, MPI_Comm_rank(comm, &par_rank);); (9, MPI, MPI_Comm_size(comm, &par_size);)", "label": "(50, OpenMP, EXPECT_TRUE(check_test_image(output_file, t_apcomp_baseline_dir()));); (9, MPI, MPI_Comm_rank(comm, &par_rank);); (10, MPI, apcomp::mpi_comm(MPI_Comm_c2f(comm));); (8, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(41, MPI, MPI_Grequest_end(&request, MPI_SUCCESS);); (27, MPI, MPI_Grequest_start(query_fn, free_fn, cancel_fn, NULL, &request);); (16, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (20, MPI, MPI_Grequest_create(&request);)", "label": "(27, MPI, MPI_Wait(&request, &status);); (40, MPI, MPI_Waitall(1, &request, &status);); (16, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (31, MPI, MPI_Waitsome(1, &request, &outcount, indices, &status);); (19, MPI, MPI_Grequest_start(query_fn, free_fn, cancel_fn, NULL, &request);)"}
{"predict": "(6, OpenMP, omp_set_dynamic(0);)", "label": "(6, OpenMP, omp_set_dynamic(0);)"}
{"predict": "(14, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize());); (12, CUDA, qmc_cuda::cuda_check(cudaGetLastError());)", "label": "(14, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize());); (12, CUDA, qmc_cuda::cuda_check(cudaGetLastError());)"}
{"predict": "(6, TBB, tbb::flow::queue_node< q3_input_type >  q3(g);); (24, TBB, CHECK_MESSAGE( tbb::flow::input_port<0>(my_split).my_successors.empty(), \"after reset, split has a successor.\");); (18, TBB, CHECK_MESSAGE( tbb::flow::input_port<0>(my_split).my_successors.empty(), \"after reset, split has a successor.\");); (5, TBB, tbb::flow::make_edge( q3, my_split );); (22, TBB, g.reset(tbb::flow::rf_clear_edges);); (16, TBB, CHECK_MESSAGE( tbb::flow::input_port<0>(my_split).my_successors.empty(), \"after reset, split has a successor.\");)", "label": "(6, TBB, tbb::flow::queue_node< q3_input_type >  q3(g);); (7, TBB, tbb::flow::make_edge( q3, my_split );)"}
{"predict": "(3, CUDA, cudaGetDevice(&device_id);); (3, CUDA, cudaGetDeviceProperties(&device_prop, device_id);)", "label": "(3, CUDA, cudaGetDevice(&device_id);); (3, CUDA, cudaGetDeviceProperties(&device_prop, device_id);)"}
{"predict": "(5, OpenMP, #pragma omp atomic)", "label": "(3, OpenMP, #pragma omp atomic)"}
{"predict": "(18, MPI, timers[t] = MPI_Wtime();); (21, MPI, timers[t] = (MPI_Wtime() - timers[t]) / cycles;); (4, MPI, MPI_Type_size(sdt, &outsize);); (12, MPI, MPI_Pack(sbuf, scount, sdt, packed_buf, outsize, &position, MPI_COMM_WORLD);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myself);)", "label": "(18, MPI, timers[t] = MPI_Wtime();); (22, MPI, timers[t] = (MPI_Wtime() - timers[t]) / cycles;); (11, MPI, MPI_Pack(sbuf, scount, sdt, packed_buf, outsize, &position, MPI_COMM_WORLD);); (2, MPI, MPI_Type_size(sdt, &outsize);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myself);)"}
{"predict": "(26, OpenMP, #pragma omp target parallel device(global + a) depend(in); (102, OpenMP, #pragma omp target enter data map(to : global) if(0) depend(out); (84, OpenMP, #pragma omp target enter data map(to : global) depend(inout)", "label": "(86, OpenMP, #pragma omp target teams device(global + a) nowait depend(inout); (25, OpenMP, #pragma omp target teams device(global + a) depend(in); (100, OpenMP, #pragma omp target teams if (0) firstprivate(global) depend(out)"}
{"predict": "(16, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize(), \"batched_dot_wabn_wban\");); (14, CUDA, qmc_cuda::cuda_check(cudaGetLastError(), \"batched_dot_wabn_wban\");)", "label": "(16, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize(), \"batched_dot_wabn_wban\");); (14, CUDA, qmc_cuda::cuda_check(cudaGetLastError(), \"batched_dot_wabn_wban\");)"}
{"predict": "(48, CUDA, __shared__ TB smemB[cosize_v<BSmemLayout>];); (46, CUDA, __shared__ TA smemA[cosize_v<ASmemLayout>];)", "label": "(48, CUDA, __shared__ TB smemB[cosize_v<BSmemLayout>];); (46, CUDA, __shared__ TA smemA[cosize_v<ASmemLayout>];)"}
{"predict": "(39, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (12, CUDA, checkCudaErrors(cudaMallocPitch((void **)&src, &DeviceStride, Size.width * sizeof(float), Size.height));); (12, CUDA, checkCudaErrors(cudaMallocPitch((void **)&dst, &DeviceStride, Size.width * sizeof(float), Size.height));); (77, CUDA, checkCudaErrors(cudaFree(src));); (77, CUDA, checkCudaErrors(cudaFree(dst));); (10, CUDA, checkCudaErrors(cudaMemcpy2D(dst, DeviceStride * sizeof(float), ImgF1, StrideF * sizeof(float), Size.width * sizeof(float), Size.height, cudaMemcpyHostToDevice));)", "label": "(13, CUDA, checkCudaErrors(cudaMallocPitch((void **)&src, &DeviceStride, Size.width * sizeof(float), Size.height));); (79, CUDA, checkCudaErrors(cudaFree(src));); (12, CUDA, checkCudaErrors(cudaMallocPitch((void **)&dst, &DeviceStride, Size.width * sizeof(float), Size.height));); (35, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (40, CUDA, getLastCudaError(\"Kernel execution failed\");); (74, CUDA, checkCudaErrors(cudaFree(dst));)"}
{"predict": "(14, OpenMP, #pragma omp taskloop simd shared(c) num_tasks(a) simdlen(8))", "label": "(14, OpenMP, #pragma omp master taskloop shared(c) num_tasks(a))"}
{"predict": "(35, CUDA, __shared__ _VOLATILE_ float bSmem[16][128+4];); (33, CUDA, __shared__ _VOLATILE_ float aSmem[16][128+4];)", "label": "(35, CUDA, __shared__ _VOLATILE_ float bSmem[16][128+4];); (33, CUDA, __shared__ _VOLATILE_ float aSmem[16][128+4];)"}
{"predict": "(2, OpenMP, #pragma omp parallel); (3, OpenMP, #pragma omp master taskloop simd)", "label": "(4, OpenMP, #pragma omp master taskloop simd); (1, OpenMP, #pragma omp parallel)"}
{"predict": "(26, CUDA, cudaMalloc((void**)&deviceC, size);); (54, CUDA, cudaFree(deviceB);); (54, CUDA, cudaFree(deviceC);); (30, CUDA, cudaMemcpy(deviceA, hostA, size, cudaMemcpyHostToDevice);); (30, CUDA, cudaMemcpy(deviceB, hostB, size, cudaMemcpyHostToDevice);); (50, CUDA, cudaFree(deviceA);); (38, CUDA, cudaMemcpy(hostC, deviceC, size, cudaMemcpyDeviceToHost);); (21, CUDA, cudaMalloc((void**)&deviceB, size);); (22, CUDA, cudaMalloc((void**)&deviceA, size);)", "label": "(28, CUDA, cudaMalloc((void**)&deviceC, size);); (55, CUDA, cudaFree(deviceB);); (40, CUDA, cudaMemcpy(hostC, deviceC, size, cudaMemcpyDeviceToHost);); (54, CUDA, cudaFree(deviceC);); (51, CUDA, cudaFree(deviceA);); (22, CUDA, cudaMalloc((void**)&deviceB, size);); (25, CUDA, cudaMemcpy(deviceA, hostA, size, cudaMemcpyHostToDevice);); (25, CUDA, cudaMemcpy(deviceB, hostB, size, cudaMemcpyHostToDevice);); (18, CUDA, cudaMalloc((void**)&deviceA, size);)"}
{"predict": "(77, MPI, MPI_Comm_size(fd->comm, &nprocs);); (77, MPI, MPI_Comm_rank(fd->comm, &myrank);)", "label": "(77, MPI, MPI_Comm_size(fd->comm, &nprocs);); (77, MPI, MPI_Comm_rank(fd->comm, &myrank);)"}
{"predict": "(26, OpenMP, chunk = omp_get_chunk_size(sched);); (13, OpenMP, #pragma omp parallel); (12, OpenMP, ref_sched = omp_get_reference_schedule();); (13, OpenMP, sched = omp_get_schedule();); (20, OpenMP, #pragma omp single)", "label": "(11, OpenMP, ref_sched = str2omp_sched(argv[2]);); (13, OpenMP, omp_get_schedule(&sched, &chunk);)"}
{"predict": "(7, CUDA, cudaDeviceSynchronize();); (7, CUDA, checkCudaErrors(cudaMemcpy2D(&d_vbo_buffer, 0, 0, imageData, mesh_width * mesh_height * sizeof(float), mesh_width * mesh_height * sizeof(float), devID, cudaMemcpyHostToDevice));)", "label": "(7, CUDA, cudaDeviceSynchronize();); (7, CUDA, getLastCudaError(\"launch_kernel failed\");)"}
{"predict": "(6, TBB, tbb::task_scheduler_init init(2);); (1, TBB, tbb::global_control c(tbb::global_control::max_allowed_parallelism, 2);)", "label": "(6, TBB, tbb::global_control global_limit(tbb::global_control::max_allowed_parallelism, idx);)"}
{"predict": "(5, CUDA, __shared__ DT labels[BW * BH * BD];)", "label": "(5, CUDA, __shared__ DT labels[BW * BH * BD];)"}
{"predict": "(10, CUDA, checkCudaErrors(cudaMalloc(&ctx->d_dataset,   batch_size * dataset_size * sizeof(uint32_t)));); (9, CUDA, checkCudaErrors(cudaMemcpy(ctx->d_dataset,   dataset,     batch_size * dataset_size * sizeof(uint32_t), cudaMemcpyHostToDevice));); (4, CUDA, checkCudaErrors(cudaMallocHost(&ctx->rx_dataset_host, batch_size * dataset_size * sizeof(uint32_t)));); (5, CUDA, checkCudaErrors(cudaMalloc(&ctx->d_scratchpads, ctx->d_scratchpads_size));); (8, CUDA, checkCudaErrors(cudaMallocHost(&ctx->rx_dataset_port, batch_size * dataset_size * sizeof(uint32_t)));); (2, CUDA, checkCudaErrors(cudaMalloc(&ctx->d_rx_rounding, batch_size * sizeof(uint32_t)));); (9, CUDA, checkCudaErrors(cudaMalloc(&ctx->d_rx_vm_states, batch_size * sizeof(uint32_t)));); (10, CUDA, checkCudaErrors(cudaMalloc(&ctx->d_rx_thread_states, batch_size * sizeof(uint32_t)));); (0, CUDA, checkCudaErrors(cudaMalloc(&ctx->d_rx_data,   batch_size * dataset_size * sizeof(uint32_t)));); (10, CUDA, checkCudaErrors(cudaMalloc(&ctx->d_rx_user_states, batch_size * sizeof(uint32_t)));); (2, CUDA, checkCudaErrors(cudaMalloc(&ctx->d_rx_count,   batch_size * sizeof(uint32_t)));); (2, CUDA, checkCudaErrors(cudaMalloc(&ctx->d_rx_sum,   batch_size * sizeof(uint32_t)));); (2, CUDA, checkCudaErrors(cudaMalloc(&ctx->d_rx_max,   batch_size * sizeof(uint32_t)));)", "label": "(9, CUDA, CUDA_CHECK(ctx->device_id, cudaMalloc(&ctx->d_rx_dataset, dataset_size));); (15, CUDA, CUDA_CHECK(ctx->device_id, cudaMalloc(&ctx->d_rx_vm_states, batch_size * 2560));); (4, CUDA, CUDA_CHECK(ctx->device_id, cudaHostGetDevicePointer(&ctx->d_rx_dataset, const_cast<void *>(dataset), 0));); (7, CUDA, CUDA_CHECK(ctx->device_id, cudaMemcpy(ctx->d_rx_dataset, dataset, dataset_size, cudaMemcpyHostToDevice));); (11, CUDA, CUDA_CHECK(ctx->device_id, cudaMalloc(&ctx->d_rx_entropy, batch_size * (128 + 2560)));); (12, CUDA, CUDA_CHECK(ctx->device_id, cudaMalloc(&ctx->d_rx_rounding, batch_size * sizeof(uint32_t)));); (8, CUDA, CUDA_CHECK(ctx->device_id, cudaMalloc(&ctx->d_rx_hashes, batch_size * 64));); (6, CUDA, CUDA_CHECK(ctx->device_id, cudaMalloc(&ctx->d_long_state, ctx->d_scratchpads_size));)"}
{"predict": "(39, CUDA, cudaFree(res_dev);); (32, CUDA, cudaMalloc(&res_dev, sizeof(float));); (34, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)", "label": "(39, CUDA, cudaFree(res_dev);); (32, CUDA, cudaMalloc(&res_dev, sizeof(float));); (34, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(16, MPI, MPI_Error_string(err, msg, &len);); (7, MPI, err = MPI_Probe(MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &status);)", "label": "(16, MPI, MPI_Error_string(err, msg, &len);); (8, MPI, err = MPI_Probe(-80, 1, MPI_COMM_WORLD, &status);); (5, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(83, OpenMP, elapsed_time = omp_get_wtime() - start_time;); (52, OpenMP, #pragma omp target map(tofrom : xnew[0 : Ndim], xold[0 : Ndim])); (86, OpenMP, #pragma omp target exit data map(from : xnew[0 : Ndim], xold[0 : Ndim])); (39, OpenMP, start_time = omp_get_wtime();); (47, OpenMP, #pragma omp target enter data map(to : xnew[0 : Ndim], xold[0 : Ndim],); (62, OpenMP, #pragma omp target map(tofrom : conv)); (81, OpenMP, #pragma omp target update from(xnew[0 : Ndim], xold[0 : Ndim]))", "label": "(69, OpenMP, #pragma omp target map(tofrom : conv)); (87, OpenMP, elapsed_time = omp_get_wtime() - start_time;); (53, OpenMP, #pragma omp teams distribute parallel for simd); (83, OpenMP, #pragma omp target exit data map(from : xold[0 : Ndim], xnew[0 : Ndim])); (37, OpenMP, start_time = omp_get_wtime();); (43, OpenMP, #pragma omp target enter data map(to : xold[0 : Ndim], xnew[0 : Ndim],); (48, OpenMP, #pragma omp target); (63, OpenMP, #pragma omp teams distribute parallel for simd reduction(+ : conv))"}
{"predict": "(17, CUDA, CUDA_CHECK(cudaGetLastError());)", "label": "(16, CUDA, CUDA_CHECK(cudaGetLastError());)"}
{"predict": "(55, CUDA, result = cudaFree(0);); (56, CUDA, errPtr = cudaGetErrorName(result);); (292, CUDA, DPRINTF(\"Error in synchronizing stream = %s\\n\", cudaGetErrorName(result));); (199, CUDA, result = cudaMalloc(&inputBufferGPU, inputTensorDesc[0].size);); (89, CUDA, result = cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking);); (57, CUDA, result = cudaSetDevice(0);); (205, CUDA, result = cudaMalloc(&outputBufferGPU, outputTensorDesc[0].size);); (285, CUDA, result = cudaStreamSynchronize(stream);); (290, CUDA, DPRINTF(\"Error in synchronizing stream %d\\n\", stream);)", "label": "(55, CUDA, result = cudaFree(0);); (56, CUDA, errPtr = cudaGetErrorName(result);); (320, CUDA, cudaFree(outputBufferGPU);); (318, CUDA, cudaFree(inputBufferGPU);); (205, CUDA, result = cudaMalloc(&inputBufferGPU, inputTensorDesc[0].size);); (88, CUDA, result = cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking);); (56, CUDA, result = cudaSetDevice(0);); (211, CUDA, result = cudaMalloc(&outputBufferGPU, outputTensorDesc[0].size);); (324, CUDA, result = cudaStreamDestroy(stream);); (282, CUDA, result = cudaStreamSynchronize(stream);)"}
{"predict": "(49, CUDA, thrust::host_vector<TB> h_B(n*k);); (1, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (49, CUDA, thrust::host_vector<TC> h_C(m*n);); (46, CUDA, thrust::host_vector<TA> h_A(m*k);); (2, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;); (55, CUDA, thrust::device_vector<TC> d_C = h_C;); (50, CUDA, thrust::device_vector<TB> d_B = h_B;); (84, CUDA, thrust::host_vector<TC> cute_result = d_C;); (47, CUDA, thrust::device_vector<TA> d_A = h_A;); (49, CUDA, thrust::device_vector<TK> d_C = h_C;); (43, CUDA, thrust::host_vector<TB> h_B(m*k);); (51, CUDA, thrust::device_vector<TK> d_B = h_B;); (43, CUDA, thrust::host_vector<TC> h_C(m*k);)", "label": "(49, CUDA, thrust::host_vector<TB> h_B(n*k);); (4, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;); (1, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (55, CUDA, thrust::device_vector<TC> d_C = h_C;); (46, CUDA, thrust::host_vector<TC> h_C(m*n);); (52, CUDA, thrust::device_vector<TB> d_B = h_B;); (86, CUDA, thrust::host_vector<TC> cute_result = d_C;); (49, CUDA, thrust::device_vector<TA> d_A = h_A;); (40, CUDA, thrust::host_vector<TA> h_A(m*k);)"}
{"predict": "(7, OpenMP, #pragma omp target teams distribute parallel for map(tofrom : counter_N0)); (6, OpenMP, #pragma omp target data map(tofrom : counter_N0))", "label": "(8, OpenMP, #pragma omp taskloop shared(counter_N0)); (5, OpenMP, #pragma omp target data map(tofrom : counter_N0)); (8, OpenMP, #pragma omp target teams distribute parallel for map(tofrom : counter_N0) nowait); (9, OpenMP, #pragma omp atomic update)"}
{"predict": "(201, MPI, MPI_Recv(&(ARRAY(0,0)),1,MPI_DOUBLE,root,888,MPI_COMM_WORLD,&status);); (87, MPI, MPI_Bcast(&m,          1, MPI_INT, root, MPI_COMM_WORLD);); (87, MPI, MPI_Bcast(&n,          1, MPI_INT, root, MPI_COMM_WORLD);); (201, MPI, MPI_Send(&corner_val,1,MPI_DOUBLE,root,888,MPI_COMM_WORLD);); (84, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (87, MPI, MPI_Bcast(&group_factor, 1, MPI_INT, root, MPI_COMM_WORLD);); (22, MPI, MPI_Init(&argc,&argv);); (188, MPI, MPI_Send(&(ARRAY(end,j)), 1, MPI_DOUBLE, my_ID+1, j, MPI_COMM_WORLD);); (99, MPI, MPI_Bcast(&n,          1, MPI_INT, root, MPI_COMM_WORLD);); (93, MPI, MPI_Bcast(&m,          1, MPI_INT, root, MPI_COMM_WORLD);); (388, MPI, MPI_Finalize();); (170, MPI, MPI_Recv(inbuf, jjsize, MPI_DOUBLE, my_ID-1, j, MPI_COMM_WORLD, &status);); (167, MPI, MPI_Send(outbuf, jjsize, MPI_DOUBLE, my_ID+1, j, MPI_COMM_WORLD);); (17, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (77, MPI, MPI_Bcast(&grp,        1, MPI_INT, root, MPI_COMM_WORLD);); (342, MPI, MPI_Reduce(&local_pipeline_time, &pipeline_time, 1, MPI_DOUBLE, MPI_MAX, root, MPI_COMM_WORLD);); (281, MPI, MPI_Barrier(MPI_COMM_WORLD);); (341, MPI, MPI_Reduce(&avgtime, &avgtime, 1, MPI_DOUBLE, MPI_SUM, root, MPI_COMM_WORLD);); (13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)", "label": "(177, MPI, MPI_Recv(inbuf, jjsize, MPI_DOUBLE, my_ID-1, j, MPI_COMM_WORLD, &status);); (88, MPI, MPI_Bcast(&m,          1, MPI_INT, root, MPI_COMM_WORLD);); (88, MPI, MPI_Bcast(&n,          1, MPI_INT, root, MPI_COMM_WORLD);); (89, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (241, MPI, MPI_Finalize();); (163, MPI, MPI_Send(&(ARRAY(end,j)), 1, MPI_DOUBLE, my_ID+1, j, MPI_COMM_WORLD);); (195, MPI, MPI_Send(&corner_val,1,MPI_DOUBLE,0,888,MPI_COMM_WORLD);); (197, MPI, MPI_Recv(&(ARRAY(0,0)),1,MPI_DOUBLE,root,888,MPI_COMM_WORLD,&status);); (18, MPI, MPI_Init(&argc,&argv);); (183, MPI, MPI_Send(outbuf, jjsize, MPI_DOUBLE, my_ID+1, j, MPI_COMM_WORLD);); (138, MPI, MPI_Barrier(MPI_COMM_WORLD);); (17, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (79, MPI, MPI_Bcast(&grp,        1, MPI_INT, root, MPI_COMM_WORLD);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(9, TBB, using range_t = oneapi::tbb::blocked_nd_range<utils_t::template data_type<int, 1> >;)", "label": "(10, TBB, range_t r = utils_t::make_range((1 << (20 / DimAmount - 1)) - DimAmount, false, &MakeInt, oneapi::tbb::detail::make_index_sequence<DimAmount>());); (1, TBB, using range_t = oneapi::tbb::blocked_nd_range<int, DimAmount>;)"}
{"predict": "(44, MPI, MPI_Send( recvbuf, knsupc * nrhs, MPI_FLOAT, receiver, k,  grid3d->zscp.comm);); (51, MPI, MPI_Recv( recvbuf, knsupc * nrhs, MPI_FLOAT, sender, k, grid3d->zscp.comm, &status );)", "label": "(52, MPI, MPI_Recv( recvbuf, knsupc * nrhs, MPI_FLOAT, sender, k, grid3d->zscp.comm, &status );); (42, MPI, MPI_Send( recvbuf, knsupc * nrhs, MPI_FLOAT, receiver, k,  grid3d->zscp.comm);)"}
{"predict": "(33, MPI, MPI_Barrier(scr_comm_world);)", "label": "(33, MPI, MPI_Barrier(scr_comm_world);)"}
{"predict": "(28, CUDA, cudaGridDependencySynchronize();)", "label": "(27, CUDA, cudaGridDependencySynchronize();); (85, CUDA, cudaTriggerProgrammaticLaunchCompletion();)"}
{"predict": "(16, OpenCL, error = clCreateContext(nullptr, 1, &device, nullptr, nullptr, &context);)", "label": "(16, OpenCL, context = clCreateContext(nullptr, 1, &device, nullptr, nullptr, &error);)"}
{"predict": "(24, OpenCL, clReleaseEvent(event);)", "label": "(24, OpenCL, clReleaseEvent(event);)"}
{"predict": "(26, CUDA, __shared__ __align__(sizeof(MoveEltType)) unsigned char loadSmemBuffer[MaxLayerCount];)", "label": "(31, CUDA, __shared__ char loadSmemBuffer[kUpdateKVCacheKernelShmSize];)"}
{"predict": "(5, OpenMP, #pragma omp target update to (u0_real, u0_imag, u1_real, u1_imag, twiddle)); (7, OpenMP, #pragma omp target teams distribute parallel for collapse(3))", "label": "(6, OpenMP, #pragma omp teams distribute); (7, OpenMP, #pragma omp parallel for); (8, OpenMP, #pragma omp simd); (1, OpenMP, #pragma omp target map (alloc: u0_real,u0_imag,u1_real,u1_imag,twiddle))"}
{"predict": "(16, MPI, MPI_Abort(MPI_COMM_WORLD, MPIT_result);); (17, MPI, MPIT_result = MPI_T_pvar_session_create(session);); (12, MPI, MPIT_result = MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);)", "label": "(16, MPI, MPI_Abort(MPI_COMM_WORLD, MPIT_result);); (17, MPI, MPIT_result = MPI_T_pvar_session_create(session);); (11, MPI, MPIT_result = MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);)"}
{"predict": "(19, OpenCL, retVal = clEnqueueUnmapMemObject(cmdQ.get(), buffer.get(), mappedPtr, 1, &clUnmapEvent, nullptr);); (10, OpenCL, void *mappedPtr = clEnqueueMapBuffer(cmdQ.get(), buffer.get(), CL_FALSE, CL_MAP_WRITE, offset, size, 1, &clMapEvent, nullptr, &retVal);)", "label": "(19, OpenCL, retVal = clEnqueueUnmapMemObject(cmdQ.get(), buffer.get(), mappedPtr, 1, &clUnmapEvent, nullptr);); (10, OpenCL, void *mappedPtr = clEnqueueMapBuffer(cmdQ.get(), buffer.get(), CL_FALSE, CL_MAP_WRITE, offset, size, 1, &clMapEvent, nullptr, &retVal);)"}
{"predict": "(8, TBB, tbb::task_scheduler_init init(tbb::task_scheduler_init::default_num_threads());); (61, TBB, tbb::task_scheduler_init init( use_pinning );); (2, TBB, tbb::task_scheduler_init init(2);); (96, TBB, tbb::task_scheduler_init init( tbb::task_scheduler_init::default_num_threads() );); (265, TBB, tbb::task_scheduler_init init( tbb::task_scheduler_init::default_num_threads() );); (55, TBB, tbb::task_scheduler_init init( tbb::task_scheduler_init::default_num_threads() );); (205, TBB, tbb::task_scheduler_init init(2);); (282, TBB, tbb::task_scheduler_init init(2));); (260, TBB, tbb::task_scheduler_init init(1));)", "label": "(62, TBB, PRINT( tbb::TBB_runtime_interface_version() );); (68, TBB, while (tracker.get_concurrency() < tbb::task_scheduler_init::default_num_threads()) tbb_pi<double> (N);); (11, TBB, tbb::task_scheduler_init init(tbb::task_scheduler_init::default_num_threads());); (82, TBB, TaskSchedulerTBB::create(0);); (90, TBB, TaskSchedulerTBB::destroy();)"}
{"predict": "(16, OpenMP, #pragma omp target data map(tofrom: W[:M])); (10, OpenMP, #pragma omp target enter data map(to: W[:M])); (5, OpenMP, #pragma omp target enter data map(tofrom: W[0:M])); (11, OpenMP, #pragma omp target exit data map(from: W[:M]))", "label": "(7, OpenMP, #pragma omp target data map(tofrom: W[:M])); (12, OpenMP, #pragma omp target teams); (12, OpenMP, #pragma omp distribute parallel for)"}
{"predict": "(66, MPI, MPI_Bcast( ToRecv, nsupers, MPI_INT, 0,  grid3d->zscp.comm);); (39, MPI, MPI_Bcast( supno, n, mpi_int_t, 0,  grid3d->zscp.comm);); (27, MPI, MPI_Bcast( &nsupers, 1, mpi_int_t, 0,  grid3d->zscp.comm);); (69, MPI, MPI_Bcast( ToSendR[i], Pc, MPI_INT, 0,  grid3d->zscp.comm);); (233, MPI, MPI_Bcast(*((void**)&(Llu->fsendx_plist[lk])), Pr * sizeof (int), MPI_BYTE, 0, grid3d->zscp.comm);); (84, MPI, MPI_Bcast( &(Llu->ldalsum), 1, MPI_INT, 0,  grid3d->zscp.comm);); (23, MPI, MPI_Bcast( xsup, nsupers + 1, mpi_int_t, 0,  grid3d->zscp.comm);); (83, MPI, MPI_Bcast( &(Llu->nbsendx), 1, MPI_INT, 0,  grid3d->zscp.comm);); (81, MPI, MPI_Bcast( &(Llu->nfsendx), 1, MPI_INT, 0,  grid3d->zscp.comm);); (18, MPI, MPI_Bcast( &nsupers, 1, mpi_int_t, 0,  grid3d->zscp.comm);); (73, MPI, MPI_Bcast( ToSendD, nbr, MPI_INT, 0,  grid3d->zscp.comm);); (22, MPI, MPI_Bcast( bufmax, NBUFFERS, mpi_int_t, 0,  grid3d->zscp.comm);)", "label": "(66, MPI, MPI_Bcast( ToRecv, nsupers, MPI_INT, 0,  grid3d->zscp.comm);); (39, MPI, MPI_Bcast( supno, n, mpi_int_t, 0,  grid3d->zscp.comm);); (27, MPI, MPI_Bcast( &nsupers, 1, mpi_int_t, 0,  grid3d->zscp.comm);); (69, MPI, MPI_Bcast( ToSendR[i], Pc, MPI_INT, 0,  grid3d->zscp.comm);); (11, MPI, MPI_Bcast( etree, n, mpi_int_t, 0,  grid3d->zscp.comm);); (233, MPI, MPI_Bcast(*((void**)&(Llu->fsendx_plist[lk])), Pr * sizeof (int), MPI_BYTE, 0, grid3d->zscp.comm);); (84, MPI, MPI_Bcast( &(Llu->ldalsum), 1, mpi_int_t, 0,  grid3d->zscp.comm);); (61, MPI, MPI_Bcast( ToSendD, nbr, MPI_INT, 0,  grid3d->zscp.comm);); (29, MPI, MPI_Bcast( xsup, nsupers + 1, mpi_int_t, 0,  grid3d->zscp.comm);); (80, MPI, MPI_Bcast( &(Llu->nbsendx), 1, MPI_INT, 0,  grid3d->zscp.comm);); (78, MPI, MPI_Bcast( &(Llu->nfsendx), 1, MPI_INT, 0,  grid3d->zscp.comm);); (45, MPI, MPI_Bcast( bufmax, NBUFFERS, mpi_int_t, 0,  grid3d->zscp.comm);)"}
{"predict": "(16, OpenCL, retVal = clEnqueueUnmapMemObject(cmdQ.get(), image.get(), mappedPtr, 0, nullptr, nullptr);); (8, OpenCL, void *mappedPtr = clEnqueueMapImage(cmdQ.get(), image.get(), CL_TRUE, CL_MAP_READ, &origin[0], &region[0], nullptr, nullptr, 0, nullptr, nullptr, &retVal);)", "label": "(16, OpenCL, retVal = clEnqueueUnmapMemObject(cmdQ.get(), image.get(), mappedPtr, 0, nullptr, nullptr);); (8, OpenCL, void *mappedPtr = clEnqueueMapImage(cmdQ.get(), image.get(), CL_TRUE, CL_MAP_READ, &origin[0], &region[0], nullptr, nullptr, 0, nullptr, nullptr, &retVal);)"}
{"predict": "(12, OpenMP, #pragma omp parallel sections private(i,sum0) lastprivate(i0)); (22, OpenMP, #pragma omp critical); (11, OpenMP, #pragma omp section)", "label": "(13, OpenMP, #pragma omp section); (10, OpenMP, #pragma omp parallel); (21, OpenMP, #pragma omp critical)"}
{"predict": "(57, MPI, MPI_Finalize();); (10, MPI, MPI_Init(&argc,&argv);)", "label": "(57, MPI, MPI_Finalize();); (10, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(16, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (30, CUDA, assert(cudaMemcpyAsync(out.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);); (13, CUDA, cudaMalloc((void**)(&d_in1), in1_bytes);); (40, CUDA, cudaFree(d_out);); (38, CUDA, cudaFree(d_in2);); (26, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (35, CUDA, cudaFree(d_in1);); (10, CUDA, cudaMalloc((void**)(&d_in2), in2_bytes);); (11, CUDA, cudaMemcpy(d_in1, in1.data(), in1_bytes, cudaMemcpyHostToDevice);); (11, CUDA, cudaMemcpy(d_in2, in2.data(), in2_bytes, cudaMemcpyHostToDevice);); (15, CUDA, assert(cudaMemcpyAsync(in1.data(), d_in1, in1_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);); (15, CUDA, assert(cudaMemcpyAsync(in2.data(), d_in2, in2_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);)", "label": "(17, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (14, CUDA, cudaMalloc((void**)(&d_in1), in1_bytes);); (29, CUDA, assert(cudaMemcpyAsync(out.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);); (16, CUDA, cudaMemcpy(d_in1, in1.data(), in1_bytes, cudaMemcpyHostToDevice);); (40, CUDA, cudaFree(d_out);); (38, CUDA, cudaFree(d_in2);); (26, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (35, CUDA, cudaFree(d_in1);); (8, CUDA, cudaMalloc((void**)(&d_in2), in2_bytes);); (11, CUDA, cudaMemcpy(d_in2, in2.data(), in2_bytes, cudaMemcpyHostToDevice);)"}
{"predict": "(26, CUDA, cudaEventCreate(&start);); (36, CUDA, cudaEventDestroy(start);); (30, CUDA, cudaEventRecord(end);); (32, CUDA, cudaEventSynchronize(end);); (33, CUDA, cudaEventElapsedTime(&sec, start, end);); (29, CUDA, cudaEventRecord(start);); (34, CUDA, cudaEventDestroy(end);); (20, CUDA, cudaEventCreate(&end);)", "label": "(27, CUDA, cudaEventRecord(start);); (31, CUDA, cudaEventRecord(end);); (31, CUDA, cudaEventSynchronize(end);); (23, CUDA, cudaEventCreate(&end);); (21, CUDA, cudaEventCreate(&start);); (31, CUDA, cudaEventElapsedTime(&msec, start, end);)"}
{"predict": "(13, CUDA, cudaMalloc((void **)&d_arr, sizeof(int) * len);); (16, CUDA, cudaMemcpy(d_arr, arr, sizeof(int) * len, cudaMemcpyHostToDevice);); (53, CUDA, cudaFree(d_out);); (11, CUDA, cudaMalloc((void **)&d_out, sizeof(int) * len);); (23, CUDA, cudaMemcpy(out, d_out, sizeof(int) * len, cudaMemcpyDeviceToHost);); (49, CUDA, cudaFree(d_arr);)", "label": "(13, CUDA, cudaMalloc((void **)&d_arr, sizeof(int) * len);); (16, CUDA, cudaMemcpy(d_arr, arr, sizeof(int) * len, cudaMemcpyHostToDevice);); (53, CUDA, cudaFree(d_out);); (11, CUDA, cudaMalloc((void **)&d_out, sizeof(int) * len);); (23, CUDA, cudaMemcpy(out, d_out, sizeof(int) * len, cudaMemcpyDeviceToHost);); (49, CUDA, cudaFree(d_arr);)"}
{"predict": "(12, OpenMP, #pragma omp target teams map(from:lims[0:N], threads[0:N])); (11, OpenMP, #pragma omp parallel for num_threads(N)); (14, OpenMP, #pragma omp target teams map(from:Res[0:num_teams*NN], CUs[0:num_teams*NN]))", "label": "(11, OpenMP, #pragma omp distribute); (16, OpenMP, #pragma omp parallel for); (13, OpenMP, threads[j] = omp_get_num_threads();); (7, OpenMP, #pragma omp target teams num_teams(numTeams) thread_limit(1024) map (tofrom: CUs, lims, threads, Res)); (16, OpenMP, CUs[i] = omp_ext_get_smid();); (9, OpenMP, lims[j] = omp_get_num_teams();)"}
{"predict": "(9, MPI, MPI_Comm_get_attr(MPI_COMM_WORLD, MPI_UNIVERSE_SIZE, &v, &flag);); (18, MPI, rc = MPI_Comm_size(MPI_COMM_WORLD, &size);)", "label": "(19, MPI, rc = MPI_Comm_size(MPI_COMM_WORLD, &size);); (8, MPI, rc = MPI_Comm_get_attr(MPI_COMM_WORLD, MPI_UNIVERSE_SIZE, &v, &flag);)"}
{"predict": "(5, CUDA, cudaError_t result = cudaSetDevice(i);); (6, CUDA, std::cerr << \"cudaSetDevice() failed: \" << cudaGetErrorString(result) << std::endl;)", "label": "(4, CUDA, cudaError_t result = cudaSetDevice(i);); (6, CUDA, nvDebug(\"*** CUDA Error: %s\\n\", cudaGetErrorString(result));)"}
{"predict": "(207, MPI, MPI_Recv(&(ARRAY(0,0)),1,MPI_DOUBLE,root,888,MPI_COMM_WORLD,&status);); (90, MPI, MPI_Bcast(&m,          1, MPI_INT, root, MPI_COMM_WORLD);); (92, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (167, MPI, MPI_Send(&corner_val,1,MPI_DOUBLE,root,888,MPI_COMM_WORLD);); (89, MPI, MPI_Bcast(&n,          1, MPI_INT, root, MPI_COMM_WORLD);); (164, MPI, MPI_Recv(&(ARRAY(end,j)),1,MPI_DOUBLE,my_ID+1, j, MPI_COMM_WORLD,&status);); (85, MPI, MPI_Bcast(&grp,        1, MPI_INT, root, MPI_COMM_WORLD);); (20, MPI, MPI_Init(&argc,&argv);); (168, MPI, MPI_Send(&n,          1, MPI_INT, my_ID+1, j, MPI_COMM_WORLD);); (164, MPI, MPI_Send(&m,          1, MPI_INT, my_ID+1, j, MPI_COMM_WORLD);); (165, MPI, MPI_Send(&grp,        1, MPI_INT, my_ID+1, j, MPI_COMM_WORLD);); (166, MPI, MPI_Send(&n,          1, MPI_INT, my_ID+1, j, MPI_COMM_WORLD);)", "label": "(180, MPI, MPI_Recv(inbuf, jjsize, MPI_DOUBLE, my_ID-1, j, MPI_COMM_WORLD, &status);); (91, MPI, MPI_Bcast(&m,          1, MPI_INT, root, MPI_COMM_WORLD);); (91, MPI, MPI_Bcast(&n,          1, MPI_INT, root, MPI_COMM_WORLD);); (92, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (244, MPI, MPI_Finalize();); (166, MPI, MPI_Send(&(ARRAY(end,j)), 1, MPI_DOUBLE, my_ID+1, j, MPI_COMM_WORLD);); (198, MPI, MPI_Send(&corner_val,1,MPI_DOUBLE,0,888,MPI_COMM_WORLD);); (200, MPI, MPI_Recv(&(ARRAY(0,0)),1,MPI_DOUBLE,root,888,MPI_COMM_WORLD,&status);); (19, MPI, MPI_Init(&argc,&argv);); (186, MPI, MPI_Send(outbuf, jjsize, MPI_DOUBLE, my_ID+1, j, MPI_COMM_WORLD);); (141, MPI, MPI_Barrier(MPI_COMM_WORLD);); (18, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (82, MPI, MPI_Bcast(&grp,        1, MPI_INT, root, MPI_COMM_WORLD);); (15, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(35, CUDA, CUDA_CHECK(cudaFree(d_work));); (14, CUDA, CUDA_CHECK(cudaMalloc(&d_work, workspaceInBytesOnDevice));); (16, CUDA, CUDA_CHECK(cudaMallocHost(&h_work, workspaceInBytesOnHost));); (30, CUDA, CUDA_CHECK(cudaFreeHost(h_work));)", "label": "(33, CUDA, CUDA_CHECK(cudaFree(d_work));); (14, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_work), workspaceInBytesOnDevice));)"}
{"predict": "(12, OpenMP, #pragma omp target enter data map(alloc); (4, OpenMP, #pragma omp target map(tofrom : warp_size)); (137, OpenMP, #pragma omp target exit data map(release)", "label": "(12, OpenMP, #pragma omp target enter data map(alloc); (4, OpenMP, #pragma omp target map(tofrom : warp_size)); (135, OpenMP, #pragma omp target exit data map(release)"}
{"predict": "(60, OpenCL, err = clEnqueueReadBuffer(queue, bufC, CL_TRUE, 0, 9 * sizeof(cl_mem), &bufS, 0, NULL, NULL);); (82, OpenCL, clReleaseMemObject(bufC);); (67, OpenCL, err = clWaitForEvents(1, &event);); (43, OpenCL, bufS = clCreateBuffer(ctx, CL_MEM_READ_WRITE, 9 * sizeof(cl_mem), NULL, &err);); (75, OpenCL, clReleaseEvent(event);); (39, OpenCL, bufC = clCreateBuffer(ctx, CL_MEM_READ_WRITE, 9 * sizeof(cl_mem), NULL, &err);); (49, OpenCL, err = clEnqueueWriteBuffer(queue, bufSB, CL_TRUE, 0, 9 * sizeof(cl_mem), &bufS, 0, NULL, NULL);); (27, OpenCL, clReleaseContext(ctx);); (45, OpenCL, err = clEnqueueWriteBuffer(queue, bufSA, CL_TRUE, 0, 9 * sizeof(cl_mem), &bufS, 0, NULL, NULL);); (17, OpenCL, printf( \"clCreateCommandQueue() failed with %d\\n\", err );); (40, OpenCL, err = clEnqueueWriteBuffer(queue, bufC, CL_TRUE, 0, 9 * sizeof(cl_mem), &bufS, 0, NULL, NULL);); (13, OpenCL, queue = clCreateCommandQueue(ctx, device, 0, &err);); (46, OpenCL, err = clEnqueueReadBuffer(queue, bufS, CL_TRUE, 0, 9 * sizeof(cl_mem), &bufS, 0, NULL, NULL);); (27, OpenCL, clReleaseCommandQueue(queue);); (15, OpenCL, clReleaseContext(ctx);); (19, OpenCL, printf( \"clCreateMemObject() failed with %d\\n\", err );); (34, OpenCL, err = clEnqueueReadBuffer(queue, bufSB, CL_TRUE, 0, 9 * sizeof(cl_mem), &bufS, 0, NULL, NULL);); (20, OpenCL, bufSB = clCreateBuffer(ctx, CL_MEM_READ_WRITE, 9 * sizeof(cl_mem), NULL, &err);); (17, OpenCL, buf", "label": "(50, OpenCL, bufSA = clCreateBuffer(ctx, CL_MEM_READ_WRITE, sizeof(cl_float), NULL, &err);); (52, OpenCL, bufS  = clCreateBuffer(ctx, CL_MEM_READ_WRITE, sizeof(cl_float), NULL, &err);); (67, OpenCL, err = clWaitForEvents(1, &event);); (55, OpenCL, err = clEnqueueWriteBuffer(queue, bufS, CL_TRUE, 0, sizeof(cl_float), &S, 0, NULL, NULL);); (83, OpenCL, clReleaseMemObject(bufC);); (77, OpenCL, clReleaseEvent(event);); (79, OpenCL, clReleaseMemObject(bufSA);); (68, OpenCL, err = clEnqueueReadBuffer(queue, bufS, CL_TRUE, 0, sizeof(cl_float), &S, 0, NULL, NULL);); (28, OpenCL, clReleaseContext(ctx);); (63, OpenCL, err = clEnqueueReadBuffer(queue, bufSA, CL_TRUE, 0, sizeof(cl_float), &SA, 0, NULL, NULL);); (47, OpenCL, err = clEnqueueWriteBuffer(queue, bufC, CL_TRUE, 0, sizeof(cl_float), &C, 0, NULL, NULL);); (62, OpenCL, err = clEnqueueReadBuffer(queue, bufSB, CL_TRUE, 0, sizeof(cl_float), &SB, 0, NULL, NULL);); (32, OpenCL, clReleaseCommandQueue(queue);); (38, OpenCL, bufSB = clCreateBuffer(ctx, CL_MEM_READ_WRITE, sizeof(cl_float), NULL, &err);); (74, OpenCL, clReleaseMemObject(bufS);); (71, OpenCL, clReleaseMemObject(bufSB);); (19, OpenCL, printf( \"clCreateCommandQueue() failed with %d\\n\", err );); (16, OpenCL, queue = clCreateCommandQueue(ctx, device, 0, &err);); (34, OpenCL, bufC  = clCreateBuffer(ctx, CL_MEM_READ_WRITE, sizeof(cl_float), NULL, &err);); (8, OpenCL, ctx = clCreateContext(props, 1, &device, NULL, NULL, &err);); (9, OpenCL, printf( \"clCreateContext() failed with %d\\n\", err );); (34, OpenCL, err = clEnqueueWriteBuffer(queue, bufSA, CL_TRUE, 0, sizeof(cl_float), &SA, 0, NULL, NULL);); (34, OpenCL, err = clEnqueueWriteBuffer(queue, bufSB, CL_TRUE, 0, sizeof(cl_float), &SB, 0, NULL, NULL);); (51, OpenCL, err = clEnqueueReadBuffer(queue, bufC, CL_TRUE, 0, sizeof(cl_float), &C, 0, NULL, NULL);)"}
{"predict": "(172, CUDA, checkCudaErrors(cudaFree(d_output));); (91, CUDA, cudaError_t cudaResult = cudaMemset(d_directions, 0, n_dimensions * n_directions * sizeof(unsigned int));); (172, CUDA, checkCudaErrors(cudaFree(d_directions));); (56, CUDA, findCudaDevice(argc, (const char **)argv);); (121, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (84, CUDA, checkCudaErrors(cudaMemset(d_output, 0, n_vectors * n_dimensions * sizeof(float)));); (165, CUDA, checkCudaErrors(cudaFree(d_input));); (50, CUDA, checkCudaErrors(cudaMalloc((void **)&d_input, n_vectors * n_dimensions * sizeof(float)));); (164, CUDA, checkCudaErrors(cudaFree(d_long));); (160, CUDA, checkCudaErrors(cudaFree(d_short));); (76, CUDA, checkCudaErrors(cudaMalloc((void **)&d_long, n_vectors * n_dimensions * sizeof(float)));); (160, CUDA, checkCudaErrors(cudaFree(d_output));); (152, CUDA, checkCudaErrors(cudaFree(d_vector));); (159, CUDA, checkCudaErrors(cudaFree(d_directions));); (69, CUDA, checkCudaErrors(cudaMalloc((void **)&d_vector, n_vectors * n_dimensions * sizeof(float)));); (71, CUDA, checkCudaErrors(cudaMemset(d_input, 0, n_vectors * n_dimensions * sizeof(float)));); (150, CUDA, checkCudaErrors(cudaFree(d_input));); (71, CUDA, checkCudaErrors(cudaMemset(d_output, 0, n_vectors * n_dimensions * sizeof(float)));)", "label": "(93, CUDA, throw std::runtime_error(cudaGetErrorString(cudaResult));); (57, CUDA, findCudaDevice(argc, (const char **)argv);); (117, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (211, CUDA, checkCudaErrors(cudaFree(d_output));); (209, CUDA, checkCudaErrors(cudaFree(d_directions));)"}
{"predict": "(92, OpenMP, #pragma omp distribute parallel for private(m)); (84, OpenMP, #pragma omp distribute parallel for private(j)); (80, OpenMP, #pragma omp parallel private(i)); (61, OpenMP, #pragma omp distribute parallel for private(h, B::x)); (73, OpenMP, #pragma omp distribute parallel for private(i)); (40, OpenMP, #pragma omp distribute parallel for private(a, b)); (17, OpenMP, #pragma omp distribute parallel for private(argc); (51, OpenMP, #pragma omp distribute parallel for private(e, g)); (56, OpenMP, #pragma omp distribute parallel for private(e, g)); (18, OpenMP, #pragma omp distribute parallel for private(argc,); (55, OpenMP, #pragma omp distribute parallel for private(e)); (39, OpenMP, #pragma omp distribute parallel for private(argv[1])); (60, OpenMP, #pragma omp parallel); (60, OpenMP, #pragma omp distribute parallel for private(i)); (74, OpenMP, #pragma omp parallel reduction(+ : i)); (2, OpenMP, #pragma omp distribute parallel for private(); (17, OpenMP, #pragma omp distribute parallel for private(argc > 0? argv[1] : argv[2])); (-6, OpenMP, #pragma omp target); (39, OpenMP, #pragma omp distribute parallel for private(S1)); (14, OpenMP, #pragma omp distribute parallel for private(argc)); (58, OpenMP, #pragma omp distribute parallel for private(j)); (3, OpenMP, #pragma omp distribute parallel for private()); (54, OpenMP, #pragma omp parallel shared(i)); (-8, OpenMP, #pragma omp teams)", "label": "(82, OpenMP, #pragma omp parallel private(i)); (95, OpenMP, #pragma omp distribute parallel for private(m)); (5, OpenMP, #pragma omp target); (26, OpenMP, #pragma omp distribute parallel for private(argc,); (60, OpenMP, #pragma omp distribute parallel for private(h, B::x)); (64, OpenMP, #pragma omp distribute parallel for nowait); (8, OpenMP, #pragma omp distribute parallel for private(); (65, OpenMP, #pragma omp parallel); (16, OpenMP, #pragma omp distribute parallel for private(argc); (40, OpenMP, #pragma omp distribute parallel for private(a, b)); (49, OpenMP, #pragma omp distribute parallel for private(e, g)); (33, OpenMP, #pragma omp distribute parallel for private(S1)); (42, OpenMP, #pragma omp distribute parallel for private(argv[1])); (6, OpenMP, #pragma omp distribute parallel for private()); (-5, OpenMP, #pragma omp distribute parallel for private); (62, OpenMP, #pragma omp distribute parallel for private(i)); (69, OpenMP, #pragma omp distribute parallel for private(j)); (22, OpenMP, #pragma omp distribute parallel for private(argc)); (16, OpenMP, #pragma omp distribute parallel for private(argc > 0? argv[1] : argv[2])); (62, OpenMP, #pragma omp parallel shared(i)); (-12, OpenMP, #pragma omp teams)"}
{"predict": "(18, OpenMP, #pragma omp target enter data map(tofrom: i) device (argc), device (argc+1)); (13, OpenMP, #pragma omp target enter data map(tofrom: i) device (argc), device (argc+1)); (0, OpenMP, #pragma omp target enter data map(tofrom: i) device); (9, OpenMP, #pragma omp target enter data map(tofrom: i) device (argc + argc*z)); (0, OpenMP, #pragma omp target enter data map(tofrom: i) device (); (11, OpenMP, #pragma omp target enter data map(tofrom: i) device (-2)); (2, OpenMP, #pragma omp target enter data map(tofrom: i) device (argc))); (11, OpenMP, #pragma omp target enter data map(tofrom: i) device (3.14)); (2, OpenMP, #pragma omp target enter data map(tofrom: i) device (argc > 0? argv[1] : argv[2])); (-8, OpenMP, #pragma omp target enter data map(tofrom: i) device); (7, OpenMP, #pragma omp target enter data map(tofrom: i) device (-10u))", "label": "(6, OpenMP, #pragma omp target parallel for device ()); (23, OpenMP, #pragma omp target parallel for device (3.14)); (18, OpenMP, #pragma omp target parallel for device (-2)); (13, OpenMP, #pragma omp target parallel for device (argc), device (argc+1)); (14, OpenMP, #pragma omp target parallel for device (S1)); (5, OpenMP, #pragma omp target parallel for device (argc))); (-4, OpenMP, #pragma omp target parallel for device); (1, OpenMP, #pragma omp target parallel for device (argc); (14, OpenMP, #pragma omp target parallel for device (-10u)); (5, OpenMP, #pragma omp target parallel for device (z + argc)); (-6, OpenMP, #pragma omp target parallel for device (); (1, OpenMP, #pragma omp target parallel for device (argc > 0? argv[1] : argv[2]))"}
{"predict": "(6, OpenMP, #pragma omp target map(from:c))", "label": "(6, OpenMP, #pragma omp target map(from:c))"}
{"predict": "(33, CUDA, __shared__ double s_lsum[MAXSUPER];); (34, CUDA, volatile __shared__ int s_bmod;)", "label": "(35, CUDA, volatile __shared__ int s_bmod;); (32, CUDA, __shared__ double s_lsum[MAXSUPER];)"}
{"predict": "(49, MPI, MPI_Bcast(prg, sizeof(prg), MPI_BYTE, 0, comm);); (16, MPI, MPI_Comm_dup(comm, &prg->comm);); (42, MPI, MPI_Comm_rank(comm, &rank);); (40, MPI, MPI_Bcast(&rank, 1, MPI_INT, 0, comm);)", "label": "(47, MPI, MPI_Ibcast(&(prg->keep_going), 1, MPI_INT, 0, prg->comm, &(prg->bcast_req));); (15, MPI, MPI_Comm_dup(comm, &prg->comm);); (37, MPI, prg->time_start = MPI_Wtime();); (42, MPI, MPI_Comm_rank(prg->comm, &rank);)"}
{"predict": "(28, MPI, MPI_DATATYPE_ALLOC(uval_new, lenv);); (25, MPI, MPI_INT_ALLOC(usub_new, lens);); (59, MPI, MPI_INT_ALLOC(lsub_new, len1);); (60, MPI, MPI_DATATYPE_ALLOC(lnzval_new, len2);)", "label": "(28, MPI, MPI_DATATYPE_ALLOC(uval_new, lenv);); (25, MPI, MPI_INT_ALLOC(usub_new, lens);); (59, MPI, MPI_INT_ALLOC(lsub_new, len1);); (60, MPI, MPI_DATATYPE_ALLOC(lnzval_new, len2);)"}
{"predict": "(28, CUDA, getLastCudaError(\"mergeRanksAndIndicesKernel() execution failed\\n\");); (18, CUDA, getLastCudaError(\"mergeRanksAndIndicesKernel() execution failed\\n\");)", "label": "(28, CUDA, getLastCudaError(\"mergeRanksAndIndicesKernel(B)<<<>>> failed\\n\");); (18, CUDA, getLastCudaError(\"mergeRanksAndIndicesKernel(A)<<<>>> failed\\n\");)"}
{"predict": "(8, CUDA, cudaFuncGetAttributes(&attr,ptr);); (6, CUDA, cudaFuncGetBlockSize(&qblocksize,&mingridsize,ptr);)", "label": "(9, CUDA, cudaFuncGetAttributes(&attr,(void*)ptr);); (14, CUDA, fcuda::Check_CudaErroorFun(\"Error collecting kernel information.\");); (5, CUDA, cudaOccupancyMaxPotentialBlockSize(&mingridsize,&qblocksize,(void*)ptr,0,0);)"}
{"predict": "(20, TBB, tbb::task_scheduler_init init(n_threads);); (1, TBB, n_threads = tbb::task_scheduler_init::default_num_threads();); (26, TBB, tbb::enumerable_thread_specific<tbb::task_arena*> tls;); (55, TBB, tbb::task_arena::isolate(observer[i]);); (65, TBB, tbb::task_arena::remove-worker(tls[i]);); (54, TBB, tbb::task_arena::join(tls[i]);); (-2, TBB, tbb::global_control control(tbb::global_control::max_allowed_parallelism, n_threads - 1);); (18, TBB, tbb::task_arena** tls;); (62, TBB, tbb::task_arena::destroy(tls[i]);)", "label": "(20, TBB, tbb::global_control control(tbb::global_control::max_allowed_parallelism, n_threads);); (1, TBB, n_threads = tbb::this_task_arena::max_concurrency();); (29, TBB, tbb::enumerable_thread_specific<tbb::task_arena*> tls;); (25, TBB, std::vector<tbb::task_arena> arenas(n_arenas);)"}
{"predict": "(15, OpenCL, int error = clGetDeviceInfo( device, CL_DEVICE_IMAGE2D_MAX_WIDTH, sizeof( maxWidth ), &maxWidth, NULL );); (16, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_GLOBAL_MEM_SIZE, sizeof( memSize ), &memSize, NULL );); (14, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_MAX_MEM_ALLOC_SIZE, sizeof( maxAllocSize ), &maxAllocSize, NULL );)", "label": "(15, OpenCL, int error = clGetDeviceInfo( device, CL_DEVICE_IMAGE2D_MAX_WIDTH, sizeof( maxWidth ), &maxWidth, NULL );); (16, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_GLOBAL_MEM_SIZE, sizeof( memSize ), &memSize, NULL );); (14, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_MAX_MEM_ALLOC_SIZE, sizeof( maxAllocSize ), &maxAllocSize, NULL );)"}
{"predict": "(36, MPI, MPI_Comm_size(intercomm, &size);); (56, MPI, MPI_Comm_disconnect(&intercomm);); (102, MPI, MPI_Finalize();); (19, MPI, MPI_Comm_get_parent(&parentcomm);); (47, MPI, MPI_Recv(&data, 1, MPI_INT, RECEIVER_RANK, DATA_TAG, intercomm, &status);); (79, MPI, MPI_Abort(intercomm, 1);); (73, MPI, MPI_Recv(&i, 1, MPI_INT, SENDER_RANK, DATA_TAG, intercomm, &status);); (44, MPI, MPI_Send(&i, 1, MPI_INT, RECEIVER_RANK, DATA_TAG, intercomm);); (31, MPI, MPI_Comm_rank(intercomm, &rank);); (29, MPI, MPI_Comm_remote_size(intercomm, &rsize);); (27, MPI, MPI_Comm_set_name(intercomm, (char *) \"Intercomm\");); (39, MPI, MPI_Send(&data, 1, MPI_INT, SENDER_RANK, DATA_TAG, intercomm);)", "label": "(36, MPI, MPI_Comm_size(intercomm, &size);); (56, MPI, MPI_Comm_disconnect(&intercomm);); (102, MPI, MPI_Finalize();); (19, MPI, MPI_Comm_get_parent(&parentcomm);); (47, MPI, MPI_Recv(&data, 1, MPI_INT, RECEIVER_RANK, DATA_TAG, intercomm, &status);); (79, MPI, MPI_Abort(intercomm, 1);); (73, MPI, MPI_Recv(&i, 1, MPI_INT, SENDER_RANK, DATA_TAG, intercomm, &status);); (43, MPI, MPI_Send(&i, 1, MPI_INT, RECEIVER_RANK, DATA_TAG, intercomm);); (29, MPI, MPI_Comm_rank(intercomm, &rank);); (26, MPI, MPI_Comm_remote_size(intercomm, &rsize);); (76, MPI, MPI_Send(&i, 1, MPI_INT, SENDER_RANK, DATA_TAG, intercomm);)"}
{"predict": "(261, MPI, MPI_Wait(&(request[1+4]), MPI_STATUS_IGNORE);); (261, MPI, MPI_Wait(&(request[3+4]), MPI_STATUS_IGNORE);); (225, MPI, MPI_Barrier(MPI_COMM_WORLD);); (261, MPI, MPI_Wait(&(request[0+4]), MPI_STATUS_IGNORE);); (271, MPI, MPI_Finalize();); (40, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (235, MPI, MPI_Wait(&(request[0]), MPI_STATUS_IGNORE);); (255, MPI, MPI_Wait(&(request[2+4]), MPI_STATUS_IGNORE);); (256, MPI, MPI_Wait(&(request[3]), MPI_STATUS_IGNORE);); (255, MPI, MPI_Wait(&(request[2]), MPI_STATUS_IGNORE);); (32, MPI, MPI_Init(&argc,&argv);); (255, MPI, MPI_Wait(&(request[1]), MPI_STATUS_IGNORE);); (256, MPI, MPI_Wait(&(request[2+1]), MPI_STATUS_IGNORE);); (257, MPI, MPI_Wait(&(request[3+1]), MPI_STATUS_IGNORE);); (32, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);); (233, MPI, MPI_Wait(&(request[1]), MPI_STATUS_IGNORE);); (256, MPI, MPI_Wait(&(request[0+1]), MPI_STATUS_IGNORE);); (232, MPI, MPI_Wait(&(request[2]), MPI_STATUS_IGNORE);); (256, MPI, MPI_Wait(&(request[1+1]), MPI_STATUS_IGNORE);); (259, MPI, MPI_Reduce(&local_norm, &norm, 1, MPI_DTYPE, MPI_SUM, root, MPI_COMM_WORLD);); (233, MPI, MPI_Wait(&(request[3]), MPI_STATUS_IGNORE);); (234, MPI, MPI_Wait(&(request[2+1]), MPI_STATUS_IGNORE);); (235, MPI, MPI_Wait(&(request[3+1]), MPI_STATUS_IGNORE);); (256, MPI, MPI_Reduce(&my_ID, &my_IDx, 1, MPI_INT, MPI_PROD, root, MPI_COMM_WORLD);); (256, MPI, MPI_Reduce(&my_ID, &my_IDy, 1, MPI_INT, MPI_PROD, root, MPI_COMM_WORLD);); (23, MPI, MPI_Bcast(&n,          1, MPI_INT,", "label": "(295, MPI, MPI_Wait(&(request[1+4]), MPI_STATUS_IGNORE);); (301, MPI, MPI_Wait(&(request[3+4]), MPI_STATUS_IGNORE);); (234, MPI, MPI_Barrier(MPI_COMM_WORLD);); (128, MPI, MPI_Bcast(&n,          1, MPI_INT, root, MPI_COMM_WORLD);); (43, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (289, MPI, MPI_Wait(&(request[0+4]), MPI_STATUS_IGNORE);); (254, MPI, MPI_Wait(&(request[0]), MPI_STATUS_IGNORE);); (294, MPI, MPI_Wait(&(request[2+4]), MPI_STATUS_IGNORE);); (124, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (36, MPI, MPI_Init(&argc,&argv);); (251, MPI, MPI_Wait(&(request[1]), MPI_STATUS_IGNORE);); (256, MPI, MPI_Wait(&(request[2]), MPI_STATUS_IGNORE);); (365, MPI, MPI_Finalize();); (255, MPI, MPI_Wait(&(request[3]), MPI_STATUS_IGNORE);); (324, MPI, MPI_Reduce(&local_norm, &norm, 1, MPI_DTYPE, MPI_SUM, root, MPI_COMM_WORLD);); (31, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(6, CUDA, extent = make_cudaExtent(30, 40, 0);); (3, CUDA, cudaExtent extent = make_cudaExtent(10, 20, 0);)", "label": "(6, CUDA, extent = make_cudaExtent(30, 40, 0);); (3, CUDA, cudaExtent extent = make_cudaExtent(10, 20, 0);)"}
{"predict": "(14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (37, MPI, MPI_Comm_free(&cart);); (12, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);)", "label": "(13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (37, MPI, MPI_Comm_free(&cart);); (12, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);)"}
{"predict": "(5, MPI, MPI_Comm_rank(comm, &rank);); (20, MPI, MPI_Recv(buff, count, type, 0, tag, comm, MPI_STATUS_IGNORE);)", "label": "(20, MPI, MPI_Recv(buff, count, type, 0, tag, comm, MPI_STATUS_IGNORE);); (4, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(54, OpenMP, #pragma omp parallel for)", "label": "(55, OpenMP, #pragma omp for schedule(guided, 512)); (52, OpenMP, #pragma omp parallel firstprivate(transformation_matrix, constant_vector, slave_solution_vector, master_solution_vector, p_master_node, p_slave_node))"}
{"predict": "(19, MPI, MPI_Irecv(&a, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, &request);); (16, MPI, MPI_Wait(&request, MPI_STATUS_IGNORE);); (20, MPI, MPI_Send(&a, 1, MPI_INT, 1, 0, MPI_COMM_WORLD);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (2, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (14, MPI, MPI_Cancel(&request);); (19, MPI, MPI_Request_free(&request);); (4, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (8, MPI, MPI_Send_init(&a, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, &request);)", "label": "(24, MPI, MPI_Request_free(&request);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (15, MPI, MPI_Psend_init(&a, 1, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, MPI_INFO_NULL, &request);); (16, MPI, MPI_Precv_init(&a, 1, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_INFO_NULL, &request);); (21, MPI, MPI_Barrier(MPI_COMM_WORLD);); (6, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(199, MPI, MPI_Get_address(read_buf + real_off - off, &addr);); (109, MPI, MPI_Type_size_x(datatype, &size);); (70, MPI, MPI_Allreduce(&ntimes, &max_ntimes, 1, MPI_INT, MPI_MAX, fd->comm);); (110, MPI, MPI_Comm_rank(fd->comm, &rank);); (98, MPI, MPI_Alltoall(send_size, 1, MPI_COUNT, recv_size, 1, MPI_COUNT, fd->comm);)", "label": "(113, MPI, MPI_Comm_rank(fd->comm, &rank);); (180, MPI, MPI_Get_count(&read_status, MPI_BYTE, &actual_size);); (105, MPI, MPI_Type_get_extent(datatype, &lb, &buftype_extent);); (69, MPI, MPI_Allreduce(&ntimes, &max_ntimes, 1, MPI_INT, MPI_MAX, fd->comm);); (219, MPI, MPI_Get_address(read_buf + req_off - real_off, &addr);)"}
{"predict": "(8, OpenMP, double start=omp_get_wtime();); (43, OpenMP, double end=omp_get_wtime();)", "label": "(16, OpenMP, #pragma omp target teams distribute parallel for thread_limit(128)); (8, OpenMP, #pragma omp target enter data map(to: Vm[:nCells])); (6, OpenMP, #pragma omp target enter data map(to: m_gate[:nCells]))"}
{"predict": "(7, MPI, MPI_Win_get_attr(win, key[i], &val_p, &flag);); (3, MPI, MPI_Win win;)", "label": "(6, MPI, MPI_Win_get_attr(win, key[i], &val_p, &flag);)"}
{"predict": "(23, CUDA, cudaMemcpy(output, transformCoeff, sizeof(int)*nOutputDims,cudaMemcpyHostToDevice);); (20, CUDA, cudaMemcpy(workspace, transformCoeff, sizeof(int)*nOutputDims,cudaMemcpyHostToDevice);)", "label": "(23, CUDA, cudaMemcpy(output, data, copySize, cudaMemcpyDeviceToDevice);); (20, CUDA, cudaMemcpy(workspace, transformCoeff, sizeof(int)*nOutputDims,cudaMemcpyHostToDevice );)"}
{"predict": "(21, MPI, MPI_Type_free(&mystruct);); (20, MPI, MPI_Type_commit(&vecs[i]);); (15, MPI, MPI_Type_create_struct(3, blockcount, displs, vecs, &mystruct);); (14, MPI, MPI_Type_commit(&mystruct);); (15, MPI, MPI_Type_free(&vecs[i]);); (22, MPI, MPI_Type_create_resized(mystruct, 0, 10, &vecs[i]);)", "label": "(19, MPI, MPI_Type_commit(&mystruct);); (27, MPI, MPI_Type_create_hvector(0, 1, stride, MPI_INT, &vecs[i]);); (19, MPI, MPI_Type_free(&mystruct);); (8, MPI, MPI_Type_commit(&vecs[i]);); (14, MPI, MPI_Type_create_struct(3, blockcount, displs, vecs, &mystruct);); (5, MPI, MPI_Type_create_hvector(i, 1, stride, MPI_INT, &vecs[i]);); (17, MPI, MPI_Type_free(&vecs[i]);)"}
{"predict": "(14, CUDA, checkCudaErrors(cudaMalloc(&info, sizeof(int)));); (50, CUDA, checkCudaErrors(cudaFree(A));); (36, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (48, CUDA, checkCudaErrors(cudaFree(buffer));); (37, CUDA, checkCudaErrors(cudaMemcpy(x, b, sizeof(double) * n, cudaMemcpyDeviceToHost));); (26, CUDA, checkCudaErrors(cudaMemcpy(&h_info, info, sizeof(int), cudaMemcpyDeviceToHost));); (8, CUDA, checkCudaErrors(cudaMalloc(&buffer, sizeof(double) * bufferSize));); (7, CUDA, checkCudaErrors(cudaMalloc(&A, sizeof(double) * lda * n));); (46, CUDA, checkCudaErrors(cudaFree(info));); (27, CUDA, checkCudaErrors(cudaMemset(info, 0, sizeof(int)));); (5, CUDA, checkCudaErrors(cudaMemset(buffer, 0, sizeof(double) * bufferSize));); (20, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (7, CUDA, checkCudaErrors(cudaMemset(A, 0, sizeof(double) * lda * n));); (-4, CUDA, checkCudaErrors(cudaMalloc(&Acopy, sizeof(double) * lda * n));); (43, CUDA, checkCudaErrors(cudaFree(Acopy));); (11, CUDA, checkCudaErrors(cudaTimingEventRecord(start, 0));); (39, CUDA, checkCudaErrors(cudaFree(info));); (12, CUDA, checkCudaErrors(cudaTimingEventSynchronize(stop));); (37, CUDA, checkCudaErrors(cudaFree(buffer));)", "label": "(14, CUDA, checkCudaErrors(cudaMalloc(&info, sizeof(int)));); (14, CUDA, checkCudaErrors(cudaMalloc(&buffer, sizeof(double) * bufferSize));); (14, CUDA, checkCudaErrors(cudaMalloc(&A, sizeof(double) * lda * n));); (36, CUDA, checkCudaErrors(cusolverDnDpotrs(handle, uplo, n, 1, A, lda, x, n, info));); (37, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (43, CUDA, checkCudaErrors(cudaFree(info));); (45, CUDA, checkCudaErrors(cudaFree(buffer));); (14, CUDA, checkCudaErrors(cudaMemset(info, 0, sizeof(int)));); (46, CUDA, checkCudaErrors(cudaFree(A));)"}
{"predict": "(3, OpenMP, #pragma omp target map(tofrom: tmp)); (11, OpenMP, #pragma omp target map(tofrom: tmp) nowait)", "label": "(12, OpenMP, #pragma omp target map(tofrom: N) map(from:tmp)); (2, OpenMP, #pragma omp target map(to: N) map(tofrom: tmp))"}
{"predict": "(4, MPI, MPI_Init(&argc, &argv);); (52, MPI, MPI_Finalize();); (5, MPI, ierr = MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, int ierr = MPI_Comm_size(MPI_COMM_WORLD, &comm_size);)", "label": "(4, MPI, MPI_Init(&argc, &argv);); (52, MPI, MPI_Finalize();); (6, MPI, ierr = MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, int ierr = MPI_Comm_size(MPI_COMM_WORLD, &comm_size);)"}
{"predict": "(111, CUDA, FastllmCudaFree(cudaFp16Output);); (2, CUDA, state = cudaMemcpy(cudaScales, weight.scales.data(), k * sizeof(float), cudaMemcpyHostToDevice);); (107, CUDA, FastllmCudaFree(cudaFp16Input);); (22, CUDA, FastllmCudaFree(cudaMins);); (15, CUDA, state = cudaMemset(cudaBiasData, 0, k * sizeof(float));); (7, CUDA, state = cudaMalloc(&cudaBiasData, k * sizeof(float));); (46, CUDA, cudaFp16Output = (half *) FastllmCudaMalloc(n * k * sizeof(half));); (46, CUDA, cudaFp16Weight = (half *) FastllmCudaMalloc(k * m * sizeof(half));); (-4, CUDA, state = cudaMalloc(&cudaMins, k * sizeof(float));); (102, CUDA, FastllmCudaFree(cudaFp32Input);); (18, CUDA, checkCudaErrors(\"Error: CUDA error when moving bias to device!\", state);); (101, CUDA, FastllmCudaFree(cudaFp32Output);); (97, CUDA, FastllmCudaFree(cudaFp16Weight);); (100, CUDA, FastllmCudaFree(cudaFp32Input);); (15, CUDA, state = cudaMemcpy(cudaBiasData, (uint8_t*)bias.cudaData, k * sizeof(float), cudaMemcpyDeviceToDevice);); (4, CUDA, state = cudaMalloc(&cudaFp16Input, n * m * sizeof(half));); (101, CUDA, FastllmCudaFree(cudaFp16Bias);); (-9, CUDA, state = cudaMalloc(&cudaScales, k * sizeof(float));); (93, CUDA, FastllmCudaFree(cudaFp16Weight);); (96, CUDA, FastllmCudaFree(cudaFp16Scales);); (-9, CUDA, state = cudaMalloc(&cudaOutput, n * k * sizeof(float));); (96, CUDA, FastllmCudaFree(cudaFp16Output);)", "label": "(119, CUDA, FastllmCudaFinishOutput(output, cudaOutput);); (39, CUDA, cudaFp16Input = (half *) FastllmCudaMalloc(n * m * sizeof(half));); (21, CUDA, state = cudaMemset(cudaBiasData, 0, k * sizeof(float));); (115, CUDA, FastllmCudaFinishInput(input, cudaInput);); (43, CUDA, cudaFp16Output = (half *) FastllmCudaMalloc(n * k * sizeof(half));); (9, CUDA, state = cudaMemcpy(cudaMins, mins, k * sizeof(float), cudaMemcpyHostToDevice);); (-2, CUDA, state = cudaMalloc(&cudaScales, k * sizeof(float));); (94, CUDA, FastllmCudaFree(cudaFp16Input);); (103, CUDA, FastllmCudaFree(cudaFp16Output);); (32, CUDA, cudaFp16Weight = (half *) FastllmCudaMalloc(k * m * sizeof(half));); (-1, CUDA, state = cudaMalloc(&cudaMins, k * sizeof(float));); (-6, CUDA, state = cudaMemcpy(cudaScales, weight.scales.data(), k * sizeof(float), cudaMemcpyHostToDevice);); (7, CUDA, state = cudaMalloc(&cudaBiasData, k * sizeof(float));); (12, CUDA, checkCudaErrors(\"Error: CUDA error when moving bias to device!\", state);); (7, CUDA, state = cudaMemcpy(cudaBiasData, (uint8_t*)bias.cudaData, k * sizeof(float), cudaMemcpyDeviceToDevice);); (87, CUDA, FastllmCudaFree(cudaFp16Weight);); (18, CUDA, float *cudaOutput = (float*)FastllmCudaPrepareOutput(output);); (16, CUDA, float *cudaInput = (float*)FastllmCudaPrepareInput(input);)"}
{"predict": "(12, TBB, g.reset(tbb::flow::rf_clear_edges);); (5, TBB, tbb::flow::queue_node<int> qout(g);); (5, TBB, tbb::flow::make_edge(on,qout);); (-1, TBB, tbb::flow::graph g;)", "label": "(12, TBB, g.reset(tbb::flow::rf_clear_edges);); (4, TBB, tbb::flow::queue_node<int> qout(g);); (5, TBB, tbb::flow::make_edge(on,qout);); (0, TBB, tbb::flow::graph g;)"}
{"predict": "(31, CUDA, CSC(cudaGetLastError(), STATUS_FAILURE);)", "label": "(34, CUDA, CSC(cudaGetLastError(), STATUS_FAILURE);)"}
{"predict": "(8, OpenMP, #pragma omp target teams distribute parallel for map(tofrom: tmp) map(tofrom: M)); (3, OpenMP, #pragma omp target teams distribute parallel for map(tofrom: tmp2))", "label": "(8, OpenMP, #pragma omp target teams distribute parallel for reduction(+:tmp) map(A, N, M) map(tofrom:M))"}
{"predict": "(18, OpenMP, #pragma omp for simd collapse (foobool(argc)), collapse (true), collapse (-5)); (21, OpenMP, #pragma omp for simd collapse (2)); (30, OpenMP, #pragma omp for simd collapse (2)); (13, OpenMP, #pragma omp for simd collapse (S1)); (0, OpenMP, #pragma omp for simd collapse ()); (21, OpenMP, #pragma omp for simd collapse(collapse(tmain<int, char, -1, -2>(argc, argv)); (-5, OpenMP, #pragma omp for simd collapse); (-5, OpenMP, #pragma omp for simd collapse (); (-5, OpenMP, #pragma omp for simd collapse ()); (0, OpenMP, #pragma omp for simd collapse (2+2))); (2, OpenMP, #pragma omp for simd collapse (foobool(1) > 0? 1 : 2)); (-10, OpenMP, #pragma omp for simd collapse)", "label": "(12, OpenMP, #pragma omp for simd collapse (foobool(1) > 0? 1 : 2)); (8, OpenMP, #pragma omp for simd collapse (2+2))); (-1, OpenMP, #pragma omp for simd collapse); (16, OpenMP, #pragma omp for simd collapse (S1)); (-1, OpenMP, #pragma omp for simd collapse (); (0, OpenMP, #pragma omp for simd collapse ()); (26, OpenMP, #pragma omp for simd collapse (2)); (10, OpenMP, #pragma omp for simd collapse (foobool(argc)), collapse (true), collapse (-5)); (-1, OpenMP, #pragma omp for simd collapse (4); (21, OpenMP, #pragma omp for simd collapse(collapse(tmain<int, char, -1, -2>(argc, argv))"}
{"predict": "(36, MPI, MPI_Bcast(&tmp_val, 1, MPI_INT, 0, fd->comm);)", "label": "(35, MPI, MPI_Bcast(&tmp_val, 1, MPI_INT, 0, fd->comm);)"}
{"predict": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(2, TBB, tbb::task_arena sequential_arena(1);)", "label": "(1, TBB, tbb::task_arena sequential_arena(1);)"}
{"predict": "(201, OpenCL, error = clSetKernelArg(kernel, 10, sizeof(dstStride), &dstStride);); (202, OpenCL, error = clSetKernelArg(kernel, 11, sizeof(srcStride), &srcStride);); (203, OpenCL, error = clSetKernelArg(kernel, 12, sizeof(lineCopiesPerWorkItemInt), &lineCopiesPerWorkItemInt);); (204, OpenCL, error = clSetKernelArg(kernel, 13, sizeof(numElementsPerLineInt), &numElementsPerLineInt);); (205, OpenCL, error = clSetKernelArg(kernel, 14, sizeof(lineCopiesPerWorkgroup), &lineCopiesPerWorkgroup);); (200, OpenCL, error = clSetKernelArg(kernel, 8, sizeof(maxLocalWorkgroupSize), &maxLocalWorkgroupSize);); (201, OpenCL, error = clSetKernelArg(kernel, 9, sizeof(maxTotalLines), &maxTotalLines);); (202, OpenCL, error = clSetKernelArg(kernel, 10, sizeof(maxTotalLinesInt), &maxTotalLinesInt);); (200, OpenCL, error = clSetKernelArg(kernel, 7, sizeof(dstMargin), &dstMargin);); (201, OpenCL, error = clSetKernelArg(kernel, 8, sizeof(srcMargin), &srcMargin);); (202, OpenCL, error = clSetKernelArg(kernel, 10, sizeof(1), &one);); (200, OpenCL, error = clSetKernelArg(kernel, 6, sizeof(numElementsPerLine), &numElementsPerLine);); (201, OpenCL, error = clSetKernelArg(kernel, 7, sizeof(1), &one);)", "label": "(169, OpenCL, error = clSetKernelArg(kernel, 0, sizeof(streams[0]), &streams[0]);); (170, OpenCL, error = clSetKernelArg(kernel, 1, sizeof(streams[1]), &streams[1]);); (184, OpenCL, error = clSetKernelArg(kernel, 7, sizeof(dstStride), &dstStride);); (170, OpenCL, error = clSetKernelArg(kernel, 2, localBufferSize, NULL);); (180, OpenCL, error = clSetKernelArg(kernel, 6, sizeof(srcStride), &srcStride);)"}
{"predict": "(11, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(11, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(8, CUDA, __shared__ float shared_val[32];)", "label": "(4, CUDA, __shared__ float shared_val[32];)"}
{"predict": "(60, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (78, CUDA, CUDA_CHECK(cudaDeviceReset());); (75, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (40, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (41, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_y), sizeof(data_type) * y.size()));); (39, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_x), sizeof(data_type) * x.size()));); (67, CUDA, CUDA_CHECK(cudaFree(d_y));); (65, CUDA, CUDA_CHECK(cudaFree(d_x));); (63, CUDA, CUDA_CHECK(cudaFree(d_A));); (30, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)", "label": "(60, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (78, CUDA, CUDA_CHECK(cudaDeviceReset());); (75, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (40, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (41, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_y), sizeof(data_type) * y.size()));); (39, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_x), sizeof(data_type) * x.size()));); (67, CUDA, CUDA_CHECK(cudaFree(d_y));); (65, CUDA, CUDA_CHECK(cudaFree(d_x));); (63, CUDA, CUDA_CHECK(cudaFree(d_A));); (30, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(37, TBB, tbb::flow::make_edge(source2, tbb::flow::input_port<1>(merge));); (35, TBB, tbb::flow::make_edge(source1, tbb::flow::input_port<0>(merge));); (36, TBB, tbb::flow::make_edge(merge, output);); (-1, TBB, tbb::flow::graph g;)", "label": "(37, TBB, tbb::flow::make_edge(source2, tbb::flow::input_port<1>(merge));); (35, TBB, tbb::flow::make_edge(source1, tbb::flow::input_port<0>(merge));); (36, TBB, tbb::flow::make_edge(merge, output);); (-2, TBB, tbb::flow::graph g;)"}
{"predict": "(36, CUDA, CHECK(cudaMalloc(&d_matrix_org, M));); (36, CUDA, CHECK(cudaMalloc(&d_matrix_res, M));); (36, CUDA, CHECK(cudaMemcpy(d_matrix_org, h_matrix_org, M, cudaMemcpyDefault));); (43, CUDA, CHECK(cudaEventCreate(&stop));); (51, CUDA, CHECK(cudaEventRecord(stop));); (47, CUDA, CHECK(cudaMemcpy(h_matrix_res, d_matrix_res, M, cudaMemcpyDefault));); (43, CUDA, cudaEventQuery(start);); (49, CUDA, CHECK(cudaEventSynchronize(stop));); (49, CUDA, CHECK(cudaEventElapsedTime(&curr_time, start, stop));); (87, CUDA, CHECK(cudaFree(d_matrix_org));); (87, CUDA, CHECK(cudaFree(d_matrix_res));); (38, CUDA, CHECK(cudaEventRecord(start));); (34, CUDA, CHECK(cudaEventCreate(&start));); (3, CUDA, CHECK(cudaMemcpyFromSymbol(&SIZE, c_TILE_DIM, sizeof(int)));)", "label": "(37, CUDA, CHECK(cudaMalloc(&d_matrix_org, M));); (37, CUDA, CHECK(cudaMalloc(&d_matrix_res, M));); (37, CUDA, CHECK(cudaMemcpy(d_matrix_org, h_matrix_org, M, cudaMemcpyDefault));); (42, CUDA, CHECK(cudaEventCreate(&stop));); (50, CUDA, CHECK(cudaEventRecord(stop));); (46, CUDA, CHECK(cudaMemcpy(h_matrix_res, d_matrix_res, M, cudaMemcpyDefault));); (41, CUDA, cudaEventQuery(start);); (48, CUDA, CHECK(cudaEventSynchronize(stop));); (48, CUDA, CHECK(cudaEventElapsedTime(&curr_time, start, stop));); (86, CUDA, CHECK(cudaFree(d_matrix_org));); (86, CUDA, CHECK(cudaFree(d_matrix_res));); (35, CUDA, CHECK(cudaEventRecord(start));); (32, CUDA, CHECK(cudaEventCreate(&start));); (3, CUDA, CHECK(cudaMemcpyFromSymbol(&SIZE, c_TILE_DIM, sizeof(int)));)"}
{"predict": "(13, OpenCL, err = clSetKernelArg(k1, 0, sizeof(cl_mem), &buf[1]);); (10, OpenCL, err = clSetKernelArg(k0, 0, sizeof(cl_mem), &buf[0]);)", "label": "(55, OpenCL, clReleaseProgram(program);); (52, OpenCL, clReleaseKernel(k0);); (52, OpenCL, clReleaseKernel(k1);); (11, OpenCL, cl_kernel k1 = clCreateKernel(program, \"compiler_program_global1\", &err);); (8, OpenCL, cl_kernel k0 = clCreateKernel(program, \"compiler_program_global0\", &err);)"}
{"predict": "(10, OpenMP, #pragma omp target map(from: a[0:N]) map(to: aa[0:N]))", "label": "(10, OpenMP, #pragma omp target map(tofrom: a[0:100])); (12, OpenMP, #pragma omp simd)"}
{"predict": "(69, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (70, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpyAsync(device_buffer, input.data(), input_size_bytes, cudaMemcpyHostToDevice, stream));); (64, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&device_buffer, input_size_bytes));); (55, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamCreate(&stream));); (80, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(device_buffer));)", "label": "(66, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&device_buffer, input_size_bytes));); (56, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamCreate(&stream));); (80, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(device_buffer));); (67, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (65, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(device_buffer, input.data(), input_size_bytes, cudaMemcpyHostToDevice));)"}
{"predict": "(25, OpenMP, #pragma omp distribute parallel for simd, private(x);); (2, OpenMP, #pragma omp target); (2, OpenMP, #pragma omp teams); (18, OpenMP, #pragma omp distribute parallel for simd private(x);); (7, OpenMP, #pragma omp distribute parallel for simd;); (22, OpenMP, #pragma omp distribute parallel for simd, lastprivate(x);); (7, OpenMP, #pragma omp distribute parallel for simd linear(x);)", "label": "(12, OpenMP, #pragma omp distribute parallel for simd firstprivate(x);); (5, OpenMP, #pragma omp distribute parallel for simd;); (24, OpenMP, #pragma omp distribute parallel for simd, private(x);); (0, OpenMP, #pragma omp target); (15, OpenMP, #pragma omp distribute parallel for simd private(x);); (-1, OpenMP, #pragma omp teams)"}
{"predict": "(95, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(streams), &streams);); (105, OpenCL, err = clEnqueueNDRangeKernel(commQueues[0], kernel, 1, NULL, &streamCount, &local_size, 0, NULL, outEvents);); (49, OpenCL, cl_program program = clCreateProgramWithSource(ctx, 4, sources, NULL, &err);); (69, OpenCL, cl_kernel kernel = clCreateKernel(program, \"fillBufferU01\", &err);); (95, OpenCL, err |= clSetKernelArg(kernel, 1, sizeof(number_count_per_stream), &number_count_per_stream);); (87, OpenCL, err = clGetDeviceInfo(dev, CL_DEVICE_MAX_WORK_GROUP_SIZE, sizeof(local_size), &local_size, NULL);); (60, OpenCL, err = clBuildProgram(program, 0, NULL, includes, NULL, NULL);); (93, OpenCL, err |= clSetKernelArg(kernel, 2, sizeof(outBuffer), &outBuffer);); (59, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(streams), NULL);); (72, OpenCL, cl_kernel kernel = clCreateKernel(program, \"fillBufferU01\", &err);)", "label": "(95, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(streams), &streams);); (105, OpenCL, clReleaseKernel(kernel);); (85, OpenCL, err = clGetDeviceInfo(dev, CL_DEVICE_MAX_WORK_GROUP_SIZE, sizeof(local_size), &local_size, NULL);); (59, OpenCL, err = clBuildProgram(program, 0, NULL, includes, NULL, NULL);); (103, OpenCL, clReleaseProgram(program);); (48, OpenCL, cl_program program = clCreateProgramWithSource(ctx, 4, sources, NULL, &err);); (96, OpenCL, err = clEnqueueNDRangeKernel(commQueues[0], kernel, 1, NULL, &streamCount, &local_size, numWaitEvents, waitEvents, outEvents);); (89, OpenCL, err |= clSetKernelArg(kernel, 1, sizeof(number_count_per_stream), &number_count_per_stream);); (89, OpenCL, err |= clSetKernelArg(kernel, 2, sizeof(outBuffer), &outBuffer);); (68, OpenCL, cl_kernel kernel = clCreateKernel(program, \"fillBufferU01\", &err);)"}
{"predict": "(3, OpenMP, #pragma omp target data map(alloc: arr[0:100])); (5, OpenMP, #pragma omp target)", "label": "(3, OpenMP, #pragma omp target)"}
{"predict": "(28, OpenMP, #pragma omp teams reduction(+:sum)); (25, OpenMP, #pragma omp target map(tofrom: b, sum, t) map(alloc: a)); (28, OpenMP, #pragma omp distribute parallel for reduction(+:sum)); (30, OpenMP, t[i] = omp_get_team_num();)", "label": "(28, OpenMP, #pragma omp teams reduction(+:sum)); (25, OpenMP, #pragma omp target map(tofrom: b, sum, t) map(alloc: a)); (28, OpenMP, #pragma omp distribute parallel for reduction(+:sum)); (30, OpenMP, t[i] = omp_get_team_num();)"}
{"predict": "(138, CUDA, __shared__ TB smem.smem_B.data();); (132, CUDA, __shared__ TA smem.smem_A.data();)", "label": "(38, CUDA, extern __shared__ char shared_memory[];)"}
{"predict": "(36, MPI, MPI_Mrecv(&(FastLogitsInfo.logits[0]), &(FastLogitsInfo.logits[1]), count, MPI_UINT64_T, &msg, &status);); (22, MPI, MPI_Mprobe(1, kMPI_SPEC_DEC_DATA_TAG, &msg, &status);); (13, MPI, MPI_Get_count(&status, MPI_UINT64_T, &count);); (38, MPI, MPI_Mprobe(0, kMPI_SPEC_DEC_ID_TAG, &msg, &status);); (28, MPI, MPI_Dims_create(expectedSize, 2, dims);)", "label": "(28, MPI, MPICHECK(MPI_Mrecv(&dims, count, MPI_INT64_T, &msg, &status));); (36, MPI, MPICHECK(MPI_Get_count(&status, MPI_UINT8_T, &count));); (40, MPI, MPICHECK(MPI_Mrecv(tensor->data(), count, MPI_UINT8_T, &msg, &status));); (16, MPI, MPICHECK(MPI_Mrecv(&mpiId, count, MPI_UINT64_T, &msg, &status));); (20, MPI, MPICHECK(MPI_Get_count(&status, MPI_INT64_T, &count));); (11, MPI, MPICHECK(MPI_Get_count(&status, MPI_UINT64_T, &count));)"}
{"predict": "(15, CUDA, extern __shared__ float sdata[];)", "label": "(15, CUDA, extern __shared__ float sdata[];)"}
{"predict": "(11, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(11, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(15, OpenCL, clKernelWrapper kernel = clCreateKernel(prog, \"test_usersemantic_deco\", &error);); (22, OpenCL, error = clSetKernelArg(kernel, 0, sizeof(dst), &dst);)", "label": "(15, OpenCL, clKernelWrapper kernel = clCreateKernel(prog, \"usersemantic_test\", &error);); (22, OpenCL, error |= clSetKernelArg(kernel, 0, sizeof(dst), &dst);)"}
{"predict": "(15, CUDA, getLastCudaError(\"mergeElementaryIntervalsKernel<1> failed\\n\");); (20, CUDA, getLastCudaError(\"mergeElementaryIntervalsKernel<0> failed\\n\");)", "label": "(15, CUDA, getLastCudaError(\"mergeElementaryIntervalsKernel<1> failed\\n\");); (19, CUDA, getLastCudaError(\"mergeElementaryIntervalsKernel<0> failed\\n\");)"}
{"predict": "(87, MPI, MPI_Bcast( a,    nnz, MPI_DOUBLE, 0, grid.comm );); (224, MPI, MPI_Finalize();); (83, MPI, MPI_Bcast( &nnz, 1,   mpi_int_t,  0, grid.comm );); (83, MPI, MPI_Bcast( asub, nnz, mpi_int_t,  0, grid.comm );); (83, MPI, MPI_Bcast( xa,   n+1, mpi_int_t,  0, grid.comm );); (78, MPI, MPI_Bcast( &n,   1,   mpi_int_t,  0, grid.comm );); (76, MPI, MPI_Bcast( &m,   1,   mpi_int_t,  0, grid.comm );); (229, MPI, MPI_Ssend( b, m*nrhs, MPI_DOUBLE, 0, 0, grid.comm );); (227, MPI, MPI_Ssend( xtrue, n*nrhs, MPI_DOUBLE, 0, 0, grid.comm );); (81, MPI, MPI_Bcast( a,   nnz, MPI_DOUBLE, 0, grid.comm );); (14, MPI, MPI_Init( &argc, &argv );); (223, MPI, MPI_Ssend( b1, m*nrhs, MPI_DOUBLE, 0, 0, grid.comm );); (81, MPI, MPI_Bcast( xa,   n+1, mpi_int_t,  0, grid.comm );); (78, MPI, MPI_Bcast( a,    nnz, MPI_DOUBLE, 0, grid.comm );); (14, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &myid );); (212, MPI, MPI_Ssend( xtrue, n*nrhs, MPI_DOUBLE, 0, 0, grid.comm );); (79, MPI, MPI_Bcast( &nnz, 1,   mpi_int_t,  0, grid.comm );); (76, MPI, MPI_Bcast( asub, nnz, mpi_int_t,  0, grid.comm );); (77, MPI, MPI_Bcast( &n,   1,   mpi_int_t,  0, grid.comm );); (74, MPI, MPI_Bcast( a,    nnz, MPI_DOUBLE, 0", "label": "(87, MPI, MPI_Bcast( asub, nnz, mpi_int_t,  0, grid.comm );); (224, MPI, MPI_Finalize();); (83, MPI, MPI_Bcast( &nnz, 1,   mpi_int_t,  0, grid.comm );); (83, MPI, MPI_Bcast( a,    nnz, MPI_DOUBLE, 0, grid.comm );); (22, MPI, MPI_Init( &argc, &argv );); (83, MPI, MPI_Bcast( xa,   n+1, mpi_int_t,  0, grid.comm );); (78, MPI, MPI_Bcast( &n,   1,   mpi_int_t,  0, grid.comm );); (76, MPI, MPI_Bcast( &m,   1,   mpi_int_t,  0, grid.comm );)"}
{"predict": "(26, MPI, MPI_Recv(buf, cnt, MPI_INT, thread_num, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (37, MPI, t = MPI_Wtime() - t;); (22, MPI, t = MPI_Wtime();); (30, MPI, MPI_Ssend(buf, cnt, MPI_INT, 0, tag, MPI_COMM_WORLD);); (30, MPI, MPI_Recv(buf, cnt, MPI_INT, thread_num, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (23, MPI, MPI_Ssend(buf, cnt, MPI_INT, wsize - 1, tag, MPI_COMM_WORLD);)", "label": "(26, MPI, MPI_Isend(buf, cnt, MPI_INT, thread_num, tag, MPI_COMM_WORLD, &r[thread_num - 1]);); (36, MPI, t = MPI_Wtime() - t;); (21, MPI, t = MPI_Wtime();); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (29, MPI, MPI_Waitall(wsize - 1, r, MPI_STATUSES_IGNORE);); (23, MPI, MPI_Wait(&r[thread_num - 1], MPI_STATUS_IGNORE);)"}
{"predict": "(18, OpenMP, #pragma omp parallel master taskloop lastprivate(s) if (0)); (5, OpenMP, #pragma omp parallel master taskloop firstprivate(s) lastprivate(s)); (0, OpenMP, #pragma omp parallel master taskloop firstprivate(s) lastprivate(); (15, OpenMP, #pragma omp parallel master taskloop if (0) firstprivate(s) lastprivate(s)); (7, OpenMP, #pragma omp parallel master taskloop firstprivate(s) lastprivate()); (8, OpenMP, #pragma omp parallel master taskloop firstprivate(s) lastprivate(argc > 0? argv[1] : argv[2])); (-3, OpenMP, #pragma omp parallel master taskloop firstprivate(s) lastprivate); (3, OpenMP, #pragma omp parallel master taskloop firstprivate(s) lastprivate(argc,); (8, OpenMP, #pragma omp parallel master taskloop firstprivate(s) lastprivate(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: s)); (-3, OpenMP, #pragma omp parallel master taskloop firstprivate(s) lastprivate(argc)", "label": "(8, OpenMP, #pragma omp target parallel for simd order(); (13, OpenMP, #pragma omp target parallel for simd order(concurrent); (15, OpenMP, #pragma omp target parallel for simd order(concurrent)); (2, OpenMP, #pragma omp target parallel for simd order); (-2, OpenMP, #pragma omp target parallel for simd lastprivate(s) firstprivate(s)); (6, OpenMP, #pragma omp target parallel for simd order(none)"}
{"predict": "(7, CUDA, __shared__ block_radix_sort_t::TempStorage temp_storage;)", "label": "(7, CUDA, __shared__ block_radix_sort_t::TempStorage temp_storage;)"}
{"predict": "(69, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (2, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (60, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(inData->basePtr(), inVec.data(), inBufSize, cudaMemcpyHostToDevice));); (73, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(testVec.data(), outData->basePtr(), outBufSize, cudaMemcpyDeviceToHost));); (66, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamDestroy(stream));)", "label": "(68, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (2, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (59, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(inData->basePtr(), inVec.data(), inBufSize, cudaMemcpyHostToDevice));); (72, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(testVec.data(), outData->basePtr(), outBufSize, cudaMemcpyDeviceToHost));); (65, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamDestroy(stream));)"}
{"predict": "(25, CUDA, CUDA_CHECK(cudaMalloc((void **)&dev_max, sizeof(FP) * SEQLEN * DIM));); (44, CUDA, cudaFree(dev_max);); (27, CUDA, CUDA_CHECK(cudaMemcpy(dev_denom, host_denom, sizeof(FP) * SEQLEN * DIM, cudaMemcpyHostToDevice));); (23, CUDA, CUDA_CHECK(cudaMalloc((void **)&dev_denom, sizeof(FP) * SEQLEN * DIM));); (23, CUDA, CUDA_CHECK(cudaMalloc((void **)&QK, sizeof(FP) * SEQLEN * SEQLEN));); (23, CUDA, CUDA_CHECK(cudaMemcpy(dev_max, host_max, sizeof(FP) * SEQLEN * DIM, cudaMemcpyHostToDevice));); (38, CUDA, cudaFree(QK);); (39, CUDA, cudaFree(dev_denom);)", "label": "(26, CUDA, CUDA_CHECK(cudaMalloc((void **)&dev_max, sizeof(FP) * SEQLEN * DIM));); (45, CUDA, cudaFree(dev_max);); (28, CUDA, CUDA_CHECK(cudaMemcpy(dev_denom, host_denom, sizeof(FP) * SEQLEN * DIM, cudaMemcpyHostToDevice));); (24, CUDA, CUDA_CHECK(cudaMalloc((void **)&dev_denom, sizeof(FP) * SEQLEN * DIM));); (24, CUDA, CUDA_CHECK(cudaMalloc((void **)&QK, sizeof(FP) * SEQLEN * SEQLEN));); (24, CUDA, CUDA_CHECK(cudaMemcpy(dev_max, host_max, sizeof(FP) * SEQLEN * DIM, cudaMemcpyHostToDevice));); (39, CUDA, cudaFree(QK);); (40, CUDA, cudaFree(dev_denom);)"}
{"predict": "(19, OpenMP, #pragma omp for schedule(runtime) reduction(+:total) linear(idx:1)); (11, OpenMP, thread_num = omp_get_thread_num();); (15, OpenMP, #pragma omp master); (6, OpenMP, #pragma omp parallel shared(data, std::cerr, total, idx) default(none))", "label": "(19, OpenMP, #pragma omp for schedule(runtime) reduction(+:total) linear(idx:1)); (13, OpenMP, thread_num = omp_get_thread_num();); (15, OpenMP, #pragma omp master); (7, OpenMP, #pragma omp parallel shared(data, std::cerr, total, idx) default(none))"}
{"predict": "(60, MPI, MPI_Type_commit(newtype);)", "label": "(62, MPI, MPI_Type_commit(newtype);)"}
{"predict": "(14, CUDA, cudaMalloc((void **)&gpu_outputs, sizeof(typename types<params>::output_t)*count);); (60, CUDA, cudaFree(gpu_inputs);); (16, CUDA, cudaMalloc((void **)&gpu_runs, sizeof(cuda_run_t)*count);); (58, CUDA, cudaFree(gpu_outputs);); (12, CUDA, cudaMalloc((void **)&gpu_tbb, sizeof(tbb::flow::continue_node<TEST, params>));); (14, CUDA, cudaMalloc((void **)&gpu_compare, sizeof(typename types<params>::output_t)*count);); (12, CUDA, cudaMalloc((void **)&gpu_current, sizeof(tbb::flow::continue_node<TEST, params>));); (13, CUDA, cudaMalloc((void **)&gpu_next, sizeof(tbb::flow::continue_node<TEST, params>));); (15, CUDA, cudaMalloc((void **)&gpu_status, sizeof(tbb::flow::continue_node<TEST, params>));); (9, CUDA, cudaMalloc((void **)&gpu_input, sizeof(typename types<params>::input_t)*count);); (13, CUDA, cudaMalloc((void **)&gpu_addr, sizeof(tbb::flow::continue_node<TEST, params>));); (9, CUDA, cudaMalloc((void **)&gpu_long, sizeof(tbb::flow::continue_node<TEST, params>));); (13, CUDA, cudaMalloc((void **)&gpu_result, sizeof(tbb::flow::continue_node<TEST, params>));); (5, CUDA, cudaMalloc((void **)&gpu_data, sizeof(typename types<params>::data_t)*count);); (9, CUDA, cudaMalloc((void **)&gpu_x, sizeof(tbb::flow::continue_node<TEST, params>));); (11, CUDA, cudaMalloc((void **)&gpu_u, sizeof(tbb::flow::continue_node<TEST, params>));); (13, CUDA, cudaMalloc((void **)&gpu_last, sizeof(tbb::flow::continue_node<TEST, params>));); (8, CUDA, cudaMalloc((void **)&gpu_z, sizeof(tbb::flow::continue_node<TEST, params>));); (13, CUDA, cudaMalloc((void **)&gpu_expect, sizeof(tbb::flow::continue_node<TEST, params>));); (14, CUDA, cudaMalloc((void **)&gpu_start, sizeof(tbb::flow::continue_node<TEST, params>", "label": "(21, CUDA, $GPU(cudaMemcpy(compare, gpu_outputs, sizeof(typename types<params>::output_t)*count, cudaMemcpyDeviceToHost));); (15, CUDA, $GPU(cudaMalloc((void **)&gpu_outputs, sizeof(typename types<params>::output_t)*count));); (15, CUDA, $GPU(cudaMemset(gpu_outputs, 0, sizeof(typename types<params>::output_t)*count));); (58, CUDA, $GPU(cudaFree(gpu_outputs));)"}
{"predict": "(38, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (40, CUDA, CUDA_CHECK(cudaFree(d_data));)", "label": "(38, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (40, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(44, CUDA, __shared__ doublecomplex temp2[MAXSUPER];)", "label": "(45, CUDA, __shared__ doublecomplex temp2[MAXSUPER];)"}
{"predict": "(10, OpenCL, auto mappedPtr = clEnqueueMapBuffer(&cmdQ, buffer.get(), CL_FALSE, CL_MAP_READ, 0, 1, 0, nullptr, nullptr, &retVal);); (12, OpenCL, retVal = clEnqueueUnmapMemObject(&cmdQ, buffer.get(), mappedPtr, 0, nullptr, nullptr);)", "label": "(10, OpenCL, auto mappedPtr = clEnqueueMapBuffer(&cmdQ, buffer.get(), CL_TRUE, CL_MAP_READ, 0, 1, 0, nullptr, nullptr, &retVal);); (12, OpenCL, retVal = clEnqueueUnmapMemObject(&cmdQ, buffer.get(), mappedPtr, 0, nullptr, nullptr);)"}
{"predict": "(9, OpenMP, #pragma omp target teams firstprivate(); (34, OpenMP, #pragma omp target teams firstprivate(S2::S2sc)); (39, OpenMP, #pragma omp target teams firstprivate(e, g)); (46, OpenMP, #pragma omp target teams shared(i)); (6, OpenMP, #pragma omp target teams firstprivate); (29, OpenMP, #pragma omp target teams firstprivate(da)); (34, OpenMP, #pragma omp target teams firstprivate(h, B::x)); (15, OpenMP, #pragma omp target teams firstprivate(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (57, OpenMP, #pragma omp target teams firstprivate(m)); (26, OpenMP, #pragma omp target teams firstprivate(ca,z )); (5, OpenMP, #pragma omp target teams firstprivate()); (54, OpenMP, #pragma omp target teams firstprivate(i)); (28, OpenMP, #pragma omp target teams firstprivate(j)); (14, OpenMP, #pragma omp target teams firstprivate(a, b, c, d, f)); (57, OpenMP, #pragma omp target teams firstprivate(m) private(m)); (16, OpenMP, #pragma omp target teams firstprivate(argv[1])); (27, OpenMP, #pragma omp target teams private(i), firstprivate(i)); (42, OpenMP, #pragma omp target teams firstprivate(i)); (14, OpenMP, #pragma omp target teams firstprivate(ba)); (37, OpenMP, #pragma omp target teams firstprivate(i)); (22, OpenMP, #pragma omp target teams firstprivate(i)); (-3, OpenMP, #pragma omp target teams firstprivate(argc,); (38, OpenMP, #pragma omp target teams firstprivate(j)); (39, OpenMP, #pragma omp target teams firstprivate(m)); (22, OpenMP, #pragma omp target teams shared(i)); (11, OpenMP, #pragma omp target teams firstprivate(S2::S2s)); (30, OpenMP, #pragma omp target teams firstprivate(h)); (17, OpenMP, #pragma omp target teams firstprivate(j)); (36, OpenMP, #pragma omp target teams firstprivate(i)); (39", "label": "(24, OpenMP, #pragma omp target parallel firstprivate (a, b, c, d, f)); (43, OpenMP, #pragma omp target parallel shared(i)); (46, OpenMP, #pragma omp target parallel firstprivate(j)); (19, OpenMP, #pragma omp target parallel firstprivate (S1)); (6, OpenMP, #pragma omp target parallel firstprivate (); (11, OpenMP, #pragma omp target parallel firstprivate (argc,); (28, OpenMP, #pragma omp target parallel firstprivate(S2::S2s)); (35, OpenMP, #pragma omp target parallel private(i), firstprivate(i)); (12, OpenMP, #pragma omp target parallel firstprivate (argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (29, OpenMP, #pragma omp target parallel firstprivate(e, g)); (30, OpenMP, #pragma omp target parallel firstprivate(h, B::x)); (19, OpenMP, #pragma omp target parallel firstprivate(ca, z)); (16, OpenMP, #pragma omp target parallel firstprivate(ba) allocate(omp_thread_mem_alloc: ba)); (-1, OpenMP, #pragma omp target parallel firstprivate ()); (36, OpenMP, #pragma omp target parallel firstprivate(m)); (-1, OpenMP, #pragma omp target parallel firstprivate (argc); (-8, OpenMP, #pragma omp target parallel firstprivate); (29, OpenMP, #pragma omp target parallel firstprivate(i)); (18, OpenMP, #pragma omp target parallel firstprivate(S2::S2sc)); (7, OpenMP, #pragma omp target parallel firstprivate (argv[1])); (-2, OpenMP, #pragma omp target parallel firstprivate (argc > 0? argv[1] : argv[2])); (11, OpenMP, #pragma omp target parallel firstprivate(da))"}
{"predict": "(2, TBB, tbb::task_scheduler_init init(nThreads);); (6, TBB, tbb::tick_count start = tbb::tick_count::now();); (8, TBB, tbb::tick_count now = tbb::tick_count::now();)", "label": "(8, TBB, start = tbb::tick_count::now();); (0, TBB, tbb::tick_count now;); (8, TBB, now = tbb::tick_count::now();)"}
{"predict": "(3, OpenMP, #pragma omp parallel); (5, OpenMP, int me = omp_get_thread_num();); (7, OpenMP, #pragma omp barrier); (-1, OpenMP, omp_set_num_threads(2);)", "label": "(17, OpenMP, #pragma omp barrier); (2, OpenMP, #pragma omp parallel); (3, OpenMP, int me = omp_get_thread_num();)"}
{"predict": "(27, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_IMAGE_MAX_ARRAY_SIZE, sizeof( maxArraySize ), &maxArraySize, NULL );); (28, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_GLOBAL_MEM_SIZE, sizeof( memSize ), &memSize, NULL );); (26, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_MAX_MEM_ALLOC_SIZE, sizeof( maxAllocSize ), &maxAllocSize, NULL );); (24, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_IMAGE2D_MAX_WIDTH, sizeof( maxWidth ), &maxWidth, NULL );)", "label": "(27, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_IMAGE_MAX_ARRAY_SIZE, sizeof( maxArraySize ), &maxArraySize, NULL );); (28, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_GLOBAL_MEM_SIZE, sizeof( memSize ), &memSize, NULL );); (26, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_MAX_MEM_ALLOC_SIZE, sizeof( maxAllocSize ), &maxAllocSize, NULL );); (23, OpenCL, error = clGetDeviceInfo( device, CL_DEVICE_IMAGE2D_MAX_WIDTH, sizeof( maxWidth ), &maxWidth, NULL );)"}
{"predict": "(11, CUDA, __shared__ uninitialized_array<T, SUM_SIZE> sum;)", "label": "(11, CUDA, __shared__ uninitialized_array<T, SUM_SIZE> sum;)"}
{"predict": "(3, TBB, typedef tbb::concurrent_lru_cache<self_type> cache_type;); (12, TBB, tbb::task_scheduler_init init(number_of_threads);)", "label": "(10, TBB, static const size_t number_of_threads = 8 * tbb::task_scheduler_init::default_num_threads();)"}
{"predict": "(7, CUDA, __shared__ typename BlockReduce::TempStorage temp_storage;)", "label": "(7, CUDA, __shared__ typename BlockReduce::TempStorage temp_storage;)"}
{"predict": "(2, TBB, using namespace tbb::internal;)", "label": "(2, TBB, using namespace tbb::internal;)"}
{"predict": "(3, OpenMP, #pragma omp taskyield allocate(argc)); (3, OpenMP, #pragma omp taskyield untied); (3, OpenMP, #pragma omp taskyield)", "label": "(2, OpenMP, #pragma omp taskyield allocate(argc)); (3, OpenMP, #pragma omp taskyield untied); (3, OpenMP, #pragma omp taskyield)"}
{"predict": "(3, CUDA, thrust::device_vector<double> ret;); (15, CUDA, cudaDeviceSynchronize();); (7, CUDA, thrust::device_ptr<double> d_ptr = thrust::device_malloc(n * sizeof(double));); (11, CUDA, thrust::host_vector<double> ret_host(ret);); (6, CUDA, thrust::fill(ret.begin(), ret.end(), 0.);); (13, CUDA, thrust::device_free(d_ptr);)", "label": "(14, CUDA, cudaError_t cuerr = cudaMemcpy(ret_host, ret_ptr, (1)*sizeof(double), cudaMemcpyDeviceToHost);); (3, CUDA, double* d_ptr = thrust::raw_pointer_cast(dv_ptr);); (7, CUDA, thrust::device_ptr<double> ret_dev_ptr = thrust::min_element(thrust::device, dv_ptr, dv_ptr+n);); (10, CUDA, double *ret_ptr = thrust::raw_pointer_cast(ret_dev_ptr);); (-1, CUDA, thrust::device_ptr<double> dv_ptr = thrust::device_malloc(n*sizeof(double));); (14, CUDA, thrust::device_free(dv_ptr);)"}
{"predict": "(14, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(14, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(27, MPI, MPI_Type_commit(&xpose);); (39, MPI, MPI_Pack(a, 1, xpose, buffer, bufsize, &position, MPI_COMM_WORLD);); (53, MPI, MPI_Type_free(&row);); (26, MPI, MPI_Pack_size(1, xpose, MPI_COMM_WORLD, &bufsize);); (50, MPI, MPI_Type_free(&xpose);); (17, MPI, MPI_Type_extent(MPI_INT, &tmp_lb, &sizeofint);); (23, MPI, MPI_Type_commit(&row);); (28, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (36, MPI, MPI_Unpack(buffer, bufsize, &position, b, 100 * 100, MPI_INT, MPI_COMM_WORLD);); (18, MPI, MPI_Type_vector(100, 1, 100, MPI_INT, &row);); (15, MPI, MPI_Type_create_hvector(100, 1, 100 * sizeofint, row, &xpose);)", "label": "(25, MPI, MPI_Type_get_extent(MPI_INT, &tmp_lb, &sizeofint);); (39, MPI, MPI_Pack(a, 1, xpose, buffer, bufsize, &position, MPI_COMM_WORLD);); (55, MPI, MPI_Type_free(&row);); (27, MPI, MPI_Type_commit(&xpose);); (52, MPI, MPI_Type_free(&xpose);); (28, MPI, MPI_Pack_size(1, xpose, MPI_COMM_WORLD, &bufsize);); (23, MPI, MPI_Type_create_hvector(100, 1, sizeofint, row, &xpose);); (31, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (36, MPI, MPI_Unpack(buffer, bufsize, &position, b, 100 * 100, MPI_INT, MPI_COMM_WORLD);); (19, MPI, MPI_Type_vector(100, 1, 100, MPI_INT, &row);)"}
{"predict": "(34, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (36, CUDA, CUDA_CHECK(cudaFree(d_data));)", "label": "(34, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (36, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(191, OpenMP, #pragma omp barrier); (97, OpenMP, #pragma omp master); (281, OpenMP, double dTotalThreads = omp_get_max_threads();); (268, OpenMP, double nMajorThreads  = omp_get_max_threads() / ncrews;); (96, OpenMP, #pragma\tomp\ttaskloop private (t0, tInit, ratio, vgh_t, val_t, nspheremoves, dNumVGHCalls, bigClock, initClock, my_random, ecp, deltas, rOnSphere, ur, vgh_t_loc, v_t_loc, my_vals2) untied); (281, OpenMP, #pragma omp critical); (275, OpenMP, #pragma omp master); (96, OpenMP, #pragma omp parallel shared (t0, tInit, ratio, vgh_t, val_t, nspheremoves, dNumVGHCalls, bigClock, initClock, my_random, ecp, deltas, rOnSphere, ur, vgh_t_loc, v_t_loc, my_vals2))", "label": "(115, OpenMP, ncrews = omp_get_max_threads();); (98, OpenMP, const int np = omp_get_num_threads();); (189, OpenMP, #pragma omp barrier); (233, OpenMP, double dTotalThreads = omp_get_max_threads();); (268, OpenMP, double nMajorThreads  = omp_get_max_threads();); (137, OpenMP, #pragma omp parallel reduction(+ : vgh_t_loc, v_t_loc, my_vals)); (106, OpenMP, #pragma omp master); (278, OpenMP, cout << \"\\nMPI: \" << nmpi << \" and Threads: \" << omp_get_max_threads();); (92, OpenMP, const int ip = omp_get_thread_num();); (86, OpenMP, #pragma omp parallel reduction(+ : t0, tInit, ratio, vgh_t, val_t, nspheremoves, dNumVGHCalls))"}
{"predict": "(58, CUDA, cudaEventDestroy(start);); (48, CUDA, cudaEventDestroy(stop);); (144, CUDA, cudaFree(b_device);); (18, CUDA, cudaMalloc((void **)&b_device, N * sizeof(half));); (20, CUDA, cudaMalloc((void **)&c_device, N * sizeof(half));); (30, CUDA, cudaDeviceSynchronize();); (30, CUDA, cudaEventRecord(start, 0);); (8, CUDA, cudaMalloc((void **)&a_device, N * sizeof(half));); (146, CUDA, cudaFree(c_device);); (41, CUDA, cudaEventRecord(stop, 0);); (37, CUDA, cudaEventSynchronize(stop);); (37, CUDA, cudaEventElapsedTime(&time, start, stop);); (21, CUDA, cudaEventCreate(&stop);); (148, CUDA, cudaFree(a_device);); (7, CUDA, cudaEventCreate(&start);); (23, CUDA, cudaEventDestroy(start);); (139, CUDA, cudaFree(b_host);); (130, CUDA, cudaMemcpy(c_host, c_device, N * sizeof(half), cudaMemcpyDeviceToHost);); (6, CUDA, cudaEventRecord(stop, 0);); (22, CUDA, cudaEventRecord(start, 0);); (11, CUDA, cudaMemcpy(a_device, a_host, N * sizeof(half), cudaMemcpyHostToDevice);); (37, CUDA, cudaEventDestroy(stop);); (20, CUDA, cudaMemcpy(b_device, b_host, N * sizeof(half), cudaMemcpyHostToDevice);); (6, CUDA, cudaMemcpy(b_device, b_host, N * sizeof(half), cudaMemcpyHostToDevice);); (11, CUDA, cudaMemcpy(c_device, c_host, N * sizeof(half), cudaMemcpyHostToDevice);)", "label": "(25, CUDA, cudaMalloc((void **)&c_device, N * sizeof(half));); (49, CUDA, cudaEventSynchronize(stop);); (50, CUDA, cudaEventDestroy(start);); (155, CUDA, cudaFree(a_device);); (32, CUDA, cudaDeviceSynchronize();); (155, CUDA, cudaFree(c_device);); (43, CUDA, cudaEventRecord(stop, 0);); (46, CUDA, cudaEventDestroy(stop);); (151, CUDA, cudaFree(b_device);); (33, CUDA, cudaEventRecord(start, 0);); (11, CUDA, cudaMemcpy(b_device, b_host, N * sizeof(half), cudaMemcpyHostToDevice);); (30, CUDA, cudaEventCreate(&stop);); (6, CUDA, cudaMalloc((void **)&b_device, N * sizeof(half));); (-2, CUDA, cudaMalloc((void **)&a_device, N * sizeof(half));); (37, CUDA, cudaEventElapsedTime(&time, start, stop);); (-1, CUDA, cudaMemcpy(a_device, a_host, N * sizeof(half), cudaMemcpyHostToDevice);); (24, CUDA, cudaEventCreate(&start);); (39, CUDA, cudaMemcpy(c_host, c_device, N * sizeof(half), cudaMemcpyDeviceToHost);)"}
{"predict": "(8, CUDA, CHECK(cudaMallocManaged((void **)&z, M));); (28, CUDA, CHECK(cudaFree(x));); (0, CUDA, CHECK(cudaSetDevice(device_id));); (5, CUDA, CHECK(cudaMallocManaged((void **)&x, M));); (27, CUDA, CHECK(cudaFree(y));); (27, CUDA, CHECK(cudaFree(z));); (16, CUDA, CHECK(cudaDeviceSynchronize());); (0, CUDA, CHECK(cudaMallocManaged((void **)&y, M));); (15, CUDA, CHECK(cudaPeekAtLastError());)", "label": "(27, CUDA, CHECK(cudaMemPrefetchAsync(z, M, cudaCpuDeviceId, NULL));); (20, CUDA, CHECK(cudaMemPrefetchAsync(x, M, device_id, NULL));); (20, CUDA, CHECK(cudaMemPrefetchAsync(y, M, device_id, NULL));); (7, CUDA, CHECK(cudaMallocManaged((void **)&z, M));); (28, CUDA, CHECK(cudaFree(x));); (-2, CUDA, CHECK(cudaGetDevice(&device_id));); (2, CUDA, CHECK(cudaMallocManaged((void **)&x, M));); (26, CUDA, CHECK(cudaFree(y));); (26, CUDA, CHECK(cudaFree(z));); (14, CUDA, CHECK(cudaMemPrefetchAsync(z, M, device_id, NULL));); (19, CUDA, CHECK(cudaDeviceSynchronize());); (-2, CUDA, CHECK(cudaMallocManaged((void **)&y, M));)"}
{"predict": "(14, CUDA, HANDLE_ERROR( cudaGLSetGLDevice( dev ) );); (2, CUDA, HANDLE_ERROR( cudaGetDevice( &dev ) );); (51, CUDA, HANDLE_ERROR( cudaGraphicsUnmapResources( 1, &resource, 0 ) );); (1, CUDA, HANDLE_ERROR( cudaGetDeviceProperties( &prop, dev ) );)", "label": "(14, CUDA, HANDLE_ERROR( cudaGLSetGLDevice( dev ) );); (51, CUDA, HANDLE_ERROR( cudaGraphicsUnmapResources( 1, &resource, NULL ) );); (39, CUDA, HANDLE_ERROR( cudaGraphicsMapResources( 1, &resource, NULL ) );); (4, CUDA, HANDLE_ERROR( cudaChooseDevice( &dev, &prop ) );)"}
{"predict": "(20, MPI, MPI_Init(&argc, &argv);); (25, MPI, MPI_Finalize();); (19, MPI, MPI_Comm_size(MPI_COMM_WORLD, &num_procs);); (17, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myid);)", "label": "(19, MPI, MPI_Init(&argc, &argv);); (25, MPI, MPI_Finalize();); (19, MPI, MPI_Comm_size(MPI_COMM_WORLD, &num_procs);); (17, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myid);)"}
{"predict": "(24, OpenMP, #pragma omp for private()); (41, OpenMP, #pragma omp for private(x, y, z)); (0, OpenMP, #pragma omp parallel); (25, OpenMP, #pragma omp for private(int)); (39, OpenMP, #pragma omp for private(x, y)); (8, OpenMP, #pragma omp for private(,); (28, OpenMP, #pragma omp for private(0)); (3, OpenMP, #pragma omp for private(,); (12, OpenMP, #pragma omp for private()); (-3, OpenMP, #pragma omp for private()", "label": "(26, OpenMP, #pragma omp for private(int)); (10, OpenMP, #pragma omp for private(,); (35, OpenMP, #pragma omp for private(x)); (2, OpenMP, #pragma omp for private(); (-2, OpenMP, #pragma omp parallel); (26, OpenMP, #pragma omp for private(0)); (10, OpenMP, #pragma omp for private(, )); (34, OpenMP, #pragma omp for private(x, y)); (13, OpenMP, #pragma omp for private()); (36, OpenMP, #pragma omp for private(x, y, z))"}
{"predict": "(9, TBB, test_extract<tbb::flow::rejecting>();); (10, TBB, test_extract<tbb::flow::queueing>();); (3, TBB, tbb::task_scheduler_init init(p);); (7, TBB, test_extract<tbb::flow::tag_matching>();); (7, TBB, test_extract<tbb::flow::reserving>();)", "label": "(7, TBB, test_reserving_nodes<tbb::flow::write_once_node, size_t>();); (3, TBB, tbb::task_scheduler_init init(p);); (8, TBB, test_extract_on_node<tbb::flow::write_once_node, int>();)"}
{"predict": "(25, MPI, MPI_Send(buffer, buf_size, MPI_BYTE, 0, tag1, MPI_COMM_WORLD);); (13, MPI, MPI_Isend(buffer, buf_size, MPI_BYTE, 1, tag1, MPI_COMM_WORLD, &request1);); (15, MPI, MPI_Wait(&request2, &status);); (17, MPI, MPI_Irecv(buffer, buf_size, MPI_BYTE, 0, tag2, MPI_COMM_WORLD, &request2);); (18, MPI, MPI_Irecv(buffer, buf_size, MPI_BYTE, 0, tag1, MPI_COMM_WORLD, &request1);); (12, MPI, MPI_Recv(buffer, buf_size, MPI_BYTE, 1, tag1, MPI_COMM_WORLD, &status);); (11, MPI, MPI_Recv(buffer, buf_size, MPI_BYTE, 1, tag2, MPI_COMM_WORLD, &status);); (13, MPI, MPI_Wait(&request1, &status);); (8, MPI, MPI_Isend(buffer, buf_size, MPI_BYTE, 1, tag2, MPI_COMM_WORLD, &request2);)", "label": "(13, MPI, MPI_Isend(buffer, buf_size, MPI_BYTE, 1, tag1, MPI_COMM_WORLD, &request1);); (15, MPI, MPI_Wait(&request2, &status);); (12, MPI, MPI_Isend(buffer, buf_size, MPI_BYTE, 1, tag2, MPI_COMM_WORLD, &request2);); (16, MPI, MPI_Irecv(buffer, buf_size, MPI_BYTE, 0, tag1, MPI_COMM_WORLD, &request2);); (11, MPI, MPI_Wait(&request1, &status);); (13, MPI, MPI_Irecv(buffer, buf_size, MPI_BYTE, 0, tag2, MPI_COMM_WORLD, &request1);)"}
{"predict": "(15, CUDA, cudaDeviceSynchronize();)", "label": "(15, CUDA, cudaDeviceSynchronize();)"}
{"predict": "(12, OpenMP, #pragma omp target teams distribute parallel for collapse(2) private(i,j,k) map(alloc: c[0:row_b*col_b])); (3, OpenMP, #pragma omp target teams distribute parallel for collapse(2))", "label": "(12, OpenMP, #pragma omp parallel for private(i,j,k) num_threads(dtn(border, 1)))"}
{"predict": "(36, OpenMP, #pragma omp target parallel for map(a, b))", "label": "(36, OpenMP, #pragma omp target parallel for map(a, b))"}
{"predict": "(4, CUDA, extern __shared__ int shared[];)", "label": "(4, CUDA, extern __shared__ int shared[];)"}
{"predict": "(33, OpenMP, omp_target_free(d_a, DefaultDeviceNum);); (15, OpenMP, d_a = (int *)omp_target_alloc(DataSize, DefaultDeviceNum);); (22, OpenMP, #pragma omp target update to(p_a[0:N])); (5, OpenMP, int DefaultDeviceNum = omp_get_default_device();); (15, OpenMP, omp_target_associate_ptr(p_a, d_a, DataSize, 0, DefaultDeviceNum);); (23, OpenMP, omp_target_disassociate_ptr(p_a, DefaultDeviceNum);)", "label": "(32, OpenMP, omp_target_free(d_a, DefaultDeviceNum);); (23, OpenMP, #pragma omp target update to(p_a)); (12, OpenMP, d_a = (int *)omp_target_alloc(DataSize, DefaultDeviceNum);); (6, OpenMP, int DefaultDeviceNum = omp_get_default_device();); (14, OpenMP, omp_target_associate_ptr(p_a, d_a, DataSize, 0, DefaultDeviceNum);); (23, OpenMP, omp_target_disassociate_ptr(p_a, DefaultDeviceNum);)"}
{"predict": "(6, TBB, tbb::filter_t<void, void> one_filter( filter_table[i] );); (7, TBB, tbb::parallel_pipeline( n_tokens, one_filter, context...);)", "label": "(9, TBB, tbb::parallel_pipeline( n_tokens, one_filter, context... );); (5, TBB, tbb::filter<void, void> one_filter( filter_table[i], i_filter );)"}
{"predict": "(3, MPI, MPI_Comm_rank(comm, &comm_rank);); (12, MPI, MPI_Dims_create(2, RANK, dims);)", "label": "(13, MPI, MPI_Bcast(dims, 2, MPI_INT, 0, comm);); (2, MPI, MPI_Comm_rank(comm, &comm_rank);)"}
{"predict": "(7, CUDA, CHECK_CUDA_ERROR(cudaDriverGetVersion(&driver_version));); (10, CUDA, CHECK_CUDA_ERROR(cudaRuntimeGetVersion(&runtime_version));); (68, CUDA, CHECK_CUDA_ERROR(cudaDeviceSetCacheConfig(cudaFuncCachePreferL1));); (74, CUDA, CHECK_CUDA_ERROR(cudaDeviceSetCacheConfig(cudaFuncCachePreferShared));)", "label": "(10, CUDA, CHECK_CUDA_ERROR(cudaRuntimeGetVersion(&runtime_version));); (72, CUDA, CHECK_CUDA_ERROR(cudaSetDevice(dev));); (26, CUDA, CHECK_CUDA_ERROR(cudaGetDeviceProperties(&deviceProp, dev));); (3, CUDA, CHECK_CUDA_ERROR(cudaDriverGetVersion(&driver_version));); (20, CUDA, CHECK_CUDA_ERROR(cudaGetDeviceProperties(&deviceProp, i));)"}
{"predict": "(147, CUDA, ASSERT_EQ(cudaSuccess, cudaStream.Synchronize());); (141, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&cuda_stream));); (148, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamDestroy(cuda_stream));); (126, CUDA, ASSERT_EQ(cudaSuccess, cudaDeviceSynchronize());); (133, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(out_testVec.data(), outObj.GetCudaOutput(), outBufSize, cudaMemcpyDeviceToHost));); (143, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamSynchronize(cuda_stream));); (134, CUDA, ASSERT_EQ(cudaSuccess, cudaMemset(out_buf, 0, outBufSize));); (127, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(in_testVec.data(), inObj.GetCudaInput(), inBufSize, cudaMemcpyDeviceToHost));); (141, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamDestroy(cuda_stream));); (135, CUDA, ASSERT_EQ(cudaSuccess, cudaMemset(in_buf, 0, inBufSize));); (120, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&cuda_stream));); (134, CUDA, ASSERT_EQ(cudaSuccess, cudaMemset(out_buf, 0, outBufSize));)", "label": "(90, CUDA, ASSERT_EQ(cudaSuccess, cudaMallocHost(&host_ws, host_ws_bytes));); (297, CUDA, ASSERT_EQ(cudaSuccess, cudaEventDestroy(ref_start));); (230, CUDA, ASSERT_EQ(cudaSuccess, cudaEventElapsedTime(&ref_time, ref_start, ref_end));); (297, CUDA, ASSERT_EQ(cudaSuccess, cudaEventDestroy(out_start));); (297, CUDA, ASSERT_EQ(cudaSuccess, cudaEventDestroy(out_end));); (249, CUDA, ASSERT_EQ(cudaSuccess, cudaEventElapsedTime(&out_time, out_start, out_end));); (201, CUDA, ASSERT_EQ(cudaSuccess, cudaEventCreate(&ref_end));); (292, CUDA, ASSERT_EQ(cudaSuccess, cudaEventDestroy(ref_end));); (244, CUDA, ASSERT_EQ(cudaSuccess, cudaEventRecord(out_end, cuda_stream));); (199, CUDA, ASSERT_EQ(cudaSuccess, cudaEventCreate(&out_start));); (199, CUDA, ASSERT_EQ(cudaSuccess, cudaEventCreate(&out_end));); (220, CUDA, ASSERT_EQ(cudaSuccess, cudaEventSynchronize(ref_end));); (241, CUDA, ASSERT_EQ(cudaSuccess, cudaEventSynchronize(out_end));); (217, CUDA, ASSERT_EQ(cudaSuccess, cudaEventRecord(ref_end, cuda_stream));); (192, CUDA, ASSERT_EQ(cudaSuccess, cudaEventCreate(&ref_start));)"}
{"predict": "(18, CUDA, __shared__ uint32_t shuffleMem[1024];)", "label": "(18, CUDA, __shared__ uint32_t sharedMemory[1024];); (29, CUDA, extern __shared__ uint32_t shuffleMem[];)"}
{"predict": "(6, MPI, MPI_Attr_get(comm, lkey[i], &val_p, &flag);)", "label": "(6, MPI, MPI_Comm_get_attr(comm, lkey[i], &val_p, &flag);)"}
{"predict": "(91, CUDA, CUDA(cudaMemsetAsync(buffer->Map(), 0, buffer->Size(), NULL));)", "label": "(89, CUDA, CUDA(cudaGeneratePointGrid(points, GRID_N, GRID_WORLD_SIZE, apptime()));); (89, CUDA, CUDA(cudaDeviceSynchronize());)"}
{"predict": "(22, OpenMP, #pragma omp target teams distribute dist_schedule (static,); (1, OpenMP, #pragma omp target teams distribute dist_schedule); (4, OpenMP, #pragma omp target teams distribute dist_schedule (); (7, OpenMP, #pragma omp target teams distribute dist_schedule (static); (24, OpenMP, #pragma omp target teams distribute dist_schedule (static, z)); (27, OpenMP, #pragma omp target teams distribute dist_schedule (static, argv[1]=2)); (30, OpenMP, #pragma omp target teams distribute dist_schedule (static, S1)); (11, OpenMP, #pragma omp target teams distribute dist_schedule (argc))); (4, OpenMP, #pragma omp target teams distribute dist_schedule ()); (15, OpenMP, #pragma omp target teams distribute dist_schedule (static, argc > 0? argv[1] : argv[2])); (20, OpenMP, #pragma omp target teams distribute dist_schedule (static,,); (5, OpenMP, #pragma omp target teams distribute dist_schedule (static)", "label": "(29, OpenMP, #pragma omp target teams distribute num_teams (-2)); (4, OpenMP, #pragma omp target teams distribute num_teams (); (30, OpenMP, #pragma omp target teams distribute num_teams (-10u)); (5, OpenMP, #pragma omp target teams distribute num_teams ()); (7, OpenMP, #pragma omp target teams distribute num_teams (argc); (15, OpenMP, #pragma omp target teams distribute num_teams (argc + argc+z)); (17, OpenMP, #pragma omp target teams distribute num_teams (argc), num_teams (argc+1)); (28, OpenMP, #pragma omp target teams distribute num_teams (3.14)); (-6, OpenMP, #pragma omp target teams distribute num_teams); (5, OpenMP, #pragma omp target teams distribute num_teams (argc))); (7, OpenMP, #pragma omp target teams distribute num_teams (argc > 0? argv[1] : argv[2])); (15, OpenMP, #pragma omp target teams distribute num_teams (S1))"}
{"predict": "(8, CUDA, thrust::device_vector<double> dv(0);); (12, CUDA, double term = thrust::reduce(dv.begin(), dv.end()) / n;); (13, CUDA, thrust::device_free(dv);); (6, CUDA, thrust::fill(dv.begin(), dv.end(), 0.0);)", "label": "(14, CUDA, double term = thrust::reduce(thrust::device, v_temp.begin(), v_temp.end(), 0.0, thrust::plus<double>());); (7, CUDA, thrust::device_vector<double> v_temp(n);); (7, CUDA, double* dv_ptr = thrust::raw_pointer_cast(v_temp.data());)"}
{"predict": "(2, CUDA, extern __shared__ int sums[];)", "label": "(2, CUDA, extern __shared__ int sums[];)"}
{"predict": "(15, MPI, MPI_Grequest_complete(aio_reqlist[i]->req);)", "label": "(24, MPI, double start_time = MPI_Wtime();); (16, MPI, errcode = MPI_Grequest_complete(aio_reqlist[i]->req);)"}
{"predict": "(2, OpenMP, #pragma omp parallel); (11, OpenMP, #pragma omp target exit data map(from: a, b))", "label": "(13, OpenMP, printf(\"main parallel 2 from %d of %d\\n\", omp_get_thread_num(), omp_get_num_threads() );); (1, OpenMP, #pragma omp parallel); (2, OpenMP, printf(\"main parallel 1 from %d of %d\\n\", omp_get_thread_num(), omp_get_num_threads() );)"}
{"predict": "(8, OpenMP, #pragma omp parallel for)", "label": "(8, OpenMP, #pragma omp parallel for default(none) private(index) shared(count, failed, params, max_threads, local, saved_key, cur_salt, crypt_out)); (15, OpenMP, int t = omp_get_thread_num();)"}
{"predict": "(146, OpenCL, error = clSetKernelArg(kernel, 1, sizeof(tinfo->inBuf), &tinfo->inBuf);)", "label": "(146, OpenCL, error = clSetKernelArg(kernel, 1, sizeof(tinfo->inBuf), &tinfo->inBuf);)"}
{"predict": "(17, MPI, MPI_Info_get_string(info1, key1, &value, &flag);); (9, MPI, MPI_Info_set(info1, key1, \"value1\");); (4, MPI, MPI_Info_create(&info1);); (6, MPI, MPI_Info_set(info2, key2, \"value2\");); (23, MPI, MPI_Info_free(&info2);); (20, MPI, MPI_Info_free(&info1);); (6, MPI, MPI_Info_set(info1, key2, \"value2\");); (7, MPI, MPI_Info_free(&info1);); (3, MPI, MPI_Info_create(&info2);); (3, MPI, MPI_Info_set(info1, key2, \"value2\");)", "label": "(20, MPI, MPI_Info_get(i1, (char *) \"key1\", 64, value, &flag);); (28, MPI, MPI_Info_free(&i1);); (11, MPI, MPI_Info_set(i2, (char *) \"key2\", (char *) \"value2\");); (9, MPI, MPI_Info_set(i1, (char *) \"key1\", (char *) \"value1\");); (11, MPI, MPI_Info_get(i1, (char *) \"key2\", 64, value, &flag);); (25, MPI, MPI_Info_free(&i2);); (3, MPI, MPI_Info_create(&i1);); (3, MPI, MPI_Info_create(&i2);)"}
{"predict": "(2, CUDA, using namespace thrust::device_vector;); (96, CUDA, thrust::device_vector<thrust::complex<double>> D(2);); (73, CUDA, thrust::host_vector<thrust::complex<double>> H(4);); (103, CUDA, cudaDeviceSynchronize();); (39, CUDA, thrust::iota(begin(Adev[2]), end(Adev[2]), 5);); (37, CUDA, thrust::iota(begin(Adev[1]), end(Adev[1]), 5);); (35, CUDA, thrust::iota(begin(Adev[0]), end(Adev[0]), 5);)", "label": "(15, CUDA, multi::array<double, 1, thrust::device_allocator<double>> BB(10, 99.);); (34, CUDA, thrust::copy(begin(rotated(A2)[0]), end(rotated(A2)[0]), begin(rotated(B2)[0]));); (16, CUDA, multi::array<std::complex<double>, 1, thrust::device_allocator<std::complex<double> >> BBB_cpy = BBB;); (14, CUDA, multi::array<std::complex<double>, 1, thrust::device_allocator<std::complex<double> >> BBB(10, 99.);); (97, CUDA, thrust::device_vector<int> D = H;); (78, CUDA, multi::array<int, 1, thrust::device_allocator<int>> H2(4, 99); assert(size(H2) == 4);); (69, CUDA, thrust::host_vector<int> H(4);); (21, CUDA, thrust::copy(begin(B), end(B), begin(C));)"}
{"predict": "(8, OpenMP, const std::string comp_name(x.first);)", "label": "(8, OpenMP, const std::string comp_name(x.first);)"}
{"predict": "(27, OpenCL, status = clBuildProgram(program, 1, &device, buildOptions, NULL, NULL);); (9, OpenCL, cl_program program = clCreateProgramWithSource(context, 1, &source, NULL, &status);)", "label": "(27, OpenCL, status = clBuildProgram(program, 1, &device, buildOptions, NULL, NULL);); (9, OpenCL, cl_program program = clCreateProgramWithSource(context,1, &source, NULL, &status);)"}
{"predict": "(15, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)", "label": "(15, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(2, OpenMP, #pragma omp target teams distribute parallel for simd)", "label": "(2, OpenMP, #pragma omp target teams distribute parallel for simd)"}
{"predict": "(31, MPI, MPI_Win_unlock(rank, win);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (44, MPI, MPI_Win_free(&win);); (20, MPI, MPI_Win_flush_local(win);); (20, MPI, MPI_Get(locbuf, BUFCNT, MPI_INT, target_rank, 0, BUFCNT, MPI_INT, win);); (10, MPI, MPI_Alloc_mem(BUFCNT * sizeof(int), MPI_INFO_NULL, &winbuf);)", "label": "(25, MPI, MPI_Win_unlock(rank, win);); (48, MPI, MPI_Win_unlock_all(win);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (31, MPI, MPI_Win_flush_local(target_rank, win);); (45, MPI, MPI_Win_free(&win);); (23, MPI, MPI_Win_lock_all(0, win);); (11, MPI, MPI_Win_create(winbuf, BUFCNT * sizeof(int), sizeof(int), MPI_INFO_NULL, MPI_COMM_WORLD, &win);); (26, MPI, MPI_Get(locbuf, BUFCNT, MPI_INT, target_rank, 0, BUFCNT, MPI_INT, win);); (18, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(9, MPI, MPI_Wait(&tmprequest, MPI_STATUS_IGNORE);)", "label": "(27, MPI, return MPI_Waitall(num_ep, block_coll_request, MPI_STATUS_IGNORE);); (8, MPI, return MPI_Wait(&tmprequest, MPI_STATUS_IGNORE);)"}
{"predict": "(26, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_IMAGE2D_MAX_WIDTH, sizeof( maxWidth ), &maxWidth, NULL );); (26, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_IMAGE_MAX_ARRAY_SIZE, sizeof( maxArraySize ), &maxArraySize, NULL );); (30, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_GLOBAL_MEM_SIZE, sizeof( memSize ), &memSize, NULL );); (28, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_MAX_MEM_ALLOC_SIZE, sizeof( maxAllocSize ), &maxAllocSize, NULL );); (23, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_IMAGE2D_MAX_HEIGHT, sizeof( maxHeight ), &maxHeight, NULL );)", "label": "(28, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_IMAGE_MAX_ARRAY_SIZE, sizeof( maxArraySize ), &maxArraySize, NULL );); (25, OpenCL, int error = clGetDeviceInfo( device, CL_DEVICE_IMAGE2D_MAX_WIDTH, sizeof( maxWidth ), &maxWidth, NULL );); (28, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_GLOBAL_MEM_SIZE, sizeof( memSize ), &memSize, NULL );); (26, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_MAX_MEM_ALLOC_SIZE, sizeof( maxAllocSize ), &maxAllocSize, NULL );); (23, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_IMAGE2D_MAX_HEIGHT, sizeof( maxHeight ), &maxHeight, NULL );)"}
{"predict": "(12, CUDA, extern __shared__ int shist[];)", "label": "(3, CUDA, extern __shared__ int shist[];)"}
{"predict": "(35, OpenMP, #pragma omp barrier); (35, OpenMP, #pragma omp critical); (49, OpenMP, #pragma omp atomic read seq_cst); (52, OpenMP, #pragma omp atomic write seq_cst); (25, OpenMP, int me    = omp_get_thread_num();); (1, OpenMP, const int nt = omp_get_max_threads();); (61, OpenMP, #pragma omp atomic seq_cst, critical); (24, OpenMP, #pragma omp parallel shared(flags, iterations, me, end, left, val, store_model, load_model, critical, barrier))", "label": "(33, OpenMP, #pragma omp barrier); (28, OpenMP, int me    = omp_get_thread_num();); (25, OpenMP, #pragma omp parallel reduction(min:dtmin) reduction(max:dtmax) reduction(+:dtavg)); (-1, OpenMP, int nt = omp_get_max_threads();); (35, OpenMP, #pragma omp critical)"}
{"predict": "(20, CUDA, checkCudaErrors(cudaSetDevice(0));)", "label": "(20, CUDA, checkCudaErrors(cudaSetDevice(0));)"}
{"predict": "(3, TBB, using ets_tlskey_type = std::vector<T, oneapi::tbb::tbb_allocator<T> >;); (5, TBB, oneapi::tbb::global_control gc(oneapi::tbb::global_control::max_allowed_parallelism, p);); (27, TBB, oneapi::tbb::parallel_for ( std::range<int> (0, vs3.size(), 10000), parallel_vector_for_body<T, Allocator>( vs3, allocator_name ) );); (44, TBB, oneapi::tbb::flattened2d<ets_nokey_type> fvs2 = flatten2d(vs2);); (49, TBB, oneapi::tbb::flattened2d<ets_tlskey_type> cfvs2 = flatten2d(vs3);); (40, TBB, oneapi::tbb::flattened2d<ets_nokey_type> fvs = flatten2d(vs);); (9, TBB, if (Verbose && t == 0) t0 = oneapi::tbb::tick_count::now();); (1, TBB, tbb::task_scheduler_init init(p);); (9, TBB, oneapi::tbb::enumerable_thread_specific<ets_tlskey_type> vs2;); (12, TBB, oneapi::tbb::enumerable_thread_specific<ets_nokey_type> vs3;); (-4, TBB, using ets_nokey_type = std::vector<T, oneapi::tbb::tbb_allocator<T> >;); (10, TBB, oneapi::tbb::parallel_for ( std::range<int> (0, vs.size(), 10000), parallel_vector_for_body<T, Allocator>( vs, allocator_name ) );); (-6, TBB, oneapi::tbb::tick_count t0;); (43, TBB, oneapi::tbb::flattened2d<ets_tlskey_type> ffs = flatten2d(vs2);); (11, TBB, oneapi::tbb::enumerable_thread_specific<ets_tlskey_type> vs3;); (29, TBB, oneapi::tbb::flattened2d<ets_nokey_type> fvs3 = flatten2d(vs3);)", "label": "(3, TBB, typedef std::vector<T, oneapi::tbb::tbb_allocator<T> > container_type;); (33, TBB, oneapi::tbb::flattened2d<ets_nokey_type> fvs = flatten2d(vs3);); (11, TBB, typedef typename oneapi::tbb::enumerable_thread_specific< container_type, Allocator<container_type>, oneapi::tbb::ets_key_per_instance > ets_tlskey_type;); (9, TBB, typedef typename oneapi::tbb::enumerable_thread_specific< container_type, Allocator<container_type>, oneapi::tbb::ets_no_key > ets_nokey_type;); (44, TBB, oneapi::tbb::flattened2d<ets_nokey_type> fvs2 = flatten2d(vs3, vs3.begin(), std::next(vs3.begin()));); (23, TBB, oneapi::tbb::parallel_reduce ( vs3.range(1), pvrb );); (0, TBB, oneapi::tbb::global_control gc(oneapi::tbb::global_control::max_allowed_parallelism, p);); (10, TBB, oneapi::tbb::parallel_for ( oneapi::tbb::blocked_range<int> (0, N*p, RANGE_MIN), parallel_vector_for_body<T, Allocator>( vs, allocator_name ) );); (42, TBB, const oneapi::tbb::flattened2d<ets_nokey_type>& cfvs2(fvs2);)"}
{"predict": "(65, OpenMP, #define OP_GOMP_CRITICAL_READ(OP, FLAG))", "label": "(65, OpenMP, #define OP_GOMP_CRITICAL_READ(OP, FLAG))"}
{"predict": "(28, OpenMP, #pragma omp parallel for); (16, OpenMP, #pragma omp target map(from: s)); (48, OpenMP, #pragma omp target map(from: s) map(to: dv)); (30, OpenMP, #pragma omp target map(from: s) map(to: a, b, c, d, f))", "label": "(17, OpenMP, s += gsl_linalg_cholesky_decomp_unit(v, dv);)"}
{"predict": "(11, OpenMP, #pragma omp target teams distribute parallel for simd if (parallel: false)); (18, OpenMP, #pragma omp target teams distribute parallel for simd if(simd: true) nontemporal(Arg))", "label": "(19, OpenMP, #pragma omp target teams distribute parallel for simd if (parallel: false)); (10, OpenMP, #pragma omp target teams distribute parallel for simd if(simd: true) nontemporal(Arg))"}
{"predict": "(19, CUDA, cudaSetDevice(currentDevice);)", "label": "(19, CUDA, cudaSetDevice(currentDevice);)"}
{"predict": "(85, OpenMP, #pragma omp distribute parallel for private(S2::S2s)); (121, OpenMP, #pragma omp distribute parallel for private(e, g)); (135, OpenMP, #pragma omp distribute parallel for private(j)); (45, OpenMP, #pragma omp distribute parallel for private (argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (73, OpenMP, #pragma omp distribute parallel for private(ba)); (32, OpenMP, #pragma omp distribute parallel for private (argc,); (7, OpenMP, #pragma omp target); (60, OpenMP, #pragma omp distribute parallel for private(da)); (135, OpenMP, #pragma omp distribute parallel for private(p), private(p)); (128, OpenMP, #pragma omp distribute parallel for private(k)); (29, OpenMP, #pragma omp distribute parallel for private (argc > 0? argv[1] : argv[2])); (48, OpenMP, #pragma omp distribute parallel for private (argv[1])); (94, OpenMP, #pragma omp distribute parallel for private(h)); (99, OpenMP, #pragma omp distribute parallel for private(k), private(k)); (32, OpenMP, #pragma omp distribute parallel for private (S1)); (131, OpenMP, #pragma omp distribute parallel for private(r)); (4, OpenMP, #pragma omp distribute parallel for private ()); (37, OpenMP, #pragma omp distribute parallel for private (a, b, c, d, f)); (6, OpenMP, #pragma omp distribute parallel for private (argc); (43, OpenMP, #pragma omp distribute parallel for private(ca)); (130, OpenMP, #pragma omp distribute parallel for private(fl)); (17, OpenMP, #pragma omp distribute parallel for private (argc, z)); (54, OpenMP, #pragma omp distribute parallel for private(S2::S2sc)); (35, OpenMP, #pragma omp distribute parallel for private (ab)); (26, OpenMP, #pragma omp distribute parallel for private (argv[1] = 2)); (57, OpenMP, #pragma omp distribute parallel for private(S1)); (46, OpenMP, #pragma omp distribute parallel for private(h, B::x));", "label": "(109, OpenMP, #pragma omp distribute parallel for reduction(^ : fl)); (154, OpenMP, #pragma omp distribute parallel for reduction(+ : r)); (117, OpenMP, #pragma omp distribute parallel for reduction(&& : S2::S2sc)); (126, OpenMP, #pragma omp distribute parallel for reduction(+ : h, k, B::x)); (164, OpenMP, #pragma omp distribute parallel for reduction(+ : fl, z)); (39, OpenMP, #pragma omp distribute parallel for reduction(\\)); (6, OpenMP, #pragma omp target); (87, OpenMP, #pragma omp distribute parallel for reduction(+ : ba)); (81, OpenMP, #pragma omp distribute parallel for reduction(max : h.b)); (60, OpenMP, #pragma omp distribute parallel for reduction(&& : argc)); (161, OpenMP, #pragma omp parallel reduction(* : fl)); (28, OpenMP, #pragma omp distribute parallel for reduction(*)); (102, OpenMP, #pragma omp distribute parallel for reduction(&& : S2::S2s)); (6, OpenMP, #pragma omp distribute parallel for reduction +); (85, OpenMP, #pragma omp distribute parallel for reduction(* : ca)); (39, OpenMP, #pragma omp distribute parallel for reduction(| : argc,); (118, OpenMP, #pragma omp distribute parallel for reduction(+ : o)); (125, OpenMP, #pragma omp parallel private(k)); (6, OpenMP, #pragma omp distribute parallel for reduction(); (143, OpenMP, #pragma omp distribute parallel for reduction(max : j)); (44, OpenMP, #pragma omp distribute parallel for reduction(~ : argc)); (138, OpenMP, #pragma omp parallel reduction(min : i)); (143, OpenMP, #pragma omp parallel private(fl)); (26, OpenMP, #pragma omp distribute parallel for reduction(foo : argc); (50, OpenMP, #pragma omp distribute parallel for reduction(^ : S1)); (34, OpenMP, #pragma omp distribute parallel for reduction(|| : argc > 0? argv[1] : argv[2])); (-12, OpenMP, #pragma omp distribute parallel for reduction); (153, OpenMP, #pragma omp distribute parallel for reduction(+ : m)); (76, OpenMP, #pragma omp distribute parallel for reduction(- : da)); (95, OpenMP, #pragma omp distribute parallel for reduction(& : e, g)); (144, OpenMP, #pragma omp distribute parallel for reduction(+ : fl)); (-2, OpenMP, #pragma omp distribute parallel for reduction(-); (47, OpenMP, #pragma omp distribute parallel for reduction(+ : a, b, c, d, f)); (112, OpenMP, #pragma omp distribute parallel for reduction(+ : p), reduction(+ : p)); (151, OpenMP, #pragma omp distribute parallel for reduction(task, + : m)); (49, OpenMP, #pragma omp distribute parallel for reduction(min : a, b, c, d, f)); (122, OpenMP, #pragma omp parallel shared(i)); (102, OpenMP, #pragma omp distribute parallel for private(i), reduction(+ : j), reduction(+ : q)); (-4, OpenMP, #pragma omp distribute parallel for reduction()); (-26, OpenMP, #pragma omp teams)"}
{"predict": "(41, CUDA, cudaError_t result = cudaGetLastError();); (41, CUDA, EXPECT_EQ(result, cudaSuccess) << \" - CUDA error: \" << cudaGetErrorString(result);)", "label": "(41, CUDA, cudaError_t result = cudaGetLastError();); (41, CUDA, EXPECT_EQ(result, cudaSuccess) << \" - CUDA error: \" << cudaGetErrorString(result);)"}
{"predict": "(8, OpenMP, #pragma omp parallel for)", "label": "(8, OpenMP, #pragma omp parallel for)"}
{"predict": "(35, OpenMP, #pragma omp target parallel for map(a, b)); (52, OpenMP, #pragma omp target parallel for map(c)); (61, OpenMP, #pragma omp target update from(c)); (43, OpenMP, #pragma omp target exit data map(a, b, c))", "label": "(54, OpenMP, #pragma omp target parallel for map(alloc : c)); (64, OpenMP, #pragma omp barrier); (44, OpenMP, #pragma omp target exit data map(from : a)); (61, OpenMP, #pragma omp target update from(c) nowait); (21, OpenMP, #pragma omp target enter data map(to : a)); (30, OpenMP, #pragma omp target parallel for)"}
{"predict": "(159, MPI, MPI_Bcast(&log2nstarts,      1, MPI_INT,           root, MPI_COMM_WORLD);); (46, MPI, MPI_Comm_size(MPI_COMM_WORLD,&Num_procs);); (152, MPI, MPI_Bcast(&log2nproc,        1, MPI_INT,           root, MPI_COMM_WORLD);); (303, MPI, MPI_Finalize();); (151, MPI, MPI_Bcast(&log2tablesize,    1, MPI_INT,           root, MPI_COMM_WORLD);); (152, MPI, MPI_Bcast(&log2update_ratio, 1, MPI_INT,           root, MPI_COMM_WORLD);); (41, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&my_ID);); (155, MPI, MPI_Bcast(&nupdate,          1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (149, MPI, MPI_Bcast(&nstarts,          1, MPI_INT,           root, MPI_COMM_WORLD);); (150, MPI, MPI_Bcast(&tablesize,        1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (151, MPI, MPI_Bcast(&tablespace,       1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (149, MPI, MPI_Bcast(&loctablesize,     1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (209, MPI, MPI_Barrier(MPI_COMM_WORLD);); (143, MPI, MPI_Bcast(&update_ratio,     1, MPI_INT,           root, MPI_COMM_WORLD);); (276, MPI, MPI_Reduce(&error, &tot_error, 1, MPI_LONG_LONG_INT, MPI_SUM, root, MPI_COMM_WORLD);)", "label": "(162, MPI, MPI_Bcast(&log2nstarts,      1, MPI_INT,           root, MPI_COMM_WORLD);); (46, MPI, MPI_Comm_size(MPI_COMM_WORLD,&Num_procs);); (155, MPI, MPI_Bcast(&log2nproc,        1, MPI_INT,           root, MPI_COMM_WORLD);); (303, MPI, MPI_Finalize();); (154, MPI, MPI_Bcast(&log2tablesize,    1, MPI_INT,           root, MPI_COMM_WORLD);); (155, MPI, MPI_Bcast(&log2update_ratio, 1, MPI_INT,           root, MPI_COMM_WORLD);); (42, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&my_ID);); (159, MPI, MPI_Bcast(&nupdate,          1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (153, MPI, MPI_Bcast(&nstarts,          1, MPI_INT,           root, MPI_COMM_WORLD);); (154, MPI, MPI_Bcast(&tablesize,        1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (155, MPI, MPI_Bcast(&tablespace,       1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (153, MPI, MPI_Bcast(&loctablesize,     1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (213, MPI, MPI_Barrier(MPI_COMM_WORLD);); (146, MPI, MPI_Bcast(&update_ratio,     1, MPI_INT,           root, MPI_COMM_WORLD);); (280, MPI, MPI_Reduce(&error, &tot_error, 1, MPI_LONG_LONG_INT, MPI_SUM, root, MPI_COMM_WORLD);)"}
{"predict": "(46, MPI, MPI_Op_free(&dtcmp_reduceop_randroot);); (50, MPI, MPI_Op_free(&dtcmp_reduceop_mms_3uint64t);); (54, MPI, MPI_Type_free(&dtcmp_type_3int);); (58, MPI, MPI_Type_free(&dtcmp_type_3uint64t);); (62, MPI, MPI_Comm_free(&dtcmp_comm_self);)", "label": "(45, MPI, MPI_Op_free(&dtcmp_reduceop_randroot);); (49, MPI, MPI_Op_free(&dtcmp_reduceop_mms_3uint64t);); (64, MPI, MPI_Comm_free(&dtcmp_comm_self);); (52, MPI, MPI_Type_free(&dtcmp_type_3int);); (56, MPI, MPI_Type_free(&dtcmp_type_3uint64t);)"}
{"predict": "(78, OpenMP, kmp_gomp_depends_info_t gomp_depends(depend);); (96, OpenMP, __kmpc_omp_task_with_deps(&loc, gtid, task, 1, &oldInfo, 0, &taskdata);); (80, OpenMP, __kmpc_omp_task(&loc, gtid, task);); (70, OpenMP, kmp_int32 ndeps = gomp_depends.get_num_deps();); (97, OpenMP, __kmpc_omp_task_with_deps(&loc, gtid, task, ndeps_cnv, dep_list, 0, &taskdata);); (69, OpenMP, kmp_int32 ndeps_cnv = __kmp_type_convert(ndeps, &ndeps_cnv);); (76, OpenMP, __kmpc_omp_task(&loc, gtid, task);)", "label": "(70, OpenMP, kmp_gomp_depends_info_t gomp_depends(depend);); (103, OpenMP, __kmpc_omp_wait_deps(&loc, gtid, ndeps, dep_list, 0, NULL);); (77, OpenMP, __kmpc_omp_task(&loc, gtid, task);); (74, OpenMP, __kmpc_omp_task_with_deps(&loc, gtid, task, ndeps_cnv, dep_list, 0, NULL);)"}
{"predict": "(15, MPI, MPI_Comm_size(comm, &size);); (16, MPI, MPI_Comm_group(dupcomm, &g2);); (24, MPI, MPI_Comm_free(&dupcomm);); (15, MPI, MPI_Comm_group(newcomm, &g1);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &worldrank);); (28, MPI, MPI_Group_free(&basegroup);); (15, MPI, mpi_errno = MPI_Group_difference(basegroup, g1, &g2);); (8, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (9, MPI, MPI_Comm_split(comm, 0, size - rank, &dupcomm);); (4, MPI, MPI_Comm_rank(comm, &rank);); (17, MPI, MPI_Error_class(mpi_errno, &errclass);); (5, MPI, MPI_Comm_group(comm, &basegroup);); (18, MPI, MPI_Group_free(&g2);); (4, MPI, MPI_Comm_create(dupcomm, g1, &newcomm);); (13, MPI, MPI_Comm_free(&newcomm);); (7, MPI, MPI_Comm_free(&dupcomm);)", "label": "(14, MPI, MPI_Comm_size(comm, &size);); (19, MPI, MPI_Comm_group(dupcomm, &g2);); (28, MPI, MPI_Group_free(&g2);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &worldrank);); (28, MPI, MPI_Comm_free(&dupcomm);); (23, MPI, MPI_Group_free(&basegroup);); (12, MPI, MPI_Comm_group(newcomm, &g1);); (12, MPI, MPI_Comm_dup(comm, &dupcomm);); (15, MPI, mpi_errno = MPI_Group_difference(g1, g2, NULL);); (22, MPI, MPI_Comm_free(&newcomm);); (14, MPI, MPI_Error_class(mpi_errno, &errclass);); (4, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (5, MPI, MPI_Comm_split(comm, 0, size - rank, &newcomm);); (16, MPI, MPI_Group_free(&g1);); (-1, MPI, MPI_Comm_rank(comm, &rank);); (-3, MPI, MPI_Comm_group(comm, &basegroup);)"}
{"predict": "(9, CUDA, HANDLE_ERROR(cudaMalloc((void**)&colored_noise_d, sizeof(float) * full_buffer_size));); (11, CUDA, cudaStreamCreate(&stream);); (18, CUDA, HANDLE_ERROR(cudaStreamSynchronize(stream));)", "label": "(9, CUDA, HANDLE_ERROR(cudaMalloc((void**)&colored_noise_d, sizeof(float) * full_buffer_size));); (11, CUDA, cudaStreamCreate(&stream);); (18, CUDA, HANDLE_ERROR(cudaStreamSynchronize(stream));)"}
{"predict": "(25, OpenMP, #pragma omp target teams distribute collapse 4)); (59, OpenMP, #pragma omp target teams distribute collapse(4, 8)); (57, OpenMP, #pragma omp target teams distribute collapse(2.5)); (77, OpenMP, #pragma omp target teams distribute collapse(0)); (81, OpenMP, #pragma omp target teams distribute collapse(2) firstprivate(i)); (3, OpenMP, #pragma omp target teams distribute collapse(); (87, OpenMP, #pragma omp target teams distribute collapse(5 - 5)); (24, OpenMP, #pragma omp target teams distribute collapse(4); (41, OpenMP, #pragma omp target teams distribute collapse(4 4)); (6, OpenMP, #pragma omp target teams distribute collapse()); (17, OpenMP, #pragma omp target teams distribute collapse 4)); (30, OpenMP, #pragma omp target teams distribute collapse(4,); (72, OpenMP, #pragma omp target teams distribute collapse(-5)); (59, OpenMP, #pragma omp target teams distribute collapse(foo())); (34, OpenMP, #pragma omp target teams distribute collapse(4)); (0, OpenMP, #pragma omp target teams distribute collapse(,); (-12, OpenMP, #pragma omp target teams distribute collapse); (3, OpenMP, #pragma omp target teams distribute collapse(, )); (60, OpenMP, #pragma omp target teams distribute collapse(-10u)); (26, OpenMP, #pragma omp target teams distribute collapse(4, ))", "label": "(41, OpenMP, #pragma omp target teams distribute collapse(4, )); (90, OpenMP, #pragma omp target teams distribute collapse(2) firstprivate(i)); (15, OpenMP, #pragma omp target teams distribute collapse(,); (33, OpenMP, #pragma omp target teams distribute collapse(4,); (74, OpenMP, #pragma omp target teams distribute collapse(-5)); (8, OpenMP, #pragma omp target teams distribute collapse()); (15, OpenMP, #pragma omp target teams distribute collapse(, )); (79, OpenMP, #pragma omp target teams distribute collapse(5 - 5)); (66, OpenMP, #pragma omp target teams distribute collapse(foo())); (73, OpenMP, #pragma omp target teams distribute collapse(0)); (-1, OpenMP, #pragma omp target teams distribute collapse(); (55, OpenMP, #pragma omp target teams distribute collapse(4, 8)); (38, OpenMP, #pragma omp target teams distribute collapse(4 4)); (32, OpenMP, #pragma omp target teams distribute collapse(4)); (-9, OpenMP, #pragma omp target teams distribute collapse); (40, OpenMP, #pragma omp target teams distribute collapse(4,, 4)); (15, OpenMP, #pragma omp target teams distribute collapse(4); (9, OpenMP, #pragma omp target teams distribute collapse 4)); (52, OpenMP, #pragma omp target teams distribute collapse(2.5))"}
{"predict": "(17, CUDA, cudaDeviceSynchronize();); (10, CUDA, cudaMalloc((void**)&cd, size * sizeof(double));); (62, CUDA, cudaFree(cd);); (60, CUDA, cudaMemcpy(c, cd, size * sizeof(double), cudaMemcpyDeviceToHost);)", "label": "(64, CUDA, CUDA_SAFE_CALL( cudaMemcpy(c, cd, size*sizeof(double), cudaMemcpyDeviceToHost) );); (67, CUDA, CUDA_SAFE_CALL( cudaDeviceReset() );); (12, CUDA, CUDA_SAFE_CALL( cudaMemset(cd, 0, size*sizeof(double)) );); (8, CUDA, CUDA_SAFE_CALL( cudaMalloc((void**)&cd, size*sizeof(double)) );); (62, CUDA, CUDA_SAFE_CALL( cudaFree(cd) );); (12, CUDA, CUDA_SAFE_CALL( cudaThreadSynchronize() );)"}
{"predict": "(70, CUDA, cudaMemcpy(&((int *)cuda_mem)[j*size], ptrs[i], size * Mtypes(i), cudaMemcpyHostToDevice);); (67, CUDA, cudaMalloc(&cuda_mem, size * Mtypes(i) * maxGPT);); (64, CUDA, cudaMemcpy(cuda_mem, ptrs[i], size * Mtypes(i), cudaMemcpyHostToDevice);); (62, CUDA, cudaMalloc(&cuda_mem, size * Mtypes(i));)", "label": "(71, CUDA, cudaMemcpy(&((int *)cuda_mem)[j*size], ptrs[i], size * Mtypes(i), cudaMemcpyHostToDevice);); (68, CUDA, cudaMalloc(&cuda_mem, size * Mtypes(i) * maxGPT);); (65, CUDA, cudaMemcpy(cuda_mem, ptrs[i], size * Mtypes(i), cudaMemcpyHostToDevice);); (63, CUDA, cudaMalloc(&cuda_mem, size * Mtypes(i));)"}
{"predict": "(3, CUDA, __shared__ Element shared_storage[Epilogue::Params::kWorkspaceSize];)", "label": "(3, CUDA, __shared__ typename Epilogue::SharedStorage shared_storage;)"}
{"predict": "(2, MPI, using T = thrust::complex<double>;)", "label": "(3, CUDA, cudaSetDevice(WORLD.rank());); (11, CUDA, using T = thrust::complex<double>;); (40, CUDA, cudaStreamSynchronize(NULL);); (14, CUDA, thrust::host_vector<T> H = A;)"}
{"predict": "(9, OpenMP, #pragma omp target teams distribute parallel for reduction(min : min_t)); (6, OpenMP, #pragma omp target data map(to : arr_t[0 : N])); (11, OpenMP, #pragma omp target teams distribute parallel for reduction(max : max_t))", "label": "(9, OpenMP, #pragma omp target teams distribute parallel for reduction(min : min_t)); (6, OpenMP, #pragma omp target data map(to : arr_t[0 : N])); (11, OpenMP, #pragma omp target teams distribute parallel for reduction(max : max_t))"}
{"predict": "(24, OpenCL, error = clCreateBuffer( context, CL_MEM_READ_WRITE, mainSize, NULL, &mainBuffer );); (93, OpenCL, error = clEnqueueReadBuffer( queueA, mainBuffer, CL_TRUE, 0, mainSize, actualResults, 0, NULL, NULL );)", "label": "(23, OpenCL, mainBuffer = clCreateBuffer( context, CL_MEM_READ_WRITE | CL_MEM_COPY_HOST_PTR, mainSize, mainBufferContents, &error );); (93, OpenCL, error = clEnqueueReadBuffer( queueA, mainBuffer, CL_TRUE, 0, mainSize, actualResults, 0, NULL, NULL );)"}
{"predict": "(20, MPI, err = MPI_Waitany(10, requests, &index, statuses);); (33, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_ARE_FATAL);); (33, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (17, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(20, MPI, err = MPI_Waitany(10, requests, &index, statuses);); (33, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_ARE_FATAL);); (33, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (15, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(69, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (66, MPI, MPI_Finalized(&mpi_finalized);); (64, MPI, MPI_Initialized(&mpi_initialized);)", "label": "(69, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (65, MPI, MPI_Finalized(&mpi_finalized);); (63, MPI, MPI_Initialized(&mpi_initialized);)"}
{"predict": "(167, MPI, if ( SendCounts[p] ) MPI_Wait(&send_req[p], &status);); (167, MPI, if ( RecvCounts[p] ) MPI_Wait(&recv_req[p], &status);)", "label": "(166, MPI, if ( SendCounts[p] ) MPI_Wait(&send_req[p], &status);); (166, MPI, if ( RecvCounts[p] ) MPI_Wait(&recv_req[p], &status);)"}
{"predict": "(3, OpenMP, mz_uint comp_flags = TDEFL_COMPUTE_ADLER32 | tdefl_create_comp_flags_from_zip_params(level, window_bits, strategy);)", "label": "(3, OpenMP, mz_uint comp_flags = TDEFL_COMPUTE_ADLER32 | tdefl_create_comp_flags_from_zip_params(level, window_bits, strategy);)"}
{"predict": "(69, OpenMP, #pragma omp target simd simdlen(0)); (26, OpenMP, #pragma omp target simd simdlen 4)); (44, OpenMP, #pragma omp target simd simdlen(4 4)); (13, OpenMP, #pragma omp target simd simdlen(,); (8, OpenMP, #pragma omp target simd simdlen(,); (31, OpenMP, #pragma omp target simd simdlen(4,); (54, OpenMP, #pragma omp target simd simdlen(4, 8)); (37, OpenMP, #pragma omp target simd simdlen(4,, 4)); (-5, OpenMP, #pragma omp target simd simdlen); (57, OpenMP, #pragma omp target simd simdlen(2.5)); (64, OpenMP, #pragma omp target simd simdlen(-5)); (17, OpenMP, #pragma omp target simd simdlen 4)); (41, OpenMP, #pragma omp target simd simdlen(4,, 0)); (0, OpenMP, #pragma omp target simd simdlen()); (66, OpenMP, #pragma omp target simd simdlen(5 - 5)); (22, OpenMP, #pragma omp target simd simdlen(4, )); (-11, OpenMP, #pragma omp target simd simdlen()", "label": "(3, OpenMP, #pragma omp target simd simdlen); (18, OpenMP, #pragma omp target simd simdlen(, )); (70, OpenMP, #pragma omp target simd simdlen(-5)); (31, OpenMP, #pragma omp target simd simdlen(4,); (72, OpenMP, #pragma omp target simd simdlen(0)); (2, OpenMP, #pragma omp target simd simdlen(); (46, OpenMP, #pragma omp target simd simdlen(4,, 4)); (61, OpenMP, #pragma omp target simd simdlen(foo())); (3, OpenMP, #pragma omp target simd simdlen()); (55, OpenMP, #pragma omp target simd simdlen(2.5)); (5, OpenMP, #pragma omp target simd simdlen(,); (31, OpenMP, #pragma omp target simd simdlen(4)); (68, OpenMP, #pragma omp target simd simdlen(5 - 5)); (26, OpenMP, #pragma omp target simd simdlen(4, )); (33, OpenMP, #pragma omp target simd simdlen(4 4)); (45, OpenMP, #pragma omp target simd simdlen(4, 8)); (13, OpenMP, #pragma omp target simd simdlen(4); (7, OpenMP, #pragma omp target simd simdlen 4))"}
{"predict": "(141, CUDA, checkCudaErrors(cudaFree(d_StockPrice));); (139, CUDA, checkCudaErrors(cudaFree(d_OptionYears));); (141, CUDA, checkCudaErrors(cudaFree(d_PutResult));); (49, CUDA, checkCudaErrors(cudaMalloc((void **) &d_StockPrice,   OPT_SZ));); (50, CUDA, checkCudaErrors(cudaMalloc((void **) &d_OptionYears,   OPT_SZ));); (66, CUDA, checkCudaErrors(cudaMemcpy(d_PutResult,  h_PutResultGPU,  OPT_SZ, cudaMemcpyHostToDevice));); (64, CUDA, checkCudaErrors(cudaMemcpy(d_StockPrice,  h_StockPrice,    OPT_SZ, cudaMemcpyHostToDevice));); (49, CUDA, checkCudaErrors(cudaMalloc((void **) &d_OptionStrike,  OPT_SZ));); (135, CUDA, checkCudaErrors(cudaFree(d_OptionStrike));); (142, CUDA, checkCudaErrors(cudaFree(d_PutResultGPU));); (60, CUDA, checkCudaErrors(cudaMemcpy(d_OptionYears,  h_OptionYears,   OPT_SZ, cudaMemcpyHostToDevice));); (58, CUDA, checkCudaErrors(cudaMemcpy(d_OptionStrike,  h_OptionStrike,  OPT_SZ, cudaMemcpyHostToDevice));); (39, CUDA, checkCudaErrors(cudaMalloc((void **) &d_PutResult,  OPT_SZ));); (38, CUDA, checkCudaErrors(cudaMalloc((void **) &d_PutResultGPU,  OPT_SZ));); (39, CUDA, checkCudaErrors(cudaMalloc((void **) &d_StockPriceGPU,   OPT_SZ));); (39, CUDA, checkCudaErrors(cudaMalloc((void **) &d_OptionYearsGPU,   OPT_SZ));); (70, CUDA, getLastCudaError(\"BlackScholesGPU() execution failed\\n\");); (34, CUDA, checkCudaErrors(cudaMalloc((void **) &d_StockPrice,   OPT_SZ));); (127, CUDA, checkCudaErrors(cudaFree(d_StockPrice));); (32, CUDA, checkCudaErrors(cudaMalloc((void **) &d_OptionYears,   OPT_SZ));); (28, CUDA, checkCudaErrors(cudaMalloc((void **) &d_PutResult,  OPT_SZ));); (127, CUDA, checkCudaErrors(cuda", "label": "(49, CUDA, checkCudaErrors(cudaMalloc((void **)&d_CallResult,   OPT_SZ));); (155, CUDA, checkCudaErrors(cudaFree(d_OptionYears));); (155, CUDA, checkCudaErrors(cudaFree(d_OptionStrike));); (155, CUDA, checkCudaErrors(cudaFree(d_StockPrice));); (155, CUDA, checkCudaErrors(cudaFree(d_PutResult));); (66, CUDA, checkCudaErrors(cudaMemcpy(d_OptionStrike, h_OptionStrike,  OPT_SZ, cudaMemcpyHostToDevice));); (45, CUDA, checkCudaErrors(cudaMalloc((void **)&d_StockPrice,   OPT_SZ));); (43, CUDA, checkCudaErrors(cudaMalloc((void **)&d_PutResult,    OPT_SZ));); (26, CUDA, findCudaDevice(argc, (const char **)argv);); (68, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (42, CUDA, checkCudaErrors(cudaMalloc((void **)&d_OptionStrike, OPT_SZ));); (149, CUDA, checkCudaErrors(cudaFree(d_CallResult));); (81, CUDA, getLastCudaError(\"BlackScholesGPU() execution failed\\n\");); (98, CUDA, checkCudaErrors(cudaMemcpy(h_CallResultGPU, d_CallResult, OPT_SZ, cudaMemcpyDeviceToHost));); (58, CUDA, checkCudaErrors(cudaMemcpy(d_OptionYears,  h_OptionYears,   OPT_SZ, cudaMemcpyHostToDevice));); (97, CUDA, checkCudaErrors(cudaMemcpy(h_PutResultGPU,  d_PutResult,  OPT_SZ, cudaMemcpyDeviceToHost));); (37, CUDA, checkCudaErrors(cudaMalloc((void **)&d_OptionYears,  OPT_SZ));); (53, CUDA, checkCudaErrors(cudaMemcpy(d_StockPrice,  h_StockPrice,   OPT_SZ, cudaMemcpyHostToDevice));)"}
{"predict": "(7, MPI, MPI_Win_post(exposureGroup, 0, win);); (16, MPI, t[k].endSync = MPI_Wtime();); (7, MPI, t[k].startOp = MPI_Wtime();); (8, MPI, MPI_Put(&one, sz, MPI_INT, destRank, j, sz, MPI_INT, win);); (12, MPI, MPI_Win_wait(win);); (12, MPI, MPI_Win_free(&win);); (5, MPI, MPI_Win_complete(win);); (0, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(20, MPI, t[k].endSync = MPI_Wtime();); (9, MPI, t[k].startOp = MPI_Wtime();); (10, MPI, MPI_Put(&one, sz, MPI_INT, destRank, j, sz, MPI_INT, win);); (12, MPI, t[k].endOp = MPI_Wtime();); (15, MPI, MPI_Win_wait(win);); (2, MPI, MPI_Win_post(exposureGroup, 0, win);); (12, MPI, MPI_Win_complete(win);); (-1, MPI, MPI_Barrier(MPI_COMM_WORLD);); (0, MPI, MPI_Win_start(accessGroup, 0, win);)"}
{"predict": "(16, CUDA, cudaMemsetAsync(nulls.data(), 0xcccc, nulls.size(), rmm::cuda_stream_default.value());)", "label": "(17, CUDA, cudaMemsetAsync(nulls.data(), 0xcccc, nulls.size(), rmm::cuda_stream_default.value());)"}
{"predict": "(54, MPI, errcode = MPI_File_iwrite(fh, buf, nints, MPI_INT, &request);); (56, MPI, MPI_Wait(&request, &status);); (60, MPI, errcode = MPI_File_close(&fh);); (46, MPI, errcode = MPI_File_set_view(fh, 0, MPI_INT, MPI_INT, \"native\", MPI_INFO_NULL);); (26, MPI, MPI_Bcast(filename, len + 10, MPI_CHAR, 0, MPI_COMM_WORLD);); (106, MPI, MPI_Allreduce(&errs, &toterrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (61, MPI, MPI_File_open(MPI_COMM_SELF, filename, MPI_MODE_CREATE | MPI_MODE_RDWR, MPI_INFO_NULL, &fh);); (81, MPI, errcode = MPI_File_iread(fh, buf, nints, MPI_INT, &request);); (20, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (23, MPI, MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);); (60, MPI, MPI_File_close(&fh);); (3, MPI, MPI_Init(&argc, &argv);); (111, MPI, MPI_Finalize();); (54, MPI, MPI_Wait(&request, &status);); (1, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (76, MPI, MPI_Allreduce(&request, &request, 1, MPI_REQUEST, MPI_SUM, MPI_COMM_WORLD);); (52, MPI, MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_CREATE | MPI_MODE_RDWR, MPI_INFO_NULL, &fh);); (53, MPI, errcode = MPI_File_set_view(fh, 0, MPI_INT, MPI_INT, \"native\", MPI_INFO_NULL);); (-4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (53, MPI, errcode = MPI_File_iwrite(fh, buf, nints, MPI_INT, &request);); (11, MPI, MPI_File_delete(filename, MPI_INFO_NULL);); (55, MPI, MPI_File_close(&fh);); (40, MPI, MPI_File_close(&fh);); (51, MPI, MPI_File_read_all(fh, buf, SIZE, MPI_INT, &status);); (16, MPI, MPI_Bcast(&len,", "label": "(29, MPI, MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);); (51, MPI, errcode = MPI_File_set_view(fh, 0, MPI_INT, MPI_INT, \"native\", MPI_INFO_NULL);); (8, MPI, MPI_Init(&argc, &argv);); (59, MPI, MPI_Wait(&request, &status);); (112, MPI, MPI_Finalize();); (25, MPI, MPI_Bcast(filename, len + 10, MPI_CHAR, 0, MPI_COMM_WORLD);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (96, MPI, MPI_Allreduce(&errs, &toterrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (74, MPI, errcode = MPI_File_iread(fh, buf, nints, MPI_INT, &request);); (14, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (47, MPI, errcode = MPI_File_iwrite(fh, buf, nints, MPI_INT, &request);)"}
{"predict": "(18, MPI, MPI_Accumulate(&local_b[i], 1, MPI_INT, 0, i, 1, MPI_INT, MPI_SUM, win[id]);); (11, MPI, MPI_Win_fence(0, win[id]);); (12, MPI, MPI_Win_unlock_all(win[id]);); (28, MPI, MPI_Win_lock_all(0, win[id]);); (25, MPI, MPI_Win_flush_all(win[id]);)", "label": "(30, MPI, MPI_Get(local_b, COUNT, MPI_INT, 0, 0, COUNT, MPI_INT, win[id]);); (11, MPI, MPI_Win_fence(0, win[id]);); (43, MPI, MPI_Free_mem(local_b);); (11, MPI, MPI_Put(local_b, COUNT, MPI_INT, 0, 0, COUNT, MPI_INT, win[id]);); (4, MPI, MPI_Alloc_mem(COUNT * sizeof(int), MPI_INFO_NULL, &local_b);)"}
{"predict": "(10, MPI, MPI_Init(&argc, &argv);); (133, MPI, MPI_CHECK(MPI_File_close(&fh));); (99, MPI, MPI_Allreduce(&errs, &toterrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (45, MPI, MPI_Type_create_struct(1, &len, &disp, &typevec, &tmptype);); (133, MPI, MPI_Type_free(&tmptype);); (41, MPI, MPI_Type_free(&typevec);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mynod);); (133, MPI, MPI_Type_free(&newtype);); (62, MPI, MPI_CHECK(MPI_File_set_view(fh, 0, MPI_INT, newtype, \"native\", MPI_INFO_NULL));); (45, MPI, MPI_Type_commit(&newtype);); (18, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (33, MPI, MPI_Bcast(filename, len + 1, MPI_CHAR, 0, MPI_COMM_WORLD);); (29, MPI, MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);); (133, MPI, MPI_Finalize();); (-2, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (32, MPI, MPI_Bcast(&i, 1, MPI_INT, 0, MPI_COMM_WORLD);); (128, MPI, MPI_Allreduce(&errs, &errs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (62, MPI, MPI_CHECK(MPI_File_write_at_all(fh, 0, buf, 1, newtype, &status));); (37, MPI, MPI_Type_vector(SIZE / 2, 1, 2, MPI_INT, &typevec);); (39, MPI, MPI_Type_free(&tmptype);); (62, MPI, MPI_Barrier(MPI_COMM_WORLD);); (23, MPI, MPI_Bcast(&buf, SIZE, MPI_INT, 0, MPI_COMM_WORLD);); (10, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (49, MPI, MPI_Error_class(err, &errorclass);); (41, MPI, MPI_Type_create_resized(tmptype, 0, extent, &newtype);); (59, MPI, MPI_CHECK(MPI_File_close(&fh));)", "label": "(197, MPI, MPI_CHECK(MPI_File_write_all(fh, buf, SIZE, MPI_INT, &status));); (9, MPI, MPI_Init(&argc, &argv);); (80, MPI, MPI_CHECK(MPI_File_write_all(fh, buf, 1, newtype, &status));); (42, MPI, MPI_Type_vector(SIZE / 2, 1, 2, MPI_INT, &typevec);); (49, MPI, MPI_Type_create_struct(1, &len, &disp, &typevec, &tmptype);); (73, MPI, MPI_CHECK(MPI_File_set_view(fh, 0, MPI_INT, newtype, \"native\", MPI_INFO_NULL));); (67, MPI, MPI_Barrier(MPI_COMM_WORLD);); (107, MPI, MPI_CHECK(MPI_File_close(&fh));); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mynod);); (48, MPI, MPI_Type_free(&tmptype);); (136, MPI, MPI_CHECK(MPI_File_read_at_all(fh, mynod * (SIZE / 2) * sizeof(int), buf, 1, newtype, &status));); (5, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (192, MPI, MPI_CHECK(MPI_File_read_at_all(fh, 0, buf, SIZE, MPI_INT, &status));); (22, MPI, MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);); (54, MPI, MPI_Error_class(err, &errorclass);); (-4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (42, MPI, MPI_Type_commit(&newtype);); (72, MPI, MPI_CHECK(MPI_File_read_at_all(fh, 0, buf, 1, newtype, &status));); (36, MPI, MPI_Type_free(&typevec);); (37, MPI, MPI_Type_create_resized(tmptype, 0, extent, &newtype);); (220, MPI, MPI_Finalize();); (15, MPI, MPI_Bcast(filename, len + 1, MPI_CHAR, 0, MPI_COMM_WORLD);); (206, MPI, MPI_Allreduce(&errs, &toterrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (214, MPI, MPI_Type_free(&newtype);)"}
{"predict": "(10, TBB, tbb::filter_t<void, void> one_filter( filter_table[i], i_filter );); (15, TBB, tbb::atomic<int> counter;); (11, TBB, tbb::parallel_pipeline( n_tokens, one_filter );)", "label": "(10, TBB, tbb::filter_t<void, void> one_filter( filter_table[i], i_filter );); (15, TBB, tbb::atomic<int> counter;); (11, TBB, tbb::parallel_pipeline( n_tokens, one_filter );)"}
{"predict": "(48, CUDA, CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));)", "label": "(47, CUDA, CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));)"}
{"predict": "(34, CUDA, cudaFree(d_inPtr);); (18, CUDA, cudaMalloc((void**)&d_outPtr, size * sizeof(float));); (23, CUDA, cudaMemcpy(&result_gpu, &d_outPtr[0], sizeof(float), cudaMemcpyDeviceToHost);); (18, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);); (29, CUDA, cudaFree(d_outPtr);); (13, CUDA, cudaMalloc((void**)& d_inPtr, size * sizeof(float));)", "label": "(34, CUDA, cudaFree(d_inPtr);); (18, CUDA, cudaMalloc((void**)&d_outPtr, size * sizeof(float));); (24, CUDA, cudaMemcpy(&result_gpu, &d_outPtr[0], sizeof(float), cudaMemcpyDeviceToHost);); (18, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);); (29, CUDA, cudaFree(d_outPtr);); (13, CUDA, cudaMalloc((void**)& d_inPtr, size * sizeof(float));)"}
{"predict": "(185, OpenMP, #pragma omp barrier); (148, OpenMP, nthread = omp_get_num_threads();); (140, OpenMP, #pragma omp parallel private(i, j, ran, round, index, my_ID) reduction(+:error)); (216, OpenMP, #pragma omp single); (141, OpenMP, my_ID   = omp_get_thread_num();); (132, OpenMP, #pragma omp parallel for); (143, OpenMP, #pragma omp master); (50, OpenMP, omp_set_num_threads(nthread_input);)", "label": "(202, OpenMP, #pragma omp barrier); (150, OpenMP, nthread = omp_get_num_threads();); (197, OpenMP, #pragma omp for); (146, OpenMP, #pragma omp master); (54, OpenMP, omp_set_num_threads(nthread_input);); (137, OpenMP, #pragma omp parallel private(i, j, ran, round, index, my_ID) reduction(+:error)); (223, OpenMP, #pragma omp atomic); (140, OpenMP, my_ID   = omp_get_thread_num();)"}
{"predict": "(17, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (20, MPI, MPI_Type_contiguous((int)chars, MPI_CHAR, &dt_filepath);); (109, MPI, MPI_Type_get_extent(dt_key, &key_lb, &key_extent);); (201, MPI, MPI_Type_free(&dt_key);); (202, MPI, MPI_Type_free(&dt_filepath);); (93, MPI, MPI_Type_get_extent(dt_keysat, &keysat_lb, &keysat_extent);); (106, MPI, MPI_Type_get_extent(dt_sat, &sat_lb, &sat_extent);); (17, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (18, MPI, MPI_Type_commit(&dt_filepath);); (194, MPI, MPI_Type_free(&dt_keysat);); (192, MPI, MPI_Type_free(&dt_sat);); (3, MPI, MPI_Type_contiguous((int)bytes, MPI_BYTE, &dt_sat);)", "label": "(106, MPI, MPI_Type_get_extent(dt_key, &key_lb, &key_extent);); (174, MPI, MPI_Type_free(&dt_key);); (16, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (20, MPI, MPI_Type_commit(&dt_filepath);); (169, MPI, MPI_Type_free(&dt_keysat);); (170, MPI, MPI_Type_free(&dt_filepath);); (107, MPI, MPI_Type_get_extent(dt_sat, &sat_lb, &sat_extent);); (102, MPI, MPI_Type_get_extent(dt_keysat, &keysat_lb, &keysat_extent);); (5, MPI, MPI_Type_contiguous((int)bytes, MPI_BYTE, &dt_sat);); (13, MPI, MPI_Type_contiguous((int)chars, MPI_CHAR, &dt_filepath);); (171, MPI, MPI_Type_free(&dt_sat);)"}
{"predict": "(12, OpenMP, iget = omp_get_teams_thread_limit();); (3, OpenMP, omp_set_num_teams(iset);); (3, OpenMP, iget = omp_get_max_teams();); (9, OpenMP, omp_set_teams_thread_limit(iset);)", "label": "(12, OpenMP, iget = omp_get_teams_thread_limit();); (3, OpenMP, omp_set_num_teams(iset);); (3, OpenMP, iget = omp_get_max_teams();); (8, OpenMP, omp_set_teams_thread_limit(iset);)"}
{"predict": "(13, TBB, test_resets<float,tbb::flow::queue_node<float> >();); (20, TBB, test_buffer_extract<tbb::flow::queue_node<int> >().run_tests();); (10, TBB, test_resets<int,tbb::flow::queue_node<int> >();); (0, TBB, tbb::task_scheduler_init init(p);); (17, TBB, test_resets<tbb::flow::queue_node<check_type<int> >();); (5, TBB, stop = tbb::tick_count::now();); (-3, TBB, tbb::tick_count start = tbb::tick_count::now(), stop;)", "label": "(12, TBB, test_resets<int, tbb::flow::queue_node<int> >();); (2, TBB, tbb::task_scheduler_init init(p);); (7, TBB, stop = tbb::tick_count::now();); (18, TBB, test_buffer_extract<tbb::flow::queue_node<int> >().run_tests();); (-3, TBB, tbb::tick_count start = tbb::tick_count::now(), stop;); (8, TBB, test_resets<float, tbb::flow::queue_node<float> >();)"}
{"predict": "(6, CUDA, cudaMallocManaged(&x, N*sizeof(float));); (16, CUDA, cudaDeviceSynchronize();); (26, CUDA, cudaFree(y);); (3, CUDA, cudaMallocManaged(&y, N*sizeof(float));); (22, CUDA, cudaFree(x);)", "label": "(16, CUDA, cudaDeviceSynchronize();); (6, CUDA, cudaMallocManaged(&y, N*sizeof(float));); (4, CUDA, cudaMallocManaged(&x, N*sizeof(float));); (23, CUDA, cudaFree(y);); (21, CUDA, cudaFree(x);)"}
{"predict": "(13, CUDA, cudaError_t err = cudaFree(ptr);)", "label": "(13, CUDA, cudaError_t err = cudaFree(ptr);)"}
{"predict": "(5, OpenMP, #pragma omp target data map(to:vecTs)); (16, OpenMP, #pragma omp target teams distribute parallel for map(to:vecTs[i].getArr())); (20, OpenMP, #pragma omp target update from:vecTs[i].getArr()))", "label": "(17, OpenMP, #pragma omp parallel for num_threads(num_devices)); (10, OpenMP, #pragma omp target teams distribute parallel for); (3, OpenMP, size_t num_devices = omp_get_num_devices();); (17, OpenMP, #pragma omp target teams distribute parallel for device(omp_get_thread_num()))"}
{"predict": "(8, MPI, MPI_Reduce(&time, &max_time, 1, MPI_LONG_LONG_INT, MPI_MAX, 0, MPI_COMM_WORLD);); (8, MPI, MPI_Reduce(&time, &min_time, 1, MPI_LONG_LONG_INT, MPI_MIN, 0, MPI_COMM_WORLD);); (8, MPI, MPI_Reduce(&time, &sum_time, 1, MPI_LONG_LONG_INT, MPI_SUM, 0, MPI_COMM_WORLD);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)", "label": "(9, MPI, MPI_Reduce(&time, &sum_time, 1, MPI_LONG_LONG, MPI_SUM, 0, comm_);); (6, MPI, MPI_Reduce(&time, &max_time, 1, MPI_LONG_LONG, MPI_MAX, 0, comm_);); (10, MPI, MPI_Comm_size(comm_, &size);); (8, MPI, MPI_Comm_rank(comm_, &rank);); (4, MPI, MPI_Reduce(&time, &min_time, 1, MPI_LONG_LONG, MPI_MIN, 0, comm_);)"}
{"predict": "(10, MPI, MPI_Comm_size(MSG_COMM_WORLD, &size);); (133, MPI, MPI_Finalize();); (8, MPI, MPI_Comm_rank(MSG_COMM_WORLD, &rank);); (3, MPI, MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided);); (57, MPI, MPI_Op_create((MPI_User_function *) MSG_Win_flush, 0, &MSG_Win_flush);); (57, MPI, MPI_Op_create((MPI_User_function *) MSG_Win_get, 0, &MSG_Win_get);); (57, MPI, MPI_Op_create((MPI_User_function *) MSG_Win_put, 0, &MSG_Win_put);); (133, MPI, MPI_Ssend(NULL, 0, MPI_CHAR, 0, 0, MSG_COMM_WORLD);); (130, MPI, MPI_Barrier(MSG_COMM_WORLD);); (56, MPI, MPI_Op_create((MPI_User_function *) MSG_Win_acc, 0, &MSG_Win_acc);); (130, MPI, MPI_Mrecv(NULL, 0, MPI_CHAR, 0, 0, MSG_COMM_WORLD, MPI_STATUS_IGNORE);); (8, MPI, MPI_Comm_set_errhandler(MSG_COMM_WORLD, MPI_ERRORS_RETURN);); (56, MPI, MPI_Op_size(MSG_Win_get, &type_size);); (56, MPI, MPI_Op_size(MSG_Win_put, &type_size);); (53, MPI, MPI_Op_size(MSG_Win_flush, &type_size);); (8, MPI, MPI_Abort(MSG_COMM_WORLD, 1);)", "label": "(9, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &MSG_COMM_WORLD);); (4, MPI, MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided);); (63, MPI, MPI_Type_size(type, &type_size);); (16, MPI, MPI_Barrier(MSG_COMM_WORLD);); (130, MPI, MPI_Comm_free(&MSG_COMM_WORLD);); (131, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_rank(MSG_COMM_WORLD, &rank);); (5, MPI, MPI_Comm_size(MSG_COMM_WORLD, &size);)"}
{"predict": "(37, CUDA, cudaFreeHost( hptr );)", "label": "(37, CUDA, cudaFreeHost( hptr );)"}
{"predict": "(26, OpenMP, #pragma omp target simd aligned(B::ib,B:C1+C2)); (18, OpenMP, #pragma omp target simd aligned(B,rp,::z: X::x)); (0, OpenMP, #pragma omp target simd aligned(B:bfoo())); (21, OpenMP, #pragma omp target simd aligned(::z)); (6, OpenMP, #pragma omp target simd aligned(z:B:bfoo())); (0, OpenMP, #pragma omp target simd aligned(B::ib:B:bfoo())); (15, OpenMP, #pragma omp target simd aligned(X::x : ::z)); (3, OpenMP, #pragma omp target simd aligned(X::x : ::z)); (-3, OpenMP, #pragma omp target simd aligned(B::ib:B:bfoo()))", "label": "(17, OpenMP, #pragma omp target simd aligned(X::x : ::z)); (2, OpenMP, #pragma omp target simd aligned(B:bfoo())); (23, OpenMP, #pragma omp target simd aligned(B::bfoo())); (17, OpenMP, #pragma omp target simd aligned(B,rp,::z: X::x)); (24, OpenMP, #pragma omp target simd aligned(B::ib,B:C1+C2)); (6, OpenMP, #pragma omp target simd aligned(z:B:bfoo())); (0, OpenMP, #pragma omp target simd aligned(B::ib:B:bfoo())); (1, OpenMP, #pragma omp target simd aligned(B:B::bfoo())); (14, OpenMP, #pragma omp target simd aligned(::z))"}
{"predict": "(10, OpenMP, __kmpc_omp_task(NULL, 0, task);); (2, OpenMP, printf(\"Thread %i: producing task\\n\", omp_get_thread_num());)", "label": "(10, OpenMP, __kmpc_omp_task(NULL, 0, task);); (2, OpenMP, printf(\"%d: creating task\\n\", omp_get_thread_num());)"}
{"predict": "(25, TBB, tbb::task::spawn_root_and_wait(vtask);)", "label": "(25, TBB, tbb::task_scheduler_init init;); (25, TBB, tbb::parallel_for( tbb::blocked_range<int>(0,n_cams), vtask);)"}
{"predict": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(34, CUDA, CubDebugExit(cudaGetDevice(&device_ordinal));); (70, CUDA, CubDebugExit(cudaFree(0));); (41, CUDA, CubDebugExit(cudaGetDeviceProperties(&sm_count, device_ordinal));); (41, CUDA, CubDebugExit(cudaGetDeviceProperties(&max_block_threads, device_ordinal));); (41, CUDA, CubDebugExit(cudaGetDeviceProperties(&max_sm_occupancy, device_ordinal));)", "label": "(34, CUDA, CubDebugExit(cudaGetDevice(&device_ordinal));); (42, CUDA, CubDebugExit(cudaDeviceGetAttribute(&max_block_threads, cudaDevAttrMaxThreadsPerBlock, device_ordinal));); (40, CUDA, CubDebugExit(cudaDeviceGetAttribute(&sm_count, cudaDevAttrMultiProcessorCount, device_ordinal));); (69, CUDA, retval = CubDebug(cudaDeviceSynchronize());)"}
{"predict": "(91, OpenMP, double tic = omp_get_wtime();); (107, OpenMP, #pragma omp target exit data map(from: u[0:n*n])); (115, OpenMP, double stop = omp_get_wtime();); (102, OpenMP, double toc = omp_get_wtime();); (7, OpenMP, double start = omp_get_wtime();); (79, OpenMP, #pragma omp target enter data map(to: u[0:n*n], u_tmp[0:n*n]))", "label": "(92, OpenMP, double tic = omp_get_wtime();); (108, OpenMP, #pragma omp target exit data map(from: u[0:n*n])); (116, OpenMP, double stop = omp_get_wtime();); (103, OpenMP, double toc = omp_get_wtime();); (6, OpenMP, double start = omp_get_wtime();); (80, OpenMP, #pragma omp target enter data map(to: u[0:n*n], u_tmp[0:n*n]))"}
{"predict": "(46, OpenCL, actualSampler = clCreateSampler( context, CL_FALSE, CL_ADDRESS_NONE, CL_FILTER_NEAREST, &error );); (76, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 3, NULL, threads, NULL, 0, NULL, NULL );); (61, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_mem ), &read_write_image );); (98, OpenCL, clReleaseMemObject(results);); (55, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_mem ), &read_only_image );); (99, OpenCL, clReleaseSampler(actualSampler);); (62, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_mem ), &results );); (93, OpenCL, clReleaseMemObject(read_write_image);); (91, OpenCL, clReleaseMemObject(read_only_image);); (69, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_sampler ), &actualSampler );); (45, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_mem ), &results );); (44, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_mem ), &read_only_image );); (49, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_mem ), &read_write_image );); (73, OpenCL, error = clEnqueueReadImage( queue, results, CL_TRUE, 0, 0, resultValues, 0, NULL, NULL );)", "label": "(46, OpenCL, actualSampler = clCreateSampler( context, CL_FALSE, CL_ADDRESS_NONE, CL_FILTER_NEAREST, &error );); (55, OpenCL, clEnqueueWriteBuffer( queue, results, CL_TRUE, 0, resultValuesSize, resultValues, 0, NULL, NULL );); (80, OpenCL, error = clEnqueueReadBuffer( queue, results, CL_TRUE, 0, resultValuesSize, resultValues, 0, NULL, NULL );); (75, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 3, NULL, threads, NULL, 0, NULL, NULL );); (60, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_mem ), &read_write_image );); (98, OpenCL, clReleaseMemObject(results);); (96, OpenCL, clReleaseSampler(actualSampler);); (100, OpenCL, clReleaseMemObject(read_write_image);); (96, OpenCL, clReleaseMemObject(read_only_image);); (51, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_mem ), &read_only_image );); (59, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_mem ), &results );); (56, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_sampler ), &actualSampler );); (38, OpenCL, cl_mem results = clCreateBuffer( context, 0, imageInfo->width * imageInfo->height * imageInfo->arraySize * sizeof(cl_int), NULL, &error);)"}
{"predict": "(27, CUDA, cudaMallocManaged(&h_a, bufsize);); (27, CUDA, cudaMallocManaged(&h_b, bufsize);); (44, CUDA, int i = omp_get_thread_num();); (16, CUDA, cudaSetDevice(0);); (79, CUDA, cudaFree(d_c);); (79, CUDA, cudaFree(d_b);); (27, CUDA, cudaMallocManaged(&d_b, bufsize);); (25, CUDA, cudaMallocManaged(&d_a, bufsize);); (79, CUDA, cudaFreeHost(h_b);); (13, CUDA, cudaSetDevice(0);); (76, CUDA, cudaFreeHost(h_a);); (25, CUDA, cudaMallocManaged(&d_c, bufsize);); (73, CUDA, cudaFree(d_a);); (11, CUDA, cudaSetDevice(0);); (7, CUDA, cudaGetDeviceCount(&np);); (7, CUDA, cudaGetDevice(&rank);); (22, CUDA, cudaFreeHost(h_c);); (18, CUDA, cudaSetDevice(0);); (72, CUDA, cudaFreeHost(h_a);); (73, CUDA, cudaFreeHost(h_c);); (19, CUDA, cudaMallocManaged(&h_c, bufsize);); (71, CUDA, cudaFreeHost(h_b);); (72, CUDA, cudaFree(d_a);); (20, CUDA, cudaMallocManaged(&d_a, bufsize);); (18, CUDA, cudaMallocManaged(&d_b, bufsize);); (20, CUDA, cudaMallocManaged(&d_c, bufsize);); (46, OpenMP, #pragma omp parallel); (20, CUDA, cudaFreeHost(h_a);); (9, CUDA, cudaSetDevice(0);); (46, OpenMP, int i = omp_get_num_threads();); (20, CUDA, cudaFreeHost(h_b);); (2, CUDA, cudaSetDevice(0);); (44, OpenMP, #pragma omp master); (9, CUDA, cudaMallocManaged(&h_b, bufsize);); (9, CUDA, cudaMallocManaged(&h_c, bufsize);); (64, CUDA, cudaFreeHost(h_a);); (61, CUDA, cudaFree(d_a);); (8, CUDA, cudaMallocManaged(&d_a, bufsize);); (9, CUDA, cudaMallocManaged(&d_b, bufsize);); (8, CUDA, cuda", "label": "(51, OpenMP, omp_set_num_threads(num_operator);); (16, MPI, MPI_Init(&argc, &argv);); (37, CUDA, cudaMalloc((void**)&d_a, bufsize);); (80, CUDA, cudaFree(d_a);); (24, CUDA, cudaMallocHost((void**)&h_a, bufsize);); (25, CUDA, cudaMallocHost((void**)&h_c, bufsize);); (58, CUDA, cudaDeviceSynchronize();); (78, CUDA, cudaFree(d_c);); (44, OpenMP, #pragma omp parallel); (75, CUDA, cudaFree(d_b);); (30, CUDA, cudaMalloc((void**)&d_b, bufsize);); (43, OpenMP, int i = omp_get_thread_num();); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (16, CUDA, cudaMallocHost((void**)&h_b, bufsize);); (75, CUDA, cudaFreeHost(h_b);); (73, CUDA, cudaFreeHost(h_a);); (25, CUDA, cudaMalloc((void**)&d_c, bufsize);); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &np);); (74, MPI, MPI_Finalize();); (71, CUDA, cudaFreeHost(h_c);)"}
{"predict": "(7, OpenMP, #pragma omp simd private(myind, g_ind), linear(ind), aligned(arr) linear(uval(ref)))", "label": "(7, OpenMP, #pragma omp simd private(myind, g_ind), linear(ind), aligned(arr), linear(uval(ref)))"}
{"predict": "(40, MPI, MPI_Send(&sendbuf, 1, struct_type, 0, 0, MPI_COMM_SELF);); (47, MPI, MPI_Get_elements(&status, struct_type, &count);); (27, MPI, MPI_Type_create_struct(3, blens, displs, types, &struct_type);); (27, MPI, MPI_Type_commit(&contig);); (54, MPI, MPI_Type_free(&struct_type);); (47, MPI, MPI_Recv(&recvbuf, 1, struct_type, 0, 0, MPI_COMM_SELF, &status);); (24, MPI, MPI_Type_contiguous(3, struct_type, &contig);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(29, MPI, MPI_Type_contiguous(1, struct_type, &contig);); (51, MPI, MPI_Type_free(&struct_type);); (26, MPI, MPI_Type_create_struct(3, blens, displs, types, &struct_type);); (27, MPI, MPI_Type_commit(&struct_type);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (26, MPI, MPI_Type_commit(&contig);); (45, MPI, MPI_Get_elements(&status, contig, &count);); (34, MPI, MPI_Sendrecv(&sendbuf, 1, contig, 0, 0, &recvbuf, 1, contig, 0, 0, MPI_COMM_SELF, &status);); (45, MPI, MPI_Type_free(&contig);)"}
{"predict": "(3, MPI, MPI_Init(&argc, &argv);); (13, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)", "label": "(4, MPI, MPI_Init(&argc, &argv);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (11, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(28, TBB, tbb_list_time = (tbb::tick_count::now() - t0).seconds();); (43, TBB, serial_tree_time = (tbb::tick_count::now() - t0).seconds();); (15, TBB, serial_list_time = (tbb::tick_count::now() - t0).seconds();); (13, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (49, TBB, tbb_tree_time = (tbb::tick_count::now() - t0).seconds();)", "label": "(28, TBB, tbb_list_time = (tbb::tick_count::now() - t0).seconds();); (42, TBB, serial_tree_time = (tbb::tick_count::now() - t0).seconds();); (15, TBB, serial_list_time = (tbb::tick_count::now() - t0).seconds();); (12, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (50, TBB, tbb_tree_time = (tbb::tick_count::now() - t0).seconds();)"}
{"predict": "(72, OpenMP, #pragma omp target parallel for collapse(-5)); (58, OpenMP, #pragma omp target parallel for collapse(4, 8)); (90, OpenMP, #pragma omp target parallel for collapse(2)); (18, OpenMP, #pragma omp target parallel for collapse 4)); (4, OpenMP, #pragma omp target parallel for collapse(); (8, OpenMP, #pragma omp target parallel for collapse()); (30, OpenMP, #pragma omp target parallel for collapse(4,); (36, OpenMP, #pragma omp target parallel for collapse(4)); (22, OpenMP, #pragma omp target parallel for collapse 4)); (30, OpenMP, #pragma omp target parallel for collapse(4, )); (37, OpenMP, #pragma omp target parallel for collapse(4 4)); (0, OpenMP, #pragma omp target parallel for collapse(,); (16, OpenMP, #pragma omp target parallel for collapse(4); (43, OpenMP, #pragma omp target parallel for collapse(4,, 4)); (74, OpenMP, #pragma omp target parallel for collapse(5 - 5)); (49, OpenMP, #pragma omp target parallel for collapse(2.5)); (59, OpenMP, #pragma omp target parallel for collapse(foo())); (20, OpenMP, #pragma omp target parallel for collapse(4,); (64, OpenMP, #pragma omp target parallel for collapse(0)); (26, OpenMP, #pragma omp target parallel for collapse(4)); (38, OpenMP, #pragma omp target parallel for collapse(4,, 4)); (5, OpenMP, #pragma omp target parallel for collapse(, )); (74, OpenMP, #pragma omp target parallel for collapse(2) firstprivate(i)); (-18, OpenMP, #pragma omp target parallel for collapse); (15, OpenMP, #pragma omp target parallel for collapse(4,); (39, OpenMP, #pragma omp target parallel for collapse(4,, 4, 4))", "label": "(31, OpenMP, #pragma omp target parallel for collapse(4); (54, OpenMP, #pragma omp target parallel for collapse(4,, 4)); (43, OpenMP, #pragma omp target parallel for collapse(4)); (2, OpenMP, #pragma omp target parallel for collapse); (22, OpenMP, #pragma omp target parallel for collapse 4)); (73, OpenMP, #pragma omp target parallel for collapse(-5)); (3, OpenMP, #pragma omp target parallel for collapse(); (79, OpenMP, #pragma omp target parallel for collapse(5 - 5)); (82, OpenMP, #pragma omp target parallel for collapse(2) firstprivate(i)); (41, OpenMP, #pragma omp target parallel for collapse(4 4)); (60, OpenMP, #pragma omp target parallel for collapse(2.5)); (63, OpenMP, #pragma omp target parallel for collapse(foo())); (29, OpenMP, #pragma omp target parallel for collapse(4, )); (0, OpenMP, #pragma omp target parallel for collapse()); (68, OpenMP, #pragma omp target parallel for collapse(0)); (2, OpenMP, #pragma omp target parallel for collapse(,); (20, OpenMP, #pragma omp target parallel for collapse(4,); (4, OpenMP, #pragma omp target parallel for collapse(, )); (48, OpenMP, #pragma omp target parallel for collapse(4, 8))"}
{"predict": "(32, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (34, CUDA, CUDA_CHECK(cudaFree(d_data));)", "label": "(32, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (34, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(5, CUDA, err = cudaMalloc(&numErr_d, sizeof(int));); (21, CUDA, err = cudaFree(numErr_d);); (9, CUDA, err = cudaDeviceSynchronize();); (1, CUDA, cudaFree(numErr_d);); (13, CUDA, err = cudaMemcpy(&numErr_h, numErr_d, sizeof(int), cudaMemcpyDeviceToHost);); (18, CUDA, printf(\"Cuda Main: cudaFree failed with %s\\n\", cudaGetErrorString(err));)", "label": "(5, CUDA, err = cudaMalloc(&numErr_d, sizeof(int));); (25, CUDA, printf(\"Cuda Main: cudaFree failed with %s\\n\", cudaGetErrorString(err));); (11, CUDA, err = cudaDeviceSynchronize();); (4, CUDA, printf(\"Cuda Main: cudaMemcpy failed with %s\\n\", cudaGetErrorString(err));); (20, CUDA, err = cudaFree(numErr_d);); (13, CUDA, err = cudaMemcpy(&numErr_h, numErr_d, sizeof(int), cudaMemcpyDeviceToHost);); (2, CUDA, cudaFree(numErr_d);)"}
{"predict": "(5, CUDA, __shared__ uint s_Hist[HISTOGRAM256_THREADBLOCK_SIZE * HISTOGRAM256_BIN_COUNT];)", "label": "(5, CUDA, __shared__ uint s_Hist[HISTOGRAM256_THREADBLOCK_MEMORY];)"}
{"predict": "(55, MPI, ascent_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (7, MPI, MPI_Comm_rank(comm, &par_rank);); (7, MPI, MPI_Comm_size(comm, &par_size);)", "label": "(55, MPI, ascent_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (7, MPI, MPI_Comm_rank(comm, &par_rank);); (7, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(17, OpenMP, #pragma omp target teams distribute parallel for map(to:vecA) reduction(+:sum)); (22, OpenMP, #pragma omp target update from(sum)); (18, OpenMP, #pragma omp target data map(to:vecA, sum))", "label": "(17, OpenMP, #pragma omp parallel for default(shared) private(i))"}
{"predict": "(35, CUDA, cudaFree(res_dev);); (28, CUDA, cudaMalloc(&res_dev, sizeof(float));); (30, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)", "label": "(35, CUDA, cudaFree(res_dev);); (28, CUDA, cudaMalloc(&res_dev, sizeof(float));); (30, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(43, MPI, ascent_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (15, MPI, MPI_Comm_rank(comm, &par_rank);); (15, MPI, MPI_Comm_size(comm, &par_size);)", "label": "(43, MPI, ascent_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (15, MPI, MPI_Comm_rank(comm, &par_rank);); (15, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(201, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (109, CUDA, CUDA_CHECK(cudaMemcpyAsync(d_A, h_A.data(), llda * loc_n_a * sizeof(double), cudaMemcpyHostToDevice, stream));); (201, MPI, MPI_Barrier(MPI_COMM_WORLD);); (48, CUDA, CUDA_CHECK(cudaFree(nullptr));); (190, CUDA, CUDA_CHECK(cudaFreeAsync(d_C, stream));); (45, CUDA, CUDA_CHECK(cudaSetDevice(local_device));); (20, MPI, MPI_Init(nullptr, nullptr);); (191, CUDA, CUDA_CHECK(cudaFreeAsync(d_work, stream));); (99, CUDA, CUDA_CHECK(cudaMallocAsync(&d_A, llda * loc_n_a * sizeof(double), stream));); (143, MPI, const double begin = MPI_Wtime();); (183, CUDA, CUDA_CHECK(cudaFreeAsync(d_A, stream));); (99, CUDA, CUDA_CHECK(cudaMallocAsync(&d_C, lldc * loc_n_c * sizeof(double), stream));); (184, MPI, MPI_Allreduce(&end, &max_end, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);); (32, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (96, CUDA, CUDA_CHECK(cudaMemcpyAsync(d_C, h_C.data(), lldc * loc_n_c * sizeof(double), cudaMemcpyHostToDevice, stream));); (55, CUDA, CUDA_CHECK(cudaStreamCreate(&stream));); (190, MPI, MPI_Finalize();); (27, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nranks);); (128, CUDA, CUDA_CHECK(cudaMallocAsync(&d_work, workspaceInBytesOnDevice, stream));); (161, MPI, const double end = MPI_Wtime();); (174, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));)", "label": "(197, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (109, CUDA, CUDA_CHECK(cudaMemcpyAsync(d_A, h_A.data(), llda * loc_n_a * sizeof(double), cudaMemcpyHostToDevice, stream));); (197, MPI, MPI_Barrier(MPI_COMM_WORLD);); (48, CUDA, CUDA_CHECK(cudaFree(nullptr));); (186, CUDA, CUDA_CHECK(cudaFreeAsync(d_C, stream));); (45, CUDA, CUDA_CHECK(cudaSetDevice(local_device));); (20, MPI, MPI_Init(nullptr, nullptr);); (184, CUDA, CUDA_CHECK(cudaFreeAsync(d_work, stream));); (99, CUDA, CUDA_CHECK(cudaMallocAsync(&d_A, llda * loc_n_a * sizeof(double), stream));); (141, MPI, const double begin = MPI_Wtime();); (179, CUDA, CUDA_CHECK(cudaFreeAsync(d_A, stream));); (33, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (96, CUDA, CUDA_CHECK(cudaMallocAsync(&d_C, lldc * loc_n_c * sizeof(double), stream));); (98, CUDA, CUDA_CHECK(cudaMemcpyAsync(d_C, h_C.data(), lldc * loc_n_c * sizeof(double), cudaMemcpyHostToDevice, stream));); (55, CUDA, CUDA_CHECK(cudaStreamCreate(&stream));); (186, MPI, MPI_Finalize();); (27, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nranks);); (126, CUDA, CUDA_CHECK(cudaMallocAsync(&d_work, workspaceInBytesOnDevice, stream));); (157, MPI, const double end = MPI_Wtime();)"}
{"predict": "(1, CUDA, if (!IsGoogleCudaEnabled()) return;)", "label": "(1, CUDA, if (!IsGoogleCudaEnabled()) return;)"}
{"predict": "(3, CUDA, extern __shared__ unsigned scx1[];)", "label": "(3, CUDA, extern __shared__ unsigned scx1[];)"}
{"predict": "(3, CUDA, extern __shared__ float shared[];)", "label": "(3, CUDA, extern __shared__ float shared[];)"}
{"predict": "(46, MPI, MPI_Startall(2, requests);); (48, MPI, MPI_Waitall(2, requests, statuses);); (53, MPI, t1 = MPI_Wtime();); (13, MPI, MPI_Init(&argc, &argv);); (60, MPI, MPI_Finalize();); (12, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (12, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (33, MPI, MPI_Barrier(MPI_COMM_WORLD);); (33, MPI, t0 = MPI_Wtime();)", "label": "(48, MPI, MPI_Startall(2, requests);); (50, MPI, MPI_Waitall(2, requests, statuses);); (57, MPI, t1 = MPI_Wtime();); (12, MPI, MPI_Init(&argc, &argv);); (61, MPI, MPI_Finalize();); (11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (31, MPI, MPI_Barrier(MPI_COMM_WORLD);); (31, MPI, t0 = MPI_Wtime();)"}
{"predict": "(12, OpenCL, retVal = clGetDeviceInfo(device, CL_DEVICE_PROFILE, sizeof(profile), profile, NULL);)", "label": "(12, OpenCL, retVal = clGetDeviceInfo(device, CL_DEVICE_PROFILE, sizeof(profile), profile, NULL);)"}
{"predict": "(20, CUDA, nativeObject = env->NewObject(cudaStreamAttrValue_accessPolicyWindow);)", "label": "(21, CUDA, setCudaAccessPolicyWindow(env, javaObject, nativeObject.accessPolicyWindow);)"}
{"predict": "(22, CUDA, fptype ret = thrust::norm(totalAmp);)", "label": "(22, CUDA, fptype ret = thrust::norm(totalAmp);)"}
{"predict": "(6, OpenMP, #pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp parallel for)"}
{"predict": "(27, MPI, MPI_Type_get_true_extent(MPI_INT, &true_lb, &true_extent);); (34, MPI, MPI_Type_commit(&aligned_type);); (31, MPI, MPI_Type_contiguous(1, aligned_type, newtype);); (33, MPI, MPI_Type_free(&aligned_type);); (28, MPI, MPI_Type_free(&newtype);); (29, MPI, MPI_Type_create_struct(num, blocklens, displs, types, &aligned_type);)", "label": "(38, MPI, MPI_Type_free(&aligned_type);); (36, MPI, MPI_Type_commit(newtype);); (33, MPI, MPI_Type_create_struct(num, blocklens, displs, types, &aligned_type);); (33, MPI, MPI_Type_create_resized(aligned_type, 0, disp, newtype);); (23, MPI, MPI_Type_get_true_extent(oldtypes[i], &true_lb, &true_extent);)"}
{"predict": "(15, OpenCL, retVal = clEnqueueUnmapMemObject(cmdQ.get(), image.get(), mappedPtr, 0, nullptr, nullptr);); (7, OpenCL, void *mappedPtr = clEnqueueMapImage(cmdQ.get(), image.get(), CL_TRUE, CL_MAP_WRITE, &origin[0], &region[0], nullptr, nullptr, 0, nullptr, nullptr, &retVal);)", "label": "(8, OpenCL, void *mappedPtr = clEnqueueMapImage(cmdQ.get(), image.get(), CL_TRUE, CL_MAP_WRITE, &origin[0], &region[0], nullptr, nullptr, 0, nullptr, nullptr, &retVal);); (14, OpenCL, retVal = clEnqueueUnmapMemObject(cmdQ.get(), image.get(), mappedPtr, 0, nullptr, nullptr);)"}
{"predict": "(6, OpenMP, #pragma omp target teams distribute parallel for simd defaultmap); (1, OpenMP, #pragma omp target teams distribute parallel for simd defaultmap); (27, OpenMP, #pragma omp target teams distribute parallel for simd defaultmap(tofrom, scalar); (16, OpenMP, #pragma omp target teams distribute parallel for simd defaultmap(tofrom)); (19, OpenMP, #pragma omp target teams distribute parallel for simd defaultmap(tofrom scalar)); (6, OpenMP, #pragma omp target teams distribute parallel for simd defaultmap(tofrom); (11, OpenMP, #pragma omp target teams distribute parallel for simd defaultmap (tofrom:); (16, OpenMP, #pragma omp target teams distribute parallel for simd defaultmap(tofrom,)", "label": "(29, OpenMP, #pragma omp target simd defaultmap(tofrom, scalar); (1, OpenMP, #pragma omp target simd defaultmap); (9, OpenMP, #pragma omp target simd defaultmap(tofrom); (20, OpenMP, #pragma omp target simd defaultmap(tofrom,); (10, OpenMP, #pragma omp target simd defaultmap(tofrom:); (15, OpenMP, #pragma omp target simd defaultmap(tofrom scalar)); (20, OpenMP, #pragma omp target simd defaultmap(scalar:); (-2, OpenMP, #pragma omp target simd defaultmap(); (9, OpenMP, #pragma omp target simd defaultmap(tofrom)); (-1, OpenMP, #pragma omp target simd defaultmap())"}
{"predict": "(85, MPI, ascent_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (92, MPI, MPI_Barrier(comm);); (17, MPI, MPI_Comm_rank(comm, &par_rank);); (17, MPI, MPI_Comm_size(comm, &par_size);)", "label": "(85, MPI, ascent_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (91, MPI, MPI_Barrier(comm);); (16, MPI, MPI_Comm_rank(comm, &par_rank);); (16, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(9, OpenMP, #pragma omp target update from( colstat[0:2])); (2, OpenMP, #pragma omp target enter data map(to: colstat[0:2])); (5, OpenMP, #pragma omp target enter data map(to: colstat[2])); (0, OpenMP, #pragma omp target enter data map(to: colstat[1]))", "label": "(2, OpenMP, #pragma omp target map(alloc:colstat[0:NZ])); (5, OpenMP, #pragma omp target map(alloc:colstat[:0])); (9, OpenMP, #pragma omp target update from(colstat))"}
{"predict": "(3, TBB, tbb::task_scheduler_init init( 1 );)", "label": "(41, TBB, ASSERT( (int)tbb::tbb_thread::hardware_concurrency() == availableProcs, NULL );); (39, TBB, ASSERT( tbb::task_scheduler_init::default_num_threads() == availableProcs, NULL );)"}
{"predict": "(7, OpenMP, auto err = CompareDeviceResults(d_reference_begin, d_data, 1, true, g_verbose);)", "label": "(7, CUDA, auto err = thrust::mismatch(thrust::device, d_reference_begin, d_reference_end, d_data);); (2, CUDA, CubDebugExit(cudaMemcpy(d_tmp_buffer, host_reference, sizeof(T) * num_items, cudaMemcpyHostToDevice));); (9, CUDA, const auto index = thrust::distance(d_reference_begin, err.first);)"}
{"predict": "(11, CUDA, CUDA_SAFE_CALL(cudaMalloc(&device_buffer, buffer_size * sizeof(value_type)));)", "label": "(12, CUDA, CUDA_SAFE_CALL(cudaMalloc(&device_buffer, buffer_size * sizeof(value_type)));)"}
{"predict": "(6, CUDA, cudaMallocManaged(&ptr, size);); (30, CUDA, cudaFree(ptr);)", "label": "(8, CUDA, cudaSetDevice(devs[0]);); (5, CUDA, CUCH(cudaMallocManaged(&ptr, size), kmcudaMemoryAllocationFailure);); (7, CUDA, CUCH(cudaMemcpy(ptr, (*samples)[0].get(), size, cudaMemcpyDefault), kmcudaMemoryCopyError);)"}
{"predict": "(51, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (34, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_B), sizeof(data_type) * B.size()));); (62, CUDA, CUDA_CHECK(cudaFree(d_B));); (64, CUDA, CUDA_CHECK(cudaDeviceReset());); (61, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (29, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (55, CUDA, CUDA_CHECK(cudaFree(d_A));); (23, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)", "label": "(51, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (33, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_B), sizeof(data_type) * B.size()));); (61, CUDA, CUDA_CHECK(cudaFree(d_B));); (66, CUDA, CUDA_CHECK(cudaDeviceReset());); (63, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (28, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (56, CUDA, CUDA_CHECK(cudaFree(d_A));); (22, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(3, CUDA, __shared__ float local[threads];)", "label": "(3, CUDA, __shared__ float local[threads];)"}
{"predict": "(47, OpenMP, EXPECT_TRUE(check_test_image(output_file, t_apcomp_baseline_dir()));); (9, MPI, MPI_Comm_rank(comm, &par_rank);); (9, MPI, MPI_Comm_size(comm, &par_size);)", "label": "(47, OpenMP, EXPECT_TRUE(check_test_image(output_file, t_apcomp_baseline_dir()));); (9, MPI, MPI_Comm_rank(comm, &par_rank);); (10, MPI, apcomp::mpi_comm(MPI_Comm_c2f(comm));); (8, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(35, MPI, MPI_Win_allocate_shared(size, 1, MPI_INFO_NULL, shm_comm, &my_base, &shm_win);); (24, MPI, MPI_Comm_rank(shm_comm, &shm_rank);); (34, MPI, MPI_Win_allocate(0, 1, MPI_INFO_NULL, shm_comm, &my_base, &shm_win);); (25, MPI, MPI_Comm_size(shm_comm, &shm_nproc);); (16, MPI, MPI_Comm_query(MPI_COMM_SELF, MPI_PROC_NULL, &query_size, &query_disp_unit, &query_base);); (69, MPI, MPI_Comm_free(&shm_comm);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (17, MPI, MPI_Win_shared_query(shm_win, MPI_PROC_NULL, &query_size, &query_disp_unit, &query_base);); (35, MPI, MPI_Win_free(&shm_win);); (3, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (8, MPI, MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, 0, MPI_INFO_NULL, &shm_comm);); (13, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (30, MPI, MPI_Win_allocate_shared(0, 1, MPI_INFO_NULL, shm_comm, &my_base, &shm_win);)", "label": "(28, MPI, MPI_Comm_rank(shm_comm, &shm_rank);); (15, MPI, MPI_Win_shared_query(shm_win, MPI_PROC_NULL, &query_size, &query_disp_unit, &query_base);); (13, MPI, MPI_Win_allocate_shared(size, sizeof(int), MPI_INFO_NULL, MPI_COMM_SELF, &my_base, &shm_win);); (23, MPI, MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, rank, MPI_INFO_NULL, &shm_comm);); (34, MPI, MPI_Win_allocate_shared(0, sizeof(int), MPI_INFO_NULL, shm_comm, &my_base, &shm_win);); (20, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (67, MPI, MPI_Comm_free(&shm_comm);); (15, MPI, MPI_Win_free(&shm_win);); (21, MPI, MPI_Comm_size(shm_comm, &shm_nproc);); (31, MPI, MPI_Win_allocate_shared(size, sizeof(int), MPI_INFO_NULL, shm_comm, &my_base, &shm_win);)"}
{"predict": "(13, CUDA, cudaMalloc((void **)&d_arr, sizeof(int) * len);); (16, CUDA, cudaMemcpy(d_arr, arr, sizeof(int) * len, cudaMemcpyHostToDevice);); (53, CUDA, cudaFree(d_out);); (11, CUDA, cudaMalloc((void **)&d_out, sizeof(int) * len);); (23, CUDA, cudaMemcpy(out, d_out, sizeof(int) * len, cudaMemcpyDeviceToHost);); (49, CUDA, cudaFree(d_arr);)", "label": "(13, CUDA, cudaMalloc((void **)&d_arr, sizeof(int) * len);); (16, CUDA, cudaMemcpy(d_arr, arr, sizeof(int) * len, cudaMemcpyHostToDevice);); (53, CUDA, cudaFree(d_out);); (11, CUDA, cudaMalloc((void **)&d_out, sizeof(int) * len);); (23, CUDA, cudaMemcpy(out, d_out, sizeof(int) * len, cudaMemcpyDeviceToHost);); (49, CUDA, cudaFree(d_arr);)"}
{"predict": "(3, OpenMP, #pragma omp target data map(tofrom: shared_ptr)); (13, OpenMP, #pragma omp target exit data map(from: shared_ptr)); (2, OpenMP, int *shared_ptr = (int *)omp_target_alloc(N * sizeof(int), omp_get_default_device());)", "label": "(4, OpenMP, int *shared_ptr = llvm_omp_target_alloc_shared(N * sizeof(int), device);); (5, OpenMP, #pragma omp target teams distribute parallel for device(device)); (0, OpenMP, const int device = omp_get_default_device();); (13, OpenMP, omp_target_free(shared_ptr, device);)"}
{"predict": "(52, TBB, tbb::flow::make_edge( b2, b3 );); (50, TBB, tbb::flow::make_edge( b, b2 );); (73, TBB, tbb::flow::buffer_node<T> b_copy(b);); (0, TBB, tbb::flow::graph g;); (0, TBB, tbb::flow::buffer_node<T> b3(g);); (87, TBB, tbb::flow::remove_edge( b, b_copy );); (45, TBB, tbb::flow::make_edge(b, b3);); (-4, TBB, tbb::flow::buffer_node<T> b2(g);); (-6, TBB, tbb::flow::buffer_node<T> b(g);)", "label": "(52, TBB, tbb::flow::make_edge( b2, b3 );); (50, TBB, tbb::flow::make_edge( b, b2 );); (73, TBB, tbb::flow::buffer_node<T> b_copy(b);); (2, TBB, tbb::flow::buffer_node<T> b3(g);); (0, TBB, tbb::flow::buffer_node<T> b2(g);); (-3, TBB, tbb::flow::graph g;); (-3, TBB, tbb::flow::buffer_node<T> b(g);)"}
{"predict": "(22, OpenCL, clReleaseEvent(outEvent1);)", "label": "(22, OpenCL, clReleaseEvent(outEvent1);)"}
{"predict": "(59, CUDA, cudaEventDestroy( stop );); (59, CUDA, cudaFree( deviceThreadIDs );); (57, CUDA, cudaFree( deviceClockValues );); (58, CUDA, cudaFreeHost( hostOut );); (55, CUDA, cudaEventDestroy( start );)", "label": "(59, CUDA, cudaFree( deviceClockValues );); (60, CUDA, cudaFreeHost( hostOut );); (61, CUDA, cudaEventDestroy( stop );); (59, CUDA, cudaEventDestroy( start );); (56, CUDA, cudaFree( deviceThreadIDs );)"}
{"predict": "(59, MPI, t1 = MPI_Wtime();); (64, MPI, t2 = MPI_Wtime();)", "label": "(63, MPI, t2 = MPI_Wtime();); (58, MPI, t1 = MPI_Wtime();)"}
{"predict": "(186, CUDA, int accepted = thrust::count_if(flags.begin(), flags.end(), thrust::identity<bool>());)", "label": "(184, CUDA, long accepted  = thrust::count_if(flags.begin(), flags.end(), thrust::identity<bool>());)"}
{"predict": "(33, MPI, MPI_Comm_size(MPI_COMM_WORLD,&mpi_size);); (33, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&mpi_rank);)", "label": "(34, MPI, MPI_Comm_size(MPI_COMM_WORLD,&mpi_size);); (34, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&mpi_rank);)"}
{"predict": "(6, CUDA, extern __shared__ uint64_t s_QHatModp[];)", "label": "(6, CUDA, extern __shared__ uint64_t s_QHatModp[];)"}
{"predict": "(12, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (12, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (40, MPI, MPI_Win_fence(0, win);); (39, MPI, MPI_Accumulate(mine, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_REPLACE, win);); (39, MPI, MPI_Accumulate(mine_plus, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_REPLACE, win);); (66, MPI, MPI_Win_create(data, sizeof(twoint_t) * ARRAY_LEN, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &win);); (9, MPI, MPI_Info_create(&info_in);); (46, MPI, MPI_Win_unlock(0, win);); (42, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, 0, 0, win);); (48, MPI, MPI_Win_flush(0, win);); (51, MPI, MPI_Win_unlock(nproc-1, win);); (47, MPI, MPI_Win_flush(nproc-1, win);); (18, MPI, MPI_Win_create(NULL, 0, 1, info_in, MPI_COMM_WORLD, &win);); (1, MPI, MPI_Info_set(info_in, \"ordering\", \"default\");); (62, MPI, MPI_Win_free(&win);); (48, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, nproc-1, 0, win);); (54, MPI, MPI_Win_flush(0, win);); (13, MPI, MPI_Info_set(info_in, \"length\", \"2\");); (58, MPI, MPI_Win_flush(nproc-1, win);); (64, MPI, MPI_Info_free(&info_in);); (2, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (9, MPI, MPI_Info_free(&info_in);)", "label": "(81, MPI, MPI_Win_create(data, sizeof(twoint_t) * ARRAY_LEN, 1, info_in, MPI_COMM_WORLD, &win);); (81, MPI, MPI_Info_free(&info_in);); (134, MPI, MPI_Accumulate(mine_plus, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_MAXLOC, win);); (151, MPI, MPI_Accumulate(mine_plus, ARRAY_LEN, MPI_2INT, 0, 0, ARRAY_LEN, MPI_2INT, MPI_MAXLOC, win);); (34, MPI, MPI_Win_fence(0, win);); (56, MPI, MPI_Accumulate(mine, ARRAY_LEN, MPI_2INT, 0, 0, ARRAY_LEN, MPI_2INT, MPI_REPLACE, win);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (63, MPI, MPI_Win_free(&win);); (70, MPI, MPI_Info_create(&info_in);); (70, MPI, MPI_Info_set(info_in, \"accumulate_ordering\", \"none\");); (32, MPI, MPI_Accumulate(mine, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_REPLACE, win);); (181, MPI, MPI_Accumulate(mine, ARRAY_LEN, MPI_2INT, 0, 0, ARRAY_LEN, MPI_2INT, MPI_MINLOC, win);); (31, MPI, MPI_Accumulate(mine_plus, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_REPLACE, win);); (48, MPI, MPI_Accumulate(mine_plus, ARRAY_LEN, MPI_2INT, 0, 0, ARRAY_LEN, MPI_2INT, MPI_REPLACE, win);); (160, MPI, MPI_Accumulate(mine, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_MINLOC, win);); (1, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (20, MPI, MPI_Win_create(data, sizeof(twoint_t) * ARRAY_LEN, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &win);)"}
{"predict": "(50, CUDA, CUDA_CHECK(cudaEventRecord(stop, 0));); (50, CUDA, CUDA_CHECK(cudaEventSynchronize(stop));); (49, CUDA, CUDA_CHECK(cudaEventElapsedTime(&milliseconds, start, stop));); (73, CUDA, CUDA_CHECK(cudaFree(instances_data));); (19, CUDA, CUDA_CHECK(cudaDeviceSetLimit(cudaLimitStackSize, 4 * 1024));); (17, CUDA, CUDA_CHECK(cudaDeviceSetLimit(cudaLimitMallocHeapSize, heap_size));); (56, CUDA, CUDA_CHECK(cudaEventDestroy(start));); (56, CUDA, CUDA_CHECK(cudaEventDestroy(stop));); (18, CUDA, CUDA_CHECK(cudaEventCreate(&start));); (18, CUDA, CUDA_CHECK(cudaEventCreate(&stop));); (45, CUDA, CUDA_CHECK(cudaEventRecord(start, 0));); (5, CUDA, CUDA_CHECK(cudaMallocManaged((void**)&(instances_data), sizeof(CuEVM::evm_instance_t) * num_instances));); (51, CUDA, CUDA_CHECK(cudaDeviceReset());)", "label": "(56, CUDA, cudaEventRecord(stop);); (22, CUDA, CUDA_CHECK(cudaDeviceSetLimit(cudaLimitStackSize, 2 * 1024));); (55, CUDA, cudaEventSynchronize(stop);); (58, CUDA, cudaEventDestroy(start);); (5, CUDA, CUDA_CHECK(cudaDeviceReset());); (21, CUDA, CUDA_CHECK(cudaDeviceSynchronize());); (46, CUDA, cudaEventRecord(start);); (55, CUDA, cudaEventDestroy(stop);); (57, CUDA, CUDA_CHECK(cudaGetLastError());); (8, CUDA, cudaDeviceGetLimit(&size_value, cudaLimitStackSize);); (12, CUDA, CUDA_CHECK(cudaDeviceSetLimit(cudaLimitMallocHeapSize, heap_size));); (40, CUDA, cudaEventCreate(&stop);); (46, CUDA, cudaEventElapsedTime(&milliseconds, start, stop);); (37, CUDA, cudaEventCreate(&start);)"}
{"predict": "(19, OpenMP, #pragma omp parallel for reduction(+:s)); (10, OpenMP, #pragma omp parallel for)", "label": "(19, OpenMP, #pragma omp parallel for reduction(+:s)); (10, OpenMP, #pragma omp parallel for)"}
{"predict": "(10, MPI, MPI_Allreduce(input, output, 3, MPI_UINT64_T, MPI_MIN_MAX_SUM, comm);)", "label": "(10, MPI, MPI_Allreduce(input, output, 1, dtcmp_type_3uint64t, dtcmp_reduceop_mms_3uint64t, comm);)"}
{"predict": "(26, MPI, MPI_Wait(&request, &status);); (18, MPI, MPI_Bcast(&array_size, 1, MPI_INT, 0, MPI_COMM_WORLD);); (18, MPI, MPI_Bcast(&nr_iters, 1, MPI_INT, 0, MPI_COMM_WORLD);); (5, MPI, MPI_Init(&argc, &argv);); (61, MPI, MPI_Finalize();); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (25, MPI, MPI_Reduce(array1, result, array_size, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);); (11, MPI, MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);)", "label": "(19, MPI, MPI_Bcast(&array_size, 1, MPI_INT, 0, MPI_COMM_WORLD);); (26, MPI, MPI_Wait(&request, &status);); (41, MPI, end_time = MPI_Wtime();); (66, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (2, MPI, MPI_Init(NULL, NULL);); (2, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (25, MPI, start_time = MPI_Wtime();); (12, MPI, MPI_Bcast(&nr_iters, 1, MPI_INT, 0, MPI_COMM_WORLD);); (6, MPI, MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);)"}
{"predict": "(2, OpenMP, omp_autotune(self, OMP_SCALE);)", "label": "(2, OpenMP, omp_autotune(self, OMP_SCALE);)"}
{"predict": "(47, MPI, MPIU_Free( p2 );); (57, MPI, MPIU_Free( p3 );)", "label": "(127, MPI, *ierr = MPI_Comm_spawn_multiple( *v1, p2, p3, v4, v5, *v6, (MPI_Comm)(*v7), (MPI_Comm *)(v8), (int *)v9 );)"}
{"predict": "(10, OpenMP, #pragma omp target map(large, bestScore)); (95, OpenMP, #pragma omp critical)", "label": "(10, OpenMP, #pragma omp parallel); (14, OpenMP, #pragma omp for); (95, OpenMP, #pragma omp critical)"}
{"predict": "(21, MPI, MPI_Waitall(2, requests, MPI_STATUSES_IGNORE);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (28, MPI, MPI_Recv(&a, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (18, MPI, MPI_Irecv(&a, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, &requests[0]);); (18, MPI, MPI_Isend(&a, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, &requests[1]);); (28, MPI, MPI_Send(&a, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);); (4, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(22, MPI, MPI_Start(&requests[0]);); (17, MPI, MPI_Barrier_init(MPI_COMM_WORLD, MPI_INFO_NULL, &requests[1]);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (14, MPI, MPI_Bcast_init(&a, 1, MPI_INT, 0, MPI_COMM_WORLD, MPI_INFO_NULL, &requests[0]);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (18, MPI, MPI_Start(&requests[1]);); (23, MPI, MPI_Waitall(2, requests, MPI_STATUSES_IGNORE);); (5, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (27, MPI, MPI_Request_free(&requests[0]);); (27, MPI, MPI_Request_free(&requests[1]);)"}
{"predict": "(11, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:sum1) reduction(+:sum1) collapse(2)); (21, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:sum1) reduction(+:sum1) collapse(2) private(i,sum1,b) firstprivate(j))", "label": "(11, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:sum1) reduction(+:sum1) collapse(2))"}
{"predict": "(16, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (22, TBB, serial_tiled_time = (tbb::tick_count::now() - t0).seconds();); (27, TBB, tbb_time = (tbb::tick_count::now() - t0).seconds();); (15, TBB, serial_time = (tbb::tick_count::now() - t0).seconds();)", "label": "(16, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (22, TBB, serial_tiled_time = (tbb::tick_count::now() - t0).seconds();); (27, TBB, tbb_time = (tbb::tick_count::now() - t0).seconds();); (15, TBB, serial_time = (tbb::tick_count::now() - t0).seconds();)"}
{"predict": "(29, CUDA, cudaMalloc(&d_a, bytes);); (30, CUDA, cudaMalloc(&d_b, bytes);); (11, CUDA, cudaGetDevice(&deviceID);); (42, CUDA, cudaGetDeviceProperties(&prop, deviceID);); (114, CUDA, cudaFree(d_a);); (114, CUDA, cudaFree(d_b);); (24, CUDA, cudaHostRegister(h_aPageable, cudaHostRegisterPortable);); (25, CUDA, cudaHostRegister(h_bPageable, cudaHostRegisterPortable);); (17, CUDA, cudaHostRegister(h_aPinned, cudaHostRegisterPortable);); (25, CUDA, cudaHostRegister(h_bPinned, cudaHostRegisterPortable);); (17, CUDA, cudaHostRegister(h_bCached, cudaHostRegisterPortable);); (21, CUDA, cudaHostRegister(h_aCached, cudaHostRegisterPortable);)", "label": "(50, CUDA, cudaGetDeviceProperties(&prop, 0);); (125, CUDA, cudaFree(h_aCached);); (29, CUDA, cudaMallocHost((void**)&h_aCached, bytes);); (119, CUDA, cudaFreeHost(h_aPinned);); (119, CUDA, cudaFreeHost(h_bPinned);); (29, CUDA, cudaHostAlloc((void**)&h_bPinned, bytes, cudaHostAllocWriteCombined);); (24, CUDA, cudaMalloc((void**)&d_b, bytes);); (26, CUDA, cudaHostAlloc((void**)&h_aPinned, bytes, cudaHostAllocWriteCombined);); (24, CUDA, cudaMallocHost((void**)&h_bCached, bytes);); (116, CUDA, cudaFree(d_b);); (-1, CUDA, cudaSetDevice(0);); (113, CUDA, cudaFree(d_a);); (115, CUDA, cudaFree(h_bCached);); (-10, CUDA, cudaGetDeviceCount(&count);); (-2, CUDA, cudaGetDevice(&deviceID);); (14, CUDA, cudaMalloc((void**)&d_a, bytes);)"}
{"predict": "(40, OpenCL, err = clEnqueueCopyBuffer(queue, streams[0], streams[1], 0, 0, sizeof(cl_int)*num_elements, 0, NULL, &copyEvent);); (29, OpenCL, err = clEnqueueWriteBuffer(queue, streams[0], true, 0, sizeof(cl_int)*num_elements, (void *)int_input_ptr, 0, NULL, NULL);); (49, OpenCL, err = clWaitForEvents( 1, &copyEvent );); (50, OpenCL, clReleaseEvent(copyEvent);); (27, OpenCL, err |= clEnqueueWriteBuffer(queue, streams[1], true, 0, sizeof(cl_int)*num_elements, (void *)int_output_ptr, 0, NULL, NULL);); (104, OpenCL, err = clEnqueueReadBuffer(queue, streams[1], true, 0, sizeof(cl_int)*num_elements, (void *)int_output_ptr, 0, NULL, NULL);)", "label": "(30, OpenCL, err = clEnqueueWriteBuffer( queue, streams[0], true, 0, sizeof(cl_int)*num_elements, (void *)int_input_ptr, 0, NULL, NULL );); (39, OpenCL, err = clEnqueueCopyBuffer( queue, streams[0], streams[1], 0, 0, sizeof(cl_int)*num_elements, 0, NULL, &copyEvent );); (32, OpenCL, clReleaseMemObject( streams[1] );); (107, OpenCL, err = clEnqueueReadBuffer( queue, streams[1], true, 0, sizeof(cl_int)*num_elements, (void *)int_output_ptr, 0, NULL, NULL );); (48, OpenCL, err = clWaitForEvents( 1, &copyEvent );); (28, OpenCL, clReleaseMemObject( streams[0] );); (49, OpenCL, clReleaseEvent(copyEvent);)"}
{"predict": "(7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myid);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &numprocs);); (14, MPI, MPI_Recv(buffer, 10, MPI_INT, left, 0, MPI_COMM_WORLD, &status);); (13, MPI, MPI_Wait(&request, &status);); (11, MPI, MPI_Send(buffer, 10, MPI_INT, right, 0, MPI_COMM_WORLD);); (13, MPI, MPI_Request_free(&request);); (9, MPI, MPI_Irecv(buffer, 10, MPI_INT, left, 0, MPI_COMM_WORLD, &request);)", "label": "(9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myid);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &numprocs);); (16, MPI, MPI_Finalize();); (4, MPI, MPI_Init(&argc,&argv);); (12, MPI, MPI_Sendrecv_replace(buffer, 10, MPI_INT, left, 123, right, 123, MPI_COMM_WORLD, &status);)"}
{"predict": "(3, TBB, REMARK(\"is_large_object<int>::value=%d\\n\", tbb::interface6::internal::is_large_object<int>::value);); (4, TBB, REMARK(\"is_large_object<int *>::value=%d\\n\", tbb::interface6::internal::is_large_object<int *>::value);); (2, TBB, REMARK(\"is_large_object<double>::value=%d\\n\", tbb::interface6::internal::is_large_object<double>::value);); (4, TBB, REMARK(\"is_large_object<check_type<int>* >::value=%d\\n\", tbb::interface6::internal::is_large_object<check_type<int>* >::value);); (2, TBB, REMARK(\"is_large_object<check_type<int> >::value=%d\\n\", tbb::interface6::internal::is_large_object<check_type<int> >::value);); (3, TBB, REMARK(\"is_large_object<check_type<int>* >::value=%d\\n\\n\", tbb::interface6::internal::is_large_object<check_type<int>* >::value);)", "label": "(3, TBB, REMARK(\"is_large_object<int>::value=%d\\n\", tbb::interface6::internal::is_large_object<int>::value);); (4, TBB, REMARK(\"is_large_object<int *>::value=%d\\n\", tbb::interface6::internal::is_large_object<int *>::value);); (2, TBB, REMARK(\"is_large_object<double>::value=%d\\n\", tbb::interface6::internal::is_large_object<double>::value);); (4, TBB, REMARK(\"is_large_object<check_type<int>* >::value=%d\\n\", tbb::interface6::internal::is_large_object<check_type<int>* >::value);); (2, TBB, REMARK(\"is_large_object<check_type<int> >::value=%d\\n\", tbb::interface6::internal::is_large_object<check_type<int> >::value);); (3, TBB, REMARK(\"is_large_object<check_type<short> >::value=%d\\n\\n\", tbb::interface6::internal::is_large_object<check_type<short> >::value);); (8, TBB, tbb::task_scheduler_init init(nthread);)"}
{"predict": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(46, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_ARE_FATAL);); (21, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (21, MPI, MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);); (18, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (37, MPI, MPI_Error_string(ierr, str, &slen);)", "label": "(47, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_ARE_FATAL);); (21, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (21, MPI, MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);); (18, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (38, MPI, MPI_Error_string(ierr, str, &slen);)"}
{"predict": "(53, CUDA, extern __shared__ float s_float[];)", "label": "(53, CUDA, extern __shared__ float s_float[];)"}
{"predict": "(22, OpenMP, #pragma omp barrier); (15, OpenMP, #pragma omp parallel num_threads(n_threads)); (29, CUDA, RAFT_CUDA_TRY(cudaStreamSynchronize(stream));)", "label": "(26, OpenMP, #pragma omp barrier); (19, OpenMP, common::nvtx::range omp_scope(\"interruptible::thread-%d\", i);); (13, OpenMP, omp_set_dynamic(0);); (14, OpenMP, #pragma omp parallel reduction(+ : n_finished) reduction(+ : n_cancelled) num_threads(n_threads)); (12, OpenMP, omp_set_num_threads(n_threads);); (14, OpenMP, auto i = omp_get_thread_num();)"}
{"predict": "(25, MPI, MPI_Send(lnzval, len2, MPI_DOUBLE, receiver, k, grid3d->zscp.comm);)", "label": "(25, MPI, MPI_Send(lnzval, len2, MPI_DOUBLE, receiver, k, grid3d->zscp.comm);)"}
{"predict": "(47, OpenMP, lis2mdl_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)", "label": "(47, OpenMP, lis2mdl_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)"}
{"predict": "(2, OpenMP, #pragma omp for schedule(dynamic) private(a, b, c, d))", "label": "(2, OpenMP, #pragma omp for schedule(nonmonotonic: dynamic))"}
{"predict": "(21, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(21, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(37, CUDA, cudaFree(d_ret);); (7, CUDA, cudaMallocHost(&h_stride, STRIDE_MEM_SIZE);); (15, CUDA, cudaMalloc(&d_ret, sizeof(uint32_t));); (37, CUDA, cudaFree(d_clk);); (34, CUDA, cudaFree(d_stride);); (15, CUDA, cudaMalloc(&d_clk, sizeof(uint32_t));); (26, CUDA, cudaMemcpy(h_clk, d_clk, 32 * sizeof(uint32_t), cudaMemcpyDeviceToHost);); (9, CUDA, cudaMalloc(&d_stride, STRIDE_MEM_SIZE);); (18, CUDA, cudaMemset(d_ret, 0, sizeof(uint32_t));); (18, CUDA, cudaMemset(d_clk, 0, sizeof(uint32_t));)", "label": "(35, CUDA, cudaFree(d_ret);); (7, CUDA, cudaMallocHost(&h_stride, STRIDE_MEM_SIZE);); (14, CUDA, cudaMalloc(&d_ret, sizeof(uint32_t));); (31, CUDA, cudaFree(d_stride);); (32, CUDA, cudaFree(d_clk);); (32, CUDA, cudaFreeHost(h_stride);); (25, CUDA, cudaMemcpy(h_clk, d_clk, 32 * sizeof(uint32_t), cudaMemcpyDeviceToHost);); (10, CUDA, cudaMemcpy(d_stride, h_stride, STRIDE_MEM_SIZE, cudaMemcpyHostToDevice);); (7, CUDA, cudaMalloc(&d_stride, STRIDE_MEM_SIZE);); (11, CUDA, cudaMalloc(&d_clk, 32 * sizeof(uint32_t));)"}
{"predict": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(5, OpenMP, #pragma omp target map(from: a[0:N], b[0:N])); (1, OpenMP, #pragma omp target data map(from: a[0:N], b[0:N]))", "label": "(5, OpenMP, #pragma omp target enter data map(to:a[:N]))"}
{"predict": "(15, TBB, test_basic<MyCheckedStateSet>(\"concurrent_unordered_set (checked state of elements)\", tbb::internal::true_type());); (39, TBB, test_basic<MyCheckedStateSet>(\"concurrent_unordered_multiset (checked state of elements)\", tbb::internal::true_type());); (16, TBB, test_basic<MyCheckedStateSet>(\"concurrent_unordered_set (checked state of elements)\", tbb::internal::true_type());); (17, TBB, test_basic<MyCheckedStateSet>(\"concurrent_unordered_multiset (checked state of elements)\", tbb::internal::true_type());); (10, TBB, test_basic<MyCheckedStateSet>(\"concurrent_unordered_set (checked state of elements)\", tbb::internal::true_type());); (15, TBB, test_basic<MyCheckedStateSet>(\"concurrent_unordered_multiset (checked state of elements)\", tbb::internal::true_type());); (8, TBB, test_basic<MyCheckedStateSet>(\"concurrent_unordered_set (checked state of elements)\", tbb::internal::true_type());); (16, TBB, test_basic<MyCheckedStateSet>(\"concurrent_unordered_multiset (checked state of elements)\", tbb::internal::true_type());); (9, TBB, test_basic<MyCheckedStateSet>(\"concurrent_unordered_set (checked state of elements)\", tbb::internal::true_type());); (16, TBB, test_basic<MyCheckedStateSet>(\"concurrent_unordered_multiset (checked state of elements)\", tbb::internal::true_type());); (9, TBB, test_basic<MyCheckedStateSet>(\"concurrent_unordered_set (checked state of elements)\", tbb::internal::true_type());); (8, TBB, test_basic<MyCheckedStateSet>(\"concurrent_unordered_multiset (checked state of elements)\", tbb::internal::true_type());); (10, TBB, test_basic<MyCheckedStateSet>(\"concurrent_unordered_set (checked state of elements)\", tbb::internal::true_type());); (11, TBB, test_basic<MyCheckedStateSet>(\"concurrent_unordered_multiset (checked state of elements)\", tbb::internal::true_type());); (-1, TBB, test_basic<MyCheckedStateSet>(\"concurrent_unordered_set (checked state of elements)\", tbb::internal::true_type());); (11, TBB, test_basic<MyChecked", "label": "(15, TBB, test_basic<MyCheckedStateSet>(\"concurrent unordered set (checked element state)\", tbb::internal::true_type());)"}
{"predict": "(17, CUDA, extern __shared__ _VOLATILE_ float sMem[];)", "label": "(17, CUDA, extern __shared__ _VOLATILE_ float sMem[];)"}
{"predict": "(9, CUDA, factory = *internal::MakeCUDAExecutorImplementation();)", "label": "(9, CUDA, factory = *internal::MakeCUDAExecutorImplementation();)"}
{"predict": "(26, MPI, MPI_Comm_size(MPI_COMM_WORLD,&Num_procs);); (130, OpenMP, #pragma omp parallel for reduction(+:local_nstream_time)); (81, MPI, MPI_Bcast(&iterations,1, MPI_INT, root, MPI_COMM_WORLD);); (79, MPI, MPI_Bcast(&offset,1, MPI_LONG, root, MPI_COMM_WORLD);); (77, MPI, MPI_Bcast(&length,1, MPI_LONG, root, MPI_COMM_WORLD);); (23, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&my_ID);); (20, MPI, MPI_Init(&argc,&argv);); (81, OpenMP, #pragma omp parallel for); (243, MPI, MPI_Finalize();); (74, MPI, MPI_Bcast(&nthread_input,1, MPI_INT, root, MPI_COMM_WORLD);); (18, MPI, MPI_Comm_size(MPI_COMM_WORLD,&my_size);); (133, OpenMP, #pragma omp parallel for reduction(+:error)); (72, MPI, MPI_Bcast(&iterations,1, MPI_INT, root, MPI_COMM_WORLD);); (143, OpenMP, #pragma omp parallel for reduction(+:error) private(j,iter))", "label": "(103, OpenMP, printf(\"Number of threads    = %d\\n\", omp_get_max_threads());); (25, MPI, MPI_Comm_size(MPI_COMM_WORLD,&Num_procs);); (107, OpenMP, #pragma omp parallel for simd); (147, MPI, MPI_Finalize();); (76, MPI, MPI_Bcast(&length,        1, MPI_LONG, root, MPI_COMM_WORLD);); (22, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&my_ID);); (77, MPI, MPI_Bcast(&nthread_input, 1, MPI_INT,  root, MPI_COMM_WORLD);); (78, OpenMP, omp_set_num_threads(nthread_input);); (17, MPI, MPI_Init(&argc,&argv);); (73, MPI, MPI_Bcast(&iterations,    1, MPI_INT,  root, MPI_COMM_WORLD);); (114, MPI, MPI_Barrier(MPI_COMM_WORLD);); (70, MPI, MPI_Bcast(&offset,        1, MPI_LONG, root, MPI_COMM_WORLD);)"}
{"predict": "(28, CUDA, CHECK(cudaMalloc(&d_y, size));); (34, CUDA, CHECK(cudaEventCreate(&stop));); (39, CUDA, CHECK(cudaEventRecord(stop));); (27, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyDefault));); (72, CUDA, CHECK(cudaFree(d_y));); (33, CUDA, CHECK(cudaEventSynchronize(stop));); (33, CUDA, CHECK(cudaEventElapsedTime(&curr_time, start, stop));); (78, CUDA, CHECK(cudaEventDestroy(start));); (19, CUDA, CHECK(cudaEventCreate(&start));); (23, CUDA, CHECK(cudaGetLastError());); (39, CUDA, CHECK(cudaMemcpy(h_y, d_y, size, cudaMemcpyDefault));); (17, CUDA, CHECK(cudaMemset(d_y, 0, size));); (65, CUDA, CHECK(cudaEventDestroy(stop));); (18, CUDA, CHECK(cudaEventRecord(start));); (8, CUDA, CHECK(cudaMalloc(&d_x, size));); (30, CUDA, CHECK(cudaEventElapsedTime(&elap_time, start, stop));); (18, CUDA, CHECK(cudaMemcpy(h_y, d_y, size, cudaMemcpyDefault));); (68, CUDA, CHECK(cudaFree(d_x));)", "label": "(6, CUDA, CHECK(cudaMemcpyFromSymbol(&block_size, BLOCK_DIM, sizeof(real)));); (34, CUDA, CHECK(cudaEventCreate(&stop));); (43, CUDA, CHECK(cudaEventRecord(stop));); (78, CUDA, CHECK(cudaFree(d_y));); (33, CUDA, cudaEventQuery(start);); (41, CUDA, CHECK(cudaEventSynchronize(stop));); (41, CUDA, CHECK(cudaEventElapsedTime(&curr_time, start, stop));); (34, CUDA, CHECK(cudaMemcpy(h_y, d_y, size, cudaMemcpyDefault));); (28, CUDA, CHECK(cudaEventRecord(start));); (20, CUDA, CHECK(cudaMalloc(&d_x, M));); (70, CUDA, CHECK(cudaFree(d_x));); (20, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyDefault));); (30, CUDA, CHECK(cudaGetLastError());); (21, CUDA, CHECK(cudaEventCreate(&start));); (16, CUDA, CHECK(cudaMalloc(&d_y, size));)"}
{"predict": "(28, MPI, MPI_Type_get_extent(fd->filetype, &lb, &filetype_extent);); (27, MPI, MPI_Type_size_x(fd->filetype, &filetype_size);)", "label": "(28, MPI, MPI_Type_get_extent(fd->filetype, &lb, &filetype_extent);); (28, MPI, MPI_Type_size_x(fd->filetype, &filetype_size);)"}
{"predict": "(60, CUDA, CHECK_ERROR(cudaFree(device_params));); (12, CUDA, CHECK_ERROR(cudaMalloc((void **)&device_signals, complex_size_bytes));); (50, CUDA, CHECK_ERROR(cudaMemcpy(output_signals, device_signals, complex_size_bytes, cudaMemcpyDeviceToHost));); (56, CUDA, CHECK_ERROR(cudaFree(device_signals));); (12, CUDA, CHECK_ERROR(cudaMemcpy(device_signals, input_signals, complex_size_bytes, cudaMemcpyHostToDevice));); (20, CUDA, CHECK_ERROR(cudaMalloc((void **)&device_params, sizeof(cb_params)));); (20, CUDA, CHECK_ERROR(cudaMemcpy(device_params, &host_params, sizeof(cb_params), cudaMemcpyHostToDevice));); (34, CUDA, CHECK_ERROR(cudaMemcpyFromSymbol(&host_callback_ptr, device_callback_ptr, sizeof(host_callback_ptr)));)", "label": "(60, CUDA, CHECK_ERROR(cudaFree(device_params));); (12, CUDA, CHECK_ERROR(cudaMalloc((void **)&device_signals, complex_size_bytes));); (50, CUDA, CHECK_ERROR(cudaMemcpy(output_signals, device_signals, complex_size_bytes, cudaMemcpyDeviceToHost));); (56, CUDA, CHECK_ERROR(cudaFree(device_signals));); (34, CUDA, CHECK_ERROR(cudaMemcpyFromSymbol(&host_callback_ptr, device_callback_ptr, sizeof(host_callback_ptr)));); (9, CUDA, CHECK_ERROR(cudaMemcpy(device_signals, input_signals, complex_size_bytes, cudaMemcpyHostToDevice));); (17, CUDA, CHECK_ERROR(cudaMalloc((void **)&device_params, sizeof(cb_params)));); (17, CUDA, CHECK_ERROR(cudaMemcpy(device_params, &host_params, sizeof(cb_params), cudaMemcpyHostToDevice));)"}
{"predict": "(25, CUDA, cudaDeviceSynchronize();)", "label": "(25, CUDA, cudaDeviceSynchronize();)"}
{"predict": "(16, CUDA, checkCudaRuntime(cudaMemcpyAsync(image_device, image_host, size_image, cudaMemcpyHostToDevice, stream));)", "label": "(15, CUDA, checkCudaRuntime(cudaMemcpyAsync(image_device, image_host, size_image, cudaMemcpyHostToDevice, stream));)"}
{"predict": "(36, CUDA, __shared__ float sub_results[x128::size][x128::size][warp_c];)", "label": "(35, CUDA, __shared__ float sub_results[x128::size][32][bdy];)"}
{"predict": "(94, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (8, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProps, devID));); (91, CUDA, getLastCudaError(\"Kernel execution failed\");); (50, CUDA, checkCudaErrors(cudaMemcpy3D(&myparms));); (133, CUDA, checkCudaErrors(cudaFreeArray(cu_3darray));); (77, CUDA, checkCudaErrors(cudaCreateTextureObject(&tex, &texRes, &texDescr, NULL));); (43, CUDA, checkCudaErrors(cudaMalloc((void **)&d_data, size));); (127, CUDA, checkCudaErrors(cudaFree(d_data));); (-2, CUDA, int devID = findCudaDevice(argc, (const char **)argv);); (44, CUDA, checkCudaErrors(cudaMemcpy(d_data, h_data, size, cudaMemcpyHostToDevice));); (58, CUDA, checkCudaErrors(cudaMemcpy3D(&myparms.dstPos, cu_3darray, 0, 0, 0));); (58, CUDA, checkCudaErrors(cudaMemcpy3D(&myparms.srcPos, cu_3darray, 0, 0, 0));); (58, CUDA, checkCudaErrors(cudaMemcpy3D(&myparms.srcPtr, cu_3darray, 0, 0, 0));); (133, CUDA, checkCudaErrors(cudaDestroyTextureObject(tex));); (91, CUDA, checkCudaErrors(cudaMemcpy(h_odata, d_data, size, cudaMemcpyDeviceToHost));); (42, CUDA, checkCudaErrors(cudaMemcpy3D(&myparms.dstPtr, cu_3darray, 0, 0, 0));); (40, CUDA, checkCudaErrors(cudaMemcpy3D(&myparms.srcPtr, cu_3darray, 0, 0, 0));)", "label": "(59, CUDA, myparms.dstPos = make_cudaPos(0, 0, 0);); (9, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProps, devID));); (56, CUDA, myparms.srcPos = make_cudaPos(0, 0, 0);); (119, CUDA, checkCudaErrors(cudaMemcpy(h_odata, d_data, size, cudaMemcpyDeviceToHost));); (94, CUDA, getLastCudaError(\"warmup Kernel execution failed\");); (60, CUDA, checkCudaErrors(cudaMemcpy3D(&myparms));); (57, CUDA, myparms.extent = make_cudaExtent(width, width, num_faces);); (137, CUDA, checkCudaErrors(cudaFreeArray(cu_3darray));); (76, CUDA, checkCudaErrors(cudaCreateTextureObject(&tex, &texRes, &texDescr, NULL));); (91, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (36, CUDA, checkCudaErrors(cudaMalloc((void **)&d_data, size));); (99, CUDA, getLastCudaError(\"Kernel execution failed\");); (131, CUDA, checkCudaErrors(cudaFree(d_data));); (-10, CUDA, int devID = findCudaDevice(argc, (const char **)argv);); (128, CUDA, checkCudaErrors(cudaDestroyTextureObject(tex));)"}
{"predict": "(80, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (80, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)", "label": "(80, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (80, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)"}
{"predict": "(50, MPI, MPI_Waitall(WINDOW, req, status);); (22, MPI, start = MPI_Wtime();); (51, MPI, end = MPI_Wtime();)", "label": "(48, MPI, MPI_Waitall(WINDOW, req, status);); (23, MPI, start = MPI_Wtime();); (50, MPI, end = MPI_Wtime();)"}
{"predict": "(8, SYCL, MPI_Bcast(shrd_x, 1, x);)", "label": "(8, SYCL, return hipsycl::libkernel::sscp::wg_broadcast(sender, x, shrd_x);)"}
{"predict": "(62, OpenMP, #pragma omp distribute parallel for dist_schedule(static)); (18, OpenMP, #pragma omp distribute parallel for); (102, OpenMP, #pragma omp distribute parallel for dist_schedule(static, ch)); (134, OpenMP, #pragma omp distribute parallel for schedule(static)); (165, OpenMP, #pragma omp distribute parallel for schedule(static, ch)); (11, OpenMP, #pragma omp target); (196, OpenMP, #pragma omp distribute parallel for schedule(dynamic)); (224, OpenMP, #pragma omp distribute parallel for schedule(dynamic, ch)); (9, OpenMP, #pragma omp teams)", "label": "(169, OpenMP, #pragma omp distribute parallel for schedule(static, ch)); (201, OpenMP, #pragma omp distribute parallel for schedule(dynamic)); (17, OpenMP, #pragma omp distribute parallel for); (59, OpenMP, #pragma omp distribute parallel for dist_schedule(static)); (227, OpenMP, #pragma omp distribute parallel for schedule(dynamic, ch)); (132, OpenMP, #pragma omp distribute parallel for schedule(static)); (10, OpenMP, #pragma omp target); (97, OpenMP, #pragma omp distribute parallel for dist_schedule(static, ch)); (9, OpenMP, #pragma omp teams)"}
{"predict": "(40, CUDA, checkCudaErrors(cudaGetDeviceCount(&deviceCount));); (106, CUDA, checkCudaErrors(cudaEventCreate(&profileStart));); (106, CUDA, checkCudaErrors(cudaEventCreate(&profileEnd));); (41, CUDA, targetDevice = findCudaDevice(argc, (const char **)argv);); (66, CUDA, checkCudaErrors(cudaFuncGetAttributes(&funcAttrib, FiniteDifferencesKernel));); (116, CUDA, checkCudaErrors(cudaEventEnqueue(profileStart, bufferIn, 0, nullptr, nullptr));); (139, CUDA, checkCudaErrors(cudaEventEnqueue(profileEnd, bufferOut, 0, nullptr, nullptr));); (151, CUDA, checkCudaErrors(cudaEventSynchronize(profileEnd));); (120, CUDA, checkCudaErrors(cudaEventRecord(profileStart));); (147, CUDA, checkCudaErrors(cudaEventRecord(profileEnd));); (151, CUDA, checkCudaErrors(cudaEventDestroy(profileEnd));)", "label": "(109, CUDA, checkCudaErrors(cudaEventCreate(&profileStart));); (43, CUDA, checkCudaErrors(cudaSetDevice(targetDevice));); (40, CUDA, targetDevice = findCudaDevice(argc, (const char **)argv);); (193, CUDA, checkCudaErrors(cudaEventDestroy(profileEnd));); (140, CUDA, checkCudaErrors(cudaEventRecord(profileEnd, 0));); (116, CUDA, checkCudaErrors(cudaEventRecord(profileStart, 0));); (186, CUDA, checkCudaErrors(cudaEventDestroy(profileStart));); (61, CUDA, checkCudaErrors(cudaFuncGetAttributes(&funcAttrib, FiniteDifferencesKernel));); (31, CUDA, checkCudaErrors(cudaGetDeviceCount(&deviceCount));); (101, CUDA, checkCudaErrors(cudaEventCreate(&profileEnd));); (176, CUDA, checkCudaErrors(cudaFree(bufferOut));); (171, CUDA, checkCudaErrors(cudaFree(bufferIn));)"}
{"predict": "(38, CUDA, cudaFree(res_dev);); (31, CUDA, cudaMalloc(&res_dev, sizeof(float));); (33, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)", "label": "(38, CUDA, cudaFree(res_dev);); (31, CUDA, cudaMalloc(&res_dev, sizeof(float));); (33, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(19, CUDA, checkCudaErrors(cudaGetLastError());); (17, CUDA, checkCudaErrors(cudaStreamSynchronize(stream));)", "label": "(18, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (18, CUDA, checkCudaErrors(cudaGetLastError());)"}
{"predict": "(8, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (9, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;)", "label": "(8, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (9, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;)"}
{"predict": "(22, OpenMP, #pragma omp task); (29, OpenMP, #pragma omp taskwait); (9, OpenMP, #pragma omp parallel); (9, OpenMP, #pragma omp single)", "label": "(29, OpenMP, #pragma omp taskwait); (22, OpenMP, #pragma omp task); (9, OpenMP, #pragma omp parallel); (10, OpenMP, #pragma omp single)"}
{"predict": "(2, SYCL, sycl::queue q = get_queue();); (3, SYCL, int* data = sycl::malloc_shared<int>(1024, q);); (0, SYCL, sycl::table table;)", "label": "(2, SYCL, sycl::queue q = get_queue();); (3, SYCL, int* data = sycl::malloc_device<int>(1024, q);)"}
{"predict": "(49, MPI, MPI_Allreduce(&index, &scr_groupdescs, 1, MPI_INT, MPI_ORDER, comm);); (50, MPI, MPI_Allreduce(&count, &all_valid, 1, MPI_INT, MPI_LAND, comm);); (6, MPI, MPI_Comm_rank(comm, &rank);); (110, MPI, MPI_Allreduce(&all_valid, &alltrue, 1, MPI_INT, MPI_LAND, comm);)", "label": "(7, MPI, MPI_Comm_rank(comm, &rank);); (49, MPI, MPI_Bcast(&num_groups, 1, MPI_INT, 0, comm);)"}
{"predict": "(50, OpenMP, #pragma omp for); (3, OpenMP, #pragma omp section)", "label": "(4, OpenMP, #pragma omp sections); (49, OpenMP, #pragma omp sections nowait); (28, OpenMP, #pragma omp section)"}
{"predict": "(36, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (38, CUDA, CUDA_CHECK(cudaFree(d_data));)", "label": "(36, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (38, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(13, CUDA, cudaFree(ctx);)", "label": "(5, CUDA, c.CUDAConfig().device_id = 0;)"}
{"predict": "(65, MPI, MPI_Gatherv(sendbuf, bytes, MPI_BYTE, recvbuf, counts, disps, MPI_BYTE, 0, MPI_COMM_WORLD);); (13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (13, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (36, MPI, MPI_Gather(&bytes, 1, MPI_INT, counts, 1, MPI_INT, 0, MPI_COMM_WORLD);)", "label": "(65, MPI, MPI_Gatherv(sendbuf, bytes, MPI_BYTE, recvbuf, counts, disps, MPI_BYTE, 0, MPI_COMM_WORLD);); (13, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (36, MPI, MPI_Gather(&bytes, 1, MPI_INT, counts, 1, MPI_INT, 0, MPI_COMM_WORLD);)"}
{"predict": "(31, MPI, MPI_Comm_size(newcomm, &new_size);); (29, MPI, MPI_Comm_remote_size(newcomm, &new_rsize);); (18, MPI, MPI_Comm_split(intercomm, color, key, &newcomm);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (18, MPI, MPI_Comm_free(&newcomm);); (18, MPI, MPI_Comm_free(&intercomm);); (78, MPI, MPI_Comm_split(intercomm, 0, 0, &newcomm);); (18, MPI, MPI_Comm_rank(newcomm, &key);); (3, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (27, MPI, MPI_Comm_set_name(newcomm, (char *) \"Single rank in each color\");); (17, MPI, MPI_Comm_rank(intercomm, &key);); (20, MPI, MPI_Comm_size(intercomm, &orig_size);); (20, MPI, MPI_Comm_remote_size(intercomm, &orig_rsize);)", "label": "(32, MPI, MPI_Comm_test_inter(intercomm, &flag);); (23, MPI, MPI_Comm_rank(intercomm, &key);); (90, MPI, MPI_Comm_free(&intercomm);); (38, MPI, MPI_Comm_size(intercomm, &orig_size);); (38, MPI, MPI_Comm_size(newcomm, &new_size);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (33, MPI, MPI_Comm_remote_size(intercomm, &orig_rsize);); (19, MPI, MPI_Comm_split(intercomm, color, key, &newcomm);); (3, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (31, MPI, MPI_Comm_remote_size(newcomm, &new_rsize);)"}
{"predict": "(6, CUDA, extern __shared__ float shared[];)", "label": "(6, CUDA, extern __shared__ float shared[];)"}
{"predict": "(34, CUDA, cudaFree(control_d);); (31, CUDA, cudaFree(state_der_d);); (29, CUDA, cudaDeviceSynchronize();); (26, CUDA, HANDLE_ERROR(cudaMemcpy(state.data(), state_d, sizeof(float) * S_DIM * state.size(), cudaMemcpyDeviceToHost));); (21, CUDA, CudaCheckError();); (10, CUDA, HANDLE_ERROR(cudaMemcpy(control_d, control.data(), sizeof(float) * C_DIM * control.size(), cudaMemcpyHostToDevice));); (22, CUDA, HANDLE_ERROR(cudaMemcpy(control.data(), control_d, sizeof(float) * C_DIM * control.size(), cudaMemcpyDeviceToHost));); (23, CUDA, HANDLE_ERROR(cudaMemcpy(state_der.data(), state_der_d, sizeof(float) * S_DIM * state_der.size(), cudaMemcpyDeviceToHost));); (4, CUDA, HANDLE_ERROR(cudaMemcpy(state_d, state.data(), sizeof(float) * S_DIM * state.size(), cudaMemcpyHostToDevice));); (28, CUDA, cudaFree(state_d);); (5, CUDA, HANDLE_ERROR(cudaMemcpy(state_der_d, state_der.data(), sizeof(float) * S_DIM * state_der.size(), cudaMemcpyHostToDevice));); (5, CUDA, HANDLE_ERROR(cudaMemcpy(control_d, control.data(), sizeof(float) * C_DIM * control.size(), cudaMemcpyHostToDevice));)", "label": "(34, CUDA, cudaFree(control_d);); (32, CUDA, cudaFree(state_der_d);); (22, CUDA, CudaCheckError();); (29, CUDA, cudaFree(state_d);)"}
{"predict": "(6, CUDA, cudaMemcpy(data,datag+inidata,sizeof(unsigned)*ndata,cudaMemcpyDeviceToHost);); (6, CUDA, fcuda::Check_CudaErroorFun(\"DgReduSumUint\");)", "label": "(7, CUDA, fcuda::Check_CudaErroorFun(\"DgReduSumUint\");); (5, CUDA, cudaMemcpy(data,datag+inidata,sizeof(unsigned)*ndata,cudaMemcpyDeviceToHost);)"}
{"predict": "(37, TBB, oneapi::tbb::spin_mutex::scoped_lock lock (MyMutex);)", "label": "(37, TBB, tbb::spin_mutex::scoped_lock lock (MyMutex);)"}
{"predict": "(24, CUDA, extern __shared__ float sram[];)", "label": "(24, CUDA, extern __shared__ float sram[];)"}
{"predict": "(14, OpenMP, #pragma omp parallel); (14, OpenMP, #pragma omp task if (task: false))", "label": "(14, OpenMP, #pragma omp parallel); (14, OpenMP, #pragma omp task if (task: false))"}
{"predict": "(30, OpenMP, #pragma omp target teams distribute parallel for collapse (S1)); (53, OpenMP, #pragma omp target teams distribute parallel for collapse (2)); (17, OpenMP, #pragma omp target teams distribute parallel for collapse (2+2))); (5, OpenMP, #pragma omp target teams distribute parallel for collapse ()); (20, OpenMP, #pragma omp target teams distribute parallel for collapse (foobool(1) > 0? 1 : 2)); (-3, OpenMP, #pragma omp target teams distribute parallel for collapse); (44, OpenMP, #pragma omp target teams distribute parallel for collapse(collapse(tmain<int, char, -1, -2>(argc, argv)); (7, OpenMP, #pragma omp target teams distribute parallel for collapse (4); (0, OpenMP, #pragma omp target teams distribute parallel for collapse (); (23, OpenMP, #pragma omp target teams distribute parallel for collapse (foobool(argc)), collapse (true), collapse (-5)); (15, OpenMP, #pragma omp target teams distribute parallel for collapse (foobool(argc) > 0? 1 : 2))", "label": "(47, OpenMP, #pragma omp target teams distribute parallel for collapse(collapse(tmain<int, char, -1, -2>(argc, argv)); (4, OpenMP, #pragma omp target teams distribute parallel for collapse (); (7, OpenMP, #pragma omp target teams distribute parallel for collapse ()); (18, OpenMP, #pragma omp target teams distribute parallel for collapse (foobool(1) > 0? 1 : 2)); (13, OpenMP, #pragma omp target teams distribute parallel for collapse (2+2))); (23, OpenMP, #pragma omp target teams distribute parallel for collapse (foobool(argc)), collapse (true), collapse (-5)); (7, OpenMP, #pragma omp target teams distribute parallel for collapse (4); (25, OpenMP, #pragma omp target teams distribute parallel for collapse (S1)); (-7, OpenMP, #pragma omp target teams distribute parallel for collapse); (41, OpenMP, #pragma omp target teams distribute parallel for collapse (2))"}
{"predict": "(32, CUDA, checkCudaErrors(cudaFree(resourceList->outputBufferGPU));); (27, CUDA, checkCudaErrors(cudaFree(resourceList->inputBufferGPU));); (44, CUDA, checkCudaErrors(cudaStreamDestroy(resourceList->stream));)", "label": "(32, CUDA, cudaFree(resourceList->outputBufferGPU);); (27, CUDA, cudaFree(resourceList->inputBufferGPU);); (44, CUDA, cudaStreamDestroy(resourceList->stream);)"}
{"predict": "(69, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (2, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (70, CUDA, EXPECT_EQ(cudaSuccess, cudaMemcpy(test.data(), output->basePtr(), outBufSize, cudaMemcpyDeviceToHost));); (70, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamDestroy(stream));); (62, CUDA, EXPECT_EQ(cudaSuccess, cudaMemset(input->basePtr(), 0xFA, inSampleStride * inAccess->numSamples()));); (62, CUDA, EXPECT_EQ(cudaSuccess, cudaMemset(output->basePtr(), 0xFA, outSampleStride * outAccess->numSamples()));)", "label": "(77, CUDA, EXPECT_EQ(cudaSuccess, cudaMemcpy(test.data(), output->basePtr(), outBufSize, cudaMemcpyDeviceToHost));); (2, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (64, CUDA, EXPECT_EQ(cudaSuccess, cudaMemset(input->basePtr(), 0xFF, inSampleStride * inAccess->numSamples()));); (72, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (72, CUDA, EXPECT_EQ(cudaSuccess, cudaMemcpy(testIn.data(), input->basePtr(), inBufSize, cudaMemcpyDeviceToHost));); (62, CUDA, EXPECT_EQ(cudaSuccess, cudaMemset(output->basePtr(), 0xFF, outSampleStride * outAccess->numSamples()));); (78, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamDestroy(stream));)"}
{"predict": "(25, OpenMP, #pragma omp parallel num_threads(4)); (21, OpenMP, check2[omp_get_thread_num()] += omp_get_num_threads();); (8, OpenMP, #pragma omp target map(check1[:], check2[:], check3[:], threadLimit)); (15, OpenMP, #pragma omp parallel); (25, OpenMP, check3[omp_get_thread_num()] += omp_get_num_threads();); (15, OpenMP, check1[omp_get_thread_num()] += omp_get_num_threads();); (8, OpenMP, threadLimit = omp_get_max_threads();); (17, OpenMP, #pragma omp parallel num_threads(2)); (21, OpenMP, #pragma omp parallel num_threads(5))", "label": "(28, OpenMP, #pragma omp parallel num_threads(NumThreads3)); (29, OpenMP, check3[omp_get_thread_num()] += omp_get_num_threads();); (15, OpenMP, #pragma omp parallel); (10, OpenMP, threadLimit = omp_get_thread_limit();); (6, OpenMP, #pragma omp target teams num_teams(1) thread_limit(ThreadLimit)); (20, OpenMP, check2[omp_get_thread_num()] += omp_get_num_threads();); (16, OpenMP, omp_set_num_threads(NumThreads2);); (12, OpenMP, check1[omp_get_thread_num()] += omp_get_num_threads();)"}
{"predict": "(13, CUDA, getLastCudaError(\"mergeElementaryIntervalsKernel<1> failed\\n\");); (17, CUDA, getLastCudaError(\"mergeElementaryIntervalsKernel<0> failed\\n\");)", "label": "(13, CUDA, getLastCudaError(\"mergeElementaryIntervalsKernel<1> failed\\n\");); (17, CUDA, getLastCudaError(\"mergeElementaryIntervalsKernel<0> failed\\n\");)"}
{"predict": "(42, CUDA, cudaFree(B);); (109, CUDA, result = cudaMemcpy(host_cutlass.data(), C_cutlass, sizeof_C, cudaMemcpyDeviceToHost);); (59, CUDA, cudaFree(C_reference);); (52, CUDA, result = cudaMemcpy(C_reference, C_cutlass, sizeof_C, cudaMemcpyDeviceToDevice);); (47, CUDA, cudaFree(C_cutlass);); (119, CUDA, result = cudaMemcpy(host_reference.data(), C_reference, sizeof_C, cudaMemcpyDeviceToHost);); (26, CUDA, cudaFree(A);)", "label": "(42, CUDA, cudaFree(B);); (109, CUDA, result = cudaMemcpy(host_cutlass.data(), C_cutlass, sizeof_C, cudaMemcpyDeviceToHost);); (59, CUDA, cudaFree(C_reference);); (52, CUDA, result = cudaMemcpy(C_reference, C_cutlass, sizeof_C, cudaMemcpyDeviceToDevice);); (47, CUDA, cudaFree(C_cutlass);); (119, CUDA, result = cudaMemcpy(host_reference.data(), C_reference, sizeof_C, cudaMemcpyDeviceToHost);); (28, CUDA, cudaFree(A);)"}
{"predict": "(40, OpenMP, #pragma omp teams reduction(+:sum)); (37, OpenMP, #pragma omp target map(tofrom: b, sum, t) map(alloc: a)); (40, OpenMP, #pragma omp distribute parallel for reduction(+:sum)); (42, OpenMP, t[i] = omp_get_team_num();)", "label": "(40, OpenMP, #pragma omp teams reduction(+:sum)); (37, OpenMP, #pragma omp target map(tofrom: b, sum, t) map(alloc: a)); (40, OpenMP, #pragma omp distribute parallel for reduction(+:sum)); (42, OpenMP, t[i] = omp_get_team_num();)"}
{"predict": "(14, CUDA, fprintf(stderr, \"CUDA kernel failed : %s\\n\", cudaGetErrorString(err));); (11, CUDA, err = cudaGetLastError();)", "label": "(14, CUDA, fprintf(stderr, \"CUDA kernel failed : %s\\n\", cudaGetErrorString(err));); (11, CUDA, err = cudaGetLastError();)"}
{"predict": "(4, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (5, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;)", "label": "(4, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (5, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;)"}
{"predict": "(3, CUDA, CudaCheckError();)", "label": "(4, CUDA, CudaCheckError();)"}
{"predict": "(82, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &iam );); (244, MPI, MPI_Finalize();); (24, MPI, MPI_Init_thread( &argc, &argv, MPI_THREAD_MULTIPLE, &omp_mpi_level);); (25, MPI, MPI_Comm_size( MPI_COMM_WORLD, &nprocs );)", "label": "(82, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &iam );); (257, MPI, MPI_Finalize();); (24, MPI, MPI_Init_thread( &argc, &argv, MPI_THREAD_MULTIPLE, &omp_mpi_level);); (25, MPI, MPI_Comm_size( MPI_COMM_WORLD, &nprocs );)"}
{"predict": "(343, CUDA, __shared__ int cur_send_num;); (341, CUDA, __shared__ int recv_num, finish_num;)", "label": "(343, CUDA, __shared__ int cur_send_num;); (341, CUDA, __shared__ int recv_num, finish_num;)"}
{"predict": "(4, CUDA, __shared__ float sdata[THREAD_PER_BLOCK];)", "label": "(4, CUDA, __shared__ float sdata[THREAD_PER_BLOCK];)"}
{"predict": "(79, MPI, MPI_Bcast(footer, 6, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)", "label": "(10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (76, MPI, MPI_Bcast(&footer_flag, 1, MPI_INT, 0, MPI_COMM_WORLD);); (76, MPI, MPI_Bcast(&footer, 8, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(2, CUDA, __shared__ float smem[32];)", "label": "(2, CUDA, __shared__ float smem[1024];)"}
{"predict": "(38, CUDA, CUDASimulation cudaSimulation(model);)", "label": "(38, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(28, CUDA, checkCudaErrors(cudaSetDevice(gpuID));)", "label": "(29, CUDA, checkCudaErrors(cudaSetDevice(gpuID));)"}
{"predict": "(121, OpenCL, err = clEnqueueUnmapMemObject(queue, buffer, ptr, 0, nullptr, nullptr);); (73, OpenCL, err = clBuildProgram(program, 1, &device_id, NULL, NULL, NULL);); (99, OpenCL, kernel = clCreateKernel(program, \"hello\", &err);); (137, OpenCL, err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, gws, lws, 0, NULL, NULL);); (97, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(cl_mem), &buffer);); (109, OpenCL, auto ptr = clEnqueueMapBuffer(queue, buffer, CL_TRUE, CL_MAP_READ, 0, bufferSize, 0, NULL, NULL, &err);); (33, OpenCL, context = clCreateContext(NULL, 1, &device_id, NULL, NULL, &err);); (39, OpenCL, queue = clCreateCommandQueue(context, device_id, 0, &err);); (104, OpenCL, err = clEnqueueSetKernelArg(kernel, 1, sizeof(cl_uint), &bufferSize);); (143, OpenCL, err = clEnqueueMapBuffer(queue, buffer, CL_TRUE, CL_MAP_READ, 0, bufferSize, 0, NULL, NULL, &ptr);); (59, OpenCL, err = clCreateProgramWithSource(context, 1, &source, &strings, &err);); (99, OpenCL, auto ptr = clEnqueuePointerMap(queue, buffer, CL_TRUE, CL_MAP_READ, 0, bufferSize, 0, NULL, NULL, &err);); (32, OpenCL, err = clBuildContext(context, 1, &device_id, NULL, NULL, NULL);)", "label": "(69, OpenCL, program = clCreateProgramWithSource(context, 1, &strings, 0, &err);); (129, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(cl_mem), &buffer);); (190, OpenCL, clReleaseKernel(kernel);); (191, OpenCL, clReleaseCommandQueue(queue);); (189, OpenCL, clReleaseProgram(program);); (139, OpenCL, err = clEnqueueNDRangeKernel(queue, kernel, dimension, offset, gws, lws, 0, 0, nullptr);); (189, OpenCL, clReleaseContext(context);); (101, OpenCL, buffer = clCreateBuffer(context, flags, bufferSize, nullptr, &err);); (115, OpenCL, err = clEnqueueUnmapMemObject(queue, buffer, ptr, 0, nullptr, nullptr);); (106, OpenCL, void *ptr = clEnqueueMapBuffer(queue, buffer, CL_TRUE, CL_MAP_WRITE, 0, bufferSize, 0, nullptr, nullptr, &err);); (38, OpenCL, queue = clCreateCommandQueue(context, device_id, 0, &err);); (140, OpenCL, void *ptr = clEnqueueMapBuffer(queue, buffer, CL_TRUE, CL_MAP_READ, 0, bufferSize, 0, nullptr, nullptr, &err);); (64, OpenCL, err = clBuildProgram(program, 1, &device_id, nullptr, nullptr, nullptr);); (178, OpenCL, clReleaseMemObject(buffer);); (28, OpenCL, context = clCreateContext(0, 1, &device_id, NULL, NULL, &err);); (84, OpenCL, kernel = clCreateKernel(program, \"hello\", &err);)"}
{"predict": "(6, MPI, MPI_Comm_size(comm, &nproc);); (4, MPI, MPI_Comm_rank(comm, &rank);)", "label": "(6, MPI, MPI_Comm_size(comm, &nproc);); (4, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(10, TBB, auto ts_0 = tbb::tick_count::now();); (1, TBB, c.set_core_type(tbb::info::core_types().back());); (2, TBB, c.set_numa_id(tbb::info::numa_nodes()[0]);); (7, TBB, tbb::task_arena a(c, 1);); (9, TBB, auto ts_1 = tbb::tick_count::now();); (1, TBB, c.set_max_concurrency(std::min(8, tbb::info::default_concurrency(c)));); (13, TBB, double t2 = tbb::tick_count::now();); (14, TBB, double t3 = tbb::tick_count::now();); (15, TBB, double t4 = tbb::tick_count::now();); (15, TBB, double t5 = tbb::tick_count::now();); (-2, TBB, tbb::task_scheduler_init init(1);); (8, TBB, double t1 = tbb::tick_count::now();); (-6, TBB, c.set_core_type(tbb::info::core_types().front());)", "label": "(27, TBB, tbb::tick_count t1 = tbb::tick_count::now();); (23, TBB, reducePiExample<tbb::auto_partitioner>(num_intervals, gs);); (6, TBB, c.set_max_concurrency(std::min(8, tbb::info::default_concurrency(c)));); (29, TBB, double v3 = deterministicReducePiExample<tbb::simple_partitioner>(num_intervals, gs);); (27, TBB, tbb::tick_count t3 = tbb::tick_count::now();); (-1, TBB, tbb::task_arena::constraints c;); (11, TBB, tbb::tick_count ts_1 = tbb::tick_count::now();); (18, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (27, TBB, tbb::tick_count t5 = tbb::tick_count::now();); (25, TBB, double v4 = deterministicReducePiExample<tbb::static_partitioner>(num_intervals, gs);); (5, TBB, tbb::tick_count ts_0 = tbb::tick_count::now();); (-2, TBB, tbb::task_arena a(c);); (-6, TBB, c.set_core_type(tbb::info::core_types().back());); (20, TBB, tbb::tick_count t4 = tbb::tick_count::now();); (14, TBB, double v1 = reducePiExample<tbb::simple_partitioner>(num_intervals, gs);); (15, TBB, double v2 = reducePiExample<tbb::static_partitioner>(num_intervals, gs);); (-11, TBB, c.set_numa_id(tbb::info::numa_nodes()[0]);); (12, TBB, tbb::tick_count t2 = tbb::tick_count::now();); (8, TBB, double v0 = reducePiExample<tbb::auto_partitioner>(num_intervals, gs);)"}
{"predict": "(93, CUDA, cuda_safe_call(cudaStreamDestroy(stream));); (7, CUDA, cuda_safe_call(cudaMalloc(&d_ptrB, N * sizeof(double)));); (5, CUDA, cuda_safe_call(cudaMalloc(&d_ptrA, N * sizeof(double)));); (8, CUDA, cuda_safe_call(cudaStreamCreate(&stream));); (16, CUDA, cuda_safe_call(cudaStreamSynchronize(stream));); (89, CUDA, cuda_safe_call(cudaFree(d_ptrB));); (87, CUDA, cuda_safe_call(cudaFree(d_ptrA));); (82, CUDA, cuda_safe_call(cudaStreamDestroy(stream));)", "label": "(18, CUDA, cuda_safe_call(cudaStreamSynchronize(stream));); (7, CUDA, cuda_safe_call(cudaMalloc(&d_ptrB, N * sizeof(double)));); (9, CUDA, cuda_safe_call(cudaStreamCreate(&stream));); (4, CUDA, cuda_safe_call(cudaMalloc(&d_ptrA, N * sizeof(double)));)"}
{"predict": "(13, CUDA, __shared__ alignof(float) Bs[BLOCK_SIZE][BLOCK_SIZE];); (8, CUDA, __shared__ alignof(float) As[BLOCK_SIZE][BLOCK_SIZE];)", "label": "(13, CUDA, __shared__ alignas(alignof(float4)) float Bs[BLOCK_SIZE][BLOCK_SIZE];); (8, CUDA, __shared__ alignas(alignof(float4)) float As[BLOCK_SIZE][BLOCK_SIZE];)"}
{"predict": "(78, OpenMP, lis2mdl_offset_temp_comp_set(&mag_ctx, PROPERTY_ENABLE);)", "label": "(78, OpenMP, lis2mdl_offset_temp_comp_set(&mag_ctx, PROPERTY_ENABLE);)"}
{"predict": "(8, OpenMP, #pragma omp target device(i) map(from: num)); (1, OpenMP, int num_devices = omp_get_num_devices();); (2, OpenMP, #pragma omp target map(from: num_devices)); (6, OpenMP, num = omp_get_device_num();)", "label": "(2, OpenMP, const int num_devices = omp_get_num_devices();); (6, OpenMP, #pragma omp target map(from : num) device(i)); (6, OpenMP, num = omp_get_device_num();)"}
{"predict": "(55, OpenCL, clReleaseEvent(clEvent);)", "label": "(55, OpenCL, clReleaseEvent(clEvent);)"}
{"predict": "(16, CUDA, cudaError_t err = cudaGetDeviceProperties(&props, dev);); (55, CUDA, cudaError_t err = cudaDeviceEnablePeerAccess(odev, 0);); (43, CUDA, cudaDeviceCanAccessPeer(&access, dev1, dev2);); (-1, CUDA, cudaSetDevice(device);)", "label": "(47, CUDA, cudaDeviceCanAccessPeer(&access, dev1, dev2);); (53, CUDA, cudaSetDevice(dev);); (57, CUDA, auto err = cudaDeviceEnablePeerAccess(odev, 0);); (14, CUDA, auto err = cudaGetDeviceProperties(&props, dev);); (9, CUDA, INFO(\"failed to cudaSetDevice(%d)\\n\", dev);); (-2, CUDA, cudaGetDeviceCount(reinterpret_cast<int *>(&device));)"}
{"predict": "(20, TBB, tbb::critical_section cs;); (6, TBB, tbb::concurrent_set<vertex_rec> open_set;); (0, TBB, tbb::concurrent_distances<vertex_id> g_distance;); (0, TBB, tbb::concurrent_distances<vertex_id> f_distance;); (24, TBB, tbb::task_group sp_group;)", "label": "(9, TBB, oneapi::tbb::spin_mutex::scoped_lock l(locks[u]);); (20, TBB, oneapi::tbb::spin_mutex::scoped_lock l(locks[v]);)"}
{"predict": "(7, MPI, MPI_CHECK(MPI_Init(&env_world_size, &env_rank));); (15, MPI, MPI_CHECK(MPI_Bcast((void *)&nccl_id, sizeof(nccl_id), MPI_BYTE, 0, MPI_COMM_WORLD));); (21, MPI, MPI_CHECK(MPI_Finalize());)", "label": "(6, MPI, MPI_CHECK(MPI_Init(NULL, NULL));); (15, MPI, MPI_CHECK(MPI_Bcast((void *)&nccl_id, sizeof(nccl_id), MPI_BYTE, 0, MPI_COMM_WORLD));); (16, CUDA, cudaSetDevice(rank);)"}
{"predict": "(22, CUDA, __shared__ float v[_N_], r[_N_], k[_N_], gy[_N_];)", "label": "(22, CUDA, __shared__ float v[_N_], r[_N_], k[_N_], gy[_N_];)"}
{"predict": "(93, OpenMP, #pragma omp target teams distribute parallel for); (86, OpenMP, #pragma omp target parallel for)", "label": "(93, OpenMP, #pragma omp target teams distribute parallel for); (86, OpenMP, #pragma omp target parallel for)"}
{"predict": "(12, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (12, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (45, MPI, MPI_Put(&localbuf[i], 1, MPI_INT, trank, rank + i * wsize, 1, MPI_INT, win);); (45, MPI, MPI_Put(&localbuf[i], 1, MPI_INT, trank, rank + (i + NELM) * wsize, 1, MPI_INT, win);); (43, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, trank, 0, win);); (43, MPI, MPI_Win_unlock(trank, win);); (125, MPI, MPI_Get(&vals[i], 1, MPI_INT, trank, i, 1, MPI_INT, win);); (125, MPI, MPI_Put(&vals[i], 1, MPI_INT, trank, rank + i, 1, MPI_INT, win);); (40, MPI, MPI_Barrier(MPI_COMM_WORLD);); (125, MPI, MPI_Win_flush(trank, win);); (125, MPI, MPI_Win_unlock_all(win);); (5, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(12, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (208, MPI, MPI_Win_free(&win);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (46, MPI, MPI_Put(&localbuf[i], 1, MPI_INT, trank, rank + i * wsize, 1, MPI_INT, win);); (44, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, trank, 0, win);); (45, MPI, MPI_Put(&localbuf[i], 1, MPI_INT, trank, rank + (i + NELM) * wsize, 1, MPI_INT, win);); (45, MPI, MPI_Win_unlock(trank, win);); (127, MPI, MPI_Get(&vals[i], 1, MPI_INT, trank, i, 1, MPI_INT, win);); (183, MPI, MPI_Accumulate(&localbuf[i], 1, MPI_INT, trank, rank + i * wsize, 1, MPI_INT, MPI_SUM, win);); (37, MPI, MPI_Barrier(MPI_COMM_WORLD);); (6, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(14, CUDA, thrust::device_vector<Element> d_src = h_src;); (8, CUDA, thrust::host_vector<Element> h_src(kNumel);)", "label": "(14, CUDA, thrust::device_vector<Element> d_src = h_src;); (8, CUDA, thrust::host_vector<Element> h_src(kNumel);)"}
{"predict": "(51, CUDA, cudaFree(res_dev);); (44, CUDA, cudaMalloc(&res_dev, sizeof(float));); (46, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)", "label": "(51, CUDA, cudaFree(res_dev);); (44, CUDA, cudaMalloc(&res_dev, sizeof(float));); (46, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(79, OpenMP, #pragma\tomp\tparallel for private (lsum,thread_id1,rtemp,nn,lbstart,lbend,ik,usub,i,irow,frow,nsupr,pi,ii,jj,t1,t2,nnroot_send_tmp)); (142, OpenMP, #pragma omp simd); (136, OpenMP, #pragma omp parallel for private (lsum,thread_id1,rtemp,nn,lbstart,lbend,ik,usub,i,irow,frow,nsupr,pi,ii,jj,t1,t2,nnroot_send_tmp)); (77, OpenMP, #pragma\tomp\ttaskloop private (lsum,thread_id1,rtemp,nn,lbstart,lbend,ik,usub,i,irow,frow,nsupr,pi,ii,jj,t1,t2,nnroot_send_tmp) untied); (138, OpenMP, #pragma omp simd lastprivate (lsum,thread_id1,rtemp,nn,lbstart,lbend,ik,usub,i,irow,frow,nsupr,pi,ii,jj,t1,t2,nnroot_send_tmp)); (136, OpenMP, #pragma omp simd private (lsum,thread_id1,rtemp,nn,lbstart,lbend,ik,usub,i,irow,frow,nsupr,pi,ii,jj,t1,t2,nnroot_send_tmp)); (138, OpenMP, #pragma omp simd lastprivate (ii,jj,t1,t2,nnroot_send_tmp)); (129, OpenMP, #pragma omp parallel for private (lsum,thread_id1,rtemp,nn,lbstart,lbend,ik,usub,i,irow,frow,nsupr,pi,ii,jj,t1,t2,nnroot_send_tmp)); (135, OpenMP, #pragma omp parallel); (127, OpenMP, #pragma omp parallel for private (lsum,thread_id1,rtemp,nn,lbstart,lbend,ik,usub,i,irow,frow,nsupr,pi,ii,jj,t1,t2,nnroot_send_tmp)); (74, OpenMP, #pragma\tomp\ttaskloop private (lsum,thread_id1,rtemp,nn,lbstart,lbend,ik,usub,i,irow,frow,nsupr,pi", "label": "(82, OpenMP, #pragma\tomp\ttaskloop firstprivate (stat) private (thread_id1,Uinv,nn,lbstart,lbend,ub,temp,rtemp_loc,ik,lk1,gik,gikcol,usub,uval,lsub,lusup,iknsupc,il,i,irow,bmod_tmp,p,ii,jj,t1,t2,j,ikfrow,iklrow,dest,y,uptr,fnz,nsupr,nroot_send_tmp) untied nogroup); (126, OpenMP, #pragma omp simd); (146, OpenMP, #pragma omp atomic capture); (75, OpenMP, #pragma\tomp\tparallel for private (thread_id1,Uinv,nn,lbstart,lbend,ub,temp,rtemp_loc,ik,lk1,gik,gikcol,usub,uval,lsub,lusup,iknsupc,il,i,irow,bmod_tmp,p,ii,jj,t1,t2,j,ikfrow,iklrow,dest,y,uptr,fnz,nsupr))"}
{"predict": "(48, CUDA, CubDebugExit(cudaMemcpy(d_flags, h_flags, sizeof(unsigned char) * num_items, cudaMemcpyHostToDevice));); (46, CUDA, CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));)", "label": "(48, CUDA, CubDebugExit(cudaMemcpy(d_flags, h_flags, sizeof(unsigned char) * num_items, cudaMemcpyHostToDevice));); (46, CUDA, CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));)"}
{"predict": "(7, CUDA, __shared__ uint s_key[SHARED_SIZE_LIMIT];); (7, CUDA, __shared__ uint s_val[SHARED_SIZE_LIMIT];)", "label": "(6, CUDA, __shared__ uint s_key[SHARED_SIZE_LIMIT];); (6, CUDA, __shared__ uint s_val[SHARED_SIZE_LIMIT];)"}
{"predict": "(141, OpenMP, comp_add_script_line(\"Error: %s\\n\", comp_err_msg(141));); (139, OpenMP, comp_add_script_line(\"Error: %s\\n\", comp_err_msg(140));); (130, OpenMP, comp_add_script_line(\"Error: %s\\n\", comp_err_msg(131));); (143, OpenMP, comp_add_script_line(\"Error: %s\\n\", comp_err_msg(143));); (140, OpenMP, comp_add_script_line(\"Error: %s\\n\", comp_err_msg(142));); (127, OpenMP, comp_add_script_line(\"Error: %s\\n\", comp_err_msg(130));); (141, OpenMP, comp_add_script_line(\"Error: %s\\n\", comp_err_msg(141));); (138, OpenMP, comp_add_script_line(\"Error: %s\\n\", comp_err_msg(128));); (97, OpenMP, #pragma omp parallel for private(i, p, s, m, len, salt, keys, output1x, output2x, output3x, output4x, output5x, output6x, output7x, output8x, output9x, output10x))", "label": "(31, OpenMP, comp_add_script_line(\"Flag=MGF_SALT_AS_HEX_%s\\n\", tmp);); (19, OpenMP, comp_do_parse(0, nSyms-1);); (222, OpenMP, comp_add_script_line(\"Func=DynamicFunc__append_keys_pad20\\n\"); if (use_inp1) len_comp += 20; else len_comp2 += 20; }); (58, OpenMP, if (bNeedS) comp_add_script_line(\"Flag=MGF_SALTED\\n\");); (153, OpenMP, comp_add_script_line(\"Func=DynamicFunc__LargeHash_OUTMode_raw\\n\");); (18, OpenMP, comp_add_script_line(\"Expression=dynamic=%s\\n\", p->pExpr);); (306, OpenMP, comp_add_script_line(\"Flag=MGF_INPBASE64b\\n\");); (307, OpenMP, comp_add_script_line(\"Flag=MGF_INPBASE64m\\n\");); (186, OpenMP, comp_add_script_line(\"Func=DynamicFunc__append_salt%s\\n\", use_inp1?\"\":\"2\"); use_inp1? ++salt_cnt : ++salt_cnt2; }); (68, Open"}
{"predict": "(15, MPI, MPI_Type_vector(COUNT, 1, noncontig_stride, MPI_INT, &dt_vector);); (12, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (12, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (14, MPI, MPI_Type_commit(&dt_vector);); (31, MPI, MPI_Type_free(&dt_vector);); (28, MPI, MPI_Op_free(&op);); (9, MPI, MPI_Type_create_resized(dt_vector, 0, COUNT * sizeof(int), &dt_noncontig);)", "label": "(38, MPI, MPI_Op_free(&op);); (16, MPI, MPI_Type_free(&dt_vector);); (35, MPI, MPI_Type_free(&dt_noncontig);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (13, MPI, MPI_Type_commit(&dt_noncontig);); (9, MPI, MPI_Type_vector(1, 1, noncontig_stride, MPI_INT, &dt_vector);); (9, MPI, MPI_Type_create_resized(dt_vector, 0, sizeof(int) * noncontig_stride, &dt_noncontig);)"}
{"predict": "(7, CUDA, __shared__ int s_data[32][32];)", "label": "(6, CUDA, __shared__ int s_data[32][33];)"}
{"predict": "(13, CUDA, CUDA_CHECK(cudaGetDeviceProperties(&props, 0));)", "label": "(13, CUDA, CUDA_CHECK(cudaGetDeviceProperties(&props, 0));)"}
{"predict": "(13, CUDA, EXPECT_EQ(cudaSuccess, cudaFree(test_ptr));); (6, CUDA, st = cudaMalloc(&test_ptr, sizeof(ucc_mem_attr_t));)", "label": "(6, CUDA, st = cudaMalloc(&test_ptr, TEST_ALLOC_SIZE);); (11, CUDA, cudaFree(test_ptr);)"}
{"predict": "(19, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (25, TBB, serial_tiled_time = (tbb::tick_count::now() - t0).seconds();); (30, TBB, tbb_time = (tbb::tick_count::now() - t0).seconds();); (19, TBB, serial_time = (tbb::tick_count::now() - t0).seconds();)", "label": "(19, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (25, TBB, serial_tiled_time = (tbb::tick_count::now() - t0).seconds();); (30, TBB, tbb_time = (tbb::tick_count::now() - t0).seconds();); (18, TBB, serial_time = (tbb::tick_count::now() - t0).seconds();)"}
{"predict": "(32, CUDA, cudaSimulation.addLayer(devicefn_layer);); (37, CUDA, cudaSimulation.addLayer(hostfn_layer);)", "label": "(52, CUDA, flamegpu::CUDASimulation cudaSimulation(model);)"}
{"predict": "(7, OpenMP, #pragma omp sections); (6, OpenMP, #pragma omp section)", "label": "(8, OpenMP, #pragma omp section); (6, OpenMP, #pragma omp parallel)"}
{"predict": "(20, MPI, MPI_Type_get_true_extent(keysat, &keysat_lb, &keysat_extent);); (23, MPI, MPI_Type_get_true_extent(key, &key_true_lb, &key_true_extent);); (82, MPI, MPI_Allreduce(counts, all_counts, 2, MPI_UINT64_T, MPI_SUM, comm);); (85, MPI, MPI_Exscan(counts, counts, 2, MPI_UINT64_T, MPI_SUM, comm);); (14, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(25, MPI, MPI_Type_get_true_extent(key, &key_true_lb, &key_true_extent);); (15, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (83, MPI, MPI_Allreduce(counts, all_counts, 2, MPI_UINT64_T, MPI_SUM, comm);); (18, MPI, MPI_Type_get_extent(keysat, &keysat_lb, &keysat_extent);)"}
{"predict": "(39, CUDA, checkCudaErrors(cudaFree(bLines_d));); (23, CUDA, checkCudaErrors(cudaMalloc(&bLines_d, N_LINES * sizeof(BezierLine)));); (29, CUDA, checkCudaErrors(cudaGetLastError());); (29, CUDA, checkCudaErrors(cudaDeviceSynchronize());)", "label": "(24, CUDA, checkCudaErrors(cudaMalloc((void **)&bLines_d, N_LINES * sizeof(BezierLine)));); (35, CUDA, checkCudaErrors(cudaFree(bLines_d));)"}
{"predict": "(7, CUDA, __shared__ float shared_sum2[32];); (5, CUDA, __shared__ float shared_sum[32];)", "label": "(7, CUDA, __shared__ float shared_sum2[32];); (5, CUDA, __shared__ float shared_sum[32];)"}
{"predict": "(2, OpenMP, #pragma omp target data map(c[:size]) map(c[:size*2])); (2, OpenMP, #pragma omp target data map(c[:size*2])); (0, OpenMP, #pragma omp target data map(c[:size]))", "label": "(2, OpenMP, #pragma omp parallel for schedule(static))"}
{"predict": "(233, OpenMP, #pragma omp parallel for simd private(ii)); (254, OpenMP, #pragma omp parallel for simd lastprivate(ii)); (220, OpenMP, #pragma omp parallel for simd firstprivate(ii)); (224, OpenMP, #pragma omp parallel for simd private(ii, jj)); (234, OpenMP, #pragma omp parallel for simd private(ii, jj, kk)); (240, OpenMP, #pragma omp parallel for simd private(ii, jj, kk, fii, dii)); (220, OpenMP, #pragma omp parallel for simd private(ii)); (224, OpenMP, #pragma omp parallel for simd private(ii  ))", "label": "(235, OpenMP, #pragma omp parallel for simd private(ii)); (5, OpenMP, #pragma omp parallel for simd); (257, OpenMP, #pragma omp parallel for simd collapse(2)); (226, OpenMP, #pragma omp parallel for simd linear(ii)); (237, OpenMP, #pragma omp parallel for simd lastprivate(ii)); (220, OpenMP, #pragma omp parallel for simd firstprivate(ii))"}
{"predict": "(12, TBB, tbb::parallel_sort(iter, iter + n, *compare );)", "label": "(12, TBB, tbb::parallel_sort(iter, iter + n, *compare );)"}
{"predict": "(10, TBB, double serial_time = (tbb::tick_count::now() - t0).seconds();); (7, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (16, TBB, double tbb_time = (tbb::tick_count::now() - t1).seconds();); (13, TBB, tbb::tick_count t1 = tbb::tick_count::now();)", "label": "(10, TBB, double serial_time = (tbb::tick_count::now() - t0).seconds();); (7, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (16, TBB, double tbb_time = (tbb::tick_count::now() - t1).seconds();); (13, TBB, tbb::tick_count t1 = tbb::tick_count::now();)"}
{"predict": "(24, CUDA, CHECK(cudaMalloc(&d_B, M));); (24, CUDA, CHECK(cudaMemcpy(d_A, h_A, M, cudaMemcpyHostToDevice));); (42, CUDA, CHECK(cudaFree(d_B));); (18, CUDA, CHECK(cudaMalloc(&d_A, M));); (29, CUDA, CHECK(cudaMemcpy(h_B, d_B, M, cudaMemcpyDeviceToHost));); (37, CUDA, CHECK(cudaFree(d_A));)", "label": "(24, CUDA, CHECK(cudaMalloc(&d_B, M));); (24, CUDA, CHECK(cudaMemcpy(d_A, h_A, M, cudaMemcpyHostToDevice));); (42, CUDA, CHECK(cudaFree(d_B));); (20, CUDA, CHECK(cudaMalloc(&d_A, M));); (28, CUDA, CHECK(cudaMemcpy(h_B, d_B, M, cudaMemcpyDeviceToHost));); (38, CUDA, CHECK(cudaFree(d_A));)"}
{"predict": "(3, CUDA, HANDLE_ERROR(cudaStreamCreate(&stream));); (15, CUDA, HANDLE_ERROR(cudaStreamDestroy(stream));)", "label": "(4, CUDA, HANDLE_ERROR(cudaStreamCreate(&stream));); (15, CUDA, HANDLE_ERROR(cudaStreamDestroy(stream));)"}
{"predict": "(5, CUDA, __shared__ uint data[MERGE_THREADBLOCK_SIZE];)", "label": "(12, CUDA, __shared__ uint data[MERGE_THREADBLOCK_SIZE];)"}
{"predict": "(2, OpenMP, #pragma omp parallel num_threads(2)); (7, OpenMP, omp_set_dynamic(1);)", "label": "(2, OpenMP, #pragma omp critical)"}
{"predict": "(38, OpenCL, retVal = clReleaseMemObject(clBuffer);)", "label": "(38, OpenCL, retVal = clReleaseMemObject(clBuffer);)"}
{"predict": "(25, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&data, size_bytes));); (54, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (18, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamCreate(&stream));); (50, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(data));); (45, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (45, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamSynchronize(stream));)", "label": "(26, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&data, size_bytes));); (19, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamCreate(&stream));); (48, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (46, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (53, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(data));); (53, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamDestroy(stream));)"}
{"predict": "(28, OpenMP, #pragma omp target map(from: x[0:count])); (6, OpenMP, #pragma omp target enter data map(to: x[0:count])); (21, OpenMP, #pragma omp target data map(always, tofrom: x[0:count])); (30, OpenMP, #pragma omp target exit data map(delete: x[0:count])); (9, OpenMP, #pragma omp target device(px)); (18, OpenMP, #pragma omp target map(always, tofrom: x[0:count])); (11, OpenMP, #pragma omp target map(always, tofrom: always, tofrom: x[0:count]))", "label": "(15, OpenMP, #pragma omp target map(from:px)); (7, OpenMP, #pragma omp target data map(to:x[0:count])); (21, OpenMP, #pragma omp target data use_device_ptr(x)); (26, OpenMP, #pragma omp target data use_device_addr(x[0:0]))"}
{"predict": "(61, CUDA, CHECKED_CALL(cudaEventRecord(stop, 0));); (61, CUDA, CHECKED_CALL(cudaEventSynchronize(stop));); (61, CUDA, CHECKED_CALL(cudaEventElapsedTime(&time, start, stop));); (100, CUDA, CHECKED_CALL(cudaFree(d_nresults));); (87, CUDA, CHECKED_CALL(cudaEventDestroy(start));); (14, CUDA, CHECKED_CALL(cudaGetDeviceProperties(&deviceProp, devID));); (96, CUDA, CHECKED_CALL(cudaFree(d_result));); (55, CUDA, CHECKED_CALL(cudaEventRecord(start, 0));); (81, CUDA, CHECKED_CALL(cudaEventDestroy(stop));); (42, CUDA, CHECKED_CALL(cudaFuncSetCacheConfig(test_interval_newton<T>, cudaFuncCachePreferL1));); (25, CUDA, CHECKED_CALL(cudaEventCreate(&stop));); (47, CUDA, CHECKED_CALL(cudaMemset(d_nresults, 0, THREADS * sizeof(*d_nresults)));); (17, CUDA, CHECKED_CALL(cudaEventCreate(&start));); (29, CUDA, CHECKED_CALL(cudaEventCreate(&interval_start));); (14, CUDA, CHECKED_CALL(cudaEventCreate(&interval_end));); (52, CUDA, CHECKED_CALL(cudaEventElapsedTime(&time, start, stop));); (27, CUDA, CHECKED_CALL(cudaEventRecord(interval_start, 0));); (16, CUDA, CHECKED_CALL(cudaMemset(d_result, 0, THREADS * DEPTH_RESULT * sizeof(*d_result)));); (22, CUDA, CHECKED_CALL(cudaEventRecord(stop, 0));); (-11, CUDA, int devID = findCudaDevice(argc, (const char **)argv);); (17, CUDA, CHECKED_CALL(cudaEventSynchronize(stop));); (20, CUDA, CHECKED_CALL(cudaEventDestroy(interval_start));); (8, CUDA, CHECKED_CALL(cudaEventRecord(start, 0));); (19, CUDA, CHECKED_CALL(cudaEventDestroy(interval_end));); (0, CUDA, CHECKED_CALL(cudaGetDevice(&devID));); (54, CUDA, CHECKED_CALL(cudaFree(d_result));); (16, CUDA, CHECKED_CALL(cudaEventDestroy(stop));); (53, CUDA, CHECKED_CALL(cudaFree(d_result));); (-7, CUDA, CHECKED_CALL(cudaGetDeviceProperties(&deviceProp, 0));); (4,", "label": "(59, CUDA, CHECKED_CALL(cudaEventRecord(start, 0));); (52, CUDA, CHECKED_CALL(cudaDeviceSetLimit(cudaLimitStackSize, 8192));); (62, CUDA, CHECKED_CALL(cudaGetLastError());); (41, CUDA, CHECKED_CALL(cudaEventCreate(&start));); (36, CUDA, CHECKED_CALL(cudaSetDevice(devID));); (88, CUDA, CHECKED_CALL(cudaEventDestroy(start));); (89, CUDA, CHECKED_CALL(cudaFree(d_result));); (61, CUDA, CHECKED_CALL(cudaDeviceSynchronize());); (6, CUDA, cudaGetDeviceProperties(&deviceProp, devID);); (3, CUDA, int devID = findCudaDevice(argc, (const char **)argv);); (78, CUDA, CHECKED_CALL(cudaEventElapsedTime(&time, start, stop));); (56, CUDA, CHECKED_CALL(cudaEventRecord(stop, 0));); (84, CUDA, CHECKED_CALL(cudaFree(d_nresults));); (30, CUDA, CHECKED_CALL(cudaMalloc((void **)&d_nresults, THREADS * sizeof(*d_nresults)));); (80, CUDA, CHECKED_CALL(cudaEventDestroy(stop));); (30, CUDA, CHECKED_CALL(cudaEventCreate(&stop));)"}
{"predict": "(6, OpenMP, #pragma omp target teams distribute parallel for map(in1, in2,out)); (13, OpenMP, #pragma omp target update from(out[0:rowsPerBlock*w]))", "label": "(4, OpenACC, #pragma acc enter data create(in1[:w*h], in2[:h*w], out[:w*h])); (12, OpenACC, #pragma acc loop); (8, OpenACC, #pragma acc update device(in1[lower*w:(upper-lower)*w],in2[lower*w:(upper-lower)*w]) async(block%2)); (17, OpenACC, #pragma acc wait); (7, OpenACC, #pragma acc parallel loop present(in1,in2, out) async(block%2)); (13, OpenACC, #pragma acc update self(out[lower*w:(upper-lower)*w]) async(block%2)); (15, OpenACC, #pragma acc exit data delete(in1, in2, out))"}
{"predict": "(35, CUDA, cudaCreateTextureObject(&texObj, &resDesc, &texDesc, NULL);); (7, CUDA, cudaSetDevice(0);); (38, CUDA, cudaMalloc(&d_PointCounterAddr, sizeof(unsigned int));); (44, CUDA, cudaDeviceSynchronize();); (38, CUDA, cudaMalloc(&d_fstPts, sizeof(unsigned int));); (38, CUDA, cudaMalloc(&d_totPts, sizeof(unsigned int));); (45, CUDA, cudaFree(d_PointCounterAddr);); (50, CUDA, cudaFree(d_fstPts);); (50, CUDA, cudaFree(d_totPts);); (34, CUDA, cudaMalloc(&d_diffImg, nd*sizeof(diffImg[0]));); (42, CUDA, cudaFree(d_diffImg);)", "label": "(5, CUDA, safeCall(cudaGetSymbolAddress((void**)&d_PointCounterAddr, d_PointCounter));); (57, CUDA, safeCall(cudaMemcpy(&totPts, &d_PointCounterAddr[2*octave+1], sizeof(int), cudaMemcpyDeviceToHost));); (5, CUDA, safeCall(cudaMemcpy(&fstPts, &d_PointCounterAddr[2*octave-1], sizeof(int), cudaMemcpyDeviceToHost));); (50, CUDA, safeCall(cudaDestroyTextureObject(texObj));); (32, CUDA, cudaCreateTextureObject(&texObj, &resDesc, &texDesc, NULL);)"}
{"predict": "(52, TBB, tbb::flow::make_edge( q2, q3 );); (50, TBB, tbb::flow::make_edge( q, q2 );); (73, TBB, tbb::flow::queue_node<T> q_copy(q);); (2, TBB, tbb::flow::graph g;); (0, TBB, tbb::flow::queue_node<T> q;)", "label": "(4, TBB, tbb::flow::queue_node<T> q2(g);); (51, TBB, tbb::flow::make_edge( q2, q3 );); (3, TBB, tbb::flow::queue_node<T> q3(g);); (48, TBB, tbb::flow::make_edge( q, q2 );); (-2, TBB, tbb::flow::graph g;); (68, TBB, tbb::flow::queue_node<T> q_copy(q);); (-3, TBB, tbb::flow::queue_node<T> q(g);)"}
{"predict": "(24, MPI, MPI_Win_set_attr(win, key[2], &attrval[2]);); (25, MPI, MPI_Win_set_attr(win, key[0], &attrval[0]);); (57, MPI, MPI_Win_free_keyval(&key[i]);); (29, MPI, MPI_Win_delete_attr(win, key[0]);); (19, MPI, MPI_Win_set_attr(win, key[1], &attrval[1]);); (28, MPI, MPI_Win_delete_attr(win, key[2]);); (28, MPI, MPI_Win_delete_attr(win, key[1]);); (60, MPI, MPI_Win_free(&win);); (17, MPI, MPI_Win_create(buf, sizeof(int), sizeof(int), MPI_INFO_NULL, comm, &win);)", "label": "(27, MPI, MPI_Win_set_attr(win, key[0], &attrval[0]);); (12, MPI, MPI_Win_create(buf, sizeof(int), sizeof(int), MPI_INFO_NULL, comm, &win);); (29, MPI, MPI_Win_delete_attr(win, key[0]);); (30, MPI, MPI_Win_delete_attr(win, key[2]);); (58, MPI, MPI_Win_free_keyval(&key[i]);); (20, MPI, MPI_Win_set_attr(win, key[2], &attrval[2]);); (58, MPI, MPI_Win_free(&win);); (19, MPI, MPI_Win_set_attr(win, key[1], &attrval[1]);); (24, MPI, MPI_Win_delete_attr(win, key[1]);)"}
{"predict": "(39, OpenMP, #pragma omp atomic); (10, OpenMP, #pragma omp parallel for)", "label": "(11, OpenMP, #pragma omp parallel for); (38, OpenMP, #pragma omp atomic)"}
{"predict": "(19, MPI, MPI_Comm_rank(comm, &rank);)", "label": "(19, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(96, MPI, rc = MPI_Error_string(MPI_ERR_WIN, estr, &slen);); (28, MPI, MPI_Init(&argc, &argv);); (82, MPI, MPI_Comm_disconnect(&icomm);); (78, MPI, MPI_Comm_spawn(\"spawn_argv[0]\", \"spawn_argv[1]\", 1, spawn_info, 0, MPI_COMM_SELF, &icomm, spawn_err);); (54, MPI, MPI_Comm_get_errhandler(MPI_COMM_SELF, &errh);); (89, MPI, rc = MPI_Finalize();); (69, MPI, MPI_Info_set(spawn_info, \"mpi_initial_errhandler\", \"mpi_errors_return\");); (76, MPI, MPI_Barrier(icomm);); (18, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (59, MPI, MPI_Info_set(MPI_INFO_ENV, \"mpi_initial_errhandler\", init_errh_info);); (65, MPI, MPI_Info_create(&spawn_info);); (14, MPI, MPI_Init_errhandler(MPI_INIT, &errh);); (69, MPI, MPI_Info_free(&spawn_info);); (78, MPI, rc = MPI_Comm_set_errhandler(MPI_COMM_SELF, MPI_ERRORS_ARE_FATAL);); (29, MPI, MPI_Finalize();); (11, MPI, MPI_Info_set(MPI_INFO_ENV, \"mpi_initial_errhandler\", \"mpi_errors_abort\");); (36, MPI, MPI_Error_string(rc, estr, &slen);); (19, MPI, MPI_Info_set(MPI_INFO_ENV, \"mpi_initial_errhandler\", \"mpi_errors_return\");); (29, MPI, MPI_Info_free(&MPI_INFO_ENV);); (11, MPI, MPI_Info_set(MPI_INFO_ENV, \"mpi_initial_errhandler\", \"mpi_errors_fatal\");); (46, MPI, MPI_Comm_get_errhandler(icomm, &errh);); (55, MPI, MPI_Info_free(&MPI_INFO_ENV);)", "label": "(28, MPI, MPI_Init(&argc, &argv);); (82, MPI, MPI_Comm_disconnect(&icomm);); (76, MPI, MPI_Comm_spawn(argv[0], &spawn_argv[1], 1, spawn_info, 0, MPI_COMM_WORLD, &icomm, spawn_err);); (26, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (83, MPI, rc = MPI_Comm_set_errhandler(MPI_COMM_SELF, MPI_ERRORS_ARE_FATAL);); (43, MPI, MPI_Info_get(MPI_INFO_ENV, \"mpi_initial_errhandler\", MPI_MAX_INFO_VAL, init_errh_info, &flag);); (68, MPI, MPI_Info_set(spawn_info, \"mpi_initial_errhandler\", \"mpi_errors_return\");); (90, MPI, rc = MPI_Finalize();); (74, MPI, MPI_Barrier(icomm);); (80, MPI, MPI_Error_string(rc, estr, &slen);); (49, MPI, MPI_Comm_get_errhandler(MPI_COMM_SELF, &errh);); (62, MPI, MPI_Info_create(&spawn_info);); (93, MPI, rc = MPI_Error_string(MPI_ERR_WIN, estr, &slen);)"}
{"predict": "(3, OpenMP, #pragma omp target map(tofrom: threadCount)); (4, OpenMP, #pragma omp metadirective)", "label": "(3, OpenMP, #pragma omp target map(tofrom: threadCount)); (4, OpenMP, #pragma omp metadirective)"}
{"predict": "(3, OpenMP, devID = xomp_get_deviceID();); (13, OpenMP, return xomp_get_numThreadBlocks();); (1, OpenMP, major = xomp_get_maxThreadBlocksPerMultiprocessor(devID);); (1, OpenMP, minor = xomp_get_minThreadBlocksPerMultiprocessor(devID);)", "label": "(14, OpenMP, printf(\"Error: xomp_get_maxThreadBlocksPerMultiprocessor(): unhandled Compute Capability numbers%d.%d \\n\", major, minor);); (3, CUDA, major = xomp_getCudaDeviceProp(devID)-> major;); (3, CUDA, minor = xomp_getCudaDeviceProp(devID)-> minor;)"}
{"predict": "(17, MPI, MPI_Comm_dup(MPI_COMM_SELF, &self_dup);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (21, MPI, MPI_Comm_free(&self_dup);); (16, MPI, MPI_Comm_free(&comm);); (23, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &comm);)", "label": "(26, MPI, MPI_Comm_dup(comms[*(int *) arg], &comm);); (4, MPI, MPI_Comm_rank(comms[*(int *) arg], &rank);); (20, MPI, MPI_Comm_free(&self_dup);); (24, MPI, MPI_Comm_free(&comm);); (13, MPI, MPI_Comm_dup(MPI_COMM_SELF, &self_dup);)"}
{"predict": "(30, CUDA, HANDLE_ERROR(cudaMalloc((void**)&costs_dev_only_d, sizeof(float) * num_rollouts));); (34, CUDA, HANDLE_ERROR(cudaMalloc((void**)&baseline_and_normalizer_d, sizeof(float2)));); (31, CUDA, HANDLE_ERROR(cudaMalloc((void**)&costs_host_only_d, sizeof(float) * num_rollouts));); (75, CUDA, HANDLE_ERROR(cudaFree(costs_dev_only_d));); (75, CUDA, HANDLE_ERROR(cudaFree(baseline_and_normalizer_d));); (10, CUDA, HANDLE_ERROR(cudaStreamCreate(&stream));); (74, CUDA, HANDLE_ERROR(cudaFree(costs_host_only_d));); (58, CUDA, HANDLE_ERROR(cudaFreeAsync(baseline_and_normalizer_d, stream));); (52, CUDA, HANDLE_ERROR(cudaStreamSynchronize(stream));); (56, CUDA, HANDLE_ERROR(cudaFreeAsync(costs_dev_only_d, stream));)", "label": "(12, CUDA, cudaStreamCreate(&stream);); (37, CUDA, HANDLE_ERROR(cudaStreamSynchronize(stream));); (17, CUDA, HANDLE_ERROR(cudaMalloc((void**)&costs_dev_only_d, sizeof(float) * num_rollouts));); (15, CUDA, HANDLE_ERROR(cudaMalloc((void**)&baseline_and_normalizer_d, sizeof(float2)));); (16, CUDA, HANDLE_ERROR(cudaMalloc((void**)&costs_host_only_d, sizeof(float) * num_rollouts));)"}
{"predict": "(2, CUDA, __shared__ typename BlockReduce<float>::TempStorage m_storage;); (2, CUDA, __shared__ typename BlockReduce<float>::TempStorage v_storage;)", "label": "(2, CUDA, __shared__ typename BlockReduce<float>::TempStorage v_storage;); (0, CUDA, __shared__ typename BlockReduce<float>::TempStorage m_storage;)"}
{"predict": "(33, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (42, TBB, reduce_time = (tbb::tick_count::now() - t0).seconds();); (47, TBB, graph_time = (tbb::tick_count::now() - t0).seconds();); (31, TBB, serial_time = (tbb::tick_count::now() - t0).seconds();)", "label": "(33, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (41, TBB, reduce_time = (tbb::tick_count::now() - t0).seconds();); (47, TBB, graph_time = (tbb::tick_count::now() - t0).seconds();); (32, TBB, serial_time = (tbb::tick_count::now() - t0).seconds();)"}
{"predict": "(6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &world_size);)", "label": "(6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &world_size);)"}
{"predict": "(8, OpenMP, rec = get_decomp_record(code);)", "label": "(8, OpenMP, rec = get_decomp_record(code);)"}
{"predict": "(23, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (22, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (6, MPI, MPI_Abort(MPI_COMM_WORLD, -1);); (8, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(7, CUDA, __shared__ float sdata2[THREAD_PER_BLOCK];); (5, CUDA, __shared__ float sdata[THREAD_PER_BLOCK];); (7, CUDA, __shared__ float mean, var;)", "label": "(6, CUDA, __shared__ float sdata[THREAD_PER_BLOCK];); (8, CUDA, __shared__ float var;); (5, CUDA, __shared__ float sdata2[THREAD_PER_BLOCK];); (5, CUDA, __shared__ float mean;)"}
{"predict": "(99, TBB, TestFuncDefinitionPresence( parallel_deterministic_reduce, (const tbb::blocked_range<int>&, Body2&, const tbb::simple_partitioner&, tbb::task_group_context&), void );); (97, TBB, TestFuncDefinitionPresence( parallel_deterministic_reduce, (const tbb::blocked_range<int>&, Body2&, tbb::affinity_partitioner&), void );); (94, TBB, TestFuncDefinitionPresence( parallel_deterministic_reduce, (const tbb::blocked_range<int>&, Body2&, tbb::task_group_context&), void );); (81, TBB, TestFuncDefinitionPresence( parallel_for_each, (const intarray&, const Body1a&, tbb::task_group_context&), void );); (98, TBB, TestFuncDefinitionPresence( parallel_deterministic_reduce, (const tbb::blocked_range<int>&, Body2&, const tbb::simple_partitioner&, tbb::task_group_context&), void );); (77, TBB, TestFuncDefinitionPresence( parallel_for_each, (intarray&, const Body1a&, tbb::task_group_context&), void );); (78, TBB, TestFuncDefinitionPresence( parallel_for_each, (const intarray&, Body1a&, tbb::task_group_context&), void );); (69, TBB, TestFuncDefinitionPresence( parallel_deterministic_reduce, (const tbb::blocked_range<int>&, Body2&, tbb::task_group_context&), void );); (76, TBB, TestFuncDefinitionPresence( parallel_for_each, (intarray&, Body1a&, const tbb::task_group_context&), void );); (69, TBB, TestFuncDefinitionPresence( parallel_deterministic_reduce, (const tbb::blocked_range<int>&, Body2&, const tbb::simple_partitioner&, tbb::task_group_context&), void );); (73, TBB, TestFuncDefinitionPresence( parallel_for_each, (intarray&, const Body1a&, tbb::task_group_context&), void );); (67, TBB, TestFuncDefinitionPresence( parallel_for_each, (const intarray&, Body1a&, tbb::task_group_context&), void );); (74, TBB, TestFuncDefinitionPresence( parallel_for_each, (intarray&, Body1a&, tbb::task_group_context&", "label": "(103, TBB, TestFuncDefinitionPresence( parallel_deterministic_reduce, (const tbb::blocked_range<int>&, Body2&, const tbb::simple_partitioner&, tbb::task_group_context&), void );); (32, TBB, TestTypeDefinitionPresence3(flow::async_node<int, int, tbb::flow::queueing_lightweight> );); (96, TBB, TestFuncDefinitionPresence( parallel_for_each, (const intarray&, const Body1&, tbb::task_group_context&), void );); (93, TBB, TestFuncDefinitionPresence( parallel_invoke, (const Body&, const Body&, tbb::task_group_context&), void );); (93, TBB, TestFuncDefinitionPresence( parallel_do, (const intarray&, const Body1a&, tbb::task_group_context&), void );); (27, TBB, TestTypeDefinitionPresence3(flow::multifunction_node<int, intpair, tbb::flow::queueing> );); (30, TBB, TestTypeDefinitionPresence2(flow::join_node<intpair, tbb::flow::key_matching<int> > );); (82, TBB, TestFuncDefinitionPresence( parallel_scan, (const tbb::blocked_range<int>&, const int&, const Body3a&, const Body1b&), int );); (75, TBB, TestFuncDefinitionPresence( parallel_for, (const tbb::blocked_range<int>&, const Body2&, const tbb::simple_partitioner&), void );); (79, TBB, TestFuncDefinitionPresence( parallel_scan, (const tbb::blocked_range2d<int>&, Body3&, const tbb::auto_partitioner&), void );); (74, TBB, TestFuncDefinitionPresence( parallel_reduce, (const tbb::blocked_range<int>&, const int&, const Body2a&, const Body1b&), int );); (23, TBB, TestTypeDefinitionPresence2(flow::continue_node<int, tbb::flow::lightweight> );); (73, TBB, TestFuncDefinitionPresence( parallel_reduce, (const tbb::blocked_range<int>&, Body2&, tbb::affinity_partitioner&), void );); (74, TBB, TestFuncDefinitionPresence( parallel_deterministic_reduce, (const tbb::blocked_range<int>&, Body2&, const tbb::static_partitioner&), void );); (14, TBB, typedef tbb::flow::tuple<int, int> intpair;); (33, TBB, TestTypeDefinitionPresence2(flow::composite_node<tbb::flow::tuple<int>, tbb::flow::tuple<int> > );); (15, TBB, TestTypeDefinitionPresence3(flow::function_node<int, int, tbb::flow::rejecting> );); (82, TBB, TestFuncDefinitionPresence( parallel_for, (int, int, const Body1&, const tbb::auto_partitioner&, tbb::task_group_context&), void );); (68, TBB, TestFuncDefinitionPresence( parallel_deterministic_reduce, (const tbb::blocked_range<int>&, const int&, const Body2a&, const Body1b&), int );); (75, TBB, TestFuncDefinitionPresence( parallel_pipeline, (size_t, const tbb::filter_t<void,void>&), void );); (81, TBB, TestFuncDefinitionPresence( parallel_reduce, (const tbb::blocked_range<int>&, Body2&, const tbb::auto_partitioner&, tbb::task_group_context&), void );); (5, TBB, TestFuncDefinitionPresence( flow::make_edge, (tbb::flow::sender<Msg>&, tbb::flow::receiver<Msg>&), void );); (82, TBB, TestFuncDefinitionPresence( parallel_deterministic_reduce, (const tbb::blocked_range<int>&, Body2&, tbb::task_group_context&), void );); (4, TBB, TestFuncDefinitionPresence( flow::remove_edge, (tbb::flow::sender<Msg>&, tbb::flow::receiver<Msg>&), void );); (76, TBB, TestFuncDefinitionPresence( parallel_for, (int, int, const Body1&, tbb::task_group_context&), void );); (77, TBB, TestFuncDefinitionPresence( parallel_reduce, (const tbb::blocked_range<int>&, Body2&, tbb::task_group_context&), void );); (9, TBB, TestTypeDefinitionPresence2(flow::join_node<intpair, tbb::flow::reserving> );)"}
{"predict": "(31, OpenMP, __kmp_msg(kmp_ms_warning, KMP_MSG(InvalidLayer, name, layer));); (13, OpenMP, ptr = ++delim;); (13, OpenMP, while (*delim!= ',' && *delim!= '0');); (96, OpenMP, __kmp_msg(kmp_ms_warning, KMP_MSG(Chunk, name, delim));)", "label": "(30, OpenMP, __kmp_omp_schedule_restore();)"}
{"predict": "(12, MPI, MPI_Comm_rank(team.comm, &rank);); (12, MPI, MPI_Comm_size(team.comm, &nprocs);)", "label": "(12, MPI, MPI_Comm_rank(team.comm, &rank);); (12, MPI, MPI_Comm_size(team.comm, &nprocs);)"}
{"predict": "(9, OpenCL, err = clCreateKernel(program, kernel_info->kernel, &err);)", "label": "(9, OpenCL, kernel_info->kernel = clCreateKernel(program, kernel_info->kernel_name, &err);)"}
{"predict": "(16, OpenCL, error = clEnqueueMapBuffer(cmdq, num_correct, CL_TRUE, CL_MAP_READ | CL_MAP_WRITE, 0, sizeof(cl_int)*ListLength * numLists, 0, NULL,NULL);); (11, OpenCL, error = clEnqueueNDRangeKernel(cmdq, kernel_verify_lists, 1, NULL, &numLists, NULL, 0, NULL, NULL);); (4, OpenCL, cl_int pNumCorrect = (cl_int)clEnqueueMapBuffer(cmdq, num_correct, CL_TRUE, CL_MAP_READ | CL_MAP_WRITE, 0, sizeof(cl_int)*ListLength * numLists, 0, NULL,NULL, &error);); (7, OpenCL, error = clEnqueueUnmapMemObject(cmdq, num_correct, pNumCorrect, 0,NULL,NULL);)", "label": "(6, OpenCL, cl_int *pNumCorrect = (cl_int*) clEnqueueMapBuffer(cmdq, num_correct, CL_TRUE, CL_MAP_READ | CL_MAP_WRITE, 0, sizeof(cl_int), 0, NULL,NULL, &error);); (10, OpenCL, error = clEnqueueUnmapMemObject(cmdq, num_correct, pNumCorrect, 0,NULL,NULL);); (15, OpenCL, pNumCorrect = (cl_int*) clEnqueueMapBuffer(cmdq, num_correct, CL_TRUE, CL_MAP_READ | CL_MAP_WRITE, 0, sizeof(cl_int), 0, NULL,NULL, &error);); (11, OpenCL, error = clEnqueueNDRangeKernel(cmdq, kernel_verify_lists, 1, NULL, &numLists, NULL, 0, NULL, NULL);)"}
{"predict": "(8, TBB, tbb::task_scheduler_init init;)", "label": "(8, TBB, tbb::task_scheduler_init init_market_p_plus_one(MaxThread + 1);)"}
{"predict": "(8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &comm_size);); (70, MPI, MPI_Comm_split(MPI_COMM_WORLD, 0, rand(), &comm);); (58, MPI, MPI_Comm_split(MPI_COMM_WORLD, (comm_rank == comm_size - 1)? 0 : 1, 0, &comm);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &comm_rank);); (59, MPI, MPI_Comm_free(&comm);); (7, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (30, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)", "label": "(7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &comm_size);); (69, MPI, MPI_Comm_split(MPI_COMM_WORLD, 0, rand(), &comm);); (57, MPI, MPI_Comm_split(MPI_COMM_WORLD, (comm_rank == comm_size - 1)? 0 : 1, 0, &comm);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &comm_rank);); (58, MPI, MPI_Comm_free(&comm);); (7, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (30, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(1, TBB, oneapi::tbb::flow::graph g;)", "label": "(1, TBB, oneapi::tbb::flow::graph g;)"}
{"predict": "(12, CUDA, cudaMemcpy(dArray, array, size, cudaMemcpyHostToDevice);); (34, CUDA, cudaFree(dArray);); (9, CUDA, cudaMalloc(&dArray, size);); (16, CUDA, cudaMemcpy(array, dArray, size, cudaMemcpyDeviceToHost);)", "label": "(12, CUDA, checkCudaErrors(cudaMemcpy(dArray, array, size, cudaMemcpyHostToDevice));); (10, CUDA, checkCudaErrors(cudaMalloc(&dArray, size));); (19, CUDA, checkCudaErrors(cudaFree(dArray));); (17, CUDA, checkCudaErrors(cudaMemcpy(array, dArray, size, cudaMemcpyDeviceToHost));)"}
{"predict": "(9, TBB, c.set_core_type(tbb::info::core_types().back());); (6, TBB, tbb::task_arena::constraints c;); (37, TBB, tbb::affinity_partitioner aff_p;); (6, TBB, c.set_numa_id(tbb::info::numa_nodes()[0]);); (27, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (47, TBB, double static_time = (tbb::tick_count::now() - t0).seconds();); (6, TBB, c.set_max_concurrency(std::min(8, tbb::info::default_concurrency(c)));); (37, TBB, double affinity_time = (tbb::tick_count::now() - t0).seconds();); (32, TBB, tbb::parallel_for_each(v, N, a, aff_p);); (3, TBB, tbb::task_arena arena(c);); (25, TBB, double auto_time = (tbb::tick_count::now() - t0).seconds();); (0, TBB, tbb::global_control gc(tbb::global_control::max_allowed_parallelism, c);)", "label": "(9, TBB, c.set_core_type(tbb::info::core_types().back());); (6, TBB, tbb::task_arena::constraints c;); (36, TBB, tbb::affinity_partitioner aff_p;); (5, TBB, c.set_numa_id(tbb::info::numa_nodes()[0]);); (26, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (46, TBB, double static_time = (tbb::tick_count::now() - t0).seconds();); (5, TBB, c.set_max_concurrency(std::min(8, tbb::info::default_concurrency(c)));); (36, TBB, double affinity_time = (tbb::tick_count::now() - t0).seconds();); (31, TBB, t0 = tbb::tick_count::now();); (25, TBB, double auto_time = (tbb::tick_count::now() - t0).seconds();); (2, TBB, tbb::task_arena arena(c);)"}
{"predict": "(18, CUDA, checkRuntime(cudaStreamCreate(&stream));); (80, CUDA, checkRuntime(cudaMemcpyAsync(output_data_host, output_data_device, sizeof(float) * output_numel, cudaMemcpyDeviceToHost, stream));); (80, CUDA, checkRuntime(cudaStreamSynchronize(stream));); (58, CUDA, checkRuntime(cudaMemcpyAsync(input_data_device, input_data_host, input_numel * sizeof(float), cudaMemcpyHostToDevice, stream));); (24, CUDA, checkRuntime(cudaMallocHost(&input_data_host, input_numel * sizeof(float)));); (24, CUDA, checkRuntime(cudaMalloc(&input_data_device, input_numel * sizeof(float)));); (178, CUDA, checkRuntime(cudaFree(input_data_device));); (178, CUDA, checkRuntime(cudaFreeHost(input_data_host));); (179, CUDA, checkRuntime(cudaFree(output_data_device));); (65, CUDA, checkRuntime(cudaMalloc(&output_data_device, sizeof(float) * output_numel));); (-4, CUDA, auto engine = make_nvshared(runtime->deserializeCudaEngine(engine_data.data(), engine_data.size()));); (62, CUDA, checkRuntime(cudaMallocHost(&output_data_host, sizeof(float) * output_numel));); (168, CUDA, checkRuntime(cudaFreeHost(output_data_host));); (161, CUDA, checkRuntime(cudaStreamDestroy(stream));)", "label": "(18, CUDA, checkRuntime(cudaStreamCreate(&stream));); (80, CUDA, checkRuntime(cudaMemcpyAsync(output_data_host, output_data_device, sizeof(float) * output_numel, cudaMemcpyDeviceToHost, stream));); (80, CUDA, checkRuntime(cudaStreamSynchronize(stream));); (58, CUDA, checkRuntime(cudaMemcpyAsync(input_data_device, input_data_host, input_numel * sizeof(float), cudaMemcpyHostToDevice, stream));); (24, CUDA, checkRuntime(cudaMallocHost(&input_data_host, input_numel * sizeof(float)));); (24, CUDA, checkRuntime(cudaMalloc(&input_data_device, input_numel * sizeof(float)));); (191, CUDA, checkRuntime(cudaFree(input_data_device));); (188, CUDA, checkRuntime(cudaFreeHost(input_data_host));); (190, CUDA, checkRuntime(cudaFree(output_data_device));); (63, CUDA, checkRuntime(cudaMalloc(&output_data_device, sizeof(float) * output_numel));); (-5, CUDA, auto engine = make_nvshared(runtime->deserializeCudaEngine(engine_data.data(), engine_data.size()));); (60, CUDA, checkRuntime(cudaMallocHost(&output_data_host, sizeof(float) * output_numel));); (184, CUDA, checkRuntime(cudaFreeHost(output_data_host));); (181, CUDA, checkRuntime(cudaStreamDestroy(stream));)"}
{"predict": "(10, MPI, MPI_Type_create_struct(1, b, d, newtype, &newtype);); (13, MPI, MPI_Type_free(&newtype);)", "label": "(17, MPI, MPI_Sendrecv(b, 1, newtype, 0, 0, d, 0, newtype, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (10, MPI, MPI_Type_commit(&newtype);); (8, MPI, MPI_Type_create_hvector(0, 1, 10, MPI_DOUBLE, &newtype);); (9, MPI, MPI_Type_free(&newtype);); (10, MPI, MPI_Type_indexed(0, b, d, MPI_DOUBLE, &newtype);)"}
{"predict": "(15, CUDA, thrust::device_vector<key> in_keys(data_source);); (15, CUDA, thrust::device_vector<value> in_values(data_source);); (7, CUDA, thrust::host_vector<key> host_keys(data_source);); (9, CUDA, thrust::shuffle(data_source.begin(), data_source.end(), re);); (15, CUDA, thrust::device_vector<key> out_keys(data_source);); (13, CUDA, thrust::device_vector<value> out_values(data_source);); (4, CUDA, thrust::host_vector<value> host_values(data_source);); (1, CUDA, thrust::host_vector<key> host_keys(data_source);)", "label": "(9, CUDA, thrust::device_vector<std::uint8_t> data_source(elements);); (12, CUDA, thrust::sequence(data_source.begin(), data_source.end());); (12, CUDA, thrust::shuffle(data_source.begin(), data_source.end(), re);); (5, CUDA, thrust::default_random_engine re;); (15, CUDA, thrust::device_vector<ValueT> in_values(data_source);); (17, CUDA, thrust::host_vector<KeyT> host_keys(in_keys);); (17, CUDA, thrust::host_vector<ValueT> host_values(in_values);); (13, CUDA, thrust::device_vector<ValueT> out_values(elements);); (8, CUDA, thrust::device_vector<KeyT> out_keys(elements);); (6, CUDA, thrust::device_vector<KeyT> in_keys(data_source);)"}
{"predict": "(13, OpenMP, #pragma omp parallel for); (45, OpenMP, #pragma omp atomic)", "label": "(13, OpenMP, #pragma omp parallel for); (72, OpenMP, #pragma omp atomic); (43, OpenMP, #pragma omp critical)"}
{"predict": "(84, OpenCL, sub_buf = clCreateSubBuffer(buf[1], 0, CL_BUFFER_CREATE_TYPE_REGION, &region, &error );); (87, OpenCL, void * map_ptr = clEnqueueMapBuffer(queue, buf[0], 1, CL_MAP_READ, 0, 64, 0, NULL, NULL, NULL);)", "label": "(84, OpenCL, sub_buf = clCreateSubBuffer(buf[1], 0, CL_BUFFER_CREATE_TYPE_REGION, &region, &error );); (87, OpenCL, void * map_ptr = clEnqueueMapBuffer(queue, buf[0], 1, CL_MAP_READ, 0, 64, 0, NULL, NULL, NULL);)"}
{"predict": "(1, OpenMP, #pragma omp parallel num_threads(2)); (6, OpenMP, #pragma omp cancel parallel); (1, OpenMP, #pragma omp master)", "label": "(1, OpenMP, #pragma omp parallel num_threads(2)); (10, OpenMP, #pragma omp cancellation point parallel); (3, OpenMP, #pragma omp cancel parallel)"}
{"predict": "(3, OpenMP, omp_set_default_device(0);); (32, OpenMP, #pragma omp target enter data map(to: DATA[0:N]) nowait); (14, OpenMP, #pragma omp target enter data map(to: DATA[0:N])); (18, OpenMP, #pragma omp target teams distribute parallel for is_device_ptr(DATA)); (30, OpenMP, #pragma omp target exit data map(from: DATA[0:N]) nowait); (0, OpenMP, DATA = (double*)   omp_target_alloc(N*sizeof(double), omp_get_default_device()) ;); (15, OpenMP, t1=omp_get_wtime();); (17, OpenMP, #pragma omp taskwait); (-3, OpenMP, #pragma omp parallel for); (23, OpenMP, t2=omp_get_wtime();)", "label": "(4, OpenMP, omp_set_default_device(0);); (33, OpenMP, #pragma omp target teams distribute parallel for is_device_ptr(DATA) nowait); (15, OpenMP, #pragma omp target teams distribute parallel for is_device_ptr(DATA)); (35, OpenMP, #pragma omp taskwait); (19, OpenMP, t1=omp_get_wtime();); (4, OpenMP, #pragma omp parallel for); (-1, OpenMP, DATA = (double*)   omp_target_alloc(N*sizeof(double), omp_get_default_device()) ;); (22, OpenMP, t2=omp_get_wtime();)"}
{"predict": "(8, CUDA, __shared__ double sum[256];)", "label": "(8, CUDA, __shared__ double sum[block_size];)"}
{"predict": "(62, CUDA, CHECK(cudaPeekAtLastError());)", "label": "(62, CUDA, CHECK(cudaPeekAtLastError());)"}
{"predict": "(42, CUDA, cudaFree(d_inPtr);); (27, CUDA, cudaMalloc((void**)&d_outPtr, size * sizeof(float));); (32, CUDA, cudaMemcpy(&result_gpu, &d_outPtr[0], sizeof(float), cudaMemcpyDeviceToHost);); (27, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);); (36, CUDA, cudaFree(d_outPtr);); (23, CUDA, cudaMalloc((void**)& d_inPtr, size * sizeof(float));)", "label": "(42, CUDA, cudaFree(d_inPtr);); (27, CUDA, cudaMalloc((void**)&d_outPtr, size * sizeof(float));); (32, CUDA, cudaMemcpy(&result_gpu, &d_outPtr[0], sizeof(float), cudaMemcpyDeviceToHost);); (27, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);); (37, CUDA, cudaFree(d_outPtr);); (22, CUDA, cudaMalloc((void**)& d_inPtr, size * sizeof(float));)"}
{"predict": "(23, CUDA, checkCudaErrors(cudaStreamCreate(&stream));); (93, CUDA, checkCudaErrors(cudaStreamDestroy(stream));); (89, CUDA, checkCudaErrors(cudaFreeAsync(ptrs[i]));); (19, CUDA, checkCudaErrors(cudaSetDevice(shm->devices[id]));); (58, CUDA, checkCudaErrors(cudaGetLastError());); (78, CUDA, checkCudaErrors(cudaStreamSynchronize(stream));); (83, CUDA, checkCudaErrors(cudaEventDestroy(events[i]));); (18, CUDA, checkCudaErrors(cudaStreamSetFlags(cudaStreamNonBlocking));); (16, CUDA, checkCudaErrors(cudaGetDeviceProperties(&prop, shm->devices[id]));)", "label": "(21, CUDA, checkCudaErrors(cudaSetDevice(shm->devices[id]));); (54, CUDA, checkCudaErrors(cudaStreamWaitEvent(stream, events[bufferId], 0));); (71, CUDA, checkCudaErrors(cudaStreamWaitEvent(stream, events[id], 0));); (56, CUDA, checkCudaErrors(cudaGetLastError());); (92, CUDA, checkCudaErrors(cudaStreamDestroy(stream));); (18, CUDA, checkCudaErrors(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));); (86, CUDA, checkCudaErrors(cudaIpcCloseMemHandle(ptrs[i]));); (86, CUDA, checkCudaErrors(cudaEventDestroy(events[i]));); (53, CUDA, checkCudaErrors(cudaEventRecord(events[bufferId], stream));); (68, CUDA, checkCudaErrors(cudaStreamSynchronize(stream));); (12, CUDA, checkCudaErrors(cudaGetDeviceProperties(&prop, shm->devices[id]));)"}
{"predict": "(21, OpenCL, retVal = clReleaseMemObject(smallBuffer);); (6, OpenCL, cl_mem smallBuffer = clCreateBuffer(clContext, flags, size, hostPtr, &retVal);); (24, OpenCL, EXPECT_EQ(clReleaseContext(context), CL_SUCCESS);)", "label": "(21, OpenCL, retVal = clReleaseMemObject(smallBuffer);); (5, OpenCL, cl_mem smallBuffer = clCreateBuffer(clContext, flags, size, hostPtr, &retVal);); (24, OpenCL, EXPECT_EQ(clReleaseContext(context), CL_SUCCESS);)"}
{"predict": "(13, CUDA, cudaCheck(cudaDeviceSynchronize());); (13, CUDA, cudaCheck(cudaGetLastError());)", "label": "(13, CUDA, cudaCheck(cudaGetLastError());)"}
{"predict": "(69, MPI, MPI_Irecv(buf_recv, nwrite, MPI_BYTE, rank_recv, 0, comm, &request[0]);); (69, MPI, MPI_Waitall(2, request, status);); (70, MPI, MPI_Issend(buf_send, nread, MPI_BYTE, rank_send, 0, comm, &request[1]);); (83, MPI, MPI_Issend(buf_send, 0, MPI_BYTE, rank_send, 0, comm, &request[1]);)", "label": "(82, MPI, MPI_Wait(&request[1], &status[1]);); (90, MPI, MPI_Wait(&request[0], &status[0]);); (90, MPI, MPI_Get_count(&status[0], MPI_BYTE, &nwrite);); (66, MPI, MPI_Irecv(buf_recv, shuffile_mpi_buf_size, MPI_BYTE, rank_recv, 0, comm, &request[0]);); (77, MPI, MPI_Isend(buf_send, nread, MPI_BYTE, rank_send, 0, comm, &request[1]);)"}
{"predict": "(16, CUDA, CUDASimulation cudaSimulation(model);)", "label": "(16, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(62, CUDA, CHECK_CUDART(cudaFree(indata));); (62, CUDA, CHECK_CUDART(cudaFree(outdata));); (16, CUDA, CHECK_CUDART(cudaMalloc((void**)&indata, sizeof(T)));); (16, CUDA, CHECK_CUDART(cudaMalloc((void**)&outdata, sizeof(T)));); (59, CUDA, CHECK_CUDART(cudaMemcpy(&outval, outdata, sizeof(T), cudaMemcpyDeviceToHost));)", "label": "(65, CUDA, CHECK_CUDART(cudaFree(indata));); (19, CUDA, CHECK_CUDART(cudaMemcpy(indata, &inval, sizeof(T), cudaMemcpyHostToDevice));); (61, CUDA, CHECK_CUDART(cudaMemcpy(&outval, outdata, sizeof(T), cudaMemcpyDeviceToHost));); (14, CUDA, CHECK_CUDART(cudaMalloc((void**)&indata, sizeof(T)));); (14, CUDA, CHECK_CUDART(cudaMalloc((void**)&outdata, sizeof(T)));); (59, CUDA, CHECK_CUDART(cudaFree(outdata));)"}
{"predict": "(179, OpenCL, clGetDeviceInfo(vkGPU->device, CL_DEVICE_VENDOR_ID, sizeof(cl_int), &vendorID, 0);); (77, CUDA, res = cudaMalloc((void**)&buffer, bufferSize);); (87, OpenCL, buffer = clCreateBuffer(vkGPU->context, CL_MEM_READ_WRITE, bufferSize, 0, &res);)", "label": "(162, OpenCL, clGetDeviceInfo(vkGPU->device, CL_DEVICE_VENDOR_ID, sizeof(cl_int), &vendorID, 0);); (77, CUDA, res = cudaMalloc((void**)&buffer, bufferSize);); (86, OpenCL, buffer = clCreateBuffer(vkGPU->context, CL_MEM_READ_WRITE, bufferSize, 0, &res);)"}
{"predict": "(45, OpenMP, #pragma omp teams distribute reduction(|| : argc > 0? argv[1] : argv[2])); (28, OpenMP, #pragma omp teams distribute reduction(\\)); (51, OpenMP, #pragma omp teams distribute reduction(+ : a, b, c, d, f)); (18, OpenMP, #pragma omp teams distribute reduction(-); (7, OpenMP, #pragma omp teams distribute reduction); (54, OpenMP, #pragma omp teams distribute reduction(max : h.b)); (65, OpenMP, #pragma omp teams distribute reduction(^ : fl)); (62, OpenMP, #pragma omp teams distribute reduction(&& : S2::S2s)); (73, OpenMP, #pragma omp teams distribute reduction(& : e, g)); (47, OpenMP, #pragma omp teams distribute reduction(+ : ba)); (30, OpenMP, #pragma omp teams distribute reduction(^ : S1)); (18, OpenMP, #pragma omp teams distribute reduction(foo : argc); (35, OpenMP, #pragma omp teams distribute reduction(min : a, b, c, d, f)); (22, OpenMP, #pragma omp teams distribute reduction(|| : argc > 0? argv[1] : argv[2])); (64, OpenMP, #pragma omp teams distribute reduction(+ : h, k, B::x)); (18, OpenMP, #pragma omp teams distribute reduction(| : argc,); (76, OpenMP, #pragma omp parallel reduction(* : fl)); (40, OpenMP, #pragma omp teams distribute reduction(- : da)); (25, OpenMP, #pragma omp teams distribute reduction(&& : argc, z)); (59, OpenMP, #pragma omp teams distribute reduction(+ : o)); (46, OpenMP, #pragma omp teams distribute reduction(* : ca)); (51, OpenMP, #pragma omp teams distribute reduction(&& : S2::S2sc)); (68, OpenMP, #pragma omp teams distribute reduction(+ : r)); (76, OpenMP, #pragma omp parallel shared(i)); (69, OpenMP, #pragma omp teams distribute reduction(+ : fl)); (14, OpenMP, #pragma omp teams distribute reduction(|| : argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv", "label": "(113, OpenMP, #pragma omp teams distribute reduction(task, + : m)); (15, OpenMP, #pragma omp teams distribute reduction +); (29, OpenMP, #pragma omp teams distribute reduction(\\)); (37, OpenMP, #pragma omp teams distribute reduction(|| : argc > 0? argv[1] : argv[2])); (106, OpenMP, #pragma omp teams distribute reduction(+ : m)); (20, OpenMP, #pragma omp teams distribute reduction()); (6, OpenMP, #pragma omp target); (42, OpenMP, #pragma omp teams distribute reduction(^ : S1)); (44, OpenMP, #pragma omp teams distribute reduction(+ : a, b, c, d, f)); (67, OpenMP, #pragma omp teams distribute reduction(&& : S2::S2sc)); (57, OpenMP, #pragma omp teams distribute reduction(- : da)); (32, OpenMP, #pragma omp teams distribute reduction(~ : argc)); (10, OpenMP, #pragma omp teams distribute reduction(-); (45, OpenMP, #pragma omp teams distribute reduction(max : h.b)); (5, OpenMP, #pragma omp teams distribute reduction(); (75, OpenMP, #pragma omp parallel private(k)); (18, OpenMP, #pragma omp teams distribute reduction(foo : argc); (38, OpenMP, #pragma omp teams distribute reduction(min : a, b, c, d, f)); (70, OpenMP, #pragma omp teams distribute private(i), reduction(+ : j), reduction(+ : q)); (79, OpenMP, #pragma omp teams distribute reduction(+ : r)); (41, OpenMP, #pragma omp teams distribute reduction(+ : ba)); (25, OpenMP, #pragma omp teams distribute reduction(&& : argc)); (81, OpenMP, #pragma omp teams distribute reduction(max : j)); (78, OpenMP, #pragma omp parallel reduction(min : i)); (4, OpenMP, #pragma omp teams distribute reduction(*)); (67, OpenMP, #pragma omp teams distribute reduction(+ : p), reduction(+ : p)); (44, OpenMP, #pragma omp teams distribute reduction(^ : fl)); (58, OpenMP, #pragma omp teams distribute reduction(+ : o)); (54, OpenMP, #pragma omp teams distribute reduction(+ : h, k, B::x)); (35, OpenMP, #pragma omp teams distribute reduction(* : ca)); (76, OpenMP, #pragma omp teams distribute reduction(+ : fl, z)); (48, OpenMP, #pragma omp teams distribute reduction(& : e, g)); (68, OpenMP, #pragma omp parallel shared(i)); (4, OpenMP, #pragma omp teams distribute reduction(| : argc,); (-21, OpenMP, #pragma omp teams distribute reduction); (38, OpenMP, #pragma omp teams distribute reduction(&& : S2::S2s))"}
{"predict": "(3, CUDA, HANDLE_ERROR(cudaGetDevice(&whichDevice));); (3, CUDA, HANDLE_ERROR(cudaGetDeviceProperties(&prop, whichDevice));)", "label": "(4, CUDA, checkCudaErrors(cudaGetDeviceProperties(&prop, whichDevice));); (2, CUDA, checkCudaErrors(cudaGetDevice(&whichDevice));)"}
{"predict": "(4, OpenMP, #pragma omp parallel for)", "label": "(4, OpenMP, #pragma omp parallel for)"}
{"predict": "(21, CUDA, extern __shared__ float entire_buffer[];)", "label": "(21, CUDA, extern __shared__ float entire_buffer[];)"}
{"predict": "(28, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (25, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (56, MPI, MPI_Finalize();); (52, MPI, MPI_T_finalize();); (23, MPI, MPI_T_init_thread(required, &provided);); (37, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(29, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (24, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (55, MPI, MPI_Finalize();); (52, MPI, MPI_T_finalize();); (23, MPI, MPI_T_init_thread(required, &provided);); (36, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(55, TBB, tbb::enumerable_thread_specific<ets_type> vvs( typename tbb::enumerable_thread_specific<ets_type>::size_type(25), minus_one, tbb::simple_partitioner() );); (60, TBB, tbb::parallel_for ( tbb::blocked_range<int> (0, N, RANGE_MIN), parallel_vector_for_body<T,Allocator>( vvs, allocator_name ) );); (62, TBB, tbb::flattened2d<tbb::enumerable_thread_specific<ets_type> > fvvs = flatten2d(vvs);); (0, TBB, typedef std::vector<T, tbb::tbb_allocator<T> > container_type;); (60, TBB, tbb::parallel_reduce ( vvs.range(1), pvrb2 );); (-4, TBB, typedef tbb::enumerable_thread_specific<ets_type> ets_type;); (31, TBB, tbb::flattened2d<ets_type> fvs = flatten2d(vs);); (25, TBB, tbb::parallel_reduce ( vs.range(1), pvrb );); (1, TBB, tbb::global_control gc(tbb::global_control::max_allowed_parallelism, p);)", "label": "(30, TBB, tbb::parallel_reduce ( vs.range(1), pvrb );); (60, TBB, tbb::parallel_for ( tbb::blocked_range<int> (0, N, RANGE_MIN), parallel_vector_for_body<T,Allocator>( vvs, allocator_name ) );); (1, TBB, typedef std::vector<T, tbb::tbb_allocator<T> > container_type;); (5, TBB, tbb::task_scheduler_init init(p);); (10, TBB, if (Verbose && t == 0) t0 = tbb::tick_count::now();); (64, TBB, tbb::flattened2d<ets_type> fvvs = flatten2d(vvs);); (59, TBB, tbb::parallel_reduce ( vvs.range(1), pvrb2 );); (-5, TBB, tbb::tick_count t0;); (30, TBB, tbb::flattened2d<ets_type> fvs = flatten2d(vs);); (50, TBB, ets_type vvs( typename container_type::size_type(25), minus_one, tbb::tbb_allocator<T>() );); (-6, TBB, typedef tbb::enumerable_thread_specific< container_type, Allocator<container_type> > ets_type;); (66, TBB, REMARK(\"done\\nparallel %s, %d, %g, %g\\n\", test_name, p, result_value, ( tbb::tick_count::now() - t0).seconds());)"}
{"predict": "(1, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)", "label": "(1, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)"}
{"predict": "(14, OpenMP, #pragma omp parallel for)", "label": "(14, OpenMP, #pragma omp parallel for)"}
{"predict": "(12, OpenCL, status = clEnqueueNDRangeKernel(pCmdQ, mockKernel.mockMultiDeviceKernel, testedWorkDim, nullptr, globalWorkSize, localWorkSize, 0, nullptr, nullptr);)", "label": "(12, OpenCL, status = clEnqueueNDRangeKernel(pCmdQ, mockKernel.mockMultiDeviceKernel, testedWorkDim, nullptr, globalWorkSize, localWorkSize, 0, nullptr, nullptr);)"}
{"predict": "(23, CUDA, CSC(cudaGetLastError(), STATUS_FAILURE);)", "label": "(23, CUDA, CSC(cudaGetLastError(), STATUS_FAILURE);)"}
{"predict": "(21, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(exe_node), *receivers[r] );); (10, TBB, tbb::flow::continue_node< OutputType > exe_node( g, cf );); (32, TBB, tbb::flow::remove_edge( tbb::flow::output_port<0>(exe_node), *receivers[r] );); (0, TBB, tbb::flow::graph g;); (43, TBB, g.reset(tbb::flow::rf_reset_bodies);); (17, TBB, NativeParallelFor( p, parallel_puts<tbb::flow::continue_msg>(exe_node) );); (5, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(exe_node), fake_sender );); (21, TBB, tbb::flow::remove_edge( tbb::flow::output_port<0>(exe_node), *receivers[r] );)", "label": "(46, TBB, g.reset(tbb::flow::rf_reset_bodies);); (21, TBB, tbb::flow::make_edge( exe_node, *receivers[r] );); (7, TBB, tbb::flow::continue_node< OutputType > exe_node( g, cf );); (22, TBB, utils::NativeParallelFor( p, parallel_puts<tbb::flow::continue_msg>(exe_node) );); (36, TBB, body_copy = tbb::flow::copy_body< inc_functor<OutputType> >( exe_node );); (-1, TBB, tbb::flow::graph g;); (29, TBB, tbb::flow::remove_edge( exe_node, *receivers[r] );); (5, TBB, tbb::detail::d2::register_predecessor(exe_node, fake_sender);)"}
{"predict": "(7, MPI, MPI_Init(&argc, &argv);); (49, MPI, MPI_Finalize();); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (22, MPI, MPI_Barrier(MPI_COMM_WORLD);); (22, MPI, double t0 = MPI_Wtime();); (40, MPI, double t1 = MPI_Wtime();)", "label": "(6, MPI, MPI_Init(&argc, &argv);); (47, MPI, MPI_Finalize();); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (38, MPI, double t1 = MPI_Wtime();); (20, MPI, MPI_Barrier(MPI_COMM_WORLD);); (20, MPI, double t0 = MPI_Wtime();)"}
{"predict": "(1, TBB, tbb::task_arena a(1);)", "label": "(1, TBB, tbb::task_arena a(1);); (13, TBB, CHECK(tbb::this_task_arena::max_concurrency() == 2);)"}
{"predict": "(13, OpenMP, #pragma omp teams distribute parallel for proc_bind ()); (6, OpenMP, #pragma omp teams distribute parallel for proc_bind); (31, OpenMP, #pragma omp teams distribute parallel for proc_bind(master)); (18, OpenMP, #pragma omp teams distribute parallel for proc_bind (x)); (21, OpenMP, #pragma omp teams distribute parallel for proc_bind(master)); (0, OpenMP, #pragma omp target); (25, OpenMP, #pragma omp teams distribute parallel for proc_bind(close)); (28, OpenMP, #pragma omp teams distribute parallel for proc_bind(spread)); (6, OpenMP, #pragma omp teams distribute parallel for proc_bind (master); (7, OpenMP, #pragma omp teams distribute parallel for proc_bind (close), proc_bind(spread))", "label": "(31, OpenMP, #pragma omp teams distribute parallel for proc_bind(master)); (25, OpenMP, #pragma omp teams distribute parallel for proc_bind(x)); (4, OpenMP, #pragma omp teams distribute parallel for proc_bind); (34, OpenMP, #pragma omp teams distribute parallel for proc_bind(spread)); (6, OpenMP, #pragma omp teams distribute parallel for proc_bind(); (9, OpenMP, #pragma omp teams distribute parallel for proc_bind()); (16, OpenMP, #pragma omp teams distribute parallel for proc_bind(close), proc_bind(spread)); (-2, OpenMP, #pragma omp target); (27, OpenMP, #pragma omp parallel proc_bind(close)); (9, OpenMP, #pragma omp teams distribute parallel for proc_bind(master)"}
{"predict": "(42, CUDA, cudaFree(res_dev);); (35, CUDA, cudaMalloc(&res_dev, sizeof(float));); (37, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)", "label": "(42, CUDA, cudaFree(res_dev);); (35, CUDA, cudaMalloc(&res_dev, sizeof(float));); (37, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(10, CUDA, extern __shared__ __align__(sizeof(double)) unsigned char buf_[];)", "label": "(11, CUDA, extern __shared__ __align__(sizeof(double)) unsigned char buf_[];)"}
{"predict": "(22, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (130, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (22, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (132, MPI, MPI_Finalize();); (23, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (139, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(21, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (193, MPI, MPI_Finalize();); (25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (23, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (124, MPI, MPI_Test(&req, &completed, MPI_STATUS_IGNORE);)"}
{"predict": "(41, MPI, MPIU_Assert( Mpi.IsCurrentThreadMakingProgress() );); (86, MPI, Mpi.SignalProgress();)", "label": "(40, MPI, mpi_errno = Mpi.Finalize();); (86, MPI, Mpi.CompleteFinalize();)"}
{"predict": "(44, MPI, MPI_T_finalize();); (20, MPI, MPI_T_init_thread(required, &provided);)", "label": "(44, MPI, MPI_T_finalize();); (19, MPI, MPI_T_init_thread(required, &provided);)"}
{"predict": "(147, MPI, MPI_Allgather(&end_offset, 1, ADIO_OFFSET, end_offsets, 1, ADIO_OFFSET, fd->comm);); (60, MPI, MPI_Comm_rank(fd->comm, &myrank);); (144, MPI, MPI_Allgather(&start_offset, 1, ADIO_OFFSET, st_offsets, 1, ADIO_OFFSET, fd->comm);); (146, MPI, MPI_Allgather(&count_sizes, 1, ADIO_OFFSET, count_sizes, 1, ADIO_OFFSET, fd->comm);); (361, MPI, MPI_Type_size_x(datatype, &size);); (93, MPI, MPI_Type_size_x(datatype, &buftype_size);)", "label": "(141, MPI, MPI_Allgather(&end_offset, 1, ADIO_OFFSET, end_offsets, 1, ADIO_OFFSET, fd->comm);); (59, MPI, MPI_Comm_rank(fd->comm, &myrank);); (138, MPI, MPI_Allgather(&start_offset, 1, ADIO_OFFSET, st_offsets, 1, ADIO_OFFSET, fd->comm);); (395, MPI, MPI_Type_size_x(datatype, &size);); (94, MPI, MPI_Type_size_x(datatype, &buftype_size);)"}
{"predict": "(41, OpenMP, lis2mdl_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)", "label": "(41, OpenMP, lis2mdl_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)"}
{"predict": "(93, OpenMP, __kmpc_omp_task(&loc, gtid, task);); (100, OpenMP, kmp_gomp_depends_info_t gomp_depends(depend);); (70, OpenMP, kmp depend(in : ndeps, dep_list, ndeps_cnv, dep_list, ndeps_cnv)); (72, OpenMP, __kmpc_omp_task_with_deps(&loc, gtid, task, ndeps, dep_list, ndeps_cnv);); (63, OpenMP, kmp_gomp_depends_info_t gomp_depends;); (72, OpenMP, __kmpc_omp_task_with_deps(&loc, gtid, task, 0, NULL, 0);)", "label": "(67, OpenMP, kmp_gomp_depends_info_t gomp_depends(depend);); (100, OpenMP, __kmpc_omp_wait_deps(&loc, gtid, ndeps, dep_list, 0, NULL);); (74, OpenMP, __kmpc_omp_task(&loc, gtid, task);); (71, OpenMP, __kmpc_omp_task_with_deps(&loc, gtid, task, ndeps_cnv, dep_list, 0, NULL);)"}
{"predict": "(11, MPI, MPI_Comm c_comm = MPI_Comm_f2c(*comm);)", "label": "(10, MPI, MPI_Comm c_comm = MPI_Comm_f2c(*comm);)"}
{"predict": "(187, OpenMP, kernel_endtime=omp_get_wtime();); (183, OpenMP, transfer_endtime=omp_get_wtime();); (70, OpenMP, #pragma omp target enter data map(to: m_gate[0:nCells], Vm[0:nCells])); (213, OpenMP, #pragma omp target enter data map(to: m_gate[0:nCells], Vm[0:nCells]) if (nCells>40)); (226, OpenMP, #pragma omp target enter data map(to: m_gate[0:nCells], Vm[0:nCells]) if (nCells>10)); (71, OpenMP, #pragma omp target teams distribute parallel for); (169, OpenMP, kernel_starttime=omp_get_wtime();); (214, OpenMP, #pragma omp target enter data map(to: m_gate[0:nCells], Vm[0:nCells]) if (nCells>20)); (105, OpenMP, #pragma omp target teams distribute parallel for reduction(+:sum1) reduction(+:sum2)); (223, OpenMP, #pragma omp target enter data map(to: m_gate[0:nCells], Vm[0:nCells]) if (nCells>40)); (226, OpenMP, #pragma omp target enter data map(to: m_gate[0:nCells], Vm[0:nCells]) if (nCells>10)); (101, OpenMP, #pragma omp target teams distribute parallel for)", "label": "(162, OpenMP, #pragma omp target exit data map(delete:Vm[:nCells])); (66, OpenMP, #pragma omp target enter data map(to: m_gate[:nCells])); (149, OpenMP, #pragma omp target update from (m_gate[0:nCells])); (65, OpenMP, #pragma omp target enter data map(to: Vm[:nCells])); (102, OpenMP, #pragma omp target teams distribute parallel for); (156, OpenMP, #pragma omp target exit data map(delete:m_gate[:nCells]))"}
{"predict": "(6, MPI, MPI_Type_get_attr(type, key[i], &val_p, &flag);)", "label": "(6, MPI, MPI_Type_get_attr(type, key[i], &val_p, &flag);)"}
{"predict": "(7, OpenMP, #pragma omp parallel for)", "label": "(7, OpenMP, #pragma omp for nowait schedule(runtime))"}
{"predict": "(9, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(9, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(6, CUDA, extern __shared__ float s_data[];)", "label": "(6, CUDA, extern __shared__ float s_data[];)"}
{"predict": "(10, MPI, MPI_Info_create(&infos[i]);); (25, MPI, MPI_Info_get(infos[0], key, 64, value, 64);); (56, MPI, MPI_Info_get_string(infos[i], key, &flag, valstr, 64);); (73, MPI, MPI_Info_free(&infos[i]);); (34, MPI, MPI_Info_get_nkeys(infos[i]);); (18, MPI, MPI_Info_set(infos[i], key, value);)", "label": "(10, MPI, MPI_Info_create(&infos[i]);); (57, MPI, MPI_Info_get(infos[i], key, sizeof(value), value, &flag);); (47, MPI, MPI_Info_get_nthkey(infos[i], j, key);); (22, MPI, MPI_Info_get_nkeys(infos[0], &nkeys);); (33, MPI, MPI_Info_get_nkeys(infos[i], &nkeys);); (71, MPI, MPI_Info_free(&infos[i]);)"}
{"predict": "(5, OpenMP, #pragma omp parallel for)", "label": "(5, OpenMP, #pragma omp parallel for)"}
{"predict": "(13, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (9, MPI, MPI_Reduce_scatter_block(sendbuf, recvbuf, count, datatype, MPI_SUM);); (11, MPI, MPI_Ireduce_scatter_block(sendbuf, recvbuf, count, datatype, MPI_SUM, &req);)", "label": "(14, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (12, MPI, MPI_Ireduce_scatter_block_c(sendbuf, recvbuf, count, datatype, MPI_SUM, comm, &req);); (8, MPI, MPI_Reduce_scatter_block_c(sendbuf, recvbuf, count, datatype, MPI_SUM, comm);)"}
{"predict": "(30, CUDA, checkCudaErrors(cudaGetDeviceCount(&GPU_N));); (220, CUDA, checkCudaErrors(cufftXtFree(d_out_filter_kernel));); (120, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, whichGPUs[i]));); (217, CUDA, checkCudaErrors(cufftXtFree(d_out_signal));); (101, CUDA, checkCudaErrors(cufftCreate(&plan_input));); (42, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, i));); (211, CUDA, checkCudaErrors(cufftXtFree(d_signal));); (217, CUDA, checkCudaErrors(cufftDestroy(plan_input));); (211, CUDA, checkCudaErrors(cufftXtFree(d_filter_kernel));)", "label": "(29, CUDA, checkCudaErrors(cudaGetDeviceCount(&GPU_N));); (219, CUDA, checkCudaErrors(cufftXtFree(d_out_filter_kernel));); (120, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, whichGPUs[i]));); (216, CUDA, checkCudaErrors(cufftXtFree(d_out_signal));); (100, CUDA, checkCudaErrors(cufftCreate(&plan_input));); (40, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, i));); (211, CUDA, checkCudaErrors(cufftXtFree(d_signal));); (216, CUDA, checkCudaErrors(cufftDestroy(plan_input));); (210, CUDA, checkCudaErrors(cufftXtFree(d_filter_kernel));)"}
{"predict": "(62, MPI, err = MPI_Win_fence(0, win);); (60, MPI, MPI_Win_set_errhandler(win, MPI_ERRORS_RETURN);); (56, MPI, MPI_Win_create(recvBuf, totsize * sizeof(int), extent, MPI_INFO_NULL, comm, &win);); (27, MPI, MPI_Type_commit(&originType);); (25, MPI, MPI_Type_indexed(2, counts, displs, MPI_INT, &originType);); (56, MPI, MPI_Win_create(srcBuf, (totsize + 1) * sizeof(int), extent, MPI_INFO_NULL, comm, &win);); (25, MPI, MPI_Comm_size(comm, &size);); (56, MPI, MPI_Win_fence(0, win);); (24, MPI, MPI_Comm_rank(comm, &rank);); (55, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, dest, 0, win);); (55, MPI, MPI_Win_unlock(source, win);); (54, MPI, MPI_Put(srcBuf, 1, originType, dest, 0, totsize, MPI_INT, win);); (55, MPI, MPI_Barrier(comm);); (20, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (38, MPI, MPI_Alloc_mem(totsize * sizeof(int), MPI_INFO_NULL, &recvBuf);); (38, MPI, MPI_Alloc_mem((totsize + 1) * sizeof(int), MPI_INFO_NULL, &srcBuf);); (54, MPI, MPI_Win_free(&win);); (39, MPI, MPI_Win_create(NULL, 0, 1, MPI_INFO_NULL, comm, &win);); (54, MPI, MPI_Free_mem(srcBuf);); (50, MPI, MPI_Free_mem(recvBuf);); (37, MPI, MPI_Win_fence(0, win);)", "label": "(56, MPI, MPI_Type_get_extent(MPI_INT, &tmp_lb, &extent);); (33, MPI, MPI_Comm_size(comm, &size);); (55, MPI, MPI_Win_create(recvBuf, totsize * extent, extent, MPI_INFO_NULL, comm, &win);); (24, MPI, MPI_Type_indexed(2, counts, displs, MPI_INT, &originType);); (54, MPI, MPI_Win_fence(0, win);); (23, MPI, MPI_Type_commit(&originType);); (77, MPI, MPI_Win_free(&win);); (75, MPI, MPI_Type_free(&originType);); (58, MPI, err = MPI_Win_fence(0, win);); (53, MPI, MPI_Win_set_errhandler(win, MPI_ERRORS_RETURN);); (33, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (22, MPI, MPI_Comm_rank(comm, &rank);); (52, MPI, err = MPI_Put(srcBuf, 1, originType, dest, 0, totsize, MPI_INT, win);)"}
{"predict": "(27, OpenMP, #pragma omp target update to(S2::S2sc)); (20, OpenMP, #pragma omp target update to(S2::S2s)); (7, OpenMP, #pragma omp target update from()); (9, OpenMP, #pragma omp target update from(S1)); (14, OpenMP, #pragma omp target update from(S2::S2s)); (23, OpenMP, #pragma omp target update from(m)); (16, OpenMP, #pragma omp target update from(e, g)); (7, OpenMP, #pragma omp target update from(argc > 0? argv[1] : argv[2])); (18, OpenMP, #pragma omp target update from(i)); (15, OpenMP, #pragma omp target update from(h, B::x)); (2, OpenMP, #pragma omp target update from(); (17, OpenMP, #pragma omp target update from(j)); (-2, OpenMP, #pragma omp target update from); (3, OpenMP, #pragma omp target update from(S2::S2sc)); (14, OpenMP, #pragma omp target update from(m)); (5, OpenMP, #pragma omp target update from(ca, z)); (6, OpenMP, #pragma omp target update from(da)); (-6, OpenMP, #pragma omp target update from); (7, OpenMP, #pragma omp target update from(S2::S2s)); (13, OpenMP, #pragma omp target update to(i)); (5, OpenMP, #pragma omp target update from(S2::S2sc)); (0, OpenMP, #pragma omp target update from(argv[1])); (-11, OpenMP, #pragma omp target update from); (2, OpenMP, #pragma omp target update from(a, b, c, d, f))", "label": "(8, OpenMP, #pragma omp task private(); (17, OpenMP, #pragma omp task private(ca)); (20, OpenMP, #pragma omp task private(threadvar, B::x)); (10, OpenMP, #pragma omp task private(argc argv)); (17, OpenMP, #pragma omp task private(e, g)); (31, OpenMP, #pragma omp task private(m)); (1, OpenMP, #pragma omp task private); (9, OpenMP, #pragma omp task private(argv[1])); (1, OpenMP, #pragma omp task private()); (19, OpenMP, #pragma omp task private(j)); (4, OpenMP, #pragma omp task private(S1)); (0, OpenMP, #pragma omp task private(argc,); (5, OpenMP, #pragma omp task private(ba) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (7, OpenMP, #pragma omp task private(S2::S2s)); (5, OpenMP, #pragma omp task private(da)); (-3, OpenMP, #pragma omp task private(argc > 0? argv[1] : argv[2])); (11, OpenMP, #pragma omp task private(i)); (6, OpenMP, #pragma omp task shared(i), private(i)); (14, OpenMP, #pragma omp task private(i) allocate(omp_thread_mem_alloc: i)); (6, OpenMP, #pragma omp task firstprivate(i) private(i)); (10, OpenMP, #pragma omp task firstprivate(i)); (-11, OpenMP, #pragma omp task private(argc); (-7, OpenMP, #pragma omp task private(z, a, b, c, d, f))"}
{"predict": "(26, OpenMP, omp_target_free(d_a, omp_get_default_device());); (19, OpenMP, #pragma omp target update to(p_a[0:N])); (10, OpenMP, d_a = (int *)omp_target_alloc(DataSize, omp_get_default_device());); (21, OpenMP, omp_target_disassociate_ptr(p_a, omp_get_default_device());); (12, OpenMP, omp_target_associate_ptr(p_a, d_a, DataSize, 0, omp_get_default_device());); (17, OpenMP, omp_target_free(p_a, omp_get_default_device());)", "label": "(25, OpenMP, omp_target_free(d_a, omp_get_default_device());); (12, OpenMP, d_a = (int *)omp_target_alloc(DataSize, omp_get_default_device());); (18, OpenMP, #pragma omp target update to(p_a)); (20, OpenMP, omp_target_disassociate_ptr(p_a, omp_get_default_device());); (11, OpenMP, omp_target_associate_ptr(p_a, d_a, DataSize, 0, omp_get_default_device());)"}
{"predict": "(25, MPI, err = MPI_Type_create_indexed_block(count, 0, disp, MPI_INT, &vectype);); (27, MPI, MPI_Type_size(MPI_INT, &int_size);); (72, MPI, MPI_Type_free(&newtype);); (30, MPI, err = MPI_Type_commit(&newtype);); (41, MPI, err = MPI_Type_size(newtype, &size);); (57, MPI, MPI_Type_free(&vectype);); (28, MPI, MPI_Type_indexed_block(count, 0, disp, vectype, &newtype);)", "label": "(35, MPI, err = MPI_Type_create_indexed_block(count, 1, disp, vectype, &newtype);); (42, MPI, MPI_Type_size(MPI_INT, &int_size);); (58, MPI, MPI_Type_commit(&newtype);); (24, MPI, err = MPI_Type_vector(2, 1, 2, MPI_INT, &vectype);); (41, MPI, err = MPI_Type_size(newtype, &size);); (74, MPI, MPI_Type_free(&newtype);); (72, MPI, MPI_Type_free(&vectype);)"}
{"predict": "(36, OpenCL, retVal = clEnqueueNDRangeKernel(pCmdQ, oneWorkItem, 1, nullptr, &workItem, nullptr, 0, nullptr, &retVal);); (29, OpenCL, retVal = clWaitForEvents(1, &retEvent);)", "label": "(44, OpenCL, clReleaseEvent(retEvent);)"}
{"predict": "(18, CUDA, EXPECT_EQ(cudaEventQuery(wm.ready), cudaErrorNotReady);); (21, CUDA, EXPECT_EQ(cudaEventSynchronize(wm.ready), cudaErrorNotReady);); (20, CUDA, EXPECT_EQ(cudaEventElapsed(&wm.time_elapsed_ms, wmsync.begin(), wmsync.end()), cudaErrorNotReady);); (12, CUDA, cudaEventRecord(wm.ready, 0);); (13, CUDA, EXPECT_EQ(cudaEventQuery(wm.ready), cudaErrorNotReady);); (17, CUDA, EXPECT_EQ(cudaEventRecord(wmsync.begin(), 0), cudaSuccess);); (26, CUDA, EXPECT_EQ(cudaEventSynchronize(wm.release), cudaErrorNotReady);); (24, CUDA, EXPECT_EQ(cudaEventRecord(wm.release, 0), cudaSuccess);); (12, CUDA, EXPECT_EQ(cudaEventRecord(wmsync.end(), 0), cudaSuccess);); (14, CUDA, EXPECT_EQ(cudaEventElapsedTime(&wm.time_elapsed_ms, wmsync.begin(), wmsync.end()), cudaErrorNotReady);)", "label": "(24, CUDA, ASSERT_EQ(cudaEventRecord(wm.ready, 0), cudaSuccess);); (25, CUDA, cvcuda::WorkspaceMemAllocator wa(wm, cudaStream_t(0));); (51, CUDA, EXPECT_EQ(cudaEventQuery(wm.ready), cudaSuccess);); (1, CUDA, ASSERT_EQ(cudaMalloc(&_junk, junk_size), cudaSuccess);); (14, CUDA, ASSERT_EQ(cudaMemset(junk.get(), i, junk_size), cudaSuccess);); (6, CUDA, ASSERT_EQ(cudaEventCreateWithFlags(&wm.ready, cudaEventDisableTiming), cudaSuccess);); (26, CUDA, ASSERT_EQ(cudaDeviceSynchronize(), cudaSuccess);); (50, CUDA, ASSERT_EQ(cudaEventDestroy(wm.ready), cudaSuccess);); (19, CUDA, EXPECT_EQ(cudaEventQuery(wm.ready), cudaErrorNotReady);)"}
{"predict": "(67, CUDA, CHECK_CUDA(cudaMalloc(&dy, dataSize * sizeof(int32_t)));); (102, CUDA, CHECK_CUDA(cudaFree(dUpdates));); (64, CUDA, CHECK_CUDA(cudaMalloc(&dUpdates, updatesSize * sizeof(int32_t)));); (96, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (39, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (99, CUDA, CHECK_CUDA(cudaFree(dy));); (38, CUDA, CHECK_HIEDNN(hiednnSetCudaStream(handle, 0));); (97, CUDA, CHECK_CUDA(cudaFree(dIndices));); (59, CUDA, CHECK_CUDA(cudaMalloc(&dx, dataSize * sizeof(int32_t)));); (59, CUDA, CHECK_CUDA(cudaMalloc(&dIndices, indicesSize * sizeof(int32_t)));); (93, CUDA, CHECK_CUDA(cudaFree(dx));); (89, CUDA, CHECK_HIEDNN(hiednnDestroyCudaStream(handle));); (88, CUDA, CHECK_CUDA(cudaFree(dy));)", "label": "(65, CUDA, CHECK_CUDA(cudaMalloc(&dy, dataSize * sizeof(int32_t)));); (100, CUDA, CHECK_CUDA(cudaFree(dUpdates));); (62, CUDA, CHECK_CUDA(cudaMalloc(&dUpdates, updatesSize * sizeof(int32_t)));); (94, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (37, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (97, CUDA, CHECK_CUDA(cudaFree(dy));); (56, CUDA, CHECK_CUDA(cudaMalloc(&dx, dataSize * sizeof(int32_t)));); (93, CUDA, CHECK_CUDA(cudaFree(dIndices));); (36, CUDA, CHECK_HIEDNN(hiednnSetCudaStream(handle, 0));); (90, CUDA, CHECK_CUDA(cudaFree(dx));); (53, CUDA, CHECK_CUDA(cudaMalloc(&dIndices, indicesSize * sizeof(int32_t)));)"}
{"predict": "(35, TBB, tbb::flow::source_node<T> src_copy(src);); (27, TBB, tbb::flow::function_node<T,bool> dest2(g, tbb::flow::unlimited, b2);); (13, TBB, tbb::flow::source_node<T> src3(g, source_body<T>() );); (13, TBB, tbb::flow::function_node<T,bool> dest3(g, tbb::flow::serial, b3);); (14, TBB, tbb::flow::make_edge( src3, dest3 );); (39, TBB, tbb::flow::remove_edge( src, dest );); (40, TBB, tbb::flow::make_edge( src_copy, dest_c );); (0, TBB, tbb::flow::make_edge( src, dest );); (21, TBB, tbb::flow::make_edge( src2, dest2 );); (-6, TBB, tbb::flow::graph g;); (6, TBB, tbb::flow::make_edge( src3, dest );); (-6, TBB, tbb::flow::source_node<T> src(g, source_body<T>() );)", "label": "(28, TBB, tbb::flow::input_node<T> src2(g, my_input_body<T>() );); (18, TBB, tbb::flow::function_node<T,bool> dest3(g, tbb::flow::unlimited, b3 );); (31, TBB, tbb::flow::function_node<T,bool,tbb::flow::rejecting> dest2(g, tbb::flow::serial, b2 );); (3, TBB, tbb::flow::make_edge( src, dest );); (0, TBB, tbb::flow::input_node<T> src(g, my_input_body<T>() );); (29, TBB, tbb::flow::make_edge( src2, dest2 );); (36, TBB, tbb::flow::input_node<T> src_copy(src);); (13, TBB, tbb::flow::make_edge( src3, dest3 );); (-5, TBB, tbb::flow::graph g;); (6, TBB, tbb::flow::input_node<T> src3(g, my_input_body<T>() );)"}
{"predict": "(26, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&output_data, output_size_bytes));); (18, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&input_data, input_size_bytes));); (49, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(output_data));); (40, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (38, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (45, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(input_data));)", "label": "(26, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&output_data, output_size_bytes));); (18, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&input_data, input_size_bytes));); (48, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(output_data));); (39, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (37, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (44, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(input_data));)"}
{"predict": "(84, CUDA, cudaFuncGetAttributes(&cuattr, slsum_fmod_inv_gpu_mrhs_nvshmem);); (84, CUDA, cudaOccupancyMaxPotentialBlockSize(&minGridSize,&myblockSize,(const void *) swait_bcrd,0,0 );); (83, CUDA, cudaDeviceSetLimit(cudaLimitStackSize, cuattr.localSizeBytes);); (84, CUDA, cudaOccupancyMaxPotentialBlockSize(&minGridSize,&myblockSize,(const void *) send_rd,0,0 );); (79, CUDA, cudaStreamCreateWithFlags(&stream[i], cudaStreamNonBlocking);); (84, CUDA, cudaDeviceSetLimit(cudaLimitStackSize, cuattr.localSizeBytes);); (83, CUDA, cudaOccupancyMaxPotentialBlockSize(&minGridSize,&myblockSize,(const void *) check_shmem,0,0 );); (81, CUDA, cudaOccupancyMaxPotentialBlockSize(&minGridSize,&myblockSize,(const void *) check_shmemx,0,0 );); (82, CUDA, cudaOccupancyMaxPotentialBlockSize(&minGridSize,&myblockSize,(const void *) send_shmem,0,0 );); (77, CUDA, cudaStreamSetLimit(stream[i], cudaStreamLimitStackSize, cuattr.localSizeBytes);); (81, CUDA, cudaOccupancyMaxPotentialBlockSize(&minGridSize,&myblockSize,(const void *) check_shmemx_count,0,0 );); (78, CUDA, cudaOccupancyMaxPotentialBlockSize(&minGridSize,&myblockSize,(const void *) check_shmem_count,0,0 );); (77, CUDA, cudaOccupancyMaxPotentialBlockSize(&minGridSize,&myblockSize,(const void *) check_shmem_bc_dat,0,0 );); (75, CUDA, cudaOccupancyMaxPotentialBlockSize(&minGridSize,&myblockSize,(const void *) check_shmem_bc_offset,0,0 );); (81, CUDA, cudaStreamSynchronize(stream[0]);); (75, CUDA, cudaOccupancyMaxPotentialBlockSize(&minGridSize,&myblockSize,(const void *) check_shmem_bc_ptr,0,0 );); (78, CUDA, cudaStreamSynchronize(stream[1]);); (76, CUDA, cudaOccupancyMaxPotentialBlockSize(&minGridSize,&myblockSize,(const void *) check_shmem_rd_dat,0,0 );); (76, CUDA, cudaOccupancyMax", "label": "(85, CUDA, cudaDeviceSetLimit(cudaLimitStackSize, cuattr.localSizeBytes);); (140, CUDA, CUDA_CHECK(cudaDeviceSynchronize());); (78, CUDA, cudaStreamCreateWithFlags(&stream[i], cudaStreamNonBlocking);); (135, CUDA, CUDA_CHECK(cudaGetLastError());); (80, CUDA, cudaFuncGetAttributes(&cuattr, slsum_fmod_inv_gpu_mrhs_nvshmem);); (138, CUDA, CUDA_CHECK(cudaStreamDestroy(stream[i]));); (82, CUDA, cudaOccupancyMaxPotentialBlockSize(&minGridSize,&myblockSize,(const void *) swait_bcrd,0,0 );)"}
{"predict": "(2, CUDA, __shared__ unsigned int s_data[32];); (2, CUDA, __shared__ unsigned int d_data[32];)", "label": "(2, CUDA, __shared__ volatile unsigned int s_data[WARP_SIZE*2];)"}
{"predict": "(29, OpenCL, error |= clSetKernelArg(kernel, 1, sizeof(dst), &dst);); (27, OpenCL, error |= clSetKernelArg(kernel, 0, sizeof(value), &value);)", "label": "(29, OpenCL, error |= clSetKernelArg(kernel, 1, sizeof(value), &value);); (27, OpenCL, error |= clSetKernelArg(kernel, 0, sizeof(dst), &dst);)"}
{"predict": "(8, MPI, tim::settings::mpi_init()       = false;); (8, MPI, tim::settings::mpi_finalize()   = false;)", "label": "(8, MPI, tim::settings::mpi_init()       = false;); (8, MPI, tim::settings::mpi_finalize()   = false;)"}
{"predict": "(20, OpenMP, #pragma omp parallel); (0, OpenMP, int nThreads = omp_get_max_threads();)", "label": "(19, OpenMP, #pragma omp parallel); (0, OpenMP, int nThreads = omp_get_max_threads();)"}
{"predict": "(9, TBB, oneapi::tbb::parallel_for( oneapi::tbb::blocked_range<std::size_t>(0,10000*s,s), GrowToAtLeast<MyVector>(use_two_arg_form, v, copy_from), oneapi::tbb::simple_partitioner() );); (6, TBB, using MyVector = oneapi::tbb::concurrent_vector<move_support_tests::Foo, MyAllocator>;)", "label": "(4, TBB, using MyVector = oneapi::tbb::concurrent_vector<move_support_tests::Foo, MyAllocator>;); (8, TBB, oneapi::tbb::parallel_for(oneapi::tbb::blocked_range<std::size_t>(0, 10000*s, s), GrowToAtLeast<MyVector>(use_two_arg_form, v, copy_from), oneapi::tbb::simple_partitioner());)"}
{"predict": "(108, MPI, MPI_Group_compare(MPI_GROUP_EMPTY, g6, &result);); (78, MPI, MPI_Group_free(&g1);); (80, MPI, MPI_Group_free(&g45);); (78, MPI, MPI_Group_free(&g4);); (20, MPI, MPI_Group_incl(g1, 3, ranks, &g2);); (55, MPI, MPI_Group_translate_ranks(selfgroup, 1, rin, g1, rout);); (74, MPI, MPI_Group_excl(g1, g1size, lranks, &g6);); (135, MPI, MPI_Group_free(&selfgroup);); (0, MPI, MPI_Comm_group(MPI_COMM_WORLD, &selfgroup);); (14, MPI, MPI_Comm_set_group(MPI_COMM_WORLD, g1);); (0, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (35, MPI, MPI_Group_translate_ranks(selfgroup, 1, rin, g2, rout);); (134, MPI, MPI_Group_free(&g6);); (41, MPI, MPI_Group_translate_ranks(selfgroup, 1, rin, g3, rout);); (133, MPI, MPI_Group_free(&g7);); (23, MPI, MPI_Group_free(&g2);); (4, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (-7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myrank);); (59, MPI, MPI_Group_free(&g3);); (11, MPI, MPI_Group_union(g4, g5, &g45);); (26, MPI, MPI_Group_free(&g45);); (15, MPI, MPI_Group_free(&g7);); (46, MPI, MPI_Group_compare(MPI_GROUP_EMPTY, g1, &result);); (-2, MPI, MPI_Group_incl(g2, 1, ranks, &g7);)", "label": "(84, MPI, MPI_Group_excl(g1, g1size, lranks, &g6);); (27, MPI, MPI_Group_size(g2, &size);); (41, MPI, MPI_Group_translate_ranks(g2, 3, rin, g1, rout);); (70, MPI, MPI_Group_free(&selfgroup);); (105, MPI, MPI_Group_free(&g2);); (24, MPI, MPI_Group_rank(g2, &rank);); (117, MPI, MPI_Group_compare(MPI_GROUP_EMPTY, g4, &result);); (123, MPI, MPI_Group_free(&g45);); (113, MPI, MPI_Group_union(g4, g5, &g45);); (105, MPI, MPI_Group_range_excl(g1, 1, range, &g5);); (109, MPI, MPI_Group_range_incl(g1, 1, range, &g4);); (118, MPI, MPI_Group_free(&g5);); (-1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (12, MPI, MPI_Group_incl(g1, 3, ranks, &g2);); (114, MPI, MPI_Group_free(&g4);); (140, MPI, MPI_Group_free(&g1);); (72, MPI, MPI_Group_free(&g6);); (62, MPI, MPI_Group_size(g1, &g1size);); (34, MPI, MPI_Comm_group(MPI_COMM_SELF, &selfgroup);); (-10, MPI, MPI_Comm_group(MPI_COMM_WORLD, &g1);); (-10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myrank);); (41, MPI, MPI_Group_translate_ranks(g1, size, rin, selfgroup, rout);); (32, MPI, MPI_Group_translate_ranks(selfgroup, 1, rin, g1, rout);)"}
{"predict": "(1, MPI, MPI_Init(&argc, &argv);); (59, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &P);)", "label": "(1, MPI, MPI_Init(&argc, &argv);); (58, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &P);)"}
{"predict": "(6, CUDA, extern __shared__ __align__(sizeof(float)) uchar s[];)", "label": "(7, CUDA, extern __shared__ __align__(sizeof(float)) uchar s[];)"}
{"predict": "(26, OpenMP, #pragma omp parallel sections); (18, OpenMP, #pragma omp sections); (5, OpenMP, #pragma omp cancel sections); (5, OpenMP, #pragma omp section); (0, OpenMP, #pragma omp cancel for); (20, OpenMP, #pragma omp parallel for); (30, OpenMP, #pragma omp for); (-8, OpenMP, #pragma omp cancel); (-7, OpenMP, #pragma omp cancel(); (-8, OpenMP, #pragma omp cancel +); (-6, OpenMP, #pragma omp cancel()); (13, OpenMP, #pragma omp cancel for)", "label": "(5, OpenMP, #pragma omp barrier); (23, OpenMP, #pragma omp task); (18, OpenMP, #pragma omp for); (0, OpenMP, #pragma omp cancel parallel if(flag)); (4, OpenMP, #pragma omp sections); (7, OpenMP, #pragma omp section); (22, OpenMP, #pragma omp parallel sections); (-6, OpenMP, #pragma omp parallel); (18, OpenMP, #pragma omp cancel taskgroup); (32, OpenMP, #pragma omp parallel for reduction(+: r)); (0, OpenMP, #pragma omp cancel sections); (11, OpenMP, #pragma omp cancel for)"}
{"predict": "(31, OpenCL, clReleaseMemObject(buffer);)", "label": "(31, OpenCL, clReleaseMemObject(buffer);)"}
{"predict": "(12, TBB, tbb::parallel_for(tbb::blocked_range<size_t>(0,n,n/10),AddOne<Counter<M> >(counter));); (9, TBB, tbb::tick_count t1 = tbb::tick_count::now();)", "label": "(5, TBB, tbb::profiling::set_name(counter.mutex, name);); (11, TBB, tbb::parallel_for(tbb::blocked_range<size_t>(0,n,n/10),AddOne<Counter<M> >(counter));); (11, TBB, tbb::tick_count t1 = tbb::tick_count::now();)"}
{"predict": "(232, MPI, MPI_Finalize();); (35, MPI, MPI_Init_thread( &argc, &argv, MPI_THREAD_MULTIPLE, &omp_mpi_level);)", "label": "(232, MPI, MPI_Finalize();); (35, MPI, MPI_Init_thread( &argc, &argv, MPI_THREAD_MULTIPLE, &omp_mpi_level);)"}
{"predict": "(29, OpenMP, #pragma omp teams distribute parallel for simd linear(argc > 0? argv[1] : argv[2])); (45, OpenMP, #pragma omp teams distribute parallel for simd linear(argv[1])); (50, OpenMP, #pragma omp teams distribute parallel for simd linear(e, g)); (53, OpenMP, #pragma omp teams distribute parallel for simd linear(i)); (34, OpenMP, #pragma omp teams distribute parallel for simd linear(a, b:B::ib)); (27, OpenMP, #pragma omp teams distribute parallel for simd linear(argc : 5) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (18, OpenMP, #pragma omp teams distribute parallel for simd linear(argc,); (0, OpenMP, #pragma omp teams distribute parallel for simd linear); (29, OpenMP, #pragma omp teams distribute parallel for simd linear(S1)); (51, OpenMP, #pragma omp teams distribute parallel for simd linear(j)); (4, OpenMP, #pragma omp teams distribute parallel for simd linear(); (9, OpenMP, #pragma omp teams distribute parallel for simd linear(argc); (34, OpenMP, #pragma omp teams distribute parallel for simd linear(h, C::x)); (1, OpenMP, #pragma omp teams distribute parallel for simd linear()); (-10, OpenMP, #pragma omp target); (9, OpenMP, #pragma omp teams distribute parallel for simd linear(argc,)", "label": "(34, OpenMP, #pragma omp teams distribute parallel for simd linear (argc : 5) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (19, OpenMP, #pragma omp teams distribute parallel for simd linear (argc); (27, OpenMP, #pragma omp teams distribute parallel for simd linear (argc > 0? argv[1] : argv[2])); (43, OpenMP, #pragma omp teams distribute parallel for simd linear (argv[1])); (38, OpenMP, #pragma omp teams distribute parallel for simd linear (a, b:B::ib)); (55, OpenMP, #pragma omp teams distribute parallel for simd linear(i)); (19, OpenMP, #pragma omp teams distribute parallel for simd linear (argc,); (8, OpenMP, #pragma omp teams distribute parallel for simd linear ()); (30, OpenMP, #pragma omp teams distribute parallel for simd linear (S1)); (-2, OpenMP, #pragma omp teams distribute parallel for simd linear); (41, OpenMP, #pragma omp teams distribute parallel for simd linear(e, g)); (-5, OpenMP, #pragma omp target); (-1, OpenMP, #pragma omp teams distribute parallel for simd linear (); (42, OpenMP, #pragma omp teams distribute parallel for simd linear(h))"}
{"predict": "(135, MPI, MPI_Bcast(&size2,      1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (135, MPI, MPI_Bcast(&radius,     1, MPI_INT,           root, MPI_COMM_WORLD);); (297, MPI, MPI_Finalize();); (130, MPI, MPI_Bcast(&lsize2,     1, MPI_INT,           root, MPI_COMM_WORLD);); (128, MPI, MPI_Bcast(&lsize,      1, MPI_INT,           root, MPI_COMM_WORLD);); (129, MPI, MPI_Bcast(&size,       1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (130, MPI, MPI_Bcast(&iterations, 1, MPI_INT,           root, MPI_COMM_WORLD);); (125, MPI, MPI_Bcast(&nent,     1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (131, MPI, MPI_Bcast(&jstart,     1, MPI_INT,           root, MPI_COMM_WORLD);); (132, MPI, MPI_Bcast(&jend,     1, MPI_INT,           root, MPI_COMM_WORLD);); (293, MPI, MPI_Reduce(&vector_sum, &check_sum, 1, MPI_DOUBLE, MPI_SUM, root, MPI_COMM_WORLD);); (42, MPI, MPI_Init(&argc,&argv);); (226, MPI, MPI_Barrier(MPI_COMM_WORLD);); (42, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (129, MPI, MPI_Bcast(&nrows,     1, MPI_INT,           root, MPI_COMM_WORLD);); (40, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)", "label": "(134, MPI, MPI_Bcast(&size2,      1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (134, MPI, MPI_Bcast(&radius,     1, MPI_INT,           root, MPI_COMM_WORLD);); (316, MPI, MPI_Finalize();); (129, MPI, MPI_Bcast(&lsize2,     1, MPI_INT,           root, MPI_COMM_WORLD);); (127, MPI, MPI_Bcast(&lsize,      1, MPI_INT,           root, MPI_COMM_WORLD);); (128, MPI, MPI_Bcast(&size,       1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (290, MPI, MPI_Reduce(&vector_sum, &check_sum, 1, MPI_DOUBLE, MPI_SUM, root, MPI_COMM_WORLD);); (41, MPI, MPI_Init(&argc,&argv);); (243, MPI, MPI_Barrier(MPI_COMM_WORLD);); (41, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (126, MPI, MPI_Bcast(&iterations, 1, MPI_INT,           root, MPI_COMM_WORLD);); (38, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(15, OpenMP, #pragma omp target map(tofrom: Inner, Outer, wayout, A)); (18, OpenMP, #pragma omp parallel for); (16, OpenMP, #pragma omp master); (20, OpenMP, #pragma omp atomic)", "label": "(18, OpenMP, #pragma omp parallel for); (14, OpenMP, #pragma omp target map(tofrom: Inner, Outer, wayout)); (14, OpenMP, #pragma omp teams num_teams(1  ) thread_limit(tl))"}
{"predict": "(6, OpenMP, #pragma omp parallel for firstprivate(length, v_cut))", "label": "(6, OpenMP, #pragma omp parallel for firstprivate(length, v_cut))"}
{"predict": "(172, MPI, MPI_Barrier(comm);); (22, MPI, MPI_Comm_rank(comm, &par_rank);); (22, MPI, MPI_Comm_size(comm, &par_size);)", "label": "(166, MPI, opts[\"mpi_comm\"] = MPI_Comm_c2f(MPI_COMM_WORLD);); (171, MPI, MPI_Barrier(comm);); (21, MPI, MPI_Comm_rank(comm, &par_rank);); (21, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (20, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (51, MPI, MPI_Finalize();); (48, MPI, MPI_T_finalize();); (19, MPI, MPI_T_init_thread(required, &provided);); (33, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (20, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (51, MPI, MPI_Finalize();); (48, MPI, MPI_T_finalize();); (19, MPI, MPI_T_init_thread(required, &provided);); (32, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(32, CUDA, cudaDeviceSynchronize();)", "label": "(101, CUDA, checkHIPErrors( ::cudaDeviceSynchronize() );)"}
{"predict": "(25, OpenMP, #pragma omp target if(Gc>0 && c>0 && Sc>0)); (31, OpenMP, #pragma omp target if(Gd>0 && d>0 && Sd>0))", "label": "(32, OpenMP, #pragma omp parallel if(Gc>0.0 && c>0 && Sc>0.0)); (22, OpenMP, #pragma omp target if(Ga>0.0 && a>0 && Sa>0.0))"}
{"predict": "(7, CUDA, __shared__ block_radix_sort_t::TempStorage temp_storage;)", "label": "(7, CUDA, __shared__ block_radix_sort_t::TempStorage temp_storage;)"}
{"predict": "(18, OpenMP, #pragma omp target teams distribute if(cond) firstprivate(fp) reduction(+:rd) map(alloc:map))", "label": "(18, OpenMP, #pragma omp target teams distribute if(cond) firstprivate(fp) reduction(+:rd) map(alloc:map))"}
{"predict": "(22, CUDA, thrust::host_vector<uint64_t> h_increments(num_increments);); (52, CUDA, thrust::device_vector<uint64_t> bins_in(NUM_ITEMS);); (50, CUDA, thrust::device_vector<uint64_t> increments_in(num_increments);); (45, CUDA, thrust::device_vector<uint64_t> counts_out(NUM_ITEMS);); (55, CUDA, thrust::fill(increments_in.begin(), increments_in.end(), 0);); (55, CUDA, thrust::fill(bins_in.begin(), bins_in.end(), 0);); (20, CUDA, thrust::host_vector<uint64_t> h_bins(NUM_ITEMS);); (43, CUDA, thrust::device_vector<uint64_t> dev_counts_out(NUM_ITEMS);); (58, CUDA, thrust::host_vector<uint64_t> h_counts = counts_out;); (40, CUDA, thrust::device_vector<uint64_t> dev_counts_in(NUM_ITEMS);)", "label": "(14, CUDA, thrust::host_vector<uint32_t> h_increments(num_increments);); (43, CUDA, thrust::device_vector<uint32_t> increments_in(num_increments);); (53, CUDA, thrust::fill(counts_out.begin(), counts_out.end(), 814920U);); (40, CUDA, thrust::device_vector<uint32_t> bins_in(num_increments);); (41, CUDA, thrust::device_vector<uint32_t> counts_out(NUM_ITEMS);); (47, CUDA, thrust::host_vector<uint32_t> host_counts(num_increments);); (7, CUDA, thrust::host_vector<uint32_t> h_bins(num_increments);)"}
{"predict": "(20, OpenMP, #pragma omp target update from(a[:n])); (14, OpenMP, #pragma omp target map(from:a[:n]) map(to:b[:n])); (15, OpenMP, #pragma omp target data map(tofrom: a[:n], b[:n])); (18, OpenMP, #pragma omp target exit data map(from: a[:n])); (9, OpenMP, #pragma omp target enter data map(to: a[:n]))", "label": "(21, OpenMP, #pragma omp target update from(a[:n])); (12, OpenMP, #pragma omp target teams distribute parallel for map(to:b[:n])); (15, OpenMP, #pragma omp target teams distribute parallel for)"}
{"predict": "(6, CUDA, gpuErrchk(cudaPointerGetAttributes(&attributes, d_int));); (28, CUDA, gpuErrchk(cudaFree(d_int));); (1, CUDA, gpuErrchk(cudaMalloc(&d_int, sizeof(int)));)", "label": "(6, CUDA, gpuErrchk(cudaPointerGetAttributes(&attributes, d_int));); (28, CUDA, gpuErrchk(cudaFree(d_int));); (1, CUDA, gpuErrchk(cudaMalloc(&d_int, sizeof(int)));)"}
{"predict": "(2, CUDA, __shared__ float2 color;); (2, CUDA, __shared__ int isContained;); (0, CUDA, __shared__ float2 cellImagePos;)", "label": "(4, CUDA, __shared__ float3 color;); (2, CUDA, __shared__ bool isContained;); (0, CUDA, __shared__ float2 cellImagePos;)"}
{"predict": "(13, CUDA, cudaMemsetAsync(d_sums, 0, sizeof(Type) * nkeys, stream);)", "label": "(12, CUDA, cudaMemset(d_sums, 0, sizeof(Type) * nkeys * ncols);)"}
{"predict": "(12, OpenMP, omp_set_num_threads(nr_threads);); (13, OpenMP, #pragma omp parallel default(none) shared(a)); (14, OpenMP, #pragma omp sections); (7, OpenMP, omp_set_dynamic(0);); (14, OpenMP, #pragma omp section); (15, OpenMP, int thread_nr = omp_get_thread_num();)", "label": "(11, OpenMP, omp_set_num_threads(nr_threads);); (12, OpenMP, #pragma omp parallel default(none) shared(a)); (13, OpenMP, #pragma omp sections); (7, OpenMP, omp_set_dynamic(0);); (13, OpenMP, #pragma omp section); (15, OpenMP, int thread_nr = omp_get_thread_num();)"}
{"predict": "(10, CUDA, extern __shared__ uint64_t buffer[];)", "label": "(10, CUDA, extern __shared__ uint64_t buffer[];)"}
{"predict": "(2, CUDA, extern __shared__ float shared[];)", "label": "(22, CUDA, __shared__ float shared[forward_threads];)"}
{"predict": "(12, CUDA, cudaDeviceSynchronize();); (7, CUDA, cudaMalloc((void **)&sm_o, sizeof(float) * m * m);); (34, CUDA, cudaFree(sm_o);)", "label": "(12, CUDA, cudaDeviceSynchronize();); (7, CUDA, cudaMalloc((void **)&sm_o, sizeof(float) * m * m);); (34, CUDA, cudaFree(sm_o);)"}
{"predict": "(46, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (33, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_AP), sizeof(data_type) * AP.size()));); (63, CUDA, CUDA_CHECK(cudaDeviceReset());); (60, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (32, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (53, CUDA, CUDA_CHECK(cudaFree(d_AP));); (53, CUDA, CUDA_CHECK(cudaFree(d_A));); (22, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)", "label": "(46, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (32, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_AP), sizeof(data_type) * AP.size()));); (63, CUDA, CUDA_CHECK(cudaDeviceReset());); (60, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (30, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (53, CUDA, CUDA_CHECK(cudaFree(d_AP));); (53, CUDA, CUDA_CHECK(cudaFree(d_A));); (22, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(12, OpenCL, status = clSetKernelArg(kernel, 0, sizeof(cl_mem), &buf[0]);); (13, OpenCL, OCL_ASSERT(clSetKernelArg(kernel, 1, sizeof(cl_mem), &buf[1]));); (12, OpenCL, status = clSetKernelArg(kernel, 2, sizeof(cl_mem), &buf_data[1]);); (9, OpenCL, status = clSetKernelArg(kernel, 0, sizeof(cl_mem), &buf[0]);); (11, OpenCL, status = clSetKernelArg(kernel, 3, sizeof(cl_mem), &buf_data[0]);)", "label": "(40, OpenCL, clReleaseEvent(exec_event);); (11, OpenCL, profiling_queue = clCreateCommandQueue(ctx, device, CL_QUEUE_PROFILING_ENABLE, &status);); (47, OpenCL, clReleaseCommandQueue(profiling_queue);)"}
{"predict": "(83, MPI, MPI_Win_flush(target_rank, win);); (83, MPI, MPI_Barrier(comm);); (79, MPI, MPI_Win_flush(orig_rank, win);); (21, MPI, MPI_Comm_size(comm, &size);); (131, MPI, MPI_Win_unlock(target_rank, win);); (21, MPI, MPI_Comm_rank(comm, &rank);); (77, MPI, MPI_Win_lock(MPI_LOCK_SHARED, target_rank, 0, win);); (130, MPI, MPI_Win_flush_all(win);); (37, MPI, MPI_Barrier(comm);); (75, MPI, MPI_Win_lock(MPI_LOCK_SHARED, orig_rank, 0, win);); (133, MPI, MPI_Win_unlock(orig_rank, win);)", "label": "(163, MPI, MPI_Win_unlock(rank, win);); (22, MPI, MPI_Comm_size(comm, &size);); (125, MPI, MPI_Barrier(comm);); (134, MPI, MPI_Win_unlock(target_rank, win);); (135, MPI, MPI_Win_unlock_all(win);); (76, MPI, MPI_Win_lock(MPI_LOCK_SHARED, target_rank, 0, win);); (120, MPI, MPI_Win_flush_all(win);); (76, MPI, MPI_Win_lock_all(0, win);); (150, MPI, MPI_Win_lock(MPI_LOCK_SHARED, rank, 0, win);); (111, MPI, MPI_Win_flush_local_all(win);); (12, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(39, CUDA, return cudaGetLastError();)", "label": "(39, CUDA, return cudaGetLastError();)"}
{"predict": "(19, MPI, MPIT_result = MPI_T_pvar_session_create(&session);); (12, MPI, MPIT_result = MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);)", "label": "(19, MPI, MPIT_result = MPI_T_pvar_session_create(&session);); (12, MPI, MPIT_result = MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);)"}
{"predict": "(58, CUDA, checkCudaErrors(cudaBindTextureToArray(tex, cuArray, channelDesc, 0, width, height, 0, NULL, NULL));); (62, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (73, CUDA, getLastCudaError(\"Kernel execution failed\");); (32, CUDA, checkCudaErrors(cudaMalloc((void **) &dData, size));); (123, CUDA, checkCudaErrors(cudaFreeArray(cuArray));); (62, CUDA, checkCudaErrors(cudaMemcpy(hDataRef, dData, size, cudaMemcpyDeviceToHost));); (121, CUDA, checkCudaErrors(cudaFree(dData));)", "label": "(127, CUDA, checkCudaErrors(cudaFreeArray(cuArray));); (33, CUDA, checkCudaErrors(cudaMalloc((void **) &dData, size));); (56, CUDA, checkCudaErrors(cudaBindTextureToArray(tex, cuArray, channelDesc));); (63, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (71, CUDA, getLastCudaError(\"Kernel execution failed\");); (-3, CUDA, int devID = findCudaDevice(argc, (const char **) argv);); (120, CUDA, checkCudaErrors(cudaFree(dData));)"}
{"predict": "(9, MPI, MPI_Group_compare(group, wgroup, &result);); (6, MPI, MPI_File_get_group(cFile, &group);); (0, MPI, MPI_File cFile = MPI_File_f2c(*file);); (6, MPI, MPI_Comm_group(MPI_COMM_WORLD, &wgroup);); (14, MPI, MPI_Group_free(&wgroup);); (12, MPI, MPI_Group_free(&group);); (2, MPI, MPI_Group_compare(wgroup, group, &result);)", "label": "(9, MPI, MPI_Group_compare(group, wgroup, &result);); (5, MPI, MPI_File_get_group(cFile, &group);); (0, MPI, MPI_File cFile = MPI_File_f2c(*file);); (4, MPI, MPI_Comm_group(MPI_COMM_WORLD, &wgroup);); (12, MPI, MPI_Group_free(&wgroup);); (10, MPI, MPI_Group_free(&group);)"}
{"predict": "(78, CUDA, cudaCheckErrors(\"Kernel c failed\");); (95, CUDA, cudaGraphGetNodes(graph, nodes, &numNodes);); (94, CUDA, cudaGraphLaunch(FIXED, 0, 0, graph, NULL);); (65, CUDA, cudaCheckErrors(\"Event record failed\");); (74, CUDA, cudaCheckErrors(\"Kernel b failed\");); (13, CUDA, cudaStreamCreateWithFlags(&streams[i], cudaStreamNonBlocking);); (58, CUDA, cudaCheckErrors(\"Stream begin capture failed\");); (66, CUDA, cudaCheckErrors(\"Kernel d failed\");); (51, CUDA, cudaStreamSynchronize(streams[0]);); (52, CUDA, cudaCheckErrors(\"Event wait failed\");); (39, CUDA, cudaCheckErrors(\"Kernel a failed\");); (59, CUDA, cudaCheckErrors(\"Event capture failed\");); (17, CUDA, cudaHostAlloc((void**) &h_x, N * sizeof(float), cudaHostAllocDefault);); (17, CUDA, cudaHostAlloc((void**) &h_y, N * sizeof(float), cudaHostAllocDefault);); (62, CUDA, cudaCheckErrors(\"Kernel c failed\");); (39, CUDA, cudaStreamEndCapture(streams[0], &graph);); (71, CUDA, cudaCheckErrors(\"Graph instantiation failed\");); (54, CUDA, cudaStreamWaitEvent(streams[1], event1);); (25, CUDA, cudaMalloc((void**) &d_x, N * sizeof(float));); (25, CUDA, cudaMalloc((void**) &d_y, N * sizeof(float));); (45, CUDA, cudaDeviceSynchronize();); (58, CUDA, cudaStreamWaitEvent(streams[0], event2);); (13, CUDA, cudaCheckErrors(\"Stream creation failed\");); (69, CUDA, cudaGraphInstantiate(&instance, graph, NULL, NULL, 0);); (46, CUDA, cudaCheckErrors(\"Kernel b failed\");); (51, CUDA, cudaStreamBeginCapture(streams[0], cudaStreamCaptureModeGlobal);); (39, CUDA, cudaCheckErrors(\"Kernel a failed\");); (76, CUDA, cudaCheckErrors(\"Graph launch failed\");); (-6, CUDA, cudaEventCreateWithFlags(&event1, cudaEventDisableTiming);); (-6, CUDA, cudaEventCreateWithFlags(&event2, cudaEventDisableTiming););", "label": "(75, CUDA, cudaCheckErrors(\"Kernel c failed\");); (99, CUDA, cudaCheckErrors(\"Launching graph failed\");); (13, CUDA, cudaStreamCreateWithFlags(&streams[i], cudaStreamNonBlocking);); (63, CUDA, cudaCheckErrors(\"Event record failed\");); (56, CUDA, cudaCheckErrors(\"Stream begin capture failed\");); (31, CUDA, cudaMalloc((void**) &d_x, N * sizeof(float));); (44, CUDA, cudaGraphCreate(FIXME, 0);); (62, CUDA, cudaCheckErrors(\"Kernel b failed\");); (104, CUDA, cudaDeviceSynchronize();); (98, CUDA, cudaGraphGetNodes(graph, nodes, &numNodes);); (121, CUDA, cudaMemcpy(h_y, d_y, N, cudaMemcpyDeviceToHost);); (73, CUDA, cudaCheckErrors(\"Kernel d failed\");); (89, CUDA, cudaStreamSynchronize(streams[0]);); (58, CUDA, cudaStreamWaitEvent(streams[1], event1);); (72, CUDA, cudaStreamEndCapture(streams[0], &graph);); (48, CUDA, cudaCheckErrors(\"Kernel a failed\");); (64, CUDA, cudaStreamWaitEvent(streams[0], event2);); (-11, CUDA, cudaEventCreateWithFlags(&event1, cudaEventDisableTiming);); (-11, CUDA, cudaEventCreateWithFlags(&event2, cudaEventDisableTiming);); (53, CUDA, cudaCheckErrors(\"Event wait failed\");); (88, CUDA, cudaCheckErrors(\"Graph get nodes failed\");); (44, CUDA, cudaEventRecord(event1, streams[0]);); (19, CUDA, cudaMemcpy(d_y, h_y, N, cudaMemcpyHostToDevice);); (54, CUDA, cudaEventRecord(event2, streams[1]);); (16, CUDA, cudaMemcpy(d_x, h_x, N, cudaMemcpyHostToDevice);); (107, CUDA, cudaCheckErrors(\"Finishing memcpy failed\");); (11, CUDA, cudaMalloc((void**) &d_y, N * sizeof(float));); (32, CUDA, cudaStreamBeginCapture(streams[0], cudaStreamCaptureModeGlobal);); (91, CUDA, cudaGraphLaunch(instance, streams[0]);); (63, CUDA, cudaCheckErrors(\"instantiating graph failed\");); (8, CUDA, cudaCheckErrors(\"cudaMalloc failed\");); (56, CUDA, cudaCheckErrors(\"Stream end capture failed\");); (59, CUDA, cudaGraphInstantiate(FIXME, graph, NULL, NULL, 0);); (66, CUDA, cudaGraphLaunch(FIXME, streams[0]);)"}
{"predict": "(19, MPI, MPI_Reduce_local(outbuf, inbuf, count, type, op);)", "label": "(19, MPI, MPI_Reduce_local((void*)inbuf, outbuf, count, type, op);)"}
{"predict": "(35, CUDA, __shared__ _VOLATILE_ float aSmem2[8][128+4];); (33, CUDA, __shared__ _VOLATILE_ float bSmem1[8][128+4];); (34, CUDA, __shared__ _VOLATILE_ float bSmem2[8][128+4];); (30, CUDA, __shared__ _VOLATILE_ float aSmem1[8][128+4];)", "label": "(35, CUDA, __shared__ _VOLATILE_ float aSmem2[8][128+4];); (33, CUDA, __shared__ _VOLATILE_ float bSmem1[8][128+4];); (34, CUDA, __shared__ _VOLATILE_ float bSmem2[8][128+4];); (30, CUDA, __shared__ _VOLATILE_ float aSmem1[8][128+4];)"}
{"predict": "(3, CUDA, extern __shared__ unsigned sfdatx[];)", "label": "(3, CUDA, extern __shared__ unsigned sfdatx[];)"}
{"predict": "(5, MPI, MPI_Init( & argc, & argv );); (43, MPI, MPI_Finalize();)", "label": "(9, MPI, test_integration_sin<2,1,double> t1( mpi.vm()[\"hsize\"].as<double>() );); (31, MPI, test_integration_sin<2,4,double> t2( mpi.vm()[\"hsize\"].as<double>() );); (14, MPI, test_integration_sin<2,2,double> t2( mpi.vm()[\"hsize\"].as<double>() );); (2, MPI, std::cout << \"Order = \" << mpi.vm()[\"order\"].as<int>() << \" / \" << FEELPP_MESH_MAX_ORDER << \"\\n\";); (20, MPI, test_integration_sin<2,3,double> t2( mpi.vm()[\"hsize\"].as<double>() );); (37, MPI, test_integration_sin<2,5,double> t2( mpi.vm()[\"hsize\"].as<double>() );)"}
{"predict": "(13, CUDA, cudaMemcpy(pointers, cpuPointers, sizeof(uint8_t*) * part, cudaMemcpyHostToDevice);); (0, CUDA, uint8_t ** pointers = (uint8_t**)FastllmCudaMalloc(sizeof(uint8_t*) * part);); (15, CUDA, FastllmCudaFree(pointers);)", "label": "(16, CUDA, FastllmCudaFree(pointers);); (12, CUDA, cudaMemcpy(pointers, cpuPointers, sizeof(uint8_t*) * part, cudaMemcpyHostToDevice);); (6, CUDA, uint8_t ** pointers = (uint8_t**)FastllmCudaMalloc(sizeof(uint8_t*) * part);)"}
{"predict": "(6, MPI, MPI_Init(&argc, &argv);); (16, MPI, MPI_Send(buf, 10, MPI_CHAR, 3, 0, MPI_COMM_WORLD);); (64, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (16, MPI, MPI_Send(&errs, 1, MPI_INT, 3, 0, MPI_COMM_WORLD);); (22, MPI, MPI_Irecv(buf, 10, MPI_CHAR, 0, 0, MPI_COMM_WORLD, &request);); (25, MPI, MPI_Irecv(&errs, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, &request);); (16, MPI, MPI_Wait(&request, MPI_STATUS_IGNORE);); (5, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (11, MPI, MPI_Recv(&errs, 1, MPI_INT, 3, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)", "label": "(30, MPI, errs = MPI_Irecv(buf, 10, MPI_CHAR, 0, 0, MPI_COMM_WORLD, &request);); (5, MPI, MPI_Init(&argc, &argv);); (65, MPI, MPI_Finalize();); (18, MPI, errs = MPI_Issend(\"No Errors\", 10, MPI_CHAR, 3, 0, MPI_COMM_WORLD, &request);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (16, MPI, errs += MPI_Wait(&request, MPI_STATUS_IGNORE);); (2, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (5, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (34, MPI, errs = MPI_Issend(\"No Errors\", 10, MPI_CHAR, 2, 0, MPI_COMM_WORLD, &request);)"}
{"predict": "(19, OpenMP, #pragma omp target update from(t) nowait); (6, OpenMP, #pragma omp target update to(t) nowait); (11, OpenMP, #pragma omp target update to(t) nowait nowait); (0, OpenMP, #pragma omp target update to(t) nowait); (13, OpenMP, #pragma omp target update from(t) nowait nowait); (-3, OpenMP, #pragma omp target update to(t) nowait(); (6, OpenMP, #pragma omp target update to(t) nowait nowait); (0, OpenMP, #pragma omp target update to(t) nowait nowait); (-5, OpenMP, #pragma omp target update to(t) nowait)", "label": "(7, OpenMP, #pragma omp target teams distribute parallel for simd order(); (15, OpenMP, #pragma omp target teams distribute parallel for simd order(concurrent)); (-1, OpenMP, #pragma omp target teams distribute parallel for simd nowait nowait); (7, OpenMP, #pragma omp target teams distribute parallel for simd order(none); (9, OpenMP, #pragma omp target teams distribute parallel for simd order(concurrent); (-1, OpenMP, #pragma omp target teams distribute parallel for simd order)"}
{"predict": "(24, CUDA, CHECK_CUDA_RETURN(cudaGetLastError());)", "label": "(24, CUDA, CHECK_CUDA_RETURN(cudaGetLastError());)"}
{"predict": "(14, TBB, ASSERT( num_threads > 0, \"tbb::task_scheduler_init::default_num_threads() has returned an incorrect value\" );); (12, TBB, int num_threads = tbb::task_scheduler_init::default_num_threads();)", "label": "(7, TBB, tbb::tick_count t1 = tbb::tick_count::now();); (14, TBB, ASSERT( num_threads > 0, \"tbb::thread::hardware_concurrency() has returned an incorrect value\" );); (3, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (11, TBB, int num_threads = tbb::tbb_thread::hardware_concurrency();); (5, TBB, tbb::tick_count t2 = tbb::tick_count::now();)"}
{"predict": "(330, OpenMP, #pragma omp distribute parallel for simd lastprivate(ii)); (317, OpenMP, #pragma omp distribute parallel for simd private(ii)); (334, OpenMP, #pragma omp distribute parallel for simd collapse(2)); (356, OpenMP, #pragma omp distribute parallel for simd collapse(3)); (307, OpenMP, #pragma omp distribute parallel for simd linear(ii)); (320, OpenMP, #pragma omp distribute parallel for simd lastprivate(jj)); (354, OpenMP, #pragma omp distribute parallel for simd collapse(2)); (306, OpenMP, #pragma omp distribute parallel for simd linear(jj)); (317, OpenMP, #pragma omp distribute parallel for simd lastprivate(kk)); (330, OpenMP, #pragma omp distribute parallel for simd lastprivate(di)); (318, OpenMP, #pragma omp distribute parallel for simd lastprivate(h)); (322, OpenMP, #pragma omp distribute parallel for simd lastprivate(B::x)); (323, OpenMP, #pragma omp distribute parallel for simd lastprivate(e)); (330, OpenMP, #pragma omp distribute parallel for simd lastprivate(dii)); (326, OpenMP, #pragma omp distribute parallel for simd lastprivate(fii)); (343, OpenMP, #pragma omp distribute parallel for simd lastprivate(d)); (314, OpenMP, #pragma omp distribute parallel for simd lastprivate(ii)); (317, OpenMP, #pragma omp distribute parallel for simd lastprivate(j)); (322, OpenMP, #pragma omp distribute parallel for simd lastprivate(ref)); (330, OpenMP, #pragma omp distribute parallel for simd lastprivate(dii)); (324, OpenMP, #pragma omp distribute parallel for simd lastprivate(&ref)); (326, OpenMP, #pragma omp distribute parallel for simd lastprivate(fii)); (337, OpenMP, #pragma omp distribute parallel for simd lastprivate(d)); (310, OpenMP, #pragma omp distribute parallel for simd lastprivate(kk)); (324, OpenMP, #pragma omp distribute parallel for simd lastprivate(&&ref)); (330, OpenMP, #pragma omp distribute parallel for simd lastprivate(dii)); (311, OpenMP, #pragma omp distribute parallel for simd lastprivate(h)); (315, OpenMP, #pragma omp distribute parallel for simd lastprivate(e)); (323, OpenMP, #pragma omp distribute parallel for simd lastprivate(ref)); (330, OpenMP, #pragma omp distribute", "label": "(311, OpenMP, #pragma omp distribute parallel for simd firstprivate(ii)); (332, OpenMP, #pragma omp distribute parallel for simd lastprivate(ii)); (6, OpenMP, #pragma omp distribute parallel for simd); (322, OpenMP, #pragma omp distribute parallel for simd private(ii)); (313, OpenMP, #pragma omp distribute parallel for simd linear(ii)); (1, OpenMP, #pragma omp target); (347, OpenMP, #pragma omp distribute parallel for simd collapse(2)); (0, OpenMP, #pragma omp teams)"}
{"predict": "(25, CUDA, checkCudaErrors(cudaFree(d_Mcolumn_));); (16, CUDA, checkCudaErrors(cudaFree(d_Vcolumn_));); (17, CUDA, checkCudaErrors(cudaFree(d_Mrow_));); (18, CUDA, checkCudaErrors(cudaFree(d_Krow_));); (23, CUDA, checkCudaErrors(cudaFree(d_Rrow_));); (24, CUDA, checkCudaErrors(cudaFree(d_Kcol_));); (12, CUDA, checkCudaErrors(cudaFree(d_Vrow_));); (21, CUDA, checkCudaErrors(cudaFree(d_Rcol_));)", "label": "(17, CUDA, cudaFree(devx_);); (27, CUDA, cudaFree(d_T_);); (24, CUDA, cudaFree(d_P_);); (24, CUDA, cudaFree(d_Q_);); (12, CUDA, cudaFree(devr_);); (1, CUDA, cudaFree(d_work_);)"}
{"predict": "(8, OpenMP, num_teams[j] = omp_get_num_teams();); (8, OpenMP, num_threads[j] = omp_get_num_threads();); (6, OpenMP, #pragma omp target teams distribute parallel for)", "label": "(7, OpenMP, printf(\"#pragma omp target teams distribute parallel for thread_limit(4)\\n\");); (10, OpenMP, num_teams[j] = omp_get_num_teams();); (13, OpenMP, printf(\"#pragma omp target teams distribute parallel for\\n\");); (7, OpenMP, num_threads[j] = omp_get_num_threads();); (4, OpenMP, #pragma omp target teams distribute parallel for)"}
{"predict": "(19, OpenCL, clReleaseContext_ExpectAndReturn(make_context(0), CL_SUCCESS);)", "label": "(19, OpenCL, clReleaseContext_ExpectAndReturn(make_context(0), CL_SUCCESS);)"}
{"predict": "(76, MPI, *time -= MPI_Wtime();); (44, MPI, MPI_Type_size(c_info->r_data_type, &r_size);); (62, MPI, *time += MPI_Wtime();); (42, MPI, MPI_Type_size(c_info->s_data_type, &s_size);); (73, MPI, MPI_ERRHAND(MPI_Recv(&r_num, 1, MPI_INT, source, r_tag, c_info->communicator, &stat));); (71, MPI, MPI_ERRHAND(MPI_Recv(&s_num, 1, MPI_INT, dest, s_tag, c_info->communicator, &stat));); (70, MPI, MPI_ERRHAND(MPI_Recv(&s_tag, 1, MPI_INT, dest, s_tag, c_info->communicator, &stat));); (63, MPI, MPI_ERRHAND(MPI_Send(&s_num, 1, MPI_INT, dest, s_tag, c_info->communicator));); (63, MPI, MPI_ERRHAND(MPI_Send(&s_tag, 1, MPI_INT, dest, s_tag, c_info->communicator));); (64, MPI, MPI_ERRHAND(MPI_Recv(&r_tag, 1, MPI_INT, source, r_tag, c_info->communicator, &stat));); (62, MPI, MPI_ERRHAND(MPI_Send(&r_num, 1, MPI_INT, dest, r_tag, c_info->communicator));); (63, MPI, MPI_ERRHAND(MPI_Recv(&r_size, 1, MPI_INT, source, r_tag, c_info->communicator, &stat));); (60, MPI, MPI_ERRHAND(MPI_Send(&r_size, 1, MPI_INT, dest, r_tag, c_info->communicator));)", "label": "(63, MPI, *time -= MPI_Wtime();); (43, MPI, MPI_Type_size(c_info->r_data_type, &r_size);); (41, MPI, MPI_Type_size(c_info->s_data_type, &s_size);); (73, MPI, *time += MPI_Wtime();)"}
{"predict": "(3, TBB, typedef typename tbb::flow::tuple_element<0,OutputTuple>::type OutputType;); (26, TBB, tbb::flow::remove_edge( tbb::flow::output_port<0>(exe_node), receivers[r] );); (11, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(exe_node), receivers[r] );); (1, TBB, tbb::flow::multifunction_node< InputType, OutputTuple, tbb::flow::rejecting > exe_node( g, tbb::flow::unlimited, body );); (0, TBB, tbb::flow::graph g;)", "label": "(14, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(exe_node), receivers[r] );); (1, TBB, typedef typename tbb::flow::tuple_element<0,OutputTuple>::type OutputType;); (27, TBB, tbb::flow::remove_edge( tbb::flow::output_port<0>(exe_node), receivers[r] );); (3, TBB, tbb::flow::multifunction_node< InputType, OutputTuple, tbb::flow::rejecting > exe_node( g, tbb::flow::unlimited, body );); (1, TBB, tbb::flow::graph g;)"}
{"predict": "(11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &nodeID);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nr_nodes);); (73, MPI, MPI_Allgather(&local_n0, 1, MPI_INT, slice_width, 1, MPI_INT, MPI_COMM_WORLD);)", "label": "(11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &nodeID);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nr_nodes);); (73, MPI, MPI_Allgather(&local_n0, 1, MPI_INT, slice_width, 1, MPI_INT, MPI_COMM_WORLD);)"}
{"predict": "(6, OpenMP, #pragma omp teams distribute simd); (287, OpenMP, #pragma omp teams distribute simd firstprivate(ii)); (271, OpenMP, #pragma omp teams distribute simd private(ii)); (287, OpenMP, #pragma omp teams distribute simd linear(ii)); (4, OpenMP, #pragma omp target); (260, OpenMP, #pragma omp teams distribute simd lastprivate(ii)); (268, OpenMP, #pragma omp teams distribute simd); (261, OpenMP, #pragma omp teams distribute simd collapse(2)); (275, OpenMP, #pragma omp teams distribute simd firstprivate(jj)); (280, OpenMP, #pragma omp teams distribute simd firstprivate(kk)); (3, OpenMP, #pragma omp teams distribute simd collapse(2)); (285, OpenMP, #pragma omp teams distribute simd firstprivate(ref)); (275, OpenMP, #pragma omp teams distribute simd firstprivate(ii)); (288, OpenMP, #pragma omp teams distribute simd lastprivate(ii)); (3, OpenMP, #pragma omp teams distribute simd collapse(3)); (268, OpenMP, #pragma omp teams distribute simd firstprivate(a, b)); (2, OpenMP, #pragma omp teams distribute simd collapse(4)); (276, OpenMP, #pragma omp teams distribute simd lastprivate(ii)); (280, OpenMP, #pragma omp teams distribute simd lastprivate(conditional: ii)); (3, OpenMP, #pragma omp teams distribute simd collapse(5)); (277, OpenMP, #pragma omp teams distribute simd lastprivate(S1)); (5, OpenMP, #pragma omp teams distribute simd collapse(0)); (270, OpenMP, #pragma omp teams distribute simd firstprivate(ba)); (5, OpenMP, #pragma omp teams distribute simd collapse(1)); (277, OpenMP, #pragma omp teams distribute simd lastprivate(conditional: S1)); (-9, OpenMP, #pragma omp teams distribute simd collapse); (271, OpenMP, #pragma omp teams distribute simd lastprivate(aa))", "label": "(266, OpenMP, #pragma omp teams distribute simd firstprivate(ii)); (6, OpenMP, #pragma omp teams distribute simd); (270, OpenMP, #pragma omp teams distribute simd private(ii)); (275, OpenMP, #pragma omp teams distribute simd lastprivate(ii)); (287, OpenMP, #pragma omp teams distribute simd collapse(2)); (1, OpenMP, #pragma omp target)"}
{"predict": "(84, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (84, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)", "label": "(84, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (84, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)"}
{"predict": "(6, OpenMP, #pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp parallel for)"}
{"predict": "(8, MPI, dray::dray::mpi_comm(MPI_Comm_c2f(comm));)", "label": "(20, MPI, int rank = dray::dray::mpi_rank();); (9, MPI, dray::dray::mpi_comm(MPI_Comm_c2f(comm));); (19, MPI, int size = dray::dray::mpi_size();)"}
{"predict": "(6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (104, MPI, MPI_Barrier(MPI_COMM_WORLD);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(141, MPI, MPI_Waitall(num_procs_to_send, recv_reqs, MPI_STATUSES_IGNORE);); (140, MPI, MPI_Isend(&num_procs_to_send, 1, MPI_INT, 0, tag, grid3d->zscp.comm, &recv_reqs[0]);); (161, MPI, MPI_Isend(send_bufs[0], send_count, MPI_DOUBLE, 0, tag, grid3d->zscp.comm, &recv_reqs[0]);); (161, MPI, MPI_Irecv(recv_bufs[p], recv_count, MPI_DOUBLE, p, tag, grid3d->zscp.comm, &recv_reqs[procs_to_send_list[p]]);); (134, MPI, MPI_Allgather(&(A2d->m_loc), 1, MPI_INT, m_loc_3d_counts, 1, MPI_INT, grid3d->zscp.comm);); (161, MPI, MPI_Waitall(num_procs_to_recv, recv_reqs, MPI_STATUSES_IGNORE);); (132, MPI, MPI_Allgather(&nrhs, 1, MPI_INT, x_recv_counts, 1, MPI_INT, grid3d->zscp.comm);)", "label": "(270, MPI, MPI_Wait(&recv_req, &recv_status);)"}
{"predict": "(24, MPI, MPI_Type_get_extent(key, &key_true_lb, &key_true_extent);); (20, MPI, MPI_Type_get_extent(keysat, &keysat_lb, &keysat_extent);); (25, MPI, MPI_Alloc_mem(size_int_with_key, MPI_INFO_NULL, &weighted_median_scratch);); (71, MPI, MPI_Allgather(&count, 1, MPI_INT, k, 1, MPI_INT, lwgcomm);); (58, MPI, MPI_Allgather(&sendcounts[0], 1, MPI_INT, sendcounts, 1, MPI_INT, lwgcomm);); (58, MPI, MPI_Allgather(&senddispls, 1, MPI_INT, senddispls, 1, MPI_INT, lwgcomm);); (52, MPI, MPI_Allgather(&recvcounts[0], 1, MPI_INT, recvcounts, 1, MPI_INT, lwgcomm);); (52, MPI, MPI_Allgather(&recvdispls, 1, MPI_INT, recvdispls, 1, MPI_INT, lwgcomm);); (70, MPI, MPI_Exscan(sendcounts, senddispls, 1, MPI_INT, MPI_SUM, lwgcomm);); (68, MPI, MPI_Exscan(recvcounts, recvdispls, 1, MPI_INT, MPI_SUM, lwgcomm);); (15, MPI, MPI_Type_commit(&type_int_with_key);); (58, MPI, MPI_Allgather(&num[0], 1, MPI_INT, num, 1, MPI_INT, lwgcomm);); (57, MPI, MPI_Allgather(&index[0], 1, MPI_INT, index, 1, MPI_INT, lwgcomm);); (18, MPI, MPI_Alloc_mem(size_int_with_key, MPI_INFO_NULL, &my_num_with_median);); (18, MPI, MPI_Alloc_mem(size_int_with_key, MPI_INFO_NULL, &out_num_with_median);); (232, MPI, MPI_Finalize();); (15, MPI, MPI_Type_free(&type_int_with_key);)", "label": "(24, MPI, MPI_Type_get_true_extent(key, &key_true_lb, &key_true_extent);); (204, MPI, MPI_Type_free(&type_int_with_key);); (280, MPI, MPI_Comm_rank(comm, &comm_rank);); (17, MPI, MPI_Type_get_extent(keysat, &keysat_lb, &keysat_extent);)"}
{"predict": "(8, MPI, MPI_Errhandler_free( &newerr );); (6, MPI, MPI_Errhandler_create( newerrhandler, &newerr );); (7, MPI, MPI_Errhandler_set( newerr );); (8, MPI, MPI_Put( &myint, 1, MPI_INT, -5, 0, 1, MPI_INT, newerr );); (8, MPI, MPI_Errhandler_free( &newerr );)", "label": "(10, MPI, MPI_Comm_call_errhandler( comm, MPI_ERR_OTHER );); (8, MPI, MPI_Comm_set_errhandler( comm, newerr );); (14, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_create_errhandler( eh, &newerr );); (1, MPI, MPI_Init( &argc, &argv );); (6, MPI, MPI_Errhandler_free( &newerr );)"}
{"predict": "(10, OpenMP, #pragma omp parallel for)", "label": "(11, OpenMP, #pragma omp parallel for)"}
{"predict": "(14, MPI, MPI_Comm_rank(intra, &rank);); (34, MPI, MPI_Comm_free(&inter);); (31, MPI, MPI_Barrier(inter);); (14, MPI, MPI_Comm_set_errhandler(inter, MPI_ERRORS_RETURN);); (8, MPI, MPI_Comm_set_errhandler(parent, MPI_ERRORS_RETURN);); (11, MPI, err = MPI_Intercomm_create(intra, 0, MPI_COMM_WORLD, 0, %d, &inter);); (25, MPI, err = MPI_Intercomm_merge(inter, first, &merge1);); (27, MPI, MPI_Comm_free(&merge1);); (17, MPI, MPI_Barrier(intra);); (3, MPI, MPI_Comm_rank(parent, &rank);); (24, MPI, MPI_Comm_free(&intra);); (6, MPI, err = MPI_Intercomm_create(intra, 0, inter, 0, %d, &merge1);)", "label": "(38, MPI, MPI_Comm_disconnect(&parent);); (27, MPI, err = MPI_Intercomm_merge(inter, 0, &merge1);); (34, MPI, MPI_Comm_free(&intra);); (31, MPI, MPI_Comm_free(&merge1);); (6, MPI, err = MPI_Intercomm_merge(parent, 1, &intra);); (30, MPI, MPI_Comm_free(&inter);); (25, MPI, err = MPI_Barrier(merge1);); (18, MPI, err = MPI_Barrier(inter);); (21, MPI, MPI_Comm_rank(merge1, &rank);); (7, MPI, err = MPI_Intercomm_create(intra, 0, MPI_COMM_NULL, 0, tag, &inter);); (1, MPI, MPI_Comm_rank(intra, &rank);); (10, MPI, err = MPI_Intercomm_create(MPI_COMM_WORLD, 0, intra, 0, tag, &inter);)"}
{"predict": "(94, MPI, MPI_Type_commit(&send_type);); (139, MPI, MPI_Waitall(nprocs_send, requests + nprocs_recv, statuses + nprocs_recv);); (20, MPI, MPI_Alltoall(send_size, 1, MPI_COUNT, recv_size, 1, MPI_COUNT, fd->comm);); (94, MPI, MPI_Type_free(&send_type);); (119, MPI, MPI_Get_count(&statuses[i], MPI_BYTE, &count_recved);); (121, MPI, MPI_Get_address(recv_buf[i], &addr);); (18, MPI, MPI_Alltoall(&nprocs_send, 1, MPI_INT, &nprocs_recv, 1, MPI_INT, fd->comm);)", "label": "(94, MPI, MPI_Type_commit(&send_type);); (137, MPI, MPI_Waitall(nprocs_send, requests + nprocs_recv, MPI_STATUSES_IGNORE);); (19, MPI, MPI_Alltoall(send_size, 1, MPI_COUNT, recv_size, 1, MPI_COUNT, fd->comm);); (118, MPI, MPI_Get_count_c(&statuses[j], MPI_BYTE, &count_recved);); (93, MPI, MPI_Type_free(&send_type);)"}
{"predict": "(98, OpenMP, #pragma omp task); (99, OpenMP, #pragma omp taskgroup)", "label": "(101, OpenMP, #pragma omp task); (102, OpenMP, #pragma omp taskgroup)"}
{"predict": "(17, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize());); (15, CUDA, qmc_cuda::cuda_check(cudaGetLastError());)", "label": "(17, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize());); (15, CUDA, qmc_cuda::cuda_check(cudaGetLastError());)"}
{"predict": "(9, MPI, MPI_Get_processor_name(mach_name,&mach_len);); (21, MPI, MPI_Barrier(MPI_COMM_WORLD);); (23, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);); (23, MPI, MPI_Comm_size(MPI_COMM_WORLD, &my_size);); (21, MPI, MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, 0, MPI_INFO_NULL, &my_comm);); (7, MPI, MPI_Abort(MPI_COMM_WORLD,1);); (23, MPI, MPI_Comm_free(&my_comm);); (19, MPI, MPI_Comm_free(&global);)", "label": "(28, MPI, MPI_Finalize();); (8, MPI, MPI_Get_processor_name(mach_name,&mach_len);); (22, MPI, MPI_Allreduce(&local,&global,1,MPI_INT,MPI_SUM,MPI_COMM_WORLD);)"}
{"predict": "(19, OpenMP, #pragma omp distribute parallel for if (parallel: Arg)); (10, OpenMP, #pragma omp distribute parallel for if (false)); (0, OpenMP, #pragma omp distribute parallel for if (true)); (-3, OpenMP, #pragma omp target); (0, OpenMP, #pragma omp distribute parallel for if (Arg)); (-5, OpenMP, #pragma omp teams)", "label": "(3, OpenMP, #pragma omp distribute parallel for if (true)); (11, OpenMP, #pragma omp distribute parallel for if (false)); (19, OpenMP, #pragma omp distribute parallel for if (Arg)); (-2, OpenMP, #pragma omp target); (-2, OpenMP, #pragma omp teams)"}
{"predict": "(3, OpenMP, #pragma omp parallel for)", "label": "(3, OpenMP, #pragma omp parallel for)"}
{"predict": "(19, OpenCL, clReleaseEvent(event);)", "label": "(19, OpenCL, clReleaseEvent(event);)"}
{"predict": "(9, MPI, MPI_Init(&argc, &argv);); (12, MPI, MPI_Comm_size(world_mpi_comm, &mpi_size);); (12, MPI, MPI_Comm_rank(world_mpi_comm, &mpi_rank);); (186, MPI, MPI_Barrier(world_mpi_comm);); (22, MPI, MPI_Finalize();); (8, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(10, MPI, MPI_Init(&argc, &argv);); (206, MPI, MPI_Finalize();); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (185, MPI, MPI_Barrier(MPI_COMM_WORLD);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);)"}
{"predict": "(8, OpenMP, #pragma omp target firstprivate(t_var, vec, s_arr, s_arr, var, var))", "label": "(8, OpenMP, #pragma omp master taskloop lastprivate(t_var, vec, s_arr, s_arr, var, var))"}
{"predict": "(11, OpenMP, #pragma omp target teams distribute parallel for map(tofrom : a[ : n], b[ : n])); (15, OpenMP, #pragma omp target update from( : a[ : n]))", "label": "(11, OpenMP, #pragma omp target teams distribute parallel for map(tofrom : a[ : n]) map(to : b[ : n]))"}
{"predict": "(8, OpenMP, #pragma omp target update CLAUSE(present: i)); (6, OpenMP, #pragma omp target data map(alloc: i))", "label": "(8, OpenMP, #pragma omp target update CLAUSE(present: i)); (6, OpenMP, #pragma omp target enter data map(alloc: i)); (7, OpenMP, #pragma omp target exit data map(delete: i))"}
{"predict": "(114, OpenMP, #pragma omp for simd collapse(2)); (108, OpenMP, #pragma omp master); (99, OpenMP, #pragma omp parallel); (3, OpenMP, #pragma omp for simd collapse); (94, OpenMP, #pragma omp master); (105, OpenMP, #pragma omp for simd collapse(5 - 5)); (99, OpenMP, #pragma omp for simd collapse(2)); (93, OpenMP, #pragma omp for simd collapse(3)); (0, OpenMP, #pragma omp for simd collapse(); (99, OpenMP, #pragma omp for simd collapse(6))", "label": "(163, OpenMP, #pragma omp for collapse(2))"}
{"predict": "(58, OpenMP, #pragma omp distribute parallel for firstprivate(e, g)); (81, OpenMP, #pragma omp distribute parallel for firstprivate(j)); (77, OpenMP, #pragma omp parallel private(i)); (72, OpenMP, #pragma omp distribute parallel for firstprivate(i)); (4, OpenMP, #pragma omp distribute parallel for firstprivate); (52, OpenMP, #pragma omp distribute parallel for firstprivate(argv[1])); (39, OpenMP, #pragma omp distribute parallel for firstprivate(a, b)); (28, OpenMP, #pragma omp distribute parallel for firstprivate(argc > 0? argv[1] : argv[2])); (0, OpenMP, #pragma omp target); (38, OpenMP, #pragma omp distribute parallel for firstprivate(S1)); (16, OpenMP, #pragma omp distribute parallel for firstprivate(argc); (5, OpenMP, #pragma omp distribute parallel for firstprivate()); (18, OpenMP, #pragma omp distribute parallel for firstprivate(argc,); (42, OpenMP, #pragma omp distribute parallel for firstprivate(h)); (19, OpenMP, #pragma omp distribute parallel for firstprivate(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (58, OpenMP, #pragma omp parallel); (91, OpenMP, #pragma omp parallel reduction(+ : i)); (70, OpenMP, #pragma omp distribute parallel for lastprivate(g) firstprivate(g)); (45, OpenMP, #pragma omp distribute parallel for firstprivate(k, v : i)); (-4, OpenMP, #pragma omp distribute parallel for firstprivate(); (-13, OpenMP, #pragma omp teams)", "label": "(38, OpenMP, #pragma omp distribute parallel for firstprivate(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (78, OpenMP, #pragma omp parallel private(i)); (56, OpenMP, #pragma omp distribute parallel for firstprivate(e, g)); (30, OpenMP, #pragma omp distribute parallel for firstprivate(argc > 0? argv[1] : argv[2])); (68, OpenMP, #pragma omp distribute parallel for firstprivate(i)); (1, OpenMP, #pragma omp target); (57, OpenMP, #pragma omp distribute parallel for firstprivate(k, h)); (59, OpenMP, #pragma omp parallel); (40, OpenMP, #pragma omp distribute parallel for firstprivate(a, b)); (93, OpenMP, #pragma omp parallel reduction(+ : i)); (3, OpenMP, #pragma omp distribute parallel for firstprivate(); (71, OpenMP, #pragma omp distribute parallel for firstprivate(j)); (41, OpenMP, #pragma omp distribute parallel for firstprivate(argv[1])); (5, OpenMP, #pragma omp distribute parallel for firstprivate()); (9, OpenMP, #pragma omp distribute parallel for firstprivate(argc); (78, OpenMP, #pragma omp distribute parallel for lastprivate(g) firstprivate(g)); (27, OpenMP, #pragma omp distribute parallel for firstprivate(S1)); (11, OpenMP, #pragma omp distribute parallel for firstprivate(argc,); (60, OpenMP, #pragma omp parallel shared(i)); (-11, OpenMP, #pragma omp distribute parallel for firstprivate); (-13, OpenMP, #pragma omp teams)"}
{"predict": "(18, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (18, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (45, MPI, MPI_Barrier(MPI_COMM_WORLD);)", "label": "(25, MPI, MPI_Comm_free(&comms[x]);); (18, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &comms[x]);)"}
{"predict": "(4, OpenCL, clReleaseContext_ExpectAndReturn(make_context(0), CL_SUCCESS);)", "label": "(5, OpenCL, clReleaseContext_ExpectAndReturn(make_context(0), CL_SUCCESS);); (3, OpenCL, clCreateContext_StubWithCallback(clCreateContext_testContextNonNullProperties);)"}
{"predict": "(44, OpenMP, #pragma omp master taskloop firstprivate(x)); (31, OpenMP, #pragma omp master taskloop firstprivate(int)); (48, OpenMP, #pragma omp master taskloop firstprivate(x, y)); (0, OpenMP, #pragma omp master taskloop firstprivate(); (15, OpenMP, #pragma omp master taskloop firstprivate(, )); (19, OpenMP, #pragma omp master taskloop firstprivate()); (-4, OpenMP, #pragma omp parallel); (37, OpenMP, #pragma omp master taskloop firstprivate(x, y, z)); (20, OpenMP, #pragma omp master taskloop firstprivate(,); (43, OpenMP, #pragma omp master taskloop firstprivate(x :)); (0, OpenMP, #pragma omp master taskloop firstprivate(,); (46, OpenMP, #pragma omp master taskloop firstprivate(x :, )); (4, OpenMP, #pragma omp master taskloop firstprivate(,); (24, OpenMP, #pragma omp master taskloop firstprivate(0))", "label": "(5, OpenMP, #pragma omp sections firstprivate(); (24, OpenMP, #pragma omp sections firstprivate()); (35, OpenMP, #pragma omp sections firstprivate(0)); (51, OpenMP, #pragma omp sections lastprivate(x, y, z) firstprivate(x, y, z)); (45, OpenMP, #pragma omp sections lastprivate(x, y) firstprivate(x, y)); (39, OpenMP, #pragma omp sections lastprivate(x) firstprivate(x)); (-4, OpenMP, #pragma omp parallel); (6, OpenMP, #pragma omp sections firstprivate(,); (11, OpenMP, #pragma omp sections firstprivate(, )); (22, OpenMP, #pragma omp sections firstprivate(int))"}
{"predict": "(3, OpenMP, #pragma omp target data map(tofrom: i)); (3, OpenMP, #pragma omp target parallel for)", "label": "(10, OpenMP, #pragma omp target update to(i) nowait (3.14) device (-10u)); (3, OpenMP, #pragma omp target nowait update to(i)); (7, OpenMP, #pragma omp target update to(i) nowait device (-10u)); (5, OpenMP, #pragma omp target update to(i) nowait (argc))); (7, OpenMP, #pragma omp target update to(i) nowait nowait); (7, OpenMP, #pragma omp target update nowait to(i) nowait); (-3, OpenMP, #pragma omp nowait target update to(i)); (-1, OpenMP, #pragma omp target update nowait() to(i)); (-1, OpenMP, #pragma omp target update to(i) nowait()"}
{"predict": "(7, OpenMP, #pragma omp target is_device_ptr( position, cell_keys ) map( tofrom: energy_change ) map( tofrom: limit_inf ) map( tofrom: direction )", "label": "(7, OpenACC, #pragma acc loop gang worker vector); (5, OpenACC, #pragma acc parallel deviceptr(position,cell_keys))"}
{"predict": "(6, TBB, tbb::flow::queue_node< q3_input_type >  q3(g);); (10, TBB, ASSERT(tbb::flow::input_port<0>(my_indexer).predecessor_count() == 0, NULL);); (8, TBB, tbb::flow::input_port<0>(my_indexer).try_put( v );); (7, TBB, tbb::flow::make_edge( my_indexer, q3 );)", "label": "(6, TBB, tbb::flow::queue_node< q3_input_type >  q3(g);); (11, TBB, ASSERT(tbb::flow::input_port<0>(my_indexer).predecessor_count() == 0, NULL);); (7, TBB, tbb::flow::make_edge(my_indexer, q3);)"}
{"predict": "(8, TBB, tbb::task::enqueue(*w);); (65, TBB, tbb::global_control c(tbb::global_control::max_allowed_parallelism, 1);); (5, TBB, WaiterTask* w = new( tbb::task::allocate_root() ) WaiterTask(&taskDoneFlag);); (6, TBB, tbb::task::enqueue(*w);); (83, TBB, tbb::task::enqueue(*t, p);); (0, TBB, tbb::task_scheduler_init tsi(1, 0, /*blocking=*/true);); (83, TBB, tbb::task::remove_priority(*t);); (77, TBB, tbb::task::add_priority(*t, i);); (8, TBB, tbb::task::enqueue(*t);); (4, TBB, tbb::global_control c(tbb::global_control::max_allowed_parallelism, 1);); (102, TBB, tbb::atomic<int> counter;); (108, TBB, tbb::atomic<bool> signalToLeave;); (51, TBB, tbb::global_control c(tbb::global_control::max_allowed_parallelism, 1);); (99, TBB, tbb::task_scheduler_init tsi(2, 0, /*blocking=*/true);); (94, TBB, tbb::atomic<int> counter2;); (95, TBB, tbb::task::enqueue(*t2);); (88, TBB, tbb::task* t2 = new( tbb::task::allocate_root() ) tbb::empty_task;); (-2, TBB, tbb::task::allocate_root();); (2, TBB, tbb::task::enqueue(*w, *t);)", "label": "(5, TBB, tbb::atomic<bool> taskDoneFlag;); (85, TBB, tbb::task::enqueue(*t, p);); (9, TBB, tbb::task::enqueue(*w);); (13, TBB, FFTask* t = new( tbb::task::allocate_root() ) FFTask(&flag);); (81, TBB, const tbb::priority_t p = getPriorityByInt(i);); (98, TBB, tbb::atomic<int> counter;); (11, TBB, tbb::task::enqueue(*t);); (108, TBB, tbb::atomic<bool> signalToLeave;); (-4, TBB, tbb::atomic<int> flag;); (1, TBB, WaiterTask* w = new( tbb::task::allocate_root() ) WaiterTask(&taskDoneFlag);); (3, TBB, tbb::global_control c(tbb::global_control::max_allowed_parallelism, 1);)"}
{"predict": "(7, MPI, MPI_Comm_rank(comm, &par_rank);); (7, MPI, MPI_Comm_size(comm, &par_size);)", "label": "(8, MPI, MPI_Comm_rank(comm, &par_rank);); (24, MPI, open_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (7, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(5, OpenMP, #pragma omp target map(tofrom : A[0:1000])); (11, OpenMP, #pragma omp target exit data map(from : A[0:1000]))", "label": "(5, OpenMP, #pragma omp workshare)"}
{"predict": "(6, OpenCL, retVal = clEnqueueWriteIntoGPUPart(unifiedMemoryPtr, input.data(), dataSize, unifiedMemoryType, 0, NULL, NULL);)", "label": "(6, OpenCL, retVal = clEnqueueMemcpyINTEL(this->pCmdQ, true, unifiedMemoryPtr, input.data(), dataSize, 0, nullptr, nullptr);)"}
{"predict": "(95, OpenMP, #pragma omp master taskloop reduction(+ : r)); (79, OpenMP, #pragma omp master taskloop reduction(^ : fl)); (58, OpenMP, #pragma omp master taskloop reduction(+ : ba)); (28, OpenMP, #pragma omp master taskloop reduction(\\)); (48, OpenMP, #pragma omp master taskloop reduction(+ : a, b, c, d, f)); (76, OpenMP, #pragma omp master taskloop reduction(&& : S2::S2sc)); (45, OpenMP, #pragma omp master taskloop reduction(+ : ad)); (68, OpenMP, #pragma omp master taskloop reduction(& : e, g)); (17, OpenMP, #pragma omp master taskloop reduction(*)); (97, OpenMP, #pragma omp master taskloop reduction(+ : m)); (60, OpenMP, #pragma omp master taskloop reduction(&& : S2::S2s)); (39, OpenMP, #pragma omp master taskloop reduction(min : a, b, c, d, f)); (14, OpenMP, #pragma omp master taskloop reduction(foo : argc); (57, OpenMP, #pragma omp master taskloop reduction(&& : h, k, B::x)); (27, OpenMP, #pragma omp master taskloop reduction(|| : argc > 0? argv[1] : argv[2])); (-1, OpenMP, #pragma omp master taskloop reduction); (74, OpenMP, #pragma omp parallel reduction(* : fl)); (81, OpenMP, #pragma omp master taskloop reduction(+ : fl)); (34, OpenMP, #pragma omp master taskloop reduction(^ : S1)); (39, OpenMP, #pragma omp master taskloop reduction(max : argv[1], z)); (27, OpenMP, #pragma omp master taskloop reduction(&& : argc)); (49, OpenMP, #pragma omp master taskloop reduction(+ : h, k, B::x)); (75, OpenMP, #pragma omp parallel shared(i)); (20, OpenMP, #pragma omp master taskloop reduction(&& : argc, z)); (62, OpenMP, #pragma omp master taskloop private(i), reduction(+ : j), reduction(+ : q)); (57, OpenMP, #pragma omp master taskloop reduction(+ : p), reduction(+ : p)); (76, OpenMP,", "label": "(15, OpenMP, #pragma omp master taskloop reduction +); (65, OpenMP, #pragma omp master taskloop reduction(- : da)); (61, OpenMP, #pragma omp master taskloop reduction(* : ca)); (54, OpenMP, #pragma omp master taskloop reduction(max : h.b)); (47, OpenMP, #pragma omp master taskloop reduction(+ : a, b, c, d, f)); (55, OpenMP, #pragma omp master taskloop reduction(+ : ba)); (30, OpenMP, #pragma omp master taskloop reduction(| : argc,); (84, OpenMP, #pragma omp master taskloop reduction(+ : p), reduction(+ : p)); (46, OpenMP, #pragma omp master taskloop reduction(min : a, b, c, d, f)); (33, OpenMP, #pragma omp master taskloop reduction(~ : argc)); (38, OpenMP, #pragma omp master taskloop reduction(^ : S1)); (73, OpenMP, #pragma omp master taskloop reduction(+ : o)); (90, OpenMP, #pragma omp master taskloop reduction(max : j)); (11, OpenMP, #pragma omp master taskloop reduction()); (95, OpenMP, #pragma omp parallel reduction(* : fl)); (63, OpenMP, #pragma omp master taskloop reduction(& : e, g)); (65, OpenMP, #pragma omp master taskloop reduction(+ : h, k, B::x)); (73, OpenMP, #pragma omp parallel private(k)); (96, OpenMP, #pragma omp master taskloop reduction(+ : m)); (20, OpenMP, #pragma omp master taskloop reduction(|| : argc > 0? argv[1] : argv[2])); (-2, OpenMP, #pragma omp master taskloop reduction(); (24, OpenMP, #pragma omp master taskloop reduction(&& : argc, z)); (53, OpenMP, #pragma omp master taskloop reduction(&& : S2::S2sc)); (46, OpenMP, #pragma omp master taskloop reduction(^ : fl)); (6, OpenMP, #pragma omp master taskloop reduction(\\)); (-13, OpenMP, #pragma omp master taskloop reduction); (7, OpenMP, #pragma omp master taskloop reduction(foo : argc); (74, OpenMP, #pragma omp parallel reduction(min : i)); (77, OpenMP, #pragma omp parallel private(fl)); (91, OpenMP, #pragma omp master taskloop reduction(task, + : m)); (-9, OpenMP, #pragma omp master taskloop reduction(-); (86, OpenMP, #pragma omp master taskloop nogroup reduction(+ : m)); (65, OpenMP, #pragma omp master taskloop reduction(+ : r)); (-6, OpenMP, #pragma omp master taskloop reduction(*)); (53, OpenMP, #pragma omp master taskloop private(i), reduction(+ : j), reduction(+ : q)); (65, OpenMP, #pragma omp parallel shared(i)); (36, OpenMP, #pragma omp master taskloop reduction(&& : S2::S2s)); (69, OpenMP, #pragma omp master taskloop reduction(+ : fl))"}
{"predict": "(31, CUDA, __shared__ float4 pillarSumSM[pillar_idx_inBlock];); (28, CUDA, __shared__ float4 pillarSM[pillar_idx_inBlock][max_points];); (29, CUDA, __shared__ float4 pillarOutSM[pillar_idx_inBlock][max_points][64*11];); (27, CUDA, __shared__ float4 pillarInSM[pillar_idx_inBlock][max_points];); (29, CUDA, __shared__ float4 pillarOutSM[pillar_idx_inBlock][max_points][64*11];); (26, CUDA, __shared__ float4 pillarSM[pillar_idx_inBlock][:max_points];); (28, CUDA, __shared__ float4 pillarOutSM[pillar_idx_inBlock][max_points][64*11];); (25, CUDA, __shared__ float4 pillarInSM[pillar_idx_inBlock][:max_points];)", "label": "(34, CUDA, __shared__ int pointsNumSM[4];); (30, CUDA, __shared__ float4 pillarSM[4][64];); (30, CUDA, __shared__ float4 pillarSumSM[4];); (30, CUDA, __shared__ int4 cordsSM[4];); (31, CUDA, __shared__ float pillarOutSM[4][64][11];)"}
{"predict": "(6, OpenMP, int p = omp_alloc(0, omp_default_mem_alloc);); (0, OpenMP, #pragma omp target map(from: default_alloc))", "label": "(1, OpenMP, omp_allocator_handle_t default_alloc = omp_get_default_allocator();); (4, OpenMP, int *p = (int *)omp_alloc(123456*sizeof(int));)"}
{"predict": "(11, OpenMP, #pragma omp for simd reduction(+:a[:n]) reduction(&:b)); (44, OpenMP, #pragma omp scan inclusive(a[:n], b)); (192, OpenMP, #pragma omp scan exclusive(a[:n], b))", "label": "(194, OpenMP, #pragma omp scan exclusive(a[:n], b)); (44, OpenMP, #pragma omp scan inclusive(a[:n], b)); (9, OpenMP, #pragma omp for reduction(inscan, +:a[:n], b))"}
{"predict": "(62, MPI, err = MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_WRONLY, MPI_INFO_NULL, &fh);); (76, MPI, err = MPI_File_open(MPI_COMM_SELF, filename, MPI_MODE_RDONLY, MPI_INFO_NULL, &fh);); (31, MPI, err = MPI_File_close(&fh);); (76, MPI, err = MPI_File_read(fh, buf, MAXLEN, MPI_INT, &status);); (22, MPI, err = MPI_File_write(fh, buf, MAXLEN, MPI_INT, &status);); (54, MPI, MPI_Type_commit(&filetype);); (59, MPI, err = MPI_File_set_view(fh, 0, MPI_INT, filetype, \"native\", MPI_INFO_NULL);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &np);); (61, MPI, err = MPI_File_write_all(fh, buf, buflen, MPI_INT, &status);); (65, MPI, err = MPI_File_close(&fh);); (45, MPI, MPI_Type_create_hindexed(num, blocklen, addrs, MPI_INT, &filetype);); (17, MPI, err = MPI_File_open(MPI_COMM_SELF, filename, MPI_MODE_RDONLY, MPI_INFO_NULL, &fh);); (43, MPI, MPI_Type_indexed(num, blocklen, indices, MPI_INT, &filetype);); (78, MPI, err = MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_RDONLY, MPI_INFO_NULL, &fh);); (34, MPI, MPI_Type_create_struct(num, blocklen, addrs, types, &filetype);); (13, MPI, err = MPI_File_close(&fh);); (56, MPI, err = MPI_File_open(MPI_COMM_SELF, filename, MPI_MODE_WRONLY, MPI_INFO_NULL, &fh);); (1, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &np);); (68, MPI, err = MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_WRONLY, MPI_INFO_NULL, &fh);); (73, MPI, err = MPI_File_write_all(fh, buf, buflen, MPI_INT, &status);); (29, MPI, err = MPI_File_close(&fh);); (60, MPI, MPI_Type_commit(&filetype);); (24, MPI, err = MPI_File_write(fh, buf, MAXLEN, MPI_INT, &status);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (65, MPI, err = MPI_File_set_view(fh, 0, MPI_INT, filetype, \"native\", MPI_INFO_NULL);); (45, MPI, MPI_Type_create_hindexed(num, blocklen, addrs, MPI_INT, &filetype);); (76, MPI, err = MPI_File_open(MPI_COMM_SELF, filename, MPI_MODE_RDONLY, MPI_INFO_NULL, &fh);); (40, MPI, MPI_Type_indexed(num, blocklen, indices, MPI_INT, &filetype);); (67, MPI, MPI_Type_free(&filetype);); (76, MPI, err = MPI_File_read(fh, buf, MAXLEN, MPI_INT, &status);); (23, MPI, MPI_Barrier(MPI_COMM_WORLD);); (42, MPI, MPI_Type_create_struct(num, blocklen, addrs, types, &filetype);)"}
{"predict": "(8, OpenMP, #pragma omp parallel sections default()); (9, OpenMP, #pragma omp parallel sections default(none); (0, OpenMP, #pragma omp parallel sections default); (17, OpenMP, #pragma omp parallel sections default(none)); (10, OpenMP, #pragma omp parallel sections default(shared), default(shared)); (21, OpenMP, #pragma omp parallel sections default(shared))", "label": "(5, OpenMP, #pragma omp parallel sections default()); (2, OpenMP, #pragma omp parallel sections default(); (7, OpenMP, #pragma omp parallel sections default(shared), default(shared)); (4, OpenMP, #pragma omp parallel sections default(none); (7, OpenMP, #pragma omp parallel sections default(x)); (16, OpenMP, #pragma omp parallel sections default(none)); (-5, OpenMP, #pragma omp parallel sections default); (21, OpenMP, #pragma omp parallel sections default(shared))"}
{"predict": "(17, CUDA, __shared__ float s_b[2][BK][BN];); (15, CUDA, __shared__ float s_a[2][BK][BM];)", "label": "(16, CUDA, __shared__ float s_a[2][BK][BM];); (16, CUDA, __shared__ float s_b[2][BK][BN];)"}
{"predict": "(17, OpenMP, volatile  int_t thread_id = omp_get_thread_num();)", "label": "(17, OpenMP, volatile  int_t thread_id = omp_get_thread_num();)"}
{"predict": "(9, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (9, TBB, tbb::parallel_for(tbb::blocked_range<size_t>(0,n,n/100),TwiddleInvariant<Invariant<M,8> >(invariant));); (9, TBB, tbb::tick_count t1 = tbb::tick_count::now();)", "label": "(8, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (9, TBB, tbb::tick_count t1 = tbb::tick_count::now();); (7, TBB, tbb::parallel_for(tbb::blocked_range<size_t>(0,n,n/100),TwiddleInvariant<Invariant<M,8> >(invariant));)"}
{"predict": "(28, MPI, errcode = MPI_File_close(&fh);); (33, MPI, errcode = MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_CREATE | MPI_MODE_RDWR, info, &fh);); (13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (22, MPI, errcode = MPI_File_iwrite_at_all(fh, off, buffer, BUFSIZE, MPI_INT, &request);); (31, MPI, MPI_Wait(&request, &status);); (33, MPI, errcode = MPI_File_iread_at_all(fh, off, buf2, BUFSIZE, MPI_INT, &request);); (19, MPI, MPI_File_set_view(fh, 0, MPI_INT, MPI_INT, \"native\", MPI_INFO_NULL);); (23, MPI, errcode = MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_CREATE | MPI_MODE_RDWR, info, &fh);); (6, MPI, MPI_Info_create(&info);); (43, MPI, MPI_Info_free(&info);); (19, MPI, MPI_File_close(&fh);); (12, MPI, MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_CREATE | MPI_MODE_RDWR, info, &fh);); (5, MPI, MPI_Info_set(info, \"romio_no_indep_rw\", \"true\");); (11, MPI, MPI_File_set_view(fh, 0, MPI_INT, MPI_INT, \"native\", info);)", "label": "(19, MPI, MPI_Info_set(info, \"cb_nodes\", \"1\");); (25, MPI, errcode = MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_WRONLY | MPI_MODE_CREATE, info, &fh);); (27, MPI, errcode = MPI_File_iwrite_at_all(fh, off, buffer, BUFSIZE, MPI_INT, &request);); (30, MPI, errcode = MPI_File_close(&fh);); (28, MPI, MPI_Wait(&request, &status);); (12, MPI, MPI_Info_create(&info);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (33, MPI, errcode = MPI_File_iread_at_all(fh, off, buf2, BUFSIZE, MPI_INT, &request);); (10, MPI, MPI_Info_set(info, \"romio_cb_write\", \"enable\");); (43, MPI, MPI_Info_free(&info);); (27, MPI, errcode = MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_RDONLY, info, &fh);)"}
{"predict": "(110, OpenMP, #pragma omp barrier); (77, OpenMP, nthread = omp_get_num_threads();); (94, OpenMP, #pragma omp for); (73, OpenMP, #pragma omp master); (70, OpenMP, #pragma omp parallel private(j,iter))", "label": "(109, OpenMP, #pragma omp barrier); (75, OpenMP, nthread = omp_get_num_threads();); (92, OpenMP, #pragma omp for); (71, OpenMP, #pragma omp master); (68, OpenMP, #pragma omp parallel private(j,iter))"}
{"predict": "(14, OpenMP, #pragma omp target parallel for)", "label": "(14, OpenMP, #pragma omp target parallel for)"}
{"predict": "(16, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(16, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &scri_rank);); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &scri_ranks);)"}
{"predict": "(250, OpenCL, error = clSetKernelArg( kernel, 0, sizeof( cl_mem ), &inputStream );); (232, OpenCL, error = clSetKernelArg( kernel, 2, sizeof( int ), &lod );); (219, OpenCL, error = clSetKernelArg( kernel, 1, sizeof( cl_mem ), &image );); (250, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 2, NULL, threads, NULL, 0, NULL, NULL );); (216, OpenCL, error = clSetKernelArg( kernel, 0, sizeof( cl_mem ), &inputStream );); (218, OpenCL, error = clSetKernelArg( kernel, 2, sizeof( int ), &lod );)", "label": "(242, OpenCL, error = clSetKernelArg( kernel, 0, sizeof( cl_mem ), &inputStream );); (228, OpenCL, error = clSetKernelArg( kernel, 2, sizeof( int ), &lod );); (215, OpenCL, error = clSetKernelArg( kernel, 1, sizeof( cl_mem ), &image );); (244, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 1, NULL, threads, NULL, 0, NULL, NULL );); (260, OpenCL, error = clEnqueueReadImage( queue, image, CL_TRUE, origin, region, gEnablePitch? imageInfo->rowPitch : 0, 0, resultValues, 0, NULL, NULL );)"}
{"predict": "(9, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &iam );)", "label": "(9, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &iam );)"}
{"predict": "(36, MPI, MPI_File_close(&c_info->fh);)", "label": "(36, MPI, MPI_File_close(&c_info->fh);)"}
{"predict": "(8, OpenMP, #pragma omp parallel)", "label": "(8, OpenMP, #pragma omp parallel); (10, OpenMP, #pragma omp for); (14, OpenMP, #pragma omp critical)"}
{"predict": "(38, OpenMP, std::cout << \"Number of threads    = \" << omp_get_max_threads() << std::endl;)", "label": "(38, OpenMP, std::cout << \"Number of threads    = \" << omp_get_max_threads() << std::endl;)"}
{"predict": "(32, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (34, CUDA, CUDA_CHECK(cudaFree(d_data));)", "label": "(32, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (34, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(8, SYCL, sycl::accessor a(Aval);); (16, SYCL, sycl::accessor b(Bval);)", "label": "(16, SYCL, auto b = B.get_access<sycl::access_mode::write>(cgh);); (5, SYCL, auto a = A.get_access<sycl::access_mode::write>(cgh);)"}
{"predict": "(20, OpenCL, clReleaseMemObject(mobjA_);); (16, OpenCL, clReleaseMemObject(mobjX_);)", "label": "(20, OpenCL, clReleaseMemObject(mobjA_);); (16, OpenCL, clReleaseMemObject(mobjX_);)"}
{"predict": "(4, TBB, tbb::flow::graph g;); (39, TBB, tbb::flow::make_edge( q, q2 );); (26, TBB, tbb::flow::make_edge(s, q);); (49, TBB, tbb::flow::make_edge( q2, q3 );); (0, TBB, tbb::flow::queue_node<T> q2(g);); (0, TBB, tbb::flow::queue_node<T> q3(g);); (54, TBB, tbb::flow::remove_edge( q, q2 );); (58, TBB, tbb::flow::make_edge( q, q2 );); (18, TBB, tbb::flow::make_edge( q, s );); (88, TBB, tbb::flow::remove_edge( q,  q2 );); (87, TBB, tbb::flow::remove_edge( q,  q3 );); (82, TBB, tbb::flow::make_edge( q3, q2 );); (45, TBB, tbb::flow::remove_edge( q, q2 );); (-5, TBB, tbb::flow::queue_node<T> s(g);); (74, TBB, tbb::flow::make_edge( q2, q3 );); (74, TBB, tbb::flow::make_edge( q3, q2 );); (61, TBB, tbb::flow::remove_edge( q,  q2 );); (57, TBB, tbb::flow::remove_edge( q,  q3 );); (-14, TBB, tbb::flow::graph_init init(1);); (58, TBB, tbb::flow::make_edge( q, q2 );); (75, TBB, tbb::flow::remove_edge( q3, q2 );); (60, TBB, tbb::flow::remove_edge( q,  q2 );); (-22, TBB, tbb::flow::graph destroy_graph(tbb::flow::graph_init::automatic);); (63, TBB, tbb::flow::remove_edge( q3, q2 );); (-10, TBB, tbb::flow::queue_node<T> q(g);); (55, TBB, tbb", "label": "(4, TBB, tbb::flow::queue_node<T> q2(g);); (70, TBB, tbb::flow::make_edge( q2, q3 );); (67, TBB, tbb::flow::queue_node<T> q3(g);); (37, TBB, tbb::flow::make_edge( q, q2 );); (56, TBB, tbb::flow::remove_edge( q, q2 );); (87, TBB, tbb::flow::remove_edge( q,  q2 );); (-3, TBB, tbb::flow::queue_node<T> q(g);); (-5, TBB, tbb::flow::graph g;)"}
{"predict": "(39, OpenCL, retVal = clReleaseCommandQueue(cmdQ);); (13, OpenCL, cl_command_queue cmdQ = clCreateCommandQueueWithProperties(context.get(), clDevice, properties, &retVal);); (1, OpenCL, clGetDeviceInfo(clDevice, CL_DEVICE_SLICE_COUNT_INTEL, sizeof(size_t), &maxSliceCount, nullptr);)", "label": "(14, OpenCL, cl_command_queue cmdQ = clCreateCommandQueueWithProperties(context.get(), clDevice, properties, &retVal);); (38, OpenCL, retVal = clReleaseCommandQueue(cmdQ);); (1, OpenCL, clGetDeviceInfo(clDevice, CL_DEVICE_SLICE_COUNT_INTEL, sizeof(size_t), &maxSliceCount, nullptr);)"}
{"predict": "(155, OpenMP, const int th=omp_get_thread_num();); (19, OpenMP, #pragma omp parallel for schedule (guided))", "label": "(205, OpenMP, const int th=omp_get_thread_num();); (19, OpenMP, #pragma omp parallel for schedule (guided))"}
{"predict": "(8, MPI, MPI_Init(&argc, &argv);); (89, MPI, MPI_Finalize();); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (58, MPI, MPI_Barrier(MPI_COMM_WORLD);); (22, MPI, MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);); (0, MPI, MPI_Init(&argc, &argv);)", "label": "(7, MPI, MPI_Init(&argc, &argv);); (85, MPI, MPI_Finalize();); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (57, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(13, CUDA, os << \"const unsigned int group_start_id = \" << m.getIndex() << \"\\\"\";); (10, CUDA, CUDA_CHECK(cudaMalloc(&m.getGroups().data(), sizeBytes));)", "label": "(14, CUDA, os << \"__device__ \";); (8, CUDA, os << \"__device__ __constant__ \";)"}
{"predict": "(11, OpenMP, #pragma omp target teams distribute parallel for)", "label": "(12, OpenMP, #pragma omp target teams); (9, OpenMP, #pragma omp target update to(vec.v[0:N])); (16, OpenMP, #pragma omp target update from(vec.v[0:N])); (10, OpenMP, #pragma omp distribute parallel for)"}
{"predict": "(12, OpenMP, #pragma omp target enter data map(to: data)); (16, OpenMP, #pragma omp target exit data map(from: data)); (13, OpenMP, #pragma omp target teams); (19, OpenMP, #pragma omp target teams distribute parallel for reduction(+:sum))", "label": "(12, OpenMP, #pragma omp target teams distribute parallel for)"}
{"predict": "(21, OpenCL, retVal = clReleaseMemObject(buffer);); (0, OpenCL, cl_mem buffer = clCreateBuffer(clContext, flags, size, hostPtr, &retVal);); (25, OpenCL, EXPECT_EQ(clReleaseContext(context), CL_SUCCESS);); (22, OpenCL, retVal = clReleaseMemObject(subBuffer);); (15, OpenCL, cl_mem subBuffer = clCreateSubBuffer(buffer, flags, CL_BUFFER_CREATE_TYPE_REGION, &region, &retVal);); (8, OpenCL, EXPECT_EQ(clReleaseCommandQueue(commandQueue), CL_SUCCESS);); (3, OpenCL, EXPECT_EQ(clReleaseMemObject(unusedBuffer), CL_SUCCESS);)", "label": "(21, OpenCL, retVal = clReleaseMemObject(subBuffer);); (26, OpenCL, retVal = clReleaseMemObject(notUsedBuffer);); (28, OpenCL, EXPECT_EQ(clReleaseContext(context), CL_SUCCESS);); (-2, OpenCL, cl_mem notUsedBuffer = clCreateBuffer(clContext, flags, size, hostPtr, &retVal);); (20, OpenCL, retVal = clReleaseMemObject(buffer);); (0, OpenCL, cl_mem buffer = clCreateBuffer(clContext, flags, size, hostPtr, &retVal);); (8, OpenCL, cl_mem subBuffer = clCreateSubBuffer(buffer, flags, CL_BUFFER_CREATE_TYPE_REGION, &region, &retVal);)"}
{"predict": "(157, MPI, rc = MPI_Reduce(usinbuf, usoutbuf, 3, MPI_UNSIGNED_SHORT, MPI_BXOR, 0, comm);); (23, MPI, MPI_Comm_size(comm, &size);); (125, MPI, rc = MPI_Reduce(cinbuf, coutbuf, 3, MPI_BYTE, MPI_BXOR, 0, comm);); (64, MPI, rc = MPI_Reduce(scinbuf, scoutbuf, 3, MPI_SIGNED_CHAR, MPI_BXOR, 0, comm);); (183, MPI, rc = MPI_Reduce(iinbuf, ioutbuf, 3, MPI_INT, MPI_BXOR, 0, comm);); (212, MPI, rc = MPI_Reduce(llinbuf, lloutbuf, 3, MPI_LONG_LONG, MPI_BXOR, 0, comm);); (305, MPI, MPI_Finalize();); (270, MPI, MPI_Comm_free(&comm);); (16, MPI, MPI_Comm_set_errhandler(comm, MPI_ERRORS_RETURN);); (276, MPI, MPI_Reduce(llinbuf, lloutbuf, 3, MPI_LONG_LONG, MPI_BXOR, 0, comm);); (17, MPI, MPI_Comm_rank(comm, &rank);); (207, MPI, rc = MPI_Reduce(ulinbuf, uloutbuf, 3, MPI_UNSIGNED_LONG, MPI_BXOR, 0, comm);); (12, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &comm);); (143, MPI, rc = MPI_Reduce(sinbuf, soutbuf, 3, MPI_SHORT, MPI_BXOR, 0, comm);); (84, MPI, rc = MPI_Reduce(ucinbuf, ucoutbuf, 3, MPI_UNSIGNED_CHAR, MPI_BXOR, 0, comm);); (239, MPI, rc = MPI_Reduce(ulinbuf, uloutbuf, 3, MPI_UNSIGNED_LONG, MPI_BXOR, 0, comm);)", "label": "(25, MPI, MPI_Comm_size(comm, &size);); (97, MPI, rc = MPI_Reduce(ucinbuf, ucoutbuf, 3, MPI_UNSIGNED_CHAR, MPI_BXOR, 0, comm);); (156, MPI, rc = MPI_Reduce(sinbuf, soutbuf, 3, MPI_SHORT, MPI_BXOR, 0, comm);); (125, MPI, rc = MPI_Reduce(cinbuf, coutbuf, 3, MPI_BYTE, MPI_BXOR, 0, comm);); (274, MPI, rc = MPI_Reduce(linbuf, loutbuf, 3, MPI_LONG, MPI_BXOR, 0, comm);); (63, MPI, rc = MPI_Reduce(scinbuf, scoutbuf, 3, MPI_SIGNED_CHAR, MPI_BXOR, 0, comm);); (31, MPI, rc = MPI_Reduce(cinbuf, coutbuf, 3, MPI_CHAR, MPI_BXOR, 0, comm);); (335, MPI, rc = MPI_Reduce(llinbuf, lloutbuf, 3, MPI_LONG_LONG, MPI_BXOR, 0, comm);); (14, MPI, MPI_Comm_set_errhandler(comm, MPI_ERRORS_RETURN);); (299, MPI, rc = MPI_Reduce(ulinbuf, uloutbuf, 3, MPI_UNSIGNED_LONG, MPI_BXOR, 0, comm);); (238, MPI, rc = MPI_Reduce(iinbuf, ioutbuf, 3, MPI_INT, MPI_BXOR, 0, comm);); (13, MPI, MPI_Comm_rank(comm, &rank);); (206, MPI, rc = MPI_Reduce(uinbuf, uoutbuf, 3, MPI_UNSIGNED, MPI_BXOR, 0, comm);); (175, MPI, rc = MPI_Reduce(usinbuf, usoutbuf, 3, MPI_UNSIGNED_SHORT, MPI_BXOR, 0, comm);)"}
{"predict": "(19, CUDA, ColorSpinorParam cudaParam(cpuParam, *inv_param, QUDA_CUDA_FIELD_LOCATION);)", "label": "(19, CUDA, ColorSpinorParam cudaParam(cpuParam, *inv_param, QUDA_CUDA_FIELD_LOCATION);)"}
{"predict": "(145, OpenMP, lsm303ah_mg_offset_temp_comp_set(&dev_ctx_mg, PROPERTY_ENABLE);)", "label": "(145, OpenMP, lsm303ah_mg_offset_temp_comp_set(&dev_ctx_mg, PROPERTY_ENABLE);)"}
{"predict": "(87, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (87, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)", "label": "(87, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (87, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)"}
{"predict": "(22, CUDA, nvinfer1::ICudaEngine* engine = builder->buildCudaEngine(*network);)", "label": "(22, CUDA, nvinfer1::ICudaEngine* engine = builder->buildCudaEngine(*network);)"}
{"predict": "(37, OpenCL, streams[1] = clCreateBuffer(context, CL_MEM_READ_WRITE, length, NULL, &err);); (57, OpenCL, err |= clSetKernelArg(kernel, 0, sizeof(cl_mem), &streams[0]);); (57, OpenCL, err |= clSetKernelArg(kernel, 1, sizeof(cl_mem), &streams[1]);); (57, OpenCL, err |= clSetKernelArg(kernel, 2, sizeof(cl_sampler), &sampler);); (53, OpenCL, err |= clSetKernelArg(kernel, 3, sizeof(int), &img_depth);); (53, OpenCL, err |= clSetKernelArg(kernel, 4, sizeof(int), &img_width);); (53, OpenCL, err |= clSetKernelArg(kernel, 5, sizeof(int), &img_height);)", "label": "(58, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(streams[0]), &streams[0]);); (58, OpenCL, err |= clSetKernelArg(kernel, 1, sizeof(streams[1]), &streams[1]);); (58, OpenCL, err |= clSetKernelArg(kernel, 2, sizeof(sampler), &sampler);); (33, OpenCL, streams[1] = clCreateBuffer(context, CL_MEM_READ_WRITE, length, NULL, &err);)"}
{"predict": "(41, CUDA, cudaDeviceSynchronize();)", "label": "(41, CUDA, cudaDeviceSynchronize();)"}
{"predict": "(7, OpenMP, #pragma omp atomic)", "label": "(7, OpenMP, #pragma omp atomic)"}
{"predict": "(22, CUDA, cudaMemcpy(&max, tempCfl, sizeof(float), cudaMemcpyDeviceToHost);); (15, CUDA, cudaDeviceSynchronize();)", "label": "(21, CUDA, CUDA_SAFE_CALL(cudaMemcpy(&max, tempCfl, sizeof(float), cudaMemcpyDeviceToHost));)"}
{"predict": "(24, CUDA, cudaSetDevice(deviceId);); (20, CUDA, cudaGetDeviceCount(&nDevices);); (9, MPI, MPIManager mpi_manager(MPI_COMM_WORLD);)", "label": "(24, CUDA, cudaSetDevice(deviceId);); (20, CUDA, cudaGetDeviceCount(&nDevices);); (9, MPI, MPIManager mpi_manager(MPI_COMM_WORLD);)"}
{"predict": "(8, OpenMP, #pragma omp distribute parallel for schedule(guided, argc) default(none) copyin(g) dist_schedule(static, a) private(a) shared(argc)); (20, OpenMP, #pragma omp distribute parallel for private(argc, b), firstprivate(argv, c), lastprivate(d, f) collapse(2) schedule(auto) if (argc) num_threads(a) default(shared) shared(e) reduction(+ : h) dist_schedule(static, b)); (5, OpenMP, #pragma omp threadprivate(g)); (1, OpenMP, #pragma omp target); (1, OpenMP, #pragma omp teams)", "label": "(5, OpenMP, #pragma omp threadprivate(g)); (21, OpenMP, #pragma omp distribute parallel for private(argc, b), firstprivate(argv, c), lastprivate(d, f) collapse(2) schedule(auto) if (argc) num_threads(a) default(shared) shared(e) reduction(+ : h) dist_schedule(static, b)); (7, OpenMP, #pragma omp distribute parallel for schedule(guided, argc) default(none) copyin(g) dist_schedule(static, a) private(a) shared(argc)); (4, OpenMP, #pragma omp target); (4, OpenMP, #pragma omp teams)"}
{"predict": "(22, MPI, mpi_errno = MPI_Comm_dup(MPI_COMM_WORLD, &comm_hdls[i]);); (31, MPI, mpi_errno = MPI_Comm_set_errhandler(comm_hdls[i], MPI_ERRORS_RETURN);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (46, MPI, mpi_errno = MPI_Comm_dup(comm_hdls[0], &comm_hdls[block]);); (26, MPI, MPI_ERRHAND(MPI_File_create_errhandler(ErrorsReturn, &mpi_errno));); (31, MPI, mpi_errno = MPI_Comm_create(comm_hdls[i], block, &comm_hdls[j]);); (20, MPI, MPI_ERRHAND(MPI_File_create_errhandler(ErrorsPending, &mpi_errno));); (18, MPI, MPI_ERRHAND(MPI_File_set_errhandler(comm_hdls[i], MPI_ERRORS_RETURN));)", "label": "(11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (22, MPI, mpi_errno = MPI_Waitall(block, req, error_status);); (10, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (47, MPI, mpi_errno = MPI_Waitall(block, req, MPI_STATUSES_IGNORE);); (17, MPI, MPI_Comm_idup(MPI_COMM_WORLD, &comm_hdls[i], &req[block++]);); (28, MPI, mpi_errno = MPI_Wait(&req[j], MPI_STATUSES_IGNORE);)"}
{"predict": "(13, TBB, double serial_time = (tbb::tick_count::now() - t0).seconds();); (10, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (19, TBB, double tbb_time = (tbb::tick_count::now() - t1).seconds();); (16, TBB, tbb::tick_count t1 = tbb::tick_count::now();)", "label": "(13, TBB, double serial_time = (tbb::tick_count::now() - t0).seconds();); (10, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (19, TBB, double tbb_time = (tbb::tick_count::now() - t1).seconds();); (16, TBB, tbb::tick_count t1 = tbb::tick_count::now();)"}
{"predict": "(3, CUDA, extern __shared__ double sdat[];)", "label": "(3, CUDA, extern __shared__ double sdat[];)"}
{"predict": "(79, OpenMP, #pragma\tomp\tparallel for private (lsum,thread_id1,rtemp,nn,lbstart,lbend,ik,usub,i,irow,frow,nnroot_send_tmp)); (84, OpenMP, #pragma\tomp\ttaskloop private (lsum,thread_id1,rtemp,nn,lbstart,lbend,ik,usub,i,irow,frow,nnroot_send_tmp) untied); (84, OpenMP, #pragma\tomp\ttaskloop private (lsum,thread_id1,rtemp,nn,lbstart,lbend,ik,usub,i,irow,frow,nnroot_send_tmp) if (taskloop: true))", "label": "(82, OpenMP, #pragma\tomp\ttaskloop firstprivate (stat) private (thread_id1,Uinv,nn,lbstart,lbend,ub,temp,rtemp_loc,ik,lk1,gik,gikcol,usub,uval,lsub,lusup,iknsupc,il,i,irow,bmod_tmp,p,ii,jj,t1,t2,j,ikfrow,iklrow,dest,y,uptr,fnz,nsupr,nroot_send_tmp) untied nogroup); (126, OpenMP, #pragma omp simd); (141, OpenMP, #pragma omp atomic capture); (75, OpenMP, #pragma\tomp\tparallel for private (thread_id1,Uinv,nn,lbstart,lbend,ub,temp,rtemp_loc,ik,lk1,gik,gikcol,usub,uval,lsub,lusup,iknsupc,il,i,irow,bmod_tmp,p,ii,jj,t1,t2,j,ikfrow,iklrow,dest,y,uptr,fnz,nsupr))"}
{"predict": "(8, CUDA, thrust::device_vector<T> input(d_input);)", "label": "(8, CUDA, auto input   = thrust::host_vector<coordinate<T>>(d_input);)"}
{"predict": "(20, TBB, my_thread_array = tbb::cache_aligned_allocator<padded_ipc_worker>().allocate( my_n_thread );); (27, TBB, my_waker = tbb::cache_aligned_allocator<tbb::flow::waker>().allocate(1);); (30, TBB, my_stopper = tbb::cache_aligned_allocator<tbb::flow::stopper>().allocate(1);)", "label": "(20, TBB, my_thread_array = tbb::cache_aligned_allocator<padded_ipc_worker>().allocate( my_n_thread );); (14, TBB, my_n_thread = tbb::detail::r1::AvailableHwConcurrency();); (25, TBB, my_waker = tbb::cache_aligned_allocator<ipc_waker>().allocate(1);); (27, TBB, my_stopper = tbb::cache_aligned_allocator<ipc_stopper>().allocate(1);)"}
{"predict": "(81, OpenMP, #pragma omp teams distribute parallel for num_teams(20) num_threads(512)); (18, OpenMP, #pragma omp distribute parallel for); (60, OpenMP, #pragma omp teams num_teams(20) thread_limit(768)); (34, OpenMP, #pragma omp distribute parallel for num_threads(64)); (52, OpenMP, #pragma omp teams num_teams(20) thread_limit(128)); (63, OpenMP, #pragma omp distribute parallel for num_threads(32)); (35, OpenMP, #pragma omp target teams); (92, OpenMP, #pragma omp teams distribute parallel for); (77, OpenMP, #pragma omp teams distribute parallel for num_teams(20) thread_limit(768)); (18, OpenMP, #pragma omp target teams); (81, OpenMP, #pragma omp teams distribute parallel for num_teams(20) thread_limit(128)); (40, OpenMP, #pragma omp teams); (12, OpenMP, #pragma omp target); (20, OpenMP, #pragma omp teams num_teams(20)); (58, OpenMP, #pragma omp teams distribute parallel for num_teams(20) thread_limit(64)); (46, OpenMP, #pragma omp teams distribute parallel for); (29, OpenMP, #pragma omp teams num_teams(20))", "label": "(19, OpenMP, #pragma omp distribute parallel for); (17, OpenMP, #pragma omp target); (45, OpenMP, #pragma omp distribute parallel for num_threads(128)); (72, OpenMP, #pragma omp teams distribute parallel for thread_limit(512) num_threads(128)); (22, OpenMP, #pragma omp target teams thread_limit(64)); (76, OpenMP, #pragma omp teams distribute parallel for); (29, OpenMP, #pragma omp teams)"}
{"predict": "(66, CUDA, checkCudaErrors(cudaMalloc((void **) &d_odata, numBlocks * sizeof(float)));); (100, CUDA, cudaFree(d_idata);); (12, CUDA, checkCudaErrors(cudaSetDevice(device));); (100, CUDA, cudaFree(d_odata);); (63, CUDA, checkCudaErrors(cudaMalloc((void **) &d_idata, bytes));); (96, CUDA, cudaDeviceReset();)", "label": "(68, CUDA, checkCudaErrors(cudaMalloc((void **)&d_odata, numBlocks * sizeof(float)));); (105, CUDA, cudaFree(d_odata);); (65, CUDA, checkCudaErrors(cudaMalloc((void **)&d_idata, bytes));); (10, CUDA, checkCudaErrors(cudaGetDeviceProperties(&prop, device));); (8, CUDA, checkCudaErrors(cudaSetDevice(device));); (100, CUDA, cudaFree(d_idata);); (65, CUDA, checkCudaErrors(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));)"}
{"predict": "(20, CUDA, cudaFree(res_dev);); (13, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);); (9, CUDA, cudaMalloc(&res_dev, sizeof(float));)", "label": "(20, CUDA, cudaFree(res_dev);); (10, CUDA, cudaMalloc(&res_dev, sizeof(float));); (12, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (1, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(2, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (2, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(13, TBB, tbb::parallel_for(tbb::blocked_range<size_t>(0, end, 10000), accum);)", "label": "(14, TBB, return tbb::parallel_reduce(range, 0, accum, std::plus<>());); (3, TBB, auto range = tbb::blocked_range<size_t>(UL(0), end);)"}
{"predict": "(12, CUDA, cudaDeviceSynchronize();)", "label": "(12, CUDA, cudaDeviceSynchronize();)"}
{"predict": "(11, MPI, MPI_Type_size_x(type, &typesize);)", "label": "(11, MPI, MPI_Type_size_x(type, &typesize);)"}
{"predict": "(15, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)", "label": "(15, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(81, CUDA, flamegpu::CUDASimulation cudaSimulation(model);)", "label": "(80, CUDA, flamegpu::CUDASimulation cudaSimulation(model);)"}
{"predict": "(16, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);)", "label": "(16, MPI, MPI_Comm_size(MPI_COMM_WORLD,&mpi_size);); (16, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&mpi_rank);)"}
{"predict": "(58, MPI, MPI_Unpack(unpack_buf, unpack_size, &pos, check_unpack_buf, unpack_size, MPI_BYTE, MPI_COMM_WORLD);); (74, MPI, MPI_Type_iov_len(datatype, offset, &idx, &actual_bytes);); (55, MPI, MPI_Pack(check_buf, 1, datatype, pack_buf, pack_size, MPI_BYTE, MPI_COMM_WORLD);); (63, MPI, MPI_Pack(unpack_buf, 1, datatype, pack_buf, pack_size, MPI_BYTE, MPI_COMM_WORLD);); (55, MPI, MPI_Type_size_x(datatype, &type_size);)", "label": "(63, MPI, MPI_Unpack(check_pack_buf, pack_size, &pos, unpack_buf, 1, datatype, MPI_COMM_WORLD);); (6, MPI, MPI_Type_size_c(datatype, &type_size);); (34, MPI, MPI_Pack_size(1, datatype, MPI_COMM_WORLD, &pack_size);); (54, MPI, MPI_Pack(check_unpack_buf, 1, datatype, pack_buf, pack_size, &pos, MPI_COMM_WORLD);)"}
{"predict": "(4, OpenMP, #pragma omp parallel for)", "label": "(4, OpenMP, #pragma omp parallel for)"}
{"predict": "(201, OpenCL, err = clSetKernelArg( kernel, 0, sizeof( cl_mem ), &bufA );); (202, OpenCL, err |= clSetKernelArg( kernel, 1, sizeof( cl_mem ), &bufB );); (200, OpenCL, err |= clSetKernelArg( kernel, 2, sizeof( int ), &num_elements );); (201, OpenCL, err |= clSetKernelArg( kernel, 3, sizeof( int ), &vectorSize );); (202, OpenCL, err |= clSetKernelArg( kernel, 4, sizeof( int ), &num_runs_shift );); (200, OpenCL, err |= clSetKernelArg( kernel, 5, sizeof( int ), &testID );); (201, OpenCL, err |= clSetKernelArg( kernel, 6, sizeof( int ), &end_test_ID );); (199, OpenCL, err |= clSetKernelArg( kernel, 7, sizeof( int ), &start_test_ID );); (200, OpenCL, err |= clSetKernelArg( kernel, 8, sizeof( int ), &num_threads );); (201, OpenCL, err |= clSetKernelArg( kernel, 9, sizeof( int ), &thread_num );); (192, OpenCL, err = clSetKernelArg( kernel, 0, sizeof( cl_mem ), &bufA );); (193, OpenCL, err |= clSetKernelArg( kernel, 1, sizeof( cl_mem ), &bufB );); (193, OpenCL, err |= clSetKernelArg( kernel, 2, sizeof( int ), &num_elements );); (194, OpenCL, err |= clSetKernelArg( kernel, 3, sizeof( int ), &vectorSize );); (195, OpenCL, err |= clSetKernelArg( kernel, 4, sizeof( int ), &num_runs_shift );); (192, OpenCL, err |= clSetKernelArg( kernel, 5, sizeof( int ), &testID );); (193, OpenCL, err |= clSetKernelArg( kernel, 6, sizeof( int ), &end_test_ID );); (194, OpenCL, err |= clSetKernel(123, OpenCL, err = clEnqueueReadBuffer( queue, pThreadData->m_output_buf, CL_TRUE, 0, num_elements *", "label": "(113, OpenCL, err = clEnqueueWriteBuffer( queue, pThreadData->m_streams[1], CL_FALSE, 0, pThreadData->m_type_size*num_elements * inputBVecSize, (void *)pThreadData->m_input_ptr[1], 0, NULL, NULL );); (110, OpenCL, err = clEnqueueWriteBuffer(queue, pThreadData->m_streams[0], CL_FALSE, 0, pThreadData->m_type_size*num_elements * inputAVecSize, (void *)pThreadData->m_input_ptr[0], 0, NULL, NULL);); (117, OpenCL, err = clEnqueueReadBuffer( queue, pThreadData->m_streams[2], CL_TRUE, 0, pThreadData->m_type_size*num_elements * vectorSize, (void *)pThreadData->m_output_ptr, 0, NULL, NULL );); (113, OpenCL, err = clEnqueueNDRangeKernel( queue, pThreadData->m_kernel[i], 1, NULL, threads, NULL, 0, NULL, NULL );)"}
{"predict": "(91, OpenMP, stencil_time = omp_get_wtime();); (113, OpenMP, if (iter==1) stencil_time = omp_get_wtime();)", "label": "(114, OpenMP, if (iter==1) stencil_time = omp_get_wtime();); (59, OpenMP, std::cout << \"Number of threads (max)   = \" << omp_get_max_threads() << std::endl;); (125, OpenMP, stencil_time = omp_get_wtime() - stencil_time;)"}
{"predict": "(36, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (83, CUDA, cudaFree(d_in);); (83, CUDA, cudaFree(d_out);); (32, CUDA, cudaMalloc((void**)(&d_in), in_bytes);); (35, CUDA, cudaMemcpy(d_in, tensor.data(), in_bytes, cudaMemcpyHostToDevice);); (45, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (33, CUDA, cudaMemcpy(d_out, tensor_arg.data(), out_bytes, cudaMemcpyHostToDevice);); (42, CUDA, assert(cudaMemcpyAsync(tensor_arg.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);)", "label": "(36, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (83, CUDA, cudaFree(d_in);); (83, CUDA, cudaFree(d_out);); (32, CUDA, cudaMalloc((void**)(&d_in), in_bytes);); (45, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (33, CUDA, cudaMemcpy(d_in, tensor.data(), in_bytes, cudaMemcpyHostToDevice);); (42, CUDA, assert(cudaMemcpyAsync(tensor_arg.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);)"}
{"predict": "(28, MPI, MPI_Recv(buf, count, newtype, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (22, MPI, MPI_Type_commit(&newtype);); (12, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (37, MPI, MPI_Type_free(&newtype);); (20, MPI, MPI_Type_free(&types[0]);); (17, MPI, MPI_Type_create_struct(2, lens, displs, types, &newtype);); (23, MPI, MPI_Send(buf, count, newtype, 0, 0, MPI_COMM_WORLD);); (14, MPI, MPI_Type_contiguous(count, MPI_INTEGER, &types[0]);)", "label": "(30, MPI, MPI_Send(MPI_BOTTOM, 1, newtype, 1, 0, MPI_COMM_WORLD);); (23, MPI, MPI_Type_commit(&newtype);); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (20, MPI, MPI_Type_create_struct(2, lens, displs, types, &newtype);); (15, MPI, MPI_Get_address(&count, &displs[0]);); (27, MPI, MPI_Recv(buf, 6, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (35, MPI, MPI_Type_free(&newtype);); (15, MPI, types[1] = MPI_Type_f2c(*ftype);)"}
{"predict": "(44, OpenMP, #pragma omp for simd linear(i : 4)); (39, OpenMP, #pragma omp for simd linear(j)); (30, OpenMP, #pragma omp for simd linear(argv[1])); (45, OpenMP, #pragma omp for simd linear(i : i)); (29, OpenMP, #pragma omp for simd linear(e, g)); (38, OpenMP, #pragma omp for simd linear(i)); (9, OpenMP, #pragma omp for simd linear(); (40, OpenMP, #pragma omp for simd linear(i : z)); (9, OpenMP, #pragma omp for simd linear(argc); (42, OpenMP, #pragma omp for simd linear(i : i)); (3, OpenMP, #pragma omp for simd linear); (22, OpenMP, #pragma omp for simd linear(a, b)); (11, OpenMP, #pragma omp for simd linear(argc > 0? argv[1] : argv[2])); (20, OpenMP, #pragma omp for simd linear(h, C::x)); (7, OpenMP, #pragma omp for simd linear(argc,); (20, OpenMP, #pragma omp for simd linear(k); (11, OpenMP, #pragma omp for simd linear(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (-2, OpenMP, #pragma omp for simd linear()); (15, OpenMP, #pragma omp for simd linear(S1)); (32, OpenMP, #pragma omp for simd linear(i : i)); (5, OpenMP, #pragma omp for simd linear(argc)", "label": "(11, OpenMP, #pragma omp taskloop simd linear(f) linear(f)); (54, OpenMP, #pragma omp taskloop simd linear(ref(j))); (35, OpenMP, #pragma omp taskloop simd linear(a, b)); (26, OpenMP, #pragma omp taskloop simd linear (argc > 0? argv[1] : argv[2])); (27, OpenMP, #pragma omp taskloop simd linear (argc, z)); (10, OpenMP, #pragma omp taskloop simd linear (); (39, OpenMP, #pragma omp taskloop simd linear(h, C::x)); (16, OpenMP, #pragma omp taskloop simd linear (foo())); (49, OpenMP, #pragma omp taskloop simd linear(i)); (38, OpenMP, #pragma omp parallel); (17, OpenMP, #pragma omp taskloop simd linear (argc,); (2, OpenMP, #pragma omp taskloop simd linear); (13, OpenMP, #pragma omp taskloop simd linear (argc); (37, OpenMP, #pragma omp taskloop simd linear(val(i))); (19, OpenMP, #pragma omp taskloop simd linear (S1)); (24, OpenMP, #pragma omp taskloop simd linear (argv[1])); (3, OpenMP, #pragma omp taskloop simd linear (val); (4, OpenMP, #pragma omp taskloop simd linear (ref())); (34, OpenMP, #pragma omp taskloop simd linear(uval(i) : 4)); (24, OpenMP, #pragma omp taskloop simd linear(val(e, g))); (-3, OpenMP, #pragma omp taskloop simd linear ())"}
{"predict": "(2, OpenMP, #pragma omp taskyield allocate(argc)); (2, OpenMP, #pragma omp taskyield untied); (2, OpenMP, #pragma omp taskyield)", "label": "(3, OpenMP, #pragma omp taskyield untied); (0, OpenMP, #pragma omp taskyield); (2, OpenMP, #pragma omp taskyield unknown)"}
{"predict": "(9, CUDA, cudaFuncGetAttributes(&attr,ptr);); (6, CUDA, cudaFuncGetBlockSize(&qblocksize, &mingridsize, ptr);)", "label": "(9, CUDA, cudaFuncGetAttributes(&attr,(void*)ptr);); (14, CUDA, fcuda::Check_CudaErroorFun(\"Error collecting kernel information.\");); (5, CUDA, cudaOccupancyMaxPotentialBlockSize(&mingridsize,&qblocksize,(void*)ptr,0,0);)"}
{"predict": "(52, TBB, tbb::flow::make_edge( b2, b3 );); (50, TBB, tbb::flow::make_edge( b, b2 );); (73, TBB, tbb::flow::buffer_node<T> b_copy(b);); (0, TBB, tbb::flow::graph g;); (0, TBB, tbb::flow::buffer_node<T> b3(g);); (0, TBB, tbb::flow::buffer_node<T> b2(g);); (-4, TBB, tbb::flow::graph g;); (45, TBB, tbb::flow::make_edge( b3, b_copy );); (-7, TBB, tbb::flow::graph g;); (-7, TBB, tbb::flow::buffer_node<T> b(g);)", "label": "(52, TBB, tbb::flow::make_edge( b2, b3 );); (50, TBB, tbb::flow::make_edge( b, b2 );); (73, TBB, tbb::flow::buffer_node<T> b_copy(b);); (2, TBB, tbb::flow::buffer_node<T> b3(g);); (0, TBB, tbb::flow::buffer_node<T> b2(g);); (-3, TBB, tbb::flow::graph g;); (-3, TBB, tbb::flow::buffer_node<T> b(g);)"}
{"predict": "(131, OpenCL, err |= clSetKernelArg(kernel, 0, sizeof(streams[0]), &streams[0]);); (132, OpenCL, err |= clSetKernelArg(kernel, 1, sizeof(streams[1]), &streams[1]);); (133, OpenCL, err |= clSetKernelArg(kernel, 2, sizeof(streams[2]), &streams[2]);); (130, OpenCL, err |= clSetKernelArg(kernel, 3, sizeof(streams[3]), &streams[3]);); (131, OpenCL, err |= clSetKernelArg(kernel, 4, sizeof(streams[4]), &streams[4]);)", "label": "(132, OpenCL, err |= clSetKernelArg(kernel, 1, sizeof(cl_mem), &streams[1]);); (132, OpenCL, err |= clSetKernelArg(kernel, 2, sizeof(cl_mem), &streams[2]);); (132, OpenCL, err |= clSetKernelArg(kernel, 3, sizeof(cl_mem), &streams[3]);); (128, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(cl_mem), &streams[0]);)"}
{"predict": "(25, OpenMP, #pragma omp parallel default(shared)); (10, OpenMP, #pragma omp parallel); (39, OpenMP, #pragma omp parallel num_threads(x)); (30, OpenMP, #pragma omp parallel default(none)); (15, OpenMP, #pragma omp parallel default(none)); (27, OpenMP, #pragma omp parallel default(firstprivateKind)); (16, OpenMP, #pragma omp parallel default(shared))", "label": "(19, OpenMP, #pragma omp parallel default(shared)); (11, OpenMP, #pragma omp parallel); (38, OpenMP, #pragma omp parallel num_threads(x)); (23, OpenMP, #pragma omp parallel default(none)); (29, OpenMP, #pragma omp parallel default(firstprivate))"}
{"predict": "(3, TBB, typedef oneapi::tbb::blocked_range2d<row_type,col_type> range_type;); (19, TBB, AssertSameType( r.cols(), oneapi::tbb::blocked_range<col_type>( coli, colj, 1 ));); (17, TBB, AssertSameType( r.rows(), oneapi::tbb::blocked_range<row_type>( rowi, rowj, 1 ));); (21, TBB, range_type r2(r,oneapi::tbb::split());)", "label": "(24, TBB, range_type r2(r,tbb::split());); (2, TBB, typedef tbb::blocked_range2d<row_type,col_type> range_type;); (18, TBB, AssertSameType( r.cols(), tbb::blocked_range<col_type>( coli, colj, 1 ));); (16, TBB, AssertSameType( r.rows(), tbb::blocked_range<row_type>( rowi, rowj, 1 ));)"}
{"predict": "(8, OpenMP, #pragma omp parallel); (11, OpenMP, #pragma omp flush); (13, OpenMP, #pragma omp for private(sum0, i)); (20, OpenMP, #pragma omp critical)", "label": "(13, OpenMP, #pragma omp for private(sum0) schedule(static,1)); (7, OpenMP, #pragma omp parallel); (22, OpenMP, #pragma omp critical); (13, OpenMP, #pragma omp flush)"}
{"predict": "(122, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&reset_l2norm_done, cudaEventDisableTiming));); (199, MPI, MPI_CALL(MPI_Barrier(MPI_COMM_WORLD));); (20, MPI, MPI_CALL(MPI_Comm_free(&local_comm));); (213, MPI, double stop = MPI_Wtime();); (222, CUDA, CUDA_RT_CALL(cudaStreamDestroy(compute_stream));); (224, CUDA, CUDA_RT_CALL(cudaFree(a));); (224, CUDA, CUDA_RT_CALL(cudaFree(l2_norm_d));); (227, CUDA, CUDA_RT_CALL(cudaFreeHost(a_h));); (20, CUDA, CUDA_RT_CALL(cudaFree(0));); (17, CUDA, CUDA_RT_CALL(cudaSetDevice(local_rank%num_devices));); (62, CUDA, CUDA_RT_CALL(cudaMalloc(&a, nx * (chunk_size + 2) * sizeof(real)));); (17, MPI, MPI_CALL(MPI_Comm_rank(MPI_COMM_WORLD, &rank));); (60, CUDA, CUDA_RT_CALL(cudaMalloc(&a_new, nx * (chunk_size + 2) * sizeof(real)));); (219, CUDA, CUDA_RT_CALL(cudaFree(a_new));); (15, MPI, MPI_CALL(MPI_Comm_size(MPI_COMM_WORLD, &size));); (141, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(push_stream, reset_l2norm_done, 0));); (156, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(compute_stream, push_done, 0));); (62, CUDA, CUDA_RT_CALL(cudaMemset(a_new, 0, nx * (chunk_size + 2) * sizeof(real)));); (221, MPI, MPI_CALL(MPI_Finalize());); (16, CUDA, CUDA_RT_CALL(cudaGetDeviceCount(&num_devices));); (141, CUDA, CUDA_RT_CALL(cudaStreamSynchronize(push_stream));); (7, MPI, MPI_CALL(MPI_Init(&argc, &argv));); (13, CUDA, CUDA_RT_CALL(cudaRank(local_comm, &local_rank));); (92, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&push_prep_done, cudaEventDisableTiming));); (46, CUDA, CUDA_RT_CALL(cudaMallocHost(&a_h, nx * ny * sizeof(real)));); (44, CUDA, CUDA_RT_CALL(cudaMallocHost(&a_ref_h, nx * ny * sizeof(real)));); (218, CUDA, CUDA_RT_CALL(cudaFreeHost(a_ref_h));); (", "label": "(118, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&reset_l2norm_done, cudaEventDisableTiming));); (143, MPI, MPI_CALL(MPI_Barrier(MPI_COMM_WORLD));); (21, MPI, MPI_CALL(MPI_Comm_free(&local_comm));); (249, CUDA, CUDA_RT_CALL(cudaStreamDestroy(compute_stream));); (201, MPI, double stop = MPI_Wtime();); (244, CUDA, CUDA_RT_CALL(cudaEventDestroy(push_done));); (257, CUDA, CUDA_RT_CALL(cudaFree(a));); (-4, MPI, MPI_CALL(MPI_Comm_rank(MPI_COMM_WORLD, &rank));); (94, CUDA, CUDA_RT_CALL(cudaDeviceSynchronize());); (18, CUDA, CUDA_RT_CALL(cudaFree(0));); (16, CUDA, CUDA_RT_CALL(cudaSetDevice(local_rank%num_devices));); (58, CUDA, CUDA_RT_CALL(cudaMalloc(&a, nx * (chunk_size + 2) * sizeof(real)));); (243, CUDA, CUDA_RT_CALL(cudaFree(l2_norm_d));); (138, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(push_stream, reset_l2norm_done, 0));); (100, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&push_prep_done, cudaEventDisableTiming));); (28, CUDA, CUDA_RT_CALL(cudaMallocHost(&a_h, nx * ny * sizeof(real)));); (25, CUDA, CUDA_RT_CALL(cudaMallocHost(&a_ref_h, nx * ny * sizeof(real)));); (84, CUDA, CUDA_RT_CALL(cudaGetLastError());); (145, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(compute_stream, push_prep_done, 0));); (89, CUDA, CUDA_RT_CALL(cudaStreamCreateWithPriority(&compute_stream, cudaStreamDefault, leastPriority));); (51, CUDA, CUDA_RT_CALL(cudaMalloc(&a_new, nx * (chunk_size + 2) * sizeof(real)));); (230, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_stream));); (-17, MPI, MPI_CALL(MPI_Comm_size(MPI_COMM_WORLD, &size));); (170, CUDA, CUDA_RT_CALL(cudaStreamSynchronize(compute_stream));); (125, CUDA, CUDA_RT_CALL(cudaEventRecord(reset_l2norm_done, compute_stream));); (-18, CUDA, CUDA_RT_CALL(cudaGetDeviceCount(&num_devices));); (102, CUDA, CUDA_RT_CALL(cudaMallocHost(&l2_norm_h, sizeof(real)));); (121, CUDA, CUDA_RT_CALL(cudaMemsetAsync(l2_norm_d, 0, sizeof(real), compute_stream));); (-27, MPI, MPI_CALL(MPI_Init(&argc, &argv));); (46, CUDA, CUDA_RT_CALL(cudaMemset(a_new, 0, nx * (chunk_size + 2) * sizeof(real)));); (76, CUDA, CUDA_RT_CALL(cudaDeviceGetStreamPriorityRange(&leastPriority, &greatestPriority));); (219, CUDA, CUDA_RT_CALL(cudaEventDestroy(push_prep_done));); (94, CUDA, CUDA_RT_CALL(cudaMalloc(&l2_norm_d, sizeof(real)));); (-12, MPI, MPI_CALL(MPI_Comm_rank(local_comm, &local_rank));); (233, CUDA, CUDA_RT_CALL(cudaFreeHost(a_ref_h));); (159, MPI, MPI_CALL(MPI_Allreduce(l2_norm_h, &l2_norm, 1, MPI_REAL_TYPE, MPI_SUM, MPI_COMM_WORLD));); (109, MPI, double start = MPI_Wtime();); (79, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&push_done, cudaEventDisableTiming));); (210, CUDA, CUDA_RT_CALL(cudaEventDestroy(reset_l2norm_done));); (215, CUDA, CUDA_RT_CALL(cudaFreeHost(l2_norm_h));); (120, CUDA, CUDA_RT_CALL(cudaEventRecord(push_prep_done, push_stream));)"}
{"predict": "(11, TBB, typedef tbb::combinable<T> sums;); (27, TBB, tbb::parallel_for( tbb::blocked_range<int> (0, N, 10000), ParallelScalarBody<T>(sums) );); (3, TBB, typedef tbb::task_scheduler_init init(tbb::task_scheduler_init::deferred);); (27, TBB, tbb::combinable<T> assigned;); (24, TBB, tbb::combinable<T> sums;); (18, TBB, tbb::combinable<T> sums;); (14, TBB, tbb::task_group_context tbb::task_group_context::isolated;); (-2, TBB, typedef tbb::combinable<T> sums;); (26, TBB, tbb::parallel_for( tbb::blocked_range<int> (0, N, 10000), ParallelScalarBodyNoInit );)", "label": "(30, TBB, tbb::combinable<T> finit_combinable(my_finit_decl);); (32, TBB, tbb::parallel_for( tbb::blocked_range<int>( 0, N, 10000 ), ParallelScalarBodyNoInit<T>( finit_combinable ) );); (24, TBB, if (Verbose && t == 0) t0 = tbb::tick_count::now();); (31, TBB, tbb::parallel_for( tbb::blocked_range<int>( 0, N, 10000 ), ParallelScalarBody<T>( sums ) );); (40, TBB, tbb::combinable<T> assigned;); (-2, TBB, tbb::task_scheduler_init init(tbb::task_scheduler_init::deferred);); (7, TBB, tbb::tick_count t0;); (21, TBB, tbb::combinable<T> sums;)"}
{"predict": "(12, OpenMP, omp_set_num_threads(nr_threads);); (13, OpenMP, #pragma omp parallel default(none) shared(a)); (14, OpenMP, #pragma omp sections); (7, OpenMP, omp_set_dynamic(0);); (14, OpenMP, #pragma omp section); (17, OpenMP, int thread_nr = omp_get_thread_num();)", "label": "(12, OpenMP, omp_set_num_threads(nr_threads);); (13, OpenMP, #pragma omp parallel default(none) shared(a)); (14, OpenMP, #pragma omp sections); (8, OpenMP, omp_set_dynamic(0);); (14, OpenMP, #pragma omp section); (16, OpenMP, int thread_nr = omp_get_thread_num();)"}
{"predict": "(12, CUDA, extern __shared__ float _shared_centroids[];)", "label": "(12, CUDA, extern __shared__ float _shared_centroids[];)"}
{"predict": "(21, CUDA, extern __shared__ float entire_buffer[];)", "label": "(21, CUDA, extern __shared__ float entire_buffer[];)"}
{"predict": "(9, OpenMP, #pragma omp flush (a)); (4, OpenMP, #pragma omp flush acq_rel); (1, OpenMP, #pragma omp flush); (4, OpenMP, #pragma omp flush acquire); (5, OpenMP, #pragma omp flush release); (0, OpenMP, #pragma omp flush a)", "label": "(5, OpenMP, #pragma omp flush acquire); (5, OpenMP, #pragma omp flush release); (1, OpenMP, #pragma omp flush); (4, OpenMP, #pragma omp flush(a)); (0, OpenMP, #pragma omp flush acq_rel)"}
{"predict": "(38, CUDA, CUDA_CHECK(cudaMalloc(&d_A, sizeof(data_type) * lda * n));); (41, CUDA, CUDA_CHECK(cudaMemcpyAsync(d_A, h_A, sizeof(data_type) * lda * n, cudaMemcpyHostToDevice, stream));); (65, CUDA, CUDA_CHECK(cudaFree(d_A_inv));); (65, CUDA, CUDA_CHECK(cudaFree(d_info));); (38, CUDA, CUDA_CHECK(cudaMalloc(&d_info, sizeof(int)));); (42, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (61, CUDA, CUDA_CHECK(cudaDeviceReset());); (44, CUDA, CUDA_CHECK(cudaMemcpyAsync(&h_info, d_info, sizeof(int), cudaMemcpyDeviceToHost, stream));); (26, CUDA, CUDA_CHECK(cudaMalloc(&d_A_inv, sizeof(data_type) * lda * n));); (57, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (29, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));); (59, CUDA, CUDA_CHECK(cudaFree(d_A));)", "label": "(43, CUDA, CUDA_CHECK(cudaMemcpy(d_A_inv, d_A, sizeof(data_type) * lda * n, cudaMemcpyDeviceToDevice));); (47, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (69, CUDA, CUDA_CHECK(cudaFree(d_info));); (34, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * lda * n));); (34, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A_inv), sizeof(data_type) * lda * n));); (65, CUDA, CUDA_CHECK(cudaFree(d_A_inv));); (58, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (32, CUDA, CUDA_CHECK(cudaMalloc(&d_info, sizeof(int)));); (34, CUDA, CUDA_CHECK(cudaMemcpy(d_A, h_A, sizeof(data_type) * lda * n, cudaMemcpyHostToDevice));); (42, CUDA, CUDA_CHECK(cudaMemcpy(&h_info, d_info, sizeof(int), cudaMemcpyDeviceToHost));); (34, CUDA, CUDA_CHECK(cudaMemset(d_info, 0, sizeof(int)));); (58, CUDA, CUDA_CHECK(cudaFree(d_A));); (20, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(28, CUDA, thrust::complex<T> const* v1_(v1 + Q * nchol_max * nwalk);); (28, CUDA, thrust::complex<T2>* vb_(vb + Q * nchol_max * nwalk);); (34, CUDA, for (int w = threadIdx.y; w < nwalk; w += blockDim.y) cudaDeviceSynchronize();); (32, CUDA, for (int n = threadIdx.x; n < nc; n += blockDim.x) cudaDeviceSynchronize();); (62, CUDA, cudaDeviceSynchronize();); (20, CUDA, thrust::complex<T> const* v2_(v1 + Qm * nchol_max * nwalk);)", "label": "(31, CUDA, thrust::complex<T> const* v1_(v1 + Q * nchol_max * nwalk);); (31, CUDA, thrust::complex<T> const* v2_(v1 + Qm * nchol_max * nwalk);); (52, CUDA, thrust::complex<T2> ialpha(alpha * thrust::complex<T2>(0.0, 1.0));); (27, CUDA, thrust::complex<T2>* vb_(vb + nc0 * nwalk);); (46, CUDA, thrust::complex<T2>* vb_(vb + (nc0 + nc) * nwalk);)"}
{"predict": "(83, OpenACC, #pragma acc update host(state[(nz + 2 * hs) * (nx + 2 * hs) * NUM_VARS])); (45, OpenACC, #pragma acc data copyin(state_tmp[(nz + 2 * hs) * (nx + 2 * hs) * NUM_VARS], hy_dens_cell[nz + 2 * hs], hy_dens_theta_cell[nz + 2 * hs], hy_dens_int[nz + 1], hy_dens_theta_int[nz + 1], hy_pressure_int[nz + 1]))", "label": "(83, OpenACC, #pragma acc update host(state[(nz + 2 * hs) * (nx + 2 * hs) * NUM_VARS])); (45, OpenACC, #pragma acc data copyin(state_tmp[(nz + 2 * hs) * (nx + 2 * hs) * NUM_VARS], hy_dens_cell[nz + 2 * hs], hy_dens_theta_cell[nz + 2 * hs], hy_dens_int[nz + 1], hy_dens_theta_int[nz + 1], hy_pressure_int[nz + 1]))"}
{"predict": "(5, MPI, auto host_srcs_ = MpiTraits<T>::get_source();)", "label": "(32, MPI, CALL(MPI_Waitall(static_cast<int>(reqs.size()), reqs.data(), MPI_STATUSES_IGNORE));); (20, MPI, CALL(MPI_Waitall(reqs.size(), reqs.data(), MPI_STATUSES_IGNORE));)"}
{"predict": "(33, MPI, MPI_Allreduce(&my_bcast_time, &total_my_bcast_time, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);); (22, MPI, MPI_Barrier(MPI_COMM_WORLD);); (30, MPI, MPI_Allreduce(&mpi_bcast_time, &total_mpi_bcast_time, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);); (7, MPI, MPI_Init(&argc, &argv);); (42, MPI, MPI_Finalize();); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (25, MPI, MPI_Bcast(data, num_elements, MPI_INT, 0, MPI_COMM_WORLD);); (19, MPI, MPI_time(my_bcast(data, num_elements, MPI_INT, 0, MPI_COMM_WORLD);)", "label": "(28, MPI, total_my_bcast_time += MPI_Wtime();); (31, MPI, total_mpi_bcast_time -= MPI_Wtime();); (45, MPI, MPI_Finalize();); (32, MPI, total_mpi_bcast_time += MPI_Wtime();); (5, MPI, MPI_Init(NULL, NULL);); (28, MPI, MPI_Bcast(data, num_elements, MPI_INT, 0, MPI_COMM_WORLD);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (17, MPI, total_my_bcast_time -= MPI_Wtime();); (15, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(4, CUDA, __shared__ float Bs[block_size][block_size];); (2, CUDA, __shared__ float As[block_size][block_size];)", "label": "(4, CUDA, __shared__ float Bs[block_size][block_size];); (2, CUDA, __shared__ float As[block_size][block_size];)"}
{"predict": "(11, MPI, MPI_Type_get_true_extent(item, &lb, &extent);)", "label": "(11, MPI, MPI_Type_get_extent(item, &lb, &extent);)"}
{"predict": "(20, CUDA, env.profileStream.reset(cudaStreamCreate(&profileStream));); (44, CUDA, env.engine.reset(runtime->deserializeCudaEngine(env.serializedEngine->data(), env serializedEngine->size()));)", "label": "(46, CUDA, env.engine.reset(runtime->deserializeCudaEngine(env.serializedEngine->data(), env.serializedEngine->size()));); (19, CUDA, auto profileStream = samplesCommon::makeCudaStream();)"}
{"predict": "(29, CUDA, cudaFree( p );)", "label": "(28, CUDA, cudaFree( p );)"}
{"predict": "(34, OpenMP, #pragma omp target teams is_device_ptr(raa)); (38, OpenMP, #pragma omp target teams is_device_ptr(h)); (42, OpenMP, #pragma omp target teams is_device_ptr(rh)); (26, OpenMP, #pragma omp target teams is_device_ptr(aa)); (15, OpenMP, #pragma omp target teams is_device_ptr(k)); (46, OpenMP, #pragma omp target teams is_device_ptr(da)); (17, OpenMP, #pragma omp target teams is_device_ptr(z)); (39, OpenMP, #pragma omp target teams is_device_ptr(da))", "label": "(49, OpenMP, #pragma omp target teams is_device_ptr(da)); (23, OpenMP, #pragma omp target teams is_device_ptr(z)); (27, OpenMP, #pragma omp target teams is_device_ptr(aa)); (16, OpenMP, #pragma omp target teams is_device_ptr(k)); (30, OpenMP, #pragma omp target teams is_device_ptr(raa)); (39, OpenMP, #pragma omp target teams is_device_ptr(rh)); (33, OpenMP, #pragma omp target teams is_device_ptr(h))"}
{"predict": "(17, MPI, tim::mpi::barrier(tim::mpi::comm_world_v);); (54, MPI, tim::mpi::report_elapsed_time((tim::mpi::wtime() - start).seconds());); (0, MPI, tim::tick_count start = tim::mpi::tick_count::now();)", "label": "(35, MPI, _measurements = get_measure()->mpi_get();)"}
{"predict": "(42, MPI, MPI_T_finalize();); (17, MPI, MPI_T_init_thread(required, &provided);)", "label": "(42, MPI, MPI_T_finalize();); (17, MPI, MPI_T_init_thread(required, &provided);)"}
{"predict": "(5, CUDA, __shared__ float reduce_smem[NUM_WARPS];)", "label": "(5, CUDA, __shared__ float reduce_smem[NUM_WARPS];)"}
{"predict": "(3, TBB, tbb::task_scheduler_init init( nthread );); (4, TBB, tbb::parallel_for( tbb::blocked_range<int>(0,m,m/10), GrowBy<MyVector>(v, m, tbb::simple_partitioner(), nthread ) );); (0, TBB, typedef tbb::concurrent_vector< tbb::flow::continue_msg > MyVector;)", "label": "(3, TBB, tbb::parallel_for( tbb::blocked_range<int>(0,m,1000), GrowBy(v) );)"}
{"predict": "(68, OpenMP, #pragma omp parallel reduction(+ : r)); (78, OpenMP, #pragma omp for reduction(- : fl)); (72, OpenMP, #pragma omp parallel shared(i)); (74, OpenMP, #pragma omp parallel private(i)); (60, OpenMP, #pragma omp parallel reduction(+ : o)); (53, OpenMP, #pragma omp parallel reduction(+ : h, k, B::x)); (58, OpenMP, #pragma omp parallel reduction(+ : p), reduction(+ : p)); (59, OpenMP, #pragma omp parallel reduction(+ : qa[1], qa[0])); (52, OpenMP, #pragma omp parallel reduction(+ : ba)); (55, OpenMP, #pragma omp parallel reduction(^ : fl)); (60, OpenMP, #pragma omp parallel reduction(& : e, g)); (59, OpenMP, #pragma omp parallel reduction(+ : o)); (60, OpenMP, #pragma omp parallel reduction(+ : z, r)); (58, OpenMP, #pragma omp parallel reduction(+ : fl)); (51, OpenMP, #pragma omp parallel reduction(+ : a, b, c, d, f)); (44, OpenMP, #pragma omp parallel reduction(&& : S2::S2s)); (45, OpenMP, #pragma omp parallel reduction(+ : da)); (68, OpenMP, #pragma omp for private(fl)); (62, OpenMP, #pragma omp parallel reduction(+ : fl)); (56, OpenMP, #pragma omp parallel reduction(+ : h, k)); (44, OpenMP, #pragma omp parallel reduction(^ : S2::S2sc)); (69, OpenMP, #pragma omp parallel reduction(task, + : m)); (46, OpenMP, #pragma omp parallel reduction(* : ca)); (52, OpenMP, #pragma omp parallel reduction(+ : ba)); (43, OpenMP, #pragma omp parallel reduction(min : a, b, c, d, f)); (54, OpenMP, #pragma omp parallel reduction(+ : o)); (54, OpenMP, #pragma omp parallel reduction(+ : z, r)); (61, OpenMP, #pragma omp parallel reduction(+ : m)); (57, OpenMP, #pragma omp parallel reduction(+ : fl)); (62, OpenMP, #pragma omp parallel shared(i)); (48, OpenMP, #pragma omp parallel reduction(+ : qa[1", "label": "(54, OpenMP, #pragma omp parallel reduction(&& : S2::S2sc)); (25, OpenMP, #pragma omp parallel reduction(foo : argc); (44, OpenMP, #pragma omp parallel reduction(* : ca)); (78, OpenMP, #pragma omp for reduction(- : fl)); (28, OpenMP, #pragma omp parallel reduction(~ : argc)); (53, OpenMP, #pragma omp parallel reduction(+ : h, k, B::x)); (82, OpenMP, #pragma omp parallel reduction(task, + : m)); (79, OpenMP, #pragma omp parallel reduction(+ : m)); (36, OpenMP, #pragma omp parallel reduction(+ : ba)); (39, OpenMP, #pragma omp parallel reduction(- : da)); (59, OpenMP, #pragma omp parallel reduction(+ : r)); (5, OpenMP, #pragma omp parallel reduction(); (50, OpenMP, #pragma omp parallel private(i), reduction(+ : j), reduction(+ : q)); (47, OpenMP, #pragma omp parallel reduction(+ : o)); (-2, OpenMP, #pragma omp parallel); (37, OpenMP, #pragma omp parallel reduction(&& : S2::S2s)); (2, OpenMP, #pragma omp parallel reduction(-); (17, OpenMP, #pragma omp parallel reduction(&& : argc, z)); (-4, OpenMP, #pragma omp parallel reduction); (52, OpenMP, #pragma omp parallel shared(i)); (44, OpenMP, #pragma omp parallel private(k)); (21, OpenMP, #pragma omp parallel reduction(max : h.b)); (6, OpenMP, #pragma omp parallel reduction(| : argc,); (15, OpenMP, #pragma omp parallel reduction(+ : a, b, c, d, f)); (49, OpenMP, #pragma omp parallel reduction(max : j)); (5, OpenMP, #pragma omp parallel reduction(|| : argc > 0? argv[1] : argv[2])); (14, OpenMP, #pragma omp parallel reduction(min : a, b, c, d, f)); (45, OpenMP, #pragma omp parallel reduction(min : i)); (-4, OpenMP, #pragma omp parallel reduction(\\)); (-9, OpenMP, #pragma omp parallel reduction()); (60, OpenMP, #pragma omp parallel reduction(+:gptr[:argc])); (45, OpenMP, #pragma omp for private(fl)); (-10, OpenMP, #pragma omp parallel reduction(*)); (17, OpenMP, #pragma omp parallel reduction(^ : fl)); (2, OpenMP, #pragma omp parallel reduction(^ : S1)); (30, OpenMP, #pragma omp parallel reduction(+ : p), reduction(+ : p)); (20, OpenMP, #pragma omp parallel reduction(& : e, g))"}
{"predict": "(3, TBB, typedef tbb::cache<self_type> cache_type;); (11, TBB, static const size_t number_of_threads = tbb::task_scheduler_init::default_num_threads();)", "label": "(10, TBB, static const size_t number_of_threads = 4 * tbb::task_scheduler_init::default_num_threads();)"}
{"predict": "(8, CUDA, cuda_safe_call(cudaMalloc((void**) &dZ, N * sizeof(double)));); (53, CUDA, cuda_safe_call(cudaFree(dX));); (6, CUDA, cuda_safe_call(cudaMalloc((void**) &dY, N * sizeof(double)));); (54, CUDA, cuda_safe_call(cudaFree(dZ));); (7, CUDA, cuda_safe_call(cudaMalloc((void**) &dX, N * sizeof(double)));); (50, CUDA, cuda_safe_call(cudaFree(dY));)", "label": "(8, CUDA, cuda_safe_call(cudaMalloc((void**) &dX, N * sizeof(double)));); (8, CUDA, cuda_safe_call(cudaMalloc((void**) &dY, N * sizeof(double)));); (8, CUDA, cuda_safe_call(cudaMalloc((void**) &dZ, N * sizeof(double)));); (11, CUDA, cuda_safe_call(cudaDeviceSynchronize());)"}
{"predict": "(8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);)", "label": "(8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);)"}
{"predict": "(19, CUDA, cudaCheckError();); (75, CUDA, cudaEventRecord(start[i], stream0[i]);); (60, CUDA, cudaEventRecord(stop[i], stream1[i]);); (128, CUDA, cudaEventDestroy(start[d]);); (128, CUDA, cudaEventDestroy(stop[d]);); (9, CUDA, cudaStreamCreateWithFlags(&stream1[d], cudaStreamNonBlocking);); (5, CUDA, cudaSetDevice(d);); (127, CUDA, cudaStreamDestroy(stream1[d]);); (7, CUDA, cudaEventCreate(&start[d]);); (58, CUDA, cudaDeviceEnablePeerAccess(i, 0);); (4, CUDA, cudaEventCreate(&stop[d]);); (60, CUDA, cudaDeviceEnablePeerAccess(j, 0);); (124, CUDA, cudaStreamDestroy(stream0[d]);); (5, CUDA, cudaStreamCreateWithFlags(&stream0[d], cudaStreamNonBlocking);); (121, CUDA, cudaFree(buffersD2D[d]);); (62, CUDA, cudaStreamSynchronize(stream1[j]);); (121, CUDA, cudaFreeHost((void *)flag);); (62, CUDA, cudaEventRecord(stop[0], stream0[0]);); (58, CUDA, cudaStreamSynchronize(stream0[i]);); (121, CUDA, cudaFree(buffers[i]);); (46, CUDA, cudaDeviceEnablePeerAccess(j, 1);); (0, CUDA, cudaSetDevice(i);); (120, CUDA, cudaFreeHost((void *)flag);); (117, CUDA, cudaFree(buffersD2D[0]);); (43, CUDA, cudaStreamWaitEvent(stream1[i], start[0], 0);); (117, CUDA, cudaFree(buffers[0]);); (-10, CUDA, cudaHostAlloc((void **)&flag, sizeof(*flag), cudaHostAllocPortable);); (52, CUDA, cudaEventElapsedTime(&time_ms, start[0], stop[0]);); (39, CUDA, cudaDeviceCanAccessPeer(&access, i, j);); (29, CUDA, cudaDeviceCanAccessPeer(&access, j, i);); (106, CUDA, cudaFree(buffersD2D[1]);); (106, CUDA, cudaFree(buffers[1]);); (106, CUDA, cudaFreeHost((void *)flag);); (105, CUDA, cudaStreamDestroy", "label": "(147, CUDA, cudaStreamDestroy(stream0[d]);); (16, CUDA, cudaMalloc(&buffersD2D[d], numElems * sizeof(int));); (141, CUDA, cudaEventDestroy(start[d]);); (41, CUDA, cudaDeviceEnablePeerAccess(i, 0);); (7, CUDA, cudaCheckError();); (136, CUDA, cudaFree(buffersD2D[d]);); (35, CUDA, cudaDeviceEnablePeerAccess(j, 0);); (138, CUDA, cudaEventDestroy(stop[d]);); (25, CUDA, cudaSetDevice(i);); (106, CUDA, cudaDeviceDisablePeerAccess(i);); (139, CUDA, cudaStreamDestroy(stream1[d]);); (83, CUDA, cudaEventRecord(stop[i], stream0[i]);); (10, CUDA, cudaEventCreate(&stop[d]);); (140, CUDA, cudaFreeHost((void *)flag);); (79, CUDA, cudaStreamWaitEvent(stream0[i], stop[j], 0);); (3, CUDA, cudaMemset(buffersD2D[d], 0, numElems * sizeof(int));); (35, CUDA, cudaStreamSynchronize(stream1[j]);); (52, CUDA, cudaStreamWaitEvent(stream1[j], start[i], 0);); (-2, CUDA, cudaMemset(buffers[d], 0, numElems * sizeof(int));); (-4, CUDA, cudaMalloc(&buffers[d], numElems * sizeof(int));); (-10, CUDA, cudaHostAlloc((void **)&flag, sizeof(*flag), cudaHostAllocPortable);); (119, CUDA, cudaFree(buffers[d]);); (28, CUDA, cudaStreamSynchronize(stream0[i]);); (-9, CUDA, cudaSetDevice(d);); (0, CUDA, cudaStreamCreateWithFlags(&stream0[d], cudaStreamNonBlocking);); (13, CUDA, cudaDeviceCanAccessPeer(&access, i, j);); (42, CUDA, cudaEventRecord(start[i], stream0[i]);); (16, CUDA, cudaSetDevice(j);); (64, CUDA, cudaEventRecord(stop[j], stream1[j]);); (74, CUDA, cudaEventElapsedTime(&time_ms, start[i], stop[i]);); (-10, CUDA, cudaEventCreate(&start[d]);); (-5, CUDA, cudaStreamCreateWithFlags(&stream1[d], cudaStreamNonBlocking);); (81, CUDA, cudaDeviceDisablePeerAccess(j);)"}
{"predict": "(5, OpenMP, #pragma omp parallel for private(k, ll, ind_r, ind_u, ind_t, i, z))", "label": "(5, OpenACC, #pragma acc parallel loop)"}
{"predict": "(2, OpenMP, #pragma omp master taskloop collapse); (10, OpenMP, #pragma omp master taskloop collapse (ST); (31, OpenMP, #pragma omp master taskloop collapse (N)); (11, OpenMP, #pragma omp master taskloop collapse (1)); (18, OpenMP, #pragma omp master taskloop collapse (foobool(argc)), collapse (true), collapse (-5)); (5, OpenMP, #pragma omp master taskloop collapse (argc); (27, OpenMP, #pragma omp master taskloop collapse (2)); (-3, OpenMP, #pragma omp master taskloop collapse (); (16, OpenMP, #pragma omp master taskloop collapse (S)); (7, OpenMP, #pragma omp master taskloop collapse ((ST > 0)? 1 + ST : 2)); (-8, OpenMP, #pragma omp master taskloop collapse)", "label": "(10, OpenMP, #pragma omp master taskloop collapse (argc); (1, OpenMP, #pragma omp master taskloop collapse); (13, OpenMP, #pragma omp master taskloop collapse (1)); (14, OpenMP, #pragma omp master taskloop collapse ((ST > 0)? 1 + ST : 2)); (31, OpenMP, #pragma omp master taskloop collapse (N)); (32, OpenMP, #pragma omp master taskloop collapse (2)); (16, OpenMP, #pragma omp master taskloop collapse (foobool(argc)), collapse (true), collapse (-5)); (-3, OpenMP, #pragma omp master taskloop collapse (); (-2, OpenMP, #pragma omp master taskloop collapse ()); (15, OpenMP, #pragma omp master taskloop collapse (S)); (3, OpenMP, #pragma omp master taskloop collapse (ST)"}
{"predict": "(5, CUDA, __shared__ float reduce_smem[NUM_WARPS];)", "label": "(5, CUDA, __shared__ float reduce_smem[NUM_WARPS];)"}
{"predict": "(145, CUDA, result.error = cudaEventRecord(events[0]);); (174, CUDA, std::cerr << \"cudaEventSynchronize() failed: \" << cudaGetErrorString(result.error) << std::endl;); (141, CUDA, std::cerr << \"cudaEventRecord() failed: \" << cudaGetErrorString(result.error) << std::endl;); (250, CUDA, cudaDeviceSynchronize();); (161, CUDA, result.error = cudaEventRecord(events[1]);); (190, CUDA, std::cerr << \"cudaEventElapsed() failed: \" << cudaGetErrorString(result.error) << std::endl;); (132, CUDA, std::cerr << \"cudaEventCreate() failed: \" << cudaGetErrorString(result.error) << std::endl;); (173, CUDA, result.error = cudaEventSynchronize(events[1]);); (256, CUDA, (void)cudaEventDestroy(event);); (125, CUDA, result.error = cudaEventCreate(&event);); (186, CUDA, (void)cudaEventDestroy(events[0]);); (141, CUDA, result.error = cudaEventRecord(events[0]);); (161, CUDA, result.error = cudaEventElapsedTime(&runtime_ms, events[0], events[1]);); (165, CUDA, std::cerr << \"cudaEventElapsedTime() failed: \" << cudaGetErrorString(result.error) << std::endl;); (126, CUDA, std::cerr << \"cudaEventCreate() failed: \" << cudaGetErrorString(result.error) << std::endl;); (161, CUDA, result.error = cudaEventSynchronize(events[0]);); (226, CUDA, std::cerr << \"cudaEventSynchronize() failed: \" << cudaGetErrorString(result.error) << std::endl;); (161, CUDA, result.error = cudaEventRecord(events[1]);); (164, CUDA, std::cerr << \"cudaEventRecord() failed: \" << cudaGetErrorString(result.error) << std::endl;); (125, CUDA, result.error = cudaEventRecord(events[1]);); (225, CUDA, result.error = cudaEventSynchronize(events[1]);); (227, CUDA, std::cerr << \"cudaEventSynchronize() failed: \" << cudaGetErrorString(result.error) << std::endl;); (173, CUDA, result.error = cudaEventElapsedTime(&runtime_ms, events[1],", "label": "(161, CUDA, thrust::device_vector<ElementA> d_A = h_A;); (183, CUDA, cuda_err = cudaDeviceSynchronize();); (148, CUDA, thrust::host_vector<ElementB> h_B(N_size * K_size * L_size);); (184, CUDA, std::cerr << cudaGetErrorString(cuda_err) << \"\\n\";); (148, CUDA, thrust::host_vector<ElementD> h_D(M_size * N_size * L_size);); (157, CUDA, thrust::device_vector<ElementB> d_B = h_B;); (158, CUDA, thrust::device_vector<ElementD> cutlass_result = h_D;); (144, CUDA, thrust::host_vector<ElementC> h_C(M_size * N_size * L_size);); (213, CUDA, thrust::host_vector<ElementD> h_cutlass_result = cutlass_result;); (156, CUDA, thrust::device_vector<ElementD> reference_result = h_D;); (153, CUDA, thrust::device_vector<ElementC> d_C = h_C;); (138, CUDA, thrust::host_vector<ElementA> h_A(M_size * K_size * L_size);)"}
{"predict": "(3, CUDA, detail::cuda::cuDeviceInit init(0);); (15, CUDA, gpuErrchk(cudaGetDeviceCount(&deviceCount));); (8, CUDA, gpuErrchk(cudaDeviceReset());)", "label": "(18, CUDA, gpuErrchk(cudaGetDeviceCount(&deviceCount));); (1, CUDA, gpuErrchk(cudaSetDevice(0));); (2, CUDA, gpuErrchk(cudaFree(0));); (7, CUDA, gpuErrchk(cudaDeviceReset());)"}
{"predict": "(36, MPI, MPI_Error_string(ERR_CODE, aux_string, &L);)", "label": "(36, MPI, MPI_Error_string(ERR_CODE, aux_string, &L);)"}
{"predict": "(28, OpenMP, #pragma omp parallel for private(k, j, i, m, zeta, eta, xi)); (17, OpenMP, #pragma omp target teams); (167, OpenMP, #pragma omp target teams private(tmp, u21i, u31i, u41i, u51i, u21im1, u31im1, u41im1, u51im1)); (42, OpenMP, #pragma omp parallel for private(k, j, i, m, zeta, eta, xi, u21, q)); (24, OpenMP, #pragma omp distribute); (163, OpenMP, #pragma omp distribute private(tmp, u21i, u31i, u41i, u51i, u21im1, u31im1, u41im1, u51im1))", "label": "(22, OpenMP, #pragma omp distribute); (17, OpenMP, #pragma omp target data map(alloc: frct, rsd, ce, flux_G)); (19, OpenMP, #pragma omp target teams)"}
{"predict": "(22, CUDA, CUDA_RT_CALL(cudaMemset(a_new, 0, nx * ny * sizeof(real)));); (100, CUDA, CUDA_RT_CALL(cudaStreamDestroy(compute_stream));); (105, CUDA, CUDA_RT_CALL(cudaFree(a));); (23, CUDA, CUDA_RT_CALL(cudaDeviceSynchronize());); (100, CUDA, CUDA_RT_CALL(cudaFree(l2_norm_d));); (27, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&push_top_done, cudaEventDisableTiming));); (24, CUDA, CUDA_RT_CALL(cudaStreamCreate(&push_bottom_stream));); (26, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&push_bottom_done, cudaEventDisableTiming));); (49, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(compute_stream, push_bottom_done, 0));); (67, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(push_bottom_stream, compute_done, 0));); (69, CUDA, CUDA_RT_CALL(cudaEventRecord(push_bottom_done, push_bottom_stream));); (89, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_top_stream));); (86, CUDA, CUDA_RT_CALL(cudaFree(a_new));); (81, CUDA, CUDA_RT_CALL(cudaMemcpy(a_ref_h, a, nx * ny * sizeof(real), cudaMemcpyDeviceToHost));); (81, CUDA, CUDA_RT_CALL(cudaEventDestroy(compute_done));); (82, CUDA, CUDA_RT_CALL(cudaEventDestroy(push_bottom_done));); (14, CUDA, CUDA_RT_CALL(cudaStreamCreate(&push_top_stream));); (81, CUDA, CUDA_RT_CALL(cudaEventDestroy(push_top_done));); (65, CUDA, CUDA_RT_CALL(cudaStreamSynchronize(compute_stream));); (15, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&compute_done, cudaEventDisableTiming));); (57, CUDA, CUDA_RT_CALL(cudaEventRecord(compute_done, compute_stream));); (44, CUDA, CUDA_RT_CALL(cudaEventRecord(push_top_done, push_top_stream));); (82, CUDA, CUDA_RT_CALL(cudaFree(l2_norm_h));); (6, CUDA, CUDA_RT_CALL(cudaMemset(a_new, 0, nx * ny * sizeof(real)));); (4, CUDA, CUDA_RT_CALL(cudaStreamCreate(&compute_stream));); (7, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&push_norm_done, cudaEventDisableTiming));); (69, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push", "label": "(22, CUDA, CUDA_RT_CALL(cudaMemset(a_new, 0, nx * ny * sizeof(real)));); (100, CUDA, CUDA_RT_CALL(cudaStreamDestroy(compute_stream));); (105, CUDA, CUDA_RT_CALL(cudaFree(a));); (23, CUDA, CUDA_RT_CALL(cudaDeviceSynchronize());); (100, CUDA, CUDA_RT_CALL(cudaFree(l2_norm_d));); (27, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&push_top_done, cudaEventDisableTiming));); (24, CUDA, CUDA_RT_CALL(cudaStreamCreate(&push_bottom_stream));); (26, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&push_bottom_done, cudaEventDisableTiming));); (49, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(compute_stream, push_bottom_done, 0));); (67, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(push_bottom_stream, compute_done, 0));); (69, CUDA, CUDA_RT_CALL(cudaEventRecord(push_bottom_done, push_bottom_stream));); (89, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_top_stream));); (86, CUDA, CUDA_RT_CALL(cudaEventDestroy(compute_done));); (81, CUDA, CUDA_RT_CALL(cudaMemcpy(a_ref_h, a, nx * ny * sizeof(real), cudaMemcpyDeviceToHost));); (82, CUDA, CUDA_RT_CALL(cudaEventDestroy(push_bottom_done));); (14, CUDA, CUDA_RT_CALL(cudaStreamCreate(&push_top_stream));); (81, CUDA, CUDA_RT_CALL(cudaEventDestroy(push_top_done));); (65, CUDA, CUDA_RT_CALL(cudaStreamSynchronize(compute_stream));); (44, CUDA, CUDA_RT_CALL(cudaEventRecord(compute_done, compute_stream));); (17, CUDA, CUDA_RT_CALL(cudaMallocHost(&l2_norm_h, sizeof(real)));); (54, CUDA, CUDA_RT_CALL(cudaEventRecord(push_top_done, push_top_stream));); (33, CUDA, CUDA_RT_CALL(cudaMemsetAsync(l2_norm_d, 0, sizeof(real), compute_stream));); (6, CUDA, CUDA_RT_CALL(cudaStreamCreate(&compute_stream));); (-2, CUDA, CUDA_RT_CALL(cudaMemset(a, 0, nx * ny * sizeof(real)));); (7, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&compute_done, cudaEventDisableTiming));); (81, CUDA, CUDA_RT_CALL(cudaFree(a_new));); (45, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(push_top_stream, compute_done, 0));); (72, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_bottom_stream));); (28, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(compute_stream, push_top_done, 0));); (6, CUDA, CUDA_RT_CALL(cudaMalloc(&l2_norm_d, sizeof(real)));); (21, MPI, double start = MPI_Wtime();); (-13, CUDA, CUDA_RT_CALL(cudaMalloc(&a, nx * ny * sizeof(real)));); (71, CUDA, CUDA_RT_CALL(cudaFreeHost(l2_norm_h));); (-14, CUDA, CUDA_RT_CALL(cudaMalloc(&a_new, nx * ny * sizeof(real)));)"}
{"predict": "(30, MPI, MPI_Bcast( work, lwork, MPI_DOUBLE, pkk, grid->comm );); (27, MPI, MPI_Bcast( &lwork, 1, MPI_INT, pkk, grid->comm );)", "label": "(30, MPI, MPI_Bcast( work, diag_len[p], MPI_DOUBLE, pkk, grid->comm );); (27, MPI, MPI_Bcast( work, lwork, MPI_DOUBLE, pkk, grid->comm );)"}
{"predict": "(4, CUDA, const T** input_ptrs = GetCudaDeviceArrayOnDevice(&input_ptr_data);)", "label": "(4, CUDA, const T** input_ptrs = GetCudaDeviceArrayOnDevice(&input_ptr_data);)"}
{"predict": "(16, MPI, MPI_Allreduce(bins_ptr, global_ptr, bins_size, MPI_DOUBLE, MPI_MIN, mpi_comm);); (11, MPI, MPI_Allreduce(bins_ptr, global_ptr, bins_size, MPI_DOUBLE, MPI_SUM, mpi_comm);); (19, MPI, MPI_Allreduce(bins_ptr, global_ptr, bins_size, MPI_DOUBLE, MPI_MAX, mpi_comm);); (7, MPI, MPI_Comm mpi_comm = MPI_Comm_f2c(flow::Workspace::default_mpi_comm());)", "label": "(17, MPI, MPI_Allreduce(bins_ptr, global_ptr, bins_size, MPI_DOUBLE, MPI_MIN, mpi_comm);); (12, MPI, MPI_Allreduce(bins_ptr, global_ptr, bins_size, MPI_DOUBLE, MPI_SUM, mpi_comm);); (19, MPI, MPI_Allreduce(bins_ptr, global_ptr, bins_size, MPI_DOUBLE, MPI_MAX, mpi_comm);); (2, MPI, MPI_Comm mpi_comm = MPI_Comm_f2c(flow::Workspace::default_mpi_comm());)"}
{"predict": "(71, OpenMP, const int th=omp_get_thread_num();); (10, OpenMP, #pragma omp parallel for schedule (guided))", "label": "(71, OpenMP, const int th=omp_get_thread_num();); (10, OpenMP, #pragma omp parallel for schedule (guided))"}
{"predict": "(11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (16, MPI, MPI_Dims_create(tests[k].size, tests[k].dim, dims);)", "label": "(10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (15, MPI, MPI_Dims_create(tests[k].size, tests[k].dim, dims);)"}
{"predict": "(17, CUDA, __shared__ float b_smem[BN * BK];); (15, CUDA, __shared__ float a_smem[BK * BK];)", "label": "(17, CUDA, __shared__ float a_smem[BM * BK];); (17, CUDA, __shared__ float b_smem[BK * BN];)"}
{"predict": "(83, OpenCL, err = clReleaseMemObject(buffer);); (87, OpenCL, err = clReleaseMemObject(image_buffer);)", "label": "(86, OpenCL, err = clReleaseMemObject(buffer);); (82, OpenCL, err = clReleaseMemObject(image_buffer);)"}
{"predict": "(12, CUDA, cudaMallocHost((void**)&h_a, bufsize);); (25, CUDA, cudaMalloc((void**)&d_c, bufsize);); (44, CUDA, cudaDeviceSynchronize();); (62, CUDA, cudaFree(d_c);); (8, CUDA, cudaMallocHost((void**)&h_b, bufsize);); (60, CUDA, cudaFree(d_b);); (18, CUDA, cudaMalloc((void**)&d_a, bufsize);); (62, CUDA, cudaFreeHost(h_b);); (62, CUDA, cudaFreeHost(h_c);); (60, CUDA, cudaFreeHost(h_a);); (16, CUDA, cudaMalloc((void**)&d_b, bufsize);); (62, CUDA, cudaFreeHost(h_d);); (14, CUDA, cudaMallocHost((void**)&h_c, bufsize);); (56, CUDA, cudaFree(d_a);); (5, CUDA, cudaMallocHost((void**)&h_d, bufsize);)", "label": "(12, CUDA, cudaMallocHost((void**)&h_a, bufsize);); (13, CUDA, cudaMallocHost((void**)&h_c, bufsize);); (43, CUDA, cudaDeviceSynchronize();); (20, CUDA, cudaMalloc((void**)&d_a, bufsize);); (63, CUDA, cudaFree(d_c);); (61, CUDA, cudaFree(d_b);); (7, CUDA, cudaMallocHost((void**)&h_b, bufsize);); (64, CUDA, cudaFreeHost(h_b);); (64, CUDA, cudaFreeHost(h_c);); (56, CUDA, cudaFree(d_a);); (60, CUDA, cudaFreeHost(h_a);); (14, CUDA, cudaMalloc((void**)&d_c, bufsize);); (12, CUDA, cudaMalloc((void**)&d_b, bufsize);)"}
{"predict": "(43, OpenMP, lis2mdl_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)", "label": "(43, OpenMP, lis2mdl_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)"}
{"predict": "(4, TBB, tbb::task_scheduler_init init(1);); (17, TBB, ASSERT( tbb::parallel_reduce(r2) == tbb::task::reduce(r2), NULL );); (1, TBB, typedef tbb::task::reduce tbb::task::reduction_task;); (0, TBB, typedef tbb::task_scheduler_init tbb::task_scheduler_init;); (2, TBB, tbb::task* r1 = new( tbb::task::allocate_root() ) tbb::reduction_task(r1_value, r1_sum, partitioner);); (2, TBB, tbb::task* r2 = new( tbb::task::allocate_root() ) tbb::reduction_task(r2_value, r2_sum, partitioner);)", "label": "(3, TBB, ValueType r1 = tbb::parallel_reduce( m_range, I, Accumulator(), Sum(), partitioner );)"}
{"predict": "(11, CUDA, __shared__ float r[_N_], k[_N_], u[_N_], w[_N_];)", "label": "(11, CUDA, __shared__ float r[_N_], k[_N_], u[_N_], w[_N_];)"}
{"predict": "(51, CUDA, flamegpu::CUDASimulation cudaSimulation(model);)", "label": "(50, CUDA, flamegpu::CUDASimulation cudaSimulation(model, argc, argv);)"}
{"predict": "(12, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (81, CUDA, cudaError_t result = cudaDeviceSynchronize();)", "label": "(12, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (13, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;); (80, CUDA, cudaError_t result = cudaDeviceSynchronize();)"}
{"predict": "(22, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)", "label": "(22, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(11, MPI, MPI_Type_get_extent(int_with_key, &lb, &extent);)", "label": "(11, MPI, MPI_Type_get_extent(int_with_key, &lb, &extent);)"}
{"predict": "(39, OpenCL, clReleaseEvent(eventOut);); (14, OpenCL, retVal = clEnqueueNDRangeKernel(clCommandQ, clKernel, 1, offset, gws, nullptr, 1, &eventBlocking, &eventOut);); (13, OpenCL, clCommandQ = clCreateCommandQueue(context, devices[0], properties, &retVal);); (37, OpenCL, clReleaseCommandQueue(clCommandQ);); (30, OpenCL, clWaitForEvents(1, &callbackEvent);)", "label": "(15, OpenCL, retVal = clEnqueueNDRangeKernel(clCommandQ, clKernel, 1, offset, gws, nullptr, 1, &eventBlocking, &eventOut);); (13, OpenCL, clCommandQ = clCreateCommandQueue(context, devices[0], properties, &retVal);); (38, OpenCL, clReleaseEvent(eventOut);); (31, OpenCL, clWaitForEvents(1, &callbackEvent);); (37, OpenCL, clReleaseCommandQueue(clCommandQ);); (28, OpenCL, clWaitForEvents(1, &eventOut);)"}
{"predict": "(38, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (40, CUDA, CUDA_CHECK(cudaFree(d_data));)", "label": "(38, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (40, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(12, OpenMP, #pragma omp target)", "label": "(12, OpenMP, #pragma omp target)"}
{"predict": "(10, MPI, MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);)", "label": "(10, MPI, MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);)"}
{"predict": "(33, MPI, MPI_Type_free(&type);); (16, MPI, MPI_Type_get_extent(type, &lb, &extent);); (7, MPI, err = MPI_Type_indexed(3, len, disp, MPI_INT, &type);)", "label": "(33, MPI, MPI_Type_free(&type);); (16, MPI, MPI_Type_get_extent(type, &lb, &extent);); (7, MPI, err = MPI_Type_indexed(3, len, disp, MPI_INT, &type);)"}
{"predict": "(23, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (17, MPI, MPI_Alltoallw_c(sendbuf, counts, displs, types, recvbuf, counts, displs, types, comm);)", "label": "(23, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (17, MPI, MPI_Alltoallw_c(sendbuf, counts, displs, types, recvbuf, counts, displs, types, comm);)"}
{"predict": "(9, OpenMP, #pragma omp target data map(present, tofrom: i)); (6, OpenMP, #pragma omp target data map(alloc: i)); (1, OpenMP, #pragma omp target data map(alloc: i) map(present, tofrom: i))", "label": "(3, OpenMP, #pragma omp target enter data map(alloc:i)); (6, OpenMP, #pragma omp target data map(present, alloc: i)); (7, OpenMP, #pragma omp target exit data map(delete:i))"}
{"predict": "(9, MPI, errcode = MPI_Grequest_complete(aio_req->req);)", "label": "(10, MPI, errcode = MPI_Grequest_complete(aio_req->req);)"}
{"predict": "(18, OpenMP, #pragma omp for); (8, OpenMP, #pragma omp parallel); (9, OpenMP, #pragma omp single); (11, OpenMP, #pragma omp task private(sum)); (16, OpenMP, #pragma omp critical)", "label": "(20, OpenMP, #pragma omp flush); (13, OpenMP, #pragma omp task private(sum) shared(result, known_sum)); (7, OpenMP, #pragma omp parallel); (8, OpenMP, #pragma omp single); (21, OpenMP, #pragma omp critical)"}
{"predict": "(20, OpenCL, clReleaseMemObject(mobjAP_);); (16, OpenCL, clReleaseMemObject(mobjX_);)", "label": "(20, OpenCL, clReleaseMemObject(mobjAP_);); (16, OpenCL, clReleaseMemObject(mobjX_);)"}
{"predict": "(31, OpenCL, TEST_OCL_CHECK(clReleaseCommandQueue(ocl_queue));)", "label": "(27, OpenCL, TEST_OCL_CHECK(clReleaseCommandQueue(ocl_queue));)"}
{"predict": "(9, MPI, MPI_Comm_rank(comm, &comm_rank);); (10, MPI, MPI_Comm_size(comm, &comm_size);)", "label": "(9, MPI, MPI_Comm_rank(comm, &comm_rank);); (10, MPI, MPI_Comm_size(comm, &comm_size);)"}
{"predict": "(23, CUDA, cudaMalloc(reinterpret_cast<void**>(&d_secret_keys), length_secret_key * batch);); (35, CUDA, cudaMemcpy(signatures.data(), d_signatures, length_signature_aligned * batch, cudaMemcpyDeviceToHost);); (37, CUDA, cudaFree(d_secret_keys);); (20, CUDA, cudaMalloc(reinterpret_cast<void**>(&d_messages), length_message * batch);); (18, CUDA, cudaMalloc(reinterpret_cast<void**>(&d_signatures), length_signature_aligned * batch);); (23, CUDA, cudaMemcpy(messages.data(), d_messages, length_message * batch, cudaMemcpyDeviceToHost);); (37, CUDA, cudaFree(d_messages);); (35, CUDA, cudaFree(d_signatures);); (22, CUDA, cudaMemcpy(secret_keys.data(), d_secret_keys, length_secret_key * batch, cudaMemcpyDeviceToHost);)", "label": "(36, CUDA, cudaMemcpy(signatures.data(), d_signatures, length_signature_aligned * batch, cudaMemcpyDeviceToHost);); (40, CUDA, cudaFree(d_secret_keys);); (26, CUDA, cudaMemcpy(d_secret_keys, secret_keys.data(), length_secret_key * batch, cudaMemcpyHostToDevice);); (26, CUDA, cudaMemcpy(d_messages, messages.data(), message_size * batch, cudaMemcpyHostToDevice);); (39, CUDA, cudaFree(d_messages);); (17, CUDA, cudaMalloc(reinterpret_cast<void**>(&d_secret_keys), length_secret_key * batch);); (15, CUDA, cudaMalloc(reinterpret_cast<void**>(&d_signatures), length_signature_aligned * batch);); (35, CUDA, cudaFree(d_signatures);); (15, CUDA, cudaMalloc(reinterpret_cast<void**>(&d_messages), message_size * batch);)"}
{"predict": "(19, OpenMP, #pragma omp parallel for)", "label": "(19, OpenMP, #pragma omp parallel for)"}
{"predict": "(20, MPI, MPI_Wait(&request, MPI_STATUS_IGNORE);); (26, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Init(NULL, NULL);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (11, MPI, MPI_Recv(&buffer, 1, MPI_INT, left, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (13, MPI, MPI_Isend(&value, 1, MPI_INT, right, tag, MPI_COMM_WORLD, &request);); (4, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(27, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Init(NULL, NULL);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (20, MPI, MPI_Wait(&request, MPI_STATUS_IGNORE);); (6, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (9, MPI, MPI_Bcast(&factor, 1, MPI_INT, root, MPI_COMM_WORLD);)"}
{"predict": "(3, OpenMP, #pragma omp target); (4, OpenMP, #pragma omp distribute parallel for simd); (2, OpenMP, #pragma omp teams)", "label": "(3, OpenMP, #pragma omp target); (4, OpenMP, #pragma omp distribute parallel for simd); (2, OpenMP, #pragma omp teams)"}
{"predict": "(8, OpenMP, #pragma omp target teams distribute parallel for map(a[:n]))", "label": "(5, OpenMP, #pragma omp target teams distribute parallel for)"}
{"predict": "(41, MPI, err = MPI_Type_size(eviltype, &val);); (98, MPI, MPI_Type_free(&inttype);); (17, MPI, err = MPI_Type_create_resized(tmptype, 6, -9, &inttype);); (96, MPI, MPI_Type_free(&tmptype);); (28, MPI, err = MPI_Type_vector(3, 4, -5, inttype, &eviltype);); (54, MPI, err = MPI_Type_get_extent(eviltype, &lb, &extent);); (3, MPI, err = MPI_Type_contiguous(4, MPI_BYTE, &tmptype);); (93, MPI, MPI_Type_free(&eviltype);); (69, MPI, err = MPI_Type_get_true_extent(eviltype, &true_lb, &aval);)", "label": "(43, MPI, err = MPI_Type_size(eviltype, &val);); (100, MPI, MPI_Type_free(&inttype);); (19, MPI, err = MPI_Type_create_resized(tmptype, 6, -9, &inttype);); (55, MPI, err = MPI_Type_get_extent(eviltype, &lb, &extent);); (28, MPI, err = MPI_Type_vector(3, 4, 5, inttype, &eviltype);); (95, MPI, MPI_Type_free(&tmptype);); (3, MPI, err = MPI_Type_contiguous(4, MPI_BYTE, &tmptype);); (95, MPI, MPI_Type_free(&eviltype);); (71, MPI, err = MPI_Type_get_true_extent(eviltype, &true_lb, &aval);)"}
{"predict": "(78, CUDA, CHECK(cudaFree(d_y));); (75, CUDA, CHECK(cudaFree(d_NN));); (56, CUDA, CHECK(cudaMemcpy(NL, d_NL, N*M*sizeof(int), cudaMemcpyDefault));); (54, CUDA, CHECK(cudaMemcpy(NN, d_NN, N*sizeof(int), cudaMemcpyDefault));); (32, CUDA, CHECK(cudaMalloc(&d_NL, N*M*sizeof(int)));); (30, CUDA, CHECK(cudaMalloc(&d_NN, N*sizeof(int)));); (46, CUDA, CHECK(cudaMemcpy(d_y, y.data(), N*sizeof(real), cudaMemcpyDefault));); (32, CUDA, CHECK(cudaMalloc(&d_y, N*sizeof(real)));); (70, CUDA, CHECK(cudaFree(d_x));); (30, CUDA, CHECK(cudaMemcpy(d_x, x.data(), N*sizeof(real), cudaMemcpyDefault));); (28, CUDA, CHECK(cudaMalloc(&d_x, N*sizeof(real)));); (67, CUDA, CHECK(cudaFree(d_NL));)", "label": "(78, CUDA, CHECK(cudaFree(d_y));); (74, CUDA, CHECK(cudaFree(d_NN));); (56, CUDA, CHECK(cudaMemcpy(NL, d_NL, N*M*sizeof(int), cudaMemcpyDefault));); (34, CUDA, CHECK(cudaMalloc(&d_NL, N*M*sizeof(int)));); (32, CUDA, CHECK(cudaMalloc(&d_NN, N*sizeof(int)));); (46, CUDA, CHECK(cudaMemcpy(d_y, y.data(), N*sizeof(real), cudaMemcpyDefault));); (51, CUDA, CHECK(cudaMemcpy(NN, d_NN, N*sizeof(int), cudaMemcpyDefault));); (33, CUDA, CHECK(cudaMalloc(&d_y, N*sizeof(real)));); (69, CUDA, CHECK(cudaFree(d_x));); (30, CUDA, CHECK(cudaMalloc(&d_x, N*sizeof(real)));); (66, CUDA, CHECK(cudaFree(d_NL));)"}
{"predict": "(38, MPI, MPI_Recv(rmsg2, 2, MPI_DOUBLE, src, tag, comm, MPI_STATUS_IGNORE);); (55, MPI, MPI_Buffer_detach(&bbuf, &bsize);); (31, MPI, MPI_Pack(msg2, 2, MPI_DOUBLE, buf, bufsize, MPI_COMM_WORLD);); (28, MPI, MPI_Pack(msg1, 7, MPI_CHAR, buf, bufsize, MPI_COMM_WORLD);); (29, MPI, MPI_Pack(msg3, 17, MPI_CHAR, buf, bufsize, MPI_COMM_WORLD);); (33, MPI, MPI_Recv(rmsg1, 7, MPI_CHAR, src, tag, comm, MPI_STATUS_IGNORE);); (33, MPI, MPI_Recv(rmsg3, 17, MPI_CHAR, src, tag, comm, MPI_STATUS_IGNORE);); (54, MPI, MPI_Comm_detach(&comm);); (8, MPI, MPI_Comm_rank(comm, &rank);); (23, MPI, MPI_Buffer_attach(buf, bufsize);); (48, MPI, MPI_Barrier(comm);); (18, MPI, MPI_Pack_size(3, MPI_DOUBLE, MPI_COMM_WORLD, &s3);); (16, MPI, MPI_Pack_size(2, MPI_CHAR, MPI_COMM_WORLD, &s2);); (14, MPI, MPI_Pack_size(1, MPI_CHAR, MPI_COMM_WORLD, &s1);)", "label": "(19, MPI, MPI_Pack_size(2, MPI_DOUBLE, comm, &s2);); (38, MPI, MPI_Recv(rmsg2, 10, MPI_DOUBLE, src, tag, comm, MPI_STATUS_IGNORE);); (21, MPI, MPI_Buffer_attach(buf, bufsize);); (56, MPI, MPI_Buffer_detach(&bbuf, &bsize);); (34, MPI, MPI_Recv(rmsg1, 7, MPI_CHAR, src, tag, comm, MPI_STATUS_IGNORE);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (27, MPI, MPI_Bsend(msg2, 2, MPI_DOUBLE, dest, tag, comm);); (13, MPI, MPI_Pack_size(17, MPI_CHAR, comm, &s3);); (24, MPI, MPI_Bsend(msg1, 7, MPI_CHAR, dest, tag, comm);); (25, MPI, MPI_Bsend(msg3, 17, MPI_CHAR, dest, tag, comm);); (8, MPI, MPI_Pack_size(7, MPI_CHAR, comm, &s1);); (29, MPI, MPI_Recv(rmsg3, 17, MPI_CHAR, src, tag, comm, MPI_STATUS_IGNORE);)"}
{"predict": "(9, SYCL, sycl::device device;)", "label": "(9, SYCL, sycl::device device;)"}
{"predict": "(171, OpenMP, #pragma omp interop destroy(CI)); (175, OpenMP, #pragma omp interop destroy(IRef)); (178, OpenMP, #pragma omp interop destroy(dev)); (3, OpenMP, #pragma omp interop use(I)); (178, OpenMP, #pragma omp interop destroy(I)); (163, OpenMP, #pragma omp interop destroy(m)); (2, OpenMP, #pragma omp interop destroy(I)); (13, OpenMP, #pragma omp interop use(IRef)); (159, OpenMP, #pragma omp interop destroy(h)); (3, OpenMP, #pragma omp interop use(CI)); (13, OpenMP, #pragma omp interop destroy(CI)); (42, OpenMP, #pragma omp interop device(dev) depend(inout : ap) use(I)); (45, OpenMP, #pragma omp interop device(dev) depend(inout : ap) depend(in : I)); (48, OpenMP, #pragma omp interop device(dev) depend(inout : ap) destroy(I)); (13, OpenMP, #pragma omp interop init(target : IRef)); (50, OpenMP, #pragma omp interop device(dev) depend(inout : ap) init(targetsync : I)); (23, OpenMP, #pragma omp interop destroy(dev)); (53, OpenMP, #pragma omp interop device(dev) depend(inout : ap) depend(in : I)); (24, OpenMP, #pragma omp interop destroy(IRef)); (13, OpenMP, #pragma omp interop init(prefer_type(2,4,6,1), targetsync : I)); (30, OpenMP, #pragma omp interop init(prefer_type(\"level_zero\",2,4), targetsync : I)); (-18, OpenMP, #pragma omp interop device(dev) depend(inout : ap)); (-9, OpenMP, #pragma omp interop destroy(I)); (31, OpenMP, #pragma omp interop destroy(m)); (1, OpenMP, #pragma omp interop destroy(CI)); (13, OpenMP, #pragma omp interop destroy(Ap)); (2, OpenMP, #pragma omp interop destroy(I)); (15, OpenMP, #pragma omp interop destroy(Bp)); (-12, OpenMP, #pragma omp interop use(CI)); (39, OpenMP,", "label": "(140, OpenMP, #pragma omp interop use(I) use(J)); (129, OpenMP, #pragma omp interop init(target:I) init(targetsync:J)); (24, OpenMP, #pragma omp interop init(target:IRef)); (31, OpenMP, #pragma omp interop destroy(IRef)); (71, OpenMP, #pragma omp interop device(dev) depend(inout:ap) destroy(I)); (159, OpenMP, #pragma omp interop destroy(I) use(J)); (4, OpenMP, #pragma omp interop use(I)); (141, OpenMP, #pragma omp interop destroy(I) destroy(J)); (47, OpenMP, #pragma omp interop device(dev) depend(inout:ap) init(targetsync:I)); (7, OpenMP, #pragma omp interop destroy(I)); (-2, OpenMP, #pragma omp interop init(target:I)); (127, OpenMP, #pragma omp interop init(target:I) use(J)); (29, OpenMP, #pragma omp interop use(CI)); (84, OpenMP, #pragma omp interop init(prefer_type(2,4,6,1),targetsync:I)); (14, OpenMP, #pragma omp interop use(IRef)); (50, OpenMP, #pragma omp interop device(dev) depend(inout:ap) use(I)); (71, OpenMP, #pragma omp interop init(prefer_type(1,2,3,4,5,6),targetsync:I)); (145, OpenMP, #pragma omp interop init(target:I) destroy(J)); (-14, OpenMP, #pragma omp interop init()"}
{"predict": "(36, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (38, CUDA, CUDA_CHECK(cudaFree(d_data));)", "label": "(36, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (38, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(86, CUDA, cudaFreeHost( outHost );); (84, CUDA, cudaFree( texDevice );)", "label": "(86, CUDA, cudaFreeHost( outHost );); (84, CUDA, cudaFree( texDevice );)"}
{"predict": "(31, OpenMP, #pragma omp teams reduction(+:sum)); (28, OpenMP, #pragma omp target map(tofrom: b, sum, t) map(alloc: a)); (31, OpenMP, #pragma omp distribute parallel for reduction(+:sum)); (33, OpenMP, t[i] = omp_get_team_num();)", "label": "(31, OpenMP, #pragma omp teams reduction(+:sum)); (28, OpenMP, #pragma omp target map(tofrom: b, sum, t) map(alloc: a)); (31, OpenMP, #pragma omp distribute parallel for reduction(+:sum)); (33, OpenMP, t[i] = omp_get_team_num();)"}
{"predict": "(92, OpenMP, _NumThreads = omp_get_num_threads();); (94, OpenMP, #pragma omp parallel for)", "label": "(91, OpenMP, const int NumThreads = omp_get_max_threads();)"}
{"predict": "(44, CUDA, checkCudaErrors(cudaMemset(d_Output, 0, QRNG_DIMENSIONS * N * sizeof(float)));); (21, CUDA, int dev = findCudaDevice(argc, (const char **)argv);); (49, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (40, CUDA, checkCudaErrors(cudaMalloc((void **)&d_Output, QRNG_DIMENSIONS * N * sizeof(float)));); (122, CUDA, checkCudaErrors(cudaFree(d_Output));); (20, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, dev));); (60, CUDA, checkCudaErrors(cudaMemcpy(h_OutputGPU, d_Output, QRNG_DIMENSIONS * N * sizeof(float), cudaMemcpyDeviceToHost));)", "label": "(44, CUDA, checkCudaErrors(cudaMemset(d_Output, 0, QRNG_DIMENSIONS * N * sizeof(float)));); (20, CUDA, int dev = findCudaDevice(argc, (const char **)argv);); (49, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (123, CUDA, checkCudaErrors(cudaFree(d_Output));); (18, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, dev));); (28, CUDA, checkCudaErrors(cudaMalloc((void **)&d_Output, QRNG_DIMENSIONS * N * sizeof(float)));); (60, CUDA, checkCudaErrors(cudaMemcpy(h_OutputGPU, d_Output, QRNG_DIMENSIONS * N * sizeof(float), cudaMemcpyDeviceToHost));)"}
{"predict": "(247, OpenMP, nthreads = omp_get_num_threads();); (232, OpenMP, #pragma omp parallel firstprivate(niter) private(step)); (213, OpenMP, #pragma omp parallel); (236, OpenMP, #pragma omp master)", "label": "(249, OpenMP, nthreads = omp_get_num_threads();); (234, OpenMP, #pragma omp parallel firstprivate(niter) private(step)); (214, OpenMP, #pragma omp parallel); (237, OpenMP, #pragma omp master)"}
{"predict": "(7, MPI, MPI_Init(&argc, &argv);); (32, MPI, MPI_Finalize();); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (22, MPI, MPI_Allreduce(&errs, &toterrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);)", "label": "(6, MPI, MPI_Init(&argc, &argv);); (32, MPI, MPI_Finalize();); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (20, MPI, MPI_Reduce(&errs, &toterrs, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);)"}
{"predict": "(147, MPI, MPI_Allreduce(MPI_IN_PLACE, &n, 1, MPI_INT, MPI_MAX, leader_comm);); (116, MPI, MPI_Wait(&sreq, &status);); (43, MPI, MPI_Comm_split(MPI_COMM_WORLD, 0, hpmp_myrank, &local_comm);); (43, MPI, MPI_Comm_free(&local_comm);)", "label": "(150, MPI, MPI_Type_commit(&mydt);); (154, MPI, MPI_Op_free(&myop);); (154, MPI, MPI_Type_free(&mydt);); (146, MPI, MPI_Type_contiguous(sizeof(comm_method_string_conversion_t), MPI_BYTE, &mydt);); (147, MPI, MPI_Op_create(myfn, 1, &myop);)"}
{"predict": "(13, MPI, MPI_Attr_put(base_ptr[i], keyval, &windows[i]);); (16, MPI, MPI_Keyval_free(&keyval_copy);); (7, MPI, MPI_Win_create_keyval(MPI_NULL_COPY_FN, MPI_NULL_DELETE_FN, &keyval, NULL);); (12, MPI, MPI_Win_free(&windows[i]);); (7, MPI, MPI_Win_create_keyval(MPI_NULL_COPY_FN, MPI_NULL_DELETE_FN, &keyval, NULL);); (10, MPI, MPI_Keyval_free(&keyval);); (6, MPI, MPI_Win_set_attr(base_ptr[i], keyval, &windows[i]);)", "label": "(8, MPI, MPI_Alloc_mem(DATA_SZ, MPI_INFO_NULL, &base_ptr[0]);); (20, MPI, MPI_Win_free_keyval(&keyval);); (21, MPI, MPI_Win_free(&windows[1]);); (6, MPI, MPI_Win_create(base_ptr[0], DATA_SZ, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &windows[0]);); (12, MPI, MPI_Win_set_attr(windows[0], keyval, NULL);); (5, MPI, MPI_Alloc_mem(DATA_SZ, MPI_INFO_NULL, &base_ptr[1]);); (14, MPI, MPI_Free_mem(base_ptr[0]);); (15, MPI, MPI_Win_free_keyval(&keyval_copy);); (16, MPI, MPI_Free_mem(base_ptr[1]);); (4, MPI, MPI_Win_create_keyval(MPI_WIN_NULL_COPY_FN, delete_fn, &keyval, NULL);); (9, MPI, MPI_Win_free(&windows[0]);); (0, MPI, MPI_Win_create(base_ptr[1], DATA_SZ, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &windows[1]);); (5, MPI, MPI_Win_set_attr(windows[1], keyval, NULL);)"}
{"predict": "(47, CUDA, thrust::fill(thrust::begin(range), thrust::end(range), 0.0);); (43, CUDA, thrust::matrix<double> A(order,0);); (43, CUDA, thrust::matrix<double> B(order,0);)", "label": "(46, CUDA, thrust::fill(thrust::device, B.begin(), B.end(), 0.0);); (44, CUDA, thrust::sequence(thrust::device, A.begin(), A.end() );); (40, CUDA, thrust::device_vector<double> A(order*order);); (40, CUDA, thrust::device_vector<double> B(order*order);)"}
{"predict": "(2, CUDA, extern __shared__ char shared_memory[];)", "label": "(2, CUDA, extern __shared__ char shared_memory[];)"}
{"predict": "(3, CUDA, __shared__ float s_data[32];)", "label": "(5, CUDA, extern __shared__ float s_data[];)"}
{"predict": "(97, OpenMP, #pragma omp sections reduction(+ : z, o)); (145, OpenMP, #pragma omp sections reduction(+ : r)); (108, OpenMP, #pragma omp sections reduction(^ : fl)); (98, OpenMP, #pragma omp sections reduction(^ : S1)); (139, OpenMP, #pragma omp sections reduction(+ : p), reduction(+ : p)); (72, OpenMP, #pragma omp sections reduction(&& : S2::S2sc)); (63, OpenMP, #pragma omp sections reduction(+ : ba)); (34, OpenMP, #pragma omp sections reduction(\\)); (55, OpenMP, #pragma omp sections reduction(+ : a, b, c, d, f)); (18, OpenMP, #pragma omp sections reduction(-); (145, OpenMP, #pragma omp sections reduction(+ : m)); (54, OpenMP, #pragma omp sections reduction(max : h.b)); (63, OpenMP, #pragma omp sections reduction(min : a, b, c, d, f)); (99, OpenMP, #pragma omp sections reduction(&& : S2::S2s)); (35, OpenMP, #pragma omp sections reduction(foo : argc); (92, OpenMP, #pragma omp sections reduction(& : e, g)); (77, OpenMP, #pragma omp sections reduction(+ : h, k, B::x)); (16, OpenMP, #pragma omp sections reduction(*)); (59, OpenMP, #pragma omp sections reduction(* : ca)); (94, OpenMP, #pragma omp sections reduction(+ : o)); (30, OpenMP, #pragma omp sections reduction(|| : argc > 0? argv[1] : argv[2])); (128, OpenMP, #pragma omp parallel reduction(* : fl)); (138, OpenMP, #pragma omp sections reduction(+ : fl)); (100, OpenMP, #pragma omp sections reduction(+ : k), reduction(+ : k)); (39, OpenMP, #pragma omp sections reduction(&& : argc)); (8, OpenMP, #pragma omp sections reduction()); (16, OpenMP, #pragma omp sections reduction(|| : argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (95, OpenMP, #pragma", "label": "(58, OpenMP, #pragma omp sections reduction(|| : argc > 0? argv[1] : argv[2])); (102, OpenMP, #pragma omp sections reduction(- : da)); (131, OpenMP, #pragma omp sections reduction(+ : o)); (75, OpenMP, #pragma omp sections reduction(+ : a, b, c, d, f)); (160, OpenMP, #pragma omp sections reduction(+ : fl)); (83, OpenMP, #pragma omp sections reduction(max : h.b)); (117, OpenMP, #pragma omp sections reduction(& : e, g)); (146, OpenMP, #pragma omp sections reduction(+ : r)); (20, OpenMP, #pragma omp sections reduction(-); (129, OpenMP, #pragma omp sections private(i), reduction(+ : j), reduction(+ : q)); (164, OpenMP, #pragma omp sections reduction(+ : m)); (102, OpenMP, #pragma omp sections reduction(&& : S2::S2s)); (61, OpenMP, #pragma omp sections reduction(^ : S1)); (80, OpenMP, #pragma omp sections reduction(+ : ba)); (154, OpenMP, #pragma omp parallel reduction(* : fl)); (-3, OpenMP, #pragma omp parallel); (67, OpenMP, #pragma omp sections reduction(min : a, b, c, d, f)); (111, OpenMP, #pragma omp sections reduction(+ : h, k, B::x)); (0, OpenMP, #pragma omp sections reduction +); (-6, OpenMP, #pragma omp sections reduction); (122, OpenMP, #pragma omp parallel private(k)); (87, OpenMP, #pragma omp sections reduction(^ : fl)); (31, OpenMP, #pragma omp sections reduction(| : argc,); (120, OpenMP, #pragma omp sections reduction(+ : p), reduction(+ : p)); (154, OpenMP, #pragma omp sections reduction(task, + : m)); (13, OpenMP, #pragma omp sections reduction(*)); (132, OpenMP, #pragma omp parallel reduction(min : i)); (136, OpenMP, #pragma omp parallel private(fl)); (40, OpenMP, #pragma omp sections reduction(&& : argc, z)); (69, OpenMP, #pragma omp sections reduction(* : ca)); (3, OpenMP, #pragma omp sections reduction()); (87, OpenMP, #pragma omp sections reduction(&& : S2::S2sc)); (127, OpenMP, #pragma omp sections reduction(max : j)); (-10, OpenMP, #pragma omp sections reduction(); (9, OpenMP, #pragma omp sections reduction(\\)); (13, OpenMP, #pragma omp sections reduction(foo : argc); (121, OpenMP, #pragma omp parallel shared(i)); (26, OpenMP, #pragma omp sections reduction(~ : argc))"}
{"predict": "(14, CUDA, cuda::CUDAFft *blas = new cuda::CUDAFft(cuda_executor);)", "label": "(14, CUDA, cuda::CUDABlas *blas = new cuda::CUDABlas(cuda_executor);)"}
{"predict": "(13, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize());); (11, CUDA, qmc_cuda::cuda_check(cudaGetLastError());)", "label": "(13, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize());); (11, CUDA, qmc_cuda::cuda_check(cudaGetLastError());)"}
{"predict": "(12, MPI, MPI_Abort(MPI_COMM_WORLD, code);); (9, MPI, MPI_Error_string(code, msg, &resultLen);); (8, MPI, MPI_Error_class(code, &class);)", "label": "(11, MPI, MPI_Error_string(code, msg, &resultLen);); (12, MPI, MPI_Abort(MPI_COMM_WORLD, code);); (8, MPI, MPI_Error_class(code, &class);)"}
{"predict": "(2, CUDA, __shared__ uchar as[18][66];)", "label": "(2, CUDA, __shared__ uchar as[18][66];)"}
{"predict": "(37, OpenMP, #pragma omp target parallel for is_device_ptr(raa)); (48, OpenMP, #pragma omp target parallel for is_device_ptr(rh)); (29, OpenMP, #pragma omp target parallel for is_device_ptr(aa)); (13, OpenMP, #pragma omp target parallel for is_device_ptr(k)); (39, OpenMP, #pragma omp target parallel for is_device_ptr(h)); (16, OpenMP, #pragma omp target parallel for is_device_ptr(z)); (49, OpenMP, #pragma omp target parallel for is_device_ptr(da)); (43, OpenMP, #pragma omp target parallel for is_device_ptr(rh)); (34, OpenMP, #pragma omp target parallel for is_device_ptr(raa)); (18, OpenMP, #pragma omp target parallel for is_device_ptr(z)); (51, OpenMP, #pragma omp target parallel for is_device_ptr(da))", "label": "(25, OpenMP, #pragma omp target parallel for is_device_ptr(z)); (36, OpenMP, #pragma omp target parallel for is_device_ptr(raa)); (29, OpenMP, #pragma omp target parallel for is_device_ptr(aa)); (46, OpenMP, #pragma omp target parallel for is_device_ptr(rh)); (39, OpenMP, #pragma omp target parallel for is_device_ptr(h)); (14, OpenMP, #pragma omp target parallel for is_device_ptr(k)); (49, OpenMP, #pragma omp target parallel for is_device_ptr(da))"}
{"predict": "(4, OpenMP, #pragma omp target map(tofrom: is_larger)); (7, OpenMP, #pragma omp for collapse(2) private(i,j) map(tofrom: my_islarger))", "label": "(11, OpenMP, #pragma omp ordered); (3, OpenMP, #pragma omp parallel); (13, OpenMP, #pragma omp critical); (5, OpenMP, #pragma omp for private(i,j) schedule(static,1) collapse(2) ordered)"}
{"predict": "(5, OpenMP, #pragma omp sections private(g, sivar)); (59, OpenMP, #pragma omp sections private(t_var, vec, s_arr, s_arr, var, var, sivar)); (1, OpenMP, #pragma omp parallel); (33, OpenMP, #pragma omp sections private(g, sivar)); (68, OpenMP, #pragma omp sections private(A::x, B::x)); (5, OpenMP, #pragma omp section)", "label": "(60, OpenMP, #pragma omp sections lastprivate(t_var, vec, s_arr, var, sivar)); (4, OpenMP, #pragma omp section); (2, OpenMP, #pragma omp parallel); (28, OpenMP, #pragma omp sections lastprivate(g, sivar)); (65, OpenMP, #pragma omp sections lastprivate(CONDITIONAL A::x, B::x))"}
{"predict": "(4, CUDA, cudaMalloc(&d_ret, THREAD * sizeof(void *));); (31, CUDA, cudaFree(d_ret);); (22, CUDA, cudaMemcpy(h_clk, d_clk, sizeof(uint32_t) * THREAD, cudaMemcpyDeviceToHost);); (4, CUDA, cudaMalloc(&d_clk, sizeof(uint32_t) * THREAD);); (28, CUDA, cudaFree(d_clk);); (-2, CUDA, cudaMalloc(&d_ptr, sizeof(void *) * THREAD);); (18, CUDA, cudaDeviceSynchronize();); (9, CUDA, cudaMemcpy(d_ptr, h_ptr, sizeof(void *) * THREAD, cudaMemcpyHostToDevice);); (23, CUDA, cudaFree(d_ptr);); (9, CUDA, cudaFree(d_clk);)", "label": "(10, CUDA, cudaMallocHost(&h_ptr, THREAD * sizeof(void *));); (29, CUDA, cudaFree(d_ptr);); (25, CUDA, cudaMemcpy(h_clk, d_clk, THREAD * sizeof(uint32_t), cudaMemcpyDeviceToHost);); (28, CUDA, cudaFree(d_ret);); (12, CUDA, cudaMemcpy(d_ptr, h_ptr, THREAD * sizeof(void *), cudaMemcpyHostToDevice);); (27, CUDA, cudaFree(d_clk);); (-1, CUDA, cudaMalloc(&d_ptr, THREAD * sizeof(void *));); (26, CUDA, cudaFreeHost(h_ptr);); (-1, CUDA, cudaMalloc(&d_clk, THREAD * sizeof(uint32_t));); (-3, CUDA, cudaMalloc(&d_ret, sizeof(void *));)"}
{"predict": "(14, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)", "label": "(14, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(20, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(exe_node), *receivers[r] );); (11, TBB, tbb::flow::function_node< InputType, OutputType > exe_node( g, lc, cf );); (49, TBB, tbb::flow::remove_edge( tbb::flow::output_port<0>(exe_node), *receivers[r] );); (0, TBB, tbb::flow::graph g;); (26, TBB, tbb::flow::make_edge( senders[s], exe_node );); (60, TBB, inc_functor body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (64, TBB, g.reset(tbb::flow::rf_reset_bodies);)", "label": "(69, TBB, g.reset(tbb::flow::rf_reset_bodies);); (28, TBB, tbb::flow::make_edge( senders[s], exe_node );); (19, TBB, tbb::flow::make_edge( exe_node, *receivers[r] );); (61, TBB, inc_functor body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (66, TBB, body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (0, TBB, tbb::flow::graph g;); (44, TBB, tbb::flow::remove_edge( exe_node, *receivers[r] );); (4, TBB, tbb::flow::function_node< InputType, OutputType > exe_node( g, lc, cf );)"}
{"predict": "(24, TBB, oneapi::tbb::parallel_for( oneapi::tbb::blocked_range<int> (0, N, 10000), ParallelVectorForBody<T>( vs ) );); (3, TBB, typedef oneapi::tbb::combinable< ContainerType > CombinableType;); (58, TBB, oneapi::tbb::combinable< ContainerType > vs4;); (55, TBB, oneapi::tbb::combinable< ContainerType > vs3;); (20, TBB, oneapi::tbb::combinable< ContainerType > vs2;); (17, TBB, oneapi::tbb::combinable< ContainerType > vs;); (0, TBB, typedef std::vector<T, oneapi::tbb::tbb_allocator<T> > ContainerType;); (53, TBB, oneapi::tbb::combinable< ContainerType > vs4;); (48, TBB, oneapi::tbb::combinable< ContainerType > vs3;); (17, TBB, oneapi::tbb::combinable< ContainerType > vs2;); (53, TBB, oneapi::tbb::combinable< ContainerType > vs4;); (47, TBB, oneapi::tbb::combinable< ContainerType > vs2;); (17, TBB, oneapi::tbb::combinable< ContainerType > vs3;); (44, TBB, oneapi::tbb::combinable< ContainerType > vs;); (49, TBB, oneapi::tbb::combinable< ContainerType > vs4;); (11, TBB, oneapi::tbb::combinable< ContainerType > vs2;); (15, TBB, oneapi::tbb::combinable< ContainerType > vs3;); (44, TBB, oneapi::tbb::combinable< ContainerType > vs2;); (8, TBB, oneapi::tbb::combinable< ContainerType > vs2;); (11, TBB, oneapi::tbb::combinable< ContainerType > vs3;); (44, TBB, oneapi::tbb::combinable< ContainerType > vs3;); (41", "label": "(8, TBB, oneapi::tbb::global_control gc(oneapi::tbb::global_control::max_allowed_parallelism, p);); (23, TBB, oneapi::tbb::parallel_for( oneapi::tbb::blocked_range<int> (0, N, 10000), ParallelVectorForBody<T>( vs ) );); (0, TBB, typedef std::vector<T, oneapi::tbb::tbb_allocator<T> > ContainerType;); (17, TBB, typedef typename oneapi::tbb::combinable< ContainerType > CombinableType;)"}
{"predict": "(16, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(exe_node), receivers[r] );); (60, TBB, inc_functor body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (22, TBB, tbb::flow::make_edge( senders[s], exe_node );); (0, TBB, tbb::flow::graph g;); (43, TBB, tbb::flow::remove_edge( tbb::flow::output_port<0>(exe_node), receivers[r] );); (8, TBB, tbb::flow::function_node< InputType, OutputType > exe_node( g, lc, cf );)", "label": "(16, TBB, tbb::flow::make_edge( exe_node, receivers[r] );); (23, TBB, tbb::flow::make_edge( senders[s], exe_node );); (43, TBB, tbb::flow::remove_edge( exe_node, receivers[r] );); (53, TBB, inc_functor body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (1, TBB, tbb::flow::graph g;); (6, TBB, tbb::flow::function_node< InputType, OutputType > exe_node( g, lc, cf );)"}
{"predict": "(29, CUDA, cudaMalloc((void **)&d_C_ref, m * n * sizeof(float));); (68, CUDA, cudaEventElapsedTime(&elapsed_time, start, stop);); (29, CUDA, cudaEventRecord(start);); (30, CUDA, cudaEventRecord(stop);); (30, CUDA, cudaEventSynchronize(stop);); (61, CUDA, cudaEventCreate(&stop);); (62, CUDA, cudaEventDestroy(start);); (17, CUDA, cudaMalloc((void **)&d_B, k * n * sizeof(float));); (65, CUDA, cudaEventDestroy(stop);); (17, CUDA, cudaMalloc((void **)&d_C, m * n * sizeof(float));); (61, CUDA, cudaFree(d_A);); (62, CUDA, cudaFree(d_B);); (17, CUDA, cudaEventCreate(&start);); (60, CUDA, cudaFree(d_C);); (58, CUDA, cudaFree(d_C_ref);); (20, CUDA, cudaEventRecord(d_A);); (10, CUDA, cudaMalloc((void **)&d_A, m * k * sizeof(float));)", "label": "(29, CUDA, cudaMalloc((void **)&d_C_ref, m * n * sizeof(float));); (68, CUDA, cudaEventElapsedTime(&elapsed_time, start, stop);); (30, CUDA, cudaMemcpy(d_A, A, m * k * sizeof(float), cudaMemcpyHostToDevice);); (32, CUDA, cudaMemcpy(d_C_ref, C_ref, m * n * sizeof(float), cudaMemcpyHostToDevice);); (36, CUDA, cudaMemcpy(C, d_C, m * n * sizeof(float), cudaMemcpyDeviceToHost);); (61, CUDA, cudaEventRecord(stop);); (61, CUDA, cudaEventSynchronize(stop);); (15, CUDA, cudaMalloc((void **)&d_B, k * n * sizeof(float));); (13, CUDA, cudaMalloc((void **)&d_A, m * k * sizeof(float));); (52, CUDA, cudaEventRecord(start);); (13, CUDA, cudaMalloc((void **)&d_C, m * n * sizeof(float));); (23, CUDA, cudaMemcpy(d_C, C, m * n * sizeof(float), cudaMemcpyHostToDevice);); (21, CUDA, cudaMemcpy(d_B, B, k * n * sizeof(float), cudaMemcpyHostToDevice);); (46, CUDA, cudaEventCreate(&stop);); (44, CUDA, cudaEventCreate(&start);)"}
{"predict": "(6, OpenMP, #pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp parallel for)"}
{"predict": "(12, CUDA, A[blockIdx.z * stride + i * lda + i] = thrust::complex<T>(0.0, 0.0);); (7, CUDA, A[blockIdx.z * stride + i * lda + j] = thrust::complex<T>(1.0, 0.0);)", "label": "(8, CUDA, A[blockIdx.z * stride + i * lda + i] = thrust::complex<T>(1.0, 0.0);); (11, CUDA, A[blockIdx.z * stride + j * lda + i] = thrust::complex<T>(0.0, 0.0);)"}
{"predict": "(25, CUDA, cudaMalloc((void**)(&d_t_right), t_right_bytes);); (25, CUDA, cudaMalloc((void**)(&d_t_result), t_result_bytes);); (52, CUDA, cudaFree(d_t_right);); (26, CUDA, cudaMemcpy(d_t_right, t_right.data(), t_right_bytes, cudaMemcpyHostToDevice);); (19, CUDA, cudaMalloc((void**)(&d_t_left), t_left_bytes);); (50, CUDA, cudaFree(d_t_result);); (26, CUDA, cudaMemcpy(d_t_left, t_left.data(), t_left_bytes, cudaMemcpyHostToDevice);); (45, CUDA, cudaFree(d_t_left);); (37, CUDA, cudaMemcpy(t_result_gpu.data(), d_t_result, t_result_bytes, cudaMemcpyDeviceToHost);)", "label": "(25, CUDA, cudaMalloc((void**)(&d_t_right), t_right_bytes);); (25, CUDA, cudaMalloc((void**)(&d_t_result), t_result_bytes);); (50, CUDA, cudaFree((void*)d_t_left);); (26, CUDA, cudaMemcpy(d_t_right, t_right.data(), t_right_bytes, cudaMemcpyHostToDevice);); (20, CUDA, cudaMalloc((void**)(&d_t_left), t_left_bytes);); (23, CUDA, cudaMemcpy(d_t_left, t_left.data(), t_left_bytes, cudaMemcpyHostToDevice);); (48, CUDA, cudaFree((void*)d_t_result);); (46, CUDA, cudaFree((void*)d_t_right);); (36, CUDA, cudaMemcpy(t_result_gpu.data(), d_t_result, t_result_bytes, cudaMemcpyDeviceToHost);)"}
{"predict": "(35, MPI, MPI_Comm_rank(adio_fh->comm, &myrank);); (110, MPI, MPI_Barrier(adio_fh->comm);)", "label": "(37, MPI, MPI_Comm_rank(adio_fh->comm, &myrank);); (24, MPI, MPI_Bcast(&tmp_whence, 1, MPI_INT, 0, adio_fh->comm);); (12, MPI, MPI_Bcast(&tmp_offset, 1, ADIO_OFFSET, 0, adio_fh->comm);); (110, MPI, MPI_Barrier(adio_fh->comm);)"}
{"predict": "(2, OpenMP, #pragma omp parallel num_threads(2) shared(data, out, flag))", "label": "(22, OpenMP, #pragma omp flush(out)); (17, OpenMP, #pragma omp atomic read); (10, OpenMP, #pragma omp atomic write); (-1, OpenMP, #pragma omp target parallel num_threads(64) map(tofrom); (6, OpenMP, #pragma omp flush(data))"}
{"predict": "(11, TBB, tbb::task_scheduler_init init(nthread);)", "label": "(11, TBB, tbb::task_scheduler_init init(nthread);)"}
{"predict": "(2, MPI, MPI_Init(&argc, &argv);); (32, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (11, MPI, MPI_Comm_get_processor_name(procname, &namelen);); (28, MPI, MPI_Barrier(comm_node);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (29, MPI, MPI_Comm_free(&comm_node);); (5, MPI, MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, 0, MPI_INFO_NULL, &comm_node);); (5, MPI, MPI_Comm_rank(comm_node, &rank_node);); (6, MPI, MPI_Comm_size(comm_node, &size_node);)", "label": "(12, MPI, MPI_Comm_size(comm_node, &size_node);); (1, MPI, MPI_Init(&argc, &argv);); (32, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (13, MPI, MPI_Get_processor_name(procname, &namelen);); (5, MPI, MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, 0, MPI_INFO_NULL, &comm_node);); (-1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (6, MPI, MPI_Comm_rank(comm_node, &rank_node);); (19, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(12, OpenMP, #pragma omp parallel for)", "label": "(12, OpenMP, #pragma omp parallel for)"}
{"predict": "(36, CUDA, HANDLE_ERROR( cudaEventRecord( start, 0 ) );); (51, CUDA, HANDLE_ERROR( cudaFreeHost( b ) );); (53, CUDA, HANDLE_ERROR( cudaFreeHost( partial_c ) );); (37, CUDA, HANDLE_ERROR( cudaEventSynchronize( stop ) );); (52, CUDA, HANDLE_ERROR( cudaFreeHost( c ) );); (44, CUDA, HANDLE_ERROR( cudaEventDestroy( start ) );); (44, CUDA, HANDLE_ERROR( cudaEventDestroy( stop ) );); (4, CUDA, HANDLE_ERROR( cudaEventCreate( &stop ) );); (31, CUDA, HANDLE_ERROR( cudaEventRecord( stop, 0 ) );); (11, CUDA, HANDLE_ERROR( cudaHostGetDevicePointer( &dev_b, b, 0 ) );); (34, CUDA, HANDLE_ERROR( cudaEventSynchronize( start ) );); (22, CUDA, HANDLE_ERROR( cudaEventRecord( start, 0 ) );); (4, CUDA, HANDLE_ERROR( cudaEventCreate( &start ) );); (7, CUDA, HANDLE_ERROR( cudaHostGetDevicePointer( &dev_a, a, 0 ) );)", "label": "(56, CUDA, HANDLE_ERROR( cudaEventDestroy( start ) );); (6, CUDA, HANDLE_ERROR( cudaEventCreate( &stop ) );); (55, CUDA, HANDLE_ERROR( cudaEventDestroy( stop ) );); (36, CUDA, HANDLE_ERROR( cudaThreadSynchronize() );); (20, CUDA, HANDLE_ERROR( cudaHostGetDevicePointer( &dev_b, b, 0 ) );); (18, CUDA, HANDLE_ERROR( cudaHostGetDevicePointer( &dev_a, a, 0 ) );); (28, CUDA, HANDLE_ERROR( cudaEventRecord( start, 0 ) );); (33, CUDA, HANDLE_ERROR( cudaEventRecord( stop, 0 ) );); (33, CUDA, HANDLE_ERROR( cudaEventSynchronize( stop ) );); (43, CUDA, HANDLE_ERROR( cudaFreeHost( b ) );); (41, CUDA, HANDLE_ERROR( cudaFreeHost( a ) );); (42, CUDA, HANDLE_ERROR( cudaFreeHost( partial_c ) );); (-6, CUDA, HANDLE_ERROR( cudaEventCreate( &start ) );)"}
{"predict": "(13, MPI, mpi_err = MPI_Type_commit(&type);); (10, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(13, MPI, mpi_err = MPI_Type_commit(&type);); (9, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(3, TBB, tbb::parallel_for( range, CheckRecursiveRange< ContainerType,Iterator, RangeType>( sum ) );); (7, TBB, tbb::split_range< RangeType > range2( range );)", "label": "(7, TBB, RangeType range2( range, tbb::split() );)"}
{"predict": "(20, MPI, MPI_Bcast( & tmp, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD );)", "label": "(20, MPI, MPI_Allreduce( & tmp, & result, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD );)"}
{"predict": "(3, CUDA, __shared__ float local[threads];)", "label": "(3, CUDA, __shared__ float local[threads];)"}
{"predict": "(8, CUDA, __shared__ float s_variance;); (6, CUDA, __shared__ float s_mean;)", "label": "(9, CUDA, __shared__ float s_variance;); (7, CUDA, __shared__ float s_mean;)"}
{"predict": "(100, MPI, MPI_File_open(MPI_COMM_WORLD, file_name.c_str(), MPI_MODE_CREATE | MPI_MODE_WRONLY, MPI_INFO_NULL, &file);); (104, MPI, MPI_File_write(file, &size_array, 1, MPI_INT, &status);); (109, MPI, MPI_File_close(&file);); (98, MPI, MPI_File_seek(file, mpi_offset, MPI_SEEK_SET);); (106, MPI, MPI_File_write(file, &data_size[rank], 1, MPI_INT, &status);); (99, MPI, MPI_File_close(&file);); (105, MPI, MPI_File_read(file, &data_size[0], 1, MPI_INT, &status);)", "label": "(99, MPI, MPI_File_open(MPI_COMM_WORLD, file_name.c_str(), MPI_MODE_CREATE | MPI_MODE_WRONLY, MPI_INFO_NULL, &file);); (105, MPI, MPI_File_write(file, &size_array, 1, MPI_INT, &status);); (110, MPI, MPI_File_close(&file);); (102, MPI, MPI_File_seek(file, mpi_offset, MPI_SEEK_SET);); (107, MPI, MPI_File_write(file, &pos_values[0], data_size[rank], MPI_DOUBLE, &status);)"}
{"predict": "(15, CUDA, __shared__ float Bsh[_M_];)", "label": "(16, CUDA, extern __shared__ volatile float Bsh[];)"}
{"predict": "(7, OpenMP, #pragma omp target); (28, OpenMP, #pragma omp parallel for reduction(+:i))", "label": "(8, MPI, MPI_Init(&argc, &argv);); (28, MPI, MPI_Finalize();)"}
{"predict": "(20, CUDA, cudaStream_t stream = at::cuda::getCurrentCUDAStream();)", "label": "(20, CUDA, cudaStream_t stream = at::cuda::getCurrentCUDAStream();)"}
{"predict": "(60, CUDA, cudaMalloc(&p.workspace, p.workspace_size());); (113, CUDA, cudaEventRecord(events[0]);); (88, CUDA, cudaEventRecord(events[1]);); (60, CUDA, cudaEventCreate(&events[0]);); (113, CUDA, cudaEventSynchronize(events[1]);); (60, CUDA, cudaEventCreate(&events[1]);); (111, CUDA, cudaEventElapsedTime(&runtime_ms, events[0], events[1]);); (113, CUDA, cudaEventDestroy(event);)", "label": "(85, CUDA, cudaDeviceSynchronize();); (59, CUDA, cudaMalloc(&p.workspace, p.workspace_size());); (114, CUDA, CUDA_CHECK(cudaEventSynchronize(events[1]));); (112, CUDA, CUDA_CHECK(cudaEventRecord(events[1]));); (84, CUDA, CUDA_CHECK(cudaFuncSetAttribute(kernel_fn, cudaFuncAttributeMaxDynamicSharedMemorySize, int(smem_bytes)));); (114, CUDA, CUDA_CHECK(cudaEventElapsedTime(&runtime_ms, events[0], events[1]));); (9, CUDA, std::cerr << \"FATAL: \" #FN \" failed: \" << cudaGetErrorString(cudaError) << std::endl;); (102, CUDA, CUDA_CHECK(cudaEventCreate(&event));); (103, CUDA, CUDA_CHECK(cudaEventRecord(events[0]));)"}
{"predict": "(19, CUDA, cudaMemcpy(pointers, cpuPointers, sizeof(uint8_t*) * total * 3, cudaMemcpyHostToDevice);); (22, CUDA, FastllmCudaFree(pointers);); (16, CUDA, uint8_t ** pointers = (uint8_t**)FastllmCudaMalloc(sizeof(uint8_t*) * total * 3);)", "label": "(19, CUDA, cudaMemcpy(pointers, cpuPointers, sizeof(uint8_t*) * total * 3, cudaMemcpyHostToDevice);); (6, CUDA, uint8_t ** pointers = (uint8_t**)FastllmCudaMalloc(sizeof(uint8_t*) * total * 3);); (20, CUDA, FastllmCudaFree(pointers);)"}
{"predict": "(104, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ j + 1 ], 0, MPI_COMM_WORLD );); (99, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ j - 1 ], 1, MPI_COMM_WORLD );); (10, MPI, MPI_Comm_size( MPI_COMM_WORLD, &Parallel::nprocs );); (130, MPI, MPI_Allgather( &tmp, 1, MPI_DOUBLE, x.data(), 1, MPI_DOUBLE, MPI_COMM_WORLD );); (104, MPI, MPI_Recv( A[ 3 ].data(), N + 1, MPI_DOUBLE, activep[ j - 1 ], 0, MPI_COMM_WORLD, &status );); (7, MPI, MPI_Init( &argc, &argv );); (104, MPI, MPI_Recv( A[ 4 ].data(), N + 1, MPI_DOUBLE, activep[ j + 1 ], 1, MPI_COMM_WORLD, &status );); (97, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ numactivep - 2 ], 1, MPI_COMM_WORLD );); (125, MPI, MPI_Finalize();); (4, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &Parallel::pid );); (92, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ 1 ], 0, MPI_COMM_WORLD );); (93, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ numactivep - 2 ], 1, MPI_COMM_WORLD );)", "label": "(106, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ j + 1 ], 0, MPI_COMM_WORLD );); (104, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ j - 1 ], 1, MPI_COMM_WORLD );); (108, MPI, MPI_Recv( A[ 3 ].data(), N + 1, MPI_DOUBLE, activep[ j - 1 ], 0, MPI_COMM_WORLD, &status );); (7, MPI, MPI_Comm_size( MPI_COMM_WORLD, &Parallel::nprocs );); (97, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ numactivep - 2 ], 1, MPI_COMM_WORLD );); (135, MPI, MPI_Allgather( &tmp, 1, MPI_DOUBLE, x.data(), 1, MPI_DOUBLE, MPI_COMM_WORLD);); (187, MPI, MPI_Finalize();); (157, MPI, MPI_Allgather( &tmp, 1, MPI_DOUBLE, x.data(), 1, MPI_DOUBLE, MPI_COMM_WORLD );); (1, MPI, MPI_Init( &argc, &argv );); (2, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &Parallel::pid );); (101, MPI, MPI_Recv( A[ 4 ].data(), N + 1, MPI_DOUBLE, activep[ j + 1 ], 1, MPI_COMM_WORLD, &status );); (86, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ 1 ], 0, MPI_COMM_WORLD );)"}
{"predict": "(25, OpenMP, #pragma omp parallel default(shared)); (10, OpenMP, #pragma omp parallel); (37, OpenMP, #pragma omp parallel num_threads(x)); (15, OpenMP, #pragma omp parallel default(none)); (28, OpenMP, #pragma omp parallel default(firstprivate))", "label": "(25, OpenMP, #pragma omp parallel default(shared)); (10, OpenMP, #pragma omp parallel); (37, OpenMP, #pragma omp parallel num_threads(x)); (15, OpenMP, #pragma omp parallel default(none)); (28, OpenMP, #pragma omp parallel default(firstprivate))"}
{"predict": "(41, CUDA, __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];); (36, CUDA, __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];)", "label": "(41, CUDA, __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];); (36, CUDA, __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];)"}
{"predict": "(12, CUDA, __shared__ unsigned char sharedHistogram[NUM_THREADS][cBuckets];)", "label": "(12, CUDA, __shared__ unsigned char sharedHistogram[NUM_THREADS][cBuckets];)"}
{"predict": "(25, TBB, tbb::flow::make_edge(bnode, tbb::flow::input_port<0>(jnode));); (7, TBB, tbb::flow::make_edge(bnode, fnode);); (0, TBB, tbb::flow::sequencer_node<int> bnode(g, seq_body());); (-2, TBB, tbb::flow::graph g;); (22, TBB, tbb::flow::join_node<std::tuple<int,int>,tbb::flow::reserving> jnode(g);); (28, TBB, g.reset(tbb::flow::rf_clear_edges);); (-4, TBB, tbb::flow::function_node<int> fnode(g, tbb::flow::serial, serial_fn_body<int>(serial_fn_state0));); (17, TBB, tbb::flow::remove_edge(bnode, fnode);)", "label": "(25, TBB, tbb::flow::make_edge(bnode, tbb::flow::input_port<0>(jnode));); (7, TBB, tbb::flow::make_edge(bnode, fnode);); (22, TBB, tbb::flow::join_node<std::tuple<int,int>,tbb::flow::reserving> jnode(g);); (-1, TBB, tbb::flow::sequencer_node<int> bnode(g, seq_body());); (-3, TBB, tbb::flow::graph g;); (32, TBB, g.reset(tbb::flow::rf_clear_edges);); (-2, TBB, tbb::flow::function_node<int> fnode(g, tbb::flow::serial, serial_fn_body<int>(serial_fn_state0));); (15, TBB, tbb::flow::remove_edge(bnode, fnode);)"}
{"predict": "(34, TBB, oneapi::tbb::tick_count start = oneapi::tbb::tick_count::now();); (69, TBB, utility::report_elapsed_time((oneapi::tbb::tick_count::now() - start).seconds());); (49, TBB, oneapi::tbb::flow::make_edge(the_source, the_value_pool);); (53, TBB, oneapi::tbb::flow::make_edge(the_value_pool, *(bins[i]));); (44, TBB, oneapi::tbb::flow::graph g;); (30, TBB, oneapi::tbb::flow::graph_init init(p);)", "label": "(34, TBB, oneapi::tbb::tick_count start = oneapi::tbb::tick_count::now();); (69, TBB, utility::report_elapsed_time((oneapi::tbb::tick_count::now() - start).seconds());); (49, TBB, oneapi::tbb::flow::make_edge(the_source, the_value_pool);); (53, TBB, oneapi::tbb::flow::make_edge(the_value_pool, *(bins[i]));); (44, TBB, oneapi::tbb::flow::graph g;); (31, TBB, oneapi::tbb::global_control c(oneapi::tbb::global_control::max_allowed_parallelism, p);)"}
{"predict": "(22, OpenMP, #pragma omp target teams distribute parallel for simd safelen (ST); (30, OpenMP, #pragma omp target teams distribute parallel for simd safelen (foobool(argc)), safelen (true), safelen (-5)); (49, OpenMP, #pragma omp target teams distribute parallel for simd safelen (N)); (37, OpenMP, #pragma omp target teams distribute parallel for simd safelen (S)); (5, OpenMP, #pragma omp target teams distribute parallel for simd safelen ()); (17, OpenMP, #pragma omp target teams distribute parallel for simd safelen (1))); (-3, OpenMP, #pragma omp target teams distribute parallel for simd safelen); (27, OpenMP, #pragma omp target teams distribute parallel for simd safelen (foobool(1) > 0? 1 : 2)); (46, OpenMP, #pragma omp target teams distribute parallel for simd safelen (4)); (4, OpenMP, #pragma omp target teams distribute parallel for simd safelen (argc); (7, OpenMP, #pragma omp target teams distribute parallel for simd safelen (ST); (-4, OpenMP, #pragma omp target teams distribute parallel for simd safelen (); (12, OpenMP, #pragma omp target teams distribute parallel for simd safelen ((ST > 0)? 1 + ST : 2)); (-7, OpenMP, #pragma omp target teams distribute parallel for simd safelen)", "label": "(37, OpenMP, #pragma omp target teams distribute parallel for simd safelen (S)); (17, OpenMP, #pragma omp target teams distribute parallel for simd safelen (ST); (20, OpenMP, #pragma omp target teams distribute parallel for simd safelen (1))); (51, OpenMP, #pragma omp target teams distribute parallel for simd safelen (N)); (46, OpenMP, #pragma omp target teams distribute parallel for simd safelen (4)); (2, OpenMP, #pragma omp target teams distribute parallel for simd safelen (); (20, OpenMP, #pragma omp target teams distribute parallel for simd safelen ((ST > 0)? 1 + ST : 2)); (26, OpenMP, #pragma omp target teams distribute parallel for simd safelen (foobool(argc)), safelen (true), safelen (-5)); (3, OpenMP, #pragma omp target teams distribute parallel for simd safelen ()); (5, OpenMP, #pragma omp target teams distribute parallel for simd safelen (argc); (-7, OpenMP, #pragma omp target teams distribute parallel for simd safelen)"}
{"predict": "(13, CUDA, extern __shared__ char shared_mem[];)", "label": "(13, CUDA, extern __shared__ char shared_mem[];)"}
{"predict": "(14, MPI, MPI_Abort(MPI_COMM_WORLD, 127);)", "label": "(14, MPI, MPI_Abort(MPI_COMM_WORLD, 127);)"}
{"predict": "(51, CUDA, cuda_safe_call(cudaStreamSynchronize(stream));)", "label": "(51, CUDA, cuda_safe_call(cudaStreamSynchronize(stream));)"}
{"predict": "(34, OpenCL, TEST_OCL_CHECK(clReleaseCommandQueue(interop_ocl_queue));)", "label": "(34, OpenCL, TEST_OCL_CHECK(clReleaseCommandQueue(interop_ocl_queue));)"}
{"predict": "(15, OpenMP, #pragma omp taskloop simd private(argc, b), firstprivate(c, d), lastprivate(d, f) collapse(N) shared(g) if (c) final(d) mergeable priority(f) simdlen(N) nogroup num_tasks(N)); (13, OpenMP, #pragma omp parallel); (5, OpenMP, #pragma omp taskloop simd allocate(d) if(taskloop: argc > N) default(shared) untied priority(N) safelen(N) linear(c) aligned(ptr) grainsize(N) reduction(+:g) in_reduction(+: d)); (3, OpenMP, #pragma omp taskgroup task_reduction(+: d) allocate(d))", "label": "(15, OpenMP, #pragma omp taskloop simd private(argc, b), firstprivate(c, d), lastprivate(d, f) collapse(N) shared(g) if (c) final(d) mergeable priority(f) simdlen(N) nogroup num_tasks(N)); (13, OpenMP, #pragma omp parallel); (5, OpenMP, #pragma omp taskloop simd allocate(d) if(taskloop: argc > N) default(shared) untied priority(N) safelen(N) linear(c) aligned(ptr) grainsize(N) reduction(+:g) in_reduction(+: d)); (3, OpenMP, #pragma omp taskgroup task_reduction(+: d) allocate(d))"}
{"predict": "(10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (43, MPI, mpi_errno = MPI_Comm_free(&stream_comm);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (16, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (43, MPI, mpi_errno = MPI_Comm_free(&stream_comm);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (15, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(18, MPI, *ierr = MPI_Info_delete( (MPI_Info)(*v1), p2 );)", "label": "(18, MPI, *ierr = MPI_Info_delete( (MPI_Info)(*v1), p2 );)"}
{"predict": "(2, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (11, TBB, tbb::tick_count t1 = tbb::tick_count::now();)", "label": "(2, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (11, TBB, tbb::tick_count t1 = tbb::tick_count::now();)"}
{"predict": "(12, CUDA, A_[j] = static_cast<thrust::complex<T1>>(B_[j]);)", "label": "(12, CUDA, A_[j] = static_cast<thrust::complex<T1>>(B_[j]);)"}
{"predict": "(21, OpenMP, #pragma omp for linear(argv[1])); (22, OpenMP, #pragma omp for linear(e, g)); (5, OpenMP, #pragma omp for linear(); (8, OpenMP, #pragma omp for linear(argc); (25, OpenMP, #pragma omp for linear(h)); (2, OpenMP, #pragma omp for linear); (35, OpenMP, #pragma omp for linear(j)); (26, OpenMP, #pragma omp parallel); (31, OpenMP, #pragma omp for linear(v : j)); (36, OpenMP, #pragma omp for linear(v : j)); (21, OpenMP, #pragma omp for linear(i)); (2, OpenMP, #pragma omp for linear()); (9, OpenMP, #pragma omp for linear(argc > 0? argv[1] : argv[2])); (22, OpenMP, #pragma omp for linear(v : i)); (7, OpenMP, #pragma omp for linear(argc) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (-3, OpenMP, #pragma omp for linear(argc,); (12, OpenMP, #pragma omp for linear(a, b : B::ib)); (1, OpenMP, #pragma omp for linear(argc,)", "label": "(7, OpenMP, #pragma omp for linear (); (24, OpenMP, #pragma omp for linear (argv[1])); (17, OpenMP, #pragma omp for linear (S1)); (12, OpenMP, #pragma omp for linear (argc > 0? argv[1] : argv[2])); (1, OpenMP, #pragma omp for linear); (22, OpenMP, #pragma omp for linear(e, g, k)); (27, OpenMP, #pragma omp parallel); (33, OpenMP, #pragma omp for linear(j)); (3, OpenMP, #pragma omp for linear (argc); (34, OpenMP, #pragma omp for linear(v:j)); (-1, OpenMP, #pragma omp for linear ()); (2, OpenMP, #pragma omp for linear (argc,); (11, OpenMP, #pragma omp for linear (a, b:B::ib)); (34, OpenMP, #pragma omp for linear(i) ordered(1)); (15, OpenMP, #pragma omp for linear(h)); (2, OpenMP, #pragma omp for linear (argc : 5) allocate, allocate(, allocate(omp_default, allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (21, OpenMP, #pragma omp for linear(v:i)); (14, OpenMP, #pragma omp for linear(i))"}
{"predict": "(16, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)", "label": "(16, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(8, CUDA, cudaStreamSynchronize(LU_v1->A_gpu.cuStreams[0]);)", "label": "(8, CUDA, cudaStreamSynchronize(LU_v1->A_gpu.cuStreams[0]);)"}
{"predict": "(31, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(31, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(5, CUDA, __shared__ float reduce_smem[NUM_WARPS];)", "label": "(5, CUDA, __shared__ float reduce_smem[NUM_WARPS];)"}
{"predict": "(16, CUDA, extern __shared__ float buffer[];)", "label": "(16, CUDA, extern __shared__ float buffer[];)"}
{"predict": "(85, MPI, MPI_Bcast(&min_source, 1, MPI_INT, source, scr_comm_world);); (96, MPI, MPI_Allreduce(&min_depth, &min_global, 1, MPI_INT, MPI_MIN, scr_comm_world);); (96, MPI, MPI_Allreduce(&max_depth, &max_global, 1, MPI_INT, MPI_MAX, scr_comm_world);); (168, MPI, MPI_Barrier(scr_comm_world);); (76, MPI, MPI_Allreduce(&source_rank, &source, 1, MPI_INT, MPI_MIN, scr_comm_world);)", "label": "(85, MPI, MPI_Bcast(&min_source, 1, MPI_INT, source, scr_comm_world);); (96, MPI, MPI_Allreduce(&min_depth, &min_global, 1, MPI_INT, MPI_MIN, scr_comm_world);); (76, MPI, MPI_Allreduce(&source_rank, &source, 1, MPI_INT, MPI_MIN, scr_comm_world);); (95, MPI, MPI_Allreduce(&max_depth, &max_global, 1, MPI_INT, MPI_MAX, scr_comm_world);); (168, MPI, MPI_Barrier(scr_comm_world);)"}
{"predict": "(8, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc((void**)&d_out, size * sizeof(DataType)));); (32, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(h_out.data(), d_out, size * sizeof(DataType), cudaMemcpyDeviceToHost));); (32, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(h_ref.data(), d_ref, size * sizeof(DataType), cudaMemcpyDeviceToHost));); (49, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(states));); (26, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (18, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc((void**)&states, block_dim * grid_size * sizeof(RNG)));); (23, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (46, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(d_out));); (7, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(d_out, NULL, size * sizeof(DataType), cudaMemcpyNull));); (42, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(d_ref));); (11, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc((void**)&d_ref, size * sizeof(DataType)));)", "label": "(8, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc((void**)&d_out, size * sizeof(DataType)));); (32, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(h_out.data(), d_out, size * sizeof(DataType), cudaMemcpyDeviceToHost));); (28, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (35, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc((void**)&d_ref, size * sizeof(DataType)));); (48, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(d_out));); (16, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc((void**)&states, block_dim * grid_size * sizeof(RNG)));); (19, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (44, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(states));); (45, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(d_ref));); (39, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(h_ref.data(), d_ref, size * sizeof(DataType), cudaMemcpyDeviceToHost));)"}
{"predict": "(36, OpenCL, err = clSetKernelArg(scan_kernel, 1, dyn_mem_size, NULL);); (31, OpenCL, err = clSetKernelArg(scan_kernel, 0, sizeof(cl_mem), &buffer);)", "label": "(36, OpenCL, err = clSetKernelArg(scan_kernel, 1, dyn_mem_size, NULL);); (31, OpenCL, err = clSetKernelArg(scan_kernel, 0, sizeof(buffer), &buffer);)"}
{"predict": "(22, MPI, err = MPI_Type_free(&tmptype);); (13, MPI, err = MPI_Type_create_struct(1, &blen, &displ, &type, &tmptype);)", "label": "(22, MPI, err = MPI_Type_create_resized(tmptype, 0, aext * sizeof(int), datatype);); (28, MPI, MPI_Type_free(&tmptype);); (12, MPI, err = MPI_Type_create_struct(1, &blen, &displ, &type, &tmptype);)"}
{"predict": "(9, OpenMP, #pragma omp atomic seq_cst seq_cst); (3, OpenMP, #pragma omp atomic seq_cst); (10, OpenMP, #pragma omp atomic update seq_cst)", "label": "(4, OpenMP, #pragma omp atomic seq_cst); (8, OpenMP, #pragma omp atomic seq_cst seq_cst); (10, OpenMP, #pragma omp atomic update seq_cst)"}
{"predict": "(12, CUDA, img[idx] = cudaAlphaBlend(img[idx], color);)", "label": "(12, CUDA, img[idx] = cudaAlphaBlend(img[idx], color);)"}
{"predict": "(25, MPI, MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);); (40, MPI, err = MPI_File_close(&fh);); (5, MPI, MPI_Init(&argc, &argv);); (142, MPI, MPI_Finalize();); (22, MPI, MPI_Bcast(filename, len + 10, MPI_CHAR, 0, MPI_COMM_WORLD);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (39, MPI, MPI_Barrier(MPI_COMM_WORLD);); (12, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)", "label": "(25, MPI, MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);); (40, MPI, err = MPI_File_close(&fh);); (4, MPI, MPI_Init(&argc, &argv);); (143, MPI, MPI_Finalize();); (22, MPI, MPI_Bcast(filename, len + 10, MPI_CHAR, 0, MPI_COMM_WORLD);); (2, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (39, MPI, MPI_Barrier(MPI_COMM_WORLD);); (12, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(106, MPI, MPI_Reduce(ldinbuf, ldoutbuf, 3, MPI_LONG_DOUBLE, MPI_MAX, 0, comm);); (137, MPI, MPI_Reduce(llinbuf, lloutbuf, 3, MPI_LONG_LONG, MPI_MAX, 0, comm);); (12, MPI, MPI_Comm_size(comm, &size);); (23, MPI, MPI_Reduce(cinbuf, coutbuf, 3, MPI_CHAR, MPI_MAX, 0, comm);); (49, MPI, MPI_Reduce(scinbuf, scoutbuf, 3, MPI_SIGNED_CHAR, MPI_MAX, 0, comm);); (72, MPI, MPI_Reduce(ucinbuf, ucoutbuf, 3, MPI_UNSIGNED_CHAR, MPI_MAX, 0, comm);); (7, MPI, MPI_Comm_rank(comm, &rank);)", "label": "(106, MPI, MPI_Reduce(ldinbuf, ldoutbuf, 3, MPI_LONG_DOUBLE, MPI_MAX, 0, comm);); (13, MPI, MPI_Comm_size(comm, &size);); (24, MPI, MPI_Reduce(cinbuf, coutbuf, 3, MPI_CHAR, MPI_MAX, 0, comm);); (49, MPI, MPI_Reduce(scinbuf, scoutbuf, 3, MPI_SIGNED_CHAR, MPI_MAX, 0, comm);); (73, MPI, MPI_Reduce(ucinbuf, ucoutbuf, 3, MPI_UNSIGNED_CHAR, MPI_MAX, 0, comm);); (8, MPI, MPI_Comm_rank(comm, &rank);); (132, MPI, MPI_Reduce(llinbuf, lloutbuf, 3, MPI_LONG_LONG, MPI_MAX, 0, comm);)"}
{"predict": "(17, OpenMP, #pragma omp distribute parallel for copyin(argc > 0? argv[1] : argv[2])); (10, OpenMP, #pragma omp distribute parallel for copyin()); (35, OpenMP, #pragma omp distribute parallel for copyin(m)); (22, OpenMP, #pragma omp distribute parallel for copyin(l)); (15, OpenMP, #pragma omp distribute parallel for copyin(k); (4, OpenMP, #pragma omp distribute parallel for copyin(); (29, OpenMP, #pragma omp distribute parallel for copyin(i)); (-3, OpenMP, #pragma omp distribute parallel for copyin); (21, OpenMP, #pragma omp distribute parallel for copyin(argv[1])); (-8, OpenMP, #pragma omp target); (0, OpenMP, #pragma omp distribute parallel for copyin()); (30, OpenMP, #pragma omp distribute parallel for copyin(ST<int>::s, B::x)); (-9, OpenMP, #pragma omp teams)", "label": "(19, OpenMP, #pragma omp distribute parallel for proc_bind(master); (3, OpenMP, #pragma omp distribute parallel for proc_bind); (12, OpenMP, #pragma omp distribute parallel for proc_bind()); (6, OpenMP, #pragma omp distribute parallel for proc_bind(); (25, OpenMP, #pragma omp distribute parallel for proc_bind(x)); (37, OpenMP, #pragma omp distribute parallel for proc_bind(spread)); (18, OpenMP, #pragma omp distribute parallel for proc_bind(close), proc_bind(spread)); (-5, OpenMP, #pragma omp target); (31, OpenMP, #pragma omp parallel proc_bind(close)); (26, OpenMP, #pragma omp distribute parallel for proc_bind(master)); (-7, OpenMP, #pragma omp teams)"}
{"predict": "(19, OpenCL, retVal = clEnqueueReleaseGLObjects(commandQueue, 1, &glBuffer, 0, nullptr, &retEvent);); (30, OpenCL, retVal = clReleaseMemObject(glBuffer);); (3, OpenCL, auto glBuffer = clCreateFromGLBuffer(&context, 0, bufferId, &retVal);); (1, OpenCL, auto commandQueue = clCreateCommandQueue(&context, context.getDevice(0), 0, &retVal);); (12, OpenCL, retVal = clReleaseEvent(retEvent);); (5, OpenCL, retVal = clEnqueueAcquireGLObjects(commandQueue, 1, &glBuffer, 0, nullptr, &retEvent);); (24, OpenCL, retVal = clReleaseCommandQueue(commandQueue);)", "label": "(19, OpenCL, retVal = clEnqueueReleaseGLObjects(commandQueue, 1, &glBuffer, 0, nullptr, &retEvent);); (30, OpenCL, retVal = clReleaseMemObject(glBuffer);); (3, OpenCL, auto glBuffer = clCreateFromGLBuffer(&context, 0, bufferId, &retVal);); (1, OpenCL, auto commandQueue = clCreateCommandQueue(&context, context.getDevice(0), 0, &retVal);); (12, OpenCL, retVal = clReleaseEvent(retEvent);); (1, OpenCL, retVal = clEnqueueAcquireGLObjects(commandQueue, 1, &glBuffer, 0, nullptr, &retEvent);); (23, OpenCL, retVal = clReleaseCommandQueue(commandQueue);)"}
{"predict": "(21, MPI, MPI_Type_set_attr(type, key[2], &attrval[2]);); (22, MPI, MPI_Type_set_attr(type, key[0], &attrval[0]);); (54, MPI, MPI_Type_free_keyval(&key[i]);); (19, MPI, MPI_Type_set_attr(type, key[1], &attrval[1]);); (23, MPI, MPI_Type_delete_attr(type, key[0]);); (23, MPI, MPI_Type_delete_attr(type, key[1]);); (23, MPI, MPI_Type_delete_attr(type, key[2]);)", "label": "(21, MPI, MPI_Type_set_attr(type, key[2], &attrval[2]);); (22, MPI, MPI_Type_set_attr(type, key[0], &attrval[0]);); (56, MPI, MPI_Type_free_keyval(&key[i]);); (19, MPI, MPI_Type_set_attr(type, key[1], &attrval[1]);); (23, MPI, MPI_Type_delete_attr(type, key[0]);); (23, MPI, MPI_Type_delete_attr(type, key[1]);); (23, MPI, MPI_Type_delete_attr(type, key[2]);)"}
{"predict": "(5, CUDA, __shared__ float reduce_smem[NUM_WARPS];)", "label": "(5, CUDA, __shared__ float reduce_smem[NUM_WARPS];)"}
{"predict": "(8, TBB, tbb::enumerable_thread_specific<HyperedgeWeight> deltas(0);)", "label": "(8, TBB, tbb::enumerable_thread_specific<HyperedgeWeight> deltas(0);)"}
{"predict": "(8, OpenCL, retVal = clEnqueueMemcpyINTEL(commandQueues[0][0].get(), true, deviceMemAlloc, buffer.data(), bufferSize, 0, nullptr, nullptr);); (16, OpenCL, retVal = clEnqueueMemcpyINTEL(commandQueues[0][0].get(), true, hostMemAlloc, deviceMemAlloc, bufferSize, 0, nullptr, nullptr);)", "label": "(8, OpenCL, retVal = clEnqueueMemcpyINTEL(commandQueues[0][0].get(), true, deviceMemAlloc, buffer.data(), bufferSize, 0, nullptr, nullptr);); (16, OpenCL, retVal = clEnqueueMemcpyINTEL(commandQueues[0][0].get(), true, hostMemAlloc, deviceMemAlloc, bufferSize, 0, nullptr, nullptr);)"}
{"predict": "(42, MPI, MPI_Recv(receiveBuffer.data(), msgsize, MPI_INT, 1, MPI_ANY_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (7, MPI, MPI_Init(&argc, &argv);); (49, MPI, MPI_Send(message.data(), msgsize, MPI_INT, 0, 1, MPI_COMM_WORLD);); (54, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (30, MPI, MPI_Send(message.data(), msgsize, MPI_INT, 1, tag, MPI_COMM_WORLD);); (29, MPI, MPI_Recv(receiveBuffer.data(), msgsize, MPI_INT, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status);); (43, MPI, MPI_Recv(receiveBuffer.data(), msgsize, MPI_INT, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status);); (43, MPI, MPI_Send(message.data(), msgsize, MPI_INT, 1, 0, MPI_COMM_WORLD);); (43, MPI, MPI_Recv(receiveBuffer.data(), msgsize, MPI_INT, 1, MPI_ANY_TAG, MPI_COMM_WORLD, &status);); (0, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (26, MPI, MPI_Recv(receiveBuffer.data(), msgsize, MPI_INT, 1, MPI_ANY_TAG, MPI_COMM_WORLD, &status);)", "label": "(42, MPI, MPI_Recv(receiveBuffer.data(), msgsize, MPI_INT, 1, MPI_ANY_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (7, MPI, MPI_Init(&argc, &argv);); (48, MPI, MPI_Send(message.data(), msgsize, MPI_INT, 0, 1, MPI_COMM_WORLD);); (54, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (33, MPI, MPI_Send(message.data(), msgsize, MPI_INT, 1, tag, MPI_COMM_WORLD);); (8, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (41, MPI, MPI_Recv(receiveBuffer.data(), msgsize, MPI_INT, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status);)"}
{"predict": "(9, CUDA, CUDA_CHECK(cudaGetDeviceCount(&ndevices));); (9, CUDA, CUDA_CHECK(cudaSetDevice(rank % ndevices));); (0, MPI, MPI_Init(&argc, &argv);); (37, MPI, MPI_Finalize();); (1, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)", "label": "(9, CUDA, CUDA_CHECK(cudaGetDeviceCount(&ndevices));); (9, CUDA, CUDA_CHECK(cudaSetDevice(rank % ndevices));); (0, MPI, MPI_Init(&argc, &argv);); (37, MPI, MPI_Finalize();); (1, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)"}
{"predict": "(2, CUDA, extern __shared__ char smem[];)", "label": "(2, CUDA, extern __shared__ char smem[];)"}
{"predict": "(10, CUDA, __shared__ FP sV[Bc][dim];); (13, CUDA, __shared__ FP sMax[Br];); (20, CUDA, __shared__ FP sNewO[Br][dim];); (9, CUDA, __shared__ FP sK[Bc][dim];); (11, CUDA, __shared__ FP sO[Br][dim];); (13, CUDA, __shared__ FP sDenom[Br];); (9, CUDA, __shared__ FP sQK[Br][Bc];); (11, CUDA, __shared__ FP sSafeE[Br][Bc];); (9, CUDA, __shared__ FP sQ[Br][dim];); (11, CUDA, __shared__ FP sV[k][i * Bc + tx];)", "label": "(12, CUDA, __shared__ FP sV[Bc][dim];); (20, CUDA, __shared__ FP sMax[Br];); (9, CUDA, __shared__ FP sK[Bc][dim];); (11, CUDA, __shared__ FP sQK[Br][Bc];); (14, CUDA, __shared__ FP sSafeE[Br][Bc];); (8, CUDA, __shared__ FP sO[Br][dim];); (10, CUDA, __shared__ FP sNewO[Br][dim];); (3, CUDA, __shared__ FP sQ[Br][dim];); (12, CUDA, __shared__ FP sDenom[Br];)"}
{"predict": "(39, OpenMP, #pragma omp parallel for)", "label": "(7, OpenMP, #pragma omp parallel for)"}
{"predict": "(12, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (13, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;)", "label": "(12, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (13, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;)"}
{"predict": "(26, OpenCL, clCreateCommandQueueWithProperties_StubWithCallback(clCreateCommandQueueWithProperties_testCommandQueueFromSpecifiedContext);); (36, OpenCL, clReleaseCommandQueue_ExpectAndReturn(make_command_queue(0), CL_SUCCESS);); (40, OpenCL, clReleaseMemObject_ExpectAndReturn(expected, CL_SUCCESS);); (27, OpenCL, clCreateCommandQueue_StubWithCallback(clCreateCommandQueue_testCommandQueueFromSpecifiedContext);); (29, OpenCL, clReleaseEvent_StubWithCallback(clReleaseEvent_testCopyHostToBuffer);); (40, OpenCL, clReleaseContext_ExpectAndReturn(make_context(0), CL_SUCCESS);); (29, OpenCL, clEnqueueMapBuffer_StubWithCallback(clEnqueueMapBuffer_testCopyHostToBuffer);); (29, OpenCL, clEnqueueUnmapMemObject_StubWithCallback(clEnqueueUnmapMemObject_testCopyHostToBuffer);)", "label": "(16, OpenCL, clCreateBuffer_StubWithCallback(clCreateBuffer_testBufferConstructorContextIterator);); (35, OpenCL, clReleaseCommandQueue_ExpectAndReturn(make_command_queue(0), CL_SUCCESS);); (43, OpenCL, clReleaseMemObject_ExpectAndReturn(expected, CL_SUCCESS);); (24, OpenCL, clCreateCommandQueueWithProperties_StubWithCallback(clCreateCommandQueueWithProperties_testCommandQueueFromSpecifiedContext);); (31, OpenCL, clReleaseEvent_StubWithCallback(clReleaseEvent_testCopyHostToBuffer);); (41, OpenCL, clReleaseContext_ExpectAndReturn(make_context(0), CL_SUCCESS);); (26, OpenCL, clEnqueueMapBuffer_StubWithCallback(clEnqueueMapBuffer_testCopyHostToBuffer);); (26, OpenCL, clEnqueueUnmapMemObject_StubWithCallback(clEnqueueUnmapMemObject_testCopyHostToBuffer);)"}
{"predict": "(33, MPI, MPI_Cart_shift(cart_comm, 0, 1, &source, &destination);); (52, MPI, t1 = MPI_Wtime();); (12, MPI, MPI_Init(&argc, &argv);); (57, MPI, MPI_Finalize();); (12, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (27, MPI, MPI_Cart_create(MPI_COMM_WORLD, ndims, dims, periods, 1, &cart_comm);); (12, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (52, MPI, MPI_Comm_free(&cart_comm);); (26, MPI, MPI_Comm_rank(cart_comm, &cart_id);); (29, MPI, MPI_Barrier(MPI_COMM_WORLD);); (29, MPI, t0 = MPI_Wtime();)", "label": "(33, MPI, MPI_Cart_shift(cart_comm, 0, 1, &source, &destination);); (52, MPI, t1 = MPI_Wtime();); (12, MPI, MPI_Init(&argc, &argv);); (57, MPI, MPI_Finalize();); (11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (27, MPI, MPI_Cart_create(MPI_COMM_WORLD, ndims, dims, periods, 1, &cart_comm);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (52, MPI, MPI_Comm_free(&cart_comm);); (27, MPI, MPI_Comm_rank(cart_comm, &cart_id);); (29, MPI, MPI_Barrier(MPI_COMM_WORLD);); (29, MPI, t0 = MPI_Wtime();)"}
{"predict": "(8, MPI, MPI_Comm_rank(mpi_comm, &rank);); (6, MPI, MPI_Comm mpi_comm = MPI_Comm_f2c(vtkh::GetMPICommHandle());); (56, MPI, MPI_Bcast(&entropy, 1, MPI_DOUBLE, 0, mpi_comm);)", "label": "(8, MPI, MPI_Comm_rank(mpi_comm, &rank);); (6, MPI, MPI_Comm mpi_comm = MPI_Comm_f2c(vtkh::GetMPICommHandle());); (56, MPI, MPI_Bcast(&entropy, 1, MPI_DOUBLE, 0, mpi_comm);)"}
{"predict": "(68, CUDA, cudaCheck(cudaMemset(d_dinp, 0, B * T * 3 * C * sizeof(float)));); (60, CUDA, cudaCheck(cudaMalloc(&d_dpreatt, B * NH * T * T * sizeof(float)));); (157, CUDA, cudaCheck(cudaFree(d_inp));); (69, CUDA, cudaCheck(cudaMemcpy(h_dinp, d_dinp, B * T * 3 * C * sizeof(float), cudaMemcpyDeviceToHost));); (157, CUDA, cudaCheck(cudaFree(d_qkvr));); (158, CUDA, cudaCheck(cudaFree(d_preatt));); (59, CUDA, cudaCheck(cudaMemset(d_dqkvr, 0, B * T * 3 * C * sizeof(float)));); (150, CUDA, cudaCheck(cudaFree(d_att));); (59, CUDA, cudaCheck(cudaMemset(d_datt, 0, B * NH * T * T * sizeof(float)));); (150, CUDA, cudaCheck(cudaFree(d_vaccum));); (55, CUDA, cudaCheck(cudaMemcpy(d_dout, dout, B * T * C * sizeof(float), cudaMemcpyHostToDevice));); (150, CUDA, cudaCheck(cudaFree(d_out));); (54, CUDA, cudaCheck(cudaMemcpy(d_dinp, dinp, B * T * 3 * C * sizeof(float), cudaMemcpyHostToDevice));); (148, CUDA, cudaCheck(cudaFree(d_qkvr));); (54, CUDA, cudaCheck(cudaMemcpy(d_dqkvr, qkvr, B * T * 3 * C * sizeof(float), cudaMemcpyHostToDevice));); (49, CUDA, cudaCheck(cudaMalloc(&d_dvaccum, B * NH * T * T * sizeof(float)));); (49, CUDA, cudaCheck(cudaMalloc(&d_dout, B * T * C * sizeof(float)));); (144, CUDA, cudaCheck(cudaFree(d_preatt));); (149, CUDA, cudaCheck(cudaFree(d_dinp));); (50, CUDA, cudaCheck(cudaMemcpy(d_datt, datt, B * NH * T * T * sizeof(float), cudaMemcpyHostToDevice));); (39, CUDA, cudaCheck(cudaMalloc(&d_dqkvr, B * T * 3 * C * sizeof(float)));); (38, CUDA, cudaCheck(cudaMalloc(&d_dinp, B * T * 3 * C", "label": "(68, CUDA, cudaCheck(cudaMemset(d_dinp, 0, B * T * 3 * C * sizeof(float)));); (60, CUDA, cudaCheck(cudaMalloc(&d_dpreatt, B * NH * T * T * sizeof(float)));); (157, CUDA, cudaCheck(cudaFree(d_inp));); (69, CUDA, cudaCheck(cudaMemset(d_dvaccum, 0, B * T * C * sizeof(float)));); (86, CUDA, cudaCheck(cudaMemcpy(h_dinp, d_dinp, B * T * 3 * C * sizeof(float), cudaMemcpyDeviceToHost));); (162, CUDA, cudaCheck(cudaFree(d_dpreatt));); (159, CUDA, cudaCheck(cudaFree(d_dinp));); (159, CUDA, cudaCheck(cudaFree(d_dqkvr));); (152, CUDA, cudaCheck(cudaFree(d_qkvr));); (23, CUDA, cudaCheck(cudaMalloc(&d_att, B * NH * T * T * sizeof(float)));); (158, CUDA, cudaCheck(cudaFree(d_datt));); (58, CUDA, cudaCheck(cudaMemset(d_dqkvr, 0, B * T * 3 * C * sizeof(float)));); (149, CUDA, cudaCheck(cudaFree(d_preatt));); (50, CUDA, cudaCheck(cudaMalloc(&d_dvaccum, B * T * C * sizeof(float)));); (50, CUDA, cudaCheck(cudaMalloc(&d_dout, B * T * C * sizeof(float)));); (15, CUDA, cudaCheck(cudaMalloc(&d_qkvr, B * T * 3 * C * sizeof(float)));); (17, CUDA, cudaCheck(cudaMalloc(&d_vaccum, B * T * C * sizeof(float)));); (14, CUDA, cudaCheck(cudaMalloc(&d_preatt, B * NH * T * T * sizeof(float)));); (48, CUDA, cudaCheck(cudaMemcpy(d_dout, dout, B * T * C * sizeof(float), cudaMemcpyHostToDevice));); (144, CUDA, cudaCheck(cudaFree(d_vaccum));); (40, CUDA, cudaCheck(cudaMalloc(&d_dqkvr, B * T * 3 * C * sizeof(float)));); (38, CUDA, cudaCheck(cudaMalloc(&d_dinp, B * T * 3 * C * sizeof(float)));); (49, CUDA, cudaCheck(cudaMemset(d_datt, 0, B * NH * T * T * sizeof(float)));); (141, CUDA, cudaCheck(cudaFree(d_out));); (145, CUDA, cudaCheck(cudaFree(d_dvaccum));); (9, CUDA, cudaCheck(cudaMalloc(&d_out, B * T * C * sizeof(float)));); (36, CUDA, cudaCheck(cudaMalloc(&d_datt, B * NH * T * T * sizeof(float)));); (2, CUDA, cudaCheck(cudaMalloc(&d_inp, B * T * 3 * C * sizeof(float)));); (134, CUDA, cudaCheck(cudaFree(d_att));); (41, CUDA, cudaCheck(cudaMemset(d_dpreatt, 0, B * NH * T * T * sizeof(float)));); (6, CUDA, cudaCheck(cudaMemcpy(d_inp, inp, B * T * 3 * C * sizeof(float), cudaMemcpyHostToDevice));); (139, CUDA, cudaCheck(cudaFree(d_dout));)"}
{"predict": "(9, CUDA, CHECK(cudaMallocManaged((void **)&z, M));); (23, CUDA, CHECK(cudaFree(x));); (5, CUDA, CHECK(cudaMallocManaged((void **)&x, M));); (22, CUDA, CHECK(cudaFree(y));); (22, CUDA, CHECK(cudaFree(z));); (16, CUDA, CHECK(cudaDeviceSynchronize());); (2, CUDA, CHECK(cudaMallocManaged((void **)&y, M));)", "label": "(9, CUDA, CHECK(cudaMallocManaged((void **)&z, M));); (23, CUDA, CHECK(cudaFree(x));); (5, CUDA, CHECK(cudaMallocManaged((void **)&x, M));); (22, CUDA, CHECK(cudaFree(y));); (22, CUDA, CHECK(cudaFree(z));); (16, CUDA, CHECK(cudaDeviceSynchronize());); (2, CUDA, CHECK(cudaMallocManaged((void **)&y, M));)"}
{"predict": "(26, MPI, MPI_Allreduce(&value, &tempvalue, 1, MPI_DOUBLE, MPI_MAX, grid->comm);); (50, MPI, MPI_Allreduce(rwork, temprwork, A->ncol, MPI_DOUBLE, MPI_SUM, grid->comm);)", "label": "(26, MPI, MPI_Allreduce(&value, &tempvalue, 1, MPI_DOUBLE, MPI_MAX, grid->comm);); (50, MPI, MPI_Allreduce(rwork, temprwork, A->ncol, MPI_DOUBLE, MPI_SUM, grid->comm);)"}
{"predict": "(17, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (26, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(17, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (26, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(25, OpenCL, clReleaseMemObject(scratchBuff);); (20, OpenCL, clReleaseMemObject(mobjDP_);); (15, OpenCL, clReleaseMemObject(mobjY_);); (10, OpenCL, clReleaseMemObject(mobjX_);)", "label": "(25, OpenCL, clReleaseMemObject(scratchBuff);); (20, OpenCL, clReleaseMemObject(mobjDP_);); (15, OpenCL, clReleaseMemObject(mobjY_);); (10, OpenCL, clReleaseMemObject(mobjX_);)"}
{"predict": "(5, TBB, oneapi::tbb::global_control c(oneapi::tbb::global_control::max_allowed_parallelism, nthreads);); (21, TBB, static oneapi::tbb::affinity_partitioner g_ap;)", "label": "(5, TBB, oneapi::tbb::global_control c(oneapi::tbb::global_control::max_allowed_parallelism, nthreads);); (21, TBB, static oneapi::tbb::affinity_partitioner g_ap;)"}
{"predict": "(16, SYCL, using namespace cl::sycl::access;); (4, SYCL, cl::sycl::queue queue;)", "label": "(16, SYCL, using namespace cl::sycl::access;); (4, SYCL, cl::sycl::queue queue;)"}
{"predict": "(34, OpenMP, omp_target_free(d_a, DefaultDeviceNum);); (15, OpenMP, d_a = (int *)omp_target_alloc(DataSize, DefaultDeviceNum);); (23, OpenMP, #pragma omp target update to(d_a)); (16, OpenMP, omp_target_associate_ptr(p_a, d_a, DataSize, 0, DefaultDeviceNum);); (25, OpenMP, omp_target_disassociate_ptr(p_a, DefaultDeviceNum);); (7, OpenMP, int DefaultDeviceNum = omp_get_default_device();); (15, OpenMP, omp_target_associate_ptr(d_a, p_a, DataSize, 0, DefaultDeviceNum);)", "label": "(33, OpenMP, omp_target_free(d_a, DefaultDeviceNum);); (15, OpenMP, d_a = (int *)omp_target_alloc(DataSize, DefaultDeviceNum);); (23, OpenMP, #pragma omp target update to(d_a)); (5, OpenMP, int DefaultDeviceNum = omp_get_default_device();); (16, OpenMP, omp_target_associate_ptr(p_a, d_a, DataSize, 0, DefaultDeviceNum);); (24, OpenMP, omp_target_disassociate_ptr(p_a, DefaultDeviceNum);)"}
{"predict": "(51, OpenCL, clReleaseEvent(ev[i]);); (25, OpenCL, clEnqueueNDRangeKernel(queue, kernel, 1, NULL, globals, locals, 2, &ev[0], &ev[2]);); (15, OpenCL, clEnqueueWriteBuffer(queue, buf[0], CL_FALSE, 0, BUFFERSIZE*sizeof(int), (void *)cpu_src, 0, NULL, &ev[0]);); (29, OpenCL, buf_data[0] = clEnqueueMapBuffer(queue, buf[0], CL_FALSE, 0, 0, BUFFERSIZE*sizeof(int), (void *)cpu_src, 0, NULL, &ev[2]);)", "label": "(51, OpenCL, clReleaseEvent(ev[i]);); (24, OpenCL, clEnqueueNDRangeKernel(queue, kernel, 1, NULL, globals, locals, 2, &ev[0], &ev[2]);); (30, OpenCL, buf_data[0] = clEnqueueMapBuffer(queue, buf[0], CL_FALSE, 0, 0, BUFFERSIZE*sizeof(int), 1, &ev[2], NULL, NULL);); (14, OpenCL, clEnqueueWriteBuffer(queue, buf[0], CL_FALSE, 0, BUFFERSIZE*sizeof(int), (void *)cpu_src, 1, &ev[0], &ev[1]);)"}
{"predict": "(103, MPI, MPI_Allreduce(  MPI_IN_PLACE, myusize, 1, MPI_INT, MPI_MAX, grid->comm );); (101, MPI, MPI_Allreduce( MPI_IN_PLACE, mylsize, 1, MPI_INT, MPI_MAX, grid->comm );); (50, MPI, MPI_Bcast( &(size), 1, mpi_int_t, gridid,  grid3d->zscp.comm);); (105, MPI, MPI_Allreduce( MPI_IN_PLACE, &(treeList[0].scuWeight), 1, MPI_DOUBLE, MPI_MAX, grid->comm );); (49, MPI, MPI_Allreduce(MPI_IN_PLACE, &(flag), 1, mpi_int_t, MPI_MAX, grid3d->zscp.comm);); (50, MPI, MPI_Bcast( &(flag), 1, mpi_int_t, gridid,  grid3d->zscp.comm);); (50, MPI, MPI_Bcast( &(index), size, mpi_int_t, gridid,  grid3d->zscp.comm);); (69, MPI, MPI_Allreduce(MPI_IN_PLACE, index, 1, MPI_INT, MPI_MAX, grid3d->zscp.comm);)", "label": "(50, MPI, MPI_Allreduce(MPI_IN_PLACE, &(flag), 1, mpi_int_t, MPI_MAX, grid3d->zscp.comm);); (105, MPI, MPI_Allreduce(  MPI_IN_PLACE, myusize, nsupers, mpi_int_t, MPI_MAX, grid->comm );); (102, MPI, MPI_Allreduce( MPI_IN_PLACE, mylsize, nsupers, mpi_int_t, MPI_MAX, grid->comm );); (50, MPI, MPI_Bcast( &(size), 1, mpi_int_t, gridid,  grid3d->zscp.comm);); (48, MPI, MPI_Allreduce(MPI_IN_PLACE, &(gridid), 1, mpi_int_t, MPI_MAX, grid3d->zscp.comm);)"}
{"predict": "(15, CUDA, checkCudaErrors(cudaMalloc((void **)&ringbuf, sizeof(qsortRingbuf)));); (40, CUDA, checkCudaErrors(cudaEventRecord(ev2));); (40, CUDA, checkCudaErrors(cudaEventSynchronize(ev2));); (62, CUDA, checkCudaErrors(cudaFree(ringbuf));); (38, CUDA, checkCudaErrors(cudaEventElapsedTime(&elapse, ev1, ev2));); (62, CUDA, checkCudaErrors(cudaFree(gpustack));); (22, CUDA, checkCudaErrors(cudaEventRecord(ev1));); (19, CUDA, checkCudaErrors(cudaEventCreate(&ev1));); (19, CUDA, checkCudaErrors(cudaEventCreate(&ev2));); (0, CUDA, checkCudaErrors(cudaMemsetAsync(gpustack, 0, stacksize * sizeof(qsortAtomicData), stream));)", "label": "(15, CUDA, checkCudaErrors(cudaMalloc((void **)&ringbuf, sizeof(qsortRingbuf)));); (28, CUDA, checkCudaErrors(cudaEventRecord(ev1));); (40, CUDA, checkCudaErrors(cudaEventRecord(ev2));); (38, CUDA, checkCudaErrors(cudaGetLastError());); (61, CUDA, checkCudaErrors(cudaFree(ringbuf));); (38, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (21, CUDA, checkCudaErrors(cudaEventCreate(&ev1));); (21, CUDA, checkCudaErrors(cudaEventCreate(&ev2));); (58, CUDA, checkCudaErrors(cudaFree(gpustack));)"}
{"predict": "(21, MPI, MPI_Comm_create_group(MPI_COMM_WORLD, newgroup, 0, &newcomm);); (21, MPI, MPI_Group_excl(world, 1, deadprocs, &newgroup);); (7, MPI, MPI_Init(&argc, &argv);); (82, MPI, MPI_Group_free(&newgroup);); (84, MPI, MPI_Finalize();); (54, MPI, rc = MPI_Scatter(sendbuf, 100000, MPI_CHAR, rbuf, 100000, MPI_CHAR, 0, MPI_COMM_WORLD);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (54, MPI, MPI_Error_class(rc, &errclass);); (25, MPI, rc = MPI_Scatter(sendbuf, 10, MPI_CHAR, rbuf, 10, MPI_CHAR, 0, MPI_COMM_WORLD);); (77, MPI, MPI_Comm_free(&newcomm);); (3, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (25, MPI, MPI_Error_class(rc, &errclass);); (5, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (18, MPI, MPI_Comm_group(MPI_COMM_WORLD, &world);)", "label": "(21, MPI, MPI_Comm_create_group(MPI_COMM_WORLD, newgroup, 0, &newcomm);); (19, MPI, MPI_Group_excl(world, 1, deadprocs, &newgroup);); (7, MPI, MPI_Init(&argc, &argv);); (82, MPI, MPI_Group_free(&newgroup);); (84, MPI, MPI_Finalize();); (53, MPI, rc = MPI_Scatter(sendbuf, 100000, MPI_CHAR, rbuf, 100000, MPI_CHAR, 0, MPI_COMM_WORLD);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (76, MPI, MPI_Comm_free(&newcomm);); (3, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (76, MPI, MPI_Group_free(&world);); (25, MPI, rc = MPI_Scatter(sendbuf, 10, MPI_CHAR, rbuf, 10, MPI_CHAR, 0, MPI_COMM_WORLD);); (4, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (26, MPI, MPI_Error_class(rc, &errclass);); (5, MPI, MPI_Comm_group(MPI_COMM_WORLD, &world);)"}
{"predict": "(8, OpenMP, omp_set_nest_lock(&lock);); (10, OpenMP, omp_unset_nest_lock(&lock);); (4, OpenMP, #pragma omp parallel num_threads(8) shared(var)); (1, OpenMP, omp_init_nest_lock(&lock);); (13, OpenMP, omp_destroy_nest_lock(&lock);)", "label": "(6, OpenMP, #pragma omp parallel num_threads(8) shared(var)); (7, OpenMP, omp_set_nest_lock(&lock);); (9, OpenMP, omp_unset_nest_lock(&lock);); (1, OpenMP, omp_init_nest_lock(&lock);); (13, OpenMP, omp_destroy_nest_lock(&lock);)"}
{"predict": "(8, CUDA, sim->CUDAConfig().device_id = device;)", "label": "(8, CUDA, sim->CUDAConfig().device_id = device;)"}
{"predict": "(203, OpenMP, #pragma omp parallel); (206, OpenMP, #pragma omp for schedule(dynamic,2) nowait)", "label": "(203, OpenMP, #pragma omp parallel); (206, OpenMP, #pragma omp for schedule(dynamic,2) nowait)"}
{"predict": "(19, OpenMP, #pragma omp parallel for)", "label": "(19, OpenMP, #pragma omp parallel for)"}
{"predict": "(25, OpenCL, test_clEnqueueCopyBuffer(&clEnqueueCopyBufferData[i]);); (20, OpenCL, test_clEnqueueFillBuffer(&clEnqueueFillBufferData[i]);); (7, OpenCL, test_clEnqueueWriteBuffer(&clEnqueueWriteBufferData[i]);); (9, OpenCL, test_clEnqueueReadBufferRect(&clEnqueueReadBufferRectData[i]);); (12, OpenCL, test_clEnqueueWriteBufferRect(&clEnqueueWriteBufferRectData[i]);); (39, OpenCL, test_clEnqueueUnmapMemObject(&clEnqueueUnmapMemObjectData[i]);); (-2, OpenCL, test_clEnqueueReadBuffer(&clEnqueueReadBufferData[i]);); (25, OpenCL, test_clEnqueueMapBuffer(&clEnqueueMapBufferData[i]);)", "label": "(24, OpenCL, test_clEnqueueCopyBuffer(&clEnqueueCopyBufferData[i]);); (27, OpenCL, test_clEnqueueCopyBufferRect(&clEnqueueCopyBufferRectData[i]);); (6, OpenCL, test_clEnqueueWriteBuffer(&clEnqueueWriteBufferData[i]);); (41, OpenCL, test_clEnqueueUnmapMemObject(&clEnqueueUnmapMemObjectData[i]);); (8, OpenCL, test_clEnqueueReadBufferRect(&clEnqueueReadBufferRectData[i]);); (-1, OpenCL, test_clEnqueueReadBuffer(&clEnqueueReadBufferData[i]);); (14, OpenCL, test_clEnqueueFillBuffer(&clEnqueueFillBufferData[i]);); (25, OpenCL, test_clEnqueueMapBuffer(&clEnqueueMapBufferData[i]);); (8, OpenCL, test_clEnqueueWriteBufferRect(&clEnqueueWriteBufferRectData[i]);)"}
{"predict": "(7, OpenMP, #pragma omp parallel sections reduction(task,*:f)); (14, OpenMP, #pragma omp parallel sections default(none), private(argc, b) firstprivate(argv) if (argc > 0) num_threads(ee) copyin(a) proc_bind(spread) reduction(| : c, d) reduction(* : e) lastprivate(argv)); (16, OpenMP, #pragma omp section)", "label": "(7, OpenMP, #pragma omp parallel sections reduction(task,*:f)); (14, OpenMP, #pragma omp parallel sections default(none), private(argc, b) firstprivate(argv) if (argc > 0) num_threads(ee) copyin(a) proc_bind(spread) reduction(| : c, d) reduction(* : e) lastprivate(argv)); (2, OpenMP, #pragma omp threadprivate(a)); (16, OpenMP, #pragma omp section)"}
{"predict": "(15, OpenMP, #pragma omp target teams distribute parallel for collapse(2) depend(out:image[y_start]) nowait); (6, OpenMP, double st = omp_get_wtime();); (28, OpenMP, double et = omp_get_wtime();); (25, OpenMP, #pragma omp taskwait); (6, OpenMP, #pragma omp target data map(alloc:image[0:width*height])); (20, OpenMP, #pragma omp target update from(image[block*block_size:block_size]))", "label": "(15, OpenMP, #pragma omp target teams distribute parallel for collapse(2) depend(out:image[y_start]) nowait); (6, OpenMP, double st = omp_get_wtime();); (28, OpenMP, double et = omp_get_wtime();); (25, OpenMP, #pragma omp taskwait); (5, OpenMP, #pragma omp target data map(alloc:image[0:width*height])); (18, OpenMP, #pragma omp target update from(image[block*block_size:block_size]))"}
{"predict": "(36, MPI, MPI_Type_size(c_info->s_data_type, &s_size);); (36, MPI, MPI_Type_size(c_info->r_data_type, &r_size);)", "label": "(36, MPI, MPI_Type_size(c_info->s_data_type, &s_size);); (36, MPI, MPI_Type_size(c_info->r_data_type, &r_size);)"}
{"predict": "(13, CUDA, PyObject* shape = PyCudaImage_GetShape(self, closure);)", "label": "(13, CUDA, PyObject* shape = PyCudaImage_GetShape(self, closure);)"}
{"predict": "(32, MPI, MPI_Type_commit(&newtype);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (42, MPI, MPI_Send(buf, 1, newtype, 1, i, MPI_COMM_WORLD);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (66, MPI, MPI_Type_free(&newtype);); (24, MPI, MPI_Type_vector(i, 128, 129, MPI_CHAR, &newtype);); (69, MPI, MPI_Recv(&recv_errs, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (76, MPI, MPI_Send(&errs, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);); (38, MPI, MPI_Recv(buf, 1, newtype, 0, i, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)", "label": "(31, MPI, MPI_Type_commit(&newtype);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (42, MPI, MPI_Send(buf, 1, newtype, 1, i, MPI_COMM_WORLD);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (66, MPI, MPI_Type_free(&newtype);); (24, MPI, MPI_Type_vector(i, 128, 129, MPI_CHAR, &newtype);); (70, MPI, MPI_Recv(&recv_errs, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (76, MPI, MPI_Send(&errs, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);); (39, MPI, MPI_Recv(buf, 1, newtype, 0, i, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)"}
{"predict": "(106, MPI, MPI_Allgather(st_end_offset, 2, ADIO_OFFSET, all_st_end_offsets, 2, ADIO_OFFSET, fd->comm);); (71, MPI, MPI_Comm_rank(fd->comm, &myrank);); (527, MPI, MPI_Type_free(&client_comm_dtype_arr[i]);); (587, MPI, MPI_Type_free(&fd->file_realm_types[0]);); (67, MPI, MPI_Comm_size(fd->comm, &nprocs);); (589, MPI, MPI_Type_free(&fd->file_realm_types[i]);); (155, MPI, MPI_Type_get_extent(datatype, &lb, &extent);); (307, MPI, MPI_Type_size_x(agg_comm_dtype_arr[i], &size);); (301, MPI, MPI_Type_free(&agg_dtype);)", "label": "(106, MPI, MPI_Allgather(st_end_offset, 2, ADIO_OFFSET, all_st_end_offsets, 2, ADIO_OFFSET, fd->comm);); (71, MPI, MPI_Comm_rank(fd->comm, &myrank);); (527, MPI, MPI_Type_free(&client_comm_dtype_arr[i]);); (587, MPI, MPI_Type_free(&fd->file_realm_types[0]);); (67, MPI, MPI_Comm_size(fd->comm, &nprocs);); (589, MPI, MPI_Type_free(&fd->file_realm_types[i]);); (155, MPI, MPI_Type_get_extent(datatype, &lb, &extent);); (307, MPI, MPI_Type_size_x(agg_comm_dtype_arr[i], &size);); (301, MPI, MPI_Type_free(&agg_dtype);)"}
{"predict": "(99, MPI, MPI_Alltoall(sendcounts, 1, MPI_INT, recvcounts, 1, MPI_INT, MPI_COMM_WORLD);); (46, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (40, MPI, MPI_Type_free(&keytype);)", "label": "(99, MPI, MPI_Alltoall(sendcounts, 1, MPI_INT, recvcounts, 1, MPI_INT, MPI_COMM_WORLD);); (46, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (40, MPI, MPI_Type_free(&keytype);)"}
{"predict": "(13, OpenMP, #pragma omp parallel default(none) shared(data,x,y,z,repeats))", "label": "(13, OpenMP, #pragma omp parallel default(none) shared(data,x,y,z,repeats))"}
{"predict": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(207, MPI, MPI_Finalize();); (79, MPI, MPI_Bcast(&btype,         1, MPI_INTEGER, root, MPI_COMM_WORLD);); (240, MPI, MPI_Reduce(&total, &total_sum, 1, MPI_INTEGER, MPI_SUM, root, MPI_COMM_WORLD);); (22, MPI, MPI_Init(&argc,&argv);); (93, MPI, MPI_Barrier(MPI_COMM_WORLD);); (77, MPI, MPI_Bcast(&iterations,    1, MPI_INTEGER, root, MPI_COMM_WORLD);); (77, MPI, MPI_Bcast(&vector_length, 1, MPI_INTEGER, root, MPI_COMM_WORLD);); (78, MPI, MPI_Bcast(&my_ID,          1, MPI_INTEGER, root, MPI_COMM_WORLD);); (94, MPI, MPI timingEventRecord(0);)", "label": "(207, MPI, MPI_Reduce(&total, &total_sum, 1, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD);); (230, MPI, MPI_Finalize();); (79, MPI, MPI_Bcast(&iterations,    1, MPI_INT, root, MPI_COMM_WORLD);); (21, MPI, MPI_Init(&argc,&argv);); (76, MPI, MPI_Bcast(&vector_length, 1, MPI_INT, root, MPI_COMM_WORLD);); (77, MPI, MPI_Bcast(&btype,         1, MPI_INT, root, MPI_COMM_WORLD);); (93, MPI, MPI_Barrier(MPI_COMM_WORLD);); (19, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (17, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(36, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (38, CUDA, CUDA_CHECK(cudaFree(d_data));)", "label": "(36, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (38, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(48, MPI, MPI_Buffer_detach(&bptr, &bl);); (28, MPI, MPI_Recv(b, 10, MPI_INT, i, 27 + j, MPI_COMM_WORLD, &status);); (15, MPI, MPI_Bsend(a, 10, MPI_INT, 0, 27 + j, MPI_COMM_WORLD);); (6, MPI, MPI_Buffer_attach(buf, BUFSIZE);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (11, MPI, MPI_Bsend(b, 10, MPI_INT, 0, 27 + j, MPI_COMM_WORLD);); (14, MPI, MPI_Buffer_attach(b, BUFSIZE);)", "label": "(44, MPI, MPI_Buffer_detach(&bptr, &bl);); (24, MPI, MPI_Recv(b, 10, MPI_INT, i, 27 + j, MPI_COMM_WORLD, &status);); (14, MPI, MPI_Bsend(a, 10, MPI_INT, 0, 27 + j, MPI_COMM_WORLD);); (7, MPI, MPI_Buffer_attach(buf, BUFSIZE);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)"}
{"predict": "(2, CUDA, __shared__ int sharedMemory [BLOCK_SIZE] [BLOCK_SIZE + 1];)", "label": "(2, CUDA, __shared__ int sharedMemory [BLOCK_SIZE] [BLOCK_SIZE + 1];)"}
{"predict": "(57, MPI, MPI_Send(sendBuffer, n_send, MPI_DOUBLE, neighbors[i], MPI_MY_TAG, MPI_COMM_WORLD);); (15, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (36, MPI, MPI_Irecv(x_external, n_recv, MPI_DOUBLE, neighbors[i], MPI_MY_TAG, MPI_COMM_WORLD, request+i);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(57, MPI, MPI_Send(sendBuffer, n_send, MPI_DOUBLE, neighbors[i], MPI_MY_TAG, MPI_COMM_WORLD);); (15, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (36, MPI, MPI_Irecv(x_external, n_recv, MPI_DOUBLE, neighbors[i], MPI_MY_TAG, MPI_COMM_WORLD, request+i);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(5, OpenMP, #pragma omp parallel num_threads(4)); (8, OpenMP, #pragma omp for schedule(static) nowait); (19, OpenMP, #pragma omp for schedule(static) release_and_increment); (8, OpenMP, #pragma omp master); (9, OpenMP, #pragma omp for schedule(static) nowait release); (1, OpenMP, omp_set_dynamic(0);); (7, OpenMP, #pragma omp for schedule(static) nowait); (17, OpenMP, #pragma omp atomic)", "label": "(5, OpenMP, #pragma omp parallel num_threads(4)); (11, OpenMP, #pragma omp for schedule(static) nowait); (8, OpenMP, rank = omp_get_thread_num();); (16, OpenMP, #pragma omp atomic); (20, OpenMP, #pragma omp for schedule(static))"}
{"predict": "(4, OpenMP, #pragma omp parallel for)", "label": "(4, OpenMP, #pragma omp parallel for)"}
{"predict": "(17, OpenMP, #pragma omp for order(concurrent)); (0, OpenMP, #pragma omp for order); (14, OpenMP, #pragma omp for order(concurrent); (9, OpenMP, #pragma omp for order(none); (4, OpenMP, #pragma omp for order(); (1, OpenMP, #pragma omp for order()); (5, OpenMP, #pragma omp for order(concurrent)", "label": "(1, OpenMP, #pragma omp parallel for ordered ordered); (15, OpenMP, #pragma omp parallel for order(concurrent)); (8, OpenMP, #pragma omp parallel for order(none); (4, OpenMP, #pragma omp parallel for order(); (0, OpenMP, #pragma omp parallel for order); (8, OpenMP, #pragma omp parallel for order(concurrent)"}
{"predict": "(14, OpenMP, printf(\"Thread %d of %d\\n\", omp_get_thread_num(), omp_get_num_threads());)", "label": "(14, OpenMP, printf(\"Thread %d of %d\\n\", omp_get_thread_num(), omp_get_num_threads());)"}
{"predict": "(43, CUDA, CUDA_CHECK_GOTO(cudaGetDevice(&self->device), free_mpool, status);); (15, CUDA, cuda_st = cudaGetDeviceCount(&num_devices);)", "label": "(43, CUDA, CUDA_CHECK_GOTO(cudaGetDevice(&self->device), free_mpool, status);); (15, CUDA, cuda_st = cudaGetDeviceCount(&num_devices);)"}
{"predict": "(25, CUDA, assert(cudaMemcpyAsync(out.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);); (11, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (9, CUDA, cudaMalloc((void**)(&d_in1), in1_bytes);); (11, CUDA, cudaMemcpy(d_in1, in1.data(), in1_bytes, cudaMemcpyHostToDevice);); (39, CUDA, cudaFree(d_out);); (20, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (36, CUDA, cudaFree(d_in1);)", "label": "(25, CUDA, assert(cudaMemcpyAsync(out.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);); (11, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (9, CUDA, cudaMalloc((void**)(&d_in1), in1_bytes);); (11, CUDA, cudaMemcpy(d_in1, in1.data(), in1_bytes, cudaMemcpyHostToDevice);); (39, CUDA, cudaFree(d_out);); (21, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (36, CUDA, cudaFree(d_in1);)"}
{"predict": "(14, OpenMP, #pragma omp teams default(none), private(argc,b) num_teams(f) firstprivate(argv) reduction(| : c, d) reduction(* : e) thread_limit(f+g)); (6, OpenMP, #pragma omp target); (1, OpenMP, #pragma omp threadprivate(a)); (6, OpenMP, #pragma omp teams)", "label": "(14, OpenMP, #pragma omp teams default(none), private(argc,b) num_teams(f) firstprivate(argv) reduction(| : c, d) reduction(* : e) thread_limit(f+g)); (6, OpenMP, #pragma omp target); (2, OpenMP, #pragma omp threadprivate(a)); (5, OpenMP, #pragma omp teams)"}
{"predict": "(341, CUDA, __shared__ int cur_send_num;); (339, CUDA, __shared__ int recv_num, finish_num;)", "label": "(341, CUDA, __shared__ int cur_send_num;); (339, CUDA, __shared__ int recv_num, finish_num;)"}
{"predict": "(4, TBB, tbb::flow::queue_node<T> q2(g);); (70, TBB, tbb::flow::make_edge( q2, q3 );); (67, TBB, tbb::flow::queue_node<T> q3(g);); (37, TBB, tbb::flow::make_edge( q, q2 );); (56, TBB, tbb::flow::remove_edge( q, q2 );); (87, TBB, tbb::flow::remove_edge( q,  q2 );); (87, TBB, tbb::flow::remove_edge( q,  q3 );); (51, TBB, tbb::flow::make_edge( q,  q2 );); (0, TBB, tbb::flow::queue_node<T> q(g);); (37, TBB, tbb::flow::remove_edge( q, q2 );); (60, TBB, tbb::flow::remove_edge( q,  q2 );); (-8, TBB, tbb::flow::graph g;)", "label": "(4, TBB, tbb::flow::queue_node<T> q2(g);); (70, TBB, tbb::flow::make_edge( q2, q3 );); (67, TBB, tbb::flow::queue_node<T> q3(g);); (37, TBB, tbb::flow::make_edge( q, q2 );); (56, TBB, tbb::flow::remove_edge( q, q2 );); (87, TBB, tbb::flow::remove_edge( q,  q2 );); (-3, TBB, tbb::flow::queue_node<T> q(g);); (-5, TBB, tbb::flow::graph g;)"}
{"predict": "(23, OpenCL, clReleaseEvent(event);)", "label": "(23, OpenCL, clReleaseEvent(event);)"}
{"predict": "(25, MPI, MPI_Comm_rank(comm, &mpi_rank);); (25, MPI, MPI_Comm_size(comm, &mpi_size);); (25, MPI, MPI_Info_create(&info);); (25, MPI, MPI_Info_set(info, \"romio_no_indep_rw\", \"true\");); (136, MPI, MPI_Barrier(comm);); (22, MPI, MPI_Info_free(&info);); (24, MPI, MPI_Info_set(info, \"nc_var_par_access\", \"true\");); (18, MPI, MPI_Init(&argc, &argv);); (23, MPI, MPI_Info_set(info, \"nc_var_par_access_type\", \"collective\");); (133, MPI, MPI_Finalize();); (22, MPI, MPI_Info_set(info, \"nc_var_par_access_subtype\", \"slab\");); (19, MPI, MPI_Info_set(info, \"nc_var_par_access_type\", \"independent\");)", "label": "(25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (25, MPI, MPI_Get_processor_name(mpi_name, &mpi_namelen);); (22, MPI, MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);)"}
{"predict": "(43, OpenCL, clReleaseMemObject(imageDesc.mem_object);); (14, OpenCL, imageDesc.mem_object = clCreateBuffer(&context, CL_MEM_READ_WRITE, imageDesc.image_height * imageDesc.image_width, nullptr, &retVal);)", "label": "(43, OpenCL, clReleaseMemObject(imageDesc.mem_object);); (14, OpenCL, imageDesc.mem_object = clCreateBuffer(&context, CL_MEM_READ_WRITE, imageDesc.image_height * imageDesc.image_width, nullptr, &retVal);)"}
{"predict": "(114, OpenMP, omp_target_free(y1, omp_get_default_device());); (41, OpenMP, #pragma omp teams distribute parallel for default(shared) collapse(3)); (39, OpenMP, #pragma omp target is_device_ptr(x1, y1) map(tofrom: or[0:m3k*m2k*m1k]) map(tofrom: os[0:m3j*m2j*m1j])); (13, OpenMP, y1 = (double*)omp_target_alloc(m3k*m2k*m1k*sizeof(double), omp_get_default_device());)", "label": "(114, OpenMP, omp_target_free(y1, omp_get_default_device());); (41, OpenMP, #pragma omp teams distribute parallel for default(shared) collapse(3)); (39, OpenMP, #pragma omp target is_device_ptr(x1, y1) map(tofrom: or[0:m3k*m2k*m1k]) map(tofrom: os[0:m3j*m2j*m1j])); (13, OpenMP, y1 = (double*)omp_target_alloc(m3k*m2k*m1k*sizeof(double), omp_get_default_device());)"}
{"predict": "(9, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)", "label": "(9, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(17, CUDA, extern __shared__ _VOLATILE_ float sMem[];)", "label": "(17, CUDA, extern __shared__ _VOLATILE_ float sMem[];)"}
{"predict": "(1, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)", "label": "(1, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)"}
