{"predict": "(17,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for)", "label": "(20, CUDA, CUDA_CHECK(cudaMalloc((void **)&dev_max, sizeof(FP) * SEQLEN * DIM));); (39, CUDA, cudaFree(dev_max);); (22, CUDA, CUDA_CHECK(cudaMemcpy(dev_denom, host_denom, sizeof(FP) * SEQLEN * DIM, cudaMemcpyHostToDevice));); (18, CUDA, CUDA_CHECK(cudaMalloc((void **)&dev_denom, sizeof(FP) * SEQLEN * DIM));); (18, CUDA, CUDA_CHECK(cudaMalloc((void **)&QK, sizeof(FP) * SEQLEN * SEQLEN));); (18, CUDA, CUDA_CHECK(cudaMemcpy(dev_max, host_max, sizeof(FP) * SEQLEN * DIM, cudaMemcpyHostToDevice));); (33, CUDA, cudaFree(QK);); (34, CUDA, cudaFree(dev_denom);)"}
{"predict": "(7,MPI,*w = MPI_Wtime();); (26,OpenMP,#pragma omp critical(out))", "label": "(32, OpenMP, #pragma omp critical(cerr)); (15, OpenMP, #pragma omp critical(out))"}
{"predict": "(20,OpenMP,#pragma omp parallel for)", "label": "(31, MPI, MPI_T_finalize();); (4, MPI, MPI_T_init_thread(required, &provided);); (6, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for reduction(+:totalSum)), (20,OpenMP,totalSum += data[i];)", "label": "(19, OpenMP, #pragma omp parallel for simd reduction(+:totalSum) schedule(guided) aligned(data: 64) num_threads(N_THREADS)); (10, OpenMP, #pragma omp parallel for num_threads(N_THREADS))"}
{"predict": "(9,OpenMP,#pragma omp parallel for private(bufptr, endptr, printfbuf_local))", "label": "(15, CUDA, cudaMemcpy(printfbuf_local, bufptr, CUPRINTF_MAX_LEN, cudaMemcpyDeviceToHost);)"}
{"predict": "(6,MPI,*w = MPI_Wtime(););\n(8,OpenMP,#pragma omp parallel for),", "label": "(26, OpenMP, #pragma omp parallel sections firstprivate(int)); (10, OpenMP, #pragma omp parallel sections firstprivate(,); (14, OpenMP, #pragma omp parallel sections firstprivate(, )); (34, OpenMP, #pragma omp parallel sections lastprivate(x) firstprivate(x)); (17, OpenMP, #pragma omp parallel sections firstprivate()); (36, OpenMP, #pragma omp parallel sections lastprivate(x, y) firstprivate(x, y)); (25, OpenMP, #pragma omp parallel sections firstprivate(0)); (-3, OpenMP, #pragma omp parallel sections firstprivate(); (37, OpenMP, #pragma omp parallel sections lastprivate(x, y, z) firstprivate(x, y, z))"}
{"predict": "(10,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);) (15,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for), (30,MPI,MPI_Waitall(1, &request, MPI_STATUSES_IGNORE);)", "label": "(29, MPI, MPI_Recv(NULL, 0, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &status);); (16, MPI, MPI_Isend(a, 0, MPI_INT, rank, 0, MPI_COMM_WORLD, &request);); (24, MPI, MPI_Wait(&request, &status);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (24, MPI, MPI_Isend(NULL, 0, MPI_INT, rank, 0, MPI_COMM_WORLD, &request);); (14, MPI, MPI_Get_count(&status, MPI_INT, &count);); (12, MPI, MPI_Recv(b, 1, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &status);)"}
{"predict": "(5,MPI,*w = MPI_Wtime(););", "label": "(14, OpenCL, retVal = clReleaseMemObject(image);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (66,OpenMP,#pragma omp parallel for collapse(2))", "label": "(60, OpenMP, #pragma omp parallel for)"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(17, CUDA, CHECK(cudaMalloc((void**) &anchorsDev, 4 * params.anchorsRatioCount * params.anchorsScaleCount * sizeof(float)));)"}
{"predict": "(14,OpenMP,#pragma omp parallel for)", "label": "(3, CUDA, extern __shared__ float entire_buffer[];)"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(24, CUDA, cudaFree(res_dev);); (17, CUDA, cudaMalloc(&res_dev, sizeof(float));); (19, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(20,OpenMP,#pragma omp parallel for)", "label": "(16, OpenCL, auto b1 = clCreateBuffer(context, CL_MEM_READ_WRITE, 20, nullptr, nullptr);); (24, OpenCL, void *ptr2 = clEnqueueMapBuffer(&cmdQ, b2, CL_TRUE, CL_MAP_READ, 0, 8, 0, nullptr, nullptr, nullptr);); (32, OpenCL, clReleaseMemObject(b2);); (17, OpenCL, void *ptr1 = clEnqueueMapBuffer(&cmdQ, b1, CL_FALSE, CL_MAP_READ, 0, 8, 1, &gatingEvent, nullptr, nullptr);); (29, OpenCL, clReleaseMemObject(b1);); (31, OpenCL, clReleaseEvent(gatingEvent);); (25, OpenCL, clEnqueueUnmapMemObject(pCmdQ, b2, ptr2, 0, nullptr, nullptr);); (12, OpenCL, auto gatingEvent = clCreateUserEvent(context, nullptr);); (13, OpenCL, clEnqueueUnmapMemObject(&cmdQ, b1, ptr1, 0, nullptr, nullptr);); (8, OpenCL, auto b2 = clCreateBuffer(context, CL_MEM_READ_WRITE, 20, nullptr, nullptr);)"}
{"predict": "(30,OpenMP,#pragma omp parallel for), (65,OpenMP,#pragma omp parallel for), (95,OpenMP,#pragma omp parallel for)", "label": "(53, CUDA, cudaEventSynchronize(stop);); (86, CUDA, cudaMemcpy(TransposeMatrix, gpuTransposeMatrix, NUM * sizeof(float), cudaMemcpyDeviceToHost);); (9, CUDA, cudaGetDeviceProperties(&devProp, 0);); (44, CUDA, cudaMemcpy(gpuMatrix, Matrix, NUM * sizeof(float), cudaMemcpyHostToDevice);); (114, CUDA, cudaFree(gpuMatrix);); (32, CUDA, cudaMalloc((void**)&gpuMatrix, NUM * sizeof(float));); (49, CUDA, cudaEventElapsedTime(&eventMs, start, stop);); (36, CUDA, cudaEventRecord(start, NULL);); (112, CUDA, cudaFree(gpuTransposeMatrix);); (42, CUDA, cudaEventRecord(stop, NULL);); (10, CUDA, cudaEventCreate(&stop);); (28, CUDA, cudaMalloc((void**)&gpuTransposeMatrix, NUM * sizeof(float));); (6, CUDA, cudaEventCreate(&start);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for), (42,OpenMP,#pragma omp parallel for)", "label": "(56, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (4, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (35, CUDA, ASSERT_EQ(cudaSuccess, cudaMemsetAsync(dstData->basePtr(), 0, dstBufSize * sizeof(T), stream));); (54, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamDestroy(stream));)"}
{"predict": "(38,MPI,int ranks = MPI_Comm_size(MPI_COMM_WORLD);)\n(43,MPI,int* rank_disps = (int*) MFU_MALLOC(ranks * sizeof(int));)\n(69,MPI,DTAR_rank_disps = rank_disps;)\n(76,MPI,CIRCLE_init(0, NULL, CIRCLE_SPLIT_EQUAL | CIRCLE_CREATE_GLOBAL | CIRCLE_TERM_TREE);)\n(87,MPI,CIRCLE_set_reduce_period(reduce_secs);)\n(95,MPI,CIRCLE_begin();)\n(96,MPI,CIRCLE_finalize();)\n(103,MPI,mfu_file_chunk_list_free(&data_chunks);)", "label": "(38, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (44, MPI, MPI_Allgather(&item_offset, 1, MPI_INT, rank_disps, 1, MPI_INT, MPI_COMM_WORLD);); (80, MPI, reduce_start = MPI_Wtime();)"}
{"predict": "(27,OpenMP,#pragma omp parallel for), (52,OpenMP,#pragma omp parallel for), (57,OpenMP,#pragma omp parallel for), (62,OpenMP,#pragma omp parallel for)", "label": "(58, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 1, NULL, threads, localThreads, 0, NULL, NULL );); (46, OpenCL, error = clSetKernelArg( kernel, 0, sizeof( streams[ 0 ] ), &streams[0] );); (60, OpenCL, error = clEnqueueReadBuffer( queue, streams[1], true, 0, sizeof( cl_double ) * TEST_SIZE, outData, 0, NULL, NULL );); (46, OpenCL, error = clSetKernelArg( kernel, 1, sizeof( streams[ 1 ] ), &streams[1] );)"}
{"predict": "(24,OpenMP,#pragma omp parallel for collapse(2)), (25,OpenMP,#pragma omp parallel for collapse(2)), (26,OpenMP,#pragma omp parallel for collapse(2)), (27,OpenMP,#pragma omp parallel for collapse(2)), (30,OpenMP,#pragma omp parallel for collapse(2)), (32,OpenMP,#pragma omp parallel for collapse(2))", "label": "(26, CUDA, cudaMemcpy(d_A, A, m * k * sizeof(float), cudaMemcpyHostToDevice);); (56, CUDA, cudaFree(d_A);); (31, CUDA, cudaMemcpy(C, d_C, m * n * sizeof(float), cudaMemcpyDeviceToHost);); (55, CUDA, cudaFree(d_B);); (55, CUDA, cudaFree(d_C);); (17, CUDA, cudaMalloc((void **)&d_B, k * n * sizeof(float));); (15, CUDA, cudaMalloc((void **)&d_A, m * k * sizeof(float));); (16, CUDA, cudaMalloc((void **)&d_C, m * n * sizeof(float));); (20, CUDA, cudaMemcpy(d_C, C, m * n * sizeof(float), cudaMemcpyHostToDevice);); (18, CUDA, cudaMemcpy(d_B, B, k * n * sizeof(float), cudaMemcpyHostToDevice);)"}
{"predict": ", attention_backward,\n70:             cublas_handle, d_dinp, d_dqkvr, d_dpreatt, d_datt, d_scratch, d_dout, d_qkvr, d_att, B, T, C, NH\n71:         );\n72:         printf(\"block_size %4d | time %.4f ms\\n\", block_size, elapsed_time);\n73:     }\n74: \n75:     // clean up\n76:     free(qkv);\n77:     free(out);\n78:     free(dout);\n79:     free(dqkv);\n80:     cublasCheck(cublasDestroy(cublas_handle));\n81:     cudaCheck(cudaFree(d_inp));\n82:     cudaCheck(cudaFree(d_qkvr));\n83:     cudaCheck(cudaFree(d_out));\n84:     cudaCheck(cudaFree(d_preatt));\n85:     cudaCheck(cudaFree(d_att));\n86:     cudaCheck(cudaFree(d_dout));\n87:     cudaCheck(cudaFree(d_dinp));\n88:     cudaCheck(cudaFree(d_dqkvr));\n89:     cudaCheck(cudaFree(d_dpreatt));\n90:     cudaCheck(cudaFree(d_datt));\n91:     cudaCheck(cudaFree(d_scratch));\n92:     return 0;\n93: }", "label": "(45, CUDA, cudaCheck(cudaMemset(d_dinp, 0, B * T * 3 * C * sizeof(float)));); (47, CUDA, cudaCheck(cudaMalloc(&d_dpreatt, B * NH * T * T * sizeof(float)));); (99, CUDA, cudaCheck(cudaFree(d_inp));); (38, CUDA, cudaCheck(cudaMemset(d_att, 0, B * NH * T * T * sizeof(float)));); (105, CUDA, cudaCheck(cudaFree(d_dpreatt));); (102, CUDA, cudaCheck(cudaFree(d_dinp));); (102, CUDA, cudaCheck(cudaFree(d_dqkvr));); (28, CUDA, cudaCheck(cudaMemset(d_qkvr, 0, B * T * 3 * C * sizeof(float)));); (31, CUDA, cudaCheck(cudaMemset(d_preatt, 0, B * NH * T * T * sizeof(float)));); (93, CUDA, cudaCheck(cudaFree(d_qkvr));); (30, CUDA, cudaCheck(cudaMalloc(&d_att, B * NH * T * T * sizeof(float)));); (99, CUDA, cudaCheck(cudaFree(d_datt));); (35, CUDA, cudaCheck(cudaMemset(d_dqkvr, 0, B * T * 3 * C * sizeof(float)));); (24, CUDA, cudaCheck(cudaMemset(d_out, 0, B * T * C * sizeof(float)));); (90, CUDA, cudaCheck(cudaFree(d_preatt));); (27, CUDA, cudaCheck(cudaMalloc(&d_dout, B * T * C * sizeof(float)));); (18, CUDA, cudaCheck(cudaMalloc(&d_qkvr, B * T * 3 * C * sizeof(float)));); (16, CUDA, cudaCheck(cudaMemcpy(d_inp, qkv, B * T * 3 * C * sizeof(float), cudaMemcpyHostToDevice));); (35, CUDA, cudaCheck(cudaMemset(d_scratch, 0, B * T * C * sizeof(float)));); (19, CUDA, cudaCheck(cudaMalloc(&d_preatt, B * NH * T * T * sizeof(float)));); (91, CUDA, cudaCheck(cudaFree(d_scratch));); (22, CUDA, cudaCheck(cudaMemcpy(d_dout, dout, B * T * C * sizeof(float), cudaMemcpyHostToDevice));); (81, CUDA, cudaCheck(cudaFree(d_out));); (23, CUDA, cudaCheck(cudaMalloc(&d_dqkvr, B * T * 3 * C * sizeof(float)));); (20, CUDA, cudaCheck(cudaMalloc(&d_dinp, B * T * 3 * C * sizeof(float)));); (26, CUDA, cudaCheck(cudaMemset(d_datt, 0, B * NH * T * T * sizeof(float)));); (26, CUDA, cudaCheck(cudaMalloc(&d_scratch, B * T * C * sizeof(float)));); (9, CUDA, cudaCheck(cudaMalloc(&d_out, B * T * C * sizeof(float)));); (22, CUDA, cudaCheck(cudaMalloc(&d_datt, B * NH * T * T * sizeof(float)));); (3, CUDA, cudaCheck(cudaMalloc(&d_inp, B * T * 3 * C * sizeof(float)));); (75, CUDA, cudaCheck(cudaFree(d_att));); (18, CUDA, cudaCheck(cudaMemset(d_dpreatt, 0, B * NH * T * T * sizeof(float)));); (74, CUDA, cudaCheck(cudaFree(d_dout));)"}
{"predict": "(12,OpenMP,#pragma omp parallel for private(i)), (15,OpenMP,#pragma omp parallel for private(i)), (20,OpenMP,#pragma omp parallel for private(k)), (27,OpenMP,#pragma omp parallel for private(i)), (32,OpenMP,#pragma omp parallel for private(k))", "label": "(48, MPI, MPI_Buffer_detach(&bptr, &bl);); (17, MPI, MPI_Start(&request);); (27, MPI, MPI_Recv(b, 10, MPI_INT, i, 27 + j, MPI_COMM_WORLD, &status);); (8, MPI, MPI_Buffer_attach(buf, BUFSIZE);); (15, MPI, MPI_Wait(&request, &status);); (15, MPI, MPI_Request_free(&request);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (6, MPI, MPI_Bsend_init(a, 10, MPI_INT, 0, 27 + j, MPI_COMM_WORLD, &request);)"}
{"predict": "(104,OpenMP,#pragma omp parallel for)", "label": "(31, MPI, auto err = MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided);); (119, MPI, MPI_Finalize();); (37, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (40, MPI, ascent_opt[\"mpi_comm\"] = MPI_Comm_c2f(MPI_COMM_WORLD);); (111, MPI, MPI_Reduce(&run_time, &max_run_time, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);); (33, MPI, MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);)"}
{"predict": "(19,OpenMP,#pragma omp parallel for), (44,OpenMP,#pragma omp parallel for)", "label": "(15, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for), (42,OpenMP,#pragma omp parallel for)", "label": "(47, TBB, double t2d_auto = pforTranspose2d<tbb::auto_partitioner>(N, a, b, gs);); (45, TBB, pforTranspose2d<tbb::auto_partitioner>(N, a, b, gs);); (4, TBB, c.set_core_type(tbb::info::core_types().back());); (1, TBB, tbb::task_arena::constraints c;); (48, TBB, double t2d_simple = pforTranspose2d<tbb::simple_partitioner>(N, a, b, gs);); (37, TBB, double t1d_simple = pforTranspose<tbb::simple_partitioner>(N, a, b, gs);); (-1, TBB, c.set_numa_id(tbb::info::numa_nodes()[0]);); (29, TBB, pforTranspose<tbb::auto_partitioner>(N, a, b, gs);); (29, TBB, double t1d_auto = pforTranspose<tbb::auto_partitioner>(N, a, b, gs);); (32, TBB, pforTranspose<tbb::simple_partitioner>(N, a, b, gs);); (-2, TBB, c.set_max_concurrency(std::min(8, tbb::info::default_concurrency(c)));); (-2, TBB, tbb::task_arena a(c);); (39, TBB, pforTranspose2d<tbb::simple_partitioner>(N, a, b, gs);)"}
{"predict": "(16,OpenMP,#pragma omp parallel for collapse(2)), (40,OpenMP,#pragma omp parallel for collapse(2)), (90,OpenMP,#pragma omp parallel for collapse(2)), (118,OpenMP,#pragma omp parallel for collapse(2))", "label": "(12, CUDA, __shared__ FP sV[Bc][dim];); (19, CUDA, __shared__ FP sMax[Br];); (9, CUDA, __shared__ FP sK[Bc][dim];); (12, CUDA, __shared__ FP sQK[Br][Bc];); (13, CUDA, __shared__ FP sSafeE[Br][Bc];); (9, CUDA, __shared__ FP sO[Br][dim];); (4, CUDA, __shared__ FP sQ[Br][dim];); (12, CUDA, __shared__ FP sDenom[Br];)"}
{"predict": "(40,OpenMP,#pragma omp parallel for)", "label": "(22, CUDA, CudaCheck(cudaSetDevice(device_id_));)"}
{"predict": "(11,OpenMP,#pragma omp parallel for)", "label": "(14, OpenCL, retVal = clEnqueueUnmapMemObject(cmdQ.get(), buffer.get(), mappedPtr, 0, nullptr, nullptr);); (6, OpenCL, void *mappedPtr = clEnqueueMapBuffer(cmdQ.get(), buffer.get(), CL_FALSE, CL_MAP_WRITE, offset, size, 0, nullptr, nullptr, nullptr);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (10,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for), (43,OpenMP,#pragma omp parallel for), (45,OpenMP,#pragma omp parallel for), (50,OpenMP,#pragma omp parallel for), (57,OpenMP,#pragma omp parallel for)", "label": "(69, OpenMP, #pragma omp parallel private(i)); (42, OpenMP, #pragma omp master taskloop lastprivate(a, b)); (49, OpenMP, #pragma omp master taskloop lastprivate(z, e, g)); (16, OpenMP, #pragma omp master taskloop lastprivate(argc); (2, OpenMP, #pragma omp parallel); (18, OpenMP, #pragma omp master taskloop lastprivate(argc,); (25, OpenMP, #pragma omp master taskloop lastprivate(argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (0, OpenMP, #pragma omp master taskloop lastprivate); (47, OpenMP, #pragma omp master taskloop lastprivate(h)); (18, OpenMP, #pragma omp master taskloop lastprivate(argc > 0 ? argv[1] : argv[2])); (64, OpenMP, #pragma omp master taskloop lastprivate(i)); (51, OpenMP, #pragma omp master taskloop allocate(omp_thread_mem_alloc: i) lastprivate(i)); (27, OpenMP, #pragma omp master taskloop lastprivate(S1)); (57, OpenMP, #pragma omp master taskloop lastprivate(j)); (-3, OpenMP, #pragma omp master taskloop lastprivate(); (32, OpenMP, #pragma omp master taskloop lastprivate(argv[1])); (-1, OpenMP, #pragma omp master taskloop lastprivate()); (18, OpenMP, #pragma omp master taskloop lastprivate(conditional: argc) lastprivate(conditional:); (50, OpenMP, #pragma omp parallel shared(i))"}
{"predict": "(11,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for), (44,OpenMP,#pragma omp parallel for), (55,OpenMP,#pragma omp parallel for)", "label": "(5, CUDA, cudaStream_t stream = at::cuda::getCurrentCUDAStream();)"}
{"predict": "(8,MPI,MPI_Isend(sbuf, size, MPI_DOUBLE, to, tagno, MPI_COMM_WORLD, &request); MPI_Irecv(rbuf, size, MPI_DOUBLE, from, tagno, MPI_COMM_WORLD, &request); MPI_Wait(&request, MPI_STATUS_IGNORE);)", "label": "(13, MPI, MPI_Recv(NULL, 0, MPI_BYTE, to, 100, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (11, MPI, MPI_Send(NULL, 0, MPI_BYTE, from, 100, MPI_COMM_WORLD);); (4, MPI, MPI_Irecv(rbuf, size, MPI_BYTE, from, tagno, MPI_COMM_WORLD, &request);); (18, MPI, MPI_Send(sbuf, size, MPI_BYTE, to, tagno, MPI_COMM_WORLD);); (20, MPI, MPI_Wait(&request, MPI_STATUS_IGNORE);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for)", "label": "(13, MPI, MPI_Abort(MPI_COMM_WORLD, MPIT_result);); (14, MPI, MPIT_result = MPI_T_pvar_session_create(session);); (8, MPI, MPIT_result = MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for schedule(MY_SCHEDULE))", "label": "(9, OpenMP, #pragma omp atomic); (3, OpenMP, #pragma omp parallel); (5, OpenMP, #pragma omp for schedule(MY_SCHEDULE))"}
{"predict": "(36,OpenMP,#pragma omp parallel for), (43,OpenMP,#pragma omp parallel for), (50,OpenMP,#pragma omp parallel for)", "label": "(14, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (11, CUDA, CUDA_CHECK(cudaGetDevice(&current_device_id));); (11, CUDA, CUDA_CHECK(cudaGetDeviceProperties(&props, current_device_id));)"}
{"predict": "(5,MPI,*w = MPI_Wtime(););", "label": "(4, CUDA, auto err = cudaFree(handle->deviceWs);); (8, CUDA, err = cudaEventDestroy(handle->wsMutex);)"}
{"predict": "(13, MPI, *w = MPI_Wtime();)", "label": "(18, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "78: \t\tlen += strlen(_p->pScriptLine[i]);\n79: \tcomp_add_script_line(\"TotalScriptLen=%d\\n\", len);\n80: \n81: \treturn 0;\n82: }", "label": "(39, OpenMP, comp_add_script_line(\"Func=DynamicFunc__\" #C \"_crypt_input%d_to_output1_FINAL\\n\",side);); (18, OpenMP, else                 comp_add_script_line(\"Func=DynamicFunc__append_salt%s\\n\", side==2?\"2\":\"\");); (9, OpenMP, comp_add_script_line(\"Func=DynamicFunc__clean_input2_kwik\\n\");); (61, OpenMP, comp_add_script_line(\"MaxInputLen=110\\n\");); (5, OpenMP, comp_add_script_line(\"Func=DynamicFunc__set_input_len_%d\\n\", keys_hex_len);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(50, OpenCL, clReleaseEvent(event);)"}
{"predict": "(tmp, pr1, dirs[1]);\n142:     myCovDev.MCD(acc, pr1, dirs[1] + 4);\n143:     quda::blas::xpy(acc, tmp);\n144:     applySpinTaste(acc, tmp, gDirs[dirs[1]]);\n145:     // YX result in pr1\n146:     myCovDev.MCD(pr1, out, dirs[1]);\n147:     myCovDev.MCD(acc, pr1, dirs[1] + 4);\n148:     quda::blas::xpy(acc, pr1);\n149:     applySpinTaste(pr1, pr1, gDirs[dirs[1]]);\n150:     myCovDev.MCD(pr1, pr1, dirs[0]);\n151:     myCovDev.MCD(acc, pr1, dirs[0] + 4);\n152:     quda::blas::xpy(acc, pr1);\n153:     applySpinTaste(pr1, pr1, gDirs[dirs[0]]);\n154:     // XY result in out\n155:     myCovDev.MCD(out, pr1, dirs[0]);\n156:     myCovDev.MCD(acc, pr1, dirs[0] + 4);\n157:     quda::blas::xpy(acc, out);\n158:     applySpinTaste(out, out, gDirs[dirs[0]]);\n159:     myCovDev.MCD(out, out, dirs[1]);\n160:     myCovDev.MCD(acc, out, dirs[1] + 4);\n161:     quda::blas::xpy(acc, out);\n162:     applySpinTaste(out, out, gDirs[dirs[1]]);\n163:     break;\n164:   }\n165:   }\n166: \n167:   profileCovDev.TPSTOP(QUDA_PROFILE_COMPUTE);\n168:   profileCovDev.TPSTART(QUDA_PROFILE_MEMORY);\n169: \n170:   quda::blas::axpy(out_h, out);\n171:   profileCovDev.TPSTOP(Q", "label": "(17, CUDA, ColorSpinorParam cudaParam(cpuParam, inv_param, QUDA_CUDA_FIELD_LOCATION);)"}
{"predict": "(4,MPI,*w = MPI_Wtime();); (12,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp critical), (62,OpenMP,#pragma omp barrier)", "label": "(55, MPI, MPI_Irecv(buf_recv, size_recv, MPI_BYTE, rank_recv, 0, comm, &request[num_req]);); (25, MPI, MPI_Irecv(&size_recv, 1, MPI_INT, rank_recv, 0, comm, &request[num_req]);); (60, MPI, MPI_Isend(buf_send, size_send, MPI_BYTE, rank_send, 0, comm, &request[num_req]);); (38, MPI, MPI_Isend(&size_send, 1, MPI_INT, rank_send, 0, comm, &request[num_req]);); (41, MPI, MPI_Waitall(num_req, request, status);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(3, CUDA, extern __shared__ float shared[];)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp atomic)", "label": "(4, OpenMP, #pragma omp parallel for)"}
{"predict": "(5,OpenMP,#pragma omp parallel for), (9,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for), (38,OpenMP,#pragma omp parallel for), (43,OpenMP,#pragma omp parallel for), (48,OpenMP,#pragma omp parallel for), (56,OpenMP,#pragma omp parallel for), (61,OpenMP,#pragma omp parallel for), (66,OpenMP,#pragma omp parallel for)", "label": "(11, OpenMP, #pragma omp distribute simd safelen (); (71, OpenMP, #pragma omp distribute simd safelen (4)); (3, OpenMP, #pragma omp distribute simd safelen); (19, OpenMP, #pragma omp distribute simd safelen (argc); (51, OpenMP, #pragma omp distribute simd safelen (S)); (44, OpenMP, #pragma omp distribute simd safelen (foobool(argc)), safelen (true), safelen (-5)); (72, OpenMP, #pragma omp distribute simd safelen (N)); (27, OpenMP, #pragma omp distribute simd safelen (1))); (58, OpenMP, #pragma omp distribute simd safelen (argv[1]=2)); (-6, OpenMP, #pragma omp target); (18, OpenMP, #pragma omp distribute simd safelen (ST); (29, OpenMP, #pragma omp distribute simd safelen ((ST > 0) ? 1 + ST : 2)); (5, OpenMP, #pragma omp distribute simd safelen ()); (-9, OpenMP, #pragma omp teams)"}
{"predict": "(27,OpenMP,#pragma omp parallel for)", "label": "(33, OpenMP, #pragma omp parallel for simd private(argv[1])); (29, OpenMP, #pragma omp parallel for simd private(z, a, b)); (47, OpenMP, #pragma omp parallel for simd private(i)); (15, OpenMP, #pragma omp parallel for simd private(argc,); (23, OpenMP, #pragma omp parallel for simd private(S1)); (50, OpenMP, #pragma omp parallel shared(i)); (33, OpenMP, #pragma omp parallel for simd private(h)); (49, OpenMP, #pragma omp parallel private(i)); (-2, OpenMP, #pragma omp parallel); (12, OpenMP, #pragma omp parallel for simd private(argc > 0 ? argv[1] : argv[2])); (26, OpenMP, #pragma omp parallel for simd private(e, g)); (-2, OpenMP, #pragma omp parallel for simd private); (30, OpenMP, #pragma omp parallel for simd nowait); (44, OpenMP, #pragma omp parallel for simd private(j)); (10, OpenMP, #pragma omp parallel for simd private(argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (0, OpenMP, #pragma omp parallel for simd private(argc); (-4, OpenMP, #pragma omp parallel for simd private()"}
{"predict": "(4,OpenMP,#pragma omp parallel for), (7,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for)", "label": "(3, OpenMP, #pragma omp parallel); (3, OpenMP, #pragma omp master taskloop)"}
{"predict": "(27,MPI,*w = MPI_Wtime(););\n(55,MPI,RING_Bcast(work1, mm[myrow]*updt, MPI_DOUBLE, curcol, comm_row););\n(57,MPI,RING_Bcast(work2, nn[mycol]*updt, MPI_DOUBLE, currow, comm_col););\n(60,OpenMP,#pragma omp parallel for)", "label": "(25, MPI, MPI_Comm_rank(comm_row, &mycol);); (25, MPI, MPI_Comm_rank(comm_col, &myrow);); (25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (34,OpenMP,#pragma omp parallel for), (57,OpenMP,#pragma omp parallel for)", "label": "(56, OpenMP, #pragma omp simd)"}
{"predict": "(40,MPI,MPI_Comm_size(MPI_COMM_WORLD, &size); MPI_Comm_rank(MPI_COMM_WORLD, &rank);) (51,OpenMP,#pragma omp parallel for private(i)) (61,MPI,MPI_Barrier(comm);)", "label": "(52, MPI, MPI_Comm_size(comm, &size);); (62, MPI, MPI_Barrier(comm);); (52, MPI, MPI_Win_create(target.buf, target.maxbufsize, base_type_size, MPI_INFO_NULL, comm, &win);); (69, MPI, MPI_Win_free(&win);); (25, MPI, MPI_Type_get_extent(dtp.DTP_base_type, &lb, &extent);); (46, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(1, CUDA, __shared__ float cache[threadsPerBlock];)"}
{"predict": "hs, n, -1.0, A,\n130: \t\t\t\t     lda, X, ldx, 1.0, B, ldb);\n131: \n132: \t\t\tdouble residual = LAPACKE_dlange_work(LAPACK_COL_MAJOR, 'F',\n133: \t\t\t\t\t\t\t    '1', n, nrhs, B, ldb);\n134: \n135: \t\t\tif (residual > tol) {\n136: \t\t\t\tparam[PARAM_PASS].i = 0;\n137: \t\t\t}\n138: \t\t}\n139: \t}\n140: \n141: \t//================================================================\n142: \t// Free arrays.\n143: \t//================================================================\n144: \tfree(A);\n145: \tfree(T);\n146: \tfree(ipiv);\n147: \tfree(ipiv2);\n148: \tfree(B);\n149: \tfree(X);\n150: \tif (test)\n151: \t\tfree(Aref);\n152: }", "label": "(117, OpenMP, plasma_time_t stop = omp_get_wtime();); (113, OpenMP, plasma_time_t start = omp_get_wtime();)"}
{"predict": "(20,MPI,*w = MPI_Wtime();); (32,OpenMP,#pragma omp parallel for), (41,OpenMP,#pragma omp parallel for)", "label": "(7, CUDA, __shared__ float block_acc[32];)"}
{"predict": "(2,OpenMP,#pragma omp parallel for)", "label": "(3, OpenMP, #pragma omp for collapse(2) schedule(runtime))"}
{"predict": "(9,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for), (44,OpenMP,#pragma omp parallel for)", "label": "(32, OpenMP, #pragma omp target parallel for simd linear(a, b : B::ib)); (23, OpenMP, #pragma omp target parallel for simd linear(argc : 5) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (4, OpenMP, #pragma omp target parallel for simd linear); (12, OpenMP, #pragma omp target parallel for simd linear(argc); (40, OpenMP, #pragma omp target parallel for simd linear(i)); (33, OpenMP, #pragma omp target parallel for simd linear(e, g, z)); (45, OpenMP, #pragma omp target parallel for simd linear(v : i)); (34, OpenMP, #pragma omp target parallel for simd linear(h)); (39, OpenMP, #pragma omp parallel); (12, OpenMP, #pragma omp target parallel for simd linear(argc > 0 ? argv[1] : argv[2])); (-1, OpenMP, #pragma omp target parallel for simd linear(); (7, OpenMP, #pragma omp target parallel for simd linear(argc,); (49, OpenMP, #pragma omp target parallel for simd linear(v : j)); (22, OpenMP, #pragma omp target parallel for simd linear(argv[1])); (13, OpenMP, #pragma omp target parallel for simd linear(S1)); (42, OpenMP, #pragma omp target parallel for simd linear(j)); (-4, OpenMP, #pragma omp target parallel for simd linear())"}
{"predict": "(25,MPI,*w = MPI_Wtime();); (28,OpenMP,#pragma omp parallel for), (35,OpenMP,#pragma omp parallel for), (46,OpenMP,#pragma omp parallel for)", "label": "(25, OpenMP, #pragma omp parallel for)"}
{"predict": "(6,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for), (36,OpenMP,#pragma omp parallel for), (56,OpenMP,#pragma omp parallel for), (60,OpenMP,#pragma omp parallel for), (63,OpenMP,#pragma omp parallel for), (66,OpenMP,#pragma omp parallel for)", "label": "(41, OpenMP, #pragma omp target simd collapse(4, )); (12, OpenMP, #pragma omp target simd collapse()); (48, OpenMP, #pragma omp target simd collapse(4 4)); (63, OpenMP, #pragma omp target simd collapse(4, 8)); (82, OpenMP, #pragma omp target simd collapse(5 - 5)); (4, OpenMP, #pragma omp target simd collapse(); (15, OpenMP, #pragma omp target simd collapse(, )); (10, OpenMP, #pragma omp target simd collapse(,); (74, OpenMP, #pragma omp target simd collapse(0)); (36, OpenMP, #pragma omp target simd collapse(4)); (45, OpenMP, #pragma omp target simd collapse(4, , 4)); (63, OpenMP, #pragma omp target simd collapse(foo())); (66, OpenMP, #pragma omp target simd collapse(-5)); (23, OpenMP, #pragma omp target simd collapse(4,); (17, OpenMP, #pragma omp target simd collapse(4); (11, OpenMP, #pragma omp target simd collapse 4)); (-11, OpenMP, #pragma omp target simd collapse); (53, OpenMP, #pragma omp target simd collapse(2.5))"}
{"predict": "(66,OpenMP,#pragma omp parallel), (84,OpenMP,#pragma omp parallel for), (90,OpenMP,#pragma omp parallel for)", "label": "(97, OpenMP, nthreads = omp_get_num_threads();); (82, OpenMP, #pragma omp parallel firstprivate(niter) private(step)); (62, OpenMP, #pragma omp parallel); (85, OpenMP, #pragma omp master)"}
{"predict": "(30,OpenMP,#pragma omp parallel for), (43,OpenMP,#pragma omp parallel for)", "label": "(35, OpenMP, iis3dhhc_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)"}
{"predict": "(5,OpenMP,#pragma omp parallel for)", "label": "(72, OpenMP, #pragma omp target exit data depend(in : argc,argv[i:argc],x[:]) map(release: c) nowait if(b>g)); (17, OpenMP, #pragma omp target exit data map(release: c) if(b>g)); (52, OpenMP, #pragma omp target exit data nowait map(from: c) map(release: d)); (57, OpenMP, #pragma omp target exit data nowait depend(in : argc,argv[i:argc],x[:]) map(from: a)); (1, OpenMP, #pragma omp target exit data map(from: a)); (16, OpenMP, #pragma omp target exit data map(from: x[0:10], c)); (24, OpenMP, #pragma omp target exit data map(from: c) map(release: d)); (41, OpenMP, #pragma omp target exit data map(release: c) nowait if(b>g)); (58, OpenMP, #pragma omp target exit data map(from: a) if (b > g) nowait depend(in : argc,argv[i:argc],x[:])); (72, OpenMP, #pragma omp target exit data nowait depend(in : argc,argv[i:argc],x[:]) map(always,release: e)); (5, OpenMP, #pragma omp target exit data map(from: c)); (58, OpenMP, #pragma omp target exit data map(from: c) depend(in : argc,argv[i:argc],x[:]) nowait); (30, OpenMP, #pragma omp target exit data map(from: a) if (b > g) nowait); (38, OpenMP, #pragma omp target exit data nowait map(from: x[0:10], c)); (19, OpenMP, #pragma omp target exit data map(always,release: e)); (30, OpenMP, #pragma omp target exit data map(from: c) nowait); (62, OpenMP, #pragma omp target exit data nowait map(from: c) depend(in : argc,argv[i:argc],x[:]) map(release: d)); (58, OpenMP, #pragma omp target exit data nowait map(from: x[0:10], c) depend(in : argc,argv[i:argc],x[:])); (-6, OpenMP, #pragma omp target exit data map(from: a) if (b > g)); (5, OpenMP, #pragma omp target exit data map(delete: x[0:10])); (43, OpenMP, #pragma omp target exit data nowait map(from: a) depend(in : argc,argv[i:argc],x[:]) if (target exit data: b)); (36, OpenMP, #pragma omp target exit data nowait map(always,release: e)); (17, OpenMP, #pragma omp target exit data nowait map(from: a) if (target exit data: b)); (-14, OpenMP, #pragma omp target exit data map(from: a) if (target exit data: b)); (12, OpenMP, #pragma omp target exit data nowait map(from: a)); (2, OpenMP, #pragma omp target exit data map(always, delete: x[0:10]))"}
{"predict": "(40,OpenMP,#pragma omp parallel for)\n(55,OpenMP,#pragma omp parallel for)", "label": "(62, OpenCL, TEST_OCL_CHECK(clReleaseEvent(read_buffer_event));); (64, OpenCL, TEST_OCL_CHECK(clReleaseCommandQueue(ocl_queue));); (52, OpenCL, TEST_OCL_CHECK(clWaitForEvents(1, &read_buffer_event));); (61, OpenCL, TEST_OCL_CHECK(clReleaseEvent(eltwise_event));); (59, OpenCL, TEST_OCL_CHECK(clReleaseEvent(write_buffer_event));)"}
{"predict": "(11,OpenMP,#pragma omp parallel for)", "label": "(10, CUDA, extent = make_cudaExtent(4, 5, 0);); (3, CUDA, cudaExtent extent = make_cudaExtent(4, 5, 6);)"}
{"predict": "(17,cudaMalloc,&device_array = cudaMalloc(num_bytes);)", "label": "(16, CUDA, cudaMalloc((void**)&device_array, num_bytes);); (49, CUDA, printf(\"CUDA Error: %s\\n\", cudaGetErrorString(error));); (35, CUDA, cudaThreadSynchronize();); (42, CUDA, cudaError_t error = cudaGetLastError();); (66, CUDA, cudaFree(device_array);); (54, CUDA, cudaMemcpy(host_array, device_array, num_bytes, cudaMemcpyDeviceToHost);)"}
{"predict": "(26,OpenMP,#pragma omp parallel for reduction(+:result[:srcDesc.length]) reduction(+:resultDu[:srcDesc.length]) reduction(+:resultDv[:srcDesc.length]))", "label": "(19, OpenMP, int numThreads = omp_get_max_threads();); (25, OpenMP, #pragma omp parallel for); (35, OpenMP, int threadId = omp_get_thread_num();)"}
{"predict": "(29,MPI,MPI_Datatype *params_type = alloc_mpi_params_type();), (30,MPI,free_mpi_params_type(params_type);), (31,MPI,MPI_Scatter(limits, 2, MPI_DOUBLE, local_limits, 2, MPI_DOUBLE, root, MPI_COMM_WORLD);), (44,MPI,MPI_Reduce(&partial_result, &result, 1, MPI_DOUBLE, MPI_SUM, root, MPI_COMM_WORLD);)", "label": "(56, MPI, MPI_Finalize();); (22, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (29, MPI, MPI_Bcast(&params, 1, *params_type, root, MPI_COMM_WORLD);); (14, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (10, MPI, MPI_Init_thread(NULL, NULL, MPI_THREAD_FUNNELED, &thread_level);)"}
{"predict": "(3,MPI,#ifdef HAVE_MPI), (5,MPI,int iparams[num_int_params] = {params.nx, params.ny, params.nz, params.numthreads, params.mv_overlap_comm_comp, params.use_locking, params.elem_group_size, params.use_elem_mat_fields, params.verify_solution, params.device, params.num_devices, params.skip_device, params.numa};), (22,MPI,float fparams[1] = {params.load_imbalance};), (25,MPI,#endif)", "label": "(23, MPI, MPI_Bcast(&fparams[0], 1, MPI_FLOAT, 0, MPI_COMM_WORLD);); (6, MPI, MPI_Bcast(&iparams[0], num_int_params, MPI_INT, 0, MPI_COMM_WORLD);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for)", "label": "(35, MPI, MPI_Finalize();); (2, MPI, MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &mpi_threads_provided);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);)"}
{"predict": "0;\n140:     if (ranks > 1) {\n141:         MPI_Allreduce(&max_offset, &starting_pos, 1, MPI_UINT64_T, MPI_MAX, MPI_COMM_WORLD);\n142:     } else {\n143:         starting_pos = max_offset;\n144:     }\n145: \n146:     /* calculate the number of entries that we found */\n147:     uint64_t count = (starting_pos - pos) / 512;\n148: \n149:     /* store the starting position and count in output parameters */\n150:     *out_starting_pos = starting_pos;\n151:     *out_count = count;\n152:     *out_offsets = NULL;\n153: \n154:     return rc;\n155: }", "label": "(141, MPI, MPI_Exscan(&max_offset, &starting_pos, 1, MPI_UINT64_T, MPI_MAX, MPI_COMM_WORLD);); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (258, MPI, MPI_Allreduce(&pos, &max_pos, 1, MPI_UINT64_T, MPI_MAX, MPI_COMM_WORLD);); (13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(20,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for), (54,OpenMP,#pragma omp parallel for)", "label": "(72, CUDA, cudaStreamDestroy(s);); (5, CUDA, cudaStreamCreate(&s);); (31, CUDA, cudaEventRecord(end, s);); (31, CUDA, cudaEventSynchronize(end);); (23, CUDA, cudaEventRecord(begin, s);); (66, CUDA, cudaEventDestroy(end);); (3, CUDA, cudaEventCreate(&end);); (63, CUDA, cudaEventDestroy(begin);); (0, CUDA, cudaEventCreate(&begin);); (27, CUDA, cudaEventElapsedTime(&time, begin, end);)"}
{"predict": "(11,MPI,MPI_Init(NULL, NULL);) (30,OpenMP,#pragma omp parallel for)", "label": "(11, MPI, ::dray::dray::mpi_comm(MPI_Comm_c2f(comm));)"}
{"predict": "(2,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for), (39,OpenMP,#pragma omp parallel for)", "label": "(12, OpenMP, #pragma omp target parallel defaultmap(tofrom,); (1, OpenMP, #pragma omp target parallel defaultmap(); (14, OpenMP, #pragma omp target parallel defaultmap(tofrom, scalar); (24, OpenMP, #pragma omp target parallel defaultmap(none:pointer) defaultmap(none:scalar) map(g)); (35, OpenMP, #pragma omp target defaultmap(none:aggregate)); (26, OpenMP, #pragma omp target parallel defaultmap(none:aggregate) defaultmap(none:scalar) map(argc)); (8, OpenMP, #pragma omp target parallel defaultmap (scalar:); (47, OpenMP, #pragma omp target defaultmap(none:pointer)); (32, OpenMP, #pragma omp parallel); (14, OpenMP, #pragma omp target parallel defaultmap(none:pointer)); (0, OpenMP, #pragma omp target parallel defaultmap(tofrom)); (-7, OpenMP, #pragma omp target parallel defaultmap()); (7, OpenMP, #pragma omp target parallel defaultmap(tofrom:scalar) defaultmap(to:scalar)); (8, OpenMP, #pragma omp target parallel defaultmap(alloc:pointer) defaultmap(to:scalar) defaultmap(firstprivate:pointer)); (15, OpenMP, #pragma omp target parallel defaultmap(none:pointer) defaultmap(none:scalar) defaultmap(none:aggregate) map(g) firstprivate(argc)); (-7, OpenMP, #pragma omp target parallel defaultmap (tofrom:); (9, OpenMP, #pragma omp target parallel defaultmap(none:pointer) defaultmap(none:scalar) map(argc)); (-11, OpenMP, #pragma omp target parallel defaultmap(tofrom)"}
{"predict": "(67,OpenMP,#pragma omp critical), (82,OpenMP,#pragma omp parallel), (125,OpenMP,#pragma omp barrier)", "label": "(57, MPI, MPI_Allreduce(&my_bytes, &max_bytes, 1, MPI_UNSIGNED_LONG, MPI_MAX, comm);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for reduction(+:local_sum)), (21,OpenMP,#pragma omp single)", "label": "(15, CUDA, __shared__ typename BlockReduceT::TempStorage temp_storage;)"}
{"predict": "140:     }; // if\n141:   }; // if\n142:   return err;\n143: }", "label": "(7, OpenMP, int tid = omp_get_thread_num();)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(14, OpenMP, return comp_raw(lv, rv, by_number, ascendingly, casesensitively);)"}
{"predict": ",k+1) * -0.4f);\n100:        VERIFY_IS_APPROX(expected, result);\n101:      }\n102:    }\n103:  }\n104:  }", "label": "(27, CUDA, assert(cudaMemcpy(out.data(), d_out, out_bytes, cudaMemcpyDeviceToHost) == cudaSuccess);); (15, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (12, CUDA, cudaMalloc((void**)(&d_in1), in1_bytes);); (69, CUDA, assert(cudaMemcpyAsync(out.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, context.device().stream()) == cudaSuccess);); (14, CUDA, cudaMemcpy(d_in1, in1.data(), in1_bytes, cudaMemcpyHostToDevice);); (10, CUDA, cudaMalloc((void**)(&d_in2), in2_bytes);); (67, CUDA, assert(cudaStreamSynchronize(context.device().stream()) == cudaSuccess);); (12, CUDA, cudaMemcpy(d_in2, in2.data(), in2_bytes, cudaMemcpyHostToDevice);)"}
{"predict": "(18,MPI,MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &pmode);) (26,MPI,MPI_Send(&data, 1, MPI_INT, dest, tag, MPI_COMM_WORLD);) (31,MPI,MPI_Recv(&data, 1, MPI_INT, source, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)", "label": "(11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (19, MPI, MPI_Barrier(MPI_COMM_WORLD);); (13, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (4, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(27,MPI,send_buffer.resize(send_size);) (28,MPI,send_sizes.resize(world_size);) (29,MPI,send_offsets.resize(world_size);) (44,MPI,mpi_world_communicator.Scatterv(send_buffer, send_sizes, send_offsets, recv_buffer, send_rank);) (67,MPI,scatterv_message.resize(world_size);) (68,MPI,mpi_world_communicator.Scatterv(scatterv_message, send_rank);)", "label": "(6, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(30,OpenMP,#pragma omp parallel for reduction(+:tmp)), (40,OpenMP,#pragma omp parallel for)", "label": "(15, CUDA, __shared__ thrust::cuda_cub::core::uninitialized_array<T, 1024> tmp;)"}
{"predict": "(34,OpenMP,#pragma omp parallel for)", "label": "(65, CUDA, cudaFree( outGPU );); (63, CUDA, cudaEventDestroy( evStop );); (64, CUDA, cudaFree( inGPU );); (60, CUDA, cudaEventDestroy( evStart );)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (15,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for), (41,OpenMP,#pragma omp parallel for), (48,OpenMP,#pragma omp parallel for)", "label": "(3, CUDA, __shared__ float maxScore[FMC2H];); (1, CUDA, __shared__ float siftPoint[128];); (2, CUDA, __shared__ float maxScor2[FMC2H];); (2, CUDA, __shared__ int maxIndex[FMC2H];)"}
{"predict": "30: \t\t\t\t\tlbend = (nn+1)*nlb_loc;\n131: \t\t\t\t}\n132: \n133: \t\t\t\t// printf(\"ya4 %5d k %5d lk %5d nn %5d lbstart %5d lbend %5d\\n\",thread_id,k,lk,nn,lbstart,lbend);\n134: \t\t\t\t// fflush(stdout);\n135: \n136: \t\t\t\tlsub1 = &lsub[lbstart];\n137: \t\t\t\tlusup1 = &lusup[lbstart];\n138: \n139: \t\t\t\tlptr1 = lloc[lbstart];\n140: \t\t\t\tluptr1 = lloc[lbstart+1];\n141: \n142: \t\t\t\tif(luptr_tmp>=luptr1){\n143: \n144: #ifdef _OPENMP\n145: #pragma omp parallel for schedule(static,1)\n146: #else\n147: #endif\n148: \t\t\t\t\tfor(lb = lbstart; lb < lbend; ++lb){\n149: \n150: #ifdef _OPENMP\n151: #pragma omp single\n152: #endif\n153: \n154: \t\t\t\t\t\tif(lsub1[lb]==k){\n155: \t\t\t\t\t\t\ti = lptr1-lptr;\n156: \t\t\t\t\t\t\tlptr = lptr1;\n157: \t\t\t\t\t\t\tluptr1 = lloc[lb+1];\n158: \t\t\t\t\t\t\tlptr1_tmp = lloc[lb+1];\n159: \n160: #ifdef _OPENMP\n161: #pragma omp single\n162: #endif\n163: \n164: \t\t\t\t\t\t\tlsub1[lb] = -1;\n165: \n166: #ifdef _OPENMP\n167: #pragma omp single\n168: #endif\n169: \n170: \t\t\t\t\t\t\tfor(j=0; j<i; ++j){\n171: \n172: #ifdef _OPENMP\n173: #pragma omp single\n174: #endif\n175: \n176: \t\t\t\t\t\t\t\tif(lsub[lb+j]==", "label": "(174, OpenMP, #pragma omp simd); (112, OpenMP, #pragma\tomp\ttaskloop private (lptr1,luptr1,nlb1,thread_id1,lsub1,lusup1,nsupr1,Linv,nn,lbstart,lbend,luptr_tmp1,nbrow,lb,lptr1_tmp,rtemp_loc,nbrow_ref,lptr,nbrow1,ik,rel,lk,iknsupc,il,i,irow,fmod_tmp,ikcol,p,ii,jj,t1,t2,j) untied); (229, OpenMP, #pragma omp simd lastprivate(irow))"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(12, MPI, rc = MPI_Unpublish_name(serv_name, MPI_INFO_NULL, port_name);); (12, MPI, MPI_Error_class(rc, &errclass);); (8, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();)\n(18,OpenMP,#pragma omp parallel for)\n(34,OpenMP,#pragma omp parallel for)", "label": "(11, OpenMP, #pragma omp target enter data map(alloc : a [0:array_size], b [0:array_size])); (18, OpenMP, #pragma omp target teams distribute parallel for); (3, OpenMP, #pragma omp target map(tofrom : warp_size)); (98, OpenMP, #pragma omp target exit data map(release : a [0:array_size], b [0:array_size]))"}
{"predict": "(27,OpenMP,#pragma omp parallel for collapse(2)), (28,MPI,for(int k=0;k<nacc;k++){ *w = MPI_Wtime(); gpumult0<<<blocks,threads>>>(dev_C.data().get(),dev_A.data().get(),dev_B.data().get(),Arow,Acol,Bcol); *w = MPI_Wtime();}),", "label": "(18, CUDA, thrust::device_vector<float> dev_B(Brow*Bcol);); (12, CUDA, thrust::host_vector<float>       A(Arow*Acol);); (14, CUDA, thrust::device_vector<float> dev_C(Crow*Ccol);); (32, CUDA, cudaDeviceSynchronize();); (10, CUDA, thrust::host_vector<float>       B(Brow*Bcol);); (10, CUDA, thrust::host_vector<float>       C(Crow*Ccol);); (11, CUDA, thrust::device_vector<float> dev_A(Arow*Acol);)"}
{"predict": "(23,MPI,auto const tree = makeDistributedTree<DeviceType, ArborX::Box<3>>(comm, ExecutionSpace{}, {{{{(float)comm_rank, 0., 0.}}, {{(float)comm_rank + 1, 1., 1.}}}});)", "label": "(8, MPI, MPI_Comm_rank(comm, &comm_rank);); (9, MPI, MPI_Comm_size(comm, &comm_size);)"}
{"predict": ";\n103: \n104:     /* Open the file */\n105:     err = MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_WRONLY | MPI_MODE_CREATE,\n106:                         MPI_INFO_NULL, &fh);\n107:     ERROR(\"MPI_File_open\");\n108: \n109:     /* Set file view */\n110:     err = MPI_File_set_view(fh, (MPI_Offset) 0, MPI_BYTE, filetype, \"native\", MPI_INFO_NULL);\n111:     ERROR(\"MPI_File_set_view\");\n112: \n113:     /* Write the buffer */\n114:     err = MPI_File_write_all(fh, buf, buf_len, MPI_BYTE, &status);\n115:     ERROR(\"MPI_File_write_all\");\n116:     MPI_File_close(&fh);\n117: \n118:     /* Free the buffer */\n119:     free(buf);\n120:     MPI_Type_free(&filetype);\n121:     MPI_Type_free(&buftype);\n122:     return 0;\n123: }", "label": "(14, MPI, MPI_Init(&argc, &argv);); (130, MPI, err = MPI_File_iwrite_all(fh, buf, 1, buftype, &req);); (100, MPI, err = MPI_Type_commit(&buftype);); (56, MPI, err = MPI_Type_create_subarray(2, gsize, count, start, MPI_ORDER_C, MPI_BYTE, &subType);); (57, MPI, err = MPI_Type_commit(&subType);); (68, MPI, err = MPI_Type_commit(&filetype);); (69, MPI, err = MPI_Type_free(&subType);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (115, MPI, err = MPI_File_write_all(fh, buf, 1, buftype, &status);); (125, MPI, err = MPI_File_read_all(fh, buf, 1, buftype, &status);); (129, MPI, err = MPI_File_iread_all(fh, buf, 1, buftype, &req);); (121, MPI, MPI_Wait(&req, &status);); (8, MPI, MPI_Finalize();); (106, MPI, err = MPI_File_set_view(fh, 0, MPI_BYTE, filetype, \"native\", MPI_INFO_NULL);); (134, MPI, err = MPI_Type_free(&buftype);); (131, MPI, err = MPI_Type_free(&filetype);); (127, MPI, MPI_File_close(&fh);); (10, MPI, err = MPI_Dims_create(nprocs, 2, psize);); (109, MPI, err = MPI_File_seek(fh, 0, MPI_SEEK_SET);)"}
{"predict": "(15,OpenMP,#pragma omp parallel sections)", "label": "(5, OpenMP, #pragma omp sections); (3, OpenMP, #pragma omp parallel); (5, OpenMP, #pragma omp section)"}
{"predict": "(45,MPI,*w = MPI_Wtime();); (48,OpenMP,#pragma omp parallel for), (...)", "label": "(42, OpenMP, #pragma omp target); (42, OpenMP, #pragma omp target firstprivate(val))"}
{"predict": "(16,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for)", "label": "(23, MPI, MPI_Error_class(err, &errclass);); (36, MPI, MPI_Free_mem(ap);); (16, MPI, MPI_Info_set(hints, \"mpich_buf_type\", hintstrs[i]);); (10, MPI, MPI_Info_set(hints, \"mpi_minimum_memory_alignment\", \"4096\");); (8, MPI, MPI_Info_create(&hints);); (20, MPI, err = MPI_Alloc_mem(count, MPI_INFO_NULL, &ap);); (3, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (35, MPI, MPI_Info_free(&hints);); (12, MPI, err = MPI_Alloc_mem(count, hints, &ap);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for)", "label": "(24, CUDA, cuda_safe_call(cudaStreamSynchronize(stream));); (10, CUDA, cuda_safe_call(cudaGetDeviceCount(&ndevs));)"}
{"predict": "(16,OpenMP,#pragma omp parallel for collapse(2)), (20,OpenMP,#pragma omp parallel for collapse(2))", "label": "(2, CUDA, __shared__ float s[Ny][Nx];)"}
{"predict": "(int i = 0; i < 10; ++i)\n94:     foo();\n95: #pragma omp target\n96: #pragma omp teams\n97:   for (int i = 0; i < 10; ++i)\n98:     foo();\n99: #pragma omp target\n100: #pragma omp teams\n101:   for (int i = 0; i < 10; ++i)\n102:     foo();\n103: #pragma omp target\n104: #pragma omp teams\n105:   for (int i = 0; i < 10; ++i)\n106:     foo();\n107: #pragma omp target\n108: #pragma omp teams\n109:   for (int i = 0; i < 10; ++i)\n110:     foo();\n111: #pragma omp target\n112: #pragma omp teams\n113:   for (int i = 0; i < 10; ++i)\n114:     foo();\n115: #pragma omp target\n116: #pragma omp teams\n117:   for (int i = 0; i < 10; ++i)\n118:     foo();\n119: #pragma omp target\n120: #pragma omp teams\n121:   for (int i = 0; i < 10; ++i)\n122:     foo();\n123: #pragma omp target\n124: #pragma omp teams\n125:   for (int i = 0; i < 10; ++i)\n126:     foo();\n127: #pragma omp target\n128: #pragma omp teams\n129:   for (int i = 0; i < 10; ++i)\n130:     foo();\n131: #pragma omp target\n132: #pragma omp teams\n133:   for (int i = 0; i < 10; ++i)\n134:     foo();\n135: #pragma omp target\n136: #pragma omp teams\n137:   for (", "label": "(109, OpenMP, #pragma omp distribute parallel for simd reduction(^ : fl)); (118, OpenMP, #pragma omp distribute parallel for simd reduction(&& : S2::S2sc)); (132, OpenMP, #pragma omp distribute parallel for simd reduction(+ : z, o)); (21, OpenMP, #pragma omp distribute parallel for simd reduction(); (45, OpenMP, #pragma omp distribute parallel for simd reduction(foo : argc); (140, OpenMP, #pragma omp distribute parallel for simd reduction(+ : p), reduction(+ : p)); (6, OpenMP, #pragma omp target); (87, OpenMP, #pragma omp distribute parallel for simd reduction(+ : ba)); (131, OpenMP, #pragma omp distribute parallel for simd private(i), reduction(+ : j), reduction(+ : q)); (146, OpenMP, #pragma omp distribute parallel for simd reduction(+ : r)); (161, OpenMP, #pragma omp parallel reduction(* : fl)); (151, OpenMP, #pragma omp distribute parallel for simd reduction(max : j)); (112, OpenMP, #pragma omp distribute parallel for simd reduction(& : e, g)); (172, OpenMP, #pragma omp distribute parallel for simd reduction(task, + : m)); (166, OpenMP, #pragma omp distribute parallel for simd reduction(+ : m)); (127, OpenMP, #pragma omp parallel private(k)); (63, OpenMP, #pragma omp distribute parallel for simd reduction(+ : a, b, c, d, f)); (67, OpenMP, #pragma omp distribute parallel for simd reduction(min : a, b, c, d, f)); (71, OpenMP, #pragma omp distribute parallel for simd reduction(max : h.b)); (55, OpenMP, #pragma omp distribute parallel for simd reduction(^ : S1)); (109, OpenMP, #pragma omp distribute parallel for simd reduction(+ : h, k, B::x)); (48, OpenMP, #pragma omp distribute parallel for simd reduction(&& : argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (17, OpenMP, #pragma omp distribute parallel for simd reduction(*)); (136, OpenMP, #pragma omp parallel reduction(min : i)); (20, OpenMP, #pragma omp distribute parallel for simd reduction(\\)); (140, OpenMP, #pragma omp parallel private(fl)); (8, OpenMP, #pragma omp distribute parallel for simd reduction()); (72, OpenMP, #pragma omp distribute parallel for simd reduction(* : ca)); (140, OpenMP, #pragma omp distribute parallel for simd reduction(+ : fl)); (30, OpenMP, #pragma omp distribute parallel for simd reduction(|| : argc > 0 ? argv[1] : argv[2])); (-1, OpenMP, #pragma omp distribute parallel for simd reduction(-); (73, OpenMP, #pragma omp distribute parallel for simd reduction(- : da)); (-13, OpenMP, #pragma omp distribute parallel for simd reduction +); (-19, OpenMP, #pragma omp distribute parallel for simd reduction); (20, OpenMP, #pragma omp distribute parallel for simd reduction(| : argc,); (123, OpenMP, #pragma omp parallel shared(i)); (78, OpenMP, #pragma omp distribute parallel for simd reduction(&& : S2::S2s)); (27, OpenMP, #pragma omp distribute parallel for simd reduction(~ : argc)); (-25, OpenMP, #pragma omp teams)"}
{"predict": "(6,OpenMP,#pragma omp parallel for), (9,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for)", "label": "(5, OpenCL, clReleaseMemObject(bufX);); (8, OpenCL, clReleaseMemObject(bufY);); (11, OpenCL, clReleaseMemObject(bufParam);)"}
{"predict": "(9,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);) (10,MPI,MPI_Comm_size(MPI_COMM_WORLD, &size);) (12,OpenMP,#pragma omp parallel for) (14,OpenMP,#pragma omp parallel for) (16,OpenMP,#pragma omp parallel for) (18,OpenMP,#pragma omp parallel for) (20,OpenMP,#pragma omp parallel for) (22,OpenMP,#pragma omp parallel for) (24,OpenMP,#pragma omp parallel for) (26,OpenMP,#pragma omp parallel for) (28,OpenMP,#pragma omp parallel for) (30,OpenMP,#pragma omp parallel for) (32,OpenMP,#pragma omp parallel for) (34,OpenMP,#pragma omp parallel for) (36,OpenMP,#pragma omp parallel for) (38,OpenMP,#pragma omp parallel for)", "label": "(9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (15, MPI, MPI_Cart_map(MPI_COMM_WORLD, 1, dims, periods, &newrank);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (29, MPI, MPI_Cart_map(MPI_COMM_WORLD, 0, dims, periods, &newrank);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for)", "label": "(26, OpenCL, status = clReleaseEvent(event);); (13, OpenCL, auto status = clWaitForEvents(1, &event);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(41, TBB, tbb::combinable<T> copy_constructed(sums);); (31, TBB, tbb::combinable<T> finit_combinable(my_finit_decl);); (31, TBB, tbb::parallel_for( tbb::blocked_range<int>( 0, N, 10000 ), ParallelScalarBodyNoInit<T>( finit_combinable ) );); (19, TBB, if (Verbose && t == 0) t0 = tbb::tick_count::now();); (22, TBB, tbb::parallel_for( tbb::blocked_range<int>( 0, N, 10000 ), ParallelScalarBody<T>( sums ) );); (40, TBB, tbb::combinable<T> assigned;); (-3, TBB, tbb::task_scheduler_init init(tbb::task_scheduler_init::deferred);); (44, TBB, tbb::combinable<T> moved1(std::move(sums));); (2, TBB, tbb::tick_count t0;); (16, TBB, tbb::combinable<T> sums;); (45, TBB, tbb::combinable<T> moved2;)"}
{"predict": "(98,OpenMP,#pragma omp parallel for), (112,MPI,MPI_Send(&retCode, 1, MPI_INT, dest, tag, MPI_COMM_WORLD));", "label": "(119, OpenCL, clReleaseMemObject( streams[2] );); (80, OpenCL, error = clSetKernelArg(kernel, 0, sizeof( streams[0] ), &streams[0] );); (119, OpenCL, clReleaseProgram( program );); (82, OpenCL, error = clSetKernelArg(kernel, 2, sizeof( streams[2] ), &streams[2] );); (114, OpenCL, clReleaseMemObject( streams[1] );); (115, OpenCL, clReleaseKernel( kernel );); (77, OpenCL, error = clSetKernelArg(kernel, 1, sizeof( streams[1] ), &streams[1] );); (92, OpenCL, error = clEnqueueReadBuffer( queue, streams[2], CL_TRUE, 0, outStride * count, outData, 0, NULL, NULL );); (86, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 1, NULL, &threadSize, &groupSize, 0, NULL, NULL );); (108, OpenCL, clReleaseMemObject( streams[0] );)"}
{"predict": "(14,MPI,MPI_Init(&argc, &argv);) (16,MPI,init_status(&status);) (17,MPI,if (!flag) {) (18,MPI,++errs;)) (20,MPI,errs += check_empty_status(status);) (22,MPI,init_status(&status);) (23,MPI,errs += check_empty_status(status);) (28,MPI,MPI_Finalize();)", "label": "(25, MPI, MPI_Wait(&req, &status);); (12, MPI, MPI_Bcast_init(&a, 1, MPI_INT, 0, MPI_COMM_WORLD, MPI_INFO_NULL, &req);); (15, MPI, MPI_Test(&req, &flag, &status);); (25, MPI, MPI_Request_free(&req);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for private(index,master,i,lens,pin,pout)), (33,OpenMP,#pragma omp atomic), (34,OpenMP,#pragma omp atomic)", "label": "(10, OpenMP, #pragma omp parallel for); (32, OpenMP, #pragma omp atomic)"}
{"predict": "(15,OpenMP,#pragma omp parallel for)", "label": "(25, OpenCL, clReleaseEvent(retEvent2);); (25, OpenCL, clReleaseEvent(retEvent);)"}
{"predict": "(7,MPI,MPI_Comm_rank(comm, &rank);)", "label": "(3, MPI, MPI_Barrier(MPI_COMM_WORLD);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (16, MPI, MPI_Comm_free(&comm);)"}
{"predict": "92:     for (i = 0; i < size; ++i) {\n93:         recvcounts[i] = COUNT;\n94:         for (j = 0; j < COUNT; ++j) {\n95:             buf_h[i * COUNT + j] = rank + i;\n96:             recvbuf_h[i * COUNT + j] = 0xdeadbeef;\n97:         }\n98:     }\n99:     MTestCopyContent(buf_h, buf, size * COUNT * sizeof(int), memtype);\n100:    MTestCopyContent(recvbuf_h, recvbuf, COUNT * sizeof(int), memtype);\n101:    MPI_Wait(&ireq, MPI_STATUS_IGNORE);\n102:    MTestCopyContent(recvbuf, recvbuf_h, COUNT * sizeof(int), memtype);\n103:    for (j = 0; j < COUNT; ++j) {\n104:        my_assert(recvbuf_h[j] == (size * rank + ((size - 1) * size) / 2));\n105:    }\n106:    for (i = 1; i < size; ++i) {\n107:        for (j = 0; j < COUNT; ++j) {\n108:            /* check we didn't corrupt the rest of the recvbuf_h */\n109:            my_assert(recvbuf_h[i * COUNT + j] == 0xdeadbeef);\n110:        }\n111:    }\n112: }\n113: #endif", "label": "(169, MPI, MPI_Scan(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (28, MPI, MPI_Ireduce(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD, &req);); (82, MPI, MPI_Reduce_scatter(buf, recvbuf, recvcounts, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (45, MPI, MPI_Allreduce(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (124, MPI, MPI_Reduce_scatter_block(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (59, MPI, MPI_Iallreduce(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD, &req);); (100, MPI, MPI_Ireduce_scatter(buf, recvbuf, recvcounts, MPI_INT, MPI_SUM, MPI_COMM_WORLD, &req);); (23, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (3, MPI, MPI_Reduce(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD);); (187, MPI, MPI_Exscan(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (139, MPI, MPI_Ireduce_scatter_block(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD, &req);); (171, MPI, MPI_Iscan(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD, &req);); (198, MPI, MPI_Iexscan(buf, recvbuf, COUNT, MPI_INT, MPI_SUM, MPI_COMM_WORLD, &req);)"}
{"predict": "(7,MPI,*t = MPI_Wtime();); (11,MPI,MPI_Recv(buf, cnt, MPI_INT, 0, tag, MPI_COMM_WORLD, &status);)", "label": "(9, MPI, t = MPI_Wtime();); (11, MPI, t = MPI_Wtime() - t;)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for)", "label": "(14, OpenMP, #pragma omp parallel for proc_bind(close), proc_bind(spread)); (10, OpenMP, #pragma omp parallel for proc_bind(master); (19, OpenMP, #pragma omp parallel for proc_bind(master)); (5, OpenMP, #pragma omp parallel for proc_bind()); (-2, OpenMP, #pragma omp parallel for proc_bind); (12, OpenMP, #pragma omp parallel for proc_bind(x)); (19, OpenMP, #pragma omp parallel proc_bind(close)); (19, OpenMP, #pragma omp parallel for proc_bind(spread)); (-3, OpenMP, #pragma omp parallel for proc_bind()"}
{"predict": "(7,MPI,*w = MPI_Wtime(););\n(13,OpenMP,#pragma omp parallel for),", "label": "(6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &comm_size);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (7, MPI, vtkh::SetMPICommHandle(MPI_Comm_c2f(MPI_COMM_WORLD));)"}
{"predict": "(20,OpenMP,#pragma omp parallel for), (47,OpenMP,#pragma omp parallel for)", "label": "(16, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(4,OpenMP,#pragma omp parallel for), (10,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for), (35,OpenMP,#pragma omp parallel for), (41,OpenMP,#pragma omp parallel for), (47,OpenMP,#pragma omp parallel for), (52,OpenMP,#pragma omp parallel for), (56,OpenMP,#pragma omp parallel for), (60,OpenMP,#pragma omp parallel for), (65,OpenMP,#pragma omp parallel for), (69,OpenMP,#pragma omp parallel for), (74,OpenMP,#pragma omp parallel for), (78,OpenMP,#pragma omp parallel for), (83,OpenMP,#pragma omp parallel for), (88,OpenMP,#pragma omp parallel for), (96,OpenMP,#pragma omp parallel for), (101,OpenMP,#pragma omp parallel for), (106,OpenMP,#pragma omp parallel for), (112,OpenMP,#pragma omp parallel for), (117,OpenMP,#pragma omp parallel for)", "label": "(19, OpenMP, #pragma omp distribute simd aligned(, )); (75, OpenMP, #pragma omp distribute simd aligned(x :)); (80, OpenMP, #pragma omp distribute simd aligned(x :, )); (9, OpenMP, #pragma omp distribute simd aligned(,); (-2, OpenMP, #pragma omp target); (82, OpenMP, #pragma omp distribute simd aligned(x : 1)); (52, OpenMP, #pragma omp distribute simd aligned(x, y, z)); (24, OpenMP, #pragma omp distribute simd aligned(int)); (62, OpenMP, #pragma omp distribute simd aligned(z)); (-4, OpenMP, #pragma omp distribute simd aligned(); (40, OpenMP, #pragma omp distribute simd aligned(x, y)); (32, OpenMP, #pragma omp distribute simd aligned(x)); (80, OpenMP, #pragma omp distribute simd aligned(x : 2 * 2)); (12, OpenMP, #pragma omp distribute simd aligned()); (23, OpenMP, #pragma omp distribute simd aligned(0)); (83, OpenMP, #pragma omp distribute simd aligned(x : 1, y)); (88, OpenMP, #pragma omp distribute simd aligned(x : 1, y, z : 1)); (108, OpenMP, #pragma omp distribute simd aligned(x) aligned(z, x)); (116, OpenMP, #pragma omp distribute simd aligned(x, y, z) aligned(y, z)); (-16, OpenMP, #pragma omp teams)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for)", "label": "(14, OpenMP, #pragma omp parallel for collapse(2)); (12, OpenMP, #pragma omp target map(tofrom: data))"}
{"predict": "(27,OpenMP,#pragma omp parallel for collapse(2)), (48,OpenMP,#pragma omp parallel for collapse(2)), (53,OpenMP,#pragma omp parallel for collapse(2)), (58,OpenMP,#pragma omp parallel for collapse(2))", "label": "(78, CUDA, CUDA_CHECK(cudaFree(d_C));); (63, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (46, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_B), sizeof(data_type) * B.size()));); (74, CUDA, CUDA_CHECK(cudaFree(d_B));); (45, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_C), sizeof(data_type) * C.size()));); (79, CUDA, CUDA_CHECK(cudaDeviceReset());); (76, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (40, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (68, CUDA, CUDA_CHECK(cudaFree(d_A));); (34, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(25,OpenMP,#pragma omp parallel for)", "label": "(7, OpenCL, err = clGetDeviceInfo(device, CL_DEVICE_IL_VERSION, 0, NULL, &str_size);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for reduction(min:minval))", "label": "(10, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:minval) reduction(min:minval))"}
{"predict": "Free the intercomm */\n128:         MPI_Comm_free(&intercomm);\n129: \n130:         /* Free the intracomm */\n131:         MPI_Comm_free(&intracomm);\n132:         MPI_Comm_free(&intracomm2);\n133:         MPI_Comm_free(&intracomm3);\n134: \n135:         /* Finalize MPI */\n136:         MPI_Finalize();\n137:     }\n138: \n139:     MTest_Finalize(errs);\n140:     return MTestReturnValue(errs);\n141: }", "label": "(65, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (56, MPI, MPI_Intercomm_merge(intercomm, isChild, &intracomm);); (161, MPI, MPI_Comm_free(&intracomm3);); (95, MPI, MPI_Comm_rank(intracomm2, &icrank);); (12, MPI, MPI_Comm_get_parent(&parentcomm);); (34, MPI, MPI_Send(&i, 1, MPI_INT, i, 0, intercomm);); (141, MPI, MPI_Ssend(&errs, 1, MPI_INT, 0, 1, intercomm);); (56, MPI, MPI_Comm_size(intracomm, &icsize);); (146, MPI, MPI_Recv(&err, 1, MPI_INT, i, 1, intercomm, MPI_STATUS_IGNORE);); (117, MPI, MPI_Intercomm_merge(intercomm, 0, &intracomm3);); (65, MPI, MPI_Comm_remote_size(parentcomm, &psize);); (80, MPI, MPI_Intercomm_merge(intercomm, !isChild, &intracomm2);); (149, MPI, MPI_Comm_free(&intracomm);); (102, MPI, MPI_Comm_remote_size(intercomm, &csize);); (83, MPI, MPI_Comm_size(intracomm2, &icsize);); (49, MPI, MPI_Comm_rank(intracomm, &icrank);); (150, MPI, MPI_Comm_free(&intercomm);); (32, MPI, MPI_Recv(&i, 1, MPI_INT, 0, 0, intercomm, &status);); (114, MPI, MPI_Comm_rank(intracomm3, &icrank);); (10, MPI, MPI_Comm_rank(intercomm, &rank);); (7, MPI, MPI_Comm_remote_size(intercomm, &rsize);); (7, MPI, MPI_Comm_size(intercomm, &size);); (109, MPI, MPI_Comm_size(intracomm3, &icsize);); (139, MPI, MPI_Comm_free(&intracomm2);); (150, MPI, MPI_Finalize();)"}
{"predict": "_BASE_ZERO);\n85:   // CHECK: hipsparseSetMatType(descrC, HIPSPARSE_MATRIX_TYPE_GENERAL);\n86:   cusparseSetMatType(descrC, CUSPARSE_MATRIX_TYPE_GENERAL);\n87: \n88:   /* step 3: allocate memory on device */\n89:   cudaStat1 = cudaMalloc((void**)&d_csrRowPtrA, (m + 1) * sizeof(int));\n90:   assert(cudaSuccess == cudaStat1);\n91:   cudaStat1 = cudaMalloc((void**)&d_csrColIndA, nnzA * sizeof(int));\n92:   assert(cudaSuccess == cudaStat1);\n93:   cudaStat1 = cudaMalloc((void**)&d_csrValA, nnzA * sizeof(float));\n94:   assert(cudaSuccess == cudaStat1);\n95: \n96:   cudaStat1 = cudaMalloc((void**)&d_csrRowPtrC, (m + 1) * sizeof(int));\n97:   assert(cudaSuccess == cudaStat1);\n98:   cudaStat1 = cudaMalloc((void**)&d_csrColIndC, nnzA * sizeof(int));\n99:   assert(cudaSuccess == cudaStat1);\n100:   cudaStat1 = cudaMalloc((void**)&d_csrValC, nnzA * sizeof(float));\n101:   assert(cudaSuccess == cudaStat1);\n102: \n103:   /* step 4: copy matrix A to device */\n104:   cudaStat1 = cudaMemcpy(d_csrRowPtrA, csrRowPtrA, (m + 1) * sizeof(int), cudaMemcpyHostToDevice);\n105:   assert(cudaSuccess == cudaStat1);\n106:   cudaStat1 = cudaMemcpy(d_csrColIndA, csrColIndA, nnzA * sizeof(int), cudaMemcpyHostToDevice);\n107:   assert(cudaSuccess == cudaStat1);\n108:   cudaStat1 = cudaMemcpy(d_csrValA, csrValA, nnzA * sizeof(float), cudaMemcpyHostToDevice);\n109:   assert(cudaSuccess == cudaStat1);\n110: \n111:   /* step 5: allocate workspace */\n112:   // CHECK: status = hipsparseXcoo2csr(handle, d_csrRowPtrA,", "label": "(106, CUDA, cudaStat1 = cudaMemcpy(d_csrRowPtrA, csrRowPtrA, sizeof(int)*(m + 1), cudaMemcpyHostToDevice);); (234, CUDA, if (d_csrColIndC) cudaFree(d_csrColIndC);); (211, CUDA, cudaStat1 = cudaMemcpy(csrRowPtrC, d_csrRowPtrC, sizeof(int)*(m + 1), cudaMemcpyDeviceToHost);); (99, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrRowPtrC, sizeof(int)*(m + 1));); (94, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrValA, sizeof(float)*nnzA);); (109, CUDA, cudaStat1 = cudaMemcpy(d_csrValA, csrValA, sizeof(float)*nnzA, cudaMemcpyHostToDevice);); (221, CUDA, if (d_csrRowPtrA) cudaFree(d_csrRowPtrA);); (237, CUDA, if (stream) cudaStreamDestroy(stream);); (225, CUDA, if (d_csrRowPtrC) cudaFree(d_csrRowPtrC);); (170, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrValC, sizeof(float) * nnzC);); (227, CUDA, if (d_csrValC) cudaFree(d_csrValC);); (154, CUDA, cudaStat1 = cudaDeviceSynchronize();); (78, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrRowPtrA, sizeof(int)*(m + 1));); (204, CUDA, cudaStat1 = cudaMemcpy(csrColIndC, d_csrColIndC, sizeof(int)*nnzC, cudaMemcpyDeviceToHost);); (80, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrColIndA, sizeof(int)*nnzA);); (41, CUDA, cudaStat1 = cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking);); (94, CUDA, cudaStat1 = cudaMemcpy(d_csrColIndA, csrColIndA, sizeof(int)*nnzA, cudaMemcpyHostToDevice);); (125, CUDA, cudaStat1 = cudaMalloc((void**)&d_work, lworkInBytes);); (211, CUDA, if (d_csrColIndA) cudaFree(d_csrColIndA);); (202, CUDA, cudaStat1 = cudaMemcpy(csrValC, d_csrValC, sizeof(float)*nnzC, cudaMemcpyDeviceToHost);); (155, CUDA, cudaStat1 = cudaMalloc((void**)&d_csrColIndC, sizeof(int) * nnzC);); (210, CUDA, if (d_csrValA) cudaFree(d_csrValA);); (228, CUDA, cudaDeviceReset();)"}
{"predict": "(38,OpenMP,#pragma omp parallel for)", "label": "(81, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (81, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)"}
{"predict": "(14,MPI,*w = MPI_Wtime();); (17,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for)", "label": "(26, OpenCL, clReleaseContext(knode->key.context);); (24, OpenCL, clReleaseProgram(kern->program);)"}
{"predict": "= MPI_Wtime();\n138:         while (repsleft--) {\n139:             MPI_Send(sbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD);\n140:             MPI_Recv(rbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n141:         }\n142:         t1 = MPI_Wtime() - t1;\n143:         times[2][k] = t1 / reps;\n144:         if (wrank == 0) {\n145:             t1 = t1 / reps;\n146:             if (t1 > 0) {\n147:                 t1 = t1 * 1.e6;\n148:                 if (verbose)\n149:                     printf(\"%d\\t%g\\t%g\\n\", len, t1, len / t1);\n150:             } else {\n151:                 t1 = t1 * 1.e6;\n152:                 if (verbose)\n153:                     printf(\"%d\\t%g\\tINF\\n\", len, t1);\n154:             }\n155:             if (verbose)\n156:                 fflush(stdout);\n157:         }\n158: \n159:         len *= 2;\n160:     }\n161: \n162:     MPI_Finalize();\n163:     return 0;\n164: }", "label": "(13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (60, MPI, MPI_Irecv(rbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD, &rreq);); (57, MPI, t1 = MPI_Wtime();); (60, MPI, MPI_Wait(&rreq, MPI_STATUS_IGNORE);); (146, MPI, MPI_Recv(rbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (57, MPI, MPI_Send(sbuf, len, MPI_BYTE, partner, k, MPI_COMM_WORLD);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (34, MPI, MPI_Barrier(MPI_COMM_WORLD);); (57, MPI, t1 = MPI_Wtime() - t1;); (8, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (24, MPI, MPI_Abort(MPI_COMM_WORLD, 2);)"}
{"predict": "(20,MPI,g.wait_for_all());", "label": "(1, TBB, using namespace tbb::flow;)"}
{"predict": "(2,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for), (29,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for), (43,OpenMP,#pragma omp parallel for)", "label": "(7, OpenMP, #pragma omp teams distribute parallel for simd safelen (); (21, OpenMP, #pragma omp teams distribute parallel for simd safelen (2+2))); (10, OpenMP, #pragma omp teams distribute parallel for simd safelen ()); (54, OpenMP, #pragma omp teams distribute parallel for simd safelen(safelen(tmain<int, char, -1, -2>(argc, argv)); (37, OpenMP, #pragma omp teams distribute parallel for simd safelen (S1)); (12, OpenMP, #pragma omp teams distribute parallel for simd safelen (4); (45, OpenMP, #pragma omp teams distribute parallel for simd safelen (argv[1]=2)); (21, OpenMP, #pragma omp teams distribute parallel for simd safelen (foobool(1) > 0 ? 1 : 2)); (28, OpenMP, #pragma omp teams distribute parallel for simd safelen (foobool(argc)), safelen (true), safelen (-5)); (-8, OpenMP, #pragma omp target); (-8, OpenMP, #pragma omp teams distribute parallel for simd safelen)"}
{"predict": "(15,MPI,pcu::PCU PCUObj = pcu::PCU(MPI_COMM_WORLD);); (58,MPI,MPI_Finalize();)", "label": "(24, MPI, MPI_Finalize();); (12, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(4,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for), (53,OpenMP,#pragma omp parallel for)", "label": "(20, TBB, tbb::flow::make_edge( n, r[i] );); (54, TBB, tbb::flow::remove_edge( n, r[i] );); (1, TBB, tbb::flow::write_once_node<R> n(g);); (-1, TBB, tbb::flow::graph g;)"}
{"predict": "(6,OpenMP,#pragma omp parallel for) (9,OpenMP,#pragma omp parallel for) (12,OpenMP,#pragma omp parallel for) (15,OpenMP,#pragma omp parallel for) (18,OpenMP,#pragma omp parallel for) (21,OpenMP,#pragma omp parallel for) (24,OpenMP,#pragma omp parallel for) (27,OpenMP,#pragma omp parallel for) (30,OpenMP,#pragma omp parallel for)", "label": "(16, OpenMP, #pragma omp distribute parallel for schedule (auto); (45, OpenMP, #pragma omp distribute parallel for schedule (guided, foobool(argc)), schedule (static, true), schedule (dynamic, -5)); (48, OpenMP, #pragma omp distribute parallel for schedule (guided, S1)); (52, OpenMP, #pragma omp distribute parallel for schedule (static, argv[1]=2)); (0, OpenMP, #pragma omp distribute parallel for schedule); (31, OpenMP, #pragma omp distribute parallel for schedule (static, 2+2))); (2, OpenMP, #pragma omp distribute parallel for schedule (); (5, OpenMP, #pragma omp distribute parallel for schedule ()); (32, OpenMP, #pragma omp distribute parallel for schedule (dynamic, foobool(1) > 0 ? 1 : 2 - z)); (23, OpenMP, #pragma omp distribute parallel for schedule (guided, 4); (51, OpenMP, #pragma omp distribute parallel for schedule(dynamic, schedule(tmain<int, char, -1, -2>(argc, argv)); (17, OpenMP, #pragma omp distribute parallel for schedule (runtime, 3)); (-10, OpenMP, #pragma omp target); (11, OpenMP, #pragma omp distribute parallel for schedule (auto,); (6, OpenMP, #pragma omp distribute parallel for schedule (auto_dynamic); (-12, OpenMP, #pragma omp teams)"}
{"predict": "(12,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for)", "label": "(18, CUDA, cuda_safe_call(cudaGetLastError());)"}
{"predict": "(26,OpenMP,#pragma omp parallel for), (43,OpenMP,#pragma omp parallel for), (60,OpenMP,#pragma omp parallel for), (77,OpenMP,#pragma omp parallel for)", "label": "(21, CUDA, cudaMalloc((void**)&deviceDiagonalResult, sizeof(bool));); (55, CUDA, cudaMemcpy(hostDiagonalResult, deviceDiagonalResult, sizeof(bool), cudaMemcpyDeviceToHost);); (99, CUDA, cudaFree(deviceSymmetricResult);); (20, CUDA, cudaMalloc((void**)&deviceSkewSymmetricResult, sizeof(bool));); (54, CUDA, cudaMemcpy(hostSkewSymmetricResult, deviceSkewSymmetricResult, sizeof(bool), cudaMemcpyDeviceToHost);); (17, CUDA, cudaMalloc((void**)&deviceSymmetricResult, sizeof(bool));); (33, CUDA, cudaMemcpy(deviceMatrix, hostMatrix, width * height * sizeof(float), cudaMemcpyHostToDevice);); (37, CUDA, cudaMemcpy(deviceIdempotentResult, hostIdempotentResult, sizeof(bool), cudaMemcpyHostToDevice);); (11, CUDA, cudaMalloc((void**)&deviceMatrix, width * height * sizeof(float));); (31, CUDA, cudaMemcpy(deviceToeplitzResult, hostToeplitzResult, sizeof(bool), cudaMemcpyHostToDevice);); (49, CUDA, cudaMemcpy(hostIdempotentResult, deviceIdempotentResult, sizeof(bool), cudaMemcpyDeviceToHost);); (91, CUDA, cudaFree(deviceSkewSymmetricResult);); (31, CUDA, cudaMemcpy(deviceSkewSymmetricResult, hostSkewSymmetricResult, sizeof(bool), cudaMemcpyHostToDevice);); (44, CUDA, cudaMemcpy(hostSymmetricResult, deviceSymmetricResult, sizeof(bool), cudaMemcpyDeviceToHost);); (28, CUDA, cudaMemcpy(deviceSymmetricResult, hostSymmetricResult, sizeof(bool), cudaMemcpyHostToDevice);); (85, CUDA, cudaFree(deviceDiagonalResult);); (4, CUDA, cudaMalloc((void**)&deviceToeplitzResult, sizeof(bool));); (82, CUDA, cudaFree(deviceToeplitzResult);); (85, CUDA, cudaFree(deviceIdempotentResult);); (36, CUDA, cudaMemcpy(hostToeplitzResult, deviceToeplitzResult, sizeof(bool), cudaMemcpyDeviceToHost);); (78, CUDA, cudaFree(deviceMatrix);); (20, CUDA, cudaMemcpy(deviceDiagonalResult, hostDiagonalResult, sizeof(bool), cudaMemcpyHostToDevice);); (2, CUDA, cudaMalloc((void**)&deviceIdempotentResult, sizeof(bool));)"}
{"predict": "1:             clReleaseMemObject( streams[ii] );\n122:             clReleaseMemObject( streams[ii+1] );\n123:             free( outptr[i] );\n124:             return -1;\n125:         }\n126: \n127:         err = clSetKernelArg( kernel[i], 0, sizeof( cl_mem ), &streams[ii] );\n128:         if( err != CL_SUCCESS ){\n129:             clReleaseEvent(writeEvent);\n130:             clReleaseMemObject( streams[ii] );\n131:             clReleaseMemObject( streams[ii+1] );\n132:             free( outptr[i] );\n133:             return -1;\n134:         }\n135:         err = clSetKernelArg( kernel[i], 1, sizeof( cl_mem ), &streams[ii+1] );\n136:         if( err != CL_SUCCESS ){\n137:             clReleaseEvent(writeEvent);\n138:             clReleaseMemObject( streams[ii] );\n139:             clReleaseMemObject( streams[ii+1] );\n140:             free( outptr[i] );\n141:             return -1;\n142:         }\n143:         err = clSetKernelArg( kernel[i], 2, sizeof( size_t ), &threads[0] );\n144:         if( err != CL_SUCCESS ){\n145:             clReleaseEvent(writeEvent);\n146:             clReleaseMemObject( streams[ii] );\n147:             clReleaseMemObject( streams[ii+1] );\n148:             free( outptr[i] );\n149:             return -1;\n150:         }\n151: \n152:         err = clEnqueueNDRangeKernel( queue, kernel[i], 1, NULL, threads, NULL, 0, NULL, &writeEvent );\n153:         if( err != CL_SUCCESS ){\n154:             clReleaseEvent(writeEvent);\n155:             clReleaseMemObject( streams[ii] );\n156:             clReleaseMemObject( streams[ii+1] );\n157:             free( outptr[i] );\n158:             return -1;\n159:         }\n160", "label": "(57, OpenCL, err = clEnqueueWriteBuffer( queue, streams[ii], false, 0, ptrSizes[i]*num_elements, inptr[i], 0, NULL, &writeEvent );); (132, OpenCL, err = clSetKernelArg( kernel[i], 0, sizeof( cl_mem ), (void *)&streams[ii] );); (49, OpenCL, clReleaseMemObject(streams[ii]);); (135, OpenCL, clReleaseProgram( program[i] );); (123, OpenCL, clReleaseMemObject(streams[ii+1]);); (55, OpenCL, clReleaseMemObject( streams[ii+1] );); (140, OpenCL, err = clEnqueueNDRangeKernel( queue, kernel[i], 1, NULL, threads, NULL, 0, NULL, NULL );); (52, OpenCL, clReleaseMemObject( streams[ii] );); (64, OpenCL, clReleaseEvent(writeEvent);); (59, OpenCL, err = clWaitForEvents( 1, &writeEvent );); (127, OpenCL, clReleaseKernel( kernel[i] );); (123, OpenCL, err |= clSetKernelArg( kernel[i], 1, sizeof( cl_mem ), (void *)&streams[ii+1] );); (148, OpenCL, err = clEnqueueReadBuffer( queue, streams[ii+1], true, 0, outPtrSizes[i]*num_elements, outptr[i], 0, NULL, NULL );)"}
{"predict": "(13,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (29,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for), (35,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for), (39,OpenMP,#pragma omp parallel for)", "label": "(15, OpenMP, #pragma omp target teams distribute parallel for simd linear (); (11, OpenMP, #pragma omp target teams distribute parallel for simd linear); (34, OpenMP, #pragma omp target teams distribute parallel for simd linear (S1)); (37, OpenMP, #pragma omp target teams distribute parallel for simd linear (a, b)); (39, OpenMP, #pragma omp target teams distribute parallel for simd linear (argv[1])); (44, OpenMP, #pragma omp target teams distribute parallel for simd linear(h, C::x)); (23, OpenMP, #pragma omp target teams distribute parallel for simd linear (argc > 0 ? argv[1] : argv[2])); (15, OpenMP, #pragma omp target teams distribute parallel for simd linear (argc); (10, OpenMP, #pragma omp target teams distribute parallel for simd linear ()); (24, OpenMP, #pragma omp target teams distribute parallel for simd linear (argc)); (36, OpenMP, #pragma omp target teams distribute parallel for simd linear(e, g)); (15, OpenMP, #pragma omp target teams distribute parallel for simd linear (argc,)"}
{"predict": "(47,OpenMP,#pragma omp parallel for)", "label": "(56, OpenCL, err = clEnqueueWriteBuffer(queue, bufY, CL_TRUE, 0, (lenY*sizeof(cl_float)), Y, 0, NULL, NULL);); (82, OpenCL, clReleaseMemObject(bufX);); (44, OpenCL, clReleaseCommandQueue(queue);); (28, OpenCL, printf( \"clCreateContext() failed with %d\\n\", err );); (75, OpenCL, clReleaseEvent(event);); (32, OpenCL, printf( \"clCreateCommandQueue() failed with %d\\n\", err );); (29, OpenCL, queue = clCreateCommandQueue(ctx, device, 0, &err);); (46, OpenCL, bufY = clCreateBuffer(ctx, CL_MEM_READ_WRITE, (lenY*sizeof(cl_float)), NULL, &err);); (30, OpenCL, clReleaseContext(ctx);); (73, OpenCL, clReleaseMemObject(bufY);); (42, OpenCL, bufX = clCreateBuffer(ctx, CL_MEM_READ_ONLY, (lenX*sizeof(cl_float)), NULL, &err);); (44, OpenCL, err = clEnqueueWriteBuffer(queue, bufX, CL_TRUE, 0, (lenX*sizeof(cl_float)), X, 0, NULL, NULL);); (17, OpenCL, ctx = clCreateContext(props, 1, &device, NULL, NULL, &err);); (53, OpenCL, err = clWaitForEvents(1, &event);)"}
{"predict": "(27,MPI,*w = MPI_Wtime(););\n(40,OpenMP,#pragma omp parallel for)\n(50,OpenMP,#pragma omp parallel for)", "label": "(62, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (6, CUDA, CUDA_CHECK(cudaStreamCreate(&stream));); (46, CUDA, CUDA_CHECK(cudaGetLastError());); (52, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));)"}
{"predict": "(13,OpenMP,#pragma omp parallel for)", "label": "(7, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)"}
{"predict": "(12,OpenMP,#pragma omp parallel for private(devi, last_row, first_row, blocks, size, avg, blockDim, bintotal, max_binavgcount, binavgcount, binsize, bins, grid_sizes->at(devi), plan, offset))", "label": "(11, OpenMP, #pragma omp parallel for)"}
{"predict": "(4,MPI,*w = MPI_Wtime();)", "label": "(5, OpenCL, clReleaseMemObject(objA);); (8, OpenCL, clReleaseMemObject(objB);); (11, OpenCL, clReleaseMemObject(objC);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for)", "label": "(7, OpenMP, #pragma omp parallel); (7, OpenMP, #pragma omp single); (3, OpenMP, my_dev = omp_get_initial_device();); (6, OpenMP, nthreads = omp_get_num_threads();); (10, OpenMP, #pragma omp parallel shared(nthreads))"}
{"predict": "(11,OpenMP,#pragma omp parallel for reduction(+:sum2)), (32,OpenMP,#pragma omp parallel for)", "label": "(7, CUDA, __shared__ float scale;); (5, CUDA, __shared__ float sdata2[THREAD_PER_BLOCK];)"}
{"predict": "(39,OpenMP,#pragma omp parallel for), (42,OpenMP,#pragma omp parallel for), (45,OpenMP,#pragma omp parallel for), (48,OpenMP,#pragma omp parallel for), (51,OpenMP,#pragma omp parallel for)", "label": "(8, OpenMP, #pragma omp teams shared); (61, OpenMP, #pragma omp teams shared(i)); (45, OpenMP, #pragma omp teams shared(e, g, z)); (4, OpenMP, #pragma omp target); (13, OpenMP, #pragma omp teams shared (argc); (51, OpenMP, #pragma omp teams firstprivate(i), shared(i)); (17, OpenMP, #pragma omp teams shared (argc > 0 ? argv[1] : argv[2])); (22, OpenMP, #pragma omp teams shared (S1)); (18, OpenMP, #pragma omp teams shared (argc)); (35, OpenMP, #pragma omp teams shared(da)); (31, OpenMP, #pragma omp teams shared(ca)); (54, OpenMP, #pragma omp teams shared(j)); (23, OpenMP, #pragma omp teams shared (argv[1])); (37, OpenMP, #pragma omp teams shared(h, B::x)); (39, OpenMP, #pragma omp teams private(i)); (-1, OpenMP, #pragma omp teams shared ()); (22, OpenMP, #pragma omp teams shared(ba)); (3, OpenMP, #pragma omp teams shared (argc,); (14, OpenMP, #pragma omp teams shared (a, b, c, d, f)); (-8, OpenMP, #pragma omp teams shared (); (48, OpenMP, #pragma omp teams firstprivate(i))"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(15, OpenMP, #pragma omp target map(l_val)); (9, OpenMP, #pragma omp task); (3, OpenMP, #pragma omp parallel); (4, OpenMP, #pragma omp single)"}
{"predict": "(18,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for), (43,OpenMP,#pragma omp parallel for)", "label": "(8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (41, MPI, mpi_errno = MPI_Comm_free(&stream_comm);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (13, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(36,MPI,nprocs_for_coll = fd->hints->cb_nodes;)\n(44,MPI,if (fd->hints->cb_pfr != ADIOI_HINT_ENABLE) {)\n(60,MPI,if (fd->file_realm_st_offs == NULL) {)\n(66,MPI,ADIOI_Calc_file_realms_aar(fd, nprocs_for_coll, fd->hints->cb_pfr, min_st_offset, max_end_offset, file_realm_st_offs, file_realm_types);)\n(74,MPI,ADIOI_Calc_file_realms_fsize(fd, nprocs_for_coll, max_end_offset, file_realm_st_offs, file_realm_types);)\n(80,MPI,ADIOI_Calc_file_realms_user_size(fd, file_realm_calc_type, nprocs_for_coll, file_realm_st_offs, file_realm_types);)", "label": "(58, MPI, MPI_Type_commit(file_realm_types);); (56, MPI, MPI_Type_contiguous_c((max_end_offset - min_st_offset + 1), MPI_BYTE, file_realm_types);)"}
{"predict": "(37,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for), (63,OpenMP,#pragma omp parallel for)", "label": "(25, OpenCL, OCL_ASSERT(clBuildProgram(program, 1, &device, NULL, NULL, NULL) == CL_SUCCESS);); (26, OpenCL, kernel = clCreateKernel(program, \"compiler_ceil\", &status);)"}
{"predict": "(16,OpenMP,#pragma omp parallel for collapse(2))", "label": "(4, CUDA, CudaCheckError();); (49, CUDA, dynamics.freeCudaMem();)"}
{"predict": "(11,OpenMP,#pragma omp parallel for)", "label": "(18, CUDA, CUDA_CHECK(cudaGetLastError());)"}
{"predict": "(37,OpenMP,#pragma omp parallel for collapse(3)), (55,OpenMP,#pragma omp parallel for collapse(3))", "label": "(39, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(inData->basePtr(), inVec.data(), inBufSize, cudaMemcpyHostToDevice));); (71, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (71, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamDestroy(stream));); (51, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));)"}
{"predict": "(14,OpenMP,#pragma omp parallel for)", "label": "(5, CUDA, cudaSetDevice(0);); (8, CUDA, cudaMemGetInfo(&freeCUDAMem, &totalCUDAMem);)"}
{"predict": "(4,OpenMP,#pragma omp parallel)", "label": "(48, OpenMP, std::cout << \"Number of threads    = \" << omp_get_max_threads() << std::endl;)"}
{"predict": "(12,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(35, MPI, merr = MPI_Unpublish_name(serv_name, MPI_INFO_NULL, port_name);); (24, MPI, merr = MPI_Publish_name(serv_name, MPI_INFO_NULL, port_name);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (25, MPI, MPI_Error_string(merr, errmsg, &msglen);); (41, MPI, merr = MPI_Lookup_name(serv_name, MPI_INFO_NULL, port_name_out);); (64, MPI, MPI_Error_class(merr, &mclass);); (16, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (25, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(3,OpenMP,#pragma omp parallel for), (8,OpenMP,#pragma omp task), (14,OpenMP,#pragma omp task)", "label": "(13, OpenMP, __kmpc_omp_task(nullptr, gtid, task1);); (1, OpenMP, #pragma omp parallel for); (18, OpenMP, __kmpc_omp_task(nullptr, gtid, task2);)"}
{"predict": "(37,OpenMP,#pragma omp parallel for), (64,OpenMP,#pragma omp parallel for)", "label": "(55, TBB, tbb::concurrent_bounded_queue<int> iq4;); (22, TBB, tbb::concurrent_bounded_queue<int> iq2;); (35, TBB, tbb::concurrent_bounded_queue<int> iq3;); (3, TBB, tbb::concurrent_bounded_queue<int> iq1;)"}
{"predict": "(15,OpenMP,#pragma omp parallel for)", "label": "(22, CUDA, cudaFree(res_dev);); (15, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(bool), cudaMemcpyDeviceToHost);); (11, CUDA, cudaMalloc(&res_dev, sizeof(bool));)"}
{"predict": "(28,MPI,*w = MPI_Wtime();)", "label": "(27, OpenMP, auto start_clock = omp_get_wtime();); (44, OpenMP, auto stop_clock = omp_get_wtime();)"}
{"predict": "(10,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for)", "label": "(4, OpenMP, #pragma omp target teams distribute simd firstprivate(g, g1, sivar))"}
{"predict": "(18, CUDA, <<< dim3(1,1), dim3(N, 1) >>>)\n(35, CUDA, <<< dim3(1,1), dim3(N, 1) >>>)", "label": "(19, CUDA, ASSERT_EQ(cudaGetLastError(), cudaSuccess) << \"Kernel launch error.\";)"}
{"predict": "(26,OpenMP,#pragma omp parallel for collapse(2))", "label": "(23, OpenCL, kernel = clCreateKernel(program, \"simplemov\", &status);); (20, OpenCL, OCL_ASSERT(clBuildProgram(program, 1, &device, NULL, NULL, NULL) == CL_SUCCESS);)"}
{"predict": "(6,OpenMP,#pragma omp parallel for)", "label": "(2, CUDA, cudaGraphicsMapResources(1, &cuda_pbo_resource, 0);); (7, CUDA, cudaGraphicsUnmapResources(1, &cuda_pbo_resource, 0);)"}
{"predict": "(9,OpenMP,#pragma omp parallel for)", "label": "(3, OpenMP, #pragma omp target map(tofrom: threadCount)); (4, OpenMP, #pragma omp metadirective)"}
{"predict": "(3,MPI,int rank = MPI_Comm_rank(MPI_COMM_WORLD);) (5,MPI,MPI_Init(NULL, NULL);) (12,MPI,MPI_Finalize();) (8,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp critical)", "label": "(5, MPI, MPI_Barrier(MPI_COMM_WORLD);); (5, MPI, double start = MPI_Wtime();); (12, MPI, double end = MPI_Wtime();); (0, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(10,MPI,MPI_Init(&argc, &argv);), (15,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);), (16,MPI,MPI_Comm_size(MPI_COMM_WORLD, &size);), (21,MPI,MPI_Isend(&send_value, 1, MPI_INT, (rank + 1) % size, 4711, MPI_COMM_WORLD, &send_request);), (27,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for)", "label": "(9, MPI, MPI_Init(&argc, &argv);); (18, MPI, MPI_Finalize();); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (31, MPI, MPI_Wait(&recv_request, &status);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (17, MPI, MPI_Isend(&send_value, 1, MPI_INT, (rank + 1) % size, 4711, MPI_COMM_WORLD, &send_request);); (27, MPI, MPI_Wait(&send_request, &status);)"}
{"predict": "(27,OpenMP,#pragma omp parallel for)", "label": "(33, OpenCL, clReleaseEvent(retEvent);)"}
{"predict": "(6,MPI,*ensemble = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (11,MPI,*ensemble = MPI_Wtime();); (16,MPI,*ensemble = MPI_Wtime(););", "label": "(16, CUDA, EXPECT_NO_THROW(ensemble = new flamegpu::CUDAEnsemble(model, sizeof(argv) / sizeof(char*), argv));); (6, CUDA, EXPECT_NO_THROW(ensemble = new flamegpu::CUDAEnsemble(model, 0, nullptr));)"}
{"predict": "(6,MPI,*w = MPI_Wtime(););", "label": "(2, MPI, MPI_Comm cComm = MPI_Comm_f2c(*comm);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wSize);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wRank);); (5, MPI, MPI_Comm_rank(cComm, &cRank);); (3, MPI, MPI_Comm_size(cComm, &cSize);)"}
{"predict": "i < opts.N; i++) {\n139:       if (opts.test_method == tmBatched) {\n140:         cublasSetStream(handle, streamArray[i]);\n141:         status1 = cublasGemmBatched<T_ELEM>(handle, CUBLAS_OP_N, CUBLAS_OP_N,\n142:                                            rowsA, colsB, colsA, &params.alpha,\n143:                                            devPtrA_dev, rowsA, devPtrB_dev, rowsB,\n144:                                            &params.beta, devPtrC_dev, rowsC, opts.N);\n145:       } else {\n146:         cublasSetStream(handle, streamArray[i]);\n147:         status1 = cublasGemm<T_ELEM>(handle, CUBLAS_OP_N, CUBLAS_OP_N, rowsA, colsB,\n148:                                    colsA, &params.alpha, devPtrA[i], rowsA, devPtrB[i],\n149:                                    rowsB, &params.beta, devPtrC[i], rowsC);\n150:       }\n151: \n152:       if (status1 != CUBLAS_STATUS_SUCCESS) {\n153:         CLEANUP();\n154:         fprintf(stderr, \"!!!! GEMM error %d\\n\", status1);\n155:         return CUBLASTEST_FAILED;\n156:       }\n157:     }\n158: \n159:     start = MPI_Wtime();\n160:     for (int i = 0; i < opts.N; i++) {\n161:       if (opts.test_method == tmBatched) {\n162:         status1 = cublasGemmBatched<T_ELEM>(handle, CUBLAS_OP_N, CUBLAS_OP_N,\n163:                                            rowsA, colsB, colsA, &params.alpha,\n164:                                            devPtrA_dev, rowsA, devPtrB_dev, rowsB,\n165:                                            &params.beta, devPtrC_dev, rowsC, opts.N);\n166:       } else {\n167:         status1 = cublasGemm<T_ELEM>(handle, CUBLAS_OP_N, CUBLAS_OP_N, rowsA, colsB,\n168:                                    colsA, &params.alpha, dev", "label": "(193, CUDA, cudaError_t cudaStatus = cudaDeviceSynchronize();); (103, CUDA, cudaError_t cudaErr = cudaStreamCreate(&streamArray[i]);); (163, CUDA, cudaError_t cudaStatus = cudaGetLastError();)"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(19, MPI, *ierr = MPI_Info_get_valuelen( (MPI_Info)(*v1), p2, v3, &l4 );)"}
{"predict": "ADIOI_Malloc(nprocs_send * sizeof(MPI_Request));\n141:         send_req = requests;\n142:     }\n143: \n144:     /* Post the receives */\n145:     if (nprocs_recv) {\n146:         ADIOI_Recv_contig(buf, recv_types, recv_size, curr_to_proc, requests, statuses, nprocs_recv, &err);\n147:         /* --BEGIN ERROR HANDLING-- */\n148:         if (err != MPI_SUCCESS) {\n149:             *error_code = MPIO_Err_create_code(err,\n150:                                                    MPIR_ERR_RECOVERABLE, myname,\n151:                                                    __LINE__, MPI_ERR_IO, \"**ioRecvContig\", 0);\n152:             return;\n153:         }\n154:         /* --END ERROR HANDLING-- */\n155:     }\n156: \n157:     /* Post the sends */\n158:     if (nprocs_send) {\n159:         ADIOI_Send_contig(buf, send_buf, send_size, curr_to_proc, send_req, requests, nprocs_send, &err);\n160:         /* --BEGIN ERROR HANDLING-- */\n161:         if (err != MPI_SUCCESS) {\n162:             *error_code = MPIO_Err_create_code(err,\n163:                                                    MPIR_ERR_RECOVERABLE, myname,\n164:                                                    __LINE__, MPI_ERR_IO, \"**ioSendContig\", 0);\n165:             return;\n166:         }\n167:         /* --END ERROR HANDLING-- */\n168:     }\n169: \n170:     /* Wait for the receives */\n171:     if (nprocs_recv) {\n172:         MPI_Waitall(nprocs_recv, requests, statuses);\n173:     }\n174: \n175:     /* Wait for the sends */\n176:     if (nprocs_send) {\n177:         MPI_Waitall(nprocs_send, requests, statuses);\n178:     }\n179: \n180:     /* Free recv_types */\n181:     ADIOI_Free(recv_types);\n182: \n183:", "label": "(65, MPI, MPI_Type_commit(recv_types + j);); (30, MPI, MPI_Alltoall(recv_size, 1, MPI_COUNT, send_size, 1, MPI_COUNT, fd->comm);)"}
{"predict": "(9,OpenMP,#pragma omp parallel for reduction(+:cEvents)), (13,OpenMP,#pragma omp parallel for reduction(+:cEvents))", "label": "(3, CUDA, cudaFree( 0 );)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (38,OpenMP,#pragma omp parallel for)", "label": "(52, CUDA, CHECK_CUDA(cudaFreeHost(hIndices));); (15, CUDA, CHECK_CUDA(cudaMallocHost(&hIndicesRef, ySize * sizeof(uint32_t)));); (19, CUDA, CHECK_CUDA(cudaMalloc(&dx, xSize * sizeof(float)));); (50, CUDA, CHECK_CUDA(cudaFreeHost(hIndicesRef));); (8, CUDA, CHECK_CUDA(cudaMallocHost(&hx, xSize * sizeof(float)));); (-3, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (44, CUDA, CHECK_CUDA(cudaFreeHost(hx));); (42, CUDA, CHECK_CUDA(cudaFree(dIndices));); (40, CUDA, CHECK_CUDA(cudaFree(dx));); (4, CUDA, CHECK_CUDA(cudaMallocHost(&hyRef, ySize * sizeof(float)));); (5, CUDA, CHECK_CUDA(cudaMallocHost(&hIndices, ySize * sizeof(uint32_t)));); (40, CUDA, CHECK_CUDA(cudaFreeHost(hyRef));); (11, CUDA, CHECK_CUDA(cudaMalloc(&dIndices, ySize * sizeof(uint32_t)));)"}
{"predict": "(14,OpenMP,#pragma omp parallel for), (39,OpenMP,#pragma omp parallel for)", "label": "(4, CUDA, cudaExtent extent = make_cudaExtent(4, 5, 6);)"}
{"predict": "(10,MPI,*w = MPI_Wtime();); (34,OpenMP,#pragma omp parallel for), (78,MPI,nstream_time = prk::wtime() - nstream_time;);", "label": "(39, SYCL, sycl::accessor B(d_B, h, sycl::read_only);); (37, SYCL, sycl::accessor A(d_A, h);); (38, SYCL, sycl::accessor C(d_C, h, sycl::read_only);); (81, SYCL, prk::SYCL::print_exception_details(e);)"}
{"predict": "()", "label": "(2, CUDA, LogDebug(LOG_PY_UTILS \"PyCudaMemory_New()\\n\");)"}
{"predict": "(49,OpenMP,#pragma omp parallel for), (76,OpenMP,#pragma omp parallel for)", "label": "(3, MPI, MPI_Init(nullptr, nullptr);); (100, MPI, MPI_Finalize();); (3, MPI, dray::dray::mpi_comm(MPI_Comm_c2f(comm));)"}
{"predict": "(17,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for)", "label": "(9, SYCL, sycl::queue handle = getSYCLDefaultDeviceDefaultQueue();)"}
{"predict": "(13,OpenMP,#pragma omp parallel for)", "label": "(24, OpenMP, double *dbl_min = (double*) omp_target_alloc(sizeof(double)*1, omp_get_default_device() );); (22, OpenMP, int *max = (int*) omp_target_alloc(sizeof(int)*1, omp_get_default_device() );); (23, OpenMP, double *dbl_max = (double*) omp_target_alloc(sizeof(double)*1, omp_get_default_device() );); (33, OpenMP, #pragma omp atomic compare); (6, OpenMP, #pragma omp target teams distribute parallel for); (38, OpenMP, #pragma omp atomic compare hint(ompx_fast_fp_atomics)); (50, OpenMP, #pragma omp target teams distribute parallel for  map(tofrom:min2,max2))"}
{"predict": ".input_size.w()\n167:       << \"_\"\n168:       << options.filter_size.n() << \"x\" << options.filter_size.h() << \"x\" << options.filter_size.w()\n169:       << \"_\"\n170:       << options.conv_stride.n() << \"x\" << options.conv_stride.h() << \"x\" << options.conv_stride.w()\n171:       << \"_\"\n172:       << options.dilation.n() << \"x\" << options.dilation.h() << \"x\" << options.dilation.w()\n173:       << \"_\"\n174:       << options.output_size.n() << \"x\" << options.output_size.h() << \"x\" << options.output_size.w()\n175:       << \".bin\";\n176: \n177:     std::string filename = ss.str();\n178: \n179:     std::ofstream file(filename, std::ios::binary);\n180: \n181:     if (file.is_open()) {\n182:       file.write(reinterpret_cast<char*>(workspace.get()), workspace_size);\n183:       file.close();\n184:     }\n185:   }\n186: \n187:   return result;\n188: }", "label": "(202, CUDA, result.error = cudaEventRecord(events[0]);); (223, CUDA, std::cerr << \"cudaEventSynchronize() failed: \" << cudaGetErrorString(result.error) << std::endl;); (230, CUDA, std::cerr << \"cudaEventElapsed() failed: \" << cudaGetErrorString(result.error) << std::endl;); (201, CUDA, std::cerr << \"cudaEventRecord() failed: \" << cudaGetErrorString(result.error) << std::endl;); (211, CUDA, result.error = cudaEventRecord(events[1]);); (237, CUDA, (void)cudaEventDestroy(event);); (190, CUDA, std::cerr << \"cudaEventCreate() failed: \" << cudaGetErrorString(result.error) << std::endl;); (223, CUDA, result.error = cudaEventElapsedTime(&runtime_ms, events[0], events[1]);); (214, CUDA, result.error = cudaEventSynchronize(events[1]);); (185, CUDA, result.error = cudaEventCreate(&event);)"}
{"predict": "(11,MPI,*w = MPI_Wtime();); (17,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for), (38,OpenMP,#pragma omp parallel for)", "label": "(9, MPI, MPI_Init(&argc, &argv);); (25, MPI, MPI_Finalize();); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (30, MPI, MPI_Isend(&send_value, 9, MPI_INT, (rank + 1) % size, 4711, MPI_COMM_WORLD, &send_request);); (34, MPI, MPI_Wait(&recv_request, &status);); (32, MPI, MPI_Wait(&send_request, &status);)"}
{"predict": "= write(fd->fd_sys,\n132:                                 (char *) buf + (ADIO_Offset) j * (ADIO_Offset) buftype_extent +\n133:                                 flat_buf->indices[i], flat_buf->blocklens[i]);\n134: #ifdef ADIOI_MPE_LOGGING\n135:                     MPE_Log_event(ADIOI_MPE_write_b, 0, NULL);\n136: #endif\n137:                     if (err == -1)\n138:                         err_flag = 1;\n139:                 } else {\n140:                     /* copy the data from the input buffer into the combine buffer */\n141:                     ADIOI_Assert(flat_buf->blocklens[i] == (unsigned) flat_buf->blocklens[i]);\n142:                     ADIOI_Assert((((ADIO_Offset) (uintptr_t) buf) +\n143:                                   (ADIO_Offset) j * (ADIO_Offset) buftype_extent +\n144:                                   flat_buf->indices[i]) ==\n145:                                  (ADIO_Offset) ((uintptr_t) buf +\n146:                                                 (ADIO_Offset) j * (ADIO_Offset) buftype_extent +\n147:                                                 flat_buf->indices[i]));\n148:                     memcpy(combine_buf_ptr, (char *) buf + (ADIO_Offset) j * (ADIO_Offset) buftype_extent +\n149:                            flat_buf->indices[i], flat_buf->blocklens[i]);\n150:                     combine_buf_ptr += flat_buf->blocklens[i];\n151:                     combine_buf_remain -= flat_buf->blocklens[i];\n152:                 }\n153:             }\n154:         }\n155: \n156:         /* write out the last bit of the combine buffer */\n157:         if (combine_buf_ptr != combine_buf) {\n158: #ifdef IO_DEBUG\n159:             printf(\"[%d/%d] nc mem c file (2) writing loc = %Ld sz = %Ld\\n\",\n160:                    rank, nprocs, off, combine_buf_ptr - combine_buf);\n161: #endif\n162: #ifdef ADIOI_MPE_LOGGING\n163:             MPE_Log_event(ADIOI_MPE_write_a, 0, NULL);\n164", "label": "(44, MPI, MPI_Type_size_x(fd->filetype, &filetype_size);); (52, MPI, MPI_Comm_rank(fd->comm, &rank);); (52, MPI, MPI_Comm_size(fd->comm, &nprocs);); (56, MPI, MPI_Type_get_extent(datatype, &lb, &buftype_extent);); (54, MPI, MPI_Type_size_x(datatype, &buftype_size);)"}
{"predict": "\\n\", iterations);\n125: \n126:   }\n127: \n128: ENDOFTESTS:\n129: \n130:   if (error) {\n131:     MPI_Finalize();\n132:     return 1;\n133:   }\n134: \n135:   /* broadcast input parameters to all ranks */\n136:   MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);\n137:   MPI_Bcast(&size, 1, MPI_INT64_T, root, MPI_COMM_WORLD);\n138:   MPI_Bcast(&size2, 1, MPI_INT64_T, root, MPI_COMM_WORLD);\n139:   MPI_Bcast(&radius, 1, MPI_INT, root, MPI_COMM_WORLD);\n140:   MPI_Bcast(&sparsity, 1, MPI_DOUBLE, root, MPI_COMM_WORLD);\n141: \n142:   /* allocate vectors and matrix */\n143:   vector_space = (size2+1)*sizeof(double);\n144:   matrix_space = size2*(2*radius+1)*sizeof(double);\n145:   index_space = size2*(2*radius+1)*sizeof(s64Int);\n146:   matrix = (double *) prk_malloc(matrix_space);\n147:   vector = (double *) prk_malloc(vector_space);\n148:   result = (double *) prk_malloc(vector_space);\n149:   colIndex = (s64Int *) prk_malloc(index_space);\n150: \n151:   /* initialize vector */\n152:   for (i=0; i<size2; i++){\n153:     vector[i] = 1.0;\n154:   }\n155: \n156:   /* initialize matrix */\n157:   elm_start = 0;\n158:   for (row=0; row<size; row++){\n159:     for (col=0; col<size; col++){\n160:       elm = elm_start + (col+row*size);\n161:       colIndex[elm] = elm;\n162:       matrix[elm] = 1.0;\n163:       n", "label": "(146, MPI, MPI_Bcast(&size2,      1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (146, MPI, MPI_Bcast(&radius,     1, MPI_INT,           root, MPI_COMM_WORLD);); (303, MPI, MPI_Finalize();); (141, MPI, MPI_Bcast(&lsize2,     1, MPI_INT,           root, MPI_COMM_WORLD);); (139, MPI, MPI_Bcast(&lsize,      1, MPI_INT,           root, MPI_COMM_WORLD);); (140, MPI, MPI_Bcast(&size,       1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (277, MPI, MPI_Reduce(&vector_sum, &check_sum, 1, MPI_DOUBLE, MPI_SUM, root, MPI_COMM_WORLD);); (44, MPI, MPI_Init(&argc,&argv);); (230, MPI, MPI_Barrier(MPI_COMM_WORLD);); (44, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (138, MPI, MPI_Bcast(&iterations, 1, MPI_INT,           root, MPI_COMM_WORLD);); (41, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "i)\n123: \t\t\t\t\t{\n124: #ifdef _OPENMP\n125: #pragma omp parallel for\n126: #endif\n127: \t\t\t\t\t\tDynamicFunc__CryptInput2OverwriteInput1OffsetX(0, i, 0);\n128: \t\t\t\t\t}\n129: \t\t\t\t}\n130: \t\t\t}\n131: \t\t}\n132: \t}\n133: \n134: \t// The actual work happens here, and it is parallelized using OpenMP.\n135: \t// #pragma omp parallel for\n136: \t// {\n137: \t// \tfor (int i = 0; i < m_count; ++i)\n138: \t// \t{\n139: \t// \t\tint idx = i * salt->count;\n140: \t// \t\tint idx2 = idx + salt->count;\n141: \t// \t\tstruct crypt_data data;\n142: \t// \t\tdata.out = out[idx2];\n143: \t// \t\tdata.in = in[idx];\n144: \t// \t\tdata.count0 = data.count1 = salt->count;\n145: \t// \t\tdata.salt = salt->salt;\n146: \t// \t\tdata.rounds = salt->rounds;\n147: \t// \t\tdata.flags = salt->flags;\n148: \t// \t\tcrypt_core(&data);\n149: \t// \t}\n150: \t// }\n151: \t// The work is commented out, but the OpenMP directive is left in place.\n152: \t// Uncommenting it would allow the work to be parallelized.\n153: \n154: \treturn crypt_count;\n155: }", "label": "(251, OpenMP, #pragma omp parallel for shared(curdat, inc, m_count))"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for), (35,OpenMP,#pragma omp parallel for), (39,OpenMP,#pragma omp parallel for), (43,OpenMP,#pragma omp parallel for), (47,OpenMP,#pragma omp parallel for), (56,OpenMP,#pragma omp parallel for)", "label": "(21, OpenMP, #pragma omp target parallel for simd collapse(, )); (54, OpenMP, #pragma omp target parallel for simd collapse(4, , 4)); (34, OpenMP, #pragma omp target parallel for simd collapse(4,); (79, OpenMP, #pragma omp target parallel for simd collapse(0)); (46, OpenMP, #pragma omp target parallel for simd collapse(4 4)); (85, OpenMP, #pragma omp target parallel for simd collapse(2) firstprivate(i)); (72, OpenMP, #pragma omp target parallel for simd collapse(-5)); (6, OpenMP, #pragma omp target parallel for simd collapse()); (-3, OpenMP, #pragma omp target parallel for simd collapse); (57, OpenMP, #pragma omp target parallel for simd collapse(4, 8)); (7, OpenMP, #pragma omp target parallel for simd collapse(,); (63, OpenMP, #pragma omp target parallel for simd collapse(foo())); (74, OpenMP, #pragma omp target parallel for simd collapse(5 - 5)); (57, OpenMP, #pragma omp target parallel for simd collapse(2.5)); (17, OpenMP, #pragma omp target parallel for simd collapse(4); (11, OpenMP, #pragma omp target parallel for simd collapse 4)); (-7, OpenMP, #pragma omp target parallel for simd collapse(); (24, OpenMP, #pragma omp target parallel for simd collapse(4, )); (27, OpenMP, #pragma omp target parallel for simd collapse(4))"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (12,OpenMP,#pragma omp parallel for), (...)", "label": "(7, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)"}
{"predict": "}\n153: \tcleanup<T, T>(h_in_signal, d_in_signal, h_output_signal, d_output_signal, outPlace);\n154: \treturn bTestResult;\n155: }", "label": "(83, CUDA, cudaDeviceSynchronize();); (20, CUDA, checkCudaErrors(cudaMalloc((void **)&d_output_signal, mem_size));); (73, CUDA, checkCudaErrors(cufftExecZ2Z(plan, (cufftDoubleComplex *)d_in_signal, (cufftDoubleComplex *)d_output_signal, direction));); (69, CUDA, checkCudaErrors(cufftExecC2C(plan, (cufftComplex *)d_in_signal, (cufftComplex *)d_output_signal, direction));); (155, CUDA, checkCudaErrors(cufftDestroy(plan));); (9, CUDA, checkCudaErrors(cudaMalloc((void **)&d_in_signal, mem_size));); (163, CUDA, cudaDeviceReset();)"}
{"predict": "(27,OpenMP,#pragma omp parallel for)", "label": "(48, CUDA, CHECK(cudaFree(d_y));); (26, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice));); (47, CUDA, CHECK(cudaFree(d_z));); (35, CUDA, CHECK(cudaMemcpy(h_z, d_z, M, cudaMemcpyDeviceToHost));); (18, CUDA, CHECK(cudaMalloc((void**)&d_x, M));); (18, CUDA, CHECK(cudaMalloc((void**)&d_y, M));); (41, CUDA, CHECK(cudaFree(d_x));); (17, CUDA, CHECK(cudaMalloc((void**)&d_z, M));); (26, CUDA, CHECK(cudaGetLastError());); (26, CUDA, CHECK(cudaDeviceSynchronize());); (18, CUDA, CHECK(cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice));)"}
{"predict": "(17,OpenMP,#pragma omp parallel for), (46,OpenMP,#pragma omp parallel for)", "label": "(5, MPI, MPI_Get_library_version(version, &versionlen);)"}
{"predict": "(6,OpenMP,#pragma omp parallel for reduction(+:count))", "label": "(6, OpenMP, #pragma omp for reduction(+:count) lastprivate(obj)); (9, OpenMP, #pragma omp cancel for); (2, OpenMP, #pragma omp parallel private(i) num_threads(1))"}
{"predict": "(34,OpenMP,#pragma omp parallel for), (36,OpenMP,#pragma omp parallel for)", "label": "(31, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&device_buffer, size_bytes));); (32, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(device_buffer, input.data(), size_bytes, cudaMemcpyHostToDevice));); (41, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(device_buffer));); (35, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (33, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (37, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(output.data(), device_buffer, size_bytes, cudaMemcpyDeviceToHost));)"}
{"predict": "} else {\n130:                 off = fd->disp + (fd->etype_size) * fd->datarep->fd->etype_size * offset;\n131:             }\n132:             ADIOI_FOP(fd, buf, count, datatype, off, status, error_code);\n133:         } else {\n134:             ADIOI_FOP(fd, buf, count, datatype, offset, status, error_code);\n135:         }\n136:         return;\n137:     }\n138: \n139:     /* setup the aggregated file view */\n140:     ADIOI_AGGREGATE(fd, buf, count, datatype, file_ptr_type, offset, &agg_disp,\n141:                     &agg_dtype, &agg_rank, &is_contig, &nprocs_for_coll);\n142: \n143:     /* allocate memory for the collective comm. datatypes and sizes */\n144:     client_comm_dtype_arr = (MPI_Datatype *)\n145:         ADIOI_Malloc(nprocs_for_coll * sizeof(MPI_Datatype));\n146:     client_comm_sz_arr = (ADIO_Offset *)\n147:         ADIOI_Malloc(nprocs_for_coll * sizeof(ADIO_Offset));\n148:     agg_comm_dtype_arr = (MPI_Datatype *)\n149:         ADIOI_Malloc(nprocs_for_coll * sizeof(MPI_Datatype));\n150:     agg_comm_sz_arr = (ADIO_Offset *)\n151:         ADIOI_Malloc(nprocs_for_coll * sizeof(ADIO_Offset));\n152: \n153:     /* set up the communication datatypes and sizes for the collective comm. */\n154:     ADIOI_Setup_comm_datatypes(fd, datatype, agg_dtype, file_ptr_type, offset,\n155:                                &client_comm_dtype_arr, &client_comm_sz_arr,\n156:                                &agg_comm_dtype_arr, &agg_comm_sz_arr);\n157: \n158:     /* setup the collective comm. buffers */\n159:     ADIOI_AGGREGATE_BUF(fd, buf, count, datatype, file_ptr_type, offset, &cb_buf);\n160: \n161:     /* setup the collective comm. requests */\n162:     ADIOI_AGGREG", "label": "(101, MPI, MPI_Allgather(st_end_offset, 2, ADIO_OFFSET, all_st_end_offsets, 2, ADIO_OFFSET, fd->comm);); (66, MPI, MPI_Comm_rank(fd->comm, &myrank);); (522, MPI, MPI_Type_free(&client_comm_dtype_arr[i]);); (582, MPI, MPI_Type_free(&fd->file_realm_types[0]);); (62, MPI, MPI_Comm_size(fd->comm, &nprocs);); (584, MPI, MPI_Type_free(&fd->file_realm_types[i]);); (150, MPI, MPI_Type_get_extent(datatype, &lb, &extent);); (302, MPI, MPI_Type_size_x(agg_comm_dtype_arr[i], &size);); (296, MPI, MPI_Type_free(&agg_dtype);)"}
{"predict": "(15,OpenMP,#pragma omp parallel for reduction(+:errs)), (20,OpenMP,#pragma omp parallel for reduction(+:errs)), (25,OpenMP,#pragma omp parallel for reduction(+:errs)), (30,OpenMP,#pragma omp parallel for reduction(+:errs)), (35,OpenMP,#pragma omp parallel for reduction(+:errs))", "label": "(13, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(11,MPI,MPI_Win_fence(0, win);)", "label": "(12, MPI, MPI_Win_unlock(0, win);); (9, MPI, MPI_Accumulate(one, nlevels, MPI_INT, 0, 0, 1, acc_type, MPI_SUM, win);); (9, MPI, MPI_Get(get_array, nlevels, MPI_INT, 0, 0, 1, get_type, win);); (6, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, 0, 0, win);)"}
{"predict": "(38,MPI,*w = MPI_Wtime();); (50,OpenMP,#pragma omp parallel for), (73,OpenMP,#pragma omp parallel for), (98,OpenMP,#pragma omp parallel for reduction(+:asum))", "label": "(31, OpenMP, std::cout << \"OpenMP default device = \" << omp_get_default_device() << std::endl;)"}
{"predict": "(11,OpenMP,#pragma omp parallel for)", "label": "(18, CUDA, cudaFree(res_dev);); (8, CUDA, cudaMalloc(&res_dev, sizeof(float));); (10, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(30,OpenMP,#pragma omp parallel for)\n(36,OpenMP,#pragma omp parallel for)", "label": "(29, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (37, TBB, reduce_time = (tbb::tick_count::now() - t0).seconds();); (43, TBB, graph_time = (tbb::tick_count::now() - t0).seconds();); (28, TBB, serial_time = (tbb::tick_count::now() - t0).seconds();)"}
{"predict": "(13,OpenMP,#pragma omp parallel for)", "label": "(6, CUDA, cudaMalloc((void**) &raw_ptr, N * sizeof(int));); (11, CUDA, thrust::fill(dev_ptr, dev_ptr + N, (int) 0);); (7, CUDA, thrust::device_ptr<int> dev_ptr = thrust::device_pointer_cast(raw_ptr);); (15, CUDA, cudaFree(raw_ptr);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for reduction(min:minx,miny,minz):shared(celmin),private(cell,px,py,pz))\n(26,OpenMP,minx=min(pminh.x,px); miny=min(pminh.y,py); minz=min(pminh.z,pz);)\n(27,OpenMP,maxx=max(pmaxh.x,px); maxy=max(pmaxh.y,py); maxz=max(pmaxh.z,pz);)", "label": "(19, CUDA, cudaMemcpy(poscellh,poscell+pini,sizeof(float4)*np,cudaMemcpyDeviceToHost);); (19, CUDA, cudaMemcpy(checkh,check+pini,sizeof(byte)*np,cudaMemcpyDeviceToHost);)"}
{"predict": "(15,OpenMP,#pragma omp parallel for)", "label": "(29, MPI, Mpi.SignalProgress();)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (50,OpenMP,#pragma omp parallel for), (76,OpenMP,#pragma omp parallel for)", "label": "(3, OpenMP, #pragma omp parallel for default(none) private(idx) shared(count, sapH_cur_salt, saved_plain, crypt_key))"}
{"predict": "(2,MPI,MPI_Init(&argc, &argv);) (9,MPI,if (MPI_Comm_rank(MPI_COMM_WORLD, &rank) != MPI_SUCCESS)) (13,MPI,rc = SCR_Init();) (23,MPI,if (rank == 0))", "label": "(23, MPI, rc = MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (32, MPI, MPI_Finalize();); (6, MPI, rc = MPI_Init(NULL, NULL);)"}
{"predict": "(10,MPI,*w = MPI_Wtime();); (24,OpenMP,#pragma omp parallel for), (47,OpenMP,#pragma omp parallel for)", "label": "(9, CUDA, uintptr_t iov_unpack_threshold = yaksuri_cudai_get_iov_unpack_threshold(info);); (29, CUDA, cudaMemcpyAsync(iov[i].iov_base, sbuf, iov[i].iov_len, cudaMemcpyDefault, stream);); (16, CUDA, YAKSURI_CUDAI_CUDA_ERR_CHKANDJUMP(cerr, rc, fn_fail);); (52, CUDA, cerr = cudaSetDevice(cur_device);); (42, CUDA, cerr = cudaGetDevice(&cur_device);); (32, CUDA, rc = yaksuri_cudai_md_alloc(type);); (43, CUDA, cerr = cudaSetDevice(target);)"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(15, OpenMP, #pragma omp target parallel for)"}
{"predict": "(17,OpenMP,#pragma omp parallel for reduction(+:dbias_shared[:C],dweight_shared[:C])),(45,OpenMP,#pragma omp parallel for reduction(+:dnorm_mean,dnorm_norm_mean)),(69,OpenMP,#pragma omp parallel for reduction(+:dbias[i],dweight[i]))", "label": "(4, CUDA, extern __shared__ float shared[];)"}
{"predict": "(15,OpenMP,#pragma omp parallel for)", "label": "(13, OpenMP, #pragma omp parallel for)"}
{"predict": "(16,OpenMP,#pragma omp parallel for)", "label": "(31, TBB, generate_test<parallel_test, tbb::flow::tuple<float, double, int, double, double, long, int, float, long> >::do_test();); (27, TBB, generate_test<parallel_test, tbb::flow::tuple<float, int, double, float, long, float, long> >::do_test();); (23, TBB, generate_test<parallel_test, tbb::flow::tuple<double, double, int, int, short> >::do_test();); (15, TBB, generate_test<serial_test, tbb::flow::tuple<float, double, int, double, double, float, long, int, float, long> >::do_test();); (3, TBB, generate_test<serial_test, tbb::flow::tuple<float, double> >::do_test();); (4, TBB, generate_test<serial_test, tbb::flow::tuple<float, double, int, long> >::do_test();); (16, TBB, generate_test<parallel_test, tbb::flow::tuple<float, int, long> >::do_test();); (5, TBB, generate_test<serial_test, tbb::flow::tuple<double, double, int, long, int, short> >::do_test();); (7, TBB, generate_test<serial_test, tbb::flow::tuple<float, double, double, double, float, int, float, long> >::do_test();)"}
{"predict": "(11,MPI,MPI_Comm_rank(comm, &rank); MPI_Comm_size(comm, &nproc);)", "label": "(25, MPI, MPI_Isend(&out_buf[elems * i], elems, MPI_FLOAT, i, 0, comm, &reqs[i + nproc]);); (20, MPI, MPI_Irecv(&in_buf[elems * i], elems, MPI_FLOAT, i, 0, comm, &reqs[i]);); (11, MPI, MPI_Comm_size(comm, &nproc);); (9, MPI, MPI_Comm_rank(comm, &rank);); (24, MPI, MPI_Waitall(nproc * 2, reqs, MPI_STATUSES_IGNORE);)"}
{"predict": "(32,OpenMP,#pragma omp parallel for)", "label": "(22, CUDA, PyErr_SetString(PyExc_Exception, LOG_PY_UTILS \"cudaOverlay() failed to get input/output image pointers (should be cudaImage)\");); (32, CUDA, PYCUDA_ASSERT_NOGIL(cudaOverlay(input->base.ptr, input->width, input->height, output->base.ptr, output->width, output->height, output->format, x, y, stream));); (26, CUDA, PyErr_SetString(PyExc_Exception, LOG_PY_UTILS \"cudaOverlay() input and output image formats are different\");)"}
{"predict": "(12,OpenMP,#pragma omp parallel for reduction(+:local[id]))", "label": "(3, CUDA, __shared__ float local[threads];)"}
{"predict": "(15,OpenMP,#pragma omp parallel for private(dx, dy, dz, r, ig2))", "label": "(33, OpenMP, #pragma omp atomic); (15, OpenMP, #pragma omp target teams distribute parallel for collapse(2) private(dx, dy, dz, r, ig2))"}
{"predict": "(33,OpenMP,#pragma omp parallel for collapse(3))", "label": "(19, OpenMP, #pragma omp for private(k)); (37, OpenMP, #pragma omp for private ( k)); (66, OpenMP, #pragma omp for private (k))"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (14,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for)", "label": "(8, MPI, t1 = MPI_Wtime();); (6, MPI, MPI_Init(&argc, &argv);); (38, MPI, MPI_Finalize();); (31, MPI, tick = MPI_Wtick();); (5, MPI, t2 = MPI_Wtime();)"}
{"predict": "(30,MPI,*w = MPI_Wtime();); (32,OpenMP,#pragma omp parallel for), (50,OpenMP,#pragma omp parallel for)", "label": "(29, OpenCL, err = clGetDeviceInfo(device, CL_DEVICE_EXTENSIONS, 0, NULL, &len);); (42, OpenCL, err = clGetDeviceInfo(device, CL_DEVICE_EXTENSIONS, len, extensions, NULL);)"}
{"predict": "(3,MPI,MPI_Init(&argc, &argv);) (5,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &comm.rank);) (5,MPI,MPI_Comm_size(MPI_COMM_WORLD, &comm.size);) (126,MPI,MPI_Send(&X[domain[i]], domain[i+1] - domain[i], MPI_DOUBLE, 0, 42, comm);)", "label": "(2, MPI, MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided);); (136, MPI, MPI_Send(x.data(), chunk, MPI_DOUBLE, 0, 42, comm);); (2, MPI, MPI_Finalize();); (65, MPI, MPI_Allgather(&chunk, 1, MPI_INT, &domain[1], 1, MPI_INT, comm);)"}
{"predict": "MPI_Attr_put(lo_comm, MPI_TAG_UB, &value); */\n109:         /* MPI_Attr_get(lo_comm, MPI_TAG_UB, &vvalue, &flag); */\n110:         /* MPI_Attr_delete(lo_comm, MPI_TAG_UB); */\n111:         /* MPI_Attr_put(lo_comm, MPI_TAG_UB, &value); */\n112:         /* MPI_Attr_get(lo_comm, MPI_TAG_UB, &vvalue, &flag); */\n113:         /* MPI_Attr_delete(lo_comm, MPI_TAG_UB); */\n114:         /* MPI_Attr_put(lo_comm, MPI_TAG_UB, &value); */\n115:         /* MPI_Attr_get(lo_comm, MPI_TAG_UB, &vvalue, &flag); */\n116:         /* MPI_Attr_delete(lo_comm, MPI_TAG_UB); */\n117:     }\n118: \n119:     MPI_Comm_dup(lo_comm, &dup_comm);\n120:     if (lo_comm != MPI_COMM_NULL) {\n121:         MPI_Comm_free(&lo_comm);\n122:     }\n123: \n124:     if (dup_comm != MPI_COMM_NULL) {\n125:         MPI_Comm_free(&dup_comm);\n126:     }\n127: \n128:     /*\n129:      * Check Comm_split by splitting world_comm into two equal groups\n130:      */\n131: #ifdef DEBUG\n132:     if (world_rank == 0) {\n133:         printf(\"    Comm_split\\n\");\n134:         fflush(stdout);\n135:     }\n136: #endif\n137: \n138:     color = world_rank % 2;\n139:     key = world_rank;\n140:     MPI_Comm_split(world_comm, color, key, &split_comm);\n141:     if (color == 0) {\n142:         MPI_Comm_free(&split_comm);\n143:     }\n144:     MPI_Comm_free(&world_comm);\n145: \n146:     return errs;\n147: }\n148: \n149: int main(int argc, char *argv[])\n150:", "label": "(172, MPI, MPI_Comm_split(dup_comm_world, color, key, &split_comm);); (120, MPI, MPI_Attr_put(lo_comm, key_1, (void *) (MPI_Aint) world_rank);); (111, MPI, MPI_Keyval_create(copy_fn, delete_fn, &key_1, &value);); (142, MPI, MPI_Abort(MPI_COMM_WORLD, 3005);); (118, MPI, MPI_Attr_put(lo_comm, key_3, (void *) 0);); (238, MPI, MPI_Comm_free(&split_comm);); (210, MPI, MPI_Comm_compare(world_comm, rev_comm, &result);); (28, MPI, MPI_Comm_create(dup_comm_world, world_group, &world_comm);); (28, MPI, MPI_Comm_rank(world_comm, &rank);); (2, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (114, MPI, MPI_Comm_dup(lo_comm, &dup_comm);); (146, MPI, MPI_Keyval_free(&key_3);); (230, MPI, MPI_Comm_free(&rev_comm);); (135, MPI, MPI_Attr_get(dup_comm, key_3, (void **) &vvalue, &flag);); (210, MPI, MPI_Comm_compare(world_comm, lo_comm, &result);); (190, MPI, MPI_Abort(MPI_COMM_WORLD, 3011);); (71, MPI, MPI_Abort(MPI_COMM_WORLD, 3003);); (228, MPI, MPI_Group_free(&world_group);); (155, MPI, MPI_Comm_size(split_comm, &size);); (-7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &world_size);); (194, MPI, MPI_Comm_create(world_comm, rev_group, &rev_comm);); (135, MPI, MPI_Keyval_free(&key_1);); (157, MPI, MPI_Abort(MPI_COMM_WORLD, 3009);); (190, MPI, MPI_Group_range_incl(world_group, 1, ranges, &rev_group);); (92, MPI, MPI_Keyval_create(MPI_NULL_COPY_FN, MPI_NULL_DELETE_FN, &key_3, &value);); (129, MPI, MPI_Abort(MPI_COMM_WORLD, 3008);); (148, MPI, MPI_Comm_rank(split_comm, &rank);); (201, MPI, MPI_Abort(MPI_COMM_WORLD, 3013);); (173, MPI, MPI_Comm_compare(lo_comm, dup_comm, &result);); (47, MPI, MPI_Abort(MPI_COMM_WORLD, 3002);); (152, MPI, MPI_Barrier(split_comm);); (40, MPI, MPI_Comm_rank(lo_comm, &rank);); (106, MPI, MPI_Abort(MPI_COMM_WORLD, 3004);); (213, MPI, MPI_Group_free(&rev_group);); (163, MPI, MPI_Abort(MPI_COMM_WORLD, 3010);); (205, MPI, MPI_Comm_free(&dup_comm_world);); (4, MPI, MPI_Abort(MPI_COMM_WORLD, 3001);); (213, MPI, MPI_Comm_free(&dup_comm);); (41, MPI, MPI_Barrier(lo_comm);); (210, MPI, MPI_Comm_free(&lo_comm);); (91, MPI, MPI_Attr_get(dup_comm, key_1, (void **) &vvalue, &flag);); (179, MPI, MPI_Abort(MPI_COMM_WORLD, 3012);)"}
{"predict": "else\n128:         {\n129:             mmha::vec_from_smem_transpose(k, k_smem, transpose_idx, smem_pitch);\n130:             mmha::write_smem_transpose(k, k_smem, transpose_idx, smem_pitch);\n131:         }\n132: \n133:         __syncthreads();\n134: \n135:         if (do_rotary)\n136:         {\n137:             *reinterpret_cast<Vec_k*>(k_smem + half_idx * smem_pitch + intra_half_idx) = k;\n138:         }\n139:         break;\n140:     }\n141:     default:\n142:     {\n143:         break;\n144:     }\n145:     }\n146: \n147:     // Write back k cache\n148:     Vec_k_cache k_cache_out;\n149:     if constexpr (INT8_K_CACHE)\n150:     {\n151:         using Packed_Float_t = typename mmha::packed_type<float, vec_size>::type;\n152:         k_cache_out = mmha::convert_to_int8(mmha::mul<Packed_Float_t, float>(k, k_scale_quant_orig));\n153:     }\n154:     #ifdef ENABLE_FP8\n155:     else if constexpr (FP8_K_CACHE)\n156:     {\n157:         mmha::convert_to_8bit_kv_cache<Vec_k_cache, Vec_k, T_cache, float>(&k_cache_out, k, k_scale_quant_orig);\n158:     }\n159:     #endif // ENABLE_FP8\n160:     else\n161:     {\n162:         k_cache_out = k;\n163:     }\n164:     k_cache_batch[inBlockIdx_r] = k_cache_out;\n165: \n166:     return;\n167: }", "label": "(16, CUDA, extern __shared__ __align__(sizeof(float2)) char smem_[];)"}
{"predict": "initial conditions\n90:     if (p2p == true) {\n91:         // Halo exchange\n92:         exchange_halos<<<(ny / num_devices) / 128 + 1, 128>>>(a[dev_id], a_new[dev_id], a_ref_h, a_h, ny);\n93:     }\n94: \n95:     // Run the main simulation loop\n96:     for (int iter = 0; iter < iter_max; ++iter) {\n97:         // Perform the computation on each device\n98:         for (int dev_id = 0; dev_id < num_devices; ++dev_id) {\n99:             // Perform computation on device\n100:             compute<<<(ny / num_devices) / 128 + 1, 128>>>(a[dev_id], a_new[dev_id], ny, chunk_size[dev_id]);\n101:             CUDA_RT_CALL(cudaDeviceSynchronize());\n102:         }\n103: \n104:         // Perform halo exchange on each device\n105:         for (int dev_id = 0; dev_id < num_devices; ++dev_id) {\n106:             // Halo exchange\n107:             exchange_halos<<<(ny / num_devices) / 128 + 1, 128>>>(a[dev_id], a_new[dev_id], a_ref_h, a_h, ny);\n108:             CUDA_RT_CALL(cudaDeviceSynchronize());\n109:         }\n110:     }\n111: \n112:     // Finalize\n113:     for (int dev_id = 0; dev_id < num_devices; ++dev_id) {\n114:         CUDA_RT_CALL(cudaFree(a[dev_id]));\n115:         CUDA_RT_CALL(cudaFree(a_new[dev_id]));\n116:     }\n117: \n118:     return 0;\n119: }", "label": "(207, OpenMP, double stop = omp_get_wtime();); (146, CUDA, CUDA_RT_CALL(cudaStreamSynchronize(push_top_stream[bottom]));); (77, CUDA, CUDA_RT_CALL(cudaDeviceSynchronize());); (28, CUDA, CUDA_RT_CALL(cudaFree(0));); (88, CUDA, CUDA_RT_CALL(cudaDeviceCanAccessPeer(&canAccessPeer, dev_id, top));); (250, CUDA, CUDA_RT_CALL(cudaFreeHost(a_h));); (29, CUDA, CUDA_RT_CALL(cudaMallocHost(&a_ref_h, nx * ny * sizeof(float)));); (71, CUDA, CUDA_RT_CALL(cudaGetLastError());); (91, CUDA, CUDA_RT_CALL(cudaDeviceCanAccessPeer(&canAccessPeer, dev_id, bottom));); (92, CUDA, CUDA_RT_CALL(cudaDeviceEnablePeerAccess(bottom, 0));); (235, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_bottom_stream[dev_id]));); (83, CUDA, CUDA_RT_CALL(cudaDeviceEnablePeerAccess(top, 0));); (74, CUDA, CUDA_RT_CALL(cudaMalloc(l2_norm_d + dev_id, sizeof(float)));); (44, CUDA, CUDA_RT_CALL(cudaMalloc(a + dev_id, nx * (chunk_size[dev_id] + 2) * sizeof(float)));); (157, CUDA, CUDA_RT_CALL(cudaStreamSynchronize(compute_stream[dev_id]));); (45, CUDA, CUDA_RT_CALL(cudaMemset(a[dev_id], 0, nx * (chunk_size[dev_id] + 2) * sizeof(float)));); (132, CUDA, CUDA_RT_CALL(cudaStreamSynchronize(push_bottom_stream[top]));); (9, CUDA, CUDA_RT_CALL(cudaGetDeviceCount(&num_devices));); (228, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_top_stream[dev_id]));); (11, CUDA, CUDA_RT_CALL(cudaSetDevice(dev_id));); (38, CUDA, CUDA_RT_CALL(cudaMalloc(a_new + dev_id, nx * (chunk_size[dev_id] + 2) * sizeof(float)));); (228, CUDA, CUDA_RT_CALL(cudaFreeHost(l2_norm_h[dev_id]));); (230, CUDA, CUDA_RT_CALL(cudaFree(a_new[dev_id]));); (59, CUDA, CUDA_RT_CALL(cudaStreamCreate(compute_stream + dev_id));); (60, CUDA, CUDA_RT_CALL(cudaStreamCreate(push_bottom_stream + dev_id));); (36, CUDA, CUDA_RT_CALL(cudaMemset(a_new[dev_id], 0, nx * (chunk_size[dev_id] + 2) * sizeof(float)));); (230, CUDA, CUDA_RT_CALL(cudaFreeHost(a_ref_h));); (9, CUDA, CUDA_RT_CALL(cudaMallocHost(&a_h, nx * ny * sizeof(float)));); (219, CUDA, CUDA_RT_CALL(cudaStreamDestroy(compute_stream[dev_id]));); (102, OpenMP, double start = omp_get_wtime();); (57, CUDA, CUDA_RT_CALL(cudaMallocHost(l2_norm_h + dev_id, sizeof(float)));); (222, CUDA, CUDA_RT_CALL(cudaFree(a[dev_id]));); (218, CUDA, CUDA_RT_CALL(cudaFree(l2_norm_d[dev_id]));); (50, CUDA, CUDA_RT_CALL(cudaStreamCreate(push_top_stream + dev_id));)"}
{"predict": "shape.Y_shape[2], shape.Y_shape[3])); \\\n145:         \\\n146:         hiednnTensorDesc_t zDesc; \\\n147:         CHECK_HIEDNN(hiednnCreateTensorDesc(&zDesc)); \\\n148:         CHECK_HIEDNN(hiednnSet4dTensorDesc( \\\n149:             zDesc, OUTTYPE, Z_TENSORFORMAT, \\\n150:             shape.Z_shape[0], shape.Z_shape[1], \\\n151:             shape.Z_shape[2], shape.Z_shape[3])); \\\n152:         \\\n153:         CHECK_HIEDNN(hiednnCreateCudaHandle(&handle)); \\\n154:         \\\n155:         x_cu = static_cast<ST *>(malloc(sizeof(ST) * shape.X_len)); \\\n156:         y_cu = static_cast<ST *>(malloc(sizeof(ST) * shape.Y_len)); \\\n157:         z_cu = static_cast<DT *>(malloc(sizeof(DT) * shape.Z_len)); \\\n158:         \\\n159:         CHECK_HIEDNN(hiednnMalloc(handle, x_cu, shape.X_len * sizeof(ST))); \\\n160:         CHECK_HIEDNN(hiednnMalloc(handle, y_cu, shape.Y_len * sizeof(ST))); \\\n161:         CHECK_HIEDNN(hiednnMalloc(handle, z_cu, shape.Z_len * sizeof(DT))); \\\n162:         \\\n163:         CHECK_HIEDNN(hiednnMemcpy(handle, x_cu, x, shape.X_len * sizeof(ST))); \\\n164:         CHECK_HIEDNN(hiednnMemcpy(handle, y_cu, y, shape.Y_len * sizeof(ST))); \\\n165:         CHECK_HIEDNN(hiednnMemcpy(handle, z_cu, z, shape.Z_len * sizeof(DT))); \\\n166:         \\\n167:         hiednnOperation_t op; \\\n168:         CHECK_HIEDNN(hiednnCreateOperation(&op)); \\\n169:         CHECK_HIEDNN(hiednnSetOperationDesc(op, HIE_BINARY_OP, 2, \\\n170:             1, 1, alpha, beta, extParam)); \\\n171:         \\\n172:         CHECK_HIEDNN(hiednnExecuteOperation(handle, op, 2", "label": "(129, CUDA, cudaMemcpy(z_cu, z, shape.Z_len * sizeof(DT), cudaMemcpyHostToDevice);); (221, CUDA, cudaFree(x_cu);); (122, CUDA, cudaMalloc(&x_cu, shape.X_len * sizeof(ST));); (125, CUDA, cudaMemcpy(y_cu, y, shape.Y_len * sizeof(ST), cudaMemcpyHostToDevice);); (123, CUDA, cudaMemcpy(x_cu, x, shape.X_len * sizeof(ST), cudaMemcpyHostToDevice);); (218, CUDA, cudaFree(y_cu);); (120, CUDA, cudaMalloc(&z_cu, shape.Z_len * sizeof(DT));); (118, CUDA, cudaMalloc(&y_cu, shape.Y_len * sizeof(ST));); (128, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (220, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (214, CUDA, cudaFree(z_cu);)"}
{"predict": "(21,OpenMP,#pragma omp parallel for collapse(2)), (22,OpenMP,#pragma omp for collapse(2))", "label": "(22, CUDA, __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];); (20, CUDA, __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];)"}
{"predict": "18: \t                          FunCallStream)) ;\n119: \n120: \tcheckCuda(cudaMemcpyAsync(A_gpu->scubufs[streamId].usub, usub,\n121: \t                          usub_len * sizeof(int_t), cudaMemcpyHostToDevice,\n122: \t                          FunCallStream)) ;\n123: \n124: \tcheckCuda(cudaMemcpyAsync(A_gpu->scubufs[streamId].xsup, xsup,\n125: \t                          ldt * sizeof(int_t), cudaMemcpyHostToDevice,\n126: \t                          FunCallStream)) ;\n127: \n128: \tdouble tTmp1 = SuperLU_timer_();\n129: \tdouble tTmp2 = SuperLU_timer_();\n130: \n131: \tcheckCuda(cudaStreamSynchronize(FunCallStream));\n132: \n133: \tdouble tTmp3 = SuperLU_timer_();\n134: \n135: \tcheckCuda(cudaMemcpyAsync(dA_gpu->usub_IndirectJ3, A_gpu->scubufs[streamId].usub_IndirectJ3,\n136: \t                          ncols * sizeof(int_t), cudaMemcpyDeviceToDevice,\n137: \t                          FunCallStream)) ;\n138: \n139: \tcheckCuda(cudaMemcpyAsync(dA_gpu->Remain_L_buff, A_gpu->scubufs[streamId].Remain_L_buff,\n140: \t                          Remain_lbuf_send_size * sizeof(double),\n141: \t                          cudaMemcpyDeviceToDevice, FunCallStream)) ;\n142: \n143: \tcheckCuda(cudaMemcpyAsync(dA_gpu->bigU, A_gpu->scubufs[streamId].bigU,\n144: \t                          bigu_send_size * sizeof(double),\n145: \t                          cudaMemcpyDeviceToDevice, FunCallStream) );\n146: \n147: \tcheckCuda(cudaMemcpyAsync(dA_gpu->Remain_info, A_gpu->scubufs[streamId].Remain_info,\n148: \t                          RemainBlk * sizeof(Remain_info_t),\n149: \t                          cudaMemcpyDeviceToDevice, FunCallStream) );\n150: \n151: \tcheckCuda(cudaMemcpyAsync(dA_gpu->Ublock_info, A_gpu->scubufs", "label": "(93, CUDA, cudaEventRecord(A_gpu->ePCIeH2D[k0], FunCallStream);); (231, CUDA, cudaEventRecord(A_gpu->GemmStart[k0], FunCallStream);); (239, CUDA, cudaStreamSynchronize(FunCallStream);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (38,OpenMP,#pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp atomic  hint(AMD_safe_fp_atomics)); (34, OpenMP, #pragma omp atomic  hint(AMD_unsafe_fp_atomics)); (2, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:sum)); (17, OpenMP, #pragma omp atomic  hint(AMD_fast_fp_atomics))"}
{"predict": "(19,OpenMP,#pragma omp parallel for private(tid,x,x_dot,x_next,u,y))", "label": "(6, CUDA, extern __shared__ float entire_buffer[];)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for)", "label": "(37, CUDA, CHECK(cudaFree(d_y));); (14, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice));); (36, CUDA, CHECK(cudaFree(d_z));); (10, CUDA, CHECK(cudaMalloc(&d_y, M));); (8, CUDA, CHECK(cudaMalloc(&d_x, M));); (31, CUDA, CHECK(cudaFree(d_x));); (8, CUDA, CHECK(cudaMalloc(&d_z, M));); (9, CUDA, CHECK(cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice));)"}
{"predict": "(10,OpenMP,#pragma omp parallel for collapse(2)), (28,OpenMP,#pragma omp parallel for collapse(2))", "label": "(10, CUDA, cudaDeviceSynchronize();); (5, CUDA, cudaMalloc((void **)&sm_o, sizeof(float) * m * m);); (35, CUDA, cudaFree(sm_o);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for private(i)), (47,OpenMP,#pragma omp parallel for private(i))", "label": "(49, MPI, MPI_Win_unlock(1, win);); (32, MPI, MPI_Win_unlock(rank, win);); (45, MPI, MPI_Win_lock(MPI_LOCK_SHARED, 1, 0, win);); (68, MPI, MPI_Type_free(&vector_dtp);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (64, MPI, MPI_Win_free(&win);); (41, MPI, MPI_Accumulate(orig_buf, 1, vector_dtp, 1, 0, 1, vector_dtp, MPI_MAXLOC, win);); (14, MPI, MPI_Type_commit(&vector_dtp);); (64, MPI, MPI_Free_mem(orig_buf);); (65, MPI, MPI_Free_mem(tar_buf);); (17, MPI, MPI_Win_lock(MPI_LOCK_SHARED, rank, 0, win);); (22, MPI, MPI_Barrier(MPI_COMM_WORLD);); (1, MPI, MPI_Alloc_mem(sizeof(long_double_int_t) * DATA_SIZE, MPI_INFO_NULL, &orig_buf);)"}
{"predict": "(1,MPI,)#include <mpi.h> (3,MPI,int rank, size_t size; MPI_Comm_rank(MPI_COMM_WORLD, &rank); MPI_Comm_size(MPI_COMM_WORLD, &size);) (3,MPI,if (rank == 0) {) (10,MPI,for (int batch = rank; batch < img.batchCount(); batch += size) {) (3,MPI,} MPI_Finalize();)", "label": "(2, CUDA, __shared__ int hist[256];)"}
{"predict": "(33,OpenMP,#pragma omp parallel for collapse(3)), (70,OpenMP,#pragma omp parallel for collapse(3))", "label": "(74, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(test.data(), dstDev->basePtr(), test.size(), cudaMemcpyDeviceToHost));); (66, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (50, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(srcDev->basePtr(), srcVec.data(), srcVec.size(), cudaMemcpyHostToDevice));); (60, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (64, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamDestroy(stream));)"}
{"predict": "(28,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for)", "label": "(18, OpenCL, clCreateCommandQueueWithProperties_StubWithCallback(clCreateCommandQueueWithProperties_testCommandQueueFromSpecifiedContext);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for)", "label": "(25, OpenCL, clReleaseEvent(outEvent);)"}
{"predict": "NULL,\n149:                                             &vectorCount, &sizeValues[j],\n150:                                             0, NULL, NULL)))\n151:         {\n152:             vlog_error(\"Error: clEnqueueNDRangeKernel failed! err: %d\\n\", error);\n153:             return error;\n154:         }\n155:     }\n156: \n157:     // Wait for all kernels to finish\n158:     if ((error = clFinish(tinfo->tQueue))) vlog(\"clFinish failed\\n\");\n159: \n160:     if (gHostFill)\n161:     {\n162:         // Read the results back to the host\n163:         for (j = gMinVectorSizeIndex; j < gMaxVectorSizeIndex; j++)\n164:         {\n165:             out[j] = (cl_ushort *)clEnqueueMapBuffer(\n166:                 tinfo->tQueue, tinfo->outBuf[j], CL_TRUE, CL_MAP_READ, 0,\n167:                 buffer_size, 0, NULL, e + j, &error);\n168:             if (error || NULL == out[j])\n169:             {\n170:                 vlog_error(\"Error: clEnqueueMapBuffer %d failed! err: %d\\n\", j,\n171:                            error);\n172:                 return error;\n173:             }\n174:         }\n175: \n176:         // Wait for the map to finish\n177:         if ((error = clWaitForEvents(1, e + j)))\n178:         {\n179:             vlog_error(\"Error: clWaitForEvents failed! err: %d\\n\", error);\n180:             return error;\n181:         }\n182:         if ((error = clReleaseEvent(e[j])))\n183:         {\n184:             vlog_error(\"Error: clReleaseEvent failed! err: %d\\n\", error);\n185:             return error;\n186:         }\n187:     }\n188: \n189:     // Collect the results from the host.\n190:     for (j = gMinVector", "label": "(142, OpenCL, error = clSetKernelArg(kernel, 1, sizeof(tinfo->inBuf), &tinfo->inBuf);)"}
{"predict": "access to smemExpertCount to ensure no bank conflicts\n135:     int32_t* smemExpertCountStrided = reinterpret_cast<int32_t*>(\n136:         reinterpret_cast<uintptr_t>(smemExpertCount) + (threadIdx.x << params.mPaddingLog2));\n137:     int32_t expertOffset = atomicAdd(smemExpertCountStrided, 1);\n138: \n139:     // Make local histogram (token counts per expert) available to all threads in the cluster.\n140:     cg::cluster_group::sync();\n141: \n142:     // Store the final expert index and the offset within the expert\n143:     if (expertOffset < expandedIdxSize)\n144:     {\n145:         int32_t expertIdx = expertIndexes[expertOffset >> params.mPaddingLog2];\n146:         int32_t offset = expertOffsets[expertOffset >> params.mPaddingLog2];\n147:         smemExpertIdx[expertOffset] = expertIdx;\n148:         smemExpertOffset[expertOffset] = offset;\n149:     }\n150: }", "label": "(10, CUDA, __shared__ typename Scan::TempStorage tempStorage;); (33, CUDA, cudaGridDependencySynchronize();); (4, CUDA, __shared__ int32_t __attribute((aligned(128))) smemExpertCount[NumThreads];); (4, CUDA, __shared__ int32_t __attribute((aligned(128))) smemExpertOffset[NumThreads];)"}
{"predict": "t0;\n133:   MPI_Reduce(&trans_time, &t0, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);\n134:   if (me==0) {\n135:       printf(\"Time = %f\\n\", t0);\n136:   }\n137:   prk_free(A);\n138:   prk_free(B);\n139:   prk_free(T);\n140:   MPI_Finalize();\n141:   return 0;\n142: }\n143: #endif // USE_DATATYPES\n144: #endif // USE_MPI", "label": "(9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &np);); (142, MPI, t1 = MPI_Wtime();); (20, MPI, MPI_Finalize();); (98, MPI, MPI_Type_commit(&trans_dt);); (144, MPI, MPI_Type_free(&stride_dt);); (159, MPI, MPI_Allreduce(MPI_IN_PLACE, &abserr, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);); (2, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (90, MPI, MPI_Type_size(MPI_DOUBLE,&dsize);); (141, MPI, MPI_Type_free(&trans_dt);); (70, MPI, MPI_Abort(MPI_COMM_WORLD,99);); (-5, MPI, if (provided < requested) MPI_Abort(MPI_COMM_WORLD,provided);); (89, MPI, MPI_Type_hvector(block_order, 1, dsize, stride_dt, &trans_dt);); (83, MPI, MPI_Type_vector(block_order, 1, block_order, MPI_DOUBLE, &stride_dt);); (54, MPI, MPI_Barrier(MPI_COMM_WORLD);); (96, MPI, t0 = MPI_Wtime();); (-11, MPI, MPI_Init_thread(&argc, &argv, requested, &provided);)"}
{"predict": "(29,OpenMP,#pragma omp parallel for)", "label": "(55, CUDA, cudaMalloc((void**)&result_data, axis_size * sizeof(float));); (52, CUDA, cudaMalloc((void**)&result_data, tensor->size * sizeof(float));)"}
{"predict": "(5,MPI,*w = MPI_Wtime(););", "label": "(8, OpenCL, cl_status = clGetPlatformInfo(id, param_name, param_size, some_info, NULL);)"}
{"predict": "(10,MPI,MPI_Init(&argc, &argv);)\n(46,MPI,MPI_Finalize();)\n(15,OpenMP,#pragma omp parallel for)\n(30,OpenMP,#pragma omp atomic)\n(41,OpenMP,#pragma omp parallel for)", "label": "(37, MPI, MPI_Group_free(&comm_group);); (29, MPI, mpi_errno = MPI_Comm_create(MPI_COMM_WORLD, comm_group, &comm_hdls[i]);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (8, MPI, MPI_Comm_group(MPI_COMM_WORLD, &world_group);); (45, MPI, MPI_Group_free(&world_group);); (8, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(7,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for)", "label": "(6, MPI, MPI_Comm_size(comm, &size);); (4, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(1, MPI, #include <mpi.h>)\n(7, MPI, int rank, size; MPI_Comm_rank(MPI_COMM_WORLD, &rank); MPI_Comm_size(MPI_COMM_WORLD, &size);)\n(8, MPI, if (rank == 0) {)\n(9, MPI, MPI_Bcast(_r, T * H * N, MPI_FLOAT, 0, MPI_COMM_WORLD);)\n(10, MPI, MPI_Bcast(_k, T * H * N, MPI_FLOAT, 0, MPI_COMM_WORLD);)\n(11, MPI, MPI_Bcast(_v, T * H * N, MPI_FLOAT, 0, MPI_COMM_WORLD);)\n(12, MPI, MPI_Bcast(_w, H * N, MPI_FLOAT, 0, MPI_COMM_WORLD);)\n(13, MPI, MPI_Bcast(_u, H * N, MPI_FLOAT, 0, MPI_COMM_WORLD);)\n(14, MPI, MPI_Bcast(_y, T * H * N, MPI_FLOAT, 0, MPI_COMM_WORLD);)\n(15, MPI, MPI_Bcast(state, N * N, MPI_FLOAT, 0, MPI_COMM_WORLD);)\n(16, MPI, } else {)\n(17, MPI, MPI_Scatter(_r, T * H * N / size, MPI_FLOAT, _r, T * H * N / size, MPI_FLOAT, 0, MPI_COMM_WORLD);)\n(18, MPI, MPI_Scatter(_k, T * H * N / size, MPI_FLOAT, _k, T * H * N / size, MPI_FLOAT, 0, MPI_COMM_WORLD);)\n(19, MPI, MPI_Scatter(_v, T * H * N / size, MPI_FLOAT, _v, T * H * N / size, MPI_FLOAT, 0, MPI_COMM_WORLD);)\n(20, MPI, MPI_Scatter(_w, H * N / size, MPI_FLOAT, _w, H * N / size, MPI_FLOAT, 0, MPI_COMM_WORLD);)\n(21, MPI, MPI_Scatter(_u, H * N / size, MPI_FLOAT, _u, H * N / size, MPI_FLOAT, 0, MPI_COMM_WORLD);)\n(22, MPI, MPI_Scatter(_y, T * H * N / size, MPI_FLOAT, _y, T *", "label": "(15, CUDA, __shared__ float rr[TILE_T][N], kk[TILE_T][N];)"}
{"predict": "(13,OpenMP,#pragma omp parallel for private(i,nameLen,descLen,numCvars,numPvars,numSubcat,name,desc))", "label": "(51, MPI, MPI_T_category_get_pvars(i, numPvars, pvarIndex);); (28, MPI, MPI_T_category_get_cvars(i, numCvars, cvarIndex);); (3, MPI, MPI_T_category_get_num(&numCat);); (72, MPI, MPI_T_category_get_categories(i, numSubcat, subcatIndex);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(13, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "*************************************************\n115:      * Perturbed the 1st and last diagonal of the matrix to lower\n116:      * values. Intention is to change perm_r[].   */\n117:     nzval_loc[0] *= 0.01;\n118:     nzval_loc[nnz_loc-1] *= 0.0001; \n119: \n120:     /* Create local matrix A in NR_loc format. */\n121:     sCreate_CompRow_Matrix_dist(&A, m_loc, n, nnz_loc, nzval_loc, rowptr, colind,\n122:                                 SLU_NR_loc, SLU_S, SLU_GE);\n123: \n124:     /* Free local memory. */\n125:     free(rowptr);\n126:     free(marker);\n127:     free(nzval_loc);\n128:     free(colind);\n129: \n130:     /* Set up right-hand side. */\n131:     if ( !(rhs[0] = floatMalloc_dist(m_loc*nrhs)) )\n132:         ABORT(\"Malloc fails for rhs[]\");\n133:     *ldb = m_loc;\n134: \n135:     /* Transfer the right-hand side into the local format. */\n136:     for (i = 0; i < nrhs; ++i) {\n137:       for (j = 0; j < m_loc; ++j) {\n138: \trhs[i][j] = b_global[i*m_loc + j];\n139:       }\n140:     }\n141: \n142:     /* Free global memory. */\n143:     free(b_global);\n144:     free(xtrue_global);\n145: \n146:     return 0;\n147: }", "label": "(33, MPI, MPI_Bcast( nzval,  nnz, MPI_FLOAT, 0, grid->comm );); (33, MPI, MPI_Bcast( rowind, nnz, mpi_int_t,  0, grid->comm );); (45, MPI, MPI_Bcast( colptr,  n+1, mpi_int_t,  0, grid->comm );); (43, MPI, MPI_Bcast( rowind,  nnz, mpi_int_t,  0, grid->comm );); (34, MPI, MPI_Bcast( &m,   1,   mpi_int_t,  0, grid->comm );); (26, MPI, MPI_Bcast( &n,     1,   mpi_int_t,  0, grid->comm );); (24, MPI, MPI_Bcast( &m,     1,   mpi_int_t,  0, grid->comm );); (33, MPI, MPI_Bcast( &nnz, 1,   mpi_int_t,  0, grid->comm );); (27, MPI, MPI_Bcast( colptr, n+1, mpi_int_t,  0, grid->comm );); (23, MPI, MPI_Bcast( &nnz,   1,   mpi_int_t,  0, grid->comm );); (29, MPI, MPI_Bcast( &n,   1,   mpi_int_t,  0, grid->comm );); (34, MPI, MPI_Bcast( nzval,   nnz, MPI_FLOAT, 0, grid->comm );)"}
{"predict": "(16,OpenMP,#pragma omp parallel for)", "label": "(15, TBB, bool ok = tbb::finalize(schBlock, std::nothrow);); (11, TBB, bool ok = tbb::finalize(schBlock1, std::nothrow);); (4, TBB, tbb::task_scheduler_handle schBlock1;)"}
{"predict": "(13,OpenMP,#pragma omp parallel for)", "label": "(9, MPI, MPI_Type_free(&(c->key));)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for), (43,OpenMP,#pragma omp parallel for), (46,OpenMP,#pragma omp parallel for), (49,OpenMP,#pragma omp parallel for), (52,OpenMP,#pragma omp parallel for), (55,OpenMP,#pragma omp parallel for), (60,OpenMP,#pragma omp parallel for)", "label": "(2, OpenMP, #pragma omp dispatch)"}
{"predict": "(24,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for)", "label": "(16, OpenCL, status = clSetKernelArg(global_check, 0, sizeof(cl_mem), &buffer);)"}
{"predict": "9:   for (int i = 0; i < 10; ++i)\n80:     foo();\n81:   for (int i = 0; i < 10; ++i)\n82:     foo();\n83:   for (int i = 0; i < 10; ++i)\n84:     foo();\n85:   for (int i = 0; i < 10; ++i)\n86:     foo();\n87:   for (int i = 0; i < 10; ++i)\n88:     foo();\n89:   for (int i = 0; i < 10; ++i)\n90:     foo();\n91:   for (int i = 0; i < 10; ++i)\n92:     foo();\n93:   for (int i = 0; i < 10; ++i)\n94:     foo();\n95:   for (int i = 0; i < 10; ++i)\n96:     foo();\n97:   for (int i = 0; i < 10; ++i)\n98:     foo();\n99:   for (int i = 0; i < 10; ++i)\n100:  foo();\n101:  foo();\n102:  foo();\n103:  foo();\n104:  foo();\n105:  foo();\n106:  foo();\n107:  foo();\n108:  foo();\n109:  foo();\n110:  foo();\n111:  foo();\n112:  foo();\n113:  foo();\n114:  foo();\n115:  foo();\n116:  foo();\n117:  foo();\n118:  foo();\n119:  foo();\n120:  foo();\n121:  foo();\n122:  foo();\n123:  foo();\n124:  foo();\n125:  foo();\n126:  foo();\n127:  foo();\n128:  foo();\n129:  foo();\n130:  foo();\n131:", "label": "(45, OpenMP, #pragma omp master taskloop simd reduction(&& : argc, z)); (101, OpenMP, #pragma omp master taskloop simd reduction(max : j)); (67, OpenMP, #pragma omp master taskloop simd reduction(^ : fl)); (111, OpenMP, #pragma omp master taskloop simd reduction(+ : m)); (20, OpenMP, #pragma omp master taskloop simd reduction()); (46, OpenMP, #pragma omp master taskloop simd reduction(+ : a, b, c, d, f)); (48, OpenMP, #pragma omp master taskloop simd reduction(min : a, b, c, d, f)); (53, OpenMP, #pragma omp master taskloop simd reduction(+ : ba)); (34, OpenMP, #pragma omp master taskloop simd reduction(~ : argc)); (100, OpenMP, #pragma omp parallel reduction(* : fl)); (17, OpenMP, #pragma omp master taskloop simd reduction(*)); (1, OpenMP, #pragma omp master taskloop simd reduction); (63, OpenMP, #pragma omp master taskloop simd reduction(&& : S2::S2sc)); (68, OpenMP, #pragma omp master taskloop simd reduction(+ : h, k, B::x)); (106, OpenMP, #pragma omp master taskloop simd reduction(task, + : m)); (75, OpenMP, #pragma omp parallel private(k)); (62, OpenMP, #pragma omp master taskloop simd reduction(& : e, g)); (67, OpenMP, #pragma omp master taskloop simd reduction(+ : o)); (-3, OpenMP, #pragma omp master taskloop simd reduction +); (72, OpenMP, #pragma omp master taskloop simd reduction(+ : p), reduction(+ : p)); (43, OpenMP, #pragma omp master taskloop simd reduction(* : ca)); (9, OpenMP, #pragma omp master taskloop simd reduction(\\)); (14, OpenMP, #pragma omp master taskloop simd reduction(| : argc,); (78, OpenMP, #pragma omp parallel reduction(min : i)); (81, OpenMP, #pragma omp parallel private(fl)); (72, OpenMP, #pragma omp master taskloop simd reduction(+ : r)); (46, OpenMP, #pragma omp master taskloop simd reduction(&& : S2::S2s)); (30, OpenMP, #pragma omp master taskloop simd reduction(max : h.b)); (20, OpenMP, #pragma omp master taskloop simd reduction(^ : S1)); (10, OpenMP, #pragma omp master taskloop simd reduction(|| : argc > 0 ? argv[1] : argv[2])); (-12, OpenMP, #pragma omp master taskloop simd reduction(); (56, OpenMP, #pragma omp master taskloop simd private(i), reduction(+ : j), reduction(+ : q)); (85, OpenMP, #pragma omp master taskloop simd reduction(+ : m) nogroup); (-12, OpenMP, #pragma omp master taskloop simd reduction(-); (66, OpenMP, #pragma omp parallel shared(i)); (-2, OpenMP, #pragma omp master taskloop simd reduction(foo : argc); (30, OpenMP, #pragma omp master taskloop simd reduction(- : da)); (69, OpenMP, #pragma omp master taskloop simd reduction(+ : fl))"}
{"predict": "(25,OpenMP,#pragma omp parallel for reduction(min:out_min(Eigen::array<DenseIndex, 1>(0))))", "label": "(18, CUDA, cudaMemcpy(d_in, in.data(), in_bytes, cudaMemcpyHostToDevice);); (32, CUDA, cudaFree(d_in);); (13, CUDA, cudaMalloc((void**)(&d_in), in_bytes);); (13, CUDA, cudaMalloc((void**)(&d_out_min), out_bytes);); (25, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (23, CUDA, assert(cudaMemcpyAsync(out_min.data(), d_out_min, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);); (28, CUDA, cudaFree(d_out_min);)"}
{"predict": "(31,OpenMP,#pragma omp parallel for collapse(2))", "label": "(3, CUDA, cudaExtent extent = make_cudaExtent(10, 20, 0);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(7, CUDA, err = cudaMemcpy( dst, src, sz, type );); (10, CUDA, ss << cudaGetErrorString(err) << endl;)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (11,MPI,MPI_Send(&t, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);)", "label": "(4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (9, MPI, MPI_Recv(&t, 1, MPI_DOUBLE, i, 11, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (1, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (11, MPI, MPI_Send(&t, 1, MPI_DOUBLE, 0, 11, MPI_COMM_WORLD);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for)\n(40,OpenMP,#pragma omp parallel for)", "label": "(39, MPI, MPI_Type_commit(&columntype);); (7, MPI, MPI_Init(&argc, &argv);); (61, MPI, MPI_Finalize();); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (34, MPI, MPI_Type_vector(8, 1, 8, MPI_INT, &columntype);); (45, MPI, MPI_Type_free(&columntype);); (37, MPI, MPI_Send(&array[0][1], 1, columntype, 1, 1, MPI_COMM_WORLD);)"}
{"predict": "(27,OpenMP,#pragma omp parallel for), (91,OpenMP,#pragma omp parallel for)", "label": "(41, SYCL, sycl::platform Plt = Dev.get_platform();)"}
{"predict": "(9,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for), (49,OpenMP,#pragma omp parallel for), (58,OpenMP,#pragma omp parallel for), (62,OpenMP,#pragma omp parallel for simd aligned(f))", "label": "(45, OpenMP, #pragma omp teams distribute parallel for simd aligned(e, g)); (7, OpenMP, #pragma omp teams distribute parallel for simd aligned); (14, OpenMP, #pragma omp teams distribute parallel for simd aligned ()); (9, OpenMP, #pragma omp teams distribute parallel for simd aligned (); (45, OpenMP, #pragma omp teams distribute parallel for simd aligned(h)); (51, OpenMP, #pragma omp parallel); (18, OpenMP, #pragma omp teams distribute parallel for simd aligned (argc,); (66, OpenMP, #pragma omp teams distribute parallel for simd aligned(f:j)); (33, OpenMP, #pragma omp teams distribute parallel for simd aligned (argv[1])); (24, OpenMP, #pragma omp teams distribute parallel for simd aligned (argc : 5)); (43, OpenMP, #pragma omp teams distribute parallel for simd aligned(i)); (56, OpenMP, #pragma omp teams distribute parallel for simd aligned(f)); (17, OpenMP, #pragma omp teams distribute parallel for simd aligned (*argc > 0 ? argv[1] : argv[2])); (-6, OpenMP, #pragma omp target); (47, OpenMP, #pragma omp teams distribute parallel for simd aligned(v:16)); (22, OpenMP, #pragma omp teams distribute parallel for simd aligned (S1)); (4, OpenMP, #pragma omp teams distribute parallel for simd aligned (argc)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(10, TBB, __TBB_STATIC_ASSERT( !HasImplicitConversionToInt< tbb::atomic<ScopedColor1> >::value, \"The implicit conversion is not expected.\" );); (12, TBB, __TBB_STATIC_ASSERT( sizeof(tbb::atomic<ScopedColor2>) == sizeof(ScopedColor2), \"tbb::atomic instantiated with scoped enum should have the same size as scoped enum.\" );); (10, TBB, __TBB_STATIC_ASSERT( sizeof(tbb::atomic<ScopedColor1>) == sizeof(ScopedColor1), \"tbb::atomic instantiated with scoped enum should have the same size as scoped enum.\" );)"}
{"predict": "(28,OpenMP,#pragma omp parallel for private(i,n)), (33,OpenMP,#pragma omp parallel for private(i,n)), (54,OpenMP,#pragma omp parallel for private(i,n))", "label": "(18, OpenMP, #pragma omp threadprivate (ident))"}
{"predict": "(4,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for)", "label": "(17, OpenMP, #pragma omp taskwait); (0, OpenMP, #pragma omp task); (2, OpenMP, #pragma omp parallel for)"}
{"predict": "(10,OpenMP,#pragma omp parallel for collapse(2)), (25,OpenMP,#pragma omp parallel for collapse(2)), (52,OpenMP,#pragma omp parallel for collapse(2))", "label": "(52, MPI, MPI_Type_free(&vector);); (6, MPI, MPI_Init(&argc, &argv);); (42, MPI, MPI_Type_create_resized(vector, 0, sizeof(int), &vector);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (64, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (37, MPI, MPI_Type_vector(8, 1, 6, MPI_INT, &vector);); (38, MPI, MPI_Type_commit(&vector);); (57, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(35,OpenMP,#pragma omp parallel for)", "label": "(66, CUDA, CUDA_CHECK(cudaFree(d_C));); (52, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (37, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_C), sizeof(data_type) * C.size()));); (69, CUDA, CUDA_CHECK(cudaDeviceReset());); (66, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (33, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (59, CUDA, CUDA_CHECK(cudaFree(d_A));); (27, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(12,MPI,*w = MPI_Wtime();); (15,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for)", "label": "(7, MPI, MPI_Comm_rank(team.comm, &rank);)"}
{"predict": "(3,MPI,MPI_Init(&argc, &argv);) (4,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);) (5,MPI,MPI_Comm_size(MPI_COMM_WORLD, &size);) (28,MPI,MPI_Sendrecv(send_buff, msg_size, MPI_INT, (rank + 1) % size, 0, recv_buff, msg_size, MPI_INT, (rank + size - 1) % size, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)", "label": "(59, MPI, double end_time = MPI_Wtime();); (60, MPI, MPI_Finalize();); (47, MPI, MPI_Recv(recv_buff, msg_size, MPI_INT, 1, TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (17, MPI, MPI_Bcast(&msg_size, 1, MPI_INT, 0, MPI_COMM_WORLD);); (2, MPI, MPI_Init(NULL, NULL);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (13, MPI, MPI_Bcast(&nr_msgs, 1, MPI_INT, 0, MPI_COMM_WORLD);); (48, MPI, MPI_Ssend(send_buff, msg_size, MPI_INT, 0, TAG, MPI_COMM_WORLD);); (24, MPI, double start_time = MPI_Wtime();); (31, MPI, MPI_Recv(recv_buff, msg_size, MPI_INT, 0, TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (28, MPI, MPI_Ssend(send_buff, msg_size, MPI_INT, 1, TAG, MPI_COMM_WORLD);); (0, MPI, MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);)"}
{"predict": "(20,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for)", "label": "(5, CUDA, thrust::complex<Q> Bi;)"}
{"predict": "(33,OpenMP,#pragma omp parallel for collapse(2)), (60,OpenMP,#pragma omp parallel for collapse(2))", "label": "(51, CUDA, cudaError_t result = cudaGetLastError();)"}
{"predict": "(2,OpenMP,#pragma omp parallel for), (10,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for)", "label": "(4, TBB, tbb::concurrent_queue<Foo> queue;)"}
{"predict": "(cudaEvent_t) * nsupers);\n90: \tA_gpu->eHostPCIeD2H = (cudaEvent_t *) malloc(sizeof(cudaEvent_t) * nsupers);\n91: \tA_gpu->eHostPCIeH2D = (cudaEvent_t *) malloc(sizeof(cudaEvent_t) * nsupers);\n92: \tA_gpu->ScatterMOPStart = (cudaEvent_t *) malloc(sizeof(cudaEvent_t) * nsupers);\n93: \tA_gpu->ScatterMOPEnd = (cudaEvent_t *) malloc(sizeof(cudaEvent_t) * nsupers);\n94: \n95: \tfor (int_t i = 0; i < nsupers; i++)\n96: \t{\n97: \t\tA_gpu->isOffloaded[i] = 0;\n98: \t\tA_gpu->GemmStart[i] = 0;\n99: \t\tA_gpu->GemmEnd[i] = 0;\n100: \t\tA_gpu->ScatterEnd[i] = 0;\n101: \t\tA_gpu->ePCIeH2D[i] = 0;\n102: \t\tA_gpu->ePCIeD2H_Start[i] = 0;\n103: \t\tA_gpu->ePCIeD2H_End[i] = 0;\n104: \t\tA_gpu->eHostPCIeD2H[i] = 0;\n105: \t\tA_gpu->eHostPCIeH2D[i] = 0;\n106: \t\tA_gpu->ScatterMOPStart[i] = 0;\n107: \t\tA_gpu->ScatterMOPEnd[i] = 0;\n108: \t}\n109: \n110: \tA_gpu->dGemmStart = (cudaEvent_t *) malloc(sizeof(cudaEvent_t) * nsupers);\n111: \tA_gpu->dGemmEnd = (cudaEvent_t *) malloc(sizeof(cudaEvent_t) * nsupers);\n112: \tA_gpu->dScatterEnd = (cudaEvent_t *) malloc(sizeof(cudaEvent_t) * nsupers);\n113: \tA_gpu->dScatterMOPStart = (cudaEvent_t *) malloc(sizeof(cudaEvent_t) * nsupers);\n114: \tA_gpu->dScatterMOPEnd =", "label": "(61, CUDA, checkCudaErrors(cudaMallocHost(  &tmp_ptr,  remain_l_max * sizeof(double) )) ;); (50, CUDA, checkCudaErrors(cudaMallocHost(  &tmp_ptr, (n) * sizeof(int_t) )) ;); (256, CUDA, checkCudaErrors(cudaMemcpy( (A_gpu->local_u_blk_infoVec), local_u_blk_infoVec, cum_num_u_blocks * sizeof(local_u_blk_info_t), cudaMemcpyHostToDevice)) ;); (476, CUDA, checkCudaErrors(cudaMemcpy( (A_gpu->UrowindVec), indtemp1, u_ind_len * sizeof(int_t), cudaMemcpyHostToDevice)) ;); (62, CUDA, checkCudaErrors(cudaMallocHost ( &tmp_ptr, sizeof(double) * (A_host->bufmax[1])));); (67, CUDA, checkCudaErrors(cudaMallocHost ( &tmp_ptr, sizeof(int_t) * (A_host->bufmax[2])));); (81, CUDA, checkCudaErrors(cudaMalloc(  &tmp_ptr,  buffer_size * sizeof(double))) ;); (179, CUDA, checkCudaErrors(cudaMemcpy( (A_gpu->local_l_blk_infoVec), local_l_blk_infoVec, cum_num_l_blocks * sizeof(local_l_blk_info_t), cudaMemcpyHostToDevice)) ;); (51, CUDA, checkCudaErrors(cudaMallocHost(  &tmp_ptr, mcb * sizeof(Ublock_info_t) )) ;); (174, CUDA, checkCudaErrors(cudaMalloc(  &tmp_ptr,  cum_num_l_blocks * sizeof(local_l_blk_info_t))) ;); (477, CUDA, checkCudaErrors(cudaMemcpy( (A_gpu->UrowindPtr), temp_UrowindPtr, u_k * sizeof(int_t), cudaMemcpyHostToDevice))"}
{"predict": "(12,OpenMP,#pragma omp parallel for private(degree_idx, out_prime_idx, accum, obase_value, obase_ratio, padded_out_prime_idx, out, out2)),", "label": "(5, CUDA, extern __shared__ uint64_t s_qiHat_mod_pj[];)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for), (29,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for), (35,OpenMP,#pragma omp parallel for), (38,OpenMP,#pragma omp parallel for), (41,OpenMP,#pragma omp parallel for), (44,OpenMP,#pragma omp parallel for), (47,OpenMP,#pragma omp parallel for), (50,OpenMP,#pragma omp parallel for), (53,OpenMP,#pragma omp parallel for)", "label": "(26, OpenMP, #pragma omp teams distribute shared (argc)); (22, OpenMP, #pragma omp teams distribute shared (argc > 0 ? argv[1] : argv[2])); (39, OpenMP, #pragma omp teams distribute shared(ca)); (41, OpenMP, #pragma omp teams distribute shared(da)); (16, OpenMP, #pragma omp teams distribute shared (argc,); (3, OpenMP, #pragma omp teams distribute shared); (5, OpenMP, #pragma omp teams distribute shared (); (49, OpenMP, #pragma omp teams distribute firstprivate(i), shared(i)); (27, OpenMP, #pragma omp teams distribute shared (argv[1])); (-2, OpenMP, #pragma omp target); (55, OpenMP, #pragma omp teams distribute shared(j)); (57, OpenMP, #pragma omp teams distribute firstprivate(i)); (17, OpenMP, #pragma omp teams distribute shared (S1)); (37, OpenMP, #pragma omp teams distribute shared(h, B::x)); (18, OpenMP, #pragma omp teams distribute shared (a, b, c, d, f)); (2, OpenMP, #pragma omp teams distribute shared (argc); (46, OpenMP, #pragma omp teams distribute shared(i)); (21, OpenMP, #pragma omp teams distribute shared(ba)); (35, OpenMP, #pragma omp teams distribute private(i), shared(i)); (28, OpenMP, #pragma omp teams distribute shared(e, g)); (39, OpenMP, #pragma omp teams distribute private(i)); (-7, OpenMP, #pragma omp teams distribute shared ())"}
{"predict": "(40,OpenMP,#pragma omp parallel for)", "label": "(33, CUDA, cudaMalloc( (void**) &fn_d,  XYZ_SIZE*sizeof(FLOAT));); (30, OpenMP, start_time_total = omp_get_wtime();); (55, OpenMP, elapsed_time_total = omp_get_wtime() - start_time_total;); (33, OpenMP, start_computation_time = omp_get_wtime();); (50, OpenMP, elapsed_computation_time = omp_get_wtime() - start_computation_time;); (29, CUDA, cudaMemcpy(f_d,f,XYZ_SIZE*sizeof(FLOAT), cudaMemcpyHostToDevice);); (50, CUDA, cudaMemcpy(f, f_d,XYZ_SIZE*sizeof(FLOAT), cudaMemcpyDeviceToHost);); (25, CUDA, cudaMalloc( (void**) &f_d,  XYZ_SIZE*sizeof(FLOAT));); (51, CUDA, cudaFree(fn_d);); (49, CUDA, cudaFree(f_d);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (27,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for), (41,OpenMP,#pragma omp parallel for), (50,OpenMP,#pragma omp parallel for), (63,OpenMP,#pragma omp parallel for), (70,OpenMP,#pragma omp parallel for), (77,OpenMP,#pragma omp parallel for), (84,OpenMP,#pragma omp parallel for), (91,OpenMP,#pragma omp parallel for), (98,OpenMP,#pragma omp parallel for), (105,OpenMP,#pragma omp parallel for), (112,OpenMP,#pragma omp parallel for), (120,OpenMP,#pragma omp parallel for)", "label": "(12, OpenMP, #pragma omp master); (9, OpenMP, #pragma omp parallel default(shared))"}
{"predict": "(10,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for)", "label": "(10, MPI, MPI_Allreduce(MPI_IN_PLACE, buf, N, MPIX_BFLOAT16, MPI_SUM, comm);); (20, MPI, MPI_Allreduce(MPI_IN_PLACE, buf_f16, N, MPIX_C_FLOAT16, MPI_SUM, comm);)"}
{"predict": "(11,MPI,MPI_Init(NULL, NULL);) (28,MPI,MPI_Finalize();) (29,OpenMP,#pragma omp parallel for)", "label": "(32, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (34, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "155:   if (odd_rank_out) {\n156:     MPI_Send(\n157:       recvbuf, count, type, right_rank, LWGRP_MSG_TAG_0, comm\n158:     );\n159:   }\n160: \n161:   /* clean up */\n162:   if (tempbuf != recvbuf) {\n163:     lwgrp_type_dtbuf_free(tempbuf);\n164:   }\n165:   return 0;\n166: }", "label": "(133, MPI, MPI_Waitall(k, request, status);); (90, MPI, MPI_Reduce_local(recvbuf, tempbuf, count, type, op);)"}
{"predict": "(8,MPI,MPI_Comm_size(comm, &size);)\n(17,OpenMP,#pragma omp parallel for reduction(+:errs))\n(24,OpenMP,#pragma omp parallel for reduction(+:errs))", "label": "(10, MPI, MPI_Comm_size(comm, &size);); (15, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (22, MPI, MPI_Allreduce(bufin, bufout, count, dtype, MPI_SUM, comm);)"}
{"predict": "(74,OpenMP,#pragma omp parallel for), (76,OpenMP,#pragma omp parallel for), (78,OpenMP,#pragma omp parallel for)", "label": "(68, CUDA, checkCudaErrors(cudaMemset(d_Kernel, 0, DATA_SIZE));); (110, CUDA, checkCudaErrors(cudaFree(d_Kernel));); (40, CUDA, findCudaDevice(argc, (const char **)argv);); (72, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (49, CUDA, checkCudaErrors(cudaMalloc((void **)&d_Kernel, DATA_SIZE));); (105, CUDA, checkCudaErrors(cudaFree(d_Data));); (48, CUDA, checkCudaErrors(cudaMalloc((void **)&d_Data, DATA_SIZE));)"}
{"predict": "(28,MPI,mpirc = MPI_File_open(comm, name, amode, MPI_INFO_NULL, &fh);) (43,MPI,mpirc = MPI_File_set_view(fh, disp, MPI_UINT64_T, MPI_UINT64_T, datarep, MPI_INFO_NULL);) (49,MPI,mpirc = MPI_File_read(fh, &version_packed, 1, MPI_UINT64_T, &status);) (55,MPI,mpirc = MPI_File_read(fh, data, count, MPI_BYTE, &status);) (70,MPI,mpirc = MPI_File_close(&fh);)", "label": "(39, MPI, MPI_Error_string(mpirc, mpierrstr, &mpierrlen);); (98, MPI, MPI_Barrier(MPI_COMM_WORLD);); (22, MPI, int mpirc = MPI_File_open(MPI_COMM_WORLD, (char*)name, amode, MPI_INFO_NULL, &fh);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (79, MPI, double end_read = MPI_Wtime();); (3, MPI, double start_read = MPI_Wtime();); (67, MPI, mpirc = MPI_File_close(&fh);); (30, MPI, mpirc = MPI_File_set_view(fh, disp, MPI_BYTE, MPI_BYTE, datarep, MPI_INFO_NULL);); (47, MPI, MPI_Bcast(&version, 1, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (36, MPI, mpirc = MPI_File_read_at(fh, 0, &version_packed, 8, MPI_BYTE, &status);)"}
{"predict": ":             ADIOI_Free(len_list);\n110:         }\n111:         ADIOI_WriteStridedCollInd(fd, buf, count, datatype, file_ptr_type, offset,\n112:                                   status, error_code);\n113:         return;\n114:     }\n115: \n116:     /* initialize arrays to store the number of requests from each process */\n117:     count_my_req_per_proc = (MPI_Count *) ADIOI_Malloc(nprocs * sizeof(MPI_Count));\n118:     count_others_req_per_proc = (MPI_Count *) ADIOI_Malloc(nprocs * sizeof(MPI_Count));\n119: \n120:     /* each process sends its request to other processes */\n121:     ADIOI_Gen_access_reqs(fd, count, datatype, file_ptr_type, offset, &others_req,\n122:                           &count_others_req_per_proc, &count_others_req_procs, &count_my_req_per_proc,\n123:                           &count_my_req_procs);\n124: \n125:     /* calculate the number of processes that need to be involved in\n126:      * collective I/O */\n127:     for (i = 0; i < nprocs; i++)\n128:         if (count_others_req_per_proc[i] > 0)\n129:             nprocs_for_coll++;\n130: \n131:     /* each process determines the start and end offsets for collective I/O */\n132:     ADIOI_Calc_io_offs(fd, count, datatype, file_ptr_type, offset, &fd_start, &fd_end,\n133:                        &min_st_offset, &contig_access_count);\n134: \n135:     /* allocate space for the request array */\n136:     ADIOI_Datatype_contig_access_count(datatype, &contig_access_count);\n137:     my_req = (ADIOI_Access *) ADIOI_Malloc(nprocs_for_coll * sizeof(ADIOI_Access));\n138:     others_req = (ADIOI_Access *) ADIOI_Malloc(nprocs_for_coll * sizeof(ADIOI_Access));\n139: \n140:     /* fill in the request arrays */\n141:     ADIOI", "label": "(191, MPI, MPI_Allreduce(&tmp_error, error_code, 1, MPI_INT, MPI_MAX, fd->comm);); (65, MPI, MPI_Comm_size(fd->comm, &nprocs);); (93, MPI, MPI_Allgather(&end_offset, 1, ADIO_OFFSET, end_offsets, 1, ADIO_OFFSET, fd->comm);); (91, MPI, MPI_Allgather(&start_offset, 1, ADIO_OFFSET, st_offsets, 1, ADIO_OFFSET, fd->comm);); (63, MPI, MPI_Comm_rank(fd->comm, &myrank);); (210, MPI, MPI_Type_size_x(datatype, &size);)"}
{"predict": "(3,MPI,atom->m  = (real*) MPI_Scatter(sendbuf, N, MPI_REAL, atom->m, N, MPI_REAL, root, MPI_COMM_WORLD);) (4,MPI,atom->x  = (real*) MPI_Scatter(sendbuf, N, MPI_REAL, atom->x, N, MPI_REAL, root, MPI_COMM_WORLD);) (5,MPI,atom->y  = (real*) MPI_Scatter(sendbuf, N, MPI_REAL, atom->y, N, MPI_REAL, root, MPI_COMM_WORLD);) (6,MPI,atom->z  = (real*) MPI_Scatter(sendbuf, N, MPI_REAL, atom->z, N, MPI_REAL, root, MPI_COMM_WORLD);) (7,MPI,atom->vx = (real*) MPI_Scatter(sendbuf, N, MPI_REAL, atom->vx, N, MPI_REAL, root, MPI_COMM_WORLD);) (8,MPI,atom->vy = (real*) MPI_Scatter(sendbuf, N, MPI_REAL, atom->vy, N, MPI_REAL, root, MPI_COMM_WORLD);) (9,MPI,atom->vz = (real*) MPI_Scatter(sendbuf, N, MPI_REAL, atom->vz, N, MPI_REAL, root, MPI_COMM_WORLD);) (10,MPI,atom->fx = (real*) MPI_Scatter(sendbuf, N, MPI_REAL, atom->fx, N, MPI_REAL, root, MPI_COMM_WORLD);) (11,MPI,atom->fy = (real*) MPI_Scatter(sendbuf, N, MPI_REAL, atom->fy, N, MPI_REAL, root, MPI_COMM_WORLD);) (12,MPI,atom->fz = (real*) MPI_Scatter(sendbuf, N, MPI_REAL, atom->fz, N, MPI_REAL, root, MPI_COMM_WORLD);) (13,MPI,atom->pe = (real*) MPI_Scatter(sendbuf, N, MPI_REAL, atom->pe, N, MPI_REAL, root, MPI_COMM_WORLD);) (14,MPI,atom->ke = (real*) MPI_Scatter(sendbuf, N, MPI_REAL, atom->ke, N, MPI_REAL, root, MPI_COMM_WORLD);) (15,MPI,atom->box = (real*) MPI_Scatter(sendbuf", "label": "(23, CUDA, CHECK(cudaMalloc((void**)&atom->g_fz, sizeof(real) * N));); (19, CUDA, CHECK(cudaMalloc((void**)&atom->g_z, sizeof(real) * N));); (17, CUDA, CHECK(cudaMalloc((void**)&atom->g_y, sizeof(real) * N));); (21, CUDA, CHECK(cudaMalloc((void**)&atom->g_pe, sizeof(real) * N));); (14, CUDA, CHECK(cudaMalloc((void**)&atom->g_x, sizeof(real) * N));); (12, CUDA, CHECK(cudaMalloc((void**)&atom->g_NL, sizeof(int) * N * MN));); (16, CUDA, CHECK(cudaMalloc((void**)&atom->g_fy, sizeof(real) * N));); (9, CUDA, CHECK(cudaMalloc((void**)&atom->g_NN, sizeof(int) * N));); (13, CUDA, CHECK(cudaMalloc((void**)&atom->g_fx, sizeof(real) * N));)"}
{"predict": "(21,OpenMP,#pragma omp parallel for private(dim_arg_global))", "label": "(25, OpenCL, err = clEnqueueWriteBuffer( queue, buf[1], CL_TRUE, 0, sizeof(int), &dim_arg_global, 0, NULL, NULL);)"}
{"predict": "(40,OpenMP,#pragma omp parallel for), (62,OpenMP,#pragma omp parallel for), (76,OpenMP,#pragma omp parallel for), (86,OpenMP,#pragma omp parallel for)", "label": "(106, OpenMP, #pragma omp parallel for firstprivate(numElemReg)); (53, OpenMP, #pragma omp for firstprivate(numElemReg)); (65, OpenMP, #pragma omp for nowait firstprivate(numElemReg, eosvmin)); (34, OpenMP, #pragma omp parallel); (74, OpenMP, #pragma omp for nowait firstprivate(numElemReg, eosvmax)); (36, OpenMP, #pragma omp for nowait firstprivate(numElemReg))"}
{"predict": "(11,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for)", "label": "(13, CUDA, PYCUDA_ASSERT_NOGIL(cudaStreamCreateWithPriority(&stream, flags, priority));)"}
{"predict": "(30,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for)", "label": "(38, CUDA, cudaFree(res_dev);); (31, CUDA, cudaMalloc(&res_dev, sizeof(float));); (33, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(30, MPI, MPI_Comm_size(comm, &mpi_size);) (44, OpenMP, #pragma omp parallel for)", "label": "(40, MPI, MPI_Error_class(*error_code, &rc);); (12, MPI, MPI_Comm_size(comm, &mpi_size);); (42, MPI, MPI_Bcast(&rc, 1, MPI_INT, 0, comm);)"}
{"predict": "(20,OpenMP,#pragma omp parallel for), (47,OpenMP,#pragma omp parallel for), (72,OpenMP,#pragma omp parallel for)", "label": "(81, CUDA, cudaError result = cudaDeviceSynchronize();); (30, CUDA, Tensor tensor_S = make_tensor(make_gmem_ptr(thrust::raw_pointer_cast(d_S.data())), make_layout(tensor_shape));); (14, CUDA, thrust::host_vector<Element> h_S(size(tensor_shape));); (22, CUDA, thrust::device_vector<Element> d_D = h_D;); (79, CUDA, std::cerr << \"CUDA Runtime error: \" << cudaGetErrorString(result) << std::endl;); (12, CUDA, thrust::host_vector<Element> h_D(size(tensor_shape));); (26, CUDA, Tensor tensor_D = make_tensor(make_gmem_ptr(thrust::raw_pointer_cast(d_D.data())), make_layout(tensor_shape));); (17, CUDA, thrust::device_vector<Element> d_S = h_S;)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for), (36,OpenMP,#pragma omp parallel for), (44,OpenMP,#pragma omp parallel for), (50,OpenMP,#pragma omp parallel for)", "label": "(3, CUDA, __shared__ float siftParts2[16*64];); (1, CUDA, __shared__ float siftParts1[17*64];)"}
{"predict": "(14,OpenMP,#pragma omp parallel for)", "label": "(12, TBB, my_thread_array = tbb::cache_aligned_allocator<padded_private_worker>().allocate( my_n_thread );)"}
{"predict": "(2,OpenMP,#pragma omp parallel for), (75,OpenMP,#pragma omp barrier), (92,OpenMP,#pragma omp barrier)", "label": "(52, CUDA, __shared__ bool isAccepted;); (52, CUDA, __shared__ T sSumVal;)"}
{"predict": "(27,OpenMP,#pragma omp parallel for private(target)), (40,OpenMP,#pragma omp parallel for private(i,j))", "label": "(54, MPI, MPI_Free_mem(dst_buf);); (9, MPI, MPI_Alloc_mem(sizeof(double) * nproc * MAXELEMS, MPI_INFO_NULL, &dst_buf);); (30, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, target, 0, dst_win);); (50, MPI, MPI_Win_free(&dst_win);); (13, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, me, 0, dst_win);); (17, MPI, MPI_Win_unlock(me, dst_win);); (18, MPI, MPI_Barrier(MPI_COMM_WORLD);); (28, MPI, MPI_Win_unlock(target, dst_win);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for reduction(+:total))", "label": "(7, OpenMP, omp_init_lock(&l);); (27, OpenMP, omp_destroy_lock(&l);); (7, OpenMP, #pragma omp parallel); (14, OpenMP, omp_set_lock(&l);); (21, OpenMP, omp_unset_lock(&l);); (-1, OpenMP, auto numThreads = omp_get_max_threads();)"}
{"predict": "(13,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for)", "label": "(22, OpenMP, #pragma omp target parallel for simd private(argc > 0 ? argv[1] : argv[2])); (36, OpenMP, #pragma omp target parallel for simd private(e, g)); (44, OpenMP, #pragma omp target parallel for simd private(i)); (49, OpenMP, #pragma omp target parallel for simd private(j)); (47, OpenMP, #pragma omp parallel private(i)); (54, OpenMP, #pragma omp target parallel for simd private(m)); (4, OpenMP, #pragma omp target parallel for simd private(); (36, OpenMP, #pragma omp parallel); (32, OpenMP, #pragma omp target parallel for simd private(h, B::x)); (22, OpenMP, #pragma omp target parallel for simd private(a, b)); (24, OpenMP, #pragma omp target parallel for simd private(argv[1])); (17, OpenMP, #pragma omp target parallel for simd private(S1)); (38, OpenMP, #pragma omp parallel shared(i)); (12, OpenMP, #pragma omp target parallel for simd private(argc, z)); (2, OpenMP, #pragma omp target parallel for simd private(argc); (-2, OpenMP, #pragma omp target parallel for simd private()); (-9, OpenMP, #pragma omp target parallel for simd private); (2, OpenMP, #pragma omp target parallel for simd private(argc,)"}
{"predict": "(30,OpenMP,#pragma omp parallel for)", "label": "(81, MPI, MPI_Finalize();)"}
{"predict": "(11,MPI,MPI_Request req;); (17,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp barrier)", "label": "(9, MPI, MPI_Scan_c(sendbuf, recvbuf, count, datatype, MPI_SUM, comm);); (12, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (10, MPI, MPI_Iscan_c(sendbuf, recvbuf, count, datatype, MPI_SUM, comm, &req);)"}
{"predict": "(27,OpenMP,#pragma omp task shared(i,j))", "label": "(45, OpenMP, #pragma omp parallel firstprivate(i)); (40, OpenMP, #pragma omp parallel private(i)); (17, OpenMP, #pragma omp task shared(argc, z)); (39, OpenMP, #pragma omp task shared(i)); (7, OpenMP, #pragma omp task shared()); (20, OpenMP, #pragma omp task shared(argv[1])); (29, OpenMP, #pragma omp task shared(h, B::x)); (26, OpenMP, #pragma omp task shared(e, g)); (31, OpenMP, #pragma omp task firstprivate(i), shared(i)); (34, OpenMP, #pragma omp task shared(j)); (17, OpenMP, #pragma omp task shared(ba)); (6, OpenMP, #pragma omp task shared(argc > 0 ? argv[1] : argv[2])); (-5, OpenMP, #pragma omp task shared); (-4, OpenMP, #pragma omp task shared(); (23, OpenMP, #pragma omp task private(i), shared(i)); (0, OpenMP, #pragma omp task shared(argc,); (7, OpenMP, #pragma omp task shared(a, b, c, d, f)); (4, OpenMP, #pragma omp task shared(S1)); (13, OpenMP, #pragma omp task shared(da)); (-6, OpenMP, #pragma omp task shared(argc); (9, OpenMP, #pragma omp task shared(ca))"}
{"predict": "(3,OpenMP,#pragma omp parallel for), (10,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for)", "label": "(14, OpenMP, #pragma omp target map(p)); (51, OpenMP, #pragma omp target map(from: ps->ps->ps)); (38, OpenMP, #pragma omp target map(ps->i)); (31, OpenMP, #pragma omp target map(s.ps->ps->ps)); (32, OpenMP, #pragma omp target map(to: s.ps->ps->s.f[:22])); (27, OpenMP, #pragma omp target map(to: s.ps->ps)); (20, OpenMP, #pragma omp target map(to: s.p[:22])); (17, OpenMP, #pragma omp target map(s.p)); (2, OpenMP, #pragma omp target map(i)); (37, OpenMP, #pragma omp target map(to: ps->p[:22])); (18, OpenMP, #pragma omp target map(s.ps)); (27, OpenMP, #pragma omp target map(ps)); (30, OpenMP, #pragma omp target map(ps->s.f)); (41, OpenMP, #pragma omp target map(ps->ps->ps->ps)); (8, OpenMP, #pragma omp target map(s.s.f)); (-3, OpenMP, #pragma omp target map(i[1:23])); (-8, OpenMP, #pragma omp target map(d)); (3, OpenMP, #pragma omp target map(s.i)); (26, OpenMP, #pragma omp target map(from: ps->p)); (41, OpenMP, #pragma omp target map(from: s.f[:22]) map(to: ps->p[:33])); (38, OpenMP, #pragma omp target map(to: s.f[:22]) map(from: s.p[:33])); (41, OpenMP, #pragma omp target map(from: s.f[:22], s.s) map(to: ps->p[:33])); (-4, OpenMP, #pragma omp target map(s)); (25, OpenMP, #pragma omp target map(ps->ps)); (6, OpenMP, #pragma omp target map(from: s.ps->s.i)); (31, OpenMP, #pragma omp target map(to: ps->ps->ps->s.f[:22])); (-10, OpenMP, #pragma omp target map(p[1:24])); (23, OpenMP, #pragma omp target map(from: ps->ps->s.i))"}
{"predict": "(13,OpenMP,#pragma omp parallel for)", "label": "(15, CUDA, getLastCudaError(\"generateSampleRanksKernel<1U><<<>>> failed\\n\");); (19, CUDA, getLastCudaError(\"generateSampleRanksKernel<0U><<<>>> failed\\n\");)"}
{"predict": "(12,MPI,MPI_Comm_rank(comm, &rank); MPI_Comm_size(comm, &ranks);) (34,OpenMP,#pragma omp parallel for)", "label": "(13, MPI, MPI_Comm_size(comm, &ranks);); (11, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(16,OpenMP,#pragma omp parallel for)", "label": "(18, OpenCL, rc = clEnqueueNDRangeKernel(queue, write_kernel, 1, NULL, global_work_size, local_work_size, 0, NULL, NULL); CLERR;); (23, OpenCL, rc = clSetKernelArg(read_kernel, 4, sizeof(TYPE), &p2); CLERR;); (35, OpenCL, clReleaseKernel(read_kernel);); (28, OpenCL, rc = clEnqueueNDRangeKernel(queue, read_kernel, 1, NULL, global_work_size, local_work_size, 0, NULL, NULL);CLERR;); (5, OpenCL, cl_kernel read_kernel = clCreateKernel(program, \"kernel_modtest_read\", &rc); CLERR;); (20, OpenCL, rc = clSetKernelArg(read_kernel, 5, sizeof(cl_mem), &mc->err_count); CLERR;); (14, OpenCL, rc = clSetKernelArg(read_kernel, 0, sizeof(cl_mem), &mc->device_mem); CLERR;); (22, OpenCL, rc = clSetKernelArg(read_kernel, 9, sizeof(cl_mem), &mc->err_second_read); CLERR;); (0, OpenCL, cl_kernel write_kernel = clCreateKernel(program, \"kernel_modtest_write\", &rc); CLERR;); (8, OpenCL, rc = clSetKernelArg(write_kernel, 4, sizeof(TYPE), &p2); CLERR;); (11, OpenCL, rc = clSetKernelArg(read_kernel, 1, sizeof(cl_ulong), &mc->mem_size); CLERR;); (3, OpenCL, rc = clSetKernelArg(write_kernel, 1, sizeof(cl_ulong), &mc->mem_size); CLERR;); (14, OpenCL, rc = clSetKernelArg(read_kernel, 6, sizeof(cl_mem), &mc->err_addr); CLERR;); (0, OpenCL, rc = clSetKernelArg(write_kernel, 0, sizeof(cl_mem), &mc->device_mem); CLERR;); (22, OpenCL, clReleaseKernel(write_kernel);); (7, OpenCL, rc = clSetKernelArg(read_kernel, 2, sizeof(unsigned int), &offset); CLERR;); (0, OpenCL, rc = clSetKernelArg(write_kernel, 3, sizeof(TYPE), &p1); CLERR;); (6, OpenCL, rc = clSetKernelArg(read_kernel, 3, sizeof(TYPE), &p1); CLERR;); (-3, OpenCL, rc = clSetKernelArg(write_kernel, 2, sizeof(unsigned int), &offset); CLERR;); (9, OpenCL, rc = clSetKernelArg(read_kernel, 8, sizeof(cl_mem), &mc->err_current); CLERR;); (7, OpenCL, rc = clSetKernelArg(read_kernel, 7, sizeof(cl_mem), &mc->err_expect); CLERR;)"}
{"predict": "vector[i] -= 2*aux;\n140:         }\n141:         for (i=0; i<vector_length; i++) {\n142:           aux = (3 - (i&7));\n143:           if (aux>0) vector[i] -= 2*vector[index[i]];\n144:           else       vector[i] -= 2*aux;\n145:         }\n146:       }\n147:       break;\n148: \n149:     case INS_HEAVY:\n150:       for (iter=0; iter<iterations; iter+=2) {\n151:         for (i=0; i<vector_length; i++) {\n152:           aux = -(3 - (i&7));\n153:           if (vector[index[i]]>0) vector[i] -= 2*vector[i];\n154:           else                    vector[i] -= 2*aux;\n155:         }\n156:         for (i=0; i<vector_length; i++) {\n157:           aux = (3 - (i&7));\n158:           if (vector[index[i]]>0) vector[i] -= 2*vector[i];\n159:           else                    vector[i] -= 2*aux;\n160:         }\n161:       }\n162:       break;\n163:   }\n164: \n165:   branch_time = wtime() - branch_time;\n166: \n167:   /* sum up the total and check against reference */\n168: \n169:   for (i=0; i<vector_length; i++) total += vector[i];\n170:   total_sum = total;\n171:   MPI_Reduce(&total_sum, &total_ref, 1, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD);\n172: \n173:   if (my_ID == root) {\n174:     printf(\"Total = %d;  Reference = %d;  Error = %d\\n\", total, total_ref, total-error);\n175:   }\n176: \n177:   prk_free(vector);\n178:   return 0;\n179:", "label": "(211, MPI, MPI_Reduce(&total, &total_sum, 1, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD);); (234, MPI, MPI_Finalize();); (83, MPI, MPI_Bcast(&iterations,    1, MPI_INT, root, MPI_COMM_WORLD);); (25, MPI, MPI_Init(&argc,&argv);); (80, MPI, MPI_Bcast(&vector_length, 1, MPI_INT, root, MPI_COMM_WORLD);); (81, MPI, MPI_Bcast(&btype,         1, MPI_INT, root, MPI_COMM_WORLD);); (97, MPI, MPI_Barrier(MPI_COMM_WORLD);); (23, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (21, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": ":     /* write out the pre-read data */\n118:     if (*hole) {\n119:         to_ptr = (char *) write_buf;\n120:         for (i = 0; i < sum; i++) {\n121:             len = srt_len[i];\n122:             memcpy(to_ptr, all_recv_buf + srt_off[i], len);\n123:             to_ptr += len;\n124:         }\n125:     }\n126:     ADIOI_Free(all_recv_buf);\n127:     ADIOI_Free(rdispls);\n128:     ADIOI_Free(rtail);\n129:     *error_code = MPI_SUCCESS;\n130: }", "label": "(121, MPI, gpfsmpio_prof_cw[GPFSMPIO_CIO_T_DEXCH_SORT] += MPI_Wtime() - io_time;); (90, MPI, gpfsmpio_prof_cw[GPFSMPIO_CIO_T_DEXCH_NET] += MPI_Wtime() - io_time;); (23, MPI, int ret = MPI_Alltoall(recv_size, 1, MPI_COUNT, send_size, 1, MPI_COUNT, fd->comm);); (19, MPI, io_time = MPI_Wtime();); (24, MPI, gpfsmpio_prof_cw[GPFSMPIO_CIO_T_DEXCH_RECV_EXCH] += MPI_Wtime() - io_time;); (132, MPI, gpfsmpio_prof_cw[GPFSMPIO_CIO_T_DEXCH_SIEVE] += MPI_Wtime() - io_time;); (75, MPI, gpfsmpio_prof_cw[GPFSMPIO_CIO_T_DEXCH_SETUP] += MPI_Wtime() - io_time;)"}
{"predict": "(1, MPI, #include <mpi.h>;)\n(2, MPI, MPI_Init(&argc, &argv);)\n(6, MPI, const int rank = MPI_Comm_rank(MPI_COMM_WORLD, &rank);)\n(6, MPI, const int size = MPI_Comm_size(MPI_COMM_WORLD, &size);)\n(6, MPI, const int block_size = B / size;)\n(6, MPI, const int start = rank * block_size;)\n(6, MPI, const int end = (rank == size - 1) ? B : start + block_size;)\n(6, MPI, for (int b = start; b < end; b++) {)\n(6, MPI, MPI_Barrier(MPI_COMM_WORLD);)\n(6, MPI, })", "label": "(10, CUDA, __shared__ float v[_N_], gy[_N_];)"}
{"predict": "(11,MPI,*w = MPI_Wtime();); (14,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for)", "label": "(14, CUDA, cudaDeviceSynchronize();); (12, CUDA, cudaMemcpy(&count,listp+fluidnum,sizeof(unsigned),cudaMemcpyDeviceToHost);); (6, CUDA, cudaMemset(listp+fluidnum,0,sizeof(unsigned));)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (...)", "label": "(6, CUDA, st = cudaMalloc(&test_ptr, TEST_ALLOC_SIZE);); (25, CUDA, cudaFree(test_ptr);)"}
{"predict": "(8,MPI,*w = MPI_Wtime();); (10,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (29,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for), (35,OpenMP,#pragma omp parallel for)", "label": "(45, MPI, MPI_Buffer_detach(&bptr, &bl);); (25, MPI, MPI_Recv(b, 10, MPI_INT, i, 27 + j, MPI_COMM_WORLD, &status);); (8, MPI, MPI_Buffer_attach(buf, BUFSIZE);); (14, MPI, MPI_Wait(&request, &status);); (12, MPI, MPI_Ibsend(a, 10, MPI_INT, 0, 27 + j, MPI_COMM_WORLD, &request);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)"}
{"predict": "(3,OpenMP,#pragma omp parallel for), (10,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for), (43,OpenMP,#pragma omp parallel for), (46,OpenMP,#pragma omp parallel for), (49,OpenMP,#pragma omp parallel for), (51,OpenMP,#pragma omp parallel for), (53,OpenMP,#pragma omp parallel for), (56,OpenMP,#pragma omp parallel for), (59,OpenMP,#pragma omp parallel for)", "label": "(32, OpenMP, #pragma omp for lastprivate(argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (73, OpenMP, #pragma omp parallel private(i)); (22, OpenMP, #pragma omp for lastprivate(argc,); (45, OpenMP, #pragma omp for lastprivate(a, b)); (12, OpenMP, #pragma omp for lastprivate()); (2, OpenMP, #pragma omp parallel); (69, OpenMP, #pragma omp for lastprivate(j)); (60, OpenMP, #pragma omp for lastprivate(i)); (4, OpenMP, #pragma omp for lastprivate(); (11, OpenMP, #pragma omp for lastprivate(argc); (30, OpenMP, #pragma omp for lastprivate(foo:argc)); (-3, OpenMP, #pragma omp for lastprivate); (32, OpenMP, #pragma omp for lastprivate(conditional: argc,s) lastprivate(conditional:); (15, OpenMP, #pragma omp for lastprivate(argc > 0 ? argv[1] : argv[2])); (46, OpenMP, #pragma omp for lastprivate(h)); (37, OpenMP, #pragma omp for lastprivate(argv[1])); (40, OpenMP, #pragma omp for lastprivate(k, e, g)); (56, OpenMP, #pragma omp parallel shared(i)); (18, OpenMP, #pragma omp for lastprivate(S1))"}
{"predict": "(29,MPI,*w = MPI_Wtime(););", "label": "(30, CUDA, cudaCheck(cudaGetLastError());)"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(8, MPI, MPI_Finalize();); (0, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(13,OpenMP,#pragma omp critical), (27,OpenMP,#pragma omp critical), (42,OpenMP,#pragma omp critical)", "label": "(25, CUDA, auto UserTarget = SemaRef.IdentifyCUDATarget(FD);); (23, CUDA, auto VarTarget = SemaRef.IdentifyCUDATarget(Var);)"}
{"predict": "(26,OpenMP,#pragma omp parallel for collapse(2))", "label": "(35, CUDA, CHECK_CUDA(cudaMalloc((void**)&imgs_dev, img_N * 3 * img_w * img_h * sizeof(uchar)));)"}
{"predict": "(1,MPI,)#include <mpi.h> (9,MPI,MPI_Init(&argc, &argv);), (11,MPI,me = MPI_Comm_rank(MPI_COMM_WORLD);), (12,MPI,nproc = MPI_Comm_size(MPI_COMM_WORLD);), (13,MPI,MPI_Win_create(NULL, 0, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &win);), (27,MPI,MPI_Win_fence(0, win);), (32,MPI,MPI_Win_fence(0, win);), (38,MPI,MPI_Win_fence(0, win);), (45,MPI,MPI_Win_fence(0, win);), (52,MPI,MPI_Win_fence(MPI_MODE_NOPRECEDE, win);), (60,MPI,MPI_Win_fence(MPI_MODE_NOSUCCEED, win);), (67,MPI,MPI_Win_lock(MPI_LOCK_SHARED, 0, MPI_MODE_NOCHECK, win);), (70,MPI,MPI_Win_unlock(0, win);), (97,MPI,MPI_Barrier(MPI_COMM_WORLD);), (100,MPI,MPI_Win_fence(0, win);), (109,MPI,MPI_Finalize();), (110,MPI,return MTestReturnValue(errs);)", "label": "(119, MPI, MPI_Free_mem(data);); (30, MPI, MPI_Win_fence(0, win);); (73, MPI, MPI_Win_fence(MPI_MODE_NOSUCCEED, win);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (12, MPI, MPI_Win_fence(MPI_MODE_NOPRECEDE, win);); (110, MPI, MPI_Win_free(&win);); (28, MPI, MPI_Accumulate(&mine, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_MAXLOC, win);); (5, MPI, MPI_Alloc_mem(sizeof(twoint_t), MPI_INFO_NULL, &data);); (71, MPI, MPI_Win_lock(MPI_LOCK_SHARED, 0, MPI_MODE_NOCHECK, win);); (72, MPI, MPI_Win_unlock(0, win);); (73, MPI, MPI_Barrier(MPI_COMM_WORLD);); (4, MPI, MPI_Win_create(data, me == 0 ? sizeof(twoint_t) : 0, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &win);); (41, MPI, MPI_Accumulate(&mine, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_MINLOC, win);)"}
{"predict": "(6,MPI,MPI_Init(&argc, &argv);) (7,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);) (8,MPI,MPI_Comm_size(MPI_COMM_WORLD, &size);) (11,OpenMP,#pragma omp parallel for) (35,OpenMP,#pragma omp parallel for)", "label": "(6, MPI, MPI_Init(&argc, &argv);); (25, MPI, MPI_Send(&message, 1, MPI_INT, next, tag, MPI_COMM_WORLD);); (60, MPI, MPI_Finalize();); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (34, MPI, MPI_Recv(&message, 1, MPI_INT, prev, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)"}
{"predict": "(7,MPI,*w = MPI_Wtime();); (15,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for)", "label": "(12, CUDA, CUCH(cudaMalloc(&ptr, plan_size * sizeof(int32_t)), mhcudaMemoryAllocationFailure);); (6, CUDA, CUCH(cudaSetDevice(devs[devi]), mhcudaNoSuchDevice);)"}
{"predict": "(12,MPI,*w = MPI_Wtime(););\n(18,OpenMP,#pragma omp parallel for),", "label": "(9, MPI, MPI_Comm_rank(comm, &comm_rank);); (10, MPI, MPI_Comm_size(comm, &comm_size);)"}
{"predict": "(57,OpenMP,#pragma omp parallel for reduction(+:nstream_time)), (62,MPI,MPI_Bcast(&nstream_time, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD)), (64,OpenMP,#pragma omp parallel for), (72,MPI,MPI_Gather(h_A.data(), bytes, MPI_DOUBLE, h_A.data(), bytes, MPI_DOUBLE, 0, MPI_COMM_WORLD)), (90,OpenMP,#pragma omp parallel for reduction(+:asum)), (102,MPI,MPI_Reduce(&nstream_time, &nstream_time, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD))", "label": "(80, SYCL, sycl::free(d_A, q);); (32, SYCL, prk::SYCL::print_device_platform(q);); (51, SYCL, double *d_C = sycl::malloc_device<double>(length, q);); (76, SYCL, sycl::free(d_B, q);); (74, SYCL, sycl::free(d_C, q);); (47, SYCL, double *d_B = sycl::malloc_device<double>(length, q);); (26, SYCL, sycl::queue q(sycl::default_selector_v);); (44, SYCL, double *d_A = sycl::malloc_device<double>(length, q);)"}
{"predict": "image_host           = cpu_workspace;\n146: \n147:             uint8_t* image_device_2       = (uint8_t*)workspace->gpu(size_image);\n148:             uint8_t* image_host_2         = (uint8_t*)workspace->cpu(size_image);\n149: \n150:             // Copy image to GPU\n151:             cudaMemcpyAsync(image_device, image_host, size_image, cudaMemcpyHostToDevice, preprocess_stream);\n152: \n153:             // Normalize image\n154:             normalize_->run(image_device, image_device, size_image, preprocess_stream);\n155: \n156:             // Copy normalized image back to CPU\n157:             cudaMemcpyAsync(image_host, image_device, size_image, cudaMemcpyDeviceToHost, preprocess_stream);\n158: \n159:             // Copy normalized image to GPU again\n160:             cudaMemcpyAsync(image_device_2, image_host, size_image, cudaMemcpyHostToDevice, preprocess_stream);\n161: \n162:             // Resize image\n163:             CUDAKernel::resize(\n164:                 preprocess_stream, image_device_2, image_device_2, input_width_, input_height_,\n165:                 input_width_, input_height_, CUDAKernel::Interpolation::BILINEAR\n166:             );\n167: \n168:             // Copy resized image to GPU\n169:             cudaMemcpyAsync(image_device, image_device_2, size_image, cudaMemcpyDeviceToDevice, preprocess_stream);\n170: \n171:             // Set tensor stream\n172:             tensor->set_stream(preprocess_stream, true);\n173: \n174:             return true;\n175:         }\n176: \n177:     private:\n178:         InferImpl(\n179:             const shared_ptr<TRT::Engine>& engine,\n180:             const tuple<string, int>& start_param,\n181:             bool use_multi_preprocess_stream\n182:         ) : ControllerImpl(engine, start_param, use_multi_preprocess_stream), use_multi_preprocess_stream_(use_multi_preprocess_stream)\n183:         {\n184:             input_width_  = 0;\n185:             input_height", "label": "(151, CUDA, checkCudaRuntime(cudaMemcpyAsync(image_device, image_host, size_image, cudaMemcpyHostToDevice, preprocess_stream));); (125, CUDA, checkCudaRuntime(cudaStreamCreate(&preprocess_stream));); (67, CUDA, checkCudaRuntime(cudaStreamSynchronize(mono->get_stream()));)"}
{"predict": "::ElementsAreArray({3, 4, 5}));\n104: \n105:     // Offload all 3 blocks, fill with predictable pattern, onboard\n106:     for (auto cacheBlockId : cacheBlockIds1)\n107:     {\n108:         auto block = blockManager.getBlockById(cacheBlockId, maxAttentionWindow);\n109:         EXPECT_TRUE(block->isPrimary());\n110:         // offload so we can write to block in CPU code\n111:         blockManager.offloadBlock(block, maxAttentionWindow);\n112:         EXPECT_FALSE(block->isPrimary());\n113:         // need to sync so D2H transfer is done before accessing blocks\n114:         // fill with predictable pattern\n115:         auto memoryPoolIndex = block->getMemoryPoolBlockIndex();\n116:         auto blockPtr{tr::ITensor::slice(secondaryPoolPtr, memoryPoolIndex, 1)};\n117:         auto rawBlockPtr = reinterpret_cast<T*>(blockPtr->data());\n118:         for (int i = 0; i < blockSize; ++i)\n119:         {\n120:             rawBlockPtr[i] = i & mask;\n121:         }\n122:         // onboard\n123:         blockManager.onboardBlock(block, maxAttentionWindow);\n124:         EXPECT_TRUE(block->isPrimary());\n125:         EXPECT_EQ(cudaDeviceSynchronize(), cudaSuccess);\n126:         EXPECT_TRUE(blockManager.verifyQueueIntegrity(maxAttentionWindow));\n127:     }\n128:     blockManager.releaseBlocks(seq1, llmRequest1);\n129: \n130:     // Verify that the cache block IDs are sequential\n131:     EXPECT_TRUE(cacheBlockIds1[0] == cacheBlockIds[0] + 3);\n132: }", "label": "(114, CUDA, EXPECT_EQ(cudaMemset(block2Ptr->data(), 0, blockSize * sizeof(T)), cudaSuccess);); (74, CUDA, EXPECT_EQ(cudaDeviceSynchronize(), cudaSuccess);)"}
{"predict": "(21,OpenMP,#pragma omp parallel for)", "label": "(25, CUDA, cudaMemcpy(hostFloats2, gpuFloats2, 4 * sizeof(float), cudaMemcpyDeviceToHost);); (37, CUDA, cudaFree(gpuFloats1);); (8, CUDA, cudaMalloc((void**)(&gpuFloats2), N * sizeof(float));); (21, CUDA, cudaMemcpy(hostFloats1, gpuFloats1, 4 * sizeof(float), cudaMemcpyDeviceToHost);); (3, CUDA, cudaMalloc((void**)(&gpuFloats1), N * sizeof(float));); (35, CUDA, cudaFree(gpuFloats3);); (20, CUDA, cudaMemcpy(hostFloats3, gpuFloats3, 4 * sizeof(float), cudaMemcpyDeviceToHost);); (32, CUDA, cudaFree(gpuFloats2);); (5, CUDA, cudaMalloc((void**)(&gpuFloats3), N * sizeof(float));)"}
{"predict": "(27,MPI,*w = MPI_Wtime();); (32,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for)", "label": "(28, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for reduction(+:threadSum)), (26,OpenMP,#pragma omp parallel for reduction(+:threadSum)), (44,OpenMP,#pragma omp parallel for)", "label": "(9, CUDA, __shared__ cuda::barrier<cuda::thread_scope_block> barrier;)"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(6, CUDA, auto points_geometry_it = thrust::make_counting_iterator(0);); (8, CUDA, auto linestring_geometry_it  = thrust::make_counting_iterator(0);)"}
{"predict": "1i = tmp * u[3][k][j][i];\n111:           u51i = tmp * u[4][k][j][i];\n112: \n113:           u21im1 = rho_i[k][j][i-1] * u[1][k][j][i-1];\n114:           u31im1 = rho_i[k][j][i-1] * u[2][k][j][i-1];\n115:           u41im1 = rho_i[k][j][i-1] * u[3][k][j][i-1];\n116:           u51im1 = rho_i[k][j][i-1] * u[4][k][j][i-1];\n117: \n118:           u21j = tmp * u[1][k][j+1][i];\n119:           u31j = tmp * u[2][k][j+1][i];\n120:           u41j = tmp * u[3][k][j+1][i];\n121:           u51j = tmp * u[4][k][j+1][i];\n122: \n123:           u21k = tmp * u[1][k+1][j][i];\n124:           u31k = tmp * u[2][k+1][j][i];\n125:           u41k = tmp * u[3][k+1][j][i];\n126:           u51k = tmp * u[4][k+1][j][i];\n127: \n128:           u[1][k][j][i] = ( u21i + u21j + u21k - u21im1 ) * tx1;\n129:           u[2][k][j][i] = ( u31i + u31j + u31k - u31im1 ) * tx1;\n130:           u[3][k][j][i] = ( u41i + u41j + u41k - u41im1 ) * tx1;\n1", "label": "(569, OpenMP, #pragma omp teams distribute parallel for collapse(3) private(i,j,k) num_teams(jend - jst)); (26, OpenMP, #pragma omp distribute parallel for collapse (3) private(tmp)); (106, OpenMP, #pragma omp simd simdlen(32) private (tmp, u21i, u31i, u41i, u51i, u21im1, u31im1, u41im1, u51im1)); (21, OpenMP, #pragma omp target teams); (15, OpenMP, #pragma omp target data); (59, OpenMP, #pragma omp distribute)"}
{"predict": "(2, CUDA, extern \"C\" __global__ void computeAttentionScoresKernel(\n(20, CUDA, if (key_idx < head_dim) {\n(33, CUDA, if (key_idx < seq_len) {", "label": "(11, CUDA, extern __shared__ char shared_mem[];)"}
{"predict": "(10,OpenMP,#pragma omp parallel for private(io_start, io_end))", "label": "(59, CUDA, CHECK_CUDA(cudaMalloc((void**)&ibuf[i].pixel_data[c], sz));); (56, CUDA, CHECK_CUDA(cudaFree(ibuf[i].pixel_data[c]));)"}
{"predict": "(14,MPI, MPI_Init(&argc, &argv);) (15,MPI, MPI_Comm_rank(comm, &par_rank);) (16,MPI, MPI_Comm_size(comm, &par_size);) (20,MPI, MPI_Allreduce(&par_rank, &par_size, 1, MPI_INT, MPI_MAX, comm);) (22,MPI, MPI_Allreduce(&par_rank, &par_size, 1, MPI_INT, MPI_MAX, comm);) (25,MPI, MPI_Allreduce(&par_rank, &par_size, 1, MPI_INT, MPI_MAX, comm);) (40,MPI, MPI_Bcast(&output_path, output_path.size(), MPI_CHAR, 0, comm);) (44,MPI, MPI_Bcast(&output_path, output_path.size(), MPI_CHAR, 0, comm);) (71,MPI, MPI_Allreduce(&par_rank, &par_size, 1, MPI_INT, MPI_MAX, comm);) (77,MPI, MPI_Allreduce(&par_rank, &par_size, 1, MPI_INT, MPI_MAX, comm);) (80,MPI, MPI_Allreduce(&par_rank, &par_size, 1, MPI_INT, MPI_MAX, comm);)", "label": "(93, MPI, ascent_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (99, MPI, MPI_Barrier(comm);); (16, MPI, MPI_Comm_rank(comm, &par_rank);); (16, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for reduction(+:s))", "label": "(10, OpenMP, f = test_choleskyc_decomp_dim(&p3.matrix, 2 * 8.0 * GSL_DBL_EPSILON);)"}
{"predict": "M, N, K, ldA, ldB, ldC, alpha, beta);\n116: }", "label": "(54, SYCL, std::vector<sycl::device> sycl_devices(1);); (3, SYCL, sycl::queue sycl_queue;)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for)", "label": "(15, TBB, tbb::enumerable_thread_specific<size_t> num_high_degree_nodes(0);)"}
{"predict": "- meas_st_off[i]) / meas_st_off[i]);\n140:   }\n141: \n142:   /* Compare values with threshold */\n143:   for (i = 0; i < 3; i++) {\n144:     if (test_val[i] > THRESHOLD_XL) {\n145:       st_result = ST_FAIL;\n146:       break;\n147:     }\n148:   }\n149: \n150:   /* Disable Self Test */\n151:   ism303dac_xl_self_test_set(&dev_ctx_xl, ISM303DAC_XL_ST_DISABLE);\n152: \n153:   /* Check if the test was successful */\n154:   if (st_result == ST_PASS) {\n155:     printf(\"Self Test Passed!\\n\");\n156:   } else {\n157:     printf(\"Self Test Failed!\\n\");\n158:   }\n159: \n160:   /* Restore default configuration */\n161:   ism303dac_xl_reset_set(&dev_ctx_xl, PROPERTY_ENABLE);\n162: \n163:   do {\n164:     ism303dac_xl_reset_get(&dev_ctx_xl, &reg.byte);\n165:   } while (reg.byte);\n166: \n167:   ism303dac_mg_reset_set(&dev_ctx_mg, PROPERTY_ENABLE);\n168: \n169:   do {\n169:   } while (reg.byte);\n170: \n171:   /* Wait sensor boot time */\n172:   platform_delay(BOOT_TIME);\n173: \n174:   /* Disable Block Data Update */\n175:   ism303dac_xl_block_data_update_set(&dev_ctx_xl, PROPERTY_DISABLE);\n176:   ism303dac_mg_block_data_update_set(&dev_ctx_mg, PROPERTY_DISABLE);\n177: \n178:   /* Disable sensor */\n179:   ism303dac_xl_power_off(&dev_ctx_xl);\n180:   ism303dac_mg_power_off(&dev_ctx_mg", "label": "(162, OpenMP, ism303dac_mg_offset_temp_comp_set(&dev_ctx_mg, PROPERTY_ENABLE);)"}
{"predict": "(17,OpenMP,#pragma omp parallel for)\n(29,OpenMP,#pragma omp parallel for)", "label": "(4, OpenMP, #pragma omp master taskloop simd); (2, OpenMP, #pragma omp parallel)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (12,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for)", "label": "(7, MPI, MPI_Info_get(info, key, MPI_MAX_INFO_VAL, buf, &flag);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (...)", "label": "(36, CUDA, __shared__ _VOLATILE_ float bSmem[16][128+4];); (34, CUDA, __shared__ _VOLATILE_ float aSmem[16][128+4];)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for)", "label": "(11, OpenMP, #pragma omp teams distribute simd shared (); (43, OpenMP, #pragma omp teams distribute simd shared(da)); (30, OpenMP, #pragma omp teams distribute simd shared (a, b, c, d, f)); (5, OpenMP, #pragma omp teams distribute simd shared); (34, OpenMP, #pragma omp teams distribute simd shared(ba)); (2, OpenMP, #pragma omp target); (35, OpenMP, #pragma omp teams distribute simd shared(ca)); (55, OpenMP, #pragma omp teams distribute simd shared(i)); (57, OpenMP, #pragma omp teams distribute simd shared(j)); (17, OpenMP, #pragma omp teams distribute simd shared (argc)); (43, OpenMP, #pragma omp teams distribute simd private(i), shared(i)); (24, OpenMP, #pragma omp teams distribute simd shared (argv[1])); (17, OpenMP, #pragma omp teams distribute simd shared (S1)); (43, OpenMP, #pragma omp teams distribute simd firstprivate(i), shared(i)); (9, OpenMP, #pragma omp teams distribute simd shared (argc > 0 ? argv[1] : argv[2])); (35, OpenMP, #pragma omp teams distribute simd shared(h, B::x)); (43, OpenMP, #pragma omp teams distribute simd private(i)); (51, OpenMP, #pragma omp teams distribute simd firstprivate(i)); (2, OpenMP, #pragma omp teams distribute simd shared (argc,); (28, OpenMP, #pragma omp teams distribute simd shared(e, g, z)); (-6, OpenMP, #pragma omp teams distribute simd shared ()); (-4, OpenMP, #pragma omp teams distribute simd shared (argc)"}
{"predict": "1:    }\n172: \n173:    /* Assemble the matrix */\n174:    HYPRE_IJMatrixAssemble(A);\n175: \n176:    /* Convert to ParCSR format */\n177:    HYPRE_IJMatrixGetObject(A, (void **) &parcsr_A);\n178: \n179:    /* Create the vectors */\n180:    HYPRE_IJVectorCreate(MPI_COMM_WORLD, ilower, iupper, &b);\n181:    HYPRE_IJVectorSetObjectType(b, HYPRE_PARCSR);\n182:    HYPRE_IJVectorInitialize(b);\n183: \n184:    HYPRE_IJVectorCreate(MPI_COMM_WORLD, ilower, iupper, &x);\n185:    HYPRE_IJVectorSetObjectType(x, HYPRE_PARCSR);\n186:    HYPRE_IJVectorInitialize(x);\n187: \n188:    /* Initialize vectors */\n189:    {\n190:       double values[1];\n191:       int i;\n192: \n193:       values[0] = 1.0;\n194: \n195:       for (i = ilower; i <= iupper; i++)\n196:       {\n197:          HYPRE_IJVectorSetValues(b, 1, &i, values);\n198:       }\n199: \n200:       HYPRE_IJVectorAssemble(b);\n201:       HYPRE_IJVectorAssemble(x);\n202:    }\n203: \n204:    /* Set up the preconditioner and solver */\n205:    HYPRE_ParCSRSetupInterpretor(&interpreter, parcsr_A, blockSize);\n206:    HYPRE_ParCSRSetupMatvecFunctions(&matvec_fn, parcsr_A, blockSize);\n207: \n208:    /* Set up solver */\n209:    HYPRE_LOBPCGCreate(MPI_COMM_WORLD, &lobpcg_solver);\n210:    HYPRE_LOBPCGSetBlockSize(lobpcg_solver, blockSize);\n211:    HYPRE_LOBPCGSetMaxIter(lobpcg_solver, 1000);\n21", "label": "(25, MPI, MPI_Init(&argc, &argv);); (79, MPI, MPI_Finalize();); (25, MPI, MPI_Comm_size(MPI_COMM_WORLD, &num_procs);); (23, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myid);)"}
{"predict": "(17,OpenMP,#pragma omp parallel for reduction(+:sum))", "label": "(1, TBB, using namespace oneapi::tbb::flow;)"}
{"predict": "(15,OpenMP,#pragma omp parallel for private(i,x1,y1,x2,y2,nomx,nomy,deno,errx,erry,err2))", "label": "(3, CUDA, __shared__ float homo[8*TESTHOMO_LOOPS];); (3, CUDA, __shared__ int cnts[TESTHOMO_TESTS*TESTHOMO_LOOPS];)"}
{"predict": "(10,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp target), (12,OpenMP,#pragma omp target), (13,OpenMP,#pragma omp target), (14,OpenMP,#pragma omp target), (15,OpenMP,#pragma omp target), (16,OpenMP,#pragma omp target), (17,OpenMP,#pragma omp target), (18,OpenMP,#pragma omp target), (19,OpenMP,#pragma omp target), (20,OpenMP,#pragma omp target), (21,OpenMP,#pragma omp target), (22,OpenMP,#pragma omp target), (23,OpenMP,#pragma omp target), (24,OpenMP,#pragma omp target), (25,OpenMP,#pragma omp target), (26,OpenMP,#pragma omp target), (27,OpenMP,#pragma omp target), (28,OpenMP,#pragma omp target), (29,OpenMP,#pragma omp target), (30,OpenMP,#pragma omp target), (31,OpenMP,#pragma omp target), (32,OpenMP,#pragma omp target), (33,OpenMP,#pragma omp target), (34,OpenMP,#pragma omp target), (35,OpenMP,#pragma omp target), (36,OpenMP,#pragma omp target), (37,OpenMP,#pragma omp target), (38,OpenMP,#pragma omp target), (39,OpenMP,#pragma omp target), (40,OpenMP,#pragma omp target), (41,OpenMP,#pragma omp target), (42,OpenMP,#pragma omp target), (43,OpenMP,#pragma omp target), (44,OpenMP,#pragma omp target), (45,OpenMP,#pragma omp target), (46,OpenMP,#pragma omp target), (47,OpenMP,#pragma omp target), (48,OpenMP,#pragma omp target), (49,OpenMP,#pragma omp target), (50,OpenMP,#pragma omp target), (51,OpenMP,#pragma omp target), (52,OpenMP,#pragma omp target), (53,OpenMP,#pragma omp target), (54,OpenMP,#pragma omp target), (55,OpenMP,#pragma omp target), (56,Open", "label": "(34, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (85, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(j)); (7, OpenMP, #pragma omp target); (59, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(S2::S2s)); (10, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (); (21, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argc,); (36, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (a, b, c, d, f)); (47, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(ca)); (66, OpenMP, #pragma omp teams distribute parallel for simd private(i), firstprivate(i)); (13, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argc); (40, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(ba, z)); (59, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(h)); (26, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (S1)); (33, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argv[1])); (52, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(S2::S2sc)); (63, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(i)); (2, OpenMP, #pragma omp teams distribute parallel for simd firstprivate ()); (-7, OpenMP, #pragma omp teams distribute parallel for simd firstprivate); (40, OpenMP, #pragma omp teams distribute parallel for simd firstprivate(da)); (72, OpenMP, #pragma omp teams distribute parallel for simd lastprivate(argc), firstprivate(argc)); (10, OpenMP, #pragma omp teams distribute parallel for simd firstprivate (argc > 0 ? argv[1] : argv[2]))"}
{"predict": "(1,MPI,*w = MPI_Wtime();); (17,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for), (41,OpenMP,#pragma omp parallel for)", "label": "(25, MPI, s[i] = MPI_Win_allocate_shared((rank==0?99:0)*sizeof(char), 1, MPI_INFO_NULL, node, &base_ptr[i], &win[i]);); (36, MPI, MPI_Barrier(node);); (-1, MPI, MPI_Init(&argc, &argv);); (44, MPI, if(win[i] != MPI_WIN_NULL) MPI_Win_free(&win[i]);); (46, MPI, MPI_Finalize();); (2, MPI, MPI_Comm_rank(node, &rank);); (16, MPI, int sam = MPI_Alloc_mem((rank==0?99:0)*sizeof(char), MPI_INFO_NULL, &base_ptr[i]);); (25, MPI, MPI_Win_shared_query(win[i], 0, &size[i], &a[i], &ptr[i]);); (-5, MPI, int ss = MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, 0, MPI_INFO_NULL, &node);)"}
{"predict": ".get<size_t>(\"buffer-size\");\n156:             }\n157:         });\n158:     parser\n159:         .add_argument({ \"--network\", \"--network-stats\" }, \"Enable network statistics\")\n160:         .count(0)\n161:         .action([](parser_t& p) {\n162:             tim::trait::apply<tim::trait::runtime_enabled>::set<network_stats>(true);\n163:         });\n164:     parser\n165:         .add_argument({ \"--output\", \"--output-file\" }, \"Write results to file\")\n166:         .count(0)\n167:         .action([](parser_t& p) {\n168:             tim::settings::file_output() = true;\n169:         });\n170:     parser\n171:         .add_argument({ \"--auto-output\" }, \"Automatically write results to file\")\n172:         .count(0)\n173:         .action([](parser_t& p) {\n174:             tim::settings::auto_output() = true;\n175:         });\n176:     parser\n177:         .add_argument({ \"--banner\", \"--banner-output\" }, \"Enable banner output\")\n178:         .count(0)\n179:         .action([](parser_t& p) {\n180:             tim::settings::banner() = true;\n181:         });\n182:     parser\n183:         .add_argument({ \"--scientific\", \"--scientific-notation\" },\n184:                       \"Use scientific notation for output\")\n185:         .count(0)\n186:         .action([](parser_t& p) {\n187:             tim::settings::scientific() = true;\n188:         });\n189:     parser\n190:         .add_argument({ \"--width\", \"--output-width\" },\n191:                       \"Set the width of the output\")\n192:         .count(1)\n193:         .dtype(\"size_t\")\n194:         .set_default(tim::settings::width())\n195:         .action([](parser_t& p) {\n196:             tim::settings::width() = p", "label": "(20, MPI, tim::settings::mpi_init()       = false;); (20, MPI, tim::settings::mpi_finalize()   = false;)"}
{"predict": "(35,MPI,*w = MPI_Wtime();)", "label": "(9, SYCL, const bool doHalfs = dev.has(sycl::aspect::fp16);); (47, SYCL, sycl::free(cd, queue);); (12, SYCL, const bool doDoubles = dev.has(sycl::aspect::fp64);)"}
{"predict": "(1, CUDA, __global__ void MatrixMulCUDA)", "label": "(42, CUDA, __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];); (37, CUDA, __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (...)", "label": "(8, CUDA, cudaStreamSynchronize(LuH->A_gpu.cuStreams[0]);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for)", "label": "(8, OpenMP, #pragma omp target teams device (argc); (5, OpenMP, #pragma omp target teams device ()); (10, OpenMP, #pragma omp target teams device (argc > 0 ? argv[1] : argv[2])); (15, OpenMP, #pragma omp target teams device (S1)); (12, OpenMP, #pragma omp target teams device (argc), device (argc+1)); (15, OpenMP, #pragma omp target teams device (-2)); (8, OpenMP, #pragma omp target teams device (argc + argc + z)); (15, OpenMP, #pragma omp target teams device (-10u)); (2, OpenMP, #pragma omp target teams device (argc))); (-7, OpenMP, #pragma omp target teams device); (14, OpenMP, #pragma omp target teams device (3.14)); (-7, OpenMP, #pragma omp target teams device ()"}
{"predict": "(49,OpenMP,#pragma omp parallel for), (83,OpenMP,#pragma omp parallel for)", "label": "(32, CUDA, checkCudaErrors(cudaMallocHost(&b, N_ROWS * sizeof(double)));); (27, CUDA, int dev = findCudaDevice(argc, (const char **)argv);); (67, CUDA, checkCudaErrors(cudaMalloc(&d_A, sizeof(float) * N_ROWS * N_ROWS));); (99, CUDA, checkCudaErrors(cudaFree(d_x_new));); (101, CUDA, checkCudaErrors(cudaFreeHost(b));); (95, CUDA, checkCudaErrors(cudaFree(d_A));); (64, CUDA, checkCudaErrors(cudaMalloc(&d_x, sizeof(double) * N_ROWS));); (92, CUDA, checkCudaErrors(cudaFree(d_b));); (93, CUDA, checkCudaErrors(cudaFree(d_x));); (59, CUDA, checkCudaErrors(cudaMalloc(&d_b, sizeof(double) * N_ROWS));); (57, CUDA, checkCudaErrors(cudaStreamCreateWithFlags(&stream1, cudaStreamNonBlocking));); (60, CUDA, checkCudaErrors(cudaMalloc(&d_x_new, sizeof(double) * N_ROWS));); (61, CUDA, checkCudaErrors(cudaMemsetAsync(d_x, 0, sizeof(double) * N_ROWS, stream1));); (21, CUDA, checkCudaErrors(cudaMallocHost(&A, N_ROWS * N_ROWS * sizeof(float)));); (90, CUDA, checkCudaErrors(cudaFreeHost(A));)"}
{"predict": "(27,OpenMP,#pragma omp parallel for)", "label": "(11, OpenMP, #pragma omp single private()); (39, OpenMP, #pragma omp parallel private(i)); (21, OpenMP, #pragma omp single private(a, b)); (16, OpenMP, #pragma omp single private(argc)); (23, OpenMP, #pragma omp single private(e, g)); (2, OpenMP, #pragma omp single private); (19, OpenMP, #pragma omp single private(argv[1])); (26, OpenMP, #pragma omp parallel); (9, OpenMP, #pragma omp single private(argc > 0 ? argv[1] : argv[2])); (20, OpenMP, #pragma omp single private(h, B::x)); (11, OpenMP, #pragma omp single private(S1)); (2, OpenMP, #pragma omp single private(argc); (34, OpenMP, #pragma omp single private(m)); (2, OpenMP, #pragma omp single private(argc,); (27, OpenMP, #pragma omp single private(j)); (16, OpenMP, #pragma omp single shared(i)); (-7, OpenMP, #pragma omp single private(); (22, OpenMP, #pragma omp parallel shared(i)); (18, OpenMP, #pragma omp single private(i))"}
{"predict": "(7,MPI,time_reference = MPI_Wtime();)\n(50,MPI,smpi->barrier();)\n(57,MPI,for( unsigned int num_dump=0; num_dump<restart_files.size(); num_dump++ ) {)\n(79,MPI,int problem = (prev_number != dump_number);)\n(85,MPI,MPI_Sendrecv(", "label": "(96, MPI, MPI_Finalize();); (88, MPI, MPI_Allreduce( &problem, &any_problem, 1, MPI_INT, MPI_LOR, smpi->world() );)"}
{"predict": "(20,OpenMP,#pragma omp parallel for simd is_device_ptr(k)) (26,OpenMP,#pragma omp parallel for simd is_device_ptr(z)) (31,OpenMP,#pragma omp parallel for simd is_device_ptr(aa)) (36,OpenMP,#pragma omp parallel for simd is_device_ptr(raa)) (41,OpenMP,#pragma omp parallel for simd is_device_ptr(h)) (46,OpenMP,#pragma omp parallel for simd is_device_ptr(rh)) (51,OpenMP,#pragma omp parallel for simd is_device_ptr(da))", "label": "(37, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(raa)); (48, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(rh)); (41, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(h)); (28, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(aa)); (51, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(da)); (20, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(z)); (13, OpenMP, #pragma omp target teams distribute parallel for simd is_device_ptr(k))"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(34, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(25,MPI,SCT->commVolRed += lenv * sizeof(double);)", "label": "(24, MPI, MPI_Send(unzval, lenv, MPI_DOUBLE, receiver, k, grid3d->zscp.comm);)"}
{"predict": "(31,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for), (45,OpenMP,#pragma omp parallel for), (56,OpenMP,#pragma omp parallel for), (71,OpenMP,#pragma omp parallel for)", "label": "(5, CUDA, __shared__ float4 buffer2[M7H*NUM];); (3, CUDA, __shared__ float4 buffer1[M7W*NDIM/4];)"}
{"predict": "details on how to use nvshmem are provided in the NVSHMEM documentation.\n100:     CUFFT_CHECK(cufftMpExecReshapeAsync(handle, src, dst, nullptr, 0, &stream));\n101:     CUFFT_CHECK(cufftMpDestroyReshape(handle));\n102:     nvshmem_free(src);\n103:     nvshmem_free(dst);\n104:     return 0;\n105: }", "label": "(15, CUDA, CUDA_CHECK(cudaGetDeviceCount(&ndevices));); (77, CUDA, CUDA_CHECK(cudaStreamCreate(&stream));); (5, MPI, MPI_Comm_size(comm, &size);); (119, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (12, CUDA, CUDA_CHECK(cudaSetDevice(rank % ndevices));); (-3, MPI, MPI_Init(&argc, &argv);); (90, MPI, MPI_Allreduce(&my_io_max, &global_io_max, 1, MPI_UNSIGNED_LONG_LONG, MPI_MAX, comm);); (116, CUDA, CUDA_CHECK(cudaMemcpy(dst_host.data(), dst, dst_host_expected.size() * sizeof(int), cudaMemcpyDefault));); (139, MPI, MPI_Finalize();); (118, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (94, CUDA, CUDA_CHECK(cudaMemsetAsync(dst, 0, dst_host_expected.size(), stream));); (92, CUDA, CUDA_CHECK(cudaMemcpyAsync(src, src_host.data(), src_host.size() * sizeof(int), cudaMemcpyDefault, stream));); (-6, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for), (54,OpenMP,#pragma omp parallel for)", "label": "(66, TBB, g.reset(tbb::flow::rf_reset_bodies);); (20, TBB, tbb::flow::make_edge( exe_node, *receivers[r] );); (58, TBB, inc_functor body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (64, TBB, body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (1, TBB, tbb::flow::graph g;); (45, TBB, tbb::flow::remove_edge( exe_node, *receivers[r] );); (24, TBB, tbb::flow::make_edge( *senders.back(), exe_node );); (4, TBB, tbb::flow::function_node< InputType, OutputType > exe_node( g, lc, cf );)"}
{"predict": "(10,MPI,self = m->getPCU()->Self();); (12,MPI,expandedPCU = m->getPCU();); (20,MPI,groupRank = inMap(self);); (21,MPI,group = groupMap(self);); (26,MPI,m->switchPCU(groupedPCU.get());); (33,MPI,m->switchPCU(expandedPCU););", "label": "(23, MPI, MPI_Comm_split(expandedPCU->GetMPIComm(), group, groupRank, &groupComm);); (31, MPI, MPI_Comm_free(&groupComm);)"}
{"predict": "(19,OpenMP,#pragma omp parallel for)", "label": "(19, CUDA, CudaCheckError();); (23, CUDA, cudaFree(du_new_dev);)"}
{"predict": "(6,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (56,OpenMP,#pragma omp parallel for), (70,OpenMP,#pragma omp parallel for)", "label": "(87, OpenCL, clReleaseMemObject(bufX);); (43, OpenCL, clReleaseCommandQueue(queue);); (27, OpenCL, printf( \"clCreateContext() failed with %d\\n\", err );); (85, OpenCL, clReleaseMemObject(bufA);); (79, OpenCL, clReleaseEvent(event);); (30, OpenCL, printf( \"clCreateCommandQueue() failed with %d\\n\", err );); (27, OpenCL, queue = clCreateCommandQueue(ctx, device, 0, &err);); (29, OpenCL, clReleaseContext(ctx);); (78, OpenCL, clReleaseMemObject(bufY);); (18, OpenCL, ctx = clCreateContext(props, 1, &device, NULL, NULL, &err);); (63, OpenCL, err = clWaitForEvents(1, &event);)"}
{"predict": "@__kmpc_omp_task_alloc(%struct.ident_t* [[DEFLOC]], i32 [[GTID]], i32 1, i64 80, i64 1, i32 (i32, i8*)* bitcast (i32 (i32, [[TDP_TY:%.+]]*)* [[TASK3:@.+]] to i32 (i32, i8*)*))\n55: // CHECK: [[TASK:%.+]] = bitcast i8* [[TASKV]] to [[TDP_TY]]*\n56: // CHECK: [[TASK_DATA:%.+]] = getelementptr inbounds [[TDP_TY]], [[TDP_TY]]* [[TASK]], i32 0, i32 0\n57: // CHECK: [[DOWN:%.+]] = getelementptr inbounds [[TD_TY:%.+]], [[TD_TY]]* [[TASK_DATA]], i32 0, i32 5\n58: // CHECK: store i64 0, i64* [[DOWN]],\n59: // CHECK: [[UP:%.+]] = getelementptr inbounds [[TD_TY]], [[TD_TY]]* [[TASK_DATA]], i32 0, i32 6\n60: // CHECK: store i64 9, i64* [[UP]],\n61: // CHECK: [[ST:%.+]] = getelementptr inbounds [[TD_TY]], [[TD_TY]]* [[TASK_DATA]], i32 0, i32 7\n62: // CHECK: store i64 1, i64* [[ST]],\n63: // CHECK: [[ST_VAL:%.+]] = load i64, i64* [[ST]],\n64: // CHECK: [[GRAINSIZE:%.+]] = zext i32 %{{.+}} to i64\n65: // CHECK: call void @__kmpc_taskloop(%struct.ident_t* [[DEFLOC]], i32 [[GTID]], i8* [[TASKV]], i32 1, i64* [[DOWN]], i64* [[UP]], i64 [[ST_VAL]], i32 1, i32 1, i64 [[GRAINSIZE]], i8* null)\n66:", "label": "(4, OpenMP, #pragma omp task); (25, OpenMP, #pragma omp master taskloop priority(argc)); (46, OpenMP, #pragma omp master taskloop); (71, OpenMP, #pragma omp master taskloop if(argc) shared(argc, argv) collapse(2) num_tasks(4)); (87, OpenMP, #pragma omp cancel taskgroup); (87, OpenMP, #pragma omp cancellation point taskgroup)"}
{"predict": "(23,MPI,cudaEventCreate(&start)); (24,MPI,cudaEventCreate(&stop)); (41,MPI,cudaEventRecord(start, 0)); (47,MPI,cudaEventRecord(stop, 0)); (49,MPI,cudaEventSynchronize(stop)); (50,MPI,cudaEventElapsedTime(&elapsedTime, start, stop));", "label": "(67, CUDA, cudaFree(B);); (54, CUDA, cudaEventRecord(stop);); (54, CUDA, cudaEventSynchronize(stop);); (65, CUDA, cudaFree(C);); (44, CUDA, cudaEventRecord(start);); (29, CUDA, cudaGetDeviceCount(&num_of_total_devices);); (18, CUDA, cudaEventCreate(&stop);); (53, CUDA, cudaEventElapsedTime(&elapsedTime, start, stop);); (15, CUDA, cudaEventCreate(&start);); (57, CUDA, cudaFree(A);)"}
{"predict": "(26,OpenMP,#pragma omp parallel for)", "label": "(11, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (9, CUDA, cudaMalloc((void**)(&d_in1), in1_bytes);); (11, CUDA, cudaMemcpy(d_in1, in1.data(), in1_bytes, cudaMemcpyHostToDevice);); (31, CUDA, cudaFree(d_out);); (23, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (28, CUDA, cudaFree(d_in1);)"}
{"predict": "(6,OpenMP,#pragma omp parallel for reduction(+:z)), (14,OpenMP,#pragma omp parallel for reduction(+:z)), (16,OpenMP,#pragma omp parallel for reduction(+:z)), (27,OpenMP,#pragma omp parallel for reduction(+:v)), (29,OpenMP,#pragma omp parallel for reduction(+:v))", "label": "(17, OpenMP, #pragma omp parallel for simd linear (argc : 5) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (39, OpenMP, #pragma omp parallel for simd linear(j)); (41, OpenMP, #pragma omp parallel for simd linear(v:j)); (34, OpenMP, #pragma omp parallel for simd linear(v:i)); (27, OpenMP, #pragma omp parallel for simd linear(i, z)); (4, OpenMP, #pragma omp parallel for simd linear ()); (13, OpenMP, #pragma omp parallel for simd linear (S1)); (6, OpenMP, #pragma omp parallel for simd linear (argc,); (-3, OpenMP, #pragma omp parallel); (36, OpenMP, #pragma omp parallel for simd linear(i)); (17, OpenMP, #pragma omp parallel for simd linear(e, g)); (12, OpenMP, #pragma omp parallel for simd linear (a, b:B::ib)); (-1, OpenMP, #pragma omp parallel for simd linear (argc); (12, OpenMP, #pragma omp parallel for simd linear (argv[1])); (15, OpenMP, #pragma omp parallel for simd linear(h)); (-8, OpenMP, #pragma omp parallel for simd linear (); (-1, OpenMP, #pragma omp parallel for simd linear (argc > 0 ? argv[1] : argv[2]))"}
{"predict": "(10,OpenMP,#pragma omp parallel for private(pfs1,numpairsp1,defgradp1,p1,vol0p1,posp1,pos0p1,kercorrp1))", "label": "(8, OpenMP, #pragma omp parallel for schedule (guided))"}
{"predict": "] << std::endl;\n124: \n125:   return 0;\n126: }", "label": "(9, SYCL, sycl::queue q = get_queue();); (36, SYCL, sycl::free(data, q);); (8, SYCL, int* data = sycl::malloc_device<int>(1,q);)"}
{"predict": "(5,MPI,*w = MPI_Wtime(););", "label": "(6, CUDA, HANDLE_ERROR( cudaGetDeviceProperties( &prop, i ) );); (3, CUDA, HANDLE_ERROR( cudaGetDeviceCount( &count ) );)"}
{"predict": "(17,MPI,nnzL = 0; n = 0;)\n(155,MPI,n = 0;)", "label": "(49, MPI, MPI_Allreduce(MPI_IN_PLACE,&n,1,mpi_int_t,MPI_MAX,grid->comm);); (47, MPI, MPI_Allreduce(MPI_IN_PLACE,&nnzL,1,mpi_int_t,MPI_SUM,grid->comm);)"}
{"predict": ") ? 'F' : 'U'),\n100:            (double) file_size, nerrs, (double) rates_g[0],\n101:            (double) rates_g[2], (double) rates_g[1],\n102:            (double) rates_g[3], (double) file_size,\n103:            (double) rates_g[0], (double) rates_g[2]);\n104:   }\n105: \n106:   return 0;\n107: }", "label": "(17, MPI, MPI_Comm_size(MPI_COMM_WORLD,&totpes);); (39, MPI, MPI_Cart_create(MPI_COMM_WORLD,3,numpes,isperiodic,reorder,&comm_cart);); (39, MPI, MPI_Comm_rank(comm_cart,&mype);); (19, MPI, MPI_Finalize();); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&rank);); (72, MPI, MPI_Barrier(comm_cart);); (36, MPI, MPI_Cart_coords(comm_cart,mype,3,pe_coords);); (94, MPI, MPI_Allreduce(t, t_g, 20, MPI_DOUBLE, MPI_MAX, comm_cart);); (31, MPI, MPI_Dims_create(totpes,3,numpes);); (18, MPI, MPI_Bcast(filename, 256, MPI_CHAR, 0, MPI_COMM_WORLD);); (115, MPI, MPI_Allreduce(MPI_IN_PLACE, &nerrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (5, MPI, MPI_Init(&argc,&argv);); (102, MPI, MPI_Comm_free(&comm_cart);); (87, MPI, MPI_Allreduce(rates_l, rates_g, 4, MPI_DOUBLE, MPI_MIN, comm_cart);); (105, MPI, MPI_Reduce(&malloc_size, &sum_size, 1, MPI_OFFSET, MPI_SUM, 0, MPI_COMM_WORLD);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (9,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for)", "label": "(10, TBB, tbb::flow::make_edge( lim, r );); (22, TBB, tbb::flow::limiter_node< T > lim_copy( lim );); (3, TBB, tbb::flow::graph g;); (3, TBB, tbb::flow::limiter_node< T > lim(g, i);); (4, TBB, tbb::atomic<int> accept_count;)"}
{"predict": "(12,OpenMP,#pragma omp parallel for collapse(2))\n(25,OpenMP,#pragma omp parallel for collapse(2))", "label": "(15, OpenMP, #pragma omp loop); (19, OpenMP, #pragma omp target teams distribute parallel for map(MTX) reduction(*:EMTX)); (11, OpenMP, #pragma omp target teams loop map(MTX) reduction(*:MTX))"}
{"predict": "(27,MPI,*w = MPI_Wtime();); (40,OpenMP,#pragma omp parallel for), (54,OpenMP,#pragma omp parallel for)", "label": "(67, TBB, tbb::task_group tg;)"}
{"predict": "(12,OpenMP,#pragma omp parallel for), (24,OpenMP,counts_team += 1;)", "label": "(26, OpenMP, #pragma omp for reduction(+:counts_team)); (8, OpenMP, #pragma omp for); (15, OpenMP, #pragma omp target teams map(from:counts2)); (4, OpenMP, #pragma omp parallel); (0, OpenMP, #pragma omp target teams map(from:counts1)); (17, OpenMP, #pragma omp allocate(counts_team) allocator(omp_pteam_mem_alloc))"}
{"predict": "\\n\", checksum, basesum*Num_procs);\n140:       error = 1;\n141:     }\n142:   }\n143: \n144:   bail_out(error);\n145: \n146:   /* Free memory */\n147:   prk_free(basestring);\n148:   prk_free(catstring);\n149:   prk_free(iterstring);\n150: \n151:   return 0;\n152: }", "label": "(115, MPI, MPI_Type_commit(&mpi_word);); (122, MPI, MPI_Allgather(iterstring,1,mpi_word, catstring,1,mpi_word, MPI_COMM_WORLD);); (154, MPI, MPI_Finalize();); (63, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (110, MPI, MPI_Type_contiguous(proc_length,MPI_CHAR, &mpi_word);); (17, MPI, MPI_Init(&argc,&argv);); (61, MPI, MPI_Bcast(&length,     1, MPI_LONG, root, MPI_COMM_WORLD);); (110, MPI, MPI_Barrier(MPI_COMM_WORLD);); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for)", "label": "(10, MPI, MPI_Barrier(MPI_COMM_WORLD);); (12, MPI, MPI_Allreduce(data,gr_comm_buf,n,mpi_bool,MPI_LAND,MPI_COMM_WORLD);)"}
{"predict": "(20,OpenMP,#pragma omp parallel for reduction(+:errs)), (26,OpenMP,#pragma omp parallel for reduction(+:errs)), (31,OpenMP,#pragma omp parallel for reduction(+:errs)), (37,OpenMP,#pragma omp parallel for reduction(+:errs)), (42,OpenMP,#pragma omp parallel for reduction(+:errs)), (48,OpenMP,#pragma omp parallel for reduction(+:errs))", "label": "(16, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(42, TBB, tbb::flow::make_edge(buffer2, tbb::flow::input_port<1>(join));); (43, TBB, tbb::flow::make_edge(join, function);); (19, TBB, tbb::flow::join_node<tuple_type, tbb::flow::reserving> join(g);); (38, TBB, tbb::flow::make_edge(buffer1, tbb::flow::input_port<0>(join));); (-3, TBB, tbb::task_arena arena(1);); (-1, TBB, tbb::flow::graph g;); (11, TBB, tbb::flow::queue_node<int> buffer1(g);); (12, TBB, tbb::flow::queue_node<int> buffer3(g);); (35, TBB, tbb::flow::make_edge(buffer3, tbb::flow::input_port<2>(join));); (9, TBB, tbb::flow::queue_node<int> buffer2(g);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (30,OpenMP,#pragma omp parallel for reduction(+:localpi)), (40,MPI,pi += localpi;);", "label": "(42, MPI, MPI_Recv(&localpi, 1, MPI_DOUBLE, MPI_ANY_SOURCE, 0, MPI_COMM_WORLD, &status);); (47, MPI, MPI_Send(&localpi, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);); (4, MPI, MPI_Init(&argc, &argv);); (48, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(13, CUDA, CP_host->freeCudaMem();)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (13,MPI,ASSERT( GetRoundingMode() == mode, NULL );)", "label": "(12, TBB, tbb::parallel_for( 0, N, 1, RoundingModeCheckBody(mode, sse_mode) );); (10, TBB, tbb::task_scheduler_init init;)"}
{"predict": "(16,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for), (41,OpenMP,#pragma omp parallel for)", "label": "(13, OpenMP, fprintf(stderr, \"#pragma omp target teams distribute thread_limit(thdLim) 27\\n\");); (43, OpenMP, #pragma omp target teams distribute thread_limit(1024)); (22, OpenMP, fprintf(stderr, \"#pragma omp target teams distribute thread_limit(thdLim) 1024\\n\");); (31, OpenMP, #pragma omp target teams distribute thread_limit(27)); (34, OpenMP, constLimit[i] = omp_get_num_threads();); (9, OpenMP, #pragma omp target teams distribute thread_limit(thdLim)); (42, OpenMP, constLimitHuge[i] = omp_get_num_threads();); (21, OpenMP, varLimitHuge[i] = omp_get_num_threads();); (9, OpenMP, varLimit[i] = omp_get_num_threads();)"}
{"predict": "(28,OpenMP,#pragma omp parallel for)", "label": "(50, CUDA, cudaFreeHost( foutHost );); (27, CUDA, CUDART_CHECK(surf1Dmemset( array, 3.141592654f, 0, NUM_VALUES ));)"}
{"predict": "(35,MPI,*w = MPI_Wtime();); (62,OpenMP,#pragma omp parallel for), (75,OpenMP,#pragma omp parallel for), (90,OpenMP,#pragma omp parallel for)", "label": "(72, OpenMP, #pragma omp target data map(h_x [0:nconf * numatm], h_y [0:nconf * numatm], h_z [0:nconf * numatm], h_g2 [0:nbin]))"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp parallel for)"}
{"predict": "(16,OpenMP,#pragma omp parallel for), (50,OpenMP,#pragma omp parallel for)", "label": "(4, SYCL, int* data = sycl::malloc_shared<int>(7, q);); (19, SYCL, hipsycl::rt::runtime_keep_alive_token rt;); (1, SYCL, sycl::queue q = get_queue();); (67, SYCL, sycl::free(data, q);)"}
{"predict": "(14,MPI,#pragma omp parallel for private(i)), (21,MPI,#pragma omp parallel for private(i)), (26,MPI,#pragma omp parallel for private(i))", "label": "(13, MPI, MPI_Comm_size( comm, &size );); (22, MPI, MPI_Type_commit( &vec );); (26, MPI, MPI_Gather( vecin, 1, vec, vecout, n, MPI_DOUBLE, root, comm );); (46, MPI, MPI_Finalize();); (5, MPI, MPI_Init( &argc, &argv );); (35, MPI, MPI_Type_free( &vec );); (6, MPI, MPI_Comm_rank( comm, &rank );); (15, MPI, MPI_Type_vector( n, 1, stride, MPI_DOUBLE, &vec );); (39, MPI, MPI_Gather( NULL, 0, MPI_BYTE, NULL, 0, MPI_BYTE, 0, MPI_COMM_WORLD );)"}
{"predict": "(11,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);)\n(12,MPI,MPI_Comm_size(MPI_COMM_WORLD, &size);)\n(21,OpenMP,#pragma omp parallel for reduction(+:sum)),", "label": "(34, OpenMP, #pragma omp for reduction(+:sum)); (32, OpenMP, printf(\"rank %d:%d has %d threads\\n\", rank, thread_num, omp_get_num_threads());); (11, MPI, MPI_Init(&argc, &argv);); (42, MPI, MPI_Finalize();); (26, OpenMP, #pragma omp parallel); (36, MPI, MPI_Reduce(&sum, &avg, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (25, OpenMP, int thread_num = omp_get_thread_num();); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (7, MPI, MPI_Dims_create(size, NR_DIMS, dims);); (13, MPI, MPI_Cart_coords(cart_comm, rank, NR_DIMS, cart_coords);); (10, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for)", "label": "(8, OpenCL, clReleaseKernel_ExpectAndReturn(make_kernel(0), CL_SUCCESS);); (6, OpenCL, clReleaseDevice_ExpectAndReturn(make_device_id(0), CL_SUCCESS);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for private(i,j,y_i)), (24,OpenMP,#pragma omp atomic)", "label": "(40, CUDA, CUDA_SAFE_CALL(cudaFree(x.data()));); (38, CUDA, CUDA_SAFE_CALL(cudaFree(A.data()));); (36, CUDA, CUDA_SAFE_CALL(cudaDeviceSynchronize());); (38, CUDA, CUDA_SAFE_CALL(cudaFree(y.data()));)"}
{"predict": "(21,OpenMP,#pragma omp parallel for)", "label": "(22, MPI, *w = MPI_Wtime();)"}
{"predict": "(29,MPI, MPI_Allreduce(hs, hs, grid.size(), MPI_HALF, MPI_SUM, comm);)", "label": "(2, CUDA, extern __shared__ half  hs[][256];)"}
{"predict": "(6,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for), (35,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for), (39,OpenMP,#pragma omp parallel for), (55,OpenMP,#pragma omp parallel for)", "label": "(7, TBB, tbb::task_scheduler_init init( p );)"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(13, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(12,MPI,MPI_Init(&argc, &argv);) (33,OpenMP,#pragma omp parallel for)", "label": "(18, MPI, MPI_Comm_rank(shm_comm, &shm_rank);); (41, MPI, MPI_Win_lock_all(MPI_MODE_NOCHECK, shm_win);); (24, MPI, MPI_Win_shared_query(shm_win, MPI_PROC_NULL, &size, &disp_unit, &base);); (13, MPI, MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, rank, MPI_INFO_NULL, &shm_comm);); (10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (58, MPI, MPI_Win_unlock_all(shm_win);); (58, MPI, MPI_Win_free(&shm_win);); (58, MPI, MPI_Comm_free(&shm_comm);); (10, MPI, MPI_Comm_size(shm_comm, &shm_nproc);); (40, MPI, MPI_Barrier(shm_comm);); (38, MPI, MPI_Win_sync(shm_win);)"}
{"predict": "(46,OpenMP,#pragma omp parallel for collapse(2))", "label": "(41, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&b, global_b_size * sizeof(TB)));); (71, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(c));); (68, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(a));); (37, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&a, global_a_size * sizeof(TA)));); (47, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(a, host_a.data(), global_a_size * sizeof(TA), cudaMemcpyHostToDevice));); (49, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (47, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(c, host_c.data(), global_a_size * sizeof(TC), cudaMemcpyHostToDevice));); (45, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(b, host_b.data(), global_a_size * sizeof(TB), cudaMemcpyHostToDevice));); (58, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(host_output.data(), c, global_c_size * sizeof(TC), cudaMemcpyDeviceToHost));); (33, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&c, global_c_size * sizeof(TC)));); (61, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(b));)"}
{"predict": "(17,MPI,*w = MPI_Wtime();); (47,OpenMP,#pragma omp parallel for), (62,OpenMP,#pragma omp parallel for), (63,OpenMP,#pragma omp parallel for)", "label": "(62, CUDA, cudaFree( ddata );)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for)", "label": "(10, CUDA, cudaStreamSynchronize(LU_v1->A_gpu.cuStreams[0]);)"}
{"predict": "(5,MPI,*w = MPI_Wtime(););", "label": "(2, OpenMP, omp_autotune(self, OMP_SCALE);)"}
{"predict": "(16,OpenMP,#pragma omp parallel for private(st))", "label": "(18, CUDA, cudaMemcpy(dB_buff, B_buff, sizeof(float*) * batch_num, cudaMemcpyHostToDevice);); (12, CUDA, cudaMalloc(&dA_buff, sizeof(float*) * batch_num);); (31, CUDA, cudaFree(dC_buff);); (29, CUDA, cudaFree(dB_buff);); (27, CUDA, cudaFree(dA_buff);); (12, CUDA, cudaMemcpy(dA_buff, A_buff, sizeof(float*) * batch_num, cudaMemcpyHostToDevice);); (8, CUDA, cudaMalloc(&dB_buff, sizeof(float*) * batch_num);); (12, CUDA, cudaMemcpy(dC_buff, C_buff, sizeof(float*) * batch_num, cudaMemcpyHostToDevice);); (7, CUDA, cudaMalloc(&dC_buff, sizeof(float*) * batch_num);)"}
{"predict": "(9,OpenMP,#pragma omp parallel for)", "label": "(7, CUDA, cudaMalloc((void **)&buffer_fp32, length * sizeof(float));); (9, CUDA, cudaMalloc((void **)&buffer, length * sizeof(T));); (10, CUDA, cudaFree(buffer_fp32);)"}
{"predict": "(23,OpenMP,#pragma omp parallel for)", "label": "(16, OpenMP, #pragma omp target); (22, OpenMP, #pragma omp teams distribute parallel for if(cond) firstprivate(fp) reduction(+:rd))"}
{"predict": "(12,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for)", "label": "(19, CUDA, cudaMemcpy(resf,dat,sizeof(unsigned)*2,cudaMemcpyDeviceToHost);)"}
{"predict": "(19,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for)", "label": "(29, CUDA, CUDART_CHECK(surf2DmemsetArray_time( &ms, texArray, 3.141592654f, threadWidth, threadHeight ));); (50, CUDA, cudaFreeArray( texArray );)"}
{"predict": "{\n182:         if (getCmdLineArgumentString(argc, argv, \"start\", &modeStr))\n183:         {\n184:             start = atoi(modeStr);\n185:         }\n186: \n187:         if (getCmdLineArgumentString(argc, argv, \"end\", &modeStr))\n188:         {\n189:             end = atoi(modeStr);\n190:         }\n191: \n192:         if (getCmdLineArgumentString(argc, argv, \"increment\", &modeStr))\n193:         {\n194:             increment = atoi(modeStr);\n195:         }\n196:     }\n197: \n198:     if (QUICK_MODE == mode)\n199:     {\n200:         start = DEFAULT_SIZE;\n201:         end = DEFAULT_SIZE;\n202:         increment = DEFAULT_INCREMENT;\n203:     }\n204: \n205:     if (SHMOO_MODE == mode)\n206:     {\n207:         start = MIN_SIZE;\n208:         end = MAX_SIZE;\n209:         increment = INCREMENT;\n210:     }\n211: \n212:     printf(\"Bandwidth test for size %d to %d with increment %d\\n\", start, end, increment);\n213: \n214:     int *h_input = NULL;\n215:     int *h_output = NULL;\n216:     int *d_input = NULL;\n217:     int *d_output = NULL;\n218: \n219:     for (int currentDevice = startDevice; currentDevice <= endDevice; currentDevice++)\n220:     {\n221:         checkCudaErrors(cudaSetDevice(currentDevice));\n222: \n223:         cudaMallocHost((void **)&h_input, end * sizeof(int));\n224:         cudaMallocHost((void **)&h_output, end * sizeof(int));\n225:         cudaMalloc((void **)&d_input, end * sizeof(int));\n226:         cudaMalloc((void **)&d_output, end * sizeof(int));\n227: \n228:         if (wc)\n229:         {", "label": "(267, CUDA, cudaSetDevice(nDevice);); (108, CUDA, printf(\"cudaGetDeviceProperties returned %d\\n-> %s\\n\", (int)error_id, cudaGetErrorString(error_id));); (54, CUDA, cudaError_t error_id = cudaGetDeviceCount(&deviceCount);); (98, CUDA, fprintf(stderr, \"Error: device is running in <Compute Mode Prohibited>, no threads can use ::cudaSetDevice().\\n\");); (56, CUDA, printf(\"cudaGetDeviceCount returned %d\\n-> %s\\n\", (int)error_id, cudaGetErrorString(error_id));); (88, CUDA, cudaError_t error_id = cudaGetDeviceProperties(&deviceProp, currentDevice);); (96, CUDA, checkCudaErrors(cudaSetDevice(currentDevice));)"}
{"predict": "(1,MPI,*w = MPI_Wtime(););", "label": "(5, CUDA, extern __shared__ float cache[];)"}
{"predict": "(15,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for), (62,OpenMP,#pragma omp parallel for)", "label": "(77, OpenMP, #pragma omp parallel num_threads(NumThreads1)); (14, OpenMP, #pragma omp parallel); (26, OpenMP, int nestedThreadNum = omp_get_thread_num();); (7, OpenMP, inParallel = omp_in_parallel();); (22, OpenMP, int nestedNumThreads = omp_get_num_threads();); (19, OpenMP, int nestedInParallel = omp_in_parallel();); (12, OpenMP, check1[id] += omp_get_num_threads() + omp_in_parallel();); (4, OpenMP, numThreads = omp_get_num_threads();); (21, OpenMP, #pragma omp atomic); (3, OpenMP, threadNum = omp_get_thread_num();); (-2, OpenMP, #pragma omp target map(inParallel, numThreads, threadNum, check1[:], check2[:])); (6, OpenMP, int id = omp_get_thread_num();)"}
{"predict": "(13,MPI,*w = MPI_Wtime();); (16,OpenMP,#pragma omp parallel for reduction(+:gpu_sum)), (20,OpenMP,#pragma omp parallel for reduction(+:gpu_sum)),", "label": "(11, CUDA, float *dptr = thrust::raw_pointer_cast(&dsums[0]);); (14, CUDA, double gpu_sum = thrust::reduce(dsums.begin(),dsums.end());); (8, CUDA, thrust::device_vector<float> dsums(steps);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for private(fat1)), (27,OpenMP,#pragma omp parallel for private(tempmat1)), (35,OpenMP,#pragma omp parallel for private(tempmat1))", "label": "(12, OpenMP, #pragma omp parallel for)"}
{"predict": "(15,OpenMP,#pragma omp parallel for)", "label": "(12, TBB, my_thread_array = tbb::cache_aligned_allocator<padded_private_worker>().allocate( my_n_thread );)"}
{"predict": "(19,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for)", "label": "(40, CUDA, cudaFree(d_inPtr);); (24, CUDA, cudaMalloc((void**)&d_outPtr, size * sizeof(float));); (30, CUDA, cudaMemcpy(&result_gpu, &d_outPtr[0], sizeof(float), cudaMemcpyDeviceToHost);); (24, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);); (35, CUDA, cudaFree(d_outPtr);); (19, CUDA, cudaMalloc((void**)& d_inPtr, size * sizeof(float));)"}
{"predict": "(7,MPI,*w = MPI_Wtime();); (11,MPI,cudaEventCreate(&start)); (13,MPI,cudaEventCreate(&stop)); (17,MPI,cudaMalloc((void**)&d_A, length)); (21,MPI,cudaMalloc((void**)&d_B, length)); (25,MPI,cudaMalloc((void**)&d_C, length)); (29,MPI,cudaMemcpy(d_A, A, length, cudaMemcpyHostToDevice)); (33,MPI,cudaMemcpy(d_B, B, length, cudaMemcpyHostToDevice)); (42,MPI,vector_add<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, numElements)); (44,MPI,cudaEventRecord(start, 0)); (46,MPI,cudaEventRecord(stop, 0)); (48,MPI,cudaDeviceSynchronize()); (50,MPI,cudaEventElapsedTime(elapsed_time, start, stop)); (52,MPI,cudaMemcpy(C, d_C, length, cudaMemcpyDeviceToHost)); (56,MPI,cudaFree(d_A)); (58,MPI,cudaFree(d_B)); (60,MPI,cudaFree(d_C)); (62,MPI,cudaEventDestroy(start)); (64,MPI,cudaEventDestroy(stop));", "label": "(88, CUDA, cudaEventSynchronize(stop);); (70, CUDA, err = cudaGetLastError();); (19, CUDA, err = cudaMalloc(&d_A, length);); (12, CUDA, cudaEventRecord(start, 0);); (78, CUDA, err = cudaFree(d_C);); (87, CUDA, cudaEventDestroy(start);); (74, CUDA, err = cudaFree(d_B);); (21, CUDA, err = cudaMalloc(&d_B, length);); (78, CUDA, cudaEventRecord(stop, 0);); (32, CUDA, err = cudaMemcpy(d_A, A, length, cudaMemcpyHostToDevice);); (80, CUDA, cudaEventElapsedTime(elapsed_time, start, stop);); (1, CUDA, cudaEventCreate(&start);); (66, CUDA, err = cudaFree(d_A);); (30, CUDA, err = cudaMemcpy(d_B, B, length, cudaMemcpyHostToDevice);); (16, CUDA, err = cudaMalloc(&d_C, length);); (78, CUDA, cudaEventDestroy(stop);); (-3, CUDA, cudaEventCreate(&stop);); (57, CUDA, err = cudaMemcpy(C, d_C, length, cudaMemcpyDeviceToHost);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for)", "label": "(16, OpenMP, #pragma omp target defaultmap (scalar:); (3, OpenMP, #pragma omp target defaultmap(); (10, OpenMP, #pragma omp target defaultmap(tofrom)); (11, OpenMP, #pragma omp target defaultmap(tofrom,); (16, OpenMP, #pragma omp target defaultmap(tofrom:scalar) defaultmap(tofrom:scalar)); (-3, OpenMP, #pragma omp target defaultmap); (19, OpenMP, #pragma omp target defaultmap(tofrom: something)); (11, OpenMP, #pragma omp target defaultmap(tofrom, scalar); (2, OpenMP, #pragma omp target defaultmap (tofrom:); (-3, OpenMP, #pragma omp target defaultmap()); (-2, OpenMP, #pragma omp target defaultmap(tofrom)"}
{"predict": "(28,OpenMP,#pragma omp parallel for)", "label": "(52, CUDA, CUDA_CALL(cudaFree(data));); (28, CUDA, CUDA_CALL(cudaDeviceSynchronize());); (6, CUDA, CUDA_CALL(cudaMalloc((void**)&data, size * sizeof(T)));)"}
{"predict": "(18,MPI,MPI_Comm_rank(comm, &rank); MPI_Comm_size(comm, &size);); (52,OpenMP,#pragma omp parallel for), (71,OpenMP,#pragma omp parallel for), (83,OpenMP,#pragma omp parallel for)", "label": "(29, MPI, MPI_Abort(comm, 1);); (15, MPI, MPI_Comm_size(comm, &size);); (15, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for)", "label": "(25, OpenMP, #pragma omp parallel sections shared(argv[1])); (34, OpenMP, #pragma omp parallel sections shared(h, B::x)); (41, OpenMP, #pragma omp parallel sections shared(i)); (20, OpenMP, #pragma omp parallel sections shared(a, b, c, d, f, k)); (9, OpenMP, #pragma omp parallel sections shared(argc); (10, OpenMP, #pragma omp parallel sections shared(argc,); (33, OpenMP, #pragma omp parallel sections firstprivate(i)); (22, OpenMP, #pragma omp parallel sections shared(ca)); (19, OpenMP, #pragma omp parallel sections shared(ba)); (28, OpenMP, #pragma omp parallel sections private(i)); (-3, OpenMP, #pragma omp parallel sections shared); (20, OpenMP, #pragma omp parallel sections shared(da)); (5, OpenMP, #pragma omp parallel sections shared(argc > 0 ? argv[1] : argv[2])); (8, OpenMP, #pragma omp parallel sections shared(S1)); (5, OpenMP, #pragma omp parallel sections shared(argc)); (30, OpenMP, #pragma omp parallel sections shared(j)); (-5, OpenMP, #pragma omp parallel sections shared()); (-8, OpenMP, #pragma omp parallel sections shared(); (15, OpenMP, #pragma omp parallel sections shared(e, g))"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (45,MPI,if (d > sizeof(envname) - 1) {MPI_Barrier(comm); fprintf(stderr, \"Environment variable name too long (%d)\\n\", d);}); (49,MPI,if (envval) {MPI_Bcast(envval, strlen(envval) + 1, MPI_CHAR, 0, comm);});", "label": "(45, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(11,MPI,*w = MPI_Wtime();); (17,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (45,OpenMP,#pragma omp parallel for)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(25,OpenMP,#pragma omp parallel for private(i, offset, zone_offset)), (29,OpenMP,#pragma omp for private(offset)), (35,OpenMP,#pragma omp for private(zone_offset))", "label": "(25, OpenMP, #pragma omp parallel for)"}
{"predict": "(10,MPI,*w = MPI_Wtime();); (12,OpenMP,#pragma omp parallel for), (72,OpenMP,#pragma omp parallel for)", "label": "(13, CUDA, extern __shared__ char buf[];)"}
{"predict": "(94,MPI,*vars->st_offsets = (ADIO_Offset *) ADIOI_Malloc(nprocs * 2 * sizeof(ADIO_Offset));); (95,MPI,vars->end_offsets = vars->st_offsets + nprocs;); (96,MPI,*error_code = MPI_Iallgather(&vars->start_offset, 1, ADIO_OFFSET, vars->st_offsets, 1, ADIO_OFFSET, fd->comm, &vars->req_offset[0]);); (98,MPI,*error_code = MPI_Iallgather(&vars->end_offset, 1, ADIO_OFFSET, vars->end_offsets, 1, ADIO_OFFSET, fd->comm, &vars->req_offset[1]);)", "label": "(66, MPI, MPI_Comm_size(fd->comm, &nprocs);); (66, MPI, MPI_Comm_rank(fd->comm, &myrank);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for reduction(+:errs))", "label": "(6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &world_size);)"}
{"predict": "@+3 {{defined as lastprivate}}\n125: #pragma omp target\n126: #pragma omp teams\n127:   for (i = 0; i < argc; ++i)\n128:     foo();\n129:   int xb;\n130: #pragma omp target\n131: #pragma omp teams\n132: #pragma omp distribute parallel for firstprivate(xb)\n133:   for (i = 0; i < argc; ++i)\n134:     foo();\n135: #pragma omp target\n136: #pragma omp teams\n137: #pragma omp distribute parallel for firstprivate(xb)\n138:   for (i = 0; i < argc; ++i)\n139:     foo();\n140: #pragma omp target\n141: #pragma omp teams\n142:   for (i = 0; i < argc; ++i)\n143:     foo();\n144: #pragma omp target\n145: #pragma omp teams\n146:   for (i = 0; i < argc; ++i)\n147:     foo();\n148: #pragma omp target\n149: #pragma omp teams\n150:   for (i = 0; i < argc; ++i)\n151:     foo();\n152: #pragma omp target\n153: #pragma omp teams\n154:   for (i = 0; i < argc; ++i)\n155:     foo();\n156: #pragma omp target\n157: #pragma omp teams\n158:   for (i = 0; i < argc; ++i)\n159:     foo();\n160: #pragma omp target\n161: #pragma omp teams\n162:   for (i = 0; i < argc; ++i)\n163:     foo();\n164: #pragma omp target\n165: #pragma omp teams\n166:   for (i = 0; i < argc; ++i)\n167:     foo();\n168: #pragma omp target\n169: #pragma omp teams\n170:   for (", "label": "(82, OpenMP, #pragma omp distribute parallel for lastprivate(xa)); (15, OpenMP, #pragma omp distribute parallel for lastprivate(); (59, OpenMP, #pragma omp distribute parallel for lastprivate(2 * 2)); (73, OpenMP, #pragma omp distribute parallel for lastprivate(da)); (93, OpenMP, #pragma omp distribute parallel for safelen(5)); (137, OpenMP, #pragma omp distribute parallel for lastprivate(j)); (25, OpenMP, #pragma omp distribute parallel for lastprivate(argc,); (2, OpenMP, #pragma omp target); (109, OpenMP, #pragma omp distribute parallel for lastprivate(B::x)); (62, OpenMP, #pragma omp distribute parallel for lastprivate(ca)); (138, OpenMP, #pragma omp distribute parallel for firstprivate(m) lastprivate(m)); (116, OpenMP, #pragma omp distribute parallel for lastprivate(i)); (142, OpenMP, #pragma omp distribute parallel for lastprivate(n) firstprivate(n)); (94, OpenMP, #pragma omp distribute parallel for lastprivate(m)); (32, OpenMP, #pragma omp distribute parallel for lastprivate(S1)); (97, OpenMP, #pragma omp distribute parallel for lastprivate(h)); (-5, OpenMP, #pragma omp distribute parallel for lastprivate); (39, OpenMP, #pragma omp distribute parallel for lastprivate(argv[1])); (18, OpenMP, #pragma omp distribute parallel for lastprivate(argc > 0 ? argv[1] : argv[2])); (32, OpenMP, #pragma omp distribute parallel for lastprivate(a, b, c, d, f)); (140, OpenMP, #pragma omp distribute parallel for lastprivate(si)); (20, OpenMP, #pragma omp distribute parallel for lastprivate(argc)); (80, OpenMP, #pragma omp distribute parallel for lastprivate(e, g)); (3, OpenMP, #pragma omp distribute parallel for lastprivate(argc); (42, OpenMP, #pragma omp distribute parallel for lastprivate(ba)); (62, OpenMP, #pragma omp distribute parallel for lastprivate(S2::S2s)); (66, OpenMP, #pragma omp distribute parallel for lastprivate(S2::S2sc)); (95, OpenMP, #pragma omp distribute parallel for private(xa), lastprivate(xa)); (-7, OpenMP, #pragma omp distribute parallel for lastprivate()); (-19, OpenMP, #pragma omp teams)"}
{"predict": "(26,MPI,if (my_ID == root) { MPI_Bcast(&nthread_input, 1, MPI_INT, root, MPI_COMM_WORLD); MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD); MPI_Bcast(&total_length, 1, MPI_LONG, root, MPI_COMM_WORLD); MPI_Bcast(&offset, 1, MPI_LONG, root, MPI_COMM_WORLD); }), (79,MPI,if (my_ID == root) { MPI_Bcast(a, space, MPI_DOUBLE, root, MPI_COMM_WORLD); }), (117,OpenMP,#pragma omp parallel for), (127,MPI,MPI_Reduce(&local_nstream_time, &nstream_time, 1, MPI_DOUBLE, MPI_MIN, root, MPI_COMM_WORLD);)", "label": "(101, OpenMP, printf(\"Number of threads    = %d\\n\", omp_get_max_threads());); (24, MPI, MPI_Comm_size(MPI_COMM_WORLD,&Num_procs);); (148, MPI, MPI_Finalize();); (104, OpenMP, #pragma omp parallel for); (74, MPI, MPI_Bcast(&length,        1, MPI_LONG, root, MPI_COMM_WORLD);); (21, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&my_ID);); (75, MPI, MPI_Bcast(&nthread_input, 1, MPI_INT,  root, MPI_COMM_WORLD);); (76, OpenMP, omp_set_num_threads(nthread_input);); (16, MPI, MPI_Init(&argc,&argv);); (71, MPI, MPI_Bcast(&iterations,    1, MPI_INT,  root, MPI_COMM_WORLD);); (113, MPI, MPI_Barrier(MPI_COMM_WORLD);); (68, MPI, MPI_Bcast(&offset,        1, MPI_LONG, root, MPI_COMM_WORLD);)"}
{"predict": "(21,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for), (39,OpenMP,#pragma omp parallel for), (45,OpenMP,#pragma omp parallel for)", "label": "(18, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(16,OpenMP,#pragma omp atomic) (21,OpenMP,#pragma omp atomic) (22,OpenMP,#pragma omp atomic) (27,OpenMP,#pragma omp atomic) (28,OpenMP,#pragma omp atomic) (29,OpenMP,#pragma omp atomic)", "label": "(9, OpenMP, #pragma omp target); (10, OpenMP, #pragma omp distribute simd firstprivate(g, g1, svar, sfvar)); (46, OpenMP, #pragma omp distribute simd firstprivate(t_var, vec, s_arr, s_arr, var, var, svar)); (7, OpenMP, #pragma omp teams)"}
{"predict": "(29,OpenMP,#pragma omp parallel for)", "label": "(7, CUDA, __shared__ block_radix_sort_t::TempStorage temp_storage;)"}
{"predict": "(16,OpenMP,#pragma omp parallel for)", "label": "(12, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(18,OpenMP,#pragma omp parallel for reduction(+:result_host))", "label": "(33, CUDA, cudaFree(d_inPtr);); (21, CUDA, cudaMalloc((void**)&d_outPtr, size * sizeof(float));); (26, CUDA, cudaMemcpy(&result_gpu, &d_outPtr[0], sizeof(float), cudaMemcpyDeviceToHost);); (21, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);); (28, CUDA, cudaFree(d_outPtr);); (16, CUDA, cudaMalloc((void**)& d_inPtr, size * sizeof(float));)"}
{"predict": "(9,OpenMP,#pragma omp parallel for)", "label": "(3, TBB, tbb::flow::broadcast_node<my_type> my_broadcast_node(g);); (7, TBB, const tbb::tick_count t0 = tbb::tick_count::now();); (14, TBB, return (tbb::tick_count::now() - t0).seconds();); (-1, TBB, tbb::flow::queue_node<my_type> my_queue(g);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for reduction(+:err,xnorm,errcomp)), (22,MPI,MPI_Allreduce(&temperr, &err, 1, MPI_DOUBLE, MPI_MAX, slucomm, MPI_STATUS_IGNORE)), (23,MPI,MPI_Allreduce(&tempxnorm, &xnorm, 1, MPI_DOUBLE, MPI_MAX, slucomm, MPI_STATUS_IGNORE)), (24,MPI,MPI_Allreduce(&temperr, &errcomp, 1, MPI_DOUBLE, MPI_MAX, slucomm, MPI_STATUS_IGNORE)), (26,OpenMP,#pragma omp critical), (27,OpenMP,#pragma omp atomic)", "label": "(26, MPI, MPI_Allreduce( &temperr, &errcomp, 1, MPI_FLOAT, MPI_MAX, slucomm);); (23, MPI, MPI_Allreduce( &tempxnorm, &xnorm, 1, MPI_DOUBLE, MPI_MAX, slucomm);); (20, MPI, MPI_Allreduce( &temperr, &err, 1, MPI_DOUBLE, MPI_MAX, slucomm);)"}
{"predict": "(27,OpenMP,#pragma omp parallel for)", "label": "(8, OpenMP, #pragma omp target map(present, alloc: arr[0:0])); (6, OpenMP, #pragma omp target data map(alloc: arr[0:5])); (19, OpenMP, #pragma omp target data map(alloc: arr[0:0]))"}
{"predict": "(14,OpenMP,#pragma omp barrier); (17,OpenMP,#pragma omp barrier);", "label": "(14, MPI, err = MPI_Barrier(ab_c_inter);); (16, MPI, err = MPI_Intercomm_merge(ab_c_inter, 0, &abc_intra);); (25, MPI, MPI_Comm_disconnect(&ab_inter);); (21, MPI, MPI_Comm_free(&ab_intra);); (18, MPI, MPI_Comm_free(&abc_intra);); (5, MPI, err = MPI_Intercomm_create(ab_intra, 0, ac_intra, count, tag, &ab_c_inter);); (17, MPI, MPI_Comm_free(&ab_c_inter);); (18, MPI, MPI_Comm_free(&ac_intra);); (20, MPI, MPI_Comm_disconnect(&ac_inter);); (10, MPI, err = MPI_Barrier(abc_intra);)"}
{"predict": "(30,MPI,double t0 = pcu::Time();), (31,OpenMP,#pragma omp parallel for), (32,MPI,double t1 = pcu::Time();)", "label": "(2, MPI, MPI_Init(&argc, &argv);); (55, MPI, MPI_Finalize();)"}
{"predict": "(11,OpenMP,#pragma omp parallel for)", "label": "(5, OpenMP, #pragma omp parallel num_threads(2)); (3, OpenMP, omp_set_nested(1);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(23, OpenCL, clReleaseEvent(events[i]);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp parallel for)"}
{"predict": ":             if (!silent)\n162:                 output.display();\n163:             assert(output.get_value() == high);\n164:         }\n165: \n166:         { // test two-input xor_gate: 00, 01, 10, 11\n167:             xor_gate<2> x(g);\n168:             toggle input0(g);\n169:             toggle input1(g);\n170:             led output(g, \"OUTPUT\", false);\n171: \n172:             make_edge(input0.get_out(), input_port<0>(x));\n173:             make_edge(input1.get_out(), input_port<1>(x));\n174:             make_edge(output_port<0>(x), output.get_in());\n175: \n176:             if (!silent)\n177:                 printf(\"Testing xor_gate...\\n\");\n178:             input0.activate(); // 0 0\n179:             input1.activate();\n180:             g.wait_for_all();\n181:             if (!silent)\n182:                 output.display();\n183:             assert(output.get_value() == low);\n184:             input0.flip(); // 0 1\n185:             g.wait_for_all();\n186:             if (!silent)\n187:                 output.display();\n188:             assert(output.get_value() == high);\n189:             input1.flip();\n190:             input0.flip(); // 1 0\n191:             g.wait_for_all();\n192:             if (!silent)\n193:                 output.display();\n194:             assert(output.get_value() == high);\n195:             input0.flip(); // 1 1\n196:             g.wait_for_all();\n197:             if (!silent)\n198:                 output.display();\n199:             assert(output.get_value() == low);\n200:         }\n201: \n202:         { // test two-input nand_gate: 00, 01, 10, 11\n203:             nand_gate<2> n(g);\n204:             toggle input0(g);\n205:             toggle input", "label": "(720, TBB, utility::report_elapsed_time((oneapi::tbb::tick_count::now() - start).seconds());); (15, TBB, oneapi::tbb::global_control c(oneapi::tbb::global_control::max_allowed_parallelism, p);); (19, TBB, oneapi::tbb::flow::graph g;); (11, TBB, oneapi::tbb::tick_count start = oneapi::tbb::tick_count::now();)"}
{"predict": "(10,MPI,*w = MPI_Wtime();)\n(13,OpenMP,#pragma omp parallel for)\n(22,MPI,*w = MPI_Wtime();)\n(25,OpenMP,#pragma omp parallel for)\n(28,MPI,*w = MPI_Wtime();)\n(31,OpenMP,#pragma omp parallel for)\n(34,MPI,*w = MPI_Wtime();)\n(37,OpenMP,#pragma omp parallel for)\n(47,OpenMP,#pragma omp parallel for)\n(70,OpenMP,#pragma omp parallel for)", "label": "(47, CUDA, CHECK(cudaStreamCreate(&(streams[n])));); (84, CUDA, CHECK(cudaStreamDestroy(streams[i]));); (94, CUDA, CHECK(cudaFree(d_y));); (19, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice));); (93, CUDA, CHECK(cudaFree(d_z));); (53, CUDA, CHECK(cudaStreamDestroy(streams[n]));); (58, CUDA, CHECK(cudaMallocHost(&h_x2, M));); (82, CUDA, CHECK(cudaFreeHost(h_y2));); (82, CUDA, CHECK(cudaFreeHost(h_z2));); (66, CUDA, CHECK(cudaStreamCreate(&(streams[i])));); (10, CUDA, CHECK(cudaMalloc(&d_y, M));); (8, CUDA, CHECK(cudaMalloc(&d_x, M));); (83, CUDA, CHECK(cudaFree(d_x));); (53, CUDA, CHECK(cudaMallocHost(&h_z2, M));); (7, CUDA, CHECK(cudaMalloc(&d_z, M));); (73, CUDA, CHECK(cudaFreeHost(h_x2));); (7, CUDA, CHECK(cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice));); (48, CUDA, CHECK(cudaMallocHost(&h_y2, M));)"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(14, MPI, MPI_Init(&argc, &argv);); (47, MPI, MPI_Finalize();); (33, MPI, MPI_Barrier(MPI_COMM_WORLD);); (12, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (40, MPI, CHECK(MPI_File_delete(TEST_FILENAME, MPI_INFO_NULL));); (27, MPI, CHECK(MPI_File_close(&fileh));); (19, MPI, CHECK(MPI_File_set_view(fileh, 0, MPI_INT, MPI_INT, datarep[i], MPI_INFO_NULL));)"}
{"predict": "(14,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for)", "label": "(28, MPI, MPI_Recv(NULL, 0, MPI_BYTE, p->nbor2, 1, MPI_COMM_WORLD, &status);); (22, MPI, MPI_Recv(NULL, 0, MPI_BYTE, p->nbor, 1, MPI_COMM_WORLD, &status);); (25, MPI, MPI_Send(NULL, 0, MPI_BYTE, p->nbor2, 1, MPI_COMM_WORLD);); (34, MPI, latency = (MPI_Wtime() - t0) / (2 * g_latency012_reps);); (18, MPI, MPI_Send(NULL, 0, MPI_BYTE, p->nbor, 1, MPI_COMM_WORLD);); (10, MPI, t0 = MPI_Wtime();)"}
{"predict": "(26,MPI,MPI_Init(&argc, &argv);) (27,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);) (28,MPI,MPI_Comm_size(MPI_COMM_WORLD, &size);) (34,MPI,MPI_Bcast(&startTriggerIteration, 1, MPI_INT, 0, MPI_COMM_WORLD);) (36,MPI,MPI_Bcast(&stopTriggerIteration, 1, MPI_INT, 0, MPI_COMM_WORLD);) (39,MPI,MPI_Scatter(Matrix, NUM / size, MPI_FLOAT, Matrix, NUM / size, MPI_FLOAT, 0, MPI_COMM_WORLD);) (42,MPI,MPI_Scatter(TransposeMatrix, NUM / size, MPI_FLOAT, TransposeMatrix, NUM / size, MPI_FLOAT, 0, MPI_COMM_WORLD);) (45,MPI,MPI_Scatter(cpuTransposeMatrix, NUM / size, MPI_FLOAT, cpuTransposeMatrix, NUM / size, MPI_FLOAT, 0, MPI_COMM_WORLD);) (48,MPI,MPI_Gather(TransposeMatrix, NUM / size, MPI_FLOAT, TransposeMatrix, NUM / size, MPI_FLOAT, 0, MPI_COMM_WORLD);) (51,MPI,MPI_Gather(cpuTransposeMatrix, NUM / size, MPI_FLOAT, cpuTransposeMatrix, NUM / size, MPI_FLOAT, 0, MPI_COMM_WORLD);) (53,MPI,MPI_Barrier(MPI_COMM_WORLD);) (83,MPI,MPI_Finalize();)", "label": "(20, CUDA, cudaGetDeviceProperties(&devProp, 0);); (81, CUDA, cudaFree(gpuMatrix);); (43, CUDA, cudaMalloc((void**)&gpuMatrix, NUM * sizeof(float));); (81, CUDA, cudaFree(gpuTransposeMatrix);); (43, CUDA, cudaMalloc((void**)&gpuTransposeMatrix, NUM * sizeof(float));)"}
{"predict": "(26,OpenMP,#pragma omp parallel for)", "label": "(24, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (19, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (50, MPI, MPI_Finalize();); (47, MPI, MPI_T_finalize();); (18, MPI, MPI_T_init_thread(required, &provided);); (31, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for)", "label": "(22, MPI, MPI_Gatherv_c(sendbuf, count, datatype, recvbuf, counts, displs, datatype, root, comm);); (26, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);)"}
{"predict": "comparison ('<', '<=', '>', or '>=') of loop variable 'i'}}\n119: #pragma omp taskloop\n120:   for (int i = 0; i <= 10; i++)\n121:     c[i] = a[i];\n122: \n123: #pragma omp parallel\n124: // omp4-error@+2 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', or '>=') of loop variable 'i'}}\n125: #pragma omp taskloop\n126:   for (int i = 0; i >= 10; i--)\n127:     c[i] = a[i];\n128: \n129: #pragma omp parallel\n130: // omp4-error@+2 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', or '>=') of loop variable 'i'}}\n131: #pragma omp taskloop\n132:   for (int i = 0; i < 10; i -= 1)\n133:     c[i] = a[i];\n134: \n135: #pragma omp parallel\n136: // omp4-error@+2 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', or '>=') of loop variable 'i'}}\n137: #pragma omp taskloop\n138:   for (int i = 0; i > 10; i--)\n139:     c[i] = a[i];\n140: \n141: #pragma omp parallel\n142: // omp4-error@+2 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', or '>=') of loop variable 'i'}}\n143: #pragma omp taskloop\n144:   for (int i = 0; i != 10; i--)\n145:     c[i] = a[i];\n146: \n147: #pragma omp parallel\n148: // omp4-error@+2 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', or '>=') of loop variable 'i'}}\n149: #pragma", "label": "(286, OpenMP, #pragma omp taskloop lastprivate(ii)); (280, OpenMP, #pragma omp taskloop private(ii)); (5, OpenMP, #pragma omp parallel); (5, OpenMP, #pragma omp taskloop); (317, OpenMP, #pragma omp taskloop collapse(2)); (265, OpenMP, #pragma omp taskloop firstprivate(ii)); (270, OpenMP, #pragma omp taskloop linear(ii))"}
{"predict": "(17,MPI,MTest_Init_thread(&argc, &argv, required, &provided);); (23,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for), (35,OpenMP,#pragma omp parallel for)", "label": "(40, MPI, MPI_T_finalize();); (15, MPI, MPI_T_init_thread(required, &provided);)"}
{"predict": "(30,OpenMP,#pragma omp parallel for)", "label": "(104, OpenCL, retVal = clReleaseKernel(kernel);); (34, OpenCL, context = clCreateContext(nullptr, 1, &device, nullptr, nullptr, &retVal);); (108, OpenCL, retVal = clReleaseCommandQueue(cmdQ);); (110, OpenCL, retVal = clReleaseContext(context);); (57, OpenCL, kernel = clCreateKernel(pProgram, \"CopyBuffer\", &retVal);); (102, OpenCL, retVal = clReleaseProgram(pProgram);); (33, OpenCL, cmdQ = clCreateCommandQueue(context, device, properties, &retVal);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for collapse(2))", "label": "(14, CUDA, return CUDA(cudaGetLastError());)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(1, OpenMP, #pragma omp target teams); (5, OpenMP, #pragma omp parallel)"}
{"predict": "(\"Data type              = double\\n\");\n119: #else\n120:     printf(\"Data type              = single\\n\");\n121: #endif\n122:     printf(\"Grid size per tile     = %d\\n\", n/Num_procs);\n123:     printf(\"Number of iterations   = %d\\n\", iterations);\n124:   }\n125: \n126:   /* Allocate memory for input and output arrays */\n127:   total_length_in  = nsquare;\n128:   total_length_out = nsquare;\n129:   in = (DTYPE *) malloc(total_length_in * sizeof(DTYPE));\n130:   out = (DTYPE *) malloc(total_length_out * sizeof(DTYPE));\n131:   if (in == NULL || out == NULL) {\n132:     printf(\"ERROR: Can't allocate enough memory\\n\");\n133:     error = 1;\n134:     goto ENDOFTESTS;\n135:   }\n136: \n137:   /* Initialize the input array */\n138:   init_array(n, in, RADIUS);\n139: \n140:   /* Initialize the output array */\n141:   init_array(n, out, RADIUS);\n142: \n143:   /* Initialize the weights */\n144:   init_weights(weight, RADIUS);\n145: \n146:   /* Initialize the RMA windows */\n147:   MPI_Win_create(out, total_length_out * sizeof(DTYPE), sizeof(DTYPE),\n148:                 MPI_INFO_NULL, MPI_COMM_WORLD, &rma_winx);\n149:   MPI_Win_create(out, total_length_out * sizeof(DTYPE), sizeof(DTYPE),\n150:                 MPI_INFO_NULL, MPI_COMM_WORLD, &rma_winy);\n151: \n152:   /* Set up the RMA windows */\n153:   MPI_Win_fence(0, rma_winx);\n154:   MPI_Win_fence(0, rma_winy);\n155: \n156:   /* Compute the L1 norm of the input array */\n157:   local_norm = 0.0;\n158:   for (i = 0;", "label": "(210, MPI, MPI_Info_create(&rma_winfo);); (212, MPI, MPI_Info_set(rma_winfo, \"no_locks\", \"true\");); (370, MPI, MPI_Info_free(&rma_winfo);); (371, MPI, MPI_Finalize();); (128, MPI, MPI_Bcast(&n,          1, MPI_INT, root, MPI_COMM_WORLD);); (128, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (267, MPI, MPI_Win_fence(MPI_MODE_NOSTORE, rma_winx);); (39, MPI, MPI_Init(&argc,&argv);); (232, MPI, MPI_Barrier(MPI_COMM_WORLD);); (321, MPI, MPI_Reduce(&local_norm, &norm, 1, MPI_DTYPE, MPI_SUM, root, MPI_COMM_WORLD);); (38, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (234, MPI, MPI_Win_fence(MPI_MODE_NOSTORE, rma_winy);); (35, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(16,OpenMP,#pragma omp parallel for)", "label": "(28, OpenCL, retVal = clReleaseMemObject(image);); (6, OpenCL, auto status = clGetDeviceInfo(&mockClDevice, CL_DEVICE_IMAGE_SUPPORT, sizeof(imageSupportInfo), &imageSupportInfo, nullptr);)"}
{"predict": "(14,MPI,MPI_Comm cart_comm;); (22,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp barrier)", "label": "(12, MPI, MPI_Init(&argc, &argv);); (34, MPI, MPI_Finalize();); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (11, MPI, MPI_Dims_create(size, NR_DIMS, dims);); (17, MPI, MPI_Cart_coords(cart_comm, rank, NR_DIMS, cart_coords);); (27, MPI, MPI_Barrier(MPI_COMM_WORLD);); (13, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(33,MPI,*val = 0;); (35,MPI,if (envval) { printf(\" envval = %s\\n\", envval); if (strcmp(envval, \"yes\") == 0 || strcmp(envval, \"YES\") == 0 || strcmp(envval, \"true\") == 0 || strcmp(envval, \"TRUE\") == 0) *val = 1; })", "label": "(27, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(40,OpenMP,#pragma omp parallel for)\n(42,OpenMP,#pragma omp parallel for)\n(44,OpenMP,#pragma omp parallel for)\n(46,OpenMP,#pragma omp parallel for)\n(48,OpenMP,#pragma omp parallel for)\n(50,OpenMP,#pragma omp parallel for)\n(52,OpenMP,#pragma omp parallel for)", "label": "(39, CUDA, cudaMalloc((void **)&df_C, M * N * sizeof(float));); (62, CUDA, cudaFree(df_A);); (36, CUDA, cudaMalloc((void **)&df_B, K * N * sizeof(float));); (38, CUDA, cudaStat = cudaStreamCreate(&stream);); (61, CUDA, cudaFree(df_C);); (32, CUDA, cudaMalloc((void **)&df_A, M * K * sizeof(float));); (55, CUDA, cudaStreamDestroy(stream);); (48, CUDA, cudaStreamSynchronize(stream);); (56, CUDA, cudaFree(df_B);)"}
{"predict": "%d and GPU%d\\n\", gpuid[0],\n104:          gpuid[1]);\n105: \n106:   // Clean up\n107:   cudaFreeHost(h0);\n108:   checkCudaErrors(cudaSetDevice(gpuid[0]));\n109:   cudaFree(g0);\n110:   checkCudaErrors(cudaSetDevice(gpuid[1]));\n111:   cudaFree(g1);\n112:   printf(\"[%s] - Test finished.\\n\", argv[0]);\n113:   return 0;\n114: }", "label": "(46, CUDA, checkCudaErrors(cudaDeviceCanAccessPeer(&can_access_peer, i, j));); (99, CUDA, checkCudaErrors(cudaEventCreateWithFlags(&stop_event, eventflags));); (101, CUDA, checkCudaErrors(cudaEventRecord(start_event, 0));); (107, CUDA, checkCudaErrors(cudaMemcpy(g1, g0, buf_size, cudaMemcpyDefault));); (190, CUDA, checkCudaErrors(cudaFreeHost(h0));); (142, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (156, CUDA, checkCudaErrors(cudaMemcpy(h0, g0, buf_size, cudaMemcpyDefault));); (109, CUDA, checkCudaErrors(cudaEventRecord(stop_event, 0));); (6, CUDA, checkCudaErrors(cudaGetDeviceCount(&gpu_n));); (66, CUDA, checkCudaErrors(cudaSetDevice(gpuid[0]));); (181, CUDA, checkCudaErrors(cudaFree(g0));); (20, CUDA, checkCudaErrors(cudaGetDeviceProperties(&prop[i], i));); (74, CUDA, checkCudaErrors(cudaMalloc(&g0, buf_size));); (176, CUDA, checkCudaErrors(cudaEventDestroy(stop_event));); (62, CUDA, checkCudaErrors(cudaDeviceEnablePeerAccess(gpuid[1], 0));); (97, CUDA, checkCudaErrors(cudaMemcpy(g0, g1, buf_size, cudaMemcpyDefault));); (102, CUDA, checkCudaErrors(cudaEventElapsedTime(&time_memcpy, start_event, stop_event));); (171, CUDA, checkCudaErrors(cudaEventDestroy(start_event));); (59, CUDA, checkCudaErrors(cudaSetDevice(gpuid[1]));); (70, CUDA, checkCudaErrors(cudaMalloc(&g1, buf_size));); (58, CUDA, checkCudaErrors(cudaDeviceEnablePeerAccess(gpuid[0], 0));); (163, CUDA, checkCudaErrors(cudaDeviceDisablePeerAccess(gpuid[0]));); (160, CUDA, checkCudaErrors(cudaDeviceDisablePeerAccess(gpuid[1]));); (76, CUDA, checkCudaErrors(cudaEventCreateWithFlags(&start_event, eventflags));); (173, CUDA, checkCudaErrors(cudaSetDevice(i));); (107, CUDA, checkCudaErrors(cudaMemcpy(g0, h0, buf_size, cudaMemcpyDefault));); (91, CUDA, checkCudaErrors(cudaEventSynchronize(stop_event));); (166, CUDA, checkCudaErrors(cudaFree(g1));)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(3, CUDA, Check_CudaErroorFun(\"At the beginning.\");)"}
{"predict": "(11,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for)", "label": "(4, MPI, MPI_Comm_size(result.global, &result.size);); (4, MPI, MPI_Comm_rank(result.global, &result.rank);); (1, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &result.global);); (15, MPI, MPI_Comm_rank(result.local, &result.local_rank);); (12, MPI, MPI_Comm_split(result.global, result.group, result.rank, &result.local);); (-3, MPI, MPI_Init(nullptr, nullptr);); (11, MPI, MPI_Comm_size(result.local, &result.local_size);)"}
{"predict": "map_item_update(dst_map, key, DCMPF_CONTENT, DCMPS_DIFFER);\n143:             }\n144:         } else {\n145:             /* compare regular files */\n146:             int compare_rc = dcmp_compare_regular_files(src_list, src_map, src_index,\n147:                 dst_list, dst_map, dst_index,\n148:                 key, strlen_prefix, copy_opts);\n149:             if (compare_rc == -1) {\n150:                 /* we hit an error while comparing the files */\n151:                 rc = -1;\n152:                 MFU_LOG(MFU_LOG_ERR,\n153:                     \"Failed to compare regular files on %s and/or %s. Assuming contents are different.\",\n154:                     src_name, dst_name);\n155:             }\n156:             if (!compare_rc) {\n157:                 /* update to say contents of the regular files were found to be the same */\n158:                 dcmp_strmap_item_update(src_map, key, DCMPF_CONTENT, DCMPS_COMMON);\n159:                 dcmp_strmap_item_update(dst_map, key, DCMPF_CONTENT, DCMPS_COMMON);\n160:             } else {\n161:                 /* update to say contents of the regular files were found to be different */\n162:                 dcmp_strmap_item_update(src_map, key, DCMPF_CONTENT, DCMPS_DIFFER);\n163:                 dcmp_strmap_item_update(dst_map, key, DCMPF_CONTENT, DCMPS_DIFFER);\n164:             }\n165:         }\n166:     }\n167: \n168:     /* free the lists */\n169:     mfu_flist_free(&src_compare_list);\n170:     mfu_flist_free(&dst_compare_list);\n171: \n172:     /* check for errors during the compare */\n173:     if (rc) {\n174:         return rc;\n175:     }\n176: \n177:     /* compare metadata if needed */\n178:     if (dcmp_option_need_compare(DCMPF_METADATA)) {\n179:         rc = dcmp_compare_metadata(src_list, src_map, 0,\n1", "label": "(17, MPI, MPI_Barrier(MPI_COMM_WORLD);); (26, MPI, double start_compare = MPI_Wtime();); (199, MPI, double end_compare = MPI_Wtime();)"}
{"predict": "(17,OpenMP,#pragma omp parallel for private(instance)), (28,OpenMP,#pragma omp parallel for private(instance))", "label": "(24, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(9,MPI,MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided);) (11,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);) (12,MPI,MPI_Comm_size(MPI_COMM_WORLD, &size);) (32,OpenMP,#pragma omp parallel for)", "label": "(54, MPI, MPI_Comm_free(&comm2);); (46, MPI, MPI_Ssend(buffer, 0, MPI_INT, rank, 1, MPI_COMM_WORLD);); (31, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &comm2);); (43, MPI, MPI_Barrier(comm4);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (38, MPI, MPI_Comm_dup(comm1, &comm4);); (46, MPI, MPI_Comm_free(&comm4);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (45, MPI, MPI_Comm_free(&comm1);); (39, MPI, MPI_Recv(buffer, 0, MPI_INT, rank, 2, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (30, MPI, MPI_Ssend(buffer, 0, MPI_INT, rank, 0, MPI_COMM_WORLD);); (21, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &comm1);)"}
{"predict": "report.time;\n113:                 }\n114:             }\n115:             bandwidth = ((double)aggregate_number_of_messages * size) / (aggregate_time * 1024 * 1024);\n116:             message_rate = (double)aggregate_number_of_messages / aggregate_time;\n117:             latency = max_time / (aggregate_number_of_messages * 2);\n118:             fprintf(unit, \"%12lu %12lu %12.4f %12.4f %12.4f\\n\", size, number_of_iterations, aggregate_time, bandwidth, message_rate);\n119:             fflush(unit);\n120:         }\n121:     }\n122:     for (i = 0; i < NUMBER_OF_NEIGHBORS; i++) {\n123:         imb_p2p_free_mem(send_buffers[i]);\n124:     }\n125:     for (i = 0; i < NUMBER_OF_NEIGHBORS; i++) {\n126:         imb_p2p_free_mem(recv_buffers[i]);\n127:     }\n128: }\n\n### **Explanation**\nThe code is designed to perform a stencil2d benchmark using MPI for communication. The benchmark involves sending and receiving messages to neighboring ranks in a 2D grid. The goal is to optimize the communication phase by parallelizing the send and receive operations.\n\n### **Answer**", "label": "(75, MPI, MPI_Waitall((NUMBER_OF_NEIGHBORS * 2), requests, MPI_STATUSES_IGNORE);); (97, MPI, time = MPI_Wtime() - time;); (71, MPI, MPI_Isend(send_buffers[i], size, MPI_BYTE, neighbours[i], 0, MPI_COMM_WORLD, &requests[NUMBER_OF_NEIGHBORS + i]);); (78, MPI, time = MPI_Wtime();); (101, MPI, MPI_Send(&report, sizeof(imb_p2p_report_t), MPI_BYTE, 0, 1, MPI_COMM_WORLD);); (65, MPI, MPI_Irecv(recv_buffers[i], size, MPI_BYTE, neighbours[i], 0, MPI_COMM_WORLD, &requests[i]);); (108, MPI, MPI_Recv(&report, sizeof(imb_p2p_report_t), MPI_BYTE, source_rank, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)"}
{"predict": "(30,OpenMP,#pragma omp parallel for), (55,OpenMP,#pragma omp parallel for)", "label": "(59, CUDA, cudaEventRecord( evStop );); (75, CUDA, cudaFree( in );); (72, CUDA, cudaEventDestroy( evStart );); (52, CUDA, cudaEventRecord( evStart );); (73, CUDA, cudaFree( out );); (70, CUDA, cudaEventDestroy( evStop );)"}
{"predict": "(37,OpenMP,#pragma omp parallel for)", "label": "(35, OpenMP, #pragma omp target teams distribute parallel for map(a, b))"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (38,OpenMP,#pragma omp parallel for), (72,OpenMP,#pragma omp parallel for), (114,OpenMP,#pragma omp parallel for)", "label": "(40, CUDA, checkCudaErrors(cudaMemset(d_Output, 0, QRNG_DIMENSIONS * N * sizeof(float)));); (44, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (124, CUDA, checkCudaErrors(cudaFree(d_Output));)"}
{"predict": "(15,MPI,*w = MPI_Wtime(););", "label": "(15, CUDA, qmc_cuda::cuda_check(cudaGetLastError(), \"vKKwij_to_vwKiKj\");); (15, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize(), \"vKKwij_to_vwKiKj\");)"}
{"predict": "(12,MPI,*w = MPI_Wtime();); (15,OpenMP,#pragma omp parallel for), (26,MPI,*e = MPI_Wtime();)", "label": "(9, OpenCL, auto userEvent = clCreateUserEvent(pContext, &retVal);); (30, OpenCL, retVal = clReleaseEvent(userEvent);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for)", "label": "(10, OpenMP, #pragma omp teams distribute parallel for); (8, OpenMP, #pragma omp target); (12, OpenMP, #pragma omp teams distribute parallel for if (parallel: false))"}
{"predict": "(3,MPI,MPI_Init(&argc, &argv);) (5,MPI,MPI_Wtime();) (20,MPI,MPI_Type_size(MPI_INT, &sizeofint);) (25,MPI,MPI_Type_size(parent_type, &sizeoftype);) (33,MPI,MPI_Pack(array, 20, MPI_INT, buf, sizeoftype, &position, parent_type);) (36,MPI,MPI_Unpack(buf, sizeoftype, &position, array, 20, MPI_INT, parent_type);) (60,MPI,MPI_Finalize();)", "label": "(12, MPI, MPI_Pack_external_size((char *) \"external32\", 1, MPI_INT, &sizeofint);); (66, MPI, MPI_Type_free(&parent_type);); (36, MPI, MPI_Pack_external((char *) \"external32\", array, 1, parent_type, buf, sizeoftype, &position);); (46, MPI, MPI_Unpack_external((char *) \"external32\", buf, sizeoftype, &position, array, 1, parent_type);); (21, MPI, MPI_Pack_external_size((char *) \"external32\", 1, parent_type, &sizeoftype);); (18, MPI, MPI_Type_commit(&parent_type);); (15, MPI, MPI_Type_vector(10, 2, 2, MPI_INT, &parent_type);)"}
{"predict": "(28,OpenMP,#pragma omp parallel for), (50,OpenMP,#pragma omp parallel for)", "label": "(82, SYCL, auto isA_fp16 = (typeid(TA) == typeid(sycl::half));); (28, SYCL, sycl::free(h_a, q);); (6, SYCL, auto h_c = sycl::malloc_host<TC>( nelems, q);); (4, SYCL, auto h_b = sycl::malloc_host<TB>( nelems, q);); (111, SYCL, sycl::free(h_c, q);); (14, SYCL, auto  A = sycl::malloc_device<TA>( nelems, q);); (94, SYCL, auto isC_fp16 = (typeid(TC) == typeid(sycl::half));); (23, SYCL, sycl::free(h_b, q);); (49, SYCL, sycl::free(A, q);); (47, SYCL, sycl::free(B, q);); (-4, SYCL, auto h_a = sycl::malloc_host<TA>( nelems, q);); (80, SYCL, auto isB_fp16 = (typeid(TB) == typeid(sycl::half));); (9, SYCL, auto  C = sycl::malloc_device<TC>( nelems, q);); (7, SYCL, auto  B = sycl::malloc_device<TB>( nelems, q);); (41, SYCL, sycl::free(C, q);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(16, MPI, mpi_errno = MPI_Comm_create_group(dup_comm, group, 10, NULL);); (20, MPI, MPI_Comm_free(&dup_comm);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (18, MPI, MPI_Group_free(&group);); (12, MPI, MPI_Error_class(mpi_errno, &errclass);); (7, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (5, MPI, MPI_Comm_group(dup_comm, &group);); (3, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &dup_comm);)"}
{"predict": "(17,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for)", "label": "(24, MPI, t[i] = MPI_Wtime() - ttmp;); (19, MPI, MPI_Type_commit(&xpose[j]);); (34, MPI, MPI_Type_free(&column[j]);); (32, MPI, MPI_Type_free(&xpose[j]);); (11, MPI, ttmp = MPI_Wtime();); (13, MPI, MPI_Type_vector(nrows, 1, ncols, MPI_INT, &column[j]);); (13, MPI, MPI_Type_create_hvector(ncols, 1, sizeof(int), column[j], &xpose[j]);)"}
{"predict": "(15,MPI,sum.sync_device();); (17,OpenMP,#pragma omp parallel for)", "label": "(18, CUDA, ASSERT_EQ(cudaGetLastError(), cudaSuccess) << \"Kernel launch error.\";)"}
{"predict": "(14,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for), (42,OpenMP,#pragma omp parallel for), (75,OpenMP,#pragma omp parallel for)", "label": "(3, CUDA, __shared__ float buffer[128];); (3, CUDA, __shared__ float sums[4];); (0, CUDA, __shared__ float gauss[16];)"}
{"predict": "(7,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);) (10,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for), (38,OpenMP,#pragma omp parallel for), (47,OpenMP,#pragma omp parallel for), (61,OpenMP,#pragma omp parallel for)", "label": "(26, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (7,OpenMP,#pragma omp parallel for), (10,OpenMP,#pragma omp parallel for)", "label": "(12, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(17,OpenMP,#pragma omp parallel for collapse(2))", "label": "(15, CUDA, checkCudaErrors(cudaGetLastError());); (13, CUDA, checkCudaErrors(cudaStreamSynchronize(stream));)"}
{"predict": "22: \n123:     *new_count = count;\n124:     MPI_Type_commit(newtype);\n125:     return rc;\n126: }", "label": "(14, MPI, rc = MPI_Type_get_extent(type, &tmp_lb, &attr->child_type_extent);); (127, MPI, rc = MPI_Type_free(&type);)"}
{"predict": "(16,OpenMP,#pragma omp parallel for)", "label": "(20, OpenMP, #pragma omp target update to(mapper(id): fd)); (10, OpenMP, #pragma omp declare mapper(id: N1::vec v) map(v.len)); (14, OpenMP, #pragma omp target map(mapper(id) alloc: fd)); (15, OpenMP, #pragma omp target map(mapper(idd) alloc: fd.b)); (19, OpenMP, #pragma omp target update from(mapper(idd): fd.b)); (4, OpenMP, #pragma omp declare mapper(id: struct foodat v) map(v.a)); (15, OpenMP, #pragma omp target update to(mapper(idd): fd.b)); (3, OpenMP, #pragma omp declare mapper(idd: struct foodatchild v) map(v.k)); (14, OpenMP, #pragma omp target update from(mapper(id): fd))"}
{"predict": "(3,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for)", "label": "(4, CUDA, cudaMemcpy(state.vx, state.dvx, size_f, cudaMemcpyDeviceToHost);); (4, CUDA, cudaMemcpy(state.vy, state.dvy, size_f, cudaMemcpyDeviceToHost);); (4, CUDA, cudaMemcpy(state.rho, state.drho, size_f, cudaMemcpyDeviceToHost);)"}
{"predict": "(15,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for)", "label": "(12, CUDA, __shared__ T tails[SMALL_KERNEL_BLOCK * SMALL_KERNEL_BLOCK];); (18, CUDA, extern __shared__ char _arrays[];)"}
{"predict": "2 failed\\n\");\n150: \n151:     // Collect the results\n152:     if (gHostFill)\n153:     {\n154:         for (auto j = gMinVectorSizeIndex; j < gMaxVectorSizeIndex; j++)\n155:         {\n156:             if ((error = clEnqueueMapBuffer(tinfo->tQueue, tinfo->outBuf[j],\n157:                                             CL_TRUE, CL_MAP_READ, 0,\n158:                                             buffer_size, 0, NULL, e + j, &error)))\n159:             {\n160:                 vlog_error(\"Error: clEnqueueMapBuffer %d failed! err: %d\\n\", j,\n161:                            error);\n162:                 return error;\n163:             }\n164: \n165:             // Copy the results back to the host\n166:             memcpy(r + idx, out[j], buffer_size);\n167:             idx += buffer_elements;\n168: \n169:             if ((error = clWaitForEvents(1, e + j)))\n170:             {\n171:                 vlog_error(\"Error: clWaitForEvents failed! err: %d\\n\", error);\n172:                 return error;\n173:             }\n174:             if ((error = clReleaseEvent(e[j])))\n175:             {\n176:                 vlog_error(\"Error: clReleaseEvent failed! err: %d\\n\", error);\n177:                 return error;\n178:             }\n179:         }\n180:     }\n181: \n182:     return CL_SUCCESS;\n183: }", "label": "(134, OpenCL, error = clSetKernelArg(kernel, 1, sizeof(tinfo->inBuf), &tinfo->inBuf);)"}
{"predict": "(17,MPI,*w = MPI_Wtime();); (26,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for)", "label": "(45, MPI, ierr = MPI_Finalize();); (15, MPI, ierr = MPI_Comm_rank(MPI_COMM_WORLD, &myid);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for private(k, dims)), (27,OpenMP,#pragma omp barrier)", "label": "(9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (14, MPI, MPI_Dims_create(tests[k].size, tests[k].dim, dims);)"}
{"predict": "(24,OpenMP,#pragma omp parallel for collapse(2)), (37,OpenMP,#pragma omp parallel for collapse(2)), (52,OpenMP,#pragma omp parallel for collapse(2)), (66,OpenMP,#pragma omp parallel for collapse(2)), (80,OpenMP,#pragma omp parallel for collapse(2))", "label": "(8, MPI, MPI_Finalize();); (0, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(7,MPI,int rank; MPI_Comm_rank(MPI_COMM_WORLD, &rank);) (8,OpenMP,#pragma omp parallel for)", "label": "(5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(21,OpenMP,#pragma omp parallel for collapse(2)), (33,OpenMP,#pragma omp parallel for collapse(2)), (40,OpenMP,#pragma omp parallel for collapse(2)), (77,OpenMP,#pragma omp parallel for collapse(2)), (82,OpenMP,#pragma omp parallel for collapse(2))", "label": "(21, CUDA, CUASSERT(cudaMemsetAsync(sortClassValidCountPtr, 0, N * sizeof(int), stream));); (19, CUDA, CUASSERT(cudaMemsetAsync(argMaxBBoxPtr, 0, N * samples * 4 * sizeof(float), stream));); (20, CUDA, CUASSERT(cudaMemsetAsync(sortClassPosPtr, 0, N * (NClass + 1) * sizeof(int), stream));); (20, CUDA, CUASSERT(cudaMemsetAsync(sortClassSampleIdxPtr, 0, N * (samples + 1) * sizeof(int), stream));); (15, CUDA, CUASSERT(cudaMemsetAsync(argMaxScorePtr, 0, N * samples * sizeof(float), stream));)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (14,MPI,MPI_Barrier(MPI_COMM_WORLD);)", "label": "(11, MPI, MPI_Send(pre_info, 2, MPI_INT, 0, kTag, MPI_COMM_WORLD);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (56,OpenMP,#pragma omp parallel for)", "label": "(32, OpenCL, err = clEnqueueWriteBuffer(queue, in, CL_TRUE, 0, bytes, &h_in[0], 0, NULL, NULL);); (44, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(cl_mem), &in);); (51, OpenCL, err = clEnqueueReadBuffer(queue, in, CL_TRUE, 0, bytes, &h_out[0], 0, NULL, NULL);); (26, OpenCL, clMemWrapper in = clCreateBuffer(context, CL_MEM_READ_WRITE, bytes, NULL, &err);); (38, OpenCL, clKernelWrapper kernel = clCreateKernel(prog, spvName, &err);); (44, OpenCL, err = clEnqueueNDRangeKernel(queue, kernel, 1, NULL, &global, NULL, 0, NULL, NULL);)"}
{"predict": "(5,MPI,int rank = MPI_COMM_WORLD.Get_rank();), (7,MPI,MPI_Error_string(err, errorString, &errorStringLen);)", "label": "(6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (6, MPI, MPI_Error_string(err, errorString, &errorStringLen);); (10, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(7,MPI,MPI_Init(&argc, &argv);)", "label": "(7, MPI, MPI_Get_library_version(version, &resultlen);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for), (50,OpenMP,#pragma omp parallel for)", "label": "(61, TBB, tbb::internal::segmented_iterator<nested_vec, const T> my_csi(my_vec);); (18, TBB, tbb::internal::segmented_iterator<nested_vec, T> my_si(my_vec);)"}
{"predict": "(34,OpenMP,#pragma omp parallel for), (50,OpenMP,#pragma omp parallel for)", "label": "(61, OpenCL, clReleaseMemObject(mem);); (33, OpenCL, cl_mem buf = clCreateBuffer(ctx, CL_MEM_USE_HOST_PTR, sz, data, &err);); (23, OpenCL, ctx->image_queue = clCreateCommandQueueWithProperties(ctx, ctx->devices[0], 0, &err);); (55, OpenCL, err = clEnqueueCopyBufferToImage(ctx->image_queue, buf, mem, 0, origin, region, 0, NULL, NULL);); (38, OpenCL, clReleaseMemObject(buf);)"}
{"predict": "20: #pragma omp target\n121: #pragma omp teams\n122:   for (i = 0; i < argc; ++i)\n123:     foo();\n124: #pragma omp target\n125: #pragma omp teams\n126:   for (i = 0; i < argc; ++i)\n127:     foo();\n128: #pragma omp target\n129: #pragma omp teams\n130:   for (i = 0; i < argc; ++i)\n131:     foo();\n132: #pragma omp target\n133: #pragma omp teams\n134:   for (i = 0; i < argc; ++i)\n135:     foo();\n136: #pragma omp target\n137: #pragma omp teams\n138:   for (i = 0; i < argc; ++i)\n139:     foo();\n140: #pragma omp target\n141: #pragma omp teams\n142:   for (i = 0; i < argc; ++i)\n143:     foo();\n144: #pragma omp target\n145: #pragma omp teams\n146:   for (i = 0; i < argc; ++i)\n147:     foo();\n148: #pragma omp target\n149: #pragma omp teams\n150:   for (i = 0; i < argc; ++i)\n151:     foo();\n152: #pragma omp target\n153: #pragma omp teams\n154:   for (i = 0; i < argc; ++i)\n155:     foo();\n156: #pragma omp target\n157: #pragma omp teams\n158:   for (i = 0; i < argc; ++i)\n159:     foo();\n160: #pragma omp target\n161: #pragma omp teams\n162:   for (i = 0; i < argc; ++i)\n163:     foo();\n164: #pragma omp target\n165: #pragma omp teams\n166:   for (i = 0", "label": "(51, OpenMP, #pragma omp distribute parallel for firstprivate(a, b, c, d, f)); (86, OpenMP, #pragma omp distribute parallel for firstprivate(S2::S2s)); (158, OpenMP, #pragma omp parallel private(i)); (94, OpenMP, #pragma omp distribute parallel for safelen(5)); (98, OpenMP, #pragma omp distribute parallel for firstprivate(e, g)); (170, OpenMP, #pragma omp distribute parallel for firstprivate(si)); (70, OpenMP, #pragma omp distribute parallel for firstprivate(da)); (75, OpenMP, #pragma omp distribute parallel for firstprivate(xa)); (28, OpenMP, #pragma omp distribute parallel for firstprivate(argc > 0 ? argv[1] : argv[2])); (113, OpenMP, #pragma omp distribute parallel for firstprivate(i)); (-1, OpenMP, #pragma omp target); (30, OpenMP, #pragma omp distribute parallel for firstprivate(argc)); (133, OpenMP, #pragma omp distribute parallel for lastprivate(n) firstprivate(n)); (99, OpenMP, #pragma omp distribute parallel for firstprivate(k, h, B::x)); (111, OpenMP, #pragma omp parallel); (151, OpenMP, #pragma omp parallel reduction(+ : i)); (0, OpenMP, #pragma omp distribute parallel for firstprivate(); (44, OpenMP, #pragma omp distribute parallel for firstprivate(2 * 2)); (53, OpenMP, #pragma omp distribute parallel for firstprivate(ca)); (114, OpenMP, #pragma omp distribute parallel for firstprivate(j)); (36, OpenMP, #pragma omp distribute parallel for firstprivate(argv[1])); (96, OpenMP, #pragma omp distribute parallel for private(xa), firstprivate(xa)); (70, OpenMP, #pragma omp distribute parallel for firstprivate(S2::S2sc)); (-2, OpenMP, #pragma omp distribute parallel for firstprivate()); (2, OpenMP, #pragma omp distribute parallel for firstprivate(argc); (114, OpenMP, #pragma omp distribute parallel for lastprivate(g) firstprivate(g)); (20, OpenMP, #pragma omp distribute parallel for firstprivate(S1)); (4, OpenMP, #pragma omp distribute parallel for firstprivate(argc,); (79, OpenMP, #pragma omp distribute parallel for firstprivate(m)); (37, OpenMP, #pragma omp distribute parallel for firstprivate(ba)); (-19, OpenMP, #pragma omp distribute parallel for firstprivate); (-21, OpenMP, #pragma omp teams)"}
{"predict": "(11,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for)", "label": "(11, OpenCL, auto commandQueue = clCreateCommandQueueWithProperties(pContext, testedClDevice, properties.data(), &retVal);); (21, OpenCL, clReleaseCommandQueue(commandQueue);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for), (47,OpenMP,#pragma omp parallel for)", "label": "(6, MPI, MPI_Type_get_extent(sddt, &lb, &extent);)"}
{"predict": "(9,MPI,mpi_init();), (12,OpenMP,#pragma omp parallel for), (75,MPI,mpi_finalize();)", "label": "(71, OpenMP, const int th=omp_get_thread_num();); (10, OpenMP, #pragma omp parallel for schedule (guided))"}
{"predict": "(57,OpenMP,#pragma omp parallel for), (71,OpenMP,#pragma omp parallel for)", "label": "(11, CUDA, CUDA_CHECK(cudaGetDeviceProperties(&props, 0));)"}
{"predict": "(5,MPI,*w = MPI_Wtime();)", "label": "(3, MPI, MPI_Init(&argc, &argv);); (9, MPI, MPI_Finalize();)"}
{"predict": "(11,MPI,*w = MPI_Wtime(););\n(27,OpenMP,#pragma omp parallel for)", "label": "(12, OpenCL, result = clGetPlatformInfo(platform, CL_PLATFORM_EXTENSIONS, 0, NULL, &set_size);); (14, OpenCL, result = clGetPlatformInfo(platform, CL_PLATFORM_EXTENSIONS, extensions.size(), extensions.data(), NULL);)"}
{"predict": "(5,OpenMP,#pragma omp parallel for), (8,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for), (29,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for), (35,OpenMP,#pragma omp parallel for)", "label": "(47, OpenMP, #pragma omp teams num_teams (3.14)); (42, OpenMP, #pragma omp teams num_teams (-10u)); (17, OpenMP, #pragma omp teams num_teams (argc))); (4, OpenMP, #pragma omp teams num_teams (); (7, OpenMP, #pragma omp teams num_teams ()); (10, OpenMP, #pragma omp teams num_teams (argc); (25, OpenMP, #pragma omp teams num_teams (argc), num_teams (argc+1)); (32, OpenMP, #pragma omp teams num_teams (-2)); (15, OpenMP, #pragma omp teams num_teams (argc > 0 ? argv[1] : argv[2])); (-7, OpenMP, #pragma omp target); (25, OpenMP, #pragma omp teams num_teams (S1)); (16, OpenMP, #pragma omp teams num_teams (argc + argc-z)); (-9, OpenMP, #pragma omp teams num_teams)"}
{"predict": "(shuffile_truncate(file_send, write_pos) < 0) {\n132:       shuffile_abort(-1, \"Truncating file: shuffile_truncate(%s, %ld) errno=%d %s @ %s:%d\",\n133:               file_send, write_pos, errno, strerror(errno), __FILE__, __LINE__\n134:       );\n135:     }\n136:     shuffile_rename(file_send, file_recv);\n137:   } else if (have_outgoing) {\n138:     /* only sent a file; close it and remove the old file */\n139:     shuffile_close(file_send, fd);\n140:     shuffile_unlink(file_send);\n141:   } else if (have_incoming) {\n142:     /* only received a file; close it */\n143:     shuffile_close(file_recv, fd);\n144:   }\n145: \n146:   /* free buffers */\n147:   if (buf_send != NULL) {\n148:     shuffile_free(buf_send);\n149:   }\n150:   if (buf_recv != NULL) {\n151:     shuffile_free(buf_recv);\n152:   }\n153: \n154:   return rc;\n155: }\n156: ```", "label": "(101, MPI, MPI_Wait(&request[1], &status[1]);); (110, MPI, MPI_Wait(&request[0], &status[0]);); (110, MPI, MPI_Get_count(&status[0], MPI_BYTE, &nwrite);); (74, MPI, MPI_Irecv(buf_recv, shuffile_mpi_buf_size, MPI_BYTE, rank_recv, 0, comm, &request[0]);); (96, MPI, MPI_Isend(buf_send, nread, MPI_BYTE, rank_send, 0, comm, &request[1]);)"}
{"predict": "(10,MPI,*group = MPI_COMM_NULL;); (12,OpenMP,#pragma omp parallel for), (42,MPI,MPI_Intercomm_merge(pgroup_new, 0 /* LOW */ , &pgroup);), (57,MPI,MPI_Intercomm_merge(pgroup_new, 1 /* HIGH */ , &pgroup);), (67,MPI,MPI_Intercomm_merge(pgroup_new, 0 /* HIGH */ , &pgroup);)", "label": "(48, MPI, MPI_Comm_accept(port, MPI_INFO_NULL, 0, pgroup, &pgroup_new);); (52, MPI, MPI_Comm_free(&pgroup_old);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (64, MPI, MPI_Comm_connect(port, MPI_INFO_NULL, 0, MPI_COMM_SELF, &pgroup_new);); (42, MPI, MPI_Send(port, PORT_SIZE, MPI_CHAR, members[i], 0, MPI_COMM_WORLD);); (35, MPI, MPI_Open_port(port_info, port);); (44, MPI, MPI_Comm_free(&pgroup_new);); (48, MPI, MPI_Close_port(port);); (66, MPI, MPI_Comm_connect(port, MPI_INFO_NULL, 0, pgroup, &pgroup_new);)"}
{"predict": "(1, MPI, #include <mpi.h>); (3, MPI, MPI_Init(&argc, &argv);); (5, MPI, int rank, size; MPI_Comm_rank(MPI_COMM_WORLD, &rank); MPI_Comm_size(MPI_COMM_WORLD, &size);); (7, MPI, KeyT thread_keys[ItemsPerThread] = (rank == 0) ? thread_keys : nullptr;); (8, MPI, ValueT thread_vals[ItemsPerThread] = (rank == 0) ? thread_vals : nullptr;); (11, MPI, const int thread_offset = static_cast<int>(threadIdx.x) * ItemsPerThread;); (12, MPI, if (rank == 0) { for (int item = 0; item < ItemsPerThread; item++) { const int idx = thread_offset + item; thread_keys[item] = idx < valid_items ? keys[idx] : KeyT{}; thread_vals[item] = idx < valid_items ? vals[idx] : ValueT{}; } __syncthreads(); }); (20, MPI, if (rank == 0) { BlockMergeSort sort(temp_storage_shuffle); action(sort, thread_keys, thread_vals, valid_items, oob_default); }); (24, MPI, if (rank == 0) { for (int item = 0; item < ItemsPerThread; item++) { const int idx = thread_offset + item; if (idx >= valid_items) { break; } keys[idx] = thread_keys[item]; vals[idx] = thread_vals[item]; } }); (36, MPI, MPI_Finalize(););", "label": "(5, CUDA, __shared__ typename BlockMergeSort::TempStorage temp_storage_shuffle;)"}
{"predict": "(11,MPI,*w = MPI_Wtime();); (13,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for)", "label": "(22, MPI, MPI_Allreduce(MPI_IN_PLACE, buf, count, MPI_INT, op, comm);); (13, MPI, MPI_Comm_size(comm, &size);); (35, MPI, MPI_Op_free(&op);); (6, MPI, MPI_Op_create(mysum, 0, &op);); (11, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(14,MPI,MPI_Get_true_extent(keysat, &true_lb, &true_extent);) (26,OpenMP,#pragma omp parallel for) (32,MPI,MPI_Comm_rank(comm, &rank); MPI_Comm_size(comm, &ranks);) (43,OpenMP,#pragma omp parallel for)", "label": "(34, MPI, MPI_Comm_size(comm, &ranks);); (32, MPI, MPI_Comm_rank(comm, &rank);); (12, MPI, MPI_Type_get_true_extent(keysat, &true_lb, &true_extent);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(10, OpenMP, #pragma omp parallel for)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp parallel for)"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(11, CUDA, cudaStreamCreate(&stream);); (14, CUDA, cudaStreamDestroy(stream);)"}
{"predict": "(7,OpenMP,#pragma omp parallel for reduction(+:my_sum))", "label": "(3, CUDA, __shared__ clock_t s_clocks[32];)"}
{"predict": "(9,MPI,MPI_Init(&argc, &argv);), (20,MPI,MPI_Finalize();), (12,OpenMP,#pragma omp parallel for)", "label": "(11, MPI, MPI_Info_get(info, key, sizeof(buff) - 1, buff, &flag);); (8, MPI, MPI_Info_create(&info);); (19, MPI, MPI_Info_free(&info);); (7, MPI, MPI_Info_set(info, key, val);)"}
{"predict": "\\n\");\n141:     }\n142:   }\n143: \n144:   /* free all memory */\n145:   free(basestring);\n146:   free(iterstring);\n147:   free(catstring);\n148:   MPI_Finalize();\n149:   return 0;\n150: }", "label": "(113, MPI, MPI_Type_commit(&mpi_word);); (120, MPI, MPI_Allgather(iterstring,1,mpi_word, catstring,1,mpi_word, MPI_COMM_WORLD);); (152, MPI, MPI_Finalize();); (61, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (108, MPI, MPI_Type_contiguous(proc_length,MPI_CHAR, &mpi_word);); (17, MPI, MPI_Init(&argc,&argv);); (59, MPI, MPI_Bcast(&length,     1, MPI_LONG, root, MPI_COMM_WORLD);); (108, MPI, MPI_Barrier(MPI_COMM_WORLD);); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(6,OpenMP,#pragma omp parallel for collapse(2)), (8,MPI,*w = MPI_Wtime();), (9,MPI,float As[maxPipelineStages][BLOCK_SIZE_X][BLOCK_SIZE_X];), (11,MPI,float Bs[maxPipelineStages][BLOCK_SIZE_X][BLOCK_SIZE_X];), (38,MPI,auto pipe = cuda::make_pipeline(cta, &shared_state, thread_role);), (64,MPI,pipe.producer_commit();), (70,MPI,pipe.producer_commit();), (77,MPI,pipe.consumer_wait();), (80,MPI,Csub += As[j][threadIdx.y][k] * Bs[j][k][threadIdx.x];), (87,MPI,pipe.consumer_release();), (91,MPI,C[c + wB * threadIdx.y + threadIdx.x] = Csub;), (94,MPI,*e = MPI_Wtime();), (95,MPI,MPI_Reduce(e, w, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);)", "label": "(13, CUDA, __shared__ float Bs[maxPipelineStages][BLOCK_SIZE_X][BLOCK_SIZE_X];); (8, CUDA, __shared__ float As[maxPipelineStages][BLOCK_SIZE_X][BLOCK_SIZE_X];)"}
{"predict": "(19,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for)", "label": "(11, CUDA, extern __shared__ float sdata[];)"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(7, OpenMP, #pragma omp target map(tofrom : device)); (9, OpenMP, printf(\"omp_is_initial_device() returned false on host\\n\");); (12, OpenMP, printf(\"omp_is_initial_device() returned true on device\\n\");)"}
{"predict": "(2,OpenMP,#pragma omp parallel), (7,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for), (57,OpenMP,#pragma omp parallel for), (74,OpenMP,#pragma omp parallel for)", "label": "(32, TBB, tbb::tbb_allocator<t1>().destroy(my_storage);); (35, TBB, tbb::tbb_allocator<t2>().destroy(my_storage);); (0, TBB, tbb::atomic<int> counter;)"}
{"predict": "(10,OpenMP,#pragma omp parallel for), (29,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for), (41,OpenMP,#pragma omp parallel for), (45,OpenMP,#pragma omp parallel for), (49,OpenMP,#pragma omp parallel for), (53,OpenMP,#pragma omp parallel for)", "label": "(56, CUDA, checkCudaErrors(cufftPlan2d(&planc2r, DIM, DIM, CUFFT_C2R));); (54, CUDA, checkCudaErrors(cufftPlan2d(&planr2c, DIM, DIM, CUFFT_R2C));); (37, CUDA, checkCudaErrors(cudaMallocPitch((void **)&dvfield, &tPitch, sizeof(cData)*DIM, DIM));); (38, CUDA, checkCudaErrors(cudaMemcpy(dvfield, hvfield, sizeof(cData) * DS, cudaMemcpyHostToDevice));); (40, CUDA, checkCudaErrors(cudaMalloc((void **)&vyfield, sizeof(cData) * PDS));); (38, CUDA, checkCudaErrors(cudaMalloc((void **)&vxfield, sizeof(cData) * PDS));)"}
{"predict": "(35,OpenMP,#pragma omp parallel for reduction(min:sum2)), (47,OpenMP,#pragma omp parallel for), (54,OpenMP,#pragma omp parallel for reduction(+:sum3))", "label": "(46, OpenMP, #pragma omp teams reduction(+:sum)); (43, OpenMP, #pragma omp target map(tofrom: b, sum, t) map(alloc: a)); (46, OpenMP, #pragma omp distribute parallel for reduction(+:sum)); (48, OpenMP, t[i] = omp_get_team_num();)"}
{"predict": "\", num_debug_state_params);\n92:     printf(\"states: total state size: %.3f MB\\n\", (float)num_debug_state_params * sizeof(float) / 1024 / 1024);\n93: \n94:     float* states_memory_cpu = (float*)mallocCheck(num_debug_state_params * sizeof(float));\n95:     freadCheck(states_memory_cpu, sizeof(float), num_debug_state_params, states_file);\n96:     fcloseCheck(states_file);\n97: \n98:     // allocate device memory\n99:     float* states_memory_gpu;\n100:    cudaMallocCheck((void**)&states_memory_gpu, num_debug_state_params * sizeof(float));\n101:    cudaMemcpyCheck(states_memory_gpu, states_memory_cpu, num_debug_state_params * sizeof(float), cudaMemcpyHostToDevice);\n102: \n103:    // set states pointers\n104:    set_resblock_states_ptrs(&acts, states_memory_gpu);\n105:    set_resblock_states_ptrs(&back_acts, states_memory_gpu);\n106: \n107:    // initialize cublas\n108:    cublasCheck(cublasSetStream(cublas_handle, stream));\n109: \n110:    // set up stream\n111:    cudaStream_t stream;\n112:    cudaStreamCreate(&stream);\n113: \n114:    // set up memory\n115:    float* input_gpu;\n116:    cudaMallocCheck((void**)&input_gpu, state_sizes[0] * sizeof(float));\n117:    cudaMemcpyCheck(input_gpu, states_memory_gpu, state_sizes[0] * sizeof(float), cudaMemcpyDeviceToDevice);\n118: \n119:    float* emb_gpu;\n1120:    cudaMallocCheck((void**)&emb_gpu, state_sizes[1] * sizeof(float));\n1121:    cudaMemcpyCheck(emb_gpu, states_memory_gpu + state_sizes[0], state_sizes[1] * sizeof(float), cudaMemcpyDeviceToDevice);\n1122: \n1123:    float* h_gn1_gpu;\n1124:    cudaMallocCheck((void**)&h_gn1_gpu, state_sizes[2] * sizeof(float));\n1125:    cudaMemcpyCheck(h_gn1_gpu, states_memory_gpu", "label": "(125, CUDA, cudaCheck(cudaMalloc(&acts_memory_gpu, num_acts_params * sizeof(float)));); (257, CUDA, cudaCheck(cudaFree(acts_memory_gpu));); (173, CUDA, cudaCheck(cudaMalloc(&back_acts_memory_gpu, num_back_acts * sizeof(float)));); (254, CUDA, cudaCheck(cudaFree(params_memory_gpu));); (259, CUDA, cudaCheck(cudaFree(acts.emb));); (53, CUDA, cudaCheck(cudaMemset(grads_memory_gpu, 0, num_parameters * sizeof(float)));); (170, CUDA, cudaCheck(cudaMemset(back_acts_memory_gpu, 0, num_back_acts * sizeof(float)));); (182, CUDA, cudaCheck(cudaMemcpy(back_acts.dout, debug_states.dout, B * C_out * H_out * W_out * sizeof(float), cudaMemcpyHostToDevice));); (254, CUDA, cudaCheck(cudaFree(acts.input));); (117, CUDA, cudaCheck(cudaMemset(acts_memory_gpu, 0, num_acts_params * sizeof(float)));); (43, CUDA, cudaCheck(cudaMemcpy(params_memory_gpu, params_memory_cpu, num_parameters * sizeof(float), cudaMemcpyHostToDevice));); (249, CUDA, cudaCheck(cudaFree(back_acts_memory_gpu));); (122, CUDA, cudaCheck(cudaMemcpy(acts.input, debug_states.input, state_sizes[0] * sizeof(float), cudaMemcpyHostToDevice));); (122, CUDA, cudaCheck(cudaMemcpy(acts.emb, debug_states.emb, state_sizes[1] * sizeof(float), cudaMemcpyHostToDevice));); (43, CUDA, cudaCheck(cudaMalloc(&grads_memory_gpu, num_parameters * sizeof(float)));); (116, CUDA, cudaCheck(cudaMalloc(&acts.input, B * C * H * W * sizeof(float)));); (36, CUDA, cudaCheck(cudaMalloc(&params_memory_gpu, num_parameters * sizeof(float)));); (115, CUDA, cudaCheck(cudaMalloc(&acts.emb, B * C_emb * sizeof(float)));); (241, CUDA, cudaCheck(cudaFree(grads_memory_gpu));)"}
{"predict": "(5,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel)", "label": "(6, OpenMP, omp_unset_nest_lock(&nest_lock);); (2, OpenMP, omp_init_nest_lock(&nest_lock);); (3, OpenMP, omp_test_nest_lock(&nest_lock);); (5, OpenMP, omp_set_nest_lock(&nest_lock);); (9, OpenMP, omp_destroy_nest_lock(&nest_lock);)"}
{"predict": "NBC_Error(\"Error in NBC_Copy (%lu, %i, %p, %lu, %i, %p) (%i)\", (unsigned long)buf1, copyargs.srccount,\n130:                     copyargs.srctype, (unsigned long)buf2, copyargs.tgtcount, copyargs.tgttype, res);\n131:           return res;\n132:         }\n133:         break;\n134:       case UNPACK:\n135:         NBC_DEBUG(5, \"  UNPACK   (offset %li) \", offset);\n136:         NBC_GET_BYTES(ptr,unpackargs);\n137:         NBC_DEBUG(5, \"*buf: %p, count: %i, type: %p)\\n\", unpackargs.buf, unpackargs.count, unpackargs.datatype);\n138:         /* get buffer */\n139:         if(unpackargs.tmpbuf) {\n140:           buf1=(char*)handle->tmpbuf+(long)unpackargs.buf;\n141:         } else {\n142:           buf1=unpackargs.buf;\n143:         }\n144:         res = NBC_Unpack (buf1, unpackargs.count, unpackargs.datatype, handle->comm);\n145:         if (OPAL_UNLIKELY(OMPI_SUCCESS != res)) {\n146:           NBC_Error(\"Error in NBC_Unpack (%lu, %i, %p) (%i)\", (unsigned long)buf1, unpackargs.count,\n147:                     unpackargs.datatype, res);\n148:           return res;\n149:         }\n150:         break;\n151:       default:\n152:         NBC_Error(\"Unknown operation %i\", type);\n153:         return OMPI_ERR_UNKNOWN;\n154:     }\n155:     ptr += NBC_GET_SIZE(type);\n156:   }\n157:   return OMPI_SUCCESS;\n158: }", "label": "(73, MPI, Irecv_time -= MPI_Wtime();); (55, MPI, Isend_time += MPI_Wtime();); (88, MPI, Irecv_time += MPI_Wtime();); (35, MPI, Isend_time -= MPI_Wtime();)"}
{"predict": "(17,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for)", "label": "(24, CUDA, CHECK(cudaMalloc(&d_B, M));); (24, CUDA, CHECK(cudaMemcpy(d_A, h_A, M, cudaMemcpyHostToDevice));); (46, CUDA, CHECK(cudaFree(d_B));); (20, CUDA, CHECK(cudaMalloc(&d_A, M));); (32, CUDA, CHECK(cudaMemcpy(h_B, d_B, M, cudaMemcpyDeviceToHost));); (42, CUDA, CHECK(cudaFree(d_A));)"}
{"predict": "0)/REPEAT;\n131: \n132:             /* pur 4x4 w/ stride-1 loads */\n133:             t0 = getticks();\n134:             for ( int t=0 ; t<REPEAT ; t++ )\n135:             {\n136:                 for ( int i=0 ; i<n ; i+=4 )\n137:                     for ( int j=0 ; j<n ; j+=4 )\n138:                         for ( int k=0 ; k<4 ; k++ )\n139:                             for ( int l=0 ; l<4 ; l++ )\n140:                                 B[i*n+j+l] = A[j*n+i+k];\n141:             }\n142:             t1 = getticks();\n143:             d4[n] = (t1-t0)/REPEAT;\n144: \n145:             /* mur 4x4 w/ stride-1 loads */\n146:             t0 = getticks();\n147:             for ( int t=0 ; t<REPEAT ; t++ )\n148:             {\n149:                 for ( int i=0 ; i<n ; i+=4 )\n150:                     for ( int j=0 ; j<n ; j+=4 )\n151:                         for ( int k=0 ; k<4 ; k++ )\n152:                             for ( int l=0 ; l<4 ; l++ )\n153:                                 B[i*n+j+l] = A[j*n+i+k];\n154:             }\n155:             t1 = getticks();\n156:             d5[n] = (t1-t0)/REPEAT;\n157: \n158:             /* murv 4x4 w/ stride-1 loads */\n159:             t0 = getticks();\n160:             for ( int t=0 ; t<REPEAT ; t++ )\n161:             {\n162:                 for ( int i=0 ; i<n ; i+=4 )\n163:                     for ( int j=0 ; j<n ; j+=4 )\n164:                         for ( int k=0 ; k<4 ; k++ )\n165:                             for ( int l", "label": "(8, MPI, MPI_Comm_rank( MPI_COMM_WORLD , &rank );); (364, MPI, MPI_Finalize();); (34, OpenMP, fprintf( stderr , \"starting test: OpenMP ON with %d threads... \\n\", omp_get_max_threads() );); (164, OpenMP, #pragma omp parallel if(n>OMP_MIN_SIZE)); (168, OpenMP, #pragma omp for private(i,j,n4)); (63, OpenMP, #pragma omp parallel for if(n>OMP_MIN_SIZE)); (1, MPI, MPI_Init_thread( &argc , &argv , MPI_THREAD_FUNNELED , &provided );); (28, OpenMP, printf( \"# starting test: OpenMP ON with %d threads... \\n\", omp_get_max_threads() );)"}
{"predict": "(33,OpenMP,#pragma omp parallel for), (50,OpenMP,#pragma omp parallel for), (76,OpenMP,#pragma omp parallel for)", "label": "(5, CUDA, extern __shared__ char shared_memory[];)"}
{"predict": "(1,MPI,MPI_Init(&argc, &argv);) (122,MPI,MPI_Finalize();)", "label": "(23, MPI, err = MPI_Type_size(eviltype, &val);); (11, MPI, err = MPI_Type_struct(3, blocks, disps, types, &eviltype);); (66, MPI, err = MPI_Type_get_extent(eviltype, &lb, &extent);); (86, MPI, err = MPI_Type_ub(eviltype, &aval);); (121, MPI, MPI_Type_free(&eviltype);); (99, MPI, err = MPI_Type_get_true_extent(eviltype, &true_lb, &aval);); (32, MPI, err = MPI_Type_extent(eviltype, &aval);); (46, MPI, err = MPI_Type_lb(eviltype, &aval);)"}
{"predict": "(16,MPI,MPI_Comm_dup(MPI_COMM_WORLD, &CommDeuce);) (31,OpenMP,#pragma omp parallel for) (52,OpenMP,#pragma omp parallel for) (61,OpenMP,#pragma omp parallel for) (66,OpenMP,#pragma omp parallel for)", "label": "(67, MPI, MPI_Win_unlock(rank, win);); (28, MPI, i = MPI_Alloc_mem(SIZE * sizeof(int), MPI_INFO_NULL, &B);); (45, MPI, MPI_Win_start(group, 0, win);); (92, MPI, MPI_Win_free(&win);); (13, MPI, MPI_Comm_split(MPI_COMM_WORLD, (rank < 2), rank, &CommDeuce);); (66, MPI, MPI_Win_post(group, 0, win);); (44, MPI, MPI_Send(B, SIZE, MPI_INT, 1, 100, MPI_COMM_WORLD);); (34, MPI, MPI_Win_allocate(0, 1, MPI_INFO_NULL, CommDeuce, &base_ptr, &win);); (47, MPI, MPI_Win_allocate(SIZE * sizeof(int), sizeof(int), MPI_INFO_NULL, CommDeuce, &B, &win);); (63, MPI, MPI_Recv(A, SIZE, MPI_INT, 0, 100, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (63, MPI, MPI_Win_wait(win);); (41, MPI, MPI_Free_mem(B);); (37, MPI, MPI_Win_complete(win);); (35, MPI, MPI_Put(A, SIZE, MPI_INT, 1, 0, SIZE, MPI_INT, win);); (0, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (5, MPI, i = MPI_Alloc_mem(SIZE * sizeof(int), MPI_INFO_NULL, &A);); (82, MPI, MPI_Comm_free(&CommDeuce);); (77, MPI, MPI_Group_free(&comm_group);); (-9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (7, MPI, MPI_Comm_group(CommDeuce, &comm_group);); (-10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (75, MPI, MPI_Free_mem(A);); (71, MPI, MPI_Group_free(&group);); (23, MPI, MPI_Group_incl(comm_group, 1, &destrank, &group);); (38, MPI, MPI_Win_create(B, SIZE * sizeof(int), sizeof(int), MPI_INFO_NULL, CommDeuce, &win);)"}
{"predict": "(35,OpenMP,#pragma omp parallel for), (67,OpenMP,#pragma omp parallel for)", "label": "(52, CUDA, CubDebugExit(cudaMemcpy(d_flags, h_flags, sizeof(unsigned char) * num_items, cudaMemcpyHostToDevice));); (50, CUDA, CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));)"}
{"predict": "(1,MPI,MPI_Comm_rank(comm, &myrank);)", "label": "(58, MPI, MPI_Start(&req);); (13, MPI, MPI_Comm_rank(comm, &myrank);); (76, MPI, MPI_Type_free(&stype[i][j]);); (50, MPI, MPI_Info_create(&info);); (55, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (72, MPI, MPI_Type_free(&xtype[i][j]);); (56, MPI, MPI_Request_free(&req);); (6, MPI, MPI_Comm_size(comm, &p);); (55, MPI, MPI_Info_free(&info);)"}
{"predict": "float* buffer_input = (float*)malloc(bufferSize);\n76: \t\t\tif (!buffer_input) return VKFFT_ERROR_MALLOC_FAILED;\n77: \n78: \t\t\t//Copy input data from host to device.\n79: #if(VKFFT_BACKEND==5)\n80: \t\t\thipMemcpy(buffer_input, input, bufferSize, hipMemcpyHostToDevice);\n81: #elif(VKFFT_BACKEND==3)\n82: \t\t\tclEnqueueWriteBuffer(vkGPU->queue, input, CL_TRUE, 0, bufferSize, buffer_input, 0, NULL, NULL);\n83: #elif(VKFFT_BACKEND==4)\n84: \t\t\tzeEnqueueCommandListCopy(vkGPU->commandQueue, buffer_input, input, bufferSize, NULL, 0, NULL, NULL);\n85: #elif(VKFFT_BACKEND==0)\n86: \t\t\tvkCmdCopyBuffer(vkGPU->queue, input, buffer_input, bufferSize, 0, 0);\n87: #endif\n88: \n89: \t\t\t//Allocate buffer for the output data.\n90: \t\t\tfloat* buffer_output = (float*)malloc(bufferSize);\n91: \t\t\tif (!buffer_output) return VKFFT_ERROR_MALLOC_FAILED;\n92: \n93: \t\t\t//Copy output data from device to host.\n94: #if(VKFFT_BACKEND==5)\n95: \t\t\thipMemcpy(buffer_output, output, bufferSize, hipMemcpyDeviceToHost);\n96: #elif(VKFFT_BACKEND==3)\n97: \t\t\tclEnqueueReadBuffer(vkGPU->queue, output, CL_TRUE, 0, bufferSize, buffer_output, 0, NULL, NULL);\n98: #elif(VKFFT_BACKEND==4)\n99: \t\t\tzeEnqueueCommandListCopy(vkGPU->commandQueue, buffer_output, output, bufferSize, NULL, 0, NULL, NULL);\n100: #elif(VKFFT_BACKEND==0)\n101: \t\t\tvkCmdCopyBuffer(vkGPU->queue, output, buffer_output, bufferSize, 0, 0);\n102: #endif\n103: \n104: \t\t\t//Free input and output buffers.\n105: \t\t\tfree(buffer_input);\n106: \t\t\tfree(buffer_output);\n107: \n108: \t\t\t//Set up FFT plan.\n1", "label": "(161, OpenCL, clGetDeviceInfo(vkGPU->device, CL_DEVICE_VENDOR_ID, sizeof(cl_int), &vendorID, 0);); (81, CUDA, res = cudaMalloc((void**)&buffer, bufferSize);); (90, OpenCL, buffer = clCreateBuffer(vkGPU->context, CL_MEM_READ_WRITE, bufferSize, 0, &res);)"}
{"predict": "(17,MPI,MPI_Comm_spawn((char *) \"./spawn1\", MPI_ARGV_NULL, np, MPI_INFO_NULL, 0, MPI_COMM_WORLD, &intercomm, errcodes);)\n(33,OpenMP,#pragma omp parallel for)\n(42,OpenMP,#pragma omp parallel for)\n(49,OpenMP,#pragma omp parallel for)", "label": "(26, MPI, MPI_Comm_size(intercomm, &size);); (84, MPI, MPI_Comm_disconnect(&intercomm);); (91, MPI, MPI_Finalize();); (11, MPI, MPI_Comm_get_parent(&parentcomm);); (66, MPI, MPI_Recv(&i, 1, MPI_INT, 0, 0, intercomm, &status);); (32, MPI, MPI_Send(&i, 1, MPI_INT, i, 0, intercomm);); (52, MPI, MPI_Comm_get_name(intercomm, cname, &rlen);); (69, MPI, MPI_Ssend(&errs, 1, MPI_INT, 0, 1, intercomm);); (35, MPI, MPI_Recv(&err, 1, MPI_INT, i, 1, intercomm, MPI_STATUS_IGNORE);); (18, MPI, MPI_Comm_rank(intercomm, &rank);); (15, MPI, MPI_Comm_remote_size(intercomm, &rsize);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for), (42,OpenMP,#pragma omp parallel for), (49,OpenMP,#pragma omp parallel for), (58,OpenMP,#pragma omp parallel for)", "label": "(31, OpenMP, #pragma omp teams distribute parallel for collapse (1)); (35, OpenMP, #pragma omp teams distribute parallel for collapse ((ST > 0) ? 1 + ST : 2)); (1, OpenMP, #pragma omp teams distribute parallel for collapse); (10, OpenMP, #pragma omp teams distribute parallel for collapse ()); (44, OpenMP, #pragma omp distribute collapse (S)); (21, OpenMP, #pragma omp teams distribute parallel for collapse (ST); (62, OpenMP, #pragma omp teams distribute parallel for collapse (N)); (13, OpenMP, #pragma omp teams distribute parallel for collapse (argc); (65, OpenMP, #pragma omp teams distribute parallel for collapse (2)); (49, OpenMP, #pragma omp teams distribute parallel for collapse (argv[1]=2)); (-8, OpenMP, #pragma omp target); (-3, OpenMP, #pragma omp teams distribute parallel for collapse (); (32, OpenMP, #pragma omp teams distribute parallel for collapse (foobool(argc)), collapse (true), collapse (-5))"}
{"predict": "45:                 ssize_t bytes_read = mfu_read(filename, fd, ptr, remaining);\n146:                 if (bytes_read < 0) {\n147:                     MFU_LOG(MFU_LOG_ERR, \"Failed to read archive: '%s' (errno=%d %s)\",\n148:                         filename, errno, strerror(errno));\n149:                     rc = MFU_FAILURE;\n150:                 } else if (bytes_read == 0) {\n151:                     MFU_LOG(MFU_LOG_ERR, \"Unexpected end of file while reading archive: '%s'\",\n152:                         filename);\n153:                     rc = MFU_FAILURE;\n154:                 } else {\n155:                     total_read += (size_t)bytes_read;\n156:                 }\n157:             }\n158:         }\n159:     \n160:         /* if read was successful, process data and record offsets */\n161:         if (rc == MFU_SUCCESS) {\n162:             /* process data in our buffer */\n163:             uint64_t count = process_data(buf, offset_start, offset_last, &out_count, &out_offsets);\n164:             if (count > 0) {\n165:                 /* record offsets for entries we found */\n166:                 for (uint64_t i = 0; i < count; i++) {\n167:                     uint64_t offset = out_offsets[i];\n168:                     uint64_t offset_adjusted = offset - offset_start + overlap_before;\n169:                     all_offsets = mfu_realloc(all_offsets, (total_count + count) * sizeof(uint64_t));\n170:                     all_offsets[total_count + i] = offset_adjusted;\n171:                 }\n172:                 total_count += count;\n173:             }\n174:         }\n175:     \n176:         /* free buffer */\n177:         if (buf != NULL) {\n178:             MFU_FREE(&buf);\n179:         }\n180:     \n181:         /* update our starting position */\n182:         starting_pos += segment_size;\n183:     }\n184:     \n185:     /* stop timer for", "label": "(45, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (47, MPI, double start = MPI_Wtime();); (237, MPI, double now = MPI_Wtime();); (41, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(14,MPI,int par_rank; int par_size; MPI_Comm comm = MPI_COMM_WORLD; MPI_Comm_rank(comm, &par_rank); MPI_Comm_size(comm, &par_size);), (20,OpenMP,#pragma omp parallel for), (33,MPI,conduit::blueprint::mesh::verify(data, verify_info);), (35,MPI,runtime::expressions::ExpressionEval eval(&data);), (47,MPI,expr = \"sum(topo('mesh').cell.volume)\";), (52,MPI,EXPECT_ANY_THROW(res = eval.evaluate(expr));)", "label": "(33, MPI, flow::Workspace::set_default_mpi_comm(MPI_Comm_c2f(comm));); (16, MPI, MPI_Comm_rank(comm, &par_rank);); (16, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (...)\n\n### **Explanation**:\n- **MPI**: The code starts with a test case that could benefit from measuring execution time across multiple processes. Inserting `MPI_Wtime()` at line 5 allows for the measurement of the time taken for the entire test case.\n- **OpenMP**: The code contains a loop or iteration that could be parallelized to improve performance. Since the specific loop is not provided, the most likely place to insert OpenMP is at line 8, assuming there is a loop that processes each buffer in parallel.", "label": "(13, OpenCL, retVal = clReleaseMemObject(smallBuffer);); (17, OpenCL, EXPECT_EQ(clReleaseContext(context), CL_SUCCESS);); (1, OpenCL, cl_mem smallBuffer = clCreateBufferWithProperties(clContext, &memProperties, flags, size, hostPtr, &retVal);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(8, OpenMP, int tid = omp_get_thread_num();); (4, OpenMP, #pragma omp parallel num_threads(8)); (2, OpenMP, omp_set_affinity_format(formats[i]);)"}
{"predict": "(11,MPI,*w = MPI_Wtime();); (13,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for), (41,OpenMP,#pragma omp parallel for)", "label": "(16, CUDA, __shared__ uninitialized_array<thrust::complex<T>, 1024> cache;); (29, CUDA, cache[id] += static_cast<thrust::complex<T>>(A_[(a * nchol + i) * nocc + b] * A_[(b * nchol + i) * nocc + a]);); (21, CUDA, thrust::complex<T2> const* A_(Tab + blockIdx.x * nocc * nocc * nchol);); (18, CUDA, cache[id]              = thrust::complex<T>(0.0);); (18, CUDA, thrust::complex<T> alp = static_cast<thrust::complex<T>>(alpha);)"}
{"predict": "_array, d_A.data(),\n114:                                sizeof(data_type *) * gemm_count,\n115:                                cudaMemcpyHostToDevice, stream));\n116:     CUDA_CHECK(cudaMemcpyAsync(d_B_array, d_B.data(),\n117:                                sizeof(data_type *) * gemm_count,\n118:                                cudaMemcpyHostToDevice, stream));\n119:     CUDA_CHECK(cudaMemcpyAsync(d_C_array, d_C.data(),\n120:                                sizeof(data_type *) * gemm_count,\n121:                                cudaMemcpyHostToDevice, stream));\n122: \n123:     /* step 3: run gemm */\n124:     for (int i = 0; i < gemm_count; i++) {\n125:         CUBLAS_CHECK(cublasGemmEx(cublasH, transa_array[i], transb_array[i],\n126:                                   m_array[i], n_array[i], k_array[i],\n127:                                   &alpha_array[i], d_A_array[i], lda_array[i],\n128:                                   d_B_array[i], ldb_array[i], &beta_array[i],\n129:                                   d_C_array[i], ldc_array[i], compute_type));\n130:     }\n131: \n132:     /* step 4: copy data back to host */\n133:     for (int i = 0; i < gemm_count; i++) {\n134:         CUDA_CHECK(cudaMemcpyAsync(C_array[i].data(), d_C[i],\n135:                                    sizeof(data_type) * C_array[i].size(),\n136:                                    cudaMemcpyDeviceToHost, stream));\n137:     }\n138: \n139:     /* step 5: clean up */\n140:     for (int i = 0; i < gemm_count; i++) {\n141:         CUDA_CHECK(cudaFree(d_A[i]));\n142:         CUDA_CHECK(cudaFree(d_B[i]));\n143:         CUDA_CHECK(cudaFree(d_C[i]));\n144:     }\n145:     CUDA_CHECK(cudaFree(d_A_array));\n146:     CUDA_CHECK(cudaFree(d_B_array));\n147:     CUDA_CHECK(cudaFree(d_C_array));\n148: \n149:     CUBLAS_CHECK(cublasDestroy(cublasH));\n15", "label": "(169, CUDA, CUDA_CHECK(cudaFree(d_B_array));); (169, CUDA, CUDA_CHECK(cudaFree(d_C_array));); (177, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (165, CUDA, CUDA_CHECK(cudaFree(d_A_array));); (168, CUDA, CUDA_CHECK(cudaFree(d_A[i]));); (169, CUDA, CUDA_CHECK(cudaFree(d_C[i]));); (133, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (174, CUDA, CUDA_CHECK(cudaDeviceReset());); (165, CUDA, CUDA_CHECK(cudaFree(d_B[i]));); (75, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(18,OpenMP,#pragma omp parallel for reduction(+:c))", "label": "(48, CUDA, HANDLE_ERROR( cudaFree( dev_a ) );); (49, CUDA, HANDLE_ERROR( cudaFree( dev_partial_c ) );); (47, CUDA, HANDLE_ERROR( cudaFree( dev_b ) );)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(12, OpenMP, omp_set_nest_lock(&lck);); (18, OpenMP, omp_unset_nest_lock(&lck);); (8, OpenMP, #pragma omp for); (10, OpenMP, #pragma omp flush); (4, OpenMP, #pragma omp parallel shared(lck)); (2, OpenMP, omp_init_nest_lock(&lck);); (16, OpenMP, omp_destroy_nest_lock(&lck);)"}
{"predict": "(17,MPI,MPI_Get_address(inbuf, &key_true_lb);), (18,MPI,MPI_Get_address(inbuf + count * keysat_true_extent, &key_true_extent);), (35,MPI,MPI_Get_address(inbuf, &keysat_true_lb);), (36,MPI,MPI_Get_address(inbuf + count * keysat_true_extent, &keysat_true_extent);), (56,OpenMP,#pragma omp parallel for), (66,OpenMP,#pragma omp parallel for), (72,OpenMP,#pragma omp parallel for), (79,OpenMP,#pragma omp parallel for), (86,OpenMP,#pragma omp parallel for), (93,OpenMP,#pragma omp parallel for), (100,OpenMP,#pragma omp parallel for), (107,OpenMP,#pragma omp parallel for)", "label": "(19, MPI, MPI_Type_get_true_extent(inkeysat, &keysat_true_lb, &keysat_true_extent);); (76, MPI, MPI_Type_dup(inkey,    &values->key);); (13, MPI, MPI_Type_get_true_extent(inkey, &key_true_lb, &key_true_extent);); (75, MPI, MPI_Type_dup(inkeysat, &values->keysat);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (9,OpenMP,#pragma omp parallel for), (...)", "label": "(6, TBB, tbb::task_group g;); (6, TBB, tbb::task_handle h1 = make_task(g, x, n-1);); (6, TBB, tbb::task_handle h2 = make_task(g, y, n-2);)"}
{"predict": "(16,OpenMP,#pragma omp parallel for)", "label": "(17, CUDA, CUDA_FUNC(cudaStreamDestroy(resources->exec_streams[i]));)"}
{"predict": "MPI_SUCCESS) && (amode & MPI_MODE_SEQUENTIAL) &&\n125:         (ADIO_Feature((*fh), ADIO_LOCKS)) {\n126:         char *shared_fname = ADIOI_Dup_Filename(filename);\n127:         ADIO_Open(comm, dupcomm, shared_fname, file_system, fsops, amode, 0,\n128:                   MPI_BYTE, MPI_BYTE, info, ADIO_PERM_NULL, &error_code);\n129:         ADIOI_Free(shared_fname);\n130:     }\n131: \n132:     fn_exit:\n133:     if (error_code != MPI_SUCCESS) {\n134:         fn_fail:\n135:         if (fh) {\n136:             MPIR_File_close_impl(*fh, error_code);\n137:         }\n138:     }\n139:     return error_code;\n140: }\n141:", "label": "(31, MPI, error_code = MPI_Comm_test_inter(comm, &flag);); (56, MPI, MPI_Comm_dup(comm, &dupcomm);); (126, MPI, MPI_Comm_rank(dupcomm, &rank);); (136, MPI, MPI_Barrier(dupcomm);); (67, MPI, MPI_Allreduce(&amode, &tmp_amode, 1, MPI_INT, ADIO_same_amode, dupcomm);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(5, OpenMP, #pragma omp parallel for)"}
{"predict": ":         }\n78:     }\n79: }", "label": "(224, MPI, MPI_Bcast( &bocotype, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (190, MPI, MPI_Bcast( &nbocos, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (202, MPI, MPI_Bcast( &location, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (38, MPI, MPI_Bcast( &nzones, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (84, MPI, MPI_Bcast( zonename, 33, MPI_CHAR, Parallel::serverid, MPI_COMM_WORLD );); (295, MPI, MPI_Bcast( range.data(), npnts * index_dim, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (219, MPI, MPI_Bcast( &ptset_type, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (251, MPI, MPI_Bcast( pnts.data(), npnts * index_dim, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (81, MPI, MPI_Bcast( isize.data(), index_dim * 3, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (113, MPI, MPI_Bcast( &zoneType, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (220, MPI, MPI_Bcast( normalIndex.data(), index_dim, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (216, MPI, MPI_Bcast( &npnts, 1, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (289, MPI, MPI_Bcast( donor_range.data(), npnts * index_dim, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (164, MPI, MPI_Bcast( pcoor, nNodes, MPI_DOUBLE, Parallel::serverid, MPI_COMM_WORLD );); (145, MPI, MPI_Bcast( &dataType, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (283, MPI, MPI_Bcast( connectname, 33, MPI_CHAR, Parallel::serverid, MPI_COMM_WORLD );); (243, MPI, MPI_Bcast( normalList.data(), nNodes * iphysdim * sizeof( double ), MPI_CHAR, Parallel::serverid, MPI_COMM_WORLD );); (-3, MPI, MPI_Bcast( &nbases, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (205, MPI, MPI_Bcast( boconame, 33, MPI_CHAR, Parallel::serverid, MPI_COMM_WORLD );); (139, MPI, MPI_Bcast( coordname, 33, MPI_CHAR, Parallel::serverid, MPI_COMM_WORLD );); (208, MPI, MPI_Bcast( &normalListSize, 1, MPI_LONG_LONG, Parallel::serverid, MPI_COMM_WORLD );); (256, MPI, MPI_Bcast( &n1to1, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (6, MPI, MPI_Bcast( &icelldim, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (6, MPI, MPI_Bcast( &iphysdim, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (205, MPI, MPI_Bcast( &ndataset, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (274, MPI, MPI_Bcast( donorname, 33, MPI_CHAR, Parallel::serverid, MPI_COMM_WORLD );); (104, MPI, MPI_Bcast( &ncoords, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );); (199, MPI, MPI_Bcast( &normalDataType, 1, MPI_INT, Parallel::serverid, MPI_COMM_WORLD );)"}
{"predict": "(14,OpenMP,#pragma omp parallel), (15,OpenMP,#pragma omp for), (22,OpenMP,#pragma omp for), (28,OpenMP,#pragma omp for), (32,OpenMP,#pragma omp for), (36,OpenMP,#pragma omp for), (38,OpenMP,#pragma omp for), (40,OpenMP,#pragma omp for), (42,OpenMP,#pragma omp for), (44,OpenMP,#pragma omp for), (46,OpenMP,#pragma omp for), (72,OpenMP,#pragma omp for), (77,OpenMP,#pragma omp for), (87,OpenMP,#pragma omp for), (90,OpenMP,#pragma omp for), (91,OpenMP,#pragma omp for)", "label": "(58, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (58, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for reduction(+:y))", "label": "(3, CUDA, using alloc = thrust::cuda::allocator<double>;); (11, CUDA, cudaDeviceSynchronize(); benchmark::DoNotOptimize(y.base()); benchmark::ClobberMemory();)"}
{"predict": "(27,OpenMP,#pragma omp parallel for)", "label": "(32, OpenMP, #pragma omp target); (35, OpenMP, int device_id = omp_get_default_device();); (54, OpenMP, #pragma omp target data use_device_ptr(a))"}
{"predict": "(12,MPI,*w = MPI_Wtime();); (15,OpenMP,#pragma omp parallel for), (17,MPI,*e = MPI_Wtime();), (20,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for)", "label": "(28, CUDA, cudaCheckErrors(\"kernel execution failure or cudaMemcpy H2D failure\");); (14, CUDA, cudaMemcpy(d_A, h_A, DSIZE*sizeof(float), cudaMemcpyHostToDevice);); (16, CUDA, cudaCheckErrors(\"cudaMemcpy H2D failure\");); (23, CUDA, cudaMemcpy(h_C, d_C, DSIZE*sizeof(float), cudaMemcpyDeviceToHost);); (13, CUDA, cudaMemcpy(d_B, h_B, DSIZE*sizeof(float), cudaMemcpyHostToDevice);); (7, CUDA, cudaMalloc(&d_C, DSIZE*sizeof(float));); (5, CUDA, cudaMalloc(&d_B, DSIZE*sizeof(float));); (6, CUDA, cudaCheckErrors(\"cudaMalloc failure\");); (15, CUDA, cudaCheckErrors(\"kernel launch failure\");); (1, CUDA, cudaMalloc(&d_A, DSIZE*sizeof(float));)"}
{"predict": "(17,OpenMP,#pragma omp parallel for private(j))", "label": "(17, OpenMP, #pragma omp target teams distribute parallel for collapse(2)); (23, OpenMP, #pragma omp target exit data map(delete:A[0:N1],B[0:N1],C[0:N2])); (14, OpenMP, #pragma omp target enter data map(to: A[0:N1],B[0:N1],C[0:N2])); (20, OpenMP, #pragma omp target update from(C[0:N2]))"}
{"predict": "(17,OpenMP,#pragma omp parallel for), (19,OpenMP,pdf_vals[b] = hist_bins[b] / sum;)", "label": "(16, OpenMP, #pragma omp parallel for)"}
{"predict": "(44,MPI,*w = MPI_Wtime();); (50,MPI,*w = MPI_Wtime(););", "label": "(54, OpenCL, clReleaseEvent(event);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(17, MPI, *ierr = MPI_Cart_create( (MPI_Comm)(*v1), *v2, v3, l4, l5, (MPI_Comm *)(v6) );)"}
{"predict": "OF A.\n126:        ------------------------------------------------------------*/\n127:     for (i = 0; i < m_loc; ++i) {\n128:         for (j = Astore->rowptr[i]; j < Astore->rowptr[i+1]; ++j) {\n129:             irow = perm_c[perm_r[i+fst_row]];  /* Row number in Pc*Pr*A */\n130:             jcol = Astore->colind[j];\n131:             gbi = BlockNum( irow );\n132:             gbj = BlockNum( jcol );\n133:             p = PNUM( PROW(gbi,grid), PCOL(gbj,grid), grid );\n134:             if ( p != iam ) {\n135:                 /* Store in the arrays to send */\n136:                 ia_send[p][2*ptr_to_send[p]] = irow;\n137:                 ia_send[p][2*ptr_to_send[p]+1] = jcol;\n138:                 aij_send[p][ptr_to_send[p]] = Astore->nzval[j];\n139:                 ++ptr_to_send[p];\n140:             }\n141:         }\n142:     }\n143: \n144:     /* ------------------------------------------------------------\n145:        SEND THE NONZERO ENTRIES TO EACH PROCESS.\n146:        ------------------------------------------------------------*/\n147:     for (p = 0; p < procs; ++p) {\n148:         if ( p != iam ) {\n149:             MPI_Isend( ia_send[p], 2*nnzToSend[p], mpi_int_t, p, 0,\n150:                      grid->comm, &send_req[2*p]);\n151:             MPI_Isend( aij_send[p], nnzToSend[p], MPI_FLOAT, p, 1,\n152:                      grid->comm, &send_req[2*p+1]);\n153:         }\n154:     }\n155: \n156:     /* ------------------------------------------------------------\n157:        RECEIVE THE NONZERO ENTRIES FROM EACH PROCESS.\n158:        ------------------------------------------------------------*/\n159:     for (p = 0; p < procs; ++p) {\n160:         if (", "label": "(191, MPI, MPI_Wait( &send_req[p], &status);); (191, MPI, MPI_Wait( &send_req[procs+p], &status);); (170, MPI, MPI_Recv( itemp, it, mpi_int_t, p, p, grid->comm, &status );)"}
{"predict": "(15,OpenMP,#pragma omp parallel for)", "label": "(16, CUDA, return CUDA(cudaGetLastError());)"}
{"predict": "(5,MPI,*w = MPI_Wtime(););", "label": "(50, CUDA, CHECK_CUDA(cudaMalloc(&dy, dataSize * sizeof(int32_t)));); (75, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (32, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (76, CUDA, CHECK_CUDA(cudaFree(dy));); (45, CUDA, CHECK_CUDA(cudaMalloc(&dx, dataSize * sizeof(int32_t)));); (32, CUDA, CHECK_HIEDNN(hiednnSetCudaStream(handle, 0));); (72, CUDA, CHECK_CUDA(cudaFree(dx));)"}
{"predict": "(17,MPI,MPI_Init (&argc, &argv);)", "label": "(23, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for)", "label": "(28, OpenMP, #pragma omp parallel for reduction(&&: Red)); (7, OpenMP, #pragma omp parallel for reduction(-: Red)); (14, OpenMP, #pragma omp parallel for reduction(&: Red)); (17, OpenMP, #pragma omp parallel for reduction(|: Red)); (0, OpenMP, #pragma omp parallel for reduction(+: Red)); (19, OpenMP, #pragma omp parallel for reduction(^: Red)); (6, OpenMP, #pragma omp parallel for reduction(*: Red)); (25, OpenMP, #pragma omp parallel for reduction(||: Red))"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for)", "label": "(25, OpenMP, #pragma omp simd aligned(e, g)); (18, OpenMP, #pragma omp simd aligned (argc : 5)); (29, OpenMP, #pragma omp simd aligned(i)); (5, OpenMP, #pragma omp simd aligned (); (17, OpenMP, #pragma omp simd aligned (S1)); (28, OpenMP, #pragma omp parallel); (40, OpenMP, #pragma omp simd aligned(f:j)); (10, OpenMP, #pragma omp simd aligned (*argc > 0 ? argv[1] : argv[2])); (6, OpenMP, #pragma omp simd aligned (argc,); (14, OpenMP, #pragma omp simd aligned (argv[1])); (-4, OpenMP, #pragma omp simd aligned); (-1, OpenMP, #pragma omp simd aligned ()); (25, OpenMP, #pragma omp simd aligned(v:16)); (28, OpenMP, #pragma omp simd aligned(f)); (14, OpenMP, #pragma omp simd aligned(h)); (-3, OpenMP, #pragma omp simd aligned (argc)"}
{"predict": "(1, MPI, MPI_Init(&argc, &argv);) (8, OpenMP, #pragma omp parallel for) (15, MPI, MPI_Finalize();)", "label": "(13, MPI, MPI_Type_size(type, &size);); (14, MPI, mpi_errno = MPI_Type_ub(type, NULL);); (18, MPI, MPI_Type_free(&type);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (11, MPI, MPI_Error_class(mpi_errno, &errclass);); (4, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(11,MPI,comm = MPI_COMM_WORLD;); (15,MPI,sendbuf = (int *) malloc(size * sizeof(int));); (18,MPI,recvcounts = (int *) malloc(size * sizeof(int));); (22,MPI,for (i = 0; i < 10; ++i) {), (24,MPI,sumval = size * rank + ((size - 1) * size) / 2;); (26,MPI,if (recvbuf != sumval) {), (34,MPI,free(sendbuf);); (35,MPI,free(recvcounts););", "label": "(26, MPI, MPI_Start(&req);); (11, MPI, MPI_Comm_size(comm, &size);); (13, MPI, MPI_Info_create(&info);); (24, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (36, MPI, MPI_Info_free(&info);); (34, MPI, MPI_Request_free(&req);); (18, MPI, MPI_Reduce_scatter_init(sendbuf, &recvbuf, recvcounts, MPI_INT, MPI_SUM, comm, info, &req);); (6, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(25,OpenMP,#pragma omp parallel for)", "label": "(4, MPI, MPI_Init(&argc, &argv);); (30, MPI, MPI_Finalize();); (3, MPI, int ierr = MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(29,OpenMP,#pragma omp parallel for)", "label": "(7, CUDA, __shared__ block_radix_sort_t::TempStorage temp_storage;)"}
{"predict": ")\n116:     {\n117:         const float sum = __half2float(stats_local.x);\n118:         const float mean = sum / (float(HEADS) * float(HEAD_SIZE));\n119:         const float var = __half2float(stats_local.y) / (float(HEADS) * float(HEAD_SIZE));\n120:         const float rsqrt_var = rsqrt(var);\n121:         const float eps = 1e-6f;\n122:         const float var_eps = var + eps;\n123:         const float inv_var_eps = 1.0f / var_eps;\n124:         const float inv_rsqrt_var = 1.0f / rsqrt_var;\n125:         const float scale = qScale * rsqrt_var;\n126:         const float bias = qSkipScale * mean;\n127:         const half scale_half = __float2half(scale);\n128:         const half bias_half = __float2half(bias);\n129: \n130:         for (int32_t ii = 0; ii < LDGS * 4; ii++)\n131:         {\n132:             const float tmp = hdata[ii][0] * scale + bias;\n133:             hdata[ii][0] = tmp;\n134:             const float tmp1 = hdata[ii][1] * scale + bias;\n135:             hdata[ii][1] = tmp1;\n136:             const float tmp2 = hdata[ii][2] * scale + bias;\n137:             hdata[ii][2] = tmp2;\n138:             const float tmp3 = hdata[ii][3] * scale + bias;\n139:             hdata[ii][3] = tmp3;\n140:         }\n141:     }\n142: \n143:     // write back to global memory\n144:     const int32_t out_offset = bidx * VECS_PER_CTA + pos;\n145:     const bool my_pred_out = out_offset < total;\n146:     const int32_t gmem_offset_out = out_offset * row_stride_bytes + (bidx * THREADS_PER_ROW + col) * BYTES", "label": "(24, CUDA, extern __shared__ char smem_[];); (26, CUDA, __shared__ half2 smem_red[VECS_PER_CTA][WARPS];)"}
{"predict": "(18,OpenMP,#pragma omp parallel for reduction(|:isPow2)), (33,OpenMP,#pragma omp parallel for), (68,OpenMP,#pragma omp parallel for), (71,OpenMP,#pragma omp parallel for), (110,OpenMP,#pragma omp parallel for reduction(max:max_kernel_wg_size)), (123,OpenMP,#pragma omp parallel for)", "label": "(76, OpenCL, err = clBuildProgram(plan->program, 1, &devices[i], coptions, NULL, NULL);); (59, OpenCL, plan->program = clCreateProgramWithSource(context, 1, (const char**) &source_str, NULL, &err);); (68, OpenCL, err = clGetDeviceInfo(devices[i], CL_DEVICE_TYPE, sizeof(device_type), &device_type, NULL);); (88, OpenCL, err = clGetDeviceInfo(devices[i], CL_DEVICE_NAME, sizeof(devicename), devicename, NULL);)"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(24, MPI, MPI_Recv(&ack, 1, MPI_CHAR, right, tag, comm, MPI_STATUS_IGNORE);); (26, MPI, MPI_Irecv(out, count, type, set_src ? left : MPI_ANY_SOURCE, tag, comm, &requests[w]);); (28, MPI, MPI_Isend(in, count, type, left, (tag == MPI_ANY_TAG ? 0 : tag), comm, &requests[w + window_size]);); (20, MPI, MPI_Waitall(2 * window_size, requests, MPI_STATUSES_IGNORE);); (17, MPI, MPI_Isend(in, count, type, right, (tag == MPI_ANY_TAG ? 0 : tag), comm, &requests[w + window_size]);); (13, MPI, MPI_Irecv(out, count, type, set_src ? right : MPI_ANY_SOURCE, tag, comm, &requests[w]);); (27, MPI, MPI_Send(&ack, 1, MPI_CHAR, left, (tag == MPI_ANY_TAG ? 0 : tag), comm);)"}
{"predict": "(3,OpenMP,#pragma omp parallel for), (8,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for)", "label": "(13, OpenMP, #pragma omp parallel master taskloop simd collapse (ST); (5, OpenMP, #pragma omp parallel master taskloop simd collapse ()); (15, OpenMP, #pragma omp parallel master taskloop simd collapse ((ST > 0) ? 1 + ST : 2)); (1, OpenMP, #pragma omp parallel master taskloop simd collapse (); (20, OpenMP, #pragma omp parallel master taskloop simd collapse (S)); (10, OpenMP, #pragma omp parallel master taskloop simd collapse (1))); (27, OpenMP, #pragma omp parallel master taskloop simd collapse (1)); (30, OpenMP, #pragma omp parallel master taskloop simd collapse (2)); (14, OpenMP, #pragma omp parallel master taskloop simd collapse (foobool(argc)), collapse (true), collapse (-5)); (-7, OpenMP, #pragma omp parallel master taskloop simd collapse); (0, OpenMP, #pragma omp parallel master taskloop simd collapse (argc); (24, OpenMP, #pragma omp parallel master taskloop simd collapse (N))"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (10,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for)", "label": "(5, TBB, tbb::task_group_context tgc;); (15, TBB, tgc.set_priority(tbb::priority_high);); (4, TBB, tgc.set_priority(tbb::priority_normal);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(8, OpenMP, #pragma omp parallel for)"}
{"predict": "()", "label": "(13, OpenMP, #pragma omp parallel); (13, OpenMP, #pragma omp task if (task: false))"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(42, OpenMP, #pragma omp distribute parallel for schedule(static, ch)); (49, OpenMP, #pragma omp distribute parallel for schedule(dynamic)); (7, OpenMP, #pragma omp distribute parallel for); (15, OpenMP, #pragma omp distribute parallel for dist_schedule(static)); (30, OpenMP, #pragma omp distribute parallel for schedule(static)); (53, OpenMP, #pragma omp distribute parallel for schedule(dynamic, ch)); (5, OpenMP, #pragma omp cancel for); (0, OpenMP, #pragma omp target); (18, OpenMP, #pragma omp distribute parallel for dist_schedule(static, ch)); (-1, OpenMP, #pragma omp teams)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (41,OpenMP,#pragma omp parallel for), (46,OpenMP,#pragma omp parallel for), (51,OpenMP,#pragma omp parallel for), (56,OpenMP,#pragma omp parallel for)", "label": "(35, OpenMP, #pragma omp distribute private(e, g)); (51, OpenMP, #pragma omp parallel private(i)); (5, OpenMP, #pragma omp distribute private); (12, OpenMP, #pragma omp distribute private (argc,); (9, OpenMP, #pragma omp distribute private (argc); (36, OpenMP, #pragma omp target); (19, OpenMP, #pragma omp distribute private (argv[1])); (12, OpenMP, #pragma omp distribute private (argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (9, OpenMP, #pragma omp distribute private (argc > 0 ? argv[1] : argv[2])); (14, OpenMP, #pragma omp distribute private (a, b, c, d, f)); (17, OpenMP, #pragma omp distribute private(ba)); (39, OpenMP, #pragma omp distribute private(j)); (-1, OpenMP, #pragma omp distribute private ()); (24, OpenMP, #pragma omp distribute private(h)); (44, OpenMP, #pragma omp teams reduction(+:i)); (18, OpenMP, #pragma omp distribute private(S2::S2s)); (43, OpenMP, #pragma omp distribute private(i)); (14, OpenMP, #pragma omp distribute private(da)); (-9, OpenMP, #pragma omp distribute private (); (2, OpenMP, #pragma omp distribute private (S1)); (29, OpenMP, #pragma omp teams private(i)); (8, OpenMP, #pragma omp distribute private(ca)); (17, OpenMP, #pragma omp distribute); (22, OpenMP, #pragma omp distribute firstprivate(i)); (30, OpenMP, #pragma omp teams firstprivate(i)); (17, OpenMP, #pragma omp teams)"}
{"predict": "(1, CUDA, extern \"C\" { __global__ void get_write_address(", "label": "(22, CUDA, __shared__ volatile ll_t smem[_TPB_];)"}
{"predict": "// Use exact solution\n144:   int maxiters = 1000;\n145:   int failure = 0;\n146:   double t_begin = mytimer();\n147:   for (int i=0; i< numberOfCalls; ++i) {\n148:     failure = CG_ref(A, x, b, niters, normr, normr0, tolerance, maxiters, data);\n149:     if (failure) {\n150:       HPCG_fout << \"Error in call to CG: \" << failure << \".\\n\" << endl;\n151:       global_failure = failure;\n152:     }\n153:     totalNiters_ref += niters;\n154:   }\n155:   times[7] = (mytimer() - t_begin)/((double) numberOfCalls);  // Total time divided by number of calls.\n156:   ref_times[0] = times[7];\n157:   #ifdef HPCG_DEBUG\n158:   if (rank==0) HPCG_fout << \"Total CG timing phase execution time in main (sec) = \" << mytimer() - t1 << endl;\n159:   #endif\n160: \n161:   // Free the memory\n162:   Free(x_overlap);\n163:   Free(b_computed);\n164:   Free(x);\n165:   Free(b);\n166:   Free(xexact);\n167:   Free(geom);\n168:   Free(A);\n169:   Free(data);\n170:   return 0;\n171: }", "label": "(3, MPI, MPI_Init(&argc, &argv);); (303, MPI, MPI_Finalize();); (23, MPI, MPI_Barrier(MPI_COMM_WORLD);); (222, MPI, MPI_Allreduce(&local_opt_worst_time, &opt_worst_time, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for reduction(+:res.x,res.y,res.z))", "label": "(7, CUDA, fcuda::Check_CudaErroorFun(\"DgReduSumFloat3\");); (5, CUDA, cudaMemcpy(data,datag+inidata,sizeof(float3)*ndata,cudaMemcpyDeviceToHost);)"}
{"predict": ", 8i,\n82:                            7,\n83:                            KMP_ARCH_X86) // __kmpc_atomic_fixed8_div_cpt_rev_fp\n84: ATOMIC_CMPXCHG_CPT_REV_MIX(fixed8u, kmp_uint64, div_cpt_rev, 64, /, fp, _Quad,\n85:                            8i, 7,\n86:                            KMP_ARCH_X86) // __kmpc_atomic_fixed8u_div_cpt_rev_fp\n87: \n88: ATOMIC_CMPXCHG_CPT_REV_MIX(fixed16, kmp_int128, sub_cpt_rev, 128, -, fp, _Quad,\n89:                            16i, 15,\n90:                            KMP_ARCH_X86) // __kmpc_atomic_fixed16_sub_cpt_rev_fp\n91: ATOMIC_CMPXCHG_CPT_REV_MIX(fixed16u, kmp_uint128, sub_cpt_rev, 128, -, fp, _Quad,\n92:                            16i, 15,\n93:                            KMP_ARCH_X86) // __kmpc_atomic_fixed16u_sub_cpt_rev_fp\n94: ATOMIC_CMPXCHG_CPT_REV_MIX(fixed16, kmp_int128, div_cpt_rev, 128, /, fp, _Quad,\n95:                            16i, 15,\n96:                            KMP_ARCH_X86) // __kmpc_atomic_fixed16_div_cpt_rev_fp\n97: ATOMIC_CMPXCHG_CPT_REV_MIX(fixed16u, kmp_uint128, div_cpt_rev, 128, /, fp, _Quad,\n98:                            16i, 15,\n99:                            KMP_ARCH_X86) // __kmpc_atomic_fixed16u_div_cpt_rev_fp\n100: \n101: ATOMIC_CMPXCHG_CPT_REV_MIX(fixed32, kmp_int32, sub_cpt_rev, 32, -, fp, _Quad, 4i,\n102:                             3, 0) // __kmpc_atomic_fixed3", "label": "(127, OpenMP, #define GOMP_CRITICAL_SWP(FLAG))"}
{"predict": "(24,OpenMP,#pragma omp parallel for)", "label": "(27, CUDA, cudaMemcpy(dst, src, bytes, kind);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for)", "label": "(9, TBB, tbb::atomic<int> b;); (6, TBB, std::list< std::pair< const int, tbb::atomic<int> > > arrIntTbb;)"}
{"predict": ". */\n164:     superlu_dist_set_default_options (&options);\n165:     options.ColPerm = COLMMD_REFACTORING;\n166:     options.Trans = NOTRANS;\n167:     options.IterRefine = NOREFINE;\n168:     options.PivotGrowth = NOPIVOTGROWTH;\n169:     options.DistMode = BLOCK_CYCLIC;\n170:     options.ColPerm = MY_PERMC;\n171:     options.Trans = NOTRANS;\n172:     options.IterRefine = NOREFINE;\n173:     options.PivotGrowth = NOPIVOTGROWTH;\n174:     options.PrintStat = YES;\n175:     options.ColPerm = COLMMD_REFACTORING;\n176:     options.Trans = NOTRANS;\n177:     options.IterRefine = NOREFINE;\n178:     options.PivotGrowth = NOPIVOTGROWTH;\n179:     options.PrintStat = YES;\n180:     options.ColPerm = MY_PERMC;\n181:     options.Trans = NOTRANS;\n182:     options.IterRefine = NOREFINE;\n183:     options.PivotGrowth = NOPIVOTGROWTH;\n184:     options.PrintStat = YES;\n185:     options.ColPerm = COLMMD_REFACTORING;\n186:     options.Trans = NOTRANS;\n187:     options.IterRefine = NOREFINE;\n188:     options.PivotGrowth = NOPIVOTGROWTH;\n189:     options.PrintStat = YES;\n190:     options.ColPerm = MY_PERMC;\n191:     options.Trans = NOTRANS;\n192:     options.IterRefine = NOREFINE;\n193:     options.PivotGrowth = NOPIVOTGROWTH;\n194:     options.PrintStat = YES;\n195:     options.ColPerm = COLMMD_REFACTORING;\n196:     options.Trans = NOTRANS;\n197:     options.IterRefine = NOREFINE;\n198:     options.PivotGrowth = NOPIVOTGROWTH;\n199:", "label": "(97, MPI, MPI_Query_thread(&omp_mpi_level);); (38, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (33, MPI, MPI_Init_thread(&argc, &argv, required, &provided);)"}
{"predict": "3:     broken_iov_arrays = (struct iovec **) malloc ( fh->f_num_aggrs * sizeof(struct iovec *));\n134:     broken_decoded_iovs = (struct iovec **) malloc ( fh->f_num_aggrs * sizeof(struct iovec *));\n135:     broken_counts = (int *) malloc ( fh->f_num_aggrs * sizeof(int));\n136:     broken_iov_counts = (int *) malloc ( fh->f_num_aggrs * sizeof(int));\n137:     broken_total_lengths = (MPI_Aint *) malloc ( fh->f_num_aggrs * sizeof(MPI_Aint));\n138:     \n139:     for ( i=0; i< fh->f_num_aggrs; i++ ) {\n140:         broken_iov_arrays[i] = (struct iovec *) malloc ( fh->f_num_aggrs * sizeof(struct iovec));\n141:         broken_decoded_iovs[i] = (struct iovec *) malloc ( fh->f_num_aggrs * sizeof(struct iovec));\n142:     }\n143:     \n144:     ret = mca_fcoll_vulcan_break_iov (local_iov_array, local_count, fh->f_num_aggrs,\n145:                                        broken_iov_arrays, broken_counts, broken_decoded_iovs,\n146:                                        broken_iov_counts, broken_total_lengths);\n147:     if (ret != OMPI_SUCCESS){\n148:         goto exit;\n149:     }\n150:     \n151:     /*************************************************************************\n152:      ** 3.  Generate the displacements for the write_all operation\n153:      *************************************************************************/\n154:     displs = (ptrdiff_t *) malloc ( fh->f_num_aggrs * sizeof(ptrdiff_t));\n155:     \n156:     ret = fh->f_generate_displacements(fh->f_num_aggrs, displs, fh->f_file_size, fh->f_num_io_procs);\n157:     if (ret != OMPI_SUCCESS){\n158:         goto exit;\n159:     }\n160:     \n161:     // Prepare the requests for the asynchronous write\n162:     reqs = (ompi_request", "label": "(391, MPI, start_exch = MPI_Wtime();); (143, MPI, start_comm_time = MPI_Wtime();); (156, MPI, end_comm_time = MPI_Wtime();); (435, MPI, start_write_time = MPI_Wtime();); (442, MPI, end_write_time = MPI_Wtime();); (495, MPI, end_exch = MPI_Wtime();)"}
{"predict": "(11,MPI,*w = MPI_Wtime();); (30,MPI,cudaStreamSynchronize(stream);)", "label": "(33, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (35, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(17,OpenMP,#pragma omp parallel for collapse(2))", "label": "(43, MPI, MPI_Pack(a, 1, xpose, buffer, bufsize, &position, MPI_COMM_WORLD);); (59, MPI, MPI_Type_free(&row);); (31, MPI, MPI_Type_commit(&xpose);); (56, MPI, MPI_Type_free(&xpose);); (28, MPI, MPI_Type_hvector(100, 1, sizeofint, row, &xpose);); (23, MPI, MPI_Type_extent(MPI_INT, &sizeofint);); (30, MPI, MPI_Pack_size(1, xpose, MPI_COMM_WORLD, &bufsize);); (34, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (39, MPI, MPI_Unpack(buffer, bufsize, &position, b, 100 * 100, MPI_INT, MPI_COMM_WORLD);); (22, MPI, MPI_Type_vector(100, 1, 100, MPI_INT, &row);)"}
{"predict": "(22,OpenMP,#pragma omp parallel for)", "label": "(10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (42, MPI, MPI_Waitall(nprocs, sendreqs, sendstats);); (13, MPI, MPI_Barrier(MPI_COMM_WORLD);); (21, MPI, MPI_Isend(sendbuf, BUF_COUNT, MPI_INT, i, 0, MPI_COMM_WORLD, &sendreqs[i]);)"}
{"predict": "120: \n121:     if (rankId == 0)\n122:     {\n123:         h_A = (double*)malloc(M * lda * sizeof(double));\n124:         h_QR = (double*)malloc(M * lda * sizeof(double));\n125:         h_tau = (double*)malloc(N * sizeof(double));\n126: \n127:         /* Initialize matrix A */\n128:         initializeMatrix(h_A, M, N, lda);\n129:     }\n130: \n131:     /* Allocate distributed matrix descriptors */\n132:     descrA = cusolverMpCreateMatrixDescriptor();\n133:     assert(descrA != NULL);\n134: \n135:     /* Set matrix descriptors */\n136:     cusolverMpSetMatrixDescriptor(descrA, M, N, lda, lda, CUDA_R_64F, CUDA_R_64F, MA, NA, IA, JA);\n137: \n138:     /* Allocate device workspace */\n139:     cudaStat = cudaMalloc((void**)&d_A, M * lda * sizeof(double));\n140:     cudaStat = cudaMalloc((void**)&d_tau, N * sizeof(double));\n141:     cudaStat = cudaMalloc((void**)&d_work_geqrf, workspaceInBytesOnDevice_geqrf);\n142:     cudaStat = cudaMalloc((void**)&d_info_geqrf, sizeof(int));\n143:     assert(cudaStat == cudaSuccess);\n144: \n145:     /* Allocate host workspace */\n146:     h_work_geqrf = (void*)malloc(workspaceInBytesOnHost_geqrf);\n147:     assert(h_work_geqrf != NULL);\n148: \n149:     /* Copy data to device */\n150:     if (rankId == 0)\n151:     {\n152:         cudaMemcpy(d_A, h_A, M * lda * sizeof(double), cudaMemcpyHostToDevice);\n153:     }\n154: \n155:     /* =========================================== */\n156:     /*          Compute QR decomposition         */\n157:     /* =========================================== */\n158: \n159:     /* Compute QR decomposition */\n160:", "label": "(358, CUDA, cudaStat = cudaMemcpy(h_A, d_global_Q, sizeof(double) * lda * colsA, cudaMemcpyDeviceToHost);); (196, CUDA, cudaStat = cudaMalloc((void**)&d_info_geqrf, sizeof(int));); (27, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rankId);); (429, CUDA, cudaStat = cudaFree(d_tau);); (-1, MPI, MPI_Init(NULL, NULL);); (274, CUDA, cudaStat = cudaMemcpyAsync(&h_info_geqrf, d_info_geqrf, sizeof(int), cudaMemcpyDeviceToHost, localStream);); (471, CUDA, cudaStat = cudaStreamDestroy(localStream);); (474, MPI, MPI_Barrier(MPI_COMM_WORLD);); (42, CUDA, cudaStat = cudaSetDevice(localDeviceId);); (43, CUDA, cudaStat = cudaFree(0);); (194, CUDA, cudaStat = cudaMemset(d_info_geqrf, 0, sizeof(int));); (428, CUDA, cudaStat = cudaFree(d_work_geqrf);); (178, CUDA, cudaStat = cudaMalloc((void**)&d_tau, localColsA * sizeof(double));); (301, CUDA, cudaStat = cudaMalloc((void**)&d_global_Q, lda * colsA * sizeof(double));); (213, CUDA, cudaStat = cudaMalloc((void**)&d_work_geqrf, workspaceInBytesOnDevice_geqrf);); (431, CUDA, cudaStat = cudaFree(d_info_geqrf);); (12, MPI, MPI_Comm_size(MPI_COMM_WORLD, &rankSize);); (154, CUDA, cudaStat = cudaMalloc((void**)&d_A, localColsA * LLDA * sizeof(double));); (305, CUDA, cudaStat = cudaMemcpy(d_global_R, d_global_Q, sizeof(double) * lda * colsA, cudaMemcpyDeviceToDevice);); (302, CUDA, cudaStat = cudaMemcpy(d_global_Q, h_A, sizeof(double) * lda * colsA, cudaMemcpyHostToDevice);); (53, CUDA, cudaStat = cudaStreamCreate(&localStream);); (463, MPI, MPI_Finalize();); (403, CUDA, cudaStat = cudaFree(d_A);); (293, CUDA, cudaStat = cudaMalloc((void**)&d_global_R, lda * colsA * sizeof(double));); (294, CUDA, cudaStat = cudaMalloc((void**)&d_global_tau, colsA * sizeof(double));)"}
{"predict": "(13,OpenMP,#pragma omp parallel for reduction(+:threadVal)), (27,OpenMP,#pragma omp parallel for reduction(+:threadVal)), (49,OpenMP,#pragma omp parallel for reduction(+:threadVal))", "label": "(4, CUDA, __shared__ cg::block_tile_memory<BlockSize> scratch;)"}
{"predict": "(46,MPI,*w = MPI_Wtime();); (48,OpenMP,#pragma omp parallel for), (50,MPI,MPI_Barrier(MPI_COMM_WORLD);)", "label": "(59, CUDA, cudaFree( deviceInt );); (59, CUDA, cudaFreeHost( hostInt );)"}
{"predict": "(18,MPI,chTimerTimestamp start, end;)\n(21,OpenMP,#pragma omp parallel for reduction(+:ret))\n(27,MPI,cuda(SetDevice( i ));)\n(33,MPI,cuda(Malloc( &dptrPosMass[i], 4*N*sizeof(float) ));)\n(36,MPI,cuda(Malloc( &dptrForce[i], 3*N*sizeof(float) ));)\n(38,MPI,cuda(MemcpyAsync( dptrPosMass[i], g_hostAOS_PosMass, 4*N*sizeof(float), cudaMemcpyHostToDevice ));)\n(45,MPI,cuda(SetDevice( i ));)\n(48,MPI,ComputeNBodyGravitation_multiGPU_onethread<<<300,256,256*sizeof(float4)>>>( dptrForce[i], dptrPosMass[i], softeningSquared, 0, N, N );)\n(52,MPI,cuda(MemcpyAsync( g_hostAOS_gpuCrossCheckForce[i], dptrForce[i], 3*N*sizeof(float), cudaMemcpyDeviceToHost ));)\n(55,MPI,cuda(MemcpyAsync( g_hostAOS_Force+3*bodiesPerGPU*i, dptrForce[i]+3*bodiesPerGPU*i, 3*bodiesPerGPU*sizeof(float), cudaMemcpyDeviceToHost ));)\n(64,MPI,ComputeNBodyGravitation_multiGPU_onethread<<<300,256,256*sizeof(float4)>>>( dptrForce[i], dptrPosMass[i], softeningSquared, i*bodiesPerGPU, bodiesPerGPU, N );)\n(67,MPI,cuda(MemcpyAsync( g_hostAOS_Force+3*bodiesPerGPU*i, dptrForce[i], 3*bodiesPerGPU*sizeof(float), cudaMemcpyDeviceToHost ));)\n(72,MPI,cuda(DeviceSynchronize());)\n(80,MPI,cuda(SetDevice( i ));)\n(85,MPI,chTimerGetTime( &end );)\n(87,MPI,ret = chTimerElapsedTime( &start, &end ) * 1000.0f;)", "label": "(98, CUDA, cudaFree( dptrPosMass[i] );); (100, CUDA, cudaSetDevice( oldDevice );); (97, CUDA, cudaFree( dptrForce[i] );)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (...)", "label": "(5, OpenCL, clReleaseMemObject(objX);); (12, OpenCL, clReleaseMemObject(objScratch);); (7, OpenCL, clReleaseMemObject(objiAmax);)"}
{"predict": "(20,OpenMP,#pragma omp parallel for)", "label": "(19, TBB, REMARK(\"Wraparound on tbb::concurrent_bounded_queue...\");); (2, TBB, tbb::concurrent_queue<T>* q = new tbb::concurrent_queue<T>;); (16, TBB, tbb::concurrent_bounded_queue<T>* q = new tbb::concurrent_bounded_queue<T>;)"}
{"predict": "(14,OpenMP,#pragma omp parallel)", "label": "(25, OpenMP, printf(\"Thread %d sleeps now for %d seconds\\n\", omp_get_thread_num(), SLEEPTIME);); (8, OpenMP, #pragma omp parallel); (31, OpenMP, printf(\"Thread %d wakes up again\\n\", omp_get_thread_num());)"}
{"predict": "(12,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for)", "label": "(45, SYCL, GEP->setMetadata(hipsycl::compiler::MDKind::Arrayified, MDAlloca);); (36, SYCL, Alloca->setMetadata(hipsycl::compiler::MDKind::Arrayified, MDAlloca);)"}
{"predict": "expected-note {{defined as firstprivate}}\n105:   for (i = 0; i < argc; ++i) // expected-error {{loop iteration variable in the associated loop of 'omp parallel master taskloop simd' directive may not be firstprivate, predetermined as linear}}\n106:     foo();\n107:   return 0;\n108: }", "label": "(22, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argc); (78, OpenMP, #pragma omp parallel master taskloop simd safelen(5)); (81, OpenMP, #pragma omp parallel master taskloop simd firstprivate(e, g)); (125, OpenMP, #pragma omp parallel private(i)); (87, OpenMP, #pragma omp parallel master taskloop simd firstprivate(h)); (53, OpenMP, #pragma omp parallel master taskloop simd firstprivate(ca)); (48, OpenMP, #pragma omp parallel master taskloop simd firstprivate(ba)); (27, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (38, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argv[1])); (90, OpenMP, #pragma omp parallel master taskloop simd firstprivate(i)); (-1, OpenMP, #pragma omp parallel); (64, OpenMP, #pragma omp parallel master taskloop simd firstprivate(S2::S2sc)); (128, OpenMP, #pragma omp parallel master taskloop simd firstprivate(B::x)); (119, OpenMP, #pragma omp parallel reduction(+ : i)); (88, OpenMP, #pragma omp parallel shared(xa)); (23, OpenMP, #pragma omp parallel master taskloop simd firstprivate(S1)); (14, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argc > 0 ? argv[1] : argv[2])); (78, OpenMP, #pragma omp parallel master taskloop simd private(xa), firstprivate(xa)); (53, OpenMP, #pragma omp parallel master taskloop simd firstprivate(S2::S2s)); (125, OpenMP, #pragma omp parallel master taskloop simd firstprivate(si)); (-6, OpenMP, #pragma omp parallel master taskloop simd firstprivate(); (-3, OpenMP, #pragma omp parallel master taskloop simd firstprivate()); (65, OpenMP, #pragma omp parallel master taskloop simd firstprivate(m)); (84, OpenMP, #pragma omp parallel master taskloop simd firstprivate(j)); (18, OpenMP, #pragma omp parallel master taskloop simd firstprivate(a, b, c, d, f)); (86, OpenMP, #pragma omp parallel master taskloop simd lastprivate(g) firstprivate(g)); (89, OpenMP, #pragma omp parallel master taskloop simd lastprivate(n) firstprivate(n)); (-1, OpenMP, #pragma omp parallel master taskloop simd firstprivate(argc,); (39, OpenMP, #pragma omp parallel master taskloop simd firstprivate(xa)); (21, OpenMP, #pragma omp parallel master taskloop simd firstprivate(2 * 2)); (32, OpenMP, #pragma omp parallel master taskloop simd firstprivate(da)); (-21, OpenMP, #pragma omp parallel master taskloop simd firstprivate)"}
{"predict": "(8,OpenMP,#pragma omp parallel for reduction(+:sum)), (17,OpenMP,sum = sum + i2;)", "label": "(14, OpenMP, #pragma omp flush); (9, OpenMP, #pragma omp parallel for reduction(+:sum) schedule(static,1) private(i) private(i2))"}
{"predict": "(17,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for)", "label": "(21, MPI, MPI_Recv(&ack, 1, MPI_CHAR, right, tag, comm, MPI_STATUS_IGNORE);); (23, MPI, MPI_Irecv(out, count, type, set_src ? left : MPI_ANY_SOURCE, tag, comm, &requests[w]);); (18, MPI, MPI_Waitall(window_size, requests, MPI_STATUSES_IGNORE);); (15, MPI, MPI_Isend(in, count, type, right, (tag == MPI_ANY_TAG ? 0 : tag), comm, &requests[w]);); (23, MPI, MPI_Send(&ack, 1, MPI_CHAR, left, (tag == MPI_ANY_TAG ? 0 : tag), comm);)"}
{"predict": "(27,OpenMP,#pragma omp parallel for), (62,OpenMP,#pragma omp parallel for), (77,OpenMP,#pragma omp parallel for)", "label": "(34, OpenMP, comp_do_parse(cur, tail);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (15,MPI,*w = MPI_Wtime();); (18,OpenMP,#pragma omp parallel for)", "label": "(3, OpenMP, #pragma omp target device(0)); (13, OpenMP, #pragma omp target device(1))"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(4, OpenMP, #pragma omp master); (5, OpenMP, #pragma omp taskgroup); (6, OpenMP, #pragma omp task shared(var, a)); (0, OpenMP, #pragma omp parallel num_threads(2) shared(var, a))"}
{"predict": "(3,MPI,MPI_Comm_rank(comm, &rank); MPI_Comm_size(comm, &size);) (7,OpenMP,#pragma omp parallel for) (15,OpenMP,#pragma omp parallel for)", "label": "(3, MPI, MPI_Comm_size(comm, &size);); (3, MPI, MPI_Comm_rank(comm, &rank);); (13, MPI, MPI_Alltoallv(data_out, counts, displs, MPI_INT, data_in, counts, displs, MPI_INT, comm);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();)\n(10,MPI,if (error != MPI_SUCCESS) { MPI_Error_string(error, err_string, &length); printf(\"MPI_Finalized failed: %s\\n\", err_string); fflush(stdout); return error; })\n(20,MPI,if (error != MPI_SUCCESS) { MPI_Error_string(error, err_string, &length); printf(\"MPI_Init failed: %s\\n\", err_string); fflush(stdout); return error; })\n(27,MPI,if (error != MPI_SUCCESS) { MPI_Error_string(error, err_string, &length); printf(\"MPI_Comm_rank failed: %s\\n\", err_string); fflush(stdout); return error; })\n(34,MPI,flag = 0; error = MPI_Finalized(&flag); if (error != MPI_SUCCESS) { MPI_Error_string(error, err_string, &length); printf(\"MPI_Finalized failed: %s\\n\", err_string); fflush(stdout); return error; })\n(36,MPI,if (error != MPI_SUCCESS) { MPI_Error_string(error, err_string, &length); printf(\"MPI_Finalized failed: %s\\n\", err_string); fflush(stdout); MPI_Abort(MPI_COMM_WORLD, error); return error; })\n(40,MPI,if (error != MPI_SUCCESS) { MPI_Error_string(error, err_string, &length); printf(\"MPI_Barrier failed: %s\\n\", err_string); fflush(stdout); MPI_Abort(MPI_COMM_WORLD, error); return error; })\n(43,MPI,if (error != MPI_SUCCESS) { MPI_Error_string(error, err_string, &length); printf(\"MPI_Finalize failed: %s\\n\", err_string); fflush(stdout); return error; })\n(46,MPI,if (error != MPI_SUCCESS) { MPI_Error_string(error, err_string, &length); printf(\"MPI_Finalize failed: %s\\n\", err_string); fflush(stdout); return error; })\n(51,MPI,if (error != MPI_SUCCESS) { MPI_Error_string(error, err_string, &length); printf(\"MPI_Error_string failed: %s\\n\", err_string); fflush(stdout); return error; })\n(56,MPI,if (error != MPI_SUCCESS) { MPI_Error_string(error, err_string, &length); printf(\"MPI_Finalize failed: %s\\n\", err_string); fflush(stdout); return error; })\n(58,MPI,if (", "label": "(11, MPI, MPI_Error_string(error, err_string, &length);); (20, MPI, error = MPI_Init(&argc, &argv);); (27, MPI, error = MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (60, MPI, error = MPI_Finalize();); (50, MPI, error = MPI_Barrier(MPI_COMM_WORLD);); (29, MPI, MPI_Abort(MPI_COMM_WORLD, error);); (3, MPI, error = MPI_Finalized(&flag);)"}
{"predict": "(18,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for)", "label": "(13, CUDA, cudaMalloc(&dev_B, size_b);); (13, CUDA, cudaDeviceSynchronize();); (10, CUDA, cudaMalloc(&dev_A, size_a);); (27, CUDA, cudaFree(dev_B);); (25, CUDA, cudaFree(dev_A);)"}
{"predict": "(3,MPI,s = MPI_Init(NULL, NULL););\n(5,MPI,*w = MPI_Wtime(););\n(8,OpenMP,#pragma omp parallel for),", "label": "(35, OpenMP, #pragma omp target teams map(marr[:arg][n:])); (166, OpenMP, #pragma omp target teams map(r.S.Arr[4:5])); (53, OpenMP, #pragma omp target teams map(marr[:arg][:2][:])); (120, OpenMP, #pragma omp target teams map(r.PtrS[0], r.PtrS->B)); (117, OpenMP, #pragma omp target teams map(r.ArrS[0].A, t.ArrS[1].A)); (136, OpenMP, #pragma omp target teams map(r.C, r.D)); (9, OpenMP, #pragma omp target teams map(r)); (80, OpenMP, #pragma omp target teams map(mvla2[:1][:2][:10])); (123, OpenMP, #pragma omp target teams map(r.RPtrS, r.RPtrS->B)); (136, OpenMP, #pragma omp target teams map(r.C) map(r.C)); (21, OpenMP, #pragma omp target teams map(marr[arg:][:arg][:])); (189, OpenMP, #pragma omp target data map(to: t)); (157, OpenMP, #pragma omp target teams map(r.S.Ptr[4:5])); (54, OpenMP, #pragma omp target teams map(marr[:2][2+2-4:][0:5+5])); (35, OpenMP, #pragma omp target teams map(marr[:2][:][1:])); (12, OpenMP, #pragma omp target teams map(marr[arg:][:][:])); (93, OpenMP, #pragma omp target teams map(r.ArrS[:arg].B)); (0, OpenMP, #pragma omp target teams map(marr[2][0:2][0:2])); (80, OpenMP, #pragma omp target teams map(mptr[:1][:2][0:2])); (53, OpenMP, #pragma omp target teams map(marr2[:1][:2][0])); (65, OpenMP, #pragma omp target teams map(mvla2[:1][:2][:11])); (86, OpenMP, #pragma omp target teams map(r.ArrS[:1].B)); (35, OpenMP, #pragma omp target teams map(marr[:1][3:1][:2])); (6, OpenMP, #pragma omp target teams map(marr[arg:])); (29, OpenMP, #pragma omp target teams map(marr[:1][0][:])); (134, OpenMP, #pragma omp target teams map(r.Ptr[3:5])); (74, OpenMP, #pragma omp target teams map(mptr[:1][:][0:2])); (108, OpenMP, #pragma omp target teams map(r.S.Arr[:12])); (125, OpenMP, #pragma omp target teams map(r.A)); (90, OpenMP, #pragma omp target teams map(r.ArrS[0].A, r.ArrS[1].A)); (95, OpenMP, #pragma omp target teams map(r.PtrS, r.PtrS->B)); (164, OpenMP, #pragma omp target teams map(t.D)); (149, OpenMP, #pragma omp target teams map(r.D)); (96, OpenMP, #pragma omp target teams map(r.RPtrS[0], r.RPtrS->B)); (77, OpenMP, #pragma omp target teams map(r.ArrS[0].Arr[1:23])); (98, OpenMP, #pragma omp target teams map(r.RPtrS->A, r.RPtrS->B)); (45, OpenMP, #pragma omp target teams map(mvla[1][2:arg][:])); (142, OpenMP, #pragma omp target data map(to: r.C)); (21, OpenMP, #pragma omp target teams map(marr[:1][3:arg][:2])); (8, OpenMP, #pragma omp target teams map(marr[:2][:][:1])); (37, OpenMP, #pragma omp target teams map(mvla[:2][:arg][:])); (61, OpenMP, #pragma omp target teams map(mptr[:2][:1][0:2])); (71, OpenMP, #pragma omp target teams map(r.ArrS[0].Arr[1:arg])); (142, OpenMP, #pragma omp target data map(to: t.Ptr)); (31, OpenMP, #pragma omp target teams map(mvla[:1][:][0])); (112, OpenMP, #pragma omp target teams map(r.Arr[3:5])); (46, OpenMP, #pragma omp target teams map(mptr[:1][:2-1][2:4-3])); (108, OpenMP"}
{"predict": "(10,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for)", "label": "(12, OpenMP, #pragma omp target map(tofrom: C) map(to: tt, tt.p1[:10]))"}
{"predict": "()", "label": "(7, CUDA, cudaFree(bufferOffset.W);); (3, CUDA, cudaFree(bufferOffset.srcShiftSum);); (1, CUDA, cudaFree(bufferOffset.dstMean);); (10, CUDA, cudaFree(cusolverData.cusolverBuffer);); (5, CUDA, cudaFree(bufferOffset.J);); (3, CUDA, cudaFree(bufferOffset.r);); (4, CUDA, cudaFree(bufferOffset.calc_buffer);); (-1, CUDA, cudaFree(bufferOffset.LtL);); (-6, CUDA, cudaFree(bufferOffset.srcMean);); (-4, CUDA, cudaFree(bufferOffset.dstShiftSum);); (4, CUDA, cudaFree(cusolverData.cusolverInfo);)"}
{"predict": "_values(cufftdx_smemio_results.output, cufft_results.output);\n116:         std::cout << \"cuFFTDx with shared memory IO\\n\";\n117:         std::cout << \"L2 error: \" << fft_error.l2_relative_error << \"\\n\";\n118:         std::cout << \"Peak error (index: \" << fft_error.peak_error_index << \"): \" << fft_error.peak_error << \"\\n\";\n119:         if(success) {\n120:             success = (fft_error.l2_relative_error < 0.001);\n121:         }\n122:     }\n123: \n124:     return;\n125: }", "label": "(77, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamCreate(&stream));); (71, CUDA, CUDA_CHECK_AND_EXIT(cudaMemset(output, 0b11111111, flat_fft_size_bytes));); (71, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(input, input_host.data(), flat_fft_size_bytes, cudaMemcpyHostToDevice));); (91, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(output));); (70, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (88, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(input));); (62, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&input, flat_fft_size_bytes));); (83, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamDestroy(stream));); (61, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&output, flat_fft_size_bytes));)"}
{"predict": ")fabs(expected - outData[i]) > ulpLimit )\n120:         {\n121:             printf(\"Error: %s( %f, %f ) = %f (expected %f), ulp = %f\\n\", fnName, src1[0], src2[0], outData[i], expected, (float)fabs(expected - outData[i]));\n122:             skipCount++;\n123:         }\n124:     }\n125:     if( skipCount > 0 )\n126:     {\n127:         printf(\"Warning: %d out of %d (%f%%) values are within ulp limit of %f\\n\", skipCount, TEST_SIZE, (float)skipCount / TEST_SIZE * 100, ulpLimit);\n128:     }\n129:     else\n129:     {\n130:         printf(\"All values are within ulp limit of %f\\n\", ulpLimit);\n131:     }\n132:     return 0;\n133: }", "label": "(108, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 1, NULL, threads, localThreads, 0, NULL, NULL );); (22, OpenCL, error = clGetDeviceInfo( device, CL_DEVICE_SINGLE_FP_CONFIG, sizeof( config ), &config, NULL );); (16, OpenCL, error = clGetDeviceInfo( device, CL_DEVICE_PROFILE, sizeof( kernelSource ), kernelSource, NULL );); (95, OpenCL, error = clSetKernelArg(kernel, (int)i, sizeof( streams[i] ), &streams[i]);); (108, OpenCL, error = clEnqueueReadBuffer( queue, streams[2], true, 0, sizeof( cl_float ) * TEST_SIZE, outData, 0, NULL, NULL );)"}
{"predict": "(12,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp task), (33,OpenMP,#pragma omp task), (36,OpenMP,#pragma omp task), (40,OpenMP,#pragma omp task), (43,OpenMP,#pragma omp task), (47,OpenMP,#pragma omp taskwait), (50,OpenMP,#pragma omp parallel for), (72,OpenMP,#pragma omp task), (75,OpenMP,#pragma omp task), (78,OpenMP,#pragma omp task), (81,OpenMP,#pragma omp taskwait), (99,OpenMP,#pragma omp target data map(to: hostSrcMatA[0:N*M], hostSrcMatB[0:M*P]) map(from: hostDstMat[0:N*P])), (100,OpenMP,#pragma omp target teams distribute parallel for), (104,OpenMP,#pragma omp target data map(to: hostSrcMatA[0:N*M], hostSrcMatB[0:M*P]) map(from: hostDstMat[0:N*P]))", "label": "(11, OpenMP, #pragma omp parallel for schedule(static,1)); (12, OpenMP, #pragma omp task)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(8, CUDA, safeCall(cudaMemcpy(h_data, data.d_data, sizeof(SiftPoint)*data.numPts, cudaMemcpyDeviceToHost));)"}
{"predict": "(15,OpenMP,#pragma omp parallel for)", "label": "(12, TBB, tbb::mutex *m = new tbb::mutex;)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(7, OpenMP, #pragma omp parallel for schedule(dynamic) default(none) copyin(g) linear(a) allocate(a) lastprivate(conditional: d, e,f) order(concurrent)); (5, OpenMP, #pragma omp threadprivate(g)); (11, OpenMP, #pragma omp parallel for allocate(argc) private(argc, b), firstprivate(c, d), lastprivate(d, f) collapse(N) schedule(static, N) ordered(N) if (parallel :argc) num_threads(N) default(shared) shared(e) reduction(+ : h))"}
{"predict": "(24,MPI,envval = getenv(envname);)", "label": "(19, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(7,MPI,*w = MPI_Wtime();); (10,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for)", "label": "(31, TBB, ASSERT(default_ss == tbb::global_control::active_value(tbb::global_control::thread_stack_size), NULL);); (3, TBB, const unsigned h_c = tbb::tbb_thread::hardware_concurrency();); (5, TBB, tbb::task_scheduler_init t(h_c+1);); (20, TBB, size_t default_ss = tbb::global_control::active_value(tbb::global_control::thread_stack_size);)"}
{"predict": "Buffer\");\n155:             return 0;\n156:         }\n157:         found_size = 1;\n158:     }\n159: \n160:     if (!found_size)\n161:     {\n162:         log_info(\"Could not allocate any memory for testing.\\n\");\n163:         return 0;\n164:     }\n165: \n166:     err = clSetKernelArg(kernel, 0, sizeof(array), &array);\n167:     test_error(err, \"clSetKernelArg failed\");\n168: \n169:     err = clEnqueueNDRangeKernel(queue, kernel, dimensions, NULL,\n170:                                 global_work_size, local_work_size, 0, NULL, NULL);\n171:     test_error(err, \"clEnqueueNDRangeKernel failed\");\n172: \n173:     err = clFinish(queue);\n174:     test_error(err, \"clFinish failed\");\n175: \n176:     clReleaseMemObject(array);\n177:     clReleaseKernel(kernel);\n178:     clReleaseProgram(program);\n179: \n180:     return 0;\n181: }", "label": "(558, OpenCL, clReleaseKernel(kernel);); (558, OpenCL, clReleaseProgram(program);); (555, OpenCL, clReleaseMemObject(array);)"}
{"predict": "(37,OpenMP,#pragma omp parallel for), (41,OpenMP,#pragma omp parallel for)", "label": "(34, CUDA, HANDLE_ERROR(cudaMalloc((void**)&state_deriv_GPU, sizeof(float) * DYN::STATE_DIM));); (37, CUDA, HANDLE_ERROR(cudaMemcpyAsync(s_d, s_gpu.data(), sizeof(float) * DYN::STATE_DIM, cudaMemcpyHostToDevice, s1));); (46, CUDA, CudaCheckError();); (34, CUDA, HANDLE_ERROR(cudaMemcpyAsync(u_d, u.data(), sizeof(float) * DYN::CONTROL_DIM, cudaMemcpyHostToDevice, s1));); (29, CUDA, HANDLE_ERROR(cudaMalloc((void**)&s_d, sizeof(float) * DYN::STATE_DIM));); (27, CUDA, HANDLE_ERROR(cudaMalloc((void**)&u_d, sizeof(float) * DYN::CONTROL_DIM));); (45, CUDA, HANDLE_ERROR(cudaMemcpyAsync(s_gpu.data(), s_d, sizeof(float) * DYN::STATE_DIM, cudaMemcpyDeviceToHost, s1));); (32, CUDA, HANDLE_ERROR(cudaStreamSynchronize(s1));); (19, CUDA, cudaStreamCreate(&s1);)"}
{"predict": "2:           batch_size, n_spheres,\n143:           n_links, n_joints, store_n_links);\n144:       }));\n145:     }\n146:   }\n147:   else\n148:   {\n149:     AT_DISPATCH_FLOATING_TYPES(\n150:       grad_spheres.scalar_type(), \"kin_fused_backward_16t\", ([&] {\n151:       kin_fused_backward_kernel3<scalar_t, double, false, true, 16, parallel_write>\n152:         << < blocksPerGrid, threadsPerBlock, sharedMemSize, stream >> > (\n153:         grad_out.data_ptr<float>(),\n154:         grad_nlinks_pos.data_ptr<float>(),\n155:         grad_nlinks_quat.data_ptr<float>(),\n156:         grad_spheres.data_ptr<scalar_t>(),\n157:         global_cumul_mat.data_ptr<float>(),\n158:         joint_vec.data_ptr<float>(),\n159:         fixed_transform.data_ptr<float>(),\n160:         robot_spheres.data_ptr<float>(),\n161:         joint_map_type.data_ptr<int8_t>(),\n162:         joint_map.data_ptr<int16_t>(), link_map.data_ptr<int16_t>(),\n163:         store_link_map.data_ptr<int16_t>(),\n164:         link_sphere_map.data_ptr<int16_t>(),\n165:         link_chain_map.data_ptr<int16_t>(),\n166:         joint_offset_map.data_ptr<float>(),\n167:         batch_size, n_spheres,\n168:         n_links, n_joints, store_n_links);\n169:     }));\n170:   }\n171:   return grad_out;\n172: }", "label": "(69, CUDA, cudaStream_t stream = at::cuda::getCurrentCUDAStream();)"}
{"predict": "(12,OpenMP,#pragma omp parallel for reduction(+:s)), (15,OpenMP,#pragma omp parallel for reduction(max:s))", "label": "(24, MPI, MPI_Allreduce( &s, &berr, 1, MPI_FLOAT, MPI_MAX, grid->comm );)"}
{"predict": "(30,MPI,*w = MPI_Wtime();); (32,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for), (53,OpenMP,#pragma omp parallel for), (79,OpenMP,#pragma omp parallel for)", "label": "(34, CUDA, checkCudaErrors(cudaDeviceSynchronize());)"}
{"predict": "(28,MPI,*w = MPI_Wtime();); (52,OpenMP,#pragma omp parallel for), (82,MPI,MPI_Finalize(););", "label": "(18, CUDA, checkRuntime(cudaStreamCreate(&stream));); (79, CUDA, checkRuntime(cudaMemcpyAsync(output_data_host, output_data_device, sizeof(float) * output_numel, cudaMemcpyDeviceToHost, stream));); (79, CUDA, checkRuntime(cudaStreamSynchronize(stream));); (57, CUDA, checkRuntime(cudaMemcpyAsync(input_data_device, input_data_host, input_numel * sizeof(float), cudaMemcpyHostToDevice, stream));); (24, CUDA, checkRuntime(cudaMallocHost(&input_data_host, input_numel * sizeof(float)));); (24, CUDA, checkRuntime(cudaMalloc(&input_data_device, input_numel * sizeof(float)));); (89, CUDA, checkRuntime(cudaFree(input_data_device));); (86, CUDA, checkRuntime(cudaFreeHost(input_data_host));); (88, CUDA, checkRuntime(cudaFree(output_data_device));); (62, CUDA, checkRuntime(cudaMalloc(&output_data_device, sizeof(float) * output_numel));); (-5, CUDA, auto engine = make_nvshared(runtime->deserializeCudaEngine(engine_data.data(), engine_data.size()));); (59, CUDA, checkRuntime(cudaMallocHost(&output_data_host, sizeof(float) * output_numel));); (82, CUDA, checkRuntime(cudaFreeHost(output_data_host));); (79, CUDA, checkRuntime(cudaStreamDestroy(stream));)"}
{"predict": "4: // expected-error@+1 {{argument of a reduction clause of a for construct must not appear in a firstprivate clause on a task construct}}\n125: #pragma omp task firstprivate(r)\n126:     // expected-error@+1 {{reduction variables may not be accessed in an explicit task}}\n127:     ++r;\n128: #pragma omp parallel for reduction(+ : r)\n129: #pragma omp task\n130:     // expected-error@+1 {{reduction variables may not be accessed in an explicit task}}\n131:     ++r;\n132:   return 0;\n133: }\n\n### ** Analysis **\n\nThe code contains various constructs and errors related to parallelization. The main issues are with the use of tasks, reduction clauses, and shared variables. The goal is to identify the lines where appropriate parallelization instructions can be inserted to improve performance without violating the existing errors.\n\n### ** Expected Output **\n\nAssistant's answer:\n\n(13,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for), (56,OpenMP,#pragma omp parallel for), (61,OpenMP,#pragma omp parallel for), (66,OpenMP,#pragma omp parallel for), (71,OpenMP,#pragma omp parallel for), (76,OpenMP,#pragma omp parallel for), (81,OpenMP,#pragma omp parallel for), (86,OpenMP,#pragma omp parallel for), (92,OpenMP,#pragma omp parallel for), (97,OpenMP,#pragma omp parallel for), (102,OpenMP,#pragma omp parallel for), (107,OpenMP,#pragma omp parallel for), (112,OpenMP,#pragma omp parallel for), (117,OpenMP,#pragma omp parallel for), (122,OpenMP,#pragma omp parallel for), (127,OpenMP,#pragma omp parallel for)", "label": "(174, OpenMP, #pragma omp task mergeable detach(evt)); (19, OpenMP, #pragma omp task unknown()); (14, OpenMP, #pragma omp task }); (154, OpenMP, #pragma omp task untied untied); (106, OpenMP, #pragma omp parallel shared(sa, sb)); (125, OpenMP, #pragma omp for reduction(+ : r)); (109, OpenMP, #pragma omp task firstprivate(r)); (160, OpenMP, #pragma omp task detach(); (-2, OpenMP, #pragma omp task); (104, OpenMP, #pragma omp parallel reduction(+ : r)); (160, OpenMP, #pragma omp task detach(evt) detach(evt)); (161, OpenMP, #pragma omp task detach(evt) mergeable); (60, OpenMP, #pragma omp parallel); (162, OpenMP, #pragma omp task detach(-evt)); (-6, OpenMP, #pragma omp task(); (48, OpenMP, #pragma omp task default(shared)); (-6, OpenMP, #pragma omp task[); (152, OpenMP, #pragma omp task detach(a)); (-6, OpenMP, #pragma omp task]); (58, OpenMP, #pragma omp parallel shared(a, b)); (157, OpenMP, #pragma omp task detach(evt) shared(evt)); (-7, OpenMP, #pragma omp task)); (144, OpenMP, #pragma omp task detach); (148, OpenMP, #pragma omp task detach(cevt) detach(revt)); (136, OpenMP, #pragma omp task mergeable mergeable); (153, OpenMP, #pragma omp task detach(evt) firstprivate(evt)); (19, OpenMP, #pragma omp task default(none)); (141, OpenMP, #pragma omp task detach()); (66, OpenMP, #pragma omp parallel shared(sa))"}
{"predict": "(10,MPI,MPI_Wait(&r, &s);)", "label": "(8, MPI, MPI_Start(&r);); (11, MPI, MPI_Send_init(buf, 10, MPI_INT, MPI_PROC_NULL, tag, MPI_COMM_WORLD, &r);); (7, MPI, MPI_Wait(&r, &s);); (4, MPI, MPI_Recv_init(buf, 10, MPI_INT, MPI_PROC_NULL, tag, MPI_COMM_WORLD, &r);); (6, MPI, MPI_Request_free(&r);)"}
{"predict": "(15,OpenMP,#pragma omp parallel for collapse(2))", "label": "(16, CUDA, CUDA_CHECK(cudaGetLastError());)"}
{"predict": "Pg = new ProcessGroupImpl(MPI_COMM_WORLD);\n141:         return ret;\n142:     }", "label": "(35, MPI, MPI_Get_library_version(mpiVersion, &resultLen);); (132, MPI, ret = MPI_Init_thread(argc, argv, MPI_THREAD_MULTIPLE, &provided);); (146, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (126, MPI, MPI_Initialized(&isMpiInited);); (0, MPI, MPI_Finalized(&isMpiFinalized);)"}
{"predict": "(1,MPI,*w = MPI_Wtime();)", "label": "(14, OpenMP, #pragma omp parallel reduction(+ : incomplete, ([10])p)); (1, OpenMP, #pragma omp parallel reduction(); (2, OpenMP, #pragma omp parallel reduction(unknown); (9, OpenMP, #pragma omp parallel reduction(inscan, + : a)); (6, OpenMP, #pragma omp parallel reduction(default, + : a)); (1, OpenMP, #pragma omp parallel reduction(default,); (2, OpenMP, #pragma omp parallel reduction(unknown, +: a))"}
{"predict": "(27,OpenMP,#pragma omp parallel for private(ij))", "label": "(13, OpenMP, int _dev_thread_id = omp_get_thread_num();); (11, OpenMP, int _dev_thread_num = omp_get_num_threads();)"}
{"predict": "(10,MPI,*w = MPI_Wtime();); (17,OpenMP,#pragma omp parallel for), (...)", "label": "(14, CUDA, cudaError_t err = cudaMalloc(&m_histogram, sizeof(int) * 256 * maxBatchSize);)"}
{"predict": "73: // CHECK-NOT: !llvm.access.group\n74:   for(int i = 0; i < 10; ++i);\n75: // CHECK: @__kmpc_dispatch_init\n76: // CHECK: !llvm.access.group\n77:   for(int i = 0; i < 10; ++i);\n78: // CHECK: @__kmpc_dispatch_init\n79: // CHECK-NOT: !llvm.access.group\n80:   for(int i = 0; i < 10; ++i);\n81: // CHECK: @__kmpc_dispatch_init\n82: // CHECK: !llvm.access.group\n83:   for(int i = 0; i < 10; ++i);\n84: // CHECK: @__kmpc_dispatch_init\n85: // CHECK-NOT: !llvm.access.group\n86:   for(int i = 0; i < 10; ++i);\n87: // CHECK: @__kmpc_dispatch_init\n88: // CHECK: !llvm.access.group\n89:   for(int i = 0; i < 10; ++i);\n90: // CHECK: @__kmpc_dispatch_init\n91: // CHECK-NOT: !llvm.access.group\n92:   for(int i = 0; i < 10; ++i);\n93: // CHECK: @__kmpc_dispatch_init\n94: // CHECK: !llvm.access.group\n95:   for(int i = 0; i < 10; ++i);\n96: // CHECK: @__kmpc_dispatch_init\n97: // CHECK-NOT: !llvm.access.group\n98:   for(int i = 0; i < 10; ++i);\n99: // CHECK: @__kmpc_dispatch_init\n100: // CHECK: !llvm.access.group\n101:   for(int i = 0; i < 10; ++i);\n102: // CHECK: @__kmpc_dispatch_init\n103: // CHECK-NOT: !llvm.access.group\n104:   for(int i = 0; i < 10; ++i);\n105: // CHECK: @__kmpc_dispatch", "label": "(137, OpenMP, #pragma omp for schedule(nonmonotonic: auto)); (3, OpenMP, #pragma omp for); (47, OpenMP, #pragma omp for schedule(guided)); (106, OpenMP, #pragma omp for schedule(monotonic: dynamic)); (41, OpenMP, #pragma omp for simd schedule(runtime)); (56, OpenMP, #pragma omp for simd schedule(dynamic)); (107, OpenMP, #pragma omp for simd schedule(monotonic: dynamic)); (46, OpenMP, #pragma omp for simd schedule(guided)); (89, OpenMP, #pragma omp for simd schedule(monotonic: runtime)); (10, OpenMP, #pragma omp for simd schedule(static)); (83, OpenMP, #pragma omp for schedule(monotonic: runtime)); (90, OpenMP, #pragma omp for schedule(monotonic: guided)); (144, OpenMP, #pragma omp for schedule(static, 2) ordered(1)); (28, OpenMP, #pragma omp for schedule(runtime)); (62, OpenMP, #pragma omp for schedule(monotonic: static, 2)); (-1, OpenMP, #pragma omp for schedule(static)); (21, OpenMP, #pragma omp for simd schedule(auto)); (144, OpenMP, #pragma omp for simd schedule(static, 2) ordered); (152, OpenMP, #pragma omp for simd schedule(auto) ordered); (66, OpenMP, #pragma omp for schedule(monotonic: auto)); (105, OpenMP, #pragma omp for schedule(nonmonotonic: dynamic)); (145, OpenMP, #pragma omp for schedule(auto) ordered(1)); (67, OpenMP, #pragma omp for simd schedule(monotonic: auto)); (110, OpenMP, #pragma omp for simd schedule(nonmonotonic: static)); (57, OpenMP, #pragma omp for simd schedule(monotonic: static, 2)); (46, OpenMP, #pragma omp for simd schedule(monotonic: static)); (-2, OpenMP, #pragma omp for schedule(static, 2)); (39, OpenMP, #pragma omp for schedule(monotonic: static)); (113, OpenMP, #pragma omp for simd schedule(nonmonotonic: runtime)); (28, OpenMP, #pragma omp for schedule(dynamic)); (87, OpenMP, #pragma omp for schedule(nonmonotonic: guided)); (149, OpenMP, #pragma omp for simd schedule(runtime) ordered); (97, OpenMP, #pragma omp for simd schedule(nonmonotonic: dynamic)); (157, OpenMP, #pragma omp for simd schedule(guided) ordered); (-25, OpenMP, #pragma omp for simd); (111, OpenMP, #pragma omp for schedule(static) ordered); (69, OpenMP, #pragma omp for simd schedule(monotonic: guided)); (-4, OpenMP, #pragma omp for schedule(auto)); (83, OpenMP, #pragma omp for simd schedule(nonmonotonic: guided)); (156, OpenMP, #pragma omp for schedule(dynamic) ordered(1)); (111, OpenMP, #pragma omp for simd schedule(static) ordered); (144, OpenMP, #pragma omp for schedule(guided) ordered(1)); (-13, OpenMP, #pragma omp for simd schedule(static, 2)); (132, OpenMP, #pragma omp for schedule(runtime) ordered)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (15,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for), (38,OpenMP,#pragma omp parallel for), (42,OpenMP,#pragma omp parallel for), (45,OpenMP,#pragma omp parallel for), (49,OpenMP,#pragma omp parallel for)", "label": "(51, OpenCL, clReleaseEvent(event);)"}
{"predict": "93: \t  tx2 * (u[i+1][j][k][4]*up1 - \n94: \t\t u[i-1][j][k][4]*um1);\n95:       }\n96:     }\n97:   }\n98: \n99:   /*--------------------------------------------------------------\n100:  c     compute yi-direction fluxes \n101:  c--------------------------------------------------------------*/\n102:   for (i = 1; i < grid_points[0]-1; i++) {\n103:     for (j = 1; j < grid_points[1]-1; j++) {\n104:       for (k = 1; k < grid_points[2]-1; k++) {\n105: \tvijk = vs[i][j][k];\n106: \tvp1  = vs[i][j+1][k];\n107: \tvm1  = vs[i][j-1][k];\n108: \n109: \trhs[i][j][k][0] = rhs[i][j][k][0] + dy1tx2 * \n110: \t  (v[i][j+1][k][0] - 2.0*v[i][j][k][0] + \n111: \t   v[i][j-1][k][0]) -\n112: \t  ty2 * (v[i][j+1][k][1] - v[i][j-1][k][1]);\n113: \n114: \trhs[i][j][k][1] = rhs[i][j][k][1] + dy2tx2 * \n115: \t  (v[i][j+1][k][1] - 2.0*v[i][j][k][1] + \n116: \t   v[i][j-1][k][1]) +\n117: \t  yycon2*con43 * (vp1 - 2.0*vijk + vm1) -\n118: \t  ty2 * (v[i][j+1][k][1]*vp1 - \n119: \t\t v[i][j-1][k][1]*vm1 +\n120: \t\t (v[i][j+1][k][4]- square[i][", "label": "(171, OpenMP, #pragma omp for private(j,k,vijk,vp1,vm1)); (397, OpenMP, #pragma omp for private(j,m)); (7, OpenMP, #pragma omp for nowait private(j,k,rho_inv)); (127, OpenMP, #pragma omp for nowait private(j,k,m)); (103, OpenMP, #pragma omp for nowait private(k,m)); (343, OpenMP, #pragma omp for nowait private(j,m)); (151, OpenMP, #pragma omp for private(k,m)); (25, OpenMP, #pragma omp for private(j,k,m)); (38, OpenMP, #pragma omp for private(j,k,uijk,up1,um1)); (282, OpenMP, #pragma omp for private(j,k,wijk,wp1,wm1)); (399, OpenMP, #pragma omp for private(k,m,i))"}
{"predict": "(3,MPI,int rank, size; MPI_Comm_rank(MPI_COMM_WORLD, &rank); MPI_Comm_size(MPI_COMM_WORLD, &size);) (15,OpenMP,#pragma omp parallel for) (20,OpenMP,#pragma omp parallel for) (25,OpenMP,#pragma omp parallel for) (30,OpenMP,#pragma omp parallel for) (35,OpenMP,#pragma omp parallel for) (40,OpenMP,#pragma omp parallel for) (45,OpenMP,#pragma omp parallel for) (50,OpenMP,#pragma omp parallel for) (55,OpenMP,#pragma omp parallel for) (60,OpenMP,#pragma omp parallel for)", "label": "(28, MPI, MPI_Type_free(&ddt);); (7, MPI, MPI_Init(&argc, &argv);); (12, MPI, MPI_Finalize();); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (21, MPI, MPI_DDT_DUMP(ddt);)"}
{"predict": "(15,MPI,MPI_Init(&argc, &argv);)\n(32,MPI,MPI_Comm_size(comm, &np); MPI_Comm_rank(comm, &rank);)\n(37,MPI,MPI_Sendrecv(outbufs[j], count, types[j], partner, tag, inbufs[j], count, types[j], partner, tag, comm, MPI_STATUS_IGNORE);)\n(47,MPI,MPI_Bcast(&partner, 1, MPI_INT, 0, comm);)\n(52,MPI,MPI_Allreduce(&err, &errloc, 1, MPI_INT, MPI_SUM, comm);)\n(72,MPI,MPI_Allreduce(&err, &errloc, 1, MPI_INT, MPI_SUM, comm);)", "label": "(57, MPI, MPI_Recv(outbufs[j], counts[j], types[j], partner, tag, comm, &status);); (45, MPI, MPI_Type_get_name(types[j], myname, &mynamelen);); (49, MPI, MPI_Send(inbufs[j], counts[j], types[j], partner, tag, comm);); (35, MPI, MPI_Comm_size(comm, &np);); (25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (32, MPI, MPI_Comm_rank(comm, &rank);); (53, MPI, MPI_Get_count(&status, types[j], &count);)"}
{"predict": "(11,MPI,MPI_Init(NULL, NULL);) (12,OpenMP,#pragma omp parallel for) (25,MPI,MPI_Init(NULL, NULL);) (26,OpenMP,#pragma omp parallel for) (28,MPI,MPI_Finalize();) (35,MPI,MPI_Init(NULL, NULL);) (37,MPI,MPI_Finalize();)", "label": "(16, MPI, MPI_Init(&argc, &argv);); (20, MPI, MPI_Finalize();)"}
{"predict": ", DMAPP_BYTE);\n128:             t1 = MPI_Wtime();\n129:             assert(status==DMAPP_RC_SUCCESS);\n130:             dt = t1-t0;\n131:             bw = 1e-6 * (double)i / dt;\n132:             fprintf(stderr,\"%d: %12d bytes %12lf seconds = %lf MB/s \\n\", mype, i, dt, bw);\n133:         }\n134:     }\n135:     fflush(stderr);\n136:     PMI_Barrier();\n137: \n138:     /* Free the remotely accessible memory. */\n139:     dmapp_sheap_free(source);\n140:     dmapp_sheap_free(target);\n141: \n142:     /* Clean up DMAPP resources after executing all other DMAPP calls. */\n143:     //status = dmapp_fini();\n144:     return 0;\n145: }", "label": "(14, MPI, MPI_Init(&argc, &argv);); (148, MPI, MPI_Finalize();); (72, MPI, t0 = MPI_Wtime();); (73, MPI, t1 = MPI_Wtime();)"}
{"predict": "(11,MPI,MPIT_result = MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided);)\n(14,MPI,MPIT_result = MPI_T_pvar_session_create(&session);)", "label": "(11, MPI, MPI_Abort(MPI_COMM_WORLD, MPIT_result);); (12, MPI, MPIT_result = MPI_T_pvar_session_create(session);); (6, MPI, MPIT_result = MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for reduction(+:t0)), (27,OpenMP,#pragma omp parallel for reduction(+:local_sum)), (36,OpenMP,#pragma omp parallel for reduction(+:acc))", "label": "(6, CUDA, __shared__ float block_acc[32];)"}
{"predict": "(11,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for)", "label": "(3, OpenMP, auto maxThreads = omp_get_max_threads();)"}
{"predict": "(5,MPI,w = MPI_Wtime();)", "label": "(6, MPI, MPI_Init(&argc, &argv);); (19, MPI, MPI_Finalize();); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (7, MPI, MPI_Comm_get_parent(&intercomm);); (11, MPI, err = MPI_Recv(str, 4, MPI_CHAR, 3, 0, intercomm, MPI_STATUS_IGNORE);); (8, MPI, err = MPI_Send(\"hi\", 3, MPI_CHAR, 3, 0, intercomm);)"}
{"predict": "(27,OpenMP,#pragma omp parallel for collapse(2))", "label": "(31, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (29, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());)"}
{"predict": "(5,MPI,*w = MPI_Wtime(););", "label": "(5, OpenMP, #pragma omp task final (argc); (3, OpenMP, #pragma omp task final ()); (8, OpenMP, #pragma omp task final (argv[1]=2)); (4, OpenMP, #pragma omp task final (argc > 0 ? argv[1] : argv[2])); (8, OpenMP, #pragma omp task final (1 0)); (1, OpenMP, #pragma omp task final (z+argc))); (2, OpenMP, #pragma omp task final (foobool(argc)), final (true)); (6, OpenMP, #pragma omp task final(if(tmain(argc, argv)); (-5, OpenMP, #pragma omp task final (); (2, OpenMP, #pragma omp task final (argc argc)); (-8, OpenMP, #pragma omp task final); (-2, OpenMP, #pragma omp task final (S1))"}
{"predict": "(16,MPI,nnzL = 0; n = 0;); (19,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for), (60,OpenMP,#pragma omp parallel for), (73,OpenMP,#pragma omp parallel for)", "label": "(49, MPI, MPI_Allreduce(MPI_IN_PLACE,&n,1,mpi_int_t,MPI_MAX,grid->comm);); (47, MPI, MPI_Allreduce(MPI_IN_PLACE,&nnzL,1,mpi_int_t,MPI_SUM,grid->comm);)"}
{"predict": "] : vrefSelects[stype][1];\n113:         sfunc(src1_host.data(), src2_host.data(), cmp_host.data(), dest_host.data(), BUFFER_SIZE);\n114: \n115:         err = clEnqueueReadBuffer(queue, dest, CL_FALSE, 0, BUFFER_SIZE,\n116:                                   dest_host.data(), 0, NULL, NULL);\n117:         test_error_count(err, \"Error: Could not read dest\");\n118: \n119:         compareResults(ref.data(), dest_host.data(), BUFFER_SIZE, cmp_host.data());\n120:     }\n121: \n122:     freeRef(ref);\n123:     freeRef(sref);\n124:     freeRef(src1_host);\n125:     freeRef(src2_host);\n126:     freeRef(cmp_host);\n127:     freeRef(dest_host);\n128:     return 0;\n129: }", "label": "(51, OpenCL, dest = clCreateBuffer( context, CL_MEM_WRITE_ONLY, BUFFER_SIZE, NULL, &err );); (44, OpenCL, src1 = clCreateBuffer( context, CL_MEM_READ_ONLY, BUFFER_SIZE, NULL, &err );); (72, OpenCL, err = clSetKernelArg(kernels[vecsize], 0, sizeof dest, &dest);); (44, OpenCL, src2 = clCreateBuffer( context, CL_MEM_READ_ONLY, BUFFER_SIZE, NULL, &err );); (76, OpenCL, err = clSetKernelArg(kernels[vecsize], 3, sizeof cmp, &cmp);); (71, OpenCL, err = clSetKernelArg(kernels[vecsize], 1, sizeof src1, &src1);); (134, OpenCL, err = clEnqueueNDRangeKernel(queue, kernels[vecsize], 1, NULL, &vector_count, NULL, 0, NULL, NULL);); (42, OpenCL, cmp = clCreateBuffer( context, CL_MEM_READ_ONLY, BUFFER_SIZE, NULL, &err );); (70, OpenCL, err = clSetKernelArg(kernels[vecsize], 2, sizeof src2, &src2);)"}
{"predict": "(9,MPI,MPI_Init(&argc, &argv);) (23,MPI,MPI_Finalize();)", "label": "(17, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (21, MPI, MPI_Reduce(&data, &result, 1, eq_type, op, 0, MPI_COMM_WORLD);); (25, MPI, MPI_Op_free(&op);); (10, MPI, MPI_Type_create_struct(2, blklens, displs, types, &eq_type);); (10, MPI, MPI_Type_commit(&eq_type);); (3, MPI, MPI_Op_create(myop, 0, &op);); (10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (21, MPI, MPI_Type_free(&eq_type);)"}
{"predict": "(5,MPI,*w = MPI_Wtime(););", "label": "(7, OpenMP, #pragma omp critical(add_first)); (3, OpenMP, #pragma omp parallel shared(var1, var2)); (7, OpenMP, #pragma omp atomic); (8, OpenMP, #pragma omp single)"}
{"predict": "(6,OpenMP,#pragma omp parallel for), (10,OpenMP,#pragma omp parallel), (15,OpenMP,#pragma omp parallel)", "label": "(9, TBB, test_ports_return_references<tbb::flow::rejecting>();); (9, TBB, lightweight_testing::test<tbb::flow::multifunction_node>(10);); (13, TBB, test_extract<tbb::flow::rejecting>();); (5, TBB, test_ports_return_references<tbb::flow::queueing>();); (12, TBB, test_extract<tbb::flow::queueing>();)"}
{"predict": "ptr < end && *ptr != '\\n') {\n138:                 ptr++;\n139:             }\n140:             /* if we hit the end of the buffer, we need to scan the\n141:              * next buffer as well */\n142:             if (ptr == end) {\n143:                 bufoffset = 0;\n144:                 buf = buf2;\n145:                 if (mpirc != MPI_SUCCESS) {\n146:                     MPI_Error_string(mpirc, mpierrstr, &mpierrlen);\n147:                     MFU_LOG(MFU_LOG_ERR, \"Failed to read file: `%s' rc=%d %s\", name, mpirc, mpierrstr);\n148:                 }\n149:                 /* read next chunk */\n150:                 if (mpirc != MPI_SUCCESS) {\n151:                     MPI_Error_string(mpirc, mpierrstr, &mpierrlen);\n152:                     MFU_LOG(MFU_LOG_ERR, \"Failed to read file: `%s' rc=%d %s\", name, mpirc, mpierrstr);\n153:                 }\n154:                 /* update read offset for next time */\n155:                 read_offset += (MPI_Offset) read_count;\n156:                 /* reset pointers */\n157:                 ptr = (char*) buf;\n158:                 end = ptr + bufoffset + read_count;\n159:             }\n160:             /* otherwise we've found the next newline */\n161:             else {\n162:                 scan = 0;\n163:             }\n164:         }\n165: \n166:         /* read records from buffer */\n167:         while (ptr < end) {\n168:             /* decode record from buffer */\n169:             /* TODO: decode record */\n170: \n171:             /* update pointers */\n172:             ptr++;\n173:         }\n174: \n175:         /* update buffer offset for next time */\n176:         bufoffset = (size_t)(ptr - (char*) buf);\n177: \n178:         /* if we've read a full buffer, we need to switch buffers */\n179:", "label": "(64, MPI, MPI_Error_string(mpirc, mpierrstr, &mpierrlen);); (45, MPI, MPI_Exscan(&chunk_count, &chunk_offset, 1, MPI_UINT64_T, MPI_SUM, MPI_COMM_WORLD);); (88, MPI, mpirc = MPI_File_read_at(fh, pos, buf, 1, MPI_CHAR, &status);); (120, MPI, mpirc = MPI_File_read_at(fh, read_offset, bufstart, read_count, MPI_CHAR, &status);); (58, MPI, int mpirc = MPI_File_set_view(fh, disp, MPI_CHAR, MPI_CHAR, datarep, MPI_INFO_NULL);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (19, MPI, MPI_Bcast(&filesize, 1, MPI_UINT64_T, 0, MPI_COMM_WORLD);)"}
{"predict": "(1,MPI,sum_kernel<<<num_blocks, threads_per_block>>>(input, sum, max_val, N);)", "label": "(1, CUDA, __shared__ float s_mem[32];)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(15, MPI, rc = MPI_Type_dup(type, newtype);); (11, MPI, rc = MPI_Type_get_extent(type, &tmp_lb, &attr->child_type_extent);); (19, MPI, rc = MPI_Type_free(&type);)"}
{"predict": "(28,MPI,tim::mpi::initialize(argc, argv);)\n(31,OpenMP,#pragma omp parallel for reduction(+:ret_sum))", "label": "(60, OpenMP, #pragma omp parallel for reduction(+ : ret_sum))"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(3, TBB, tbb::task* dummy_root = new( tbb::task::allocate_root() ) tbb::empty_task;); (22, TBB, tbb::task::destroy( *dummy_root );); (13, TBB, tbb::task::spawn_root_and_wait(t);); (16, TBB, tbb::task::enqueue(t);)"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for), (38,OpenMP,#pragma omp parallel for)", "label": "(28, OpenMP, #pragma omp target update from(z[:10])); (17, OpenMP, #pragma omp target update to(z[:10])); (18, OpenMP, #pragma omp target map(to:k[:5]) map(always, tofrom:x) map(tofrom:y))"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for)", "label": "(28, OpenMP, #pragma omp target update from(z[:10])); (17, OpenMP, #pragma omp target update to(z[:10])); (18, OpenMP, #pragma omp target map(to:k[:5]) map(always, tofrom:x) map(tofrom:y))"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(18, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(4,OpenMP,#pragma omp parallel for reduction(+:c[:10]))", "label": "(4, OpenMP, #pragma omp teams distribute); (2, OpenMP, #pragma omp target)"}
{"predict": "(12,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for)", "label": "(41, CUDA, return cudaGetLastError();)"}
{"predict": "(15,MPI,*w = MPI_Wtime();); (32,OpenMP,#pragma omp parallel for), (58,OpenMP,#pragma omp parallel for), (105,OpenMP,#pragma omp parallel for)", "label": "(14, CUDA, extern __shared__ float alpha_buffer_sh[];)"}
{"predict": "(19,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for), (49,OpenMP,#pragma omp parallel for), (64,OpenMP,#pragma omp parallel for)", "label": "(65, MPI, rc = MPI_Type_free(&type);)"}
{"predict": "(8,MPI,MPI_Init(&argc, &argv);) (10,MPI,MPI_Comm_size(MPI_COMM_WORLD, &ntasks);) (11,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);) (30,MPI,MPI_Cart_create(MPI_COMM_WORLD, 2, dims, period, 0, &comm2d);) (31,MPI,MPI_Cart_coords(comm2d, rank, 2, coords);) (32,MPI,MPI_Cart_shift(comm2d, 0, 1, &neighbors[0], &neighbors[1]);) (33,MPI,MPI_Cart_shift(comm2d, 1, 1, &neighbors[2], &neighbors[3]);) (35,OpenMP,#pragma omp parallel for)", "label": "(37, MPI, MPI_Cart_shift(comm2d, 0, 1, &neighbors[0], &neighbors[1]);); (8, MPI, MPI_Init(&argc, &argv);); (26, MPI, MPI_Finalize();); (38, MPI, MPI_Cart_coords(comm2d, crank, 2, coords);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (28, MPI, MPI_Cart_create(MPI_COMM_WORLD, 2, dims, period, 1, &comm2d);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (42, MPI, MPI_Barrier(comm2d);); (26, MPI, MPI_Comm_rank(comm2d, &crank);); (29, MPI, MPI_Cart_shift(comm2d, 1, 1, &neighbors[2], &neighbors[3]);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for collapse(2))", "label": "(21, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize(), \"KaKjw_to_KKwaj\");); (19, CUDA, qmc_cuda::cuda_check(cudaGetLastError(), \"KaKjw_to_KKwaj\");)"}
{"predict": "(3,OpenMP,#pragma omp parallel for reduction(+:a))", "label": "(12, OpenMP, #pragma omp atomic hint(1) hint(1)); (7, OpenMP, #pragma omp atomic hint(a); (4, OpenMP, #pragma omp atomic hint(+); (1, OpenMP, #pragma omp atomic hint(); (6, OpenMP, #pragma omp atomic hint(a)); (-3, OpenMP, #pragma omp atomic hint)"}
{"predict": "imer gpuTimer;\n164:         gpuTimer.Start();\n165:         HANDLE_ERROR(cutensorCompute(handle, desc, alpha, A_d, C_d, 0, nullptr));\n166:         gpuTimer.Stop();\n167:         double timeCUTENSOR = gpuTimer.GetElapsed();\n168:         minTimeCUTENSOR = std::min(minTimeCUTENSOR, timeCUTENSOR);\n169:     }\n170:     printf(\"minTimeCUTENSOR: %.3f ms\\n\", minTimeCUTENSOR * 1e3);\n171: \n172:     /**************************\n173:      * Free Resources\n174:      **************************/\n175: \n176:     cutensorDestroy(handle);\n177:     return 0;\n178: }", "label": "(63, CUDA, HANDLE_CUDA_ERROR(cudaMallocHost((void**) &C, sizeof(floatTypeC) * elementsC));); (78, CUDA, HANDLE_CUDA_ERROR(cudaMemcpy2DAsync(A_d, sizeA, A, sizeA, sizeA, 1, cudaMemcpyDefault, nullptr));); (192, CUDA, if (C_d) cudaFree(C_d);); (188, CUDA, if (A) cudaFreeHost(A);); (51, CUDA, HANDLE_CUDA_ERROR(cudaMalloc((void**) &C_d, sizeC));); (49, CUDA, HANDLE_CUDA_ERROR(cudaMalloc((void**) &A_d, sizeA));); (187, CUDA, if (A_d) cudaFree(A_d);); (185, CUDA, if (C) cudaFreeHost(C);); (54, CUDA, HANDLE_CUDA_ERROR(cudaMallocHost((void**) &A, sizeof(floatTypeA) * elementsA));)"}
{"predict": "(6,OpenMP,#pragma omp parallel for reduction(+:p1Area)), (17,OpenMP,#pragma omp atomic), (20,OpenMP,resultMap->push_back(RPolygon(newxMin, newyMin, newxMax, newyMax, myr, myg, myb)));", "label": "(18, TBB, tbb::spin_mutex::scoped_lock lock(*rMutex);)"}
{"predict": "(15,MPI,*d_RanksA = MPI_Wtime();); (17,OpenMP,#pragma omp parallel for), (22,MPI,*w = MPI_Wtime(););", "label": "(21, CUDA, checkCudaErrors(cudaMalloc((void **)&d_LimitsA, MAX_SAMPLE_COUNT * sizeof(uint)));); (19, CUDA, checkCudaErrors(cudaMalloc((void **)&d_RanksB,  MAX_SAMPLE_COUNT * sizeof(uint)));); (17, CUDA, checkCudaErrors(cudaMalloc((void **)&d_RanksA,  MAX_SAMPLE_COUNT * sizeof(uint)));); (19, CUDA, checkCudaErrors(cudaMalloc((void **)&d_LimitsB, MAX_SAMPLE_COUNT * sizeof(uint)));)"}
{"predict": "(61,OpenMP,#pragma omp parallel for collapse(2)), (93,OpenMP,#pragma omp parallel for collapse(2))", "label": "(79, OpenACC, #pragma acc parallel loop independent); (70, OpenACC, #pragma acc data pcopy(grid[0:n*n])); (86, OpenACC, #pragma acc parallel loop gang); (99, OpenACC, #pragma acc kernels); (89, OpenACC, #pragma acc loop vector)"}
{"predict": "(14,OpenMP,#pragma omp parallel for reduction(+:errs))", "label": "(6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &world_size);)"}
{"predict": "(2,MPI, int rank; MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(25, MPI, MPI_Finalize();); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Init(NULL, NULL);)"}
{"predict": "set up row communicators                                      */\n118:   for (i=0; i<Num_procs; i++) {\n119:     ranks[i] = i;\n120:     mm[i] = i/npcol;\n121:     nn[i] = i%npcol;\n122:   }\n123:   MPI_Group world_group = MPI_COMM_WORLD->group;\n124:   MPI_Group_incl(world_group, Num_procs, ranks, &temp_group);\n125:   MPI_Comm_create(MPI_COMM_WORLD, temp_group, &comm_row);\n126:   MPI_Group_free(&temp_group);\n127: \n128:   /*  set up column communicators                                    */\n129:   for (i=0; i<Num_procs; i++) {\n130:     ranks[i] = i;\n131:     mm[i] = i/npcol;\n132:     nn[i] = i%npcol;\n133:   }\n134:   MPI_Group_incl(world_group, Num_procs, ranks, &temp_group);\n135:   MPI_Comm_create(MPI_COMM_WORLD, temp_group, &comm_col);\n136:   MPI_Group_free(&temp_group);\n137: \n138:   /* allocate space for local matrices                               */\n139: \n140:   lda = nprow*nb;\n141:   ldb = npcol*nb;\n142:   ldc = lda;\n143:   mynrows = nprow*nb;\n144:   myncols = npcol*nb;\n145:   myfrow = nb*mm[my_ID];\n146:   mylrow = myfrow + mynrows - 1;\n147:   myfcol = nb*nn[my_ID];\n148:   mylcol = myfcol + myncols - 1;\n149: \n150:   a = (double *) malloc (lda*mynrows*sizeof(double));\n151:   if (!a) {\n152:     printf(\"ERROR: Proc %d could not allocate local a\\n\", my_ID);\n153:     error = 1;\n154:   }\n155:   bail_out(error);\n1", "label": "(124, MPI, MPI_Comm_group( MPI_COMM_WORLD, &world_group );); (143, MPI, MPI_Comm_rank( comm_col, &myrow );); (205, MPI, MPI_Barrier(MPI_COMM_WORLD);); (83, MPI, MPI_Bcast(&inner_block_flag, 1, MPI_INT, root, MPI_COMM_WORLD);); (135, MPI, MPI_Group_incl( world_group, nprow, ranks, &temp_group );); (39, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &my_ID );); (78, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (124, MPI, MPI_Group_incl( world_group, npcol, ranks, &temp_group );); (35, MPI, MPI_Init(&argc,&argv);); (131, MPI, MPI_Comm_create( MPI_COMM_WORLD, temp_group, &comm_col );); (35, MPI, MPI_Comm_size( MPI_COMM_WORLD, &Num_procs );); (121, MPI, MPI_Comm_create( MPI_COMM_WORLD, temp_group, &comm_row );); (245, MPI, MPI_Finalize();); (70, MPI, MPI_Bcast(&order,  1, MPI_INT, root, MPI_COMM_WORLD);); (167, MPI, MPI_Allgather( &mynrows, 1, MPI_INT, mm, 1, MPI_INT, comm_col );); (70, MPI, MPI_Bcast(&nb, 1, MPI_INT, root, MPI_COMM_WORLD);); (127, MPI, MPI_Comm_rank( comm_row, &mycol );); (172, MPI, MPI_Allgather( &myncols, 1, MPI_INT, nn, 1, MPI_INT, comm_row );)"}
{"predict": "(4,OpenMP,#pragma omp parallel for), (7,OpenMP,#pragma omp parallel for), (10,OpenMP,#pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp target teams distribute parallel for if (false)); (9, OpenMP, #pragma omp target teams distribute parallel for if (parallel: Arg)); (0, OpenMP, #pragma omp target teams distribute parallel for if (true))"}
{"predict": "(1,MPI,template<typename T> void MPIDataCommunicatorMinIntegralTypeTest())", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(8, MPI, MPI_Reduce_local_c(inbuf, inoutbuf, count, datatype, MPI_SUM);)"}
{"predict": "(11,MPI,*w = MPI_Wtime();); (14,OpenMP,#pragma omp parallel for), (21,MPI,*w = MPI_Wtime();); (24,OpenMP,#pragma omp parallel for)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": ">= 0 */\n153: \n154:     return;\n155: }", "label": "(102, MPI, MPI_Barrier(c_info->File_comm);); (108, MPI, MPI_ERRHAND(GEN_File_read(c_info->fh, c_info->r_buffer, Locsize, c_info->etype, &stat));); (121, MPI, MPI_ERRHAND(GEN_File_read_at(c_info->fh, Offset, c_info->r_buffer, Locsize, c_info->etype, &stat));); (133, MPI, MPI_ERRHAND(GEN_File_read_shared(c_info->fh, c_info->r_buffer, Locsize, c_info->etype, &stat));); (92, MPI, *time = MPI_Wtime();)"}
{"predict": "(11,OpenMP,#pragma omp parallel for reduction(+:res))", "label": "(6, CUDA, thrust::device_ptr<thrust::complex<double>> d_ptr = thrust::device_malloc<thrust::complex<double>>(1);); (14, CUDA, thrust::device_free(d_ptr);); (9, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize());); (7, CUDA, qmc_cuda::cuda_check(cudaGetLastError());)"}
{"predict": "(9,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (29,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for), (35,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for), (39,OpenMP,#pragma omp parallel for), (41,OpenMP,#pragma omp parallel for), (43,OpenMP,#pragma omp parallel for), (45,OpenMP,#pragma omp parallel for), (47,OpenMP,#pragma omp parallel for), (51,OpenMP,#pragma omp parallel for), (53,OpenMP,#pragma omp parallel for), (55,OpenMP,#pragma omp parallel for), (57,OpenMP,#pragma omp parallel for), (61,OpenMP,#pragma omp parallel for), (63,OpenMP,#pragma omp parallel for), (69,OpenMP,#pragma omp parallel for)", "label": "(63, OpenMP, #pragma omp distribute parallel for simd private(h)); (86, OpenMP, #pragma omp distribute parallel for simd private(j)); (82, OpenMP, #pragma omp parallel private(i)); (65, OpenMP, #pragma omp distribute parallel for simd nowait); (9, OpenMP, #pragma omp distribute parallel for simd private(); (1, OpenMP, #pragma omp target); (27, OpenMP, #pragma omp distribute parallel for simd private(argc > 0 ? argv[1] : argv[2])); (1, OpenMP, #pragma omp distribute parallel for simd private); (35, OpenMP, #pragma omp distribute parallel for simd private(S1)); (19, OpenMP, #pragma omp distribute parallel for simd private(argc,); (61, OpenMP, #pragma omp parallel); (12, OpenMP, #pragma omp distribute parallel for simd private(argc); (36, OpenMP, #pragma omp distribute parallel for simd private(a, b)); (25, OpenMP, #pragma omp distribute parallel for simd private(argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (39, OpenMP, #pragma omp distribute parallel for simd private(argv[1])); (43, OpenMP, #pragma omp distribute parallel for simd private(e, g)); (2, OpenMP, #pragma omp distribute parallel for simd private()); (60, OpenMP, #pragma omp distribute parallel for simd private(i)); (65, OpenMP, #pragma omp parallel shared(i)); (-12, OpenMP, #pragma omp teams)"}
{"predict": "(5,MPI,*w = MPI_Wtime();)", "label": "(5, OpenCL, clReleaseMemObject(objX);); (12, OpenCL, clReleaseMemObject(objScratch);); (7, OpenCL, clReleaseMemObject(objNrm2);)"}
{"predict": "data2[1] + data0[1] - data2[-1] - data0[-1]);\n107:       float dys = 0.25f*(data2[pitch+1] + data0[pitch+1] - data2[-pitch-1] - data0[-pitch-1]);\n108:       float *d = d_Data0 + ptr - pitch;\n109:       float *d2 = d_Data0 + ptr + pitch;\n110:       float dds = 0.5f*(d[0] - d2[0]);\n111:       float dds2 = 2.0f*val - d2[0] - d[0];\n112:       float ddxs = 0.25f*(d2[1] + d[1] - d2[-1] - d[1]);\n113:       float ddys = 0.25f*(d2[pitch+1] + d[pitch+1] - d2[-pitch-1] - d[pitch-1]);\n114:       float det0 = dxx*dyy - dxy*dxy;\n115:       float det1 = dxx*dyy - dxy*dxy;\n116:       float det2 = dxx*dyy - dxy*dxy;\n117:       float det3 = dxx*dyy - dxy*dxy;\n118:       float det4 = dxx*dyy - dxy*dxy;\n119:       float det5 = dxx*dyy - dxy*dxy;\n120:       float det6 = dxx*dyy - dxy*dxy;\n121:       float det7 = dxx*dyy - dxy*dxy;\n122:       float det8 = dxx*dyy - dxy*dxy;\n123:       float det9 = dxx*dyy - dxy*dxy;\n124:       float det10 = dxx*dyy - dxy*dxy;\n125:       float det11 = dxx*dyy - dxy*dxy;\n126:       float det12 = dxx*dyy - dxy*dxy;\n127:       float det13 = dxx*dyy - dxy*dxy;\n12", "label": "(3, CUDA, __shared__ unsigned short points[2*MEMWID];)"}
{"predict": "(17,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for)", "label": "(13, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(63,OpenMP,#pragma omp parallel for)", "label": "(67, MPI, MPI_Waitall(2, request, status);)"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(28, MPI, MPI_Group_free(&wingroup);); (19, MPI, MPI_Win_create(buf, sizeof(int) * 10, sizeof(int), MPI_INFO_NULL, comm, &win);); (28, MPI, MPI_Win_free(&win);); (20, MPI, MPI_Group_compare(group, wingroup, &result);); (25, MPI, MPI_Group_free(&group);); (17, MPI, MPI_Comm_group(comm, &group);); (15, MPI, MPI_Win_get_group(win, &wingroup);)"}
{"predict": "(19,OpenMP,#pragma omp parallel for reduction(+:norm2)), (47,OpenMP,#pragma omp parallel for reduction(+:norm2))", "label": "(14, CUDA, ColorSpinorParam cudaParam(cpuParam, *inv_param, QUDA_CUDA_FIELD_LOCATION);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for)", "label": "(15, OpenMP, #pragma omp target parallel for)"}
{"predict": "(34,FIXME,grid, block);", "label": "(58, CUDA, thrust::host_vector<mytype> th_data = t_data;); (40, CUDA, thrust::device_vector<mytype> t_data(h_data, h_data+test_dsize);); (49, CUDA, cudaEventElapsedTime(&et, start, stop);); (22, CUDA, cudaStreamCreate(&str);); (27, CUDA, cudaLaunchCooperativeKernel((void *)my_remove_if<mytype>, FIXME);); (8, CUDA, cudaError_t err = cudaGetDeviceProperties(&prop, 0);); (26, CUDA, cudaMemcpy(h_data, d_odata, tsize, cudaMemcpyDeviceToHost);); (49, CUDA, thrust::remove(t_data.begin(), t_data.end(), -1);); (15, CUDA, cudaMemcpy(d_idata, h_data, tsize, cudaMemcpyHostToDevice);); (39, CUDA, cudaEventRecord(stop);); (-1, CUDA, cudaMalloc(&d_idxs, test_dsize*sizeof(unsigned));); (33, CUDA, cudaEventCreate(&start);); (-5, CUDA, cudaMalloc(&d_odata, tsize);); (33, CUDA, cudaEventRecord(start);); (-6, CUDA, cudaMemset(d_odata, 0, tsize);); (-9, CUDA, cudaMalloc(&d_idata, tsize);); (29, CUDA, cudaEventCreate(&stop);); (2, CUDA, cudaOccupancyMaxActiveBlocksPerMultiprocessor(&numBlkPerSM, my_remove_if<mytype>, nTPB, 0);)"}
{"predict": "(27,MPI,vars->nprocs_recv = nprocs_recv;); (32,OpenMP,#pragma omp parallel for), (52,OpenMP,#pragma omp parallel for), (62,MPI,sum = 0;); (82,MPI,*hole = 0;);", "label": "(48, MPI, MPI_Type_commit(recv_types + j);)"}
{"predict": "(31,OpenMP,#pragma omp parallel for private(nominal_idx, real_idx, initial_real_state, initial_nominal_state, trajectory_costs_cpu)), (52,OpenMP,#pragma omp parallel for private(nominal_idx, initial_x_d, cost_trajectories_d, trajectory_costs_gpu)), (64,OpenMP,#pragma omp parallel for reduction(+:cost_diff))", "label": "(25, CUDA, HANDLE_ERROR(cudaStreamSynchronize(stream));); (9, CUDA, HANDLE_ERROR(cudaMalloc((void**)&initial_x_d, sizeof(float) * 2 * DYN_T::STATE_DIM));); (9, CUDA, HANDLE_ERROR(cudaMalloc((void**)&cost_trajectories_d, sizeof(float) * 2 * num_rollouts));)"}
{"predict": "(1,OpenMP,#pragma omp parallel for), (5,OpenMP,#pragma omp parallel for), (7,OpenMP,#pragma omp parallel for), (8,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for), (35,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for), (41,OpenMP,#pragma omp parallel for), (47,OpenMP,#pragma omp parallel for)", "label": "(6, CUDA, extern __shared__ float s_buffer[];)"}
{"predict": "(10,OpenMP,#pragma omp parallel for private(i, flag)), (15,OpenMP,#pragma omp atomic), (19,OpenMP,#pragma omp atomic)", "label": "(19, MPI, MPI_Grequest_complete(aio_reqlist[i]->req);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for collapse(2))\n(36,OpenMP,#pragma omp parallel for collapse(2))", "label": "(67, CUDA, cudaEventSynchronize(stop);); (39, CUDA, cudaMalloc((void **) &d_a, sizeof(int)*m*n);); (114, CUDA, cudaFreeHost(h_cc);); (34, CUDA, cudaEventRecord(start, 0);); (60, CUDA, cudaThreadSynchronize();); (36, CUDA, cudaMalloc((void **) &d_b, sizeof(int)*n*k);); (39, CUDA, cudaMemcpy(d_a, h_a, sizeof(int)*m*n, cudaMemcpyHostToDevice);); (103, CUDA, cudaFree(d_a);); (62, CUDA, cudaEventElapsedTime(&gpu_elapsed_time_ms, start, stop);); (3, CUDA, cudaMallocHost((void **) &h_c, sizeof(int)*m*k);); (3, CUDA, cudaMallocHost((void **) &h_cc, sizeof(int)*m*k);); (35, CUDA, cudaMemcpy(d_b, h_b, sizeof(int)*n*k, cudaMemcpyHostToDevice);); (-1, CUDA, cudaMallocHost((void **) &h_b, sizeof(int)*n*k);); (99, CUDA, cudaFree(d_c);); (97, CUDA, cudaFree(d_b);); (66, CUDA, cudaEventElapsedTime(&cpu_elapsed_time_ms, start, stop);); (26, CUDA, cudaMalloc((void **) &d_c, sizeof(int)*m*k);); (49, CUDA, cudaEventRecord(stop, 0);); (45, CUDA, cudaMemcpy(h_c, d_c, sizeof(int)*m*k, cudaMemcpyDeviceToHost);); (95, CUDA, cudaFreeHost(h_b);); (93, CUDA, cudaFreeHost(h_a);); (12, CUDA, cudaEventCreate(&start);); (55, OpenMP, omp_mm(h_a, m, n, h_b, n, k, h_cc);); (-13, CUDA, cudaMallocHost((void **) &h_a, sizeof(int)*m*n);); (10, CUDA, cudaEventCreate(&stop);); (90, CUDA, cudaFreeHost(h_c);)"}
{"predict": "(26,OpenMP,#pragma omp parallel for), (38,OpenMP,#pragma omp parallel for)", "label": "(37, CUDA, cudaMalloc((void **)&result_data, tensor1->size * sizeof(float));)"}
{"predict": "(13,OpenMP,#pragma omp parallel for)", "label": "(1, OpenMP, #pragma omp parallel firstprivate(a, b, c, e)); (4, OpenMP, #pragma omp parallel firstprivate(a, b, c))"}
{"predict": "(19,OpenMP,#pragma omp parallel for collapse(2)), (21,OpenMP,#pragma omp parallel for collapse(2))", "label": "(29, CUDA, CUDA_CHECK(cudaMemcpy(P_host, P_device, size, cudaMemcpyDeviceToHost));); (35, CUDA, cudaFree(P_device);); (10, CUDA, CUDA_CHECK(cudaMemcpy(M_device, M_host, size, cudaMemcpyHostToDevice));); (34, CUDA, cudaFree(N_device);); (34, CUDA, cudaFree(M_device);); (25, CUDA, CUDA_CHECK(cudaDeviceSynchronize());); (7, CUDA, CUDA_CHECK(cudaMemcpy(N_device, N_host, size, cudaMemcpyHostToDevice));); (2, CUDA, CUDA_CHECK(cudaMalloc((void**)&N_device, size));); (9, CUDA, CUDA_CHECK(cudaMalloc((void**)&P_device, size));;); (-1, CUDA, CUDA_CHECK(cudaMalloc((void**)&M_device, size));)"}
{"predict": "(16,OpenMP,#pragma omp parallel for)", "label": "(11, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)"}
{"predict": "(4,OpenMP,#pragma omp parallel for), (7,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for)", "label": "(17, OpenMP, #pragma omp task default(firstprivate)); (4, OpenMP, #pragma omp task default(shared), default(shared)); (0, OpenMP, #pragma omp task default(); (0, OpenMP, #pragma omp task default()); (5, OpenMP, #pragma omp task default(none)); (1, OpenMP, #pragma omp task default(x)); (7, OpenMP, #pragma omp task default(shared)); (-6, OpenMP, #pragma omp task default); (-4, OpenMP, #pragma omp task default(none)"}
{"predict": "(2,OpenMP,#pragma omp parallel for collapse(3)), (7,OpenMP,#pragma omp parallel for collapse(3))", "label": "(4, TBB, const tbb::blocked_range3d<int> r( 0, i, 5, 0, j, 3, 0, k, 1 );); (4, TBB, tbb::parallel_for( r, Striker() );)"}
{"predict": "4:                   //    printf(\"  afa kernel:   %i %i %i %i %i %i %i %i %i %i\\n\", threadIdx_x, bid, grid->npcol, nsupers,myrow,krow,wrp,k,uind_br[0],bmod[lib*aln_i]);\n125:               }\n126:               x[ii+rel+i] = temp1;\n127:           }\n128:           #ifdef HAVE_CUDA\n111:           __syncwarp();\n112:           #endif\n129: \n130:           if(lne==0){\n131:               bmod[lib*aln_i]=bmod[lib*aln_i]+1;\n132:               __threadfence();\n133:           }\n134:           #ifdef HAVE_CUDA\n135:           __syncwarp();\n116:           #endif\n136:         }\n137:         }\n138:     }\n139: }\n140: }", "label": "(31, CUDA, volatile __shared__ int s_bmod;); (28, CUDA, __shared__ double s_lsum[MAXSUPER];)"}
{"predict": "(19,MPI,*w = MPI_Wtime();); (26,OpenMP,#pragma omp parallel for), (86,OpenMP,#pragma omp parallel for)", "label": "(52, MPI, MPI_Allreduce(MPI_IN_PLACE,&n,1,mpi_int_t,MPI_MAX,grid->comm);); (50, MPI, MPI_Allreduce(MPI_IN_PLACE,&nnzL,1,mpi_int_t,MPI_SUM,grid->comm);)"}
{"predict": "(26,OpenMP,#pragma omp parallel for)", "label": "(34, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (50, CUDA, CUDA_CHECK(cudaDeviceReset());); (47, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (23, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (41, CUDA, CUDA_CHECK(cudaFree(d_A));); (17, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(13,MPI,MPI_Wtime();)", "label": "(11, MPI, err = MPI_Type_struct(3, blocks, disps, types, &inttype);); (32, MPI, err = MPI_Type_size(eviltype, &val);); (134, MPI, MPI_Type_free(&inttype);); (75, MPI, err = MPI_Type_get_extent(eviltype, &lb, &extent);); (96, MPI, err = MPI_Type_ub(eviltype, &aval);); (17, MPI, err = MPI_Type_vector(3, 4, -5, inttype, &eviltype);); (131, MPI, MPI_Type_free(&eviltype);); (108, MPI, err = MPI_Type_get_true_extent(eviltype, &true_lb, &aval);); (40, MPI, err = MPI_Type_extent(eviltype, &aval);); (54, MPI, err = MPI_Type_lb(eviltype, &aval);)"}
{"predict": "(78,OpenMP,#pragma omp parallel for reduction(+:val_st_off[0:3])), (103,OpenMP,#pragma omp parallel for reduction(+:val_st_on[0:3]))", "label": "(48, OpenMP, lis2mdl_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for collapse(3))", "label": "(4, TBB, const tbb::blocked_range3d<int> r( 0, i, 5, 0, j, 3, 0, k, 1 );); (4, TBB, tbb::parallel_for( r, Striker() );)"}
{"predict": "(23,OpenMP,#pragma omp parallel for)", "label": "(36, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(27,OpenMP,#pragma omp parallel for)", "label": "(9, OpenMP, #pragma omp target); (65, OpenMP, #pragma omp distribute parallel for lastprivate(t_var, vec, s_arr, s_arr, var, var, svar)); (9, OpenMP, #pragma omp distribute parallel for lastprivate(g, g1, svar, sfvar)); (7, OpenMP, #pragma omp teams)"}
{"predict": "(21,MPI,*w = MPI_Wtime();); (23,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for), (36,OpenMP,#pragma omp parallel for), (42,OpenMP,#pragma omp parallel for)", "label": "(45, CUDA, cudaCheck(cudaGetLastError());)"}
{"predict": "2: \n123:       /* Read magnetometer data */\n124:       ism330dhcx_magnetic_raw_get(&mag_ctx, data_raw.i16bit);\n125:       mag_fielg_mG[0] =\n126:         ism330dhcx_from_fs16lps_to_mG(data_raw.i16bit[0]);\n127:       mag_fielg_mG[1] =\n128:         ism330dhcx_from_fs16lps_to_mG(data_raw.i16bit[1]);\n129:       mag_fielg_mG[2] =\n130:         ism330dhcx_from_fs16lps_to_mG(data_raw.i16bit[2]);\n131:     }\n132:   }\n133: }\n134: \n135: /* Local Functions -----------------------------------------------------------*/\n136: static void platform_write(void* handle, uint16_t reg, uint8_t* bufp, uint16_t len) {\n137:   /* TODO: implement platform specific i2c write function */\n138: }\n139: \n140: static void platform_read(void* handle, uint16_t reg, uint8_t* bufp, uint16_t len) {\n141:   /* TODO: implement platform specific i2c read function */\n142: }\n143: \n144: static void platform_delay(uint32_t ms) {\n145:   /* TODO: implement platform specific delay function */\n146: }\n147: \n148: /* -------------------------------------------------------------------------- */\n149: /* End of File ---------------------------------------------------------------- */", "label": "(66, OpenMP, iis2mdc_offset_temp_comp_set(&mag_ctx, PROPERTY_ENABLE);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(13, CUDA, CP_host->freeCudaMem();)"}
{"predict": "bmod, UBtree_ptr, URtree_ptr, ilsum, Ucolind_bc_dat, Ucolind_bc_offset, Unzval_bc_dat, Unzval_bc_offset, Uinv_bc_dat, Uinv_bc_offset, Uindval_loc_bc_dat, Uindval_loc_bc_offset, xsup, grid);\n109:     //} else {\n110:     if (mype == 0) {\n111:         slsum_bmod_inv_gpu_mrhs<<< nbcol_loc, dimBlock >>>(nbcol_loc, lsum, x, nrhs, nsupers, bmod, UBtree_ptr, URtree_ptr, ilsum, Ucolind_bc_dat, Ucolind_bc_offset, Unzval_bc_dat, Unzval_bc_offset, Uinv_bc_dat, Uinv_bc_offset, Uindval_loc_bc_dat, Uindval_loc_bc_offset, xsup, grid);\n112:     }\n113:     //printf(\"(%d) U End\\n\",mype);\n114:     //fflush(stdout);\n115:     //} //npes\n116:     //printf(\"(%d) U End\\n\",mype);\n117:     //fflush(stdout);\n118:     //if (npes == 1) {\n119:     //    slsum_bmod_inv_gpu_1rhs_new_rowdata<<< nbrow_loc, dimBlock >>>(nbrow_loc, lsum, x, nrhs, nsupers, bmod, UBtree_ptr, URtree_ptr, ilsum, Ucolind_br_dat, Ucolind_br_offset, Unzval_br_new_dat, Unzval_br_new_offset, Uinv_bc_dat, Uinv_bc_offset, xsup, grid);\n120:     //} else {\n121:     if (mype == 0) {\n122:     #ifdef U_BLOCK_PER_ROW_ROWDATA\n123:         slsum_bmod_inv_gpu_1rhs_new_rowdata<<< nbrow_loc, dimBlock >>>(nbrow_loc, lsum, x, nrhs, nsupers, bmod, UBtree_ptr, URtree_ptr, ilsum, Ucolind_br_dat, Ucolind_br_offset, Unzval_br_new_dat, Unzval_br_new_offset, Uinv_bc_dat, Uinv_bc_offset, xsup", "label": "(117, CUDA, cudaDeviceSetLimit(cudaLimitStackSize, cuattr.localSizeBytes);); (93, CUDA, cudaStreamCreateWithFlags(&stream[i], cudaStreamNonBlocking);); (121, CUDA, cudaOccupancyMaxPotentialBlockSize(&minGridSize, &myblockSize, (const void *) swait_bcrd_u, 0, 0);); (173, CUDA, CUDA_CHECK(cudaGetLastError());); (112, CUDA, cudaFuncGetAttributes(&cuattr, slsum_bmod_inv_gpu_mrhs_nvshmem);); (181, CUDA, CUDA_CHECK(cudaStreamDestroy(stream[i]));)"}
{"predict": "(77,OpenMP,#pragma omp parallel for), (80,OpenMP,#pragma omp parallel for)", "label": "(101, CUDA, checkHIPErrors( ::cudaDeviceSynchronize() );)"}
{"predict": "(11,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for)", "label": "(27, OpenMP, #pragma omp atomic update); (21, OpenMP, #pragma omp loop order(concurrent)); (29, OpenMP, num_threads = omp_get_num_threads();); (17, OpenMP, #pragma omp target parallel num_threads(THREADS) map(tofrom: x[0:N], num_threads, total_wait_errors) map(to: y[0:N], z[0:N]))"}
{"predict": "(3,OpenMP,#pragma omp parallel for) (6,OpenMP,#pragma omp parallel for) (9,OpenMP,#pragma omp parallel for) (12,OpenMP,#pragma omp parallel for) (14,OpenMP,#pragma omp parallel for) (18,OpenMP,#pragma omp parallel for) (20,OpenMP,#pragma omp parallel for) (22,OpenMP,#pragma omp parallel for) (25,OpenMP,#pragma omp parallel for)", "label": "(9, OpenMP, #pragma omp target defaultmap(tofrom:); (4, OpenMP, #pragma omp target defaultmap()); (25, OpenMP, #pragma omp target defaultmap(none:pointer)); (10, OpenMP, #pragma omp target defaultmap(tofrom,); (40, OpenMP, #pragma omp parallel); (-4, OpenMP, #pragma omp target defaultmap); (27, OpenMP, #pragma omp target defaultmap(none:pointer) defaultmap(none:scalar) defaultmap(none:aggregate) map(g) firstprivate(argc)); (8, OpenMP, #pragma omp target defaultmap(scalar:); (11, OpenMP, #pragma omp target defaultmap(tofrom: scalar) defaultmap(tofrom: scalar)); (20, OpenMP, #pragma omp target defaultmap(none:pointer) defaultmap(none:scalar) map(argc)); (1, OpenMP, #pragma omp target defaultmap(tofrom)); (6, OpenMP, #pragma omp target defaultmap(tofrom, scalar); (23, OpenMP, #pragma omp target defaultmap(none:aggregate)); (-10, OpenMP, #pragma omp target defaultmap(); (9, OpenMP, #pragma omp target defaultmap(tofrom : scalar) defaultmap(to: scalar)); (16, OpenMP, #pragma omp target defaultmap(none:pointer) defaultmap(none:scalar) map(g)); (9, OpenMP, #pragma omp target defaultmap(alloc: pointer) defaultmap(to: scalar) defaultmap(firstprivate: pointer)); (20, OpenMP, #pragma omp target defaultmap(none:aggregate) defaultmap(none:scalar) map(argc)); (-11, OpenMP, #pragma omp target defaultmap(tofrom)"}
{"predict": "(8,OpenMP,#pragma omp parallel for reduction(+:vector[i]))", "label": "(139, MPI, MPI_Finalize();); (63, MPI, MPI_Bcast(&iterations,    1, MPI_INT, root, MPI_COMM_WORLD);); (63, MPI, MPI_Bcast(&vector_length, 1, MPI_LONG, root, MPI_COMM_WORLD);); (17, MPI, MPI_Init(&argc,&argv);); (81, MPI, MPI_Barrier(MPI_COMM_WORLD);); (17, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (15, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for collapse(1)), (25,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for), (36,OpenMP,#pragma omp parallel for), (45,OpenMP,#pragma omp parallel for), (50,OpenMP,#pragma omp parallel for)", "label": "(25, OpenMP, #pragma omp target teams distribute parallel for simd collapse (1))); (60, OpenMP, #pragma omp target teams distribute parallel for simd collapse (2)); (4, OpenMP, #pragma omp target teams distribute parallel for simd collapse (); (54, OpenMP, #pragma omp target teams distribute parallel for simd collapse (N)); (36, OpenMP, #pragma omp target teams distribute parallel for simd collapse (S)); (24, OpenMP, #pragma omp target teams distribute parallel for simd collapse ((ST > 0) ? 1 + ST : 2)); (-4, OpenMP, #pragma omp target teams distribute parallel for simd collapse); (9, OpenMP, #pragma omp target teams distribute parallel for simd collapse (argc); (28, OpenMP, #pragma omp target teams distribute parallel for simd collapse (foobool(argc)), collapse (true), collapse (-5)); (12, OpenMP, #pragma omp target teams distribute parallel for simd collapse (ST); (43, OpenMP, #pragma omp target teams distribute parallel for simd collapse (1)); (-1, OpenMP, #pragma omp target teams distribute parallel for simd collapse ())"}
{"predict": "}\n124: \n125:     // enqueue the copy command\n126:     err = clEnqueueCopyImage(queue, memobjs[0], memobjs[1], srcPt, destPt, region, 0, NULL, &copyEvent);\n127:     if (err != CL_SUCCESS){\n128:         print_error( err, \"clEnqueueCopyImage failed\" );\n129:         clReleaseKernel( kernel[0] );\n130:         clReleaseProgram( program[0] );\n131:         clReleaseMemObject( memobjs[0] );\n132:         clReleaseMemObject( memobjs[1] );\n133:         clReleaseMemObject( memobjs[2] );\n134:         free( dst );\n135:         free( inptr );\n136:         return -1;\n137:     }\n138: \n139:     // now read the result\n140:     err = clEnqueueReadBuffer(queue, memobjs[1], CL_TRUE, 0, num_bytes, dst, 0, NULL, NULL);\n141:     if (err != CL_SUCCESS){\n142:         print_error( err, \"clEnqueueReadBuffer failed\" );\n143:         clReleaseKernel( kernel[0] );\n144:         clReleaseProgram( program[0] );\n145:         clReleaseMemObject( memobjs[0] );\n146:         clReleaseMemObject( memobjs[1] );\n147:         clReleaseMemObject( memobjs[2] );\n148:         free( dst );\n149:         free( inptr );\n150:         return -1;\n151:     }\n152: \n153:     clFinish(queue);\n154: \n155:     // free memory\n156:     clReleaseKernel( kernel[0] );\n157:     clReleaseProgram( program[0] );\n158:     clReleaseMemObject( memobjs[0] );\n159:     clReleaseMemObject( memobjs[1] );\n160:     clReleaseMemObject( memobjs[2] );\n161:     free( dst );\n162:", "label": "(85, OpenCL, clReleaseMemObject( memobjs[2] );); (140, OpenCL, clReleaseEvent(copyEvent);); (103, OpenCL, err = clEnqueueNDRangeKernel( queue, kernel[0], 2, NULL, threads, NULL, 0, NULL, NULL );); (72, OpenCL, err = clEnqueueWriteBuffer( queue, memobjs[1], true, 0, num_bytes, inptr, 0, NULL, NULL );); (87, OpenCL, err = clSetKernelArg( kernel[0], 0, sizeof( cl_mem ), (void *)&memobjs[1] );); (91, OpenCL, clReleaseProgram( program[0] );); (77, OpenCL, clReleaseMemObject( memobjs[0] );); (88, OpenCL, clReleaseKernel( kernel[0] );); (84, OpenCL, err |= clSetKernelArg( kernel[0], 1, sizeof( cl_mem ), (void *)&memobjs[0] );); (48, OpenCL, clReleaseMemObject(memobjs[0]);); (113, OpenCL, err = clEnqueueCopyImage( queue, memobjs[0], memobjs[2], srcPt, destPt, region, 0, NULL, &copyEvent );); (57, OpenCL, clReleaseMemObject(memobjs[1]);); (201, OpenCL, err = clEnqueueReadImage( queue, memobjs[2], true, origin, region2, 0, 0, dst, 0, NULL, NULL );); (125, OpenCL, err = clWaitForEvents( 1, &copyEvent );); (70, OpenCL, clReleaseMemObject( memobjs[1] );)"}
{"predict": "(10,OpenMP,#pragma omp parallel for reduction(+:milliseconds))", "label": "(12, CUDA, cudaEventRecord(stop);); (12, CUDA, cudaEventSynchronize(stop);); (9, CUDA, cudaMemcpy(d_data, h_data, SIZE * sizeof(float), cudaMemcpyHostToDevice);); (7, CUDA, cudaEventRecord(start);); (-1, CUDA, cudaMalloc((void **)&d_data, SIZE * sizeof(float));); (10, CUDA, cudaEventElapsedTime(&milliseconds, start, stop);); (1, CUDA, cudaEventCreate(&stop);); (13, CUDA, cudaMemcpy(h_data, d_data, SIZE * sizeof(float), cudaMemcpyDeviceToHost);); (19, CUDA, cudaFree(d_data);); (-3, CUDA, cudaEventCreate(&start);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp tile sizes(5)); (0, OpenMP, #pragma omp for)"}
{"predict": "(23,MPI,MPI_Init(&argc, &argv);)\n(54,MPI,MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);)\n(55,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)\n(76,OpenMP,#pragma omp parallel for reduction(+:vector[:vector_length]))", "label": "(143, MPI, MPI_Finalize();); (67, MPI, MPI_Bcast(&iterations,    1, MPI_INT, root, MPI_COMM_WORLD);); (67, MPI, MPI_Bcast(&vector_length, 1, MPI_LONG, root, MPI_COMM_WORLD);); (19, MPI, MPI_Init(&argc,&argv);); (85, MPI, MPI_Barrier(MPI_COMM_WORLD);); (19, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (17, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for private(index, seed, i, lens, pin, pout)), (46,OpenMP,#pragma omp atomic)", "label": "(10, OpenMP, #pragma omp parallel for); (45, OpenMP, #pragma omp atomic)"}
{"predict": "(11,MPI,*w = MPI_Wtime();); (20,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp critical), (36,OpenMP,#pragma omp critical)", "label": "(20, CUDA, thrust::complex<T> const* m_ = m + batch * mstride;); (16, CUDA, tmp[t] = thrust::complex<T>(0.0);); (9, CUDA, __shared__ thrust::cuda_cub::core::uninitialized_array<thrust::complex<T>, 64> tmp;)"}
{"predict": "(1, MPI, *w = MPI_Wtime();); (8, OpenMP, #pragma omp parallel for), (13, OpenMP, #pragma omp parallel for collapse(2)), (32, OpenMP, #pragma omp parallel for collapse(2)), (49, OpenMP, #pragma omp atomic)", "label": "(19, CUDA, __shared__ T B_thread_block_tile[BLOCK_TILE_SIZE_K][BLOCK_TILE_SIZE_X];); (17, CUDA, __shared__ T A_thread_block_tile[BLOCK_TILE_SIZE_Y][BLOCK_TILE_SIZE_K];)"}
{"predict": "(12,OpenMP,#pragma omp parallel for private(rank,numRanks)), (24,OpenMP,#pragma omp parallel for private(M_local,N_local,nnz,rhs,A_mat,Minv_mat,A_opr,Minv_opr,pcg_solver)), (49,OpenMP,#pragma omp parallel for private(is_solved)), (64,OpenMP,#pragma omp parallel for private(M_local,N_local,nnz,rhs,A_mat,Minv_mat,A_opr,Minv_opr,pcg_solver)), (92,OpenMP,#pragma omp parallel for private(is_solved))", "label": "(109, MPI, MPI_Finalize();); (5, MPI, err = MPI_Init(&argc, &argv);); (8, MPI, err = MPI_Comm_size(MPI_COMM_WORLD, &numRanks);); (5, MPI, err = MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (53,OpenMP,#pragma omp parallel for), (55,OpenMP,#pragma omp parallel for)", "label": "(29, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (72, CUDA, CHECK_CUDA(cudaFree(dX));); (43, CUDA, CHECK_CUDA(cudaMalloc(&dX, size * sizeof(int32_t)));); (29, CUDA, CHECK_HIEDNN(hiednnSetCudaStream(handle, 0));); (67, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (69, CUDA, CHECK_CUDA(cudaFree(dY));); (40, CUDA, CHECK_CUDA(cudaMalloc(&dY, size * sizeof(int32_t)));)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (12,OpenMP,#pragma omp parallel for), (59,OpenMP,#pragma omp parallel for), (75,OpenMP,#pragma omp parallel for), (81,OpenMP,#pragma omp parallel for), (103,OpenMP,#pragma omp parallel for)", "label": "(29, CUDA, __shared__ float x_s[x_s_sz];); (27, CUDA, __shared__ float w_s[w_s_sz];)"}
{"predict": "(11,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp barrier)", "label": "(16, SYCL, using namespace cl::sycl::access;); (4, SYCL, cl::sycl::queue queue;)"}
{"predict": "(3,OpenMP,#pragma omp parallel for), (8,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for)", "label": "(36, OpenMP, #pragma omp distribute collapse (N)); (9, OpenMP, #pragma omp distribute collapse (argc); (36, OpenMP, #pragma omp distribute collapse (2)); (22, OpenMP, #pragma omp distribute collapse (S)); (2, OpenMP, #pragma omp distribute collapse ()); (-1, OpenMP, #pragma omp distribute collapse (); (9, OpenMP, #pragma omp distribute collapse (1))); (27, OpenMP, #pragma omp distribute collapse (1)); (9, OpenMP, #pragma omp distribute collapse ((ST > 0) ? 1 + ST : 2)); (-7, OpenMP, #pragma omp distribute collapse); (3, OpenMP, #pragma omp distribute collapse (ST); (12, OpenMP, #pragma omp distribute collapse (foobool(argc)), collapse (true), collapse (-5))"}
{"predict": "(11,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for)", "label": "(8, MPI, rc = MPI_Type_get_extent(type, &tmp_lb, &attr->child_type_extent);)"}
{"predict": "(7,OpenMP,#pragma omp parallel for reduction(+: sivar)), (14,OpenMP,#pragma omp parallel for reduction(+: sivar))", "label": "(5, OpenMP, #pragma omp target teams distribute simd reduction(+: sivar))"}
{"predict": "14: \n115:     /* Create a local matrix for A. */\n116:     zCreate_CompCol_Matrix_loc(&GA, m_loc, n, nnz_loc, nzval_loc, colind, rowptr,\n117: \t\t\t    SLU_NC, SLU_Z, SLU_GE);\n118: \n119:     /* Solve the linear system Ax = b. */\n120:     if ( !(x[0] = doublecomplexMalloc_dist(m_loc*nrhs)) )\n121:         ABORT(\"Malloc fails for x[]\");\n122:     if ( !(rhs[0] = doublecomplexMalloc_dist(m_loc*nrhs)) )\n123:         ABORT(\"Malloc fails for rhs[]\");\n124:     zcopy_dist(m_loc*nrhs, b_global, ldb, rhs[0], ldb);\n125:     zgetrs_dist(trans, &GA, nrhs, rhs, ldb, x, ldx);\n126: \n127:     /* Compute the residual and check the solution. */\n128:     zresid_dist(trans, nrhs, b_global, ldb, xtrue_global, n, &GA, x, ldx, fp);\n129: \n130:     /* Free memory. */\n131:     Destroy_CompCol_Matrix_dist(&GA);\n132:     Destroy_SuperMatrix_Store(&GA);\n133:     SUPERLU_FREE(marker);\n134:     SUPERLU_FREE(rowptr);\n135:     SUPERLU_FREE(nzval_loc);\n136:     SUPERLU_FREE(colind);\n137:     SUPERLU_FREE(b_global);\n138:     SUPERLU_FREE(xtrue_global);\n139:     SUPERLU_FREE(x[0]);\n140:     SUPERLU_FREE(rhs[0]);\n141: \n142:     return 0;\n143: }", "label": "(50, MPI, MPI_Bcast( nzval,   nnz, SuperLU_MPI_DOUBLE_COMPLEX, 0, grid->comm );); (38, MPI, MPI_Bcast( rowind, nnz, mpi_int_t,  0, grid->comm );); (50, MPI, MPI_Bcast( colptr,  n+1, mpi_int_t,  0, grid->comm );); (48, MPI, MPI_Bcast( rowind,  nnz, mpi_int_t,  0, grid->comm );); (39, MPI, MPI_Bcast( &m,   1,   mpi_int_t,  0, grid->comm );); (31, MPI, MPI_Bcast( &n,     1,   mpi_int_t,  0, grid->comm );); (29, MPI, MPI_Bcast( &m,     1,   mpi_int_t,  0, grid->comm );); (38, MPI, MPI_Bcast( &nnz, 1,   mpi_int_t,  0, grid->comm );); (32, MPI, MPI_Bcast( colptr, n+1, mpi_int_t,  0, grid->comm );); (28, MPI, MPI_Bcast( &nnz,   1,   mpi_int_t,  0, grid->comm );); (28, MPI, MPI_Bcast( nzval,  nnz, SuperLU_MPI_DOUBLE_COMPLEX, 0, grid->comm );); (33, MPI, MPI_Bcast( &n,   1,   mpi_int_t,  0, grid->comm );)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for)", "label": "(22, MPI, MPI_Request_free(&r);); (14, MPI, MPI_Recv_init(buff, count, type, 0, tag, comm, &r);); (2, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(4,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (47,OpenMP,#pragma omp parallel for), (53,OpenMP,#pragma omp parallel for)", "label": "(20, TBB, tbb::flow::make_edge( n, r[i] );); (54, TBB, tbb::flow::remove_edge( n, r[i] );); (1, TBB, tbb::flow::write_once_node<R> n(g);); (-1, TBB, tbb::flow::graph g;)"}
{"predict": "(30,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for)", "label": "(64, OpenCL, clReleaseMemObject(clCompressedImage);); (25, OpenCL, auto clCompressedImage = clCreateImage(context, CL_MEM_READ_WRITE, &imageFormat, &imageDescriptor, nullptr, &retVal);)"}
{"predict": "(3,OpenMP,#pragma omp parallel for reduction(+:sum1,sum2,sum3)), (14,OpenMP,#pragma omp parallel for)", "label": "(7, CUDA, extern __shared__ float share_mem[];)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(7, OpenMP, #pragma omp parallel for)"}
{"predict": "DYNAMIC_SECTION(\"Hamiltonian::toString - \"\n129:                             << StateVectorMPIToName<StateVectorT>::name) {\n130:                 auto X0 = std::make_shared<NamedObsT>(\n131:                     \"PauliX\", std::vector<std::size_t>{0});\n132:                 auto Y1 = std::make_shared<NamedObsT>(\n133:                     \"PauliY\", std::vector<std::size_t>{1});\n134:                 auto Z2 = std::make_shared<NamedObsT>(\n135:                     \"PauliZ\", std::vector<std::size_t>{2});\n136: \n137:                 auto ham = HamiltonianT::create(\n138:                     {0.8, 0.5},\n139:                     {\n140:                         std::make_shared<TensorProdObsT>(X0, Y1, Z2),\n141:                         std::make_shared<TensorProdObsT>(Z0, X1, Y2),\n142:                     });\n143: \n144:                 REQUIRE(ham->toString() ==\n145:                         \"Hamiltonian: { 'coeffs' : [0.8, 0.5], \"\n146:                         \"'observables' : [PauliX[0] * PauliY[1] * PauliZ[2], \"\n147:                         \"PauliZ[0] * PauliX[1] * PauliY[2]]}\");\n148:             }\n149:         }\n150:     }\n151: }", "label": "(24, CUDA, cudaSetDevice(deviceId);); (20, CUDA, cudaGetDeviceCount(&nDevices);); (6, MPI, MPIManager mpi_manager(MPI_COMM_WORLD);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for)", "label": "(73, CUDA, cudaMalloc((void**) &d_gradData, m * c * sizeof(double));); (116, CUDA, cudaFree(d_fcLayer);); (118, CUDA, cudaFree(d_diffData);); (116, CUDA, cudaFree(d_gradData);); (12, CUDA, cudaMalloc((void**) &d_fcLayer, m * c * sizeof(double));); (17, CUDA, cudaMalloc((void**) &d_softmaxData, m * c * sizeof(double));); (43, CUDA, cudaDeviceSynchronize();); (11, CUDA, cudaMemcpy(d_fcLayer, fcLayer, m * c * sizeof(double), cudaMemcpyHostToDevice);); (72, CUDA, cudaMalloc((void**) &d_diffData, m * c * sizeof(double));); (109, CUDA, cudaFree(d_softmaxData);); (83, CUDA, cudaMemcpy(result_backward, d_gradData, m * c * sizeof(double), cudaMemcpyDeviceToHost);); (44, CUDA, cudaMemcpy(result, d_softmaxData, m * c * sizeof(double), cudaMemcpyDeviceToHost);); (69, CUDA, cudaMemcpy(d_diffData, diffData, m * c * sizeof(double), cudaMemcpyHostToDevice);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for private(index,K3,ddata,checksum,rckey)), (79,OpenMP,any_cracked |= 1;)", "label": "(11, OpenMP, #pragma omp parallel for); (77, OpenMP, #pragma omp atomic)"}
{"predict": "->etype_size,\n118:                             &contig_access_count);\n119: \n120:     ADIOI_Datatype_iscontig(datatype, &buftype_is_contig);\n121: \n122:     /* calculate the amount of data that each process needs to send */\n123:     ADIOI_Calc_my_req_per_proc(st_offsets, end_offsets, nprocs, nprocs_for_coll,\n124:                               count_my_req_per_proc, &count_my_req_procs);\n125: \n126:     /* fill the send buffer */\n127:     ADIOI_Fill_send_buffer(fd, (char *) buf, flat_buf, send_buf, offset_list,\n128:                             len_list, send_size, requests, sent_to_proc, nprocs,\n129:                             myrank, contig_access_count, min_st_offset, fd_size,\n130:                             fd_start, fd_end, send_buf_idx, curr_to_proc, done_to_proc,\n131:                             iter, buftype_extent);\n132: \n133:     /* if the current process is not part of the collective I/O group, then\n134:      * the function will return at this point */\n135:     if (nprocs_for_coll == 1) {\n136:         return;\n137:     }\n138: \n139:     /* initiate the collective I/O operations */\n140:     ADIOI_Initiate_coll_io(fd, requests, send_size, sent_to_proc, nprocs,\n141:                            myrank, count_my_req_procs, count_my_req_per_proc,\n142:                            send_buf_idx, curr_to_proc, done_to_proc, iter, flat_buf,\n143:                            status, error_code);\n144: }", "label": "(173, MPI, MPI_Allreduce(&tmp_error, error_code, 1, MPI_INT, MPI_MAX, fd->comm);); (47, MPI, MPI_Comm_size(fd->comm, &nprocs);); (75, MPI, MPI_Allgather(&end_offset, 1, ADIO_OFFSET, end_offsets, 1, ADIO_OFFSET, fd->comm);); (73, MPI, MPI_Allgather(&start_offset, 1, ADIO_OFFSET, st_offsets, 1, ADIO_OFFSET, fd->comm);); (45, MPI, MPI_Comm_rank(fd->comm, &myrank);); (192, MPI, MPI_Type_size_x(datatype, &size);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (...)\n\n### ** Code with Parallelization Instructions Inserted **\n\n1: void SCT_printComm3D(gridinfo3d_t *grid3d, SCT_t* SCT)\n2: {\n3:     //\n4:     double cvolFactor;\n5:     double cvolRed;\n6:     *w = MPI_Wtime();  // Start timing\n7: \n8:     int_t Np = (grid3d->npcol) * (grid3d->nprow) * (grid3d->npdep);\n9:     if (!grid3d->iam)\n10:     {\n11:         /* code */\n12:         printf(\"| commVolRed | %g   | %g |\\n\", cvolRed, cvolRed/Np );\n13:         printf(\"| commVolFactor | %g   | %g |\\n\", cvolFactor, cvolFactor/Np );\n14:     }\n15: }", "label": "(4, MPI, MPI_Reduce( &SCT->commVolFactor, &cvolFactor,  1, MPI_DOUBLE, MPI_SUM, 0, grid3d->comm );); (5, MPI, MPI_Reduce( &SCT->commVolRed, &cvolRed,  1, MPI_DOUBLE, MPI_SUM, 0, grid3d->comm );)"}
{"predict": "CUFFT_CHECK(cufftDestroy(plan));\n103:     CUDA_CHECK(cudaStreamDestroy(stream));\n104:     MPI_Finalize();\n105:     return 0;\n106: }", "label": "(10, CUDA, CUDA_CHECK(cudaGetDeviceCount(&ndevices));); (54, CUDA, CUDA_CHECK(cudaStreamCreate(&stream));); (88, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (8, CUDA, CUDA_CHECK(cudaSetDevice(rank % ndevices));); (-2, MPI, MPI_Init(&argc, &argv);); (113, MPI, MPI_Finalize();); (1, MPI, MPI_Comm_size(mpi_comm, &size);); (103, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (78, CUDA, CUDA_CHECK(cudaGetLastError());); (-3, MPI, MPI_Comm_rank(mpi_comm, &rank);)"}
{"predict": "(11,OpenMP,#pragma omp task), (12,OpenMP,#pragma omp task)", "label": "(5, TBB, if(tbb::is_current_task_group_canceling()) return;)"}
{"predict": "(10,OpenMP,#pragma omp parallel for), (20,MPI,if (MPI_Comm_rank(MPI_COMM_WORLD) == 0) {)", "label": "(14, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(4,OpenMP,#pragma omp parallel for reduction(+:total_worktime)), (10,OpenMP,#pragma omp parallel reduction(min:worktime), reduction(min:delta))", "label": "(4, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (5, TBB, tbb::tick_count t1 = tbb::tick_count::now();)"}
{"predict": "(7,MPI,int ntasks, rank, size=12;); (14,MPI,MPI_Init(&argc, &argv);); (15,MPI,MPI_Comm_size(MPI_COMM_WORLD, &size);); (16,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (20,MPI,MPI_Bcast(&ntasks, 1, MPI_INT, 0, MPI_COMM_WORLD);); (25,MPI,int block_size = size/ntasks;); (27,MPI,MPI_Scatter(sendbuf.data(), block_size, MPI_INT, recvbuf.data(), block_size, MPI_INT, 0, MPI_COMM_WORLD);); (36,MPI,MPI_Finalize(););", "label": "(11, MPI, MPI_Init(&argc, &argv);); (44, MPI, MPI_Finalize();); (10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (35, MPI, MPI_Recv(recvbuf.data(), block_size, MPI_INT, 0, rank, MPI_COMM_WORLD, &status);); (26, MPI, MPI_Send(&sendbuf[i*block_size], block_size, MPI_INT, i, i, MPI_COMM_WORLD);); (20, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for reduction(+:gFrstCost[:numForests],gFrstWt[:numForests]))", "label": "(23, MPI, MPI_Reduce(gFrstCost, gFrstCostAcc, numForests, MPI_DOUBLE, MPI_SUM, 0, grid3d->zscp.comm);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(17, MPI, MPI_Finalize();); (2, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(1,MPI,*w = MPI_Wtime();); (3,OpenMP,#pragma omp parallel for), (10,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for), (56,OpenMP,#pragma omp parallel for), (78,OpenMP,#pragma omp parallel for), (100,OpenMP,#pragma omp parallel for)", "label": "(11, CUDA, __shared__ float u_[_N_];); (11, CUDA, __shared__ float r[_N_], k[_N_], v[_N_], w_[_N_], gy[_N_];)"}
{"predict": "(14,MPI,MPI_Comm_size(MPI_COMM_WORLD, &size);), (15,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);), (16,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for), (36,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for), (44,OpenMP,#pragma omp parallel for), (48,OpenMP,#pragma omp parallel for)", "label": "(12, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(4,MPI,int world_rank = MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);)", "label": "(7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);)"}
{"predict": "(16,OpenMP,#pragma omp parallel for reduction(+:etsSum, etsMax, etsMin, etsSigmaSq)), (30,OpenMP,#pragma omp parallel for reduction(+:etsSigmaSq)), (44,OpenMP,#pragma omp parallel for reduction(min:etsMin)), (45,OpenMP,#pragma omp parallel for reduction(max:etsMax))", "label": "(3, TBB, tbb::enumerable_thread_specific<double> test_locals;); (13, TBB, REMARK(\"%d threads, work ratio %d per cent, time %g\", nThreads, csWorkRatio, (tbb::tick_count::now() - t0).seconds());); (0, TBB, tbb::task_scheduler_init init(tbb::task_scheduler_init::deferred);); (5, TBB, tbb::tick_count t0;); (6, TBB, t0 = tbb::tick_count::now();)"}
{"predict": "(33,OpenMP,#pragma omp parallel for collapse(2))", "label": "(43, CUDA, CSC(cudaGetLastError(), STATUS_FAILURE);)"}
{"predict": "2: \n173:     /* determine the number of files to rename */\n174:     uint64_t nfiles = mfu_flist_size(flist);\n175: \n176:     /* print number of files to rename */\n177:     if (rank == 0) {\n178:         printf(\"Renaming %llu files\\n\", nfiles);\n179:     }\n180: \n181:     /* create new flist */\n182:     mfu_flist flist_new = mfu_flist_new();\n183: \n184:     /* iterate over each file in the list */\n185:     for (uint64_t i = 0; i < nfiles; i++) {\n186:         /* get the old path */\n187:         mfu_path* path_oldfile = mfu_flist_get_path(flist, i);\n188:         /* construct the new path */\n189:         mfu_path* path_newfile = mfu_path_rename(path_oldfile, path_old, path_new, relative_targets);\n190:         /* add the new path to the new flist */\n191:         mfu_flist_add_path(flist_new, path_newfile);\n192:         /* free the new path */\n193:         mfu_path_delete(&path_newfile);\n194:     }\n195: \n196:     /* create output cache file */\n197:     if (rank == 0) {\n198:         mfu_flist_write_cache(flist_new, newpath);\n199:     }\n200: \n201:     /* free the flists */\n202:     mfu_flist_delete(&flist);\n203:     mfu_flist_delete(&flist_new);\n204: \n205:     /* free the path objects */\n206:     mfu_path_delete(&path_old);\n207:     mfu_path_delete(&path_new);\n208: \n209:     /* free the file object */\n210:     mfu_file_delete(&mfu_file);\n211: \n212:     /* free the mfu_walk_opts_t structure */\n213:     mfu_walk_opts_delete(w", "label": "(2, MPI, MPI_Init(&argc, &argv);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (105, MPI, MPI_Finalize();); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(27,OpenMP,#pragma omp parallel for)", "label": "(38, TBB, CHECK_MESSAGE( (tbb::flow::output_port<0>(outer_node).try_get(out) == output.try_get(out)), \"output from outer_node does not match output from graph\");); (13, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<1>(outer_node) == &tbb::flow::input_port<1>(inner_node1)), \"input port 1 of inner_node1 not bound to input port 1 in outer_node\");); (0, TBB, tbb::flow::composite_node<std::tuple<int, int>, std::tuple<int> > outer_node(g);); (8, TBB, outer_node.set_external_ports(base_type::input_ports_type(tbb::flow::input_port<0>(inner_node1), tbb::flow::input_port<1>(inner_node1)), base_type::output_ports_type(tbb::flow::output_port<0>(inner_node3)));); (-1, TBB, typedef tbb::flow::composite_node<std::tuple<int, int>, std::tuple<int> > base_type;); (18, TBB, tbb::flow::make_edge(inner_node2, tbb::flow::input_port<0>(inner_node3));); (18, TBB, tbb::flow::make_edge(inner_node2, tbb::flow::input_port<1>(inner_node3));); (13, TBB, tbb::flow::make_edge(inner_node1, tbb::flow::input_port<0>(inner_node2));); (-7, TBB, tbb::flow::graph g;); (6, TBB, CHECK_MESSAGE( (&tbb::flow::output_port<0>(outer_node) == &tbb::flow::output_port<0>(inner_node3)), \"output port 0 of inner_node3 not bound to output port 0 in outer_node\");); (11, TBB, tbb::flow::make_edge(inner_node1, tbb::flow::input_port<1>(inner_node2));); (2, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<0>(outer_node) == &tbb::flow::input_port<0>(inner_node1)), \"input port 0 of inner_node1 not bound to input port 0 in outer_node\");); (-8, TBB, tbb::flow::broadcast_node<int> input(g);); (-8, TBB, tbb::flow::queue_node<int> output(g);)"}
{"predict": "()", "label": "(16, CUDA, CudaFile aCudaFile( aReader, GPUSPHOptions(), aTemplDir );)"}
{"predict": "(4,OpenMP,#pragma omp parallel for)\n(7,OpenMP,#pragma omp parallel for)\n(10,OpenMP,#pragma omp parallel for)\n(13,OpenMP,#pragma omp parallel for)\n(16,OpenMP,#pragma omp parallel for)\n(19,OpenMP,#pragma omp parallel for)", "label": "(29, OpenMP, #pragma omp target teams distribute simd safelen (foobool(argc)), safelen (true), safelen (-5)); (46, OpenMP, #pragma omp target teams distribute simd safelen(safelen(tmain<int, char, -1, -2>(argc, argv)); (-1, OpenMP, #pragma omp target teams distribute simd safelen); (19, OpenMP, #pragma omp target teams distribute simd safelen (foobool(1) > 0 ? 1 : 2)); (9, OpenMP, #pragma omp target teams distribute simd safelen (4); (0, OpenMP, #pragma omp target teams distribute simd safelen (); (27, OpenMP, #pragma omp target teams distribute simd safelen (S1)); (10, OpenMP, #pragma omp target teams distribute simd safelen (2+2))); (1, OpenMP, #pragma omp target teams distribute simd safelen ())"}
{"predict": "(11,OpenMP,#pragma omp parallel for reduction(+:entropy)), (17,OpenMP,#pragma omp atomic update entropy += -p * std::log(p))", "label": "(11, OpenMP, #pragma omp parallel for reduction(+ : entropy))"}
{"predict": "(17,OpenMP,#pragma omp parallel for private(k, lk, lwork, knsupc, ii, i))", "label": "(28, MPI, MPI_Bcast( work, lwork, MPI_FLOAT, pkk, grid->comm );); (29, MPI, MPI_Bcast( work, diag_len[p], MPI_FLOAT, pkk, grid->comm );)"}
{"predict": "(10,MPI,*w = MPI_Wtime(););", "label": "(6, MPI, MPI_Comm_dup(MPI_COMM_SELF, &lwgrp_comm_self);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for)", "label": "(4, OpenMP, #pragma omp target teams distribute parallel for private(g, g1, sivar))"}
{"predict": "totTime;\n57: \t\t\trun_time[r][1] = num_iter;\n58: \n59: \t\t\tcufftDestroy(planZ2Z);\n60: \t\t}\n61: \t\tbenchmark_result[0] += (run_time[0][0] + run_time[1][0] + run_time[2][0]) / 3.0;\n62: \t\tbenchmark_result[1] += (run_time[0][1] + run_time[1][1] + run_time[2][1]) / 3.0;\n63: \t}\n64: \tprintf(\"average time per iteration = %f\\n\", benchmark_result[0]);\n65: \tprintf(\"number of iterations = %f\\n\", benchmark_result[1]);\n66: \tfree(inputC);\n67: }", "label": "(27, CUDA, cudaMalloc((void**)&dataC, sizeof(cufftDoubleComplex) * dims[0] * dims[1] * dims[2]);); (55, CUDA, cudaDeviceSynchronize();); (27, CUDA, cudaMemcpy(dataC, inputC, sizeof(cufftDoubleComplex) * dims[0] * dims[1] * dims[2], cudaMemcpyHostToDevice);); (78, CUDA, cudaFree(dataC);); (1, CUDA, cudaSetDevice(device_id);)"}
{"predict": "(7,OpenMP,#pragma omp parallel for reduction(+:total)), (7,OpenMP,#pragma omp parallel for reduction(+:newlyAllocated))", "label": "(6, CUDA, cudaCheck(cudaMemGetInfo(&free, &total));)"}
{"predict": "(25,OpenMP,#pragma omp parallel for private(bi)), (30,OpenMP,#pragma omp parallel for private(bi))", "label": "(48, CUDA, cudaFree(curandStates);); (4, CUDA, cudaMalloc(&curandStates, sizeof(curandState_t) * 2 * batchSize);)"}
{"predict": "(16,MPI,MPI_Init(&argc, &argv);); (27,MPI,MPI_Finalize(errs););", "label": "(29, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_ARE_FATAL);); (23, MPI, MPI_Error_string(ierr, str, &slen);); (14, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(24,OpenMP,#pragma omp parallel for)", "label": "(40, TBB, tbb::flow::make_edge(src, gamma);); (40, TBB, tbb::flow::make_edge(gamma, tint);); (40, TBB, tbb::flow::make_edge(tint, write);); (0, TBB, tbb::flow::graph g;)"}
{"predict": "(11,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for)", "label": "(3, CUDA, __shared__ unsigned sidxs[nTPB];)"}
{"predict": "(10,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (29,OpenMP,#pragma omp parallel for)", "label": "(19, MPI, MPI_Reduce(&mysize, &totalsize, 1, MPI_INT64_T, MPI_SUM, 0, MPI_COMM_WORLD);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&me);); (0, MPI, MPI_Init(&argc, &argv);); (23, MPI, MPI_Gather(&mysizeint, 1, MPI_INT, &(counts[0]), 1, MPI_INT, 0, MPI_COMM_WORLD);); (37, MPI, MPI_Finalize();); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD,&np);); (29, MPI, MPI_Gatherv(in.c_str(), mysizeint, MPI_CHAR, &(out[0]), &(counts[0]), &(displs[0]), MPI_CHAR, 0, MPI_COMM_WORLD);)"}
{"predict": "(5,OpenMP,#pragma omp parallel for)", "label": "(18, OpenMP, #pragma omp teams distribute parallel for order(concurrent); (1, OpenMP, #pragma omp teams distribute parallel for order); (20, OpenMP, #pragma omp teams distribute parallel for order(concurrent)); (-2, OpenMP, #pragma omp target); (10, OpenMP, #pragma omp teams distribute parallel for order(none); (5, OpenMP, #pragma omp teams distribute parallel for order()"}
{"predict": "(11,OpenMP,#pragma omp parallel for)", "label": "(13, CUDA, fprintf(stderr, \"CUDA kernel failed : %s\\n\", cudaGetErrorString(err));); (10, CUDA, auto err = cudaGetLastError();)"}
{"predict": "& FLG_SHOW_CHK) && !options.loader.showuncracked)\n169: \tif (john_main_process && options.target_enc != options.input_enc &&\n170: \t    (!database.format ||\n171: \t     !(database.format->params.flags & FMT_UNICODE))) {\n172: \t\tlog_event(\"- Target encoding: %s\",\n173: \t\t          cp_id2name(options.target_enc));\n174: \t}\n175: \n176: \tif (john_main_process && options.target_enc != options.input_enc &&\n177: \t    (!database.format ||\n178: \t     !(database.format->params.flags & FMT_UNICODE))) {\n179: \t\tlog_event(\"- Target encoding: %s\",\n180: \t\t          cp_id2name(options.target_enc));\n181: \t}\n182: \n183: \tif (john_main_process && options.target_enc != options.input_enc &&\n184: \t    (!database.format ||\n185: \t     !(database.format->params.flags & FMT_UNICODE))) {\n186: \t\tlog_event(\"- Target encoding: %s\",\n187: \t\t          cp_id2name(options.target_enc));\n188: \t}\n189: \n190: \tif (john_main_process && options.target_enc != options.input_enc &&\n191: \t    (!database.format ||\n192: \t     !(database.format->params.flags & FMT_UNICODE))) {\n193: \t\tlog_event(\"- Target encoding: %s\",\n194: \t\t          cp_id2name(options.target_enc));\n195: \t}\n196: \n197: \tif (john_main_process && options.target_enc != options.input_enc &&\n198: \t    (!database.format ||\n199: \t     !(database.format->params.flags & FMT_UNICODE))) {\n200: \t\tlog_event(\"- Target encoding: %s\",\n201: \t\t          cp_id2name(options.target_enc));\n202: \t}\n203: \n204: \tif (john_main_process && options.target_enc != options.input_enc &&\n205: \t    (!database.format ||\n206: \t     !(database.format->params.flags & FMT_UNICODE))) {\n207: \t\tlog_event(\"- Target encoding: %s\",\n208: \t\t          cp", "label": "(14, OpenMP, john_omp_fallback(argv);); (18, MPI, mpi_setup(argc, argv);); (78, OpenMP, john_omp_maybe_adjust_or_fallback(argv);); (6, OpenMP, john_omp_init();)"}
{"predict": "(2,MPI,MPI_Init(&argc, &argv);) (5,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);) (13,MPI,MPI_Comm_split(MPI_COMM_WORLD, rank % 2, rank, &newcomm);) (26,MPI,MPI_Barrier(newcomm);) (36,OpenMP,#pragma omp parallel for private(randval))", "label": "(44, MPI, MPI_Comm_split(MPI_COMM_WORLD, MPI_UNDEFINED, rank, &newcomm);); (21, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (40, MPI, MPI_Comm_free(&newcomm);); (21, MPI, startTime = MPI_Wtime();); (24, MPI, double rate = MPI_Wtime() - startTime;); (36, MPI, MPI_Comm_split(MPI_COMM_WORLD, 1, rank, &newcomm);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (93,OpenMP,#pragma omp parallel for), (101,OpenMP,#pragma omp parallel for)", "label": "(56, CUDA, flamegpu::CUDASimulation  cudaSimulation(model, argc, argv);)"}
{"predict": "(26,OpenMP,#pragma omp parallel for)", "label": "(36, TBB, typedef tbb::concurrent_hash_map<MyKey,MyData2,YourHashCompare> YourTable2;); (34, TBB, typedef tbb::concurrent_hash_map<MyKey,MyData2,YourHashCompare,MyAllocator> YourTable1;)"}
{"predict": "!= MPI_SUCCESS) {\n116:             MPI_Error_string(mpirc, mpierrstr, &mpierrlen);\n117:             MFU_ABORT(1, \"Failed to set view on file: `%s' rc=%d %s\", name, mpirc, mpierrstr);\n118:         }\n119: \n120:         /* read data */\n121:         int group_buf_size = (int) buft_pack_size(groups);\n122:         if (rank == 0) {\n123:             char* group_buf = (char*) MFU_MALLOC(group_buf_size);\n124:             if (mpirc != MPI_SUCCESS) {\n125:                 MPI_Error_string(mpirc, mpierrstr, &mpierrlen);\n126:                 MFU_ABORT(1, \"Failed to read file: `%s' rc=%d %s\", name, mpirc, mpierrstr);\n127:             }\n128:             buft_unpack(group_buf, groups);\n129:             mfu_free(&group_buf);\n130:         }\n131:         disp += (MPI_Offset) group_buf_size;\n132:     }\n133: \n134:     /* update displacement */\n135:     *outdisp = disp;\n136: }", "label": "(28, MPI, MPI_Error_string(mpirc, mpierrstr, &mpierrlen);); (142, MPI, MPI_Bcast(groups->buf, (int)groups->count, groups->dt, 0, MPI_COMM_WORLD);); (96, MPI, mpirc = MPI_File_read_at(fh, 0, user_buf, user_buf_size, MPI_BYTE, &status);); (132, MPI, mpirc = MPI_File_read_at(fh, 0, group_buf, group_buf_size, MPI_BYTE, &status);); (102, MPI, MPI_Bcast(users->buf, (int)users->count, users->dt, 0, MPI_COMM_WORLD);); (15, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (28, MPI, mpirc = MPI_File_read_at(fh, 0, header_packed, header_size, MPI_BYTE, &status);); (41, MPI, MPI_Bcast(header, 6, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (60, MPI, MPI_Exscan(&count, &offset, 1, MPI_UINT64_T, MPI_SUM, MPI_COMM_WORLD);); (12, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (16, MPI, int mpirc = MPI_File_set_view(fh, disp, MPI_BYTE, MPI_BYTE, datarep, MPI_INFO_NULL);); (69, MPI, MPI_Type_get_extent(users->dt, &lb_user, &extent_user);); (105, MPI, MPI_Type_get_extent(groups->dt, &lb_group, &extent_group);); (190, MPI, mpirc = MPI_File_read_at(fh, read_offset, buf, read_size, MPI_BYTE, &status);); (74, MPI, mpirc = MPI_File_set_view(fh, disp, MPI_BYTE, MPI_BYTE, datarep, MPI_INFO_NULL);); (159, MPI, MPI_Allreduce(&iters, &all_iters, 1, MPI_UINT64_T, MPI_MAX, MPI_COMM_WORLD);)"}
{"predict": "(18,MPI,int mpi_rank; int mpi_size; MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank); MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);); (24,OpenMP,#pragma omp parallel for), (35,OpenMP,#pragma omp parallel for)", "label": "(22, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (22, MPI, MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);)"}
{"predict": "(3,OpenMP,#pragma omp parallel for)", "label": "(8, CUDA, CHECK(cudaFree(m_output_boxes_device));); (9, CUDA, CHECK(cudaFree(m_output_classes_device));); (2, CUDA, CHECK(cudaFree(m_input_resize_device));); (0, CUDA, CHECK(cudaFree(m_input_src_device));); (3, CUDA, CHECK(cudaFree(m_output_num_device));); (0, CUDA, CHECK(cudaFree(m_input_rgb_device));); (3, CUDA, CHECK(cudaFree(m_output_scores_device));)"}
{"predict": "(18,MPI, *w = MPI_Wtime();); (21,OpenMP, #pragma omp parallel for), (41,OpenMP, #pragma omp parallel for), (95,OpenMP, #pragma omp parallel for)", "label": "(19, CUDA, extern __shared__ bool kept_bboxinfo_flag[];)"}
{"predict": "(22,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (36,OpenMP,#pragma omp parallel for), (43,OpenMP,#pragma omp parallel for), (47,OpenMP,#pragma omp parallel for), (51,OpenMP,#pragma omp parallel for), (57,OpenMP,#pragma omp parallel for)", "label": "(9, MPI, MPI_Comm_rank(comm_world, &rank_world);)"}
{"predict": "(9,MPI,*w = MPI_Wtime();); (16,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for), (39,OpenMP,#pragma omp parallel for)", "label": "(39, MPI, MPI_Request_free(&reqs[i]);)"}
{"predict": "(12,MPI,gk_imalloc(npes, \"lsendcounts\");) (13,MPI,gk_imalloc(npes, \"lsdispls\");) (17,MPI,lsendcounts[i] = sendcounts[i];) (18,MPI,lsdispls[i] = sdispls[i];) (25,MPI,sendcounts[i] = lsendcounts[i];) (26,MPI,sdispls[i] = lsdispls[i];)", "label": "(11, MPI, MPI_Comm_size(comm, &npes);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (11,MPI,d_B = (float*)malloc(DSIZE * sizeof(float));); (12,MPI,d_C = (float*)malloc(DSIZE * sizeof(float));); (16,MPI,cudaMalloc((void**)&d_B, DSIZE * sizeof(float))); (17,MPI,cudaMalloc((void**)&d_C, DSIZE * sizeof(float));); (22,MPI,cudaMemcpy(d_C, h_C, DSIZE * sizeof(float), cudaMemcpyHostToDevice));", "label": "(15, CUDA, cudaMemcpy(d_A, h_A, DSIZE*sizeof(float), cudaMemcpyHostToDevice);); (20, CUDA, cudaCheckErrors(\"kernel launch failure\");); (8, CUDA, cudaMalloc(&d_A, DSIZE*sizeof(float));)"}
{"predict": "(7,MPI,MPI_Comm_size(MPI_COMM_WORLD, &comm_size); MPI_Comm_rank(MPI_COMM_WORLD, &rank);) (16,OpenMP,#pragma omp parallel for)", "label": "(35, MPI, MPI_Finalize();); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &comm_size);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (8, MPI, vtkh::SetMPICommHandle(MPI_Comm_c2f(MPI_COMM_WORLD));)"}
{"predict": "(18,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for)", "label": "(19, CUDA, result = cudaMalloc((void**)&devPtrA, M*N*sizeof(*a));); (30, CUDA, cudaFree(devPtrA);)"}
{"predict": "(9,OpenMP,#pragma omp parallel for private(pidx,start,end,permutation,error))", "label": "(6, CUDA, __shared__ uint s_permutations[160];)"}
{"predict": "27: \t\t/* Update X = X + dx. */\n128: \t\tfor (i = 0; i < m_loc; ++i) X_col[i] += dx[i];\n129: \n130: \t\tlstres = berr[j];\n131: \t\t++count;\n132: \t\tcontinue;\n133: \t    } else {\n134: \t\tbreak;\n135: \t    }\n136: \t}\n137: \n138:     }\n139: \n140:     /* Free workspace. */\n141:     SUPERLU_FREE(work);\n142: \n143:     /* Check the status of the solution. */\n144:     psgstrsv3d(options, n, LUstruct, ScalePermstruct, trf3Dpartition, grid3d, X,\n145:                m_loc, fst_row, m_loc, nrhs, SOLVEstruct, stat, info);\n146:     return;\n147: }", "label": "(110, MPI, MPI_Allreduce( &s, &berr[j], 1, MPI_FLOAT, MPI_MAX, grid->comm );); (111, MPI, MPI_Bcast( &berr[j], 1, MPI_FLOAT, 0,  grid3d->zscp.comm);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for)", "label": "(62, TBB, tbb::internal::segmented_iterator<nested_vec, const T> my_csi(my_vec);); (19, TBB, tbb::internal::segmented_iterator<nested_vec, T> my_si(my_vec);)"}
{"predict": "(3,OpenMP,#pragma omp parallel for private(row, col, s_idx)), (11,OpenMP,#pragma omp parallel for private(s_img)), (32,OpenMP,#pragma omp parallel for private(s))", "label": "(4, CUDA, __shared__ uint4 s_img[TPB];)"}
{"predict": "(11,MPI,*group = MPI_COMM_NULL;); (14,MPI,*group = MPI_COMM_SELF;); (18,OpenMP,#pragma omp parallel for), (34,MPI,pgroup = MPI_COMM_SELF;); (43,OpenMP,#pragma omp parallel for), (46,MPI,MPI_Intercomm_merge(pgroup_new, 0 /* LOW */ , &pgroup);); (52,MPI,pgroup = MPI_COMM_SELF;); (57,OpenMP,#pragma omp parallel for), (60,MPI,MPI_Recv(port, PORT_SIZE, MPI_CHAR, members[0], 0, MPI_COMM_WORLD,MPI_STATUS_IGNORE);); (62,MPI,MPI_Intercomm_merge(pgroup_new, 1 /* HIGH */ , &pgroup);); (65,MPI,MPI_Intercomm_merge(pgroup_new, 0 /* HIGH */ , &pgroup);); (70,MPI,MPI_Intercomm_merge(pgroup_new, 0 /* HIGH */ , &pgroup););", "label": "(49, MPI, MPI_Comm_accept(port, MPI_INFO_NULL, 0, pgroup, &pgroup_new);); (53, MPI, MPI_Comm_free(&pgroup_old);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (65, MPI, MPI_Comm_connect(port, MPI_INFO_NULL, 0, MPI_COMM_SELF, &pgroup_new);); (43, MPI, MPI_Send(port, PORT_SIZE, MPI_CHAR, members[i], 0, MPI_COMM_WORLD);); (36, MPI, MPI_Open_port(port_info, port);); (45, MPI, MPI_Comm_free(&pgroup_new);); (49, MPI, MPI_Close_port(port);); (67, MPI, MPI_Comm_connect(port, MPI_INFO_NULL, 0, pgroup, &pgroup_new);)"}
{"predict": "(1, MPI, MPI_Comm_rank(comm, &rank);)", "label": "(10, MPI, MPICHECK(MPI_Group_size(group, &groupSize));); (6, MPI, MPICHECK(MPI_Comm_group(comm, &group));); (13, MPI, MPICHECK(MPI_Group_translate_ranks(group, groupSize, ranks.data(), worldGroup, worldRanks.data()));); (13, MPI, MPICHECK(MPI_Group_free(&group));); (2, MPI, MPICHECK(MPI_Comm_group(MPI_COMM_WORLD, &worldGroup));); (12, MPI, MPICHECK(MPI_Group_free(&worldGroup));)"}
{"predict": "(31,OpenMP,#pragma omp parallel for)", "label": "(33, OpenMP, #pragma omp taskloop simd private(argv[1])); (38, OpenMP, #pragma omp taskloop simd private(h)); (13, OpenMP, #pragma omp taskloop simd private(argc); (52, OpenMP, #pragma omp parallel shared(i)); (52, OpenMP, #pragma omp parallel private(i)); (1, OpenMP, #pragma omp taskloop simd private); (15, OpenMP, #pragma omp taskloop simd private(argc > 0 ? argv[1] : argv[2])); (38, OpenMP, #pragma omp parallel); (41, OpenMP, #pragma omp taskloop simd private(i)); (21, OpenMP, #pragma omp taskloop simd private(a, b)); (32, OpenMP, #pragma omp taskloop simd shared(i)); (13, OpenMP, #pragma omp taskloop simd private(argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (15, OpenMP, #pragma omp taskloop simd private(S1)); (5, OpenMP, #pragma omp taskloop simd private(argc,); (-5, OpenMP, #pragma omp taskloop simd private(); (-3, OpenMP, #pragma omp taskloop simd private()); (20, OpenMP, #pragma omp taskloop simd private(e, g, z)); (40, OpenMP, #pragma omp taskloop simd private(j))"}
{"predict": "(9,MPI,generic_composite(inputs, comp_outputs, task_id, false, false, true);), (11,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for)", "label": "(6, OpenMP, std::vector<BabelFlow::Payload> comp_outputs( 1 );)"}
{"predict": "(1,MPI, *w = MPI_Wtime(););", "label": "(43, MPI, printf(\" MPI library version      : %s\\n\", mpi_version());)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(15, CUDA, checkCudaErrors(cudaFree(workload->d_data));); (15, CUDA, checkCudaErrors(cudaFreeHost(workload->h_data));); (3, CUDA, checkCudaErrors(cudaSetDevice(workload->cudaDeviceID));); (14, CUDA, checkCudaErrors(cudaStreamDestroy(workload->stream));)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (11,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for)", "label": "(9, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(11,MPI,MPI_Init(&argc, &argv);) (13,OpenMP,#pragma omp parallel for private(num_selected)) (46,MPI,MPI_Finalize();)", "label": "(10, CUDA, int *d_num_out = thrust::raw_pointer_cast(num_out.data());); (10, CUDA, int *d_data = thrust::raw_pointer_cast(data.data());); (24, CUDA, d_tmp_storage = thrust::raw_pointer_cast(tmp_storage.data());); (32, CUDA, thrust::fill(flags.begin(), flags.begin() + num_selected, true);); (3, CUDA, thrust::device_vector<bool> flags(num_items);); (20, CUDA, thrust::device_vector<char> tmp_storage(tmp_storage_size);); (0, CUDA, thrust::device_vector<int> data(num_items);); (29, CUDA, thrust::fill(flags.begin() + num_selected, flags.end(), false);); (26, CUDA, thrust::sequence(data.begin(), data.end());); (28, CUDA, thrust::shuffle(flags.begin(), flags.end(), g);); (-5, CUDA, thrust::device_vector<int> num_out(1);); (36, CUDA, cudaMemcpy(&h_num_out, d_num_out, sizeof(int), cudaMemcpyDeviceToHost);); (0, CUDA, bool *d_flags = thrust::raw_pointer_cast(flags.data());)"}
{"predict": "(32,OpenMP,#pragma omp parallel for private(i))", "label": "(12, OpenCL, status = clSetKernelArg(kernel, 0, sizeof(cl_mem), &test_buf);); (20, OpenCL, status = clSetKernelArg(kernel, 0, sizeof(cl_mem), NULL);)"}
{"predict": "(34,OpenMP,#pragma omp parallel for)", "label": "(76, CUDA, checkCudaErrors(cudaEventRecord(stop, 0));); (15, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, cuda_device));); (94, CUDA, checkCudaErrors(cudaFree(d_partial_sums));); (57, CUDA, checkCudaErrors(cudaEventCreate(&start));); (57, CUDA, checkCudaErrors(cudaEventCreate(&stop));); (73, CUDA, checkCudaErrors(cudaEventElapsedTime(&inc, start, stop));); (66, CUDA, checkCudaErrors(cudaEventRecord(start, 0));); (70, CUDA, checkCudaErrors(cudaEventSynchronize(stop));); (86, CUDA, checkCudaErrors(cudaFreeHost(h_partial_sums));); (84, CUDA, checkCudaErrors(cudaFreeHost(h_result));); (55, CUDA, checkCudaErrors(cudaMalloc((void **)&d_data, sz));); (84, CUDA, checkCudaErrors(cudaFree(d_data));); (18, CUDA, checkCudaErrors(cudaMallocHost((void **)&h_result, sizeof(int)*n_elements));); (79, CUDA, checkCudaErrors(cudaFreeHost(h_data));); (56, CUDA, checkCudaErrors(cudaMemcpy(d_data, h_data, sz, cudaMemcpyHostToDevice));); (-4, CUDA, cuda_device = findCudaDevice(argc, (const char **)argv);); (65, CUDA, checkCudaErrors(cudaMemcpy(h_result, d_data, sz, cudaMemcpyDeviceToHost));); (50, CUDA, checkCudaErrors(cudaMemset(d_partial_sums, 0, partial_sz));); (11, CUDA, checkCudaErrors(cudaMallocHost((void **)&h_data, sizeof(int)*n_elements));); (-5, CUDA, checkCudaErrors(cudaGetDevice(&cuda_device));); (46, CUDA, checkCudaErrors(cudaMalloc((void **)&d_partial_sums, partial_sz));); (48, CUDA, checkCudaErrors(cudaMallocHost((void **)&h_partial_sums, partial_sz));)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(6, CUDA, RAFT_CUDA_TRY(cudaStreamSynchronize(stream));)"}
{"predict": "(13,MPI,*w = MPI_Wtime(););", "label": "(1, OpenMP, #pragma omp target teams defaultmap); (2, OpenMP, #pragma omp target teams defaultmap(); (9, OpenMP, #pragma omp target teams defaultmap(tofrom)); (12, OpenMP, #pragma omp target teams defaultmap(tofrom,); (3, OpenMP, #pragma omp target teams defaultmap(tofrom); (4, OpenMP, #pragma omp target teams defaultmap (tofrom:); (13, OpenMP, #pragma omp target teams defaultmap(tofrom, scalar); (-2, OpenMP, #pragma omp target teams defaultmap()); (5, OpenMP, #pragma omp target teams defaultmap(tofrom scalar)); (8, OpenMP, #pragma omp target teams defaultmap (scalar:)"}
{"predict": "(10,MPI,*w = MPI_Wtime();); (15,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(1, MPI, *w = MPI_Wtime();)", "label": "(19, MPI, MPI_Comm_size(d->comm, &ranks_comm);); (28, MPI, MPI_Comm_rank(parent_comm, &parent_rank);); (28, MPI, MPI_Allgather(&parent_rank, 1, MPI_INT, ranklist, 1, MPI_INT, d->comm);)"}
{"predict": "(20,OpenMP,#pragma omp parallel for)", "label": "(52, CUDA, CHECK(cudaFree(d_y));); (30, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyHostToDevice));); (51, CUDA, CHECK(cudaFree(d_z));); (39, CUDA, CHECK(cudaMemcpy(h_z, d_z, M, cudaMemcpyDeviceToHost));); (22, CUDA, CHECK(cudaMalloc((void**)&d_x, M));); (22, CUDA, CHECK(cudaMalloc((void**)&d_y, M));); (45, CUDA, CHECK(cudaFree(d_x));); (21, CUDA, CHECK(cudaMalloc((void**)&d_z, M));); (30, CUDA, CHECK(cudaGetLastError());); (30, CUDA, CHECK(cudaDeviceSynchronize());); (22, CUDA, CHECK(cudaMemcpy(d_y, h_y, M, cudaMemcpyHostToDevice));)"}
{"predict": "(16,OpenMP,#pragma omp parallel for private(pc, ljb, lsub, lnzval, len, len2, status)), (30,MPI,MPI_Bcast(lnzval, len2, MPI_DOUBLE, pc, grid->comm, &status))", "label": "(32, MPI, MPI_Send(lnzval, len2, MPI_DOUBLE, 0, jb, grid3d->zscp.comm);); (36, MPI, MPI_Recv(lnzval, len2, MPI_DOUBLE, layer, jb, grid3d->zscp.comm, &status);)"}
{"predict": "(30,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for), (50,OpenMP,#pragma omp parallel for), (58,OpenMP,#pragma omp parallel for), (64,OpenMP,#pragma omp parallel for), (72,OpenMP,#pragma omp parallel for), (79,OpenMP,#pragma omp parallel for), (105,OpenMP,#pragma omp parallel for)", "label": "(76, OpenMP, #define OP_GOMP_CRITICAL_CPT(TYPE, OP, FLAG))"}
{"predict": "0;\n118:         if (need_s8_comp) {\n119:             const int64_t src_comp_off = src_idle_off + f * nelems_reduce;\n120:             comp_val += s8_comp.get_s32_elem(src_comp_off);\n121:         }\n122:         if (need_zp_comp) {\n123:             const int64_t src_zp_off = src_idle_off + f * nelems_reduce;\n124:             comp_val += zp_comp.get_s32_elem(src_zp_off);\n125:         }\n126:         if (need_comp) {\n127:             dst.set_s32_elem(src_idle_off, comp_val);\n128:         }\n129:     });\n130: }", "label": "(101, OpenMP, dims_t comp_dims(ndims, 1);)"}
{"predict": "(3,OpenMP,#pragma omp parallel for), (6,OpenMP,#pragma omp parallel for), (9,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for)", "label": "(24, OpenMP, #pragma omp target parallel device (3.14)); (9, OpenMP, #pragma omp target parallel device (argc))); (20, OpenMP, #pragma omp target parallel device (-10u)); (5, OpenMP, #pragma omp target parallel device (argc); (12, OpenMP, #pragma omp target parallel device (k), device (argc+1)); (-3, OpenMP, #pragma omp target parallel device); (6, OpenMP, #pragma omp target parallel device (argc > 0 ? argv[1] : argv[2])); (11, OpenMP, #pragma omp target parallel device (S1)); (12, OpenMP, #pragma omp target parallel device (-2)); (-3, OpenMP, #pragma omp target parallel device ()); (4, OpenMP, #pragma omp target parallel device (argc + argc)); (-7, OpenMP, #pragma omp target parallel device ()"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (13,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for)", "label": "(1, CUDA, __shared__ float cache[threadsPerBlock];)"}
{"predict": "(29,OpenMP,#pragma omp parallel for)", "label": "(33, CUDA, cudaFree( dptrPosMass );); (33, CUDA, cudaFree( dptrForce );)"}
{"predict": "(30,OpenMP,#pragma omp parallel for), (42,OpenMP,#pragma omp parallel for), (44,OpenMP,#pragma omp parallel for)", "label": "(63, OpenMP, double stop = omp_get_wtime();); (70, CUDA, CUDA_RT_CALL(cudaFree(a));); (18, CUDA, CUDA_RT_CALL(cudaDeviceSynchronize());); (10, CUDA, CUDA_RT_CALL(cudaMemset(a, 0, nx * ny * sizeof(float)));); (6, CUDA, CUDA_RT_CALL(cudaMalloc(&a, nx * ny * sizeof(float)));); (63, CUDA, CUDA_RT_CALL(cudaFree(l2_norm_d));); (13, CUDA, CUDA_RT_CALL(cudaGetLastError());); (32, CUDA, CUDA_RT_CALL(cudaMemset(l2_norm_d, 0, sizeof(float)));); (36, CUDA, CUDA_RT_CALL(cudaMemcpy(l2_norm_h, l2_norm_d, sizeof(float), cudaMemcpyDeviceToHost));); (5, CUDA, CUDA_RT_CALL(cudaMemset(a_new, 0, nx * ny * sizeof(float)));); (14, CUDA, CUDA_RT_CALL(cudaMallocHost(&l2_norm_h, sizeof(float)));); (54, CUDA, CUDA_RT_CALL(cudaMemcpy(a_ref_h, a, nx * ny * sizeof(float), cudaMemcpyDeviceToHost));); (58, CUDA, CUDA_RT_CALL(cudaFree(a_new));); (-2, CUDA, CUDA_RT_CALL(cudaMalloc(&a_new, nx * ny * sizeof(float)));); (22, OpenMP, double start = omp_get_wtime();); (8, CUDA, CUDA_RT_CALL(cudaMalloc(&l2_norm_d, sizeof(float)));); (51, CUDA, CUDA_RT_CALL(cudaFreeHost(l2_norm_h));)"}
{"predict": "(15,MPI,MPI_Init(&argc, &argv);) (17,MPI,comm = MPI_COMM_WORLD;) (82,MPI,MPI_Waitall(2, r, s);) (88,MPI,MPI_Finalize();)", "label": "(20, MPI, MPI_Comm_size(comm, &size);); (64, MPI, MPI_Error_class(errval, &errclass);); (95, MPI, MPI_Ssend(NULL, 0, MPI_INT, dest, 10, comm);); (39, MPI, errval = MPI_Recv(NULL, 0, MPI_INT, src, 100, comm, MPI_STATUS_IGNORE);); (85, MPI, MPI_Ssend(NULL, 0, MPI_INT, dest, 100, comm);); (59, MPI, errval = MPI_Waitsome(2, r, &outcount, indices, s);); (87, MPI, MPI_Send(b1, 10, MPI_INT, dest, 0, comm);); (25, MPI, errval = MPI_Irecv(b2, 10, MPI_INT, src, 10, comm, &r[1]);); (44, MPI, errval = MPI_Recv(NULL, 0, MPI_INT, src, 10, comm, MPI_STATUS_IGNORE);); (16, MPI, MPI_Comm_set_errhandler(comm, MPI_ERRORS_RETURN);); (16, MPI, errval = MPI_Irecv(b1, 10, MPI_INT, src, 0, comm, &r[0]);); (8, MPI, MPI_Comm_rank(comm, &rank);); (82, MPI, MPI_Send(b2, 11, MPI_INT, dest, 10, comm);)"}
{"predict": "(4,OpenMP,#pragma omp parallel for reduction(+:sum)), (9,OpenMP,#pragma omp parallel for reduction(+:gsum))", "label": "(3, OpenMP, #pragma omp parallel for reduction(+:sum)); (8, OpenMP, #pragma omp target teams distribute parallel for reduction(+:gsum))"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (14,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for)", "label": "(35, MPI, MPI_Start(&reqs[root]);); (20, MPI, MPI_Type_vector(n, 1, stride, MPI_DOUBLE, &vec);); (11, MPI, MPI_Info_create(&info);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (32, MPI, MPI_Wait(&reqs[root], MPI_STATUS_IGNORE);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (48, MPI, MPI_Type_free(&vec);); (15, MPI, MPI_Type_commit(&vec);); (39, MPI, MPI_Request_free(&reqs[root]);); (44, MPI, MPI_Info_free(&info);)"}
{"predict": ":                     CHECK( ex_caught );\n106:                 }\n107:             }\n108:         }\n109: \n110:         {\n111:             SAY( \"Now unload library and do various tests.\" );\n112:             tbb::runtime_loader rtl( tbb::runtime_loader::em_status );\n113:             SAY( \"Unload current version.\" );\n114:             rtl.unload();\n115:             CHECK( rtl.status() == tbb::runtime_loader::ec_ok );\n116:             SAY( \"Unload again, it should return a error.\" );\n117:             rtl.unload();\n118:             CHECK( rtl.status() == tbb::runtime_loader::ec_not_loaded );\n119:         }\n120: \n121:         {\n122:             SAY( \"Create a proxy object and call load() with good arguments.\" );\n123:             char const * path[] = { \".\", NULL };\n124:             tbb::runtime_loader rtl( tbb::runtime_loader::em_status );\n125:             SAY( \"Load current version.\" );\n126:             rtl.load( path, TBB_INTERFACE_VERSION, TBB_INTERFACE_VERSION );\n127:             CHECK( rtl.status() == tbb::runtime_loader::ec_ok );\n128:             if ( rtl.status() == tbb::runtime_loader::ec_ok ) {\n129:                 {\n130:                     SAY( \"Make sure the library really loaded.\" );\n131:                     CHECK_TBB_IS_LOADED();\n132:                 }\n133:                 SAY( \"Unload current version.\" );\n134:                 rtl.unload();\n135:                 CHECK( rtl.status() == tbb::runtime_loader::ec_ok );\n136:             }\n137:         }\n138:     } __TBB_CATCH ( ... ) {\n139:         return -1;\n140:     }; // __TBB_TRY\n141: \n142:     return 0;\n143: }\n144: #endif // __TBB_runtime_loader_H", "label": "(172, TBB, CHECK( rtl1.status() == tbb::runtime_loader::ec_bad_ver );); (97, TBB, tbb::task_scheduler_init init( 1 );); (136, TBB, CHECK_EXCEPTION( eid_invalid_multiple_scheduling, tbb::invalid_multiple_scheduling );); (40, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_no_lib );); (8, TBB, tbb::runtime_loader rtl( tbb::runtime_loader::em_status );); (158, TBB, tbb::runtime_loader rtl1( tbb::runtime_loader::em_status );); (158, TBB, CHECK( rtl0.status() == tbb::runtime_loader::ec_ok );); (126, TBB, CHECK_EXCEPTION( eid_bad_last_alloc,              tbb::bad_last_alloc              );); (154, TBB, tbb::runtime_loader rtl0( tbb::runtime_loader::em_status );); (130, TBB, CHECK_EXCEPTION( eid_improper_lock,               tbb::improper_lock               );); (113, TBB, tbb::internal::throw_exception( tbb::internal::id );); (84, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_bad_call );); (96, TBB, tbb::internal::throw_exception( tbb::internal::exception_id( id ) );); (39, TBB, tbb::runtime_loader rtl( tbb::runtime_loader::em_throw );); (133, TBB, tbb::internal::handle_perror( EAGAIN, \"apple\" );); (-1, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_ok );); (37, TBB, tbb::runtime_loader::error_code code = tbb::runtime_loader::ec_ok;); (8, TBB, CHECK( rtl.status() == tbb::runtime_loader::ec_bad_arg );); (147, TBB, CHECK( rtl1.status() == tbb::runtime_loader::ec_ok );); (46, TBB, CHECK( code == tbb::runtime_loader::ec_no_lib );); (117, TBB, CHECK_EXCEPTION( eid_missing_wait,                tbb::missing_wait                );); (121, TBB, CHECK_EXCEPTION( eid_user_abort,                  tbb::user_abort                  );); (37, TBB, CHECK( code == tbb::runtime_loader::ec_bad_arg );)"}
{"predict": ":         }/*end for j*/\n114:       }/*end for i*/\n115:     }/*end for k*/\n116:   }/*end parallel region*/\n117: }/*end function*/", "label": "(31, OpenMP, #pragma omp teams distribute collapse(2)); (47, OpenMP, #pragma omp parallel for collapse(2)); (34, OpenMP, #pragma omp parallel for)"}
{"predict": "(4,OpenMP,#pragma omp parallel for), (8,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for)", "label": "(20, OpenMP, #pragma omp distribute parallel for proc_bind(master); (4, OpenMP, #pragma omp distribute parallel for proc_bind); (13, OpenMP, #pragma omp distribute parallel for proc_bind()); (7, OpenMP, #pragma omp distribute parallel for proc_bind(); (26, OpenMP, #pragma omp distribute parallel for proc_bind(x)); (38, OpenMP, #pragma omp distribute parallel for proc_bind(spread)); (19, OpenMP, #pragma omp distribute parallel for proc_bind(close), proc_bind(spread)); (-4, OpenMP, #pragma omp target); (32, OpenMP, #pragma omp parallel proc_bind(close)); (27, OpenMP, #pragma omp distribute parallel for proc_bind(master)); (-6, OpenMP, #pragma omp teams)"}
{"predict": "(13,OpenMP,#pragma omp parallel for)", "label": "(33, MPI, MPI_Bcast(&rc, 1, MPI_INT, 0, scr_storedesc_cntl->comm);)"}
{"predict": "(2,OpenMP,#pragma omp parallel for)\n(4,OpenMP,#pragma omp parallel for)\n(6,OpenMP,#pragma omp parallel for)\n(11,OpenMP,#pragma omp parallel for)\n(14,OpenMP,#pragma omp parallel for)\n(22,OpenMP,#pragma omp parallel for)\n(26,OpenMP,#pragma omp parallel for)\n(34,OpenMP,#pragma omp parallel for)\n(37,OpenMP,#pragma omp parallel for)\n(41,OpenMP,#pragma omp parallel for)\n(44,OpenMP,#pragma omp parallel for)\n(47,OpenMP,#pragma omp parallel for)", "label": "(69, OpenMP, #pragma omp cancellation point parallel); (66, OpenMP, #pragma omp cancellation point taskgroup); (60, OpenMP, #pragma omp cancel for); (57, OpenMP, #pragma omp cancel); (64, OpenMP, #pragma omp cancel parallel); (50, OpenMP, #pragma omp target parallel copyin(pvt)); (-5, OpenMP, #pragma omp target parallel); (-4, OpenMP, #pragma omp target parallel (); (53, OpenMP, #pragma omp cancellation point); (57, OpenMP, #pragma omp cancel taskgroup); (-1, OpenMP, #pragma omp target parallel )); (0, OpenMP, #pragma omp target parallel }); (4, OpenMP, #pragma omp target parallel unknown()); (50, OpenMP, #pragma omp cancellation point for); (-9, OpenMP, #pragma omp target parallel [); (49, OpenMP, #pragma omp cancel sections); (49, OpenMP, #pragma omp cancellation point sections); (-10, OpenMP, #pragma omp target parallel ])"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(9, CUDA, cudaGetDeviceProperties( &prop, 0 );); (5, CUDA, cudaSetDeviceFlags( cudaDeviceMapHost );); (3, CUDA, cudaSetDevice( 0 );)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (46,OpenMP,#pragma omp parallel for), (51,OpenMP,#pragma omp parallel for)", "label": "(1, MPI, MPI_Init(&argc, &argv);); (61, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &P);)"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(43, CUDA, setCommonICUDataPointer(&U_ICUDATA_ENTRY_POINT, FALSE, pErrorCode);)"}
{"predict": "(24,OpenMP,#pragma omp parallel for collapse(2)), (32,OpenMP,#pragma omp atomic)", "label": "(3, TBB, tbb::task_arena big_arena(static_cast<int>(num_threads));); (28, TBB, int thread_id = tbb::this_task_arena::current_thread_index();); (0, TBB, tbb::global_control concurrency(tbb::global_control::max_allowed_parallelism, num_threads);)"}
{"predict": "(3,OpenMP,#pragma omp parallel for) (11,OpenMP,#pragma omp parallel for) (17,OpenMP,#pragma omp parallel for)", "label": "(13, OpenMP, #pragma omp parallel for simd collapse (ST); (34, OpenMP, #pragma omp parallel for simd collapse (N)); (20, OpenMP, #pragma omp parallel for simd collapse (foobool(argc)), collapse (true), collapse (-5)); (-1, OpenMP, #pragma omp parallel for simd collapse); (13, OpenMP, #pragma omp parallel for simd collapse ((ST > 0) ? 1 + ST : 2)); (19, OpenMP, #pragma omp parallel for simd collapse (S)); (9, OpenMP, #pragma omp parallel for simd collapse (1)); (-3, OpenMP, #pragma omp parallel for simd collapse (); (-2, OpenMP, #pragma omp parallel for simd collapse ()); (1, OpenMP, #pragma omp parallel for simd collapse (argc); (27, OpenMP, #pragma omp parallel for simd collapse (2))"}
{"predict": "(20,OpenMP,#pragma omp parallel for reduction(+:sum_total)), (40,OpenMP,#pragma omp parallel for reduction(+:sum_total))", "label": "(61, TBB, tbb::flow::make_edge( a1, tbb::flow::input_port<1>(a3) );); (58, TBB, tbb::flow::remove_edge( a2, s2 );); (60, TBB, tbb::flow::make_edge( a3, s2 );); (0, TBB, tbb::flow::function_node<int,int> c(g, tbb::flow::unlimited, cube_body());); (19, TBB, CHECK_MESSAGE( (&std::get<1>(a3.input_ports()) == &tbb::flow::input_port<1>(a3.j)), \"input_port1 of j not bound to input port 1 in composite_node a3\");); (25, TBB, tbb::flow::make_edge( c, tbb::flow::input_port<1>(a0) );); (21, TBB, tbb::flow::queue_node<int> q(g);); (25, TBB, tbb::flow::make_edge( c, tbb::flow::input_port<0>(a1) );); (27, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(a0), tbb::flow::input_port<0>(a2) );); (20, TBB, tbb::flow::make_edge( s, tbb::flow::input_port<0>(a0) );); (16, TBB, tbb::flow::function_node<int,int> s2(g, tbb::flow::unlimited, square_body());); (22, TBB, tbb::flow::make_edge( c, tbb::flow::input_port<1>(a1) );); (24, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(a1), tbb::flow::input_port<1>(a2) );); (9, TBB, CHECK_MESSAGE( (&std::get<0>(a3.input_ports()) == &tbb::flow::input_port<0>(a3.j)), \"input_port 0 of j not bound to input port 0 in composite_node a3\");); (3, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<0>(a2) == &tbb::flow::input_port<0>(a2.j)), \"input_port 0 of j not bound to input port 0 in composite_node a2\");); (24, TBB, tbb::flow::make_edge( s2, q );); (-15, TBB, tbb::flow::graph g;); (-15, TBB, tbb::flow::function_node<int,int> s(g, tbb::flow::unlimited, square_body());); (0, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<1>(a2) == &tbb::flow::input_port<1>(a2.j)), \"input_port 1 of j not bound to input port 1 in composite_node a2\");); (-12, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<0>(a0) == &tbb::flow::input_port<0>(a0.j)), \"input_port 0 of j not bound to input port 0 in composite_node a0\");); (18, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(a2), s2 );); (-12, TBB, CHECK_MESSAGE( (&tbb::flow::output_port<0>(a0) == &a0.f), \"f not bound to output port 0 in composite_node a0\");); (-18, TBB, tbb::flow::function_node<int,int> p(g, tbb::flow::unlimited, passthru_body());); (35, TBB, tbb::flow::remove_edge(s2, q);); (-11, TBB, CHECK_MESSAGE( (&std::get<1>(a0.input_ports()) == &tbb::flow::input_port<1>(a0.j)), \"input_port1 of j not bound to input port 1 in composite_node a1\");); (-17, TBB, CHECK_MESSAGE( (&tbb::flow::input_port<1>(a0) == &tbb::flow::input_port<1>(a0.j)), \"input_port 1 of j not bound to input port 1 in composite_node a0\");); (-14, TBB, CHECK_MESSAGE( (&std::get<0>(a0.input_ports()) == &tbb::flow::input_port<0>(a0.j)), \"input_port 0 of j not bound to input port 0 in composite_node a1\");); (-8, TBB, CHECK_MESSAGE( (&tbb::flow::output_port<0>(a2) == &a2.f), \"f not bound to output port 0 in composite_node a2\");); (32, TBB, tbb::flow::make_edge( a0, a3 );)"}
{"predict": "(37,OpenMP,#pragma omp parallel for private(i)), (39,OpenMP,#pragma omp parallel for private(i)), (41,OpenMP,#pragma omp parallel for private(i))", "label": "(29, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(6,OpenMP,#pragma omp parallel for)", "label": "(10, TBB, test_extract<tbb::flow::rejecting>();); (10, TBB, test_extract<tbb::flow::queueing>();)"}
{"predict": "(10,MPI,*w = MPI_Wtime();); (38,OpenMP,#pragma omp parallel for)", "label": "(13, CUDA, checkCudaErrors(cudaEventCreate(&start));); (13, CUDA, checkCudaErrors(cudaEventCreate(&end));); (41, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (38, CUDA, checkCudaErrors(cudaEventRecord(end));); (35, CUDA, checkCudaErrors(cudaEventRecord(start));); (50, CUDA, checkCudaErrors(cudaEventElapsedTime(&elapsedTime, start, end));)"}
{"predict": "(5,MPI,*w = MPI_Wtime();)\n(14,OpenMP,#pragma omp parallel for)", "label": "(5, OpenMP, #pragma omp parallel num_threads(2)); (1, OpenMP, omp_set_nested(1);); (1, OpenMP, omp_set_max_active_levels(1);)"}
{"predict": "(49,OpenMP,#pragma omp parallel for), (80,OpenMP,#pragma omp parallel for), (91,OpenMP,#pragma omp parallel for)", "label": "(3, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (73, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (40, CUDA, EXPECT_EQ(cudaSuccess, cudaMemset(outAccess->sampleData(0), 0xFA, outBufferSize));); (104, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamDestroy(stream));)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (10,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for)", "label": "(25, OpenMP, #pragma omp target teams distribute parallel for simd shared (argv[1])); (28, OpenMP, #pragma omp target teams distribute parallel for simd shared(ca)); (19, OpenMP, #pragma omp target teams distribute parallel for simd shared (S1)); (42, OpenMP, #pragma omp target teams distribute parallel for simd shared(j)); (29, OpenMP, #pragma omp target teams distribute parallel for simd shared(e, g)); (34, OpenMP, #pragma omp target teams distribute parallel for simd firstprivate(i)); (11, OpenMP, #pragma omp target teams distribute parallel for simd shared (argc > 0 ? argv[1] : argv[2])); (30, OpenMP, #pragma omp target teams distribute parallel for simd private(i), shared(i)); (7, OpenMP, #pragma omp target teams distribute parallel for simd shared (argc,); (22, OpenMP, #pragma omp target teams distribute parallel for simd shared(da)); (3, OpenMP, #pragma omp target teams distribute parallel for simd shared (argc); (24, OpenMP, #pragma omp target teams distribute parallel for simd shared(h, B::x)); (31, OpenMP, #pragma omp target teams distribute parallel for simd shared(i)); (28, OpenMP, #pragma omp target teams distribute parallel for simd private(i)); (-5, OpenMP, #pragma omp target teams distribute parallel for simd shared (); (12, OpenMP, #pragma omp target teams distribute parallel for simd shared(ba)); (-9, OpenMP, #pragma omp target teams distribute parallel for simd shared); (-6, OpenMP, #pragma omp target teams distribute parallel for simd shared ()); (1, OpenMP, #pragma omp target teams distribute parallel for simd shared (argc)); (4, OpenMP, #pragma omp target teams distribute parallel for simd shared (a, b, c, d, f, z))"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(65, OpenMP, #pragma omp target update to(u1_real, u1_imag))"}
{"predict": "(10,OpenMP,#pragma omp parallel for private(p1, pt1, j))", "label": "(3, CUDA, __shared__ float maxScore[FMC2H];); (4, CUDA, __shared__ int maxIndex[FMC2H];); (2, CUDA, __shared__ float maxScor2[FMC2H];); (-1, CUDA, __shared__ float siftPoint[128*FMC2H];)"}
{"predict": "(14,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for)", "label": "(25, MPI, MPI_Bcast(&tmp_val, 1, MPI_INT, 0, fd->comm);)"}
{"predict": "(9,OpenMP,#pragma omp parallel for)", "label": "(7, TBB, tbb::concurrent_unordered_set<int> static_init_time_set;)"}
{"predict": "(77,OpenMP,#pragma omp parallel for private(i,rel,ucol))", "label": "(56, OpenMP, #pragma omp simd)"}
{"predict": "(36,OpenMP,#pragma omp parallel for), (74,OpenMP,#pragma omp parallel for), (81,OpenMP,#pragma omp parallel for), (91,OpenMP,#pragma omp parallel for)", "label": "(132, CUDA, checkCudaErrors(cudaFree(d_idata));); (71, CUDA, checkCudaErrors(cudaMalloc((void **)&d_idata, bytes));); (131, CUDA, checkCudaErrors(cudaFree(d_odata));); (70, CUDA, checkCudaErrors(cudaMalloc((void **)&d_odata, numBlocks * sizeof(T)));)"}
{"predict": "part(A);\n132:         prof.toc(\"partition\");\n133: \n134:         // Partition the matrix:\n135:         A = product(P, A, product(P, P));\n136: \n137:         // Partition the RHS:\n138:         rhs = product(P, rhs);\n139:     }\n140: #endif\n141: \n142:     // Solve the linear system.\n143:     prof.tic(\"solve\");\n144:     Solver solve(prm, bprm);\n145:     auto x = solve(A, rhs);\n146:     prof.toc(\"solve\");\n147: \n148:     // Output the solution:\n149:     if (world.rank == 0) {\n150:         std::cout << \"Solution: \" << x << std::endl;\n151:     }\n152: \n153:     // Finalize the MPI environment.\n154:     amgcl::mpi::finalize();\n155: \n156:     return 0;\n157: }", "label": "(17, MPI, MPI_Barrier(world);)"}
{"predict": "(26,OpenMP,#pragma omp parallel for)", "label": "(11, OpenMP, #pragma omp parallel for num_threads(N_THREADS))"}
{"predict": "beamIdx] * nMSL + i;\n116:             bh.outputIds[dst] = bh.outputIdsCBA[src];\n117:             bh.logProbs[dst] = bh.logProbsCBA[src];\n118:         }\n119:     }\n120: }", "label": "(15, CUDA, extern __shared__ char smem[];)"}
{"predict": "(30,OpenMP,#pragma omp parallel for)", "label": "(31, OpenCL, retVal = clEnqueueNDRangeKernel(pCmdQ, multiDeviceKernel.get(), 1, nullptr, globalWorkSize, nullptr, 0, nullptr, nullptr);); (20, OpenCL, retVal = clEnqueueMemcpyINTEL(pCmdQ, true, compressedDeviceMemAlloc1, bufferData, bufferSize, 0, nullptr, nullptr);); (14, OpenCL, retVal = clEnqueueMemcpyINTEL(pCmdQ, true, compressedDeviceMemAlloc2, bufferData, bufferSize, 0, nullptr, nullptr);); (37, OpenCL, retVal = clEnqueueMemcpyINTEL(pCmdQ, true, uncompressibleHostMemAlloc, compressedDeviceMemAlloc2, bufferSize, 0, nullptr, nullptr);)"}
{"predict": "(13,MPI,rank = MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(77, MPI, mfu_copy_stats.wtime_ended = MPI_Wtime();); (12, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (24, MPI, mfu_copy_stats.wtime_started = MPI_Wtime();); (89, MPI, MPI_Allreduce(values, sums, 3, MPI_INT64_T, MPI_SUM, MPI_COMM_WORLD);); (120, MPI, MPI_Allreduce(&rc, &all_rc, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);)"}
{"predict": "; i < shape.Y_len; ++i) { \\\n153:             y[i] = BIAS + \\\n154:                    static_cast<ST>(rand_r(&seed)) / \\\n155:                    static_cast<ST>(RAND_MAX); \\\n156:         } \\\n157:         \\\n158:         z = static_cast<DT *>(malloc(sizeof(DT) * shape.Z_len)); \\\n159:         \\\n160:         HIE_BINARY_OP(EXT_TYPE, EXT_VAL, X_TENSORFORMAT, Y_TENSORFORMAT, Z_TENSORFORMAT, STD_EXPR); \\\n161:         \\\n162:         for (int i = 0; i < shape.Z_len; ++i) { \\\n163:             ASSERT_EQ(z[i], STD_EXPR); \\\n164:         } \\\n165:         \\\n166:         free(x); \\\n167:         free(y); \\\n168:         free(z); \\\n169:     } \\\n170: }\n171: \n172: UTEST_BINARY_ELEMENTWISE_BROADCAST(\n173:         \"Broadcast_Add_F32\",\n174:         5,\n175:         HIEDNN_BINARY_MATH_ADD,\n176:         0,\n177:         float,\n178:         0.0,\n179:         float,\n180:         float,\n181:         HIEDNN_DATATYPE_FP32,\n182:         HIEDNN_DATATYPE_FP32,\n183:         HIEDNN_DATATYPE_FP32,\n184:         x + y);\n185: UTEST_BINARY_ELEMENTWISE_BROADCAST(\n186:         \"Broadcast_Sub_F32\",\n187:         5,\n188:         HIEDNN_BINARY_MATH_SUB,\n189:         0,\n190:         float,\n191:         0.0,\n192:         float,\n193:         float,\n194:         HIEDNN_DATATYPE_FP32,\n195:         HIEDNN_DATATYPE_FP32,\n196:         HIEDNN_DATATYPE_FP32,\n197:         x - y);\n198: UTEST_BINARY_ELEMENTWISE_B", "label": "(169, CUDA, cudaMemcpy(z_cu, z, shape.Z_len * sizeof(DT), cudaMemcpyHostToDevice);); (261, CUDA, cudaFree(x_cu);); (162, CUDA, cudaMalloc(&x_cu, shape.X_len * sizeof(ST));); (165, CUDA, cudaMemcpy(y_cu, y, shape.Y_len * sizeof(ST), cudaMemcpyHostToDevice);); (163, CUDA, cudaMemcpy(x_cu, x, shape.X_len * sizeof(ST), cudaMemcpyHostToDevice);); (258, CUDA, cudaFree(y_cu);); (160, CUDA, cudaMalloc(&z_cu, shape.Z_len * sizeof(DT));); (158, CUDA, cudaMalloc(&y_cu, shape.Y_len * sizeof(ST));); (168, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (260, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (254, CUDA, cudaFree(z_cu);)"}
{"predict": "(14,MPI,MPI_Init(&argc, &argv);) (16,MPI,MPI_Comm_rank(comm, &rank);) (16,MPI,MPI_Comm_size(comm, &size);) (17,MPI,sbuf = malloc(size * count * sizeof(int));) (18,MPI,rbuf = malloc(size * count * sizeof(int));) (21,MPI,#pragma omp parallel for) (27,MPI,MPI_Info_create(&info);) (28,MPI,MPI_Info_set(info, \"mode\", \"blocking\");) (29,MPI,MPI_Irecv(rbuf, size * count, MPI_INT, rank, 0, comm, &req);) (31,MPI,MPI_Isend(sbuf, size * count, MPI_INT, rank, 0, comm, &req);) (33,MPI,MPI_Wait(&req, MPI_STATUS_IGNORE);) (45,MPI,MPI_Finalize();)", "label": "(36, MPI, MPI_Start(&req);); (13, MPI, MPI_Comm_size(comm, &size);); (28, MPI, MPI_Info_create(&info);); (34, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (48, MPI, MPI_Info_free(&info);); (45, MPI, MPI_Request_free(&req);); (27, MPI, MPI_Alltoall_init(sbuf, count, MPI_INT, rbuf, count, MPI_INT, comm, info, &req);); (8, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(18,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for)", "label": "(30, TBB, result_arena = new tbb::task_arena(tmp);); (18, TBB, result_arena = new tbb::task_arena( arena_args... );); (12, TBB, tbb::task_arena* result_arena = nullptr;); (26, TBB, tbb::task_arena tmp(dummy_max_concurrency, dummy_reserved_for_masters);); (19, TBB, result_arena = new tbb::task_arena;)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(4, TBB, using ch_map = tbb::concurrent_hash_map<int,int>;)"}
{"predict": "(10,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for)", "label": "(32, MPI, rc = MPI_Type_get_true_extent(obj->DTP_datatype, &true_lb, &true_extent);); (22, MPI, rc = MPI_Type_commit(&obj->DTP_datatype);); (32, MPI, rc = MPI_Type_get_extent(obj->DTP_datatype, &lb, &extent);)"}
{"predict": "(14,MPI,MPI_Comm_rank(comm, &rank);) (20,MPI,MPI_Comm_size(comm, &size);) (72,OpenMP,#pragma omp parallel for private(i,status,sendcount,sendtype,recvcount,recvtype,req)) (112,OpenMP,#pragma omp parallel for private(i,status,sendcount,sendtype,recvcount,recvtype,req))", "label": "(73, MPI, err = MPI_Wait(&req, &status);); (13, MPI, MPI_Comm_size(comm, &size);); (34, MPI, MPI_Comm_set_errhandler(comm, MPI_ERRORS_RETURN);); (10, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for)", "label": "(2, OpenMP, #pragma omp parallel); (4, OpenMP, #pragma omp parallel master taskloop)"}
{"predict": "axlstate = NULL;\n146:   if (scr_axl_poststage_active(storedesc)) {\n147:     char* rankfile = spath_strdup(dataset_path);\n148:     spath_append_str(rankfile, \"axlstate\");\n149:     axlstate = spath_strdup(rankfile);\n150:     scr_free(&rankfile);\n151:   }\n152: \n153:   /* initiate AXL transfer */\n154:   int ret = scr_axl_transfer(xfer_type, storedesc, scr_prefix, dataset_path, numfiles, dst_filelist, axlstate);\n155:   if (ret != SCR_SUCCESS) {\n156:     scr_err(\"scr_flush_async_start: AXL transfer failed @ %s:%d\",\n157:       __FILE__, __LINE__\n158:     );\n159:     kvtree_util_set_int(dset_hash, ASYNC_KEY_OUT_STATUS, SCR_FAILURE);\n160:     scr_dataset_delete(&dataset);\n161:     kvtree_delete(&file_list);\n162:     return SCR_FAILURE;\n163:   }\n164: \n165:   /* free dataset and file list */\n166:   scr_dataset_delete(&dataset);\n167:   kvtree_delete(&file_list);\n168: \n169:   /* end timer */\n170:   if (scr_my_rank_world == 0) {\n171:     time_t timestamp_end = scr_log_seconds();\n172:     kvtree_util_set_unsigned_long(dset_hash, ASYNC_KEY_OUT_TIME, (unsigned long)timestamp_end);\n173:     kvtree_util_set_double(dset_hash, ASYNC_KEY_OUT_WTIME, timestamp_end - timestamp_start);\n174: \n175:     /* log the end of the flush */\n176:     if (scr_log_enable) {\n177:       double time_diff = timestamp_end - timestamp_start;\n178:       scr_log_event(\"ASYNC_FLUSH_END\", NULL, &id, dset_name,\n179:                     &timestamp_end, &time_diff);\n180:     }\n181:   }\n182: \n183:   /* free dataset path */\n184:   spath_delete(&dataset", "label": "(24, MPI, MPI_Barrier(scr_comm_world);); (60, MPI, double time_end = MPI_Wtime();); (35, MPI, time_start = MPI_Wtime();)"}
{"predict": "(14,OpenMP,#pragma omp parallel for)", "label": "(9, OpenMP, #pragma omp target data map(tofrom: i) use_device_ptr(k) use_device_addr(i, j)); (13, OpenMP, #pragma omp target data map(tofrom: i) use_device_ptr(z) use_device_addr(i, j, k[:i]))"}
{"predict": "(32,MPI,Sim_readLicenseFile(0);) (33,MPI,Sim_logOn(\"countBL.log\");) (34,MPI,SimParasolid_start(1);) (35,MPI,SimAdvMeshing_start();) (37,MPI,Sim_setMessageHandler(messageHandler);) (38,MPI,pProgress progress = Progress_new();) (39,MPI,Progress_setDefaultCallback(progress);) (41,MPI,pNativeModel nmodel = ParasolidNM_createFromFile(modelFilename, 0);) (42,MPI,pGModel model = GM_load(attribFilename, nmodel, progress);) (44,MPI,pParMesh pmesh = PM_load(meshFilename, model, progress);) (45,MPI,pMesh mesh = PM_mesh(pmesh, 0);) (53,MPI,vIter = M_vertexIter(mesh);) (55,MPI,if(EN_isBLEntity(meshVertex)) counter++;) (56,MPI,VIter_delete(vIter);) (60,MPI,printf(\"rank = %d; there are %d BL entities in this part\\n\", rank, counter);) (62,MPI,if (!rank) PCU_ALWAYS_ASSERT(totalNum >= expectedNum);) (66,MPI,printf(\"there are %d BL entities in total\\n\", totalNum);) (70,MPI,M_release(pmesh);) (71,MPI,GM_release(model);) (72,MPI,NM_release(nmodel);) (74,MPI,cout<<\"**********************************\"<<endl;) (76,MPI,cout<<\"Done!\"<<endl;) (78,MPI,Progress_delete(progress);) (79,MPI,Sim_logOff();) (80,MPI,SimAdvMeshing_stop();) (81,MPI,SimParasolid_stop(1);) (82,MPI,Sim_unregisterAllKeys();) (83,MPI,SimPartitionedMesh_stop();)", "label": "(2, MPI, MPI_Init(&argc, &argv);); (86, MPI, MPI_Finalize();); (23, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&rank);); (61, MPI, MPI_Barrier(MPI_COMM_WORLD);); (59, MPI, MPI_Reduce(&counter, &totalNum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);)"}
{"predict": "(15,OpenMP,#pragma omp parallel), (49,OpenMP,#pragma omp parallel for), (54,OpenMP,#pragma omp parallel for)", "label": "(67, OpenCL, clReleaseMemObject(mem[i]);)"}
{"predict": "::sys::fs::directory_iterator Dir(\n130:                InstallPath + \"/nvvm/libdevice\", EC), End;\n131:            Dir != End && !EC; ++Dir) {\n132:         if (Dir->isRegularFile() && llvm::StringRef(Dir->path()).endswith(\".bc\")) {\n133:           StringRef Arch = llvm::sys::path::stem(Dir->path());\n134:           if (Arch.consume_front(\"libdevice.\"))\n135:             LibDeviceMap[Arch] = Dir->path();\n136:         }\n137:       }\n138:     }\n139: \n140:     if (Version >= CudaVersion::CUDA_90) {\n141:       // CUDA-9+ uses single libdevice file for all GPU variants.\n142:       std::string FilePath = LibDevicePath + \"/libdevice.10.bc\";\n143:       if (FS.exists(FilePath)) {\n144:         for (int Arch = (int)CudaArch::SM_30, E = (int)CudaArch::LAST; Arch < E;\n145:              ++Arch) {\n146:           CudaArch GpuArch = static_cast<CudaArch>(Arch);\n147:           if (!IsNVIDIAGpuArch(GpuArch))\n148:             continue;\n149:           LibDeviceMap[GpuArchName] = FilePath;\n150:         }\n151:       }\n152:     } else {\n153:       std::error_code EC;\n154:       for (llvm::sys::fs::directory_iterator Dir(\n155:                InstallPath + \"/nvvm/libdevice\", EC), End;\n156:            Dir != End && !EC; ++Dir) {\n157:         if (Dir->isRegularFile() && llvm::StringRef(Dir->path()).endswith(\".bc\")) {\n158:           StringRef Arch = llvm::sys::path::stem(Dir->path());\n159:           if (Arch.consume_front(\"libdevice.\"))\n160:             LibDeviceMap[Arch] = Dir->path();\n161:         }\n162:       }\n163:     }\n164:", "label": "(123, CUDA, std::string GpuArchName(CudaArchToString(GpuArch));)"}
{"predict": "(27,OpenMP,#pragma omp parallel for)\n(34,OpenMP,#pragma omp parallel for)\n(43,OpenMP,#pragma omp parallel for)", "label": "(19, CUDA, cudaMallocHost(&tar_host, size * sizeof(float));); (17, CUDA, cudaMallocHost(&src_host, size * sizeof(float));)"}
{"predict": "(4,MPI,*w = MPI_Wtime(););", "label": "(63, OpenMP, #define OP_GOMP_CRITICAL_READ(OP, FLAG))"}
{"predict": "(3,MPI,saved_key = MPI_Wtime();)", "label": "(2, OpenMP, omp_autotune(self, OMP_SCALE);)"}
{"predict": "(4,OpenMP,#pragma omp parallel for collapse(2))", "label": "(3, OpenMP, #pragma omp parallel num_threads(2)); (12, OpenMP, #pragma omp ordered depend(source)); (7, OpenMP, #pragma omp ordered depend(sink : i - 1, j) depend(sink : i, j - 1)); (1, OpenMP, #pragma omp for ordered(2))"}
{"predict": "(4,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for)", "label": "(3, OpenMP, #pragma omp parallel); (3, OpenMP, #pragma omp taskloop)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (...)", "label": "(9, MPI, MPI_Wait(&request, &status);); (6, MPI, MPI_Irecv(p->buff1, p->bufflen, MPI_BYTE, p->prot.nbor, 1, MPI_COMM_WORLD, &request);); (6, MPI, MPI_Send(p->buff, p->bufflen, MPI_BYTE, p->prot.nbor, 1, MPI_COMM_WORLD);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for reduction(+:DataSize)), (20,OpenMP,#pragma omp parallel for reduction(+:DataSize))", "label": "(16, OpenMP, omp_target_associate_ptr(p_a, d_a, DataSize, 0, omp_get_default_device());); (21, OpenMP, omp_target_free(d_a, omp_get_default_device());); (12, OpenMP, d_a = (int *)omp_target_alloc(DataSize, omp_get_default_device());); (17, OpenMP, omp_target_disassociate_ptr(p_a, omp_get_default_device());)"}
{"predict": "(13,MPI,if(par_size > 4) { EXPECT_TRUE(false); })\n(28,OpenMP,#pragma omp parallel for)\n(43,OpenMP,#pragma omp parallel for)", "label": "(50, OpenMP, EXPECT_TRUE(check_test_image(output_file, t_apcomp_baseline_dir()));); (9, MPI, MPI_Comm_rank(comm, &par_rank);); (10, MPI, apcomp::mpi_comm(MPI_Comm_c2f(comm));); (8, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(17,MPI,if (provided != MPI_THREAD_MULTIPLE) { printf(\"This test requires MPI_THREAD_MULTIPLE\\n\"); }), (24,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for), (36,OpenMP,#pragma omp parallel for)", "label": "(27, MPI, MPI_Wait(&request, &status);); (40, MPI, MPI_Waitall(1, &request, &status);); (16, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (31, MPI, MPI_Waitsome(1, &request, &outcount, indices, &status);); (19, MPI, MPI_Grequest_start(query_fn, free_fn, cancel_fn, NULL, &request);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for reduction(+:num_failed))", "label": "(6, OpenMP, omp_set_dynamic(0);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for collapse(2))", "label": "(14, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize());); (12, CUDA, qmc_cuda::cuda_check(cudaGetLastError());)"}
{"predict": "(11,OpenMP,#pragma omp parallel for)", "label": "(6, TBB, tbb::flow::queue_node< q3_input_type >  q3(g);); (7, TBB, tbb::flow::make_edge( q3, my_split );)"}
{"predict": "ange[rand() % 4];\n95: \n96:       ave_diff_subc += TestGemmA16W8_New<half, int8_t>(BS, M, N, K, GroupSize);\n97:     }\n98:     ave_diff_subc /= TestCases;\n99:     printf(\"Volta Fused GEMM Ave_Diff_Subc : %f\\n\", ave_diff_subc);\n100:    EXPECT_EQ(ave_diff_subc <= 5e-1, true);\n101:  }\n102: }\n103: \n104: // CUDA Device Properties\n105: void CUDADeviceProp(int device_id) {\n106:   cudaGetDeviceProperties(&device_prop, device_id);\n107:   int sm_version = (device_prop.major << 8 | device_prop.minor);\n108:   if (sm_version >= 0x0800) {\n109:     printf(\"Ampere+ GEMM kernel\\n\");\n110:   } else if (sm_version == 0x0700) {\n111:     printf(\"Volta GEMM kernel\\n\");\n112:   } else if (sm_version >= 0x0705) {\n113:     printf(\"Turing+ GEMM kernel\\n\");\n114:   }\n115: }\n116: \n117: int main(int argc, char **argv) {\n118:   srand(time(NULL));\n119:   CUDADeviceProp(0);\n120:   return 0;\n121: }", "label": "(3, CUDA, cudaGetDevice(&device_id);); (3, CUDA, cudaGetDeviceProperties(&device_prop, device_id);)"}
{"predict": "(3,MPI,MPI_Init(NULL, NULL);)\n(5,MPI,MPI_Wtime();)\n(8,OpenMP,#pragma omp parallel for)\n(11,MPI,MPI_Finalize();)", "label": "(3, OpenMP, #pragma omp atomic)"}
{"predict": "(9,OpenMP,#pragma omp parallel for collapse(2)), (15,OpenMP,#pragma omp parallel for collapse(2))", "label": "(18, MPI, timers[t] = MPI_Wtime();); (22, MPI, timers[t] = (MPI_Wtime() - timers[t]) / cycles;); (11, MPI, MPI_Pack(sbuf, scount, sdt, packed_buf, outsize, &position, MPI_COMM_WORLD);); (2, MPI, MPI_Type_size(sdt, &outsize);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myself);)"}
{"predict": ".kmp_depend_info, %struct.kmp_depend_info* %{{.+}}, i[[SZ]] 1\n63:   // CHECK:       getelementptr %struct.kmp_depend_info, %struct.kmp_depend_info* %{{.+}}, i[[SZ]] 2\n64:   // CHECK:       getelementptr %struct.kmp_depend_info, %struct.kmp_depend_info* %{{.+}}, i[[SZ]] 3\n65:   // CHECK:       [[DEP:%.+]] = bitcast %struct.kmp_depend_info* %{{.+}} to i8*\n66:   // CHECK:       call void @__kmpc_omp_wait_deps(%struct.ident_t* @{{.*}}, i32 [[GTID]], i32 4, i8* [[DEP]], i32 0, i8* null)\n67:   // CHECK:       call void @__kmpc_omp_target_task_begin_if0(%struct.ident_t* @{{.*}}, i32 [[GTID]], i8* [[TASK]])\n68:   // CHECK:       call i32 [[TASK_ENTRY1_]](i32 [[GTID]], [[TASK_TY1_]]* [[BC_TASK]])\n69:   // CHECK:       call void @__kmpc_omp_target_task_complete_if0(%struct.ident_t* @{{.*}}, i32 [[GTID]], i8* [[TASK]])\n70:   // CHECK:       br label %[[CONTINUE:.+]]\n71:   // CHECK:       [[ELSE]]:\n72:   // CHECK:       br label %[[CONTINUE]]\n73:   // CHECK:       [[CONTINUE]]:\n74:   // CHECK:       ret i32 0\n75: \n76:   return 0;\n77: }\n\n### **Explanation**\n\nThe code snippet provided appears to be a part of a larger program that involves some form of parallel computation. The code is using OpenMP for parallelism, but it's not fully optimized. The task is to identify potential parallelization points and suggest appropriate parallelization instructions.\n\n### **Answer**\n\n(30,OpenMP,#pragma omp parallel for)", "label": "(86, OpenMP, #pragma omp target teams device(global + a) nowait depend(inout); (25, OpenMP, #pragma omp target teams device(global + a) depend(in); (100, OpenMP, #pragma omp target teams if (0) firstprivate(global) depend(out)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(16, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize(), \"batched_dot_wabn_wban\");); (14, CUDA, qmc_cuda::cuda_check(cudaGetLastError(), \"batched_dot_wabn_wban\");)"}
{"predict": "MMA,MMA_M,MMA_N)\n94:   Tensor tCrA = make_fragment_like(tCrC);                              // (MMA,MMA_M,MMA_N)\n95:   Tensor tCrB = make_fragment_like(tCrC);                              // (MMA,MMA_M,MMA_N)\n96: \n97:   // Initialize accumulators to beta * C\n97:   fill(beta, tCrC);\n98:   // Initialize accumulators to beta * C\n99:   fill(beta, tCrA);\n100:   fill(beta, tCrB);\n101: \n102:   // Main loop over k-tiles\n103:   for (int k_tile = 0; k_tile < size<2>(gA); ++k_tile) {\n104:     // Partition A and B for k_tile\n105:     Tensor tAgA_k = tAgA(_,_,_,k_tile);\n106:     Tensor tBgB_k = tBgB(_,_,_,k_tile);\n107:     Tensor tAsA_k = tAsA(_,_,_,k_tile);\n108:     Tensor tBsB_k = tBsB(_,_,_,k_tile);\n109:     Tensor tArA_k = tArA(_,_,_,k_tile);\n110:     Tensor tBrB_k = tBrB(_,_,_,k_tile);\n111: \n112:     // Load A and B from shared memory\n113:     load(tCsA, tAsA_k);\n114:     load(tCsB, tBsB_k);\n115: \n116:     // Compute MMA result and accumulate to C\n117:     mma(tCrA, tCrB, tCsA, tCsB);\n118:     add(tCrC, tCrC, tCrA);\n119:     add(tCrC, tCrC, tCrB);\n120: \n121:     // Write C back to global memory\n122:     store(tCgC, tCrC);\n123:   }\n124: \n125:   // Synchronize threads\n126:   __syncthreads();\n127: \n128:   //", "label": "(48, CUDA, __shared__ TB smemB[cosize_v<BSmemLayout>];); (46, CUDA, __shared__ TA smemA[cosize_v<ASmemLayout>];)"}
{"predict": "(30,MPI,*w = MPI_Wtime();); (34,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp barrier), (56,OpenMP,#pragma omp parallel for), (61,OpenMP,#pragma omp parallel for)", "label": "(13, CUDA, checkCudaErrors(cudaMallocPitch((void **)&src, &DeviceStride, Size.width * sizeof(float), Size.height));); (79, CUDA, checkCudaErrors(cudaFree(src));); (12, CUDA, checkCudaErrors(cudaMallocPitch((void **)&dst, &DeviceStride, Size.width * sizeof(float), Size.height));); (35, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (40, CUDA, getLastCudaError(\"Kernel execution failed\");); (74, CUDA, checkCudaErrors(cudaFree(dst));)"}
{"predict": "(15,OpenMP,#pragma omp parallel for)", "label": "(14, OpenMP, #pragma omp master taskloop shared(c) num_tasks(a))"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (...)\n\n### **Explanation**\n- **MPI**: The first parallelization instruction should be inserted at the beginning of the kernel function to measure the execution time using MPI's `MPI_Wtime()`. This helps in profiling and understanding the performance of the parallelized code.\n- **OpenMP**: The second parallelization instruction should be inserted at line 8 to parallelize the loop over `itr` in the main loop. This will distribute the iterations of the loop among the threads, improving the parallel efficiency.", "label": "(35, CUDA, __shared__ _VOLATILE_ float bSmem[16][128+4];); (33, CUDA, __shared__ _VOLATILE_ float aSmem[16][128+4];)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (12,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for)", "label": "(4, OpenMP, #pragma omp master taskloop simd); (1, OpenMP, #pragma omp parallel)"}
{"predict": "(21,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for)", "label": "(28, CUDA, cudaMalloc((void**)&deviceC, size);); (55, CUDA, cudaFree(deviceB);); (40, CUDA, cudaMemcpy(hostC, deviceC, size, cudaMemcpyDeviceToHost);); (54, CUDA, cudaFree(deviceC);); (51, CUDA, cudaFree(deviceA);); (22, CUDA, cudaMalloc((void**)&deviceB, size);); (25, CUDA, cudaMemcpy(deviceA, hostA, size, cudaMemcpyHostToDevice);); (25, CUDA, cudaMemcpy(deviceB, hostB, size, cudaMemcpyHostToDevice);); (18, CUDA, cudaMalloc((void**)&deviceA, size);)"}
{"predict": "(106, MPI, *error_code = MPI_Iallgather(&vars->start_offset, 1, ADIO_OFFSET, vars->st_offsets, 1, ADIO_OFFSET, fd->comm, &vars->req_offset[0]);) (114, MPI, *error_code = MPI_Iallgather(&vars->end_offset, 1, ADIO_OFFSET, vars->end_offsets, 1, ADIO_OFFSET, fd->comm, &vars->req_offset[1]);)", "label": "(77, MPI, MPI_Comm_size(fd->comm, &nprocs);); (77, MPI, MPI_Comm_rank(fd->comm, &myrank);)"}
{"predict": "(15,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for)", "label": "(11, OpenMP, ref_sched = str2omp_sched(argv[2]);); (13, OpenMP, omp_get_schedule(&sched, &chunk);)"}
{"predict": "(6,MPI,*w = MPI_Wtime();); (11,OpenMP,#pragma omp parallel for), (16,MPI,g_TotalErrors += MPI_Reduce(&local_errors, &g_TotalErrors, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);)", "label": "(7, CUDA, cudaDeviceSynchronize();); (7, CUDA, getLastCudaError(\"launch_kernel failed\");)"}
{"predict": "(26,OpenMP,#pragma omp parallel for reduction(+:timed))", "label": "(6, TBB, tbb::global_control global_limit(tbb::global_control::max_allowed_parallelism, idx);)"}
{"predict": "(labels, label);\n98: \n99:             labels[tc.z * BW * BH + tc.y * BW + tc.x] = label;\n100:         }\n101: \n102:         __syncthreads();\n103: \n104:         if (gc.x < shape.x && gc.y < shape.y && gc.z < shape.z)\n105:         {\n106:             dst[gc] = label;\n107:         }\n108:     }\n109: }", "label": "(5, CUDA, __shared__ DT labels[BW * BH * BD];)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (10,OpenMP,#pragma omp parallel for)", "label": "(9, CUDA, CUDA_CHECK(ctx->device_id, cudaMalloc(&ctx->d_rx_dataset, dataset_size));); (15, CUDA, CUDA_CHECK(ctx->device_id, cudaMalloc(&ctx->d_rx_vm_states, batch_size * 2560));); (4, CUDA, CUDA_CHECK(ctx->device_id, cudaHostGetDevicePointer(&ctx->d_rx_dataset, const_cast<void *>(dataset), 0));); (7, CUDA, CUDA_CHECK(ctx->device_id, cudaMemcpy(ctx->d_rx_dataset, dataset, dataset_size, cudaMemcpyHostToDevice));); (11, CUDA, CUDA_CHECK(ctx->device_id, cudaMalloc(&ctx->d_rx_entropy, batch_size * (128 + 2560)));); (12, CUDA, CUDA_CHECK(ctx->device_id, cudaMalloc(&ctx->d_rx_rounding, batch_size * sizeof(uint32_t)));); (8, CUDA, CUDA_CHECK(ctx->device_id, cudaMalloc(&ctx->d_rx_hashes, batch_size * 64));); (6, CUDA, CUDA_CHECK(ctx->device_id, cudaMalloc(&ctx->d_long_state, ctx->d_scratchpads_size));)"}
{"predict": "(30,OpenMP,#pragma omp parallel for)", "label": "(39, CUDA, cudaFree(res_dev);); (32, CUDA, cudaMalloc(&res_dev, sizeof(float));); (34, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(8,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);) (12,OpenMP,#pragma omp parallel for)", "label": "(16, MPI, MPI_Error_string(err, msg, &len);); (8, MPI, err = MPI_Probe(-80, 1, MPI_COMM_WORLD, &status);); (5, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(40,OpenMP,#pragma omp parallel for reduction(+:err)), (74,OpenMP,#pragma omp parallel for reduction(+:conv)), (104,OpenMP,#pragma omp parallel for reduction(+:chksum))", "label": "(69, OpenMP, #pragma omp target map(tofrom : conv)); (87, OpenMP, elapsed_time = omp_get_wtime() - start_time;); (53, OpenMP, #pragma omp teams distribute parallel for simd); (83, OpenMP, #pragma omp target exit data map(from : xold[0 : Ndim], xnew[0 : Ndim])); (37, OpenMP, start_time = omp_get_wtime();); (43, OpenMP, #pragma omp target enter data map(to : xold[0 : Ndim], xnew[0 : Ndim],); (48, OpenMP, #pragma omp target); (63, OpenMP, #pragma omp teams distribute parallel for simd reduction(+ : conv))"}
{"predict": "(15,OpenMP,#pragma omp parallel for collapse(2))", "label": "(16, CUDA, CUDA_CHECK(cudaGetLastError());)"}
{"predict": "156:     cleanUp(&resourceList);\n157:     return 1;\n158:   }\n159: \n160:   attribute.outputTensorDesc = outputTensorDesc;\n161:   err = cudlaModuleGetAttributes(moduleHandle, CUDLA_OUTPUT_TENSOR_DESCRIPTORS,\n162:                                  &attribute);\n163:   if (err != cudlaSuccess) {\n164:     DPRINTF(\"Error in getting output tensor descriptor = %d\\n\", err);\n165:     cleanUp(&resourceList);\n166:     return 1;\n167:   }\n168: \n169:   // Allocate memory for input and output tensors.\n170:   unsigned char* inputTensorData = (unsigned char*)malloc(\n171:       inputTensorDesc[0].size * numInputTensors);\n172:   unsigned char* outputTensorData = (unsigned char*)malloc(\n173:       outputTensorDesc[0].size * numOutputTensors);\n174: \n175:   if ((inputTensorData == NULL) || (outputTensorData == NULL)) {\n176:     if (inputTensorData != NULL) {\n177:       free(inputTensorData);\n178:       inputTensorData = NULL;\n179:     }\n180: \n181:     if (outputTensorData != NULL) {\n182:       free(outputTensorData);\n183:       outputTensorData = NULL;\n184:     }\n185: \n186:     cleanUp(&resourceList);\n187:     return 1;\n188:   }\n189: \n190:   resourceList.inputTensorData = inputTensorData;\n191:   resourceList.outputTensorData = outputTensorData;\n192: \n193:   // Run the model.\n194:   cudlaExecConfig execConfig;\n195:   memset(&execConfig, 0x00, sizeof(cudlaExecConfig));\n196:   execConfig.stream = stream;\n197:   execConfig.inputTensorDesc = inputTensorDesc;\n198:   execConfig.inputTensorData = inputTensorData;\n199:   execConfig.outputTensorDesc =", "label": "(55, CUDA, result = cudaFree(0);); (56, CUDA, errPtr = cudaGetErrorName(result);); (320, CUDA, cudaFree(outputBufferGPU);); (318, CUDA, cudaFree(inputBufferGPU);); (205, CUDA, result = cudaMalloc(&inputBufferGPU, inputTensorDesc[0].size);); (88, CUDA, result = cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking);); (56, CUDA, result = cudaSetDevice(0);); (211, CUDA, result = cudaMalloc(&outputBufferGPU, outputTensorDesc[0].size);); (324, CUDA, result = cudaStreamDestroy(stream);); (282, CUDA, result = cudaStreamSynchronize(stream);)"}
{"predict": "(87,OpenMP,#pragma omp parallel for)", "label": "(49, CUDA, thrust::host_vector<TB> h_B(n*k);); (4, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;); (1, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (55, CUDA, thrust::device_vector<TC> d_C = h_C;); (46, CUDA, thrust::host_vector<TC> h_C(m*n);); (52, CUDA, thrust::device_vector<TB> d_B = h_B;); (86, CUDA, thrust::host_vector<TC> cute_result = d_C;); (49, CUDA, thrust::device_vector<TA> d_A = h_A;); (40, CUDA, thrust::host_vector<TA> h_A(m*k);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for reduction(+:counter_N0))", "label": "(8, OpenMP, #pragma omp taskloop shared(counter_N0)); (5, OpenMP, #pragma omp target data map(tofrom : counter_N0)); (8, OpenMP, #pragma omp target teams distribute parallel for map(tofrom : counter_N0) nowait); (9, OpenMP, #pragma omp atomic update)"}
{"predict": "0;\n128:   }\n129: \n130:   /* Initialize the array                                                      */\n131:   for (j=0; j<n; j++) for (i=start; i<=end; i++) {\n132:     ARRAY(i-start,j) = i * (j+1);\n133:   }\n134: \n135:   /* Compute the corner value of the array                                       */\n136:   corner_val = (double) (end-start+1)*(n+1);\n137:   corner_val *= corner_val;\n138:   corner_val /= 4.0;\n139: \n140:   /* Start the pipeline algorithm                                                */\n141: \n142:   for (iter = 1; iter <= iterations; iter++) {\n143:     local_pipeline_time = MPI_Wtime();\n144:     for (jj=0; jj<n; jj++) {\n145:       for (i=start; i<=end; i++) {\n146:         ARRAY(i-start,jj) = ARRAY(i-start,jj-1) + ARRAY(i-start,jj) + \n147:                            ARRAY(i-start,jj+1);\n148:       }\n149:     }\n150:     local_pipeline_time = MPI_Wtime() - local_pipeline_time;\n151:     MPI_Reduce(&local_pipeline_time, &pipeline_time, 1, MPI_DOUBLE, MPI_MAX, root, MPI_COMM_WORLD);\n152:     if (my_ID == root) {\n153:       printf(\"Time for one iteration = %f\\n\", pipeline_time);\n154:     }\n155:   }\n156:   \n157:   /* Verify correctness of the results                                         */\n158:   double sum = 0.0;\n159:   for (j=0; j<n; j++) sum += ARRAY(end-start,j);\n160:   sum *= 2.0;\n161:   sum /= n;\n162:   sum -= corner_val;\n163:   sum = fabs(sum);\n164:   if (sum > epsilon) {\n165:     printf(\"ERROR: pipeline verification failed\\n\");\n166:     printf(\"       Sum = %f, Corner", "label": "(177, MPI, MPI_Recv(inbuf, jjsize, MPI_DOUBLE, my_ID-1, j, MPI_COMM_WORLD, &status);); (88, MPI, MPI_Bcast(&m,          1, MPI_INT, root, MPI_COMM_WORLD);); (88, MPI, MPI_Bcast(&n,          1, MPI_INT, root, MPI_COMM_WORLD);); (89, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (241, MPI, MPI_Finalize();); (163, MPI, MPI_Send(&(ARRAY(end,j)), 1, MPI_DOUBLE, my_ID+1, j, MPI_COMM_WORLD);); (195, MPI, MPI_Send(&corner_val,1,MPI_DOUBLE,0,888,MPI_COMM_WORLD);); (197, MPI, MPI_Recv(&(ARRAY(0,0)),1,MPI_DOUBLE,root,888,MPI_COMM_WORLD,&status);); (18, MPI, MPI_Init(&argc,&argv);); (183, MPI, MPI_Send(outbuf, jjsize, MPI_DOUBLE, my_ID+1, j, MPI_COMM_WORLD);); (138, MPI, MPI_Barrier(MPI_COMM_WORLD);); (17, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (79, MPI, MPI_Bcast(&grp,        1, MPI_INT, root, MPI_COMM_WORLD);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(10, TBB, range_t r = utils_t::make_range((1 << (20 / DimAmount - 1)) - DimAmount, false, &MakeInt, oneapi::tbb::detail::make_index_sequence<DimAmount>());); (1, TBB, using range_t = oneapi::tbb::blocked_nd_range<int, DimAmount>;)"}
{"predict": "(22,OpenMP,#pragma omp parallel for private(k,krow,kcol,lk,knsupc,ii,lsum_k,scp,status,tempv))", "label": "(52, MPI, MPI_Recv( recvbuf, knsupc * nrhs, MPI_FLOAT, sender, k, grid3d->zscp.comm, &status );); (42, MPI, MPI_Send( recvbuf, knsupc * nrhs, MPI_FLOAT, receiver, k,  grid3d->zscp.comm);)"}
{"predict": "(33,MPI,MPI_Barrier(MPI_COMM_WORLD);)", "label": "(33, MPI, MPI_Barrier(scr_comm_world);)"}
{"predict": "(32,OpenMP,#pragma omp parallel for collapse(2)), (80,OpenMP,#pragma omp parallel for collapse(2))", "label": "(27, CUDA, cudaGridDependencySynchronize();); (85, CUDA, cudaTriggerProgrammaticLaunchCompletion();)"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(16, OpenCL, context = clCreateContext(nullptr, 1, &device, nullptr, nullptr, &error);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for)", "label": "(24, OpenCL, clReleaseEvent(event);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for), (53,OpenMP,#pragma omp parallel for), (79,OpenMP,#pragma omp parallel for)", "label": "(31, CUDA, __shared__ char loadSmemBuffer[kUpdateKVCacheKernelShmSize];)"}
{"predict": "(18,OpenMP,#pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp teams distribute); (7, OpenMP, #pragma omp parallel for); (8, OpenMP, #pragma omp simd); (1, OpenMP, #pragma omp target map (alloc: u0_real,u0_imag,u1_real,u1_imag,twiddle))"}
{"predict": "(12,MPI,*MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided);)", "label": "(16, MPI, MPI_Abort(MPI_COMM_WORLD, MPIT_result);); (17, MPI, MPIT_result = MPI_T_pvar_session_create(session);); (11, MPI, MPIT_result = MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);)"}
{"predict": "(5,MPI,*w = MPI_Wtime(););", "label": "(19, OpenCL, retVal = clEnqueueUnmapMemObject(cmdQ.get(), buffer.get(), mappedPtr, 1, &clUnmapEvent, nullptr);); (10, OpenCL, void *mappedPtr = clEnqueueMapBuffer(cmdQ.get(), buffer.get(), CL_FALSE, CL_MAP_WRITE, offset, size, 1, &clMapEvent, nullptr, &retVal);)"}
{"predict": "(117,OpenMP,#pragma omp parallel for), (133,OpenMP,#pragma omp parallel for)", "label": "(62, TBB, PRINT( tbb::TBB_runtime_interface_version() );); (68, TBB, while (tracker.get_concurrency() < tbb::task_scheduler_init::default_num_threads()) tbb_pi<double> (N);); (11, TBB, tbb::task_scheduler_init init(tbb::task_scheduler_init::default_num_threads());); (82, TBB, TaskSchedulerTBB::create(0);); (90, TBB, TaskSchedulerTBB::destroy();)"}
{"predict": "(14,OpenMP,#pragma omp parallel for)", "label": "(7, OpenMP, #pragma omp target data map(tofrom: W[:M])); (12, OpenMP, #pragma omp target teams); (12, OpenMP, #pragma omp distribute parallel for)"}
{"predict": "i < nub; i++) {\n103: \t\t\tLlu->fsendx_plist[i] = &index1[i*Pr];\n104: \t\t\tLlu->bsendx_plist[i] = &index1[i*Pr];\n105: \t\t}\n106: \t#endif\n107:     }\n108:     if (grid3d->zscp.Iam)\n109:     {\n110:         /* Llu->Ucb_indptr = SUPERLU_MALLOC (nub * sizeof(Ucb_indptr_t*)); */\n111:         /* Llu->Ucb_valptr = SUPERLU_MALLOC (nub * sizeof(int_t*)); */\n112:         Llu->brecvx_plist = SUPERLU_MALLOC (nub * sizeof(int*));\n113:         Llu->frecvx_plist = SUPERLU_MALLOC (nub * sizeof(int*));\n114: \n115:         #if 0\n116:             for(int lk=0; lk<nub; lk++){\n117:                 Llu->brecvx_plist[lk]=NULL;\n118:                 Llu->frecvx_plist[lk]=NULL;\n119:             }\n120:         #else // need to use a contiguous chunk to allocate frecvx_plist and brecvx_plist on other girds, to be consistent with grid 0. Otherwise dDestroy_LU will crash.\n121:             int_t len = nub * Pr;\n122:             int   *index1;        /* temporary pointer to array of int */\n123:             if ( !(index1 = int32Malloc_dist(len)) )\n124:                 ABORT(\"Malloc fails for frecvx_plist[0]\");\n125:             for (i = 0; i < nub; i++) {\n126:                 Llu->frecvx_plist[i] = &index1[i*Pr];\n127:                 Llu->brecvx_plist[i] = &index1[i*Pr];\n128:             }\n129:         #endif\n130:     }\n131: \n132:     /* allocate Lcb_indptr and Lcb_valptr */\n133:", "label": "(66, MPI, MPI_Bcast( ToRecv, nsupers, MPI_INT, 0,  grid3d->zscp.comm);); (39, MPI, MPI_Bcast( supno, n, mpi_int_t, 0,  grid3d->zscp.comm);); (27, MPI, MPI_Bcast( &nsupers, 1, mpi_int_t, 0,  grid3d->zscp.comm);); (69, MPI, MPI_Bcast( ToSendR[i], Pc, MPI_INT, 0,  grid3d->zscp.comm);); (11, MPI, MPI_Bcast( etree, n, mpi_int_t, 0,  grid3d->zscp.comm);); (233, MPI, MPI_Bcast(*((void**)&(Llu->fsendx_plist[lk])), Pr * sizeof (int), MPI_BYTE, 0, grid3d->zscp.comm);); (84, MPI, MPI_Bcast( &(Llu->ldalsum), 1, mpi_int_t, 0,  grid3d->zscp.comm);); (61, MPI, MPI_Bcast( ToSendD, nbr, MPI_INT, 0,  grid3d->zscp.comm);); (29, MPI, MPI_Bcast( xsup, nsupers + 1, mpi_int_t, 0,  grid3d->zscp.comm);); (80, MPI, MPI_Bcast( &(Llu->nbsendx), 1, MPI_INT, 0,  grid3d->zscp.comm);); (78, MPI, MPI_Bcast( &(Llu->nfsendx), 1, MPI_INT, 0,  grid3d->zscp.comm);); (45, MPI, MPI_Bcast( bufmax, NBUFFERS, mpi_int_t, 0,  grid3d->zscp.comm);)"}
{"predict": "(5,MPI,*w = MPI_Wtime(););", "label": "(16, OpenCL, retVal = clEnqueueUnmapMemObject(cmdQ.get(), image.get(), mappedPtr, 0, nullptr, nullptr);); (8, OpenCL, void *mappedPtr = clEnqueueMapImage(cmdQ.get(), image.get(), CL_TRUE, CL_MAP_READ, &origin[0], &region[0], nullptr, nullptr, 0, nullptr, nullptr, &retVal);)"}
{"predict": "(13,OpenMP,#pragma omp parallel sections), (26,OpenMP,#pragma omp parallel sections), (39,OpenMP,#pragma omp parallel sections)", "label": "(13, OpenMP, #pragma omp section); (10, OpenMP, #pragma omp parallel); (21, OpenMP, #pragma omp critical)"}
{"predict": "(32,MPI,*w = MPI_Wtime();); (34,OpenMP,#pragma omp parallel for), (44,MPI,*w = MPI_Wtime();); (46,OpenMP,#pragma omp parallel for)", "label": "(57, MPI, MPI_Finalize();); (10, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(27,OpenMP,#pragma omp parallel for collapse(3))", "label": "(17, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (14, CUDA, cudaMalloc((void**)(&d_in1), in1_bytes);); (29, CUDA, assert(cudaMemcpyAsync(out.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);); (16, CUDA, cudaMemcpy(d_in1, in1.data(), in1_bytes, cudaMemcpyHostToDevice);); (40, CUDA, cudaFree(d_out);); (38, CUDA, cudaFree(d_in2);); (26, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (35, CUDA, cudaFree(d_in1);); (8, CUDA, cudaMalloc((void**)(&d_in2), in2_bytes);); (11, CUDA, cudaMemcpy(d_in2, in2.data(), in2_bytes, cudaMemcpyHostToDevice);)"}
{"predict": "(26,MPI,*w = MPI_Wtime();); (29,OpenMP,#pragma omp parallel for), (32,MPI,sec = *w - start;);", "label": "(27, CUDA, cudaEventRecord(start);); (31, CUDA, cudaEventRecord(end);); (31, CUDA, cudaEventSynchronize(end);); (23, CUDA, cudaEventCreate(&end);); (21, CUDA, cudaEventCreate(&start);); (31, CUDA, cudaEventElapsedTime(&msec, start, end);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for reduction(+:sum))", "label": "(13, CUDA, cudaMalloc((void **)&d_arr, sizeof(int) * len);); (16, CUDA, cudaMemcpy(d_arr, arr, sizeof(int) * len, cudaMemcpyHostToDevice);); (53, CUDA, cudaFree(d_out);); (11, CUDA, cudaMalloc((void **)&d_out, sizeof(int) * len);); (23, CUDA, cudaMemcpy(out, d_out, sizeof(int) * len, cudaMemcpyDeviceToHost);); (49, CUDA, cudaFree(d_arr);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for)", "label": "(11, OpenMP, #pragma omp distribute); (16, OpenMP, #pragma omp parallel for); (13, OpenMP, threads[j] = omp_get_num_threads();); (7, OpenMP, #pragma omp target teams num_teams(numTeams) thread_limit(1024) map (tofrom: CUs, lims, threads, Res)); (16, OpenMP, CUs[i] = omp_ext_get_smid();); (9, OpenMP, lims[j] = omp_get_num_teams();)"}
{"predict": "(10,MPI,if (rc != MPI_SUCCESS) {); (18,MPI,if (rc != MPI_SUCCESS) {); (25,MPI,vval = *(int *) v;);", "label": "(19, MPI, rc = MPI_Comm_size(MPI_COMM_WORLD, &size);); (8, MPI, rc = MPI_Comm_get_attr(MPI_COMM_WORLD, MPI_UNIVERSE_SIZE, &v, &flag);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(4, CUDA, cudaError_t result = cudaSetDevice(i);); (6, CUDA, nvDebug(\"*** CUDA Error: %s\\n\", cudaGetErrorString(result));)"}
{"predict": "; j++) {\n130:     for (i=0; i<segment_size; i++) {\n131:       vector[j*segment_size+i] = 0.0;\n132:     }\n133:   }\n134: \n135:   /* initialize the boundary conditions                                          */\n136:   for (j=0; j<n; j++) {\n137:     vector[j*segment_size] = 1.0;\n138:   }\n139: \n140:   /* set up the pipeline buffers                                                 */\n141:   local_pipeline_time = MPI_Wtime();\n142:   for (jj=0; jj<segment_size; jj++) {\n143:     for (j=0; j<n; j++) {\n144:       inbuf[jj] = vector[j*segment_size+jj];\n145:     }\n146:     for (j=0; j<n; j++) {\n147:       vector[j*segment_size+jj] = 2.0 * inbuf[jj];\n148:     }\n149:   }\n150:   local_pipeline_time = MPI_Wtime() - local_pipeline_time;\n151: \n152:   /* run the pipeline algorithm                                                  */\n153:   for (iter=0; iter<iterations; iter++) {\n154:     local_pipeline_time = MPI_Wtime();\n155:     for (jj=0; jj<segment_size; jj++) {\n156:       for (j=0; j<n; j++) {\n157:         inbuf[jj] = vector[j*segment_size+jj];\n158:       }\n159:       for (j=0; j<n; j++) {\n160:         vector[j*segment_size+jj] = 2.0 * inbuf[jj];\n161:       }\n162:     }\n163:     local_pipeline_time = MPI_Wtime() - local_pipeline_time;\n164:   }\n165: \n166:   /* verify correctness of results                                               */\n167:   if (my_ID == root) {\n168:     corner_val = vector[(m-1)*segment_size+n-1];\n169:     if (", "label": "(180, MPI, MPI_Recv(inbuf, jjsize, MPI_DOUBLE, my_ID-1, j, MPI_COMM_WORLD, &status);); (91, MPI, MPI_Bcast(&m,          1, MPI_INT, root, MPI_COMM_WORLD);); (91, MPI, MPI_Bcast(&n,          1, MPI_INT, root, MPI_COMM_WORLD);); (92, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (244, MPI, MPI_Finalize();); (166, MPI, MPI_Send(&(ARRAY(end,j)), 1, MPI_DOUBLE, my_ID+1, j, MPI_COMM_WORLD);); (198, MPI, MPI_Send(&corner_val,1,MPI_DOUBLE,0,888,MPI_COMM_WORLD);); (200, MPI, MPI_Recv(&(ARRAY(0,0)),1,MPI_DOUBLE,root,888,MPI_COMM_WORLD,&status);); (19, MPI, MPI_Init(&argc,&argv);); (186, MPI, MPI_Send(outbuf, jjsize, MPI_DOUBLE, my_ID+1, j, MPI_COMM_WORLD);); (141, MPI, MPI_Barrier(MPI_COMM_WORLD);); (18, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (82, MPI, MPI_Bcast(&grp,        1, MPI_INT, root, MPI_COMM_WORLD);); (15, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for)", "label": "(33, CUDA, CUDA_CHECK(cudaFree(d_work));); (14, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_work), workspaceInBytesOnDevice));)"}
{"predict": "* array_size};\n122:   for (unsigned int k = 0; k < 6; k++) {\n123:     double min_time = *std::min_element(timings[k].begin(), timings[k].end());\n124:     double max_time = *std::max_element(timings[k].begin(), timings[k].end());\n125:     double sum_time = std::accumulate(timings[k].begin(), timings[k].end(),\n126:                                       0.0);\n127:     double avg_time = sum_time / repeat_num_times;\n128:     double avg_bandwidth = sizes[k] / (avg_time * 1024 * 1024);\n129:     double best_bandwidth = sizes[k] / (min_time * 1024 * 1024);\n130:     std::cout << std::left << std::setw(12) << labels[k] << std::left\n131:               << std::setw(12) << best_bandwidth << std::left << std::setw(12)\n132:               << min_time << std::left << std::setw(12) << max_time << std::left\n133:               << std::setw(12) << avg_time << std::left << std::setw(12)\n134:               << avg_bandwidth << std::endl;\n135:   }\n136: }\n\n### ** Answer **", "label": "(12, OpenMP, #pragma omp target enter data map(alloc); (4, OpenMP, #pragma omp target map(tofrom : warp_size)); (135, OpenMP, #pragma omp target exit data map(release)"}
{"predict": "(14,MPI,*w = MPI_Wtime();)", "label": "(50, OpenCL, bufSA = clCreateBuffer(ctx, CL_MEM_READ_WRITE, sizeof(cl_float), NULL, &err);); (52, OpenCL, bufS  = clCreateBuffer(ctx, CL_MEM_READ_WRITE, sizeof(cl_float), NULL, &err);); (67, OpenCL, err = clWaitForEvents(1, &event);); (55, OpenCL, err = clEnqueueWriteBuffer(queue, bufS, CL_TRUE, 0, sizeof(cl_float), &S, 0, NULL, NULL);); (83, OpenCL, clReleaseMemObject(bufC);); (77, OpenCL, clReleaseEvent(event);); (79, OpenCL, clReleaseMemObject(bufSA);); (68, OpenCL, err = clEnqueueReadBuffer(queue, bufS, CL_TRUE, 0, sizeof(cl_float), &S, 0, NULL, NULL);); (28, OpenCL, clReleaseContext(ctx);); (63, OpenCL, err = clEnqueueReadBuffer(queue, bufSA, CL_TRUE, 0, sizeof(cl_float), &SA, 0, NULL, NULL);); (47, OpenCL, err = clEnqueueWriteBuffer(queue, bufC, CL_TRUE, 0, sizeof(cl_float), &C, 0, NULL, NULL);); (62, OpenCL, err = clEnqueueReadBuffer(queue, bufSB, CL_TRUE, 0, sizeof(cl_float), &SB, 0, NULL, NULL);); (32, OpenCL, clReleaseCommandQueue(queue);); (38, OpenCL, bufSB = clCreateBuffer(ctx, CL_MEM_READ_WRITE, sizeof(cl_float), NULL, &err);); (74, OpenCL, clReleaseMemObject(bufS);); (71, OpenCL, clReleaseMemObject(bufSB);); (19, OpenCL, printf( \"clCreateCommandQueue() failed with %d\\n\", err );); (16, OpenCL, queue = clCreateCommandQueue(ctx, device, 0, &err);); (34, OpenCL, bufC  = clCreateBuffer(ctx, CL_MEM_READ_WRITE, sizeof(cl_float), NULL, &err);); (8, OpenCL, ctx = clCreateContext(props, 1, &device, NULL, NULL, &err);); (9, OpenCL, printf( \"clCreateContext() failed with %d\\n\", err );); (34, OpenCL, err = clEnqueueWriteBuffer(queue, bufSA, CL_TRUE, 0, sizeof(cl_float), &SA, 0, NULL, NULL);); (34, OpenCL, err = clEnqueueWriteBuffer(queue, bufSB, CL_TRUE, 0, sizeof(cl_float), &SB, 0, NULL, NULL);); (51, OpenCL, err = clEnqueueReadBuffer(queue, bufC, CL_TRUE, 0, sizeof(cl_float), &C, 0, NULL, NULL);)"}
{"predict": "hTimer);\n151: \n152:   if (time < 1e-6) {\n153:     std::cout << \"Gsamples/s: problem size too small to measure, try \"\n154:                  \"increasing number of vectors or dimensions\"\n155:               << std::endl;\n156:   } else {\n157:     std::cout << \"Gsamples/s: \"\n158:               << (double)n_vectors * (double)n_dimensions * 1E-9 / (time * 1E-3)\n159:               << std::endl;\n160:   }\n161: \n162:   std::cout << \"Checking results...\" << std::endl;\n163:   if (checkResults(n_vectors, n_dimensions, h_outputCPU, h_outputGPU)) {\n164:     std::cout << \"Results are correct!\" << std::endl;\n165:   } else {\n166:     std::cout << \"Results are incorrect!\" << std::endl;\n167:   }\n168: \n169:   // Free the GPU memory\n170:   cudaFree(d_directions);\n171:   cudaFree(d_output);\n172: \n173:   // Free the CPU memory\n174:   delete[] h_directions;\n175:   delete[] h_outputCPU;\n176:   delete[] h_outputGPU;\n177: \n178:   // Destroy the timer\n179:   sdkDestroyTimer(&hTimer);\n180: \n181:   return 0;\n182: }", "label": "(93, CUDA, throw std::runtime_error(cudaGetErrorString(cudaResult));); (57, CUDA, findCudaDevice(argc, (const char **)argv);); (117, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (211, CUDA, checkCudaErrors(cudaFree(d_output));); (209, CUDA, checkCudaErrors(cudaFree(d_directions));)"}
{"predict": "(9,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for), (47,OpenMP,#pragma omp parallel for), (61,OpenMP,#pragma omp parallel for), (76,OpenMP,#pragma omp parallel for)", "label": "(82, OpenMP, #pragma omp parallel private(i)); (95, OpenMP, #pragma omp distribute parallel for private(m)); (5, OpenMP, #pragma omp target); (26, OpenMP, #pragma omp distribute parallel for private(argc,); (60, OpenMP, #pragma omp distribute parallel for private(h, B::x)); (64, OpenMP, #pragma omp distribute parallel for nowait); (8, OpenMP, #pragma omp distribute parallel for private(); (65, OpenMP, #pragma omp parallel); (16, OpenMP, #pragma omp distribute parallel for private(argc); (40, OpenMP, #pragma omp distribute parallel for private(a, b)); (49, OpenMP, #pragma omp distribute parallel for private(e, g)); (33, OpenMP, #pragma omp distribute parallel for private(S1)); (42, OpenMP, #pragma omp distribute parallel for private(argv[1])); (6, OpenMP, #pragma omp distribute parallel for private()); (-5, OpenMP, #pragma omp distribute parallel for private); (62, OpenMP, #pragma omp distribute parallel for private(i)); (69, OpenMP, #pragma omp distribute parallel for private(j)); (22, OpenMP, #pragma omp distribute parallel for private(argc)); (16, OpenMP, #pragma omp distribute parallel for private(argc > 0 ? argv[1] : argv[2])); (62, OpenMP, #pragma omp parallel shared(i)); (-12, OpenMP, #pragma omp teams)"}
{"predict": "(4,OpenMP,#pragma omp parallel for), (8,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp target parallel for device ()); (23, OpenMP, #pragma omp target parallel for device (3.14)); (18, OpenMP, #pragma omp target parallel for device (-2)); (13, OpenMP, #pragma omp target parallel for device (argc), device (argc+1)); (14, OpenMP, #pragma omp target parallel for device (S1)); (5, OpenMP, #pragma omp target parallel for device (argc))); (-4, OpenMP, #pragma omp target parallel for device); (1, OpenMP, #pragma omp target parallel for device (argc); (14, OpenMP, #pragma omp target parallel for device (-10u)); (5, OpenMP, #pragma omp target parallel for device (z + argc)); (-6, OpenMP, #pragma omp target parallel for device (); (1, OpenMP, #pragma omp target parallel for device (argc > 0 ? argv[1] : argv[2]))"}
{"predict": "(6,OpenMP,#pragma omp parallel for), (9,OpenMP,#pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp target map(from:c))"}
{"predict": "< nub; i+=NWARP){\n127:                 idx_i = lloc[i];\n128:                 idx_v = lloc[i+1];\n129:                 temp1 = lusup[idx_v];\n130:                 x[idx_i] -= temp1*x[idx_v];\n131:             }\n132:             if(lne==0){  /*only the first thread in a warp handles the lock */\n133:                 bmod[lib*aln_i]=bmod[lk*aln_i];\n134:                 __threadfence();\n135:             }\n136:             #ifdef HAVE_CUDA\n137:             __syncwarp();\n138:             #endif\n139:             // printf(\"  bfa kernel:   %i %i %i %i %i %i %i %i %i %i\\n\", threadIdx_x, bid, grid->npcol, nsupers,myrow,krow,wrp,k,uind_br[0],bmod[lib*aln_i]);\n140:           }\n141:         }\n142:       }\n143:   }", "label": "(35, CUDA, volatile __shared__ int s_bmod;); (32, CUDA, __shared__ double s_lsum[MAXSUPER];)"}
{"predict": "(10,MPI,*w = MPI_Wtime();); (44,OpenMP,#pragma omp parallel for), (...)", "label": "(47, MPI, MPI_Ibcast(&(prg->keep_going), 1, MPI_INT, 0, prg->comm, &(prg->bcast_req));); (15, MPI, MPI_Comm_dup(comm, &prg->comm);); (37, MPI, prg->time_start = MPI_Wtime();); (42, MPI, MPI_Comm_rank(prg->comm, &rank);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (43,OpenMP,#pragma omp parallel for)", "label": "(28, MPI, MPI_DATATYPE_ALLOC(uval_new, lenv);); (25, MPI, MPI_INT_ALLOC(usub_new, lens);); (59, MPI, MPI_INT_ALLOC(lsub_new, len1);); (60, MPI, MPI_DATATYPE_ALLOC(lnzval_new, len2);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for)", "label": "(28, CUDA, getLastCudaError(\"mergeRanksAndIndicesKernel(B)<<<>>> failed\\n\");); (18, CUDA, getLastCudaError(\"mergeRanksAndIndicesKernel(A)<<<>>> failed\\n\");)"}
{"predict": "(15,OpenMP,#pragma omp parallel for)", "label": "(9, CUDA, cudaFuncGetAttributes(&attr,(void*)ptr);); (14, CUDA, fcuda::Check_CudaErroorFun(\"Error collecting kernel information.\");); (5, CUDA, cudaOccupancyMaxPotentialBlockSize(&mingridsize,&qblocksize,(void*)ptr,0,0);)"}
{"predict": "(40,OpenMP,#pragma omp parallel for private(wakeup)), (44,OpenMP,#pragma omp for private(wakeup)), (50,OpenMP,#pragma omp for private(wakeup))", "label": "(20, TBB, tbb::global_control control(tbb::global_control::max_allowed_parallelism, n_threads);); (1, TBB, n_threads = tbb::this_task_arena::max_concurrency();); (29, TBB, tbb::enumerable_thread_specific<tbb::task_arena*> tls;); (25, TBB, std::vector<tbb::task_arena> arenas(n_arenas);)"}
{"predict": "(18,OpenMP,#pragma omp parallel for), (60,OpenMP,#pragma omp parallel for)", "label": "(15, OpenCL, int error = clGetDeviceInfo( device, CL_DEVICE_IMAGE2D_MAX_WIDTH, sizeof( maxWidth ), &maxWidth, NULL );); (16, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_GLOBAL_MEM_SIZE, sizeof( memSize ), &memSize, NULL );); (14, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_MAX_MEM_ALLOC_SIZE, sizeof( maxAllocSize ), &maxAllocSize, NULL );)"}
{"predict": "(35,MPI,MPI_Comm_spawn((char *) \"./disconnect3\", MPI_ARGV_NULL, np, MPI_INFO_NULL, 0, MPI_COMM_WORLD, &intercomm, MPI_ERRCODES_IGNORE);), (63,MPI,MPI_Comm_disconnect(&intercomm);)", "label": "(36, MPI, MPI_Comm_size(intercomm, &size);); (56, MPI, MPI_Comm_disconnect(&intercomm);); (102, MPI, MPI_Finalize();); (19, MPI, MPI_Comm_get_parent(&parentcomm);); (47, MPI, MPI_Recv(&data, 1, MPI_INT, RECEIVER_RANK, DATA_TAG, intercomm, &status);); (79, MPI, MPI_Abort(intercomm, 1);); (73, MPI, MPI_Recv(&i, 1, MPI_INT, SENDER_RANK, DATA_TAG, intercomm, &status);); (43, MPI, MPI_Send(&i, 1, MPI_INT, RECEIVER_RANK, DATA_TAG, intercomm);); (29, MPI, MPI_Comm_rank(intercomm, &rank);); (26, MPI, MPI_Comm_remote_size(intercomm, &rsize);); (76, MPI, MPI_Send(&i, 1, MPI_INT, SENDER_RANK, DATA_TAG, intercomm);)"}
{"predict": "of stencil        = Compact\\n\");\n116:   }\n117: \n118:   /* allocate buffers for communication */\n119:   top_buf_out  = (DTYPE *) malloc(sizeof(DTYPE) * n);\n120:   top_buf_in   = (DTYPE *) malloc(sizeof(DTYPE) * n);\n121:   bottom_buf_out = (DTYPE *) malloc(sizeof(DTYPE) * n);\n122:   bottom_buf_in  = (DTYPE *) malloc(sizeof(DTYPE) * n);\n123:   right_buf_out = (DTYPE *) malloc(sizeof(DTYPE) * n);\n124:   right_buf_in  = (DTYPE *) malloc(sizeof(DTYPE) * n);\n125:   left_buf_out  = (DTYPE *) malloc(sizeof(DTYPE) * n);\n126:   left_buf_in   = (DTYPE *) malloc(sizeof(DTYPE) * n);\n127: \n128:   /* initialize input grid values */\n129:   in  = (DTYPE *) malloc(sizeof(DTYPE) * nsquare);\n130:   out = (DTYPE *) malloc(sizeof(DTYPE) * nsquare);\n131: \n132:   if (my_ID == root) {\n133:     initialize(in, nsquare, n);\n134:   }\n135: \n136:   /* broadcast the input data */\n137:   MPI_Bcast(in, nsquare, MPI_DTYPE, root, MPI_COMM_WORLD);\n138: \n139:   /* compute the size of the local tile and the bounds */\n140:   compute_tile_bounds(my_ID, n, Num_procsx, Num_procsy, &istart, &iend, &jstart, &jend);\n141: \n142:   /* allocate local tile */\n143:   total_length_in  = n*n;\n144:   total_length_out = n*n;\n145:   local_length_in  = (iend-istart+1)*(jend-jstart+1);\n146:   local_length_out = local_length_in;\n147:   in_tile  = (DTYPE *) malloc(sizeof(DTYPE) * local_length_in);\n148:   out_tile = (DTYPE *) malloc(sizeof(D", "label": "(295, MPI, MPI_Wait(&(request[1+4]), MPI_STATUS_IGNORE);); (301, MPI, MPI_Wait(&(request[3+4]), MPI_STATUS_IGNORE);); (234, MPI, MPI_Barrier(MPI_COMM_WORLD);); (128, MPI, MPI_Bcast(&n,          1, MPI_INT, root, MPI_COMM_WORLD);); (43, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (289, MPI, MPI_Wait(&(request[0+4]), MPI_STATUS_IGNORE);); (254, MPI, MPI_Wait(&(request[0]), MPI_STATUS_IGNORE);); (294, MPI, MPI_Wait(&(request[2+4]), MPI_STATUS_IGNORE);); (124, MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);); (36, MPI, MPI_Init(&argc,&argv);); (251, MPI, MPI_Wait(&(request[1]), MPI_STATUS_IGNORE);); (256, MPI, MPI_Wait(&(request[2]), MPI_STATUS_IGNORE);); (365, MPI, MPI_Finalize();); (255, MPI, MPI_Wait(&(request[3]), MPI_STATUS_IGNORE);); (324, MPI, MPI_Reduce(&local_norm, &norm, 1, MPI_DTYPE, MPI_SUM, root, MPI_COMM_WORLD);); (31, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for)", "label": "(6, CUDA, extent = make_cudaExtent(30, 40, 0);); (3, CUDA, cudaExtent extent = make_cudaExtent(10, 20, 0);)"}
{"predict": "(14,MPI,MPI_Comm cart;); (24,OpenMP,#pragma omp parallel for), (...)", "label": "(13, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (37, MPI, MPI_Comm_free(&cart);); (12, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);)"}
{"predict": "(8,MPI,MPI_Barrier(comm, MPI_COMM_WORLD);)", "label": "(20, MPI, MPI_Recv(buff, count, type, 0, tag, comm, MPI_STATUS_IGNORE);); (4, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(44,MPI,*w = MPI_Wtime();); (76,OpenMP,#pragma omp parallel for), (98,OpenMP,#pragma omp parallel for)", "label": "(55, OpenMP, #pragma omp for schedule(guided, 512)); (52, OpenMP, #pragma omp parallel firstprivate(transformation_matrix, constant_vector, slave_solution_vector, master_solution_vector, p_master_node, p_slave_node))"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(24, MPI, MPI_Request_free(&request);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (15, MPI, MPI_Psend_init(&a, 1, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, MPI_INFO_NULL, &request);); (16, MPI, MPI_Precv_init(&a, 1, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_INFO_NULL, &request);); (21, MPI, MPI_Barrier(MPI_COMM_WORLD);); (6, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": ":          *   (0, 100, 1000) (0, 100, 1000) (0, 100, 1000)\n126:          * (0, 200, 2000) (0, 200, 2000) (0, 200, 2000)\n127:          * where (0, 200, 2000) and (0, 100, 1000) are for the same process.\n128:          * For this, I will have to use a more complicated scheme.\n129:          * The complication comes from the fact that I cannot assume\n130:          * that the filetypes are always the same. So I need to keep\n131:          * track of the current process number and the current off-len\n132:          * pair for each process in others_req. This can be done using\n133:          * an array of ADIOI_Access structs (curr_offlen_ptr) where each\n134:          * struct contains the process number and the current off-len\n135:          * pair for that process. The ADIOI_Access struct is defined in\n136:          * adioi_file.h. */\n137: \n138:         rank = 0;\n139:         while (rank < nprocs) {\n140:             if (curr_offlen_ptr[rank] >= ntimes) {\n141:                 rank++;\n142:                 continue;\n143:             }\n144:             if (curr_offlen_ptr[rank] < ntimes) {\n145:                 curr_offlen_ptr[rank] = 0;\n146:             }\n147:             if (curr_offlen_ptr[rank] >= ntimes) {\n148:                 rank++;\n149:                 continue;\n150:             }\n151:             if (curr_offlen_ptr[rank] < ntimes) {\n152:                 curr_offlen_ptr[rank] = 0;\n153:             }\n154:             if (curr_offlen_ptr[rank] >= ntimes", "label": "(113, MPI, MPI_Comm_rank(fd->comm, &rank);); (180, MPI, MPI_Get_count(&read_status, MPI_BYTE, &actual_size);); (105, MPI, MPI_Type_get_extent(datatype, &lb, &buftype_extent);); (69, MPI, MPI_Allreduce(&ntimes, &max_ntimes, 1, MPI_INT, MPI_MAX, fd->comm);); (219, MPI, MPI_Get_address(read_buf + req_off - real_off, &addr);)"}
{"predict": "(10,MPI,*w = MPI_Wtime();)\n(12,OpenMP,#pragma omp parallel for private(sum1,sum2,x,Mhu_l,Mhu_m,Mhu_a,k,tauR))", "label": "(16, OpenMP, #pragma omp target teams distribute parallel for thread_limit(128)); (8, OpenMP, #pragma omp target enter data map(to: Vm[:nCells])); (6, OpenMP, #pragma omp target enter data map(to: m_gate[:nCells]))"}
{"predict": "(7,OpenMP,#pragma omp parallel for reduction(+:errs))", "label": "(6, MPI, MPI_Win_get_attr(win, key[i], &val_p, &flag);)"}
{"predict": "(22,MPI,*deviceTransformCoeff = (int*)malloc(sizeof(int)*nOutputDims*comm_size);); (23,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for)", "label": "(23, CUDA, cudaMemcpy(output, data, copySize, cudaMemcpyDeviceToDevice);); (20, CUDA, cudaMemcpy(workspace, transformCoeff, sizeof(int)*nOutputDims,cudaMemcpyHostToDevice );)"}
{"predict": "(9,MPI,MPI_Init(&argc, &argv);) (18,OpenMP,#pragma omp parallel for) (22,OpenMP,#pragma omp parallel for) (30,OpenMP,#pragma omp parallel for) (35,OpenMP,#pragma omp parallel for)", "label": "(19, MPI, MPI_Type_commit(&mystruct);); (27, MPI, MPI_Type_create_hvector(0, 1, stride, MPI_INT, &vecs[i]);); (19, MPI, MPI_Type_free(&mystruct);); (8, MPI, MPI_Type_commit(&vecs[i]);); (14, MPI, MPI_Type_create_struct(3, blockcount, displs, vecs, &mystruct);); (5, MPI, MPI_Type_create_hvector(i, 1, stride, MPI_INT, &vecs[i]);); (17, MPI, MPI_Type_free(&vecs[i]);)"}
{"predict": "(21,OpenMP,#pragma omp parallel for)", "label": "(14, CUDA, checkCudaErrors(cudaMalloc(&info, sizeof(int)));); (14, CUDA, checkCudaErrors(cudaMalloc(&buffer, sizeof(double) * bufferSize));); (14, CUDA, checkCudaErrors(cudaMalloc(&A, sizeof(double) * lda * n));); (36, CUDA, checkCudaErrors(cusolverDnDpotrs(handle, uplo, n, 1, A, lda, x, n, info));); (37, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (43, CUDA, checkCudaErrors(cudaFree(info));); (45, CUDA, checkCudaErrors(cudaFree(buffer));); (14, CUDA, checkCudaErrors(cudaMemset(info, 0, sizeof(int)));); (46, CUDA, checkCudaErrors(cudaFree(A));)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (12,MPI,*w = MPI_Wtime(););", "label": "(12, OpenMP, #pragma omp target map(tofrom: N) map(from:tmp)); (2, OpenMP, #pragma omp target map(to: N) map(tofrom: tmp))"}
{"predict": "(4,MPI,int rank = MPI_Comm_rank(MPI_COMM_WORLD, &rank);)\n(6,MPI,MPI_Comm_size(MPI_COMM_WORLD, &comm_size);)\n(28,OpenMP,#pragma omp parallel for reduction(+:status))", "label": "(4, MPI, MPI_Init(&argc, &argv);); (52, MPI, MPI_Finalize();); (6, MPI, ierr = MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, int ierr = MPI_Comm_size(MPI_COMM_WORLD, &comm_size);)"}
{"predict": "Float2HalfKernel <<< (len - 1) / threadPerBlock + 1, threadPerBlock >>>(cudaOutput, cudaFp16Output,\n97:                                                                                           len);\n98:     }\n99:     return true;\n100: }", "label": "(119, CUDA, FastllmCudaFinishOutput(output, cudaOutput);); (39, CUDA, cudaFp16Input = (half *) FastllmCudaMalloc(n * m * sizeof(half));); (21, CUDA, state = cudaMemset(cudaBiasData, 0, k * sizeof(float));); (115, CUDA, FastllmCudaFinishInput(input, cudaInput);); (43, CUDA, cudaFp16Output = (half *) FastllmCudaMalloc(n * k * sizeof(half));); (9, CUDA, state = cudaMemcpy(cudaMins, mins, k * sizeof(float), cudaMemcpyHostToDevice);); (-2, CUDA, state = cudaMalloc(&cudaScales, k * sizeof(float));); (94, CUDA, FastllmCudaFree(cudaFp16Input);); (103, CUDA, FastllmCudaFree(cudaFp16Output);); (32, CUDA, cudaFp16Weight = (half *) FastllmCudaMalloc(k * m * sizeof(half));); (-1, CUDA, state = cudaMalloc(&cudaMins, k * sizeof(float));); (-6, CUDA, state = cudaMemcpy(cudaScales, weight.scales.data(), k * sizeof(float), cudaMemcpyHostToDevice);); (7, CUDA, state = cudaMalloc(&cudaBiasData, k * sizeof(float));); (12, CUDA, checkCudaErrors(\"Error: CUDA error when moving bias to device!\", state);); (7, CUDA, state = cudaMemcpy(cudaBiasData, (uint8_t*)bias.cudaData, k * sizeof(float), cudaMemcpyDeviceToDevice);); (87, CUDA, FastllmCudaFree(cudaFp16Weight);); (18, CUDA, float *cudaOutput = (float*)FastllmCudaPrepareOutput(output);); (16, CUDA, float *cudaInput = (float*)FastllmCudaPrepareInput(input);)"}
{"predict": "(7,OpenMP,#pragma omp parallel for)", "label": "(12, TBB, g.reset(tbb::flow::rf_clear_edges);); (4, TBB, tbb::flow::queue_node<int> qout(g);); (5, TBB, tbb::flow::make_edge(on,qout);); (0, TBB, tbb::flow::graph g;)"}
{"predict": "(32,OpenMP,#pragma omp parallel for)", "label": "(34, CUDA, CSC(cudaGetLastError(), STATUS_FAILURE);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp critical), (18,OpenMP,#pragma omp barrier)", "label": "(8, OpenMP, #pragma omp target teams distribute parallel for reduction(+:tmp) map(A, N, M) map(tofrom:M))"}
{"predict": "(3,OpenMP,#pragma omp parallel for), (6,OpenMP,#pragma omp parallel for), (8,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for)", "label": "(12, OpenMP, #pragma omp for simd collapse (foobool(1) > 0 ? 1 : 2)); (8, OpenMP, #pragma omp for simd collapse (2+2))); (-1, OpenMP, #pragma omp for simd collapse); (16, OpenMP, #pragma omp for simd collapse (S1)); (-1, OpenMP, #pragma omp for simd collapse (); (0, OpenMP, #pragma omp for simd collapse ()); (26, OpenMP, #pragma omp for simd collapse (2)); (10, OpenMP, #pragma omp for simd collapse (foobool(argc)), collapse (true), collapse (-5)); (-1, OpenMP, #pragma omp for simd collapse (4); (21, OpenMP, #pragma omp for simd collapse(collapse(tmain<int, char, -1, -2>(argc, argv))"}
{"predict": "(7,MPI,*w = MPI_Wtime(););", "label": "(35, MPI, MPI_Bcast(&tmp_val, 1, MPI_INT, 0, fd->comm);)"}
{"predict": "(27,OpenMP,#pragma omp parallel for private(recv_size, recv_offset)), (48,OpenMP,#pragma omp parallel for private(i)), (70,OpenMP,#pragma omp parallel for private(i))", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for)", "label": "(1, TBB, tbb::task_arena sequential_arena(1);)"}
{"predict": "PerWorkItem);\n136:     const cl_int localStorageSpacePerWorkitemInt =\n137:         static_cast<cl_int>(localStorageSpacePeritem);\n138:     const cl_int maxWorkgroupSizeInt =\n139:         static_cast<cl_int>(maxWorkgroupSize);\n140:     const cl_int localWorkgroupSizeInt =\n141:         static_cast<cl_int>(localWorkgroupSize);\n142:     const cl_int totalLinesInt = static_cast<cl_int>(totalLines);\n143:     const cl_int globalWorkgroupSizeInt =\n144:         static_cast<cl_int>(globalWorkgroupSize);\n145:     const cl_int numElementsPerLineInt = static_cast<cl_int>(numElementsPerLine);\n146:     const cl_int srcStrideInt = static_cast<cl_int>(srcStride);\n147:     const cl_int dstStrideInt = static_cast<cl_int>(dstStride);\n148:     const cl_int maxTotalLinesInt = static_cast<cl_int>(maxTotalLines);\n149:     const cl_int maxLocalWorkgroupsInt = static_cast<cl_int>(maxLocalWorkgroups);\n150:     const cl_int numberOfLocalWorkgroupsInt =\n151:         static_cast<cl_int>(numberOfLocalWorkgroups);\n152:     const cl_int maxLocalWorkgroupSizeInt =\n153:         static_cast<cl_int>(maxLocalWorkgroupSize);\n154:     const cl_int maxWorkgroupSizeInt2 = static_cast<cl_int>(maxWorkgroupSize);\n155: \n156:     // Create buffers\n157:     cl_mem inMemObj = clCreateBuffer(context, CL_MEM_READ_ONLY | CL_MEM_COPY_HOST_PTR,\n158:                                     inBufferSize, inBuffer, &error);\n159:     test_error(error, \"Failed to create input buffer\");\n160: \n161:     cl_mem outMemObj = clCreateBuffer(context, CL_MEM_WRITE_ONLY,\n162:                                      outBufferSize, NULL, &error);\n163:     test_error(error, \"Failed to create output buffer\");\n164: \n165:     cl_mem outMemObjCopy = clCreateBuffer(context, CL_MEM_WRITE_ONLY,\n166:                                          outBufferSize, NULL,", "label": "(169, OpenCL, error = clSetKernelArg(kernel, 0, sizeof(streams[0]), &streams[0]);); (170, OpenCL, error = clSetKernelArg(kernel, 1, sizeof(streams[1]), &streams[1]);); (184, OpenCL, error = clSetKernelArg(kernel, 7, sizeof(dstStride), &dstStride);); (170, OpenCL, error = clSetKernelArg(kernel, 2, localBufferSize, NULL);); (180, OpenCL, error = clSetKernelArg(kernel, 6, sizeof(srcStride), &srcStride);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for reduction(+:errs)), (22,OpenMP,#pragma omp parallel for reduction(+:errs)), (30,OpenMP,#pragma omp parallel for reduction(+:errs))", "label": "(11, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp critical), (17,OpenMP,#pragma omp barrier)", "label": "(4, CUDA, __shared__ float shared_val[32];)"}
{"predict": "(40,MPI,*w = MPI_Wtime();); (42,OpenMP,#pragma omp parallel for), (50,OpenMP,#pragma omp parallel for), (54,OpenMP,#pragma omp parallel for)", "label": "(60, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (78, CUDA, CUDA_CHECK(cudaDeviceReset());); (75, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (40, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (41, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_y), sizeof(data_type) * y.size()));); (39, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_x), sizeof(data_type) * x.size()));); (67, CUDA, CUDA_CHECK(cudaFree(d_y));); (65, CUDA, CUDA_CHECK(cudaFree(d_x));); (63, CUDA, CUDA_CHECK(cudaFree(d_A));); (30, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (15,MPI,*w = MPI_Wtime();); (20,OpenMP,#pragma omp parallel for), (28,MPI,*w = MPI_Wtime();); (32,OpenMP,#pragma omp parallel for)", "label": "(37, TBB, tbb::flow::make_edge(source2, tbb::flow::input_port<1>(merge));); (35, TBB, tbb::flow::make_edge(source1, tbb::flow::input_port<0>(merge));); (36, TBB, tbb::flow::make_edge(merge, output);); (-2, TBB, tbb::flow::graph g;)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for), (42,OpenMP,#pragma omp parallel for), (50,OpenMP,#pragma omp parallel for), (59,OpenMP,#pragma omp parallel for), (68,OpenMP,#pragma omp parallel for), (77,OpenMP,#pragma omp parallel for)", "label": "(37, CUDA, CHECK(cudaMalloc(&d_matrix_org, M));); (37, CUDA, CHECK(cudaMalloc(&d_matrix_res, M));); (37, CUDA, CHECK(cudaMemcpy(d_matrix_org, h_matrix_org, M, cudaMemcpyDefault));); (42, CUDA, CHECK(cudaEventCreate(&stop));); (50, CUDA, CHECK(cudaEventRecord(stop));); (46, CUDA, CHECK(cudaMemcpy(h_matrix_res, d_matrix_res, M, cudaMemcpyDefault));); (41, CUDA, cudaEventQuery(start);); (48, CUDA, CHECK(cudaEventSynchronize(stop));); (48, CUDA, CHECK(cudaEventElapsedTime(&curr_time, start, stop));); (86, CUDA, CHECK(cudaFree(d_matrix_org));); (86, CUDA, CHECK(cudaFree(d_matrix_res));); (35, CUDA, CHECK(cudaEventRecord(start));); (32, CUDA, CHECK(cudaEventCreate(&start));); (3, CUDA, CHECK(cudaMemcpyFromSymbol(&SIZE, c_TILE_DIM, sizeof(int)));)"}
{"predict": "(30,OpenMP,#pragma omp parallel for)", "label": "(55, OpenCL, clReleaseProgram(program);); (52, OpenCL, clReleaseKernel(k0);); (52, OpenCL, clReleaseKernel(k1);); (11, OpenCL, cl_kernel k1 = clCreateKernel(program, \"compiler_program_global1\", &err);); (8, OpenCL, cl_kernel k0 = clCreateKernel(program, \"compiler_program_global0\", &err);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for)", "label": "(10, OpenMP, #pragma omp target map(tofrom: a[0:100])); (12, OpenMP, #pragma omp simd)"}
{"predict": "0 * fft_size *\n109:                               (std::log(fft_size) / std::log(2)) / time_cufft / 1000000.0;\n110: \n111:         std::cout << \"FFT size: \" << fft_size << \", \";\n112:         std::cout << \"FFTs run: \" << ffts_per_block * cuda_blocks << \", \";\n113:         std::cout << \"cuFFTDx time [ms_n]: \" << time_cufftdx / kernel_runs << \", \";\n114:         std::cout << \"cuFFT time [ms_n]: \" << time_cufft / kernel_runs << \", \";\n115:         std::cout << \"cuFFTDx performance [GFLOPS]: \" << gflops_cufftdx << \", \";\n116:         std::cout << \"cuFFT performance [GFLOPS]: \" << gflops_cufft << std::endl;\n117:     }\n118: }", "label": "(66, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc(&device_buffer, input_size_bytes));); (56, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamCreate(&stream));); (80, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(device_buffer));); (67, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (65, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(device_buffer, input.data(), input_size_bytes, cudaMemcpyHostToDevice));)"}
{"predict": "(5,OpenMP,#pragma omp parallel for), (10,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for)", "label": "(12, OpenMP, #pragma omp distribute parallel for simd firstprivate(x);); (5, OpenMP, #pragma omp distribute parallel for simd;); (24, OpenMP, #pragma omp distribute parallel for simd, private(x);); (0, OpenMP, #pragma omp target); (15, OpenMP, #pragma omp distribute parallel for simd private(x);); (-1, OpenMP, #pragma omp teams)"}
{"predict": "(21,MPI,*w = MPI_Wtime();) (44,OpenMP,#pragma omp parallel for), (72,OpenMP,#pragma omp parallel for), (91,OpenMP,#pragma omp parallel for), (94,OpenMP,#pragma omp set_num_threads(numQueuesAndEvents)), (96,OpenMP,#pragma omp set_num_threads(numQueuesAndEvents)), (99,OpenMP,#pragma omp parallel for)", "label": "(95, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(streams), &streams);); (105, OpenCL, clReleaseKernel(kernel);); (85, OpenCL, err = clGetDeviceInfo(dev, CL_DEVICE_MAX_WORK_GROUP_SIZE, sizeof(local_size), &local_size, NULL);); (59, OpenCL, err = clBuildProgram(program, 0, NULL, includes, NULL, NULL);); (103, OpenCL, clReleaseProgram(program);); (48, OpenCL, cl_program program = clCreateProgramWithSource(ctx, 4, sources, NULL, &err);); (96, OpenCL, err = clEnqueueNDRangeKernel(commQueues[0], kernel, 1, NULL, &streamCount, &local_size, numWaitEvents, waitEvents, outEvents);); (89, OpenCL, err |= clSetKernelArg(kernel, 1, sizeof(number_count_per_stream), &number_count_per_stream);); (89, OpenCL, err |= clSetKernelArg(kernel, 2, sizeof(outBuffer), &outBuffer);); (68, OpenCL, cl_kernel kernel = clCreateKernel(program, \"fillBufferU01\", &err);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(3, OpenMP, #pragma omp target)"}
{"predict": "(17,OpenMP,#pragma omp parallel for private(j))\n(30,OpenMP,#pragma omp parallel for reduction(min:sum))", "label": "(28, OpenMP, #pragma omp teams reduction(+:sum)); (25, OpenMP, #pragma omp target map(tofrom: b, sum, t) map(alloc: a)); (28, OpenMP, #pragma omp distribute parallel for reduction(+:sum)); (30, OpenMP, t[i] = omp_get_team_num();)"}
{"predict": "Tensor cA = tAsA(_,0);   // (BLK_M,BLK_K)\n114:   Tensor cB = tBsB(_,0);   // (BLK_N,BLK_K)\n115:   Tensor cC = gC(_,0);     // (BLK_M,BLK_N)\n116: \n117:   //\n118:   // Mma Iterations\n119:   //\n120:   int mma_tile_count = 0;\n121:   int mma_tile = 0;\n122:   CUTE_UNROLL\n123:   for (; mma_tile_count < K_PIPE_MAX; ++mma_tile_count)\n124:   {\n125:     mma.sync();\n126:     if (mma_tile_count == 0) {\n127:       mma(cC, cA, cB, beta);\n128:     } else {\n129:       mma(cC, cA, cB, alpha);\n130:     }\n131:     mma_tile++;\n132:   }\n133: \n134:   //\n135:   // Synchronize and finalize\n136:   //\n137:   CUTE_UNROLL\n138:   for (int pipe = 0; pipe < K_PIPE_MAX; ++pipe)\n139:   {\n140:     if ((warp_idx == 0) && lane_predicate)\n141:     {\n142:       ProducerBarType::wait(&producer_mbar[pipe]);\n143:     }\n144:   }\n145: \n146:   //\n147:   // Store results back to global memory\n148:   //\n149:   CUTE_UNROLL\n150:   for (int pipe = 0; pipe < K_PIPE_MAX; ++pipe)\n151:   {\n152:     if ((warp_idx == 0) && lane_predicate)\n153:     {\n154:       copy(tma_a.with(consumer_mbar[pipe]), tAsA(_,k_tile), tAsA(_,pipe));\n155:       copy(tma_b.with(consumer_mbar[pipe]), tBsB(_,", "label": "(38, CUDA, extern __shared__ char shared_memory[];)"}
{"predict": "(7,MPI,*w = MPI_Wtime();); (15,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for)", "label": "(28, MPI, MPICHECK(MPI_Mrecv(&dims, count, MPI_INT64_T, &msg, &status));); (36, MPI, MPICHECK(MPI_Get_count(&status, MPI_UINT8_T, &count));); (40, MPI, MPICHECK(MPI_Mrecv(tensor->data(), count, MPI_UINT8_T, &msg, &status));); (16, MPI, MPICHECK(MPI_Mrecv(&mpiId, count, MPI_UINT64_T, &msg, &status));); (20, MPI, MPICHECK(MPI_Get_count(&status, MPI_INT64_T, &count));); (11, MPI, MPICHECK(MPI_Get_count(&status, MPI_UINT64_T, &count));)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(15, CUDA, extern __shared__ float sdata[];)"}
{"predict": "(13,OpenMP,#pragma omp parallel for private(err)), (16,OpenMP,#pragma omp parallel for private(err)), (19,OpenMP,#pragma omp parallel for private(err)), (22,OpenMP,#pragma omp parallel for private(err)), (25,OpenMP,#pragma omp parallel for private(err)), (28,OpenMP,#pragma omp parallel for private(err)), (31,OpenMP,#pragma omp parallel for private(err)), (34,OpenMP,#pragma omp parallel for private(err)), (37,OpenMP,#pragma omp parallel for private(err)), (40,OpenMP,#pragma omp parallel for private(err))", "label": "(11, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(25,OpenMP,#pragma omp parallel for)", "label": "(15, OpenCL, clKernelWrapper kernel = clCreateKernel(prog, \"usersemantic_test\", &error);); (22, OpenCL, error |= clSetKernelArg(kernel, 0, sizeof(dst), &dst);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for)", "label": "(15, CUDA, getLastCudaError(\"mergeElementaryIntervalsKernel<1> failed\\n\");); (19, CUDA, getLastCudaError(\"mergeElementaryIntervalsKernel<0> failed\\n\");)"}
{"predict": ": \n123:     /* ------------------------------------------------------------\n124:        SET UP THE SUPERLU OPTIONS.\n125:        ------------------------------------------------------------*/\n126:     set_default_options_dist(&options);\n127:     options.ColPerm = SPARSE;\n128:     options.Trans = NOTRANS;\n129:     options.PivotGrowth = ON;\n130:     options.Fact = DOFACT;\n131:     options.SolveInitialized = NO;\n132:     options.Refine = YES;\n133:     options.IterRefine = DOUBLE;\n134:     options.DiagPivotThresh = 0.0;\n135:     options.SymmetricMode = NO;\n136:     options.SwapSchurComplement = NO;\n137:     options.SparseLevel = 0;\n138:     options.DropTol = 0.0;\n139:     options.ILUstruct = NULL;\n140: \n141:     /* ------------------------------------------------------------\n142:        PERFORM THE LU DECOMPOSITION.\n143:        ------------------------------------------------------------*/\n144:     dgssv_dist(&options, &grid, &A, &ScalePermstruct, &LUstruct, b, ldb, &stat);\n145:     \n146:     /* ------------------------------------------------------------\n147:        SOLVE THE LINEAR SYSTEM.\n148:        ------------------------------------------------------------*/\n149:     dgstrs_dist(NOTRANS, &grid, &A, &ScalePermstruct, &LUstruct, b, ldb, &stat);\n150:     \n151:     /* ------------------------------------------------------------\n152:        GET THE RESULT AND CHECK IT.\n153:        ------------------------------------------------------------*/\n154:     derrchk_dist(&options, &A, &ScalePermstruct, &LUstruct, b1, b, ldb, berr, n, nrhs, &stat);\n155:     \n156:     /* ------------------------------------------------------------\n157:        DEALLOCATE STORAGE.\n158:        ------------------------------------------------------------*/\n159:     Destroy_CompCol_Matrix_dist(&A);\n160:     Destroy_SuperNode_Matrix_dist(&LUstruct.SuperNode);\n161:     Destroy_CompCol_Matrix_dist(&LUstruct.U);\n162:     Destroy_CompCol_Matrix_dist(&LUstruct", "label": "(87, MPI, MPI_Bcast( asub, nnz, mpi_int_t,  0, grid.comm );); (224, MPI, MPI_Finalize();); (83, MPI, MPI_Bcast( &nnz, 1,   mpi_int_t,  0, grid.comm );); (83, MPI, MPI_Bcast( a,    nnz, MPI_DOUBLE, 0, grid.comm );); (22, MPI, MPI_Init( &argc, &argv );); (83, MPI, MPI_Bcast( xa,   n+1, mpi_int_t,  0, grid.comm );); (78, MPI, MPI_Bcast( &n,   1,   mpi_int_t,  0, grid.comm );); (76, MPI, MPI_Bcast( &m,   1,   mpi_int_t,  0, grid.comm );)"}
{"predict": "(20,MPI,MTest_thread_barrier(nthreads);), (23,MPI,MTest_thread_barrier(nthreads);), (26,MPI,MTest_thread_barrier(nthreads);), (28,MPI,MTest_thread_barrier(nthreads);), (30,MPI,MTest_thread_barrier(nthreads);), (34,MPI,MTest_thread_barrier(nthreads);)", "label": "(26, MPI, MPI_Isend(buf, cnt, MPI_INT, thread_num, tag, MPI_COMM_WORLD, &r[thread_num - 1]);); (36, MPI, t = MPI_Wtime() - t;); (21, MPI, t = MPI_Wtime();); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (29, MPI, MPI_Waitall(wsize - 1, r, MPI_STATUSES_IGNORE);); (23, MPI, MPI_Wait(&r[thread_num - 1], MPI_STATUS_IGNORE);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (16,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for)", "label": "(8, OpenMP, #pragma omp target parallel for simd order(); (13, OpenMP, #pragma omp target parallel for simd order(concurrent); (15, OpenMP, #pragma omp target parallel for simd order(concurrent)); (2, OpenMP, #pragma omp target parallel for simd order); (-2, OpenMP, #pragma omp target parallel for simd lastprivate(s) firstprivate(s)); (6, OpenMP, #pragma omp target parallel for simd order(none)"}
{"predict": "(40,OpenMP,#pragma omp parallel for)", "label": "(7, CUDA, __shared__ block_radix_sort_t::TempStorage temp_storage;)"}
{"predict": "(27,OpenMP,#pragma omp parallel for), (55,OpenMP,#pragma omp parallel for), (72,OpenMP,#pragma omp parallel for)", "label": "(68, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (2, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (59, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(inData->basePtr(), inVec.data(), inBufSize, cudaMemcpyHostToDevice));); (72, CUDA, ASSERT_EQ(cudaSuccess, cudaMemcpy(testVec.data(), outData->basePtr(), outBufSize, cudaMemcpyDeviceToHost));); (65, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamDestroy(stream));)"}
{"predict": "(30,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for)", "label": "(26, CUDA, CUDA_CHECK(cudaMalloc((void **)&dev_max, sizeof(FP) * SEQLEN * DIM));); (45, CUDA, cudaFree(dev_max);); (28, CUDA, CUDA_CHECK(cudaMemcpy(dev_denom, host_denom, sizeof(FP) * SEQLEN * DIM, cudaMemcpyHostToDevice));); (24, CUDA, CUDA_CHECK(cudaMalloc((void **)&dev_denom, sizeof(FP) * SEQLEN * DIM));); (24, CUDA, CUDA_CHECK(cudaMalloc((void **)&QK, sizeof(FP) * SEQLEN * SEQLEN));); (24, CUDA, CUDA_CHECK(cudaMemcpy(dev_max, host_max, sizeof(FP) * SEQLEN * DIM, cudaMemcpyHostToDevice));); (39, CUDA, cudaFree(QK);); (40, CUDA, cudaFree(dev_denom);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for schedule(runtime) reduction(+:total) linear(idx:1))", "label": "(19, OpenMP, #pragma omp for schedule(runtime) reduction(+:total) linear(idx:1)); (13, OpenMP, thread_num = omp_get_thread_num();); (15, OpenMP, #pragma omp master); (7, OpenMP, #pragma omp parallel shared(data, std::cerr, total, idx) default(none))"}
{"predict": "(13,MPI,*w = MPI_Wtime();); (28,OpenMP,#pragma omp parallel for), (56,OpenMP,#pragma omp critical), (75,OpenMP,#pragma omp parallel for)", "label": "(62, MPI, MPI_Type_commit(newtype);)"}
{"predict": "(10,MPI,*w = MPI_Wtime();)\n(21,OpenMP,#pragma omp parallel for)", "label": "(21, CUDA, $GPU(cudaMemcpy(compare, gpu_outputs, sizeof(typename types<params>::output_t)*count, cudaMemcpyDeviceToHost));); (15, CUDA, $GPU(cudaMalloc((void **)&gpu_outputs, sizeof(typename types<params>::output_t)*count));); (15, CUDA, $GPU(cudaMemset(gpu_outputs, 0, sizeof(typename types<params>::output_t)*count));); (58, CUDA, $GPU(cudaFree(gpu_outputs));)"}
{"predict": "(11,MPI,*w = MPI_Wtime();); (30,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for), (40,MPI,MPI_Wtime() - w;);", "label": "(38, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (40, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "5:          // printf(\"  After kernel:   %i %i %i %i %i %i %i %i\\n\", threadIdx_x, blockIdx_x, grid->npcol, nsupers,myrow,krow,bid,tid);\n136: \n137:      }\n138: \n139:      if(myrow!=krow){   /* non-diagonal block */\n140: \n141:          RHS_ITERATE(j)\n142:              for (i = tid; i < knsupc; i+=block_size){\n143: \t\t\tz_add(&x[i + ii + j*knsupc],\n144:                 &x[i + ii + j*knsupc],\n145:                 &lsum[i + il + j*knsupc]);\n146:              }\n147:      }\n148:  }\n149:  }", "label": "(45, CUDA, __shared__ doublecomplex temp2[MAXSUPER];)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(10, OpenCL, auto mappedPtr = clEnqueueMapBuffer(&cmdQ, buffer.get(), CL_TRUE, CL_MAP_READ, 0, 1, 0, nullptr, nullptr, &retVal);); (12, OpenCL, retVal = clEnqueueUnmapMemObject(&cmdQ, buffer.get(), mappedPtr, 0, nullptr, nullptr);)"}
{"predict": "(30,OpenMP,#pragma omp parallel for)", "label": "(24, OpenMP, #pragma omp target parallel firstprivate (a, b, c, d, f)); (43, OpenMP, #pragma omp target parallel shared(i)); (46, OpenMP, #pragma omp target parallel firstprivate(j)); (19, OpenMP, #pragma omp target parallel firstprivate (S1)); (6, OpenMP, #pragma omp target parallel firstprivate (); (11, OpenMP, #pragma omp target parallel firstprivate (argc,); (28, OpenMP, #pragma omp target parallel firstprivate(S2::S2s)); (35, OpenMP, #pragma omp target parallel private(i), firstprivate(i)); (12, OpenMP, #pragma omp target parallel firstprivate (argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (29, OpenMP, #pragma omp target parallel firstprivate(e, g)); (30, OpenMP, #pragma omp target parallel firstprivate(h, B::x)); (19, OpenMP, #pragma omp target parallel firstprivate(ca, z)); (16, OpenMP, #pragma omp target parallel firstprivate(ba) allocate(omp_thread_mem_alloc: ba)); (-1, OpenMP, #pragma omp target parallel firstprivate ()); (36, OpenMP, #pragma omp target parallel firstprivate(m)); (-1, OpenMP, #pragma omp target parallel firstprivate (argc); (-8, OpenMP, #pragma omp target parallel firstprivate); (29, OpenMP, #pragma omp target parallel firstprivate(i)); (18, OpenMP, #pragma omp target parallel firstprivate(S2::S2sc)); (7, OpenMP, #pragma omp target parallel firstprivate (argv[1])); (-2, OpenMP, #pragma omp target parallel firstprivate (argc > 0 ? argv[1] : argv[2])); (11, OpenMP, #pragma omp target parallel firstprivate(da))"}
{"predict": "(19,OpenMP,#pragma omp parallel for)", "label": "(8, TBB, start = tbb::tick_count::now();); (0, TBB, tbb::tick_count now;); (8, TBB, now = tbb::tick_count::now();)"}
{"predict": "(5,MPI,*w = MPI_Wtime();), (12,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp barrier)", "label": "(17, OpenMP, #pragma omp barrier); (2, OpenMP, #pragma omp parallel); (3, OpenMP, int me = omp_get_thread_num();)"}
{"predict": "135:         log_info(\"Testing %d x %d\\n\", (int)imageInfo.width, (int)imageInfo.arraySize );\n136:         if(gTestMipmaps)\n137:             imageInfo.num_mip_levels = (size_t)random_in_range(2, (compute_max_mip_levels(imageInfo.width, 0, 0)-1), seed);\n138:         if( gDebugTrace )\n139:             log_info( \"   at max size %d,%d\\n\", (int)imageInfo.width, (int)imageInfo.arraySize );\n140:         int retCode =\n141:             test_read_image(context, queue, kernel, &imageInfo,\n142:                             imageSampler, floatCoords, outputType, seed);\n143:         if( retCode )\n144:             return retCode;\n145:     }\n146:     else\n147:     {\n148:         // Test with some specific sizes\n149:         size_t typeRange = 1LL << ( get_format_type_size( imageInfo.format ) * 8 );\n150:         typeRange /= pixelSize / get_format_type_size( imageInfo.format );\n151:         imageInfo.arraySize = 100;\n152:         imageInfo.width = (size_t)( typeRange / (cl_ulong)imageInfo.arraySize );\n153:         while( imageInfo.width >= maxWidth / 2 )\n154:             imageInfo.width >>= 1;\n155:         imageInfo.rowPitch = imageInfo.slicePitch = imageInfo.width * pixelSize;\n156:         log_info(\"Testing %d x %d\\n\", (int)imageInfo.width, (int)imageInfo.arraySize );\n157:         if(gTestMipmaps)\n158:             imageInfo.num_mip_levels = (size_t)random_in_range(2, (compute_max_mip_levels(imageInfo.width, 0, 0)-1), seed);\n159:         if( gDebugTrace )\n160:             log_info( \"   at max size %d,%d\\n\", (int)imageInfo.width, (int)imageInfo.arraySize );\n161:         int retCode =\n162:             test_read_image(context, queue,", "label": "(27, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_IMAGE_MAX_ARRAY_SIZE, sizeof( maxArraySize ), &maxArraySize, NULL );); (28, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_GLOBAL_MEM_SIZE, sizeof( memSize ), &memSize, NULL );); (26, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_MAX_MEM_ALLOC_SIZE, sizeof( maxAllocSize ), &maxAllocSize, NULL );); (23, OpenCL, error = clGetDeviceInfo( device, CL_DEVICE_IMAGE2D_MAX_WIDTH, sizeof( maxWidth ), &maxWidth, NULL );)"}
{"predict": "(6,MPI,*w = MPI_Wtime();); (20,OpenMP,#pragma omp parallel for), (33,MPI,grads_now[iw * DIM + idim] = sum[idim * COLBS];)", "label": "(11, CUDA, __shared__ uninitialized_array<T, SUM_SIZE> sum;)"}
{"predict": "(14,OpenMP,#pragma omp parallel for)", "label": "(10, TBB, static const size_t number_of_threads = 8 * tbb::task_scheduler_init::default_num_threads();)"}
{"predict": "(14,OpenMP,#pragma omp parallel for reduction(+:local_l2_norm)), (26,OpenMP,#pragma omp parallel for reduction(+:local_l2_norm))", "label": "(7, CUDA, __shared__ typename BlockReduce::TempStorage temp_storage;)"}
{"predict": "(10,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for)", "label": "(2, TBB, using namespace tbb::internal;)"}
{"predict": "(4,MPI,*w = MPI_Wtime();); (7,OpenMP,#pragma omp parallel for), (10,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for), (38,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for), (45,OpenMP,#pragma omp parallel for)", "label": "(2, OpenMP, #pragma omp taskyield allocate(argc)); (3, OpenMP, #pragma omp taskyield untied); (3, OpenMP, #pragma omp taskyield)"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (11,MPI,*ret_host = MPI_Wtime();)", "label": "(14, CUDA, cudaError_t cuerr = cudaMemcpy(ret_host, ret_ptr, (1)*sizeof(double), cudaMemcpyDeviceToHost);); (3, CUDA, double* d_ptr = thrust::raw_pointer_cast(dv_ptr);); (7, CUDA, thrust::device_ptr<double> ret_dev_ptr = thrust::min_element(thrust::device, dv_ptr, dv_ptr+n);); (10, CUDA, double *ret_ptr = thrust::raw_pointer_cast(ret_dev_ptr);); (-1, CUDA, thrust::device_ptr<double> dv_ptr = thrust::device_malloc(n*sizeof(double));); (14, CUDA, thrust::device_free(dv_ptr);)"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(14, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(15,OpenMP,#pragma omp parallel for collapse(2))", "label": "(25, MPI, MPI_Type_get_extent(MPI_INT, &tmp_lb, &sizeofint);); (39, MPI, MPI_Pack(a, 1, xpose, buffer, bufsize, &position, MPI_COMM_WORLD);); (55, MPI, MPI_Type_free(&row);); (27, MPI, MPI_Type_commit(&xpose);); (52, MPI, MPI_Type_free(&xpose);); (28, MPI, MPI_Pack_size(1, xpose, MPI_COMM_WORLD, &bufsize);); (23, MPI, MPI_Type_create_hvector(100, 1, sizeofint, row, &xpose);); (31, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (36, MPI, MPI_Unpack(buffer, bufsize, &position, b, 100 * 100, MPI_INT, MPI_COMM_WORLD);); (19, MPI, MPI_Type_vector(100, 1, 100, MPI_INT, &row);)"}
{"predict": "(10,MPI,MPI_Init(NULL, NULL);) (12,OpenMP,#pragma omp parallel for) (30,MPI,MPI_Finalize();)", "label": "(34, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (36, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "44:       RandomGenerator my_random2(random_th);\n145: \n146:       int nacc = 0;\n147:       int nvals = 0;\n148:       double t1 = 0.0;\n149:       double t2 = 0.0;\n150:       double t3 = 0.0;\n151:       double t4 = 0.0;\n152:       double t5 = 0.0;\n153:       double t6 = 0.0;\n154:       double t7 = 0.0;\n155:       double t8 = 0.0;\n156:       double t9 = 0.0;\n157:       double t10 = 0.0;\n158:       double t11 = 0.0;\n159:       double t12 = 0.0;\n160:       double t13 = 0.0;\n161:       double t14 = 0.0;\n162:       double t15 = 0.0;\n163:       double t16 = 0.0;\n164:       double t17 = 0.0;\n165:       double t18 = 0.0;\n166:       double t19 = 0.0;\n167:       double t20 = 0.0;\n168:       double t21 = 0.0;\n169:       double t22 = 0.0;\n170:       double t23 = 0.0;\n171:       double t24 = 0.0;\n172:       double t25 = 0.0;\n173:       double t26 = 0.0;\n174:       double t27 = 0.0;\n175:       double t28 = 0.0;\n176:       double t29 = 0.0;\n177:       double t30 = 0.0;\n178:       double t31 = 0.0;\n179:       double t32 = 0.0;", "label": "(115, OpenMP, ncrews = omp_get_max_threads();); (98, OpenMP, const int np = omp_get_num_threads();); (189, OpenMP, #pragma omp barrier); (233, OpenMP, double dTotalThreads = omp_get_max_threads();); (268, OpenMP, double nMajorThreads  = omp_get_max_threads();); (137, OpenMP, #pragma omp parallel reduction(+ : vgh_t_loc, v_t_loc, my_vals)); (106, OpenMP, #pragma omp master); (278, OpenMP, cout << \"\\nMPI: \" << nmpi << \" and Threads: \" << omp_get_max_threads();); (92, OpenMP, const int ip = omp_get_thread_num();); (86, OpenMP, #pragma omp parallel reduction(+ : t0, tInit, ratio, vgh_t, val_t, nspheremoves, dNumVGHCalls))"}
{"predict": "cudaEventSynchronize(stop);\n133:     cudaEventElapsedTime(&time, start, stop);\n134:     cudaEventDestroy(start);\n135:     cudaEventDestroy(stop);\n136:     printf(\"pack elementwise: %f ms\\n\", time / (float)R);\n137: \n138:     cudaMemcpy(c_host, c_device, N * sizeof(half), cudaMemcpyDeviceToHost);\n139:   }\n140: \n141:   free(a_host);\n142:   free(b_host);\n143:   free(c_host);\n144:   return 0;\n145: }", "label": "(25, CUDA, cudaMalloc((void **)&c_device, N * sizeof(half));); (49, CUDA, cudaEventSynchronize(stop);); (50, CUDA, cudaEventDestroy(start);); (155, CUDA, cudaFree(a_device);); (32, CUDA, cudaDeviceSynchronize();); (155, CUDA, cudaFree(c_device);); (43, CUDA, cudaEventRecord(stop, 0);); (46, CUDA, cudaEventDestroy(stop);); (151, CUDA, cudaFree(b_device);); (33, CUDA, cudaEventRecord(start, 0);); (11, CUDA, cudaMemcpy(b_device, b_host, N * sizeof(half), cudaMemcpyHostToDevice);); (30, CUDA, cudaEventCreate(&stop);); (6, CUDA, cudaMalloc((void **)&b_device, N * sizeof(half));); (-2, CUDA, cudaMalloc((void **)&a_device, N * sizeof(half));); (37, CUDA, cudaEventElapsedTime(&time, start, stop);); (-1, CUDA, cudaMemcpy(a_device, a_host, N * sizeof(half), cudaMemcpyHostToDevice);); (24, CUDA, cudaEventCreate(&start);); (39, CUDA, cudaMemcpy(c_host, c_device, N * sizeof(half), cudaMemcpyDeviceToHost);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(27, CUDA, CHECK(cudaMemPrefetchAsync(z, M, cudaCpuDeviceId, NULL));); (20, CUDA, CHECK(cudaMemPrefetchAsync(x, M, device_id, NULL));); (20, CUDA, CHECK(cudaMemPrefetchAsync(y, M, device_id, NULL));); (7, CUDA, CHECK(cudaMallocManaged((void **)&z, M));); (28, CUDA, CHECK(cudaFree(x));); (-2, CUDA, CHECK(cudaGetDevice(&device_id));); (2, CUDA, CHECK(cudaMallocManaged((void **)&x, M));); (26, CUDA, CHECK(cudaFree(y));); (26, CUDA, CHECK(cudaFree(z));); (14, CUDA, CHECK(cudaMemPrefetchAsync(z, M, device_id, NULL));); (19, CUDA, CHECK(cudaDeviceSynchronize());); (-2, CUDA, CHECK(cudaMallocManaged((void **)&y, M));)"}
{"predict": "(4,MPI,*w = MPI_Wtime();); (5,OpenMP,#pragma omp parallel for), (55,MPI,MPI_Finalize());", "label": "(14, CUDA, HANDLE_ERROR( cudaGLSetGLDevice( dev ) );); (51, CUDA, HANDLE_ERROR( cudaGraphicsUnmapResources( 1, &resource, NULL ) );); (39, CUDA, HANDLE_ERROR( cudaGraphicsMapResources( 1, &resource, NULL ) );); (4, CUDA, HANDLE_ERROR( cudaChooseDevice( &dev, &prop ) );)"}
{"predict": "Cells just outside of the boundary of part 1 in\n162:                its coordinates */\n163:             int b_ilower[2] = {2,1}, b_iupper[2] = {2,2};\n164:             /* The same cells in part 0's coordinates.  Since we use the same\n165:                index space across all parts, the coordinates coincide. */\n166:             int nbor_ilower[2] = {2,1}, nbor_iupper[2] = {2,2};\n167:             /* These parts have the same orientation, so no\n168:                rotation is necessary */\n169:             int index_map[2] = {0,1};\n170:             /* These parts map increasing values to increasing values \n171:                for both variables (note: if decreasing maps to increasing, use -1)*/\n172:             int index_dir[2] = {1,1};\n173: \n174:             HYPRE_SStructGridSetNeighborPart(grid, part, b_ilower, b_iupper,\n175:                                              nbor_part, nbor_ilower, nbor_iupper,\n176:                                              index_map, index_dir);\n177:          }\n178:       }\n179: \n180:       /* Processor 1 sets the neighbor relation between part 2 and part 1. */\n181:       else if (myid == 1)\n182:       {\n183:          /* Relation between part 2 and part 1 on processor 1 */\n184:          {\n185:             int part = 2;\n186:             int nbor_part = 1;\n187:             /* Cells just outside of the boundary of part 2 in\n188:                its coordinates */\n189:             int b_ilower[2] = {0,1}, b_iupper[2] = {2,2};\n190:             /* The same cells in part 1's coordinates.  Since we use the same\n191:                index space across all parts, the coordinates coincide. */\n192:             int nbor_ilower[2] = {0,1}, nbor_iupper[2] = {2,2", "label": "(19, MPI, MPI_Init(&argc, &argv);); (25, MPI, MPI_Finalize();); (19, MPI, MPI_Comm_size(MPI_COMM_WORLD, &num_procs);); (17, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myid);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for)", "label": "(26, OpenMP, #pragma omp for private(int)); (10, OpenMP, #pragma omp for private(,); (35, OpenMP, #pragma omp for private(x)); (2, OpenMP, #pragma omp for private(); (-2, OpenMP, #pragma omp parallel); (26, OpenMP, #pragma omp for private(0)); (10, OpenMP, #pragma omp for private(, )); (34, OpenMP, #pragma omp for private(x, y)); (13, OpenMP, #pragma omp for private()); (36, OpenMP, #pragma omp for private(x, y, z))"}
{"predict": "(4,OpenMP,#pragma omp parallel for)", "label": "(7, TBB, test_reserving_nodes<tbb::flow::write_once_node, size_t>();); (3, TBB, tbb::task_scheduler_init init(p);); (8, TBB, test_extract_on_node<tbb::flow::write_once_node, int>();)"}
{"predict": "(19,OpenMP,#pragma omp parallel for)", "label": "(13, MPI, MPI_Isend(buffer, buf_size, MPI_BYTE, 1, tag1, MPI_COMM_WORLD, &request1);); (15, MPI, MPI_Wait(&request2, &status);); (12, MPI, MPI_Isend(buffer, buf_size, MPI_BYTE, 1, tag2, MPI_COMM_WORLD, &request2);); (16, MPI, MPI_Irecv(buffer, buf_size, MPI_BYTE, 0, tag1, MPI_COMM_WORLD, &request2);); (11, MPI, MPI_Wait(&request1, &status);); (13, MPI, MPI_Irecv(buffer, buf_size, MPI_BYTE, 0, tag2, MPI_COMM_WORLD, &request1);)"}
{"predict": "(7,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for)", "label": "(15, CUDA, cudaDeviceSynchronize();)"}
{"predict": "(13,OpenMP,#pragma omp parallel for private(i,j,k,row_i,row_c))", "label": "(12, OpenMP, #pragma omp parallel for private(i,j,k) num_threads(dtn(border, 1)))"}
{"predict": "(38,OpenMP,#pragma omp parallel for)", "label": "(36, OpenMP, #pragma omp target parallel for map(a, b))"}
{"predict": "(18,OpenMP,#pragma omp parallel for reduction(+:total))", "label": "(4, CUDA, extern __shared__ int shared[];)"}
{"predict": "(19,OpenMP,#pragma omp parallel for)", "label": "(32, OpenMP, omp_target_free(d_a, DefaultDeviceNum);); (23, OpenMP, #pragma omp target update to(p_a)); (12, OpenMP, d_a = (int *)omp_target_alloc(DataSize, DefaultDeviceNum);); (6, OpenMP, int DefaultDeviceNum = omp_get_default_device();); (14, OpenMP, omp_target_associate_ptr(p_a, d_a, DataSize, 0, DefaultDeviceNum);); (23, OpenMP, omp_target_disassociate_ptr(p_a, DefaultDeviceNum);)"}
{"predict": "(6,OpenMP,#pragma omp parallel for)", "label": "(9, TBB, tbb::parallel_pipeline( n_tokens, one_filter, context... );); (5, TBB, tbb::filter<void, void> one_filter( filter_table[i], i_filter );)"}
{"predict": "(4,MPI,comm_rank = MPI_Comm_rank(comm, &comm_rank);)", "label": "(13, MPI, MPI_Bcast(dims, 2, MPI_INT, 0, comm);); (2, MPI, MPI_Comm_rank(comm, &comm_rank);)"}
{"predict": "(22,OpenMP,#pragma omp parallel for)", "label": "(10, CUDA, CHECK_CUDA_ERROR(cudaRuntimeGetVersion(&runtime_version));); (72, CUDA, CHECK_CUDA_ERROR(cudaSetDevice(dev));); (26, CUDA, CHECK_CUDA_ERROR(cudaGetDeviceProperties(&deviceProp, dev));); (3, CUDA, CHECK_CUDA_ERROR(cudaDriverGetVersion(&driver_version));); (20, CUDA, CHECK_CUDA_ERROR(cudaGetDeviceProperties(&deviceProp, i));)"}
{"predict": "Shape(shape_k_span)));\n113:     ASSERT_EQ(AsStatus::ALLSPARK_SUCCESS,\n114:               tensors.at(v_span_name)->SetShape(Shape(shape_v_span)));\n115:     ASSERT_EQ(AsStatus::ALLSPARK_SUCCESS,\n116:               tensors.at(ws_name)->SetShape(Shape(shape_ws)));\n117: \n118:     // allocate\n119:     ASSERT_EQ(AsStatus::ALLSPARK_SUCCESS,\n120:               tensors.at(span_pool_name)->Allocate(cuda_stream));\n121:     ASSERT_EQ(AsStatus::ALLSPARK_SUCCESS,\n122:               tensors.at(q_name)->Allocate(cuda_stream));\n123:     ASSERT_EQ(AsStatus::ALLSPARK_SUCCESS,\n124:               tensors.at(k_span_name)->Allocate(cuda_stream));\n125:     ASSERT_EQ(AsStatus::ALLSPARK_SUCCESS,\n126:               tensors.at(v_span_name)->Allocate(cuda_stream));\n127:     ASSERT_EQ(AsStatus::ALLSPARK_SUCCESS,\n128:               tensors.at(ws_name)->Allocate(cuda_stream));\n129: \n130:     // allocate host ws\n131:     if (host_ws_bytes > 0) {\n132:       host_ws = malloc(host_ws_bytes);\n133:       ASSERT_NE(host_ws, nullptr);\n134:     }\n135: \n136:     // allocate ref ws\n137:     #ifdef KERNEL_SPAN_ATTN_TEST_REF\n138:     std::vector<void*> ref_ws;\n139:     for (int i = 0; i < batch_size; ++i) {\n140:       ref_ws.push_back(malloc(ref_ws_bytes));\n141:       ASSERT_NE(ref_ws[i], nullptr);\n142:     }\n143:     #endif\n144: \n145:     // init\n146:     #ifdef KERNEL_SPAN_ATTN_TEST_REF\n147:     if (check_init) {\n148:       SpanAttnBaseline<BaselineMode::LOOP, T>::Init(refObjs, tensors);\n149:     }\n150:     #endif\n151: \n152:     // set input data\n153:     #ifdef KERNEL_SPAN_ATTN_TEST", "label": "(90, CUDA, ASSERT_EQ(cudaSuccess, cudaMallocHost(&host_ws, host_ws_bytes));); (297, CUDA, ASSERT_EQ(cudaSuccess, cudaEventDestroy(ref_start));); (230, CUDA, ASSERT_EQ(cudaSuccess, cudaEventElapsedTime(&ref_time, ref_start, ref_end));); (297, CUDA, ASSERT_EQ(cudaSuccess, cudaEventDestroy(out_start));); (297, CUDA, ASSERT_EQ(cudaSuccess, cudaEventDestroy(out_end));); (249, CUDA, ASSERT_EQ(cudaSuccess, cudaEventElapsedTime(&out_time, out_start, out_end));); (201, CUDA, ASSERT_EQ(cudaSuccess, cudaEventCreate(&ref_end));); (292, CUDA, ASSERT_EQ(cudaSuccess, cudaEventDestroy(ref_end));); (244, CUDA, ASSERT_EQ(cudaSuccess, cudaEventRecord(out_end, cuda_stream));); (199, CUDA, ASSERT_EQ(cudaSuccess, cudaEventCreate(&out_start));); (199, CUDA, ASSERT_EQ(cudaSuccess, cudaEventCreate(&out_end));); (220, CUDA, ASSERT_EQ(cudaSuccess, cudaEventSynchronize(ref_end));); (241, CUDA, ASSERT_EQ(cudaSuccess, cudaEventSynchronize(out_end));); (217, CUDA, ASSERT_EQ(cudaSuccess, cudaEventRecord(ref_end, cuda_stream));); (192, CUDA, ASSERT_EQ(cudaSuccess, cudaEventCreate(&ref_start));)"}
{"predict": "4:         }\n125:     }\n126: \n127:     if (ALGO == Algorithm::CN_HEAVY_TUBE) {\n128:         shuffle<4>(sPtr,sub, d[0], sub + 1);\n129:         shuffle<4>(sPtr,sub, d[1], sub + 2);\n130:         shuffle<4>(sPtr,sub, d[0], sub + 3);\n131:     }\n132: \n133:     d_ctx_a[thread * 4 + sub] = d[0];\n134:     d_ctx_b[thread * 4 + sub] = d[1];\n135: }", "label": "(18, CUDA, __shared__ uint32_t sharedMemory[1024];); (29, CUDA, extern __shared__ uint32_t shuffleMem[];)"}
{"predict": "(6,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (10,OpenMP,#pragma omp critical), (12,OpenMP,#pragma omp critical)", "label": "(6, MPI, MPI_Comm_get_attr(comm, lkey[i], &val_p, &flag);)"}
{"predict": "64: \t\tdisplay->Destroy();\n165: \t}\n166: \n167: \tif( texture != NULL )\n168: \t{\n169: \t\ttexture->Destroy();\n170: \t}\n171: \n172: \tif( font != NULL )\n173: \t{\n174: \t\tfont->Destroy();\n175: \t}\n176: \n177: \tif( camera != NULL )\n178: \t{\n179: \t\tcamera->Destroy();\n180: \t}\n181: \n182: \tif( buffer != NULL )\n183: \t{\n184: \t\tbuffer->Destroy();\n185: \t}\n186: \n187: \treturn 0;\n188: }", "label": "(89, CUDA, CUDA(cudaGeneratePointGrid(points, GRID_N, GRID_WORLD_SIZE, apptime()));); (89, CUDA, CUDA(cudaDeviceSynchronize());)"}
{"predict": "(3,OpenMP,#pragma omp parallel for)", "label": "(29, OpenMP, #pragma omp target teams distribute num_teams (-2)); (4, OpenMP, #pragma omp target teams distribute num_teams (); (30, OpenMP, #pragma omp target teams distribute num_teams (-10u)); (5, OpenMP, #pragma omp target teams distribute num_teams ()); (7, OpenMP, #pragma omp target teams distribute num_teams (argc); (15, OpenMP, #pragma omp target teams distribute num_teams (argc + argc+z)); (17, OpenMP, #pragma omp target teams distribute num_teams (argc), num_teams (argc+1)); (28, OpenMP, #pragma omp target teams distribute num_teams (3.14)); (-6, OpenMP, #pragma omp target teams distribute num_teams); (5, OpenMP, #pragma omp target teams distribute num_teams (argc))); (7, OpenMP, #pragma omp target teams distribute num_teams (argc > 0 ? argv[1] : argv[2])); (15, OpenMP, #pragma omp target teams distribute num_teams (S1))"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(14, CUDA, double term = thrust::reduce(thrust::device, v_temp.begin(), v_temp.end(), 0.0, thrust::plus<double>());); (7, CUDA, thrust::device_vector<double> v_temp(n);); (7, CUDA, double* dv_ptr = thrust::raw_pointer_cast(v_temp.data());)"}
{"predict": "(1,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp atomic), (60,OpenMP,#pragma omp atomic)", "label": "(2, CUDA, extern __shared__ int sums[];)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (12,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for), (40,MPI,MPIR_Err_create_code(MPI_SUCCESS, MPIR_ERR_RECOVERABLE, \"ADIOI_QUOBYTEFS_aio_wait_fn\", __LINE__, MPI_ERR_IO, \"**mpi_grequest_complete\", 0);)", "label": "(24, MPI, double start_time = MPI_Wtime();); (16, MPI, errcode = MPI_Grequest_complete(aio_reqlist[i]->req);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp barrier)", "label": "(13, OpenMP, printf(\"main parallel 2 from %d of %d\\n\", omp_get_thread_num(), omp_get_num_threads() );); (1, OpenMP, #pragma omp parallel); (2, OpenMP, printf(\"main parallel 1 from %d of %d\\n\", omp_get_thread_num(), omp_get_num_threads() );)"}
{"predict": "(10,OpenMP,#pragma omp parallel for private(kdf_out,t)), (16,OpenMP,#pragma omp for), (48,OpenMP,#pragma omp for)", "label": "(8, OpenMP, #pragma omp parallel for default(none) private(index) shared(count, failed, params, max_threads, local, saved_key, cur_salt, crypt_out)); (15, OpenMP, int t = omp_get_thread_num();)"}
{"predict": "NDRangeKernel(tinfo->tQueue, kernel, 1, NULL,\n153:                                           &vectorCount, NULL, 0, NULL, NULL)))\n154:         {\n155:             vlog_error(\"Error: clEnqueueNDRangeKernel failed! err: %d\\n\",\n156:                        error);\n157:             return error;\n158:         }\n159:     }\n160: \n161:     // Read back the results\n162:     if ((error = clEnqueueReadBuffer(tinfo->tQueue, tinfo->outBuf[0], CL_TRUE,\n163:                                      0, buffer_size, out[0], 0, NULL, NULL)))\n164:     {\n165:         vlog_error(\"Error: clEnqueueReadBuffer failed! err: %d\\n\", error);\n166:         return error;\n167:     }\n168: \n169:     // Validate the results\n170:     if (gHostFill)\n171:     {\n172:         for (auto j = gMinVectorSizeIndex; j < gMaxVectorSizeIndex; j++)\n173:         {\n174:             if ((error = clEnqueueReadBuffer(tinfo->tQueue, tinfo->outBuf[j],\n175:                                              CL_TRUE, 0, buffer_size, out[j],\n176:                                              0, NULL, NULL)))\n177:             {\n178:                 vlog_error(\"Error: clEnqueueReadBuffer failed! err: %d\\n\", error);\n179:                 return error;\n180:             }\n181:         }\n182:     }\n183: \n184:     // Unmap the output arrays\n185:     if (gHostFill)\n186:     {\n187:         for (auto j = gMinVectorSizeIndex; j < gMaxVectorSizeIndex; j++)\n188:         {\n189:             if ((error = clEnqueueUnmapMemObject(\n110:                      tinfo->tQueue, tinfo->outBuf[j], out[j], 0, NULL, NULL)))\n111:             {\n112:                 vlog_error(\"Error", "label": "(146, OpenCL, error = clSetKernelArg(kernel, 1, sizeof(tinfo->inBuf), &tinfo->inBuf);)"}
{"predict": "(9,MPI,MPI_Init(&argc, &argv);)", "label": "(20, MPI, MPI_Info_get(i1, (char *) \"key1\", 64, value, &flag);); (28, MPI, MPI_Info_free(&i1);); (11, MPI, MPI_Info_set(i2, (char *) \"key2\", (char *) \"value2\");); (9, MPI, MPI_Info_set(i1, (char *) \"key1\", (char *) \"value1\");); (11, MPI, MPI_Info_get(i1, (char *) \"key2\", 64, value, &flag);); (25, MPI, MPI_Info_free(&i2);); (3, MPI, MPI_Info_create(&i1);); (3, MPI, MPI_Info_create(&i2);)"}
{"predict": "(4,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for), (45,OpenMP,#pragma omp parallel for), (47,OpenMP,#pragma omp parallel for), (62,OpenMP,#pragma omp parallel for), (78,OpenMP,#pragma omp parallel for), (82,OpenMP,#pragma omp parallel for), (94,OpenMP,#pragma omp parallel for)", "label": "(15, CUDA, multi::array<double, 1, thrust::device_allocator<double>> BB(10, 99.);); (34, CUDA, thrust::copy(begin(rotated(A2)[0]), end(rotated(A2)[0]), begin(rotated(B2)[0]));); (16, CUDA, multi::array<std::complex<double>, 1, thrust::device_allocator<std::complex<double> >> BBB_cpy = BBB;); (14, CUDA, multi::array<std::complex<double>, 1, thrust::device_allocator<std::complex<double> >> BBB(10, 99.);); (97, CUDA, thrust::device_vector<int> D = H;); (78, CUDA, multi::array<int, 1, thrust::device_allocator<int>> H2(4, 99); assert(size(H2) == 4);); (69, CUDA, thrust::host_vector<int> H(4);); (21, CUDA, thrust::copy(begin(B), end(B), begin(C));)"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(8, OpenMP, const std::string comp_name(x.first);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for), (60,OpenMP,#pragma omp parallel for)", "label": "(27, OpenCL, status = clBuildProgram(program, 1, &device, buildOptions, NULL, NULL);); (9, OpenCL, cl_program program = clCreateProgramWithSource(context,1, &source, NULL, &status);)"}
{"predict": "(13,MPI,#MPI_Init(&argc, &argv);) (16,MPI,#MPI_Comm_rank(MPI_COMM_WORLD, &rank);) (17,MPI,#MPI_Comm_size(MPI_COMM_WORLD, &size);) (46,MPI,#MPI_Scatter(full_data[0], nx_local * ny, MPI_DOUBLE, inner_data[0], nx_local * ny, MPI_DOUBLE, 0, MPI_COMM_WORLD);) (52,OpenMP,#pragma omp parallel for) (57,OpenMP,#pragma omp parallel for) (62,OpenMP,#pragma omp parallel for)", "label": "(15, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();)\n(8,OpenMP,#pragma omp parallel for)\n(11,OpenMP,#pragma omp parallel for)\n(18,OpenMP,#pragma omp parallel for)\n(25,OpenMP,#pragma omp parallel for)", "label": "(2, OpenMP, #pragma omp target teams distribute parallel for simd)"}
{"predict": "(8,MPI,#include <mpi.h>); (8,MPI,MPI_Init(&argc, &argv);); (8,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (8,MPI,MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (11,OpenMP,#pragma omp parallel for); (29,OpenMP,#pragma omp parallel for);", "label": "(25, MPI, MPI_Win_unlock(rank, win);); (48, MPI, MPI_Win_unlock_all(win);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (31, MPI, MPI_Win_flush_local(target_rank, win);); (45, MPI, MPI_Win_free(&win);); (23, MPI, MPI_Win_lock_all(0, win);); (11, MPI, MPI_Win_create(winbuf, BUFCNT * sizeof(int), sizeof(int), MPI_INFO_NULL, MPI_COMM_WORLD, &win);); (26, MPI, MPI_Get(locbuf, BUFCNT, MPI_INT, target_rank, 0, BUFCNT, MPI_INT, win);); (18, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(19,OpenMP,#pragma omp parallel for private(start, chunk, epid))", "label": "(27, MPI, return MPI_Waitall(num_ep, block_coll_request, MPI_STATUS_IGNORE);); (8, MPI, return MPI_Wait(&tmprequest, MPI_STATUS_IGNORE);)"}
{"predict": "132:             size_t slicePadding = gEnablePitch ? 3 : 0;\n133: \n134:             set_image_dimensions(\n135:                 &imageInfo2Darray, sizes2DArray[j][0],\n136:                 sizes2DArray[j][1], sizes2DArray[j][2],\n137:                 rowPadding, slicePadding);\n138:             set_image_dimensions(&imageInfo2D, sizes2D[i][0],\n139:                                  sizes2D[i][1], 0, rowPadding, slicePadding);\n140: \n141:             if (gTestMipmaps)\n142:             {\n143:                 imageInfo2D.num_mip_levels =\n144:                     (cl_uint)random_log_in_range(\n145:                         2,\n146:                         (int)compute_max_mip_levels(\n147:                             imageInfo2D.width, imageInfo2D.height, 0),\n148:                         seed);\n149:                 imageInfo2Darray.num_mip_levels =\n150:                     (cl_uint)random_log_in_range(\n151:                         2,\n152:                         (int)compute_max_mip_levels(\n153:                             imageInfo2Darray.width,\n154:                             imageInfo2Darray.height, 0),\n155:                         seed);\n156:                 imageInfo2D.rowPitch = imageInfo2D.width\n157:                     * get_pixel_size(imageInfo2D.format);\n158:                 imageInfo2D.slicePitch = 0;\n159:                 imageInfo2Darray.rowPitch = imageInfo2Darray.width\n160:                     * get_pixel_size(imageInfo2Darray.format);\n161:                 imageInfo2Darray.slicePitch =\n162:                     imageInfo2Darray.rowPitch * imageInfo2Darray.height;\n163:             }\n164: \n165:             if( gDebugTrace )\n166:             {\n167:                 if (reverse)\n168:                     log_info(\"   at size %d,%d,%d to %d,%d\\n\",\n169:                              (int)sizes2DArray[j][0],\n170:", "label": "(28, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_IMAGE_MAX_ARRAY_SIZE, sizeof( maxArraySize ), &maxArraySize, NULL );); (25, OpenCL, int error = clGetDeviceInfo( device, CL_DEVICE_IMAGE2D_MAX_WIDTH, sizeof( maxWidth ), &maxWidth, NULL );); (28, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_GLOBAL_MEM_SIZE, sizeof( memSize ), &memSize, NULL );); (26, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_MAX_MEM_ALLOC_SIZE, sizeof( maxAllocSize ), &maxAllocSize, NULL );); (23, OpenCL, error |= clGetDeviceInfo( device, CL_DEVICE_IMAGE2D_MAX_HEIGHT, sizeof( maxHeight ), &maxHeight, NULL );)"}
{"predict": "(10,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for)", "label": "(3, CUDA, extern __shared__ int shist[];)"}
{"predict": "(27,MPI,MPI_Init(&argc, &argv);)\n(29,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &me);)\n(30,MPI,MPI_Comm_size(MPI_COMM_WORLD, &nt);)\n(32,MPI,*w = MPI_Wtime();)\n(34,MPI,#pragma omp parallel for)\n(76,MPI,#pragma omp barrier)\n(89,MPI,#pragma omp critical)\n(95,MPI,MPI_Finalize();)", "label": "(33, OpenMP, #pragma omp barrier); (28, OpenMP, int me    = omp_get_thread_num();); (25, OpenMP, #pragma omp parallel reduction(min:dtmin) reduction(max:dtmax) reduction(+:dtavg)); (-1, OpenMP, int nt = omp_get_max_threads();); (35, OpenMP, #pragma omp critical)"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(20, CUDA, checkCudaErrors(cudaSetDevice(0));)"}
{"predict": "(12,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for reduction(+:sum)), (31,OpenMP,#pragma omp parallel for reduction(+:elem_cnt)), (37,OpenMP,#pragma omp parallel for reduction(+:elem_cnt))", "label": "(3, TBB, typedef std::vector<T, oneapi::tbb::tbb_allocator<T> > container_type;); (33, TBB, oneapi::tbb::flattened2d<ets_nokey_type> fvs = flatten2d(vs3);); (11, TBB, typedef typename oneapi::tbb::enumerable_thread_specific< container_type, Allocator<container_type>, oneapi::tbb::ets_key_per_instance > ets_tlskey_type;); (9, TBB, typedef typename oneapi::tbb::enumerable_thread_specific< container_type, Allocator<container_type>, oneapi::tbb::ets_no_key > ets_nokey_type;); (44, TBB, oneapi::tbb::flattened2d<ets_nokey_type> fvs2 = flatten2d(vs3, vs3.begin(), std::next(vs3.begin()));); (23, TBB, oneapi::tbb::parallel_reduce ( vs3.range(1), pvrb );); (0, TBB, oneapi::tbb::global_control gc(oneapi::tbb::global_control::max_allowed_parallelism, p);); (10, TBB, oneapi::tbb::parallel_for ( oneapi::tbb::blocked_range<int> (0, N*p, RANGE_MIN), parallel_vector_for_body<T, Allocator>( vs, allocator_name ) );); (42, TBB, const oneapi::tbb::flattened2d<ets_nokey_type>& cfvs2(fvs2);)"}
{"predict": "(23,OpenMP,#pragma omp parallel for)", "label": "(65, OpenMP, #define OP_GOMP_CRITICAL_READ(OP, FLAG))"}
{"predict": "(27,OpenMP,#pragma omp parallel for)\n(44,OpenMP,#pragma omp parallel for)\n(56,OpenMP,#pragma omp parallel for)\n(60,OpenMP,#pragma omp parallel for)", "label": "(17, OpenMP, s += gsl_linalg_cholesky_decomp_unit(v, dv);)"}
{"predict": "(15,OpenMP,#pragma omp parallel for)", "label": "(19, OpenMP, #pragma omp target teams distribute parallel for simd if (parallel: false)); (10, OpenMP, #pragma omp target teams distribute parallel for simd if(simd: true) nontemporal(Arg))"}
{"predict": "(17,OpenMP,#pragma omp parallel for reduction(+:bandwidths[:count]))", "label": "(19, CUDA, cudaSetDevice(currentDevice);)"}
{"predict": "8:     foo();\n99: #pragma omp target\n100: #pragma omp teams\n101:   for (int i = 0; i < 10; ++i)\n102:     foo();\n103: #pragma omp target\n104: #pragma omp teams\n105:   for (int i = 0; i < 10; ++i)\n106:     foo();\n107: #pragma omp target\n108: #pragma omp teams\n109:   for (int i = 0; i < 10; ++i)\n110:     foo();\n111: #pragma omp target\n112: #pragma omp teams\n113:   for (int i = 0; i < 10; ++i)\n114:     foo();\n115: #pragma omp target\n116: #pragma omp teams\n117:   for (int i = 0; i < 10; ++i)\n118:     foo();\n119: #pragma omp target\n120: #pragma omp teams\n121:   for (int i = 0; i < 10; ++i)\n122:     foo();\n123: #pragma omp target\n124: #pragma omp teams\n125:   for (int i = 0; i < 10; ++i)\n126:     foo();\n127: #pragma omp target\n128: #pragma omp teams\n129:   for (int i = 0; i < 10; ++i)\n130:     foo();\n131: #pragma omp target\n132: #pragma omp teams\n133:   for (int i = 0; i < 10; ++i)\n134:     foo();\n135: #pragma omp target\n136: #pragma omp teams\n137:   for (int i = 0; i < 10; ++i)\n138:     foo();\n139: #pragma omp target\n140: #pragma omp teams\n141:   for (int i = 0; i < 10; ++", "label": "(109, OpenMP, #pragma omp distribute parallel for reduction(^ : fl)); (154, OpenMP, #pragma omp distribute parallel for reduction(+ : r)); (117, OpenMP, #pragma omp distribute parallel for reduction(&& : S2::S2sc)); (126, OpenMP, #pragma omp distribute parallel for reduction(+ : h, k, B::x)); (164, OpenMP, #pragma omp distribute parallel for reduction(+ : fl, z)); (39, OpenMP, #pragma omp distribute parallel for reduction(\\)); (6, OpenMP, #pragma omp target); (87, OpenMP, #pragma omp distribute parallel for reduction(+ : ba)); (81, OpenMP, #pragma omp distribute parallel for reduction(max : h.b)); (60, OpenMP, #pragma omp distribute parallel for reduction(&& : argc)); (161, OpenMP, #pragma omp parallel reduction(* : fl)); (28, OpenMP, #pragma omp distribute parallel for reduction(*)); (102, OpenMP, #pragma omp distribute parallel for reduction(&& : S2::S2s)); (6, OpenMP, #pragma omp distribute parallel for reduction +); (85, OpenMP, #pragma omp distribute parallel for reduction(* : ca)); (39, OpenMP, #pragma omp distribute parallel for reduction(| : argc,); (118, OpenMP, #pragma omp distribute parallel for reduction(+ : o)); (125, OpenMP, #pragma omp parallel private(k)); (6, OpenMP, #pragma omp distribute parallel for reduction(); (143, OpenMP, #pragma omp distribute parallel for reduction(max : j)); (44, OpenMP, #pragma omp distribute parallel for reduction(~ : argc)); (138, OpenMP, #pragma omp parallel reduction(min : i)); (143, OpenMP, #pragma omp parallel private(fl)); (26, OpenMP, #pragma omp distribute parallel for reduction(foo : argc); (50, OpenMP, #pragma omp distribute parallel for reduction(^ : S1)); (34, OpenMP, #pragma omp distribute parallel for reduction(|| : argc > 0 ? argv[1] : argv[2])); (-12, OpenMP, #pragma omp distribute parallel for reduction); (153, OpenMP, #pragma omp distribute parallel for reduction(+ : m)); (76, OpenMP, #pragma omp distribute parallel for reduction(- : da)); (95, OpenMP, #pragma omp distribute parallel for reduction(& : e, g)); (144, OpenMP, #pragma omp distribute parallel for reduction(+ : fl)); (-2, OpenMP, #pragma omp distribute parallel for reduction(-); (47, OpenMP, #pragma omp distribute parallel for reduction(+ : a, b, c, d, f)); (112, OpenMP, #pragma omp distribute parallel for reduction(+ : p), reduction(+ : p)); (151, OpenMP, #pragma omp distribute parallel for reduction(task, + : m)); (49, OpenMP, #pragma omp distribute parallel for reduction(min : a, b, c, d, f)); (122, OpenMP, #pragma omp parallel shared(i)); (102, OpenMP, #pragma omp distribute parallel for private(i), reduction(+ : j), reduction(+ : q)); (-4, OpenMP, #pragma omp distribute parallel for reduction()); (-26, OpenMP, #pragma omp teams)"}
{"predict": "(40,OpenMP,#pragma omp parallel for collapse(2))", "label": "(41, CUDA, cudaError_t result = cudaGetLastError();); (41, CUDA, EXPECT_EQ(result, cudaSuccess) << \" - CUDA error: \" << cudaGetErrorString(result);)"}
{"predict": "60, &aes_key, iv, AES_DECRYPT);\n109: \t\t\t}\n110: \t\t\tSHA512_Init(&ctx);\n111: \t\t\tSHA512_Update(&ctx, out, 90);\n112: \t\t\tSHA512_Final(hash, &ctx);\n113: \t\t\tp = (struct KGEncryptedBlock64 *)out;\n114: \t\t\tcracked[index] = (0 == memcmp(hash, p->digest, kDigestSize32));\n115: \t\t}\n116: \t}\n117: \treturn 0;\n118: }", "label": "(8, OpenMP, #pragma omp parallel for)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(54, OpenMP, #pragma omp target parallel for map(alloc : c)); (64, OpenMP, #pragma omp barrier); (44, OpenMP, #pragma omp target exit data map(from : a)); (61, OpenMP, #pragma omp target update from(c) nowait); (21, OpenMP, #pragma omp target enter data map(to : a)); (30, OpenMP, #pragma omp target parallel for)"}
{"predict": ";\n110: \n111:     /* allocate arrays                                                            */\n112:     ran = (u64Int*) malloc((nstarts+LOOKAHEAD) * sizeof(u64Int));\n113:     ranSendBucket = (u64Int**) malloc(Num_procs * sizeof(u64Int*));\n114:     ranRecvBucket = (u64Int**) malloc(Num_procs * sizeof(u64Int*));\n115:     sizeSendBucket = (int*) malloc(Num_procs * sizeof(int));\n116:     sizeRecvBucket = (int*) malloc(Num_procs * sizeof(int));\n117:     senddispls = (int*) malloc(Num_procs * sizeof(int));\n118:     recvdispls = (int*) malloc(Num_procs * sizeof(int));\n119:     Table = (u64Int*) malloc(tablesize * sizeof(u64Int));\n120: \n121:     if (!ran || !ranSendBucket || !ranRecvBucket || !sizeSendBucket ||\n122:         !sizeRecvBucket || !senddispls || !recvdispls || !Table) {\n123:       printf(\"ERROR: Could not allocate enough memory for arrays\\n\");\n124:       error = 1;\n125:       goto ENDOFTESTS;\n126:     }\n127: \n128:     /* generate random numbers                                                    */\n129:     srand(time(NULL));\n130:     for (i=0; i<nstarts+LOOKAHEAD; i++) {\n131:       ran[i] = (u64Int) rand();\n132:     }\n133: \n134:     /* initialize table                                                           */\n135:     for (i=0; i<tablesize; i++) {\n136:       Table[i] = i;\n137:     }\n138: \n139:     /* initialize send and receive buffers                                        */\n140:     for (i=0; i<Num_procs; i++) {\n141:       ranSendBucket[i] = (u64Int*) malloc(LOOKAHEAD * sizeof(u64Int));\n142:       ranRecvBucket[i] = (u64Int*) malloc(LOOKAHEAD * sizeof(u", "label": "(162, MPI, MPI_Bcast(&log2nstarts,      1, MPI_INT,           root, MPI_COMM_WORLD);); (46, MPI, MPI_Comm_size(MPI_COMM_WORLD,&Num_procs);); (155, MPI, MPI_Bcast(&log2nproc,        1, MPI_INT,           root, MPI_COMM_WORLD);); (303, MPI, MPI_Finalize();); (154, MPI, MPI_Bcast(&log2tablesize,    1, MPI_INT,           root, MPI_COMM_WORLD);); (155, MPI, MPI_Bcast(&log2update_ratio, 1, MPI_INT,           root, MPI_COMM_WORLD);); (42, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&my_ID);); (159, MPI, MPI_Bcast(&nupdate,          1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (153, MPI, MPI_Bcast(&nstarts,          1, MPI_INT,           root, MPI_COMM_WORLD);); (154, MPI, MPI_Bcast(&tablesize,        1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (155, MPI, MPI_Bcast(&tablespace,       1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (153, MPI, MPI_Bcast(&loctablesize,     1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (213, MPI, MPI_Barrier(MPI_COMM_WORLD);); (146, MPI, MPI_Bcast(&update_ratio,     1, MPI_INT,           root, MPI_COMM_WORLD);); (280, MPI, MPI_Reduce(&error, &tot_error, 1, MPI_LONG_LONG_INT, MPI_SUM, root, MPI_COMM_WORLD);)"}
{"predict": "(3,MPI,*w = MPI_Wtime(););", "label": "(45, MPI, MPI_Op_free(&dtcmp_reduceop_randroot);); (49, MPI, MPI_Op_free(&dtcmp_reduceop_mms_3uint64t);); (64, MPI, MPI_Comm_free(&dtcmp_comm_self);); (52, MPI, MPI_Type_free(&dtcmp_type_3int);); (56, MPI, MPI_Type_free(&dtcmp_type_3uint64t);)"}
{"predict": "(1,OpenMP,#pragma omp parallel), (10,OpenMP,#pragma omp parallel), (12,OpenMP,#pragma omp parallel)", "label": "(70, OpenMP, kmp_gomp_depends_info_t gomp_depends(depend);); (103, OpenMP, __kmpc_omp_wait_deps(&loc, gtid, ndeps, dep_list, 0, NULL);); (77, OpenMP, __kmpc_omp_task(&loc, gtid, task);); (74, OpenMP, __kmpc_omp_task_with_deps(&loc, gtid, task, ndeps_cnv, dep_list, 0, NULL);)"}
{"predict": "(11,MPI,comm = MPI_COMM_WORLD;); (14,MPI,if (errclass != MPI_ERR_ARG) ++errs;);", "label": "(14, MPI, MPI_Comm_size(comm, &size);); (19, MPI, MPI_Comm_group(dupcomm, &g2);); (28, MPI, MPI_Group_free(&g2);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &worldrank);); (28, MPI, MPI_Comm_free(&dupcomm);); (23, MPI, MPI_Group_free(&basegroup);); (12, MPI, MPI_Comm_group(newcomm, &g1);); (12, MPI, MPI_Comm_dup(comm, &dupcomm);); (15, MPI, mpi_errno = MPI_Group_difference(g1, g2, NULL);); (22, MPI, MPI_Comm_free(&newcomm);); (14, MPI, MPI_Error_class(mpi_errno, &errclass);); (4, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (5, MPI, MPI_Comm_split(comm, 0, size - rank, &newcomm);); (16, MPI, MPI_Group_free(&g1);); (-1, MPI, MPI_Comm_rank(comm, &rank);); (-3, MPI, MPI_Comm_group(comm, &basegroup);)"}
{"predict": "(17,MPI,MPI_Init(NULL, NULL);), (18,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);), (19,MPI,MPI_Comm_size(MPI_COMM_WORLD, &size);), (20,MPI,MPI_Bcast(&NUM_TIMESTEPS, 1, MPI_INT, 0, MPI_COMM_WORLD);), (21,MPI,MPI_Bcast(&NUM_ROLLOUTS, 1, MPI_INT, 0, MPI_COMM_WORLD);), (22,MPI,MPI_Bcast(&CONTROL_DIM, 1, MPI_INT, 0, MPI_COMM_WORLD);), (23,MPI,MPI_Bcast(exponents.data(), CONTROL_DIM, MPI_FLOAT, 0, MPI_COMM_WORLD);), (24,MPI,MPI_Bcast(&full_buffer_size, 1, MPI_INT, 0, MPI_COMM_WORLD);), (26,MPI,MPI_Scatter(colored_noise_d, full_buffer_size / size, MPI_FLOAT, colored_noise_output, full_buffer_size / size, MPI_FLOAT, 0, MPI_COMM_WORLD);), (31,MPI,MPI_Gather(colored_noise_output, full_buffer_size / size, MPI_FLOAT, colored_noise_d, full_buffer_size / size, MPI_FLOAT, 0, MPI_COMM_WORLD);), (32,MPI,MPI_Finalize();), (33,OpenMP,#pragma omp parallel for),", "label": "(9, CUDA, HANDLE_ERROR(cudaMalloc((void**)&colored_noise_d, sizeof(float) * full_buffer_size));); (11, CUDA, cudaStreamCreate(&stream);); (18, CUDA, HANDLE_ERROR(cudaStreamSynchronize(stream));)"}
{"predict": "(4,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for), (35,OpenMP,#pragma omp parallel for), (39,OpenMP,#pragma omp parallel for), (43,OpenMP,#pragma omp parallel for), (47,OpenMP,#pragma omp parallel for), (55,OpenMP,#pragma omp parallel for)", "label": "(41, OpenMP, #pragma omp target teams distribute collapse(4, )); (90, OpenMP, #pragma omp target teams distribute collapse(2) firstprivate(i)); (15, OpenMP, #pragma omp target teams distribute collapse(,); (33, OpenMP, #pragma omp target teams distribute collapse(4,); (74, OpenMP, #pragma omp target teams distribute collapse(-5)); (8, OpenMP, #pragma omp target teams distribute collapse()); (15, OpenMP, #pragma omp target teams distribute collapse(, )); (79, OpenMP, #pragma omp target teams distribute collapse(5 - 5)); (66, OpenMP, #pragma omp target teams distribute collapse(foo())); (73, OpenMP, #pragma omp target teams distribute collapse(0)); (-1, OpenMP, #pragma omp target teams distribute collapse(); (55, OpenMP, #pragma omp target teams distribute collapse(4, 8)); (38, OpenMP, #pragma omp target teams distribute collapse(4 4)); (32, OpenMP, #pragma omp target teams distribute collapse(4)); (-9, OpenMP, #pragma omp target teams distribute collapse); (40, OpenMP, #pragma omp target teams distribute collapse(4, , 4)); (15, OpenMP, #pragma omp target teams distribute collapse(4); (9, OpenMP, #pragma omp target teams distribute collapse 4)); (52, OpenMP, #pragma omp target teams distribute collapse(2.5))"}
{"predict": "(24,OpenMP,#pragma omp parallel for), (48,OpenMP,#pragma omp parallel for), (72,OpenMP,#pragma omp parallel for), (96,OpenMP,#pragma omp parallel for), (120,OpenMP,#pragma omp parallel for), (144,OpenMP,#pragma omp parallel for), (168,OpenMP,#pragma omp parallel for), (192,OpenMP,#pragma omp parallel for), (216,OpenMP,#pragma omp parallel for), (240,OpenMP,#pragma omp parallel for), (264,OpenMP,#pragma omp parallel for), (288,OpenMP,#pragma omp parallel for), (312,OpenMP,#pragma omp parallel for), (336,OpenMP,#pragma omp parallel for), (360,OpenMP,#pragma omp parallel for), (384,OpenMP,#pragma omp parallel for), (408,OpenMP,#pragma omp parallel for), (432,OpenMP,#pragma omp parallel for), (456,OpenMP,#pragma omp parallel for), (480,OpenMP,#pragma omp parallel for), (504,OpenMP,#pragma omp parallel for), (528,OpenMP,#pragma omp parallel for), (552,OpenMP,#pragma omp parallel for), (576,OpenMP,#pragma omp parallel for), (600,OpenMP,#pragma omp parallel for)", "label": "(64, CUDA, CUDA_SAFE_CALL( cudaMemcpy(c, cd, size*sizeof(double), cudaMemcpyDeviceToHost) );); (67, CUDA, CUDA_SAFE_CALL( cudaDeviceReset() );); (12, CUDA, CUDA_SAFE_CALL( cudaMemset(cd, 0, size*sizeof(double)) );); (8, CUDA, CUDA_SAFE_CALL( cudaMalloc((void**)&cd, size*sizeof(double)) );); (62, CUDA, CUDA_SAFE_CALL( cudaFree(cd) );); (12, CUDA, CUDA_SAFE_CALL( cudaThreadSynchronize() );)"}
{"predict": "(40,OpenMP,#pragma omp parallel for private(i,size,cuda_mem,isBuffer,j))", "label": "(71, CUDA, cudaMemcpy(&((int *)cuda_mem)[j*size], ptrs[i], size * Mtypes(i), cudaMemcpyHostToDevice);); (68, CUDA, cudaMalloc(&cuda_mem, size * Mtypes(i) * maxGPT);); (65, CUDA, cudaMemcpy(cuda_mem, ptrs[i], size * Mtypes(i), cudaMemcpyHostToDevice);); (63, CUDA, cudaMalloc(&cuda_mem, size * Mtypes(i));)"}
{"predict": "(16,OpenMP,#pragma omp parallel for reduction(+:accum))", "label": "(3, CUDA, __shared__ typename Epilogue::SharedStorage shared_storage;)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)\n(21,OpenMP,#pragma omp parallel for)\n(31,OpenMP,#pragma omp parallel for)", "label": "(3, CUDA, cudaSetDevice(WORLD.rank());); (11, CUDA, using T = thrust::complex<double>;); (40, CUDA, cudaStreamSynchronize(NULL);); (14, CUDA, thrust::host_vector<T> H = A;)"}
{"predict": "(7,OpenMP,#pragma omp parallel for reduction(min:min_t)), (11,OpenMP,#pragma omp parallel for reduction(max:max_t))", "label": "(9, OpenMP, #pragma omp target teams distribute parallel for reduction(min : min_t)); (6, OpenMP, #pragma omp target data map(to : arr_t[0 : N])); (11, OpenMP, #pragma omp target teams distribute parallel for reduction(max : max_t))"}
{"predict": "].mSize;\n136:                     if ( newPos < mainSize )\n137:                     {\n138:                         log_error(\"ERROR: Validation failure outside of a \"\n139:                                   \"sub-buffer! (Failure at index %zu out of %zu...)\\n\",\n140:                                   newPos, mainSize);\n141:                     }\n142:                     numErrors++;\n143:                     break;\n144:                 }\n145:             }\n146:         }\n147:     }\n148: \n149:     free( mainBufferContents );\n150:     free( actualResults );\n151: \n152:     return numErrors;\n153: }", "label": "(23, OpenCL, mainBuffer = clCreateBuffer( context, CL_MEM_READ_WRITE | CL_MEM_COPY_HOST_PTR, mainSize, mainBufferContents, &error );); (93, OpenCL, error = clEnqueueReadBuffer( queueA, mainBuffer, CL_TRUE, 0, mainSize, actualResults, 0, NULL, NULL );)"}
{"predict": "(17,MPI,MPI_Init(&argc, &argv);)", "label": "(20, MPI, err = MPI_Waitany(10, requests, &index, statuses);); (33, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_ARE_FATAL);); (33, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (15, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(69, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (65, MPI, MPI_Finalized(&mpi_finalized);); (63, MPI, MPI_Initialized(&mpi_initialized);)"}
{"predict": "8:         for (j = rowptr[i]; j < rowptr[i+1]; ++j) {\n129: \t    jcol = colind[j];\n130: \t    if ( spa[jcol] != EMPTY ) { /* External */\n131: \t        colind[j] = spa[jcol];\n132: \t    }\n133: \t}\n134:     }\n135: \n136:     /* ------------------------------------------------------------\n137:        CALCULATE THE RECEIVE COUNTS FOR EACH PROCESS.\n138:        ------------------------------------------------------------*/\n139:     for (p = 0; p < procs; ++p) RecvCounts[p] = 0;\n140:     for (i = 0; i < m_loc; ++i) {\n141:         k = extern_start[i];\n142:         for (j = rowptr[i]; j < rowptr[i+1]; ++j) {\n143: \t    jcol = colind[j];\n144:             p = row_to_proc[jcol];\n145: \t    if ( p != iam ) {\n146: \t        ++RecvCounts[p];\n147: \t    }\n148: \t}\n149:     }\n150: \n151:     /* ------------------------------------------------------------\n152:        LOAD THE VALUES TO BE SENT TO THE OTHER PROCESSES.\n153:        ------------------------------------------------------------*/\n154:     if ( TotalIndSend ) {\n155:         if ( !(val_tosend = doublecomplexMalloc_dist(TotalIndSend)) )\n156:             ABORT(\"Malloc fails for val_tosend[]\");\n157:     }\n158:     for (i = 0; i < m_loc; ++i) {\n159:         for (j = rowptr[i]; j < rowptr[i+1]; ++j) {\n160: \t    jcol = colind[j];\n161: \t    if ( spa[jcol] != EMPTY ) { /* External */\n162: \t        val_tosend[spa[jcol]] = nzval[j];\n163: \t    }\n164: \t}\n165:     }\n166: \n167:     /* ------------------------------------------------------------\n168:        SEND THE X-INDICES TO THE", "label": "(166, MPI, if ( SendCounts[p] ) MPI_Wait(&send_req[p], &status);); (166, MPI, if ( RecvCounts[p] ) MPI_Wait(&recv_req[p], &status);)"}
{"predict": "(21,OpenMP,#pragma omp parallel for)", "label": "(3, OpenMP, mz_uint comp_flags = TDEFL_COMPUTE_ADLER32 | tdefl_create_comp_flags_from_zip_params(level, window_bits, strategy);)"}
{"predict": "(4,OpenMP,#pragma omp parallel for), (7,OpenMP,#pragma omp parallel for), (10,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for), (43,OpenMP,#pragma omp parallel for), (46,OpenMP,#pragma omp parallel for), (49,OpenMP,#pragma omp parallel for), (52,OpenMP,#pragma omp parallel for), (55,OpenMP,#pragma omp parallel for), (58,OpenMP,#pragma omp parallel for), (61,OpenMP,#pragma omp parallel for), (64,OpenMP,#pragma omp parallel for)", "label": "(3, OpenMP, #pragma omp target simd simdlen); (18, OpenMP, #pragma omp target simd simdlen(, )); (70, OpenMP, #pragma omp target simd simdlen(-5)); (31, OpenMP, #pragma omp target simd simdlen(4,); (72, OpenMP, #pragma omp target simd simdlen(0)); (2, OpenMP, #pragma omp target simd simdlen(); (46, OpenMP, #pragma omp target simd simdlen(4, , 4)); (61, OpenMP, #pragma omp target simd simdlen(foo())); (3, OpenMP, #pragma omp target simd simdlen()); (55, OpenMP, #pragma omp target simd simdlen(2.5)); (5, OpenMP, #pragma omp target simd simdlen(,); (31, OpenMP, #pragma omp target simd simdlen(4)); (68, OpenMP, #pragma omp target simd simdlen(5 - 5)); (26, OpenMP, #pragma omp target simd simdlen(4, )); (33, OpenMP, #pragma omp target simd simdlen(4 4)); (45, OpenMP, #pragma omp target simd simdlen(4, 8)); (13, OpenMP, #pragma omp target simd simdlen(4); (7, OpenMP, #pragma omp target simd simdlen 4))"}
{"predict": "3:         VOLATILITY,\n114:         OPT_N\n115:     );\n116: \n117:     printf(\"...comparing GPU and CPU results.\\n\");\n118:     delta = 0.0f;\n119:     sum_delta = 0.0f;\n120:     sum_ref = 0.0f;\n121:     max_delta = 0.0f;\n122:     L1norm = 0.0f;\n123: \n124:     //Compare results\n125:     for (i = 0; i < OPT_N; i++)\n126:     {\n127:         ref = h_PutResultCPU[i];\n128:         sum_ref += ref;\n129:         delta = fabsf(ref - h_PutResultGPU[i]);\n130:         sum_delta += delta;\n131:         if (delta > max_delta)\n132:             max_delta = delta;\n133:         L1norm += delta;\n134:     }\n135: \n136:     printf(\"Comparing CPU and GPU results...\\n\");\n137:     printf(\"L1 norm = %f\\n\", L1norm);\n138:     printf(\"Max delta = %f\\n\", max_delta);\n139:     printf(\"Avg delta = %f\\n\", sum_delta / OPT_N);\n140: \n141:     printf(\"\\nDestroying timer...\\n\");\n142:     sdkDestroyTimer(&hTimer);\n143: \n144:     printf(\"\\nFreeing memory...\\n\");\n145:     free(h_CallResultCPU);\n146:     free(h_PutResultCPU);\n147:     free(h_CallResultGPU);\n148:     free(h_PutResultGPU);\n149:     free(h_StockPrice);\n150:     free(h_OptionStrike);\n151:     free(h_OptionYears);\n152: \n153:     return 0;\n154: }", "label": "(49, CUDA, checkCudaErrors(cudaMalloc((void **)&d_CallResult,   OPT_SZ));); (155, CUDA, checkCudaErrors(cudaFree(d_OptionYears));); (155, CUDA, checkCudaErrors(cudaFree(d_OptionStrike));); (155, CUDA, checkCudaErrors(cudaFree(d_StockPrice));); (155, CUDA, checkCudaErrors(cudaFree(d_PutResult));); (66, CUDA, checkCudaErrors(cudaMemcpy(d_OptionStrike, h_OptionStrike,  OPT_SZ, cudaMemcpyHostToDevice));); (45, CUDA, checkCudaErrors(cudaMalloc((void **)&d_StockPrice,   OPT_SZ));); (43, CUDA, checkCudaErrors(cudaMalloc((void **)&d_PutResult,    OPT_SZ));); (26, CUDA, findCudaDevice(argc, (const char **)argv);); (68, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (42, CUDA, checkCudaErrors(cudaMalloc((void **)&d_OptionStrike, OPT_SZ));); (149, CUDA, checkCudaErrors(cudaFree(d_CallResult));); (81, CUDA, getLastCudaError(\"BlackScholesGPU() execution failed\\n\");); (98, CUDA, checkCudaErrors(cudaMemcpy(h_CallResultGPU, d_CallResult, OPT_SZ, cudaMemcpyDeviceToHost));); (58, CUDA, checkCudaErrors(cudaMemcpy(d_OptionYears,  h_OptionYears,   OPT_SZ, cudaMemcpyHostToDevice));); (97, CUDA, checkCudaErrors(cudaMemcpy(h_PutResultGPU,  d_PutResult,  OPT_SZ, cudaMemcpyDeviceToHost));); (37, CUDA, checkCudaErrors(cudaMalloc((void **)&d_OptionYears,  OPT_SZ));); (53, CUDA, checkCudaErrors(cudaMemcpy(d_StockPrice,  h_StockPrice,   OPT_SZ, cudaMemcpyHostToDevice));)"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (12,MPI,MPI_Barrier(MPI_COMM_WORLD);)", "label": "(20, MPI, t[k].endSync = MPI_Wtime();); (9, MPI, t[k].startOp = MPI_Wtime();); (10, MPI, MPI_Put(&one, sz, MPI_INT, destRank, j, sz, MPI_INT, win);); (12, MPI, t[k].endOp = MPI_Wtime();); (15, MPI, MPI_Win_wait(win);); (2, MPI, MPI_Win_post(exposureGroup, 0, win);); (12, MPI, MPI_Win_complete(win);); (-1, MPI, MPI_Barrier(MPI_COMM_WORLD);); (0, MPI, MPI_Win_start(accessGroup, 0, win);)"}
{"predict": "(18,OpenMP,#pragma omp parallel for)", "label": "(17, CUDA, cudaMemsetAsync(nulls.data(), 0xcccc, nulls.size(), rmm::cuda_stream_default.value());)"}
{"predict": "(14,MPI,MPI_Init(&argc, &argv);), (16,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);), (17,MPI,MPI_Comm_size(MPI_COMM_WORLD, &nprocs);), (19,MPI,MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);), (28,MPI,MPI_Bcast(filename, len + 10, MPI_CHAR, 0, MPI_COMM_WORLD);), (32,MPI,MPI_Scatter(buf, nints / nprocs, MPI_INT, local_buf, nints / nprocs, MPI_INT, 0, MPI_COMM_WORLD);), (43,MPI,MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_CREATE | MPI_MODE_RDWR, MPI_INFO_NULL, &fh);), (48,MPI,MPI_File_set_view(fh, rank * (nints / nprocs) * sizeof(int), MPI_INT, MPI_INT, \"native\", MPI_INFO_NULL);), (52,MPI,MPI_File_iwrite(fh, local_buf, nints / nprocs, MPI_INT, &request);), (58,MPI,MPI_Wait(&request, &status);), (62,MPI,MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_CREATE | MPI_MODE_RDWR, MPI_INFO_NULL, &fh);), (67,MPI,MPI_File_set_view(fh, rank * (nints / nprocs) * sizeof(int), MPI_INT, MPI_INT, \"native\", MPI_INFO_NULL);), (72,MPI,MPI_File_iwrite(fh, local_buf, nints / nprocs, MPI_INT, &request);), (78,MPI,MPI_Wait(&request, &status);), (82,MPI,MPI_Reduce(&errs, &toterrs, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);), (100,MPI,MPI_Finalize();)", "label": "(29, MPI, MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);); (51, MPI, errcode = MPI_File_set_view(fh, 0, MPI_INT, MPI_INT, \"native\", MPI_INFO_NULL);); (8, MPI, MPI_Init(&argc, &argv);); (59, MPI, MPI_Wait(&request, &status);); (112, MPI, MPI_Finalize();); (25, MPI, MPI_Bcast(filename, len + 10, MPI_CHAR, 0, MPI_COMM_WORLD);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (96, MPI, MPI_Allreduce(&errs, &toterrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (74, MPI, errcode = MPI_File_iread(fh, buf, nints, MPI_INT, &request);); (14, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (47, MPI, errcode = MPI_File_iwrite(fh, buf, nints, MPI_INT, &request);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (10,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for)", "label": "(30, MPI, MPI_Get(local_b, COUNT, MPI_INT, 0, 0, COUNT, MPI_INT, win[id]);); (11, MPI, MPI_Win_fence(0, win[id]);); (43, MPI, MPI_Free_mem(local_b);); (11, MPI, MPI_Put(local_b, COUNT, MPI_INT, 0, 0, COUNT, MPI_INT, win[id]);); (4, MPI, MPI_Alloc_mem(COUNT * sizeof(int), MPI_INFO_NULL, &local_b);)"}
{"predict": "(MPI_File_set_view(fh, disp, MPI_INT, MPI_INT, \"native\", MPI_INFO_NULL));\n122:     MPI_CHECK(MPI_File_write_all(fh, buf, len, MPI_INT, &status));\n123:     MPI_CHECK(MPI_File_close(&fh));\n124: \n125:     MPI_Barrier(MPI_COMM_WORLD);\n126: \n127:     for (i = 0; i < SIZE; i++)\n128:         buf[i] = -1;\n129: \n130:     MPI_CHECK(MPI_File_open(MPI_COMM_WORLD, filename,\n131:                             MPI_MODE_RDONLY, MPI_INFO_NULL, &fh));\n132:     MPI_CHECK(MPI_File_set_view(fh, disp, MPI_INT, MPI_INT, \"native\", MPI_INFO_NULL));\n133:     MPI_CHECK(MPI_File_read_all(fh, buf, len, MPI_INT, &status));\n134:     MPI_CHECK(MPI_File_close(&fh));\n135: \n136:     MPI_Barrier(MPI_COMM_WORLD);\n137: \n138:     for (i = 0; i < SIZE; i++) {\n139:         if (!mynod) {\n140:             if ((i % 2) && (buf[i] != -1)) {\n141:                 errs++;\n142:                 fprintf(stderr, \"Process %d: buf %d is %d, should be -1\\n\", mynod, i, buf[i]);\n143:             }\n144:             if (!(i % 2) && (buf[i] != i)) {\n145:                 errs++;\n146:                 fprintf(stderr, \"Process %d: buf %d is %d, should be %d\\n\", mynod, i, buf[i], i);\n147:             }\n148:         } else {\n149:             if ((i % 2) && (buf[i] != i + mynod * SIZE)) {\n150:                 errs++;\n151:                 fprintf(stderr, \"Process %d: buf %d is %d, should be %d\\n\",\n152:                         mynod, i, buf[i], i + mynod * SIZE);\n153:             }\n154:             if (!(i % 2) && (buf[i]", "label": "(197, MPI, MPI_CHECK(MPI_File_write_all(fh, buf, SIZE, MPI_INT, &status));); (9, MPI, MPI_Init(&argc, &argv);); (80, MPI, MPI_CHECK(MPI_File_write_all(fh, buf, 1, newtype, &status));); (42, MPI, MPI_Type_vector(SIZE / 2, 1, 2, MPI_INT, &typevec);); (49, MPI, MPI_Type_create_struct(1, &len, &disp, &typevec, &tmptype);); (73, MPI, MPI_CHECK(MPI_File_set_view(fh, 0, MPI_INT, newtype, \"native\", MPI_INFO_NULL));); (67, MPI, MPI_Barrier(MPI_COMM_WORLD);); (107, MPI, MPI_CHECK(MPI_File_close(&fh));); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mynod);); (48, MPI, MPI_Type_free(&tmptype);); (136, MPI, MPI_CHECK(MPI_File_read_at_all(fh, mynod * (SIZE / 2) * sizeof(int), buf, 1, newtype, &status));); (5, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (192, MPI, MPI_CHECK(MPI_File_read_at_all(fh, 0, buf, SIZE, MPI_INT, &status));); (22, MPI, MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);); (54, MPI, MPI_Error_class(err, &errorclass);); (-4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nprocs);); (42, MPI, MPI_Type_commit(&newtype);); (72, MPI, MPI_CHECK(MPI_File_read_at_all(fh, 0, buf, 1, newtype, &status));); (36, MPI, MPI_Type_free(&typevec);); (37, MPI, MPI_Type_create_resized(tmptype, 0, extent, &newtype);); (220, MPI, MPI_Finalize();); (15, MPI, MPI_Bcast(filename, len + 1, MPI_CHAR, 0, MPI_COMM_WORLD);); (206, MPI, MPI_Allreduce(&errs, &toterrs, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);); (214, MPI, MPI_Type_free(&newtype);)"}
{"predict": "(18,OpenMP,#pragma omp parallel for)", "label": "(10, TBB, tbb::filter_t<void, void> one_filter( filter_table[i], i_filter );); (15, TBB, tbb::atomic<int> counter;); (11, TBB, tbb::parallel_pipeline( n_tokens, one_filter );)"}
{"predict": "(34,OpenMP,#pragma omp parallel for), (62,OpenMP,#pragma omp parallel for)", "label": "(47, CUDA, CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));)"}
{"predict": "(12,MPI,*w = MPI_Wtime();); (16,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for)", "label": "(34, CUDA, cudaFree(d_inPtr);); (18, CUDA, cudaMalloc((void**)&d_outPtr, size * sizeof(float));); (24, CUDA, cudaMemcpy(&result_gpu, &d_outPtr[0], sizeof(float), cudaMemcpyDeviceToHost);); (18, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);); (29, CUDA, cudaFree(d_outPtr);); (13, CUDA, cudaMalloc((void**)& d_inPtr, size * sizeof(float));)"}
{"predict": "150:     printf(\"ERROR: Requested %d threads, but got %d\\n\", nthread_input, nthread);\n151:   }\n152:   }\n153: \n154:   }\n155: \n156:   /* initialize Table */\n157:   #pragma omp parallel for\n158:   for (i=0; i<tablesize; i++) {\n159:     Table[i] = i;\n160:   }\n161: \n162:   /* initialize random number generator */\n163:   ran = (u64Int *) prk_malloc(nstarts * sizeof(u64Int));\n164:   if (!ran) {\n165:     printf(\"ERROR: Could not allocate space of \"FSTR64U\"  bytes for random numbers\\n\",\n166:            (u64Int) (nstarts * sizeof(u64Int)));\n167:     exit(EXIT_FAILURE);\n168:   }\n169: \n170:   /* initialize random number generator */\n171:   prk_srand(12345);\n172:   for (i=0; i<nstarts; i++) {\n173:     ran[i] = prk_rand();\n174:   }\n175: \n176:   /* do random accesses and update Table */\n177:   for (round = 0; round < nupdate; round++) {\n178:     index = ran[round%nstarts] % tablesize;\n179:     Table[index]++;\n180:   }\n181: \n182:   /* validate Table */\n183:   for (i=0; i<tablesize; i++) {\n184:     if (Table[i] != 1) {\n185:       error++;\n186:     }\n187:   }\n188: \n189:   /* print results */\n190:   if (num_error) {\n191:     printf(\"ERROR: number of threads is not correct\\n\");\n192:   }\n193:   if (error) {\n194:     printf(\"ERROR: %lld elements of the table are not correct\\n\", error);\n19", "label": "(202, OpenMP, #pragma omp barrier); (150, OpenMP, nthread = omp_get_num_threads();); (197, OpenMP, #pragma omp for); (146, OpenMP, #pragma omp master); (54, OpenMP, omp_set_num_threads(nthread_input);); (137, OpenMP, #pragma omp parallel private(i, j, ran, round, index, my_ID) reduction(+:error)); (223, OpenMP, #pragma omp atomic); (140, OpenMP, my_ID   = omp_get_thread_num();)"}
{"predict": ", &handle\n141:                 );\n142:     if (sort_rc != DTCMP_SUCCESS) {\n143:         MFU_ABORT(1, \"Failed to sort file list\");\n144:     }\n145: \n146:     /* copy sorted data back to flist2 */\n147:     idx = 0;\n148:     sortptr = (char*) outsortbuf;\n149:     while (idx < outsortcount) {\n150:         /* unpack file element */\n151:         int i;\n152:         for (i = 0; i < nfields; i++) {\n153:             if (fields[i] == FILENAME) {\n154:                 const char* name = sortptr;\n155:                 sortptr += lengths[i];\n156:             }\n157:         }\n158: \n159:         /* copy file element */\n160:         mfu_flist_file_set_name(flist2, idx, name);\n161: \n162:         sortptr += mfu_flist_file_unpack(sortptr, flist2, idx);\n163: \n164:         idx++;\n165:     }\n166: \n167:     /* free sort buffer */\n168:     MFU_FREE(&sortbuf);\n169:     MFU_FREE(&outsortbuf);\n170: \n171:     /* free MPI types */\n172:     MPI_Type_free(&dt_sat);\n173:     MPI_Type_free(&dt_filepath);\n174:     MPI_Type_free(&dt_key);\n175:     MPI_Type_free(&dt_keysat);\n176:     MPI_Op_free(&op_filepath);\n177:     MPI_Op_free(&op_filepath_rev);\n178:     MPI_Op_free(&op_key);\n179: \n180:     /* free keys */\n181:     for (int i = 0; i < nfields; i++) {\n182:         MPI_Type_free(&types[i]);\n183:         DTCMP_Op_free(&ops[i]);\n184:     }\n185: \n186:     /* return sorted list */\n187:     *", "label": "(106, MPI, MPI_Type_get_extent(dt_key, &key_lb, &key_extent);); (174, MPI, MPI_Type_free(&dt_key);); (16, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (20, MPI, MPI_Type_commit(&dt_filepath);); (169, MPI, MPI_Type_free(&dt_keysat);); (170, MPI, MPI_Type_free(&dt_filepath);); (107, MPI, MPI_Type_get_extent(dt_sat, &sat_lb, &sat_extent);); (102, MPI, MPI_Type_get_extent(dt_keysat, &keysat_lb, &keysat_extent);); (5, MPI, MPI_Type_contiguous((int)bytes, MPI_BYTE, &dt_sat);); (13, MPI, MPI_Type_contiguous((int)chars, MPI_CHAR, &dt_filepath);); (171, MPI, MPI_Type_free(&dt_sat);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for)", "label": "(12, OpenMP, iget = omp_get_teams_thread_limit();); (3, OpenMP, omp_set_num_teams(iset);); (3, OpenMP, iget = omp_get_max_teams();); (8, OpenMP, omp_set_teams_thread_limit(iset);)"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for)", "label": "(12, TBB, test_resets<int, tbb::flow::queue_node<int> >();); (2, TBB, tbb::task_scheduler_init init(p);); (7, TBB, stop = tbb::tick_count::now();); (18, TBB, test_buffer_extract<tbb::flow::queue_node<int> >().run_tests();); (-3, TBB, tbb::tick_count start = tbb::tick_count::now(), stop;); (8, TBB, test_resets<float, tbb::flow::queue_node<float> >();)"}
{"predict": "(17,OpenMP,#pragma omp parallel for reduction(max:maxError))", "label": "(16, CUDA, cudaDeviceSynchronize();); (6, CUDA, cudaMallocManaged(&y, N*sizeof(float));); (4, CUDA, cudaMallocManaged(&x, N*sizeof(float));); (23, CUDA, cudaFree(y);); (21, CUDA, cudaFree(x);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (12,OpenMP,#pragma omp parallel for)", "label": "(13, CUDA, cudaError_t err = cudaFree(ptr);)"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for)", "label": "(17, OpenMP, #pragma omp parallel for num_threads(num_devices)); (10, OpenMP, #pragma omp target teams distribute parallel for); (3, OpenMP, size_t num_devices = omp_get_num_devices();); (17, OpenMP, #pragma omp target teams distribute parallel for device(omp_get_thread_num()))"}
{"predict": "(2,MPI,*w = MPI_Wtime();); (9,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank); MPI_Comm_size(MPI_COMM_WORLD, &size);); (10,MPI,MPI_Reduce(&time, &min_time, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);); (11,MPI,MPI_Reduce(&time, &max_time, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);); (12,MPI,MPI_Reduce(&time, &sum_time, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD););", "label": "(9, MPI, MPI_Reduce(&time, &sum_time, 1, MPI_LONG_LONG, MPI_SUM, 0, comm_);); (6, MPI, MPI_Reduce(&time, &max_time, 1, MPI_LONG_LONG, MPI_MAX, 0, comm_);); (10, MPI, MPI_Comm_size(comm_, &size);); (8, MPI, MPI_Comm_rank(comm_, &rank);); (4, MPI, MPI_Reduce(&time, &min_time, 1, MPI_LONG_LONG, MPI_MIN, 0, comm_);)"}
{"predict": "(28,OpenMP,#pragma omp parallel for)\n(67,OpenMP,#pragma omp parallel for)\n(102,OpenMP,#pragma omp parallel for)", "label": "(9, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &MSG_COMM_WORLD);); (4, MPI, MPI_Init_thread(&argc, &argv, MPI_THREAD_MULTIPLE, &provided);); (63, MPI, MPI_Type_size(type, &type_size);); (16, MPI, MPI_Barrier(MSG_COMM_WORLD);); (130, MPI, MPI_Comm_free(&MSG_COMM_WORLD);); (131, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_rank(MSG_COMM_WORLD, &rank);); (5, MPI, MPI_Comm_size(MSG_COMM_WORLD, &size);)"}
{"predict": "(16,OpenMP,#pragma omp parallel for)\n(20,OpenMP,#pragma omp parallel for)\n(24,OpenMP,#pragma omp parallel for)\n(28,OpenMP,#pragma omp parallel for)", "label": "(37, CUDA, cudaFreeHost( hptr );)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for)", "label": "(17, OpenMP, #pragma omp target simd aligned(X::x : ::z)); (2, OpenMP, #pragma omp target simd aligned(B:bfoo())); (23, OpenMP, #pragma omp target simd aligned(B::bfoo())); (17, OpenMP, #pragma omp target simd aligned(B,rp,::z: X::x)); (24, OpenMP, #pragma omp target simd aligned(B::ib,B:C1+C2)); (6, OpenMP, #pragma omp target simd aligned(z:B:bfoo())); (0, OpenMP, #pragma omp target simd aligned(B::ib:B:bfoo())); (1, OpenMP, #pragma omp target simd aligned(B:B::bfoo())); (14, OpenMP, #pragma omp target simd aligned(::z))"}
{"predict": "(2,OpenMP,#pragma omp parallel for)", "label": "(10, OpenMP, __kmpc_omp_task(NULL, 0, task);); (2, OpenMP, printf(\"%d: creating task\\n\", omp_get_thread_num());)"}
{"predict": "(28,OpenMP,#pragma omp parallel for)", "label": "(25, TBB, tbb::task_scheduler_init init;); (25, TBB, tbb::parallel_for( tbb::blocked_range<int>(0,n_cams), vtask);)"}
{"predict": "(8,MPI,*w = MPI_Wtime(););", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(64,MPI,gpu_timer.Start();), (65,MPI,Kernel<<<grid_size, block_size>>>(global_barrier, iterations);), (66,MPI,gpu_timer.Stop();), (72,MPI,printf(\"%d iterations, %f total elapsed millis, %f avg elapsed millis\\n\", iterations, gpu_timer.ElapsedMillis(), avg_elapsed);)", "label": "(34, CUDA, CubDebugExit(cudaGetDevice(&device_ordinal));); (42, CUDA, CubDebugExit(cudaDeviceGetAttribute(&max_block_threads, cudaDevAttrMaxThreadsPerBlock, device_ordinal));); (40, CUDA, CubDebugExit(cudaDeviceGetAttribute(&sm_count, cudaDevAttrMultiProcessorCount, device_ordinal));); (69, CUDA, retval = CubDebug(cudaDeviceSynchronize());)"}
{"predict": "(91,OpenMP,#pragma omp parallel for), (93,OpenMP,#pragma omp parallel for)", "label": "(92, OpenMP, double tic = omp_get_wtime();); (108, OpenMP, #pragma omp target exit data map(from: u[0:n*n])); (116, OpenMP, double stop = omp_get_wtime();); (103, OpenMP, double toc = omp_get_wtime();); (6, OpenMP, double start = omp_get_wtime();); (80, OpenMP, #pragma omp target enter data map(to: u[0:n*n], u_tmp[0:n*n]))"}
{"predict": "(27,MPI,*w = MPI_Wtime();); (46,MPI,*w = MPI_Wtime();); (72,MPI,*w = MPI_Wtime();); (93,MPI,*w = MPI_Wtime(););", "label": "(46, OpenCL, actualSampler = clCreateSampler( context, CL_FALSE, CL_ADDRESS_NONE, CL_FILTER_NEAREST, &error );); (55, OpenCL, clEnqueueWriteBuffer( queue, results, CL_TRUE, 0, resultValuesSize, resultValues, 0, NULL, NULL );); (80, OpenCL, error = clEnqueueReadBuffer( queue, results, CL_TRUE, 0, resultValuesSize, resultValues, 0, NULL, NULL );); (75, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 3, NULL, threads, NULL, 0, NULL, NULL );); (60, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_mem ), &read_write_image );); (98, OpenCL, clReleaseMemObject(results);); (96, OpenCL, clReleaseSampler(actualSampler);); (100, OpenCL, clReleaseMemObject(read_write_image);); (96, OpenCL, clReleaseMemObject(read_only_image);); (51, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_mem ), &read_only_image );); (59, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_mem ), &results );); (56, OpenCL, error = clSetKernelArg( kernel, idx++, sizeof( cl_sampler ), &actualSampler );); (38, OpenCL, cl_mem results = clCreateBuffer( context, 0, imageInfo->width * imageInfo->height * imageInfo->arraySize * sizeof(cl_int), NULL, &error);)"}
{"predict": "(19,MPI,MPI_Init(&argc, &argv);), (21,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);), (22,MPI,MPI_Comm_size(MPI_COMM_WORLD, &np);), (43,MPI,MPI_Bcast(&num_operator, 1, MPI_INT, 0, MPI_COMM_WORLD);), (44,MPI,MPI_Scatter(h_a, size, MPI_FLOAT, &h_a[rank * size], size, MPI_FLOAT, 0, MPI_COMM_WORLD);), (45,MPI,MPI_Scatter(h_b, size, MPI_FLOAT, &h_b[rank * size], size, MPI_FLOAT, 0, MPI_COMM_WORLD);), (46,MPI,MPI_Scatter(h_c, size, MPI_FLOAT, &h_c[rank * size], size, MPI_FLOAT, 0, MPI_COMM_WORLD);), (47,MPI,MPI_Scatter(d_a, size, MPI_FLOAT, &d_a[rank * size], size, MPI_FLOAT, 0, MPI_COMM_WORLD);), (48,MPI,MPI_Scatter(d_b, size, MPI_FLOAT, &d_b[rank * size], size, MPI_FLOAT, 0, MPI_COMM_WORLD);), (49,MPI,MPI_Scatter(d_c, size, MPI_FLOAT, &d_c[rank * size], size, MPI_FLOAT, 0, MPI_COMM_WORLD);), (51,MPI,MPI_Gather(&h_c[rank * size], size, MPI_FLOAT, h_c, size, MPI_FLOAT, 0, MPI_COMM_WORLD);), (52,MPI,MPI_Finalize();)", "label": "(51, OpenMP, omp_set_num_threads(num_operator);); (16, MPI, MPI_Init(&argc, &argv);); (37, CUDA, cudaMalloc((void**)&d_a, bufsize);); (80, CUDA, cudaFree(d_a);); (24, CUDA, cudaMallocHost((void**)&h_a, bufsize);); (25, CUDA, cudaMallocHost((void**)&h_c, bufsize);); (58, CUDA, cudaDeviceSynchronize();); (78, CUDA, cudaFree(d_c);); (44, OpenMP, #pragma omp parallel); (75, CUDA, cudaFree(d_b);); (30, CUDA, cudaMalloc((void**)&d_b, bufsize);); (43, OpenMP, int i = omp_get_thread_num();); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (16, CUDA, cudaMallocHost((void**)&h_b, bufsize);); (75, CUDA, cudaFreeHost(h_b);); (73, CUDA, cudaFreeHost(h_a);); (25, CUDA, cudaMalloc((void**)&d_c, bufsize);); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &np);); (74, MPI, MPI_Finalize();); (71, CUDA, cudaFreeHost(h_c);)"}
{"predict": "(9,OpenMP,#pragma omp parallel for reduction(+:sum))", "label": "(7, OpenMP, #pragma omp simd private(myind, g_ind), linear(ind), aligned(arr), linear(uval(ref)))"}
{"predict": "(13,MPI,*w = MPI_Wtime();)", "label": "(29, MPI, MPI_Type_contiguous(1, struct_type, &contig);); (51, MPI, MPI_Type_free(&struct_type);); (26, MPI, MPI_Type_create_struct(3, blens, displs, types, &struct_type);); (27, MPI, MPI_Type_commit(&struct_type);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (26, MPI, MPI_Type_commit(&contig);); (45, MPI, MPI_Get_elements(&status, contig, &count);); (34, MPI, MPI_Sendrecv(&sendbuf, 1, contig, 0, 0, &recvbuf, 1, contig, 0, 0, MPI_COMM_SELF, &status);); (45, MPI, MPI_Type_free(&contig);)"}
{"predict": "(5,MPI, int rank = 0; int size = 1; MPI_Init(&argc, &argv); MPI_Comm_rank(MPI_COMM_WORLD, &rank); MPI_Comm_size(MPI_COMM_WORLD, &size);); (7,OpenMP, #pragma omp parallel if(size > 1)); (9,OpenMP, #pragma omp barrier);", "label": "(4, MPI, MPI_Init(&argc, &argv);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (11, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(16,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for), (50,OpenMP,#pragma omp parallel for)", "label": "(28, TBB, tbb_list_time = (tbb::tick_count::now() - t0).seconds();); (42, TBB, serial_tree_time = (tbb::tick_count::now() - t0).seconds();); (15, TBB, serial_list_time = (tbb::tick_count::now() - t0).seconds();); (12, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (50, TBB, tbb_tree_time = (tbb::tick_count::now() - t0).seconds();)"}
{"predict": "(4,OpenMP,#pragma omp parallel for collapse(4))", "label": "(31, OpenMP, #pragma omp target parallel for collapse(4); (54, OpenMP, #pragma omp target parallel for collapse(4, , 4)); (43, OpenMP, #pragma omp target parallel for collapse(4)); (2, OpenMP, #pragma omp target parallel for collapse); (22, OpenMP, #pragma omp target parallel for collapse 4)); (73, OpenMP, #pragma omp target parallel for collapse(-5)); (3, OpenMP, #pragma omp target parallel for collapse(); (79, OpenMP, #pragma omp target parallel for collapse(5 - 5)); (82, OpenMP, #pragma omp target parallel for collapse(2) firstprivate(i)); (41, OpenMP, #pragma omp target parallel for collapse(4 4)); (60, OpenMP, #pragma omp target parallel for collapse(2.5)); (63, OpenMP, #pragma omp target parallel for collapse(foo())); (29, OpenMP, #pragma omp target parallel for collapse(4, )); (0, OpenMP, #pragma omp target parallel for collapse()); (68, OpenMP, #pragma omp target parallel for collapse(0)); (2, OpenMP, #pragma omp target parallel for collapse(,); (20, OpenMP, #pragma omp target parallel for collapse(4,); (4, OpenMP, #pragma omp target parallel for collapse(, )); (48, OpenMP, #pragma omp target parallel for collapse(4, 8))"}
{"predict": "(10,MPI,*w = MPI_Wtime();); (25,OpenMP,#pragma omp parallel for)", "label": "(32, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (34, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (...)\n\n### ** Explanation **\n\n1. **MPI Parallelization (Line 5)**: The code starts with a function `cudaGetValueMismatch` which is likely executed in a parallel environment. Inserting `MPI_Wtime()` at the beginning of the function will help in measuring the execution time across different processes.\n\n2. **OpenMP Parallelization (Line 8)**: The function `getNumErrors<<<1, 1>>>(numErr_d);` is a CUDA kernel call. Since CUDA kernels are designed to run in parallel on a GPU, using OpenMP to parallelize the loop or sections of the kernel execution can help in optimizing the performance. However, OpenMP is typically used for CPU-bound tasks, so this might not be the best choice for CUDA kernels. For CUDA, it's more common to use CUDA's own parallel execution model. If the kernel is not CUDA-bound and involves CPU-intensive operations, OpenMP could be used.\n\n### ** Additional Notes **\n\n- If the goal is to parallelize the entire function across multiple processes, MPI would be the appropriate choice.\n- If the goal is to parallelize the execution of the CUDA kernel within a single process, CUDA's own parallel execution model should be used.\n\nSince the code is CUDA-based, the focus should be on optimizing the CUDA kernel execution rather than using OpenMP for parallelization within the kernel.", "label": "(5, CUDA, err = cudaMalloc(&numErr_d, sizeof(int));); (25, CUDA, printf(\"Cuda Main: cudaFree failed with %s\\n\", cudaGetErrorString(err));); (11, CUDA, err = cudaDeviceSynchronize();); (4, CUDA, printf(\"Cuda Main: cudaMemcpy failed with %s\\n\", cudaGetErrorString(err));); (20, CUDA, err = cudaFree(numErr_d);); (13, CUDA, err = cudaMemcpy(&numErr_h, numErr_d, sizeof(int), cudaMemcpyDeviceToHost);); (2, CUDA, cudaFree(numErr_d);)"}
{"predict": "(1,MPI,*w = MPI_Wtime();); (24,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for)", "label": "(5, CUDA, __shared__ uint s_Hist[HISTOGRAM256_THREADBLOCK_MEMORY];)"}
{"predict": "(5,MPI,int par_rank, int par_size; MPI_Comm_rank(comm, &par_rank); MPI_Comm_size(comm, &par_size);), (15,MPI,ASCENT_INFO(\"Rank \" << par_rank << \" of \" << par_size << \" reporting\");), (44,MPI,ascent_opts[\"mpi_comm\"] = comm;), (48,MPI,ascent.open(ascent_opts); ascent.publish(data); ascent.execute(actions); ascent.close();),", "label": "(55, MPI, ascent_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (7, MPI, MPI_Comm_rank(comm, &par_rank);); (7, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(18,OpenMP,#pragma omp parallel for reduction(+:sum))", "label": "(17, OpenMP, #pragma omp parallel for default(shared) private(i))"}
{"predict": "(26,MPI,*w = MPI_Wtime(););", "label": "(35, CUDA, cudaFree(res_dev);); (28, CUDA, cudaMalloc(&res_dev, sizeof(float));); (30, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(28,OpenMP,#pragma omp parallel for)", "label": "(43, MPI, ascent_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (15, MPI, MPI_Comm_rank(comm, &par_rank);); (15, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "zrf(\n117:         handle,\n118:         global_m_a,\n119:         global_n_a,\n120:         h_A.data(),\n121:         llda,\n122:         h_A.data(),\n123:         llda,\n124:         h_A.data(),\n125:         llda,\n126:         descA));\n127: \n128:     CUBLASMP_CHECK(cublasMpGeqrf(\n129:         handle,\n130:         global_m_c,\n131:         global_n_c,\n132:         h_C.data(),\n133:         lldc,\n134:         h_C.data(),\n135:         lldc,\n136:         h_C.data(),\n137:         lldc,\n138:         descC));\n139: \n140:     CUBLASMP_CHECK(cublasMpGezrs( // This function call can be parallelized\n141:         handle,\n142:         global_m_a,\n143:         global_n_a,\n144:         h_A.data(),\n145:         llda,\n146:         h_A.data(),\n147:         llda,\n148:         h_A.data(),\n149:         llda,\n150:         descA));\n151: \n152:     CUBLASMP_CHECK(cublasMpGeqrs( // This function call can be parallelized\n153:         handle,\n154:         global_m_c,\n155:         global_n_c,\n156:         h_C.data(),\n157:         lldc,\n158:         h_C.data(),\n159:         lldc,\n160:         h_C.data(),\n161:         lldc,\n162:         descC));\n163: \n164:     cublasMpGridDestroy(grid);\n165:     cublasMpMatrixDescriptorDestroy(descA);\n166:     cublasMpMatrixDescriptorDestroy(descC);\n167:     cublasMpDestroy(handle);\n168:     cal_comm_destroy(cal_comm);\n169: \n170:     return 0", "label": "(197, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (109, CUDA, CUDA_CHECK(cudaMemcpyAsync(d_A, h_A.data(), llda * loc_n_a * sizeof(double), cudaMemcpyHostToDevice, stream));); (197, MPI, MPI_Barrier(MPI_COMM_WORLD);); (48, CUDA, CUDA_CHECK(cudaFree(nullptr));); (186, CUDA, CUDA_CHECK(cudaFreeAsync(d_C, stream));); (45, CUDA, CUDA_CHECK(cudaSetDevice(local_device));); (20, MPI, MPI_Init(nullptr, nullptr);); (184, CUDA, CUDA_CHECK(cudaFreeAsync(d_work, stream));); (99, CUDA, CUDA_CHECK(cudaMallocAsync(&d_A, llda * loc_n_a * sizeof(double), stream));); (141, MPI, const double begin = MPI_Wtime();); (179, CUDA, CUDA_CHECK(cudaFreeAsync(d_A, stream));); (33, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (96, CUDA, CUDA_CHECK(cudaMallocAsync(&d_C, lldc * loc_n_c * sizeof(double), stream));); (98, CUDA, CUDA_CHECK(cudaMemcpyAsync(d_C, h_C.data(), lldc * loc_n_c * sizeof(double), cudaMemcpyHostToDevice, stream));); (55, CUDA, CUDA_CHECK(cudaStreamCreate(&stream));); (186, MPI, MPI_Finalize();); (27, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nranks);); (126, CUDA, CUDA_CHECK(cudaMallocAsync(&d_work, workspaceInBytesOnDevice, stream));); (157, MPI, const double end = MPI_Wtime();)"}
{"predict": "(15,OpenMP,#pragma omp parallel for)", "label": "(1, CUDA, if (!IsGoogleCudaEnabled()) return;)"}
{"predict": "(11,OpenMP,#pragma omp parallel for)", "label": "(3, CUDA, extern __shared__ unsigned scx1[];)"}
{"predict": "(1, MPI, *w = MPI_Wtime();)", "label": "(3, CUDA, extern __shared__ float shared[];)"}
{"predict": "(36,MPI,*t0 = MPI_Wtime();) (55,MPI,*t1 = MPI_Wtime();)", "label": "(48, MPI, MPI_Startall(2, requests);); (50, MPI, MPI_Waitall(2, requests, statuses);); (57, MPI, t1 = MPI_Wtime();); (12, MPI, MPI_Init(&argc, &argv);); (61, MPI, MPI_Finalize();); (11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (31, MPI, MPI_Barrier(MPI_COMM_WORLD);); (31, MPI, t0 = MPI_Wtime();)"}
{"predict": "(32,OpenMP,#pragma omp parallel for private(seed))", "label": "(12, OpenCL, retVal = clGetDeviceInfo(device, CL_DEVICE_PROFILE, sizeof(profile), profile, NULL);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for)", "label": "(21, CUDA, setCudaAccessPolicyWindow(env, javaObject, nativeObject.accessPolicyWindow);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for reduction(+:totalAmp)),", "label": "(22, CUDA, fptype ret = thrust::norm(totalAmp);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp parallel for)"}
{"predict": "(12,OpenMP,#pragma omp parallel for private(i,disp,true_lb,true_extent)), (21,OpenMP,#pragma omp parallel for private(i,disp,true_lb,true_extent)), (35,OpenMP,#pragma omp parallel for private(i,disp,true_lb,true_extent))", "label": "(38, MPI, MPI_Type_free(&aligned_type);); (36, MPI, MPI_Type_commit(newtype);); (33, MPI, MPI_Type_create_struct(num, blocklens, displs, types, &aligned_type);); (33, MPI, MPI_Type_create_resized(aligned_type, 0, disp, newtype);); (23, MPI, MPI_Type_get_true_extent(oldtypes[i], &true_lb, &true_extent);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for)", "label": "(8, OpenCL, void *mappedPtr = clEnqueueMapImage(cmdQ.get(), image.get(), CL_TRUE, CL_MAP_WRITE, &origin[0], &region[0], nullptr, nullptr, 0, nullptr, nullptr, &retVal);); (14, OpenCL, retVal = clEnqueueUnmapMemObject(cmdQ.get(), image.get(), mappedPtr, 0, nullptr, nullptr);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for)", "label": "(29, OpenMP, #pragma omp target simd defaultmap(tofrom, scalar); (1, OpenMP, #pragma omp target simd defaultmap); (9, OpenMP, #pragma omp target simd defaultmap(tofrom); (20, OpenMP, #pragma omp target simd defaultmap(tofrom,); (10, OpenMP, #pragma omp target simd defaultmap(tofrom:); (15, OpenMP, #pragma omp target simd defaultmap(tofrom scalar)); (20, OpenMP, #pragma omp target simd defaultmap(scalar:); (-2, OpenMP, #pragma omp target simd defaultmap(); (9, OpenMP, #pragma omp target simd defaultmap(tofrom)); (-1, OpenMP, #pragma omp target simd defaultmap())"}
{"predict": "(14,MPI, MPI_Init(&argc, &argv);)\n(16,MPI, MPI_Comm_rank(comm, &par_rank);)\n(17,MPI, MPI_Comm_size(comm, &par_size);)\n(29,OpenMP, #pragma omp parallel for)\n(42,OpenMP, #pragma omp parallel for)\n(52,OpenMP, #pragma omp parallel for)\n(78,OpenMP, #pragma omp parallel for)", "label": "(85, MPI, ascent_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (91, MPI, MPI_Barrier(comm);); (16, MPI, MPI_Comm_rank(comm, &par_rank);); (16, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(5,MPI,*w = MPI_Wtime(););", "label": "(2, OpenMP, #pragma omp target map(alloc:colstat[0:NZ])); (5, OpenMP, #pragma omp target map(alloc:colstat[:0])); (9, OpenMP, #pragma omp target update from(colstat))"}
{"predict": "(10,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for)", "label": "(41, TBB, ASSERT( (int)tbb::tbb_thread::hardware_concurrency() == availableProcs, NULL );); (39, TBB, ASSERT( tbb::task_scheduler_init::default_num_threads() == availableProcs, NULL );)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(7, CUDA, auto err = thrust::mismatch(thrust::device, d_reference_begin, d_reference_end, d_data);); (2, CUDA, CubDebugExit(cudaMemcpy(d_tmp_buffer, host_reference, sizeof(T) * num_items, cudaMemcpyHostToDevice));); (9, CUDA, const auto index = thrust::distance(d_reference_begin, err.first);)"}
{"predict": "(14,CUDA,#cudaMemcpyAsync(device_buffer, host_buffer.get(), buffer_size * sizeof(value_type), cudaMemcpyHostToDevice, 0));", "label": "(12, CUDA, CUDA_SAFE_CALL(cudaMalloc(&device_buffer, buffer_size * sizeof(value_type)));)"}
{"predict": "(23,OpenMP,#pragma omp parallel for)", "label": "(8, CUDA, cudaSetDevice(devs[0]);); (5, CUDA, CUCH(cudaMallocManaged(&ptr, size), kmcudaMemoryAllocationFailure);); (7, CUDA, CUCH(cudaMemcpy(ptr, (*samples)[0].get(), size, cudaMemcpyDefault), kmcudaMemoryCopyError);)"}
{"predict": "(28,MPI,CUBLAS_CHECK(cublasCreate(&cublasH));), (30,MPI,CUBLAS_CHECK(cublasSetStream(cublasH, stream));), (34,MPI,CUDA_CHECK(cudaMemcpyAsync(d_A, A.data(), sizeof(data_type) * A.size(), cudaMemcpyHostToDevice, stream));), (36,MPI,CUDA_CHECK(cudaMemcpyAsync(d_B, B.data(), sizeof(data_type) * B.size(), cudaMemcpyHostToDevice, stream));), (40,MPI,CUBLAS_CHECK(cublasAxpyEx(cublasH, A.size(), &alpha, traits<data_type>::cuda_data_type, d_A, traits<data_type>::cuda_data_type, incx, d_B, traits<data_type>::cuda_data_type, incy, traits<data_type>::cuda_data_type));), (46,MPI,CUDA_CHECK(cudaMemcpyAsync(B.data(), d_B, sizeof(data_type) * B.size(), cudaMemcpyDeviceToHost, stream));)", "label": "(51, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (33, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_B), sizeof(data_type) * B.size()));); (61, CUDA, CUDA_CHECK(cudaFree(d_B));); (66, CUDA, CUDA_CHECK(cudaDeviceReset());); (63, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (28, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (56, CUDA, CUDA_CHECK(cudaFree(d_A));); (22, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(12,OpenMP,#pragma omp parallel for reduction(+:local[id]))", "label": "(3, CUDA, __shared__ float local[threads];)"}
{"predict": "(20,OpenMP,#pragma omp parallel for private(color, pixels, depths))", "label": "(47, OpenMP, EXPECT_TRUE(check_test_image(output_file, t_apcomp_baseline_dir()));); (9, MPI, MPI_Comm_rank(comm, &par_rank);); (10, MPI, apcomp::mpi_comm(MPI_Comm_c2f(comm));); (8, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(30,MPI,MPI_Win_allocate_shared(0, size, MPI_INFO_NULL, shm_comm, &my_base, &shm_win);)\n(32,MPI,MPI_Win_fence(0, shm_win);)\n(34,MPI,MPI_Win_fence(0, shm_win);)\n(37,MPI,MPI_Win_fence(0, shm_win);)\n(41,MPI,MPI_Win_fence(0, shm_win);)\n(44,MPI,MPI_Win_fence(0, shm_win);)\n(47,MPI,MPI_Win_fence(0, shm_win);)\n(50,MPI,MPI_Win_fence(0, shm_win);)\n(53,MPI,MPI_Win_fence(0, shm_win);)\n(56,MPI,MPI_Win_fence(0, shm_win);)\n(59,MPI,MPI_Win_fence(0, shm_win);)\n(62,MPI,MPI_Win_free(&shm_win);)", "label": "(28, MPI, MPI_Comm_rank(shm_comm, &shm_rank);); (15, MPI, MPI_Win_shared_query(shm_win, MPI_PROC_NULL, &query_size, &query_disp_unit, &query_base);); (13, MPI, MPI_Win_allocate_shared(size, sizeof(int), MPI_INFO_NULL, MPI_COMM_SELF, &my_base, &shm_win);); (23, MPI, MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, rank, MPI_INFO_NULL, &shm_comm);); (34, MPI, MPI_Win_allocate_shared(0, sizeof(int), MPI_INFO_NULL, shm_comm, &my_base, &shm_win);); (20, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (67, MPI, MPI_Comm_free(&shm_comm);); (15, MPI, MPI_Win_free(&shm_win);); (21, MPI, MPI_Comm_size(shm_comm, &shm_nproc);); (31, MPI, MPI_Win_allocate_shared(size, sizeof(int), MPI_INFO_NULL, shm_comm, &my_base, &shm_win);)"}
{"predict": "(28,OpenMP,#pragma omp parallel for reduction(+:sum))", "label": "(13, CUDA, cudaMalloc((void **)&d_arr, sizeof(int) * len);); (16, CUDA, cudaMemcpy(d_arr, arr, sizeof(int) * len, cudaMemcpyHostToDevice);); (53, CUDA, cudaFree(d_out);); (11, CUDA, cudaMalloc((void **)&d_out, sizeof(int) * len);); (23, CUDA, cudaMemcpy(out, d_out, sizeof(int) * len, cudaMemcpyDeviceToHost);); (49, CUDA, cudaFree(d_arr);)"}
{"predict": "(6,OpenMP,#pragma omp parallel for)", "label": "(4, OpenMP, int *shared_ptr = llvm_omp_target_alloc_shared(N * sizeof(int), device);); (5, OpenMP, #pragma omp target teams distribute parallel for device(device)); (0, OpenMP, const int device = omp_get_default_device();); (13, OpenMP, omp_target_free(shared_ptr, device);)"}
{"predict": "(6,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for), (41,OpenMP,#pragma omp parallel for), (45,OpenMP,#pragma omp parallel for), (50,OpenMP,#pragma omp parallel for), (52,OpenMP,#pragma omp parallel for), (70,OpenMP,#pragma omp parallel for), (80,OpenMP,#pragma omp parallel for)", "label": "(52, TBB, tbb::flow::make_edge( b2, b3 );); (50, TBB, tbb::flow::make_edge( b, b2 );); (73, TBB, tbb::flow::buffer_node<T> b_copy(b);); (2, TBB, tbb::flow::buffer_node<T> b3(g);); (0, TBB, tbb::flow::buffer_node<T> b2(g);); (-3, TBB, tbb::flow::graph g;); (-3, TBB, tbb::flow::buffer_node<T> b(g);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for)", "label": "(22, OpenCL, clReleaseEvent(outEvent1);)"}
{"predict": "(23,OpenMP,#pragma omp parallel for), (41,OpenMP,#pragma omp parallel for)", "label": "(59, CUDA, cudaFree( deviceClockValues );); (60, CUDA, cudaFreeHost( hostOut );); (61, CUDA, cudaEventDestroy( stop );); (59, CUDA, cudaEventDestroy( start );); (56, CUDA, cudaFree( deviceThreadIDs );)"}
{"predict": "(37,MPI,MPI_Init(NULL, NULL);), (60,MPI,MPI_Reduce_scatter_block((char*)c_info->s_buffer + i % ITERATIONS->s_cache_iter * ITERATIONS->s_offs, (char*)c_info->r_buffer + i % ITERATIONS->r_cache_iter * ITERATIONS->r_offs, s_num, c_info->red_data_type, c_info->op_type, c_info->communicator));", "label": "(63, MPI, t2 = MPI_Wtime();); (58, MPI, t1 = MPI_Wtime();)"}
{"predict": "KstarM, KstarW, 1, M_13));\n66:     LSKRS.push_back(new Lineshapes::RBW(\"rho(770)\", RhoMass, RhoWidth, 1, M_13));\n67:     LSKRS.push_back(new Lineshapes::RBW(\"K*(892)bar\", KstarM, KstarW, 1, M_24));\n68:     LSKRS.push_back(new Lineshapes::RBW(\"rho(770)\", RhoMass, RhoWidth, 1, M_12));\n69:     LSKRS.push_back(new Lineshapes::RBW(\"K*(892)bar\", KstarM, KstarW, 1, M_34));\n70: \n71:     std::vector<Lineshape *> LSKRP;\n72:     LSKRP.push_back(new Lineshapes::RBW(\"rho(770)\", RhoMass, RhoWidth, 1, M_12));\n73:     LSKRP.push_back(new Lineshapes::RBW(\"K*(892)bar\", KstarM, KstarW, 1, M_34));\n74:     LSKRP.push_back(new Lineshapes::RBW(\"rho(770)\", RhoMass, RhoWidth, 1, M_24));\n75:     LSKRP.push_back(new Lineshapes::RBW(\"K*(892)bar\", KstarM, KstarW, 1, M_13));\n76:     LSKRP.push_back(new Lineshapes::RBW(\"rho(770)\", RhoMass, RhoWidth, 1, M_13));\n77:     LSKRP.push_back(new Lineshapes::RBW(\"K*(892)bar\", KstarM, KstarW, 1, M_24));\n78:     LSKRP.push_back(new Lineshapes::RBW(\"rho(770)\", RhoMass, RhoWidth, 1, M_12));\n79:     LSKRP.push_back(new Lineshapes::RBW(\"K*(892)bar\", KstarM, KstarW, 1, M_", "label": "(184, CUDA, long accepted  = thrust::count_if(flags.begin(), flags.end(), thrust::identity<bool>());)"}
{"predict": "H5S_SELECT_SET, start, stride, count, block);\n137:     VRFY((ret >= 0), \"H5Sselect_hyperslab succeeded\");\n138: \n139:     /* write data to dataset1 collectively */\n140:     ret = H5Dwrite(dataset1, H5T_NATIVE_INT, mem_dataspace, file_dataspace, H5P_DEFAULT, data_array1);\n141:     VRFY((ret >= 0), \"H5Dwrite succeeded\");\n142: \n143:     /* release resource */\n144:     H5Sclose(file_dataspace);\n145:     H5Sclose(mem_dataspace);\n146: \n147:     /* -------------------------\n148:      * Test writing to dataset2\n149:      * -------------------------*/\n150:     /* set up dimensions of the slab this process accesses */\n151:     slab_set(mpi_rank, mpi_size, start, count, stride, block, BYROW);\n152: \n153:     /* put some trivial data in the data_array */\n154:     dataset_fill(start, block, data_array1);\n155:     MESG(\"data_array initialized\");\n156:     if(VERBOSE_MED) {\n157: \tMESG(\"data_array created\");\n158: \tdataset_print(start, block, data_array1);\n159:     }\n160: \n161:     /* create a memory dataspace independently */\n162:     mem_dataspace = H5Screate_simple (RANK, block, NULL);\n163:     VRFY((mem_dataspace >= 0), \"\");\n164: \n165:     /* Extend its current dim sizes before writing */\n166:     dims[0] = dim0;\n167:     dims[1] = dim1;\n168:     ret = H5Dset_extent(dataset2, dims);\n169:     VRFY((ret >= 0), \"H5Dset_extent succeeded\");\n170: \n171:     /* create a file dataspace independently */\n172:     file_dataspace = H5Dget_space (dataset2);\n173:     VRFY((file_dataspace", "label": "(34, MPI, MPI_Comm_size(MPI_COMM_WORLD,&mpi_size);); (34, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&mpi_rank);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for private(tid, degree_idx, out_prime_idx, accum, obase_value, obase_ratio, scale, scale_shoup, out1, out2, input1, input2))", "label": "(6, CUDA, extern __shared__ uint64_t s_QHatModp[];)"}
{"predict": "MPI_Info_free(&info_in);\n109:     MTest_Finalize();\n110:     return 0;\n111: }", "label": "(81, MPI, MPI_Win_create(data, sizeof(twoint_t) * ARRAY_LEN, 1, info_in, MPI_COMM_WORLD, &win);); (81, MPI, MPI_Info_free(&info_in);); (134, MPI, MPI_Accumulate(mine_plus, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_MAXLOC, win);); (151, MPI, MPI_Accumulate(mine_plus, ARRAY_LEN, MPI_2INT, 0, 0, ARRAY_LEN, MPI_2INT, MPI_MAXLOC, win);); (34, MPI, MPI_Win_fence(0, win);); (56, MPI, MPI_Accumulate(mine, ARRAY_LEN, MPI_2INT, 0, 0, ARRAY_LEN, MPI_2INT, MPI_REPLACE, win);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &me);); (63, MPI, MPI_Win_free(&win);); (70, MPI, MPI_Info_create(&info_in);); (70, MPI, MPI_Info_set(info_in, \"accumulate_ordering\", \"none\");); (32, MPI, MPI_Accumulate(mine, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_REPLACE, win);); (181, MPI, MPI_Accumulate(mine, ARRAY_LEN, MPI_2INT, 0, 0, ARRAY_LEN, MPI_2INT, MPI_MINLOC, win);); (31, MPI, MPI_Accumulate(mine_plus, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_REPLACE, win);); (48, MPI, MPI_Accumulate(mine_plus, ARRAY_LEN, MPI_2INT, 0, 0, ARRAY_LEN, MPI_2INT, MPI_REPLACE, win);); (160, MPI, MPI_Accumulate(mine, 1, MPI_2INT, 0, 0, 1, MPI_2INT, MPI_MINLOC, win);); (1, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (20, MPI, MPI_Win_create(data, sizeof(twoint_t) * ARRAY_LEN, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &win);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (14,MPI,*w = MPI_Wtime();), (47,OpenMP,#pragma omp parallel for)", "label": "(56, CUDA, cudaEventRecord(stop);); (22, CUDA, CUDA_CHECK(cudaDeviceSetLimit(cudaLimitStackSize, 2 * 1024));); (55, CUDA, cudaEventSynchronize(stop);); (58, CUDA, cudaEventDestroy(start);); (5, CUDA, CUDA_CHECK(cudaDeviceReset());); (21, CUDA, CUDA_CHECK(cudaDeviceSynchronize());); (46, CUDA, cudaEventRecord(start);); (55, CUDA, cudaEventDestroy(stop);); (57, CUDA, CUDA_CHECK(cudaGetLastError());); (8, CUDA, cudaDeviceGetLimit(&size_value, cudaLimitStackSize);); (12, CUDA, CUDA_CHECK(cudaDeviceSetLimit(cudaLimitMallocHeapSize, heap_size));); (40, CUDA, cudaEventCreate(&stop);); (46, CUDA, cudaEventElapsedTime(&milliseconds, start, stop);); (37, CUDA, cudaEventCreate(&start);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for reduction(+:s)), (18,OpenMP,#pragma omp parallel for reduction(+:nErr)), (25,OpenMP,#pragma omp parallel for reduction(+:nErr)), (32,OpenMP,#pragma omp parallel for reduction(+:nErr))", "label": "(19, OpenMP, #pragma omp parallel for reduction(+:s)); (10, OpenMP, #pragma omp parallel for)"}
{"predict": "(8,OpenMP,#pragma omp parallel for reduction(+:output[MMS_SUM]))", "label": "(10, MPI, MPI_Allreduce(input, output, 1, dtcmp_type_3uint64t, dtcmp_reduceop_mms_3uint64t, comm);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (26,MPI_Ireduce,MPI_Ireduce(array1, result, array_size, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD, &request)); (33,MPI_Wait,MPI_Wait(&request, &status)); (39,MPI_Reduce,MPI_Reduce(array1, result, array_size, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD));", "label": "(19, MPI, MPI_Bcast(&array_size, 1, MPI_INT, 0, MPI_COMM_WORLD);); (26, MPI, MPI_Wait(&request, &status);); (41, MPI, end_time = MPI_Wtime();); (66, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (2, MPI, MPI_Init(NULL, NULL);); (2, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (25, MPI, start_time = MPI_Wtime();); (12, MPI, MPI_Bcast(&nr_iters, 1, MPI_INT, 0, MPI_COMM_WORLD);); (6, MPI, MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (13,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for)", "label": "(2, OpenMP, omp_autotune(self, OMP_SCALE);)"}
{"predict": "p3 );\n137:     }\n138: fn_cleanup3_1:;\n139:     if (p2 != NULL) {\n140:         MPIU_Free( p2[0] );  /* Free space allocated to args */\n141:         MPIU_Free( p2 );       /* Free space allocated to arg array */\n142:     }\n143: fn_cleanup2_2:;\n144:     if (ptmp2 != NULL) {\n145:         MPIU_Free( ptmp2 );  /* Free space allocated to args */\n146:     }\n147: fn_cleanup2_1:;\n148:     return;\n149: }", "label": "(127, MPI, *ierr = MPI_Comm_spawn_multiple( *v1, p2, p3, v4, v5, *v6, (MPI_Comm)(*v7), (MPI_Comm *)(v8), (int *)v9 );)"}
{"predict": "(16,OpenMP,#pragma omp parallel for collapse(3)), (22,OpenMP,#pragma omp parallel for collapse(3)), (28,OpenMP,#pragma omp parallel for collapse(3)), (34,OpenMP,#pragma omp parallel for collapse(3)), (40,OpenMP,#pragma omp parallel for collapse(3)), (46,OpenMP,#pragma omp parallel for collapse(3)), (52,OpenMP,#pragma omp parallel for collapse(3)), (58,OpenMP,#pragma omp parallel for collapse(3)), (64,OpenMP,#pragma omp parallel for collapse(3)), (70,OpenMP,#pragma omp parallel for collapse(3)), (76,OpenMP,#pragma omp parallel for collapse(3)), (82,OpenMP,#pragma omp parallel for collapse(3)), (88,OpenMP,#pragma omp parallel for collapse(3)), (94,OpenMP,#pragma omp parallel for collapse(3)), (100,OpenMP,#pragma omp parallel for collapse(3))", "label": "(10, OpenMP, #pragma omp parallel); (14, OpenMP, #pragma omp for); (95, OpenMP, #pragma omp critical)"}
{"predict": "(6,MPI,MPI_Comm_size(MPI_COMM_WORLD, &size);) (8,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);) (17,OpenMP,#pragma omp parallel if(size > 1)) (18,OpenMP,a = 42;) (20,OpenMP,MPI_Start(&requests[1]);) (21,OpenMP,MPI_Start(&requests[0]);)", "label": "(22, MPI, MPI_Start(&requests[0]);); (17, MPI, MPI_Barrier_init(MPI_COMM_WORLD, MPI_INFO_NULL, &requests[1]);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (14, MPI, MPI_Bcast_init(&a, 1, MPI_INT, 0, MPI_COMM_WORLD, MPI_INFO_NULL, &requests[0]);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (18, MPI, MPI_Start(&requests[1]);); (23, MPI, MPI_Waitall(2, requests, MPI_STATUSES_IGNORE);); (5, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (27, MPI, MPI_Request_free(&requests[0]);); (27, MPI, MPI_Request_free(&requests[1]);)"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for)", "label": "(11, OpenMP, #pragma omp target teams distribute parallel for map(tofrom:sum1) reduction(+:sum1) collapse(2))"}
{"predict": "(17,OpenMP,#pragma omp parallel for)\n(21,OpenMP,#pragma omp parallel for)\n(25,OpenMP,#pragma omp parallel for)\n(26,OpenMP,#pragma omp parallel for)", "label": "(16, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (22, TBB, serial_tiled_time = (tbb::tick_count::now() - t0).seconds();); (27, TBB, tbb_time = (tbb::tick_count::now() - t0).seconds();); (15, TBB, serial_time = (tbb::tick_count::now() - t0).seconds();)"}
{"predict": "(30,OpenMP,#pragma omp parallel for)", "label": "(50, CUDA, cudaGetDeviceProperties(&prop, 0);); (125, CUDA, cudaFree(h_aCached);); (29, CUDA, cudaMallocHost((void**)&h_aCached, bytes);); (119, CUDA, cudaFreeHost(h_aPinned);); (119, CUDA, cudaFreeHost(h_bPinned);); (29, CUDA, cudaHostAlloc((void**)&h_bPinned, bytes, cudaHostAllocWriteCombined);); (24, CUDA, cudaMalloc((void**)&d_b, bytes);); (26, CUDA, cudaHostAlloc((void**)&h_aPinned, bytes, cudaHostAllocWriteCombined);); (24, CUDA, cudaMallocHost((void**)&h_bCached, bytes);); (116, CUDA, cudaFree(d_b);); (-1, CUDA, cudaSetDevice(0);); (113, CUDA, cudaFree(d_a);); (115, CUDA, cudaFree(h_bCached);); (-10, CUDA, cudaGetDeviceCount(&count);); (-2, CUDA, cudaGetDevice(&deviceID);); (14, CUDA, cudaMalloc((void**)&d_a, bytes);)"}
{"predict": "(26,OpenMP,#pragma omp parallel for)", "label": "(30, OpenCL, err = clEnqueueWriteBuffer( queue, streams[0], true, 0, sizeof(cl_int)*num_elements, (void *)int_input_ptr, 0, NULL, NULL );); (39, OpenCL, err = clEnqueueCopyBuffer( queue, streams[0], streams[1], 0, 0, sizeof(cl_int)*num_elements, 0, NULL, &copyEvent );); (32, OpenCL, clReleaseMemObject( streams[1] );); (107, OpenCL, err = clEnqueueReadBuffer( queue, streams[1], true, 0, sizeof(cl_int)*num_elements, (void *)int_output_ptr, 0, NULL, NULL );); (48, OpenCL, err = clWaitForEvents( 1, &copyEvent );); (28, OpenCL, clReleaseMemObject( streams[0] );); (49, OpenCL, clReleaseEvent(copyEvent);)"}
{"predict": "(7,MPI,MPI_Init(&argc, &argv); MPI_Comm_rank(MPI_COMM_WORLD, &myid); MPI_Comm_size(MPI_COMM_WORLD, &numprocs);); (13,MPI,MPI_Send(buffer, 10, MPI_INT, right, 0, MPI_COMM_WORLD); MPI_Recv(buffer, 10, MPI_INT, left, 0, MPI_COMM_WORLD, &status);)", "label": "(9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myid);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &numprocs);); (16, MPI, MPI_Finalize();); (4, MPI, MPI_Init(&argc,&argv);); (12, MPI, MPI_Sendrecv_replace(buffer, 10, MPI_INT, left, 123, right, 123, MPI_COMM_WORLD, &status);)"}
{"predict": "(6,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for)", "label": "(3, TBB, REMARK(\"is_large_object<int>::value=%d\\n\", tbb::interface6::internal::is_large_object<int>::value);); (4, TBB, REMARK(\"is_large_object<int *>::value=%d\\n\", tbb::interface6::internal::is_large_object<int *>::value);); (2, TBB, REMARK(\"is_large_object<double>::value=%d\\n\", tbb::interface6::internal::is_large_object<double>::value);); (4, TBB, REMARK(\"is_large_object<check_type<int>* >::value=%d\\n\", tbb::interface6::internal::is_large_object<check_type<int>* >::value);); (2, TBB, REMARK(\"is_large_object<check_type<int> >::value=%d\\n\", tbb::interface6::internal::is_large_object<check_type<int> >::value);); (3, TBB, REMARK(\"is_large_object<check_type<short> >::value=%d\\n\\n\", tbb::interface6::internal::is_large_object<check_type<short> >::value);); (8, TBB, tbb::task_scheduler_init init(nthread);)"}
{"predict": "(10,MPI,*w = MPI_Wtime(););", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(24,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for)", "label": "(47, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_ARE_FATAL);); (21, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (21, MPI, MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);); (18, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (38, MPI, MPI_Error_string(ierr, str, &slen);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (106,OpenMP,#pragma omp end parallel for)", "label": "(53, CUDA, extern __shared__ float s_float[];)"}
{"predict": "(36,OpenMP,#pragma omp barrier)", "label": "(26, OpenMP, #pragma omp barrier); (19, OpenMP, common::nvtx::range omp_scope(\"interruptible::thread-%d\", i);); (13, OpenMP, omp_set_dynamic(0);); (14, OpenMP, #pragma omp parallel reduction(+ : n_finished) reduction(+ : n_cancelled) num_threads(n_threads)); (12, OpenMP, omp_set_num_threads(n_threads);); (14, OpenMP, auto i = omp_get_thread_num();)"}
{"predict": "(11,OpenMP,#pragma omp parallel for reduction(+:SCT->commVolRed)), (27,OpenMP,#pragma omp atomic)", "label": "(25, MPI, MPI_Send(lnzval, len2, MPI_DOUBLE, receiver, k, grid3d->zscp.comm);)"}
{"predict": "(77,OpenMP,#pragma omp parallel for reduction(+:val_st_off[0],val_st_off[1],val_st_off[2]))), (103,OpenMP,#pragma omp parallel for reduction(+:val_st_on[0],val_st_on[1],val_st_on[2])))", "label": "(47, OpenMP, lis2mdl_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for private(i))", "label": "(2, OpenMP, #pragma omp for schedule(nonmonotonic: dynamic))"}
{"predict": "(27,MPI,*val = 0;); (29,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for)", "label": "(21, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(18,OpenMP,#pragma omp parallel for)", "label": "(35, CUDA, cudaFree(d_ret);); (7, CUDA, cudaMallocHost(&h_stride, STRIDE_MEM_SIZE);); (14, CUDA, cudaMalloc(&d_ret, sizeof(uint32_t));); (31, CUDA, cudaFree(d_stride);); (32, CUDA, cudaFree(d_clk);); (32, CUDA, cudaFreeHost(h_stride);); (25, CUDA, cudaMemcpy(h_clk, d_clk, 32 * sizeof(uint32_t), cudaMemcpyDeviceToHost);); (10, CUDA, cudaMemcpy(d_stride, h_stride, STRIDE_MEM_SIZE, cudaMemcpyHostToDevice);); (7, CUDA, cudaMalloc(&d_stride, STRIDE_MEM_SIZE);); (11, CUDA, cudaMalloc(&d_clk, 32 * sizeof(uint32_t));)"}
{"predict": "(18,MPI,flags.Set(CONTACT, true);); (20,MPI,Flags output = mpi_world_communicator.OrReduceAll(flags, ACTIVE | RIGID | STRUCTURE | MPI_BOUNDARY | PERIODIC | INLET | OUTLET | ISOLATED);); (50,MPI,KRATOS_EXPECT_EQ(output.Is(CONTACT), world_rank == root););", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(18,OpenMP,#pragma omp parallel for)", "label": "(5, OpenMP, #pragma omp target enter data map(to:a[:N]))"}
{"predict": "(4,OpenMP,#pragma omp parallel for), (7,OpenMP,#pragma omp parallel for), (10,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for)", "label": "(15, TBB, test_basic<MyCheckedStateSet>(\"concurrent unordered set (checked element state)\", tbb::internal::true_type());)"}
{"predict": "(14,MPI,*w = MPI_Wtime(););\n(16,OpenMP,#pragma omp parallel for reduction(max:finalValue,finalIndex)), (48,OpenMP,#pragma omp atomic), (50,OpenMP,#pragma omp atomic), (52,OpenMP,#pragma omp atomic)", "label": "(17, CUDA, extern __shared__ _VOLATILE_ float sMem[];)"}
{"predict": "(14,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for)", "label": "(9, CUDA, factory = *internal::MakeCUDAExecutorImplementation();)"}
{"predict": "(27,MPI, MPI_Init(&argc, &argv);) (34,MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);) (35,MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);) (56,MPI, length = total_length/Num_procs;) (80,MPI, #include <mpi.h>) (81,MPI, MPI_Finalize();) (111,MPI, MPI_Bcast(&scalar, 1, MPI_DOUBLE, root, MPI_COMM_WORLD);) (112,MPI, MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);) (113,MPI, MPI_Bcast(&offset, 1, MPI_LONG, root, MPI_COMM_WORLD);) (115,MPI, MPI_Barrier(MPI_COMM_WORLD);) (121,MPI, MPI_Reduce(&local_nstream_time, &nstream_time, 1, MPI_DOUBLE, MPI_MAX, root, MPI_COMM_WORLD);)", "label": "(103, OpenMP, printf(\"Number of threads    = %d\\n\", omp_get_max_threads());); (25, MPI, MPI_Comm_size(MPI_COMM_WORLD,&Num_procs);); (107, OpenMP, #pragma omp parallel for simd); (147, MPI, MPI_Finalize();); (76, MPI, MPI_Bcast(&length,        1, MPI_LONG, root, MPI_COMM_WORLD);); (22, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&my_ID);); (77, MPI, MPI_Bcast(&nthread_input, 1, MPI_INT,  root, MPI_COMM_WORLD);); (78, OpenMP, omp_set_num_threads(nthread_input);); (17, MPI, MPI_Init(&argc,&argv);); (73, MPI, MPI_Bcast(&iterations,    1, MPI_INT,  root, MPI_COMM_WORLD);); (114, MPI, MPI_Barrier(MPI_COMM_WORLD);); (70, MPI, MPI_Bcast(&offset,        1, MPI_LONG, root, MPI_COMM_WORLD);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for), (41,OpenMP,#pragma omp parallel for), (53,OpenMP,#pragma omp parallel for)", "label": "(6, CUDA, CHECK(cudaMemcpyFromSymbol(&block_size, BLOCK_DIM, sizeof(real)));); (34, CUDA, CHECK(cudaEventCreate(&stop));); (43, CUDA, CHECK(cudaEventRecord(stop));); (78, CUDA, CHECK(cudaFree(d_y));); (33, CUDA, cudaEventQuery(start);); (41, CUDA, CHECK(cudaEventSynchronize(stop));); (41, CUDA, CHECK(cudaEventElapsedTime(&curr_time, start, stop));); (34, CUDA, CHECK(cudaMemcpy(h_y, d_y, size, cudaMemcpyDefault));); (28, CUDA, CHECK(cudaEventRecord(start));); (20, CUDA, CHECK(cudaMalloc(&d_x, M));); (70, CUDA, CHECK(cudaFree(d_x));); (20, CUDA, CHECK(cudaMemcpy(d_x, h_x, M, cudaMemcpyDefault));); (30, CUDA, CHECK(cudaGetLastError());); (21, CUDA, CHECK(cudaEventCreate(&start));); (16, CUDA, CHECK(cudaMalloc(&d_y, size));)"}
{"predict": "(30,MPI,*w = MPI_Wtime();); (43,OpenMP,#pragma omp parallel for), (...)", "label": "(28, MPI, MPI_Type_get_extent(fd->filetype, &lb, &filetype_extent);); (28, MPI, MPI_Type_size_x(fd->filetype, &filetype_size);)"}
{"predict": "(9,MPI,*w = MPI_Wtime();); (13,OpenMP,#pragma omp parallel for), (44,OpenMP,#pragma omp parallel for), (61,OpenMP,#pragma omp parallel for)", "label": "(60, CUDA, CHECK_ERROR(cudaFree(device_params));); (12, CUDA, CHECK_ERROR(cudaMalloc((void **)&device_signals, complex_size_bytes));); (50, CUDA, CHECK_ERROR(cudaMemcpy(output_signals, device_signals, complex_size_bytes, cudaMemcpyDeviceToHost));); (56, CUDA, CHECK_ERROR(cudaFree(device_signals));); (34, CUDA, CHECK_ERROR(cudaMemcpyFromSymbol(&host_callback_ptr, device_callback_ptr, sizeof(host_callback_ptr)));); (9, CUDA, CHECK_ERROR(cudaMemcpy(device_signals, input_signals, complex_size_bytes, cudaMemcpyHostToDevice));); (17, CUDA, CHECK_ERROR(cudaMalloc((void **)&device_params, sizeof(cb_params)));); (17, CUDA, CHECK_ERROR(cudaMemcpy(device_params, &host_params, sizeof(cb_params), cudaMemcpyHostToDevice));)"}
{"predict": "(20,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for)", "label": "(25, CUDA, cudaDeviceSynchronize();)"}
{"predict": "(16,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for)", "label": "(15, CUDA, checkCudaRuntime(cudaMemcpyAsync(image_device, image_host, size_image, cudaMemcpyHostToDevice, stream));)"}
{"predict": "(10,OpenMP,#pragma omp parallel for collapse(2)), (22,OpenMP,#pragma omp for), (35,OpenMP,#pragma omp for collapse(2)), (48,OpenMP,#pragma omp for collapse(2)), (60,OpenMP,#pragma omp atomic)", "label": "(35, CUDA, __shared__ float sub_results[x128::size][32][bdy];)"}
{"predict": "ILON_ERROR);\n124:     if (bResult) {\n125:       printf(\"Test passed\\n\");\n126:     } else {\n127:       printf(\"Test failed\\n\");\n128:     }\n129:   }\n130: \n131:   // free device memory\n132:   cudaFree(d_data);\n133:   // free host memory\n134:   free(h_data);\n135:   free(h_data_ref);\n136:   free(h_odata);\n137:   return 0;\n138: }", "label": "(59, CUDA, myparms.dstPos = make_cudaPos(0, 0, 0);); (9, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProps, devID));); (56, CUDA, myparms.srcPos = make_cudaPos(0, 0, 0);); (119, CUDA, checkCudaErrors(cudaMemcpy(h_odata, d_data, size, cudaMemcpyDeviceToHost));); (94, CUDA, getLastCudaError(\"warmup Kernel execution failed\");); (60, CUDA, checkCudaErrors(cudaMemcpy3D(&myparms));); (57, CUDA, myparms.extent = make_cudaExtent(width, width, num_faces);); (137, CUDA, checkCudaErrors(cudaFreeArray(cu_3darray));); (76, CUDA, checkCudaErrors(cudaCreateTextureObject(&tex, &texRes, &texDescr, NULL));); (91, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (36, CUDA, checkCudaErrors(cudaMalloc((void **)&d_data, size));); (99, CUDA, getLastCudaError(\"Kernel execution failed\");); (131, CUDA, checkCudaErrors(cudaFree(d_data));); (-10, CUDA, int devID = findCudaDevice(argc, (const char **)argv);); (128, CUDA, checkCudaErrors(cudaDestroyTextureObject(tex));)"}
{"predict": "(37,MPI, MPI_Init(NULL, NULL);)\n(73,MPI, MPI_Init(NULL, NULL);)\n(104,MPI, MPI_Init(NULL, NULL);)\n(111,MPI, MPI_Init(NULL, NULL);)\n(114,MPI, MPI_Init(NULL, NULL);)\n(116,MPI, MPI_Finalize();)\n(73,MPI, MPI_Finalize();)\n(104,MPI, MPI_Finalize();)\n(111,MPI, MPI_Finalize();)\n(114,MPI, MPI_Finalize();)\n(37,OpenMP, #pragma omp parallel for)\n(73,OpenMP, #pragma omp parallel for)\n(104,OpenMP, #pragma omp parallel for)\n(111,OpenMP, #pragma omp parallel for)\n(114,OpenMP, #pragma omp parallel for)", "label": "(80, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (80, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)"}
{"predict": "(9,MPI,start = MPI_Wtime();) (50,MPI,end = MPI_Wtime();) (36,OpenMP,#pragma omp parallel for) (44,OpenMP,#pragma omp parallel for)", "label": "(48, MPI, MPI_Waitall(WINDOW, req, status);); (23, MPI, start = MPI_Wtime();); (50, MPI, end = MPI_Wtime();)"}
{"predict": "(5,MPI,*w = MPI_Wtime(););", "label": "(8, SYCL, return hipsycl::libkernel::sscp::wg_broadcast(sender, x, shrd_x);)"}
{"predict": "// inner loop: for (i = IV; i < UB; i += ch) {\n181: \n182: \n183:       // check exit condition\n184: \n185:       // check that PrevLB and PrevUB are passed to the 'for'\n186:       // check that distlb and distub are properly passed to fork_call\n187: \n188:       // increment by stride (distInc - 'parallel for' executes the whole chunk) and latch\n189: \n190:       // implementation of 'parallel for'\n191: \n192:       // check that distlb and distub are properly passed to fork_call\n193: \n194:       // implementation of 'parallel for'\n195: \n196:       // implementation of 'parallel for'\n197: \n198:       // implementation of 'parallel for'\n199:       [&]() {\n200: \ta[i] = b[i] + c[i];\n201:       }();\n202:     }\n203: \n204:     // schedule: dynamic\n205:     #pragma omp target\n206:     #pragma omp teams\n207: \n208:     for (int i = 0; i < n; ++i) {\n209:       a[i] = b[i] + c[i];\n210: \n211:       // skip rest of implementation of 'distribute' as it is tested above for default dist_schedule case\n212: \n213:       // 'parallel for' implementation using outer and inner loops and PrevEUB\n214: \n215:       // initialize lb and ub to PrevLB and PrevUB\n216: \n217:       // check PrevEUB (using PrevUB instead of NumIt as upper bound)\n218: \n219:       // initialize omp.iv (IV = LB)\n220: \n221:       // outer loop: while (IV < UB) {\n222: \n223: \n224: \n225:       // inner loop: for (i = IV; i < UB; i += ch) {\n226: \n2", "label": "(169, OpenMP, #pragma omp distribute parallel for schedule(static, ch)); (201, OpenMP, #pragma omp distribute parallel for schedule(dynamic)); (17, OpenMP, #pragma omp distribute parallel for); (59, OpenMP, #pragma omp distribute parallel for dist_schedule(static)); (227, OpenMP, #pragma omp distribute parallel for schedule(dynamic, ch)); (132, OpenMP, #pragma omp distribute parallel for schedule(static)); (10, OpenMP, #pragma omp target); (97, OpenMP, #pragma omp distribute parallel for dist_schedule(static, ch)); (9, OpenMP, #pragma omp teams)"}
{"predict": "138: #endif\n139: \n140:   // Copy the output to the host output buffer\n141:   checkCudaErrors(cudaMemcpy(output, bufferOut + padding, volumeSize * sizeof(float),\n142:                              cudaMemcpyDeviceToHost));\n143: \n144:   // Free memory buffers\n145:   checkCudaErrors(cudaFree(bufferOut));\n146:   checkCudaErrors(cudaFree(bufferIn));\n147: \n148:   return true;\n149: }", "label": "(109, CUDA, checkCudaErrors(cudaEventCreate(&profileStart));); (43, CUDA, checkCudaErrors(cudaSetDevice(targetDevice));); (40, CUDA, targetDevice = findCudaDevice(argc, (const char **)argv);); (193, CUDA, checkCudaErrors(cudaEventDestroy(profileEnd));); (140, CUDA, checkCudaErrors(cudaEventRecord(profileEnd, 0));); (116, CUDA, checkCudaErrors(cudaEventRecord(profileStart, 0));); (186, CUDA, checkCudaErrors(cudaEventDestroy(profileStart));); (61, CUDA, checkCudaErrors(cudaFuncGetAttributes(&funcAttrib, FiniteDifferencesKernel));); (31, CUDA, checkCudaErrors(cudaGetDeviceCount(&deviceCount));); (101, CUDA, checkCudaErrors(cudaEventCreate(&profileEnd));); (176, CUDA, checkCudaErrors(cudaFree(bufferOut));); (171, CUDA, checkCudaErrors(cudaFree(bufferIn));)"}
{"predict": "(31,OpenMP,#pragma omp parallel for), (35,OpenMP,#pragma omp parallel for)", "label": "(38, CUDA, cudaFree(res_dev);); (31, CUDA, cudaMalloc(&res_dev, sizeof(float));); (33, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(15,OpenMP,#pragma omp parallel for)", "label": "(18, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (18, CUDA, checkCudaErrors(cudaGetLastError());)"}
{"predict": "(26,OpenMP,#pragma omp parallel for)", "label": "(8, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (9, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;)"}
{"predict": "(13,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp task)", "label": "(29, OpenMP, #pragma omp taskwait); (22, OpenMP, #pragma omp task); (9, OpenMP, #pragma omp parallel); (10, OpenMP, #pragma omp single)"}
{"predict": "(4,OpenMP,#pragma omp parallel for)", "label": "(2, SYCL, sycl::queue q = get_queue();); (3, SYCL, int* data = sycl::malloc_device<int>(1024, q);)"}
{"predict": "(20,MPI,MPI_Allreduce(&count, &scr_ngroupdescs, 1, MPI_INT, MPI_MAX, comm);)", "label": "(7, MPI, MPI_Comm_rank(comm, &rank);); (49, MPI, MPI_Bcast(&num_groups, 1, MPI_INT, 0, comm);)"}
{"predict": "(3,OpenMP,#pragma omp parallel), (3,OpenMP,)#pragma omp sections nowait, (14,OpenMP,)#pragma omp for, (26,OpenMP,)#pragma omp critical, (35,OpenMP,)#pragma omp critical, (45,OpenMP,)#pragma omp barrier", "label": "(4, OpenMP, #pragma omp sections); (49, OpenMP, #pragma omp sections nowait); (28, OpenMP, #pragma omp section)"}
{"predict": "(29,OpenMP,#pragma omp parallel for)", "label": "(36, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (38, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(28,OpenMP,#pragma omp parallel for)", "label": "(5, CUDA, c.CUDAConfig().device_id = 0;)"}
{"predict": "(7,MPI,MPI_Init(&argc, &argv);)\n(13,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);)\n(14,MPI,MPI_Comm_size(MPI_COMM_WORLD, &ranks);)\n(32,MPI,MPI_Gather(&num, 1, MPI_INT, counts, 1, MPI_INT, 0, MPI_COMM_WORLD);)\n(34,MPI,MPI_Gather(&bytes, 1, MPI_INT, recvbuf, 1, MPI_INT, 0, MPI_COMM_WORLD);)\n(63,MPI,MPI_Gather(sendbuf, bytes, MPI_BYTE, recvbuf, bytes, MPI_BYTE, 0, MPI_COMM_WORLD);)\n(106,MPI,MPI_Finalize();)", "label": "(65, MPI, MPI_Gatherv(sendbuf, bytes, MPI_BYTE, recvbuf, counts, disps, MPI_BYTE, 0, MPI_COMM_WORLD);); (13, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (36, MPI, MPI_Gather(&bytes, 1, MPI_INT, counts, 1, MPI_INT, 0, MPI_COMM_WORLD);)"}
{"predict": "(12,MPI,MPI_Comm_size(MPI_COMM_WORLD, &size);)", "label": "(32, MPI, MPI_Comm_test_inter(intercomm, &flag);); (23, MPI, MPI_Comm_rank(intercomm, &key);); (90, MPI, MPI_Comm_free(&intercomm);); (38, MPI, MPI_Comm_size(intercomm, &orig_size);); (38, MPI, MPI_Comm_size(newcomm, &new_size);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (33, MPI, MPI_Comm_remote_size(intercomm, &orig_rsize);); (19, MPI, MPI_Comm_split(intercomm, color, key, &newcomm);); (3, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (31, MPI, MPI_Comm_remote_size(newcomm, &new_rsize);)"}
{"predict": "(9,MPI,dout_perm += blockIdx.x * B * H * W;); (12,OpenMP,#pragma omp parallel for reduction(+:th_sum)), (22,OpenMP,#pragma omp parallel for reduction(+:shared[threadIdx.x]))", "label": "(6, CUDA, extern __shared__ float shared[];)"}
{"predict": "(24,OpenMP,#pragma omp parallel for)", "label": "(34, CUDA, cudaFree(control_d);); (32, CUDA, cudaFree(state_der_d);); (22, CUDA, CudaCheckError();); (29, CUDA, cudaFree(state_d);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for reduction(+:res))", "label": "(7, CUDA, fcuda::Check_CudaErroorFun(\"DgReduSumUint\");); (5, CUDA, cudaMemcpy(data,datag+inidata,sizeof(unsigned)*ndata,cudaMemcpyDeviceToHost);)"}
{"predict": "(16,MPI, *w = MPI_Wtime();)", "label": "(37, TBB, tbb::spin_mutex::scoped_lock lock (MyMutex);)"}
{"predict": "(33,OpenMP,#pragma omp parallel for private(Kj,Vj)), (74,OpenMP,#pragma omp parallel for private(sum)), (90,OpenMP,#pragma omp parallel for private(pv))", "label": "(24, CUDA, extern __shared__ float sram[];)"}
{"predict": "()", "label": "(14, OpenMP, #pragma omp parallel); (14, OpenMP, #pragma omp task if (task: false))"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for collapse(1)), (27,OpenMP,#pragma omp parallel for)", "label": "(47, OpenMP, #pragma omp target teams distribute parallel for collapse(collapse(tmain<int, char, -1, -2>(argc, argv)); (4, OpenMP, #pragma omp target teams distribute parallel for collapse (); (7, OpenMP, #pragma omp target teams distribute parallel for collapse ()); (18, OpenMP, #pragma omp target teams distribute parallel for collapse (foobool(1) > 0 ? 1 : 2)); (13, OpenMP, #pragma omp target teams distribute parallel for collapse (2+2))); (23, OpenMP, #pragma omp target teams distribute parallel for collapse (foobool(argc)), collapse (true), collapse (-5)); (7, OpenMP, #pragma omp target teams distribute parallel for collapse (4); (25, OpenMP, #pragma omp target teams distribute parallel for collapse (S1)); (-7, OpenMP, #pragma omp target teams distribute parallel for collapse); (41, OpenMP, #pragma omp target teams distribute parallel for collapse (2))"}
{"predict": "(3,MPI,*w = MPI_Wtime(););", "label": "(32, CUDA, cudaFree(resourceList->outputBufferGPU);); (27, CUDA, cudaFree(resourceList->inputBufferGPU);); (44, CUDA, cudaStreamDestroy(resourceList->stream);)"}
{"predict": "(30,OpenMP,#pragma omp parallel for)", "label": "(77, CUDA, EXPECT_EQ(cudaSuccess, cudaMemcpy(test.data(), output->basePtr(), outBufSize, cudaMemcpyDeviceToHost));); (2, CUDA, ASSERT_EQ(cudaSuccess, cudaStreamCreate(&stream));); (64, CUDA, EXPECT_EQ(cudaSuccess, cudaMemset(input->basePtr(), 0xFF, inSampleStride * inAccess->numSamples()));); (72, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamSynchronize(stream));); (72, CUDA, EXPECT_EQ(cudaSuccess, cudaMemcpy(testIn.data(), input->basePtr(), inBufSize, cudaMemcpyDeviceToHost));); (62, CUDA, EXPECT_EQ(cudaSuccess, cudaMemset(output->basePtr(), 0xFF, outSampleStride * outAccess->numSamples()));); (78, CUDA, EXPECT_EQ(cudaSuccess, cudaStreamDestroy(stream));)"}
{"predict": "(17,OpenMP,#pragma omp parallel for)\n(22,OpenMP,#pragma omp parallel for)\n(24,OpenMP,#pragma omp parallel for)", "label": "(28, OpenMP, #pragma omp parallel num_threads(NumThreads3)); (29, OpenMP, check3[omp_get_thread_num()] += omp_get_num_threads();); (15, OpenMP, #pragma omp parallel); (10, OpenMP, threadLimit = omp_get_thread_limit();); (6, OpenMP, #pragma omp target teams num_teams(1) thread_limit(ThreadLimit)); (20, OpenMP, check2[omp_get_thread_num()] += omp_get_num_threads();); (16, OpenMP, omp_set_num_threads(NumThreads2);); (12, OpenMP, check1[omp_get_thread_num()] += omp_get_num_threads();)"}
{"predict": "(10,MPI,*w = MPI_Wtime(););", "label": "(13, CUDA, getLastCudaError(\"mergeElementaryIntervalsKernel<1> failed\\n\");); (17, CUDA, getLastCudaError(\"mergeElementaryIntervalsKernel<0> failed\\n\");)"}
{"predict": "(27,MPI,*w = MPI_Wtime();); (31,MPI,MPI_Barrier(MPI_COMM_WORLD);); (35,MPI,MPI_Barrier(MPI_COMM_WORLD);); (39,MPI,MPI_Barrier(MPI_COMM_WORLD);); (43,MPI,MPI_Barrier(MPI_COMM_WORLD);); (47,MPI,MPI_Barrier(MPI_COMM_WORLD);); (51,MPI,MPI_Barrier(MPI_COMM_WORLD);); (55,MPI,MPI_Barrier(MPI_COMM_WORLD);); (61,MPI,MPI_Barrier(MPI_COMM_WORLD);); (67,MPI,MPI_Barrier(MPI_COMM_WORLD);); (73,MPI,MPI_Barrier(MPI_COMM_WORLD);); (79,MPI,MPI_Barrier(MPI_COMM_WORLD);); (85,MPI,MPI_Barrier(MPI_COMM_WORLD);); (91,MPI,MPI_Barrier(MPI_COMM_WORLD);); (97,MPI,MPI_Barrier(MPI_COMM_WORLD);); (103,MPI,MPI_Barrier(MPI_COMM_WORLD);); (109,MPI,MPI_Barrier(MPI_COMM_WORLD);); (115,MPI,MPI_Barrier(MPI_COMM_WORLD);); (121,MPI,MPI_Barrier(MPI_COMM_WORLD);); (127,MPI,MPI_Barrier(MPI_COMM_WORLD);); (133,MPI,MPI_Barrier(MPI_COMM_WORLD);); (139,MPI,MPI_Barrier(MPI_COMM_WORLD););", "label": "(42, CUDA, cudaFree(B);); (109, CUDA, result = cudaMemcpy(host_cutlass.data(), C_cutlass, sizeof_C, cudaMemcpyDeviceToHost);); (59, CUDA, cudaFree(C_reference);); (52, CUDA, result = cudaMemcpy(C_reference, C_cutlass, sizeof_C, cudaMemcpyDeviceToDevice);); (47, CUDA, cudaFree(C_cutlass);); (119, CUDA, result = cudaMemcpy(host_reference.data(), C_reference, sizeof_C, cudaMemcpyDeviceToHost);); (28, CUDA, cudaFree(A);)"}
{"predict": "(28,MPI,int MPI_Init(&argc, &argv);), (31,MPI,int MPI_Comm_rank(MPI_COMM_WORLD, &rank);), (32,MPI,int MPI_Comm_size(MPI_COMM_WORLD, &size);), (34,MPI,int MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);), (36,MPI,int MPI_Scatter(b, n*n/size, MPI_LONG, b_local, n*n/size, MPI_LONG, 0, MPI_COMM_WORLD);), (42,MPI,a[index[i]] = i + 10 * rank;), (48,MPI,MPI_Gather(a, n/size, MPI_LONG, a_global, n/size, MPI_LONG, 0, MPI_COMM_WORLD);), (51,MPI,MPI_Finalize();)", "label": "(40, OpenMP, #pragma omp teams reduction(+:sum)); (37, OpenMP, #pragma omp target map(tofrom: b, sum, t) map(alloc: a)); (40, OpenMP, #pragma omp distribute parallel for reduction(+:sum)); (42, OpenMP, t[i] = omp_get_team_num();)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(14, CUDA, fprintf(stderr, \"CUDA kernel failed : %s\\n\", cudaGetErrorString(err));); (11, CUDA, err = cudaGetLastError();)"}
{"predict": "(18,OpenMP,#pragma omp parallel for)", "label": "(4, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (5, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;)"}
{"predict": "(27,OpenMP,#pragma omp parallel for), (41,OpenMP,#pragma omp parallel for)", "label": "(4, CUDA, CudaCheckError();)"}
{"predict": "1, berr);\n150:         }\n151: \n152:         /* Free the memory used. */\n153:         dScalePermstructFree(&ScalePermstruct);\n154:         dLUstructFree(&LUstruct);\n155:         dSOLVEstructFree(&SOLVEstruct);\n156:         Destroy_CompCol_Matrix_dist(&A);\n157:         Destroy_SuperNode_Matrix_dist(&A);\n158:         Destroy_Nodal_Matrix_dist(&A);\n159:         Destroy_CompCol_Matrix(&A);\n160:         Destroy_SuperNode_Matrix(&A);\n161:         Destroy_Nodal_Matrix(&A);\n162:         SUPERLU_FREE(b);\n163:         SUPERLU_FREE(xtrue);\n164:         SUPERLU_FREE(berr);\n165:     } else if ( iam >= 6 && iam < 12 ) { /* I am in grid 2. */\n166:         iam = grid2.iam;  /* Get the logical number in the new grid. */\n167: \n168:         /* ------------------------------------------------------------\n169:            GET THE MATRIX FROM FILE AND SETUP THE RIGHT HAND SIDE. \n170:            ------------------------------------------------------------*/\n171:         dcreate_matrix_postfix(&A, nrhs, &b, &ldb, &xtrue, &ldx, fp, postfix, &grid2);\n172: \t\n173: \tif ( !(berr = doubleMalloc_dist(nrhs)) )\n174: \t    ABORT(\"Malloc fails for berr[].\");\n175: \n176: \t/* ------------------------------------------------------------\n177: \t   NOW WE SOLVE THE LINEAR SYSTEM.\n178: \t   ------------------------------------------------------------*/\n179: \t\n180:         /* Set the default input options:\n181:             options.Fact = DOFACT;\n182:             options.Equil = YES;\n183:             options.ColPerm = METIS_AT_PLUS_A;\n184:             options.RowPerm = LargeDiag_MC64;\n185:             options.ReplaceTinyPivot = NO;\n186:             options.Trans = NOTRANS;\n187:             options.IterRefine = DOUBLE;\n188:             options.SolveInitialized = NO;\n189:", "label": "(82, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &iam );); (257, MPI, MPI_Finalize();); (24, MPI, MPI_Init_thread( &argc, &argv, MPI_THREAD_MULTIPLE, &omp_mpi_level);); (25, MPI, MPI_Comm_size( MPI_COMM_WORLD, &nprocs );)"}
{"predict": "+ wm_val] = 1;\n104:                    //printf(\"WAIT3 (%d,%d) msg arrived in row %d, i=%d\\n\",mype,tid,d_colnummod[tid], i);\n105:                }\n106:            }\n107:        } else {\n108:            int delta = d_nfrecvmod[1] % WAIT_NUM_THREADS;\n109:            if (tid < delta) {\n110:                d_mynum[tid] = d_nfrecvmod[1] / WAIT_NUM_THREADS + 1;\n111:            } else {\n112:                d_mynum[tid] = d_nfrecvmod[1] / WAIT_NUM_THREADS;\n113:            }\n114:            __syncthreads();\n115:            d_mymaskstart[tid] = 0;\n116:            for (int i = 0; i < tid; i++) {\n117:                d_mymaskstart[tid] += d_mynum[i];\n118:            }\n119:            d_mymasklength[tid] = d_colnummod[d_mymaskstart[tid] + d_mynum[tid] - 1] - d_colnummod[d_mymaskstart[tid]] + 1;\n120:            __syncthreads();\n121:            //printf(\"WAIT4 (%d,%d) mynum=%d, start=%d,%d length=%d\\n\",mype,tid,d_mynum[tid],d_mymaskstart[tid],d_colnummod[d_mymaskstart[tid]],d_mymasklength[tid]);\n122: \n123:            for (int i = 0; i < d_mynum[tid]; i++) {\n124:                int wm_val = nvshmem_uint64_wait_until_any(flag_rd_q + d_colnummod[tid] * 2, 2,\n125:                                                        d_statusmod + d_colnummod[tid] * 2, NVSHMEM_CMP_EQ, 1);\n126:                d_statusmod[d_colnummod[tid] * 2 + wm_val] = 1;\n127:                //printf(\"WAIT4 (%d,%d) msg arrived in row %d, i=%d\\n\",mype,tid,d_colnum", "label": "(343, CUDA, __shared__ int cur_send_num;); (341, CUDA, __shared__ int recv_num, finish_num;)"}
{"predict": "(10,OpenMP,#pragma omp parallel for reduction(+:sdata[tid]))", "label": "(4, CUDA, __shared__ float sdata[THREAD_PER_BLOCK];)"}
{"predict": ": \n138:     /* close files */\n139:     mfu_close(src_name, fd);\n140:     mfu_close(dst_name, fd_out);\n141: \n142:     /* free memory */\n143:     mfu_free(&src_name);\n144:     mfu_free(&dst_name);\n145: \n146:     return rc;\n147: }", "label": "(10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (76, MPI, MPI_Bcast(&footer_flag, 1, MPI_INT, 0, MPI_COMM_WORLD);); (76, MPI, MPI_Bcast(&footer, 8, MPI_UINT64_T, 0, MPI_COMM_WORLD);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(1,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (44,OpenMP,#pragma omp parallel for), (60,OpenMP,#pragma omp parallel for)", "label": "(2, CUDA, __shared__ float smem[1024];)"}
{"predict": "(48,OpenMP,#pragma omp parallel for), (69,OpenMP,#pragma omp parallel for), (78,OpenMP,#pragma omp parallel for)", "label": "(38, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(30,OpenMP,#pragma omp parallel for reduction(+:sum)), (31,OpenMP,#pragma omp parallel for reduction(+:sum))", "label": "(29, CUDA, checkCudaErrors(cudaSetDevice(gpuID));)"}
{"predict": "6:         *(int *)ptr = 1;\n147:         if (err) {\n148:             cout << \"Error unmapping buffer\" << endl;\n149:             abort();\n150:         }\n151:     }\n152: \n153:     if (err) {\n154:         cout << \"Error enqueueing kernel\" << endl;\n155:         abort();\n156:     }\n157: \n158:     if (err) {\n159:         cout << \"Error reading buffer\" << endl;\n160:         abort();\n161:     }\n162: \n163:     if (err) {\n164:         cout << \"Error unmaping buffer\" << endl;\n165:         abort();\n166:     }\n167: \n168:     if (err) {\n169:         cout << \"Error releasing resources\" << endl;\n170:         abort();\n171:     }\n172: \n173:     return retVal;\n174: }", "label": "(69, OpenCL, program = clCreateProgramWithSource(context, 1, &strings, 0, &err);); (129, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(cl_mem), &buffer);); (190, OpenCL, clReleaseKernel(kernel);); (191, OpenCL, clReleaseCommandQueue(queue);); (189, OpenCL, clReleaseProgram(program);); (139, OpenCL, err = clEnqueueNDRangeKernel(queue, kernel, dimension, offset, gws, lws, 0, 0, nullptr);); (189, OpenCL, clReleaseContext(context);); (101, OpenCL, buffer = clCreateBuffer(context, flags, bufferSize, nullptr, &err);); (115, OpenCL, err = clEnqueueUnmapMemObject(queue, buffer, ptr, 0, nullptr, nullptr);); (106, OpenCL, void *ptr = clEnqueueMapBuffer(queue, buffer, CL_TRUE, CL_MAP_WRITE, 0, bufferSize, 0, nullptr, nullptr, &err);); (38, OpenCL, queue = clCreateCommandQueue(context, device_id, 0, &err);); (140, OpenCL, void *ptr = clEnqueueMapBuffer(queue, buffer, CL_TRUE, CL_MAP_READ, 0, bufferSize, 0, nullptr, nullptr, &err);); (64, OpenCL, err = clBuildProgram(program, 1, &device_id, nullptr, nullptr, nullptr);); (178, OpenCL, clReleaseMemObject(buffer);); (28, OpenCL, context = clCreateContext(0, 1, &device_id, NULL, NULL, &err);); (84, OpenCL, kernel = clCreateKernel(program, \"hello\", &err);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for)", "label": "(6, MPI, MPI_Comm_size(comm, &nproc);); (4, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(27, TBB, tbb::tick_count t1 = tbb::tick_count::now();); (23, TBB, reducePiExample<tbb::auto_partitioner>(num_intervals, gs);); (6, TBB, c.set_max_concurrency(std::min(8, tbb::info::default_concurrency(c)));); (29, TBB, double v3 = deterministicReducePiExample<tbb::simple_partitioner>(num_intervals, gs);); (27, TBB, tbb::tick_count t3 = tbb::tick_count::now();); (-1, TBB, tbb::task_arena::constraints c;); (11, TBB, tbb::tick_count ts_1 = tbb::tick_count::now();); (18, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (27, TBB, tbb::tick_count t5 = tbb::tick_count::now();); (25, TBB, double v4 = deterministicReducePiExample<tbb::static_partitioner>(num_intervals, gs);); (5, TBB, tbb::tick_count ts_0 = tbb::tick_count::now();); (-2, TBB, tbb::task_arena a(c);); (-6, TBB, c.set_core_type(tbb::info::core_types().back());); (20, TBB, tbb::tick_count t4 = tbb::tick_count::now();); (14, TBB, double v1 = reducePiExample<tbb::simple_partitioner>(num_intervals, gs);); (15, TBB, double v2 = reducePiExample<tbb::static_partitioner>(num_intervals, gs);); (-11, TBB, c.set_numa_id(tbb::info::numa_nodes()[0]);); (12, TBB, tbb::tick_count t2 = tbb::tick_count::now();); (8, TBB, double v0 = reducePiExample<tbb::auto_partitioner>(num_intervals, gs);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for), (35,OpenMP,#pragma omp parallel for), (42,OpenMP,#pragma omp parallel for), (49,OpenMP,#pragma omp parallel for), (56,OpenMP,#pragma omp parallel for), (63,OpenMP,#pragma omp parallel for), (70,OpenMP,#pragma omp parallel for)", "label": "(18, CUDA, cuda_safe_call(cudaStreamSynchronize(stream));); (7, CUDA, cuda_safe_call(cudaMalloc(&d_ptrB, N * sizeof(double)));); (9, CUDA, cuda_safe_call(cudaStreamCreate(&stream));); (4, CUDA, cuda_safe_call(cudaMalloc(&d_ptrA, N * sizeof(double)));)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for collapse(2)), (62,OpenMP,#pragma omp for reduction(+:Csub)), (78,OpenMP,#pragma omp parallel for)", "label": "(13, CUDA, __shared__ alignas(alignof(float4)) float Bs[BLOCK_SIZE][BLOCK_SIZE];); (8, CUDA, __shared__ alignas(alignof(float4)) float As[BLOCK_SIZE][BLOCK_SIZE];)"}
{"predict": "/* Poll for FIFO watermark interrupt and read samples */\n124:   while (1)\n125:   {\n126:     /* Get FIFO status */\n127:     lsm6dso_fifo_status_get(&ag_ctx, &wtm_flag);\n128: \n129:     if (wtm_flag)\n130:     {\n131:       /* Read FIFO samples */\n132:       lsm6dso_fifo_data_read(&ag_ctx, &data_raw_acceleration, &data_raw_angular_rate,\n133:                             &data_raw_magnetic, &dummy);\n134:       /* Process data */\n135:       //tx_com((uint8_t*)&data_raw_acceleration, sizeof(data_raw_acceleration));\n136:       //tx_com((uint8_t*)&data_raw_angular_rate, sizeof(data_raw_angular_rate));\n137:       //tx_com((uint8_t*)&data_raw_magnetic, sizeof(data_raw_magnetic));\n138:     }\n139:   }\n140: }\n141: \n142: /* LSM6DSO driver interface function definition */\n143: static int32_t platform_write(void *handle, uint8_t reg, const uint8_t *bufp,\n144:                               uint16_t len)\n145: {\n146:   /* Insert platform specific implementation here */\n147:   return 0;\n148: }\n149: \n149: /* LSM6DSO driver interface function definition */\n150: static int32_t platform_read(void *handle, uint8_t reg, uint8_t *bufp,\n151:                              uint16_t len)\n152: {\n153:   /* Insert platform specific implementation here */\n154:   return 0;\n155: }\n156: \n157: /* TX Buffer full callback */\n158: static void tx_com( uint8_t *tx_buffer, uint16_t len )\n159: {\n160:   /* Insert platform specific implementation here */\n161: }\n162: \n163: /* Platform Delay */\n164: static void platform_delay(uint32_t ms)\n165: {\n166:   /* Insert platform specific implementation", "label": "(78, OpenMP, lis2mdl_offset_temp_comp_set(&mag_ctx, PROPERTY_ENABLE);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (...)\n\n### **Explanation**\n1. **MPI Parallelization**: The code snippet involves a loop that runs across multiple devices. Inserting an MPI parallelization instruction at line 5 can help distribute the loop across multiple processes. The line `*w = MPI_Wtime();` should be inserted before the loop to start timing.\n\n2. **OpenMP Parallelization**: Within the loop, there is a conditional check that could be parallelized using OpenMP. Inserting `#pragma omp parallel for` at line 8 will parallelize the loop, allowing multiple threads to execute the loop iterations concurrently.\n\n3. **Additional OpenMP Directives**: You may want to add OpenMP directives to ensure that the shared variable `num` is properly handled in a thread-safe manner. For example, you can use `#pragma omp critical` around the `printf` statement to prevent race conditions.\n\n### **Improved Code with Parallelization**\n```c\n#include <omp.h>\n#include <mpi.h>\n\nint main() {\n  // MPI initialization\n  MPI_Init(NULL, NULL);\n  double w = MPI_Wtime();\n\n  // Parallelize the loop using OpenMP\n  #pragma omp parallel for\n  for (unsigned i = 0; i < (unsigned)num_devices; i++) {\n    int num;\n\n    if (num != i) {\n      // Ensure thread-safe printing\n      #pragma omp critical\n      printf(\"Fail: Device %u returned id %u\\n\", i, num);\n      return 1;\n    }\n  }\n\n  // End MPI timing\n  w = MPI_Wtime() - w;\n  MPI_Finalize();\n\n  return 0;\n}\n```\n\n### **Explanation of Changes**\n- **MPI Initialization**: Added `MPI_Init(NULL, NULL);` at the beginning to initialize MPI.\n- **MPI Timing**: Added `double w = MPI_Wtime();` at line 5 to start timing.\n- **OpenMP Parallelization**: Added `#pragma omp parallel for` at line 8 to parallelize the loop.\n- **Critical Section**: Added `#pragma omp critical` around the `printf` statement to ensure thread-safe printing.\n- **MPI Finalization**: Added `MPI_Finalize();` at the end to finalize MPI.\n\nThis should help in optimizing the parallel execution of the code using both MPI and OpenMP.", "label": "(2, OpenMP, const int num_devices = omp_get_num_devices();); (6, OpenMP, #pragma omp target map(from : num) device(i)); (6, OpenMP, num = omp_get_device_num();)"}
{"predict": "(37,OpenMP,#pragma omp parallel for)", "label": "(55, OpenCL, clReleaseEvent(clEvent);)"}
{"predict": "(9,OpenMP,#pragma omp parallel for private(dev)), (30,OpenMP,#pragma omp parallel for private(dev)), (35,OpenMP,#pragma omp parallel for private(odev)), (44,OpenMP,#pragma omp parallel for private(odev))", "label": "(47, CUDA, cudaDeviceCanAccessPeer(&access, dev1, dev2);); (53, CUDA, cudaSetDevice(dev);); (57, CUDA, auto err = cudaDeviceEnablePeerAccess(odev, 0);); (14, CUDA, auto err = cudaGetDeviceProperties(&props, dev);); (9, CUDA, INFO(\"failed to cudaSetDevice(%d)\\n\", dev);); (-2, CUDA, cudaGetDeviceCount(reinterpret_cast<int *>(&device));)"}
{"predict": "(11,MPI,*w = MPI_Wtime();); (18,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for)", "label": "(9, TBB, oneapi::tbb::spin_mutex::scoped_lock l(locks[u]);); (20, TBB, oneapi::tbb::spin_mutex::scoped_lock l(locks[v]);)"}
{"predict": "(7,MPI,MPI_Init(&argc, &argv);); (16,MPI,MPI_Bcast(&nccl_id, sizeof(ncclUniqueId), MPI_BYTE, 0, MPI_COMM_WORLD);); (19,MPI,NCCL_CHECK(ncclCommInitRank(&nccl_comm, world_size, nccl_id, rank)););", "label": "(6, MPI, MPI_CHECK(MPI_Init(NULL, NULL));); (15, MPI, MPI_CHECK(MPI_Bcast((void *)&nccl_id, sizeof(nccl_id), MPI_BYTE, 0, MPI_COMM_WORLD));); (16, CUDA, cudaSetDevice(rank);)"}
{"predict": "]) + x;\n134: \n135:             gv += k[j] * ((u1+_u2[j]) * x + s);\n136:         }\n137:         _gv[t] = F(gv);\n138:     }\n139: }", "label": "(22, CUDA, __shared__ float v[_N_], r[_N_], k[_N_], gy[_N_];)"}
{"predict": "(80,OpenMP,#pragma omp parallel for), (86,OpenMP,#pragma omp parallel for), (91,OpenMP,#pragma omp parallel for), (96,OpenMP,#pragma omp parallel for)", "label": "(93, OpenMP, #pragma omp target teams distribute parallel for); (86, OpenMP, #pragma omp target parallel for)"}
{"predict": "8:         MPI_Win_unlock(trank, win);\n109:     }\n110:     MPI_Barrier(MPI_COMM_WORLD);\n111: \n112:     /* Free the window */\n113:     MPI_Win_free(&win);\n114: \n115:     /* Free the buffers */\n116:     free(rmabuf);\n117:     free(localbuf);\n118:     free(localbuf2);\n119:     free(vals);\n120: \n121:     MTest_Finalize();\n122:     return 0;\n123: }", "label": "(12, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (208, MPI, MPI_Win_free(&win);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &wsize);); (46, MPI, MPI_Put(&localbuf[i], 1, MPI_INT, trank, rank + i * wsize, 1, MPI_INT, win);); (44, MPI, MPI_Win_lock(MPI_LOCK_EXCLUSIVE, trank, 0, win);); (45, MPI, MPI_Put(&localbuf[i], 1, MPI_INT, trank, rank + (i + NELM) * wsize, 1, MPI_INT, win);); (45, MPI, MPI_Win_unlock(trank, win);); (127, MPI, MPI_Get(&vals[i], 1, MPI_INT, trank, i, 1, MPI_INT, win);); (183, MPI, MPI_Accumulate(&localbuf[i], 1, MPI_INT, trank, rank + i * wsize, 1, MPI_INT, MPI_SUM, win);); (37, MPI, MPI_Barrier(MPI_COMM_WORLD);); (6, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for)", "label": "(14, CUDA, thrust::device_vector<Element> d_src = h_src;); (8, CUDA, thrust::host_vector<Element> h_src(kNumel);)"}
{"predict": "(47,OpenMP,#pragma omp parallel for)", "label": "(51, CUDA, cudaFree(res_dev);); (44, CUDA, cudaMalloc(&res_dev, sizeof(float));); (46, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "4: \t\t\t\tt1 = SuperLU_timer();\n115: #endif\n116: \n117: \t\t\t\tfor (irow = ikfrow; irow < iklrow; ++irow){\n118: \t\t\t\t\ty = &x[irow];\n119: \n120: #ifdef _OPENMP\n121: #pragma omp for\n122: #endif\n123: \n124: \t\t\t\t\tfor (j = i; j < usub[ik]; ++j){\n125: \t\t\t\t\t\tlsub = Llu->Ufstnz_br_ptr[ik];\n126: \t\t\t\t\t\tlk1 = lsub[j];\n127: \n128: #ifdef _OPENMP\n129: #pragma omp for\n130: #endif\n131: \n132: \t\t\t\t\t\ttemp = uval[j];\n133: \n134: #ifdef _OPENMP\n135: #pragma omp for\n136: #endif\n137: \n138: \t\t\t\t\t\tlk1 = lsub[j];\n139: \n140: #ifdef _OPENMP\n141: #pragma omp for\n142: #endif\n143: \n144: \t\t\t\t\t\ttemp = uval[j];\n145: \n146: #ifdef _OPENMP\n147: #pragma omp for\n148: #endif\n149: \n150: \t\t\t\t\t\tlk1 = lsub[j];\n151: \n152: #ifdef _OPENMP\n153: #pragma omp for\n154: #endif\n145: \n146: #ifdef _OPENMP\n147: #pragma omp for\n148: #endif\n149: \n150: #ifdef _OPENMP\n151: #pragma omp for\n152: #endif\n145: \n146: #ifdef _OPENMP\n147: #pragma omp for\n152: #endif\n145: \n146: #ifdef _OPENMP\n147: #pragma omp for\n152: #endif\n145: \n146: #ifdef _OPENMP\n14", "label": "(82, OpenMP, #pragma\tomp\ttaskloop firstprivate (stat) private (thread_id1,Uinv,nn,lbstart,lbend,ub,temp,rtemp_loc,ik,lk1,gik,gikcol,usub,uval,lsub,lusup,iknsupc,il,i,irow,bmod_tmp,p,ii,jj,t1,t2,j,ikfrow,iklrow,dest,y,uptr,fnz,nsupr,nroot_send_tmp) untied nogroup); (126, OpenMP, #pragma omp simd); (146, OpenMP, #pragma omp atomic capture); (75, OpenMP, #pragma\tomp\tparallel for private (thread_id1,Uinv,nn,lbstart,lbend,ub,temp,rtemp_loc,ik,lk1,gik,gikcol,usub,uval,lsub,lusup,iknsupc,il,i,irow,bmod_tmp,p,ii,jj,t1,t2,j,ikfrow,iklrow,dest,y,uptr,fnz,nsupr))"}
{"predict": "(32,OpenMP,#pragma omp parallel for)", "label": "(48, CUDA, CubDebugExit(cudaMemcpy(d_flags, h_flags, sizeof(unsigned char) * num_items, cudaMemcpyHostToDevice));); (46, CUDA, CubDebugExit(cudaMemcpy(d_in, h_in, sizeof(int) * num_items, cudaMemcpyHostToDevice));)"}
{"predict": "(20,OpenMP,#pragma omp parallel for)", "label": "(6, CUDA, __shared__ uint s_key[SHARED_SIZE_LIMIT];); (6, CUDA, __shared__ uint s_val[SHARED_SIZE_LIMIT];)"}
{"predict": "Code[i]);\n79: \treturn 0;\n80: }\n\n### **Analysis and Recommendations**", "label": "(31, OpenMP, comp_add_script_line(\"Flag=MGF_SALT_AS_HEX_%s\\n\", tmp);); (19, OpenMP, comp_do_parse(0, nSyms-1);); (222, OpenMP, comp_add_script_line(\"Func=DynamicFunc__append_keys_pad20\\n\"); if (use_inp1) len_comp += 20; else len_comp2 += 20; }); (58, OpenMP, if (bNeedS) comp_add_script_line(\"Flag=MGF_SALTED\\n\");); (153, OpenMP, comp_add_script_line(\"Func=DynamicFunc__LargeHash_OUTMode_raw\\n\");); (18, OpenMP, comp_add_script_line(\"Expression=dynamic=%s\\n\", p->pExpr);); (306, OpenMP, comp_add_script_line(\"Flag=MGF_INPBASE64b\\n\");); (307, OpenMP, comp_add_script_line(\"Flag=MGF_INPBASE64m\\n\");); (186, OpenMP,"}
{"predict": "(4,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank); MPI_Comm_size(MPI_COMM_WORLD, &size);) (27,OpenMP,#pragma omp parallel for) (29,OpenMP,#pragma omp parallel for)", "label": "(38, MPI, MPI_Op_free(&op);); (16, MPI, MPI_Type_free(&dt_vector);); (35, MPI, MPI_Type_free(&dt_noncontig);); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (13, MPI, MPI_Type_commit(&dt_noncontig);); (9, MPI, MPI_Type_vector(1, 1, noncontig_stride, MPI_INT, &dt_vector);); (9, MPI, MPI_Type_create_resized(dt_vector, 0, sizeof(int) * noncontig_stride, &dt_noncontig);)"}
{"predict": "(1, CUDA, __global__ void mat_trans_smem_padding_kernel(int *dev_A, int M, int N, int *dev_B) {)", "label": "(6, CUDA, __shared__ int s_data[32][33];)"}
{"predict": "(65,OpenMP,#pragma omp parallel for collapse(2))", "label": "(13, CUDA, CUDA_CHECK(cudaGetDeviceProperties(&props, 0));)"}
{"predict": "(7,OpenMP,#pragma omp parallel for)", "label": "(6, CUDA, st = cudaMalloc(&test_ptr, TEST_ALLOC_SIZE);); (11, CUDA, cudaFree(test_ptr);)"}
{"predict": "(20,MPI,*w = MPI_Wtime();)\n(23,OpenMP,#pragma omp parallel for)\n(29,OpenMP,#pragma omp parallel for)\n(35,OpenMP,#pragma omp parallel for)\n(41,OpenMP,#pragma omp parallel for)", "label": "(19, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (25, TBB, serial_tiled_time = (tbb::tick_count::now() - t0).seconds();); (30, TBB, tbb_time = (tbb::tick_count::now() - t0).seconds();); (18, TBB, serial_time = (tbb::tick_count::now() - t0).seconds();)"}
{"predict": "(43,OpenMP,#pragma omp parallel for)", "label": "(52, CUDA, flamegpu::CUDASimulation cudaSimulation(model);)"}
{"predict": "(9,OpenMP,#pragma omp parallel for)", "label": "(8, OpenMP, #pragma omp section); (6, OpenMP, #pragma omp parallel)"}
{"predict": "(28,OpenMP,#pragma omp parallel for), (39,OpenMP,#pragma omp parallel for), (60,OpenMP,#pragma omp parallel for), (72,OpenMP,#pragma omp parallel for), (76,OpenMP,#pragma omp parallel for), (82,OpenMP,#pragma omp parallel for), (85,OpenMP,#pragma omp parallel for), (88,OpenMP,#pragma omp parallel for), (92,OpenMP,#pragma omp parallel for), (95,OpenMP,#pragma omp parallel for)", "label": "(25, MPI, MPI_Type_get_true_extent(key, &key_true_lb, &key_true_extent);); (15, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (83, MPI, MPI_Allreduce(counts, all_counts, 2, MPI_UINT64_T, MPI_SUM, comm);); (18, MPI, MPI_Type_get_extent(keysat, &keysat_lb, &keysat_extent);)"}
{"predict": "(6,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for)", "label": "(24, CUDA, checkCudaErrors(cudaMalloc((void **)&bLines_d, N_LINES * sizeof(BezierLine)));); (35, CUDA, checkCudaErrors(cudaFree(bLines_d));)"}
{"predict": "(1,OpenMP,#pragma omp parallel for), (45,OpenMP,#pragma omp parallel for), (50,OpenMP,#pragma omp parallel for)", "label": "(7, CUDA, __shared__ float shared_sum2[32];); (5, CUDA, __shared__ float shared_sum[32];)"}
{"predict": "(21,OpenMP,#pragma omp parallel for)", "label": "(2, OpenMP, #pragma omp parallel for schedule(static))"}
{"predict": "i < 10; i += 2)\n110:     c[i] = a[i];\n111: \n112: // Ok.\n113: #pragma omp parallel for simd\n114:   for (int i = 0; i < 10; i += 3)\n115:     c[i] = a[i];\n116: \n117: // Ok.\n118: #pragma omp parallel for simd\n119:   for (int i = 0; i < 10; i += 4)\n120:     c[i] = a[i];\n121: \n122: // Ok.\n123: #pragma omp parallel for simd\n124:   for (int i = 0; i < 10; i += 5)\n125:     c[i] = a[i];\n126: \n127: // Ok.\n128: #pragma omp parallel for simd\n129:   for (int i = 0; i < 10; i += 6)\n130:     c[i] = a[i];\n131: \n132: // Ok.\n133: #pragma omp parallel for simd\n134:   for (int i = 0; i < 10; i += 7)\n135:     c[i] = a[i];\n136: \n137: // Ok.\n138: #pragma omp parallel for simd\n139:   for (int i = 0; i < 10; i += 8)\n140:     c[i] = a[i];\n141: \n142: // Ok.\n143: #pragma omp parallel for simd\n144:   for (int i = 0; i < 10; i += 9)\n145:     c[i] = a[i];\n146: \n147: // Ok.\n148: #pragma omp parallel for simd\n149:   for (int i = 0; i < 10; i += 10)\n150:     c[i] = a[i];\n151: \n152: // Ok.\n153:", "label": "(235, OpenMP, #pragma omp parallel for simd private(ii)); (5, OpenMP, #pragma omp parallel for simd); (257, OpenMP, #pragma omp parallel for simd collapse(2)); (226, OpenMP, #pragma omp parallel for simd linear(ii)); (237, OpenMP, #pragma omp parallel for simd lastprivate(ii)); (220, OpenMP, #pragma omp parallel for simd firstprivate(ii))"}
{"predict": "(14,OpenMP,#pragma omp parallel for)", "label": "(12, TBB, tbb::parallel_sort(iter, iter + n, *compare );)"}
{"predict": "(11,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for)", "label": "(10, TBB, double serial_time = (tbb::tick_count::now() - t0).seconds();); (7, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (16, TBB, double tbb_time = (tbb::tick_count::now() - t1).seconds();); (13, TBB, tbb::tick_count t1 = tbb::tick_count::now();)"}
{"predict": "(26,MPI,*w = MPI_Wtime(););\n(29,MPI,*w = MPI_Wtime(););\n(32,OpenMP,#pragma omp parallel for),", "label": "(24, CUDA, CHECK(cudaMalloc(&d_B, M));); (24, CUDA, CHECK(cudaMemcpy(d_A, h_A, M, cudaMemcpyHostToDevice));); (42, CUDA, CHECK(cudaFree(d_B));); (20, CUDA, CHECK(cudaMalloc(&d_A, M));); (28, CUDA, CHECK(cudaMemcpy(h_B, d_B, M, cudaMemcpyDeviceToHost));); (38, CUDA, CHECK(cudaFree(d_A));)"}
{"predict": "(14,OpenMP,#pragma omp parallel for)", "label": "(4, CUDA, HANDLE_ERROR(cudaStreamCreate(&stream));); (15, CUDA, HANDLE_ERROR(cudaStreamDestroy(stream));)"}
{"predict": "(10,OpenMP,#pragma omp parallel for reduction(+:sum))", "label": "(12, CUDA, __shared__ uint data[MERGE_THREADBLOCK_SIZE];)"}
{"predict": "(24,OpenMP,#pragma omp parallel)", "label": "(2, OpenMP, #pragma omp critical)"}
{"predict": "(4,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for)", "label": "(38, OpenCL, retVal = clReleaseMemObject(clBuffer);)"}
{"predict": "(27,OpenMP,#pragma omp parallel for), (50,OpenMP,#pragma omp parallel for)", "label": "(26, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&data, size_bytes));); (19, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamCreate(&stream));); (48, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (46, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (53, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(data));); (53, CUDA, CUDA_CHECK_AND_EXIT(cudaStreamDestroy(stream));)"}
{"predict": "(7,MPI,*w = MPI_Wtime();)", "label": "(15, OpenMP, #pragma omp target map(from:px)); (7, OpenMP, #pragma omp target data map(to:x[0:count])); (21, OpenMP, #pragma omp target data use_device_ptr(x)); (26, OpenMP, #pragma omp target data use_device_addr(x[0:0]))"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (48,OpenMP,#pragma omp parallel for), (74,OpenMP,#pragma omp parallel for), (86,OpenMP,#pragma omp parallel for)", "label": "(59, CUDA, CHECKED_CALL(cudaEventRecord(start, 0));); (52, CUDA, CHECKED_CALL(cudaDeviceSetLimit(cudaLimitStackSize, 8192));); (62, CUDA, CHECKED_CALL(cudaGetLastError());); (41, CUDA, CHECKED_CALL(cudaEventCreate(&start));); (36, CUDA, CHECKED_CALL(cudaSetDevice(devID));); (88, CUDA, CHECKED_CALL(cudaEventDestroy(start));); (89, CUDA, CHECKED_CALL(cudaFree(d_result));); (61, CUDA, CHECKED_CALL(cudaDeviceSynchronize());); (6, CUDA, cudaGetDeviceProperties(&deviceProp, devID);); (3, CUDA, int devID = findCudaDevice(argc, (const char **)argv);); (78, CUDA, CHECKED_CALL(cudaEventElapsedTime(&time, start, stop));); (56, CUDA, CHECKED_CALL(cudaEventRecord(stop, 0));); (84, CUDA, CHECKED_CALL(cudaFree(d_nresults));); (30, CUDA, CHECKED_CALL(cudaMalloc((void **)&d_nresults, THREADS * sizeof(*d_nresults)));); (80, CUDA, CHECKED_CALL(cudaEventDestroy(stop));); (30, CUDA, CHECKED_CALL(cudaEventCreate(&stop));)"}
{"predict": "(7,MPI, *w = MPI_Wtime();) (10,OpenMP, #pragma omp parallel for)", "label": "(4, OpenACC, #pragma acc enter data create(in1[:w*h], in2[:h*w], out[:w*h])); (12, OpenACC, #pragma acc loop); (8, OpenACC, #pragma acc update device(in1[lower*w:(upper-lower)*w],in2[lower*w:(upper-lower)*w]) async(block%2)); (17, OpenACC, #pragma acc wait); (7, OpenACC, #pragma acc parallel loop present(in1,in2, out) async(block%2)); (13, OpenACC, #pragma acc update self(out[lower*w:(upper-lower)*w]) async(block%2)); (15, OpenACC, #pragma acc exit data delete(in1, in2, out))"}
{"predict": "(30,OpenMP,#pragma omp parallel for)\n(41,OpenMP,#pragma omp parallel for)\n(47,OpenMP,#pragma omp parallel for)\n(48,OpenMP,#pragma omp parallel for)\n(49,OpenMP,#pragma omp parallel for)", "label": "(5, CUDA, safeCall(cudaGetSymbolAddress((void**)&d_PointCounterAddr, d_PointCounter));); (57, CUDA, safeCall(cudaMemcpy(&totPts, &d_PointCounterAddr[2*octave+1], sizeof(int), cudaMemcpyDeviceToHost));); (5, CUDA, safeCall(cudaMemcpy(&fstPts, &d_PointCounterAddr[2*octave-1], sizeof(int), cudaMemcpyDeviceToHost));); (50, CUDA, safeCall(cudaDestroyTextureObject(texObj));); (32, CUDA, cudaCreateTextureObject(&texObj, &resDesc, &texDesc, NULL);)"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for), (50,OpenMP,#pragma omp parallel for), (52,OpenMP,#pragma omp parallel for), (74,OpenMP,#pragma omp parallel for)", "label": "(4, TBB, tbb::flow::queue_node<T> q2(g);); (51, TBB, tbb::flow::make_edge( q2, q3 );); (3, TBB, tbb::flow::queue_node<T> q3(g);); (48, TBB, tbb::flow::make_edge( q, q2 );); (-2, TBB, tbb::flow::graph g;); (68, TBB, tbb::flow::queue_node<T> q_copy(q);); (-3, TBB, tbb::flow::queue_node<T> q(g);)"}
{"predict": "(16,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for), (36,OpenMP,#pragma omp parallel for), (41,OpenMP,#pragma omp parallel for), (46,OpenMP,#pragma omp parallel for)", "label": "(27, MPI, MPI_Win_set_attr(win, key[0], &attrval[0]);); (12, MPI, MPI_Win_create(buf, sizeof(int), sizeof(int), MPI_INFO_NULL, comm, &win);); (29, MPI, MPI_Win_delete_attr(win, key[0]);); (30, MPI, MPI_Win_delete_attr(win, key[2]);); (58, MPI, MPI_Win_free_keyval(&key[i]);); (20, MPI, MPI_Win_set_attr(win, key[2], &attrval[2]);); (58, MPI, MPI_Win_free(&win);); (19, MPI, MPI_Win_set_attr(win, key[1], &attrval[1]);); (24, MPI, MPI_Win_delete_attr(win, key[1]);)"}
{"predict": "call\n136: \t\t\tfor (j = 0; j < SSE_GROUP_SZ_SHA1; ++j) {\n137: \t\t\t\tlens[j] = saved_len[index+j];\n138: \t\t\t\tkeys[j] = (const unsigned char*)(saved_key[index+j]);\n139: \t\t\t}\n140: \t\t\tpkcs12_pbe_derive_key_simd_sha1(cur_salt->iteration_count,\n141: \t\t\t\t\tMBEDTLS_PKCS12_DERIVE_KEY,\n142: \t\t\t\t\tkeys, lens, cur_salt->salt,\n143: \t\t\t\t\tcur_salt->saltlen, ckey, 32);\n144: \t\t\tfor (j = 0; j < SSE_GROUP_SZ_SHA1; ++j) {\n145: \t\t\t\tTwofish_prepare_key(ckey[j], 32, &tkey);\n146: \t\t\t\tdatalen = Twofish_Decrypt(&tkey, cur_salt->store_data, store_data_decrypted, cur_salt->store_data_length, iv[j]);\n147: \t\t\t\tif (datalen < 0)\n148: \t\t\t\t\tcontinue;\n149: \t\t\t\tSHA1_Init(&ctx);\n150: \t\t\t\tSHA1_Update(&ctx, store_data_decrypted, datalen - 20);\n151: \t\t\t\tSHA1_Final(compute_checkum, &ctx);\n152: \n153: \t\t\t\tif (!memcmp(compute_checkum, store_data_decrypted + datalen - 20, 20))\n154: \t\t\t\t{\n155: \t\t\t\t\tcracked[index+j] = 1;\n156: #ifdef _OPENMP\n157: #pragma omp atomic\n158: #endif\n159: \t\t\t\t\tany_cracked |= 1;\n160: \t\t\t\t}\n161: \t\t\t}\n162: \t\t}\n163: #endif\n164: \t}\n165: \treturn any_cracked;\n166: }", "label": "(11, OpenMP, #pragma omp parallel for); (38, OpenMP, #pragma omp atomic)"}
{"predict": "(15,MPI,if (rank == 0) {MPI_Bcast(outbuf, size, key, 0, comm);}); (21,OpenMP,#pragma omp critical)", "label": "(19, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();)", "label": "(28, MPI, MPI_Init(&argc, &argv);); (82, MPI, MPI_Comm_disconnect(&icomm);); (76, MPI, MPI_Comm_spawn(argv[0], &spawn_argv[1], 1, spawn_info, 0, MPI_COMM_WORLD, &icomm, spawn_err);); (26, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (83, MPI, rc = MPI_Comm_set_errhandler(MPI_COMM_SELF, MPI_ERRORS_ARE_FATAL);); (43, MPI, MPI_Info_get(MPI_INFO_ENV, \"mpi_initial_errhandler\", MPI_MAX_INFO_VAL, init_errh_info, &flag);); (68, MPI, MPI_Info_set(spawn_info, \"mpi_initial_errhandler\", \"mpi_errors_return\");); (90, MPI, rc = MPI_Finalize();); (74, MPI, MPI_Barrier(icomm);); (80, MPI, MPI_Error_string(rc, estr, &slen);); (49, MPI, MPI_Comm_get_errhandler(MPI_COMM_SELF, &errh);); (62, MPI, MPI_Info_create(&spawn_info);); (93, MPI, rc = MPI_Error_string(MPI_ERR_WIN, estr, &slen);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for)", "label": "(3, OpenMP, #pragma omp target map(tofrom: threadCount)); (4, OpenMP, #pragma omp metadirective)"}
{"predict": "(15,MPI,*w = MPI_Wtime(););", "label": "(14, OpenMP, printf(\"Error: xomp_get_maxThreadBlocksPerMultiprocessor(): unhandled Compute Capability numbers%d.%d \\n\", major, minor);); (3, CUDA, major = xomp_getCudaDeviceProp(devID)-> major;); (3, CUDA, minor = xomp_getCudaDeviceProp(devID)-> minor;)"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for)", "label": "(26, MPI, MPI_Comm_dup(comms[*(int *) arg], &comm);); (4, MPI, MPI_Comm_rank(comms[*(int *) arg], &rank);); (20, MPI, MPI_Comm_free(&self_dup);); (24, MPI, MPI_Comm_free(&comm);); (13, MPI, MPI_Comm_dup(MPI_COMM_SELF, &self_dup);)"}
{"predict": "(20,OpenMP,#pragma omp parallel for)", "label": "(12, CUDA, cudaStreamCreate(&stream);); (37, CUDA, HANDLE_ERROR(cudaStreamSynchronize(stream));); (17, CUDA, HANDLE_ERROR(cudaMalloc((void**)&costs_dev_only_d, sizeof(float) * num_rollouts));); (15, CUDA, HANDLE_ERROR(cudaMalloc((void**)&baseline_and_normalizer_d, sizeof(float2)));); (16, CUDA, HANDLE_ERROR(cudaMalloc((void**)&costs_host_only_d, sizeof(float) * num_rollouts));)"}
{"predict": "(1,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for)", "label": "(2, CUDA, __shared__ typename BlockReduce<float>::TempStorage v_storage;); (0, CUDA, __shared__ typename BlockReduce<float>::TempStorage m_storage;)"}
{"predict": "(32,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for), (49,OpenMP,#pragma omp parallel for)", "label": "(33, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (41, TBB, reduce_time = (tbb::tick_count::now() - t0).seconds();); (47, TBB, graph_time = (tbb::tick_count::now() - t0).seconds();); (32, TBB, serial_time = (tbb::tick_count::now() - t0).seconds();)"}
{"predict": "(10,OpenMP,#pragma omp parallel for reduction(+:errs))", "label": "(6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &world_size);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for)", "label": "(8, OpenMP, rec = get_decomp_record(code);)"}
{"predict": "(26,OpenMP,#pragma omp parallel for), (43,OpenMP,#pragma omp parallel for), (60,OpenMP,#pragma omp parallel for)", "label": "(10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for)", "label": "(6, CUDA, __shared__ float sdata[THREAD_PER_BLOCK];); (8, CUDA, __shared__ float var;); (5, CUDA, __shared__ float sdata2[THREAD_PER_BLOCK];); (5, CUDA, __shared__ float mean;)"}
{"predict": "task_group );\n63:     TestTypeDefinitionPresence( task );\n64:     TestTypeDefinitionPresence( task_scheduler_observer );\n65:     TestTypeDefinitionPresence( empty_task );\n66:     TestTypeDefinitionPresence( auto_partitioner );\n67:     TestTypeDefinitionPresence( simple_partitioner );\n68:     TestTypeDefinitionPresence( affinity_partitioner );\n69:     TestTypeDefinitionPresence( fixed_grain_partitioner<int> );\n70:     TestTypeDefinitionPresence( blocked_range<int> );\n71:     TestTypeDefinitionPresence( blocked_range2d<int> );\n72:     TestTypeDefinitionPresence( blocked_range3d<int> );\n73:     TestTypeDefinitionPresence( blocked_range_n<int, 4> );\n74:     TestTypeDefinitionPresence( partitioner );\n75:     TestTypeDefinitionPresence( parallel_for );\n76:     TestTypeDefinitionPresence( parallel_for_each );\n77:     TestTypeDefinitionPresence( parallel_reduce );\n78:     TestTypeDefinitionPresence( parallel_scan );\n79:     TestTypeDefinitionPresence( parallel_sort );\n80:     TestTypeDefinitionPresence( parallel_stable_sort );\n81:     TestTypeDefinitionPresence( parallel_quick_sort );\n82:     TestTypeDefinitionPresence( parallel_radix_sort );\n83:     TestTypeDefinitionPresence( parallel_bitwise_scan );\n84:     TestTypeDefinitionPresence( parallel_for_with_reduction );\n85:     TestTypeDefinitionPresence( parallel_pipeline );\n86:     TestTypeDefinitionPresence( parallel_pipeline_with_cleanup );\n87:     TestTypeDefinitionPresence( parallel_pipeline_with_reduction );\n88:     TestTypeDefinitionPresence( parallel_pipeline_with_cleanup_and_reduction );\n89:     TestTypeDefinitionPresence( parallel_pipeline_with_cleanup_and_reduction_with_cleanup );\n90:     TestTypeDefinitionPresence( parallel_pipeline_with_cleanup_and_reduction_with_cleanup_with_cleanup );\n91:     TestTypeDefinitionPresence( parallel_pipeline_with_cleanup_and_reduction_with_cleanup_with_cleanup_with_cleanup );\n92:     TestTypeDefinitionPresence( parallel_pipeline_with_cleanup_and_reduction_with_cleanup_with_cleanup_with_cleanup_with_cleanup );\n93:     TestTypeDefinitionPresence( parallel_pipeline_with_cleanup_and_reduction_with_cleanup_with_cleanup_with_cleanup_with_cleanup_with_cleanup );\n94:     TestTypeDefinitionPresence( parallel_pipeline_with_cleanup_and_reduction_with_cleanup_with_cleanup_with_cleanup_with_cleanup_with_cleanup_with_cleanup );\n95:     TestTypeDefinitionPresence( parallel_pipeline_with_cleanup_and_reduction_with_cleanup", "label": "(103, TBB, TestFuncDefinitionPresence( parallel_deterministic_reduce, (const tbb::blocked_range<int>&, Body2&, const tbb::simple_partitioner&, tbb::task_group_context&), void );); (32, TBB, TestTypeDefinitionPresence3(flow::async_node<int, int, tbb::flow::queueing_lightweight> );); (96, TBB, TestFuncDefinitionPresence( parallel_for_each, (const intarray&, const Body1&, tbb::task_group_context&), void );); (93, TBB, TestFuncDefinitionPresence( parallel_invoke, (const Body&, const Body&, tbb::task_group_context&), void );); (93, TBB, TestFuncDefinitionPresence( parallel_do, (const intarray&, const Body1a&, tbb::task_group_context&), void );); (27, TBB, TestTypeDefinitionPresence3(flow::multifunction_node<int, intpair, tbb::flow::queueing> );); (30, TBB, TestTypeDefinitionPresence2(flow::join_node<intpair, tbb::flow::key_matching<int> > );); (82, TBB, TestFuncDefinitionPresence( parallel_scan, (const tbb::blocked_range<int>&, const int&, const Body3a&, const Body1b&), int );); (75, TBB, TestFuncDefinitionPresence( parallel_for, (const tbb::blocked_range<int>&, const Body2&, const tbb::simple_partitioner&), void );); (79, TBB, TestFuncDefinitionPresence( parallel_scan, (const tbb::blocked_range2d<int>&, Body3&, const tbb::auto_partitioner&), void );); (74, TBB, TestFuncDefinitionPresence( parallel_reduce, (const tbb::blocked_range<int>&, const int&, const Body2a&, const Body1b&), int );); (23, TBB, TestTypeDefinitionPresence2(flow::continue_node<int, tbb::flow::lightweight> );); (73, TBB, TestFuncDefinitionPresence( parallel_reduce, (const tbb::blocked_range<int>&, Body2&, tbb::affinity_partitioner&), void );); (74, TBB, TestFuncDefinitionPresence( parallel_deterministic_reduce, (const tbb::blocked_range<int>&, Body2&, const tbb::static_partitioner&), void );); (14, TBB, typedef tbb::flow::tuple<int, int> intpair;); (33, TBB, TestTypeDefinitionPresence2(flow::composite_node<tbb::flow::tuple<int>, tbb::flow::tuple<int> > );); (15, TBB, TestTypeDefinitionPresence3(flow::function_node<int, int, tbb::flow::rejecting> );); (82, TBB, TestFuncDefinitionPresence( parallel_for, (int, int, const Body1&, const tbb::auto_partitioner&, tbb::task_group_context&), void );); (68, TBB, TestFuncDefinitionPresence( parallel_deterministic_reduce, (const tbb::blocked_range<int>&, const int&, const Body2a&, const Body1b&), int );); (75, TBB, TestFuncDefinitionPresence( parallel_pipeline, (size_t, const tbb::filter_t<void,void>&), void );); (81, TBB, TestFuncDefinitionPresence( parallel_reduce, (const tbb::blocked_range<int>&, Body2&, const tbb::auto_partitioner&, tbb::task_group_context&), void );); (5, TBB, TestFuncDefinitionPresence( flow::make_edge, (tbb::flow::sender<Msg>&, tbb::flow::receiver<Msg>&), void );); (82, TBB, TestFuncDefinitionPresence( parallel_deterministic_reduce, (const tbb::blocked_range<int>&, Body2&, tbb::task_group_context&), void );); (4, TBB, TestFuncDefinitionPresence( flow::remove_edge, (tbb::flow::sender<Msg>&, tbb::flow::receiver<Msg>&), void );); (76, TBB, TestFuncDefinitionPresence( parallel_for, (int, int, const Body1&, tbb::task_group_context&), void );); (77, TBB, TestFuncDefinitionPresence( parallel_reduce, (const tbb::blocked_range<int>&, Body2&, tbb::task_group_context&), void );); (9, TBB, TestTypeDefinitionPresence2(flow::join_node<intpair, tbb::flow::reserving> );)"}
{"predict": "131:   if (layer != kmp_hier_layer_e::LAYER_THREAD)\n132:     SCHEDULE_SET_LAYER(sched, layer);\n133: #endif\n134: \n135:   return ptr;\n136: }", "label": "(30, OpenMP, __kmp_omp_schedule_restore();)"}
{"predict": "(27,MPI,*w = MPI_Wtime();); (53,OpenMP,#pragma omp parallel for), (56,MPI,MPI_Wtime() - w);", "label": "(12, MPI, MPI_Comm_rank(team.comm, &rank);); (12, MPI, MPI_Comm_size(team.comm, &nprocs);)"}
{"predict": "(9,OpenMP,#pragma omp parallel for)", "label": "(9, OpenCL, kernel_info->kernel = clCreateKernel(program, kernel_info->kernel_name, &err);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for reduction(+:correct_count))", "label": "(6, OpenCL, cl_int *pNumCorrect = (cl_int*) clEnqueueMapBuffer(cmdq, num_correct, CL_TRUE, CL_MAP_READ | CL_MAP_WRITE, 0, sizeof(cl_int), 0, NULL,NULL, &error);); (10, OpenCL, error = clEnqueueUnmapMemObject(cmdq, num_correct, pNumCorrect, 0,NULL,NULL);); (15, OpenCL, pNumCorrect = (cl_int*) clEnqueueMapBuffer(cmdq, num_correct, CL_TRUE, CL_MAP_READ | CL_MAP_WRITE, 0, sizeof(cl_int), 0, NULL,NULL, &error);); (11, OpenCL, error = clEnqueueNDRangeKernel(cmdq, kernel_verify_lists, 1, NULL, &numLists, NULL, 0, NULL, NULL);)"}
{"predict": "(9,MPI,*w = MPI_Wtime(););", "label": "(8, TBB, tbb::task_scheduler_init init_market_p_plus_one(MaxThread + 1);)"}
{"predict": "(1,MPI,*w = MPI_Wtime();); (3,MPI,MPI_Init(&argc, &argv);), (5,MPI,MPI_Comm_rank(comm, &comm_rank);), (6,MPI,MPI_Comm_size(comm, &comm_size);), (20,OpenMP,#pragma omp parallel for), (42,OpenMP,#pragma omp parallel for), (48,OpenMP,#pragma omp parallel for), (51,OpenMP,#pragma omp parallel for), (56,OpenMP,#pragma omp parallel for), (60,OpenMP,#pragma omp parallel for)", "label": "(7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &comm_size);); (69, MPI, MPI_Comm_split(MPI_COMM_WORLD, 0, rand(), &comm);); (57, MPI, MPI_Comm_split(MPI_COMM_WORLD, (comm_rank == comm_size - 1) ? 0 : 1, 0, &comm);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &comm_rank);); (58, MPI, MPI_Comm_free(&comm);); (7, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (30, MPI, MPI_Abort(MPI_COMM_WORLD, -1);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();)", "label": "(1, TBB, oneapi::tbb::flow::graph g;)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for)", "label": "(12, CUDA, checkCudaErrors(cudaMemcpy(dArray, array, size, cudaMemcpyHostToDevice));); (10, CUDA, checkCudaErrors(cudaMalloc(&dArray, size));); (19, CUDA, checkCudaErrors(cudaFree(dArray));); (17, CUDA, checkCudaErrors(cudaMemcpy(array, dArray, size, cudaMemcpyDeviceToHost));)"}
{"predict": "(27,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for)", "label": "(9, TBB, c.set_core_type(tbb::info::core_types().back());); (6, TBB, tbb::task_arena::constraints c;); (36, TBB, tbb::affinity_partitioner aff_p;); (5, TBB, c.set_numa_id(tbb::info::numa_nodes()[0]);); (26, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (46, TBB, double static_time = (tbb::tick_count::now() - t0).seconds();); (5, TBB, c.set_max_concurrency(std::min(8, tbb::info::default_concurrency(c)));); (36, TBB, double affinity_time = (tbb::tick_count::now() - t0).seconds();); (31, TBB, t0 = tbb::tick_count::now();); (25, TBB, double auto_time = (tbb::tick_count::now() - t0).seconds();); (2, TBB, tbb::task_arena arena(c);)"}
{"predict": "= 0.0f;\n107:         float image_base_top  = 0.0f;\n108:         float image_base_right  = input_width;\n109:         float image_base_bottom = input_height;\n1010:         float left_in_image = (left - image_base_left) / (image_base_right - image_base_left);\n1011:         float top_in_image  = (top - image_base_top) / (image_base_bottom - image_base_top);\n1012:         float right_in_image = (right - image_base_left) / (image_base_right - image_base_left);\n1013:         float bottom_in_image = (bottom - image_base_top) / (image_base_bottom - image_base_top);\n1014:         bboxes.emplace_back(vector<float>{left_in_image, top_in_image, right_in_image, bottom_in_image, confidence, theta, label});\n1015:     }\n1016: \n1017:     // NMS\n1018:     vector<vector<float>> bboxes_after_nms = nms(bboxes, nms_threshold);\n1019: \n1020:     // postprocess\n1021:     postprocess(bboxes_after_nms);\n1022: \n1023:     // output\n1024:     printf(\"inference done.\\n\");\n1025: }", "label": "(18, CUDA, checkRuntime(cudaStreamCreate(&stream));); (80, CUDA, checkRuntime(cudaMemcpyAsync(output_data_host, output_data_device, sizeof(float) * output_numel, cudaMemcpyDeviceToHost, stream));); (80, CUDA, checkRuntime(cudaStreamSynchronize(stream));); (58, CUDA, checkRuntime(cudaMemcpyAsync(input_data_device, input_data_host, input_numel * sizeof(float), cudaMemcpyHostToDevice, stream));); (24, CUDA, checkRuntime(cudaMallocHost(&input_data_host, input_numel * sizeof(float)));); (24, CUDA, checkRuntime(cudaMalloc(&input_data_device, input_numel * sizeof(float)));); (191, CUDA, checkRuntime(cudaFree(input_data_device));); (188, CUDA, checkRuntime(cudaFreeHost(input_data_host));); (190, CUDA, checkRuntime(cudaFree(output_data_device));); (63, CUDA, checkRuntime(cudaMalloc(&output_data_device, sizeof(float) * output_numel));); (-5, CUDA, auto engine = make_nvshared(runtime->deserializeCudaEngine(engine_data.data(), engine_data.size()));); (60, CUDA, checkRuntime(cudaMallocHost(&output_data_host, sizeof(float) * output_numel));); (184, CUDA, checkRuntime(cudaFreeHost(output_data_host));); (181, CUDA, checkRuntime(cudaStreamDestroy(stream));)"}
{"predict": "(7,MPI,MPI_Init(&argc, &argv);) (13,OpenMP,#pragma omp parallel for) (16,MPI,MPI_Finalize(0);)", "label": "(17, MPI, MPI_Sendrecv(b, 1, newtype, 0, 0, d, 0, newtype, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (10, MPI, MPI_Type_commit(&newtype);); (8, MPI, MPI_Type_create_hvector(0, 1, 10, MPI_DOUBLE, &newtype);); (9, MPI, MPI_Type_free(&newtype);); (10, MPI, MPI_Type_indexed(0, b, d, MPI_DOUBLE, &newtype);)"}
{"predict": "(10,MPI,*w = MPI_Wtime();); (26,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp barrier)", "label": "(9, CUDA, thrust::device_vector<std::uint8_t> data_source(elements);); (12, CUDA, thrust::sequence(data_source.begin(), data_source.end());); (12, CUDA, thrust::shuffle(data_source.begin(), data_source.end(), re);); (5, CUDA, thrust::default_random_engine re;); (15, CUDA, thrust::device_vector<ValueT> in_values(data_source);); (17, CUDA, thrust::host_vector<KeyT> host_keys(in_keys);); (17, CUDA, thrust::host_vector<ValueT> host_values(in_values);); (13, CUDA, thrust::device_vector<ValueT> out_values(elements);); (8, CUDA, thrust::device_vector<KeyT> out_keys(elements);); (6, CUDA, thrust::device_vector<KeyT> in_keys(data_source);)"}
{"predict": "(15,OpenMP,#pragma omp parallel for private(index, seed, salt, i, lens, slens, pin, pout, sin), reduction(+:any_cracked))", "label": "(13, OpenMP, #pragma omp parallel for); (72, OpenMP, #pragma omp atomic); (43, OpenMP, #pragma omp critical)"}
{"predict": "(116,OpenMP,#pragma omp parallel for)", "label": "(84, OpenCL, sub_buf = clCreateSubBuffer(buf[1], 0, CL_BUFFER_CREATE_TYPE_REGION, &region, &error );); (87, OpenCL, void * map_ptr = clEnqueueMapBuffer(queue, buf[0], 1, CL_MAP_READ, 0, 64, 0, NULL, NULL, NULL);)"}
{"predict": "(13,OpenMP,#pragma omp parallel)", "label": "(1, OpenMP, #pragma omp parallel num_threads(2)); (10, OpenMP, #pragma omp cancellation point parallel); (3, OpenMP, #pragma omp cancel parallel)"}
{"predict": "(1,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp target teams distribute parallel for is_device_ptr(DATA)), (26,OpenMP,t1=omp_get_wtime();), (32,OpenMP,t2=omp_get_wtime();)", "label": "(4, OpenMP, omp_set_default_device(0);); (33, OpenMP, #pragma omp target teams distribute parallel for is_device_ptr(DATA) nowait); (15, OpenMP, #pragma omp target teams distribute parallel for is_device_ptr(DATA)); (35, OpenMP, #pragma omp taskwait); (19, OpenMP, t1=omp_get_wtime();); (4, OpenMP, #pragma omp parallel for); (-1, OpenMP, DATA = (double*)   omp_target_alloc(N*sizeof(double), omp_get_default_device()) ;); (22, OpenMP, t2=omp_get_wtime();)"}
{"predict": "(12,OpenMP,#pragma omp parallel for reduction(+:thread_sum))", "label": "(8, CUDA, __shared__ double sum[block_size];)"}
{"predict": "(5,MPI,*w = MPI_Wtime(););", "label": "(62, CUDA, CHECK(cudaPeekAtLastError());)"}
{"predict": "(27,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for)", "label": "(42, CUDA, cudaFree(d_inPtr);); (27, CUDA, cudaMalloc((void**)&d_outPtr, size * sizeof(float));); (32, CUDA, cudaMemcpy(&result_gpu, &d_outPtr[0], sizeof(float), cudaMemcpyDeviceToHost);); (27, CUDA, cudaMemcpy(d_inPtr, h_inPtr, size * sizeof(float), cudaMemcpyHostToDevice);); (37, CUDA, cudaFree(d_outPtr);); (22, CUDA, cudaMalloc((void**)& d_inPtr, size * sizeof(float));)"}
{"predict": "(27,OpenMP,#pragma omp parallel for), (52,OpenMP,#pragma omp parallel for), (55,OpenMP<<<blocks, threads, 0, stream>>>), (67,OpenMP,#pragma omp parallel for)", "label": "(21, CUDA, checkCudaErrors(cudaSetDevice(shm->devices[id]));); (54, CUDA, checkCudaErrors(cudaStreamWaitEvent(stream, events[bufferId], 0));); (71, CUDA, checkCudaErrors(cudaStreamWaitEvent(stream, events[id], 0));); (56, CUDA, checkCudaErrors(cudaGetLastError());); (92, CUDA, checkCudaErrors(cudaStreamDestroy(stream));); (18, CUDA, checkCudaErrors(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));); (86, CUDA, checkCudaErrors(cudaIpcCloseMemHandle(ptrs[i]));); (86, CUDA, checkCudaErrors(cudaEventDestroy(events[i]));); (53, CUDA, checkCudaErrors(cudaEventRecord(events[bufferId], stream));); (68, CUDA, checkCudaErrors(cudaStreamSynchronize(stream));); (12, CUDA, checkCudaErrors(cudaGetDeviceProperties(&prop, shm->devices[id]));)"}
{"predict": "(21,OpenMP,#pragma omp parallel for)", "label": "(21, OpenCL, retVal = clReleaseMemObject(smallBuffer);); (5, OpenCL, cl_mem smallBuffer = clCreateBuffer(clContext, flags, size, hostPtr, &retVal);); (24, OpenCL, EXPECT_EQ(clReleaseContext(context), CL_SUCCESS);)"}
{"predict": "(13,MPI,*w = MPI_Wtime();); (18,OpenMP,#pragma omp parallel for)", "label": "(13, CUDA, cudaCheck(cudaGetLastError());)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (7,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for), (41,OpenMP,#pragma omp parallel for), (51,OpenMP,#pragma omp parallel for), (70,OpenMP,#pragma omp parallel for), (73,OpenMP,#pragma omp parallel for), (89,OpenMP,#pragma omp parallel for)", "label": "(82, MPI, MPI_Wait(&request[1], &status[1]);); (90, MPI, MPI_Wait(&request[0], &status[0]);); (90, MPI, MPI_Get_count(&status[0], MPI_BYTE, &nwrite);); (66, MPI, MPI_Irecv(buf_recv, shuffile_mpi_buf_size, MPI_BYTE, rank_recv, 0, comm, &request[0]);); (77, MPI, MPI_Isend(buf_send, nread, MPI_BYTE, rank_send, 0, comm, &request[1]);)"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(16, CUDA, CUDASimulation cudaSimulation(model);)"}
{"predict": "(22,OpenMP,#pragma omp parallel for), (46,OpenMP,#pragma omp parallel for), (51,OpenMP,#pragma omp parallel for), (56,OpenMP,#pragma omp parallel for)", "label": "(65, CUDA, CHECK_CUDART(cudaFree(indata));); (19, CUDA, CHECK_CUDART(cudaMemcpy(indata, &inval, sizeof(T), cudaMemcpyHostToDevice));); (61, CUDA, CHECK_CUDART(cudaMemcpy(&outval, outdata, sizeof(T), cudaMemcpyDeviceToHost));); (14, CUDA, CHECK_CUDART(cudaMalloc((void**)&indata, sizeof(T)));); (14, CUDA, CHECK_CUDART(cudaMalloc((void**)&outdata, sizeof(T)));); (59, CUDA, CHECK_CUDART(cudaFree(outdata));)"}
{"predict": "configuration.buffer = buffer;\n101: #endif\n102: \t\t\t//Copy input data to the buffer.\n103: #if(VKFFT_BACKEND==0)\n104: \t\t\tresFFT = copyToBuffer(vkGPU, buffer, bufferDeviceMemory, buffer_input, bufferSize);\n105: \t\t\tif (resFFT != VKFFT_SUCCESS) return resFFT;\n106: #elif(VKFFT_BACKEND==1)\n107: \t\t\tcuMemcpyHtoD(buffer, buffer_input, bufferSize);\n108: #elif(VKFFT_BACKEND==2)\n109: \t\t\thipMemcpyHtoD(buffer, buffer_input, bufferSize);\n110: #elif(VKFFT_BACKEND==3)\n111: \t\t\terrno_t err = memcpy_s(buffer, bufferSize, buffer_input, bufferSize);\n112: \t\t\tif (err) return VKFFT_ERROR_FAILED_TO_COPY;\n113: #elif(VKFFT_BACKEND==4)\n114: \t\t\tres = zeMemCopy(vkGPU->context, buffer, buffer_input, bufferSize);\n115: \t\t\tif (res != ZE_RESULT_SUCCESS) return VKFFT_ERROR_FAILED_TO_COPY;\n116: #elif(VKFFT_BACKEND==5)\n117:             memcpy(buffer->contents(), buffer_input, bufferSize);\n118: #endif\n119: \t\t\t//FFT + iFFT sample code.\n120: \t\t\tresFFT = VkFFTApply(&app, &configuration);\n121: \t\t\tif (resFFT != VKFFT_SUCCESS) return resFFT;\n122: \t\t\t//Copy result back to CPU.\n123: \t\t\t//...\n124: \t\t\t//...\n125: \t\t\t//...\n126: \t\t\t//...\n127: \t\t\t//...\n128: \t\t\t//...\n129: \t\t\t//...\n130: \t\t\t//...\n131: \t\t\t//...\n132: \t\t\t//...\n133: \t\t\t//...\n134: \t\t\t//...\n135: \t\t\t//...\n136: \t\t\t//...\n137: \t\t\t//...\n138: \t\t\t//...\n139: \t\t\t//...\n140: \t\t\t//...\n141: \t\t\t//...\n142: \t\t\t//...", "label": "(162, OpenCL, clGetDeviceInfo(vkGPU->device, CL_DEVICE_VENDOR_ID, sizeof(cl_int), &vendorID, 0);); (77, CUDA, res = cudaMalloc((void**)&buffer, bufferSize);); (86, OpenCL, buffer = clCreateBuffer(vkGPU->context, CL_MEM_READ_WRITE, bufferSize, 0, &res);)"}
{"predict": "<float>' requested here}}\n82: }\n\n### ** Answer **", "label": "(113, OpenMP, #pragma omp teams distribute reduction(task, + : m)); (15, OpenMP, #pragma omp teams distribute reduction +); (29, OpenMP, #pragma omp teams distribute reduction(\\)); (37, OpenMP, #pragma omp teams distribute reduction(|| : argc > 0 ? argv[1] : argv[2])); (106, OpenMP, #pragma omp teams distribute reduction(+ : m)); (20, OpenMP, #pragma omp teams distribute reduction()); (6, OpenMP, #pragma omp target); (42, OpenMP, #pragma omp teams distribute reduction(^ : S1)); (44, OpenMP, #pragma omp teams distribute reduction(+ : a, b, c, d, f)); (67, OpenMP, #pragma omp teams distribute reduction(&& : S2::S2sc)); (57, OpenMP, #pragma omp teams distribute reduction(- : da)); (32, OpenMP, #pragma omp teams distribute reduction(~ : argc)); (10, OpenMP, #pragma omp teams distribute reduction(-); (45, OpenMP, #pragma omp teams distribute reduction(max : h.b)); (5, OpenMP, #pragma omp teams distribute reduction(); (75, OpenMP, #pragma omp parallel private(k)); (18, OpenMP, #pragma omp teams distribute reduction(foo : argc); (38, OpenMP, #pragma omp teams distribute reduction(min : a, b, c, d, f)); (70, OpenMP, #pragma omp teams distribute private(i), reduction(+ : j), reduction(+ : q)); (79, OpenMP, #pragma omp teams distribute reduction(+ : r)); (41, OpenMP, #pragma omp teams distribute reduction(+ : ba)); (25, OpenMP, #pragma omp teams distribute reduction(&& : argc)); (81, OpenMP, #pragma omp teams distribute reduction(max : j)); (78, OpenMP, #pragma omp parallel reduction(min : i)); (4, OpenMP, #pragma omp teams distribute reduction(*)); (67, OpenMP, #pragma omp teams distribute reduction(+ : p), reduction(+ : p)); (44, OpenMP, #pragma omp teams distribute reduction(^ : fl)); (58, OpenMP, #pragma omp teams distribute reduction(+ : o)); (54, OpenMP, #pragma omp teams distribute reduction(+ : h, k, B::x)); (35, OpenMP, #pragma omp teams distribute reduction(* : ca)); (76, OpenMP, #pragma omp teams distribute reduction(+ : fl, z)); (48, OpenMP, #pragma omp teams distribute reduction(& : e, g)); (68, OpenMP, #pragma omp parallel shared(i)); (4, OpenMP, #pragma omp teams distribute reduction(| : argc,); (-21, OpenMP, #pragma omp teams distribute reduction); (38, OpenMP, #pragma omp teams distribute reduction(&& : S2::S2s))"}
{"predict": "(10,OpenMP,#pragma omp parallel), (11,OpenMP,#pragma omp single), (12,OpenMP,#pragma omp parallel), (13,OpenMP,#pragma omp single)", "label": "(4, CUDA, checkCudaErrors(cudaGetDeviceProperties(&prop, whichDevice));); (2, CUDA, checkCudaErrors(cudaGetDevice(&whichDevice));)"}
{"predict": "(11,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for)", "label": "(4, OpenMP, #pragma omp parallel for)"}
{"predict": "3:           sampling->computeLikelihoodRatioCost(u, theta_d, global_idx, t, distribution_idx, lambda, alpha);\n124:     }\n125:   }\n126:   /*<----End of simulation loop-----> */\n127: \n128:   // Copy the running cost back to global memory\n129:   int offset = (num_rollouts * thread_idz + global_idx) * num_timesteps + thread_idx;\n130:   trajectory_costs_d[offset] = running_cost[0];\n131: }\n132: \n133: template <class COST_T, class DYN_T, class SAMPLING_T, class FB_T, int NOMINAL_STATE_IDX, bool COALESCE>\n134: void rolloutRMPPICost(COST_T* costs, DYN_T* dynamics, FB_T* fb_controller, SAMPLING_T* sampling, float dt,\n135:                     const int num_timesteps, const int num_rollouts, float lambda, float alpha,\n136:                     float value_func_threshold, const float* init_x_d, const float* y_d, float* trajectory_costs_d)\n137: {\n138:   const int num_blocks = (num_rollouts * blockDim.x + blockDim.x - 1) / blockDim.x;\n139:   rolloutRMPPICostKernel<<<num_blocks, blockDim>>>(costs, dynamics, fb_controller, sampling, dt, num_timesteps,\n140:                                                   num_rollouts, lambda, alpha, value_func_threshold, init_x_d, y_d,\n141:                                                   trajectory_costs_d);\n142: }", "label": "(21, CUDA, extern __shared__ float entire_buffer[];)"}
{"predict": "(30,OpenMP,#pragma omp parallel for private(i))", "label": "(29, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (24, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (55, MPI, MPI_Finalize();); (52, MPI, MPI_T_finalize();); (23, MPI, MPI_T_init_thread(required, &provided);); (36, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (36,OpenMP,#pragma omp parallel for), (42,OpenMP,#pragma omp parallel for)", "label": "(30, TBB, tbb::parallel_reduce ( vs.range(1), pvrb );); (60, TBB, tbb::parallel_for ( tbb::blocked_range<int> (0, N, RANGE_MIN), parallel_vector_for_body<T,Allocator>( vvs, allocator_name ) );); (1, TBB, typedef std::vector<T, tbb::tbb_allocator<T> > container_type;); (5, TBB, tbb::task_scheduler_init init(p);); (10, TBB, if (Verbose && t == 0) t0 = tbb::tick_count::now();); (64, TBB, tbb::flattened2d<ets_type> fvvs = flatten2d(vvs);); (59, TBB, tbb::parallel_reduce ( vvs.range(1), pvrb2 );); (-5, TBB, tbb::tick_count t0;); (30, TBB, tbb::flattened2d<ets_type> fvs = flatten2d(vs);); (50, TBB, ets_type vvs( typename container_type::size_type(25), minus_one, tbb::tbb_allocator<T>() );); (-6, TBB, typedef tbb::enumerable_thread_specific< container_type, Allocator<container_type> > ets_type;); (66, TBB, REMARK(\"done\\nparallel %s, %d, %g, %g\\n\", test_name, p, result_value, ( tbb::tick_count::now() - t0).seconds());)"}
{"predict": "(11,OpenMP,#pragma omp parallel for)", "label": "(1, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)"}
{"predict": "(15,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for)", "label": "(14, OpenMP, #pragma omp parallel for)"}
{"predict": "(15,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (29,OpenMP,#pragma omp parallel for)", "label": "(12, OpenCL, status = clEnqueueNDRangeKernel(pCmdQ, mockKernel.mockMultiDeviceKernel, testedWorkDim, nullptr, globalWorkSize, localWorkSize, 0, nullptr, nullptr);)"}
{"predict": "(19,OpenMP,#pragma omp parallel for)", "label": "(23, CUDA, CSC(cudaGetLastError(), STATUS_FAILURE);)"}
{"predict": "(4,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for)", "label": "(46, TBB, g.reset(tbb::flow::rf_reset_bodies);); (21, TBB, tbb::flow::make_edge( exe_node, *receivers[r] );); (7, TBB, tbb::flow::continue_node< OutputType > exe_node( g, cf );); (22, TBB, utils::NativeParallelFor( p, parallel_puts<tbb::flow::continue_msg>(exe_node) );); (36, TBB, body_copy = tbb::flow::copy_body< inc_functor<OutputType> >( exe_node );); (-1, TBB, tbb::flow::graph g;); (29, TBB, tbb::flow::remove_edge( exe_node, *receivers[r] );); (5, TBB, tbb::detail::d2::register_predecessor(exe_node, fake_sender);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for)\n(25,OpenMP,#pragma omp parallel for)\n(31,OpenMP,#pragma omp parallel for)", "label": "(6, MPI, MPI_Init(&argc, &argv);); (47, MPI, MPI_Finalize();); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (38, MPI, double t1 = MPI_Wtime();); (20, MPI, MPI_Barrier(MPI_COMM_WORLD);); (20, MPI, double t0 = MPI_Wtime();)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(1, TBB, tbb::task_arena a(1);); (13, TBB, CHECK(tbb::this_task_arena::max_concurrency() == 2);)"}
{"predict": "(6,OpenMP,#pragma omp parallel for)", "label": "(31, OpenMP, #pragma omp teams distribute parallel for proc_bind(master)); (25, OpenMP, #pragma omp teams distribute parallel for proc_bind(x)); (4, OpenMP, #pragma omp teams distribute parallel for proc_bind); (34, OpenMP, #pragma omp teams distribute parallel for proc_bind(spread)); (6, OpenMP, #pragma omp teams distribute parallel for proc_bind(); (9, OpenMP, #pragma omp teams distribute parallel for proc_bind()); (16, OpenMP, #pragma omp teams distribute parallel for proc_bind(close), proc_bind(spread)); (-2, OpenMP, #pragma omp target); (27, OpenMP, #pragma omp parallel proc_bind(close)); (9, OpenMP, #pragma omp teams distribute parallel for proc_bind(master)"}
{"predict": "(34,OpenMP,#pragma omp parallel for)", "label": "(42, CUDA, cudaFree(res_dev);); (35, CUDA, cudaMalloc(&res_dev, sizeof(float));); (37, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(18,OpenMP,#pragma omp parallel for collapse(2)), (37,OpenMP,#pragma omp parallel for collapse(2))", "label": "(11, CUDA, extern __shared__ __align__(sizeof(double)) unsigned char buf_[];)"}
{"predict": "(total_done_skipped_failed[coll_num][0] > 0) {\n150:                 num_all += total_done_skipped_failed[coll_num][0];\n151:                 num_skipped += total_done_skipped_failed[coll_num][2];\n152:                 num_done += total_done_skipped_failed[coll_num][1];\n153:                 num_failed += total_done_skipped_failed[coll_num][3];\n154:                 std::cout << std::setw(22) << std::left <<\n155:                     ucc_coll_type_names[coll_type] <<\n156:                     std::setw(10) << std::right <<\n157:                     total_done_skipped_failed[coll_num][0] <<\n158:                     std::setw(10) << std::right <<\n159:                     total_done_skipped_failed[coll_num][1] <<\n160:                     std::setw(10) << std::right <<\n161:                     total_done_skipped_failed[coll_num][3] <<\n162:                     std::setw(10) << std::right <<\n163:                     total_done_skipped_failed[coll_num][2] << std::endl;\n164:             }\n165:         }\n166:         std::cout << \"\\nTotal Tests: \" << num_all << \"\\n\" <<\n167:             \"Passed: \" << num_done << \"\\n\" <<\n168:             \"Failed: \" << num_failed << \"\\n\" <<\n169:             \"Skipped: \" << num_skipped << \"\\n\";\n170:         std::cout.copyfmt(iostate);\n171:     }\n172: \n173:     MPI_Wait(&req, MPI_STATUS_IGNORE);\n174:     test->destroy_teams();\n175:     delete test;\n176:     MPI_Finalize();\n177:     return failed;\n178: }", "label": "(21, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (193, MPI, MPI_Finalize();); (25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (23, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (124, MPI, MPI_Test(&req, &completed, MPI_STATUS_IGNORE);)"}
{"predict": "(1,MPI,MPI_Init(NULL, NULL);)", "label": "(40, MPI, mpi_errno = Mpi.Finalize();); (86, MPI, Mpi.CompleteFinalize();)"}
{"predict": "(26,MPI,MPI_Wtime();); (30,MPI,MPI_Wtime();); (34,MPI,MPI_Wtime();); (38,MPI,MPI_Wtime(););", "label": "(44, MPI, MPI_T_finalize();); (19, MPI, MPI_T_init_thread(required, &provided);)"}
{"predict": "2] = 0;\n108:                 }\n109:             }\n110:             if (romio_write_aggmethod == 1) {\n111:                 /* gather all data on the root */\n112:                 ADIOI_Gather_data(fd, buf, datatype, count, file_ptr_type, offset,\n113:                                  &gpfs_offsets0, count_sizes, nprocs, myrank, my_count_size,\n114:                                  &count_my_req_per_proc, &count_my_req_procs, &count_others_req_per_proc,\n115:                                  &count_others_req_procs, &my_req, &others_req);\n116:             }\n117:             else if (romio_write_aggmethod == 2) {\n118:                 /* allreduce all data on all processes */\n119:                 ADIOI_Allreduce_data(fd, buf, datatype, count, file_ptr_type, offset,\n120:                                      &gpfs_offsets0, count_sizes, nprocs, myrank, my_count_size,\n121:                                      &count_my_req_per_proc, &count_my_req_procs, &count_others_req_per_proc,\n122:                                      &count_others_req_procs, &my_req, &others_req);\n123:             }\n124:         }\n125:         else {\n126:             if ((romio_write_aggmethod == 1) || (romio_write_aggmethod == 2)) {\n127:                 gpfs_offsets0 = (ADIO_Offset *) ADIOI_Malloc(3 * nprocs * sizeof(ADIO_Offset));\n128:                 gpfs_offsets = gpfs_offsets0 + nprocs;\n129:                 for (ii = 0; ii < nprocs; ii++) {\n130:                     gpfs_offsets0[ii * 3] = 0;\n131:                     gpfs_offsets0[ii * 3 + 1] = 0;\n132:                     gpfs_offsets0[ii * 3 + 2] = 0;\n133:                 }\n134:             }\n135:             if (romio_write_aggmethod == 1) {\n136:                 /* gather all data on the root */\n137:                 ADIOI_Gather_data(fd, buf, datatype, count, file", "label": "(141, MPI, MPI_Allgather(&end_offset, 1, ADIO_OFFSET, end_offsets, 1, ADIO_OFFSET, fd->comm);); (59, MPI, MPI_Comm_rank(fd->comm, &myrank);); (138, MPI, MPI_Allgather(&start_offset, 1, ADIO_OFFSET, st_offsets, 1, ADIO_OFFSET, fd->comm);); (395, MPI, MPI_Type_size_x(datatype, &size);); (94, MPI, MPI_Type_size_x(datatype, &buftype_size);)"}
{"predict": "(29,OpenMP,#pragma omp parallel for), (57,OpenMP,#pragma omp parallel for)", "label": "(41, OpenMP, lis2mdl_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)"}
{"predict": "(37,MPI, *w = MPI_Wtime();)", "label": "(67, OpenMP, kmp_gomp_depends_info_t gomp_depends(depend);); (100, OpenMP, __kmpc_omp_wait_deps(&loc, gtid, ndeps, dep_list, 0, NULL);); (74, OpenMP, __kmpc_omp_task(&loc, gtid, task);); (71, OpenMP, __kmpc_omp_task_with_deps(&loc, gtid, task, ndeps_cnv, dep_list, 0, NULL);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(10, MPI, MPI_Comm c_comm = MPI_Comm_f2c(*comm);)"}
{"predict": "_m; j--)\n119:                 sum2 = Mhu_a[j] + x*sum2;\n120: \n121:             m_gate[ii] = sum1 - sum2;\n122:         }\n123:         /*\n124:          * RUSH LARSEN KERNEL BEING TIMED END\n125:          */\n126:             \n127:         /* Start timer for kernel run time after 1st iteration */\n128:         if (itime==1)\n129:         {\n130:             kernel_endtime=secs_elapsed();\n131:         }\n132:     }\n133:     \n134:     /* Print summary */\n135:     rank0_printf_timestamp(\"Finished %ld iterations, %ld cells, %s\\n\", iterations, nCells, VARIANT_DESC);\n136:     rank0_printf_timestamp(\"  %s\\n\", VERSION_STRING);\n137:     rank0_printf_timestamp(\"  Kernel runtime: %f\\n\", kernel_runtime);\n138:     rank0_printf_timestamp(\"  Transfer runtime: %f\\n\", transfer_runtime);\n139:     rank0_printf_timestamp(\"  Average kernel runtime: %f\\n\", kernel_runtime/(iterations));\n140:     rank0_printf_timestamp(\"  Average transfer runtime: %f\\n\", transfer_runtime/(iterations));\n141:     \n142:     /* Free memory */\n143:     free(m_gate);\n144:     free(Vm);\n145:     \n146:     return 0;\n147: }", "label": "(162, OpenMP, #pragma omp target exit data map(delete:Vm[:nCells])); (66, OpenMP, #pragma omp target enter data map(to: m_gate[:nCells])); (149, OpenMP, #pragma omp target update from (m_gate[0:nCells])); (65, OpenMP, #pragma omp target enter data map(to: Vm[:nCells])); (102, OpenMP, #pragma omp target teams distribute parallel for); (156, OpenMP, #pragma omp target exit data map(delete:m_gate[:nCells]))"}
{"predict": "(6,MPI,MPI_Init(&argc, &argv);); (14,MPI,MPI_Finalize();); (7,OpenMP,#pragma omp parallel for private(i, flag, val_p)), (11,OpenMP,#pragma omp critical)", "label": "(6, MPI, MPI_Type_get_attr(type, key[i], &val_p, &flag);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(7, OpenMP, #pragma omp for nowait schedule(runtime))"}
{"predict": "(11,MPI,MPI_Init(&argc, &argv);) (42,MPI,MPI_Finalize();) (12,OpenMP,#pragma omp parallel for) (17,OpenMP,#pragma omp parallel for) (22,OpenMP,#pragma omp parallel for) (27,OpenMP,#pragma omp parallel for) (32,OpenMP,#pragma omp parallel for) (37,OpenMP,#pragma omp parallel for)", "label": "(9, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for collapse(2)), (15,OpenMP,#pragma omp atomic), (24,OpenMP,#pragma omp atomic)", "label": "(6, CUDA, extern __shared__ float s_data[];)"}
{"predict": "(9,MPI,MPI_Init(&argc, &argv);)\n(31,MPI,MPI_Finalize();)\n(10,OpenMP,#pragma omp parallel for)\n(72,OpenMP,#pragma omp parallel for)", "label": "(10, MPI, MPI_Info_create(&infos[i]);); (57, MPI, MPI_Info_get(infos[i], key, sizeof(value), value, &flag);); (47, MPI, MPI_Info_get_nthkey(infos[i], j, key);); (22, MPI, MPI_Info_get_nkeys(infos[0], &nkeys);); (33, MPI, MPI_Info_get_nkeys(infos[i], &nkeys);); (71, MPI, MPI_Info_free(&infos[i]);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for private(index, ctx))", "label": "(5, OpenMP, #pragma omp parallel for)"}
{"predict": "(10,MPI,MPI_Request req;), (11,MPI,MPI_Ireduce_scatter_block(sendbuf, recvbuf, recvcounts, datatype, MPI_SUM, MPI_COMM_WORLD, &req);), (14,MPI,MPI_Wait(&req, MPI_STATUS_IGNORE);)", "label": "(14, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (12, MPI, MPI_Ireduce_scatter_block_c(sendbuf, recvbuf, count, datatype, MPI_SUM, comm, &req);); (8, MPI, MPI_Reduce_scatter_block_c(sendbuf, recvbuf, count, datatype, MPI_SUM, comm);)"}
{"predict": ";\n136:   checkCudaErrors(cufftXtMalloc(plan_input, (cudaLibXtDesc **)&d_out_signal,\n137:                                 CUFFT_XT_FORMAT_INPLACE));\n138:   cudaLibXtDesc *d_filter_kernel;\n139:   checkCudaErrors(cufftXtMalloc(plan_input, (cudaLibXtDesc **)&d_filter_kernel,\n140:                                 CUFFT_XT_FORMAT_INPLACE));\n141: \n142:   // Copy data from host to device\n143:   checkCudaErrors(cufftXtMemcpy(plan_input, d_signal, h_padded_signal,\n144:                                 CUFFT_XT_MEMTYPE_HOST, CUFFT_XT_MEMTYPE_DEVICE));\n145:   checkCudaErrors(cufftXtMemcpy(plan_input, d_filter_kernel, h_padded_filter_kernel,\n146:                                 CUFFT_XT_MEMTYPE_HOST, CUFFT_XT_MEMTYPE_DEVICE));\n147: \n148:   // Call the multiplyCoefficient kernel\n149:   multiplyCoefficient(d_signal, d_filter_kernel, new_size, 1.0, 0);\n150: \n151:   // Copy data back to host\n152:   checkCudaErrors(cufftXtMemcpy(plan_input, h_padded_signal, d_signal,\n153:                                 CUFFT_XT_MEMTYPE_DEVICE, CUFFT_XT_MEMTYPE_HOST));\n154: \n155:   // Clean up\n156:   cufftDestroy(plan_input);\n157:   free(h_signal);\n158:   free(h_filter_kernel);\n159:   free(h_padded_signal);\n160:   free(h_padded_filter_kernel);\n161:   free(major_minor);\n162:   free(whichGPUs);\n163:   free(worksize);\n164: \n165:   return 0;\n166: }\n167: \n168: // Kernel for GPU\n169: void multiplyCoefficient(cudaLibXtDesc *d_signal, cudaLibXtDesc *d_filter_kernel,\n170:                          int new_size, float scale, int index) {\n171:   Complex *h_signal;\n172", "label": "(29, CUDA, checkCudaErrors(cudaGetDeviceCount(&GPU_N));); (219, CUDA, checkCudaErrors(cufftXtFree(d_out_filter_kernel));); (120, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, whichGPUs[i]));); (216, CUDA, checkCudaErrors(cufftXtFree(d_out_signal));); (100, CUDA, checkCudaErrors(cufftCreate(&plan_input));); (40, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, i));); (211, CUDA, checkCudaErrors(cufftXtFree(d_signal));); (216, CUDA, checkCudaErrors(cufftDestroy(plan_input));); (210, CUDA, checkCudaErrors(cufftXtFree(d_filter_kernel));)"}
{"predict": "(15,MPI,MPI_Init(&argc, &argv);) (14,OpenMP,#pragma omp parallel for)", "label": "(56, MPI, MPI_Type_get_extent(MPI_INT, &tmp_lb, &extent);); (33, MPI, MPI_Comm_size(comm, &size);); (55, MPI, MPI_Win_create(recvBuf, totsize * extent, extent, MPI_INFO_NULL, comm, &win);); (24, MPI, MPI_Type_indexed(2, counts, displs, MPI_INT, &originType);); (54, MPI, MPI_Win_fence(0, win);); (23, MPI, MPI_Type_commit(&originType);); (77, MPI, MPI_Win_free(&win);); (75, MPI, MPI_Type_free(&originType);); (58, MPI, err = MPI_Win_fence(0, win);); (53, MPI, MPI_Win_set_errhandler(win, MPI_ERRORS_RETURN);); (33, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (22, MPI, MPI_Comm_rank(comm, &rank);); (52, MPI, err = MPI_Put(srcBuf, 1, originType, dest, 0, totsize, MPI_INT, win);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for)", "label": "(8, OpenMP, #pragma omp task private(); (17, OpenMP, #pragma omp task private(ca)); (20, OpenMP, #pragma omp task private(threadvar, B::x)); (10, OpenMP, #pragma omp task private(argc argv)); (17, OpenMP, #pragma omp task private(e, g)); (31, OpenMP, #pragma omp task private(m)); (1, OpenMP, #pragma omp task private); (9, OpenMP, #pragma omp task private(argv[1])); (1, OpenMP, #pragma omp task private()); (19, OpenMP, #pragma omp task private(j)); (4, OpenMP, #pragma omp task private(S1)); (0, OpenMP, #pragma omp task private(argc,); (5, OpenMP, #pragma omp task private(ba) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (7, OpenMP, #pragma omp task private(S2::S2s)); (5, OpenMP, #pragma omp task private(da)); (-3, OpenMP, #pragma omp task private(argc > 0 ? argv[1] : argv[2])); (11, OpenMP, #pragma omp task private(i)); (6, OpenMP, #pragma omp task shared(i), private(i)); (14, OpenMP, #pragma omp task private(i) allocate(omp_thread_mem_alloc: i)); (6, OpenMP, #pragma omp task firstprivate(i) private(i)); (10, OpenMP, #pragma omp task firstprivate(i)); (-11, OpenMP, #pragma omp task private(argc); (-7, OpenMP, #pragma omp task private(z, a, b, c, d, f))"}
{"predict": "(14,OpenMP,#pragma omp parallel for)", "label": "(25, OpenMP, omp_target_free(d_a, omp_get_default_device());); (12, OpenMP, d_a = (int *)omp_target_alloc(DataSize, omp_get_default_device());); (18, OpenMP, #pragma omp target update to(p_a)); (20, OpenMP, omp_target_disassociate_ptr(p_a, omp_get_default_device());); (11, OpenMP, omp_target_associate_ptr(p_a, d_a, DataSize, 0, omp_get_default_device());)"}
{"predict": "(35,MPI,MPI_Type_vector(2, 1, disp, MPI_INT, &vectype);), (36,MPI,MPI_Type_commit(&vectype);), (39,MPI,MPI_Type_indexed(count, disp, int_size, MPI_INT, &newtype);), (40,MPI,MPI_Type_commit(&newtype);), (43,MPI,MPI_Type_size(newtype, &size);), (48,MPI,MPI_Type_size(MPI_INT, &int_size);), (50,MPI,MPI_Type_free(&vectype);), (51,MPI,MPI_Type_free(&newtype);), (58,OpenMP,#pragma omp parallel for)", "label": "(35, MPI, err = MPI_Type_create_indexed_block(count, 1, disp, vectype, &newtype);); (42, MPI, MPI_Type_size(MPI_INT, &int_size);); (58, MPI, MPI_Type_commit(&newtype);); (24, MPI, err = MPI_Type_vector(2, 1, 2, MPI_INT, &vectype);); (41, MPI, err = MPI_Type_size(newtype, &size);); (74, MPI, MPI_Type_free(&newtype);); (72, MPI, MPI_Type_free(&vectype);)"}
{"predict": "(28,OpenMP,#pragma omp parallel for), (42,OpenMP,#pragma omp parallel for)", "label": "(44, OpenCL, clReleaseEvent(retEvent);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for), (43,OpenMP,#pragma omp parallel for)", "label": "(24, CUDA, ASSERT_EQ(cudaEventRecord(wm.ready, 0), cudaSuccess);); (25, CUDA, cvcuda::WorkspaceMemAllocator wa(wm, cudaStream_t(0));); (51, CUDA, EXPECT_EQ(cudaEventQuery(wm.ready), cudaSuccess);); (1, CUDA, ASSERT_EQ(cudaMalloc(&_junk, junk_size), cudaSuccess);); (14, CUDA, ASSERT_EQ(cudaMemset(junk.get(), i, junk_size), cudaSuccess);); (6, CUDA, ASSERT_EQ(cudaEventCreateWithFlags(&wm.ready, cudaEventDisableTiming), cudaSuccess);); (26, CUDA, ASSERT_EQ(cudaDeviceSynchronize(), cudaSuccess);); (50, CUDA, ASSERT_EQ(cudaEventDestroy(wm.ready), cudaSuccess);); (19, CUDA, EXPECT_EQ(cudaEventQuery(wm.ready), cudaErrorNotReady);)"}
{"predict": "(73,OpenMP,#pragma omp parallel for)", "label": "(65, CUDA, CHECK_CUDA(cudaMalloc(&dy, dataSize * sizeof(int32_t)));); (100, CUDA, CHECK_CUDA(cudaFree(dUpdates));); (62, CUDA, CHECK_CUDA(cudaMalloc(&dUpdates, updatesSize * sizeof(int32_t)));); (94, CUDA, CHECK_HIEDNN(hiednnDestroyCudaHandle(handle));); (37, CUDA, CHECK_HIEDNN(hiednnCreateCudaHandle(&handle));); (97, CUDA, CHECK_CUDA(cudaFree(dy));); (56, CUDA, CHECK_CUDA(cudaMalloc(&dx, dataSize * sizeof(int32_t)));); (93, CUDA, CHECK_CUDA(cudaFree(dIndices));); (36, CUDA, CHECK_HIEDNN(hiednnSetCudaStream(handle, 0));); (90, CUDA, CHECK_CUDA(cudaFree(dx));); (53, CUDA, CHECK_CUDA(cudaMalloc(&dIndices, indicesSize * sizeof(int32_t)));)"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for)", "label": "(28, TBB, tbb::flow::input_node<T> src2(g, my_input_body<T>() );); (18, TBB, tbb::flow::function_node<T,bool> dest3(g, tbb::flow::unlimited, b3 );); (31, TBB, tbb::flow::function_node<T,bool,tbb::flow::rejecting> dest2(g, tbb::flow::serial, b2 );); (3, TBB, tbb::flow::make_edge( src, dest );); (0, TBB, tbb::flow::input_node<T> src(g, my_input_body<T>() );); (29, TBB, tbb::flow::make_edge( src2, dest2 );); (36, TBB, tbb::flow::input_node<T> src_copy(src);); (13, TBB, tbb::flow::make_edge( src3, dest3 );); (-5, TBB, tbb::flow::graph g;); (6, TBB, tbb::flow::input_node<T> src3(g, my_input_body<T>() );)"}
{"predict": "(20,OpenMP,#pragma omp parallel for)", "label": "(26, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&output_data, output_size_bytes));); (18, CUDA, CUDA_CHECK_AND_EXIT(cudaMallocManaged(&input_data, input_size_bytes));); (48, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(output_data));); (39, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (37, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (44, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(input_data));)"}
{"predict": "for(int i=0; i<2; ++i){\n113:             //status=0;\n114:             //cudaStreamCreate(&stream[i]);\n115:             status = cudaLaunchKernel<<<dimGrid_bc, dimBlock_bc, 0, stream[i]>>>(nvshmem_kernel, args);\n116:             if(status!=0){\n117:                 printf(\"Kernel launch failed for stream %d\\n\", i);\n118:             }\n119:         }\n120:     }\n121: \n122:     checkGPU(gpuGetLastError());\n123: \n124: }\n125: \n126: void slsum_fmod_inv_gpu_mrhs(int nbcol_loc, int nblock_ex, float *lsum, float *x, int nrhs, int maxsup, int nsupers, int *fmod, C_Tree *LBtree_ptr, C_Tree *LRtree_ptr, int_t *ilsum, float *Lrowind_bc_dat, long int *Lrowind_bc_offset, float *Lnzval_bc_dat, long int *Lnzval_bc_offset, float *Linv_bc_dat, long int *Linv_bc_offset, float *Lindval_loc_bc_dat, long int *Lindval_loc_bc_offset, int_t *xsup, int * bcols_masked, gridinfo_t *grid) {\n127: \n128:     /* Code not shown for brevity */\n129: \n130: }\n131: \n132: void slsum_fmod_inv_gpu_1rhs_warp(int nbcol_loc, int nblock_ex, float *lsum, float *x, int nrhs, int maxsup, int nsupers, int *fmod, C_Tree *LBtree_ptr, C_Tree *LRtree_ptr, int_t *ilsum, float *Lrowind_bc_dat, long int *Lrowind_bc_offset, float *Lnzval_bc_dat, long int *Lnzval_bc_offset, float *Linv_bc_dat, long int *Linv_bc_offset, float *Lindval_loc_bc_dat, long int *Lindval_loc_bc_offset, int_t *xsup, int * bcols_masked, gridinfo_t *grid) {\n133: \n134:     /* Code", "label": "(85, CUDA, cudaDeviceSetLimit(cudaLimitStackSize, cuattr.localSizeBytes);); (140, CUDA, CUDA_CHECK(cudaDeviceSynchronize());); (78, CUDA, cudaStreamCreateWithFlags(&stream[i], cudaStreamNonBlocking);); (135, CUDA, CUDA_CHECK(cudaGetLastError());); (80, CUDA, cudaFuncGetAttributes(&cuattr, slsum_fmod_inv_gpu_mrhs_nvshmem);); (138, CUDA, CUDA_CHECK(cudaStreamDestroy(stream[i]));); (82, CUDA, cudaOccupancyMaxPotentialBlockSize(&minGridSize,&myblockSize,(const void *) swait_bcrd ,0,0 );)"}
{"predict": "(1,OpenMP,#pragma omp parallel for private(offset, pos, bit_mask, thread_mask, active, ones, zeroes))", "label": "(2, CUDA, __shared__ volatile unsigned int s_data[WARP_SIZE*2];)"}
{"predict": "(31,OpenMP,#pragma omp parallel for private(i))", "label": "(29, OpenCL, error |= clSetKernelArg(kernel, 1, sizeof(value), &value);); (27, OpenCL, error |= clSetKernelArg(kernel, 0, sizeof(dst), &dst);)"}
{"predict": ".action([](parser_t&) { use_papi() = false; });\n152:     parser.add_argument({ \"--disable-network\" }, \"Disable network stats\")\n153:         .count(0)\n154:         .action([](parser_t&) { use_network() = false; });\n155:     parser.add_argument({ \"--disable-cuda\" }, \"Disable CUDA stats\")\n156:         .count(0)\n157:         .action([](parser_t&) { use_cuda() = false; });\n158:     parser.add_argument({ \"--disable-cuda-events\" }, \"Disable CUDA events\")\n159:         .count(0)\n160:         .action([](parser_t&) { use_cuda_events() = false; });\n161:     parser.add_argument({ \"--disable-timemory\", \"--disable-timemory-tracking\" },\n162:                         \"Disable timemory tracking\")\n163:         .count(0)\n164:         .action([](parser_t&) { use_timemory() = false; });\n165:     parser.add_argument({ \"--disable-timemory-events\" },\n166:                         \"Disable timemory events\")\n167:         .count(0)\n168:         .action([](parser_t&) { use_timemory_events() = false; });\n169:     parser.add_argument({ \"--disable-timemory-network\" },\n170:                         \"Disable timemory network stats\")\n171:         .count(0)\n172:         .action([](parser_t&) { use_timemory_network() = false; });\n173:     parser.add_argument({ \"--disable-timemory-cuda\" },\n174:                         \"Disable timemory CUDA stats\")\n175:         .count(0)\n176:         .action([](parser_t&) { use_timemory_cuda() = false; });\n177:     parser.add_argument({ \"--disable-timemory-cuda-events\" },\n178:                         \"Disable timemory CUDA events\")\n179:         .count(0)\n180:         .action([](parser_t&) { use_timemory_cuda_events() = false; });\n181:     parser.add_argument({ \"--disable-timemory-timemory\" },\n182:                         \"Disable timemory timemory tracking\")", "label": "(8, MPI, tim::settings::mpi_init()       = false;); (8, MPI, tim::settings::mpi_finalize()   = false;)"}
{"predict": "1;\n150:   }\n151: \n152:   return 0;\n153: }", "label": "(19, OpenMP, #pragma omp parallel); (0, OpenMP, int nThreads = omp_get_max_threads();)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(4, TBB, using MyVector = oneapi::tbb::concurrent_vector<move_support_tests::Foo, MyAllocator>;); (8, TBB, oneapi::tbb::parallel_for(oneapi::tbb::blocked_range<std::size_t>(0, 10000*s, s), GrowToAtLeast<MyVector>(use_two_arg_form, v, copy_from), oneapi::tbb::simple_partitioner());)"}
{"predict": "-2;\n120:     MPI_Group_range_excl(g1, 1, range, &g6);\n121: \n122:     if (result != MPI_UNEQUAL) {\n123:         errs++;\n124:         fprintf(stderr, \"Comparison with empty group gave %d, not 3\\n\", result);\n125:     }\n126: \n127:     /* Now, duplicate the test, but using negative strides */\n128:     range[0][0] = size - 1;\n129:     range[0][1] = 1;\n130:     range[0][2] = -2;\n131:     MPI_Group_range_excl(g1, 1, range, &g5);\n132: \n133:     range[0][0] = size - 1;\n134:     range[0][1] = 1;\n135:     range[0][2] = -2;\n136:     MPI_Group_range_excl(g1, 1, range, &g6);\n137: \n138:     if (result != MPI_UNEQUAL) {\n139:         errs++;\n140:         fprintf(stderr, \"Comparison with empty group gave %d, not 3\\n\", result);\n141:     }\n142: \n143:     /* Now, duplicate the test, but using negative strides */\n144:     range[0][0] = size - 1;\n145:     range[0][1] = 1;\n146:     range[0][2] = -2;\n147:     MPI_Group_range_excl(g1, 1, range, &g5);\n148: \n149:     range[0][0] = size - 1;\n150:     range[0][1] = 1;\n151:     range[0][2] = -2;\n152:     MPI_Group_range_excl(g1, 1, range, &g6);\n153: \n154:     if (result != MPI_UNEQUAL) {\n155:         errs++;\n156:         fprintf(stderr, \"Comparison with empty group gave %d, not", "label": "(84, MPI, MPI_Group_excl(g1, g1size, lranks, &g6);); (27, MPI, MPI_Group_size(g2, &size);); (41, MPI, MPI_Group_translate_ranks(g2, 3, rin, g1, rout);); (70, MPI, MPI_Group_free(&selfgroup);); (105, MPI, MPI_Group_free(&g2);); (24, MPI, MPI_Group_rank(g2, &rank);); (117, MPI, MPI_Group_compare(MPI_GROUP_EMPTY, g4, &result);); (123, MPI, MPI_Group_free(&g45);); (113, MPI, MPI_Group_union(g4, g5, &g45);); (105, MPI, MPI_Group_range_excl(g1, 1, range, &g5);); (109, MPI, MPI_Group_range_incl(g1, 1, range, &g4);); (118, MPI, MPI_Group_free(&g5);); (-1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (12, MPI, MPI_Group_incl(g1, 3, ranks, &g2);); (114, MPI, MPI_Group_free(&g4);); (140, MPI, MPI_Group_free(&g1);); (72, MPI, MPI_Group_free(&g6);); (62, MPI, MPI_Group_size(g1, &g1size);); (34, MPI, MPI_Comm_group(MPI_COMM_SELF, &selfgroup);); (-10, MPI, MPI_Comm_group(MPI_COMM_WORLD, &g1);); (-10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &myrank);); (41, MPI, MPI_Group_translate_ranks(g1, size, rin, selfgroup, rout);); (32, MPI, MPI_Group_translate_ranks(selfgroup, 1, rin, g1, rout);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for)", "label": "(1, MPI, MPI_Init(&argc, &argv);); (58, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &P);)"}
{"predict": "(1, MPI, #include <mpi.h>)\n(5, MPI, *w = MPI_Wtime();)\n(8, OpenMP, #pragma omp parallel for collapse(2))", "label": "(7, CUDA, extern __shared__ __align__(sizeof(float)) uchar s[];)"}
{"predict": "(15,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for)", "label": "(5, OpenMP, #pragma omp barrier); (23, OpenMP, #pragma omp task); (18, OpenMP, #pragma omp for); (0, OpenMP, #pragma omp cancel parallel if(flag)); (4, OpenMP, #pragma omp sections); (7, OpenMP, #pragma omp section); (22, OpenMP, #pragma omp parallel sections); (-6, OpenMP, #pragma omp parallel); (18, OpenMP, #pragma omp cancel taskgroup); (32, OpenMP, #pragma omp parallel for reduction(+: r)); (0, OpenMP, #pragma omp cancel sections); (11, OpenMP, #pragma omp cancel for)"}
{"predict": "(22,OpenMP,#pragma omp parallel for)", "label": "(31, OpenCL, clReleaseMemObject(buffer);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for reduction(+:counter.value))", "label": "(5, TBB, tbb::profiling::set_name(counter.mutex, name);); (11, TBB, tbb::parallel_for(tbb::blocked_range<size_t>(0,n,n/10),AddOne<Counter<M> >(counter));); (11, TBB, tbb::tick_count t1 = tbb::tick_count::now();)"}
{"predict": "/* Print statistics. */\n155:     if (!iam) {\n156:         printf(\"Execution time: %f seconds\\n\", PStatGetTime(&stat, SuperLU_timer));\n157:         fflush(stdout);\n158:     }\n159: \n160:     /* Free storage. */\n161:     dScalePermstructFree(&ScalePermstruct);\n162:     dLUstructFree(&LUstruct);\n163:     dSOLVEstructFree(&SOLVEstruct);\n164:     Destroy_SuperMatrix_Store(&A);\n165:     Destroy_SuperMatrix_Store(&LUstruct.L);\n166:     Destroy_SuperMatrix_Store(&LUstruct.U);\n167:     Destroy_SuperMatrix_Store(&LUstruct.LT);\n168:     Destroy_SuperMatrix_Store(&LUstruct.UT);\n169:     Destroy_SuperMatrix_Store(&LUstruct.UC);\n170:     Destroy_SuperMatrix_Store(&LUstruct.P);\n171:     Destroy_SuperMatrix_Store(&LUstruct.Pinv);\n172:     Destroy_SuperMatrix_Store(&LUstruct.Lt);\n173:     Destroy_SuperMatrix_Store(&LUstruct.Ut);\n174:     Destroy_SuperMatrix_Store(&LUstruct.Uc);\n175:     Destroy_SuperMatrix_Store(&LUstruct.Pt);\n176:     Destroy_SuperMatrix_Store(&LUstruct.Pinvt);\n177:     Destroy_SuperMatrix_Store(&LUstruct.Lt_c);\n178:     Destroy_SuperMatrix_Store(&LUstruct.Ut_c);\n179:     Destroy_SuperMatrix_Store(&LUstruct.Uc_c);\n180:     Destroy_SuperMatrix_Store(&LUstruct.Pt_c);\n181:     Destroy_SuperMatrix_Store(&LUstruct.Pinvt_c);\n182:     Destroy_SuperMatrix_Store(&LUstruct.Lt_c_c);\n183:     Destroy_SuperMatrix_Store(&LUstruct.Ut_c_c);\n184:     Destroy_SuperMatrix_Store(&LUstruct.Uc_c_c);\n185:     Destroy_SuperMatrix_Store(&LUstruct.Pt_c_c);\n186:     Destroy_SuperMatrix_Store(&LUstruct.Pinvt_c_c);\n187:     Destroy_SuperMatrix_Store(&LUstruct.Lt_c_c_c);", "label": "(232, MPI, MPI_Finalize();); (35, MPI, MPI_Init_thread( &argc, &argv, MPI_THREAD_MULTIPLE, &omp_mpi_level);)"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for), (42,OpenMP,#pragma omp parallel for), (44,OpenMP,#pragma omp parallel for), (47,OpenMP,#pragma omp parallel for)", "label": "(34, OpenMP, #pragma omp teams distribute parallel for simd linear (argc : 5) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (19, OpenMP, #pragma omp teams distribute parallel for simd linear (argc); (27, OpenMP, #pragma omp teams distribute parallel for simd linear (argc > 0 ? argv[1] : argv[2])); (43, OpenMP, #pragma omp teams distribute parallel for simd linear (argv[1])); (38, OpenMP, #pragma omp teams distribute parallel for simd linear (a, b:B::ib)); (55, OpenMP, #pragma omp teams distribute parallel for simd linear(i)); (19, OpenMP, #pragma omp teams distribute parallel for simd linear (argc,); (8, OpenMP, #pragma omp teams distribute parallel for simd linear ()); (30, OpenMP, #pragma omp teams distribute parallel for simd linear (S1)); (-2, OpenMP, #pragma omp teams distribute parallel for simd linear); (41, OpenMP, #pragma omp teams distribute parallel for simd linear(e, g)); (-5, OpenMP, #pragma omp target); (-1, OpenMP, #pragma omp teams distribute parallel for simd linear (); (42, OpenMP, #pragma omp teams distribute parallel for simd linear(h))"}
{"predict": "129:   MPI_Bcast(&iterations, 1, MPI_INT, root, MPI_COMM_WORLD);\n130:   MPI_Bcast(&size2, 1, MPI_INT64_T, root, MPI_COMM_WORLD);\n131:   MPI_Bcast(&radius, 1, MPI_INT, root, MPI_COMM_WORLD);\n132:   MPI_Bcast(&sparsity, 1, MPI_DOUBLE, root, MPI_COMM_WORLD);\n133: \n134:   /* Allocate space for sparse matrix, vector and result */\n135:   vector_space = size2 * sizeof(double);\n136:   matrix_space = size2 * sizeof(double);\n137:   index_space = size2 * sizeof(s64Int);\n138:   matrix = (double *)malloc(matrix_space);\n139:   vector = (double *)malloc(vector_space);\n140:   result = (double *)malloc(result_space);\n141:   colIndex = (s64Int *)malloc(index_space);\n142: \n143:   /* initialize matrix and vector */\n144:   init_sparse_matrix(matrix, colIndex, size2, radius, sparsity);\n145:   init_vector(vector, size2);\n146: \n147:   /* initialize result */\n148:   for (i=0; i<size2; i++){\n149:     result[i] = 0.0;\n150:   }\n151: \n152:   /* Compute matrix-vector product */\n153:   for (iter=0; iter<iterations; iter++){\n154:     /* compute matrix-vector product */\n155:     for (i=0; i<size2; i++){\n156:       result[i] = 0.0;\n157:       for (j=0; j<size2; j++){\n158:         if (matrix[i*size2+j] != 0.0){\n159:           result[i] += matrix[i*size2+j] * vector[j];\n160:         }\n161:       }\n162:     }\n163:   }\n164: \n165:   /* Compute the sum of the result vector */\n166:   vector_sum = 0.0;\n167", "label": "(134, MPI, MPI_Bcast(&size2,      1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (134, MPI, MPI_Bcast(&radius,     1, MPI_INT,           root, MPI_COMM_WORLD);); (316, MPI, MPI_Finalize();); (129, MPI, MPI_Bcast(&lsize2,     1, MPI_INT,           root, MPI_COMM_WORLD);); (127, MPI, MPI_Bcast(&lsize,      1, MPI_INT,           root, MPI_COMM_WORLD);); (128, MPI, MPI_Bcast(&size,       1, MPI_LONG_LONG_INT, root, MPI_COMM_WORLD);); (290, MPI, MPI_Reduce(&vector_sum, &check_sum, 1, MPI_DOUBLE, MPI_SUM, root, MPI_COMM_WORLD);); (41, MPI, MPI_Init(&argc,&argv);); (243, MPI, MPI_Barrier(MPI_COMM_WORLD);); (41, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (126, MPI, MPI_Bcast(&iterations, 1, MPI_INT,           root, MPI_COMM_WORLD);); (38, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(15,OpenMP,#pragma omp parallel for)", "label": "(18, OpenMP, #pragma omp parallel for); (14, OpenMP, #pragma omp target map(tofrom: Inner, Outer, wayout)); (14, OpenMP, #pragma omp teams num_teams(1  ) thread_limit(tl))"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (11,OpenMP,tmpV = Real_t(1.0);)", "label": "(6, OpenMP, #pragma omp parallel for firstprivate(length, v_cut))"}
{"predict": "actions[0][\"scenes/s2/image_prefix\"] = u_output_file;\n162: \n163:     // execute the actions\n164:     ascent::Ascent a;\n165:     a.open(actions);\n166:     a.execute(actions);\n167:     a.close();\n168: \n169:     // clean up outputs\n170:     if(par_rank == 0)\n171:     {\n172:         remove_test_image(mesh_output_file);\n173:     }\n174:     else\n175:     {\n176:         remove_test_image(u_output_file);\n177:     }\n178: }", "label": "(166, MPI, opts[\"mpi_comm\"] = MPI_Comm_c2f(MPI_COMM_WORLD);); (171, MPI, MPI_Barrier(comm);); (21, MPI, MPI_Comm_rank(comm, &par_rank);); (21, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(27,OpenMP,#pragma omp parallel for private(i)), (27,OpenMP,#pragma omp critical), (32,OpenMP,#pragma omp critical)", "label": "(25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (20, MPI, MPI_Init_thread(&argc, &argv, required, &provided);); (51, MPI, MPI_Finalize();); (48, MPI, MPI_T_finalize();); (19, MPI, MPI_T_init_thread(required, &provided);); (32, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "(101,OpenMP,#pragma omp parallel for)", "label": "(101, CUDA, checkHIPErrors( ::cudaDeviceSynchronize() );)"}
{"predict": "(23,OpenMP,#pragma omp parallel for)", "label": "(32, OpenMP, #pragma omp parallel if(Gc>0.0 && c>0 && Sc>0.0)); (22, OpenMP, #pragma omp target if(Ga>0.0 && a>0 && Sa>0.0))"}
{"predict": "(35,OpenMP,#pragma omp parallel for), (54,OpenMP,#pragma omp parallel for)", "label": "(7, CUDA, __shared__ block_radix_sort_t::TempStorage temp_storage;)"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(18, OpenMP, #pragma omp target teams distribute if(cond) firstprivate(fp) reduction(+:rd) map(alloc:map))"}
{"predict": "(27,OpenMP,#pragma omp parallel for)", "label": "(14, CUDA, thrust::host_vector<uint32_t> h_increments(num_increments);); (43, CUDA, thrust::device_vector<uint32_t> increments_in(num_increments);); (53, CUDA, thrust::fill(counts_out.begin(), counts_out.end(), 814920U);); (40, CUDA, thrust::device_vector<uint32_t> bins_in(num_increments);); (41, CUDA, thrust::device_vector<uint32_t> counts_out(NUM_ITEMS);); (47, CUDA, thrust::host_vector<uint32_t> host_counts(num_increments);); (7, CUDA, thrust::host_vector<uint32_t> h_bins(num_increments);)"}
{"predict": "(15,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for)", "label": "(21, OpenMP, #pragma omp target update from(a[:n])); (12, OpenMP, #pragma omp target teams distribute parallel for map(to:b[:n])); (15, OpenMP, #pragma omp target teams distribute parallel for)"}
{"predict": "(19,OpenMP,#pragma omp parallel for)", "label": "(6, CUDA, gpuErrchk(cudaPointerGetAttributes(&attributes, d_int));); (28, CUDA, gpuErrchk(cudaFree(d_int));); (1, CUDA, gpuErrchk(cudaMalloc(&d_int, sizeof(int)));)"}
{"predict": "(12,OpenMP,#pragma omp parallel for private(index,cell,cellImagePos,isContained,color)), (18,OpenMP,#pragma omp single), (27,OpenMP,#pragma omp single)", "label": "(4, CUDA, __shared__ float3 color;); (2, CUDA, __shared__ bool isContained;); (0, CUDA, __shared__ float2 cellImagePos;)"}
{"predict": "(13,OpenMP,#pragma omp parallel for)", "label": "(12, CUDA, cudaMemset(d_sums, 0, sizeof(Type) * nkeys * ncols);)"}
{"predict": "(13,MPI,*w = MPI_Wtime();); (18,OpenMP,#pragma omp parallel for), (29,OpenMP,#pragma omp parallel for), (49,OpenMP,#pragma omp parallel for), (61,OpenMP,#pragma omp parallel for)", "label": "(11, OpenMP, omp_set_num_threads(nr_threads);); (12, OpenMP, #pragma omp parallel default(none) shared(a)); (13, OpenMP, #pragma omp sections); (7, OpenMP, omp_set_dynamic(0);); (13, OpenMP, #pragma omp section); (15, OpenMP, int thread_nr = omp_get_thread_num();)"}
{"predict": "(22,OpenMP,#pragma omp parallel for collapse(2)), (70,OpenMP,#pragma omp parallel for collapse(2))", "label": "(10, CUDA, extern __shared__ uint64_t buffer[];)"}
{"predict": "(1,MPI,*w = MPI_Wtime();); (4,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp barrier), (35,OpenMP,#pragma omp barrier)", "label": "(22, CUDA, __shared__ float shared[forward_threads];)"}
{"predict": "(12,OpenMP,#pragma omp parallel for collapse(2)), (28,OpenMP,#pragma omp parallel for collapse(2))", "label": "(12, CUDA, cudaDeviceSynchronize();); (7, CUDA, cudaMalloc((void **)&sm_o, sizeof(float) * m * m);); (34, CUDA, cudaFree(sm_o);)"}
{"predict": "(30,OpenMP,#pragma omp parallel for collapse(2)), (40,OpenMP,#pragma omp parallel for collapse(2))", "label": "(46, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (32, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_AP), sizeof(data_type) * AP.size()));); (63, CUDA, CUDA_CHECK(cudaDeviceReset());); (60, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (30, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * A.size()));); (53, CUDA, CUDA_CHECK(cudaFree(d_AP));); (53, CUDA, CUDA_CHECK(cudaFree(d_A));); (22, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(11,MPI,*w = MPI_Wtime();)", "label": "(40, OpenCL, clReleaseEvent(exec_event);); (11, OpenCL, profiling_queue = clCreateCommandQueue(ctx, device, CL_QUEUE_PROFILING_ENABLE, &status);); (47, OpenCL, clReleaseCommandQueue(profiling_queue);)"}
{"predict": "2:             int rc;\n143:             rc = MTest_dtp_cmp(&result, &orig, count);\n144:             if (rc != 0) {\n145:                 printf(\"Result mismatch\\n\");\n146:                 errs++;\n147:             }\n148:         }\n149: \n150:         MPI_Win_fence(0, win);\n151:         MTest_dtp_free(&orig);\n152:         MTest_dtp_free(&result);\n153:     }\n154: \n155:     /* target receives data */\n156:     if (rank == target_rank) {\n157:         if (lock_type == LOCK_TYPE__LOCK) {\n158:         } else {\n159:         }\n160: \n161:         MPI_Get_accumulate((char *) result.buf + result.dtp_obj.DTP_buf_offset, resultcount,\n162:                            resulttype, (char *) orig.buf + orig.dtp_obj.DTP_buf_offset,\n163:                            origcount, origtype, rank,\n164:                            target.dtp_obj.DTP_buf_offset / base_type_size, origcount,\n165:                            origtype, MPI_REPLACE, win);\n166: \n167:         if (flush_local_type == FLUSH_LOCAL_TYPE__FLUSH_LOCAL) {\n168:             MPI_Win_flush_local(rank, win);\n169:         } else if (flush_local_type == FLUSH_LOCAL_TYPE__FLUSH_LOCAL_ALL) {\n170:         }\n171: \n172:         if (flush_type == FLUSH_TYPE__FLUSH_ALL) {\n173:             MPI_Barrier(comm);\n174:         } else if (flush_type == FLUSH_TYPE__FLUSH) {\n175:             MPI_Win_flush(rank, win);\n176:             MPI_Barrier(comm);\n177:             MPI_Barrier(comm);\n178:         }\n179: \n180:         if (lock_type == LOCK_TYPE__LOCK) {\n181:         } else {\n182:         }\n183:     }\n184: \n185:     return errs;\n186: }", "label": "(163, MPI, MPI_Win_unlock(rank, win);); (22, MPI, MPI_Comm_size(comm, &size);); (125, MPI, MPI_Barrier(comm);); (134, MPI, MPI_Win_unlock(target_rank, win);); (135, MPI, MPI_Win_unlock_all(win);); (76, MPI, MPI_Win_lock(MPI_LOCK_SHARED, target_rank, 0, win);); (120, MPI, MPI_Win_flush_all(win);); (76, MPI, MPI_Win_lock_all(0, win);); (150, MPI, MPI_Win_lock(MPI_LOCK_SHARED, rank, 0, win);); (111, MPI, MPI_Win_flush_local_all(win);); (12, MPI, MPI_Comm_rank(comm, &rank);)"}
{"predict": "(27,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for)", "label": "(39, CUDA, return cudaGetLastError();)"}
{"predict": "(10,MPI, *w = MPI_Wtime();)\n(15,MPI, PMPI_T_init_thread(MPI_THREAD_MULTIPLE, &provided);)\n(20,MPI, PMPI_T_init_thread(MPI_THREAD_MULTIPLE, &provided);)\n(23,MPI, PMPI_T_thread_init(&provided);)\n(26,MPI, PMPI_T_thread_init(&provided);)\n(29,MPI, PMPI_T_thread_init(&provided);)\n(32,MPI, PMPI_T_thread_init(&provided);)\n(35,MPI, PMPI_T_thread_init(&provided);)\n(38,MPI, PMPI_T_thread_init(&provided);)\n(41,MPI, PMPI_T_thread_init(&provided);)", "label": "(19, MPI, MPIT_result = MPI_T_pvar_session_create(&session);); (12, MPI, MPIT_result = MPI_T_init_thread(MPI_THREAD_SINGLE, &provided);)"}
{"predict": "(34,MPI,*w = MPI_Wtime();); (49,OpenMP,#pragma omp parallel for), (70,OpenMP,#pragma omp parallel for), (112,OpenMP,#pragma omp parallel for)", "label": "(127, CUDA, checkCudaErrors(cudaFreeArray(cuArray));); (33, CUDA, checkCudaErrors(cudaMalloc((void **) &dData, size));); (56, CUDA, checkCudaErrors(cudaBindTextureToArray(tex, cuArray, channelDesc));); (63, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (71, CUDA, getLastCudaError(\"Kernel execution failed\");); (-3, CUDA, int devID = findCudaDevice(argc, (const char **) argv);); (120, CUDA, checkCudaErrors(cudaFree(dData));)"}
{"predict": "(7,MPI,MPI_Comm_group(MPI_COMM_WORLD, &group);)", "label": "(9, MPI, MPI_Group_compare(group, wgroup, &result);); (5, MPI, MPI_File_get_group(cFile, &group);); (0, MPI, MPI_File cFile = MPI_File_f2c(*file);); (4, MPI, MPI_Comm_group(MPI_COMM_WORLD, &wgroup);); (12, MPI, MPI_Group_free(&wgroup);); (10, MPI, MPI_Group_free(&group);)"}
{"predict": "(48,OpenMP,#pragma omp parallel for)\n(91,OpenMP,#pragma omp parallel for)", "label": "(75, CUDA, cudaCheckErrors(\"Kernel c failed\");); (99, CUDA, cudaCheckErrors(\"Launching graph failed\");); (13, CUDA, cudaStreamCreateWithFlags(&streams[i], cudaStreamNonBlocking);); (63, CUDA, cudaCheckErrors(\"Event record failed\");); (56, CUDA, cudaCheckErrors(\"Stream begin capture failed\");); (31, CUDA, cudaMalloc((void**) &d_x, N * sizeof(float));); (44, CUDA, cudaGraphCreate(FIXME, 0);); (62, CUDA, cudaCheckErrors(\"Kernel b failed\");); (104, CUDA, cudaDeviceSynchronize();); (98, CUDA, cudaGraphGetNodes(graph, nodes, &numNodes);); (121, CUDA, cudaMemcpy(h_y, d_y, N, cudaMemcpyDeviceToHost);); (73, CUDA, cudaCheckErrors(\"Kernel d failed\");); (89, CUDA, cudaStreamSynchronize(streams[0]);); (58, CUDA, cudaStreamWaitEvent(streams[1], event1);); (72, CUDA, cudaStreamEndCapture(streams[0], &graph);); (48, CUDA, cudaCheckErrors(\"Kernel a failed\");); (64, CUDA, cudaStreamWaitEvent(streams[0], event2);); (-11, CUDA, cudaEventCreateWithFlags(&event1, cudaEventDisableTiming);); (-11, CUDA, cudaEventCreateWithFlags(&event2, cudaEventDisableTiming);); (53, CUDA, cudaCheckErrors(\"Event wait failed\");); (88, CUDA, cudaCheckErrors(\"Graph get nodes failed\");); (44, CUDA, cudaEventRecord(event1, streams[0]);); (19, CUDA, cudaMemcpy(d_y, h_y, N, cudaMemcpyHostToDevice);); (54, CUDA, cudaEventRecord(event2, streams[1]);); (16, CUDA, cudaMemcpy(d_x, h_x, N, cudaMemcpyHostToDevice);); (107, CUDA, cudaCheckErrors(\"Finishing memcpy failed\");); (11, CUDA, cudaMalloc((void**) &d_y, N * sizeof(float));); (32, CUDA, cudaStreamBeginCapture(streams[0], cudaStreamCaptureModeGlobal);); (91, CUDA, cudaGraphLaunch(instance, streams[0]);); (63, CUDA, cudaCheckErrors(\"instantiating graph failed\");); (8, CUDA, cudaCheckErrors(\"cudaMalloc failed\");); (56, CUDA, cudaCheckErrors(\"Stream end capture failed\");); (59, CUDA, cudaGraphInstantiate(FIXME, graph, NULL, NULL, 0);); (66, CUDA, cudaGraphLaunch(FIXME, streams[0]);)"}
{"predict": "(19,OpenMP,#pragma omp parallel for reduction(+:))", "label": "(19, MPI, MPI_Reduce_local((void*)inbuf, outbuf, count, type, op);)"}
{"predict": "(5,MPI,*w = MPI_Wtime(););", "label": "(35, CUDA, __shared__ _VOLATILE_ float aSmem2[8][128+4];); (33, CUDA, __shared__ _VOLATILE_ float bSmem1[8][128+4];); (34, CUDA, __shared__ _VOLATILE_ float bSmem2[8][128+4];); (30, CUDA, __shared__ _VOLATILE_ float aSmem1[8][128+4];)"}
{"predict": "(8,OpenMP,#pragma omp parallel for reduction(+:c))", "label": "(3, CUDA, extern __shared__ unsigned sfdatx[];)"}
{"predict": "(10,OpenMP,#pragma omp parallel for) (15,OpenMP,#pragma omp parallel for) (22,OpenMP,#pragma omp parallel for) (29,OpenMP,#pragma omp parallel for) (37,OpenMP,#pragma omp parallel for)", "label": "(9, MPI, test_integration_sin<2,1,double> t1( mpi.vm()[\"hsize\"].as<double>() );); (31, MPI, test_integration_sin<2,4,double> t2( mpi.vm()[\"hsize\"].as<double>() );); (14, MPI, test_integration_sin<2,2,double> t2( mpi.vm()[\"hsize\"].as<double>() );); (2, MPI, std::cout << \"Order = \" << mpi.vm()[\"order\"].as<int>() << \" / \" << FEELPP_MESH_MAX_ORDER << \"\\n\";); (20, MPI, test_integration_sin<2,3,double> t2( mpi.vm()[\"hsize\"].as<double>() );); (37, MPI, test_integration_sin<2,5,double> t2( mpi.vm()[\"hsize\"].as<double>() );)"}
{"predict": "(13,OpenMP,#pragma omp parallel for private(i)), (15,MPI,delete[] cpuPointers;)", "label": "(16, CUDA, FastllmCudaFree(pointers);); (12, CUDA, cudaMemcpy(pointers, cpuPointers, sizeof(uint8_t*) * part, cudaMemcpyHostToDevice);); (6, CUDA, uint8_t ** pointers = (uint8_t**)FastllmCudaMalloc(sizeof(uint8_t*) * part);)"}
{"predict": "(1,MPI,MPI_Init(&argc, &argv);) (2,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);) (2,MPI,MPI_Comm_size(MPI_COMM_WORLD, &size);) (11,MPI,MPI_Send(buf, 10, MPI_CHAR, 3, 0, MPI_COMM_WORLD);) (17,MPI,MPI_Recv(buf, 10, MPI_CHAR, 3, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);) (23,MPI,MPI_Irecv(buf, 10, MPI_CHAR, 0, 0, MPI_COMM_WORLD, &request);) (31,MPI,MPI_Irecv(buf, 10, MPI_CHAR, 0, 0, MPI_COMM_WORLD, &request);) (60,MPI,MPI_Finalize();)", "label": "(30, MPI, errs = MPI_Irecv(buf, 10, MPI_CHAR, 0, 0, MPI_COMM_WORLD, &request);); (5, MPI, MPI_Init(&argc, &argv);); (65, MPI, MPI_Finalize();); (18, MPI, errs = MPI_Issend(\"No Errors\", 10, MPI_CHAR, 3, 0, MPI_COMM_WORLD, &request);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (16, MPI, errs += MPI_Wait(&request, MPI_STATUS_IGNORE);); (2, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (5, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (34, MPI, errs = MPI_Issend(\"No Errors\", 10, MPI_CHAR, 2, 0, MPI_COMM_WORLD, &request);)"}
{"predict": "(2,OpenMP,#pragma omp parallel for)", "label": "(7, OpenMP, #pragma omp target teams distribute parallel for simd order(); (15, OpenMP, #pragma omp target teams distribute parallel for simd order(concurrent)); (-1, OpenMP, #pragma omp target teams distribute parallel for simd nowait nowait); (7, OpenMP, #pragma omp target teams distribute parallel for simd order(none); (9, OpenMP, #pragma omp target teams distribute parallel for simd order(concurrent); (-1, OpenMP, #pragma omp target teams distribute parallel for simd order)"}
{"predict": "(21,OpenMP,#pragma omp parallel for)", "label": "(24, CUDA, CHECK_CUDA_RETURN(cudaGetLastError());)"}
{"predict": "(6,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for)", "label": "(7, TBB, tbb::tick_count t1 = tbb::tick_count::now();); (14, TBB, ASSERT( num_threads > 0, \"tbb::thread::hardware_concurrency() has returned an incorrect value\" );); (3, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (11, TBB, int num_threads = tbb::tbb_thread::hardware_concurrency();); (5, TBB, tbb::tick_count t2 = tbb::tick_count::now();)"}
{"predict": "') of loop variable 'i'}}\n122: #pragma omp distribute parallel for simd\n123:   for (int i = 0; i < kk; i = i + 2)\n124:     c[i] = a[i];\n125: \n126: #pragma omp target\n127: #pragma omp teams\n128: // expected-error@+2 {{increment clause of OpenMP for loop must perform simple addition or subtraction on loop variable 'i'}}\n129: #pragma omp distribute parallel for simd\n130:   for (int i = 0; i < kk; i += 2.0)\n131:     c[i] = a[i];\n132: \n133: #pragma omp target\n134: #pragma omp teams\n135: // expected-error@+2 {{increment clause of OpenMP for loop must perform simple addition or subtraction on loop variable 'i'}}\n136: #pragma omp distribute parallel for simd\n137:   for (int i = 0; i < kk; i += '\\1')\n138:     c[i] = a[i];\n139: \n140: #pragma omp target\n141: #pragma omp teams\n142: // expected-error@+2 {{initialization clause of OpenMP for loop is not in canonical form ('var = init' or 'T var = init')}}\n143: #pragma omp distribute parallel for simd\n144:   for (int i = 0, j = 0; i < kk; i++, j++)\n145:     c[i] = a[i];\n146: \n147: #pragma omp target\n148: #pragma omp teams\n149: // expected-error@+2 {{initialization clause of OpenMP for loop is not in canonical form ('var = init' or 'T var = init')}}\n150: #pragma omp distribute parallel for simd\n151:   for (int i = 0, j = 0; i < kk; i++, j += 2)\n152:     c[i] = a[i];\n153: \n154: #pragma omp target\n155: #pragma omp teams\n156: // expected-error@+2 {{initialization", "label": "(311, OpenMP, #pragma omp distribute parallel for simd firstprivate(ii)); (332, OpenMP, #pragma omp distribute parallel for simd lastprivate(ii)); (6, OpenMP, #pragma omp distribute parallel for simd); (322, OpenMP, #pragma omp distribute parallel for simd private(ii)); (313, OpenMP, #pragma omp distribute parallel for simd linear(ii)); (1, OpenMP, #pragma omp target); (347, OpenMP, #pragma omp distribute parallel for simd collapse(2)); (0, OpenMP, #pragma omp teams)"}
{"predict": "(15,OpenMP,#pragma omp parallel for)", "label": "(17, CUDA, cudaFree(devx_);); (27, CUDA, cudaFree(d_T_);); (24, CUDA, cudaFree(d_P_);); (24, CUDA, cudaFree(d_Q_);); (12, CUDA, cudaFree(devr_);); (1, CUDA, cudaFree(d_work_);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for)", "label": "(7, OpenMP, printf(\"#pragma omp target teams distribute parallel for thread_limit(4)\\n\");); (10, OpenMP, num_teams[j] = omp_get_num_teams();); (13, OpenMP, printf(\"#pragma omp target teams distribute parallel for\\n\");); (7, OpenMP, num_threads[j] = omp_get_num_threads();); (4, OpenMP, #pragma omp target teams distribute parallel for)"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for)", "label": "(19, OpenCL, clReleaseContext_ExpectAndReturn(make_context(0), CL_SUCCESS);)"}
{"predict": "(64, MPI, MPI_Errhandler_set(c_info->communicator, MPI_ERRORS_RETURN);)", "label": "(63, MPI, *time -= MPI_Wtime();); (43, MPI, MPI_Type_size(c_info->r_data_type, &r_size);); (41, MPI, MPI_Type_size(c_info->s_data_type, &s_size);); (73, MPI, *time += MPI_Wtime();)"}
{"predict": "(11,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for)", "label": "(14, TBB, tbb::flow::make_edge( tbb::flow::output_port<0>(exe_node), receivers[r] );); (1, TBB, typedef typename tbb::flow::tuple_element<0,OutputTuple>::type OutputType;); (27, TBB, tbb::flow::remove_edge( tbb::flow::output_port<0>(exe_node), receivers[r] );); (3, TBB, tbb::flow::multifunction_node< InputType, OutputTuple, tbb::flow::rejecting > exe_node( g, tbb::flow::unlimited, body );); (1, TBB, tbb::flow::graph g;)"}
{"predict": "(nr_nodes * sizeof(size_t));\n139: \n140:   /* Determine the maximum size of a request */\n141:   size_t max_recv_size = 0;\n142:   MPI_Alltoall(nr_send, 1, MPI_SIZE_T, nr_recv, 1, MPI_SIZE_T, MPI_COMM_WORLD);\n143:   MPI_Allreduce(nr_send, &max_recv_size, 1, MPI_SIZE_T, MPI_MAX, MPI_COMM_WORLD);\n144: \n145:   /* Determine the displacements for the MPI receives */\n146:   size_t *recv_displs = (size_t *)malloc(nr_nodes * sizeof(size_t));\n147:   recv_displs[0] = 0;\n148:   for (int i = 1; i < nr_nodes; i++) {\n149:     recv_displs[i] = recv_displs[i - 1] + nr_recv[i - 1];\n150:   }\n151: \n152:   /* Allocate the receive buffer */\n153:   struct mesh_key_value_rho *mesh_recvbuf;\n154:   if (swift_memalign(\"mesh_recvbuf\", (void **)&mesh_recvbuf,\n155:                      SWIFT_CACHE_ALIGNMENT,\n156:                      max_recv_size * sizeof(struct mesh_key_value_rho)) != 0)\n157:     error(\"Failed to allocate array for receive buffer!\");\n158: \n159:   /* Do the MPI receives */\n160:   MPI_Irecv(mesh_recvbuf, max_recv_size, MPI_STRUCT, MPI_ANY_SOURCE,\n161:             MPI_ANY_TAG, MPI_COMM_WORLD, tp->requests + tp->nr_active_requests);\n162:   tp->nr_active_requests++;\n163: \n164:   /* Do the MPI sends */\n165:   MPI_Isend(mesh_sendbuf, nr_send[nodeID], MPI_STRUCT, nodeID, 0, MPI_COMM_WORLD,\n166:             tp->requests + tp->nr_active_requests);\n167:   tp->nr_active_requests++;\n168: \n169:   /* Wait for the MPI sends to complete */\n170:   MPI_Waitall(tp->nr_active_requests, tp->requests, MPI_STATUSES_IGNORE);\n171:   tp->nr_active_requests", "label": "(11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &nodeID);); (9, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nr_nodes);); (73, MPI, MPI_Allgather(&local_n0, 1, MPI_INT, slice_width, 1, MPI_INT, MPI_COMM_WORLD);)"}
{"predict": "') of loop variable 'i'}}\n118:   for (int i = 0; i < 10; i < 10)\n119:     c[i] = a[i];\n120: \n121: #pragma omp target\n122: #pragma omp teams distribute simd\n123: // omp4-error@+1 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', or '>=') of loop variable 'i'}}\n124:   for (int i = 0; i < 10; i <= 10)\n125:     c[i] = a[i];\n126: \n127: #pragma omp target\n128: #pragma omp teams distribute simd\n129: // omp4-error@+1 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', or '>=') of loop variable 'i'}}\n130:   for (int i = 0; i > 10; i++)\n131:     c[i] = a[i];\n132: \n133: #pragma omp target\n134: #pragma omp teams distribute simd\n135: // omp4-error@+1 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', or '>=') of loop variable 'i'}}\n136:   for (int i = 0; i >= 10; i++)\n137:     c[i] = a[i];\n138: \n139: #pragma omp target\n140: #pragma omp teams distribute simd\n141: // omp4-error@+1 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', or '>=') of loop variable 'i'}}\n142:   for (int i = 0; i < 10; i < 10)\n143:     c[i] = a[i];\n144: \n145: #pragma omp target\n146: #pragma omp teams distribute simd\n147: // omp4-error@+1 {{condition of OpenMP for loop must be a relational comparison ('<', '<=', '>', or '>=') of loop variable 'i'}}\n148", "label": "(266, OpenMP, #pragma omp teams distribute simd firstprivate(ii)); (6, OpenMP, #pragma omp teams distribute simd); (270, OpenMP, #pragma omp teams distribute simd private(ii)); (275, OpenMP, #pragma omp teams distribute simd lastprivate(ii)); (287, OpenMP, #pragma omp teams distribute simd collapse(2)); (1, OpenMP, #pragma omp target)"}
{"predict": "(41,OpenMP,#pragma omp parallel for), (72,OpenMP,#pragma omp parallel for), (100,OpenMP,#pragma omp parallel for), (103,OpenMP,#pragma omp parallel for)", "label": "(84, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (84, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)"}
{"predict": "(7,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for), (51,OpenMP,#pragma omp parallel for)", "label": "(6, OpenMP, #pragma omp parallel for)"}
{"predict": "(11,MPI,MPI_Init(NULL, NULL);) (13,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);) (14,MPI,MPI_Comm_size(MPI_COMM_WORLD, &size);) (62,MPI,MPI_Finalize();)", "label": "(20, MPI, int rank = dray::dray::mpi_rank();); (9, MPI, dray::dray::mpi_comm(MPI_Comm_c2f(comm));); (19, MPI, int size = dray::dray::mpi_size();)"}
{"predict": "(7,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank); MPI_Comm_size(MPI_COMM_WORLD, &ranks);) (34,MPI,DTCMP_Segmented_exscanv(in_count, keys, MPI_INT, DTCMP_OP_INT_ASCEND, vals, ltr, rtl, MPI_INT, MPI_SUM, DTCMP_FLAG_NONE, MPI_COMM_WORLD);)", "label": "(6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (104, MPI, MPI_Barrier(MPI_COMM_WORLD);); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "));\n121: \t    send_count_list = SUPERLU_MALLOC(nprocs * sizeof(int));\n122: \n123: \t    /* Initialize send lists */\n124: \t    for (p = 0; p < nprocs; ++p) {\n125: \t\tprocs_to_send_list[p] = -1;\n126: \t\tsend_count_list[p] = 0;\n127: \t    }\n128: \n129: \t    /* Compute prefix sum of block rows of X among all processes */\n130: \t    for (p = 0; p < nprocs; ++p) {\n131: \t\tint src = p / pxy;\n132: \t\tint dest = p % pxy;\n133: \t\tint row = src / grid2d->nprow;\n134: \t\tint col = src % grid2d->npcol;\n135: \t\tint r = row + col * grid2d->nprow;\n136: \t\tint offset = row_counts_int[r];\n137: \t\tint count = row_counts_int[r + 1] - offset;\n138: \t\tx_send_counts[p] = count;\n139: \t\tx_recv_counts[p] = count;\n140: \t\tif (src == iam) {\n141: \t\t    /* Send counts to all other processes */\n142: \t\t    for (i = 0; i < nprocs; ++i) {\n143: \t\t\tprocs_to_send_list[i] = src;\n144: \t\t\tsend_count_list[i] = count;\n145: \t\t    }\n146: \t\t}\n147: \t\telse {\n148: \t\t    /* Receive counts from the source process */\n149: \t\t    MPI_Recv(x_recv_counts, nprocs, MPI_INT, src, 1, grid3d->zscp.comm, &recv_status);\n150: \t\t}\n151: \t\t/* Update prefix sum */\n152: \t\tint prefix_sum = 0;\n153: \t\tfor (i = 0; i < nprocs; ++i) {\n154: \t\t    prefix_sum += x_send_counts[i];\n155: \t\t    x_send_counts[i] = prefix_sum;\n156: \t\t}\n157:", "label": "(270, MPI, MPI_Wait(&recv_req, &recv_status);)"}
{"predict": "2:       if (N == 0) {\n133:         can_break = 0;\n134:         break;\n135:       }\n136: \n137:       /* if we found an exact match, stop */\n138:       if (found_exact[i]) {\n139:         can_break = 0;\n140:         break;\n141:       }\n142: \n143:       /* if N is below the threshold, stop */\n144:       if (N < serial_search_threshold) {\n145:         can_break = 0;\n146:         break;\n147:       }\n148:     }\n149: \n150:     if (can_break) {\n151:       break;\n152:     }\n153: \n154:     /* gather all of the weighted medians */\n155:     lwgrp_comm_allgather(out_num_with_median, counts, ranks, MPI_INT, lwgcomm);\n156: \n157:     /* copy counts to all_counts */\n158:     memcpy(all_counts, counts, 2 * ranks * sizeof(int));\n159: \n160:     /* compute the prefix sum of all_counts */\n161:     int* prefix_sum = counts + ranks;\n162:     for (i = 0; i < ranks; i++) {\n163:       prefix_sum[i] = 0;\n164:     }\n165:     for (i = 0; i < ranks; i++) {\n166:       prefix_sum[i] = all_counts[i] + (i > 0 ? prefix_sum[i - 1] : 0);\n167:     }\n168: \n169:     /* adjust the index array for the next iteration */\n170:     for (i = 0; i < ranks; i++) {\n171:       index[i] += prefix_sum[i];\n172:     }\n173:   }\n174: \n175:   /* free resources */\n176:   dtcmp_free(k);\n177:   dtcmp_free(index);\n178:   dtcmp_free(num);\n179:   dtcmp_free(found_exact);", "label": "(24, MPI, MPI_Type_get_true_extent(key, &key_true_lb, &key_true_extent);); (204, MPI, MPI_Type_free(&type_int_with_key);); (280, MPI, MPI_Comm_rank(comm, &comm_rank);); (17, MPI, MPI_Type_get_extent(keysat, &keysat_lb, &keysat_extent);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(10, MPI, MPI_Comm_call_errhandler( comm, MPI_ERR_OTHER );); (8, MPI, MPI_Comm_set_errhandler( comm, newerr );); (14, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_create_errhandler( eh, &newerr );); (1, MPI, MPI_Init( &argc, &argv );); (6, MPI, MPI_Errhandler_free( &newerr );)"}
{"predict": "send buffer\n109:   for (local_int_t i=0; i< localNumberOfRows; i++) {\n110:     for (int j=0; j<nonzerosInRow[i]; j++) {\n111:       global_int_t curIndex = mtxIndG[i][j];\n112:       int rankIdOfColumnEntry = ComputeRankOfMatrixRow(*(A.geom), curIndex);\n113:       if (A.geom->rank==rankIdOfColumnEntry) { // My column index, so convert to local index\n114:         sendBuffer[j] = A.valG[i][j];\n115:       }\n116:     }\n117:   }\n118: \n119:   // Perform the halo exchange\n120:   ExchangeHalo(*(A.geom), sendBuffer, elementsToSend, neighbors, sendLength, receiveLength, sendList.size(), receiveList.size());\n121: \n122:   // Update the matrix values from the receive buffer\n123:   local_int_t * receiveBuffer = new local_int_t[totalToBeReceived];\n124:   for (local_int_t i=0; i< localNumberOfRows; i++) {\n125:     for (int j=0; j<nonzerosInRow[i]; j++) {\n126:       global_int_t curIndex = mtxIndG[i][j];\n127:       int rankIdOfColumnEntry = ComputeRankOfMatrixRow(*(A.geom), curIndex);\n128:       if (A.geom->rank!=rankIdOfColumnEntry) { // If column index is not a row index, then it comes from another processor\n129:         A.valG[i][j] = receiveBuffer[j];\n130:       }\n131:     }\n132:   }\n133: \n134:   delete [] sendBuffer;\n135:   delete [] elementsToSend;\n136:   delete [] neighbors;\n137:   delete [] receiveLength;\n138:   delete [] sendLength;\n139:   delete [] receiveBuffer;\n140: }\n141: \n142: // This function is not inlined by the compiler\n143: void ComputeRankOfMatrixRow(const Geometry & geom, global", "label": "(11, OpenMP, #pragma omp parallel for)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (12,OpenMP,#pragma omp parallel for), (22,MPI,MPI_Barrier(inter);), (25,MPI,MPI_Barrier(merge1);)", "label": "(38, MPI, MPI_Comm_disconnect(&parent);); (27, MPI, err = MPI_Intercomm_merge(inter, 0, &merge1);); (34, MPI, MPI_Comm_free(&intra);); (31, MPI, MPI_Comm_free(&merge1);); (6, MPI, err = MPI_Intercomm_merge(parent, 1, &intra);); (30, MPI, MPI_Comm_free(&inter);); (25, MPI, err = MPI_Barrier(merge1);); (18, MPI, err = MPI_Barrier(inter);); (21, MPI, MPI_Comm_rank(merge1, &rank);); (7, MPI, err = MPI_Intercomm_create(intra, 0, MPI_COMM_NULL, 0, tag, &inter);); (1, MPI, MPI_Comm_rank(intra, &rank);); (10, MPI, err = MPI_Intercomm_create(MPI_COMM_WORLD, 0, intra, 0, tag, &inter);)"}
{"predict": "/* free the requests and statuses */\n140:     ADIOI_Free(requests);\n141:     ADIOI_Free(statuses);\n142: \n143:     /* free recv_buf if it was allocated */\n144:     if (!buftype_is_contig)\n145:         ADIOI_Free(recv_buf[0]);\n146:     ADIOI_Free(recv_buf);\n147: }", "label": "(94, MPI, MPI_Type_commit(&send_type);); (137, MPI, MPI_Waitall(nprocs_send, requests + nprocs_recv, MPI_STATUSES_IGNORE);); (19, MPI, MPI_Alltoall(send_size, 1, MPI_COUNT, recv_size, 1, MPI_COUNT, fd->comm);); (118, MPI, MPI_Get_count_c(&statuses[j], MPI_BYTE, &count_recved);); (93, MPI, MPI_Type_free(&send_type);)"}
{"predict": "struct.kmp_depend_info* [[VLA1]], i{{.+}} 0, i{{.+}} 0\n65:   // CHECK: [[B_ADDR_CAST:%.+]] = ptrtoint i32** [[B_ADDR]] to i64\n66:   // CHECK: store i64 [[B_ADDR_CAST]], i64* [[BASE_ADDR]],\n67:   // CHECK: [[SIZE_ADDR:%.+]] = getelementptr inbounds %struct.kmp_depend_info, %struct.kmp_depend_info* [[VLA1]], i{{.+}} 0, i{{.+}} 1\n68:   // CHECK: store i64 8, i64* [[SIZE_ADDR]],\n69:   // CHECK: [[FLAGS_ADDR:%.+]] = getelementptr inbounds %struct.kmp_depend_info, %struct.kmp_depend_info* [[VLA1]], i{{.+}} 0, i{{.+}} 2\n70:   // CHECK: store i8 1, i8* [[FLAGS_ADDR]],\n71:   // CHECK: [[B:%.+]] = load i32**, i32** [[B_ADDR]],\n72:   // CHECK: [[B_CAST:%.+]] = ptrtoint i32** [[B]] to i64\n73:   // CHECK: [[SZ1:%.+]] = mul nuw i64 24, [[B_CAST]]\n74:   // CHECK: [[B:%.+]] = load i32**, i32** [[B_ADDR]],\n75:   // CHECK: [[B_CAST:%.+]] = ptrtoint i32** [[B]] to i64\n76:   // CHECK: [[SZ:%.+]] = mul nuw i64 [[SZ1]], [[B_CAST]]\n77:   // CHECK: [[SZ:%.+]] = add nuw i64 [[SZ]], [[SZ1]]\n78:   // CHECK: [[SV:%.+]] = call i8* @llvm.stacksave()\n79:   // CHECK: store i8* [[SV]], i8** [[SV_ADDR:%.+]],\n80:   // CHECK: [[VLA:%.+]] = alloca %struct.kmp_depend_info, i64 [[SZ]],\n81:   // CHECK: [[SIZE32:%.+]] = trunc i64 [[SZ", "label": "(101, OpenMP, #pragma omp task); (102, OpenMP, #pragma omp taskgroup)"}
{"predict": "(11,OpenMP,#pragma omp parallel for)", "label": "(17, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize());); (15, CUDA, qmc_cuda::cuda_check(cudaGetLastError());)"}
{"predict": "(8,MPI,MPI_Init (&argc,&argv);) (12,OpenMP,#pragma omp parallel for)", "label": "(28, MPI, MPI_Finalize();); (8, MPI, MPI_Get_processor_name(mach_name,&mach_len);); (22, MPI, MPI_Allreduce(&local,&global,1,MPI_INT,MPI_SUM,MPI_COMM_WORLD);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (6,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for)", "label": "(3, OpenMP, #pragma omp distribute parallel for if (true)); (11, OpenMP, #pragma omp distribute parallel for if (false)); (19, OpenMP, #pragma omp distribute parallel for if (Arg)); (-2, OpenMP, #pragma omp target); (-2, OpenMP, #pragma omp teams)"}
{"predict": "(9,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for), (35,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for), (42,OpenMP,#pragma omp parallel for), (45,OpenMP,#pragma omp parallel for), (47,OpenMP,#pragma omp parallel for), (50,OpenMP,#pragma omp parallel for), (52,OpenMP,#pragma omp parallel for), (54,OpenMP,#pragma omp parallel for), (56,OpenMP,#pragma omp parallel for), (59,OpenMP,#pragma omp parallel for), (60,OpenMP,#pragma omp parallel for), (62,OpenMP,#pragma omp parallel for), (66,OpenMP,#pragma omp parallel for), (69,OpenMP,#pragma omp parallel for), (71,OpenMP,#pragma omp parallel for), (74,OpenMP,#pragma omp parallel for), (76,OpenMP,#pragma omp parallel for), (80,OpenMP,#pragma omp parallel for), (82,OpenMP,#pragma omp parallel for), (84,OpenMP,#pragma omp parallel for), (88,OpenMP,#pragma omp parallel for), (90,OpenMP,#pragma omp parallel for), (92,OpenMP,#pragma omp parallel for), (94,OpenMP,#pragma omp parallel for), (96,OpenMP,#pragma omp parallel for), (103,OpenMP,#pragma omp parallel for)", "label": "(3, OpenMP, #pragma omp parallel for)"}
{"predict": "(15,OpenMP,#pragma omp parallel for)", "label": "(19, OpenCL, clReleaseEvent(event);)"}
{"predict": "smoke_check_3(H5AC_METADATA_WRITE_STRATEGY__DISTRIBUTED);\n168: #endif\n169: #if 1\n170:     smoke_check_4(H5AC_METADATA_WRITE_STRATEGY__PROCESS_0_ONLY);\n171:     smoke_check_4(H5AC_METADATA_WRITE_STRATEGY__DISTRIBUTED);\n172: #endif\n173: \n174:     /* clean up */\n175: \n176:     /* clean up the MPI environment */\n177:     MPI_Finalize();\n178: \n179:     return nerrors;\n180: }\n181: \n182: /* end main() */\n183: \n184: /* end of file */", "label": "(10, MPI, MPI_Init(&argc, &argv);); (206, MPI, MPI_Finalize();); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (185, MPI, MPI_Barrier(MPI_COMM_WORLD);); (7, MPI, MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);)"}
{"predict": "(9,OpenMP,#pragma omp parallel for)", "label": "(8, OpenMP, #pragma omp master taskloop lastprivate(t_var, vec, s_arr, s_arr, var, var))"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(11, OpenMP, #pragma omp target teams distribute parallel for map(tofrom : a[ : n]) map(to : b[ : n]))"}
{"predict": "(14,OpenMP,#pragma omp parallel for)", "label": "(8, OpenMP, #pragma omp target update CLAUSE(present: i)); (6, OpenMP, #pragma omp target enter data map(alloc: i)); (7, OpenMP, #pragma omp target exit data map(delete: i))"}
{"predict": ": call void @__kmpc_for_static_fini_8(%struct.ident_t* @{{.+}}, i32 %{{.+}})\n74:   // CHECK: br label %[[EXIT:[^,]+]]\n75:   // CHECK: [[FALSE]]:\n76:   // CHECK: br label %[[EXIT]]\n77:   // CHECK: [[EXIT]]:\n78:   // CHECK: call void @__kmpc_barrier(%struct.ident_t* @{{.+}}, i32 %{{.+}})\n79:   // CHECK: ret void\n80: }\n```", "label": "(163, OpenMP, #pragma omp for collapse(2))"}
{"predict": "(9,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (29,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for), (35,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for), (39,OpenMP,#pragma omp parallel for), (41,OpenMP,#pragma omp parallel for), (43,OpenMP,#pragma omp parallel for), (45,OpenMP,#pragma omp parallel for), (47,OpenMP,#pragma omp parallel for), (49,OpenMP,#pragma omp parallel for), (52,OpenMP,#pragma omp parallel for), (58,OpenMP,#pragma omp parallel for), (65,OpenMP,#pragma omp parallel for), (69,OpenMP,#pragma omp parallel for), (71,OpenMP,#pragma omp parallel for), (73,OpenMP,#pragma omp parallel for), (75,OpenMP,#pragma omp parallel for), (77,OpenMP,#pragma omp parallel for), (80,OpenMP,#pragma omp parallel for), (82,OpenMP,#pragma omp parallel for), (84,OpenMP,#pragma omp parallel for)", "label": "(38, OpenMP, #pragma omp distribute parallel for firstprivate(argc) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (78, OpenMP, #pragma omp parallel private(i)); (56, OpenMP, #pragma omp distribute parallel for firstprivate(e, g)); (30, OpenMP, #pragma omp distribute parallel for firstprivate(argc > 0 ? argv[1] : argv[2])); (68, OpenMP, #pragma omp distribute parallel for firstprivate(i)); (1, OpenMP, #pragma omp target); (57, OpenMP, #pragma omp distribute parallel for firstprivate(k, h)); (59, OpenMP, #pragma omp parallel); (40, OpenMP, #pragma omp distribute parallel for firstprivate(a, b)); (93, OpenMP, #pragma omp parallel reduction(+ : i)); (3, OpenMP, #pragma omp distribute parallel for firstprivate(); (71, OpenMP, #pragma omp distribute parallel for firstprivate(j)); (41, OpenMP, #pragma omp distribute parallel for firstprivate(argv[1])); (5, OpenMP, #pragma omp distribute parallel for firstprivate()); (9, OpenMP, #pragma omp distribute parallel for firstprivate(argc); (78, OpenMP, #pragma omp distribute parallel for lastprivate(g) firstprivate(g)); (27, OpenMP, #pragma omp distribute parallel for firstprivate(S1)); (11, OpenMP, #pragma omp distribute parallel for firstprivate(argc,); (60, OpenMP, #pragma omp parallel shared(i)); (-11, OpenMP, #pragma omp distribute parallel for firstprivate); (-13, OpenMP, #pragma omp teams)"}
{"predict": "(19,OpenMP,#pragma omp parallel for private(x, errs)), (45,MPI,MPI_Comm_free(&comms[x]));", "label": "(25, MPI, MPI_Comm_free(&comms[x]);); (18, MPI, MPI_Comm_dup(MPI_COMM_WORLD, &comms[x]);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for)", "label": "(5, OpenCL, clReleaseContext_ExpectAndReturn(make_context(0), CL_SUCCESS);); (3, OpenCL, clCreateContext_StubWithCallback(clCreateContext_testContextNonNullProperties);)"}
{"predict": "(9,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for), (43,OpenMP,#pragma omp parallel for), (46,OpenMP,#pragma omp parallel for)", "label": "(5, OpenMP, #pragma omp sections firstprivate(); (24, OpenMP, #pragma omp sections firstprivate()); (35, OpenMP, #pragma omp sections firstprivate(0)); (51, OpenMP, #pragma omp sections lastprivate(x, y, z) firstprivate(x, y, z)); (45, OpenMP, #pragma omp sections lastprivate(x, y) firstprivate(x, y)); (39, OpenMP, #pragma omp sections lastprivate(x) firstprivate(x)); (-4, OpenMP, #pragma omp parallel); (6, OpenMP, #pragma omp sections firstprivate(,); (11, OpenMP, #pragma omp sections firstprivate(, )); (22, OpenMP, #pragma omp sections firstprivate(int))"}
{"predict": "(5,MPI,*w = MPI_Wtime(););", "label": "(10, OpenMP, #pragma omp target update to(i) nowait (3.14) device (-10u)); (3, OpenMP, #pragma omp target nowait update to(i)); (7, OpenMP, #pragma omp target update to(i) nowait device (-10u)); (5, OpenMP, #pragma omp target update to(i) nowait (argc))); (7, OpenMP, #pragma omp target update to(i) nowait nowait); (7, OpenMP, #pragma omp target update nowait to(i) nowait); (-3, OpenMP, #pragma omp nowait target update to(i)); (-1, OpenMP, #pragma omp target update nowait() to(i)); (-1, OpenMP, #pragma omp target update to(i) nowait()"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(7, OpenACC, #pragma acc loop gang worker vector); (5, OpenACC, #pragma acc parallel deviceptr(position,cell_keys))"}
{"predict": "(14,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for)", "label": "(6, TBB, tbb::flow::queue_node< q3_input_type >  q3(g);); (11, TBB, ASSERT(tbb::flow::input_port<0>(my_indexer).predecessor_count() == 0, NULL);); (7, TBB, tbb::flow::make_edge(my_indexer, q3);)"}
{"predict": "(28,OpenMP,#pragma omp parallel for), (82,OpenMP,#pragma omp parallel for), (112,OpenMP,#pragma omp parallel for)", "label": "(5, TBB, tbb::atomic<bool> taskDoneFlag;); (85, TBB, tbb::task::enqueue(*t, p);); (9, TBB, tbb::task::enqueue(*w);); (13, TBB, FFTask* t = new( tbb::task::allocate_root() ) FFTask(&flag);); (81, TBB, const tbb::priority_t p = getPriorityByInt(i);); (98, TBB, tbb::atomic<int> counter;); (11, TBB, tbb::task::enqueue(*t);); (108, TBB, tbb::atomic<bool> signalToLeave;); (-4, TBB, tbb::atomic<int> flag;); (1, TBB, WaiterTask* w = new( tbb::task::allocate_root() ) WaiterTask(&taskDoneFlag);); (3, TBB, tbb::global_control c(tbb::global_control::max_allowed_parallelism, 1);)"}
{"predict": "(5,MPI,int par_rank, int par_size; MPI_Comm comm = MPI_COMM_WORLD; MPI_Comm_rank(comm, &par_rank); MPI_Comm_size(comm, &par_size);) (10,MPI,if(par_rank == 0) { ASCENT_INFO(ascent::about()); })", "label": "(8, MPI, MPI_Comm_rank(comm, &par_rank);); (24, MPI, open_opts[\"mpi_comm\"] = MPI_Comm_c2f(comm);); (7, MPI, MPI_Comm_size(comm, &par_size);)"}
{"predict": "(7,MPI,#include <mpi.h>;)\n(8,MPI,MPI_Init(NULL, NULL);)\n(12,MPI,MPI_Finalize();)\n(9,OpenMP,#pragma omp parallel for)", "label": "(5, OpenMP, #pragma omp workshare)"}
{"predict": "(9,OpenMP,#pragma omp parallel for)", "label": "(6, OpenCL, retVal = clEnqueueMemcpyINTEL(this->pCmdQ, true, unifiedMemoryPtr, input.data(), dataSize, 0, nullptr, nullptr);)"}
{"predict": "i = 0; i < 10; ++i)\n83:     m++;\n84:   for (int i = 0; i < 10; ++i)\n85:     m++;\n86:   for (int i = 0; i < 10; ++i)\n87:     m++;\n88:   for (int i = 0; i < 10; ++i)\n89:     m++;\n90:   for (int i = 0; i < 10; ++i)\n91:     m++;\n92:   for (int i = 0; i < 10; ++i)\n93:     m++;\n94:   for (int i = 0; i < 10; ++i)\n95:     m++;\n96:   for (int i = 0; i < 10; ++i)\n97:     m++;\n98:   for (int i = 0; i < 10; ++i)\n99:     m++;\n100:  for (int i = 0; i < 10; ++i)\n101:    m++;\n102:  for (int i = 0; i < 10; ++i)\n103:    m++;\n104:  for (int i = 0; i < 10; ++i)\n105:    m++;\n106:  for (int i = 0; i < 10; ++i)\n107:    m++;\n108:  for (int i = 0; i < 10; ++i)\n109:    m++;\n110:  for (int i = 0; i < 10; ++i)\n111:    m++;\n112:  for (int i = 0; i < 10; ++i)\n113:    m++;\n114:  for (int i = 0; i < 10; ++i)\n115:    m++;\n116:  for (int i = 0; i < 10; ++i)\n117:    m++;\n118:  for (int i = 0; i <", "label": "(15, OpenMP, #pragma omp master taskloop reduction +); (65, OpenMP, #pragma omp master taskloop reduction(- : da)); (61, OpenMP, #pragma omp master taskloop reduction(* : ca)); (54, OpenMP, #pragma omp master taskloop reduction(max : h.b)); (47, OpenMP, #pragma omp master taskloop reduction(+ : a, b, c, d, f)); (55, OpenMP, #pragma omp master taskloop reduction(+ : ba)); (30, OpenMP, #pragma omp master taskloop reduction(| : argc,); (84, OpenMP, #pragma omp master taskloop reduction(+ : p), reduction(+ : p)); (46, OpenMP, #pragma omp master taskloop reduction(min : a, b, c, d, f)); (33, OpenMP, #pragma omp master taskloop reduction(~ : argc)); (38, OpenMP, #pragma omp master taskloop reduction(^ : S1)); (73, OpenMP, #pragma omp master taskloop reduction(+ : o)); (90, OpenMP, #pragma omp master taskloop reduction(max : j)); (11, OpenMP, #pragma omp master taskloop reduction()); (95, OpenMP, #pragma omp parallel reduction(* : fl)); (63, OpenMP, #pragma omp master taskloop reduction(& : e, g)); (65, OpenMP, #pragma omp master taskloop reduction(+ : h, k, B::x)); (73, OpenMP, #pragma omp parallel private(k)); (96, OpenMP, #pragma omp master taskloop reduction(+ : m)); (20, OpenMP, #pragma omp master taskloop reduction(|| : argc > 0 ? argv[1] : argv[2])); (-2, OpenMP, #pragma omp master taskloop reduction(); (24, OpenMP, #pragma omp master taskloop reduction(&& : argc, z)); (53, OpenMP, #pragma omp master taskloop reduction(&& : S2::S2sc)); (46, OpenMP, #pragma omp master taskloop reduction(^ : fl)); (6, OpenMP, #pragma omp master taskloop reduction(\\)); (-13, OpenMP, #pragma omp master taskloop reduction); (7, OpenMP, #pragma omp master taskloop reduction(foo : argc); (74, OpenMP, #pragma omp parallel reduction(min : i)); (77, OpenMP, #pragma omp parallel private(fl)); (91, OpenMP, #pragma omp master taskloop reduction(task, + : m)); (-9, OpenMP, #pragma omp master taskloop reduction(-); (86, OpenMP, #pragma omp master taskloop nogroup reduction(+ : m)); (65, OpenMP, #pragma omp master taskloop reduction(+ : r)); (-6, OpenMP, #pragma omp master taskloop reduction(*)); (53, OpenMP, #pragma omp master taskloop private(i), reduction(+ : j), reduction(+ : q)); (65, OpenMP, #pragma omp parallel shared(i)); (36, OpenMP, #pragma omp master taskloop reduction(&& : S2::S2s)); (69, OpenMP, #pragma omp master taskloop reduction(+ : fl))"}
{"predict": "1", "label": "(34, CUDA, __shared__ int pointsNumSM[4];); (30, CUDA, __shared__ float4 pillarSM[4][64];); (30, CUDA, __shared__ float4 pillarSumSM[4];); (30, CUDA, __shared__ int4 cordsSM[4];); (31, CUDA, __shared__ float pillarOutSM[4][64][11];)"}
{"predict": "()", "label": "(1, OpenMP, omp_allocator_handle_t default_alloc = omp_get_default_allocator();); (4, OpenMP, int *p = (int *)omp_alloc(123456*sizeof(int));)"}
{"predict": "]]\n75:     // CHECK: store float [[RED]], float* [[A_BUF_IDX_ELEM]],\n76:     // CHECK: [[B_BUF_IDX_VAL:%.+]] = load double, double* [[B_BUF_IDX]],\n77:     // CHECK: [[B_BUF_IDX_SUB_K2POW_VAL:%.+]] = load double, double* [[B_BUF_IDX_SUB_K2POW]],\n78:     // CHECK: [[RED:%.+]] = fadd double [[B_BUF_IDX_VAL]], [[B_BUF_IDX_SUB_K2POW_VAL]]\n79:     // CHECK: store double [[RED]], double* [[B_BUF_IDX]],\n80:     // CHECK: [[K2POW_NEXT:%.+]] = add nuw i64 [[K2POW]], 1\n81:     // CHECK: br label %[[INNER_BODY]]\n82:     // CHECK: [[RED_DONE]]:\n83:     // CHECK: [[K_NEXT:%.+]] = add nuw i32 [[K]], 1\n84:     // CHECK: br label %[[OUTER_BODY]]\n85:     // CHECK: [[INNER_EXIT]]:\n86:     // CHECK: call void @llvm.stackrestore(i8* %\n87:     // CHECK: call void @__kmpc_barrier(\n88:     // CHECK: call void @__kmpc_for_static_fini(\n89:     // CHECK: call void @__kmpc_barrier(\n90:   }\n91:   // CHECK: call void @llvm.stackrestore(i8* %\n92:   // CHECK: ret void\n93: }\n\n### ** Analysis and Recommendations**", "label": "(194, OpenMP, #pragma omp scan exclusive(a[:n], b)); (44, OpenMP, #pragma omp scan inclusive(a[:n], b)); (9, OpenMP, #pragma omp for reduction(inscan, +:a[:n], b))"}
{"predict": "(3,MPI,#include <mpi.h>); (7,MPI,MPI_Init(&argc, &argv);); (11,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (12,MPI,MPI_Comm_size(MPI_COMM_WORLD, &np);); (32,MPI,MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_CREATE | MPI_MODE_WRONLY, MPI_INFO_NULL, &fh);); (35,MPI,MPI_File_set_view(fh, 0, MPI_INT, filetype, \"native\", MPI_INFO_NULL);); (47,MPI,MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_RDONLY, MPI_INFO_NULL, &fh);); (50,MPI,MPI_File_read(fh, buf, MAXLEN, MPI_INT, &status);); (54,MPI,MPI_Finalize(););", "label": "(11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &np);); (68, MPI, err = MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_WRONLY, MPI_INFO_NULL, &fh);); (73, MPI, err = MPI_File_write_all(fh, buf, buflen, MPI_INT, &status);); (29, MPI, err = MPI_File_close(&fh);); (60, MPI, MPI_Type_commit(&filetype);); (24, MPI, err = MPI_File_write(fh, buf, MAXLEN, MPI_INT, &status);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (65, MPI, err = MPI_File_set_view(fh, 0, MPI_INT, filetype, \"native\", MPI_INFO_NULL);); (45, MPI, MPI_Type_create_hindexed(num, blocklen, addrs, MPI_INT, &filetype);); (76, MPI, err = MPI_File_open(MPI_COMM_SELF, filename, MPI_MODE_RDONLY, MPI_INFO_NULL, &fh);); (40, MPI, MPI_Type_indexed(num, blocklen, indices, MPI_INT, &filetype);); (67, MPI, MPI_Type_free(&filetype);); (76, MPI, err = MPI_File_read(fh, buf, MAXLEN, MPI_INT, &status);); (23, MPI, MPI_Barrier(MPI_COMM_WORLD);); (42, MPI, MPI_Type_create_struct(num, blocklen, addrs, types, &filetype);)"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(5, OpenMP, #pragma omp parallel sections default()); (2, OpenMP, #pragma omp parallel sections default(); (7, OpenMP, #pragma omp parallel sections default(shared), default(shared)); (4, OpenMP, #pragma omp parallel sections default(none); (7, OpenMP, #pragma omp parallel sections default(x)); (16, OpenMP, #pragma omp parallel sections default(none)); (-5, OpenMP, #pragma omp parallel sections default); (21, OpenMP, #pragma omp parallel sections default(shared))"}
{"predict": "i;\n111:         int store_c_gmem_n = bx * BN + BN / 2 + tx * TN / 2;\n112:         int store_c_gmem_addr = OFFSET(store_c_gmem_m, store_c_gmem_n, N);\n113:         FLOAT4(c[store_c_gmem_addr]) = FLOAT4(r_c[i + TM / 2][0]);\n114:         FLOAT4(c[store_c_gmem_addr + BN / 2]) = FLOAT4(r_c[i + TM / 2][4]);\n115:     }\n116: }", "label": "(16, CUDA, __shared__ float s_a[2][BK][BM];); (16, CUDA, __shared__ float s_b[2][BK][BN];)"}
{"predict": "(23,OpenMP,#pragma omp parallel for)", "label": "(17, OpenMP, volatile  int_t thread_id = omp_get_thread_num();)"}
{"predict": "(14,OpenMP,#pragma omp parallel for reduction(+:expected_value))", "label": "(8, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (9, TBB, tbb::tick_count t1 = tbb::tick_count::now();); (7, TBB, tbb::parallel_for(tbb::blocked_range<size_t>(0,n,n/100),TwiddleInvariant<Invariant<M,8> >(invariant));)"}
{"predict": "(14,MPI,MPI_Init(&argc, &argv);) (17,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);) (18,MPI,MPI_Comm_size(MPI_COMM_WORLD, &size);) (22,MPI,MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_RDWR | MPI_MODE_CREATE, MPI_INFO_NULL, &fh);) (23,MPI,MPI_File_set_view(fh, off, MPI_INT, MPI_INT, \"native\", info);) (24,MPI,MPI_File_iwrite_at_all(fh, off, buffer, BUFSIZE, MPI_INT, &request);) (26,MPI,MPI_Wait(&request, &status);) (27,MPI,MPI_File_close(&fh);) (31,MPI,MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_RDWR, MPI_INFO_NULL, &fh);) (32,MPI,MPI_File_set_view(fh, off, MPI_INT, MPI_INT, \"native\", info);) (33,MPI,MPI_File_iread_at_all(fh, off, buf2, BUFSIZE, MPI_INT, &request);) (35,MPI,MPI_Wait(&request, &status);) (36,MPI,MPI_File_close(&fh);) (42,MPI,MPI_Finalize();)", "label": "(19, MPI, MPI_Info_set(info, \"cb_nodes\", \"1\");); (25, MPI, errcode = MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_WRONLY | MPI_MODE_CREATE, info, &fh);); (27, MPI, errcode = MPI_File_iwrite_at_all(fh, off, buffer, BUFSIZE, MPI_INT, &request);); (30, MPI, errcode = MPI_File_close(&fh);); (28, MPI, MPI_Wait(&request, &status);); (12, MPI, MPI_Info_create(&info);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (33, MPI, errcode = MPI_File_iread_at_all(fh, off, buf2, BUFSIZE, MPI_INT, &request);); (10, MPI, MPI_Info_set(info, \"romio_cb_write\", \"enable\");); (43, MPI, MPI_Info_free(&info);); (27, MPI, errcode = MPI_File_open(MPI_COMM_WORLD, filename, MPI_MODE_RDONLY, info, &fh);)"}
{"predict": "(24,OpenMP,#pragma omp parallel), (112,OpenMP,#pragma omp for collapse(2)), (120,OpenMP,#pragma omp barrier)", "label": "(109, OpenMP, #pragma omp barrier); (75, OpenMP, nthread = omp_get_num_threads();); (92, OpenMP, #pragma omp for); (71, OpenMP, #pragma omp master); (68, OpenMP, #pragma omp parallel private(j,iter))"}
{"predict": "(15,OpenMP,#pragma omp parallel for)", "label": "(14, OpenMP, #pragma omp target parallel for)"}
{"predict": "(17, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)", "label": "(16, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &scri_rank);); (16, MPI, MPI_Comm_size(MPI_COMM_WORLD, &scri_ranks);)"}
{"predict": "] = 4.3f / formatMax;\n121:                         inputValues[ i++ ] = 4.5f / formatMax;\n122:                         inputValues[ i++ ] = 4.7f / formatMax;\n123:                         inputValues[ i++ ] = 5.0f / formatMax;\n124:                         inputValues[ i++ ] = 5.3f / formatMax;\n125:                         inputValues[ i++ ] = 5.5f / formatMax;\n126:                         inputValues[ i++ ] = 5.7f / formatMax;\n127:                     }\n128:                 }\n129:             }\n130:         }\n131: \n132:         if( imageInfo->format->image_channel_data_type == CL_HALF_FLOAT )\n133:             if( DetectFloatToHalfRoundingMode(queue) )\n134:                 return 1;\n135: \n136:         threads[0] = imageInfo->width;\n137:         threads[1] = 1;\n138: \n139:         cl_mem_flags mem_flags = mem_flag_types[ mem_flag_index ];\n140:         cl_mem imageMem = clCreateImage2D( context, mem_flags, &imageInfo->format, imageInfo->width, imageInfo->height, 0, imageValues.get(), &error );\n141:         if( error != CL_SUCCESS )\n142:         {\n143:             totalErrors++;\n144:             continue;\n145:         }\n146: \n147:         if( inputType == kFloat && imageInfo->format->image_channel_data_type != CL_FLOAT && imageInfo->format->image_channel_data_type != CL_HALF_FLOAT )\n148:         {\n149:             if( !gTestMipmaps )\n150:             {\n151:                 // Convert to the correct format\n152:                 if( !convert_image_data_to_format( inputType, imageInfo, imageValues, d ) )\n153:                 {\n154:                     totalErrors++;\n155:                     continue;\n156:                 }\n157:             }\n158:         }\n159: \n160", "label": "(242, OpenCL, error = clSetKernelArg( kernel, 0, sizeof( cl_mem ), &inputStream );); (228, OpenCL, error = clSetKernelArg( kernel, 2, sizeof( int ), &lod );); (215, OpenCL, error = clSetKernelArg( kernel, 1, sizeof( cl_mem ), &image );); (244, OpenCL, error = clEnqueueNDRangeKernel( queue, kernel, 1, NULL, threads, NULL, 0, NULL, NULL );); (260, OpenCL, error = clEnqueueReadImage( queue, image, CL_TRUE, origin, region, gEnablePitch ? imageInfo->rowPitch : 0, 0, resultValues, 0, NULL, NULL );)"}
{"predict": "->d_Lindval_loc_bc_dat));\n129:     checkGPU (gpuFree (Llu->d_Lindval_loc_bc_offset));\n130:     checkGPU (gpuFree (Llu->d_Urb_indptr));\n131:     checkGPU (gpuFree (Llu->d_Urb_valptr));\n132: }\n133: #endif\n134: \n135: #if ( DEBUGlevel>=1 )\n136:     CHECK_MALLOC(iam, \"Exit sDestroy_LU()\");\n137: #endif\n138: }\n\n### **Analysis**\n\nThe code is primarily focused on freeing memory allocated for various data structures used in the LU decomposition process. The memory management is done in a sequential manner, which can be optimized for parallel execution. The most suitable parallelization technique for this scenario is OpenMP, as it can be used to parallelize the memory freeing operations that can be done independently.\n\n### **Answer**\n\n(16,OpenMP,#pragma omp parallel for), (29,OpenMP,#pragma omp parallel for), (43,OpenMP,#pragma omp parallel for), (67,OpenMP,#pragma omp parallel for), (76,OpenMP,#pragma omp parallel for), (85,OpenMP,#pragma omp parallel for), (95,OpenMP,#pragma omp parallel for), (101,OpenMP,#pragma omp parallel for)", "label": "(9, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &iam );)"}
{"predict": "(30,MPI,MPI_Init(&argc, &argv);)", "label": "(36, MPI, MPI_File_close(&c_info->fh);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for)", "label": "(8, OpenMP, #pragma omp parallel); (10, OpenMP, #pragma omp for); (14, OpenMP, #pragma omp critical)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (70,OpenMP,#pragma omp taskwait), (95,OpenMP,#pragma omp parallel for reduction(+:asum)), (98,OpenMP,#pragma omp for reduction(+:asum))", "label": "(38, OpenMP, std::cout << \"Number of threads    = \" << omp_get_max_threads() << std::endl;)"}
{"predict": "(11,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for)", "label": "(32, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (34, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for)", "label": "(16, SYCL, auto b = B.get_access<sycl::access_mode::write>(cgh);); (5, SYCL, auto a = A.get_access<sycl::access_mode::write>(cgh);)"}
{"predict": "(3,MPI,*w = MPI_Wtime(););", "label": "(20, OpenCL, clReleaseMemObject(mobjA_);); (16, OpenCL, clReleaseMemObject(mobjX_);)"}
{"predict": "(21,OpenMP,#pragma omp parallel for) (40,OpenMP,#pragma omp parallel for) (68,OpenMP,#pragma omp parallel for)", "label": "(4, TBB, tbb::flow::queue_node<T> q2(g);); (70, TBB, tbb::flow::make_edge( q2, q3 );); (67, TBB, tbb::flow::queue_node<T> q3(g);); (37, TBB, tbb::flow::make_edge( q, q2 );); (56, TBB, tbb::flow::remove_edge( q, q2 );); (87, TBB, tbb::flow::remove_edge( q,  q2 );); (-3, TBB, tbb::flow::queue_node<T> q(g);); (-5, TBB, tbb::flow::graph g;)"}
{"predict": "(23,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for)", "label": "(14, OpenCL, cl_command_queue cmdQ = clCreateCommandQueueWithProperties(context.get(), clDevice, properties, &retVal);); (38, OpenCL, retVal = clReleaseCommandQueue(cmdQ);); (1, OpenCL, clGetDeviceInfo(clDevice, CL_DEVICE_SLICE_COUNT_INTEL, sizeof(size_t), &maxSliceCount, nullptr);)"}
{"predict": "0);\n93:             const float visc2=visco_eta[p2];\n94:             const float facv=fsph::GetViscosity_Fac<ftmode>(visc2,visco_eta[p2],prs);\n95:             const float frv=frx*facv,fry=fry*facv,frz=frz*facv;\n96:             //===== Gradients =====\n97:             tfloat3 gradvelp2=gradvel[p2];\n98:             if(rsym)gradvelp2.y=-gradvelp2.y; //<vs_syymmetry>\n99:             const tfloat3 gradpr=gradvelp2*prs;\n100:             const tfloat3 gradv=gradvelp2*facv;\n101:             //===== Compute Forces =====\n102:             tfloat3 acep2=TFloat3(0);\n103:             tfloat3 acep3=TFloat3(0);\n104:             if(tker==KERNEL_Cubic) {\n105:               acep2=acep2+(frx*gradv.x+frx*gradpr.x+frz*gradv.z+frz*gradpr.z)*massp2;\n106:               acep3=acep3+(frx*gradv.y+frx*gradpr.y+frz*gradv.y+frz*gradpr.y)*massp2;\n107:             } else {\n108:               acep2=acep2+(frx*gradv.x+frx*gradpr.x+frz*gradv.z+frz*gradpr.z)*massp2;\n109:               acep3=acep3+(frx*gradv.y+frx*gradpr.y+frz*gradv.y+frz*gradpr.y)*massp2;\n110:             }\n111:             acep1=acep1+acep2+acep3;\n112:           }\n113:         }\n114:       }\n115:     }\n116: \n117:     //-Accumulate viscous forces.\n118:     if(!ftp1) {\n119:       float v2=velp", "label": "(205, OpenMP, const int th=omp_get_thread_num();); (19, OpenMP, #pragma omp parallel for schedule (guided))"}
{"predict": "(6,MPI,#include <mpi.h>); (7,MPI,int MPI_Init(&argc, &argv);); (8,MPI,MPI_Comm_size(MPI_COMM_WORLD, &size);); (9,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (58,MPI,MPI_Finalize();); (82,MPI,MPI_Finalize(););", "label": "(7, MPI, MPI_Init(&argc, &argv);); (85, MPI, MPI_Finalize();); (8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (6, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (57, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(14, CUDA, os << \"__device__ \";); (8, CUDA, os << \"__device__ __constant__ \";)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(12, OpenMP, #pragma omp target teams); (9, OpenMP, #pragma omp target update to(vec.v[0:N])); (16, OpenMP, #pragma omp target update from(vec.v[0:N])); (10, OpenMP, #pragma omp distribute parallel for)"}
{"predict": "(12,OpenMP,#pragma omp parallel for reduction(+:sum))", "label": "(12, OpenMP, #pragma omp target teams distribute parallel for)"}
{"predict": "(21,OpenMP,#pragma omp parallel for)", "label": "(21, OpenCL, retVal = clReleaseMemObject(subBuffer);); (26, OpenCL, retVal = clReleaseMemObject(notUsedBuffer);); (28, OpenCL, EXPECT_EQ(clReleaseContext(context), CL_SUCCESS);); (-2, OpenCL, cl_mem notUsedBuffer = clCreateBuffer(clContext, flags, size, hostPtr, &retVal);); (20, OpenCL, retVal = clReleaseMemObject(buffer);); (0, OpenCL, cl_mem buffer = clCreateBuffer(clContext, flags, size, hostPtr, &retVal);); (8, OpenCL, cl_mem subBuffer = clCreateSubBuffer(buffer, flags, CL_BUFFER_CREATE_TYPE_REGION, &region, &retVal);)"}
{"predict": "10, \"Reduce of MPI_BYTE\\n\");\n116:     for (i = 0; i < 3; i++) {\n117:         inbuf[i] = (rank > 0) ? 0x3c : 0xc3;\n118:         outbuf[i] = 0;\n119:     }\n120:     if (rc) {\n121:         MTestPrintErrorMsg(\"MPI_BXOR and MPI_BYTE\", rc);\n122:         errs++;\n123:     } else {\n124:         if (rank == 0) {\n125:             if (outbuf[0] != ((size % 2) ? (char) 0xff : (char) 0)) {\n126:                 errs++;\n127:                 fprintf(stderr, \"byte BXOR(1) test failed\\n\");\n128:             }\n129:             if (outbuf[1]) {\n130:                 errs++;\n131:                 fprintf(stderr, \"byte BXOR(0) test failed\\n\");\n132:             }\n133:             if (outbuf[2] != ((size % 2) ? (char) 0xc3 : (char) 0xff)) {\n134:                 errs++;\n135:                 fprintf(stderr, \"byte BXOR(>) test failed\\n\");\n136:             }\n137:         }\n138:     }\n139: \n140:     /* short */\n141:     MTestPrintfMsg(10, \"Reduce of MPI_SHORT\\n\");\n142:     for (i = 0; i < 3; i++) {\n143:         sinbuf[i] = (rank > 0) ? 0x3c : 0xc3;\n144:         soutbuf[i] = 0;\n145:     }\n146:     if (rc) {\n147:         MTestPrintErrorMsg(\"MPI_BXOR and MPI_SHORT\", rc);\n148:         errs++;\n149:     } else {\n150:         if (rank == 0) {\n151:             if (soutbuf[0] != ((size % 2) ? (short) 0xff : (", "label": "(25, MPI, MPI_Comm_size(comm, &size);); (97, MPI, rc = MPI_Reduce(ucinbuf, ucoutbuf, 3, MPI_UNSIGNED_CHAR, MPI_BXOR, 0, comm);); (156, MPI, rc = MPI_Reduce(sinbuf, soutbuf, 3, MPI_SHORT, MPI_BXOR, 0, comm);); (125, MPI, rc = MPI_Reduce(cinbuf, coutbuf, 3, MPI_BYTE, MPI_BXOR, 0, comm);); (274, MPI, rc = MPI_Reduce(linbuf, loutbuf, 3, MPI_LONG, MPI_BXOR, 0, comm);); (63, MPI, rc = MPI_Reduce(scinbuf, scoutbuf, 3, MPI_SIGNED_CHAR, MPI_BXOR, 0, comm);); (31, MPI, rc = MPI_Reduce(cinbuf, coutbuf, 3, MPI_CHAR, MPI_BXOR, 0, comm);); (335, MPI, rc = MPI_Reduce(llinbuf, lloutbuf, 3, MPI_LONG_LONG, MPI_BXOR, 0, comm);); (14, MPI, MPI_Comm_set_errhandler(comm, MPI_ERRORS_RETURN);); (299, MPI, rc = MPI_Reduce(ulinbuf, uloutbuf, 3, MPI_UNSIGNED_LONG, MPI_BXOR, 0, comm);); (238, MPI, rc = MPI_Reduce(iinbuf, ioutbuf, 3, MPI_INT, MPI_BXOR, 0, comm);); (13, MPI, MPI_Comm_rank(comm, &rank);); (206, MPI, rc = MPI_Reduce(uinbuf, uoutbuf, 3, MPI_UNSIGNED, MPI_BXOR, 0, comm);); (175, MPI, rc = MPI_Reduce(usinbuf, usoutbuf, 3, MPI_UNSIGNED_SHORT, MPI_BXOR, 0, comm);)"}
{"predict": "(26,OpenMP,#pragma omp parallel for collapse(2))", "label": "(19, CUDA, ColorSpinorParam cudaParam(cpuParam, *inv_param, QUDA_CUDA_FIELD_LOCATION);)"}
{"predict": "l_self_test_set(&dev_ctx_xl, LSM303AH_XL_ST_DISABLE);\n143: \n144:   /* Disable Block Data Update. */\n145:   lsm303ah_xl_block_data_update_set(&dev_ctx_xl, PROPERTY_DISABLE);\n146:   lsm303ah_mg_block_data_update_set(&dev_ctx_mg, PROPERTY_DISABLE);\n147: \n148:   /* Output self test result */\n149:   tx_com(tx_buffer, strlen((char const *)tx_buffer));\n150: }\n151: \n152: /* End Example ---------------------------------------------------------------*/", "label": "(145, OpenMP, lsm303ah_mg_offset_temp_comp_set(&dev_ctx_mg, PROPERTY_ENABLE);)"}
{"predict": "(42,MPI,*w = MPI_Wtime();); (46,OpenMP,#pragma omp parallel for), (50,OpenMP,#pragma omp parallel for), (57,OpenMP,#pragma omp parallel for), (61,OpenMP,#pragma omp parallel for), (70,OpenMP,#pragma omp parallel for), (75,OpenMP,#pragma omp parallel for), (80,OpenMP,#pragma omp parallel for), (84,OpenMP,#pragma omp parallel for), (101,OpenMP,#pragma omp parallel for), (106,OpenMP,#pragma omp parallel for), (116,OpenMP,#pragma omp parallel for)", "label": "(87, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group\", 1);); (87, OpenMP, test_comp_vlen_strings(COMP_VL_STRS_FILE, \"group_copy\", 0);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for)", "label": "(22, CUDA, nvinfer1::ICudaEngine* engine = builder->buildCudaEngine(*network);)"}
{"predict": "(29,OpenMP,#pragma omp parallel for), (43,OpenMP,#pragma omp parallel for), (60,OpenMP,#pragma omp parallel for)", "label": "(58, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(streams[0]), &streams[0]);); (58, OpenCL, err |= clSetKernelArg(kernel, 1, sizeof(streams[1]), &streams[1]);); (58, OpenCL, err |= clSetKernelArg(kernel, 2, sizeof(sampler), &sampler);); (33, OpenCL, streams[1] = clCreateBuffer(context, CL_MEM_READ_WRITE, length, NULL, &err);)"}
{"predict": "(39,MPI,*w = MPI_Wtime();)", "label": "(41, CUDA, cudaDeviceSynchronize();)"}
{"predict": "(4,OpenMP,#pragma omp parallel for), (8,OpenMP,#pragma omp atomic), (12,OpenMP,#pragma omp atomic), (19,OpenMP,#pragma omp atomic), (23,OpenMP,#pragma omp atomic), (27,OpenMP,#pragma omp atomic), (30,OpenMP,#pragma omp atomic), (36,OpenMP,#pragma omp atomic), (41,OpenMP,#pragma omp atomic), (46,OpenMP,#pragma omp atomic), (51,OpenMP,#pragma omp atomic), (56,OpenMP,#pragma omp atomic)", "label": "(7, OpenMP, #pragma omp atomic)"}
{"predict": "(6,MPI,*w = MPI_Wtime();); (14,MPI,*w = MPI_Wtime();); (23,MPI,*w = MPI_Wtime(););", "label": "(21, CUDA, CUDA_SAFE_CALL(cudaMemcpy(&max, tempCfl, sizeof(float), cudaMemcpyDeviceToHost));)"}
{"predict": "1:             auto prob = Measurer.probs({0, 1, 2});\n142: \n143:             REQUIRE_THAT(prob_obs, Catch::Approx(prob).margin(1e-6));\n144:         }\n145: \n146:         DYNAMIC_SECTION(\"Test TensorProd XZY\"\n147:                         << StateVectorMPIToName<StateVectorT>::name) {\n148:             auto X0 = std::make_shared<NamedObsMPI<StateVectorT>>(\n149:                 \"PauliX\", std::vector<std::size_t>{0});\n150:             auto Z1 = std::make_shared<NamedObsMPI<StateVectorT>>(\n151:                 \"PauliZ\", std::vector<std::size_t>{1});\n152:             auto Y2 = std::make_shared<NamedObsMPI<StateVectorT>>(\n153:                 \"PauliY\", std::vector<std::size_t>{2});\n154:             auto obs = TensorProdObsMPI<StateVectorT>::create({X0, Z1, Y2});\n155: \n156:             MeasurementsMPI<StateVectorT> Measurer_obs(statevector);\n157: \n158:             sv.applyOperations({\"Hadamard\", \"PauliZ\", \"S\", \"Hadamard\"},\n159:                                {{0}, {2}, {2}, {2}},\n160:                                {false, false, false, false});\n161: \n162:             MeasurementsMPI<StateVectorT> Measurer(sv);\n163: \n164:             auto prob_obs = Measurer_obs.probs(*obs);\n165:             auto prob = Measurer.probs({0, 1, 2});\n166: \n167:             REQUIRE_THAT(prob_obs, Catch::Approx(prob).margin(1e-6));\n168:         }\n169:     }\n170: }", "label": "(24, CUDA, cudaSetDevice(deviceId);); (20, CUDA, cudaGetDeviceCount(&nDevices);); (9, MPI, MPIManager mpi_manager(MPI_COMM_WORLD);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for schedule(guided, argc) default(none) copyin(g) dist_schedule(static, a) private(a) shared(argc))", "label": "(5, OpenMP, #pragma omp threadprivate(g)); (21, OpenMP, #pragma omp distribute parallel for private(argc, b), firstprivate(argv, c), lastprivate(d, f) collapse(2) schedule(auto) if (argc) num_threads(a) default(shared) shared(e) reduction(+ : h) dist_schedule(static, b)); (7, OpenMP, #pragma omp distribute parallel for schedule(guided, argc) default(none) copyin(g) dist_schedule(static, a) private(a) shared(argc)); (4, OpenMP, #pragma omp target); (4, OpenMP, #pragma omp teams)"}
{"predict": "(12,MPI,MPI_Init(&argc, &argv);) (10,OpenMP,#pragma omp parallel for) (36,OpenMP,#pragma omp parallel for)", "label": "(11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &nproc);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (22, MPI, mpi_errno = MPI_Waitall(block, req, error_status);); (10, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (47, MPI, mpi_errno = MPI_Waitall(block, req, MPI_STATUSES_IGNORE);); (17, MPI, MPI_Comm_idup(MPI_COMM_WORLD, &comm_hdls[i], &req[block++]);); (28, MPI, mpi_errno = MPI_Wait(&req[j], MPI_STATUSES_IGNORE);)"}
{"predict": "(19,OpenMP,#pragma omp parallel for)", "label": "(13, TBB, double serial_time = (tbb::tick_count::now() - t0).seconds();); (10, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (19, TBB, double tbb_time = (tbb::tick_count::now() - t1).seconds();); (16, TBB, tbb::tick_count t1 = tbb::tick_count::now();)"}
{"predict": "(18,OpenMP,#pragma omp parallel for)", "label": "(3, CUDA, extern __shared__ double sdat[];)"}
{"predict": "17: \t\t\t\tfor (irow = ikfrow; irow < iklrow; ++irow){\n118: \n119: \t\t\t\t\tfor (j = usub[i]; j < usub[i+1]; ++j){\n120: \t\t\t\t\t\tjj = uval[j];\n121: \n122: \t\t\t\t\t\t// lsum[irow] -= U_i,k * X[k];\n123: \t\t\t\t\t\tlsum[irow] -= Uinv[jj] * xk[iknsupc];\n124: \n125: #if ( PROFlevel>=1 )\n116: #endif\n126: \n127: \n128: #if ( PROFlevel>=1 )\n117: #endif\n129: \n130: #if ( PROFlevel>=1 )\n118: #endif\n131: \n132: \n133: \n134: \n135: \n136: \n137: \n138: \n139: \n140: \n141: \n142: \n143: \n144: \n145: \n146: \n147: \n148: \n149: \n150: \n151: \n152: \n153: \n154: \n155: \n156: \n157: \n158: \n159: \n160: \n161: \n162: \n163: \n164: \n165: \n166: \n167: \n168: \n169: \n170: \n171: \n172: \n173: \n174: \n175: \n176: \n177: \n178: \n179: \n180: \n181: \n182: \n183: \n184: \n185: \n186: \n187: \n188: \n189: \n190: \n191: \n192: \n193: \n194: \n1", "label": "(82, OpenMP, #pragma\tomp\ttaskloop firstprivate (stat) private (thread_id1,Uinv,nn,lbstart,lbend,ub,temp,rtemp_loc,ik,lk1,gik,gikcol,usub,uval,lsub,lusup,iknsupc,il,i,irow,bmod_tmp,p,ii,jj,t1,t2,j,ikfrow,iklrow,dest,y,uptr,fnz,nsupr,nroot_send_tmp) untied nogroup); (126, OpenMP, #pragma omp simd); (141, OpenMP, #pragma omp atomic capture); (75, OpenMP, #pragma\tomp\tparallel for private (thread_id1,Uinv,nn,lbstart,lbend,ub,temp,rtemp_loc,ik,lk1,gik,gikcol,usub,uval,lsub,lusup,iknsupc,il,i,irow,bmod_tmp,p,ii,jj,t1,t2,j,ikfrow,iklrow,dest,y,uptr,fnz,nsupr))"}
{"predict": "(16,MPI,*w = MPI_Wtime();); (19,OpenMP,#pragma omp parallel for)", "label": "(8, CUDA, auto input   = thrust::host_vector<coordinate<T>>(d_input);)"}
{"predict": "(25,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel), (27,OpenMP,#pragma omp parallel), (30,OpenMP,#pragma omp parallel for), (36,OpenMP,#pragma omp parallel for)", "label": "(20, TBB, my_thread_array = tbb::cache_aligned_allocator<padded_ipc_worker>().allocate( my_n_thread );); (14, TBB, my_n_thread = tbb::detail::r1::AvailableHwConcurrency();); (25, TBB, my_waker = tbb::cache_aligned_allocator<ipc_waker>().allocate(1);); (27, TBB, my_stopper = tbb::cache_aligned_allocator<ipc_stopper>().allocate(1);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for), (42,OpenMP,#pragma omp parallel for), (52,OpenMP,#pragma omp parallel for), (62,OpenMP,#pragma omp parallel for), (70,OpenMP,#pragma omp parallel for), (75,OpenMP,#pragma omp parallel for), (81,OpenMP,#pragma omp parallel for)", "label": "(19, OpenMP, #pragma omp distribute parallel for); (17, OpenMP, #pragma omp target); (45, OpenMP, #pragma omp distribute parallel for num_threads(128)); (72, OpenMP, #pragma omp teams distribute parallel for thread_limit(512) num_threads(128)); (22, OpenMP, #pragma omp target teams thread_limit(64)); (76, OpenMP, #pragma omp teams distribute parallel for); (29, OpenMP, #pragma omp teams)"}
{"predict": "(20,OpenMP,#pragma omp parallel for), (78,OpenMP,#pragma omp parallel for), (80,OpenMP,#pragma omp parallel for)", "label": "(68, CUDA, checkCudaErrors(cudaMalloc((void **)&d_odata, numBlocks * sizeof(float)));); (105, CUDA, cudaFree(d_odata);); (65, CUDA, checkCudaErrors(cudaMalloc((void **)&d_idata, bytes));); (10, CUDA, checkCudaErrors(cudaGetDeviceProperties(&prop, device));); (8, CUDA, checkCudaErrors(cudaSetDevice(device));); (100, CUDA, cudaFree(d_idata);); (65, CUDA, checkCudaErrors(cudaMemcpy(d_idata, h_idata, bytes, cudaMemcpyHostToDevice));)"}
{"predict": "(14,MPI,res_dev = (float*)MPI_Alloc_mem(sizeof(float), MPI_INFO_NULL, MPI_COMM_WORLD);), (17,MPI,MPI_Get_count(status, MPI_FLOAT, &count);), (18,MPI,MPI_Recv(res_dev, count, MPI_FLOAT, source, tag, MPI_COMM_WORLD, &status);), (21,MPI,MPI_Free_mem(res_dev);)", "label": "(20, CUDA, cudaFree(res_dev);); (10, CUDA, cudaMalloc(&res_dev, sizeof(float));); (12, CUDA, cudaMemcpy(&res_host, res_dev, sizeof(float), cudaMemcpyDeviceToHost);)"}
{"predict": "(16,OpenMP,#pragma omp parallel for)", "label": "(2, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (2, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(7,OpenMP,#pragma omp parallel for reduction(+:my_gain))", "label": "(14, TBB, return tbb::parallel_reduce(range, 0, accum, std::plus<>());); (3, TBB, auto range = tbb::blocked_range<size_t>(UL(0), end);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (...)", "label": "(12, CUDA, cudaDeviceSynchronize();)"}
{"predict": "(16,OpenMP,#pragma omp parallel for), (29,OpenMP,#pragma omp parallel for)", "label": "(11, MPI, MPI_Type_size_x(type, &typesize);)"}
{"predict": "(3,MPI,pcu::PCU PCUObj = pcu::PCU(MPI_COMM_WORLD);); (10,OpenMP,#pragma omp parallel for), (13,OpenMP,#pragma omp parallel for)", "label": "(15, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "// Create agents\n134:         for (unsigned int i = 0; i < env.getProperty<unsigned int>(\"POPULATION_TO_GENERATE\"); i++) {\n135:             flamegpu::AgentInstance agentInstance = agent.newInstance();\n136:             agentInstance.setVariable<float>(\"x\", position_distribution(rngEngine));\n137:             agentInstance.setVariable<float>(\"y\", position_distribution(rngEngine));\n138:             agentInstance.setVariable<float>(\"z\", position_distribution(rngEngine));\n139:             agentInstance.setVariable<float>(\"fx\", velocity_distribution(rngEngine));\n140:             agentInstance.setVariable<float>(\"fy\", velocity_distribution(rngEngine));\n141:             agentInstance.setVariable<float>(\"fz\", velocity_distribution(rngEngine));\n142:             agentInstance.setVariable<float>(\"fx\", velocity_magnitude_distribution(rngEngine) * agentInstance.getVariable<float>(\"fx\") / agentInstance.getVariable<float>(\"fx\").magnitude());\n143:             agentInstance.setVariable<float>(\"fy\", velocity_magnitude_distribution(rngEngine) * agentInstance.getVariable<float>(\"fy\") / agentInstance.getVariable<float>(\"fy\").magnitude());\n144:             agentInstance.setVariable<float>(\"fz\", velocity_magnitude_distribution(rngEngine) * agentInstance.getVariable<float>(\"fz\") / agentInstance.getVariable<float>(\"fz\").magnitude());\n145:         }\n146:     }\n147: \n148:     // Run simulation\n149:     while (cudaSimulation.step()) {\n150:         cudaSimulation.step();\n151:     }\n152: \n153:     // Cleanup\n154:     cudaSimulation.cleanup();\n155:     return 0;\n156: }", "label": "(80, CUDA, flamegpu::CUDASimulation cudaSimulation(model);)"}
{"predict": "(21,MPI,status = H5Pset_fapl_mpio(fpid, MPI_COMM_WORLD, MPI_INFO_NULL););", "label": "(16, MPI, MPI_Comm_size(MPI_COMM_WORLD,&mpi_size);); (16, MPI, MPI_Comm_rank(MPI_COMM_WORLD,&mpi_rank);)"}
{"predict": "(26,OpenMP,#pragma omp parallel for)", "label": "(63, MPI, MPI_Unpack(check_pack_buf, pack_size, &pos, unpack_buf, 1, datatype, MPI_COMM_WORLD);); (6, MPI, MPI_Type_size_c(datatype, &type_size);); (34, MPI, MPI_Pack_size(1, datatype, MPI_COMM_WORLD, &pack_size);); (54, MPI, MPI_Pack(check_unpack_buf, 1, datatype, pack_buf, pack_size, &pos, MPI_COMM_WORLD);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (11,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for)", "label": "(4, OpenMP, #pragma omp parallel for)"}
{"predict": "element-- )\n133:                     {\n134:                         for( int i = 1; i < vectorSize; i++ )\n135:                         {\n136:                             pThreadData->m_input_ptr[ 0 ][ element * vectorSize + i ] = pThreadData->m_input_ptr[ 0 ][ element ];\n137:                             pThreadData->m_input_ptr[ 1 ][ element * vectorSize + i ] = pThreadData->m_input_ptr[ 1 ][ element ];\n138:                         }\n139:                     }\n140:                 }\n141:             }\n142: \n143:             test_error(err, \"Verification failed\");\n144: \n145:             if( err )\n146:                 error_count++;\n147:         }\n148:     }\n149: \n149:     return error_count;\n150: }", "label": "(113, OpenCL, err = clEnqueueWriteBuffer( queue, pThreadData->m_streams[1], CL_FALSE, 0, pThreadData->m_type_size*num_elements * inputBVecSize, (void *)pThreadData->m_input_ptr[1], 0, NULL, NULL );); (110, OpenCL, err = clEnqueueWriteBuffer(queue, pThreadData->m_streams[0], CL_FALSE, 0, pThreadData->m_type_size*num_elements * inputAVecSize, (void *)pThreadData->m_input_ptr[0], 0, NULL, NULL);); (117, OpenCL, err = clEnqueueReadBuffer( queue, pThreadData->m_streams[2], CL_TRUE, 0, pThreadData->m_type_size*num_elements * vectorSize, (void *)pThreadData->m_output_ptr, 0, NULL, NULL );); (113, OpenCL, err = clEnqueueNDRangeKernel( queue, pThreadData->m_kernel[i], 1, NULL, threads, NULL, 0, NULL, NULL );)"}
{"predict": "endif\n157:   }\n158: \n159:   // clean up\n160:   delete [] in;\n161:   delete [] out;\n162: \n163:   return 0;\n164: }", "label": "(114, OpenMP, if (iter==1) stencil_time = omp_get_wtime();); (59, OpenMP, std::cout << \"Number of threads (max)   = \" << omp_get_max_threads() << std::endl;); (125, OpenMP, stencil_time = omp_get_wtime() - stencil_time;)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for collapse(4)), (33,OpenMP,#pragma omp parallel for collapse(4)), (41,OpenMP,#pragma omp parallel for collapse(4)), (66,OpenMP,#pragma omp parallel for collapse(4)), (70,OpenMP,#pragma omp parallel for collapse(4))", "label": "(36, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (83, CUDA, cudaFree(d_in);); (83, CUDA, cudaFree(d_out);); (32, CUDA, cudaMalloc((void**)(&d_in), in_bytes);); (45, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (33, CUDA, cudaMemcpy(d_in, tensor.data(), in_bytes, cudaMemcpyHostToDevice);); (42, CUDA, assert(cudaMemcpyAsync(tensor_arg.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(30, MPI, MPI_Send(MPI_BOTTOM, 1, newtype, 1, 0, MPI_COMM_WORLD);); (23, MPI, MPI_Type_commit(&newtype);); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (20, MPI, MPI_Type_create_struct(2, lens, displs, types, &newtype);); (15, MPI, MPI_Get_address(&count, &displs[0]);); (27, MPI, MPI_Recv(buf, 6, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (35, MPI, MPI_Type_free(&newtype);); (15, MPI, types[1] = MPI_Type_f2c(*ftype);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for), (35,OpenMP,#pragma omp parallel for)", "label": "(11, OpenMP, #pragma omp taskloop simd linear(f) linear(f)); (54, OpenMP, #pragma omp taskloop simd linear(ref(j))); (35, OpenMP, #pragma omp taskloop simd linear(a, b)); (26, OpenMP, #pragma omp taskloop simd linear (argc > 0 ? argv[1] : argv[2])); (27, OpenMP, #pragma omp taskloop simd linear (argc, z)); (10, OpenMP, #pragma omp taskloop simd linear (); (39, OpenMP, #pragma omp taskloop simd linear(h, C::x)); (16, OpenMP, #pragma omp taskloop simd linear (foo())); (49, OpenMP, #pragma omp taskloop simd linear(i)); (38, OpenMP, #pragma omp parallel); (17, OpenMP, #pragma omp taskloop simd linear (argc,); (2, OpenMP, #pragma omp taskloop simd linear); (13, OpenMP, #pragma omp taskloop simd linear (argc); (37, OpenMP, #pragma omp taskloop simd linear(val(i))); (19, OpenMP, #pragma omp taskloop simd linear (S1)); (24, OpenMP, #pragma omp taskloop simd linear (argv[1])); (3, OpenMP, #pragma omp taskloop simd linear (val); (4, OpenMP, #pragma omp taskloop simd linear (ref())); (34, OpenMP, #pragma omp taskloop simd linear(uval(i) : 4)); (24, OpenMP, #pragma omp taskloop simd linear(val(e, g))); (-3, OpenMP, #pragma omp taskloop simd linear ())"}
{"predict": "(1,OpenMP,#pragma omp parallel)", "label": "(3, OpenMP, #pragma omp taskyield untied); (0, OpenMP, #pragma omp taskyield); (2, OpenMP, #pragma omp taskyield unknown)"}
{"predict": "(14,OpenMP,#pragma omp parallel for)", "label": "(9, CUDA, cudaFuncGetAttributes(&attr,(void*)ptr);); (14, CUDA, fcuda::Check_CudaErroorFun(\"Error collecting kernel information.\");); (5, CUDA, cudaOccupancyMaxPotentialBlockSize(&mingridsize,&qblocksize,(void*)ptr,0,0);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for), (50,OpenMP,#pragma omp parallel for), (71,OpenMP,#pragma omp parallel for), (80,OpenMP,#pragma omp parallel for)", "label": "(52, TBB, tbb::flow::make_edge( b2, b3 );); (50, TBB, tbb::flow::make_edge( b, b2 );); (73, TBB, tbb::flow::buffer_node<T> b_copy(b);); (2, TBB, tbb::flow::buffer_node<T> b3(g);); (0, TBB, tbb::flow::buffer_node<T> b2(g);); (-3, TBB, tbb::flow::graph g;); (-3, TBB, tbb::flow::buffer_node<T> b(g);)"}
{"predict": "PRIu64 \" %\" PRIu64 \"\\n\",\n153:                          inputA[i], inputB[i], inputC[i]);\n154:                 log_error(\"-> expected: %\" PRIu64 \"\\n\", r);\n155:                 log_error(\"-> got: %\" PRIu64 \"\\n\", output[i]);\n156:                 return TEST_FAIL;\n157:             }\n158:         }\n159:         log_info(\"passed\\n\");\n160:     }\n161: \n162:     // Cleanup\n163:     for (size_t i = 0; i < ARRAY_SIZE(streams); i++)\n164:     {\n165:         clReleaseMemObject(streams[i]);\n166:     }\n167: \n168:     return TEST_PASS;\n169: }", "label": "(132, OpenCL, err |= clSetKernelArg(kernel, 1, sizeof(cl_mem), &streams[1]);); (132, OpenCL, err |= clSetKernelArg(kernel, 2, sizeof(cl_mem), &streams[2]);); (132, OpenCL, err |= clSetKernelArg(kernel, 3, sizeof(cl_mem), &streams[3]);); (128, OpenCL, err = clSetKernelArg(kernel, 0, sizeof(cl_mem), &streams[0]);)"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(19, OpenMP, #pragma omp parallel default(shared)); (11, OpenMP, #pragma omp parallel); (38, OpenMP, #pragma omp parallel num_threads(x)); (23, OpenMP, #pragma omp parallel default(none)); (29, OpenMP, #pragma omp parallel default(firstprivate))"}
{"predict": "(4,OpenMP,#pragma omp parallel for collapse(2)), (10,OpenMP,#pragma omp parallel for collapse(2)), (16,OpenMP,#pragma omp parallel for collapse(2))", "label": "(24, TBB, range_type r2(r,tbb::split());); (2, TBB, typedef tbb::blocked_range2d<row_type,col_type> range_type;); (18, TBB, AssertSameType( r.cols(), tbb::blocked_range<col_type>( coli, colj, 1 ));); (16, TBB, AssertSameType( r.rows(), tbb::blocked_range<row_type>( rowi, rowj, 1 ));)"}
{"predict": "(8,OpenMP,#pragma omp parallel for reduction(+:sum0)), (10,OpenMP,#pragma omp parallel for reduction(+:sum1)), (22,OpenMP,#pragma omp atomic), (24,OpenMP,#pragma omp critical)", "label": "(13, OpenMP, #pragma omp for private(sum0) schedule(static,1)); (7, OpenMP, #pragma omp parallel); (22, OpenMP, #pragma omp critical); (13, OpenMP, #pragma omp flush)"}
{"predict": "3:     int iy_end_global = iy_start_global + chunk_size;\n74:     int iy_start_local = 1;  // Local array starts at 1 to accommodate ghost cells\n75:     int iy_end_local = chunk_size + 1;  // Local array ends at chunk_size + 1 to accommodate ghost cells\n76: \n77:     // Initialize a_ref_h and a_h\n78:     init_arrays(nx, ny, a_ref_h, a_h, iy_start_global, iy_end_global);\n79: \n80:     // Run the simulation\n81:     for (int iter = 0; iter < iter_max; ++iter) {\n82:         real* a_temp = a;\n83:         a = a_new;\n84:         a_new = a_temp;\n85: \n86:         // Boundary condition on y=0 and y=ny-1\n87:         // Copy values from a_ref_h to a\n88:         if (rank == 0) {\n89:             for (int ix = 0; ix < nx; ++ix) {\n90:                 a[ix] = a_ref_h[ix];\n91:             }\n92:         }\n93:         if (rank == size - 1) {\n94:             for (int ix = 0; ix < nx; ++ix) {\n95:                 a[ix + nx * (chunk_size + 1)] = a_ref_h[ix + nx * (ny - 1)];\n96:             }\n97:         }\n98: \n99:         // Perform the computation\n100:         for (int iy = iy_start_local; iy < iy_end_local; ++iy) {\n101:             for (int ix = 0; ix < nx; ++ix) {\n102:                 a_new[ix + nx * iy] = compute(a[ix + nx * (iy - 1)], a[ix + nx * iy], a[ix + nx * (iy + 1)]);\n103:             }\n104:         }\n105: \n106:         // Copy values from a to a_ref_h\n107:         if (rank == 0) {\n108:             for (int ix = 0; ix < nx; ++ix) {\n10", "label": "(118, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&reset_l2norm_done, cudaEventDisableTiming));); (143, MPI, MPI_CALL(MPI_Barrier(MPI_COMM_WORLD));); (21, MPI, MPI_CALL(MPI_Comm_free(&local_comm));); (249, CUDA, CUDA_RT_CALL(cudaStreamDestroy(compute_stream));); (201, MPI, double stop = MPI_Wtime();); (244, CUDA, CUDA_RT_CALL(cudaEventDestroy(push_done));); (257, CUDA, CUDA_RT_CALL(cudaFree(a));); (-4, MPI, MPI_CALL(MPI_Comm_rank(MPI_COMM_WORLD, &rank));); (94, CUDA, CUDA_RT_CALL(cudaDeviceSynchronize());); (18, CUDA, CUDA_RT_CALL(cudaFree(0));); (16, CUDA, CUDA_RT_CALL(cudaSetDevice(local_rank%num_devices));); (58, CUDA, CUDA_RT_CALL(cudaMalloc(&a, nx * (chunk_size + 2) * sizeof(real)));); (243, CUDA, CUDA_RT_CALL(cudaFree(l2_norm_d));); (138, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(push_stream, reset_l2norm_done, 0));); (100, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&push_prep_done, cudaEventDisableTiming));); (28, CUDA, CUDA_RT_CALL(cudaMallocHost(&a_h, nx * ny * sizeof(real)));); (25, CUDA, CUDA_RT_CALL(cudaMallocHost(&a_ref_h, nx * ny * sizeof(real)));); (84, CUDA, CUDA_RT_CALL(cudaGetLastError());); (145, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(compute_stream, push_prep_done, 0));); (89, CUDA, CUDA_RT_CALL(cudaStreamCreateWithPriority(&compute_stream, cudaStreamDefault, leastPriority));); (51, CUDA, CUDA_RT_CALL(cudaMalloc(&a_new, nx * (chunk_size + 2) * sizeof(real)));); (230, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_stream));); (-17, MPI, MPI_CALL(MPI_Comm_size(MPI_COMM_WORLD, &size));); (170, CUDA, CUDA_RT_CALL(cudaStreamSynchronize(compute_stream));); (125, CUDA, CUDA_RT_CALL(cudaEventRecord(reset_l2norm_done, compute_stream));); (-18, CUDA, CUDA_RT_CALL(cudaGetDeviceCount(&num_devices));); (102, CUDA, CUDA_RT_CALL(cudaMallocHost(&l2_norm_h, sizeof(real)));); (121, CUDA, CUDA_RT_CALL(cudaMemsetAsync(l2_norm_d, 0, sizeof(real), compute_stream));); (-27, MPI, MPI_CALL(MPI_Init(&argc, &argv));); (46, CUDA, CUDA_RT_CALL(cudaMemset(a_new, 0, nx * (chunk_size + 2) * sizeof(real)));); (76, CUDA, CUDA_RT_CALL(cudaDeviceGetStreamPriorityRange(&leastPriority, &greatestPriority));); (219, CUDA, CUDA_RT_CALL(cudaEventDestroy(push_prep_done));); (94, CUDA, CUDA_RT_CALL(cudaMalloc(&l2_norm_d, sizeof(real)));); (-12, MPI, MPI_CALL(MPI_Comm_rank(local_comm, &local_rank));); (233, CUDA, CUDA_RT_CALL(cudaFreeHost(a_ref_h));); (159, MPI, MPI_CALL(MPI_Allreduce(l2_norm_h, &l2_norm, 1, MPI_REAL_TYPE, MPI_SUM, MPI_COMM_WORLD));); (109, MPI, double start = MPI_Wtime();); (79, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&push_done, cudaEventDisableTiming));); (210, CUDA, CUDA_RT_CALL(cudaEventDestroy(reset_l2norm_done));); (215, CUDA, CUDA_RT_CALL(cudaFreeHost(l2_norm_h));); (120, CUDA, CUDA_RT_CALL(cudaEventRecord(push_prep_done, push_stream));)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (13,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for), (39,OpenMP,#pragma omp parallel for), (42,OpenMP,#pragma omp parallel for)", "label": "(30, TBB, tbb::combinable<T> finit_combinable(my_finit_decl);); (32, TBB, tbb::parallel_for( tbb::blocked_range<int>( 0, N, 10000 ), ParallelScalarBodyNoInit<T>( finit_combinable ) );); (24, TBB, if (Verbose && t == 0) t0 = tbb::tick_count::now();); (31, TBB, tbb::parallel_for( tbb::blocked_range<int>( 0, N, 10000 ), ParallelScalarBody<T>( sums ) );); (40, TBB, tbb::combinable<T> assigned;); (-2, TBB, tbb::task_scheduler_init init(tbb::task_scheduler_init::deferred);); (7, TBB, tbb::tick_count t0;); (21, TBB, tbb::combinable<T> sums;)"}
{"predict": "(14,MPI,*w = MPI_Wtime();)\n(17,OpenMP,#pragma omp parallel for)\n(32,OpenMP,#pragma omp parallel for)\n(48,OpenMP,#pragma omp parallel for)\n(59,OpenMP,#pragma omp parallel for)\n(70,OpenMP,#pragma omp parallel for)", "label": "(12, OpenMP, omp_set_num_threads(nr_threads);); (13, OpenMP, #pragma omp parallel default(none) shared(a)); (14, OpenMP, #pragma omp sections); (8, OpenMP, omp_set_dynamic(0);); (14, OpenMP, #pragma omp section); (16, OpenMP, int thread_nr = omp_get_thread_num();)"}
{"predict": "(6,MPI,*w = MPI_Wtime();); (15,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for), (68,OpenMP,#pragma omp atomic)", "label": "(12, CUDA, extern __shared__ float _shared_centroids[];)"}
{"predict": "(46,OpenMP,#pragma omp parallel for collapse(2)), (75,OpenMP,#pragma omp parallel for collapse(2)), (91,OpenMP,#pragma omp parallel for collapse(2))", "label": "(21, CUDA, extern __shared__ float entire_buffer[];)"}
{"predict": "(9,OpenMP,#pragma omp parallel for reduction(+:a))", "label": "(5, OpenMP, #pragma omp flush acquire); (5, OpenMP, #pragma omp flush release); (1, OpenMP, #pragma omp flush); (4, OpenMP, #pragma omp flush(a)); (0, OpenMP, #pragma omp flush acq_rel)"}
{"predict": "(26,OpenMP,#pragma omp parallel for collapse(2))", "label": "(43, CUDA, CUDA_CHECK(cudaMemcpy(d_A_inv, d_A, sizeof(data_type) * lda * n, cudaMemcpyDeviceToDevice));); (47, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (69, CUDA, CUDA_CHECK(cudaFree(d_info));); (34, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A), sizeof(data_type) * lda * n));); (34, CUDA, CUDA_CHECK(cudaMalloc(reinterpret_cast<void **>(&d_A_inv), sizeof(data_type) * lda * n));); (65, CUDA, CUDA_CHECK(cudaFree(d_A_inv));); (58, CUDA, CUDA_CHECK(cudaStreamDestroy(stream));); (32, CUDA, CUDA_CHECK(cudaMalloc(&d_info, sizeof(int)));); (34, CUDA, CUDA_CHECK(cudaMemcpy(d_A, h_A, sizeof(data_type) * lda * n, cudaMemcpyHostToDevice));); (42, CUDA, CUDA_CHECK(cudaMemcpy(&h_info, d_info, sizeof(int), cudaMemcpyDeviceToHost));); (34, CUDA, CUDA_CHECK(cudaMemset(d_info, 0, sizeof(int)));); (58, CUDA, CUDA_CHECK(cudaFree(d_A));); (20, CUDA, CUDA_CHECK(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));)"}
{"predict": "(15,MPI,int Q = blockIdx.x;); (17,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for), (52,OpenMP,#pragma omp parallel for), (59,OpenMP,#pragma omp parallel for)", "label": "(31, CUDA, thrust::complex<T> const* v1_(v1 + Q * nchol_max * nwalk);); (31, CUDA, thrust::complex<T> const* v2_(v1 + Qm * nchol_max * nwalk);); (52, CUDA, thrust::complex<T2> ialpha(alpha * thrust::complex<T2>(0.0, 1.0));); (27, CUDA, thrust::complex<T2>* vb_(vb + nc0 * nwalk);); (46, CUDA, thrust::complex<T2>* vb_(vb + (nc0 + nc) * nwalk);)"}
{"predict": "(56,OpenMP,#pragma omp parallel for private(nx, nz, state_tmp, flux, tend, dt)), (77,OpenMP,#pragma omp atomic add(output_counter, dt))", "label": "(83, OpenACC, #pragma acc update host(state[(nz + 2 * hs) * (nx + 2 * hs) * NUM_VARS])); (45, OpenACC, #pragma acc data copyin(state_tmp[(nz + 2 * hs) * (nx + 2 * hs) * NUM_VARS], hy_dens_cell[nz + 2 * hs], hy_dens_theta_cell[nz + 2 * hs], hy_dens_int[nz + 1], hy_dens_theta_int[nz + 1], hy_pressure_int[nz + 1]))"}
{"predict": "(10,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for)", "label": "(32, MPI, CALL(MPI_Waitall(static_cast<int>(reqs.size()), reqs.data(), MPI_STATUSES_IGNORE));); (20, MPI, CALL(MPI_Waitall(reqs.size(), reqs.data(), MPI_STATUSES_IGNORE));)"}
{"predict": "(16,OpenMP,#pragma omp parallel for)", "label": "(28, MPI, total_my_bcast_time += MPI_Wtime();); (31, MPI, total_mpi_bcast_time -= MPI_Wtime();); (45, MPI, MPI_Finalize();); (32, MPI, total_mpi_bcast_time += MPI_Wtime();); (5, MPI, MPI_Init(NULL, NULL);); (28, MPI, MPI_Bcast(data, num_elements, MPI_INT, 0, MPI_COMM_WORLD);); (6, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);); (17, MPI, total_my_bcast_time -= MPI_Wtime();); (15, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(13,Shared Memory,As[threadIdx.y][threadIdx.x] = A[idy*ds + idx];) (14,Shared Memory,Bs[threadIdx.y][threadIdx.x] = B[idy*ds + idx];) (21,Shared Memory,temp += As[threadIdx.y][threadIdx.x] * Bs[threadIdx.y][threadIdx.x];)", "label": "(4, CUDA, __shared__ float Bs[block_size][block_size];); (2, CUDA, __shared__ float As[block_size][block_size];)"}
{"predict": "(17,OpenMP,#pragma omp parallel for)", "label": "(11, MPI, MPI_Type_get_extent(item, &lb, &extent);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for), (46,OpenMP,#pragma omp parallel for)", "label": "(46, CUDA, env.engine.reset(runtime->deserializeCudaEngine(env.serializedEngine->data(), env.serializedEngine->size()));); (19, CUDA, auto profileStream = samplesCommon::makeCudaStream();)"}
{"predict": "(15,OpenMP,#pragma omp parallel for)", "label": "(28, CUDA, cudaFree( p );)"}
{"predict": "(22,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for), (34,OpenMP,#pragma omp parallel for), (38,OpenMP,#pragma omp parallel for)", "label": "(49, OpenMP, #pragma omp target teams is_device_ptr(da)); (23, OpenMP, #pragma omp target teams is_device_ptr(z)); (27, OpenMP, #pragma omp target teams is_device_ptr(aa)); (16, OpenMP, #pragma omp target teams is_device_ptr(k)); (30, OpenMP, #pragma omp target teams is_device_ptr(raa)); (39, OpenMP, #pragma omp target teams is_device_ptr(rh)); (33, OpenMP, #pragma omp target teams is_device_ptr(h))"}
{"predict": "(33, MPI, MPI_Init(NULL, NULL);)\n(34, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)\n(35, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)\n(36, MPI, if (rank == 0) { CONDITIONAL_PRINT_HERE(debug(), \"%s (size: %i)\", \"Getting MPI measurements\", size); })\n(37, MPI, else { CONDITIONAL_PRINT_HERE(debug(), \"%s\", \"Getting serial measurement\"); })\n(51, MPI, MPI_Barrier(MPI_COMM_WORLD);)\n(103, MPI, MPI_Finalize();)", "label": "(35, MPI, _measurements = get_measure()->mpi_get();)"}
{"predict": "(14,MPI,MPI_Init_thread(&argc, &argv, required, &provided);)", "label": "(42, MPI, MPI_T_finalize();); (17, MPI, MPI_T_init_thread(required, &provided);)"}
{"predict": "(1, MPI, *w = MPI_Wtime();); (8, OpenMP, #pragma omp parallel for), (22, MPI, MPI_Allreduce(&prod, &y[0], 1, MPI_FLOAT, MPI_SUM, MPI_COMM_WORLD);)", "label": "(5, CUDA, __shared__ float reduce_smem[NUM_WARPS];)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (10,OpenMP,#pragma omp parallel for reduction(+:inversions)), (17,OpenMP,#pragma omp parallel for)", "label": "(3, TBB, tbb::parallel_for( tbb::blocked_range<int>(0,m,1000), GrowBy(v) );)"}
{"predict": "(44,OpenMP,#pragma omp parallel for), (46,OpenMP,#pragma omp parallel reduction(+ : fl)), (49,OpenMP,#pragma omp parallel reduction(+ : fl))", "label": "(54, OpenMP, #pragma omp parallel reduction(&& : S2::S2sc)); (25, OpenMP, #pragma omp parallel reduction(foo : argc); (44, OpenMP, #pragma omp parallel reduction(* : ca)); (78, OpenMP, #pragma omp for reduction(- : fl)); (28, OpenMP, #pragma omp parallel reduction(~ : argc)); (53, OpenMP, #pragma omp parallel reduction(+ : h, k, B::x)); (82, OpenMP, #pragma omp parallel reduction(task, + : m)); (79, OpenMP, #pragma omp parallel reduction(+ : m)); (36, OpenMP, #pragma omp parallel reduction(+ : ba)); (39, OpenMP, #pragma omp parallel reduction(- : da)); (59, OpenMP, #pragma omp parallel reduction(+ : r)); (5, OpenMP, #pragma omp parallel reduction(); (50, OpenMP, #pragma omp parallel private(i), reduction(+ : j), reduction(+ : q)); (47, OpenMP, #pragma omp parallel reduction(+ : o)); (-2, OpenMP, #pragma omp parallel); (37, OpenMP, #pragma omp parallel reduction(&& : S2::S2s)); (2, OpenMP, #pragma omp parallel reduction(-); (17, OpenMP, #pragma omp parallel reduction(&& : argc, z)); (-4, OpenMP, #pragma omp parallel reduction); (52, OpenMP, #pragma omp parallel shared(i)); (44, OpenMP, #pragma omp parallel private(k)); (21, OpenMP, #pragma omp parallel reduction(max : h.b)); (6, OpenMP, #pragma omp parallel reduction(| : argc,); (15, OpenMP, #pragma omp parallel reduction(+ : a, b, c, d, f)); (49, OpenMP, #pragma omp parallel reduction(max : j)); (5, OpenMP, #pragma omp parallel reduction(|| : argc > 0 ? argv[1] : argv[2])); (14, OpenMP, #pragma omp parallel reduction(min : a, b, c, d, f)); (45, OpenMP, #pragma omp parallel reduction(min : i)); (-4, OpenMP, #pragma omp parallel reduction(\\)); (-9, OpenMP, #pragma omp parallel reduction()); (60, OpenMP, #pragma omp parallel reduction(+:gptr[:argc])); (45, OpenMP, #pragma omp for private(fl)); (-10, OpenMP, #pragma omp parallel reduction(*)); (17, OpenMP, #pragma omp parallel reduction(^ : fl)); (2, OpenMP, #pragma omp parallel reduction(^ : S1)); (30, OpenMP, #pragma omp parallel reduction(+ : p), reduction(+ : p)); (20, OpenMP, #pragma omp parallel reduction(& : e, g))"}
{"predict": "(13,OpenMP,#pragma omp parallel for)", "label": "(10, TBB, static const size_t number_of_threads = 4 * tbb::task_scheduler_init::default_num_threads();)"}
{"predict": "(11,MPI,*w = MPI_Wtime(););\n(12,OpenMP,#pragma omp parallel for),\n(20,OpenMP,#pragma omp parallel for),\n(25,OpenMP,#pragma omp parallel for),\n(30,OpenMP,#pragma omp parallel for),", "label": "(8, CUDA, cuda_safe_call(cudaMalloc((void**) &dX, N * sizeof(double)));); (8, CUDA, cuda_safe_call(cudaMalloc((void**) &dY, N * sizeof(double)));); (8, CUDA, cuda_safe_call(cudaMalloc((void**) &dZ, N * sizeof(double)));); (11, CUDA, cuda_safe_call(cudaDeviceSynchronize());)"}
{"predict": "(10,OpenMP,#pragma omp parallel for private(errs)), (20,OpenMP,#pragma omp critical)", "label": "(8, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);)"}
{"predict": "}\n117: \n118: }\n\n### ** Analysis **\n\nTo optimize the given C/C++ code for parallel execution, we need to identify sections of the code that can be executed concurrently. The code involves GPU operations and memory transfers, which can be parallelized to improve performance.\n\n### ** Recommendations **\n\n1. **Parallelize the outer loop over `i`**: This loop can be parallelized using OpenMP or MPI to distribute the workload across multiple GPUs.\n2. **Parallelize the inner loop over `j`**: This loop can also be parallelized using OpenMP or MPI to distribute the workload across multiple GPUs.\n3. **Parallelize the `performP2PCopy` calls**: These calls can be parallelized using OpenMP or MPI to perform multiple memory copies concurrently.\n\n### ** Parallelization Instructions **\n\n(22,OpenMP,#pragma omp parallel for private(j)), (24,OpenMP,#pragma omp parallel for private(access)), (28,OpenMP,#pragma omp parallel for private(access)), (59,OpenMP,#pragma omp parallel for private(access)), (64,OpenMP,#pragma omp parallel for private(access))\n\n### ** Final Code **\n\n```cpp\n1: void outputBidirectionalBandwidthMatrix(int numElems, int numGPUs, bool p2p) {\n2:   int repeat = 5;\n3:   volatile int *flag = NULL;\n4:   vector<int *> buffers(numGPUs);\n5:   vector<int *> buffersD2D(numGPUs);\n6:   vector<cudaEvent_t> start(numGPUs);\n7:   vector<cudaEvent_t> stop(numGPUs);\n8:   vector<cudaStream_t> stream0(numGPUs);\n9:   vector<cudaStream_t> stream1(numGPUs);\n10: \n11: \n12:   for (int d = 0; d < numGPUs; d++) {\n13:     cudaCheckError();\n14:     cudaCheckError();\n15:     cudaCheckError();\n16:     cudaCheckError();\n17:     cudaCheckError();\n18:   }\n19: \n20:   vector<double> bandwidthMatrix(numGPUs * numGPUs);\n21: \n22:   for (int i = 0; i < numGPUs; i++) {\n23: \n24:     for (int j = 0", "label": "(147, CUDA, cudaStreamDestroy(stream0[d]);); (16, CUDA, cudaMalloc(&buffersD2D[d], numElems * sizeof(int));); (141, CUDA, cudaEventDestroy(start[d]);); (41, CUDA, cudaDeviceEnablePeerAccess(i, 0);); (7, CUDA, cudaCheckError();); (136, CUDA, cudaFree(buffersD2D[d]);); (35, CUDA, cudaDeviceEnablePeerAccess(j, 0);); (138, CUDA, cudaEventDestroy(stop[d]);); (25, CUDA, cudaSetDevice(i);); (106, CUDA, cudaDeviceDisablePeerAccess(i);); (139, CUDA, cudaStreamDestroy(stream1[d]);); (83, CUDA, cudaEventRecord(stop[i], stream0[i]);); (10, CUDA, cudaEventCreate(&stop[d]);); (140, CUDA, cudaFreeHost((void *)flag);); (79, CUDA, cudaStreamWaitEvent(stream0[i], stop[j], 0);); (3, CUDA, cudaMemset(buffersD2D[d], 0, numElems * sizeof(int));); (35, CUDA, cudaStreamSynchronize(stream1[j]);); (52, CUDA, cudaStreamWaitEvent(stream1[j], start[i], 0);); (-2, CUDA, cudaMemset(buffers[d], 0, numElems * sizeof(int));); (-4, CUDA, cudaMalloc(&buffers[d], numElems * sizeof(int));); (-10, CUDA, cudaHostAlloc((void **)&flag, sizeof(*flag), cudaHostAllocPortable);); (119, CUDA, cudaFree(buffers[d]);); (28, CUDA, cudaStreamSynchronize(stream0[i]);); (-9, CUDA, cudaSetDevice(d);); (0, CUDA, cudaStreamCreateWithFlags(&stream0[d], cudaStreamNonBlocking);); (13, CUDA, cudaDeviceCanAccessPeer(&access, i, j);); (42, CUDA, cudaEventRecord(start[i], stream0[i]);); (16, CUDA, cudaSetDevice(j);); (64, CUDA, cudaEventRecord(stop[j], stream1[j]);); (74, CUDA, cudaEventElapsedTime(&time_ms, start[i], stop[i]);); (-10, CUDA, cudaEventCreate(&start[d]);); (-5, CUDA, cudaStreamCreateWithFlags(&stream1[d], cudaStreamNonBlocking);); (81, CUDA, cudaDeviceDisablePeerAccess(j);)"}
{"predict": "(15,OpenMP,#pragma omp parallel for private(i))", "label": "(5, OpenACC, #pragma acc parallel loop)"}
{"predict": "(4,OpenMP,#pragma omp parallel for), (8,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for)", "label": "(10, OpenMP, #pragma omp master taskloop collapse (argc); (1, OpenMP, #pragma omp master taskloop collapse); (13, OpenMP, #pragma omp master taskloop collapse (1)); (14, OpenMP, #pragma omp master taskloop collapse ((ST > 0) ? 1 + ST : 2)); (31, OpenMP, #pragma omp master taskloop collapse (N)); (32, OpenMP, #pragma omp master taskloop collapse (2)); (16, OpenMP, #pragma omp master taskloop collapse (foobool(argc)), collapse (true), collapse (-5)); (-3, OpenMP, #pragma omp master taskloop collapse (); (-2, OpenMP, #pragma omp master taskloop collapse ()); (15, OpenMP, #pragma omp master taskloop collapse (S)); (3, OpenMP, #pragma omp master taskloop collapse (ST)"}
{"predict": "(12,OpenMP,#pragma omp parallel for reduction(+:sum_f16)), (19,OpenMP,#pragma omp parallel for reduction(+:sum))", "label": "(5, CUDA, __shared__ float reduce_smem[NUM_WARPS];)"}
{"predict": "A_K0 is expected to be major, but was not in the provided input!\\n\";\n101:     return 1;\n102:   }\n103: \n104:   if (example::is_left_major<ColModeStridesB>() && (ldBn[0] != 1)) {\n105:     std::cerr << \"ERROR: B_N0 is expected to be major, but was not in the provided input!\\n\";\n106:     return 1;\n107:   }\n108: \n109:   if (example::is_left_major<RedModeStridesB>() && (ldBk[0] != 1)) {\n110:     std::cerr << \"ERROR: B_K0 is expected to be major, but was not in the provided input!\\n\";\n111:     return 1;\n112:   }\n113: \n114:   //\n115:   // Initialize the tensors\n116:   //\n117:   Tensor<ElementA, Layout<RowModeStridesA, RedModeStridesA, BatModeStridesA>> A(\n118:       ldAm, ldAm[0] * ldAm[1], m.data(), m.data());\n119:   Tensor<ElementB, Layout<ColModeStridesB, RedModeStridesB, BatModeStridesB>> B(\n120:       ldBn, ldBn[0] * ldBn[1], n.data(), n.data());\n121:   Tensor<ElementC, Layout<RowModeStridesC, ColModeStridesC, BatModeStridesC>> C(\n122:       ldCm, ldCm[0] * ldCm[1], c.data(), c.data());\n123:   Tensor<ElementD, Layout<RowModeStridesC, ColModeStridesC, BatModeStridesC>> D(\n124:       ldCn, ldCn[0] * ldCn[1], d.data(), d.data());\n125: \n126:   //\n127:   // Perform the GEMM operation\n128:   //\n129:   GEMMConfig gemm_config{MaxRank_M, MaxRank_N, Max", "label": "(161, CUDA, thrust::device_vector<ElementA> d_A = h_A;); (183, CUDA, cuda_err = cudaDeviceSynchronize();); (148, CUDA, thrust::host_vector<ElementB> h_B(N_size * K_size * L_size);); (184, CUDA, std::cerr << cudaGetErrorString(cuda_err) << \"\\n\";); (148, CUDA, thrust::host_vector<ElementD> h_D(M_size * N_size * L_size);); (157, CUDA, thrust::device_vector<ElementB> d_B = h_B;); (158, CUDA, thrust::device_vector<ElementD> cutlass_result = h_D;); (144, CUDA, thrust::host_vector<ElementC> h_C(M_size * N_size * L_size);); (213, CUDA, thrust::host_vector<ElementD> h_cutlass_result = cutlass_result;); (156, CUDA, thrust::device_vector<ElementD> reference_result = h_D;); (153, CUDA, thrust::device_vector<ElementC> d_C = h_C;); (138, CUDA, thrust::host_vector<ElementA> h_A(M_size * K_size * L_size);)"}
{"predict": "(16,OpenMP,#pragma omp parallel for)", "label": "(18, CUDA, gpuErrchk(cudaGetDeviceCount(&deviceCount));); (1, CUDA, gpuErrchk(cudaSetDevice(0));); (2, CUDA, gpuErrchk(cudaFree(0));); (7, CUDA, gpuErrchk(cudaDeviceReset());)"}
{"predict": "(34,MPI,MPI_Error_string(ERR_CODE, aux_string, &L);)", "label": "(36, MPI, MPI_Error_string(ERR_CODE, aux_string, &L);)"}
{"predict": "31i, u41i, u51i, u21im1, u31im1, u41im1, u51im1)\n122: #endif\n123:     for (k = 1; k < nz - 1; k++) {\n124:       for (j = jst; j <= jend; j++) {\n125:         for (i = ist; i <= iend; i++) {\n126:           u21i = frct[1][k][j][i];\n127:           u31i = frct[2][k][j][i];\n128:           u41i = frct[3][k][j][i];\n129:           u51i = frct[4][k][j][i];\n130:           u21im1 = frct[1][k][j][i-1];\n131:           u31im1 = frct[2][k][j][i-1];\n132:           u41im1 = frct[3][k][j][i-1];\n133:           u51im1 = frct[4][k][j][i-1];\n134:           tmp = ( (u21i - u21im1) * (u21i - u21im1)\n135:                      + (u31i - u31im1) * (u31i - u31im1)\n136:                      + (u41i - u41im1) * (u41i - u41im1)\n137:                      + (u51i - u51im1) * (u51i - u51im1) );\n138:           if ( tmp > 0.0 ) {\n139:             q = 0.50 * tmp;\n140:             frct[0][k][j][i] =  frct[0][k][j][i]\n141:                                               - tx2 * ( (u21i - u21im1) * (flux_G[1][k][j][i] - flux_G[1", "label": "(22, OpenMP, #pragma omp distribute); (17, OpenMP, #pragma omp target data map(alloc: frct, rsd, ce, flux_G)); (19, OpenMP, #pragma omp target teams)"}
{"predict": "(75,MPI,double stop = MPI_Wtime();); (34,OpenMP,#pragma omp parallel for), (44,OpenMP,#pragma omp parallel for), (66,OpenMP,std::swap(a_new, a);)", "label": "(22, CUDA, CUDA_RT_CALL(cudaMemset(a_new, 0, nx * ny * sizeof(real)));); (100, CUDA, CUDA_RT_CALL(cudaStreamDestroy(compute_stream));); (105, CUDA, CUDA_RT_CALL(cudaFree(a));); (23, CUDA, CUDA_RT_CALL(cudaDeviceSynchronize());); (100, CUDA, CUDA_RT_CALL(cudaFree(l2_norm_d));); (27, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&push_top_done, cudaEventDisableTiming));); (24, CUDA, CUDA_RT_CALL(cudaStreamCreate(&push_bottom_stream));); (26, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&push_bottom_done, cudaEventDisableTiming));); (49, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(compute_stream, push_bottom_done, 0));); (67, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(push_bottom_stream, compute_done, 0));); (69, CUDA, CUDA_RT_CALL(cudaEventRecord(push_bottom_done, push_bottom_stream));); (89, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_top_stream));); (86, CUDA, CUDA_RT_CALL(cudaEventDestroy(compute_done));); (81, CUDA, CUDA_RT_CALL(cudaMemcpy(a_ref_h, a, nx * ny * sizeof(real), cudaMemcpyDeviceToHost));); (82, CUDA, CUDA_RT_CALL(cudaEventDestroy(push_bottom_done));); (14, CUDA, CUDA_RT_CALL(cudaStreamCreate(&push_top_stream));); (81, CUDA, CUDA_RT_CALL(cudaEventDestroy(push_top_done));); (65, CUDA, CUDA_RT_CALL(cudaStreamSynchronize(compute_stream));); (44, CUDA, CUDA_RT_CALL(cudaEventRecord(compute_done, compute_stream));); (17, CUDA, CUDA_RT_CALL(cudaMallocHost(&l2_norm_h, sizeof(real)));); (54, CUDA, CUDA_RT_CALL(cudaEventRecord(push_top_done, push_top_stream));); (33, CUDA, CUDA_RT_CALL(cudaMemsetAsync(l2_norm_d, 0, sizeof(real), compute_stream));); (6, CUDA, CUDA_RT_CALL(cudaStreamCreate(&compute_stream));); (-2, CUDA, CUDA_RT_CALL(cudaMemset(a, 0, nx * ny * sizeof(real)));); (7, CUDA, CUDA_RT_CALL(cudaEventCreateWithFlags(&compute_done, cudaEventDisableTiming));); (81, CUDA, CUDA_RT_CALL(cudaFree(a_new));); (45, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(push_top_stream, compute_done, 0));); (72, CUDA, CUDA_RT_CALL(cudaStreamDestroy(push_bottom_stream));); (28, CUDA, CUDA_RT_CALL(cudaStreamWaitEvent(compute_stream, push_top_done, 0));); (6, CUDA, CUDA_RT_CALL(cudaMalloc(&l2_norm_d, sizeof(real)));); (21, MPI, double start = MPI_Wtime();); (-13, CUDA, CUDA_RT_CALL(cudaMalloc(&a, nx * ny * sizeof(real)));); (71, CUDA, CUDA_RT_CALL(cudaFreeHost(l2_norm_h));); (-14, CUDA, CUDA_RT_CALL(cudaMalloc(&a_new, nx * ny * sizeof(real)));)"}
{"predict": "(17,MPI,*w = MPI_Wtime();); (18,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for)", "label": "(30, MPI, MPI_Bcast( work, diag_len[p], MPI_DOUBLE, pkk, grid->comm );); (27, MPI, MPI_Bcast( work, lwork, MPI_DOUBLE, pkk, grid->comm );)"}
{"predict": "(18,OpenMP,#pragma omp parallel for)", "label": "(4, CUDA, const T** input_ptrs = GetCudaDeviceArrayOnDevice(&input_ptr_data);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for)", "label": "(17, MPI, MPI_Allreduce(bins_ptr, global_ptr, bins_size, MPI_DOUBLE, MPI_MIN, mpi_comm);); (12, MPI, MPI_Allreduce(bins_ptr, global_ptr, bins_size, MPI_DOUBLE, MPI_SUM, mpi_comm);); (19, MPI, MPI_Allreduce(bins_ptr, global_ptr, bins_size, MPI_DOUBLE, MPI_MAX, mpi_comm);); (2, MPI, MPI_Comm mpi_comm = MPI_Comm_f2c(flow::Workspace::default_mpi_comm());)"}
{"predict": "(11,MPI,*w = MPI_Wtime();); (13,OpenMP,#pragma omp parallel for), (75,MPI,if (rank == 0) { viscdt = max(viscdt, local_viscdt); });", "label": "(71, OpenMP, const int th=omp_get_thread_num();); (10, OpenMP, #pragma omp parallel for schedule (guided))"}
{"predict": "(12,OpenMP,#pragma omp parallel for private(k,dims)), (20,OpenMP,#pragma omp critical), (22,OpenMP,#pragma omp for private(i)), (24,OpenMP,#pragma omp for private(i))", "label": "(10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &wrank);); (15, MPI, MPI_Dims_create(tests[k].size, tests[k].dim, dims);)"}
{"predict": "(1,OpenMP,#pragma omp parallel)", "label": "(17, CUDA, __shared__ float a_smem[BM * BK];); (17, CUDA, __shared__ float b_smem[BK * BN];)"}
{"predict": "(23,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for), (55,OpenMP,#pragma omp parallel for)", "label": "(86, OpenCL, err = clReleaseMemObject(buffer);); (82, OpenCL, err = clReleaseMemObject(image_buffer);)"}
{"predict": "(28,MPI,sdkCreateTimer(&timer);) (30,OpenMP,#pragma omp parallel for), (40,MPI,sdkStopTimer(&timer);)", "label": "(12, CUDA, cudaMallocHost((void**)&h_a, bufsize);); (13, CUDA, cudaMallocHost((void**)&h_c, bufsize);); (43, CUDA, cudaDeviceSynchronize();); (20, CUDA, cudaMalloc((void**)&d_a, bufsize);); (63, CUDA, cudaFree(d_c);); (61, CUDA, cudaFree(d_b);); (7, CUDA, cudaMallocHost((void**)&h_b, bufsize);); (64, CUDA, cudaFreeHost(h_b);); (64, CUDA, cudaFreeHost(h_c);); (56, CUDA, cudaFree(d_a);); (60, CUDA, cudaFreeHost(h_a);); (14, CUDA, cudaMalloc((void**)&d_c, bufsize);); (12, CUDA, cudaMalloc((void**)&d_b, bufsize);)"}
{"predict": "(11,MPI,mpi_init()); (12,MPI,dev_ctx.write_reg = platform_write_mpi;); (13,MPI,dev_ctx.read_reg = platform_read_mpi;); (14,MPI,dev_ctx.mdelay = platform_delay_mpi;); (15,MPI,dev_ctx.handle = &SENSOR_BUS_mpi;); (28,MPI,if (whoamI != LIS2MDL_ID) { ... }); (33,MPI,lis2mdl_reset_set_mpi(&dev_ctx, PROPERTY_ENABLE);); (36,MPI,do { ... } while (rst_mpi);); (46,MPI,lis2mdl_operating_mode_set_mpi(&dev_ctx, LIS2MDL_CONTINUOUS_MODE);); (47,MPI,while (1) { ... }); (52,MPI,lis2mdl_mag_data_ready_get_mpi(&dev_ctx, &reg_mpi);); (56,MPI,lis2mdl_magnetic_raw_get_mpi(&dev_ctx, data_raw_magnetic_mpi);); (62,MPI,snprintf((char *)tx_buffer, sizeof(tx_buffer), \"Magnetic field [mG]:%4.2f\\t%4.2f\\t%4.2f\\r\\n\", magnetic_mG_mpi[0], magnetic_mG_mpi[1], magnetic_mG_mpi[2]);); (68,MPI,snprintf((char *)tx_buffer, sizeof(tx_buffer), \"Temperature [degC]:%6.2f\\r\\n\", temperature_degC_mpi););", "label": "(43, OpenMP, lis2mdl_offset_temp_comp_set(&dev_ctx, PROPERTY_ENABLE);)"}
{"predict": "(6,OpenMP,#pragma omp parallel for reduction(+:value)), (12,OpenMP,return value;)", "label": "(3, TBB, ValueType r1 = tbb::parallel_reduce( m_range, I, Accumulator(), Sum(), partitioner );)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp threadprivate(state)), (32,OpenMP,#pragma omp parallel for collapse(2)), (34,OpenMP,#pragma omp simd)", "label": "(11, CUDA, __shared__ float r[_N_], k[_N_], u[_N_], w[_N_];)"}
{"predict": "(105,OpenMP,#pragma omp parallel for)", "label": "(50, CUDA, flamegpu::CUDASimulation cudaSimulation(model, argc, argv);)"}
{"predict": "(34,MPI,*w = MPI_Wtime();); (66,OpenMP,#pragma omp parallel for), (102,OpenMP,#pragma omp parallel for)", "label": "(12, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (13, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;); (80, CUDA, cudaError_t result = cudaDeviceSynchronize();)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(22, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(15,OpenMP,#pragma omp parallel for reduction(+:N)), (44,OpenMP,#pragma omp parallel for reduction(+:current_weight)), (59,OpenMP,#pragma omp parallel for reduction(+:next_weight))", "label": "(11, MPI, MPI_Type_get_extent(int_with_key, &lb, &extent);)"}
{"predict": "(16,OpenMP,#pragma omp parallel for)", "label": "(15, OpenCL, retVal = clEnqueueNDRangeKernel(clCommandQ, clKernel, 1, offset, gws, nullptr, 1, &eventBlocking, &eventOut);); (13, OpenCL, clCommandQ = clCreateCommandQueue(context, devices[0], properties, &retVal);); (38, OpenCL, clReleaseEvent(eventOut);); (31, OpenCL, clWaitForEvents(1, &callbackEvent);); (37, OpenCL, clReleaseCommandQueue(clCommandQ);); (28, OpenCL, clWaitForEvents(1, &eventOut);)"}
{"predict": "(11,MPI,*w = MPI_Wtime();); (32,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for), (41,MPI,MPI_Finalize();)", "label": "(38, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (40, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(15,OpenMP,#pragma omp parallel for)", "label": "(12, OpenMP, #pragma omp target)"}
{"predict": "(5,MPI,*w = MPI_Wtime(););", "label": "(10, MPI, MPI_Abort(MPI_COMM_WORLD, EXIT_FAILURE);)"}
{"predict": "(10,MPI,if (err != MPI_SUCCESS) { MPI_Abort(MPI_COMM_WORLD, 1); }), (16,OpenMP,#pragma omp parallel for)", "label": "(33, MPI, MPI_Type_free(&type);); (16, MPI, MPI_Type_get_extent(type, &lb, &extent);); (7, MPI, err = MPI_Type_indexed(3, len, disp, MPI_INT, &type);)"}
{"predict": "(19,OpenMP,#pragma omp parallel for)", "label": "(23, MPI, MPI_Wait(&req, MPI_STATUS_IGNORE);); (17, MPI, MPI_Alltoallw_c(sendbuf, counts, displs, types, recvbuf, counts, displs, types, comm);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(3, OpenMP, #pragma omp target enter data map(alloc:i)); (6, OpenMP, #pragma omp target data map(present, alloc: i)); (7, OpenMP, #pragma omp target exit data map(delete:i))"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (...)", "label": "(10, MPI, errcode = MPI_Grequest_complete(aio_req->req);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for reduction(+:sum))", "label": "(20, OpenMP, #pragma omp flush); (13, OpenMP, #pragma omp task private(sum) shared(result, known_sum)); (7, OpenMP, #pragma omp parallel); (8, OpenMP, #pragma omp single); (21, OpenMP, #pragma omp critical)"}
{"predict": "(18,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for)", "label": "(20, OpenCL, clReleaseMemObject(mobjAP_);); (16, OpenCL, clReleaseMemObject(mobjX_);)"}
{"predict": "(27,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for)", "label": "(27, OpenCL, TEST_OCL_CHECK(clReleaseCommandQueue(ocl_queue));)"}
{"predict": "(12,MPI,*w = MPI_Wtime();); (22,OpenMP,#pragma omp parallel for), (44,OpenMP,#pragma omp parallel for), (60,OpenMP,#pragma omp parallel for)", "label": "(9, MPI, MPI_Comm_rank(comm, &comm_rank);); (10, MPI, MPI_Comm_size(comm, &comm_size);)"}
{"predict": "(26,OpenMP,#pragma omp parallel for)", "label": "(36, CUDA, cudaMemcpy(signatures.data(), d_signatures, length_signature_aligned * batch, cudaMemcpyDeviceToHost);); (40, CUDA, cudaFree(d_secret_keys);); (26, CUDA, cudaMemcpy(d_secret_keys, secret_keys.data(), length_secret_key * batch, cudaMemcpyHostToDevice);); (26, CUDA, cudaMemcpy(d_messages, messages.data(), message_size * batch, cudaMemcpyHostToDevice);); (39, CUDA, cudaFree(d_messages);); (17, CUDA, cudaMalloc(reinterpret_cast<void**>(&d_secret_keys), length_secret_key * batch);); (15, CUDA, cudaMalloc(reinterpret_cast<void**>(&d_signatures), length_signature_aligned * batch);); (35, CUDA, cudaFree(d_signatures);); (15, CUDA, cudaMalloc(reinterpret_cast<void**>(&d_messages), message_size * batch);)"}
{"predict": "(34,OpenMP,#pragma omp parallel for collapse(3)), (38,OpenMP,#pragma omp atomic)", "label": "(19, OpenMP, #pragma omp parallel for)"}
{"predict": "(1,MPI,int MPI_Init(&argc, &argv);); (6,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (7,MPI,MPI_Comm_size(MPI_COMM_WORLD, &size);); (13,MPI,MPI_Isend(&value, 1, MPI_INT, right, tag, MPI_COMM_WORLD, &request);); (14,MPI,MPI_Recv(&buffer, 1, MPI_INT, left, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (23,MPI,MPI_Finalize(););", "label": "(27, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (3, MPI, MPI_Init(NULL, NULL);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (20, MPI, MPI_Wait(&request, MPI_STATUS_IGNORE);); (6, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (9, MPI, MPI_Bcast(&factor, 1, MPI_INT, root, MPI_COMM_WORLD);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for)", "label": "(3, OpenMP, #pragma omp target); (4, OpenMP, #pragma omp distribute parallel for simd); (2, OpenMP, #pragma omp teams)"}
{"predict": "(6,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for)", "label": "(5, OpenMP, #pragma omp target teams distribute parallel for)"}
{"predict": "(3,MPI,MPI_Init(&argc, &argv);) (4,OpenMP,#pragma omp parallel for) (8,MPI,MPI_Finalize();)", "label": "(43, MPI, err = MPI_Type_size(eviltype, &val);); (100, MPI, MPI_Type_free(&inttype);); (19, MPI, err = MPI_Type_create_resized(tmptype, 6, -9, &inttype);); (55, MPI, err = MPI_Type_get_extent(eviltype, &lb, &extent);); (28, MPI, err = MPI_Type_vector(3, 4, 5, inttype, &eviltype);); (95, MPI, MPI_Type_free(&tmptype);); (3, MPI, err = MPI_Type_contiguous(4, MPI_BYTE, &tmptype);); (95, MPI, MPI_Type_free(&eviltype);); (71, MPI, err = MPI_Type_get_true_extent(eviltype, &true_lb, &aval);)"}
{"predict": "(14,MPI,*w = MPI_Wtime();); (21,OpenMP,#pragma omp parallel for), (36,OpenMP,#pragma omp parallel for), (47,OpenMP,#pragma omp parallel for), (55,OpenMP,#pragma omp parallel for)", "label": "(78, CUDA, CHECK(cudaFree(d_y));); (74, CUDA, CHECK(cudaFree(d_NN));); (56, CUDA, CHECK(cudaMemcpy(NL, d_NL, N*M*sizeof(int), cudaMemcpyDefault));); (34, CUDA, CHECK(cudaMalloc(&d_NL, N*M*sizeof(int)));); (32, CUDA, CHECK(cudaMalloc(&d_NN, N*sizeof(int)));); (46, CUDA, CHECK(cudaMemcpy(d_y, y.data(), N*sizeof(real), cudaMemcpyDefault));); (51, CUDA, CHECK(cudaMemcpy(NN, d_NN, N*sizeof(int), cudaMemcpyDefault));); (33, CUDA, CHECK(cudaMalloc(&d_y, N*sizeof(real)));); (69, CUDA, CHECK(cudaFree(d_x));); (30, CUDA, CHECK(cudaMalloc(&d_x, N*sizeof(real)));); (66, CUDA, CHECK(cudaFree(d_NL));)"}
{"predict": "(16,MPI,MPI_Init(&argc, &argv);)", "label": "(19, MPI, MPI_Pack_size(2, MPI_DOUBLE, comm, &s2);); (38, MPI, MPI_Recv(rmsg2, 10, MPI_DOUBLE, src, tag, comm, MPI_STATUS_IGNORE);); (21, MPI, MPI_Buffer_attach(buf, bufsize);); (56, MPI, MPI_Buffer_detach(&bbuf, &bsize);); (34, MPI, MPI_Recv(rmsg1, 7, MPI_CHAR, src, tag, comm, MPI_STATUS_IGNORE);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (27, MPI, MPI_Bsend(msg2, 2, MPI_DOUBLE, dest, tag, comm);); (13, MPI, MPI_Pack_size(17, MPI_CHAR, comm, &s3);); (24, MPI, MPI_Bsend(msg1, 7, MPI_CHAR, dest, tag, comm);); (25, MPI, MPI_Bsend(msg3, 17, MPI_CHAR, dest, tag, comm);); (8, MPI, MPI_Pack_size(7, MPI_CHAR, comm, &s1);); (29, MPI, MPI_Recv(rmsg3, 17, MPI_CHAR, src, tag, comm, MPI_STATUS_IGNORE);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for)", "label": "(9, SYCL, sycl::device device;)"}
{"predict": "UMP: StringLiteral{{.*}}\"sycl\"\n95:   //DUMP: StringLiteral{{.*}}\"hip\"\n96:   //DUMP: StringLiteral{{.*}}\"level_zero\"\n97: \n98:   //PRINT: #pragma omp interop init(prefer_type(\"cuda\",\"cuda_driver\",\"opencl\",\"sycl\",\"hip\",\"level_zero\"), prefer_device(dev), targetsync : I)\n99:   //DUMP: OMPInteropDirective\n100:  //DUMP: OMPInitClause\n101:  //DUMP: DeclRefExpr{{.*}}'omp_interop_t'{{.*}}Var{{.*}}'I'\n102:  //DUMP: StringLiteral{{.*}}\"cuda\"\n103:  //DUMP: StringLiteral{{.*}}\"cuda_driver\"\n104:  //DUMP: StringLiteral{{.*}}\"opencl\"\n105:  //DUMP: StringLiteral{{.*}}\"sycl\"\n106:  //DUMP: StringLiteral{{.*}}\"hip\"\n107:  //DUMP: StringLiteral{{.*}}\"level_zero\"\n108:  //DUMP: OMPDeviceClause\n109:  //DUMP: DeclRefExpr{{.*}}'dev' 'int'\n110: \n111:  //PRINT: #pragma omp interop init(prefer_type(\"cuda\",\"cuda_driver\",\"opencl\",\"sycl\",\"hip\",\"level_zero\"), prefer_device(dev), prefer_mode(1,2,3,4,5,6), targetsync : I)\n112:  //DUMP: OMPInteropDirective\n113:  //DUMP: OMPInitClause\n114:  //DUMP: DeclRefExpr{{.*}}'omp_interop_t'{{.*}}Var{{.*}}'I'\n115:  //DUMP: StringLiteral{{.*}}\"cuda\"\n116:  //DUMP: StringLiteral{{.*}}\"cuda_driver\"\n117:  //DUMP: StringLiteral{{.*}}\"opencl\"\n118:  //DUMP: StringLiteral{{.*}}\"sycl\"\n119:  //DUMP: StringLiteral{{.*}}\"hip\"\n120:  //DUMP: StringLiteral{{.*}}\"level_zero\"\n121:  //DUMP: O", "label": "(140, OpenMP, #pragma omp interop use(I) use(J)); (129, OpenMP, #pragma omp interop init(target:I) init(targetsync:J)); (24, OpenMP, #pragma omp interop init(target:IRef)); (31, OpenMP, #pragma omp interop destroy(IRef)); (71, OpenMP, #pragma omp interop device(dev) depend(inout:ap) destroy(I)); (159, OpenMP, #pragma omp interop destroy(I) use(J)); (4, OpenMP, #pragma omp interop use(I)); (141, OpenMP, #pragma omp interop destroy(I) destroy(J)); (47, OpenMP, #pragma omp interop device(dev) depend(inout:ap) init(targetsync:I)); (7, OpenMP, #pragma omp interop destroy(I)); (-2, OpenMP, #pragma omp interop init(target:I)); (127, OpenMP, #pragma omp interop init(target:I) use(J)); (29, OpenMP, #pragma omp interop use(CI)); (84, OpenMP, #pragma omp interop init(prefer_type(2,4,6,1),targetsync:I)); (14, OpenMP, #pragma omp interop use(IRef)); (50, OpenMP, #pragma omp interop device(dev) depend(inout:ap) use(I)); (71, OpenMP, #pragma omp interop init(prefer_type(1,2,3,4,5,6),targetsync:I)); (145, OpenMP, #pragma omp interop init(target:I) destroy(J)); (-14, OpenMP, #pragma omp interop init()"}
{"predict": "(10,MPI,*w = MPI_Wtime();); (30,MPI,CUDA_CHECK(cudaMemcpyAsync(h_data.data(), d_data, sizeof(data_type) * h_data.size(), cudaMemcpyDeviceToHost, stream));)", "label": "(36, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (38, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(31,OpenMP,#pragma omp parallel for), (72,OpenMP,#pragma omp parallel for)", "label": "(86, CUDA, cudaFreeHost( outHost );); (84, CUDA, cudaFree( texDevice );)"}
{"predict": "(18,MPI,*w = MPI_Wtime();); (20,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for)", "label": "(31, OpenMP, #pragma omp teams reduction(+:sum)); (28, OpenMP, #pragma omp target map(tofrom: b, sum, t) map(alloc: a)); (31, OpenMP, #pragma omp distribute parallel for reduction(+:sum)); (33, OpenMP, t[i] = omp_get_team_num();)"}
{"predict": "(105,OpenMP,#pragma omp parallel for), (117,OpenMP,#pragma omp parallel for), (120,OpenMP,#pragma omp parallel for)", "label": "(91, OpenMP, const int NumThreads = omp_get_max_threads();)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (44,OpenMP,#pragma omp parallel for), (90,OpenMP,#pragma omp parallel for)", "label": "(44, CUDA, checkCudaErrors(cudaMemset(d_Output, 0, QRNG_DIMENSIONS * N * sizeof(float)));); (20, CUDA, int dev = findCudaDevice(argc, (const char **)argv);); (49, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (123, CUDA, checkCudaErrors(cudaFree(d_Output));); (18, CUDA, checkCudaErrors(cudaGetDeviceProperties(&deviceProp, dev));); (28, CUDA, checkCudaErrors(cudaMalloc((void **)&d_Output, QRNG_DIMENSIONS * N * sizeof(float)));); (60, CUDA, checkCudaErrors(cudaMemcpy(h_OutputGPU, d_Output, QRNG_DIMENSIONS * N * sizeof(float), cudaMemcpyDeviceToHost));)"}
{"predict": "6: static double fjac2[IMAX/2*2+1][JMAX/2*2+1][KMAX-1+1][5][5];\n117: static double xface[IMAX/2*2+1][JMAX/2*2+1][KMAX][5];\n118: static double xface2[IMAX/2*2+1][JMAX/2*2+1][KMAX][5];\n119: static double yface[IMAX/2*2+1][JMAX/2*2+1][KMAX][5];\n120: static double yface2[IMAX/2*2+1][JMAX/2*2+1][KMAX][5];\n121: static double zface[IMAX/2*2+1][JMAX/2*2+1][KMAX][5];\n122: static double zface2[IMAX/2*2+1][JMAX/2*2+1][KMAX][5];\n123: static double rhsx[IMAX/2*2+1][JMAX/2*2+1][KMAX][5];\n124: static double rhsy[IMAX/2*2+1][JMAX/2*2+1][KMAX][5];\n125: static double rhsz[IMAX/2*2+1][JMAX/2*2+1][KMAX][5];\n126: \n127: /* COMMON block: lhs_1d */\n128: static double a[PROBLEM_SIZE][5][5];\n129: static double b[PROBLEM_SIZE][5];\n130: static double c[PROBLEM_SIZE][5];\n131: static double d[PROBLEM_SIZE];\n132: \n133: /* COMMON block: lhs_1d_old */\n134: static double aold[PROBLEM_SIZE][5][5];\n135: static double bold[PROBLEM_SIZE][5];\n136: static double cold[PROBLEM_SIZE][5];\n137: \n138: /* COMMON block: rhs_1d */\n139: static double rhsx1d[PROBLEM_SIZE];\n140: static double rhsy1d[PROBLEM", "label": "(249, OpenMP, nthreads = omp_get_num_threads();); (234, OpenMP, #pragma omp parallel firstprivate(niter) private(step)); (214, OpenMP, #pragma omp parallel); (237, OpenMP, #pragma omp master)"}
{"predict": "(1,MPI, *w = MPI_Wtime();); (7,MPI,MPI_Init(&argc, &argv); MPI_Comm_size(MPI_COMM_WORLD, &size); MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (28,MPI,MPI_Finalize(););", "label": "(6, MPI, MPI_Init(&argc, &argv);); (32, MPI, MPI_Finalize();); (11, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (20, MPI, MPI_Reduce(&errs, &toterrs, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);)"}
{"predict": "28:                 rpeer += nleaderranks;\n129:             }\n130:         }\n131:     }\n132: \n133: // All host leaders now have their hoststring and can proceed to\n134: // gather them into allhoststrings.\n135:     allhoststrings = (char **) malloc(nleaderranks * sizeof(char *));\n136:     if (!allhoststrings) {\n137:         free(method);\n138:         ompi_comm_free(&local_comm);\n139:         ompi_comm_free(&leader_comm);\n140:         return;\n141:     }\n142: \n143:     MPI_Gather(hoststring, strlen(hoststring) + 1, MPI_CHAR,\n144:         allhoststrings, strlen(hoststring) + 1, MPI_CHAR,\n145:         myleaderrank, leader_comm);\n146: \n147:     // Free up the hoststring\n148:     free(hoststring);\n149: \n150:     // All host leaders now have the hoststrings from all other leaders.\n151:     // They need to sort them into a single list of unique strings.\n152:     // (The final string list is built on leader rank 0.)\n153:     {\n154:         int nstrings = nleaderranks;\n155:         char *hoststrings = (char *) malloc(nstrings * sizeof(char) * 100);\n156:         if (!hoststrings) {\n157:             free(allhoststrings);\n158:             free(method);\n159:             ompi_comm_free(&local_comm);\n160:             ompi_comm_free(&leader_comm);\n161:             return;\n162:         }\n163:         for (i=0; i<nstrings; ++i) {\n164:             int j, k, len = strlen(allhoststrings[i]);\n1155:             for (j=0; j<i; ++j) {\n1156:                 if (strncmp(allhoststrings[i], allhoststrings[j], len) == 0) {\n1157:                     nstrings--;\n1158:                     for (k=i; k<nstrings", "label": "(150, MPI, MPI_Type_commit(&mydt);); (154, MPI, MPI_Op_free(&myop);); (154, MPI, MPI_Type_free(&mydt);); (146, MPI, MPI_Type_contiguous(sizeof(comm_method_string_conversion_t), MPI_BYTE, &mydt);); (147, MPI, MPI_Op_create(myfn, 1, &myop);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for)", "label": "(8, MPI, MPI_Alloc_mem(DATA_SZ, MPI_INFO_NULL, &base_ptr[0]);); (20, MPI, MPI_Win_free_keyval(&keyval);); (21, MPI, MPI_Win_free(&windows[1]);); (6, MPI, MPI_Win_create(base_ptr[0], DATA_SZ, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &windows[0]);); (12, MPI, MPI_Win_set_attr(windows[0], keyval, NULL);); (5, MPI, MPI_Alloc_mem(DATA_SZ, MPI_INFO_NULL, &base_ptr[1]);); (14, MPI, MPI_Free_mem(base_ptr[0]);); (15, MPI, MPI_Win_free_keyval(&keyval_copy);); (16, MPI, MPI_Free_mem(base_ptr[1]);); (4, MPI, MPI_Win_create_keyval(MPI_WIN_NULL_COPY_FN, delete_fn, &keyval, NULL);); (9, MPI, MPI_Win_free(&windows[0]);); (0, MPI, MPI_Win_create(base_ptr[1], DATA_SZ, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &windows[1]);); (5, MPI, MPI_Win_set_attr(windows[1], keyval, NULL);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (36,OpenMP,#pragma omp parallel for), (53,OpenMP,#pragma omp parallel for), (74,OpenMP,#pragma omp parallel for)", "label": "(46, CUDA, thrust::fill(thrust::device, B.begin(), B.end(), 0.0);); (44, CUDA, thrust::sequence(thrust::device, A.begin(), A.end() );); (40, CUDA, thrust::device_vector<double> A(order*order);); (40, CUDA, thrust::device_vector<double> B(order*order);)"}
{"predict": "(1,OpenMP,#pragma omp parallel for collapse(2)), (5,MPI,*w = MPI_Wtime();), (8,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for), (38,OpenMP,#pragma omp parallel for), (44,OpenMP,#pragma omp parallel for), (50,OpenMP,#pragma omp parallel for), (55,OpenMP,#pragma omp parallel for)", "label": "(2, CUDA, extern __shared__ char shared_memory[];)"}
{"predict": "(2,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp single), (26,OpenMP,#pragma omp for)", "label": "(5, CUDA, extern __shared__ float s_data[];)"}
{"predict": "(16,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (38,OpenMP,#pragma omp parallel for), (49,OpenMP,#pragma omp parallel for), (60,OpenMP,#pragma omp parallel for), (71,OpenMP,#pragma omp parallel for), (82,OpenMP,#pragma omp parallel for), (93,OpenMP,#pragma omp parallel for), (104,OpenMP,#pragma omp parallel for), (115,OpenMP,#pragma omp parallel for), (126,OpenMP,#pragma omp parallel for), (137,OpenMP,#pragma omp parallel for), (146,OpenMP,#pragma omp parallel for)", "label": "(58, OpenMP, #pragma omp sections reduction(|| : argc > 0 ? argv[1] : argv[2])); (102, OpenMP, #pragma omp sections reduction(- : da)); (131, OpenMP, #pragma omp sections reduction(+ : o)); (75, OpenMP, #pragma omp sections reduction(+ : a, b, c, d, f)); (160, OpenMP, #pragma omp sections reduction(+ : fl)); (83, OpenMP, #pragma omp sections reduction(max : h.b)); (117, OpenMP, #pragma omp sections reduction(& : e, g)); (146, OpenMP, #pragma omp sections reduction(+ : r)); (20, OpenMP, #pragma omp sections reduction(-); (129, OpenMP, #pragma omp sections private(i), reduction(+ : j), reduction(+ : q)); (164, OpenMP, #pragma omp sections reduction(+ : m)); (102, OpenMP, #pragma omp sections reduction(&& : S2::S2s)); (61, OpenMP, #pragma omp sections reduction(^ : S1)); (80, OpenMP, #pragma omp sections reduction(+ : ba)); (154, OpenMP, #pragma omp parallel reduction(* : fl)); (-3, OpenMP, #pragma omp parallel); (67, OpenMP, #pragma omp sections reduction(min : a, b, c, d, f)); (111, OpenMP, #pragma omp sections reduction(+ : h, k, B::x)); (0, OpenMP, #pragma omp sections reduction +); (-6, OpenMP, #pragma omp sections reduction); (122, OpenMP, #pragma omp parallel private(k)); (87, OpenMP, #pragma omp sections reduction(^ : fl)); (31, OpenMP, #pragma omp sections reduction(| : argc,); (120, OpenMP, #pragma omp sections reduction(+ : p), reduction(+ : p)); (154, OpenMP, #pragma omp sections reduction(task, + : m)); (13, OpenMP, #pragma omp sections reduction(*)); (132, OpenMP, #pragma omp parallel reduction(min : i)); (136, OpenMP, #pragma omp parallel private(fl)); (40, OpenMP, #pragma omp sections reduction(&& : argc, z)); (69, OpenMP, #pragma omp sections reduction(* : ca)); (3, OpenMP, #pragma omp sections reduction()); (87, OpenMP, #pragma omp sections reduction(&& : S2::S2sc)); (127, OpenMP, #pragma omp sections reduction(max : j)); (-10, OpenMP, #pragma omp sections reduction(); (9, OpenMP, #pragma omp sections reduction(\\)); (13, OpenMP, #pragma omp sections reduction(foo : argc); (121, OpenMP, #pragma omp parallel shared(i)); (26, OpenMP, #pragma omp sections reduction(~ : argc))"}
{"predict": "(28,OpenMP,#pragma omp parallel for)", "label": "(14, CUDA, cuda::CUDABlas *blas = new cuda::CUDABlas(cuda_executor);)"}
{"predict": "(11, CUDA,<<<nbatch, 64>>>(N, m, lda, mstride, piv, pstride, LogOverlapFactor, res, nbatch));", "label": "(13, CUDA, qmc_cuda::cuda_check(cudaDeviceSynchronize());); (11, CUDA, qmc_cuda::cuda_check(cudaGetLastError());)"}
{"predict": "(11,OpenMP,#pragma omp parallel for)", "label": "(11, MPI, MPI_Error_string(code, msg, &resultLen);); (12, MPI, MPI_Abort(MPI_COMM_WORLD, code);); (8, MPI, MPI_Error_class(code, &class);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (...)", "label": "(2, CUDA, __shared__ uchar as[18][66];)"}
{"predict": "(20,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for), (35,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for), (45,OpenMP,#pragma omp parallel for), (50,OpenMP,#pragma omp parallel for)", "label": "(25, OpenMP, #pragma omp target parallel for is_device_ptr(z)); (36, OpenMP, #pragma omp target parallel for is_device_ptr(raa)); (29, OpenMP, #pragma omp target parallel for is_device_ptr(aa)); (46, OpenMP, #pragma omp target parallel for is_device_ptr(rh)); (39, OpenMP, #pragma omp target parallel for is_device_ptr(h)); (14, OpenMP, #pragma omp target parallel for is_device_ptr(k)); (49, OpenMP, #pragma omp target parallel for is_device_ptr(da))"}
{"predict": "(8,OpenMP,#pragma omp parallel for collapse(2))", "label": "(11, OpenMP, #pragma omp ordered); (3, OpenMP, #pragma omp parallel); (13, OpenMP, #pragma omp critical); (5, OpenMP, #pragma omp for private(i,j) schedule(static,1) collapse(2) ordered)"}
{"predict": "(27,OpenMP,#pragma omp parallel), (60,OpenMP,#pragma omp for), (67,OpenMP,#pragma omp section)", "label": "(60, OpenMP, #pragma omp sections lastprivate(t_var, vec, s_arr, var, sivar)); (4, OpenMP, #pragma omp section); (2, OpenMP, #pragma omp parallel); (28, OpenMP, #pragma omp sections lastprivate(g, sivar)); (65, OpenMP, #pragma omp sections lastprivate(CONDITIONAL A::x, B::x))"}
{"predict": "(16,OpenMP,#pragma omp parallel for)", "label": "(10, CUDA, cudaMallocHost(&h_ptr, THREAD * sizeof(void *));); (29, CUDA, cudaFree(d_ptr);); (25, CUDA, cudaMemcpy(h_clk, d_clk, THREAD * sizeof(uint32_t), cudaMemcpyDeviceToHost);); (28, CUDA, cudaFree(d_ret);); (12, CUDA, cudaMemcpy(d_ptr, h_ptr, THREAD * sizeof(void *), cudaMemcpyHostToDevice);); (27, CUDA, cudaFree(d_clk);); (-1, CUDA, cudaMalloc(&d_ptr, THREAD * sizeof(void *));); (26, CUDA, cudaFreeHost(h_ptr);); (-1, CUDA, cudaMalloc(&d_clk, THREAD * sizeof(uint32_t));); (-3, CUDA, cudaMalloc(&d_ret, sizeof(void *));)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(14, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "(4,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for), (21,OpenMP,#pragma omp parallel for), (29,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for), (41,OpenMP,#pragma omp parallel for), (45,OpenMP,#pragma omp parallel for), (55,OpenMP,#pragma omp parallel for)", "label": "(69, TBB, g.reset(tbb::flow::rf_reset_bodies);); (28, TBB, tbb::flow::make_edge( senders[s], exe_node );); (19, TBB, tbb::flow::make_edge( exe_node, *receivers[r] );); (61, TBB, inc_functor body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (66, TBB, body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (0, TBB, tbb::flow::graph g;); (44, TBB, tbb::flow::remove_edge( exe_node, *receivers[r] );); (4, TBB, tbb::flow::function_node< InputType, OutputType > exe_node( g, lc, cf );)"}
{"predict": "(4,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for), (27,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for), (37,OpenMP,#pragma omp parallel for), (42,OpenMP,#pragma omp parallel for)", "label": "(8, TBB, oneapi::tbb::global_control gc(oneapi::tbb::global_control::max_allowed_parallelism, p);); (23, TBB, oneapi::tbb::parallel_for( oneapi::tbb::blocked_range<int> (0, N, 10000), ParallelVectorForBody<T>( vs ) );); (0, TBB, typedef std::vector<T, oneapi::tbb::tbb_allocator<T> > ContainerType;); (17, TBB, typedef typename oneapi::tbb::combinable< ContainerType > CombinableType;)"}
{"predict": "(12,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for), (38,OpenMP,#pragma omp parallel for), (46,OpenMP,#pragma omp parallel for)", "label": "(16, TBB, tbb::flow::make_edge( exe_node, receivers[r] );); (23, TBB, tbb::flow::make_edge( senders[s], exe_node );); (43, TBB, tbb::flow::remove_edge( exe_node, receivers[r] );); (53, TBB, inc_functor body_copy = tbb::flow::copy_body<inc_functor>( exe_node );); (1, TBB, tbb::flow::graph g;); (6, TBB, tbb::flow::function_node< InputType, OutputType > exe_node( g, lc, cf );)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (53,OpenMP,#pragma omp parallel for), (54,OpenMP,#pragma omp parallel for)", "label": "(29, CUDA, cudaMalloc((void **)&d_C_ref, m * n * sizeof(float));); (68, CUDA, cudaEventElapsedTime(&elapsed_time, start, stop);); (30, CUDA, cudaMemcpy(d_A, A, m * k * sizeof(float), cudaMemcpyHostToDevice);); (32, CUDA, cudaMemcpy(d_C_ref, C_ref, m * n * sizeof(float), cudaMemcpyHostToDevice);); (36, CUDA, cudaMemcpy(C, d_C, m * n * sizeof(float), cudaMemcpyDeviceToHost);); (61, CUDA, cudaEventRecord(stop);); (61, CUDA, cudaEventSynchronize(stop);); (15, CUDA, cudaMalloc((void **)&d_B, k * n * sizeof(float));); (13, CUDA, cudaMalloc((void **)&d_A, m * k * sizeof(float));); (52, CUDA, cudaEventRecord(start);); (13, CUDA, cudaMalloc((void **)&d_C, m * n * sizeof(float));); (23, CUDA, cudaMemcpy(d_C, C, m * n * sizeof(float), cudaMemcpyHostToDevice);); (21, CUDA, cudaMemcpy(d_B, B, k * n * sizeof(float), cudaMemcpyHostToDevice);); (46, CUDA, cudaEventCreate(&stop);); (44, CUDA, cudaEventCreate(&start);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for private(index)), (12,OpenMP,#pragma omp for private(i)), (25,OpenMP,#pragma omp for private(i)), (30,OpenMP,#pragma omp for private(i)), (32,OpenMP,#pragma omp for private(i)), (35,OpenMP,#pragma omp for private(i))", "label": "(6, OpenMP, #pragma omp parallel for)"}
{"predict": "(7,OpenMP,#pragma omp parallel for collapse(2))", "label": "(8, CUDA, A[blockIdx.z * stride + i * lda + i] = thrust::complex<T>(1.0, 0.0);); (11, CUDA, A[blockIdx.z * stride + j * lda + i] = thrust::complex<T>(0.0, 0.0);)"}
{"predict": "(30,OpenMP,#pragma omp parallel for reduction(+:t_result_bytes)), (33,OpenMP,#pragma omp parallel for reduction(+:t_right_bytes)), (36,OpenMP,#pragma omp parallel for reduction(+:t_result_bytes)), (39,OpenMP,#pragma omp parallel for reduction(+:t_result_bytes)), (42,OpenMP,#pragma omp parallel for reduction(+:t_result_bytes))", "label": "(25, CUDA, cudaMalloc((void**)(&d_t_right), t_right_bytes);); (25, CUDA, cudaMalloc((void**)(&d_t_result), t_result_bytes);); (50, CUDA, cudaFree((void*)d_t_left);); (26, CUDA, cudaMemcpy(d_t_right, t_right.data(), t_right_bytes, cudaMemcpyHostToDevice);); (20, CUDA, cudaMalloc((void**)(&d_t_left), t_left_bytes);); (23, CUDA, cudaMemcpy(d_t_left, t_left.data(), t_left_bytes, cudaMemcpyHostToDevice);); (48, CUDA, cudaFree((void*)d_t_result);); (46, CUDA, cudaFree((void*)d_t_right);); (36, CUDA, cudaMemcpy(t_result_gpu.data(), d_t_result, t_result_bytes, cudaMemcpyDeviceToHost);)"}
{"predict": "(36,MPI,myrank = MPI_Comm_rank(MPI_COMM_WORLD, &myrank);)", "label": "(37, MPI, MPI_Comm_rank(adio_fh->comm, &myrank);); (24, MPI, MPI_Bcast(&tmp_whence, 1, MPI_INT, 0, adio_fh->comm);); (12, MPI, MPI_Bcast(&tmp_offset, 1, ADIO_OFFSET, 0, adio_fh->comm);); (110, MPI, MPI_Barrier(adio_fh->comm);)"}
{"predict": "(1, MPI, #include <mpi.h>;)\n(24, MPI, MPI_Finalize();)\n(6, MPI, MPI_Bcast(&data, 1, MPI_INT, 0, MPI_COMM_WORLD);)\n(11, MPI, MPI_Bcast(&flag, 1, MPI_INT, 0, MPI_COMM_WORLD);)\n(18, MPI, MPI_Bcast(&out, 1, MPI_INT, 32, MPI_COMM_WORLD);)", "label": "(22, OpenMP, #pragma omp flush(out)); (17, OpenMP, #pragma omp atomic read); (10, OpenMP, #pragma omp atomic write); (-1, OpenMP, #pragma omp target parallel num_threads(64) map(tofrom); (6, OpenMP, #pragma omp flush(data))"}
{"predict": "(10,OpenMP,#pragma omp parallel for private(nthread,n)), (16,OpenMP,#pragma omp parallel for private(n))", "label": "(11, TBB, tbb::task_scheduler_init init(nthread);)"}
{"predict": "(7,MPI,MPI_Comm_split(MPI_COMM_WORLD, rank_node, rank, &comm_node);)\n(13,OpenMP,#pragma omp parallel for)\n(21,OpenMP,#pragma omp barrier)", "label": "(12, MPI, MPI_Comm_size(comm_node, &size_node);); (1, MPI, MPI_Init(&argc, &argv);); (32, MPI, MPI_Finalize();); (3, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (13, MPI, MPI_Get_processor_name(procname, &namelen);); (5, MPI, MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, 0, MPI_INFO_NULL, &comm_node);); (-1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (6, MPI, MPI_Comm_rank(comm_node, &rank_node);); (19, MPI, MPI_Barrier(MPI_COMM_WORLD);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for reduction(+:index)), (44,OpenMP,#pragma omp parallel for reduction(+:i))", "label": "(12, OpenMP, #pragma omp parallel for)"}
{"predict": "(26,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for)", "label": "(56, CUDA, HANDLE_ERROR( cudaEventDestroy( start ) );); (6, CUDA, HANDLE_ERROR( cudaEventCreate( &stop ) );); (55, CUDA, HANDLE_ERROR( cudaEventDestroy( stop ) );); (36, CUDA, HANDLE_ERROR( cudaThreadSynchronize() );); (20, CUDA, HANDLE_ERROR( cudaHostGetDevicePointer( &dev_b, b, 0 ) );); (18, CUDA, HANDLE_ERROR( cudaHostGetDevicePointer( &dev_a, a, 0 ) );); (28, CUDA, HANDLE_ERROR( cudaEventRecord( start, 0 ) );); (33, CUDA, HANDLE_ERROR( cudaEventRecord( stop, 0 ) );); (33, CUDA, HANDLE_ERROR( cudaEventSynchronize( stop ) );); (43, CUDA, HANDLE_ERROR( cudaFreeHost( b ) );); (41, CUDA, HANDLE_ERROR( cudaFreeHost( a ) );); (42, CUDA, HANDLE_ERROR( cudaFreeHost( partial_c ) );); (-6, CUDA, HANDLE_ERROR( cudaEventCreate( &start ) );)"}
{"predict": "(10,MPI,MPI_Init(&argc, &argv);)", "label": "(13, MPI, mpi_err = MPI_Type_commit(&type);); (9, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(4,OpenMP,#pragma omp parallel for reduction(+:sum)), (8,OpenMP,#pragma omp task), (9,OpenMP,#pragma omp task), (10,OpenMP,sum1.first += sum2.first; sum1.second += sum2.second;)", "label": "(7, TBB, RangeType range2( range, tbb::split() );)"}
{"predict": "(12,OpenMP,#pragma omp parallel for reduction(+:result)), (15,OpenMP,#pragma omp parallel for reduction(+:result))", "label": "(20, MPI, MPI_Allreduce( & tmp , & result , 1 , MPI_DOUBLE , MPI_SUM , MPI_COMM_WORLD );)"}
{"predict": "(12,OpenMP,#pragma omp parallel for reduction(+:local[id]))", "label": "(3, CUDA, __shared__ float local[threads];)"}
{"predict": "(2,OpenMP,#pragma omp parallel for), (9,OpenMP,#pragma omp for reduction(+:sum)), (16,OpenMP,#pragma omp barrier), (24,OpenMP,#pragma omp for reduction(+:variance)), (27,OpenMP,#pragma omp barrier)", "label": "(9, CUDA, __shared__ float s_variance;); (7, CUDA, __shared__ float s_mean;)"}
{"predict": "(7,MPI,*w = MPI_Wtime();); (14,OpenMP,#pragma omp parallel for), (60,OpenMP,#pragma omp parallel for), (77,OpenMP,#pragma omp parallel for)", "label": "(99, MPI, MPI_File_open(MPI_COMM_WORLD, file_name.c_str(), MPI_MODE_CREATE | MPI_MODE_WRONLY, MPI_INFO_NULL, &file);); (105, MPI, MPI_File_write(file, &size_array, 1, MPI_INT, &status);); (110, MPI, MPI_File_close(&file);); (102, MPI, MPI_File_seek(file, mpi_offset, MPI_SEEK_SET);); (107, MPI, MPI_File_write(file, &pos_values[0], data_size[rank], MPI_DOUBLE, &status);)"}
{"predict": "(11,MPI,*w = MPI_Wtime(););\n(28,OpenMP,#pragma omp parallel for),", "label": "(16, CUDA, extern __shared__ volatile float Bsh[];)"}
{"predict": "(27,OpenMP,#pragma omp parallel for)", "label": "(8, MPI, MPI_Init(&argc, &argv);); (28, MPI, MPI_Finalize();)"}
{"predict": "(25,OpenMP,#pragma omp parallel for)", "label": "(20, CUDA, cudaStream_t stream = at::cuda::getCurrentCUDAStream();)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (107,OpenMP,#pragma omp parallel for), (111,MPI,auto w_end = MPI_Wtime();)", "label": "(85, CUDA, cudaDeviceSynchronize();); (59, CUDA, cudaMalloc(&p.workspace, p.workspace_size());); (114, CUDA, CUDA_CHECK(cudaEventSynchronize(events[1]));); (112, CUDA, CUDA_CHECK(cudaEventRecord(events[1]));); (84, CUDA, CUDA_CHECK(cudaFuncSetAttribute(kernel_fn, cudaFuncAttributeMaxDynamicSharedMemorySize, int(smem_bytes)));); (114, CUDA, CUDA_CHECK(cudaEventElapsedTime(&runtime_ms, events[0], events[1]));); (9, CUDA, std::cerr << \"FATAL: \" #FN \" failed: \" << cudaGetErrorString(cudaError) << std::endl;); (102, CUDA, CUDA_CHECK(cudaEventCreate(&event));); (103, CUDA, CUDA_CHECK(cudaEventRecord(events[0]));)"}
{"predict": "(15,OpenMP,#pragma omp parallel for), (20,MPI,*w = MPI_Wtime(););", "label": "(19, CUDA, cudaMemcpy(pointers, cpuPointers, sizeof(uint8_t*) * total * 3, cudaMemcpyHostToDevice);); (6, CUDA, uint8_t ** pointers = (uint8_t**)FastllmCudaMalloc(sizeof(uint8_t*) * total * 3);); (20, CUDA, FastllmCudaFree(pointers);)"}
{"predict": ";\n112:         }\n113:     }\n114:     Print( A, \"Matrix A 2\" );\n115:     return 0;\n116: }", "label": "(106, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ j + 1 ], 0, MPI_COMM_WORLD );); (104, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ j - 1 ], 1, MPI_COMM_WORLD );); (108, MPI, MPI_Recv( A[ 3 ].data(), N + 1, MPI_DOUBLE, activep[ j - 1 ], 0, MPI_COMM_WORLD, &status );); (7, MPI, MPI_Comm_size( MPI_COMM_WORLD, &Parallel::nprocs );); (97, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ numactivep - 2 ], 1, MPI_COMM_WORLD );); (135, MPI, MPI_Allgather( &tmp, 1, MPI_DOUBLE, x.data(), 1, MPI_DOUBLE, MPI_COMM_WORLD);); (187, MPI, MPI_Finalize();); (157, MPI, MPI_Allgather( &tmp, 1, MPI_DOUBLE, x.data(), 1, MPI_DOUBLE, MPI_COMM_WORLD );); (1, MPI, MPI_Init( &argc, &argv );); (2, MPI, MPI_Comm_rank( MPI_COMM_WORLD, &Parallel::pid );); (101, MPI, MPI_Recv( A[ 4 ].data(), N + 1, MPI_DOUBLE, activep[ j + 1 ], 1, MPI_COMM_WORLD, &status );); (86, MPI, MPI_Send( A[ 1 ].data(), N + 1, MPI_DOUBLE, activep[ 1 ], 0, MPI_COMM_WORLD );)"}
{"predict": "(28,OpenMP,#pragma omp parallel for), (33,OpenMP,#pragma omp parallel for)", "label": "(25, OpenMP, #pragma omp parallel default(shared)); (10, OpenMP, #pragma omp parallel); (37, OpenMP, #pragma omp parallel num_threads(x)); (15, OpenMP, #pragma omp parallel default(none)); (28, OpenMP, #pragma omp parallel default(firstprivate))"}
{"predict": "(1,MPI,#include <mpi.h>); (5,MPI,MPI_Init(NULL, NULL);); (7,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (8,MPI,MPI_Comm_size(MPI_COMM_WORLD, &size);); (11,MPI,MPI_Bcast(B, wB * wB, MPI_FLOAT, 0, MPI_COMM_WORLD);); (33,MPI,MPI_Bcast(A, wA * wA, MPI_FLOAT, 0, MPI_COMM_WORLD);); (71,MPI,MPI_Finalize(););", "label": "(41, CUDA, __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];); (36, CUDA, __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (7,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for)", "label": "(12, CUDA, __shared__ unsigned char sharedHistogram[NUM_THREADS][cBuckets];)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for), (30,OpenMP,#pragma omp parallel for)", "label": "(25, TBB, tbb::flow::make_edge(bnode, tbb::flow::input_port<0>(jnode));); (7, TBB, tbb::flow::make_edge(bnode, fnode);); (22, TBB, tbb::flow::join_node<std::tuple<int,int>,tbb::flow::reserving> jnode(g);); (-1, TBB, tbb::flow::sequencer_node<int> bnode(g, seq_body());); (-3, TBB, tbb::flow::graph g;); (32, TBB, g.reset(tbb::flow::rf_clear_edges);); (-2, TBB, tbb::flow::function_node<int> fnode(g, tbb::flow::serial, serial_fn_body<int>(serial_fn_state0));); (15, TBB, tbb::flow::remove_edge(bnode, fnode);)"}
{"predict": "(32,OpenMP,#pragma omp parallel for), (60,OpenMP,#pragma omp parallel for)", "label": "(34, TBB, oneapi::tbb::tick_count start = oneapi::tbb::tick_count::now();); (69, TBB, utility::report_elapsed_time((oneapi::tbb::tick_count::now() - start).seconds());); (49, TBB, oneapi::tbb::flow::make_edge(the_source, the_value_pool);); (53, TBB, oneapi::tbb::flow::make_edge(the_value_pool, *(bins[i]));); (44, TBB, oneapi::tbb::flow::graph g;); (31, TBB, oneapi::tbb::global_control c(oneapi::tbb::global_control::max_allowed_parallelism, p);)"}
{"predict": "(4,OpenMP,#pragma omp parallel for)\n(8,OpenMP,#pragma omp parallel for)\n(10,OpenMP,#pragma omp parallel for)\n(12,OpenMP,#pragma omp parallel for)\n(16,OpenMP,#pragma omp parallel for)\n(18,OpenMP,#pragma omp parallel for)\n(22,OpenMP,#pragma omp parallel for)\n(30,OpenMP,#pragma omp parallel for)\n(46,OpenMP,#pragma omp parallel for)", "label": "(37, OpenMP, #pragma omp target teams distribute parallel for simd safelen (S)); (17, OpenMP, #pragma omp target teams distribute parallel for simd safelen (ST); (20, OpenMP, #pragma omp target teams distribute parallel for simd safelen (1))); (51, OpenMP, #pragma omp target teams distribute parallel for simd safelen (N)); (46, OpenMP, #pragma omp target teams distribute parallel for simd safelen (4)); (2, OpenMP, #pragma omp target teams distribute parallel for simd safelen (); (20, OpenMP, #pragma omp target teams distribute parallel for simd safelen ((ST > 0) ? 1 + ST : 2)); (26, OpenMP, #pragma omp target teams distribute parallel for simd safelen (foobool(argc)), safelen (true), safelen (-5)); (3, OpenMP, #pragma omp target teams distribute parallel for simd safelen ()); (5, OpenMP, #pragma omp target teams distribute parallel for simd safelen (argc); (-7, OpenMP, #pragma omp target teams distribute parallel for simd safelen)"}
{"predict": "(24,OpenMP,#pragma omp parallel for private(tid)), (49,OpenMP,#pragma omp parallel for private(tid))", "label": "(13, CUDA, extern __shared__ char shared_mem[];)"}
{"predict": "(15,MPI, *w = MPI_Wtime(););", "label": "(14, MPI, MPI_Abort(MPI_COMM_WORLD, 127);)"}
{"predict": "(37,OpenMP,#pragma omp parallel for), (44,MPI,MPI_Allreduce(&sum, &sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);)", "label": "(51, CUDA, cuda_safe_call(cudaStreamSynchronize(stream));)"}
{"predict": "(14,OpenMP,#pragma omp parallel for), (28,OpenMP,#pragma omp parallel for)", "label": "(34, OpenCL, TEST_OCL_CHECK(clReleaseCommandQueue(interop_ocl_queue));)"}
{"predict": "(10,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (35,OpenMP,#pragma omp parallel for)", "label": "(15, OpenMP, #pragma omp taskloop simd private(argc, b), firstprivate(c, d), lastprivate(d, f) collapse(N) shared(g) if (c) final(d) mergeable priority(f) simdlen(N) nogroup num_tasks(N)); (13, OpenMP, #pragma omp parallel); (5, OpenMP, #pragma omp taskloop simd allocate(d) if(taskloop: argc > N) default(shared) untied priority(N) safelen(N) linear(c) aligned(ptr) grainsize(N) reduction(+:g) in_reduction(+: d)); (3, OpenMP, #pragma omp taskgroup task_reduction(+: d) allocate(d))"}
{"predict": "(18,OpenMP,#pragma omp parallel for private(i)), (33,OpenMP,#pragma omp parallel for private(i)), (43,OpenMP,#pragma omp parallel for private(i))", "label": "(10, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (43, MPI, mpi_errno = MPI_Comm_free(&stream_comm);); (7, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (15, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for)", "label": "(18, MPI, *ierr = MPI_Info_delete( (MPI_Info)(*v1), p2 );)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (...)\n\n### **Explanation**\n- **MPI**: The line `*w = MPI_Wtime();` should be inserted at the beginning of the function to record the start time. This line is not already present in the code.\n- **OpenMP**: The line `#pragma omp parallel for` should be inserted before the inner loop to parallelize the outer loop. This will allow the outer loop to be executed in parallel by multiple threads.", "label": "(2, TBB, tbb::tick_count t0 = tbb::tick_count::now();); (11, TBB, tbb::tick_count t1 = tbb::tick_count::now();)"}
{"predict": "(2,MPI,extern \"C\" { int MPI_Init(int *argc, char ***argv); int MPI_Comm_rank(MPI_Comm comm, int *rank); int MPI_Comm_size(MPI_Comm comm, int *size); int MPI_Finalize(); }); (5,MPI,int rank, size; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &rank); MPI_Comm_size(MPI_COMM_WORLD, &size);); (6,MPI,if (rank == 0)); (15,MPI,MPI_Finalize());", "label": "(12, CUDA, A_[j] = static_cast<thrust::complex<T1>>(B_[j]);)"}
{"predict": "(10,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for), (29,OpenMP,#pragma omp parallel for)", "label": "(7, OpenMP, #pragma omp for linear (); (24, OpenMP, #pragma omp for linear (argv[1])); (17, OpenMP, #pragma omp for linear (S1)); (12, OpenMP, #pragma omp for linear (argc > 0 ? argv[1] : argv[2])); (1, OpenMP, #pragma omp for linear); (22, OpenMP, #pragma omp for linear(e, g, k)); (27, OpenMP, #pragma omp parallel); (33, OpenMP, #pragma omp for linear(j)); (3, OpenMP, #pragma omp for linear (argc); (34, OpenMP, #pragma omp for linear(v:j)); (-1, OpenMP, #pragma omp for linear ()); (2, OpenMP, #pragma omp for linear (argc,); (11, OpenMP, #pragma omp for linear (a, b:B::ib)); (34, OpenMP, #pragma omp for linear(i) ordered(1)); (15, OpenMP, #pragma omp for linear(h)); (2, OpenMP, #pragma omp for linear (argc : 5) allocate , allocate(, allocate(omp_default , allocate(omp_default_mem_alloc, allocate(omp_default_mem_alloc:, allocate(omp_default_mem_alloc: argc, allocate(omp_default_mem_alloc: argv), allocate(argv)); (21, OpenMP, #pragma omp for linear(v:i)); (14, OpenMP, #pragma omp for linear(i))"}
{"predict": "(19,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for)", "label": "(16, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);)"}
{"predict": "(9,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for)", "label": "(8, CUDA, cudaStreamSynchronize(LU_v1->A_gpu.cuStreams[0]);)"}
{"predict": "(25,MPI,MPI_Init_thread(&argc, &argv, required, &provided);)", "label": "(31, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(2,OpenMP,#pragma omp parallel for collapse(2)), (18,OpenMP,#pragma omp parallel for collapse(2)), (22,OpenMP,#pragma omp atomic add)", "label": "(5, CUDA, __shared__ float reduce_smem[NUM_WARPS];)"}
{"predict": "(10,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for), (52,OpenMP,#pragma omp parallel for)", "label": "(16, CUDA, extern __shared__ float buffer[];)"}
{"predict": "scr_free(&dirs);\n151:       scr_free(&depths);\n152:       scr_free(&group_id);\n153:       scr_free(&group_ranks);\n154:       scr_free(&group_rank);\n155:       return rc;\n156:     }\n157: \n158:     /* identify leader for each directory */\n159:     uint64_t* leaders = (uint64_t*) SCR_MALLOC(sizeof(uint64_t) * num_dirs);\n160:     int i = 0;\n161:     while (i < num_dirs) {\n162:       uint64_t group_rank = group_ranks[i];\n163:       leaders[group_rank] = group_id[i];\n164:       i++;\n165:     }\n166: \n167:     /* broadcast leaders to all procs */\n168:     scr_broadcast(&leaders, sizeof(uint64_t) * num_dirs, 0, scr_comm_world);\n169: \n170:     /* determine if we are the leader for this directory */\n171:     int leader = leaders[group_rank];\n172:     scr_free(&leaders);\n173: \n174:     /* if we are the leader, delete the directory */\n175:     if (leader == group_id[source_rank]) {\n176:       /* build path to directory */\n177:       spath* dir_path = spath_dup(scr_prefix_path);\n178:       spath_append_str(dir_path, file);\n179:       spath_reduce(dir_path);\n180:       char* dir = spath_strdup(dir_path);\n181:       spath_delete(&dir_path);\n182: \n183:       /* delete the directory */\n184:       scr_file_unlink(dir);\n185: \n186:       /* free directory string */\n187:       scr_free(&dir);\n188:     }\n189: \n190:     /* free the directories and depths arrays */\n191:     scr_free(&dirs);\n192:     scr_free(&depths);\n193:     scr_free(&group_id);\n194:     scr_free(&group_ranks);\n195:     scr_free", "label": "(85, MPI, MPI_Bcast(&min_source, 1, MPI_INT, source, scr_comm_world);); (96, MPI, MPI_Allreduce(&min_depth, &min_global, 1, MPI_INT, MPI_MIN, scr_comm_world);); (76, MPI, MPI_Allreduce(&source_rank, &source, 1, MPI_INT, MPI_MIN, scr_comm_world);); (95, MPI, MPI_Allreduce(&max_depth, &max_global, 1, MPI_INT, MPI_MAX, scr_comm_world);); (168, MPI, MPI_Barrier(scr_comm_world);)"}
{"predict": "(23,MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);) (24,MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);) (25,MPI, if (rank == 0) {) (26,MPI, MPI_Bcast(&seed, 1, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);) (27,MPI, MPI_Bcast(&offset, 1, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);) (28,MPI, MPI_Bcast(&block_dim, 1, MPI_UNSIGNED_INT, 0, MPI_COMM_WORLD);) (29,MPI, MPI_Bcast(&grid_size, 1, MPI_UNSIGNED_INT, 0, MPI_COMM_WORLD);) (30,MPI, MPI_Bcast(&mean, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);) (31,MPI, MPI_Bcast(&stddev, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);) (32,MPI, MPI_Bcast(&size, 1, MPI_SIZE_T, 0, MPI_COMM_WORLD);) (34,MPI, MPI_Bcast(d_out, size, MPI_DOUBLE, 0, MPI_COMM_WORLD);) (36,MPI, MPI_Finalize();) (50,MPI, MPI_Finalize();)", "label": "(8, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc((void**)&d_out, size * sizeof(DataType)));); (32, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(h_out.data(), d_out, size * sizeof(DataType), cudaMemcpyDeviceToHost));); (28, CUDA, CUDA_CHECK_AND_EXIT(cudaDeviceSynchronize());); (35, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc((void**)&d_ref, size * sizeof(DataType)));); (48, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(d_out));); (16, CUDA, CUDA_CHECK_AND_EXIT(cudaMalloc((void**)&states, block_dim * grid_size * sizeof(RNG)));); (19, CUDA, CUDA_CHECK_AND_EXIT(cudaPeekAtLastError());); (44, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(states));); (45, CUDA, CUDA_CHECK_AND_EXIT(cudaFree(d_ref));); (39, CUDA, CUDA_CHECK_AND_EXIT(cudaMemcpy(h_ref.data(), d_ref, size * sizeof(DataType), cudaMemcpyDeviceToHost));)"}
{"predict": "(36,OpenMP,#pragma omp parallel for)", "label": "(36, OpenCL, err = clSetKernelArg(scan_kernel, 1, dyn_mem_size, NULL);); (31, OpenCL, err = clSetKernelArg(scan_kernel, 0, sizeof(buffer), &buffer);)"}
{"predict": "(20,MPI,if (err != MPI_SUCCESS) { errs++; if (verbose) { fprintf(stderr, \"MPI_Type_create_struct failed, err=%d\\n\", err); } }), (25,MPI,if (err != MPI_SUCCESS) { errs++; if (verbose) { fprintf(stderr, \"MPI_Type_create_struct failed, err=%d\\n\", err); } })", "label": "(22, MPI, err = MPI_Type_create_resized(tmptype, 0, aext * sizeof(int), datatype);); (28, MPI, MPI_Type_free(&tmptype);); (12, MPI, err = MPI_Type_create_struct(1, &blen, &displ, &type, &tmptype);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (...)\n\n### ** Code **\n\n1: template <class T>\n2: T seq_cst() {\n3:   T a, b = 0;\n4: // Test for atomic seq_cst\n5:   // expected-error@+2 {{the statement for 'atomic' must be an expression statement of form '++x;', '--x;', 'x++;', 'x--;', 'x binop= expr;', 'x = x binop expr' or 'x = expr binop x', where x is an lvalue expression with scalar type}}\n6:   // expected-note@+1 {{expected an expression statement}}\n7:   ;\n8: // expected-error@+1 {{directive '#pragma omp atomic' cannot contain more than one 'seq_cst' clause}}\n9:   a += b;\n10: \n11:   // expected-error@+2 {{the statement for 'atomic update' must be an expression statement of form '++x;', '--x;', 'x++;', 'x--;', 'x binop= expr;', 'x = x binop expr' or 'x = expr binop x', where x is an lvalue expression with scalar type}}\n12:   // expected-note@+1 {{expected an expression statement}}\n13:   ;\n14: \n15:   return T();\n16: }", "label": "(4, OpenMP, #pragma omp atomic seq_cst); (8, OpenMP, #pragma omp atomic seq_cst seq_cst); (10, OpenMP, #pragma omp atomic update seq_cst)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (10,OpenMP,#pragma omp parallel for)", "label": "(12, CUDA, img[idx] = cudaAlphaBlend(img[idx], color);)"}
{"predict": "(3,MPI,extern int rank;); (5,MPI,extern int rank;); (7,MPI,MPI_Init(&argc, &argv);); (29,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (32,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (35,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (38,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (42,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (45,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (48,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (51,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (54,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (57,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (60,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (63,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (66,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (69,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (72,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (75,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (78,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (81,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (84,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (87,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (90,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (93,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (96,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (99,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (102,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (105,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (108,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (111,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank", "label": "(25, MPI, MPI_Bcast(&len, 1, MPI_INT, 0, MPI_COMM_WORLD);); (40, MPI, err = MPI_File_close(&fh);); (4, MPI, MPI_Init(&argc, &argv);); (143, MPI, MPI_Finalize();); (22, MPI, MPI_Bcast(filename, len + 10, MPI_CHAR, 0, MPI_COMM_WORLD);); (2, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (39, MPI, MPI_Barrier(MPI_COMM_WORLD);); (12, MPI, MPI_Abort(MPI_COMM_WORLD, 1);)"}
{"predict": "MAX(>) test failed\\n\");\n145:                 }\n146:             }\n147:         }\n148:     }\n149: #endif /* HAVE_LONG_LONG */\n150: \n151:     MTest_Finalize();\n152:     MPI_Finalize();\n153:     return 0;\n154: }", "label": "(106, MPI, MPI_Reduce(ldinbuf, ldoutbuf, 3, MPI_LONG_DOUBLE, MPI_MAX, 0, comm);); (13, MPI, MPI_Comm_size(comm, &size);); (24, MPI, MPI_Reduce(cinbuf, coutbuf, 3, MPI_CHAR, MPI_MAX, 0, comm);); (49, MPI, MPI_Reduce(scinbuf, scoutbuf, 3, MPI_SIGNED_CHAR, MPI_MAX, 0, comm);); (73, MPI, MPI_Reduce(ucinbuf, ucoutbuf, 3, MPI_UNSIGNED_CHAR, MPI_MAX, 0, comm);); (8, MPI, MPI_Comm_rank(comm, &rank);); (132, MPI, MPI_Reduce(llinbuf, lloutbuf, 3, MPI_LONG_LONG, MPI_MAX, 0, comm);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (11,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for), (29,OpenMP,#pragma omp parallel for), (32,OpenMP,#pragma omp parallel for)", "label": "(19, OpenMP, #pragma omp distribute parallel for proc_bind(master); (3, OpenMP, #pragma omp distribute parallel for proc_bind); (12, OpenMP, #pragma omp distribute parallel for proc_bind()); (6, OpenMP, #pragma omp distribute parallel for proc_bind(); (25, OpenMP, #pragma omp distribute parallel for proc_bind(x)); (37, OpenMP, #pragma omp distribute parallel for proc_bind(spread)); (18, OpenMP, #pragma omp distribute parallel for proc_bind(close), proc_bind(spread)); (-5, OpenMP, #pragma omp target); (31, OpenMP, #pragma omp parallel proc_bind(close)); (26, OpenMP, #pragma omp distribute parallel for proc_bind(master)); (-7, OpenMP, #pragma omp teams)"}
{"predict": "(2,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for)", "label": "(19, OpenCL, retVal = clEnqueueReleaseGLObjects(commandQueue, 1, &glBuffer, 0, nullptr, &retEvent);); (30, OpenCL, retVal = clReleaseMemObject(glBuffer);); (3, OpenCL, auto glBuffer = clCreateFromGLBuffer(&context, 0, bufferId, &retVal);); (1, OpenCL, auto commandQueue = clCreateCommandQueue(&context, context.getDevice(0), 0, &retVal);); (12, OpenCL, retVal = clReleaseEvent(retEvent);); (1, OpenCL, retVal = clEnqueueAcquireGLObjects(commandQueue, 1, &glBuffer, 0, nullptr, &retEvent);); (23, OpenCL, retVal = clReleaseCommandQueue(commandQueue);)"}
{"predict": "(11,MPI,MPI_Init(&argc, &argv);)\n(55,MPI,MPI_Finalize();)\n(23,OpenMP,#pragma omp parallel for reduction(+:errs)), (43,OpenMP,#pragma omp parallel for reduction(+:errs))", "label": "(21, MPI, MPI_Type_set_attr(type, key[2], &attrval[2]);); (22, MPI, MPI_Type_set_attr(type, key[0], &attrval[0]);); (56, MPI, MPI_Type_free_keyval(&key[i]);); (19, MPI, MPI_Type_set_attr(type, key[1], &attrval[1]);); (23, MPI, MPI_Type_delete_attr(type, key[0]);); (23, MPI, MPI_Type_delete_attr(type, key[1]);); (23, MPI, MPI_Type_delete_attr(type, key[2]);)"}
{"predict": "(2,MPI,*w = MPI_Wtime();); (10,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for)", "label": "(5, CUDA, __shared__ float reduce_smem[NUM_WARPS];)"}
{"predict": "(21,OpenMP,#pragma omp parallel for private(cpu_id, from, to)), (26,OpenMP,if (omp_get_thread_num() == 0)), (42,OpenMP,#pragma omp parallel for reduction(+:delta))", "label": "(8, TBB, tbb::enumerable_thread_specific<HyperedgeWeight> deltas(0);)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (12,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for)", "label": "(8, OpenCL, retVal = clEnqueueMemcpyINTEL(commandQueues[0][0].get(), true, deviceMemAlloc, buffer.data(), bufferSize, 0, nullptr, nullptr);); (16, OpenCL, retVal = clEnqueueMemcpyINTEL(commandQueues[0][0].get(), true, hostMemAlloc, deviceMemAlloc, bufferSize, 0, nullptr, nullptr);)"}
{"predict": "(15,OpenMP,#pragma omp parallel for), (31,MPI,MPI_Send(&message[0], msgsize, MPI_INT, 1, 0, MPI_COMM_WORLD);), (38,MPI,MPI_Recv(&receiveBuffer[0], msgsize, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);), (43,MPI,MPI_Recv(&receiveBuffer[0], msgsize, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);), (46,MPI,MPI_Send(&message[0], msgsize, MPI_INT, 0, 0, MPI_COMM_WORLD);)", "label": "(42, MPI, MPI_Recv(receiveBuffer.data(), msgsize, MPI_INT, 1, MPI_ANY_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (7, MPI, MPI_Init(&argc, &argv);); (48, MPI, MPI_Send(message.data(), msgsize, MPI_INT, 0, 1, MPI_COMM_WORLD);); (54, MPI, MPI_Finalize();); (5, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (5, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (33, MPI, MPI_Send(message.data(), msgsize, MPI_INT, 1, tag, MPI_COMM_WORLD);); (8, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (41, MPI, MPI_Recv(receiveBuffer.data(), msgsize, MPI_INT, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status);)"}
{"predict": "(18,OpenMP,#pragma omp parallel for)", "label": "(9, CUDA, CUDA_CHECK(cudaGetDeviceCount(&ndevices));); (9, CUDA, CUDA_CHECK(cudaSetDevice(rank % ndevices));); (0, MPI, MPI_Init(&argc, &argv);); (37, MPI, MPI_Finalize();); (1, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (1, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for reduction(+:rmsscale)), (17,OpenMP,#pragma omp atomic)", "label": "(2, CUDA, extern __shared__ char smem[];)"}
{"predict": "en) {\n120:       for (int i = 0; i < groupTx; i++) {\n121:         O[row * dim + i * Bc + tx] = sNewO[ty][i * Bc + tx];\n122:       }\n123:     }\n124: \n125:     // update global max and denom\n126:     if (row < seqlen) {\n127:       gMAX[row] = newMax;\n128:       gDenom[row] = newDenom;\n129:     }\n130:   }\n131: }", "label": "(12, CUDA, __shared__ FP sV[Bc][dim];); (20, CUDA, __shared__ FP sMax[Br];); (9, CUDA, __shared__ FP sK[Bc][dim];); (11, CUDA, __shared__ FP sQK[Br][Bc];); (14, CUDA, __shared__ FP sSafeE[Br][Bc];); (8, CUDA, __shared__ FP sO[Br][dim];); (10, CUDA, __shared__ FP sNewO[Br][dim];); (3, CUDA, __shared__ FP sQ[Br][dim];); (12, CUDA, __shared__ FP sDenom[Br];)"}
{"predict": "(9,OpenMP,#pragma omp parallel for)", "label": "(7, OpenMP, #pragma omp parallel for)"}
{"predict": "(5,MPI,*w = MPI_Wtime();)", "label": "(12, CUDA, cudaError_t error = cudaGetDeviceProperties(&props, 0);); (13, CUDA, std::cerr << \"cudaGetDeviceProperties() returned an error: \" << cudaGetErrorString(error) << std::endl;)"}
{"predict": "(34,OpenMP,#pragma omp parallel for), (36,OpenMP,#pragma omp parallel for)", "label": "(16, OpenCL, clCreateBuffer_StubWithCallback(clCreateBuffer_testBufferConstructorContextIterator);); (35, OpenCL, clReleaseCommandQueue_ExpectAndReturn(make_command_queue(0), CL_SUCCESS);); (43, OpenCL, clReleaseMemObject_ExpectAndReturn(expected, CL_SUCCESS);); (24, OpenCL, clCreateCommandQueueWithProperties_StubWithCallback(clCreateCommandQueueWithProperties_testCommandQueueFromSpecifiedContext);); (31, OpenCL, clReleaseEvent_StubWithCallback(clReleaseEvent_testCopyHostToBuffer);); (41, OpenCL, clReleaseContext_ExpectAndReturn(make_context(0), CL_SUCCESS);); (26, OpenCL, clEnqueueMapBuffer_StubWithCallback(clEnqueueMapBuffer_testCopyHostToBuffer);); (26, OpenCL, clEnqueueUnmapMemObject_StubWithCallback(clEnqueueUnmapMemObject_testCopyHostToBuffer);)"}
{"predict": "(13,MPI,MPI_Init(&argc, &argv);) (14,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank);) (15,MPI,MPI_Comm_size(MPI_COMM_WORLD, &ntasks);) (22,MPI,MPI_Cart_create(MPI_COMM_WORLD, ndims, dims, periods, 0, &cart_comm);) (23,MPI,MPI_Cart_shift(cart_comm, 0, 1, &source, &destination);) (26,MPI,MPI_Comm_rank(cart_comm, &cart_id);) (32,MPI,t0 = MPI_Wtime();) (35,MPI,MPI_Sendrecv(message.data(), message.size(), MPI_INT, destination, cart_id + 1, receiveBuffer.data(), receiveBuffer.size(), MPI_INT, source, cart_id, cart_comm, &status);) (38,MPI,t1 = MPI_Wtime();) (49,MPI,MPI_Finalize();)", "label": "(33, MPI, MPI_Cart_shift(cart_comm, 0, 1, &source, &destination);); (52, MPI, t1 = MPI_Wtime();); (12, MPI, MPI_Init(&argc, &argv);); (57, MPI, MPI_Finalize();); (11, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ntasks);); (27, MPI, MPI_Cart_create(MPI_COMM_WORLD, ndims, dims, periods, 1, &cart_comm);); (10, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (52, MPI, MPI_Comm_free(&cart_comm);); (27, MPI, MPI_Comm_rank(cart_comm, &cart_id);); (29, MPI, MPI_Barrier(MPI_COMM_WORLD);); (29, MPI, t0 = MPI_Wtime();)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for), (29,OpenMP,#pragma omp parallel for), (48,OpenMP,#pragma omp parallel for)", "label": "(8, MPI, MPI_Comm_rank(mpi_comm, &rank);); (6, MPI, MPI_Comm mpi_comm = MPI_Comm_f2c(vtkh::GetMPICommHandle());); (56, MPI, MPI_Bcast(&entropy, 1, MPI_DOUBLE, 0, mpi_comm);)"}
{"predict": "reatt]\\n\"); validate_result(d_dpreatt, dpreatt, \"dpreatt\", B * NH * T * T, 5e-3f);\n68:     printf(\"[dinp]\\n\");    validate_result(d_dinp, dinp, \"dinp\", B * T * 3 * C, 1e-3f);\n69: \n70:     // clean up memory\n71:     free(inp);\n72:     free(qkvr);\n73:     free(preatt);\n74:     free(att);\n75:     free(vaccum);\n76:     free(out);\n77:     free(d_inp);\n78:     free(d_qkvr);\n79:     free(d_preatt);\n80:     free(d_att);\n81:     free(d_vaccum);\n82:     free(d_out);\n83:     free(d_dinp);\n84:     free(d_dqkvr);\n85:     free(d_dpreatt);\n86:     free(d_datt);\n87:     free(d_dvaccum);\n88:     free(d_dout);\n89:     free(dout);\n90:     free(dinp);\n91:     free(dpreatt);\n92:     free(datt);\n93:     return 0;\n94: }\n\n### **Analysis and Recommendations**\n\n1. The forward pass is executed on the CPU and GPU.\n2. The backward pass is executed on the CPU and GPU.\n3. The memory allocation and deallocation are done on the CPU.\n4. The backward pass gradients are checked on the CPU.\n\n### **Parallelization Recommendations**\n\n1. **Forward Pass on GPU:** The forward pass on the GPU can be parallelized using CUDA or OpenMP.\n2. **Backward Pass on GPU:** The backward pass on the GPU can be parallelized using CUDA or OpenMP.\n3. **Memory Allocation and Deallocation:** These operations are sequential and cannot be parallelized.\n4. **Backward Pass Gradient Checking:** This operation is sequential and cannot be parallelized.\n\n### **Parallelization Code Lines**\n\n1. **Forward Pass on GPU:** Insert OpenMP parallel for loop around the forward pass on the GPU.\n2. **Backward Pass on GPU:** Insert OpenMP parallel for loop around the backward pass on the GPU.\n\n### **Final Output**\n\n(57,OpenMP,#pragma omp parallel for", "label": "(68, CUDA, cudaCheck(cudaMemset(d_dinp, 0, B * T * 3 * C * sizeof(float)));); (60, CUDA, cudaCheck(cudaMalloc(&d_dpreatt, B * NH * T * T * sizeof(float)));); (157, CUDA, cudaCheck(cudaFree(d_inp));); (69, CUDA, cudaCheck(cudaMemset(d_dvaccum, 0, B * T * C * sizeof(float)));); (86, CUDA, cudaCheck(cudaMemcpy(h_dinp, d_dinp, B * T * 3 * C * sizeof(float), cudaMemcpyDeviceToHost));); (162, CUDA, cudaCheck(cudaFree(d_dpreatt));); (159, CUDA, cudaCheck(cudaFree(d_dinp));); (159, CUDA, cudaCheck(cudaFree(d_dqkvr));); (152, CUDA, cudaCheck(cudaFree(d_qkvr));); (23, CUDA, cudaCheck(cudaMalloc(&d_att, B * NH * T * T * sizeof(float)));); (158, CUDA, cudaCheck(cudaFree(d_datt));); (58, CUDA, cudaCheck(cudaMemset(d_dqkvr, 0, B * T * 3 * C * sizeof(float)));); (149, CUDA, cudaCheck(cudaFree(d_preatt));); (50, CUDA, cudaCheck(cudaMalloc(&d_dvaccum, B * T * C * sizeof(float)));); (50, CUDA, cudaCheck(cudaMalloc(&d_dout, B * T * C * sizeof(float)));); (15, CUDA, cudaCheck(cudaMalloc(&d_qkvr, B * T * 3 * C * sizeof(float)));); (17, CUDA, cudaCheck(cudaMalloc(&d_vaccum, B * T * C * sizeof(float)));); (14, CUDA, cudaCheck(cudaMalloc(&d_preatt, B * NH * T * T * sizeof(float)));); (48, CUDA, cudaCheck(cudaMemcpy(d_dout, dout, B * T * C * sizeof(float), cudaMemcpyHostToDevice));); (144, CUDA, cudaCheck(cudaFree(d_vaccum));); (40, CUDA, cudaCheck(cudaMalloc(&d_dqkvr, B * T * 3 * C * sizeof(float)));); (38, CUDA, cudaCheck(cudaMalloc(&d_dinp, B * T * 3 * C * sizeof(float)));); (49, CUDA, cudaCheck(cudaMemset(d_datt, 0, B * NH * T * T * sizeof(float)));); (141, CUDA, cudaCheck(cudaFree(d_out));); (145, CUDA, cudaCheck(cudaFree(d_dvaccum));); (9, CUDA, cudaCheck(cudaMalloc(&d_out, B * T * C * sizeof(float)));); (36, CUDA, cudaCheck(cudaMalloc(&d_datt, B * NH * T * T * sizeof(float)));); (2, CUDA, cudaCheck(cudaMalloc(&d_inp, B * T * 3 * C * sizeof(float)));); (134, CUDA, cudaCheck(cudaFree(d_att));); (41, CUDA, cudaCheck(cudaMemset(d_dpreatt, 0, B * NH * T * T * sizeof(float)));); (6, CUDA, cudaCheck(cudaMemcpy(d_inp, inp, B * T * 3 * C * sizeof(float), cudaMemcpyHostToDevice));); (139, CUDA, cudaCheck(cudaFree(d_dout));)"}
{"predict": "(11,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for)", "label": "(9, CUDA, CHECK(cudaMallocManaged((void **)&z, M));); (23, CUDA, CHECK(cudaFree(x));); (5, CUDA, CHECK(cudaMallocManaged((void **)&x, M));); (22, CUDA, CHECK(cudaFree(y));); (22, CUDA, CHECK(cudaFree(z));); (16, CUDA, CHECK(cudaDeviceSynchronize());); (2, CUDA, CHECK(cudaMallocManaged((void **)&y, M));)"}
{"predict": "(27,MPI,value = tempvalue;); (49,MPI,value = 0.;); (52,MPI,value = SUPERLU_MAX(value, temprwork[j]);); (71,MPI,ABORT(\"Not implemented.\"););", "label": "(26, MPI, MPI_Allreduce(&value, &tempvalue, 1, MPI_DOUBLE, MPI_MAX, grid->comm);); (50, MPI, MPI_Allreduce(rwork, temprwork, A->ncol, MPI_DOUBLE, MPI_SUM, grid->comm);)"}
{"predict": "(29,OpenMP,#pragma omp parallel for private(i,j)), (44,OpenMP,#pragma omp parallel for private(i)), (52,OpenMP,#pragma omp parallel for private(i,j))", "label": "(17, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (26, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(7,MPI,*w = MPI_Wtime();); (10,MPI,*w = MPI_Wtime();); (13,MPI,*w = MPI_Wtime();); (16,MPI,*w = MPI_Wtime();); (19,MPI,*w = MPI_Wtime();); (22,MPI,*w = MPI_Wtime(););", "label": "(25, OpenCL, clReleaseMemObject(scratchBuff);); (20, OpenCL, clReleaseMemObject(mobjDP_);); (15, OpenCL, clReleaseMemObject(mobjY_);); (10, OpenCL, clReleaseMemObject(mobjX_);)"}
{"predict": "(29,OpenMP,#pragma omp parallel for), (31,OpenMP,#pragma omp parallel for)", "label": "(5, TBB, oneapi::tbb::global_control c(oneapi::tbb::global_control::max_allowed_parallelism, nthreads);); (21, TBB, static oneapi::tbb::affinity_partitioner g_ap;)"}
{"predict": "(4,OpenMP,#pragma omp parallel for), (48,OpenMP,#pragma omp parallel for)", "label": "(16, SYCL, using namespace cl::sycl::access;); (4, SYCL, cl::sycl::queue queue;)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(33, OpenMP, omp_target_free(d_a, DefaultDeviceNum);); (15, OpenMP, d_a = (int *)omp_target_alloc(DataSize, DefaultDeviceNum);); (23, OpenMP, #pragma omp target update to(d_a)); (5, OpenMP, int DefaultDeviceNum = omp_get_default_device();); (16, OpenMP, omp_target_associate_ptr(p_a, d_a, DataSize, 0, DefaultDeviceNum);); (24, OpenMP, omp_target_disassociate_ptr(p_a, DefaultDeviceNum);)"}
{"predict": "(14,OpenMP,#pragma omp parallel for), (44,OpenMP,#pragma omp parallel for)", "label": "(51, OpenCL, clReleaseEvent(ev[i]);); (24, OpenCL, clEnqueueNDRangeKernel(queue, kernel, 1, NULL, globals, locals, 2, &ev[0], &ev[2]);); (30, OpenCL, buf_data[0] = clEnqueueMapBuffer(queue, buf[0], CL_FALSE, 0, 0, BUFFERSIZE*sizeof(int), 1, &ev[2], NULL, NULL);); (14, OpenCL, clEnqueueWriteBuffer(queue, buf[0], CL_FALSE, 0, BUFFERSIZE*sizeof(int), (void *)cpu_src, 1, &ev[0], &ev[1]);)"}
{"predict": "(28,OpenMP,#pragma omp parallel for private(perm_u, mylsize, myusize, ldu, index, flag, gridid, size)), (63,OpenMP,#pragma omp parallel for private(perm_u, mylsize, myusize, ldu, index, flag, gridid, size))", "label": "(50, MPI, MPI_Allreduce(MPI_IN_PLACE, &(flag), 1, mpi_int_t, MPI_MAX, grid3d->zscp.comm);); (105, MPI, MPI_Allreduce(  MPI_IN_PLACE, myusize, nsupers, mpi_int_t, MPI_MAX, grid->comm );); (102, MPI, MPI_Allreduce( MPI_IN_PLACE, mylsize, nsupers, mpi_int_t, MPI_MAX, grid->comm );); (50, MPI, MPI_Bcast( &(size), 1, mpi_int_t, gridid,  grid3d->zscp.comm);); (48, MPI, MPI_Allreduce(MPI_IN_PLACE, &(gridid), 1, mpi_int_t, MPI_MAX, grid3d->zscp.comm);)"}
{"predict": "(26,MPI,*w = MPI_Wtime();); (30,OpenMP,#pragma omp parallel for), (35,OpenMP,#pragma omp parallel for)", "label": "(15, CUDA, checkCudaErrors(cudaMalloc((void **)&ringbuf, sizeof(qsortRingbuf)));); (28, CUDA, checkCudaErrors(cudaEventRecord(ev1));); (40, CUDA, checkCudaErrors(cudaEventRecord(ev2));); (38, CUDA, checkCudaErrors(cudaGetLastError());); (61, CUDA, checkCudaErrors(cudaFree(ringbuf));); (38, CUDA, checkCudaErrors(cudaDeviceSynchronize());); (21, CUDA, checkCudaErrors(cudaEventCreate(&ev1));); (21, CUDA, checkCudaErrors(cudaEventCreate(&ev2));); (58, CUDA, checkCudaErrors(cudaFree(gpustack));)"}
{"predict": "(1,MPI,int rank, size, i, rc, errclass, toterrs, errs = 0;); (8,MPI,MPI_Comm_rank(MPI_COMM_WORLD, &rank); MPI_Comm_size(MPI_COMM_WORLD, &size);); (16,MPI,MPI_Comm_split(MPI_COMM_WORLD, rank != 1, rank, &newcomm);); (21,MPI,MPI_Bcast(sendbuf, 10 * size, MPI_CHAR, 0, newcomm);); (42,MPI,MPI_Bcast(sendbuf, 100000 * size, MPI_CHAR, 0, newcomm);); (59,MPI,MPI_Reduce(&errs, &toterrs, 1, MPI_INT, MPI_SUM, 0, newcomm););", "label": "(21, MPI, MPI_Comm_create_group(MPI_COMM_WORLD, newgroup, 0, &newcomm);); (19, MPI, MPI_Group_excl(world, 1, deadprocs, &newgroup);); (7, MPI, MPI_Init(&argc, &argv);); (82, MPI, MPI_Group_free(&newgroup);); (84, MPI, MPI_Finalize();); (53, MPI, rc = MPI_Scatter(sendbuf, 100000, MPI_CHAR, rbuf, 100000, MPI_CHAR, 0, MPI_COMM_WORLD);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (76, MPI, MPI_Comm_free(&newcomm);); (3, MPI, MPI_Comm_set_errhandler(MPI_COMM_WORLD, MPI_ERRORS_RETURN);); (76, MPI, MPI_Group_free(&world);); (25, MPI, rc = MPI_Scatter(sendbuf, 10, MPI_CHAR, rbuf, 10, MPI_CHAR, 0, MPI_COMM_WORLD);); (4, MPI, MPI_Abort(MPI_COMM_WORLD, 1);); (26, MPI, MPI_Error_class(rc, &errclass);); (5, MPI, MPI_Comm_group(MPI_COMM_WORLD, &world);)"}
{"predict": "(8,OpenMP,#pragma omp parallel for), (11,OpenMP,var++);", "label": "(6, OpenMP, #pragma omp parallel num_threads(8) shared(var)); (7, OpenMP, omp_set_nest_lock(&lock);); (9, OpenMP, omp_unset_nest_lock(&lock);); (1, OpenMP, omp_init_nest_lock(&lock);); (13, OpenMP, omp_destroy_nest_lock(&lock);)"}
{"predict": "(4,OpenMP,#pragma omp parallel for)", "label": "(8, CUDA, sim->CUDAConfig().device_id = device;)"}
{"predict": "0)\n128:             {\n129: \t\tsUPanelUpdate(k, factStat->IrecvPlcd_D, factored_U,\n130: \t\t\t      comReqss[offset]->L_diag_blk_recv_req,\n131: \t\t\t      dFBufs[offset]->BlockLFactor, grid, LUstruct, SCT);\n132:                 factored_U[k] = 1;\n133:             }\n134:         }\n135:         double t_apu = SuperLU_timer_();\n136:         printf(\"Level %d, apt = %f, apu = %f\\n\", topoLvl, t_apt, t_apu);\n137:     }\n138: \n139:     /* Cleanup. */\n140:     if (nnodes > 1)\n141:     {\n142:         for (int_t k0 = 0; k0 < eTreeTopLims[1]; ++k0)\n143:         {\n144:             int_t k = perm_c_supno[k0];\n145:             sDiagFactCleanUp(k, dFBufs[k0]->BlockUFactor, dFBufs[k0]->BlockLFactor,\n146:                             comReqss[k0]->U_diag_blk_recv_req,\n147:                             comReqss[k0]->L_diag_blk_recv_req,\n148:                             comReqss[k0]->U_diag_blk_send_req,\n149:                             comReqss[k0]->L_diag_blk_send_req,\n150:                             stat);\n151:         }\n152:     }\n153:     return 0;\n154: }", "label": "(203, OpenMP, #pragma omp parallel); (206, OpenMP, #pragma omp for schedule(dynamic,2) nowait)"}
{"predict": "(21,OpenMP,#pragma omp parallel for private(i))", "label": "(19, OpenMP, #pragma omp parallel for)"}
{"predict": "(4,OpenMP,#pragma omp parallel for)\n(7,OpenMP,#pragma omp parallel for)\n(10,OpenMP,#pragma omp parallel for)\n(13,OpenMP,#pragma omp parallel for)\n(16,OpenMP,#pragma omp parallel for)\n(19,OpenMP,#pragma omp parallel for)\n(22,OpenMP,#pragma omp parallel for)\n(25,OpenMP,#pragma omp parallel for)\n(28,OpenMP,#pragma omp parallel for)\n(32,OpenMP,#pragma omp parallel for)\n(35,OpenMP,#pragma omp parallel for)\n(38,OpenMP,#pragma omp parallel for)\n(40,OpenMP,#pragma omp parallel for)", "label": "(24, OpenCL, test_clEnqueueCopyBuffer(&clEnqueueCopyBufferData[i]);); (27, OpenCL, test_clEnqueueCopyBufferRect(&clEnqueueCopyBufferRectData[i]);); (6, OpenCL, test_clEnqueueWriteBuffer(&clEnqueueWriteBufferData[i]);); (41, OpenCL, test_clEnqueueUnmapMemObject(&clEnqueueUnmapMemObjectData[i]);); (8, OpenCL, test_clEnqueueReadBufferRect(&clEnqueueReadBufferRectData[i]);); (-1, OpenCL, test_clEnqueueReadBuffer(&clEnqueueReadBufferData[i]);); (14, OpenCL, test_clEnqueueFillBuffer(&clEnqueueFillBufferData[i]);); (25, OpenCL, test_clEnqueueMapBuffer(&clEnqueueMapBufferData[i]);); (8, OpenCL, test_clEnqueueWriteBufferRect(&clEnqueueWriteBufferRectData[i]);)"}
{"predict": "(14,OpenMP,#pragma omp parallel sections default(none) private(argc,b) firstprivate(argv) if(argc > 0) num_threads(ee) copyin(a) proc_bind(spread) reduction(|: c,d) reduction(*: e) lastprivate(argv))", "label": "(7, OpenMP, #pragma omp parallel sections reduction(task,*:f)); (14, OpenMP, #pragma omp parallel sections default(none), private(argc, b) firstprivate(argv) if (argc > 0) num_threads(ee) copyin(a) proc_bind(spread) reduction(| : c, d) reduction(* : e) lastprivate(argv)); (2, OpenMP, #pragma omp threadprivate(a)); (16, OpenMP, #pragma omp section)"}
{"predict": "(9,OpenMP,#pragma omp parallel for)", "label": "(15, OpenMP, #pragma omp target teams distribute parallel for collapse(2) depend(out:image[y_start]) nowait); (6, OpenMP, double st = omp_get_wtime();); (28, OpenMP, double et = omp_get_wtime();); (25, OpenMP, #pragma omp taskwait); (5, OpenMP, #pragma omp target data map(alloc:image[0:width*height])); (18, OpenMP, #pragma omp target update from(image[block*block_size:block_size]))"}
{"predict": "(31,MPI,*w = MPI_Wtime();); (32,OpenMP,#pragma omp parallel for), (51,OpenMP,#pragma omp parallel for), (57,OpenMP,#pragma omp parallel for), (61,OpenMP,#pragma omp parallel for)", "label": "(36, MPI, MPI_Type_size(c_info->s_data_type, &s_size);); (36, MPI, MPI_Type_size(c_info->r_data_type, &r_size);)"}
{"predict": "(12,OpenMP,#pragma omp parallel for), (14,OpenMP,#pragma omp parallel for), (15,OpenMP,#pragma omp parallel for), (16,OpenMP,#pragma omp parallel for), (17,OpenMP,#pragma omp parallel for), (18,OpenMP,#pragma omp parallel for), (19,OpenMP,#pragma omp parallel for), (20,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for), (23,OpenMP,#pragma omp parallel for), (24,OpenMP,#pragma omp parallel for), (25,OpenMP,#pragma omp parallel for), (26,OpenMP,#pragma omp parallel for)", "label": "(13, CUDA, PyObject* shape = PyCudaImage_GetShape(self, closure);)"}
{"predict": "(13,MPI,if (rank == 0) { MPI_Bcast(buf, 64 * 129, MPI_CHAR, 0, MPI_COMM_WORLD); } else { MPI_Recv(buf, 64 * 129, MPI_CHAR, 0, MPI_COMM_WORLD, MPI_COMM_WORLD, MPI_STATUS_IGNORE); })", "label": "(31, MPI, MPI_Type_commit(&newtype);); (9, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (42, MPI, MPI_Send(buf, 1, newtype, 1, i, MPI_COMM_WORLD);); (8, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (66, MPI, MPI_Type_free(&newtype);); (24, MPI, MPI_Type_vector(i, 128, 129, MPI_CHAR, &newtype);); (70, MPI, MPI_Recv(&recv_errs, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);); (76, MPI, MPI_Send(&errs, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);); (39, MPI, MPI_Recv(buf, 1, newtype, 0, i, MPI_COMM_WORLD, MPI_STATUS_IGNORE);)"}
{"predict": ":             is_contig = buftype_is_contig && filetype_is_contig;\n129:             /* only perform contiguous I/O if the buffer is contiguous and\n130:              * the file type is contiguous */\n131:             if (is_contig) {\n132:                 /* if there is no interleaving, then I can do contiguous I/O */\n133:                 ADIOI_Fill_buffer(fd, buf, count, datatype, file_ptr_type, offset,\n134:                                   status, error_code);\n135:                 ADIOI_Flush(fd, status, error_code);\n136:                 return;\n137:             }\n138:         }\n139:     }\n140: \n141:     /* determine whether to use the alltoallw collective or not */\n142:     /* if alltoallw is not supported, then use point-to-point */\n143:     if (!fd->hints->alltoallw_enabled) {\n144:         ADIOI_Fill_buffer(fd, buf, count, datatype, file_ptr_type, offset,\n145:                           status, error_code);\n146:         ADIOI_Flush(fd, status, error_code);\n147:         return;\n148:     }\n149: \n150:     /* if alltoallw is supported, then use it */\n151:     ADIOI_Get_alltoallw_counts(fd, &alltoallw_counts, &client_alltoallw_counts,\n152:                                 &agg_alltoallw_counts, &agg_disp, &agg_dtype,\n153:                                 &cb_buf, &client_comm_dtype_arr, &agg_comm_dtype_arr,\n154:                                 &client_comm_sz_arr, &agg_comm_sz_arr, &agg_rank);\n155: \n156:     ADIOI_Initiate_aggregators(fd, agg_rank, &cb_buf, &client_comm_dtype_arr,\n157:                                 &agg_comm_dtype_arr, &client_comm_sz_arr,\n158:                                 &agg_comm_sz_arr, &agg_comm_requests, &agg_comm_statuses,\n159:                                 &aggs_client_count);\n160: \n161:     ADIOI_Initiate_clients(fd, rdwr, &cb_buf, &client_comm", "label": "(106, MPI, MPI_Allgather(st_end_offset, 2, ADIO_OFFSET, all_st_end_offsets, 2, ADIO_OFFSET, fd->comm);); (71, MPI, MPI_Comm_rank(fd->comm, &myrank);); (527, MPI, MPI_Type_free(&client_comm_dtype_arr[i]);); (587, MPI, MPI_Type_free(&fd->file_realm_types[0]);); (67, MPI, MPI_Comm_size(fd->comm, &nprocs);); (589, MPI, MPI_Type_free(&fd->file_realm_types[i]);); (155, MPI, MPI_Type_get_extent(datatype, &lb, &extent);); (307, MPI, MPI_Type_size_x(agg_comm_dtype_arr[i], &size);); (301, MPI, MPI_Type_free(&agg_dtype);)"}
{"predict": "6:     mfu_free(&senddisps);\n137:     mfu_free(&keys);\n138:     mfu_free(&ltr);\n139: }", "label": "(99, MPI, MPI_Alltoall(sendcounts, 1, MPI_INT, recvcounts, 1, MPI_INT, MPI_COMM_WORLD);); (46, MPI, MPI_Comm_size(MPI_COMM_WORLD, &ranks);); (40, MPI, MPI_Type_free(&keytype);)"}
{"predict": "(13,OpenMP,#pragma omp parallel for private(sum)), (25,OpenMP,sum += data[k + j*z + i*z*y];)", "label": "(13, OpenMP, #pragma omp parallel default(none) shared(data,x,y,z,repeats))"}
{"predict": "(11,MPI,mpi_world_communicator.MaxAll(local, output);)", "label": "(2, MPI, MPIDataCommunicator mpi_world_communicator(MPI_COMM_WORLD);)"}
{"predict": "(aux>0) vector[i] -= 2*vector[index[i]];\n140:           else       vector[i] -= 2*aux;\n141:         }\n142:       }\n143:       break;\n144: \n145:     case INS_HEAVY:\n146:       for (iter=0; iter<iterations; iter+=2) {\n147:         for (i=0; i<vector_length; i++) {\n148:           aux = -(3 - (i&7));\n149:           if (aux>0) vector[i] -= 2*vector[index[i]];\n150:           else       vector[i] -= 2*aux;\n151:         }\n152:         for (i=0; i<vector_length; i++) {\n153:           aux = (3 - (i&7));\n154:           if (aux>0) vector[i] -= 2*vector[index[i]];\n155:           else       vector[i] -= 2*aux;\n156:         }\n157:       }\n158:       break;\n159: \n160:     default:\n161:       printf(\"Wrong branch type: %d\\n\", btype);\n162:       error = 1;\n163:       bail_out(error);\n164:   }\n165: \n166:   branch_time = wtime() - branch_time;\n167: \n168:   /* verify results */\n169: \n170:   total = 0;\n171:   for (i=0; i<vector_length; i++) {\n172:     total += vector[i];\n173:   }\n174:   total_sum = 0;\n175:   MPI_Reduce(&total, &total_sum, 1, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD);\n176: \n177:   /* compute reference result */\n178: \n179:   total_ref = 0;\n180:   for (i=0; i<vector_length; i++) {\n181:     total_ref += 3 - (i&7);\n182:   }\n183:   total_ref *=", "label": "(207, MPI, MPI_Reduce(&total, &total_sum, 1, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD);); (230, MPI, MPI_Finalize();); (79, MPI, MPI_Bcast(&iterations,    1, MPI_INT, root, MPI_COMM_WORLD);); (21, MPI, MPI_Init(&argc,&argv);); (76, MPI, MPI_Bcast(&vector_length, 1, MPI_INT, root, MPI_COMM_WORLD);); (77, MPI, MPI_Bcast(&btype,         1, MPI_INT, root, MPI_COMM_WORLD);); (93, MPI, MPI_Barrier(MPI_COMM_WORLD);); (19, MPI, MPI_Comm_size(MPI_COMM_WORLD, &Num_procs);); (17, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &my_ID);)"}
{"predict": "(29,OpenMP,#pragma omp parallel for)", "label": "(36, CUDA, CUDA_CHECK(cudaStreamSynchronize(stream));); (38, CUDA, CUDA_CHECK(cudaFree(d_data));)"}
{"predict": "(11,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for)", "label": "(44, MPI, MPI_Buffer_detach(&bptr, &bl);); (24, MPI, MPI_Recv(b, 10, MPI_INT, i, 27 + j, MPI_COMM_WORLD, &status);); (14, MPI, MPI_Bsend(a, 10, MPI_INT, 0, 27 + j, MPI_COMM_WORLD);); (7, MPI, MPI_Buffer_attach(buf, BUFSIZE);); (4, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);); (4, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);)"}
{"predict": "(1,OpenMP,#pragma omp parallel for collapse(2)), (22,OpenMP,#pragma omp barrier)", "label": "(2, CUDA, __shared__ int sharedMemory [BLOCK_SIZE] [BLOCK_SIZE + 1];)"}
{"predict": "(34,OpenMP,#pragma omp parallel for), (46,OpenMP,#pragma omp parallel for), (53,OpenMP,#pragma omp parallel for), (64,OpenMP,#pragma omp parallel for)", "label": "(57, MPI, MPI_Send(sendBuffer, n_send, MPI_DOUBLE, neighbors[i], MPI_MY_TAG, MPI_COMM_WORLD);); (15, MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);); (36, MPI, MPI_Irecv(x_external, n_recv, MPI_DOUBLE, neighbors[i], MPI_MY_TAG, MPI_COMM_WORLD, request+i);); (14, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);)"}
{"predict": "(11,OpenMP,#pragma omp parallel for), (22,OpenMP,#pragma omp parallel for)", "label": "(5, OpenMP, #pragma omp parallel num_threads(4)); (11, OpenMP, #pragma omp for schedule(static) nowait); (8, OpenMP, rank = omp_get_thread_num();); (16, OpenMP, #pragma omp atomic); (20, OpenMP, #pragma omp for schedule(static))"}
{"predict": "(7,OpenMP,#pragma omp parallel for)", "label": "(4, OpenMP, #pragma omp parallel for)"}
{"predict": "(5,MPI,*w = MPI_Wtime();); (8,OpenMP,#pragma omp parallel for)", "label": "(1, OpenMP, #pragma omp parallel for ordered ordered); (15, OpenMP, #pragma omp parallel for order(concurrent)); (8, OpenMP, #pragma omp parallel for order(none); (4, OpenMP, #pragma omp parallel for order(); (0, OpenMP, #pragma omp parallel for order); (8, OpenMP, #pragma omp parallel for order(concurrent)"}
{"predict": "(4,OpenMP,#pragma omp parallel for)", "label": "(14, OpenMP, printf(\"Thread %d of %d\\n\", omp_get_thread_num(), omp_get_num_threads());)"}
{"predict": "(3, MPI, #include <mpi.h>) (14, MPI, MPI_Init(&argc, &argv);) (18, MPI, MPI_Comm_size(MPI_COMM_WORLD, &num_devices);) (21, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);) (62, MPI, MPI_Finalize();)", "label": "(43, CUDA, CUDA_CHECK_GOTO(cudaGetDevice(&self->device), free_mpool, status);); (15, CUDA, cuda_st = cudaGetDeviceCount(&num_devices);)"}
{"predict": "(10,MPI, MPI_Init(NULL, NULL);) (11,MPI, MPI_Comm_rank(MPI_COMM_WORLD, &rank);) (12,MPI, MPI_Comm_size(MPI_COMM_WORLD, &size);) (20,MPI, MPI_Allreduce(&sum, &out, 1, MPI_FLOAT, MPI_SUM, MPI_COMM_WORLD);) (38,MPI, MPI_Finalize();)", "label": "(25, CUDA, assert(cudaMemcpyAsync(out.data(), d_out, out_bytes, cudaMemcpyDeviceToHost, gpu_device.stream()) == cudaSuccess);); (11, CUDA, cudaMalloc((void**)(&d_out), out_bytes);); (9, CUDA, cudaMalloc((void**)(&d_in1), in1_bytes);); (11, CUDA, cudaMemcpy(d_in1, in1.data(), in1_bytes, cudaMemcpyHostToDevice);); (39, CUDA, cudaFree(d_out);); (21, CUDA, assert(cudaStreamSynchronize(gpu_device.stream()) == cudaSuccess);); (36, CUDA, cudaFree(d_in1);)"}
{"predict": "(16,OpenMP,#pragma omp parallel for)", "label": "(14, OpenMP, #pragma omp teams default(none), private(argc,b) num_teams(f) firstprivate(argv) reduction(| : c, d) reduction(* : e) thread_limit(f+g)); (6, OpenMP, #pragma omp target); (2, OpenMP, #pragma omp threadprivate(a)); (5, OpenMP, #pragma omp teams)"}
{"predict": "=%d,k=%d,nsupc=%d,il=%d,cnt=%d\\n\",mype,bid,tid,d_colnummod[d_mymaskstartmod[tid]],lib,k,nsupc,il,cnt);\n111:                     dready_x[il + k * grid->npcol] += dready_lsum[d_colnummod[d_mymaskstartmod[tid]] * 2 + wm_val];\n112:                     //printf(\"dready_x[%d]=%f\\n\",il+k*grid->npcol,dready_x[il+k*grid->npcol]);\n113:                     bmod_tmp = 1;\n114:                     for (j = 0; j < cnt; j++) {\n115:                         int tmp = URtree_ptr[lib].dest[j];\n116:                         int tmpcol = URtree_ptr[lib].destCol[j];\n117:                         dready_lsum[tmp * 2 + 1] += dready_lsum[d_colnummod[d_mymaskstartmod[tid]] * 2 + wm_val];\n118:                         dready_lsum[tmp * 2] = 0;\n119:                         //printf(\"dready_lsum[%d]=%f\\n\",tmp*2+1,dready_lsum[tmp*2+1]);\n120:                     }\n121:                     //printf(\"dready_lsum[%d]=%f\\n\",tmp*2,dready_lsum[tmp*2]);\n122:                     dready_lsum[d_colnummod[d_mymaskstartmod[tid]] * 2 + wm_val] = 0;\n123:                     //printf(\"dready_lsum[%d]=%f\\n\",d_colnummod[d_mymaskstartmod[tid]]*2+wm_val,dready_lsum[d_colnummod[d_mymaskstartmod[tid]]*2+wm_val]);\n124:                     //printf(\"recv2,%d,%d,%d,%d\\n\",\n125:                     //       mype,d_colnummod[d_mymaskstartmod[tid]]*2,wm_val,lib);\n126:                     bmod[d_colnummod[d_mymaskstartmod[tid]] * 2 + wm_val] += bmod_tmp;\n127:                     //printf(\"bmod[%d]=%d\\n\",", "label": "(341, CUDA, __shared__ int cur_send_num;); (339, CUDA, __shared__ int recv_num, finish_num;)"}
{"predict": "(21,OpenMP,#pragma omp parallel for), (46,OpenMP,#pragma omp parallel for), (71,OpenMP,#pragma omp parallel for)", "label": "(4, TBB, tbb::flow::queue_node<T> q2(g);); (70, TBB, tbb::flow::make_edge( q2, q3 );); (67, TBB, tbb::flow::queue_node<T> q3(g);); (37, TBB, tbb::flow::make_edge( q, q2 );); (56, TBB, tbb::flow::remove_edge( q, q2 );); (87, TBB, tbb::flow::remove_edge( q,  q2 );); (-3, TBB, tbb::flow::queue_node<T> q(g);); (-5, TBB, tbb::flow::graph g;)"}
{"predict": "(12,OpenMP,#pragma omp parallel for)", "label": "(23, OpenCL, clReleaseEvent(event);)"}
{"predict": "(46,OpenMP,#pragma omp parallel for)", "label": "(25, MPI, MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);); (25, MPI, MPI_Get_processor_name(mpi_name, &mpi_namelen);); (22, MPI, MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);)"}
{"predict": "(31,MPI,*w = MPI_Wtime(););", "label": "(43, OpenCL, clReleaseMemObject(imageDesc.mem_object);); (14, OpenCL, imageDesc.mem_object = clCreateBuffer(&context, CL_MEM_READ_WRITE, imageDesc.image_height * imageDesc.image_width, nullptr, &retVal);)"}
{"predict": "3, i2, i1) + I3D(y1, m1k, m2k, i3, i2, i1+2) + x2 + y2);\n78:       }\n79:     }\n80:   }\n81:   if (timeron) timer_stop(T_rprj3);\n82: \n83:   if (m3k != 3) {\n84:     #pragma omp parallel for default(shared)\n85:     for (j2 = 1; j2 < m2j-1; j2++) {\n86:       for (j1 = 1; j1 < m1j-1; j1++) {\n87:         I3D(or, m1k, m2k, 0, j2, j1) = I3D(or, m1k, m2k, 2, j2, j1);\n88:         I3D(or, m1k, m2k, m3k-1, j2, j1) = I3D(or, m1k, m2k, m3k-3, j2, j1);\n89:       }\n90:     }\n91:   }\n92: \n93:   if (m2k != 3) {\n94:     #pragma omp parallel for default(shared)\n95:     for (j1 = 1; j1 < m1j-1; j1++) {\n96:       I3D(or, m1k, m2k, 0, 0, j1) = I3D(or, m1k, m2k, 2, 0, j1);\n97:       I3D(or, m1k, m2k, 0, m2k-1, j1) = I3D(or, m1k, m2k, 2, m2k-1, j1);\n98:       I3D(or, m1k, m2k, m3k-1, 0, j1) = I3D(or, m1k, m2k, m3k-3, 0, j1);\n99:       I3D(or, m1k, m2k, m3k-1, m2k", "label": "(114, OpenMP, omp_target_free(y1, omp_get_default_device());); (41, OpenMP, #pragma omp teams distribute parallel for default(shared) collapse(3)); (39, OpenMP, #pragma omp target is_device_ptr(x1, y1) map(tofrom: or[0:m3k*m2k*m1k]) map(tofrom: os[0:m3j*m2j*m1j])); (13, OpenMP, y1 = (double*)omp_target_alloc(m3k*m2k*m1k*sizeof(double), omp_get_default_device());)"}
{"predict": "(24,OpenMP,#pragma omp parallel for), (40,OpenMP,#pragma omp parallel for)", "label": "(9, MPI, MPI_Finalize();); (1, MPI, MPI_Init(&argc,&argv);)"}
{"predict": "gValue[writeAddress] = finalValue;\n155:     gIndex[writeAddress] = finalIndex;\n156:   }\n157: }\n158: \n159: void ivfpq_topk_residual_precomputed_host(\n160:   const uint8_t* data,\n161:   const float* part1,\n162:   const float* part2,\n163:   const ll_t* cells,\n164:   const float* baseSims,\n165:   const uint8_t* isEmpty,\n166:   const ll_t* cellStart,\n167:   const ll_t* cellSize,\n168:   const ll_t* totalSize,\n169:   float* gValue,\n170:   ll_t* gIndex,\n171:   int nData, int nQuery, int nProbe, int nCandidates\n172: ) {\n173:   ivfpq_topk_residual_precomputed<<<nQuery, _TPB_>>>(data, part1, part2, cells, baseSims, isEmpty, cellStart, cellSize, totalSize, gValue, gIndex, nData, nQuery, nProbe, nCandidates);\n174: }\n175:", "label": "(17, CUDA, extern __shared__ _VOLATILE_ float sMem[];)"}
{"predict": "(6,MPI,*w = MPI_Wtime();); (14,OpenMP,#pragma omp parallel for), (17,MPI,*e = MPI_Wtime();)", "label": "(1, SYCL, EasyCL *cl = EasyCL::createForFirstGpuOtherwiseCpu();)"}
